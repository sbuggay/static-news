<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720688470030" as="style"/><link rel="stylesheet" href="styles.css?v=1720688470030"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2406.03372">Training of Physical Neural Networks</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>Anon84</span> | <span>39 comments</span></div><br/><div><div id="40928849" class="c"><input type="checkbox" id="c-40928849" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#40928577">next</a><span>|</span><label class="collapse" for="c-40928849">[-]</label><label class="expand" for="c-40928849">[21 more]</label></div><br/><div class="children"><div class="content">Last time I read about this the main practical difficulty was model transferability.<p>The very thing that makes it so powerful and efficient is also the thing that make it uncopiable, because sensitivity to tiny physical differences in the devices inevitably gets encoded into the model during training.<p>It seems intuitive this is an unavoidable, fundamental problem. Maybe that scares away big tech, but I quite like the idea of having invaluable, non-transferable, irreplaceable little devices. Not so easily deprecated by technological advances, flying in the face of consumerism, getting better with age, making people want to hold onto things.</div><br/><div id="40931801" class="c"><input type="checkbox" id="c-40931801" checked=""/><div class="controls bullet"><span class="by">alexpotato</span><span>|</span><a href="#40928849">parent</a><span>|</span><a href="#40931829">next</a><span>|</span><label class="collapse" for="c-40931801">[-]</label><label class="expand" for="c-40931801">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Last time I read about this the main practical difficulty was model transferability.<p>There is a great write up of this in this old blog post: <a href="https:&#x2F;&#x2F;www.damninteresting.com&#x2F;on-the-origin-of-circuits&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.damninteresting.com&#x2F;on-the-origin-of-circuits&#x2F;</a></div><br/></div></div><div id="40931829" class="c"><input type="checkbox" id="c-40931829" checked=""/><div class="controls bullet"><span class="by">robertsdionne</span><span>|</span><a href="#40928849">parent</a><span>|</span><a href="#40931801">prev</a><span>|</span><a href="#40932817">next</a><span>|</span><label class="collapse" for="c-40931829">[-]</label><label class="expand" for="c-40931829">[1 more]</label></div><br/><div class="children"><div class="content">This is “Mortal Computation” coined in Hinton’s The Forward-Forward Algorithm: Some Preliminary Investigations <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.13345" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.13345</a>.</div><br/></div></div><div id="40932817" class="c"><input type="checkbox" id="c-40932817" checked=""/><div class="controls bullet"><span class="by">dsabanin</span><span>|</span><a href="#40928849">parent</a><span>|</span><a href="#40931829">prev</a><span>|</span><a href="#40932457">next</a><span>|</span><label class="collapse" for="c-40932817">[-]</label><label class="expand" for="c-40932817">[4 more]</label></div><br/><div class="children"><div class="content">Couldn’t you still copy by training a new network on a new device to have same outputs for the same inputs as the original?</div><br/><div id="40932924" class="c"><input type="checkbox" id="c-40932924" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40932817">parent</a><span>|</span><a href="#40932457">next</a><span>|</span><label class="collapse" for="c-40932924">[-]</label><label class="expand" for="c-40932924">[3 more]</label></div><br/><div class="children"><div class="content">Yes, but training is the most expensive part of ML, for example GPT-3 is estimated to cost something like 1-4 million USD.<p>With ANN you can do it one time and then clone the result for negligible energy cost.<p>Maybe training a batch of PNNs in parallel could save some of the energy cost, but I don&#x27;t know how feasible that is considering they could behave slightly differently during training causing divergence... Now that sarcastic comment at the bottom of this thread is starting to sound relevant &quot;Schools&quot;.</div><br/><div id="40934289" class="c"><input type="checkbox" id="c-40934289" checked=""/><div class="controls bullet"><span class="by">kmmlng</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40932924">parent</a><span>|</span><a href="#40932457">next</a><span>|</span><label class="collapse" for="c-40934289">[-]</label><label class="expand" for="c-40934289">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Yes, but training is the most expensive part of ML, for example GPT-3 is estimated to cost something like 1-4 million USD.<p>That entirely depends on how many inferences the model will perform during its lifecycle. You can find different estimates for the energy consumption of ChatGPT, but they range from something like 500-1000 MWh a day. Assuming an electricity price of $0.165 per kWh, that would put you at roughly $80,000 to a $160,000 a day.<p>Even at the lower end of $80,000 a day, you&#x27;ll reach your $4 Million in just 50 days.</div><br/><div id="40934455" class="c"><input type="checkbox" id="c-40934455" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40934289">parent</a><span>|</span><a href="#40932457">next</a><span>|</span><label class="collapse" for="c-40934455">[-]</label><label class="expand" for="c-40934455">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a proportional comparison, n simultaneous users to 1 training. How many users across how many GPUs is that 80k?<p>With PNN you would have to multiply n by 1-4 million, training cost explodes.</div><br/></div></div></div></div></div></div></div></div><div id="40932457" class="c"><input type="checkbox" id="c-40932457" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#40928849">parent</a><span>|</span><a href="#40932817">prev</a><span>|</span><a href="#40929809">next</a><span>|</span><label class="collapse" for="c-40932457">[-]</label><label class="expand" for="c-40932457">[1 more]</label></div><br/><div class="children"><div class="content">You can regularize the networks to make them transfer easier. I can&#x27;t remember the abstract&#x27;s title off the top of my head though.</div><br/></div></div><div id="40929809" class="c"><input type="checkbox" id="c-40929809" checked=""/><div class="controls bullet"><span class="by">trextrex</span><span>|</span><a href="#40928849">parent</a><span>|</span><a href="#40932457">prev</a><span>|</span><a href="#40929338">next</a><span>|</span><label class="collapse" for="c-40929809">[-]</label><label class="expand" for="c-40929809">[10 more]</label></div><br/><div class="children"><div class="content">Well, the brain is a physical neural network, and evolution seems to have figured out how to generate a (somewhat) copiable model. I bet we could learn a trick or two from biology here.</div><br/><div id="40930201" class="c"><input type="checkbox" id="c-40930201" checked=""/><div class="controls bullet"><span class="by">hansworst</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40929809">parent</a><span>|</span><a href="#40929978">next</a><span>|</span><label class="collapse" for="c-40930201">[-]</label><label class="expand" for="c-40930201">[3 more]</label></div><br/><div class="children"><div class="content">The way the brain does it is by giving users a largely untrained model that they themselves have to train over the next 20 years for it to be of any use.</div><br/><div id="40934719" class="c"><input type="checkbox" id="c-40934719" checked=""/><div class="controls bullet"><span class="by">wbillingsley</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40930201">parent</a><span>|</span><a href="#40934662">next</a><span>|</span><label class="collapse" for="c-40934719">[-]</label><label class="expand" for="c-40934719">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes. Foals are born (almost) able to walk. There are occasions where evolution baked the model into the genes.</div><br/></div></div><div id="40934662" class="c"><input type="checkbox" id="c-40934662" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40930201">parent</a><span>|</span><a href="#40934719">prev</a><span>|</span><a href="#40929978">next</a><span>|</span><label class="collapse" for="c-40934662">[-]</label><label class="expand" for="c-40934662">[1 more]</label></div><br/><div class="children"><div class="content">20 years of training is not enough.  Neuroscientists say 25.  According to my own experience, its more like 30.</div><br/></div></div></div></div><div id="40929978" class="c"><input type="checkbox" id="c-40929978" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40929809">parent</a><span>|</span><a href="#40930201">prev</a><span>|</span><a href="#40929338">next</a><span>|</span><label class="collapse" for="c-40929978">[-]</label><label class="expand" for="c-40929978">[6 more]</label></div><br/><div class="children"><div class="content">Some parts are copiable, but not the more abstract things like the human intellect, for lack of a better word.<p>We are not even born with what you might consider basic mental faculties, for example it might seem absurd, but we have to learn to see... We are born with the &quot;hardware&quot; for it, a visual cortex, an eye, all defined by our genes, but it&#x27;s actually trained from birth, there is even a feedback loop that causes the retina to physically develop properly.</div><br/><div id="40931817" class="c"><input type="checkbox" id="c-40931817" checked=""/><div class="controls bullet"><span class="by">alexpotato</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40929978">parent</a><span>|</span><a href="#40930177">next</a><span>|</span><label class="collapse" for="c-40931817">[-]</label><label class="expand" for="c-40931817">[2 more]</label></div><br/><div class="children"><div class="content">Another example:<p>Children who were &quot;raised in the wild&quot; or locked in a room by themselves have shown to be incapable of learning full human language.<p>The working theory is that our brains can only learn certain skills at certain times of brain development&#x2F;ages.</div><br/><div id="40932476" class="c"><input type="checkbox" id="c-40932476" checked=""/><div class="controls bullet"><span class="by">deepfriedchokes</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40931817">parent</a><span>|</span><a href="#40930177">next</a><span>|</span><label class="collapse" for="c-40932476">[-]</label><label class="expand" for="c-40932476">[1 more]</label></div><br/><div class="children"><div class="content">We should also consider the effects of trauma on those brains. If you’ve ever spent time around people with extreme trauma they are very much in their own heads and can’t focus outside themselves long enough to focus enough to learn anything. It definitely impacts intellectual capacity. Humans are social animals and anyone raised without proper socializing and intimacy and nurturing will inevitably end up traumatized.</div><br/></div></div></div></div><div id="40930177" class="c"><input type="checkbox" id="c-40930177" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40929978">parent</a><span>|</span><a href="#40931817">prev</a><span>|</span><a href="#40929338">next</a><span>|</span><label class="collapse" for="c-40930177">[-]</label><label class="expand" for="c-40930177">[3 more]</label></div><br/><div class="children"><div class="content">They raised some cats from birth in an environment with only vertically-oriented edges, none horizontal. Those cats could not see horizontally-oriented things. <a href="https:&#x2F;&#x2F;computervisionblog.wordpress.com&#x2F;2013&#x2F;06&#x2F;01&#x2F;cats-and-vision-is-vision-acquired-or-innate&#x2F;" rel="nofollow">https:&#x2F;&#x2F;computervisionblog.wordpress.com&#x2F;2013&#x2F;06&#x2F;01&#x2F;cats-and...</a><p>Likewise, kittens with an eye patch over an eye in the same time period remain blind in that eye forever.</div><br/><div id="40930933" class="c"><input type="checkbox" id="c-40930933" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40930177">parent</a><span>|</span><a href="#40932187">next</a><span>|</span><label class="collapse" for="c-40930933">[-]</label><label class="expand" for="c-40930933">[1 more]</label></div><br/><div class="children"><div class="content">Wow, that&#x27;s a horrific way of proving that theory.</div><br/></div></div><div id="40932187" class="c"><input type="checkbox" id="c-40932187" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40930177">parent</a><span>|</span><a href="#40930933">prev</a><span>|</span><a href="#40929338">next</a><span>|</span><label class="collapse" for="c-40932187">[-]</label><label class="expand" for="c-40932187">[1 more]</label></div><br/><div class="children"><div class="content">Geez poor kitties, but that is interesting.</div><br/></div></div></div></div></div></div></div></div><div id="40929338" class="c"><input type="checkbox" id="c-40929338" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40928849">parent</a><span>|</span><a href="#40929809">prev</a><span>|</span><a href="#40928577">next</a><span>|</span><label class="collapse" for="c-40929338">[-]</label><label class="expand" for="c-40929338">[3 more]</label></div><br/><div class="children"><div class="content">Reminds me of the evolutionary FPGA experiment that was dependent on magnetic flux or something. The same program wouldn&#x27;t work on a different FPGA.</div><br/><div id="40929761" class="c"><input type="checkbox" id="c-40929761" checked=""/><div class="controls bullet"><span class="by">cyberax</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40929338">parent</a><span>|</span><a href="#40929782">next</a><span>|</span><label class="collapse" for="c-40929761">[-]</label><label class="expand" for="c-40929761">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s the paper about it: <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;2737441_An_Evolved_Circuit_Intrinsic_in_Silicon_Entwined_With_Physics" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;2737441_An_Evolved_...</a><p>And a more approachable article: <a href="https:&#x2F;&#x2F;www.damninteresting.com&#x2F;on-the-origin-of-circuits&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.damninteresting.com&#x2F;on-the-origin-of-circuits&#x2F;</a></div><br/></div></div><div id="40929782" class="c"><input type="checkbox" id="c-40929782" checked=""/><div class="controls bullet"><span class="by">rusticpenn</span><span>|</span><a href="#40928849">root</a><span>|</span><a href="#40929338">parent</a><span>|</span><a href="#40929761">prev</a><span>|</span><a href="#40928577">next</a><span>|</span><label class="collapse" for="c-40929782">[-]</label><label class="expand" for="c-40929782">[1 more]</label></div><br/><div class="children"><div class="content">What thy did was overfitting. We later found other ways of getting around the issue.</div><br/></div></div></div></div></div></div><div id="40928577" class="c"><input type="checkbox" id="c-40928577" checked=""/><div class="controls bullet"><span class="by">ksd482</span><span>|</span><a href="#40928849">prev</a><span>|</span><a href="#40928676">next</a><span>|</span><label class="collapse" for="c-40928577">[-]</label><label class="expand" for="c-40928577">[4 more]</label></div><br/><div class="children"><div class="content"><i>PNNs resemble neural networks, however at least part of the system is analog rather than digital,
meaning that part or all the input&#x2F;output data is encoded continuously in a physical parameter, and the weights can also be physical,
with the ultimate goal of surpassing digital hardware in performance or efficiency.</i><p>I am trying to understand what format does a node take in PNNs. Is it a transistor? Or is it more complex than that? Or, is it a combination of a few things such as analog signal and some other sensors which work together to form a single node that looks like the one we are all familiar with?<p>Can anyone please help me understand what exactly is &quot;physical&quot; about PNNs?</div><br/><div id="40932472" class="c"><input type="checkbox" id="c-40932472" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#40928577">parent</a><span>|</span><a href="#40928711">next</a><span>|</span><label class="collapse" for="c-40932472">[-]</label><label class="expand" for="c-40932472">[1 more]</label></div><br/><div class="children"><div class="content">Here you go: <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;science&#x2F;2018&#x2F;07&#x2F;neural-network-implemented-with-light-instead-of-electrons&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;science&#x2F;2018&#x2F;07&#x2F;neural-network-imple...</a></div><br/></div></div><div id="40928711" class="c"><input type="checkbox" id="c-40928711" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40928577">parent</a><span>|</span><a href="#40932472">prev</a><span>|</span><a href="#40928667">next</a><span>|</span><label class="collapse" for="c-40928711">[-]</label><label class="expand" for="c-40928711">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just a general idea to implement the computation part of neurons directly in hardware instead of software. For example by calculating sums or products using voltages in circuits, i.e. analog computing. The actual implementation is up to the designer, who in turn will try to mimic a certain architecture.</div><br/></div></div></div></div><div id="40928676" class="c"><input type="checkbox" id="c-40928676" checked=""/><div class="controls bullet"><span class="by">Shawnecy</span><span>|</span><a href="#40928577">prev</a><span>|</span><a href="#40928529">next</a><span>|</span><label class="collapse" for="c-40928676">[-]</label><label class="expand" for="c-40928676">[2 more]</label></div><br/><div class="children"><div class="content">My knowledge in this area is incredibly limited, but I figured the paper would mention NanoWire Networks (NWNs) as an emerging physical neural network[0].<p>Last year, researchers from the University of Sydney and UCLA used NWNs to demonstrate online learning of handwritten digits with an accuracy of 93%.<p>[0] = <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41467-023-42470-5" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41467-023-42470-5</a></div><br/><div id="40932702" class="c"><input type="checkbox" id="c-40932702" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#40928676">parent</a><span>|</span><a href="#40928529">next</a><span>|</span><label class="collapse" for="c-40932702">[-]</label><label class="expand" for="c-40932702">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t implement a trainable network on hardware, it&#x27;s just creating a &quot;reservoir&quot; of associations between the inputs.</div><br/></div></div></div></div><div id="40932467" class="c"><input type="checkbox" id="c-40932467" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#40928529">prev</a><span>|</span><a href="#40928532">next</a><span>|</span><label class="collapse" for="c-40932467">[-]</label><label class="expand" for="c-40932467">[1 more]</label></div><br/><div class="children"><div class="content">&gt; These methods are typically slow because the number of gradient updates scales linearly with the number of learnable parameters in the network, posing a significant challenge for scaling up.<p>This is a pretty big problem, though if you use information-bottleneck training you can train each layer simultaneously.</div><br/></div></div><div id="40928532" class="c"><input type="checkbox" id="c-40928532" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#40932467">prev</a><span>|</span><a href="#40929348">next</a><span>|</span><label class="collapse" for="c-40928532">[-]</label><label class="expand" for="c-40928532">[7 more]</label></div><br/><div class="children"><div class="content">So it sounds like these PNNs are essentially analog implementations of neural nets? Seems like an odd choice of naming to call them &#x27;physical&#x27;.</div><br/><div id="40928930" class="c"><input type="checkbox" id="c-40928930" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#40928532">parent</a><span>|</span><a href="#40929089">next</a><span>|</span><label class="collapse" for="c-40928930">[-]</label><label class="expand" for="c-40928930">[5 more]</label></div><br/><div class="children"><div class="content">ANN is taken.</div><br/><div id="40932567" class="c"><input type="checkbox" id="c-40932567" checked=""/><div class="controls bullet"><span class="by">agarwaen163</span><span>|</span><a href="#40928532">root</a><span>|</span><a href="#40928930">parent</a><span>|</span><a href="#40930380">next</a><span>|</span><label class="collapse" for="c-40932567">[-]</label><label class="expand" for="c-40932567">[1 more]</label></div><br/><div class="children"><div class="content">Well, this is particularly frustrating because PNN is already taken as well, by the (imo) much more novel idea of Physics Informed Neural Networks (aka PINNs or PNNs).<p>Why this isn&#x27;t called Hardware Neural Nets is beyond me.</div><br/></div></div><div id="40930380" class="c"><input type="checkbox" id="c-40930380" checked=""/><div class="controls bullet"><span class="by">TheLoafOfBread</span><span>|</span><a href="#40928532">root</a><span>|</span><a href="#40928930">parent</a><span>|</span><a href="#40932567">prev</a><span>|</span><a href="#40929089">next</a><span>|</span><label class="collapse" for="c-40930380">[-]</label><label class="expand" for="c-40930380">[3 more]</label></div><br/><div class="children"><div class="content">I mean LoRA was taken too before LoRA became a thing</div><br/><div id="40930965" class="c"><input type="checkbox" id="c-40930965" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#40928532">root</a><span>|</span><a href="#40930380">parent</a><span>|</span><a href="#40929089">next</a><span>|</span><label class="collapse" for="c-40930965">[-]</label><label class="expand" for="c-40930965">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t mean globally, LoRA are at least in different domains. Artificial Neural Networks and Physical Neural Networks are both machine learning, discussion referring to both is highly probable, and the former far more established so it calling it an Analog Neural Network would never last long.</div><br/><div id="40933982" class="c"><input type="checkbox" id="c-40933982" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40928532">root</a><span>|</span><a href="#40930965">parent</a><span>|</span><a href="#40929089">next</a><span>|</span><label class="collapse" for="c-40933982">[-]</label><label class="expand" for="c-40933982">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t call analog NN an AnalNN either, as then the &quot;brain-gut axis&quot; people will throw a fit.</div><br/></div></div></div></div></div></div></div></div><div id="40929089" class="c"><input type="checkbox" id="c-40929089" checked=""/><div class="controls bullet"><span class="by">pessimizer</span><span>|</span><a href="#40928532">parent</a><span>|</span><a href="#40928930">prev</a><span>|</span><a href="#40929348">next</a><span>|</span><label class="collapse" for="c-40929089">[-]</label><label class="expand" for="c-40929089">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense as opposed to &quot;abstract.&quot; With the constant encoding and decoding that has to be done when things are going in an out of processors and storage (or sensors), digital processes are always in some sense simulations.</div><br/></div></div></div></div><div id="40929348" class="c"><input type="checkbox" id="c-40929348" checked=""/><div class="controls bullet"><span class="by">craigmart</span><span>|</span><a href="#40928532">prev</a><span>|</span><a href="#40928670">next</a><span>|</span><label class="collapse" for="c-40929348">[-]</label><label class="expand" for="c-40929348">[1 more]</label></div><br/><div class="children"><div class="content">Schools?</div><br/></div></div></div></div></div></div></div></body></html>