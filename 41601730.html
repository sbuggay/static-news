<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726909253206" as="style"/><link rel="stylesheet" href="styles.css?v=1726909253206"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/cupy/cupy">CuPy: NumPy and SciPy for GPU</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>tanelpoder</span> | <span>95 comments</span></div><br/><div><div id="41602197" class="c"><input type="checkbox" id="c-41602197" checked=""/><div class="controls bullet"><span class="by">gjstein</span><span>|</span><a href="#41603784">next</a><span>|</span><label class="collapse" for="c-41602197">[-]</label><label class="expand" for="c-41602197">[24 more]</label></div><br/><div class="children"><div class="content">The idea that this is a drop in replacement for numpy (e.g., `import cupy as np`) is quite nice, though I&#x27;ve gotten similar benefit out of using `pytorch` for this purpose. It&#x27;s a very popular and well-supported library with a syntax that&#x27;s similar to numpy.<p>However, the AMD-GPU compatibility for CuPy is quite an attractive feature.</div><br/><div id="41603116" class="c"><input type="checkbox" id="c-41603116" checked=""/><div class="controls bullet"><span class="by">ogrisel</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41602270">next</a><span>|</span><label class="collapse" for="c-41603116">[-]</label><label class="expand" for="c-41603116">[4 more]</label></div><br/><div class="children"><div class="content">Note that NumPy, CuPy and PyTorch are all involved in the definition of a shared subset of their API:<p><a href="https:&#x2F;&#x2F;data-apis.org&#x2F;array-api&#x2F;" rel="nofollow">https:&#x2F;&#x2F;data-apis.org&#x2F;array-api&#x2F;</a><p>So it&#x27;s possible to write array API code that consumes arrays from any of those libraries and delegate computation to them without having to explicitly import any of them in your source code.<p>The only limitation for now is that PyTorch (and to some lower extent cupy as well) array API compliance is still incomplete and in practice one needs to go through this compatibility layer (hopefully temporarily):<p><a href="https:&#x2F;&#x2F;data-apis.org&#x2F;array-api-compat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;data-apis.org&#x2F;array-api-compat&#x2F;</a></div><br/><div id="41603215" class="c"><input type="checkbox" id="c-41603215" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41603116">parent</a><span>|</span><a href="#41603381">next</a><span>|</span><label class="collapse" for="c-41603215">[-]</label><label class="expand" for="c-41603215">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting to see hardware&#x2F;software&#x2F;API co-development in practice again.<p>The last time I think this happen at market-scale was early 3d accelerator APIs? Glide&#x2F;opengl&#x2F;directx. Which has been a minute! (To a lesser extent CPU vectorization extensions)<p>Curious how much of Nvidia&#x27;s successful strategy was driven by people who were there during that period.<p>Powerful first mover flywheel: build high performing hardware that allows you to define an API -&gt; people write useful software that targets your API, because you have the highest performance -&gt; GOTO 10 (because now more software is standardized on your API, so you can build even more performant hardware to optimize its operations)</div><br/></div></div><div id="41603381" class="c"><input type="checkbox" id="c-41603381" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41603116">parent</a><span>|</span><a href="#41603215">prev</a><span>|</span><a href="#41603536">next</a><span>|</span><label class="collapse" for="c-41603381">[-]</label><label class="expand" for="c-41603381">[1 more]</label></div><br/><div class="children"><div class="content">An excellent example of Array API usage can be found in scikit-learn. Estimators written in NumPy are now operable on various backends courtesy of Array API compatible libraries such as CuPy and PyTorch.<p><a href="https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;array_api.html" rel="nofollow">https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;array_api.html</a><p>Disclosure: I&#x27;m a CuPy maintainer.</div><br/></div></div><div id="41603536" class="c"><input type="checkbox" id="c-41603536" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41603116">parent</a><span>|</span><a href="#41603381">prev</a><span>|</span><a href="#41602270">next</a><span>|</span><label class="collapse" for="c-41603536">[-]</label><label class="expand" for="c-41603536">[1 more]</label></div><br/><div class="children"><div class="content">And of course the native Python solution is memoryview. If you need to inter-operate with libraries like numpy but you cannot import numpy, use memoryview. It is specifically for fast low-level access which is why it has more C documentation than Python documentation: <a href="https:&#x2F;&#x2F;docs.python.org&#x2F;3&#x2F;c-api&#x2F;memoryview.html" rel="nofollow">https:&#x2F;&#x2F;docs.python.org&#x2F;3&#x2F;c-api&#x2F;memoryview.html</a></div><br/></div></div></div></div><div id="41602270" class="c"><input type="checkbox" id="c-41602270" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41603116">prev</a><span>|</span><a href="#41604716">next</a><span>|</span><label class="collapse" for="c-41602270">[-]</label><label class="expand" for="c-41602270">[8 more]</label></div><br/><div class="children"><div class="content">One could also &quot;import jax.numpy as jnp&quot;. All those libraries have more or less complete implementations of numpy and scipy (i believe CuPy has the most functions, especially when it comes to scipy) functionality.<p>Also: You can just mix match all those functions and tensors thanks to the __cuda_array_interface__.</div><br/><div id="41602671" class="c"><input type="checkbox" id="c-41602671" checked=""/><div class="controls bullet"><span class="by">yobbo</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41602270">parent</a><span>|</span><a href="#41608079">next</a><span>|</span><label class="collapse" for="c-41602671">[-]</label><label class="expand" for="c-41602671">[4 more]</label></div><br/><div class="children"><div class="content">Jax variables are immutable.<p>Code written for CuPy looks similar to numpy but very different from Jax.</div><br/><div id="41602882" class="c"><input type="checkbox" id="c-41602882" checked=""/><div class="controls bullet"><span class="by">bbminner</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41602671">parent</a><span>|</span><a href="#41604021">next</a><span>|</span><label class="collapse" for="c-41602882">[-]</label><label class="expand" for="c-41602882">[2 more]</label></div><br/><div class="children"><div class="content">Ah, well, that&#x27;s interesting! Does anyone know how cupy manages tensor mutability?</div><br/><div id="41603482" class="c"><input type="checkbox" id="c-41603482" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41602882">parent</a><span>|</span><a href="#41604021">next</a><span>|</span><label class="collapse" for="c-41603482">[-]</label><label class="expand" for="c-41603482">[1 more]</label></div><br/><div class="children"><div class="content">CuPy tensors (or `ndarray`) provide the same semantics as NumPy. In-place operations are permitted.</div><br/></div></div></div></div><div id="41604021" class="c"><input type="checkbox" id="c-41604021" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41602671">parent</a><span>|</span><a href="#41602882">prev</a><span>|</span><a href="#41608079">next</a><span>|</span><label class="collapse" for="c-41604021">[-]</label><label class="expand" for="c-41604021">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, stumbled over that recently, but the error message is very helpful and it&#x27;s a quick change.</div><br/></div></div></div></div><div id="41608079" class="c"><input type="checkbox" id="c-41608079" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41602270">parent</a><span>|</span><a href="#41602671">prev</a><span>|</span><a href="#41603432">next</a><span>|</span><label class="collapse" for="c-41608079">[-]</label><label class="expand" for="c-41608079">[2 more]</label></div><br/><div class="children"><div class="content">Indeed, has anyone so far successfully drop-in replaced numpy in a project with this cupy and achieved massive improvements? Because, you know, when dealing with GPU it is very important to actually understand how data flows back and forth to it, not only the algorithmic nature of the code written.<p>As a sidenote, it is funny how this gets released in 2024, and not in say 2014...</div><br/><div id="41608130" class="c"><input type="checkbox" id="c-41608130" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41608079">parent</a><span>|</span><a href="#41603432">next</a><span>|</span><label class="collapse" for="c-41608130">[-]</label><label class="expand" for="c-41608130">[1 more]</label></div><br/><div class="children"><div class="content">Oh yes, I&#x27;ve personally used CuPy for great speed ups compared to Numpy in radar signal processing. Taking a code that took 30 seconds with NumPy down to 1 second with CuPy. The code basically performed a bunch of math on like 100 MB of data, so the PCIe bottleneck was not a big issue.<p>Also CuPy was first released in 2015, this post is just a reminder for people that such things exist.</div><br/></div></div></div></div><div id="41603432" class="c"><input type="checkbox" id="c-41603432" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41602270">parent</a><span>|</span><a href="#41608079">prev</a><span>|</span><a href="#41604716">next</a><span>|</span><label class="collapse" for="c-41603432">[-]</label><label class="expand" for="c-41603432">[1 more]</label></div><br/><div class="children"><div class="content">For those interested in the NumPy&#x2F;SciPy API coverage in CuPy, here is the comparison table:<p><a href="https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;latest&#x2F;reference&#x2F;comparison.html" rel="nofollow">https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;latest&#x2F;reference&#x2F;comparison.html</a></div><br/></div></div></div></div><div id="41604716" class="c"><input type="checkbox" id="c-41604716" checked=""/><div class="controls bullet"><span class="by">paperplatter</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41602270">prev</a><span>|</span><a href="#41604342">next</a><span>|</span><label class="collapse" for="c-41604716">[-]</label><label class="expand" for="c-41604716">[1 more]</label></div><br/><div class="children"><div class="content">Hm. Tempted to try pytorch on my Mac for this. I have an AS chip rather than a Nvidia GPU.</div><br/></div></div><div id="41604342" class="c"><input type="checkbox" id="c-41604342" checked=""/><div class="controls bullet"><span class="by">WCSTombs</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41604716">prev</a><span>|</span><a href="#41605266">next</a><span>|</span><label class="collapse" for="c-41604342">[-]</label><label class="expand" for="c-41604342">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, the AMD-GPU compatibility for CuPy is quite an attractive feature.<p>Last I checked (a couple months ago) it wasn&#x27;t quite there, but I totally agree in principle. I&#x27;ve not gotten it to work on my Radeons yet.</div><br/></div></div><div id="41605266" class="c"><input type="checkbox" id="c-41605266" checked=""/><div class="controls bullet"><span class="by">sspiff</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41604342">prev</a><span>|</span><a href="#41604310">next</a><span>|</span><label class="collapse" for="c-41605266">[-]</label><label class="expand" for="c-41605266">[3 more]</label></div><br/><div class="children"><div class="content">It only supports AMD cards supported by ROCm, which is quite a limited set.<p>I know you can enable ROCm for other hardware as well, but it&#x27;s not supported and quite hit or miss. I&#x27;ve had limited success with running stuff against ROCm on unsupported cards, mainly having issues with memory management IIRC.</div><br/><div id="41606257" class="c"><input type="checkbox" id="c-41606257" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41605266">parent</a><span>|</span><a href="#41605473">next</a><span>|</span><label class="collapse" for="c-41606257">[-]</label><label class="expand" for="c-41606257">[1 more]</label></div><br/><div class="children"><div class="content">When I packaged the ROCm libraries that shipped in the Ubuntu 24.04 universe repository, I built and tested them with almost every discrete AMD GPU architecture from Vega to CDNA 2 and RDNA 3 (plus a few APUs). None of that is officially supported by AMD, but it is supported by me on a volunteer basis (for whatever that is worth).<p>I think that every library required to build cupy is available in the universe repositories, though I&#x27;ve never tried building it myself.</div><br/></div></div><div id="41605473" class="c"><input type="checkbox" id="c-41605473" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41605266">parent</a><span>|</span><a href="#41606257">prev</a><span>|</span><a href="#41604310">next</a><span>|</span><label class="collapse" for="c-41605473">[-]</label><label class="expand" for="c-41605473">[1 more]</label></div><br/><div class="children"><div class="content">Fingers crossed that all future AMD parts ship with full ROCm support.</div><br/></div></div></div></div><div id="41604310" class="c"><input type="checkbox" id="c-41604310" checked=""/><div class="controls bullet"><span class="by">Narhem</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41605266">prev</a><span>|</span><a href="#41604191">next</a><span>|</span><label class="collapse" for="c-41604310">[-]</label><label class="expand" for="c-41604310">[2 more]</label></div><br/><div class="children"><div class="content">As nice as it is to have a drop in replacement, most of the cost of GPU computing is moving memory around. Wouldn’t be surprised if this catches unsuspecting programmers in a few performance traps.</div><br/><div id="41607827" class="c"><input type="checkbox" id="c-41607827" checked=""/><div class="controls bullet"><span class="by">low_tech_love</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41604310">parent</a><span>|</span><a href="#41604191">next</a><span>|</span><label class="collapse" for="c-41607827">[-]</label><label class="expand" for="c-41607827">[1 more]</label></div><br/><div class="children"><div class="content">Exactly my experience. You end up having to juggle a whole different set of requirements and design factors in addition to whatever it is that you’re already doing. Usually after a while the results are worth it, but I found the “drop-in” idea to be slightly misleading. Just because the API is the same does not make it a drop-in replacement.</div><br/></div></div></div></div><div id="41604191" class="c"><input type="checkbox" id="c-41604191" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41604310">prev</a><span>|</span><a href="#41603619">next</a><span>|</span><label class="collapse" for="c-41604191">[-]</label><label class="expand" for="c-41604191">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of unfortunate that EagerPy didn&#x27;t get more traction to make that kind of switching even easier.</div><br/></div></div><div id="41603619" class="c"><input type="checkbox" id="c-41603619" checked=""/><div class="controls bullet"><span class="by">amarcheschi</span><span>|</span><a href="#41602197">parent</a><span>|</span><a href="#41604191">prev</a><span>|</span><a href="#41603784">next</a><span>|</span><label class="collapse" for="c-41603619">[-]</label><label class="expand" for="c-41603619">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m supposed to end my undergraduate degree with an internship at the italian national research center and i&#x27;ll have to use pytorch to write ml models from paper to code, i&#x27;ve tried looking at the tutorial but i feel like there&#x27;s a lot going on to grasp. until now i&#x27;ve only used numpy (and pandas in combo with numpy), i&#x27;m quite excited but i&#x27;m a bit on the edge because i can&#x27;t know whether i&#x27;ll be up to the task or not</div><br/><div id="41608510" class="c"><input type="checkbox" id="c-41608510" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41603619">parent</a><span>|</span><a href="#41604112">next</a><span>|</span><label class="collapse" for="c-41608510">[-]</label><label class="expand" for="c-41608510">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ll do fine :) PyTorch has an API that is somewhat similar to numpy, although if you&#x27;ve never programmed a GPU you might want to get up to speed on that first.</div><br/></div></div><div id="41604112" class="c"><input type="checkbox" id="c-41604112" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41602197">root</a><span>|</span><a href="#41603619">parent</a><span>|</span><a href="#41608510">prev</a><span>|</span><a href="#41603784">next</a><span>|</span><label class="collapse" for="c-41604112">[-]</label><label class="expand" for="c-41604112">[1 more]</label></div><br/><div class="children"><div class="content">Go for it! There&#x27;s nothing to lose.<p>You could checkout some of EuroCC&#x27;s courses. That should get you up to speed. <a href="https:&#x2F;&#x2F;www.eurocc-access.eu&#x2F;services&#x2F;training&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.eurocc-access.eu&#x2F;services&#x2F;training&#x2F;</a></div><br/></div></div></div></div></div></div><div id="41603784" class="c"><input type="checkbox" id="c-41603784" checked=""/><div class="controls bullet"><span class="by">curvilinear_m</span><span>|</span><a href="#41602197">prev</a><span>|</span><a href="#41602511">next</a><span>|</span><label class="collapse" for="c-41603784">[-]</label><label class="expand" for="c-41603784">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised to see pytorch and Jax mentioned as alternatives but not numba : <a href="https:&#x2F;&#x2F;github.com&#x2F;numba&#x2F;numba">https:&#x2F;&#x2F;github.com&#x2F;numba&#x2F;numba</a><p>I&#x27;ve recently had to implement a few kernels to lower the memory footprint and runtime of some pytorch function : it&#x27;s been really nice because numba kernels have type hints support (as opposed to raw cupy kernels).</div><br/><div id="41605512" class="c"><input type="checkbox" id="c-41605512" checked=""/><div class="controls bullet"><span class="by">killingtime74</span><span>|</span><a href="#41603784">parent</a><span>|</span><a href="#41602511">next</a><span>|</span><label class="collapse" for="c-41605512">[-]</label><label class="expand" for="c-41605512">[4 more]</label></div><br/><div class="children"><div class="content">Numba doesn&#x27;t support GPU though</div><br/><div id="41605563" class="c"><input type="checkbox" id="c-41605563" checked=""/><div class="controls bullet"><span class="by">catshamando</span><span>|</span><a href="#41603784">root</a><span>|</span><a href="#41605512">parent</a><span>|</span><a href="#41605822">next</a><span>|</span><label class="collapse" for="c-41605563">[-]</label><label class="expand" for="c-41605563">[2 more]</label></div><br/><div class="children"><div class="content">Yes it does:
<a href="https:&#x2F;&#x2F;numba.readthedocs.io&#x2F;en&#x2F;stable&#x2F;cuda&#x2F;kernels.html" rel="nofollow">https:&#x2F;&#x2F;numba.readthedocs.io&#x2F;en&#x2F;stable&#x2F;cuda&#x2F;kernels.html</a></div><br/><div id="41606372" class="c"><input type="checkbox" id="c-41606372" checked=""/><div class="controls bullet"><span class="by">killingtime74</span><span>|</span><a href="#41603784">root</a><span>|</span><a href="#41605563">parent</a><span>|</span><a href="#41605822">next</a><span>|</span><label class="collapse" for="c-41606372">[-]</label><label class="expand" for="c-41606372">[1 more]</label></div><br/><div class="children"><div class="content">ah thanks, TIL</div><br/></div></div></div></div><div id="41605822" class="c"><input type="checkbox" id="c-41605822" checked=""/><div class="controls bullet"><span class="by">the_svd_doctor</span><span>|</span><a href="#41603784">root</a><span>|</span><a href="#41605512">parent</a><span>|</span><a href="#41605563">prev</a><span>|</span><a href="#41602511">next</a><span>|</span><label class="collapse" for="c-41605822">[-]</label><label class="expand" for="c-41605822">[1 more]</label></div><br/><div class="children"><div class="content">It does (for NVIDIA at least)</div><br/></div></div></div></div></div></div><div id="41602511" class="c"><input type="checkbox" id="c-41602511" checked=""/><div class="controls bullet"><span class="by">meisel</span><span>|</span><a href="#41603784">prev</a><span>|</span><a href="#41603110">next</a><span>|</span><label class="collapse" for="c-41602511">[-]</label><label class="expand" for="c-41602511">[4 more]</label></div><br/><div class="children"><div class="content">When building something that I want to run on both CPU and GPU, depending, I’ve found it much easier to use PyTorch than some combination of NumPy and CuPy. I don’t have to fiddle around with some global replacing of numpy.* with cupy.*, and PyTorch has very nearly all the functions that those libraries have.</div><br/><div id="41604269" class="c"><input type="checkbox" id="c-41604269" checked=""/><div class="controls bullet"><span class="by">setopt</span><span>|</span><a href="#41602511">parent</a><span>|</span><a href="#41603110">next</a><span>|</span><label class="collapse" for="c-41604269">[-]</label><label class="expand" for="c-41604269">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. Any links to examples or docs on how to use PyTorch as a general linear algebra library for this purpose? Like a “SciPy to PyTorch” transition guide if I want to do the same?</div><br/><div id="41605549" class="c"><input type="checkbox" id="c-41605549" checked=""/><div class="controls bullet"><span class="by">ttyprintk</span><span>|</span><a href="#41602511">root</a><span>|</span><a href="#41604269">parent</a><span>|</span><a href="#41604811">next</a><span>|</span><label class="collapse" for="c-41605549">[-]</label><label class="expand" for="c-41605549">[1 more]</label></div><br/><div class="children"><div class="content">Mentioned above:<p><a href="https:&#x2F;&#x2F;data-apis.org&#x2F;array-api-compat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;data-apis.org&#x2F;array-api-compat&#x2F;</a></div><br/></div></div><div id="41604811" class="c"><input type="checkbox" id="c-41604811" checked=""/><div class="controls bullet"><span class="by">meisel</span><span>|</span><a href="#41602511">root</a><span>|</span><a href="#41604269">parent</a><span>|</span><a href="#41605549">prev</a><span>|</span><a href="#41603110">next</a><span>|</span><label class="collapse" for="c-41604811">[-]</label><label class="expand" for="c-41604811">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s typically just importing torch and s&#x2F;np&#x2F;torch, not too different from NumPy -&gt; CuPy. Try it in your own code and see!</div><br/></div></div></div></div></div></div><div id="41603110" class="c"><input type="checkbox" id="c-41603110" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41602511">prev</a><span>|</span><a href="#41606697">next</a><span>|</span><label class="collapse" for="c-41603110">[-]</label><label class="expand" for="c-41603110">[3 more]</label></div><br/><div class="children"><div class="content">CuPy is probably the easiest way to interface with custom CUDA kernels: <a href="https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;user_guide&#x2F;kernel.html#raw-kernels" rel="nofollow">https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;user_guide&#x2F;kernel.html#raw-k...</a><p>And I recently learned that CuPy has a JIT compiler now if you prefer Python syntax over C++. <a href="https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;user_guide&#x2F;kernel.html#jit-kernel-definition" rel="nofollow">https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;user_guide&#x2F;kernel.html#jit-k...</a></div><br/><div id="41604224" class="c"><input type="checkbox" id="c-41604224" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#41603110">parent</a><span>|</span><a href="#41606697">next</a><span>|</span><label class="collapse" for="c-41604224">[-]</label><label class="expand" for="c-41604224">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  probably the easiest way to interface with custom CUDA kernels<p>In Python? Perhaps. Generally? No, it isn&#x27;t. Try: <a href="https:&#x2F;&#x2F;github.com&#x2F;eyalroz&#x2F;cuda-api-wrappers&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;eyalroz&#x2F;cuda-api-wrappers&#x2F;</a><p>Full power of the CUDA APIs including all runtime compilation options etc.<p>(Yes, I wrote that...)</div><br/><div id="41605181" class="c"><input type="checkbox" id="c-41605181" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41603110">root</a><span>|</span><a href="#41604224">parent</a><span>|</span><a href="#41606697">next</a><span>|</span><label class="collapse" for="c-41605181">[-]</label><label class="expand" for="c-41605181">[1 more]</label></div><br/><div class="children"><div class="content">Personally, I prefer CuPy over your library. For example, your vectorAdd.cu implementation at <a href="https:&#x2F;&#x2F;github.com&#x2F;eyalroz&#x2F;cuda-api-wrappers&#x2F;blob&#x2F;master&#x2F;examples&#x2F;modified_cuda_samples&#x2F;vectorAdd&#x2F;vectorAdd.cu">https:&#x2F;&#x2F;github.com&#x2F;eyalroz&#x2F;cuda-api-wrappers&#x2F;blob&#x2F;master&#x2F;exa...</a> is much longer than a similar CuPy implementation:<p><pre><code>    import cupy as cp

    vector_add = cp.RawKernel(&quot;&quot;&quot;
    extern &quot;C&quot; __global__
    void vector_add(const float *A, const float *B, float *C, int num_elements) {
        int i = blockDim.x * blockIdx.x + threadIdx.x;

        if (i &lt; num_elements) { C[i] = A[i] + B[i]; }
    }
    &quot;&quot;&quot;, &quot;vector_add&quot;)

    num_elements = 50_000

    block_size = 256
    # round up to next multiple of block_size
    grid_size = (num_elements + block_size - 1) &#x2F;&#x2F; block_size

    a = cp.random.rand(num_elements, dtype=cp.float32)
    b = cp.random.rand(num_elements, dtype=cp.float32)
    c = cp.zeros(num_elements, dtype=cp.float32)

    args = (a, b, c, num_elements)

    print(f&quot;[Vector addition of {num_elements} elements]&quot;)
    print(f&quot;CUDA kernel launch with {grid_size} blocks of {block_size} threads each&quot;)

    vector_add((grid_size,), (block_size,), args)

    incorrect = cp.abs(a + b - c) &gt; 1e-5

    if cp.any(incorrect):
        print(&quot;Result verification failed at element&quot;, cp.argmax(incorrect))

    print(&quot;Test PASSED&quot;)
    print(&quot;SUCCESS&quot;)
</code></pre>
It could be made even shorter with a cp.ElementwiseKernel <a href="https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;user_guide&#x2F;kernel.html#basics-of-elementwise-kernels" rel="nofollow">https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;user_guide&#x2F;kernel.html#basic...</a><p>Although I have to concede that the automatic grid size computation in cuda-api-wrappers is nice.<p>A few marketing tips for your README:<p>* Put a code example directly at the top. You want to present the selling points of your library to the reader as fast as possible. For reference, look at the CuPy README <a href="https:&#x2F;&#x2F;github.com&#x2F;cupy&#x2F;cupy?tab=readme-ov-file#cupy--numpy--scipy-for-gpu">https:&#x2F;&#x2F;github.com&#x2F;cupy&#x2F;cupy?tab=readme-ov-file#cupy--numpy-...</a> which immediately shows reader what it is good for. Your README starts with lots of text, but nobody reads text anymore these days. A link to examples is almost at the end, and then the examples are deeply nested.<p>* The first links in the README should link to your own library, for example to documentation or examples. You do not want to lead the reader away from your GitHub page.<p>* Add syntax highlighting with &quot;cpp&quot; after triple backticks:<p><pre><code>    ```cpp
    &lt;code here&gt;
    ```</code></pre></div><br/></div></div></div></div></div></div><div id="41606697" class="c"><input type="checkbox" id="c-41606697" checked=""/><div class="controls bullet"><span class="by">towerwoerjrrwer</span><span>|</span><a href="#41603110">prev</a><span>|</span><a href="#41607478">next</a><span>|</span><label class="collapse" for="c-41606697">[-]</label><label class="expand" for="c-41606697">[6 more]</label></div><br/><div class="children"><div class="content">Cupy was first but at this point you&#x27;re better of using JAX.<p>Has a much larger community, a big push from Google Research, and unlike PFN&#x27;s Chainer (of which CuPy is the computational base), is not semi-abandoned.<p>Kind of sad to see CuPy&#x2F;Chainer eco-system die: not only did they pioneer the PyTorch programming model, but also stuck to Numpy API like JAX does (though the AD is layered on top in Chainer IIRC).</div><br/><div id="41606742" class="c"><input type="checkbox" id="c-41606742" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#41606697">parent</a><span>|</span><a href="#41607843">next</a><span>|</span><label class="collapse" for="c-41606742">[-]</label><label class="expand" for="c-41606742">[2 more]</label></div><br/><div class="children"><div class="content">JAX still has the<p>&quot;This is a research project, not an official Google product. Expect bugs and sharp edges. Please help by trying it out, reporting bugs, and letting us know what you think!&quot;<p>disclaimer in its readme. This is quite scary, especially coming from Google which is known to abandon projects out of the blue</div><br/><div id="41607682" class="c"><input type="checkbox" id="c-41607682" checked=""/><div class="controls bullet"><span class="by">atty</span><span>|</span><a href="#41606697">root</a><span>|</span><a href="#41606742">parent</a><span>|</span><a href="#41607843">next</a><span>|</span><label class="collapse" for="c-41607682">[-]</label><label class="expand" for="c-41607682">[1 more]</label></div><br/><div class="children"><div class="content">Jax is, as far as an outsider can tell, Google research&#x2F;DeepMind’s primary computation library now, not Tensorflow. So it’s safer than most Google projects, unless you think they’re secretly developing their third tensor computation library in 10 years (I suppose if anyone was going to do it, it would be Google).</div><br/></div></div></div></div><div id="41607843" class="c"><input type="checkbox" id="c-41607843" checked=""/><div class="controls bullet"><span class="by">low_tech_love</span><span>|</span><a href="#41606697">parent</a><span>|</span><a href="#41606742">prev</a><span>|</span><a href="#41606938">next</a><span>|</span><label class="collapse" for="c-41607843">[-]</label><label class="expand" for="c-41607843">[1 more]</label></div><br/><div class="children"><div class="content">I tried Jax last year and was not impressed. Maybe it was just me, but everything I tried to do (especially with the autograd stuff) involved huge compilation times and I simply could not get the promised performance. Maybe I’ll try again and read some new documentation, since everyone is excited about it.</div><br/></div></div><div id="41606938" class="c"><input type="checkbox" id="c-41606938" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41606697">parent</a><span>|</span><a href="#41607843">prev</a><span>|</span><a href="#41606718">next</a><span>|</span><label class="collapse" for="c-41606938">[-]</label><label class="expand" for="c-41606938">[1 more]</label></div><br/><div class="children"><div class="content">CuPy isn&#x27;t semi-abandoned as well, obviously :)</div><br/></div></div><div id="41606718" class="c"><input type="checkbox" id="c-41606718" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41606697">parent</a><span>|</span><a href="#41606938">prev</a><span>|</span><a href="#41607478">next</a><span>|</span><label class="collapse" for="c-41606718">[-]</label><label class="expand" for="c-41606718">[1 more]</label></div><br/><div class="children"><div class="content">I agree. Though it&#x27;s good to have options for GPU accelerated Numpy. Especially if Google decides to discontinue Jax at some point.</div><br/></div></div></div></div><div id="41607478" class="c"><input type="checkbox" id="c-41607478" checked=""/><div class="controls bullet"><span class="by">markkitti</span><span>|</span><a href="#41606697">prev</a><span>|</span><a href="#41602143">next</a><span>|</span><label class="collapse" for="c-41607478">[-]</label><label class="expand" for="c-41607478">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny how much easier GPU support, especially vendor agnostic GPU support, is in Julia.</div><br/></div></div><div id="41602143" class="c"><input type="checkbox" id="c-41602143" checked=""/><div class="controls bullet"><span class="by">__mharrison__</span><span>|</span><a href="#41607478">prev</a><span>|</span><a href="#41605417">next</a><span>|</span><label class="collapse" for="c-41602143">[-]</label><label class="expand" for="c-41602143">[1 more]</label></div><br/><div class="children"><div class="content">I taught my numpy class to a client who wanted to use GPUs. Installation (at that time) was a chore but afterwards it was really smooth using this library. Big gains with minimal to no code changes.</div><br/></div></div><div id="41605417" class="c"><input type="checkbox" id="c-41605417" checked=""/><div class="controls bullet"><span class="by">aterrel-nvidia</span><span>|</span><a href="#41602143">prev</a><span>|</span><a href="#41603792">next</a><span>|</span><label class="collapse" for="c-41605417">[-]</label><label class="expand" for="c-41605417">[1 more]</label></div><br/><div class="children"><div class="content">If you like cupy, definitely checkout the Multinode Multi-gpu version, cuNumeric: <a href="https:&#x2F;&#x2F;github.com&#x2F;nv-legate&#x2F;cunumeric">https:&#x2F;&#x2F;github.com&#x2F;nv-legate&#x2F;cunumeric</a><p>Would love to get any feedback from the community.</div><br/></div></div><div id="41603792" class="c"><input type="checkbox" id="c-41603792" checked=""/><div class="controls bullet"><span class="by">setopt</span><span>|</span><a href="#41605417">prev</a><span>|</span><a href="#41602269">next</a><span>|</span><label class="collapse" for="c-41603792">[-]</label><label class="expand" for="c-41603792">[6 more]</label></div><br/><div class="children"><div class="content">I’ve been using CuPy a bit and found it to be excellent.<p>It’s very easy to replace some slow NumPy&#x2F;SciPy calls with appropriate CuPy calls, with sometimes literally a 1000x performance boost from like 10min work. It’s also easy to write “hybrid code” where you can switch between NumPy and CuPy depending on what’s available.</div><br/><div id="41603953" class="c"><input type="checkbox" id="c-41603953" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#41603792">parent</a><span>|</span><a href="#41602269">next</a><span>|</span><label class="collapse" for="c-41603953">[-]</label><label class="expand" for="c-41603953">[5 more]</label></div><br/><div class="children"><div class="content">Are you able to share what functions or situations result in speedups?  In my experience, vectorized numpy is already fast, so I&#x27;m very curious.</div><br/><div id="41604386" class="c"><input type="checkbox" id="c-41604386" checked=""/><div class="controls bullet"><span class="by">setopt</span><span>|</span><a href="#41603792">root</a><span>|</span><a href="#41603953">parent</a><span>|</span><a href="#41604129">next</a><span>|</span><label class="collapse" for="c-41604386">[-]</label><label class="expand" for="c-41604386">[3 more]</label></div><br/><div class="children"><div class="content">The largest speedup I have seen was for a quantum mechanics simulation where I needed to repeatedly calculate all eigenvalues of Hermitian matrices (but not necessarily their eigenvectors).<p>This was basically the code needed:<p><pre><code>    import scipy.linalg as la

    if cuda:
        import cupy as cp
        import cupy.linalg as cla

        ε = cp.asnumpy(cla.eigvalsh(cp.asarray(H)))
    else:
        ε = la.eigvalsh(H)
</code></pre>
I was using IntelPython which already has fast (parallelized) methods for this using MKL, but CuPy blew it out of the water.</div><br/><div id="41606985" class="c"><input type="checkbox" id="c-41606985" checked=""/><div class="controls bullet"><span class="by">JBits</span><span>|</span><a href="#41603792">root</a><span>|</span><a href="#41604386">parent</a><span>|</span><a href="#41604129">next</a><span>|</span><label class="collapse" for="c-41606985">[-]</label><label class="expand" for="c-41606985">[2 more]</label></div><br/><div class="children"><div class="content">What did you need the eigenvalues for? I wouldn&#x27;ve guessed exact diagonalization but in that case you would need the eigenvectors.</div><br/><div id="41608265" class="c"><input type="checkbox" id="c-41608265" checked=""/><div class="controls bullet"><span class="by">setopt</span><span>|</span><a href="#41603792">root</a><span>|</span><a href="#41606985">parent</a><span>|</span><a href="#41604129">next</a><span>|</span><label class="collapse" for="c-41608265">[-]</label><label class="expand" for="c-41608265">[1 more]</label></div><br/><div class="children"><div class="content">In quantum mechanics, you use a “Hamiltonian matrix” H to encode, in some sense, “everything that a particle in your system is allowed to do” along with some energy associated with that. For instance, an electron in a metallic crystal is allowed to “hop” over to a neighboring atom and that is associated with some kinetic energy. Or it is in some cases allowed to stay on the same atom as another electron, and that is associated with a potential energy (Coulomb repulsion).<p>The eigenvalues of this matrix is the answer to “what are the energies of each stable electron state in this system”. If you know how many electrons you have (they tend to fill the lowest energy states they can at zero temperature), and you know what temperature you have (which gives you the probability of each “excited” state being occupied), then you can say a lot about the system. For instance, you can say what physical state lowers the “free energy” of the electrons at a given temperature (which can be  used to predict phase transitions and spin configurations), or what is the “density of states” (which can be used to predict electronic resistance). You can also obtain the system’s entropy from the eigenvalues alone.<p>There are however many cases where you might need eigenvectors too, since they usually provide all the spatial information about “where in your system is this stuff happening”. When I need the eigenvectors, CuPy is still hundreds of times faster on my hardware, but the gap is just not <i>as extreme</i> as it was for pure eigenvalue calculation in my benchmarks.</div><br/></div></div></div></div></div></div><div id="41604129" class="c"><input type="checkbox" id="c-41604129" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#41603792">root</a><span>|</span><a href="#41603953">parent</a><span>|</span><a href="#41604386">prev</a><span>|</span><a href="#41602269">next</a><span>|</span><label class="collapse" for="c-41604129">[-]</label><label class="expand" for="c-41604129">[1 more]</label></div><br/><div class="children"><div class="content">Not OP, but think about stuff like FFTs or Matmuls. It&#x27;s not even a competition, GPUs win when the algorithm is somewhat suitable and you&#x27;re dealing with FP32 or lower precision.</div><br/></div></div></div></div></div></div><div id="41602269" class="c"><input type="checkbox" id="c-41602269" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#41603792">prev</a><span>|</span><a href="#41602483">next</a><span>|</span><label class="collapse" for="c-41602269">[-]</label><label class="expand" for="c-41602269">[12 more]</label></div><br/><div class="children"><div class="content">Why not Jax?</div><br/><div id="41602855" class="c"><input type="checkbox" id="c-41602855" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41602269">parent</a><span>|</span><a href="#41602403">next</a><span>|</span><label class="collapse" for="c-41602855">[-]</label><label class="expand" for="c-41602855">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Why not Jax?<p>- JAX Windows support is lacking<p>- CuPy is much closer to CUDA than JAX, so you can get better performance<p>- CuPy is generally more mature than JAX (fewer bugs)<p>- CuPy is more flexible thanks to cp.RawKernel<p>- (For those familiar with NumPy) CuPy is closer to NumPy than jax.numpy<p>But CuPy does not support automatic gradient computation, so if you do deep learning, use JAX instead. Or PyTorch, if you do not trust Google to maintain a project for a prolonged period of time <a href="https:&#x2F;&#x2F;killedbygoogle.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;killedbygoogle.com&#x2F;</a></div><br/><div id="41603453" class="c"><input type="checkbox" id="c-41603453" checked=""/><div class="controls bullet"><span class="by">gnulinux</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41602855">parent</a><span>|</span><a href="#41603690">next</a><span>|</span><label class="collapse" for="c-41603453">[-]</label><label class="expand" for="c-41603453">[5 more]</label></div><br/><div class="children"><div class="content">What about CPU-only loads? If one wants to write code that&#x27;ll eventually run in both CPU and GPU but in the short-to-mid term will only be used in CPU? Since JAX natively support CPU (with numpy backend), but CuPy doesn&#x27;t, this seems like a potential problem for some.</div><br/><div id="41603551" class="c"><input type="checkbox" id="c-41603551" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41603453">parent</a><span>|</span><a href="#41603690">next</a><span>|</span><label class="collapse" for="c-41603551">[-]</label><label class="expand" for="c-41603551">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t there a way to dynamically select between numpy and cupy, depending on whether you want cpu or gpu code?</div><br/><div id="41603885" class="c"><input type="checkbox" id="c-41603885" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41603551">parent</a><span>|</span><a href="#41604919">next</a><span>|</span><label class="collapse" for="c-41603885">[-]</label><label class="expand" for="c-41603885">[1 more]</label></div><br/><div class="children"><div class="content">NumPy has a mechanism to dispatch execution to CuPy: <a href="https:&#x2F;&#x2F;numpy.org&#x2F;neps&#x2F;nep-0018-array-function-protocol.html" rel="nofollow">https:&#x2F;&#x2F;numpy.org&#x2F;neps&#x2F;nep-0018-array-function-protocol.html</a><p>Just prepare the input on NumPy or CuPy, and then you can just feed it to NumPy APIs. NumPy functions will handle itself if the input is NumPy ndarray, or dispatch the execution to CuPy if the input is CuPy ndarray.</div><br/></div></div><div id="41604919" class="c"><input type="checkbox" id="c-41604919" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41603551">parent</a><span>|</span><a href="#41603885">prev</a><span>|</span><a href="#41603621">next</a><span>|</span><label class="collapse" for="c-41604919">[-]</label><label class="expand" for="c-41604919">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Isn&#x27;t there a way to dynamically select between numpy and cupy, depending on whether you want cpu or gpu code?<p>CuPy is an (almost) drop-in replacement for NumPy, so the following works surprisingly often:<p><pre><code>    if use_cpu:
        import numpy as np
    else:
       import cupy as np</code></pre></div><br/></div></div><div id="41603621" class="c"><input type="checkbox" id="c-41603621" checked=""/><div class="controls bullet"><span class="by">gnulinux</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41603551">parent</a><span>|</span><a href="#41604919">prev</a><span>|</span><a href="#41603690">next</a><span>|</span><label class="collapse" for="c-41603621">[-]</label><label class="expand" for="c-41603621">[1 more]</label></div><br/><div class="children"><div class="content">There is but then you&#x27;re using two separate libraries, that seems like a fragile point of failure compared to just using jax. But regardless since jax will use different backends anyway, it&#x27;s arguably not any worse (but it ends up being your responsibility to ensure correctness as opposed to the jax team).</div><br/></div></div></div></div></div></div><div id="41603690" class="c"><input type="checkbox" id="c-41603690" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41602855">parent</a><span>|</span><a href="#41603453">prev</a><span>|</span><a href="#41602403">next</a><span>|</span><label class="collapse" for="c-41603690">[-]</label><label class="expand" for="c-41603690">[1 more]</label></div><br/><div class="children"><div class="content">&gt; CuPy does not support automatic gradient computation, so if you do deep learning, use JAX instead<p>DL is major use case; is CuPy planning on adding auto gradient comp?</div><br/></div></div></div></div><div id="41602403" class="c"><input type="checkbox" id="c-41602403" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41602269">parent</a><span>|</span><a href="#41602855">prev</a><span>|</span><a href="#41602693">next</a><span>|</span><label class="collapse" for="c-41602403">[-]</label><label class="expand" for="c-41602403">[3 more]</label></div><br/><div class="children"><div class="content">Real answer: CuPy has a name that is very similar to SciPy. I don’t know GPU, that’s why I’m using this sort of library, haha. The branding for CuPy makes it obvious. Is Jax the same thing, but implemented better somehow?</div><br/><div id="41602663" class="c"><input type="checkbox" id="c-41602663" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41602403">parent</a><span>|</span><a href="#41602517">next</a><span>|</span><label class="collapse" for="c-41602663">[-]</label><label class="expand" for="c-41602663">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, Jax provides a one-to-one reimplementation of the Numpy interface, and a decent chunk of the scipy interface. Random number handling is a bit different, but Numpy random number handling seeeeems to be trending in the Jax direction (explicitly passed RNG objects).<p>Jax also provides back-propagation wherever possible, so you can optimize.</div><br/></div></div><div id="41602517" class="c"><input type="checkbox" id="c-41602517" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41602269">root</a><span>|</span><a href="#41602403">parent</a><span>|</span><a href="#41602663">prev</a><span>|</span><a href="#41602693">next</a><span>|</span><label class="collapse" for="c-41602517">[-]</label><label class="expand" for="c-41602517">[1 more]</label></div><br/><div class="children"><div class="content">yes</div><br/></div></div></div></div><div id="41602693" class="c"><input type="checkbox" id="c-41602693" checked=""/><div class="controls bullet"><span class="by">palmy</span><span>|</span><a href="#41602269">parent</a><span>|</span><a href="#41602403">prev</a><span>|</span><a href="#41602483">next</a><span>|</span><label class="collapse" for="c-41602693">[-]</label><label class="expand" for="c-41602693">[1 more]</label></div><br/><div class="children"><div class="content">cupy came out a long time before Jax; remember using it in a project for my BSc around 2015-2016.<p>Cool to see that it&#x27;s still kicking!</div><br/></div></div></div></div><div id="41602483" class="c"><input type="checkbox" id="c-41602483" checked=""/><div class="controls bullet"><span class="by">adancalderon</span><span>|</span><a href="#41602269">prev</a><span>|</span><a href="#41602503">next</a><span>|</span><label class="collapse" for="c-41602483">[-]</label><label class="expand" for="c-41602483">[1 more]</label></div><br/><div class="children"><div class="content">If it ran in the background it could be CuPyd</div><br/></div></div><div id="41602503" class="c"><input type="checkbox" id="c-41602503" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41602483">prev</a><span>|</span><a href="#41603015">next</a><span>|</span><label class="collapse" for="c-41602503">[-]</label><label class="expand" for="c-41602503">[1 more]</label></div><br/><div class="children"><div class="content">I was just thinking we didn’t have enough CUDA-accelerated numpy libraries.<p>Jax, pytorch, vanilla TF, triton. They just don’t cut it</div><br/></div></div><div id="41602461" class="c"><input type="checkbox" id="c-41602461" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41603015">prev</a><span>|</span><a href="#41602708">next</a><span>|</span><label class="collapse" for="c-41602461">[-]</label><label class="expand" for="c-41602461">[7 more]</label></div><br/><div class="children"><div class="content">Good a place as any to ask I guess. Do any of these GPU libraries have a BiCGStab (or similar) that handles multiple right hand sides? CuPy seems to have GMRES, which would be fine, but as far as I can tell it just does one right hand side.</div><br/><div id="41602981" class="c"><input type="checkbox" id="c-41602981" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41602461">parent</a><span>|</span><a href="#41602683">next</a><span>|</span><label class="collapse" for="c-41602981">[-]</label><label class="expand" for="c-41602981">[3 more]</label></div><br/><div class="children"><div class="content">If you have many right hand sides, you could also compute an LU factorization and then solve the right hand sides via back-substitution.<p><a href="https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;reference&#x2F;generated&#x2F;cupyx.scipy.linalg.lu_factor.html" rel="nofollow">https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;reference&#x2F;generated&#x2F;cupyx.sc...</a><p>or <a href="https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;reference&#x2F;generated&#x2F;cupyx.scipy.sparse.linalg.factorized.html#cupyx.scipy.sparse.linalg.factorized" rel="nofollow">https:&#x2F;&#x2F;docs.cupy.dev&#x2F;en&#x2F;stable&#x2F;reference&#x2F;generated&#x2F;cupyx.sc...</a> if your linear system is sparse.<p>But whether that works well depends on the problem you are trying to solve.</div><br/><div id="41604151" class="c"><input type="checkbox" id="c-41604151" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41602461">root</a><span>|</span><a href="#41602981">parent</a><span>|</span><a href="#41602683">next</a><span>|</span><label class="collapse" for="c-41604151">[-]</label><label class="expand" for="c-41604151">[2 more]</label></div><br/><div class="children"><div class="content">My systems are sparse, but might not fit on the GPU when factorized. Actually, usually I do CPU stuff with lots of ram, and Pardiso, so it isn’t an issue.<p>But I was hoping to try out something like ILU+bicgstab on the GPU and the python-verse seems like it has the lowest barrier-to-entry for just playing around.</div><br/><div id="41605062" class="c"><input type="checkbox" id="c-41605062" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41602461">root</a><span>|</span><a href="#41604151">parent</a><span>|</span><a href="#41602683">next</a><span>|</span><label class="collapse" for="c-41605062">[-]</label><label class="expand" for="c-41605062">[1 more]</label></div><br/><div class="children"><div class="content">For my tasks, I had some success with algebraic multigrid solvers as preconditioner, for example from AMGCL or PyAMG. They are also reasonably easy to get started with.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;pyamg&#x2F;pyamg">https:&#x2F;&#x2F;github.com&#x2F;pyamg&#x2F;pyamg</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;ddemidov&#x2F;amgcl">https:&#x2F;&#x2F;github.com&#x2F;ddemidov&#x2F;amgcl</a><p>But I only have to deal with positive definite systems, so YMMV.<p>I am not sure whether those libraries can deal with multiple right-hand sides, but most complexity is in the preconditioners anyway.</div><br/></div></div></div></div></div></div><div id="41602683" class="c"><input type="checkbox" id="c-41602683" checked=""/><div class="controls bullet"><span class="by">trostaft</span><span>|</span><a href="#41602461">parent</a><span>|</span><a href="#41602981">prev</a><span>|</span><a href="#41602708">next</a><span>|</span><label class="collapse" for="c-41602683">[-]</label><label class="expand" for="c-41602683">[3 more]</label></div><br/><div class="children"><div class="content">IIRC jax&#x27;s `scipy.sparse.linalg.bicgstab` does support multiple right hand sides.<p>EDIT: Or rather, all the solvers under jax&#x27;s `scipy.sparse.linalg` all support multiple right hand sides.</div><br/><div id="41602790" class="c"><input type="checkbox" id="c-41602790" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41602461">root</a><span>|</span><a href="#41602683">parent</a><span>|</span><a href="#41602708">next</a><span>|</span><label class="collapse" for="c-41602790">[-]</label><label class="expand" for="c-41602790">[2 more]</label></div><br/><div class="children"><div class="content">Oh dang, that’s pretty awesome, thanks.<p>“array or tree of arrays” sounds very general, probably even better than an old fashioned 2D array.</div><br/><div id="41602945" class="c"><input type="checkbox" id="c-41602945" checked=""/><div class="controls bullet"><span class="by">trostaft</span><span>|</span><a href="#41602461">root</a><span>|</span><a href="#41602790">parent</a><span>|</span><a href="#41602708">next</a><span>|</span><label class="collapse" for="c-41602945">[-]</label><label class="expand" for="c-41602945">[1 more]</label></div><br/><div class="children"><div class="content">&#x27;tree of arrays&#x27;<p>Ahh, that&#x27;s just Jax&#x27;s concept of pytrees. It was something that they invented to make it easier (this is how I view it, not complete) to pass complex objects to function but still be able to easily consider them as a concatenated vector for AD etc.. E.g. a common pattern is to pass parameters `p` to a function and then internally break them into their physical interpretations, e.g. `mass = p[0]`, `velocity = p[1]`. Pytrees let you just use something like a dictionary `p = {&#x27;mass&#x27; = 1.0, &#x27;velocity = 1.0&#x27;}`, which is a stylistically more natural structure to pass around, and then jax is structured to understand later when AD&#x27;ing or otherwise that you&#x27;re doing so with respect to the &#x27;leaves&#x27; of the tree, or the values of the mass and velocity.<p>Hopefully someone corrects me if I&#x27;m not right about this. I&#x27;m hardly 100% on Jax&#x27;s vision on PyTrees.<p>As an aside, just a list of right hand sides `[b1, b2, ..., bm]` is valid.</div><br/></div></div></div></div></div></div></div></div><div id="41602708" class="c"><input type="checkbox" id="c-41602708" checked=""/><div class="controls bullet"><span class="by">hamilyon2</span><span>|</span><a href="#41602461">prev</a><span>|</span><a href="#41603531">next</a><span>|</span><label class="collapse" for="c-41602708">[-]</label><label class="expand" for="c-41602708">[1 more]</label></div><br/><div class="children"><div class="content">There is a bit similar project which supports Intel GPU offloading: <a href="https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;scikit-learn-intelex">https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;scikit-learn-intelex</a></div><br/></div></div><div id="41603531" class="c"><input type="checkbox" id="c-41603531" checked=""/><div class="controls bullet"><span class="by">kunalgupta022</span><span>|</span><a href="#41602708">prev</a><span>|</span><a href="#41602514">next</a><span>|</span><label class="collapse" for="c-41603531">[-]</label><label class="expand" for="c-41603531">[4 more]</label></div><br/><div class="children"><div class="content">Is anyone aware of a pandas like library that is based on something like CuPy instead of Numpy. It would be great to have the ease of use of pandas with the parallelism unlocked by gpu.</div><br/><div id="41603595" class="c"><input type="checkbox" id="c-41603595" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#41603531">parent</a><span>|</span><a href="#41603736">next</a><span>|</span><label class="collapse" for="c-41603595">[-]</label><label class="expand" for="c-41603595">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d go digging into this - <a href="https:&#x2F;&#x2F;pola.rs&#x2F;posts&#x2F;polars-on-gpu&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pola.rs&#x2F;posts&#x2F;polars-on-gpu&#x2F;</a></div><br/></div></div><div id="41603736" class="c"><input type="checkbox" id="c-41603736" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41603531">parent</a><span>|</span><a href="#41603595">prev</a><span>|</span><a href="#41603588">next</a><span>|</span><label class="collapse" for="c-41603736">[-]</label><label class="expand" for="c-41603736">[1 more]</label></div><br/><div class="children"><div class="content">cuDF is a CuPy-based library providing drop-in replacement for Pandas:
<a href="https:&#x2F;&#x2F;rapids.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rapids.ai&#x2F;</a></div><br/></div></div><div id="41603588" class="c"><input type="checkbox" id="c-41603588" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#41603531">parent</a><span>|</span><a href="#41603736">prev</a><span>|</span><a href="#41602514">next</a><span>|</span><label class="collapse" for="c-41603588">[-]</label><label class="expand" for="c-41603588">[1 more]</label></div><br/><div class="children"><div class="content">Not specifically GPU, but that’s also highly dependent on the data access pattern: <a href="https:&#x2F;&#x2F;www.dask.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.dask.org&#x2F;</a></div><br/></div></div></div></div><div id="41602514" class="c"><input type="checkbox" id="c-41602514" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#41603531">prev</a><span>|</span><a href="#41603779">next</a><span>|</span><label class="collapse" for="c-41602514">[-]</label><label class="expand" for="c-41602514">[1 more]</label></div><br/><div class="children"><div class="content">We are fans! We mostly use cudf&#x2F;cuml&#x2F;cugraph (GPU dataframes etc) in the pygraphistry ecosystem, and when things get a bit tricky, cupy is one of the main escape hatches</div><br/></div></div><div id="41602608" class="c"><input type="checkbox" id="c-41602608" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41603779">prev</a><span>|</span><label class="collapse" for="c-41602608">[-]</label><label class="expand" for="c-41602608">[14 more]</label></div><br/><div class="children"><div class="content">As an aside, since I was trying to install CuPy the other day and was having issues.<p>Open projects on github often (at least superficially) require specific versions of Cuda Toolkit (and all the specialty nvidia packages e.g. cudann), Tensorflow, etc, and changing the default versions of these for each little project, or step in a processing chain, is ridiculous.<p>pyenv et al have really made local, project specific versions of python packages much easier to manage. But I haven&#x27;t seen a similar type solution for cuda toolkit and associated packages, and the solutions I&#x27;ve encountered seem terribly hacky..but I&#x27;m sure though that this is a common issue, so what do people do?</div><br/><div id="41604084" class="c"><input type="checkbox" id="c-41604084" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41602608">parent</a><span>|</span><a href="#41605648">next</a><span>|</span><label class="collapse" for="c-41604084">[-]</label><label class="expand" for="c-41604084">[1 more]</label></div><br/><div class="children"><div class="content">As a maintainer of CuPy and also as a user of several GPU-powered Python libraries, I empathize with the frustrations and difficulties here. Indeed, one thing CuPy values is to make the installation step as easy and universal as possible. We strive to keep the binary package footprint small (currently less than 100 MiB), keep dependencies to a minimum, support wide variety of platforms including Windows and aarch64, and do not require a specific CUDA Toolkit version.<p>If anyone reading this message has encountered a roadblock while installing CuPy, please reach out. I&#x27;d be glad to help you.</div><br/></div></div><div id="41605648" class="c"><input type="checkbox" id="c-41605648" checked=""/><div class="controls bullet"><span class="by">ttyprintk</span><span>|</span><a href="#41602608">parent</a><span>|</span><a href="#41604084">prev</a><span>|</span><a href="#41605503">next</a><span>|</span><label class="collapse" for="c-41605648">[-]</label><label class="expand" for="c-41605648">[1 more]</label></div><br/><div class="children"><div class="content">You can jam the argument cudatoolkit=1.2.3 when creating conda environments.<p>NB I’m using Miniforge.</div><br/></div></div><div id="41605503" class="c"><input type="checkbox" id="c-41605503" checked=""/><div class="controls bullet"><span class="by">mardifoufs</span><span>|</span><a href="#41602608">parent</a><span>|</span><a href="#41605648">prev</a><span>|</span><a href="#41602745">next</a><span>|</span><label class="collapse" for="c-41605503">[-]</label><label class="expand" for="c-41605503">[1 more]</label></div><br/><div class="children"><div class="content">One way to do it is to explicitly add the link to say, the pytorch+CUDA wheel from the pytorch repos in your requirements.txt instead of using the normal pypi package. Which also sucks because you then have to do some other tweaks to make your requirements.txt portable across different platforms...<p>(and you can&#x27;t just add another index for pip to look for if you want to use python build so it has to be explicitly linked to the right wheel, which absolutely sucks especially since you cannot get the CUDA version from pypi)</div><br/></div></div><div id="41602745" class="c"><input type="checkbox" id="c-41602745" checked=""/><div class="controls bullet"><span class="by">coeneedell</span><span>|</span><a href="#41602608">parent</a><span>|</span><a href="#41605503">prev</a><span>|</span><a href="#41602916">next</a><span>|</span><label class="collapse" for="c-41602745">[-]</label><label class="expand" for="c-41602745">[3 more]</label></div><br/><div class="children"><div class="content">Ugh… docker containers. I also wish there was a simpler way but I don’t think there is.</div><br/><div id="41602757" class="c"><input type="checkbox" id="c-41602757" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41602608">root</a><span>|</span><a href="#41602745">parent</a><span>|</span><a href="#41602916">next</a><span>|</span><label class="collapse" for="c-41602757">[-]</label><label class="expand" for="c-41602757">[2 more]</label></div><br/><div class="children"><div class="content">this is not what I wanted to hear. NOT AT ALL. Please whisper sweet lies into my ears.</div><br/><div id="41602859" class="c"><input type="checkbox" id="c-41602859" checked=""/><div class="controls bullet"><span class="by">coeneedell</span><span>|</span><a href="#41602608">root</a><span>|</span><a href="#41602757">parent</a><span>|</span><a href="#41602916">next</a><span>|</span><label class="collapse" for="c-41602859">[-]</label><label class="expand" for="c-41602859">[1 more]</label></div><br/><div class="children"><div class="content">At the moment I’m working on a system to quickly replicate academic deep learning repos (papers) at scale. At least Amazon has a catalogue of prebuilt containers with cuda&#x2F;pytorch combos. I still occasionally have an issue where the container works on my 3090 test bench but not on the T4 cloud node…</div><br/></div></div></div></div></div></div><div id="41602916" class="c"><input type="checkbox" id="c-41602916" checked=""/><div class="controls bullet"><span class="by">m_d_</span><span>|</span><a href="#41602608">parent</a><span>|</span><a href="#41602745">prev</a><span>|</span><a href="#41603871">next</a><span>|</span><label class="collapse" for="c-41602916">[-]</label><label class="expand" for="c-41602916">[5 more]</label></div><br/><div class="children"><div class="content">conda provides cudatoolkit and associated packages. Does this solve the situation?</div><br/><div id="41603127" class="c"><input type="checkbox" id="c-41603127" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#41602608">root</a><span>|</span><a href="#41602916">parent</a><span>|</span><a href="#41605637">next</a><span>|</span><label class="collapse" for="c-41603127">[-]</label><label class="expand" for="c-41603127">[3 more]</label></div><br/><div class="children"><div class="content">The condos 200-employee threshold licence change is problematic for some.</div><br/><div id="41603318" class="c"><input type="checkbox" id="c-41603318" checked=""/><div class="controls bullet"><span class="by">boldlybold</span><span>|</span><a href="#41602608">root</a><span>|</span><a href="#41603127">parent</a><span>|</span><a href="#41605637">next</a><span>|</span><label class="collapse" for="c-41603318">[-]</label><label class="expand" for="c-41603318">[2 more]</label></div><br/><div class="children"><div class="content">As long as you stay out of the &quot;defaults&quot; and &quot;anaconda&quot; repos, you&#x27;re not subject to that license. For my needs conda-forge and bioconda have everything. I&#x27;m not sure about the nvidia repo but I assume it&#x27;s similar.</div><br/><div id="41603674" class="c"><input type="checkbox" id="c-41603674" checked=""/><div class="controls bullet"><span class="by">kmaehashi</span><span>|</span><a href="#41602608">root</a><span>|</span><a href="#41603318">parent</a><span>|</span><a href="#41605637">next</a><span>|</span><label class="collapse" for="c-41603674">[-]</label><label class="expand" for="c-41603674">[1 more]</label></div><br/><div class="children"><div class="content">Actually all CUDA Toolkit libs are already available through the conda-forge channel: <a href="https:&#x2F;&#x2F;anaconda.org&#x2F;conda-forge&#x2F;cuda-cudart" rel="nofollow">https:&#x2F;&#x2F;anaconda.org&#x2F;conda-forge&#x2F;cuda-cudart</a>, <a href="https:&#x2F;&#x2F;anaconda.org&#x2F;conda-forge&#x2F;libcublas" rel="nofollow">https:&#x2F;&#x2F;anaconda.org&#x2F;conda-forge&#x2F;libcublas</a>, etc.</div><br/></div></div></div></div></div></div><div id="41605637" class="c"><input type="checkbox" id="c-41605637" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41602608">root</a><span>|</span><a href="#41602916">parent</a><span>|</span><a href="#41603127">prev</a><span>|</span><a href="#41603871">next</a><span>|</span><label class="collapse" for="c-41605637">[-]</label><label class="expand" for="c-41605637">[1 more]</label></div><br/><div class="children"><div class="content">Actually yes it does....except I seem to remember that it doesn&#x27;t go back that far in cuda versions. I can&#x27;t seem to find it again right now.</div><br/></div></div></div></div><div id="41603871" class="c"><input type="checkbox" id="c-41603871" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41602608">parent</a><span>|</span><a href="#41602916">prev</a><span>|</span><label class="collapse" for="c-41603871">[-]</label><label class="expand" for="c-41603871">[2 more]</label></div><br/><div class="children"><div class="content">in real life everyone just uses containers, might not be the answer you want to hear though</div><br/><div id="41606296" class="c"><input type="checkbox" id="c-41606296" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41602608">root</a><span>|</span><a href="#41603871">parent</a><span>|</span><label class="collapse" for="c-41606296">[-]</label><label class="expand" for="c-41606296">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m okay with containers generally, I think. Is this a situation where you put your code into the container and run it, or does the code make calls to the container&#x27;s gpu?</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>