<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719565266066" as="style"/><link rel="stylesheet" href="styles.css?v=1719565266066"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/">CriticGPT: Finding GPT-4&#x27;s mistakes with GPT-4</a> <span class="domain">(<a href="https://openai.com">openai.com</a>)</span></div><div class="subtext"><span>davidbarker</span> | <span>214 comments</span></div><br/><div><div id="40817049" class="c"><input type="checkbox" id="c-40817049" checked=""/><div class="controls bullet"><span class="by">upwardbound</span><span>|</span><a href="#40818848">next</a><span>|</span><label class="collapse" for="c-40817049">[-]</label><label class="expand" for="c-40817049">[6 more]</label></div><br/><div class="children"><div class="content">For those new to the field of AGI safety: this is an implementation of Paul Christiano&#x27;s alignment procedure proposal called Iterated Amplification from 6 years ago.  <a href="https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;s&#x2F;EmDuGeRw749sD3GKd" rel="nofollow">https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;s&#x2F;EmDuGeRw749sD3GKd</a><p>According to his website he previously ran the language model alignment team at OpenAI.  <a href="https:&#x2F;&#x2F;paulfchristiano.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;paulfchristiano.com&#x2F;</a><p>It&#x27;s wonderful to see his idea coming to fruition!  I&#x27;m honestly a bit skeptical of the idea myself (it&#x27;s like proposing to stabilize the stack of &quot;turtles all the way down&quot; by adding more turtles - as is insightfully pointed out in this other comment <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40817017">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40817017</a>) but every innovative idea is worth a try, in a field as time-critical and urgent as AGI safety.<p>For a good summary of technical approaches to AGI safety, start with the Future of Life Institute AI Alignment Podcast, especially these two episodes which serve as an overview of the field:<p>- <a href="https:&#x2F;&#x2F;futureoflife.org&#x2F;podcast&#x2F;an-overview-of-technical-ai-alignment-with-rohin-shah-part-1&#x2F;" rel="nofollow">https:&#x2F;&#x2F;futureoflife.org&#x2F;podcast&#x2F;an-overview-of-technical-ai...</a><p>- <a href="https:&#x2F;&#x2F;futureoflife.org&#x2F;podcast&#x2F;an-overview-of-technical-ai-alignment-with-rohin-shah-part-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;futureoflife.org&#x2F;podcast&#x2F;an-overview-of-technical-ai...</a><p>In both of those episodes, Cristiano&#x27;s publication series on Iterated Amplification is link #3 in the list of recommended reading.</div><br/><div id="40818326" class="c"><input type="checkbox" id="c-40818326" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40817049">parent</a><span>|</span><a href="#40817084">next</a><span>|</span><label class="collapse" for="c-40818326">[-]</label><label class="expand" for="c-40818326">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>it&#x27;s like proposing to stabilize the stack of &quot;turtles all the way down&quot; by adding more turtles</i><p>That&#x27;s completely fine. Say each layer uses the same amount of turtles, but half the size of the layer above. Even allowing for arbitrarily small turtles, the total height of the infinite stack will be just 2x the height of the first layer.<p>Point being, some series converge to a finite result, including some defined recursively. And in practice, we can usually cut the recursion after first couple steps, as the infinitely long remainder has negligible impact on the final result.</div><br/><div id="40818474" class="c"><input type="checkbox" id="c-40818474" checked=""/><div class="controls bullet"><span class="by">topherclay</span><span>|</span><a href="#40817049">root</a><span>|</span><a href="#40818326">parent</a><span>|</span><a href="#40817084">next</a><span>|</span><label class="collapse" for="c-40818474">[-]</label><label class="expand" for="c-40818474">[1 more]</label></div><br/><div class="children"><div class="content">You seem to have interpreted the analogy as meaning &quot;you might run out of turtles&quot; instead of something like: &quot;stacks of turtles aren&#x27;t stable without stable beneath them, no matter how many turtles you use.&quot;</div><br/></div></div></div></div><div id="40817084" class="c"><input type="checkbox" id="c-40817084" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40817049">parent</a><span>|</span><a href="#40818326">prev</a><span>|</span><a href="#40818651">next</a><span>|</span><label class="collapse" for="c-40817084">[-]</label><label class="expand" for="c-40817084">[2 more]</label></div><br/><div class="children"><div class="content">I am not sure why you didnt see the citations to 3 different papers from Cristiano. A simple search in the linked PDF suffices: citations 12, 19, and 31.</div><br/><div id="40817098" class="c"><input type="checkbox" id="c-40817098" checked=""/><div class="controls bullet"><span class="by">upwardbound</span><span>|</span><a href="#40817049">root</a><span>|</span><a href="#40817084">parent</a><span>|</span><a href="#40818651">next</a><span>|</span><label class="collapse" for="c-40817098">[-]</label><label class="expand" for="c-40817098">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for providing the reference numbers, this is helpful!  I&#x27;ll update my GP comment.</div><br/></div></div></div></div><div id="40818651" class="c"><input type="checkbox" id="c-40818651" checked=""/><div class="controls bullet"><span class="by">curiousgal</span><span>|</span><a href="#40817049">parent</a><span>|</span><a href="#40817084">prev</a><span>|</span><a href="#40818848">next</a><span>|</span><label class="collapse" for="c-40818651">[-]</label><label class="expand" for="c-40818651">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>AGI safety</i><p>I genuinely laughed. Oh no somebody please save me from a chatbot that&#x27;s hallucinating half the time!<p>Joke aside, of course OpenAI is gonna play up how &quot;intelligent&quot; its models are. But it&#x27;s evident that there&#x27;s only so much data and compute that you can throw at a machine to make it smart.</div><br/></div></div></div></div><div id="40818848" class="c"><input type="checkbox" id="c-40818848" checked=""/><div class="controls bullet"><span class="by">realgenious</span><span>|</span><a href="#40817049">prev</a><span>|</span><a href="#40814847">next</a><span>|</span><label class="collapse" for="c-40818848">[-]</label><label class="expand" for="c-40818848">[1 more]</label></div><br/><div class="children"><div class="content">I wonder whether the  hallucinations effect is  rooted because the current LLMs are not designed to keep a score from the source of their knowledge, so that the knowledge could be structured by threads (math, philosophy, medicine, etc), I imagine that the current LLMs simulate that score by storing some context in the weights, but that mechanism is not enough to produce good results.</div><br/></div></div><div id="40814847" class="c"><input type="checkbox" id="c-40814847" checked=""/><div class="controls bullet"><span class="by">lowyek</span><span>|</span><a href="#40818848">prev</a><span>|</span><a href="#40815674">next</a><span>|</span><label class="collapse" for="c-40814847">[-]</label><label class="expand" for="c-40814847">[52 more]</label></div><br/><div class="children"><div class="content">I find it fascinating that while in other fields you see lot of theorums&#x2F;results much before practical results are found. But in this forefront of innovation - I have hardly seen any paper discussing hallucinations and lowerbound&#x2F;upperbound on that. Or may be I didn&#x27;t open hacker news on that right day when it was published. Would love to understand the hallucination phenomena more deeply and the mathematics behind it.</div><br/><div id="40815177" class="c"><input type="checkbox" id="c-40815177" checked=""/><div class="controls bullet"><span class="by">hbn</span><span>|</span><a href="#40814847">parent</a><span>|</span><a href="#40815160">next</a><span>|</span><label class="collapse" for="c-40815177">[-]</label><label class="expand" for="c-40815177">[41 more]</label></div><br/><div class="children"><div class="content">&gt; the hallucination phenomena<p>There isn&#x27;t really such thing as a &quot;hallucination&quot; and honestly I think people should be using the word less. Whether an LLM tells you the sky is blue or the sky is purple, it&#x27;s not doing anything different. It&#x27;s just spitting out a sequence of characters it was trained be hopefully what a user wants. There is no definable failure state you can call a &quot;hallucination,&quot; it&#x27;s operating as correctly as any other output. But sometimes we can tell either immediately or through fact checking it spat out a string of text that claims something incorrect.<p>If you start asking an LLM for political takes, you&#x27;ll get very different answers from humans about which ones are &quot;hallucinations&quot;</div><br/><div id="40815823" class="c"><input type="checkbox" id="c-40815823" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40816112">next</a><span>|</span><label class="collapse" for="c-40815823">[-]</label><label class="expand" for="c-40815823">[19 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why the narrative became &quot;don&#x27;t call it hallucination&quot;. Grantly English isn&#x27;t my mother tongue so I might miss some subtlty here. If you know how LLM works, call it &quot;hallucination&quot; doesn&#x27;t make you know less. If you don&#x27;t know how LLM works, using &quot;hallucination&quot; doesn&#x27;t make you know less either. It&#x27;s just a word meaning AI gives wrong[1] answer.<p>People say it&#x27;s &quot;anthropomorphizing&quot; but honestly I can&#x27;t see it. The I in AI stands for intelligence, is this anthropomorphizing? L in ML? Reading and writing are clearly human activities, so is using read&#x2F;write instead of input&#x2F;output anthropomorphizing? How about &quot;computer&quot;, a word once meant a human who does computing? Is there a word we can use safely without anthropomorphizing?<p>[1]: And please don&#x27;t argue what&#x27;s &quot;wrong&quot;.</div><br/><div id="40818516" class="c"><input type="checkbox" id="c-40818516" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815823">parent</a><span>|</span><a href="#40816239">next</a><span>|</span><label class="collapse" for="c-40818516">[-]</label><label class="expand" for="c-40818516">[1 more]</label></div><br/><div class="children"><div class="content">I suspect it’s about marketing. I’m not sure it would be so easy to sell these tools to enterprise organisations if you outlined that they are basically just very good at being lucky. With the abstraction of hallucinations you sort of put into language why your tool is sometimes very wrong.<p>To me the real danger comes from when the models get things wrong but also correct at the same time. Not so much in software engineering, I doubt your average programmer without LLM tools will write “better” code without getting some bad answers. What consents me is more how non-technical departments implement LLMs into their decision making or analysis systems.<p>Done right, it’ll enhance your capabilities. We had a major AI project in cancer detection, and while it actually works it also doesn’t really work on its own. Obviously it was meant to enhance the regular human detection and anyone involved with the project screamed this loudly at any chance they got. Naturally it was seen as an automation process by the upper management and all the humans parts of the process were basically replaced… until a few years later when we had a huge scandal about how the AI worked as it was meant to do, which wasn’t to be on its own. Today it works along side the human detection systems and their quality is up. It took people literally dying to get that point through.<p>Maybe it would’ve happened this way anyway if the mistakes weren’t sort of written into this technical issue we call hallucinations. Maybe it wouldn’t. From personal experience with getting projects to be approved, I think abstractions are always a great way to hide the things you don’t want your decision makers to know.</div><br/></div></div><div id="40816239" class="c"><input type="checkbox" id="c-40816239" checked=""/><div class="controls bullet"><span class="by">Affric</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815823">parent</a><span>|</span><a href="#40818516">prev</a><span>|</span><a href="#40816364">next</a><span>|</span><label class="collapse" for="c-40816239">[-]</label><label class="expand" for="c-40816239">[3 more]</label></div><br/><div class="children"><div class="content">The AI companies don’t want you “anthropomorphising” the models because it would put them at risk of increased liability.<p>You will be told that linear algebra is just a model and the fact that epistemology has never turned up a decent result for what knowledge is will be ignored.<p>We are meant to believe that we are somehow special magical creatures and that the behaviour of our minds cannot be modelled by linear algebra.</div><br/><div id="40818368" class="c"><input type="checkbox" id="c-40818368" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816239">parent</a><span>|</span><a href="#40818360">next</a><span>|</span><label class="collapse" for="c-40818368">[-]</label><label class="expand" for="c-40818368">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how anthropomorphism reduces liability.<p>If a company does a thing that&#x27;s bad, it doesn&#x27;t matter much if the work itself was performed by a blacksmith or by a robot arm in a lights-off factory.<p>&gt; We are meant to believe that we are somehow special magical creatures and that the behaviour of our minds cannot be modelled by linear algebra<p>I only hear this from people who say AI will never reach human level; of AI developers that get press time, only LeCun seems so dismissive (though I&#x27;ve not actually noticed him making this specific statement, I can believe he might have).</div><br/></div></div><div id="40818360" class="c"><input type="checkbox" id="c-40818360" checked=""/><div class="controls bullet"><span class="by">duggan</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816239">parent</a><span>|</span><a href="#40818368">prev</a><span>|</span><a href="#40816364">next</a><span>|</span><label class="collapse" for="c-40818360">[-]</label><label class="expand" for="c-40818360">[1 more]</label></div><br/><div class="children"><div class="content">No, you’re just meant not to assert that linear algebra is equivalent to any process in the human brain, when the human brain is not understood well enough to draw that conclusion.</div><br/></div></div></div></div><div id="40816364" class="c"><input type="checkbox" id="c-40816364" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815823">parent</a><span>|</span><a href="#40816239">prev</a><span>|</span><a href="#40816697">next</a><span>|</span><label class="collapse" for="c-40816364">[-]</label><label class="expand" for="c-40816364">[6 more]</label></div><br/><div class="children"><div class="content"><i>It&#x27;s just a word meaning AI gives wrong answer.</i><p>No, it’s more specific than just wrong.<p>Hallucination is when a model creates a bit of fictitious knowledge, and uses that knowledge to answer a question.</div><br/><div id="40817315" class="c"><input type="checkbox" id="c-40817315" checked=""/><div class="controls bullet"><span class="by">olalonde</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816364">parent</a><span>|</span><a href="#40818040">next</a><span>|</span><label class="collapse" for="c-40817315">[-]</label><label class="expand" for="c-40817315">[3 more]</label></div><br/><div class="children"><div class="content">Can you give an example of a &quot;wrong&quot; answer vs an &quot;hallucinated&quot; answer?</div><br/><div id="40818552" class="c"><input type="checkbox" id="c-40818552" checked=""/><div class="controls bullet"><span class="by">stefanve</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40817315">parent</a><span>|</span><a href="#40817945">next</a><span>|</span><label class="collapse" for="c-40818552">[-]</label><label class="expand" for="c-40818552">[1 more]</label></div><br/><div class="children"><div class="content">there are many types of a wrong answer, and the difference is based on how the answer came to be. In case of BS&#x2F;Hallucination there is no reason or logic behind the answer it is basically, in the case of LLM, just random text. There was no reasoning behind the output or it wasn&#x27;t based on facts.<p>You can argue if it matters how a wrong answer came about ofc but there is a difference</div><br/></div></div><div id="40817945" class="c"><input type="checkbox" id="c-40817945" checked=""/><div class="controls bullet"><span class="by">omikun</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40817315">parent</a><span>|</span><a href="#40818552">prev</a><span>|</span><a href="#40818040">next</a><span>|</span><label class="collapse" for="c-40817945">[-]</label><label class="expand" for="c-40817945">[1 more]</label></div><br/><div class="children"><div class="content">The issue is there is no difference between a right answer and a hallucinated answer.</div><br/></div></div></div></div><div id="40818040" class="c"><input type="checkbox" id="c-40818040" checked=""/><div class="controls bullet"><span class="by">ruszki</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816364">parent</a><span>|</span><a href="#40817315">prev</a><span>|</span><a href="#40816697">next</a><span>|</span><label class="collapse" for="c-40818040">[-]</label><label class="expand" for="c-40818040">[2 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t need to create wrong answers. It&#x27;s enough to recall people who gave wrong answers.</div><br/><div id="40818391" class="c"><input type="checkbox" id="c-40818391" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40818040">parent</a><span>|</span><a href="#40816697">next</a><span>|</span><label class="collapse" for="c-40818391">[-]</label><label class="expand" for="c-40818391">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard the term originated in image recognition, where models would &quot;see&quot; things that weren&#x27;t there.<p>You can still get that with zero bad labels in a supervised training set.<p>Multiple causes for the same behaviour makes progress easier, but knowing if it&#x27;s fully solved harder.</div><br/></div></div></div></div></div></div><div id="40816697" class="c"><input type="checkbox" id="c-40816697" checked=""/><div class="controls bullet"><span class="by">VanillaCafe</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815823">parent</a><span>|</span><a href="#40816364">prev</a><span>|</span><a href="#40817768">next</a><span>|</span><label class="collapse" for="c-40816697">[-]</label><label class="expand" for="c-40816697">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t know why the narrative became &quot;don&#x27;t call it hallucination&quot;.<p>Context is &quot;don&#x27;t call it hallicination&quot; picked up meme energy since <a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10676-024-09775-5" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10676-024-09775-5</a> on the thesis that &quot;Calling their mistakes ‘hallucinations’ isn’t harmless: it lends itself to the confusion that the machines are in some way misperceiving but are nonetheless trying to convey something that they believe or have perceived.&quot;<p>Which is meta-bullshit because it doesn&#x27;t matter. We want LLMs to behave more factually, whatever the non-factuality is called. And calling that non-factuality something else isn&#x27;t going to really change how we approach making them behave more factually.</div><br/><div id="40817813" class="c"><input type="checkbox" id="c-40817813" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816697">parent</a><span>|</span><a href="#40817768">next</a><span>|</span><label class="collapse" for="c-40817813">[-]</label><label class="expand" for="c-40817813">[3 more]</label></div><br/><div class="children"><div class="content">How are LLMs not behaving factually? They already predict the next most likely term.<p>If they could predict facts, then these would be gods, not machines. It would be saying that in all the written content we have, there exists a pattern that allows us to predict all answers to questions we may have.</div><br/><div id="40818014" class="c"><input type="checkbox" id="c-40818014" checked=""/><div class="controls bullet"><span class="by">ijk</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40817813">parent</a><span>|</span><a href="#40817768">next</a><span>|</span><label class="collapse" for="c-40818014">[-]</label><label class="expand" for="c-40818014">[2 more]</label></div><br/><div class="children"><div class="content">The problem is that some people are running around and saying they are gods. Which I wouldn&#x27;t care about, but an alarming number of people do believe that they can predict facts.</div><br/><div id="40818284" class="c"><input type="checkbox" id="c-40818284" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40818014">parent</a><span>|</span><a href="#40817768">next</a><span>|</span><label class="collapse" for="c-40818284">[-]</label><label class="expand" for="c-40818284">[1 more]</label></div><br/><div class="children"><div class="content">Our system can effectively predict facts.<p>It logics its way to it.<p>By predicting the next word in a sequence of words.<p>Sure? It kinda sounds plausible? But man, if it’s that straight forward, what have we been doing as a species for so many years ?</div><br/></div></div></div></div></div></div></div></div><div id="40817768" class="c"><input type="checkbox" id="c-40817768" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815823">parent</a><span>|</span><a href="#40816697">prev</a><span>|</span><a href="#40815877">next</a><span>|</span><label class="collapse" for="c-40817768">[-]</label><label class="expand" for="c-40817768">[1 more]</label></div><br/><div class="children"><div class="content">TLDR TLDR: Assuming we dont argue right&#x2F;wrong, technically everything an LLM does is a hallucination. This completely dilutes the meaning of the word no?<p>TLDR: Sure. A rose by any other name would be just as sweet. It’s when I use the name of the rose and imply aspects that are not present, that we create confusion and busy work.<p>Hey, calling it a narrative is to move it to PR speak. I know people have argued this term was incorrect since the first times it was ever shared on HN.<p>It was unpopular to say this when ChatGPT launched, because chatGPT was just that. freaking. cool.<p>It is still cool.<p>But it is not AGI. It does not “think”.<p>Hell - I understand that we will be doing multiple columns of turtles all the way down. I have a different name for this approach - statistical committees.<p>Because we couched its work in terms of “thinking”, “logic”, “creativity”, we have dumped countless man hours and money into avenues which are not fruitful. And this isnt just me saying it - even Ilya commented during some event that many people can create PoCs, but there are very few production grade tools.<p>Regarding the L in ML, and the I in AI -&gt;<p>1) ML and AI were never quite as believable as ChatGPT. Calling it learning and intelligence doesnt result in the same level of ambiguity.<p>2) A little bit of anthropomorphizing was going on.<p>Terms matter, especially at the start. New things get understood over time, as we progress we do move to better terms. Let’s use hallucinations for when a digital system really starts hallucinating.</div><br/></div></div><div id="40815877" class="c"><input type="checkbox" id="c-40815877" checked=""/><div class="controls bullet"><span class="by">th0ma5</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815823">parent</a><span>|</span><a href="#40817768">prev</a><span>|</span><a href="#40816529">next</a><span>|</span><label class="collapse" for="c-40815877">[-]</label><label class="expand" for="c-40815877">[2 more]</label></div><br/><div class="children"><div class="content">AI is a nebulous, undefined term, and many people specifically criticize the use of the word intelligent.</div><br/><div id="40816244" class="c"><input type="checkbox" id="c-40816244" checked=""/><div class="controls bullet"><span class="by">Affric</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815877">parent</a><span>|</span><a href="#40816529">next</a><span>|</span><label class="collapse" for="c-40816244">[-]</label><label class="expand" for="c-40816244">[1 more]</label></div><br/><div class="children"><div class="content">Always people who consider themselves intelligent</div><br/></div></div></div></div></div></div><div id="40816112" class="c"><input type="checkbox" id="c-40816112" checked=""/><div class="controls bullet"><span class="by">hatthew</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40815823">prev</a><span>|</span><a href="#40816847">next</a><span>|</span><label class="collapse" for="c-40816112">[-]</label><label class="expand" for="c-40816112">[2 more]</label></div><br/><div class="children"><div class="content">LLMs model their corpus, which for most models tends to be <i>factually correct</i> text (or subjective text with no factuality). Sure, there exist factually incorrect statements in the corpus, but for the vast majority of incorrect statements there exist many more equivalent but correct statements. If an LLM makes a statement that is not supported by the training data (either because it doesn&#x27;t exist or because the equivalent correct statement is more strongly supported), I think that&#x27;s an issue with the implementation of the model. I don&#x27;t think it&#x27;s an intrinsic feature&#x2F;flaw in what the model is modeling.<p>Hallucination might not be the best word, but I don&#x27;t think it&#x27;s a bad word. If a weather model predicted a storm when there isn&#x27;t a cloud in the sky, I wouldn&#x27;t have a problem with saying &quot;the weather model had a hallucination.&quot; 50 years ago, weather models made incorrect predictions quite frequently. That&#x27;s not because they weren&#x27;t modeling correct weather, it&#x27;s because we simply didn&#x27;t yet have good models and clean data.<p>Fundamentally, we could fix most LLM hallucinations with better model implementations and cleaner data. In the future we will probably be able to model factuality outside of the context of human language, and that will probably be the ultimate solution for correctness in AI, but I don&#x27;t think that&#x27;s a fundamental <i>requirement</i>.</div><br/><div id="40817837" class="c"><input type="checkbox" id="c-40817837" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816112">parent</a><span>|</span><a href="#40816847">next</a><span>|</span><label class="collapse" for="c-40817837">[-]</label><label class="expand" for="c-40817837">[1 more]</label></div><br/><div class="children"><div class="content">I suspect you would fix first response accuracy.<p>People still want it to be used for thinking.<p>This isnt going to happen with better data. Better data means it will be better at predicting the next token.<p>For questions or interactions where you need to process, consider, decompose a problem into multiple steps, solve those steps etc - you need to have a goal, tools, and the ability to split your thinking and govern the outcome.<p>That isnt predicting the next token. I think it’s easier to think of LLMs as doing decompression.<p>They take an initial set of tokens and decompress them into the most likely final set of tokens.<p>What we want is processing.<p>We would have to set up the reaction to somehow perfectly result in the next set of tokens to then set up the next set of tokens etc - till the system has an answer.<p>Or in other words, we have to figure out how to phrase an initial set of tokens so that each subsequent set looks similar enough to “logic” in the training data, that the LLM expands correctly.</div><br/></div></div></div></div><div id="40816847" class="c"><input type="checkbox" id="c-40816847" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40816112">prev</a><span>|</span><a href="#40816044">next</a><span>|</span><label class="collapse" for="c-40816847">[-]</label><label class="expand" for="c-40816847">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There isn&#x27;t really such thing as a &quot;hallucination&quot; and honestly I think people should be using the word less. Whether an LLM tells you the sky is blue or the sky is purple, it&#x27;s not doing anything different. It&#x27;s just spitting out a sequence of characters it was trained be hopefully what a user wants. There is no definable failure state you can call a &quot;hallucination,&quot; it&#x27;s operating as correctly as any other output.<p>This is a very &quot;closed world&quot; view of the phenomenon which looks at an LLM as a software component on its own.<p>But &quot;hallucination&quot; is a user experience problem, and it describes the experience very well. If you are using a code assistant and it suggests using APIs that don&#x27;t exist then the word &quot;hallucination&quot; is entirely appropriate.<p>A vaguely similar analogy is the addition of the `let` and `const` keywords in JS ES6. While the behavior of `var` was &quot;correct&quot; as-per spec the user experience was horrible: bug prone and confusing.</div><br/></div></div><div id="40816044" class="c"><input type="checkbox" id="c-40816044" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40816847">prev</a><span>|</span><a href="#40815383">next</a><span>|</span><label class="collapse" for="c-40816044">[-]</label><label class="expand" for="c-40816044">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the new &quot;serverless&quot; and I would really like people to stop making the discussion between about the word. You know what it means, I know what it means, let&#x27;s all move on.<p>We won&#x27;t, and we&#x27;ll see this constant distraction.</div><br/><div id="40816104" class="c"><input type="checkbox" id="c-40816104" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816044">parent</a><span>|</span><a href="#40816586">next</a><span>|</span><label class="collapse" for="c-40816104">[-]</label><label class="expand" for="c-40816104">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s the new &quot;serverless&quot; and I would really like people to stop making the discussion between about the word. You know what it means, I know what it means, let&#x27;s all move on.<p>Well, parent is lamenting the lack of lowerbound&#x2F;upperbound for &quot;hallucinations&quot;, something that cannot realistically exist as &quot;hallucinations&quot; don&#x27;t exist. LLMs aren&#x27;t fact-outputting machines, so when it outputs something a human would consider &quot;wrong&quot; like &quot;the sky is purple&quot;, it isn&#x27;t true&#x2F;false&#x2F;correct&#x2F;incorrect&#x2F;hallucination&#x2F;fact, it&#x27;s just the most probable character after the next.<p>That&#x27;s why it isn&#x27;t useful to ask &quot;but how much it hallucinates?&quot; when in reality what you&#x27;re out after is something more like &quot;does it only output facts?&quot;. Which, if it did, LLMs would be a lot less useful.</div><br/><div id="40818706" class="c"><input type="checkbox" id="c-40818706" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816104">parent</a><span>|</span><a href="#40816609">next</a><span>|</span><label class="collapse" for="c-40818706">[-]</label><label class="expand" for="c-40818706">[1 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s just the most probable character after the next.<p>That&#x27;s simply not true. You&#x27;re confusing how they&#x27;re trained and what they do. They don&#x27;t have some store of exactly how likely each word is (and it&#x27;s worth stopping to think about what that would even mean) for every possible sentence.</div><br/></div></div><div id="40816609" class="c"><input type="checkbox" id="c-40816609" checked=""/><div class="controls bullet"><span class="by">elif</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816104">parent</a><span>|</span><a href="#40818706">prev</a><span>|</span><a href="#40816586">next</a><span>|</span><label class="collapse" for="c-40816609">[-]</label><label class="expand" for="c-40816609">[2 more]</label></div><br/><div class="children"><div class="content">There is a huge gap between &quot;facts&quot; and &quot;nonfacts&quot; which compose the majority of human discourse. Statements, opinions, questions, when properly qualified, are not facts or nonfacts or hallucinations.<p>LLM don&#x27;t need to be perfect fact machines at all to be honest, and non-hallucinating. They simply need to ground statements in other grounded statements and identify the parts which are speculative or non-grounded.</div><br/><div id="40818358" class="c"><input type="checkbox" id="c-40818358" checked=""/><div class="controls bullet"><span class="by">kreyenborgi</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816609">parent</a><span>|</span><a href="#40816586">next</a><span>|</span><label class="collapse" for="c-40818358">[-]</label><label class="expand" for="c-40818358">[1 more]</label></div><br/><div class="children"><div class="content">If you simply want to ground statements in statements, you quickly get into GOFAI territory where you need to build up the full semantics of a sentence (in all supported languages) in order to prove that two sentences mean the same or have the same denotation or that one entails the other.<p>Otherwise, how do you prove the grounding isn&#x27;t &quot;hallucinated&quot;?</div><br/></div></div></div></div></div></div><div id="40816586" class="c"><input type="checkbox" id="c-40816586" checked=""/><div class="controls bullet"><span class="by">elif</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40816044">parent</a><span>|</span><a href="#40816104">prev</a><span>|</span><a href="#40815383">next</a><span>|</span><label class="collapse" for="c-40816586">[-]</label><label class="expand" for="c-40816586">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&quot;you know what it means, I know what it means&quot;<p>It is somewhat humorous when humans have ontological objections to the neologisms used to describe a system whose entire function is to relate the meanings of words. It is almost as if the complaint is itself a repressed philosophical rejection of the underlying LLM process, only being wrapped in the apparent misalignment of the term hallucination.<p>The complaint may as well be a defensive clinging &quot;nuh uh, you can&#x27;t decide what words mean, only I can&quot;<p>Perhaps the term &quot;gas lighting&quot; is also an appropriate replacement of &quot;hallucination,&quot; one which is not predicated on some form of truthiness standard, but rather THIS neologism focuses on the manipulative expression of the lie.</div><br/></div></div></div></div><div id="40815383" class="c"><input type="checkbox" id="c-40815383" checked=""/><div class="controls bullet"><span class="by">mortenjorck</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40816044">prev</a><span>|</span><a href="#40815526">next</a><span>|</span><label class="collapse" for="c-40815383">[-]</label><label class="expand" for="c-40815383">[5 more]</label></div><br/><div class="children"><div class="content">It is an unfortunately anthropomorphizing term for a transformer simply operating as designed, but the thing it&#x27;s become a vernacular shorthand for, &quot;outputting a sequence of tokens representing a claim that can be uncontroversially disproven,&quot; is still a useful concept.<p>There&#x27;s definitely room for a better label, though. &quot;Empirical mismatch&quot; doesn&#x27;t quite have the same ring as &quot;hallucination,&quot; but it&#x27;s probably a more accurate place to start from.</div><br/><div id="40815448" class="c"><input type="checkbox" id="c-40815448" checked=""/><div class="controls bullet"><span class="by">NovemberWhiskey</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815383">parent</a><span>|</span><a href="#40815456">next</a><span>|</span><label class="collapse" for="c-40815448">[-]</label><label class="expand" for="c-40815448">[2 more]</label></div><br/><div class="children"><div class="content">&gt;<i>&quot;outputting a sequence of tokens representing a claim that can be uncontroversially disproven,&quot; is still a useful concept.</i><p>Sure, but that would require semantic mechanisms rather than statistical ones.</div><br/><div id="40818280" class="c"><input type="checkbox" id="c-40818280" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815448">parent</a><span>|</span><a href="#40815456">next</a><span>|</span><label class="collapse" for="c-40818280">[-]</label><label class="expand" for="c-40818280">[1 more]</label></div><br/><div class="children"><div class="content">Statistics has a semantics all its own</div><br/></div></div></div></div><div id="40815456" class="c"><input type="checkbox" id="c-40815456" checked=""/><div class="controls bullet"><span class="by">hbn</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815383">parent</a><span>|</span><a href="#40815448">prev</a><span>|</span><a href="#40815526">next</a><span>|</span><label class="collapse" for="c-40815456">[-]</label><label class="expand" for="c-40815456">[2 more]</label></div><br/><div class="children"><div class="content">Regardless I don&#x27;t think there&#x27;s much to write papers on, other than maybe an anthropological look at how it&#x27;s affected people putting too much trust into LLMs for research, decision-making, etc.<p>If someone wants info to make their model to be more reliable for a specific domain, it&#x27;s in the existing papers on model training.</div><br/></div></div></div></div><div id="40815526" class="c"><input type="checkbox" id="c-40815526" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40815383">prev</a><span>|</span><a href="#40815669">next</a><span>|</span><label class="collapse" for="c-40815526">[-]</label><label class="expand" for="c-40815526">[1 more]</label></div><br/><div class="children"><div class="content">Chess engines, which are used for 25 years by the best human chess players daily, compute the best next move on the board. The total number of all possible chess positions is more than all the atoms in the universe.<p>Is is possible for a chess engine to compute the next move and be absolutely sure it is the best one? It&#x27;s not, it is a statistical approximation, but still very useful.</div><br/></div></div><div id="40818069" class="c"><input type="checkbox" id="c-40818069" checked=""/><div class="controls bullet"><span class="by">wincy</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40815669">prev</a><span>|</span><a href="#40816291">next</a><span>|</span><label class="collapse" for="c-40818069">[-]</label><label class="expand" for="c-40818069">[1 more]</label></div><br/><div class="children"><div class="content">Well what the heck was Bing Chat doing when it wrote me a message all in emojis like it was the Zodiac killer telling me a hacker had taken it over then spitting out Python code to shutdown the system, and giving me nonsense secret messages like “PKCLDUBB”?<p>What am I suppose to call that?</div><br/></div></div><div id="40816291" class="c"><input type="checkbox" id="c-40816291" checked=""/><div class="controls bullet"><span class="by">shrimp_emoji</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40818069">prev</a><span>|</span><a href="#40817067">next</a><span>|</span><label class="collapse" for="c-40816291">[-]</label><label class="expand" for="c-40816291">[1 more]</label></div><br/><div class="children"><div class="content">It should be &quot;confabulation&quot;, since that&#x27;s not carting along the notion of false sensory input.<p>Humans also confabulate but not as a result of &quot;hallucinations&quot;. They usually do it because that&#x27;s actually what brains like to do, whether it&#x27;s making up stories about how the world was created or, more infamously, in the case of neural disorders where the machinery&#x27;s penchant for it becomes totally unmoderated and a person just spits out false information that they themselves can&#x27;t realize is false. <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Confabulation" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Confabulation</a></div><br/></div></div><div id="40817067" class="c"><input type="checkbox" id="c-40817067" checked=""/><div class="controls bullet"><span class="by">sqeaky</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40816291">prev</a><span>|</span><a href="#40815902">next</a><span>|</span><label class="collapse" for="c-40817067">[-]</label><label class="expand" for="c-40817067">[1 more]</label></div><br/><div class="children"><div class="content">Yet for their value as tools the truth value of statements made by LLMs do matter.</div><br/></div></div><div id="40815902" class="c"><input type="checkbox" id="c-40815902" checked=""/><div class="controls bullet"><span class="by">sandworm101</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815177">parent</a><span>|</span><a href="#40817067">prev</a><span>|</span><a href="#40815160">next</a><span>|</span><label class="collapse" for="c-40815902">[-]</label><label class="expand" for="c-40815902">[2 more]</label></div><br/><div class="children"><div class="content">Hallucination is emergent. It cannot be found as a thing inside the AI systems.  It is a phenomena that only exists when the output is evaluated. That makes it an accurate description.  A human who has hallucinated something is not lying when they speak of something that never actually happened, nor are they making any sort of mistake in their recollection. Similarly, an AI that is hallucinating isn&#x27;t doing anything incorrect and doesn&#x27;t have any motivation.  The hallucinated data emerges just as any other output, only to evaluated by outsiders as incorrect.</div><br/></div></div></div></div><div id="40815160" class="c"><input type="checkbox" id="c-40815160" checked=""/><div class="controls bullet"><span class="by">beernet</span><span>|</span><a href="#40814847">parent</a><span>|</span><a href="#40815177">prev</a><span>|</span><a href="#40815073">next</a><span>|</span><label class="collapse" for="c-40815160">[-]</label><label class="expand" for="c-40815160">[2 more]</label></div><br/><div class="children"><div class="content">How are &#x27;hallucinations&#x27; a phenomenon? I have trouble with the term &#x27;hallucination&#x27; and believe it sets the wrong narratuve. It suggests something negative or unexpected, which it absolutely is not. Language models aim at, as their name implies, modeling language. Not facts or anything alike. This is per design and you certainly don&#x27;t have to be an AI researcher to grasp that.<p>That being said, people new to the field tend to believe that these models are fact machines. In fact, they are the complete opposite.</div><br/></div></div><div id="40815073" class="c"><input type="checkbox" id="c-40815073" checked=""/><div class="controls bullet"><span class="by">dennisy</span><span>|</span><a href="#40814847">parent</a><span>|</span><a href="#40815160">prev</a><span>|</span><a href="#40815678">next</a><span>|</span><label class="collapse" for="c-40815073">[-]</label><label class="expand" for="c-40815073">[3 more]</label></div><br/><div class="children"><div class="content">Not sure if there is a great deal of maths to understand. The output of an LLM is stochastic by nature, and will read syntactical perfect, AKA a hallucination.<p>No real way to mathematically prove this, considering there is also no way to know if the training data also had this “hallucination” inside of it.</div><br/><div id="40815487" class="c"><input type="checkbox" id="c-40815487" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815073">parent</a><span>|</span><a href="#40815678">next</a><span>|</span><label class="collapse" for="c-40815487">[-]</label><label class="expand" for="c-40815487">[2 more]</label></div><br/><div class="children"><div class="content">I think mathematical proof is the wrong framework, in the same way that chemistry is the wrong framework for precisely quantifying and explaining how LSD causes humans to hallucinate (you can point to which receptors it binds with, but AFAICT not much more than that).<p>Investigate it with the tools of psychologically, as suited for use on a new non-human creature we&#x27;ve never encountered before.</div><br/></div></div></div></div><div id="40815678" class="c"><input type="checkbox" id="c-40815678" checked=""/><div class="controls bullet"><span class="by">cainxinth</span><span>|</span><a href="#40814847">parent</a><span>|</span><a href="#40815073">prev</a><span>|</span><a href="#40816770">next</a><span>|</span><label class="collapse" for="c-40815678">[-]</label><label class="expand" for="c-40815678">[2 more]</label></div><br/><div class="children"><div class="content">Not a paper, but a startup called Vectara claimed to be investigating LLM hallucination&#x2F; confabulation rates last year:<p><a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;11&#x2F;06&#x2F;technology&#x2F;chatbots-hallucination-rates.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;11&#x2F;06&#x2F;technology&#x2F;chatbots-hallu...</a></div><br/><div id="40815909" class="c"><input type="checkbox" id="c-40815909" checked=""/><div class="controls bullet"><span class="by">lowyek</span><span>|</span><a href="#40814847">root</a><span>|</span><a href="#40815678">parent</a><span>|</span><a href="#40816770">next</a><span>|</span><label class="collapse" for="c-40815909">[-]</label><label class="expand" for="c-40815909">[1 more]</label></div><br/><div class="children"><div class="content">thank you for sharing this!</div><br/></div></div></div></div><div id="40815092" class="c"><input type="checkbox" id="c-40815092" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#40814847">parent</a><span>|</span><a href="#40816770">prev</a><span>|</span><a href="#40815674">next</a><span>|</span><label class="collapse" for="c-40815092">[-]</label><label class="expand" for="c-40815092">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see many deep theorems in the field of psychology either.</div><br/></div></div></div></div><div id="40815674" class="c"><input type="checkbox" id="c-40815674" checked=""/><div class="controls bullet"><span class="by">bluelightning2k</span><span>|</span><a href="#40814847">prev</a><span>|</span><a href="#40813439">next</a><span>|</span><label class="collapse" for="c-40815674">[-]</label><label class="expand" for="c-40815674">[4 more]</label></div><br/><div class="children"><div class="content">Evaluators with CriticGPT outperform those without 60% of the time.<p>So, slightly better than random chance. I guess a win is a win but I would have thought this would be higher. I&#x27;d kind have assume that just asking GPT itself if it&#x27;s sure would be this kind of lift.</div><br/><div id="40817120" class="c"><input type="checkbox" id="c-40817120" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40815674">parent</a><span>|</span><a href="#40813439">next</a><span>|</span><label class="collapse" for="c-40817120">[-]</label><label class="expand" for="c-40817120">[3 more]</label></div><br/><div class="children"><div class="content">I’m not sure why 60 vs 40 is slightly better than random chance. A person using this system has a 50% higher success rate than those not using it. I wouldnt call this a slight better result.</div><br/><div id="40817189" class="c"><input type="checkbox" id="c-40817189" checked=""/><div class="controls bullet"><span class="by">smrq</span><span>|</span><a href="#40815674">root</a><span>|</span><a href="#40817120">parent</a><span>|</span><a href="#40813439">next</a><span>|</span><label class="collapse" for="c-40817189">[-]</label><label class="expand" for="c-40817189">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just... obviously incorrect. It&#x27;s 10% higher. (60%, as opposed to the expected 50% from random chance.)</div><br/><div id="40817228" class="c"><input type="checkbox" id="c-40817228" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40815674">root</a><span>|</span><a href="#40817189">parent</a><span>|</span><a href="#40813439">next</a><span>|</span><label class="collapse" for="c-40817228">[-]</label><label class="expand" for="c-40817228">[1 more]</label></div><br/><div class="children"><div class="content">I’m not sure what you mean.<p>You can see the plots if you prefer, or think of it this way: out of a total of 100 trials, one team gets 40 and the other gets 60 = 40 + 40 * 50%<p>If you want to think of a 75% win rate as a more extreme example: you could say 25% above random or you could say one team wins 3 times as many cases as the other.  Both are equivalent but I think that the second way conveys the strength of the difference much better.<p>The results in this work are statistically significant and substantial.</div><br/></div></div></div></div></div></div></div></div><div id="40813439" class="c"><input type="checkbox" id="c-40813439" checked=""/><div class="controls bullet"><span class="by">neom</span><span>|</span><a href="#40815674">prev</a><span>|</span><a href="#40813469">next</a><span>|</span><label class="collapse" for="c-40813439">[-]</label><label class="expand" for="c-40813439">[7 more]</label></div><br/><div class="children"><div class="content">I was curious about the authors, did some digging, they&#x27;ve published some cool stuff:<p>Improving alignment of dialogue agents via targeted human judgements - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2209.14375" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2209.14375</a><p>Teaching language models to support answers with verified quotes - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.11147" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2203.11147</a></div><br/><div id="40814936" class="c"><input type="checkbox" id="c-40814936" checked=""/><div class="controls bullet"><span class="by">VWWHFSfQ</span><span>|</span><a href="#40813439">parent</a><span>|</span><a href="#40813469">next</a><span>|</span><label class="collapse" for="c-40814936">[-]</label><label class="expand" for="c-40814936">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an interesting dichotomy happening in the EU vs. USA in terms of how these kinds of phenomena are discovered, presented, analyzed, and approached.<p>The EU seems to be very much toward a regulate early, safety-first approach.  Where USA is very much toward unregulated, move fast, break things, assess the damage, regulate later.<p>I don&#x27;t know which is better or worse.</div><br/><div id="40815017" class="c"><input type="checkbox" id="c-40815017" checked=""/><div class="controls bullet"><span class="by">ipaddr</span><span>|</span><a href="#40813439">root</a><span>|</span><a href="#40814936">parent</a><span>|</span><a href="#40815723">next</a><span>|</span><label class="collapse" for="c-40815017">[-]</label><label class="expand" for="c-40815017">[2 more]</label></div><br/><div class="children"><div class="content">Regulate before you understand the problem seems like a poor approach</div><br/><div id="40818704" class="c"><input type="checkbox" id="c-40818704" checked=""/><div class="controls bullet"><span class="by">dns_snek</span><span>|</span><a href="#40813439">root</a><span>|</span><a href="#40815017">parent</a><span>|</span><a href="#40815723">next</a><span>|</span><label class="collapse" for="c-40818704">[-]</label><label class="expand" for="c-40818704">[1 more]</label></div><br/><div class="children"><div class="content">Which parts of the EU AI act do you disagree with? It primarily focuses on areas of use that present high or unacceptable risks.</div><br/></div></div></div></div><div id="40815723" class="c"><input type="checkbox" id="c-40815723" checked=""/><div class="controls bullet"><span class="by">l5870uoo9y</span><span>|</span><a href="#40813439">root</a><span>|</span><a href="#40814936">parent</a><span>|</span><a href="#40815017">prev</a><span>|</span><a href="#40813469">next</a><span>|</span><label class="collapse" for="c-40815723">[-]</label><label class="expand" for="c-40815723">[3 more]</label></div><br/><div class="children"><div class="content">As a European, after decades of regulations and fines without much to show, nobody in the industry believes the EU the capable of creating tech ecosystem. Perhaps even that the EU is part of the problem and that individual countries could independently move much faster.</div><br/><div id="40816279" class="c"><input type="checkbox" id="c-40816279" checked=""/><div class="controls bullet"><span class="by">Twixes</span><span>|</span><a href="#40813439">root</a><span>|</span><a href="#40815723">parent</a><span>|</span><a href="#40813469">next</a><span>|</span><label class="collapse" for="c-40816279">[-]</label><label class="expand" for="c-40816279">[2 more]</label></div><br/><div class="children"><div class="content">The difference in tech outcomes can definitely be attributed to _European_ conditions, but regulation is extremely country-specific – the _EU_ is not the detriment (e.g. Germany might be notably bureaucratic, but the culture in Sweden differs, and so do frameworks in Poland). EU institutions are out to get the US giants, but startups or scaleups? Out of sight.<p>There really is comparatively little reward for shooting for the moon though – the fragmented stock markets don&#x27;t provide great exit opportunities, so less money goes into funding ambitious companies. Then, scaling throughout all of Europe is notably hard, with dozens of languages, cultures, and legal frameworks to navigate. Some of these cultures are more risk-averse, and that&#x27;s not easy to change. Not to mention English being the _lingua franca_ of business and tech.<p>I would love Europe to reach the States&#x27; level of tech strength, but these are all really hard problems.</div><br/><div id="40816498" class="c"><input type="checkbox" id="c-40816498" checked=""/><div class="controls bullet"><span class="by">felipeerias</span><span>|</span><a href="#40813439">root</a><span>|</span><a href="#40816279">parent</a><span>|</span><a href="#40813469">next</a><span>|</span><label class="collapse" for="c-40816498">[-]</label><label class="expand" for="c-40816498">[1 more]</label></div><br/><div class="children"><div class="content">Another issue is that modern AI is notoriously energy-intensive.<p>Because of policy choices over the past several decades, as well as the ongoing war with Russia, the EU is already struggling to provide enough energy for its existing industry. There just isn’t any slack left for a newcomer.<p>This is a relatively recent (2022) comparison of the Industrial electricity prices including taxes:<p><a href="https:&#x2F;&#x2F;www.gov.uk&#x2F;government&#x2F;statistical-data-sets&#x2F;international-industrial-energy-prices" rel="nofollow">https:&#x2F;&#x2F;www.gov.uk&#x2F;government&#x2F;statistical-data-sets&#x2F;internat...</a><p>Roughly, electricity for industrial uses is 50% more expensive in France that in the USA.<p>In Germany, it is over 120% more expensive.<p>In the UK, over 150%.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40813469" class="c"><input type="checkbox" id="c-40813469" checked=""/><div class="controls bullet"><span class="by">jimmytucson</span><span>|</span><a href="#40813439">prev</a><span>|</span><a href="#40812818">next</a><span>|</span><label class="collapse" for="c-40813469">[-]</label><label class="expand" for="c-40813469">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the difference between CriticGPT and ChatGPT with a prompt that says &quot;You are a software engineer, your job is to review this code and point out bugs, here is what the code is supposed to do: {the original prompt}, here is the code {original response}, review the code,&quot; etc.</div><br/><div id="40815044" class="c"><input type="checkbox" id="c-40815044" checked=""/><div class="controls bullet"><span class="by">ipaddr</span><span>|</span><a href="#40813469">parent</a><span>|</span><a href="#40812818">next</a><span>|</span><label class="collapse" for="c-40815044">[-]</label><label class="expand" for="c-40815044">[1 more]</label></div><br/><div class="children"><div class="content">$20 a month</div><br/></div></div></div></div><div id="40812818" class="c"><input type="checkbox" id="c-40812818" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40813469">prev</a><span>|</span><a href="#40812728">next</a><span>|</span><label class="collapse" for="c-40812818">[-]</label><label class="expand" for="c-40812818">[20 more]</label></div><br/><div class="children"><div class="content">Looks like the hallucination rate doesn&#x27;t improve significantly, but I suppose it&#x27;s still a win if it helps humans review things faster? Though I could imagine reliance on the tool leading to missing less obvious problems.</div><br/><div id="40812844" class="c"><input type="checkbox" id="c-40812844" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">parent</a><span>|</span><a href="#40813441">next</a><span>|</span><label class="collapse" for="c-40812844">[-]</label><label class="expand" for="c-40812844">[17 more]</label></div><br/><div class="children"><div class="content">While it&#x27;s a net good, it would kind of kill one of the most valuable parts of ChatGPT for me, which is critiquing its output myself.<p>If I ask it a question, I try not to trust it immediately, and I independently look the answer up and I argue with it. In turn, it actually is one of my favorite learning tools, because it kind of forces me to figure out <i>why</i> it&#x27;s wrong and explain it.</div><br/><div id="40812942" class="c"><input type="checkbox" id="c-40812942" checked=""/><div class="controls bullet"><span class="by">goostavos</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40812844">parent</a><span>|</span><a href="#40814699">next</a><span>|</span><label class="collapse" for="c-40812942">[-]</label><label class="expand" for="c-40812942">[14 more]</label></div><br/><div class="children"><div class="content">Unexpectedly, I kind of agree. I&#x27;ve found GPT to be a great tutor for things I&#x27;m trying to learn. It being somewhat unreliable &#x2F; prone to confidently lying embeds a certain amount of useful skepticism and questioning of all the information, which in turn leads to an overall better understanding.<p>Fighting with the AI&#x27;s wrongness out of spike is an unexpectedly good motivator.</div><br/><div id="40813357" class="c"><input type="checkbox" id="c-40813357" checked=""/><div class="controls bullet"><span class="by">posix86</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40812942">parent</a><span>|</span><a href="#40813239">next</a><span>|</span><label class="collapse" for="c-40813357">[-]</label><label class="expand" for="c-40813357">[3 more]</label></div><br/><div class="children"><div class="content">Reminds me of a prof at uni, who&#x27;s slides always appeard to have been written 5 mins before the lecture started, resulting in students pointing out mistakes in every other slide. He defended himself saying that you learn more if you aren&#x27;t sure weather things are correct - which was right. Esp. during a lecture, it&#x27;s sometimes not that easy to figure out if you truly understood something or fooled yourself, knowing that what you&#x27;re looking at is provably right. If you know everything can be wrong, you trick your mind to verify it at a deeper level, and thus gain more understanding. It also results in a culture where you&#x27;re allowed to question the prof. It resulted in many healthy arguments with the prof why something is the way it is, often resulting with him agreeing that his slides are wrong. He never corrected the underlying PPP.</div><br/><div id="40814196" class="c"><input type="checkbox" id="c-40814196" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40813357">parent</a><span>|</span><a href="#40813239">next</a><span>|</span><label class="collapse" for="c-40814196">[-]</label><label class="expand" for="c-40814196">[2 more]</label></div><br/><div class="children"><div class="content">I thought about doing that when I was doing adjunct last year, but what made me stop was the fact that these were introductory classes, so I was afraid I might pollute the minds of students who really haven&#x27;t learned enough to question stuff yet.</div><br/></div></div></div></div><div id="40813239" class="c"><input type="checkbox" id="c-40813239" checked=""/><div class="controls bullet"><span class="by">ExtremisAndy</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40812942">parent</a><span>|</span><a href="#40813357">prev</a><span>|</span><a href="#40813420">next</a><span>|</span><label class="collapse" for="c-40813239">[-]</label><label class="expand" for="c-40813239">[4 more]</label></div><br/><div class="children"><div class="content">Wow, I’ve never thought about that, but you’re right! It really has trained me to be skeptical of what I’m being taught and confirm the veracity of it with multiple sources. A bit time-consuming, of course, but generally a good way to go about educating yourself!</div><br/><div id="40813381" class="c"><input type="checkbox" id="c-40813381" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40813239">parent</a><span>|</span><a href="#40813420">next</a><span>|</span><label class="collapse" for="c-40813381">[-]</label><label class="expand" for="c-40813381">[3 more]</label></div><br/><div class="children"><div class="content">I genuinely think that arguing with it has been almost a secret weapon for me with my grad school work.  I&#x27;ll ask it a question about temporal logic or something, it&#x27;ll say something that sounds accurate but is ultimately wrong or misleading after looking through traditional documentation, and I can fight with it, and see if it refines it to something correct, which I can then check again, etc. I keep doing this for a bunch of iterations and I end up with a pretty good understanding of the topic.<p>I guess at some level this is almost what &quot;prompt engineering&quot; is (though I really hate that term), but I use it as a learning tool and I do think it&#x27;s been really good at helping me cement concepts in my brain.</div><br/><div id="40814668" class="c"><input type="checkbox" id="c-40814668" checked=""/><div class="controls bullet"><span class="by">ramenbytes</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40813381">parent</a><span>|</span><a href="#40813420">next</a><span>|</span><label class="collapse" for="c-40814668">[-]</label><label class="expand" for="c-40814668">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  I&#x27;ll ask it a question about temporal logic or something, it&#x27;ll say something that sounds accurate but is ultimately wrong or misleading after looking through traditional documentation, and I can fight with it, and see if it refines it to something correct, which I can then check again, etc. I keep doing this for a bunch of iterations and I end up with a pretty good understanding of the topic.<p>Interesting, that&#x27;s the basic process I follow myself when learning without ChatGPT. Comparing my mental representation of the thing I&#x27;m learning to existing literature&#x2F;results, finding the disconnects between the two, reworking my understanding, wash rinse repeat.</div><br/><div id="40814884" class="c"><input type="checkbox" id="c-40814884" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40814668">parent</a><span>|</span><a href="#40813420">next</a><span>|</span><label class="collapse" for="c-40814884">[-]</label><label class="expand" for="c-40814884">[1 more]</label></div><br/><div class="children"><div class="content">I guess a large part of it is just kind of the &quot;rubber duck&quot; thing.  My thoughts can be pretty disorganized and hard to follow until I&#x27;m forced to articulate them.  Finding out why ChatGPT is wrong is useful because it&#x27;s a rubber duck that I can interrogate, not just talk to.<p>It can be hard for me to directly figure out when my mental model is wrong on something.  I&#x27;m sure it happens all the time, but a lot of the time I will think I know something until I feel compelled to prove it to someone, and I&#x27;ll often find out that <i>I&#x27;m</i> wrong.<p>That&#x27;s actually happened a bunch of times with ChatGPT, where I think it&#x27;s wrong until I actually interrogate it, look up a credible source, and realize that my understanding was incorrect.</div><br/></div></div></div></div></div></div></div></div><div id="40813420" class="c"><input type="checkbox" id="c-40813420" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40812942">parent</a><span>|</span><a href="#40813239">prev</a><span>|</span><a href="#40813603">next</a><span>|</span><label class="collapse" for="c-40813420">[-]</label><label class="expand" for="c-40813420">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, and what I like is that I can get it to say things in &quot;dumb language&quot; instead of a bunch of scary math terms. It&#x27;ll be confidently wrong, but in language that I can easily understand, forcing me to looking things up, and kind of forcing me learn the proper terminology and actually understanding it.<p>Arcane language is actually kind of a pet peeve of mine in theoretical CS and mathematics.  Sometimes it feels like academics really obfuscate relatively simple concepts but using a bunch of weird math terms. I don&#x27;t think it&#x27;s malicious, I just think that there&#x27;s value in having more approachable language and metaphors in the process of explaining thing.</div><br/></div></div><div id="40813603" class="c"><input type="checkbox" id="c-40813603" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40812942">parent</a><span>|</span><a href="#40813420">prev</a><span>|</span><a href="#40814699">next</a><span>|</span><label class="collapse" for="c-40813603">[-]</label><label class="expand" for="c-40813603">[5 more]</label></div><br/><div class="children"><div class="content">I actually learn a lot from arguing with not just AIs but people and it doesn&#x27;t really matter if they&#x27;re wrong or right.  If they&#x27;re right, it&#x27;s an obvious learning experience for me, if they&#x27;re wrong, it forced me to explain and understand _why_ they&#x27;re wrong.</div><br/><div id="40814049" class="c"><input type="checkbox" id="c-40814049" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40813603">parent</a><span>|</span><a href="#40814699">next</a><span>|</span><label class="collapse" for="c-40814049">[-]</label><label class="expand" for="c-40814049">[4 more]</label></div><br/><div class="children"><div class="content">I completely agree with that, but the problem is finding a supply of people to argue with on niche subjects.  I have occasionally argued with people on the Haskell IRC and the NixOS Matrix server about some stuff, but since they&#x27;re humans who selfishly have their own lives to live so I can&#x27;t argue with them infinitely, and since the topics I argue about are specific there just don&#x27;t exists a lot of people I can argue with even in the best of times.<p>ChatGPT (Gemini&#x2F;Anthropic&#x2F;etc) have the advantage of never getting sick of arguing with me.  I can go back and forth and argue about any weird topic that I want for as long as I want at any time of day and keep learning until I&#x27;m bored of it.<p>Obviously it depends on the person but I really like it.</div><br/><div id="40814732" class="c"><input type="checkbox" id="c-40814732" checked=""/><div class="controls bullet"><span class="by">ramenbytes</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40814049">parent</a><span>|</span><a href="#40814448">next</a><span>|</span><label class="collapse" for="c-40814732">[-]</label><label class="expand" for="c-40814732">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I completely agree with that, but the problem is finding a supply of people to argue with on niche subjects.<p>Beyond just subject-wise, finding people who argue in good faith seems to be an issue too. There are people I&#x27;m friends with almost specifically because we&#x27;re able to consistently have good-faith arguments about our strongly opposing views. It doesn&#x27;t seem to be a common skill, but perhaps that has something to do with my sample set or my own behaviors in arguments.</div><br/><div id="40814919" class="c"><input type="checkbox" id="c-40814919" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40814732">parent</a><span>|</span><a href="#40814448">next</a><span>|</span><label class="collapse" for="c-40814919">[-]</label><label class="expand" for="c-40814919">[1 more]</label></div><br/><div class="children"><div class="content">I dunno, for more niche computer science or math subjects, I don&#x27;t feel like people argue in bad faith most of the time. The people I&#x27;ve argued with on the Haskell IRC years ago genuinely believe in what they&#x27;re saying, even if I don&#x27;t agree with them (I have a lot of negative opinions on Haskell as a language).<p>Politically? Yeah, nearly impossible to find anyone who argues in good faith.</div><br/></div></div></div></div><div id="40814448" class="c"><input type="checkbox" id="c-40814448" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40814049">parent</a><span>|</span><a href="#40814732">prev</a><span>|</span><a href="#40814699">next</a><span>|</span><label class="collapse" for="c-40814448">[-]</label><label class="expand" for="c-40814448">[1 more]</label></div><br/><div class="children"><div class="content">Arguing is arguably one of humanity&#x27;s super powers, and that we&#x27;ve yet to bring it to bear in any serious way gives me reason for optimism about sorting out the various major problems we&#x27;ve foolishly gotten ourselves into.</div><br/></div></div></div></div></div></div></div></div><div id="40814699" class="c"><input type="checkbox" id="c-40814699" checked=""/><div class="controls bullet"><span class="by">julienchastang</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40812844">parent</a><span>|</span><a href="#40812942">prev</a><span>|</span><a href="#40813441">next</a><span>|</span><label class="collapse" for="c-40814699">[-]</label><label class="expand" for="c-40814699">[2 more]</label></div><br/><div class="children"><div class="content">Very good comment. In order to effectively use LLMs (I use ChatGPT4 and 4o), you have to be skeptical of them and being a good AI skeptic takes practice. Here is another technique I&#x27;ve learned along the way: When you have it generate text for some report you are writing, or something, after your initial moment of being dazzled (at least for me), resist the temptation to copy&#x2F;paste. Instead, &quot;manually&quot; rewrite the verbiage. You then realize there is a substantial amount of BS that can be excised. Nevertheless, it is a huge time saver and can be good at ideation, as well.</div><br/><div id="40814956" class="c"><input type="checkbox" id="c-40814956" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40814699">parent</a><span>|</span><a href="#40813441">next</a><span>|</span><label class="collapse" for="c-40814956">[-]</label><label class="expand" for="c-40814956">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I used it last year to generate homework assignments, and it would give me the results in Pandoc compatible markdown.  It was initially magic, but some of the problems didn&#x27;t actually make sense and might actually be unsolveable, so I would have to go through it line by line and then ask it to regenerate it [1].<p>Even with that, it took a process that had taken multiple hours before down to about 30-45 minutes.  It was super cool.<p>[1] Just to be clear, I always did the homework assignments myself beforehand to make sure that a solution was solvable and fair before I assigned it.</div><br/></div></div></div></div></div></div><div id="40813441" class="c"><input type="checkbox" id="c-40813441" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#40812818">parent</a><span>|</span><a href="#40812844">prev</a><span>|</span><a href="#40812728">next</a><span>|</span><label class="collapse" for="c-40813441">[-]</label><label class="expand" for="c-40813441">[2 more]</label></div><br/><div class="children"><div class="content">The lazy version of that, which I recommend, is always deny the first answer. Usually I deny for some obvious reason, but sometimes I just say &quot;isn&#x27;t that wrong?&quot;</div><br/><div id="40814174" class="c"><input type="checkbox" id="c-40814174" checked=""/><div class="controls bullet"><span class="by">tombert</span><span>|</span><a href="#40812818">root</a><span>|</span><a href="#40813441">parent</a><span>|</span><a href="#40812728">next</a><span>|</span><label class="collapse" for="c-40814174">[-]</label><label class="expand" for="c-40814174">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a useful trick but I have noticed when I do that it goes in circles a where it suggests &quot;A&quot;, I say it&#x27;s wrong, it suggests &quot;B&quot;, I say that&#x27;s wrong, it suggests &quot;C&quot;, I say that&#x27;s wrong, and then it suggests &quot;A&quot; again.<p>Usually for it to get a correct answer, I have to provide it a bit of context.</div><br/></div></div></div></div></div></div><div id="40812728" class="c"><input type="checkbox" id="c-40812728" checked=""/><div class="controls bullet"><span class="by">mkmk</span><span>|</span><a href="#40812818">prev</a><span>|</span><a href="#40813399">next</a><span>|</span><label class="collapse" for="c-40812728">[-]</label><label class="expand" for="c-40812728">[10 more]</label></div><br/><div class="children"><div class="content">It seems more and more that the solution to AI&#x27;s quality problems is... more AI.<p>Does Anthropic do something like this as well, or is there another reason Claude Sonnet 3.5 is so much better at coding than GPT-4o?</div><br/><div id="40812845" class="c"><input type="checkbox" id="c-40812845" checked=""/><div class="controls bullet"><span class="by">ru552</span><span>|</span><a href="#40812728">parent</a><span>|</span><a href="#40812874">next</a><span>|</span><label class="collapse" for="c-40812845">[-]</label><label class="expand" for="c-40812845">[2 more]</label></div><br/><div class="children"><div class="content">Anthropic has attributed Sonnet 3.5&#x27;s model improvement to better training data.<p>&quot;Which data specifically? Gerstenhaber wouldn’t disclose, but he implied that Claude 3.5 Sonnet draws much of its strength from these training sets.&quot;[0]<p>[0]<a href="https:&#x2F;&#x2F;techcrunch.com&#x2F;2024&#x2F;06&#x2F;20&#x2F;anthropic-claims-its-latest-model-is-best-in-class&#x2F;" rel="nofollow">https:&#x2F;&#x2F;techcrunch.com&#x2F;2024&#x2F;06&#x2F;20&#x2F;anthropic-claims-its-lates...</a></div><br/><div id="40818143" class="c"><input type="checkbox" id="c-40818143" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40812728">root</a><span>|</span><a href="#40812845">parent</a><span>|</span><a href="#40812874">next</a><span>|</span><label class="collapse" for="c-40818143">[-]</label><label class="expand" for="c-40818143">[1 more]</label></div><br/><div class="children"><div class="content">and water is wet</div><br/></div></div></div></div><div id="40812874" class="c"><input type="checkbox" id="c-40812874" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#40812728">parent</a><span>|</span><a href="#40812845">prev</a><span>|</span><a href="#40812968">next</a><span>|</span><label class="collapse" for="c-40812874">[-]</label><label class="expand" for="c-40812874">[1 more]</label></div><br/><div class="children"><div class="content">My guess, which could be completely wrong, Anthropic spent more resources on interpretability and it&#x27;s paying off.<p>I remember when I first started using activation maps when building image classification models and it was like what on earth was I doing before this... just blindly trusting the loss.<p>How do you discover biases and issues with training data without interpretability?</div><br/></div></div><div id="40812968" class="c"><input type="checkbox" id="c-40812968" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#40812728">parent</a><span>|</span><a href="#40812874">prev</a><span>|</span><a href="#40813417">next</a><span>|</span><label class="collapse" for="c-40812968">[-]</label><label class="expand" for="c-40812968">[1 more]</label></div><br/><div class="children"><div class="content">Is it really that much better? I&#x27;m really happy with GPT-4o&#x27;s coding capabilities and very seldom experience problems with hallucinations or incorrect responses, so I&#x27;m intrigued by how much better it can actually be.</div><br/></div></div><div id="40813417" class="c"><input type="checkbox" id="c-40813417" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40812728">parent</a><span>|</span><a href="#40812968">prev</a><span>|</span><a href="#40814818">next</a><span>|</span><label class="collapse" for="c-40813417">[-]</label><label class="expand" for="c-40813417">[1 more]</label></div><br/><div class="children"><div class="content">In my experience Sonnet 3.5 is about the same as 4o for coding. Sometimes one provides a better solution, sometimes the other. Both are pretty good.</div><br/></div></div><div id="40814818" class="c"><input type="checkbox" id="c-40814818" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40812728">parent</a><span>|</span><a href="#40813417">prev</a><span>|</span><a href="#40812783">next</a><span>|</span><label class="collapse" for="c-40814818">[-]</label><label class="expand" for="c-40814818">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It seems more and more that the solution to AI&#x27;s quality problems is... more AI.<p>This reminds me of the passage found in the description of the fuckitpy module:<p>&quot;This module is like violence: if it doesn&#x27;t work, you just need more of it.&quot;</div><br/></div></div><div id="40812783" class="c"><input type="checkbox" id="c-40812783" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40812728">parent</a><span>|</span><a href="#40814818">prev</a><span>|</span><a href="#40813399">next</a><span>|</span><label class="collapse" for="c-40812783">[-]</label><label class="expand" for="c-40812783">[3 more]</label></div><br/><div class="children"><div class="content">&gt;or is there another reason Claude Sonnet 3.5 is so much better at coding than GPT-4o?<p>It&#x27;s impossible to say because these models are proprietary.</div><br/><div id="40812979" class="c"><input type="checkbox" id="c-40812979" checked=""/><div class="controls bullet"><span class="by">mkmk</span><span>|</span><a href="#40812728">root</a><span>|</span><a href="#40812783">parent</a><span>|</span><a href="#40813399">next</a><span>|</span><label class="collapse" for="c-40812979">[-]</label><label class="expand" for="c-40812979">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the very article we&#x27;re commenting on an indication that you can form a basic opinion on what makes one proprietary model different from another?</div><br/><div id="40813158" class="c"><input type="checkbox" id="c-40813158" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40812728">root</a><span>|</span><a href="#40812979">parent</a><span>|</span><a href="#40813399">next</a><span>|</span><label class="collapse" for="c-40813158">[-]</label><label class="expand" for="c-40813158">[1 more]</label></div><br/><div class="children"><div class="content">Not really, we know absolutely nothing about Claude 3.5 Sonnet, except that it&#x27;s an LLM.</div><br/></div></div></div></div></div></div></div></div><div id="40813399" class="c"><input type="checkbox" id="c-40813399" checked=""/><div class="controls bullet"><span class="by">victor9000</span><span>|</span><a href="#40812728">prev</a><span>|</span><a href="#40817017">next</a><span>|</span><label class="collapse" for="c-40813399">[-]</label><label class="expand" for="c-40813399">[21 more]</label></div><br/><div class="children"><div class="content">This gets at the absolute torrent of LLM diarrhea that people are adding to PRs these days.  The worst of it seems to come from junior and first time senior devs who think more is more when it comes to LoC.  PR review has become a nightmare at my work where juniors are now producing these magnificent PRs with dynamic programming, esoteric caching, database triggers, you name it.  People are using LLMs to produce code far beyond their abilities, wisdom, or understanding, producing an absolute clusterfuck of bugs and edge cases.  Anyone else dealing with something similar?  How are you handling it?</div><br/><div id="40813517" class="c"><input type="checkbox" id="c-40813517" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813407">next</a><span>|</span><label class="collapse" for="c-40813517">[-]</label><label class="expand" for="c-40813517">[4 more]</label></div><br/><div class="children"><div class="content">How is that different from junior devs writing bad code previously? The more things change, the more things stay the same.<p>You handle it by teaching them how to write good code.<p>And if they refuse to learn, then they get bad performance reviews and get let go.<p>I&#x27;ve had junior devs come in with all sorts of bad habits, from only using single-letter variable names and zero commenting, to thinking global variables should be used for everything, to writing object-oriented monstrosities with seven layers of unnecessary abstractions instead of a simple function.<p>Bad LLM-generated code? It&#x27;s just one more category of bad code, and you treat it the same as all the rest. Explain why it&#x27;s wrong and how to redo it.<p>Or if you want to fix it at scale, identify the common bad patterns and make avoiding them part of your company&#x27;s onboarding&#x2F;orientation&#x2F;first-week-training for new devs.</div><br/><div id="40813619" class="c"><input type="checkbox" id="c-40813619" checked=""/><div class="controls bullet"><span class="by">okdood64</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813517">parent</a><span>|</span><a href="#40813407">next</a><span>|</span><label class="collapse" for="c-40813619">[-]</label><label class="expand" for="c-40813619">[3 more]</label></div><br/><div class="children"><div class="content">&gt; How is that different from junior devs writing bad code previously?<p>Because if it&#x27;s bad, at least it&#x27;s simple. Meaning simple to review, quickly correct and move on.</div><br/><div id="40814027" class="c"><input type="checkbox" id="c-40814027" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813619">parent</a><span>|</span><a href="#40814635">next</a><span>|</span><label class="collapse" for="c-40814027">[-]</label><label class="expand" for="c-40814027">[1 more]</label></div><br/><div class="children"><div class="content">Like I said, with &quot;object-oriented mostrosities&quot;, it&#x27;s not like it was always simple before either.<p>And if you know a solution should be 50 lines and they&#x27;ve given you 500, it&#x27;s not like you have to read it all -- you can quickly figure out what approach they&#x27;re using and discuss the approach they should be using instead.</div><br/></div></div><div id="40814635" class="c"><input type="checkbox" id="c-40814635" checked=""/><div class="controls bullet"><span class="by">phatfish</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813619">parent</a><span>|</span><a href="#40814027">prev</a><span>|</span><a href="#40813407">next</a><span>|</span><label class="collapse" for="c-40814635">[-]</label><label class="expand" for="c-40814635">[1 more]</label></div><br/><div class="children"><div class="content">Maybe fight fire with fire. Feed the ChatGPT PR to ChatGPT and ask it to do a review, paste that as the comment. It will even do the markdown for you!</div><br/></div></div></div></div></div></div><div id="40813407" class="c"><input type="checkbox" id="c-40813407" checked=""/><div class="controls bullet"><span class="by">Zee2</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813517">prev</a><span>|</span><a href="#40813491">next</a><span>|</span><label class="collapse" for="c-40813407">[-]</label><label class="expand" for="c-40813407">[3 more]</label></div><br/><div class="children"><div class="content">My company simply prohibits any AI generated code. Seems to work rather well.</div><br/><div id="40813775" class="c"><input type="checkbox" id="c-40813775" checked=""/><div class="controls bullet"><span class="by">ffsm8</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813407">parent</a><span>|</span><a href="#40814510">next</a><span>|</span><label class="collapse" for="c-40813775">[-]</label><label class="expand" for="c-40813775">[1 more]</label></div><br/><div class="children"><div class="content">My employer went all in and pays for both enterprise subscriptions (github Copilot+ chatgpt enterprise, which is just a company branded version of the regular interface)<p>We&#x27;ve even been getting &quot;prompt engineering&quot; meeting invites of 3+ hours to get an introduction into their usage. 100-150 participants each time I joined<p>It&#x27;s amazing how much they&#x27;re valuing it. from my experience it&#x27;s usually a negative productivity multiplier (x0.7 vs x1 without either)</div><br/></div></div><div id="40814510" class="c"><input type="checkbox" id="c-40814510" checked=""/><div class="controls bullet"><span class="by">gnicholas</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813407">parent</a><span>|</span><a href="#40813775">prev</a><span>|</span><a href="#40813491">next</a><span>|</span><label class="collapse" for="c-40814510">[-]</label><label class="expand" for="c-40814510">[1 more]</label></div><br/><div class="children"><div class="content">How is this enforced? I&#x27;m not saying it isn&#x27;t a good idea, just that it seems like it would be tricky to enforce. Separately, it could result in employees uploading code to a non-privacy-respecting AI, whereas if employees were allowed to use a particular AI then the company could better control privacy&#x2F;security concerns.</div><br/></div></div></div></div><div id="40813491" class="c"><input type="checkbox" id="c-40813491" checked=""/><div class="controls bullet"><span class="by">xhevahir</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813407">prev</a><span>|</span><a href="#40813588">next</a><span>|</span><label class="collapse" for="c-40813491">[-]</label><label class="expand" for="c-40813491">[1 more]</label></div><br/><div class="children"><div class="content">When it gives me a complicated list expression or regex or something I like to ask ChatGPT to find a simpler way of doing the same thing, and it usually gives me something simpler that still works. Of course, you do have to ask, rather than simply copy-paste its output right into an editor, which is probably one step too many for some.</div><br/></div></div><div id="40813588" class="c"><input type="checkbox" id="c-40813588" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813491">prev</a><span>|</span><a href="#40813470">next</a><span>|</span><label class="collapse" for="c-40813588">[-]</label><label class="expand" for="c-40813588">[2 more]</label></div><br/><div class="children"><div class="content">In the code review, can&#x27;t you simply say, &quot;This is too complicated for what you&#x27;re trying to do -- please simplify&quot;?</div><br/><div id="40814517" class="c"><input type="checkbox" id="c-40814517" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813588">parent</a><span>|</span><a href="#40813470">next</a><span>|</span><label class="collapse" for="c-40814517">[-]</label><label class="expand" for="c-40814517">[1 more]</label></div><br/><div class="children"><div class="content">Not quite the same but might be more relevant depending on context: &quot;If you can&#x27;t articulate what it does then please rewrite it such that you can.&quot;</div><br/></div></div></div></div><div id="40813470" class="c"><input type="checkbox" id="c-40813470" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813588">prev</a><span>|</span><a href="#40813515">next</a><span>|</span><label class="collapse" for="c-40813470">[-]</label><label class="expand" for="c-40813470">[1 more]</label></div><br/><div class="children"><div class="content">Why not deal with people who create problems like this the same way as one would have done four years ago?<p>If they&#x27;re not doing their job, then why do they still have one?</div><br/></div></div><div id="40813705" class="c"><input type="checkbox" id="c-40813705" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813515">prev</a><span>|</span><a href="#40813662">next</a><span>|</span><label class="collapse" for="c-40813705">[-]</label><label class="expand" for="c-40813705">[2 more]</label></div><br/><div class="children"><div class="content">Are they dealing with complexity which isn&#x27;t there in order to appear smarter?</div><br/><div id="40814774" class="c"><input type="checkbox" id="c-40814774" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813705">parent</a><span>|</span><a href="#40813662">next</a><span>|</span><label class="collapse" for="c-40814774">[-]</label><label class="expand" for="c-40814774">[1 more]</label></div><br/><div class="children"><div class="content">IMO they&#x27;re just impressed the AI came to a conclusion that actually runs and aren&#x27;t skilled enough to recognize there&#x27;s a simpler way to do it.</div><br/></div></div></div></div><div id="40813662" class="c"><input type="checkbox" id="c-40813662" checked=""/><div class="controls bullet"><span class="by">liampulles</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813705">prev</a><span>|</span><a href="#40814770">next</a><span>|</span><label class="collapse" for="c-40813662">[-]</label><label class="expand" for="c-40813662">[5 more]</label></div><br/><div class="children"><div class="content">Maybe time for some pair programming?</div><br/><div id="40814800" class="c"><input type="checkbox" id="c-40814800" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40813662">parent</a><span>|</span><a href="#40814770">next</a><span>|</span><label class="collapse" for="c-40814800">[-]</label><label class="expand" for="c-40814800">[4 more]</label></div><br/><div class="children"><div class="content">No.</div><br/><div id="40814997" class="c"><input type="checkbox" id="c-40814997" checked=""/><div class="controls bullet"><span class="by">wholinator2</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40814800">parent</a><span>|</span><a href="#40814770">next</a><span>|</span><label class="collapse" for="c-40814997">[-]</label><label class="expand" for="c-40814997">[3 more]</label></div><br/><div class="children"><div class="content">That would be interesting though. What happens when one programmer attempts to chatgpt in pair programming. It&#x27;s almost like they&#x27;re already pair programming, just not with you!</div><br/><div id="40815178" class="c"><input type="checkbox" id="c-40815178" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40814997">parent</a><span>|</span><a href="#40814770">next</a><span>|</span><label class="collapse" for="c-40815178">[-]</label><label class="expand" for="c-40815178">[2 more]</label></div><br/><div class="children"><div class="content">They are welcome to do so, but not on company time. We do not find those tools useful at all, because we are generally hired to write new stuff and ChatGPT or other tools are useless when there are no good examples to steal from (e.g. darker corners of AWS that people don&#x27;t bother to offer solutions for) or when there is a known bug or there are only partial workarounds available for it.</div><br/><div id="40818725" class="c"><input type="checkbox" id="c-40818725" checked=""/><div class="controls bullet"><span class="by">raunakchhatwal</span><span>|</span><a href="#40813399">root</a><span>|</span><a href="#40815178">parent</a><span>|</span><a href="#40814770">next</a><span>|</span><label class="collapse" for="c-40818725">[-]</label><label class="expand" for="c-40818725">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t you want programmers to familiarize themselves now to prepare for the time when it does? Claude 3.5 sonnet is getting close.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40814770" class="c"><input type="checkbox" id="c-40814770" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40813399">parent</a><span>|</span><a href="#40813662">prev</a><span>|</span><a href="#40817017">next</a><span>|</span><label class="collapse" for="c-40814770">[-]</label><label class="expand" for="c-40814770">[1 more]</label></div><br/><div class="children"><div class="content">I work for clients that do not allow this shit, because their security teams and lawyers won&#x27;t have it. But... they have in-house &quot;AI ambassadors&quot; (your typical useless middle managers, BAs, project managers, etc.) who see promoting AI as a survival strategy. On the business side these orgs are leaking data, internal comms, and PII like a sieve, but the software side is free of AI. For now.</div><br/></div></div></div></div><div id="40817017" class="c"><input type="checkbox" id="c-40817017" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#40813399">prev</a><span>|</span><a href="#40813477">next</a><span>|</span><label class="collapse" for="c-40817017">[-]</label><label class="expand" for="c-40817017">[1 more]</label></div><br/><div class="children"><div class="content">Next step Critic²GPT, a model based on GPT-4, writes critiques of CriticGPT responses.</div><br/></div></div><div id="40813477" class="c"><input type="checkbox" id="c-40813477" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40817017">prev</a><span>|</span><a href="#40812919">next</a><span>|</span><label class="collapse" for="c-40813477">[-]</label><label class="expand" for="c-40813477">[7 more]</label></div><br/><div class="children"><div class="content">This is a really bizarre thing to do honestly<p>It&#x27;s plausible that there are potential avenues for improving language models through adversarial learning. GANs and Actor-Critic models have done a good job in narrow-domain generative applications and task learning, and I can make a strong theoretical argument that you can do something that looks like priority learning via adversarial equilibria<p>But why in the world are you trying to present this as a human-in-the-loop system? This makes no sense to me. You take an error-prone generative language model and then present another instance of an error-prone generative language model to &quot;critique&quot; it for the benefit of... a human observer? The very best case here is that this wastes a bunch of heat and time for what can only be a pretty nebulous potential gain to the human&#x27;s understanding<p>Is this some weird gambit to get people to trust these models more? Is it OpenAI losing the plot completely because they&#x27;re unwilling to go back to open-sourcing their models but addicted to the publicity of releasing public-facing interfaces to them? This doesn&#x27;t make sense to me as a research angle or as a product<p>I can really see the Microsoft influence here</div><br/><div id="40813642" class="c"><input type="checkbox" id="c-40813642" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#40813477">parent</a><span>|</span><a href="#40812919">next</a><span>|</span><label class="collapse" for="c-40813642">[-]</label><label class="expand" for="c-40813642">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s for their RLHF pipeline to improve labeling. Honestly, this seems super reasonable to me.  I don&#x27;t get why you think this is such a bad idea for this purpose...</div><br/><div id="40813852" class="c"><input type="checkbox" id="c-40813852" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40813477">root</a><span>|</span><a href="#40813642">parent</a><span>|</span><a href="#40812919">next</a><span>|</span><label class="collapse" for="c-40813852">[-]</label><label class="expand" for="c-40813852">[5 more]</label></div><br/><div class="children"><div class="content">RLHF to me seems more as a PR play than anything else, but inasmuch as it does anything useful, adding a second LLM to influence the human that&#x27;s influencing the LLM doesn&#x27;t solve any of the fundamental problems of either system. If anything it muddies the waters more, because we have already seen that humans are probably too credulous of the information presented to them by these models. If you want adversarial learning, there are far more efficient ways to do it. If you want human auditing, the best case here is that the second LLM doesn&#x27;t influence the human&#x27;s decisions at all (because any influence reduces the degree to which this is independent feedback)</div><br/><div id="40815690" class="c"><input type="checkbox" id="c-40815690" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#40813477">root</a><span>|</span><a href="#40813852">parent</a><span>|</span><a href="#40813914">next</a><span>|</span><label class="collapse" for="c-40815690">[-]</label><label class="expand" for="c-40815690">[2 more]</label></div><br/><div class="children"><div class="content">This is not adversarial learning.  It&#x27;s really about augmenting the ability of humans to determine if a snippet of code is correct and write proper critiques of incorrect code.<p>Any system that helps you more accurately label data with good critiques should help the model.  I&#x27;m not sure how you come to your conclusion.  Do you have some data to indicate that even with improved accuracy that some LLM bias would lead to a worse trained model?  I haven&#x27;t seen that data or assertion elsewhere, but that&#x27;s the only thing I can gather you might be referring.</div><br/><div id="40815834" class="c"><input type="checkbox" id="c-40815834" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40813477">root</a><span>|</span><a href="#40815690">parent</a><span>|</span><a href="#40813914">next</a><span>|</span><label class="collapse" for="c-40815834">[-]</label><label class="expand" for="c-40815834">[1 more]</label></div><br/><div class="children"><div class="content">Well, first of all, the stated purpose of RLHF isn&#x27;t to &quot;improve model accuracy&quot; in the first place (and what we mean by accuracy here is pretty fraught by itself, as this could mean at least three different things). They initially pitched it as a &quot;safety&quot; measure (and I think if it wasn&#x27;t obvious immediately how nonsensical a claim that is, it should at least be apparent now that the company&#x27;s shucked nearly the entire subset of its members that claimed to care about &quot;AI safety&quot; that this is not a priority)<p>The idea of RLHF as a mechanism for tuning models based on the principle that humans might have some hard-to-capture insight that could steer them independent of the way they&#x27;re normally trained is the very best steelman for its value I could come up with. This aim is directly subverted by trying to use another language model to influence the human rater, so from my perspective it really brings us back to square one on what the fuck RLHF is supposed to be doing<p>Really, a lot of this comes down to what these models do versus how they are being advertised. A generative language model produces plausible prose that follows from the prompt it receives. From this, the claim that it should write working code is actually quite a bit stronger than the claim that it should write true facts, because plausibile autocompletion will learn to mimic syntactic constraints but actually has very little to do with whether something is true, or whatever proxy or heuristic we may apply in place of &quot;true&quot; when assessing information (supported by evidence, perhaps. Logically sound, perhaps. The distinction between &quot;plausible&quot; and &quot;true&quot; is in many ways the whole point of every human epistemology). Like if you ask something trained on all human writing whether the Axis or the Allies won WWII, the answer will depend on whether you phrased the question in a way that sounds like Phillip K Dick would write it. This isn&#x27;t even incorrect behavior by the standards of the model, but people want to use these things like some kind of oracle or to replace google search or whatever, which is a misconception about what the thing does, and one that&#x27;s very profitable for the people selling it</div><br/></div></div></div></div><div id="40813914" class="c"><input type="checkbox" id="c-40813914" checked=""/><div class="controls bullet"><span class="by">vhiremath4</span><span>|</span><a href="#40813477">root</a><span>|</span><a href="#40813852">parent</a><span>|</span><a href="#40815690">prev</a><span>|</span><a href="#40817185">next</a><span>|</span><label class="collapse" for="c-40813914">[-]</label><label class="expand" for="c-40813914">[1 more]</label></div><br/><div class="children"><div class="content">This is kind of what I was thinking. I don’t get it. It seems like CriticGPT was maybe trained using RM&#x2F;RL with PPO as well? So there’s gonna be mistakes with what CriticGPT pushes back on which may make the labeler doubt themselves?</div><br/></div></div><div id="40817185" class="c"><input type="checkbox" id="c-40817185" checked=""/><div class="controls bullet"><span class="by">manilbeat</span><span>|</span><a href="#40813477">root</a><span>|</span><a href="#40813852">parent</a><span>|</span><a href="#40813914">prev</a><span>|</span><a href="#40812919">next</a><span>|</span><label class="collapse" for="c-40817185">[-]</label><label class="expand" for="c-40817185">[1 more]</label></div><br/><div class="children"><div class="content">RLHF worked well for midjourney but I think that is because it is outsourcing something that is ultimately completely subjective and very subtle like human visual aesthetic choice that can&#x27;t be &quot;wrong&quot;.<p>I tried to understand the paper and I can&#x27;t really make sense of it for &quot;code&quot;.<p>It seems like this would inherit a subtler version of all the problems from expert systems.<p>A press release of this does feel rather AI bubbly. Not quite Why The Future Doesn&#x27;t Need Us level but I think we are getting close.</div><br/></div></div></div></div></div></div></div></div><div id="40812919" class="c"><input type="checkbox" id="c-40812919" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#40813477">prev</a><span>|</span><a href="#40813173">next</a><span>|</span><label class="collapse" for="c-40812919">[-]</label><label class="expand" for="c-40812919">[5 more]</label></div><br/><div class="children"><div class="content">Since the whole thing is behind an API- exposing the works adds little value. If the corrections worked at an acceptable rate, one would just want them applied at the source.</div><br/><div id="40812950" class="c"><input type="checkbox" id="c-40812950" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#40812919">parent</a><span>|</span><a href="#40813173">next</a><span>|</span><label class="collapse" for="c-40812950">[-]</label><label class="expand" for="c-40812950">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>If the corrections worked at an acceptable rate, one would just want them applied at the source.</i><p>What do you mean? The model is for improving their RLHF trainers performance. RLHF does get applied &quot;at the source&quot; so to speak. It&#x27;s a modification on the model behind the API.<p>Perhaps if you were to say what you think this thing is for and then share why you think it&#x27;s not &quot;at the source&quot;.</div><br/><div id="40812986" class="c"><input type="checkbox" id="c-40812986" checked=""/><div class="controls bullet"><span class="by">Panoramix</span><span>|</span><a href="#40812919">root</a><span>|</span><a href="#40812950">parent</a><span>|</span><a href="#40813173">next</a><span>|</span><label class="collapse" for="c-40812986">[-]</label><label class="expand" for="c-40812986">[3 more]</label></div><br/><div class="children"><div class="content">Not OP but the screenshot in the article pretty much shows something that it&#x27;s not at the source.<p>You&#x27;d like to get the &quot;correct&quot; answer straight away, not watch a discussion between two bots.</div><br/><div id="40813012" class="c"><input type="checkbox" id="c-40813012" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#40812919">root</a><span>|</span><a href="#40812986">parent</a><span>|</span><a href="#40813035">next</a><span>|</span><label class="collapse" for="c-40813012">[-]</label><label class="expand" for="c-40813012">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but this is about helping the people who are training the model.</div><br/></div></div><div id="40813035" class="c"><input type="checkbox" id="c-40813035" checked=""/><div class="controls bullet"><span class="by">ertgbnm</span><span>|</span><a href="#40812919">root</a><span>|</span><a href="#40812986">parent</a><span>|</span><a href="#40813012">prev</a><span>|</span><a href="#40813173">next</a><span>|</span><label class="collapse" for="c-40813035">[-]</label><label class="expand" for="c-40813035">[1 more]</label></div><br/><div class="children"><div class="content">You are missing the point of the model in the first place. By having higher quality RLHF datasets, you get a higher quality final model. CriticGPT is not a product, but a tool to make GPT-4 and future models better.</div><br/></div></div></div></div></div></div></div></div><div id="40813173" class="c"><input type="checkbox" id="c-40813173" checked=""/><div class="controls bullet"><span class="by">wcoenen</span><span>|</span><a href="#40812919">prev</a><span>|</span><a href="#40813380">next</a><span>|</span><label class="collapse" for="c-40813173">[-]</label><label class="expand" for="c-40813173">[2 more]</label></div><br/><div class="children"><div class="content">This is about RLHF training. But I&#x27;ve wondered if something similar could be used to automatically judge the quality of the data that is used in pre-training, and then spend more compute on the good stuff. Or throw out really bad stuff even before building the tokenizer, to avoid those &quot;glitch token&quot; problems. Etc.</div><br/><div id="40813243" class="c"><input type="checkbox" id="c-40813243" checked=""/><div class="controls bullet"><span class="by">smsx</span><span>|</span><a href="#40813173">parent</a><span>|</span><a href="#40813380">next</a><span>|</span><label class="collapse" for="c-40813243">[-]</label><label class="expand" for="c-40813243">[1 more]</label></div><br/><div class="children"><div class="content">Yup, check out Rho-1 by microsoft research.</div><br/></div></div></div></div><div id="40813380" class="c"><input type="checkbox" id="c-40813380" checked=""/><div class="controls bullet"><span class="by">rodoxcasta</span><span>|</span><a href="#40813173">prev</a><span>|</span><a href="#40813344">next</a><span>|</span><label class="collapse" for="c-40813380">[-]</label><label class="expand" for="c-40813380">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Additionally, when people use CriticGPT, the AI augments their skills, resulting in more comprehensive critiques than when people work alone, and fewer hallucinated bugs than when the model works alone.<p>But, as per the first graphic, CriticGPT alone has better comprehensiveness than CriticGPT+Human? Is that right?</div><br/></div></div><div id="40813344" class="c"><input type="checkbox" id="c-40813344" checked=""/><div class="controls bullet"><span class="by">integral_1699</span><span>|</span><a href="#40813380">prev</a><span>|</span><a href="#40813132">next</a><span>|</span><label class="collapse" for="c-40813344">[-]</label><label class="expand" for="c-40813344">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using this approach myself, albeit manually, with ChatGPT. I first ask my question, then open a new chat and ask it to find flaws with the previous answer. Quite often, it does improve the end result.</div><br/></div></div><div id="40813132" class="c"><input type="checkbox" id="c-40813132" checked=""/><div class="controls bullet"><span class="by">megaman821</span><span>|</span><a href="#40813344">prev</a><span>|</span><a href="#40812867">next</a><span>|</span><label class="collapse" for="c-40813132">[-]</label><label class="expand" for="c-40813132">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if you could apply this to training data. Like here is an example of a common mistake and why that mistake could be made, or here is a statement made in jest and why it could be found funny.</div><br/></div></div><div id="40812867" class="c"><input type="checkbox" id="c-40812867" checked=""/><div class="controls bullet"><span class="by">GiorgioG</span><span>|</span><a href="#40813132">prev</a><span>|</span><a href="#40813822">next</a><span>|</span><label class="collapse" for="c-40812867">[-]</label><label class="expand" for="c-40812867">[39 more]</label></div><br/><div class="children"><div class="content">All these LLMs make up too much stuff, I don&#x27;t see how that can be fixed.</div><br/><div id="40812930" class="c"><input type="checkbox" id="c-40812930" checked=""/><div class="controls bullet"><span class="by">elwell</span><span>|</span><a href="#40812867">parent</a><span>|</span><a href="#40812993">next</a><span>|</span><label class="collapse" for="c-40812930">[-]</label><label class="expand" for="c-40812930">[28 more]</label></div><br/><div class="children"><div class="content">&gt; All these LLMs make up too much stuff, I don&#x27;t see how that can be fixed.<p>All these humans make up too much stuff, I don&#x27;t see how that can be fixed.</div><br/><div id="40812973" class="c"><input type="checkbox" id="c-40812973" checked=""/><div class="controls bullet"><span class="by">testfrequency</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812930">parent</a><span>|</span><a href="#40813583">next</a><span>|</span><label class="collapse" for="c-40812973">[-]</label><label class="expand" for="c-40812973">[6 more]</label></div><br/><div class="children"><div class="content">I know you’re trying to be edgy here, but if I was deciding between searching online and finding a source vs trying to shortcut and use GPT, but GPT decides to hallucinate and make something up - that’s the deceiving part.<p>The biggest issue is how confidently wrong GPT enjoys being. You can press GPT in either right or wrong direction and it will concede with minimal effort, which is also an issue. It’s just really bad russian roulette nerdspining until someone gets tired.</div><br/><div id="40813182" class="c"><input type="checkbox" id="c-40813182" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812973">parent</a><span>|</span><a href="#40813583">next</a><span>|</span><label class="collapse" for="c-40813182">[-]</label><label class="expand" for="c-40813182">[5 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t call it deceiving. In order to be motivated to deceive someone, you&#x27;d need agency and some benefit out of it</div><br/><div id="40813770" class="c"><input type="checkbox" id="c-40813770" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40813182">parent</a><span>|</span><a href="#40813238">next</a><span>|</span><label class="collapse" for="c-40813770">[-]</label><label class="expand" for="c-40813770">[3 more]</label></div><br/><div class="children"><div class="content">1. Deception describes a result, not a motivation. If someone has been led to believe something that isn&#x27;t true, they have been deceived, and this doesn&#x27;t require any other agents<p>2. While I agree that it&#x27;s a stretch to call ChatGPT agentic, it&#x27;s nonetheless &quot;motivated&quot; in the sense that it&#x27;s learned based on an objective function, which we can model as a causal factor behind its behavior, which might improve our understanding of that behavior. I think it&#x27;s relatively intuitive and not deeply incorrect to say that that a learned objective of generating plausible prose can be a causal factor which has led to a tendency to generate prose which often deceives people, and I see little value in getting nitpicky about agentic assumptions in colloquial language when a vast swath of the lexicon and grammar of human languages writ large does so essentially by default. &quot;The rain got me wet!&quot; doesn&#x27;t assume that the rain has agency</div><br/><div id="40816863" class="c"><input type="checkbox" id="c-40816863" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40813770">parent</a><span>|</span><a href="#40813238">next</a><span>|</span><label class="collapse" for="c-40816863">[-]</label><label class="expand" for="c-40816863">[2 more]</label></div><br/><div class="children"><div class="content">Well the definition of deception, according to Google and how I understand it, is:<p>&gt; deliberately cause (someone) to believe something that is not true, especially for personal gain.<p>Emphasis on the personal gain part. It seems like you have a different definition.<p>There&#x27;s no point in arguing about definitions, but I&#x27;m a big believer in that if you can identify a difference in the definitions people use early into a conversation, you can settle the argument at that.</div><br/><div id="40817286" class="c"><input type="checkbox" id="c-40817286" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40816863">parent</a><span>|</span><a href="#40813238">next</a><span>|</span><label class="collapse" for="c-40817286">[-]</label><label class="expand" for="c-40817286">[1 more]</label></div><br/><div class="children"><div class="content">I both agree that it&#x27;s pointless to argue about definitions and think you&#x27;ve presented a definition that fails to capture a lot of common usage of the word. I don&#x27;t think it matters what the dictionary says when we are talking about how a word is used. Like we use &quot;deceptive&quot; to describe inanimate objects pretty frequently. I responded to someone who thought describing the outputs of a machine learning model as deceiving people implied it had agency, which is nonsense</div><br/></div></div></div></div></div></div><div id="40813238" class="c"><input type="checkbox" id="c-40813238" checked=""/><div class="controls bullet"><span class="by">testfrequency</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40813182">parent</a><span>|</span><a href="#40813770">prev</a><span>|</span><a href="#40813583">next</a><span>|</span><label class="collapse" for="c-40813238">[-]</label><label class="expand" for="c-40813238">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t that GPT Plus? Trick you into thinking you have found your new friend and they understand everything? Surely OpenAI would like people to use their GPT over a Google search.<p>How do you think leadership at OpenAI would respond to that?</div><br/></div></div></div></div></div></div><div id="40813583" class="c"><input type="checkbox" id="c-40813583" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812930">parent</a><span>|</span><a href="#40812973">prev</a><span>|</span><a href="#40813336">next</a><span>|</span><label class="collapse" for="c-40813583">[-]</label><label class="expand" for="c-40813583">[11 more]</label></div><br/><div class="children"><div class="content">The problems of epistemology and informational quality control are complicated, but humanity has developed a decent amount of social and procedural technology to do these, some of which has defined the organization of various institutions. The mere presence of LLMs doesn&#x27;t fundamentally change how we should calibrate our beliefs or verify information. However, the mythology&#x2F;marketing that LLMs are &quot;outperforming humans&quot; combined with the fact that the most popular ones are black boxes to the overwhelming majority of their users means that a lot of people aren&#x27;t applying those tools to their outputs. As a technology, they&#x27;re much more useful if you treat them with what is roughly the appropriate level of skepticism for a human stranger you&#x27;re talking to on the street</div><br/><div id="40814499" class="c"><input type="checkbox" id="c-40814499" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40813583">parent</a><span>|</span><a href="#40813336">next</a><span>|</span><label class="collapse" for="c-40814499">[-]</label><label class="expand" for="c-40814499">[10 more]</label></div><br/><div class="children"><div class="content">I wonder what ChatGPT would have to say if I ran this text through with a specialized prompt. Your choice of words is interesting, almost like you are optimizing for persuasion, but simultaneously I get a strong vibe of  intention of optimizing for truth.</div><br/><div id="40814835" class="c"><input type="checkbox" id="c-40814835" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40814499">parent</a><span>|</span><a href="#40814786">next</a><span>|</span><label class="collapse" for="c-40814835">[-]</label><label class="expand" for="c-40814835">[4 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;ll find I&#x27;m quite horseshit at optimizing for persuasion, as you can easily verify by checking any other post I&#x27;ve ever made and the response it generally elicits. I find myself less motivated by what people think of me every year I&#x27;m alive, and less interested in what GPT would say about my replies each of the many times someone replies just to ponder that instead of just satisfying their curiosity immediately via copy-paste. Also, in general it seems unlikely humans function as optimizers natively, because optimization tends to require drastically narrowing and quantifying your objectives. I would guess that if they&#x27;re describable and consistent, most human utility functions look more like noisy prioritized sets of satisfaction criteria than the kind of objectives we can train a neural network against</div><br/><div id="40815673" class="c"><input type="checkbox" id="c-40815673" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40814835">parent</a><span>|</span><a href="#40814786">next</a><span>|</span><label class="collapse" for="c-40815673">[-]</label><label class="expand" for="c-40815673">[3 more]</label></div><br/><div class="children"><div class="content">This on the other hand I like, very much!<p>Particularly:<p>&gt; Also, in general it seems unlikely humans function as optimizers natively, because optimization tends to require drastically narrowing and quantifying your objectives. I would guess that if they&#x27;re describable and consistent, most human utility functions look more like noisy prioritized sets of satisfaction criteria than the kind of objectives we can train a neural network against<p>Considering this, what do you think us humans are <i>actually</i> up to, here on HN and in general?  It seems clear that we are up to <i>something</i>, but what might it be?</div><br/><div id="40815933" class="c"><input type="checkbox" id="c-40815933" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40815673">parent</a><span>|</span><a href="#40814786">next</a><span>|</span><label class="collapse" for="c-40815933">[-]</label><label class="expand" for="c-40815933">[2 more]</label></div><br/><div class="children"><div class="content">On HN? Killing time, reading articles, and getting nerdsniped by the feedback loop of getting insipid replies that unfortunately so many of us are constantly stuck in<p>In general? Slowly dying mostly. Talking. Eating. Fucking. Staring at microbes under a microscope. Feeding cats. Planting trees. Doing cartwheels. Really depends on the human</div><br/><div id="40818466" class="c"><input type="checkbox" id="c-40818466" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40815933">parent</a><span>|</span><a href="#40814786">next</a><span>|</span><label class="collapse" for="c-40818466">[-]</label><label class="expand" for="c-40818466">[1 more]</label></div><br/><div class="children"><div class="content">I would tend to agree!!<p>&gt; Talking.<p>Have you ever noticed any talking that ~&quot;projects seriousness &amp;&#x2F;or authority about important matters&quot; around here?</div><br/></div></div></div></div></div></div></div></div><div id="40814786" class="c"><input type="checkbox" id="c-40814786" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40814499">parent</a><span>|</span><a href="#40814835">prev</a><span>|</span><a href="#40813336">next</a><span>|</span><label class="collapse" for="c-40814786">[-]</label><label class="expand" for="c-40814786">[5 more]</label></div><br/><div class="children"><div class="content">FWIW I don&#x27;t understand a lot of what either of you mean, but I&#x27;m very interested. Quick run-through, excuse the editorial tone, I don&#x27;t know how to give feedback on writing without it.<p># Post 1<p>&gt; The problems of epistemology and informational quality control are complicated, but humanity has developed a decent amount of social and procedural technology  to do these, some of which has defined the organization of various institutions.<p><i>Very</i> fluffy, creating <i>very</i> uncertain parsing for reader.<p><i>Should</i> cut down, then <i>could</i> add specificity:<p>ex. &quot;Dealing with misinformation is complicated. But we have things like dictionaries and the internet, there&#x27;s even specialization in fact-checking, like Snopes.com&quot;<p>(I assume the specifics I added aren&#x27;t what you meant, just wanted to give an example)<p>&gt; The mere presence of LLMs doesn&#x27;t fundamentally change how we should calibrate our beliefs or verify information. However, the mythology&#x2F;marketing that LLMs are &quot;outperforming humans&quot;<p>They do, or are clearly at par, at many tasks.<p>Where is the quote from?<p>Is bringing this up relevant to the discussion?<p>Would us quibbling over that be relevant to this discussion?<p>&gt; combined with the fact that the most popular ones are black boxes to the overwhelming majority of their users means that a lot of people aren&#x27;t applying those tools to their outputs.<p>Are there unpopular ones aren&#x27;t black boxes?<p>What tools? (this may just indicate the benefit of a clearer intro)<p>&gt; As a technology, they&#x27;re much more useful if you treat them with what is roughly the appropriate level of skepticism for a human stranger you&#x27;re talking to on the street<p>This is a sort of obvious conclusion compared to the complicated language leading into it, and doesn&#x27;t add to the posts before it. Is there a stronger claim here?<p># Post 2<p>&gt; I wonder what ChatGPT would have to say if I ran this text through with a specialized prompt.<p>Why do you wonder that?<p>What does &quot;specialized&quot; mean in this context?<p>My guess is there&#x27;s a prompt you have in mind, which then would clarify A) what you&#x27;re wondering about B) what you meant by specialized prompt. But a prompt is a question, so it may be better to just ask the question?<p>&gt; Your choice of words is interesting, almost like you are optimizing for persuasion,<p>What language optimizes for persuasion? I&#x27;m guessing the fluffy advanced verbiage indicates that?<p>Does this boil down to &quot;Your word choice creates persuasive writing&quot;?<p>&gt; but simultaneously, I get a strong vibe of intention of optimizing for truth.<p>Is there a distinction here? What would &quot;optimizing for truth&quot; vs. &quot;optimizing for persuasion&quot; look like?<p>Do people usually write not-truthful things, to the point it&#x27;s worth noting that when you think people are writing with the intention of truth?</div><br/><div id="40815259" class="c"><input type="checkbox" id="c-40815259" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40814786">parent</a><span>|</span><a href="#40813336">next</a><span>|</span><label class="collapse" for="c-40815259">[-]</label><label class="expand" for="c-40815259">[4 more]</label></div><br/><div class="children"><div class="content">As long as we&#x27;re doing unsolicited advice, this revision seems predicated on the assumption that we are writing for a general audience, which ill suits the context in which the posts were made. This is especially bizarre because you then interject to defend the benchmarking claim I&#x27;ve called &quot;marketing&quot;, and having an opinion on that subject at all makes it clear that you also at the very least understand the shared context somewhat, despite being unable to parse the fairly obvious implication that treating models with undue credulity is a direct result of the outsized and ill-defined claims about their capabilities to which I refer. I agree that I could stand to be more concise, but if you find it difficult to parse my writing, perhaps this is simply because you are not its target audience</div><br/><div id="40815331" class="c"><input type="checkbox" id="c-40815331" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40815259">parent</a><span>|</span><a href="#40813336">next</a><span>|</span><label class="collapse" for="c-40815331">[-]</label><label class="expand" for="c-40815331">[3 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s go ahead and say the LLM stuff is all marketing and it&#x27;s all clearly worse than all humans. It&#x27;s plainly unrelated to anything else in the post, we don&#x27;t need to focus on it.<p>Like I said, I&#x27;m very interested!<p>Maybe it doesn&#x27;t mean anything other than what it says on the tin? You think people should treat an LLM like a stranger making claims? Makes sense!<p>It&#x27;s just unclear what a lot of it means and the word choice makes it seem like there&#x27;s something grander going on, <i>coughs</i> as our compatriots in this intricately weaved thread on the international network known as the world wide web have also explicated, and imparted via the written word, as their scrivening also remarks on the lexicographical phenomenae. <i>coughs</i><p>My only other guess is you are doing some form of performance art to teach us a broader lesson?<p>There&#x27;s something very &quot;off&quot; here, and I&#x27;m not the only to note it.  Like, my instinct is it&#x27;s iterated writing <i>using</i> an LLM asked to make it more graduate-school level.</div><br/><div id="40818480" class="c"><input type="checkbox" id="c-40818480" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40815331">parent</a><span>|</span><a href="#40815499">next</a><span>|</span><label class="collapse" for="c-40818480">[-]</label><label class="expand" for="c-40818480">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s something very &quot;off&quot; here<p>You mean on this planet?<p>If not, what do you think of that idea?  Does something not seem....<i>weird</i>?</div><br/></div></div><div id="40815499" class="c"><input type="checkbox" id="c-40815499" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40815331">parent</a><span>|</span><a href="#40818480">prev</a><span>|</span><a href="#40813336">next</a><span>|</span><label class="collapse" for="c-40815499">[-]</label><label class="expand" for="c-40815499">[1 more]</label></div><br/><div class="children"><div class="content">Your post and the one I originally responded to are good evidence against something I said earlier. The mere existence of LLMs <i>does</i> clearly change the landscape of epistemology, because whether or not they&#x27;re even involved in a conversation people will constantly invoke them when they think your prose is stilted (which is, by the way, exactly the wrong instinct), or to try to posture that they occupy some sort of elevated remove from the conversation (which I&#x27;d say they demonstrate false by replying at all). I guess dehumanizing people by accusing them of being &quot;robots&quot; is probably as old as the usage of that word if not older, but recently interest in talking robots has dramatically increased and so here we are<p>I can&#x27;t tell you exactly what you find &quot;off&quot; about my prose, because while you have advocated precision your objection is impossibly vague. I talk funny. Okay. Cool. Thanks.<p>Anyway, most benchmarks are garbage, and even if we take the validity of these benchmarks for granted, these AI companies don&#x27;t release their datasets or even weights, so we have no idea what&#x27;s out of distribution. To be clear, this means the claims can&#x27;t be verified <i>even by the standards of ML benchmarks</i>, and thus should be taken as marketing, because companies lying about their tech has both a clearly defined motivation and a constant stream of unrelenting precedent</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40813336" class="c"><input type="checkbox" id="c-40813336" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812930">parent</a><span>|</span><a href="#40813583">prev</a><span>|</span><a href="#40812959">next</a><span>|</span><label class="collapse" for="c-40813336">[-]</label><label class="expand" for="c-40813336">[1 more]</label></div><br/><div class="children"><div class="content">In reality, humans are often blunt and rude pessimists who say things can&#x27;t be done. But &quot;helpful chatbot&quot; LLM&#x27;s are specifically trained not to do that for anything but crude swaths of political&#x2F;social&#x2F;safety alignment.<p>When it comes to technical details, current LLM&#x27;s have a bias towards sycophancy and bullshitting that humans only show when especially desperate to impress or totally fearful.<p>Humans make mistakes too, but the distribution of those mistakes is wildly different and generally much easier to calibrate for and work around.</div><br/></div></div><div id="40812959" class="c"><input type="checkbox" id="c-40812959" checked=""/><div class="controls bullet"><span class="by">urduntupu</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812930">parent</a><span>|</span><a href="#40813336">prev</a><span>|</span><a href="#40815037">next</a><span>|</span><label class="collapse" for="c-40812959">[-]</label><label class="expand" for="c-40812959">[7 more]</label></div><br/><div class="children"><div class="content">Exactly, you can&#x27;t even fix the problem at the root, b&#x2F;c the problem is already with the humans, making up stuff.</div><br/><div id="40812997" class="c"><input type="checkbox" id="c-40812997" checked=""/><div class="controls bullet"><span class="by">testfrequency</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812959">parent</a><span>|</span><a href="#40815037">next</a><span>|</span><label class="collapse" for="c-40812997">[-]</label><label class="expand" for="c-40812997">[6 more]</label></div><br/><div class="children"><div class="content">Believe it or not, there are websites that have real things posted. This is honestly my biggest shock that OpenAI thought Reddit of all places is a trustworthy source for knowledge.</div><br/><div id="40813365" class="c"><input type="checkbox" id="c-40813365" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812997">parent</a><span>|</span><a href="#40814320">next</a><span>|</span><label class="collapse" for="c-40813365">[-]</label><label class="expand" for="c-40813365">[1 more]</label></div><br/><div class="children"><div class="content">Reddit has been the most trustworthy source for me in the last ~5 years, especially when I want to buy something.</div><br/></div></div><div id="40814320" class="c"><input type="checkbox" id="c-40814320" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812997">parent</a><span>|</span><a href="#40813365">prev</a><span>|</span><a href="#40813122">next</a><span>|</span><label class="collapse" for="c-40814320">[-]</label><label class="expand" for="c-40814320">[1 more]</label></div><br/><div class="children"><div class="content">While Reddit is often helpful for me (Google site:reddit.com), it&#x27;s nice to toggle between reddit and non-reddit.<p>I hope LLMs will offer a &quot;-reddit&quot; model to switch to when needed.</div><br/></div></div><div id="40813122" class="c"><input type="checkbox" id="c-40813122" checked=""/><div class="controls bullet"><span class="by">QuesnayJr</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812997">parent</a><span>|</span><a href="#40814320">prev</a><span>|</span><a href="#40813625">next</a><span>|</span><label class="collapse" for="c-40813122">[-]</label><label class="expand" for="c-40813122">[1 more]</label></div><br/><div class="children"><div class="content">Reddit is so much better than the average SEO-optimized site that adding &quot;reddit&quot; to your search is a common trick for using Google.</div><br/></div></div><div id="40813625" class="c"><input type="checkbox" id="c-40813625" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812997">parent</a><span>|</span><a href="#40813122">prev</a><span>|</span><a href="#40815037">next</a><span>|</span><label class="collapse" for="c-40813625">[-]</label><label class="expand" for="c-40813625">[2 more]</label></div><br/><div class="children"><div class="content">The websites with content authored by people is full of bullshit, intentional and unintentional.</div><br/><div id="40816019" class="c"><input type="checkbox" id="c-40816019" checked=""/><div class="controls bullet"><span class="by">testfrequency</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40813625">parent</a><span>|</span><a href="#40815037">next</a><span>|</span><label class="collapse" for="c-40816019">[-]</label><label class="expand" for="c-40816019">[1 more]</label></div><br/><div class="children"><div class="content">It’s genuinely concerning to me how many people replied with thinking reddit is the gospel for factual information.<p>Reddit, while it has some niche communities with tribal info and knowledge, is FULL of spam, bots, companies masquerading as users, etc etc etc. If people are truly relying on reddit as a source of truth (which OpenAI is now being influenced by), then the world is just going to be amplify all the spam that already exists</div><br/></div></div></div></div></div></div></div></div><div id="40815037" class="c"><input type="checkbox" id="c-40815037" checked=""/><div class="controls bullet"><span class="by">nonameiguess</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812930">parent</a><span>|</span><a href="#40812959">prev</a><span>|</span><a href="#40814776">next</a><span>|</span><label class="collapse" for="c-40815037">[-]</label><label class="expand" for="c-40815037">[1 more]</label></div><br/><div class="children"><div class="content">advael&#x27;s answer was fine, but since people seem to be hung up on the wording, a more direct response:<p>We have human institutions dedicated at least nominally to finding and publishing truth (I hate having to qualify this, but Hacker News is so cynical and post-modernist at this point that I don&#x27;t know what else to do). These include, for instance, court systems. These include a notion of evidentiary standards. Eyewitnesses are treated as more reliable than hearsay. Written or taped recordings are more reliable than both. Multiple witnesses who agree are more reliable than one. Another example is science. Science utilizes peer review, along with its own notion of hierarchy of evidence, similar to but separate from the court&#x27;s. Interventional trials are better evidence than observational studies. Randomization and statistical testing is used to try and tease out effects from noise. Results that replicate are more reliable than a single study. Journalism is yet another example. This is probably the arena in which Hacker News is most cynical and will declare all of it is useless trash, but nonetheless reputable news organizations do have methods they use to try and be correct more often than they are not. They employ their own fact checkers. They seek out multiple expert sources. They send journalists directly to a scene to bear witness themselves to events as they unfold.<p>You&#x27;re free to think this isn&#x27;t sufficient, but this is how we deal with humans making up stuff and it&#x27;s gotten us modern civilization at least, full of warts but also full of wonders, seemingly because we&#x27;re actually right about a lot of stuff.<p>At some point, something analogous will presumably be the answer for how LLMs deal with this, too. The training will have to be changed to make the system aware of quality of evidence. Place greater trust in direct sensor output versus reading something online. Place greater trust in what you read from a reputable academic journal versus a Tweet. Etc. As it stands now, unlike human learners, the objective function of an LLM is just to produce a string in which each piece is in some reasonably high-density region of the probability distribution of possible next pieces as observed from historical recorded text. Luckily, producing strings in this way happens to generate a whole lot of true statements, but it does not have truth as an explicit goal and, until it does, we shouldn&#x27;t forget that. Treat it with the treatment it deserves, as if some human savant with perfect recall had never left a dark room to experience the outside world, but had read everything ever written, unfortunately without any understanding of the difference between reading a textbook and reading 4chan.</div><br/></div></div><div id="40814776" class="c"><input type="checkbox" id="c-40814776" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812930">parent</a><span>|</span><a href="#40815037">prev</a><span>|</span><a href="#40812993">next</a><span>|</span><label class="collapse" for="c-40814776">[-]</label><label class="expand" for="c-40814776">[1 more]</label></div><br/><div class="children"><div class="content">If I am going to trust a machine then it should perform at the level of a very competent human, not a general human.<p>Why would I want to ask your average person a physics question? Of course, their answer will probably be wrong and partly made up. Why should that be the bar?<p>I want it to answer at the level of a physics expert. And a physics expert is far less likely to make basic mistakes.</div><br/></div></div></div></div><div id="40812993" class="c"><input type="checkbox" id="c-40812993" checked=""/><div class="controls bullet"><span class="by">ssharp</span><span>|</span><a href="#40812867">parent</a><span>|</span><a href="#40812930">prev</a><span>|</span><a href="#40813057">next</a><span>|</span><label class="collapse" for="c-40812993">[-]</label><label class="expand" for="c-40812993">[8 more]</label></div><br/><div class="children"><div class="content">I keep hearing about people using these for coding. Seems like it would be extremely easy to miss something and then spend more time debugging than it would be to do yourself.<p>I tried recently to have ChatGPT an .htaccess RewriteCond&#x2F;Rule for me and it was extremely confident you couldn&#x27;t do something I needed to do. When I told it that it just needed to add a flag to the end of the rule (I was curious and was purposely non-specific about what flag it needed), it suddenly knew exactly what to do. Thankfully I knew what it needed but otherwise I might have walked away thinking it couldn&#x27;t be accomplished.</div><br/><div id="40813069" class="c"><input type="checkbox" id="c-40813069" checked=""/><div class="controls bullet"><span class="by">GiorgioG</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812993">parent</a><span>|</span><a href="#40813128">next</a><span>|</span><label class="collapse" for="c-40813069">[-]</label><label class="expand" for="c-40813069">[4 more]</label></div><br/><div class="children"><div class="content">My experience is that it will simply make up methods, properties and fields that do NOT exist in well-documented APIs.  If something isn&#x27;t possible, that&#x27;s fine, just tell me it&#x27;s not possible. I spent an hour trying to get ChatGPT (4&#x2F;4o and 3.5) to write some code to do one specific thing (dump&#x2F;log detailed memory allocation data from the current .NET application process) for diagnosing an intermittent out of memory exception in a production application.  The answer as far as I can tell is that it&#x27;s not possible in-process.  Maybe it&#x27;s possible out of process using the profiling API, but that doesn&#x27;t help me in a locked-down k8s pod&#x2F;container in AWS.</div><br/><div id="40813681" class="c"><input type="checkbox" id="c-40813681" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40813069">parent</a><span>|</span><a href="#40813231">next</a><span>|</span><label class="collapse" for="c-40813681">[-]</label><label class="expand" for="c-40813681">[1 more]</label></div><br/><div class="children"><div class="content">I think once you understand that they&#x27;re prone to do that, it&#x27;s less of a problem in practice.  You just don&#x27;t ask it questions that requires detailed knowledge of an API unless it&#x27;s _extremely_ popular.  Like in kubernetes terms, it&#x27;s safe to ask it about a pod spec, less safe to ask it details about istio configuration and even less safe to ask it about some random operator with 50 stars on github.<p>Mostly it&#x27;s good at structure and syntax, so I&#x27;ll often find the library&#x2F;spec I want, paste in the relevant documentation and ask it to write my function for me.<p>This may seem like a waste of time because once you&#x27;ve got the documentation you can just write the code yourself, but A: that takes 5 times as long and B: I think people 
 underestimate how much general domain knowledge is buried in chatgpt so it&#x27;s pretty good at inferring the details of what you&#x27;re looking for or what you should have asked about.<p>In general, I think the more your interaction with chatgpt is framed as a dialogue and less as a &#x27;fill in the blanks&#x27; exercise, the more you&#x27;ll get out of it.</div><br/></div></div><div id="40813174" class="c"><input type="checkbox" id="c-40813174" checked=""/><div class="controls bullet"><span class="by">neonsunset</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40813069">parent</a><span>|</span><a href="#40813231">prev</a><span>|</span><a href="#40813128">next</a><span>|</span><label class="collapse" for="c-40813174">[-]</label><label class="expand" for="c-40813174">[1 more]</label></div><br/><div class="children"><div class="content">From within the process it might be difficult*, but please do give this a read <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;dotnet&#x2F;core&#x2F;diagnostics&#x2F;dumps" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;dotnet&#x2F;core&#x2F;diagnostics&#x2F;du...</a> and dotnet-dump + dotnet-trace a try.<p>If you are still seeing the issue with memory and GC, you can submit it to <a href="https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtime&#x2F;issues">https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtime&#x2F;issues</a> especially if you are doing something that is expected to just work(tm).<p>* difficult as in retrieving data detailed enough to trace individual allocations, otherwise `GC.GetGCMemoryInfo()` and adjacent methods can give you high-level overview. There are more advanced tools but I always had the option to either use remote debugging in Windows Server days and dotnet-dump and dotnet-trace for containerized applications to diagnose the issues, so haven&#x27;t really explored what is needed for the more locked down environments.</div><br/></div></div></div></div><div id="40813259" class="c"><input type="checkbox" id="c-40813259" checked=""/><div class="controls bullet"><span class="by">BurningFrog</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812993">parent</a><span>|</span><a href="#40813128">prev</a><span>|</span><a href="#40813653">next</a><span>|</span><label class="collapse" for="c-40813259">[-]</label><label class="expand" for="c-40813259">[1 more]</label></div><br/><div class="children"><div class="content">If I ever let it AI write code, I&#x27;d write serious tests for it.<p>Just like I do with my own code.<p>Both AI and I &quot;hallucinate&quot; sometimes, but with good tests you make things work.</div><br/></div></div><div id="40813653" class="c"><input type="checkbox" id="c-40813653" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#40812867">root</a><span>|</span><a href="#40812993">parent</a><span>|</span><a href="#40813259">prev</a><span>|</span><a href="#40813057">next</a><span>|</span><label class="collapse" for="c-40813653">[-]</label><label class="expand" for="c-40813653">[1 more]</label></div><br/><div class="children"><div class="content">This problem applies almost universally as far as I can tell.<p>If you are knowledgeable on a subject matter you&#x27;re asking for help with, the LLM can be guided to value.  This means you do have to throw out bad or flat out wrong output regularly.<p>This becomes a problem when you have no prior experience in a domain. For example reviewing legal contracts about a real estate transaction. If you aren&#x27;t familiar enough with the workflow and details of steps you can&#x27;t provide critique and follow-on guidance.<p>However, the response still stands before you, and it can be tempting to glom onto it.<p>This is not all that different from the current experience with search engines, though. Where if you&#x27;re trying to get an answer to a question, you may wade through and even initially accept answers from websites that are completely wrong.<p>For example, products to apply to the foundation of an old basement. Some sites will recommend products that are not good at all, but do so because the content owners get associate compensation for it.<p>The difference is that LLM responses appear less biased (no associate links, no SEO keyword targeting), but are still wrong.<p>All that said, sometimes LLMs just crush it when details don&#x27;t matter.  For example, building a simple cross-platform pyqt-based application. Search engine results can not do this. Wheras, at least for rapid prototyping, GPT is very, very good.</div><br/></div></div></div></div><div id="40812940" class="c"><input type="checkbox" id="c-40812940" checked=""/><div class="controls bullet"><span class="by">spiderfarmer</span><span>|</span><a href="#40812867">parent</a><span>|</span><a href="#40813057">prev</a><span>|</span><a href="#40813822">next</a><span>|</span><label class="collapse" for="c-40812940">[-]</label><label class="expand" for="c-40812940">[1 more]</label></div><br/><div class="children"><div class="content">Mixture of agents prevents a lot of fact fabrication.</div><br/></div></div></div></div><div id="40813822" class="c"><input type="checkbox" id="c-40813822" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#40812867">prev</a><span>|</span><a href="#40812970">next</a><span>|</span><label class="collapse" for="c-40813822">[-]</label><label class="expand" for="c-40813822">[1 more]</label></div><br/><div class="children"><div class="content">And both can still be wrong as they have no understanding of the mistake.</div><br/></div></div><div id="40812970" class="c"><input type="checkbox" id="c-40812970" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#40813822">prev</a><span>|</span><a href="#40812730">next</a><span>|</span><label class="collapse" for="c-40812970">[-]</label><label class="expand" for="c-40812970">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a combination of a GAN[1] and RLHF. Not surprising that this works.<p>[1] - <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generative_adversarial_network" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Generative_adversarial_network</a></div><br/></div></div><div id="40812730" class="c"><input type="checkbox" id="c-40812730" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40812970">prev</a><span>|</span><a href="#40812922">next</a><span>|</span><label class="collapse" for="c-40812730">[-]</label><label class="expand" for="c-40812730">[30 more]</label></div><br/><div class="children"><div class="content">How do they know the critic did not make a mistake? Do they have a critic for the critic?</div><br/><div id="40812848" class="c"><input type="checkbox" id="c-40812848" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#40812730">parent</a><span>|</span><a href="#40813139">next</a><span>|</span><label class="collapse" for="c-40812848">[-]</label><label class="expand" for="c-40812848">[7 more]</label></div><br/><div class="children"><div class="content">Per the article, the critic for the critic is human RLHF trainers. More specifically those humans are exploited third world workers making between $1.32 and $2 an hour, but OpenAI would rather you didn&#x27;t know about that.<p><a href="https:&#x2F;&#x2F;time.com&#x2F;6247678&#x2F;openai-chatgpt-kenya-workers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;time.com&#x2F;6247678&#x2F;openai-chatgpt-kenya-workers&#x2F;</a></div><br/><div id="40818810" class="c"><input type="checkbox" id="c-40818810" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812848">parent</a><span>|</span><a href="#40813088">next</a><span>|</span><label class="collapse" for="c-40818810">[-]</label><label class="expand" for="c-40818810">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI may well still be employing plenty of people in third world countries for this. But there are also contracts providing anywhere from $20 to $100+ an hour to do this kind of work for more complex prompt&#x2F;response pairs.<p>I&#x27;ve done work on what (at least to my belief) is the very high end of that scale (not for OpenAI) to fill gaps, so I know firsthand that it&#x27;s available, and sometimes the work is complex enough that a single response can take over an hour to evaluate because the requirements often include not just reading and reviewing the code, but ensuring it works, including fixing bugs. Most of the responses then pass through at least one more round of reviews of the fixed&#x2F;updated responses. One project I did work on involved 3 reviewers (none of whom were on salaries anywhere close to the Kenyan workers you referred to) reviewing my work and providing feedback and a second pass of adjustments. So four high-paid workers altogether to process every response.<p>Of course, I&#x27;m sure plenty lower-level&#x2F;simpler work had been filtered out to be addressed with cheaper labour, but I wouldn&#x27;t be so sure their costs for things like code is particularly low.</div><br/></div></div><div id="40813088" class="c"><input type="checkbox" id="c-40813088" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812848">parent</a><span>|</span><a href="#40818810">prev</a><span>|</span><a href="#40813462">next</a><span>|</span><label class="collapse" for="c-40813088">[-]</label><label class="expand" for="c-40813088">[1 more]</label></div><br/><div class="children"><div class="content">That is more than the average entry level position in Kenya. The work is probably also much easier (physically, that is).</div><br/></div></div><div id="40813462" class="c"><input type="checkbox" id="c-40813462" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812848">parent</a><span>|</span><a href="#40813088">prev</a><span>|</span><a href="#40812912">next</a><span>|</span><label class="collapse" for="c-40813462">[-]</label><label class="expand" for="c-40813462">[1 more]</label></div><br/><div class="children"><div class="content">Exploited? Are you saying that these employees are forced to work for below market rates, and would be better off with other opportunities available to them? If that&#x27;s the case, it&#x27;s truly horrible on OpenAI&#x27;s part.</div><br/></div></div><div id="40812912" class="c"><input type="checkbox" id="c-40812912" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812848">parent</a><span>|</span><a href="#40813462">prev</a><span>|</span><a href="#40813139">next</a><span>|</span><label class="collapse" for="c-40812912">[-]</label><label class="expand" for="c-40812912">[3 more]</label></div><br/><div class="children"><div class="content">Every leap of civilization was built off the back of a disposable workforce. - Niander Wallace</div><br/><div id="40813211" class="c"><input type="checkbox" id="c-40813211" checked=""/><div class="controls bullet"><span class="by">wmeredith</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812912">parent</a><span>|</span><a href="#40813139">next</a><span>|</span><label class="collapse" for="c-40813211">[-]</label><label class="expand" for="c-40813211">[2 more]</label></div><br/><div class="children"><div class="content">He was the bad guy, right?</div><br/></div></div></div></div></div></div><div id="40813139" class="c"><input type="checkbox" id="c-40813139" checked=""/><div class="controls bullet"><span class="by">ertgbnm</span><span>|</span><a href="#40812730">parent</a><span>|</span><a href="#40812848">prev</a><span>|</span><a href="#40812784">next</a><span>|</span><label class="collapse" for="c-40813139">[-]</label><label class="expand" for="c-40813139">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the human&#x27;s job for now.<p>A human reviewer might have trouble catching a mistake, but they are generally pretty good at discerning a report about a mistake is valid or not. For example, finding a bug in a codebase is hard. But if a junior sends you a code snippet and says &quot;I think this is a bug for xyz reason&quot;, do you agree? It&#x27;s much easier to confidently say yes or no. So basically it changes the problem from finding a needle in a haystack to discerning if a statement is a hallucination or not.</div><br/></div></div><div id="40812784" class="c"><input type="checkbox" id="c-40812784" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40812730">parent</a><span>|</span><a href="#40813139">prev</a><span>|</span><a href="#40812978">next</a><span>|</span><label class="collapse" for="c-40812784">[-]</label><label class="expand" for="c-40812784">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s called iteration. Humans do the same thing.</div><br/><div id="40812890" class="c"><input type="checkbox" id="c-40812890" checked=""/><div class="controls bullet"><span class="by">citizen_friend</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812784">parent</a><span>|</span><a href="#40812803">next</a><span>|</span><label class="collapse" for="c-40812890">[-]</label><label class="expand" for="c-40812890">[2 more]</label></div><br/><div class="children"><div class="content">It’s not a human, and we shouldn’t assume it will have traits we do without evidence.<p>Iteration also is when your brain meets the external world and corrects. This is a closed system.</div><br/><div id="40818813" class="c"><input type="checkbox" id="c-40818813" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812890">parent</a><span>|</span><a href="#40812803">next</a><span>|</span><label class="collapse" for="c-40818813">[-]</label><label class="expand" for="c-40818813">[1 more]</label></div><br/><div class="children"><div class="content">We are not assuming that. The iteration happens by taking the report and passing it to another reviewer who reviews the first review. Their comparison is between a human reviewer passing reports to a human reviewer vs. CriticGPT -&gt; human reviewer vs. CriticGPT+human reviewer -&gt; human reviewer.</div><br/></div></div></div></div><div id="40812803" class="c"><input type="checkbox" id="c-40812803" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812784">parent</a><span>|</span><a href="#40812890">prev</a><span>|</span><a href="#40812978">next</a><span>|</span><label class="collapse" for="c-40812803">[-]</label><label class="expand" for="c-40812803">[1 more]</label></div><br/><div class="children"><div class="content">Are you sure it&#x27;s not called recursion?</div><br/></div></div></div></div><div id="40812978" class="c"><input type="checkbox" id="c-40812978" checked=""/><div class="controls bullet"><span class="by">finger</span><span>|</span><a href="#40812730">parent</a><span>|</span><a href="#40812784">prev</a><span>|</span><a href="#40812864">next</a><span>|</span><label class="collapse" for="c-40812978">[-]</label><label class="expand" for="c-40812978">[2 more]</label></div><br/><div class="children"><div class="content">There is already a mistake. It refers to a function by the wrong name: os.path.comonpath &gt; commonpath</div><br/><div id="40812991" class="c"><input type="checkbox" id="c-40812991" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812978">parent</a><span>|</span><a href="#40812864">next</a><span>|</span><label class="collapse" for="c-40812991">[-]</label><label class="expand" for="c-40812991">[1 more]</label></div><br/><div class="children"><div class="content">In the critical limit every GPT critic chain is essentially a spellchecker.</div><br/></div></div></div></div><div id="40812864" class="c"><input type="checkbox" id="c-40812864" checked=""/><div class="controls bullet"><span class="by">nmca</span><span>|</span><a href="#40812730">parent</a><span>|</span><a href="#40812978">prev</a><span>|</span><a href="#40812753">next</a><span>|</span><label class="collapse" for="c-40812864">[-]</label><label class="expand" for="c-40812864">[3 more]</label></div><br/><div class="children"><div class="content">A critic for the critic would be “Recursive Reward Modelling”, an exciting idea that has not been made to work in the real world yet.</div><br/><div id="40812881" class="c"><input type="checkbox" id="c-40812881" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812864">parent</a><span>|</span><a href="#40812753">next</a><span>|</span><label class="collapse" for="c-40812881">[-]</label><label class="expand" for="c-40812881">[2 more]</label></div><br/><div class="children"><div class="content">Most of my ideas are not original but where can I learn more about this recursive reward modeling problem?</div><br/><div id="40812938" class="c"><input type="checkbox" id="c-40812938" checked=""/><div class="controls bullet"><span class="by">nmca</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812881">parent</a><span>|</span><a href="#40812753">next</a><span>|</span><label class="collapse" for="c-40812938">[-]</label><label class="expand" for="c-40812938">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.07871" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.07871</a></div><br/></div></div></div></div></div></div><div id="40812753" class="c"><input type="checkbox" id="c-40812753" checked=""/><div class="controls bullet"><span class="by">OlleTO</span><span>|</span><a href="#40812730">parent</a><span>|</span><a href="#40812864">prev</a><span>|</span><a href="#40812842">next</a><span>|</span><label class="collapse" for="c-40812753">[-]</label><label class="expand" for="c-40812753">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s critics all the way down</div><br/><div id="40812777" class="c"><input type="checkbox" id="c-40812777" checked=""/><div class="controls bullet"><span class="by">azulster</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812753">parent</a><span>|</span><a href="#40812765">next</a><span>|</span><label class="collapse" for="c-40812777">[-]</label><label class="expand" for="c-40812777">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s literally just the oracle problem all over again</div><br/></div></div></div></div><div id="40812842" class="c"><input type="checkbox" id="c-40812842" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40812730">parent</a><span>|</span><a href="#40812753">prev</a><span>|</span><a href="#40812922">next</a><span>|</span><label class="collapse" for="c-40812842">[-]</label><label class="expand" for="c-40812842">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s written in the article, the critic makes mistakes, but it&#x27;s better than not having it.</div><br/><div id="40812862" class="c"><input type="checkbox" id="c-40812862" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812842">parent</a><span>|</span><a href="#40812922">next</a><span>|</span><label class="collapse" for="c-40812862">[-]</label><label class="expand" for="c-40812862">[7 more]</label></div><br/><div class="children"><div class="content">How do they know it&#x27;s better? The rate of mistakes is the same for both GPTs so now they have 2 sources of errors. If the error rate was lower for one then they could always apply it and reduce the error rate of the other. They&#x27;re just shuffling the deck chairs and hoping the boat with a hole goes a slightly longer distance before disappearing completely underwater.</div><br/><div id="40818187" class="c"><input type="checkbox" id="c-40818187" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812862">parent</a><span>|</span><a href="#40813234">next</a><span>|</span><label class="collapse" for="c-40818187">[-]</label><label class="expand" for="c-40818187">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How do they know it&#x27;s better?<p>From the article:<p>&quot;In our experiments a second random trainer preferred critiques from the Human+CriticGPT team over those from an unassisted person more than 60% of the time.&quot;<p>Of course the second trainer could be wrong, but when the outcome tilts 60% to 40% in favour of the *combination of a human + CriticGPT that&#x27;s pretty significant.<p>From experience doing contract work in this space, it&#x27;s common to use multiple layers of reviewers to generate additional data for RLHF, and if you can improve the output from the first layer that much it&#x27;ll have a fairly massive effect on the amount of training data you can produce at the same cost.</div><br/></div></div><div id="40813234" class="c"><input type="checkbox" id="c-40813234" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812862">parent</a><span>|</span><a href="#40818187">prev</a><span>|</span><a href="#40812924">next</a><span>|</span><label class="collapse" for="c-40813234">[-]</label><label class="expand" for="c-40813234">[3 more]</label></div><br/><div class="children"><div class="content">Whether adding unreliable components increases the overall reliability of a system depends on whether the system requires <i>all</i> components to work (in which case adding components can only make matters worse) or only <i>some</i> (in which case adding components can improve redundancy and make it more likely that the final result is correct).<p>In the particular case of spotting mistakes made by ChatGPT, a mistake is spotted if it is spotted by the human reviewer <i>or</i> by the critic, so even a critic that makes many mistakes itself can still increase the number of spotted errors. (But it might decrease the spotting rate per unit time, so there are still trade-offs to be made.)</div><br/><div id="40813345" class="c"><input type="checkbox" id="c-40813345" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40813234">parent</a><span>|</span><a href="#40812924">next</a><span>|</span><label class="collapse" for="c-40813345">[-]</label><label class="expand" for="c-40813345">[2 more]</label></div><br/><div class="children"><div class="content">I see what you&#x27;re saying so what OpenAI will do next is create an army of GPT critics and then run them all in parallel to take some kind of quorum vote on correctness. I guess it should work in theory if the error rate is small enough and adding more critics actually reduces the error rate. My guess is that in practice they&#x27;ll converge to the population average rate of error and then pat themselves on the back for a job well done.</div><br/><div id="40813857" class="c"><input type="checkbox" id="c-40813857" checked=""/><div class="controls bullet"><span class="by">svachalek</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40813345">parent</a><span>|</span><a href="#40812924">next</a><span>|</span><label class="collapse" for="c-40813857">[-]</label><label class="expand" for="c-40813857">[1 more]</label></div><br/><div class="children"><div class="content">That description is remarkably apt for almost every business meeting I&#x27;ve ever been in.</div><br/></div></div></div></div></div></div><div id="40813125" class="c"><input type="checkbox" id="c-40813125" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40812730">root</a><span>|</span><a href="#40812862">parent</a><span>|</span><a href="#40812924">prev</a><span>|</span><a href="#40812922">next</a><span>|</span><label class="collapse" for="c-40813125">[-]</label><label class="expand" for="c-40813125">[1 more]</label></div><br/><div class="children"><div class="content">&gt;How do they know it&#x27;s better?<p>Probably just evaluation on benchmarks.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>