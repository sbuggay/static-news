<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684486857597" as="style"/><link rel="stylesheet" href="styles.css?v=1684486857597"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://orlp.net/blog/bitwise-binary-search/">Bitwise Binary Search: Elegant and Fast</a> <span class="domain">(<a href="https://orlp.net">orlp.net</a>)</span></div><div class="subtext"><span>usefulcat</span> | <span>15 comments</span></div><br/><div><div id="35994910" class="c"><input type="checkbox" id="c-35994910" checked=""/><div class="controls bullet"><span class="by">ot</span><span>|</span><a href="#35997105">next</a><span>|</span><label class="collapse" for="c-35994910">[-]</label><label class="expand" for="c-35994910">[3 more]</label></div><br/><div class="children"><div class="content">&gt; However, I’ve found that some compilers, e.g. GCC on x86-64 will refuse to make this variant branchless. I hate how fickle compilers can be sometimes, and I wish compilers exposed not just the likely&#x2F;unlikely attributes, but also an attribute that allows you to mark something as unpredictable to nudge the compiler towards using branchless techniques like CMOV’s.<p>There has been a Clang bug open about this for more than 4 years: <a href="https:&#x2F;&#x2F;bugs.llvm.org&#x2F;show_bug.cgi?id=40027" rel="nofollow">https:&#x2F;&#x2F;bugs.llvm.org&#x2F;show_bug.cgi?id=40027</a><p>Naturally, this came up in the context of binary searches, but it is the same for any tree traversal where each branch has maximum entropy (which means it&#x27;s optimal from a traversal time point of view). Clang is especially reluctant to emit predicated instructions (this is a major pet peeve of mine).<p>In my codebase I ended up having a function<p><pre><code>    template &lt;class T, class I, class Compare&gt;
    I argmax(const T&amp; a, const T&amp; b, I i, I j, Compare cmp) {
      return cmp(a, b) ? j : i;
    }
</code></pre>
which I then specialize for various integer types and code it with inline ASM to use a CMOV. This allows to write all kinds of binary searches and tree traversals without having to do inline ASM every time. I&#x27;d really prefer not to have to do that, but the performance impact is significant.<p>In this post, I don&#x27;t think there is a comparison with a classical binary search implemented in a branch-free fashion? Searching along dyadic intervals is a cute trick, but it&#x27;s not clear whether the improvement comes from that or simply from being branch-free.</div><br/><div id="35997829" class="c"><input type="checkbox" id="c-35997829" checked=""/><div class="controls bullet"><span class="by">usefulcat</span><span>|</span><a href="#35994910">parent</a><span>|</span><a href="#35997105">next</a><span>|</span><label class="collapse" for="c-35997829">[-]</label><label class="expand" for="c-35997829">[2 more]</label></div><br/><div class="children"><div class="content">Interesting, I’ve often heard that introducing inline assembly is likely to confuse the optimizer more than anything else. Do you happen to have any examples of this technique that you can share?</div><br/><div id="35999285" class="c"><input type="checkbox" id="c-35999285" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#35994910">root</a><span>|</span><a href="#35997829">parent</a><span>|</span><a href="#35997105">next</a><span>|</span><label class="collapse" for="c-35999285">[-]</label><label class="expand" for="c-35999285">[1 more]</label></div><br/><div class="children"><div class="content">Generally if you specify the list of clobbers tightly and don’t spoil memory things can end up mostly reasonable.</div><br/></div></div></div></div></div></div><div id="35997105" class="c"><input type="checkbox" id="c-35997105" checked=""/><div class="controls bullet"><span class="by">mlochbaum</span><span>|</span><a href="#35994910">prev</a><span>|</span><a href="#35996464">next</a><span>|</span><label class="collapse" for="c-35997105">[-]</label><label class="expand" for="c-35997105">[1 more]</label></div><br/><div class="children"><div class="content">As the article says, overlapping halves turns out faster than using powers of two in the general case. But for small enough searches the bitwise method can be run in vector registers! I talked about this at [0], slides as zipped html at [1] (and I forgot the last click on the multi-byte data slide that shifts the indices for the final result!). The overlap method is probably also possible, but the simpler bitwise operations work out better in SIMD.<p>[0] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;paxIkKBzqBU?t=610" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;paxIkKBzqBU?t=610</a><p>[1] <a href="https:&#x2F;&#x2F;www.dyalog.com&#x2F;uploads&#x2F;conference&#x2F;dyalog19&#x2F;presentations&#x2F;D04_Tacit_Techniques.zip" rel="nofollow">https:&#x2F;&#x2F;www.dyalog.com&#x2F;uploads&#x2F;conference&#x2F;dyalog19&#x2F;presentat...</a></div><br/></div></div><div id="35996464" class="c"><input type="checkbox" id="c-35996464" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#35997105">prev</a><span>|</span><a href="#35996047">next</a><span>|</span><label class="collapse" for="c-35996464">[-]</label><label class="expand" for="c-35996464">[1 more]</label></div><br/><div class="children"><div class="content">Done in hardware decades ago.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Successive-approximation_ADC" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Successive-approximation_ADC</a><p>Branchless and everything.</div><br/></div></div><div id="35996047" class="c"><input type="checkbox" id="c-35996047" checked=""/><div class="controls bullet"><span class="by">juliusgeo</span><span>|</span><a href="#35996464">prev</a><span>|</span><a href="#35998517">next</a><span>|</span><label class="collapse" for="c-35996047">[-]</label><label class="expand" for="c-35996047">[1 more]</label></div><br/><div class="children"><div class="content">I made an implementation of that same articles algorithm in Python using C-extensions, and it beat the stdlib implementation across many sizes of arrays. <a href="https:&#x2F;&#x2F;github.com&#x2F;juliusgeo&#x2F;branchless_bisect">https:&#x2F;&#x2F;github.com&#x2F;juliusgeo&#x2F;branchless_bisect</a></div><br/></div></div><div id="35998517" class="c"><input type="checkbox" id="c-35998517" checked=""/><div class="controls bullet"><span class="by">abainbridge</span><span>|</span><a href="#35996047">prev</a><span>|</span><a href="#35990603">next</a><span>|</span><label class="collapse" for="c-35998517">[-]</label><label class="expand" for="c-35998517">[1 more]</label></div><br/><div class="children"><div class="content">From the article, &quot;A commonly heard advice is to not use binary search for small arrays, but to use a linear search instead. I find that not to be true on the Apple M1 for integers, at least compared to my branchless binary search, when searching a runtime-sized but otherwise fixed size array&quot;.<p>I expect the linear search is better when the data being searched isn&#x27;t yet in cache because it allows the memory subsystem to better predict what the CPU will access next. I&#x27;m not sure how large this effect is. It would be interesting to see the benchmarks redone on uncached data.</div><br/></div></div><div id="35990603" class="c"><input type="checkbox" id="c-35990603" checked=""/><div class="controls bullet"><span class="by">williamkuszmaul</span><span>|</span><a href="#35998517">prev</a><span>|</span><a href="#35995612">next</a><span>|</span><label class="collapse" for="c-35990603">[-]</label><label class="expand" for="c-35990603">[2 more]</label></div><br/><div class="children"><div class="content">One thing I&#x27;m confused about: Did the author try vectorizing the linear search implementation? (Of course, it is possible that even if they did not, the compiler did.) I would imagine that vectorization is behind the advice to use linear searches on small arrays.</div><br/><div id="35991940" class="c"><input type="checkbox" id="c-35991940" checked=""/><div class="controls bullet"><span class="by">dkersten</span><span>|</span><a href="#35990603">parent</a><span>|</span><a href="#35995612">next</a><span>|</span><label class="collapse" for="c-35991940">[-]</label><label class="expand" for="c-35991940">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I would imagine that vectorization is behind the advice to use linear searches on small arrays.<p>I always thought a core part is that searching linearly is simply very cache efficient due to easy prefetching of linear data and that this is simply faster when you don’t have to search too much. Of course, that <i>and</i> vectorisation together is even better.</div><br/></div></div></div></div><div id="35995612" class="c"><input type="checkbox" id="c-35995612" checked=""/><div class="controls bullet"><span class="by">tehjoker</span><span>|</span><a href="#35990603">prev</a><span>|</span><a href="#35990289">next</a><span>|</span><label class="collapse" for="c-35995612">[-]</label><label class="expand" for="c-35995612">[1 more]</label></div><br/><div class="children"><div class="content">Related: <a href="https:&#x2F;&#x2F;algorithmica.org&#x2F;en&#x2F;eytzinger" rel="nofollow">https:&#x2F;&#x2F;algorithmica.org&#x2F;en&#x2F;eytzinger</a></div><br/></div></div><div id="35990289" class="c"><input type="checkbox" id="c-35990289" checked=""/><div class="controls bullet"><span class="by">peter_d_sherman</span><span>|</span><a href="#35995612">prev</a><span>|</span><label class="collapse" for="c-35990289">[-]</label><label class="expand" for="c-35990289">[4 more]</label></div><br/><div class="children"><div class="content">Random Idea: It might be interesting to try to implement a bitwise binary search in x86 (or other ISA) as <i>a single additional x86 (or other ISA) instruction in microcode</i> (it could be implemented behind the scenes with multiple supporting microcode steps&#x2F;instructions) -- the idea being that <i>if</i> it could be implemented in microcode, it could potentially be even faster than a software (compiling to multiple x86 (or other ISA) instructions) version...<p>Anyway, great article!</div><br/><div id="35993647" class="c"><input type="checkbox" id="c-35993647" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#35990289">parent</a><span>|</span><label class="collapse" for="c-35993647">[-]</label><label class="expand" for="c-35993647">[3 more]</label></div><br/><div class="children"><div class="content">That would only help if the the bottleneck of your algorithm is getting the code from memory to the CPU to decode and for the CPU to decode the instructions. But if your code is that big then it wouldn&#x27;t fit microcode either.<p>Pipelined CPUs are insanely efficient at executing code at the maximum speed the execution units would permit, so having more complex code encoded in a macro instruction offers diminishing returns until it actually becomes detrimental because the surface area you could devote for general caches is now occupied for stuff that doesn&#x27;t help all that much.</div><br/><div id="35994163" class="c"><input type="checkbox" id="c-35994163" checked=""/><div class="controls bullet"><span class="by">peter_d_sherman</span><span>|</span><a href="#35990289">root</a><span>|</span><a href="#35993647">parent</a><span>|</span><label class="collapse" for="c-35994163">[-]</label><label class="expand" for="c-35994163">[2 more]</label></div><br/><div class="children"><div class="content">&gt;That would only help if the the bottleneck of your algorithm is getting the code from memory to the CPU to decode and for the CPU to decode the instructions.<p>In the context you use them in, kindly define: &quot;bottleneck&quot;, &quot;algorithm&quot;, &quot;code&quot;, and &quot;decode&quot;...<p>&gt;But if your code is that big then it wouldn&#x27;t fit microcode either.<p>How big is the code for binary search ?<p>&gt;Pipelined CPUs are insanely efficient at executing code at the maximum speed the execution units would permit, so having more complex code encoded in a macro instruction offers diminishing returns until it actually becomes detrimental because the surface area you could devote for general caches is now occupied for stuff that doesn&#x27;t help all that much.<p>Does microcode occupy the &quot;surface area&quot; devoted to &quot;general caches&quot; ?</div><br/><div id="35998579" class="c"><input type="checkbox" id="c-35998579" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#35990289">root</a><span>|</span><a href="#35994163">parent</a><span>|</span><label class="collapse" for="c-35998579">[-]</label><label class="expand" for="c-35998579">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry I don&#x27;t have time at the moment to go deeper on this but I&#x27;ll add two things that may be useful for the reader:<p>1. The Intel Optimization Reference, under Section 3.5.1, advises:<p>&quot;Favor single-micro-operation instructions.&quot;<p>&quot;Avoid using complex instructions (for example, enter, leave, or loop) that have more than 4 micro-ops and require multiple cycles to decode. Use sequences of simple instructions instead.&quot;<p>2. By surface I meant literally silicon surface area. That and transistor count (and other aspects like fan-out and clock domains etc) are the major aspects you trade when engineering a CPU.<p>If you need larger microcode ROM to store larger microprograms you also need more bits to address into the microcode ROM and that makes microcode programs even larger! All this consumes surface area and transistors that could be devoted to something else.<p>Sure, if you could fit the binary search instruction microcode in the existing spare space of the microcode ROM you wouldn&#x27;t have that problem but you&#x27;d still be competing with other potential use cases of those alleged &quot;microcode speedups&quot;. What about a UTF-8 string length instruction, would that be more important? Etc</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>