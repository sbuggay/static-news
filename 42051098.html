<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730883683750" as="style"/><link rel="stylesheet" href="styles.css?v=1730883683750"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://spectrum.ieee.org/watermark">DeepMind debuts watermarks for AI-generated text</a> <span class="domain">(<a href="https://spectrum.ieee.org">spectrum.ieee.org</a>)</span></div><div class="subtext"><span>ambigious7777</span> | <span>99 comments</span></div><br/><div><div id="42056152" class="c"><input type="checkbox" id="c-42056152" checked=""/><div class="controls bullet"><span class="by">blintz</span><span>|</span><a href="#42057650">next</a><span>|</span><label class="collapse" for="c-42056152">[-]</label><label class="expand" for="c-42056152">[4 more]</label></div><br/><div class="children"><div class="content">These watermarks are not robust to paraphrasing attacks: AUC ROC falls from 0.95 to 0.55 (barely better than guessing) for a 100 token passage.<p>The existing impossibility results imply that these attacks are essentially unavoidable (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04378" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04378</a>) and not very costly, so this line of inquiry into LLM watermarking seems like a dead end.</div><br/><div id="42056346" class="c"><input type="checkbox" id="c-42056346" checked=""/><div class="controls bullet"><span class="by">jkhdigital</span><span>|</span><a href="#42056152">parent</a><span>|</span><a href="#42057932">next</a><span>|</span><label class="collapse" for="c-42056346">[-]</label><label class="expand" for="c-42056346">[1 more]</label></div><br/><div class="children"><div class="content">I spent the last five years doing PhD research into steganography, with a particular focus on how to embed messages into LLM outputs. Watermarking is basically one-bit steganography.<p>The first serious investigations into &quot;secure&quot; steganography were about 30 years ago and it was clearly a dead end even back then. Sure, watermarking might be effective against lazy adversaries--college students, job applicants, etc.--but can be trivially defeated otherwise.<p>All this time I&#x27;d been lamenting my research area as unpopular and boring when I should&#x27;ve been submitting to <i>Nature</i>!</div><br/></div></div><div id="42057932" class="c"><input type="checkbox" id="c-42057932" checked=""/><div class="controls bullet"><span class="by">sbszllr</span><span>|</span><a href="#42056152">parent</a><span>|</span><a href="#42056346">prev</a><span>|</span><a href="#42056302">next</a><span>|</span><label class="collapse" for="c-42057932">[-]</label><label class="expand" for="c-42057932">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been working in the space since 2018. Watermarking and fingerprinting (of models themselves and outputs) are useful tools but they have a weak adversary model.<p>Yet, it doesn’t stop companies from making claims like these, and what’s worse, people buying into them.</div><br/></div></div></div></div><div id="42057650" class="c"><input type="checkbox" id="c-42057650" checked=""/><div class="controls bullet"><span class="by">ruuda</span><span>|</span><a href="#42056152">prev</a><span>|</span><a href="#42051387">next</a><span>|</span><label class="collapse" for="c-42057650">[-]</label><label class="expand" for="c-42057650">[1 more]</label></div><br/><div class="children"><div class="content">Some comments here point at impossibility results, but after screening hundreds of job applications at work, it&#x27;s not hard to pick out the LLM writing, even without watermark. My internal LLM detector is now so sensitive that I can tell when my confirmed-human colleagues used an LLM to rephrase something when it&#x27;s longer than just one sentence. The writing style is just so different.<p>Maybe if you prompt it right, it can do a better job of masking itself, but people don&#x27;t seem to do that.</div><br/></div></div><div id="42051387" class="c"><input type="checkbox" id="c-42051387" checked=""/><div class="controls bullet"><span class="by">bko</span><span>|</span><a href="#42057650">prev</a><span>|</span><a href="#42051308">next</a><span>|</span><label class="collapse" for="c-42051387">[-]</label><label class="expand" for="c-42051387">[9 more]</label></div><br/><div class="children"><div class="content">This article goes into it a little bit, but an interview with Scott Aaronson goes into some detail about how watermarking works[0].<p>He&#x27;s a theoretical computer scientist but he was recruited by OpenAI to work on AI safety. He has a very practical view on the matter and is focusing his efforts on leveraging the probabilistic nature of LLMs to provide a digital undetectable watermark. So it nudges certain words to be paired together slightly more than random and you can mathematically derive with some level of certainty whether an output or even a section of an output was generated by the LLM. It&#x27;s really clever and apparently he has a working prototype in development.<p>Some work arounds he hasn&#x27;t figured out yet is asking for an output in language X and then translating it into language Y. But those may still be eventually figured out.<p>I think watermarking would be a big step forward to practical AI safety and ideally this method would be adopted by all major LLMs.<p>That part starts around 1 hour 25 min in.<p>&gt; Scott Aaronson: Exactly. In fact, we have a pseudorandom function that maps the N-gram to, let’s say, a real number from zero to one. Let’s say we call that real number ri for each possible choice i of the next token. And then let’s say that GPT has told us that the ith token should be chosen with probability pi.<p><a href="https:&#x2F;&#x2F;axrp.net&#x2F;episode&#x2F;2023&#x2F;04&#x2F;11&#x2F;episode-20-reform-ai-alignment-scott-aaronson.html" rel="nofollow">https:&#x2F;&#x2F;axrp.net&#x2F;episode&#x2F;2023&#x2F;04&#x2F;11&#x2F;episode-20-reform-ai-ali...</a></div><br/><div id="42054451" class="c"><input type="checkbox" id="c-42054451" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#42051387">parent</a><span>|</span><a href="#42056186">next</a><span>|</span><label class="collapse" for="c-42054451">[-]</label><label class="expand" for="c-42054451">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that provable watermarking is possible in practice. The method you mention is clever, but before it can work, you would need to know the probability of the every other source which could also be used to generate the output for the same purpose. If you can claim that the probability of that model is much higher on that model than in any other place, including humans, then watermark might give some stronger indications.<p>You would also need to define probability graph based on the output length. The longer the output, more certain you can be. What is the smallest amount of tokens that cannot be proved at all?<p>You would also need include humans. Can you define that for human? All LLMs should use the same system uniformally.<p>Otherwise, &quot;watermaking&quot; is doomed to be misused and not being reliable enough. False accusations will be take a place.</div><br/><div id="42056580" class="c"><input type="checkbox" id="c-42056580" checked=""/><div class="controls bullet"><span class="by">A_D_E_P_T</span><span>|</span><a href="#42051387">root</a><span>|</span><a href="#42054451">parent</a><span>|</span><a href="#42056186">next</a><span>|</span><label class="collapse" for="c-42056580">[-]</label><label class="expand" for="c-42056580">[1 more]</label></div><br/><div class="children"><div class="content">I agree. I&#x27;d add that not only could human-written content fail the test -- it&#x27;s also the case that humans will <i>detect</i> the word pairing, just as they detected &quot;delve&quot; and various other LLM tells.<p>In time most forms of watermarking along those lines will seem like elements of an LLM&#x27;s writing style, and will quickly be edited out by savvy users.</div><br/></div></div></div></div><div id="42056186" class="c"><input type="checkbox" id="c-42056186" checked=""/><div class="controls bullet"><span class="by">123yawaworht456</span><span>|</span><a href="#42051387">parent</a><span>|</span><a href="#42054451">prev</a><span>|</span><a href="#42051484">next</a><span>|</span><label class="collapse" for="c-42056186">[-]</label><label class="expand" for="c-42056186">[3 more]</label></div><br/><div class="children"><div class="content">&gt;So it nudges certain words to be paired together slightly more than random and you can mathematically derive with some level of certainty whether an output or even a section of an output was generated by the LLM.<p>hah, every single LLM already watermarks its output by starting the second paragraph with &quot;It is important&#x2F;essential to remember that...&quot; followed by inane gibberish, no matter what question you ask.</div><br/><div id="42056325" class="c"><input type="checkbox" id="c-42056325" checked=""/><div class="controls bullet"><span class="by">AlienRobot</span><span>|</span><a href="#42051387">root</a><span>|</span><a href="#42056186">parent</a><span>|</span><a href="#42051484">next</a><span>|</span><label class="collapse" for="c-42056325">[-]</label><label class="expand" for="c-42056325">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always felt you&#x27;d be able to  tell someone uses Reddit because they&#x27;ll reply to a comment starting the sentence with &quot;The problem is that...&quot;<p>Now LLMs are trained on Reddit users.</div><br/><div id="42057227" class="c"><input type="checkbox" id="c-42057227" checked=""/><div class="controls bullet"><span class="by">badsectoracula</span><span>|</span><a href="#42051387">root</a><span>|</span><a href="#42056325">parent</a><span>|</span><a href="#42051484">next</a><span>|</span><label class="collapse" for="c-42057227">[-]</label><label class="expand" for="c-42057227">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like LLMs are trained on my posts because i tend to use both of those phrases :-P</div><br/></div></div></div></div></div></div><div id="42051484" class="c"><input type="checkbox" id="c-42051484" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42051387">parent</a><span>|</span><a href="#42056186">prev</a><span>|</span><a href="#42057291">next</a><span>|</span><label class="collapse" for="c-42051484">[-]</label><label class="expand" for="c-42051484">[2 more]</label></div><br/><div class="children"><div class="content">Sounds interesting, but it also sounds like something that could very well be circumvented by using a technique similar to speculative decoding: you use the censored model like you&#x27;d use the fast llm in speculative decoding, and you check whether the other model agrees with it or not. But instead of correcting the token every time both models disagree like you&#x27;d do with speculative decoding, you just need to change it often enough to mess with the watermark detection function (maybe you&#x27;d change every other mismatched token, or maybe one every 5 tokens would be enough to reduce the signal-to-noise ratio below the detection threshold).<p>You wouldn&#x27;t even need to have access to an unwatermarked model, the “correcting model” could even be watermaked itself as long as it&#x27;s not the same watermarking function applied to both.<p>Or am I misunderstanding something?</div><br/><div id="42056372" class="c"><input type="checkbox" id="c-42056372" checked=""/><div class="controls bullet"><span class="by">jkhdigital</span><span>|</span><a href="#42051387">root</a><span>|</span><a href="#42051484">parent</a><span>|</span><a href="#42057291">next</a><span>|</span><label class="collapse" for="c-42056372">[-]</label><label class="expand" for="c-42056372">[1 more]</label></div><br/><div class="children"><div class="content">No you&#x27;ve got it right. Watermarks like this are trivial to defeat, which means they are only effective against lazy users like cheating college students and job applicants.</div><br/></div></div></div></div><div id="42057291" class="c"><input type="checkbox" id="c-42057291" checked=""/><div class="controls bullet"><span class="by">nprateem</span><span>|</span><a href="#42051387">parent</a><span>|</span><a href="#42051484">prev</a><span>|</span><a href="#42051308">next</a><span>|</span><label class="collapse" for="c-42057291">[-]</label><label class="expand" for="c-42057291">[1 more]</label></div><br/><div class="children"><div class="content">Or just check whether text contains the word delve and it&#x27;s most likely AI generated. I fucking hate that word now.</div><br/></div></div></div></div><div id="42051308" class="c"><input type="checkbox" id="c-42051308" checked=""/><div class="controls bullet"><span class="by">namanyayg</span><span>|</span><a href="#42051387">prev</a><span>|</span><a href="#42051230">next</a><span>|</span><label class="collapse" for="c-42051308">[-]</label><label class="expand" for="c-42051308">[9 more]</label></div><br/><div class="children"><div class="content">&quot;An LLM generates text one token at a time. These tokens can represent a single character, word or part of a phrase. To create a sequence of coherent text, the model predicts the next most likely token to generate. These predictions are based on the preceding words and the probability scores assigned to each potential token.<p>For example, with the phrase “My favorite tropical fruits are __.” The LLM might start completing the sentence with the tokens “mango,” “lychee,” “papaya,” or “durian,” and each token is given a probability score. When there’s a range of different tokens to choose from, SynthID can adjust the probability score of each predicted token, in cases where it won’t compromise the quality, accuracy and creativity of the output.<p>This process is repeated throughout the generated text, so a single sentence might contain ten or more adjusted probability scores, and a page could contain hundreds. The final pattern of scores for both the model’s word choices combined with the adjusted probability scores are considered the watermark. This technique can be used for as few as three sentences. And as the text increases in length, SynthID’s robustness and accuracy increases.&quot;<p>Better link: <a href="https:&#x2F;&#x2F;deepmind.google&#x2F;technologies&#x2F;synthid&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;technologies&#x2F;synthid&#x2F;</a></div><br/><div id="42051493" class="c"><input type="checkbox" id="c-42051493" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#42051308">parent</a><span>|</span><a href="#42051397">next</a><span>|</span><label class="collapse" for="c-42051493">[-]</label><label class="expand" for="c-42051493">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m fascinated that this approach works <i>at all</i>, but that said, I don&#x27;t believe watermarking text will ever be practical. Yes, you can do an academic study where you have exactly 1 version of an LLM in exactly 1 parameter configuration, and you can have an algorithm that tweaks the logits of different tokens in a way that produces a recognizable pattern. But you should note that the pattern will be recognizable only when the LLM version is locked and the parameter configuration is locked. Which they won&#x27;t be in the real world. You will have a bunch of different models, and people will use them with a bunch of different parameter combinations. If your &quot;detector&quot; has to be able to recognize AI generated text from a variety of models and a variety of parameter combinations, it&#x27;s no longer going to work. Even if you imagine someone bruteforcing all these different combos, trouble is that some of the combos will produce false positives just because you tested so many of them. Want to get rid off those false positives? Go ahead, make the pattern stronger. And now you&#x27;re visibly altering the generated text to an extent where that is a quality issue.<p>In summary, this will not work in practice. Ever.</div><br/><div id="42056293" class="c"><input type="checkbox" id="c-42056293" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#42051308">root</a><span>|</span><a href="#42051493">parent</a><span>|</span><a href="#42056241">next</a><span>|</span><label class="collapse" for="c-42056293">[-]</label><label class="expand" for="c-42056293">[1 more]</label></div><br/><div class="children"><div class="content">In practice, every programmer or a writer who gets the LLM output, does a lot of rewriting for already existing code, or already existing text. Stitching together parts of many LLM outputs is the only way to use an LLM effectively, even stitching together parts of different LLMs, which i do all the time.<p>Recognizing only parts of a watermark, and many watermarked parts scattered all around doesn&#x27;t seem possible at all, in my mind.<p>They can however develop a software to sell very expensively to universities, schools etc, and it will occasionally catch a very guilty person who uses it all the time and doesn&#x27;t even try to make the answer better, who always hands over the LLM answer in one piece.<p>At the end of the day, it will lead to so many false accusations people will stop trusting it. In chess players and tournaments false accusations of cheating happen all the time, for 15 years or more. Right now former world chess champion Kramnik has accused over 50 top chess players of cheating, including the 5 times US champion Nakamura, in the span of 2 months.<p>If a software like that gets applied to schools and universities, we are gonna have the fun of our lives.</div><br/></div></div><div id="42056241" class="c"><input type="checkbox" id="c-42056241" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42051308">root</a><span>|</span><a href="#42051493">parent</a><span>|</span><a href="#42056293">prev</a><span>|</span><a href="#42051397">next</a><span>|</span><label class="collapse" for="c-42056241">[-]</label><label class="expand" for="c-42056241">[3 more]</label></div><br/><div class="children"><div class="content">Even with temperature = 0, LLMs are still non-deterministic, as their internal, massively parallelized calculations are done with <i>floating point arithmetic</i>, which is order-dependent. Running the same LLM with the exact same parameters multiple times might still yield slightly different probabilities in the output, making this watermarking scheme even less robust.</div><br/><div id="42056385" class="c"><input type="checkbox" id="c-42056385" checked=""/><div class="controls bullet"><span class="by">jkhdigital</span><span>|</span><a href="#42051308">root</a><span>|</span><a href="#42056241">parent</a><span>|</span><a href="#42057315">next</a><span>|</span><label class="collapse" for="c-42056385">[-]</label><label class="expand" for="c-42056385">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t necessarily true, it just depends on the implementation. I can say that because I&#x27;ve published research which embeds steganographic text into the output of GPT-2 and we had to deal with this. Running everything locally was usually fine--the model was deterministic as long as you had the same initial conditions. The problems occurred when trying to run the model on different hardware.</div><br/></div></div><div id="42057315" class="c"><input type="checkbox" id="c-42057315" checked=""/><div class="controls bullet"><span class="by">nprateem</span><span>|</span><a href="#42051308">root</a><span>|</span><a href="#42056241">parent</a><span>|</span><a href="#42056385">prev</a><span>|</span><a href="#42051397">next</a><span>|</span><label class="collapse" for="c-42057315">[-]</label><label class="expand" for="c-42057315">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not my experience unless LLM providers are caching results. It&#x27;s frustratingly difficult to get it to output substantially different text for a given prompt. It&#x27;s like internally it always follows mostly the same reasoning for step 1, then step 2 applies light fudging of the output to give the appearance of randomness, but the underlying structure is generally the same. That&#x27;s why there&#x27;s so much blog spam that all pretty much read the same, but while one &quot;delves&quot; into a topic another &quot;dives&quot; into it.<p>How long until they can write genuinely unique output without piles of additional prompting?</div><br/></div></div></div></div></div></div><div id="42051397" class="c"><input type="checkbox" id="c-42051397" checked=""/><div class="controls bullet"><span class="by">bgro</span><span>|</span><a href="#42051308">parent</a><span>|</span><a href="#42051493">prev</a><span>|</span><a href="#42051230">next</a><span>|</span><label class="collapse" for="c-42051397">[-]</label><label class="expand" for="c-42051397">[3 more]</label></div><br/><div class="children"><div class="content">Couldn’t this be easily disrupted as a watermark system by simply changing the words to interfere with the relative checksum?<p>I suspect sentence structure is also being used or, more likely, the primary “watermark”. Similar to how you can easily identify if something is at least NOT a Yoda quote based on it having incorrect structure. Combine that with other negative patterns like the quote containing Harry Potter references instead of Star Wars, and you can start to build up a profile of trends like this statement.<p>By rewriting the sentence structure and altering usual wording instead of directly copying the raw output, it seems like you could defeat any current raw watermarking.<p>Though this hasn’t stopped Google and others in the past using bad science and stats to make unhinged entitled claims like when they added captcha problems everybody said would be “literally impossible“ for bots to solve.<p>What a surprise how trivial they were to automate and the data they produce can be sold for profit at the expense of mass consumer time.</div><br/><div id="42051582" class="c"><input type="checkbox" id="c-42051582" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#42051308">root</a><span>|</span><a href="#42051397">parent</a><span>|</span><a href="#42051230">next</a><span>|</span><label class="collapse" for="c-42051582">[-]</label><label class="expand" for="c-42051582">[2 more]</label></div><br/><div class="children"><div class="content">In principle, it seems like you could have semantic watermarking. For instance, suppose I want a short story. There are lots of different narrative and semantic aspects of it that each carry some number of bits of information: setting, characters, events, and those lay on a probability distribution like anything else. You just subtly shift the probability distribution of those choices, and then it&#x27;s resistant to word choice, reordering, and any transformation that maintains its semantic meaning.</div><br/><div id="42056859" class="c"><input type="checkbox" id="c-42056859" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#42051308">root</a><span>|</span><a href="#42051582">parent</a><span>|</span><a href="#42051230">next</a><span>|</span><label class="collapse" for="c-42056859">[-]</label><label class="expand" for="c-42056859">[1 more]</label></div><br/><div class="children"><div class="content">Much simpler: make every sentence contain an even number of words. Then the chances of 10 sentences in a row to be all even is about 0.1%.</div><br/></div></div></div></div></div></div></div></div><div id="42051230" class="c"><input type="checkbox" id="c-42051230" checked=""/><div class="controls bullet"><span class="by">ksaj</span><span>|</span><a href="#42051308">prev</a><span>|</span><a href="#42056760">next</a><span>|</span><label class="collapse" for="c-42051230">[-]</label><label class="expand" for="c-42051230">[8 more]</label></div><br/><div class="children"><div class="content">Some of the watermarking is really obvious. If you write song lyrics in ChatGPT, watch for phrases like &quot;come what may&quot; and &quot;I stand tall.&quot;<p>It&#x27;s not just that they are (somewhat) unusual phrases, it&#x27;s that ChatGPT comes up with those phrases so very often.<p>It&#x27;s quite like how earlier versions always had a &quot;However&quot; in between explanations.</div><br/><div id="42051288" class="c"><input type="checkbox" id="c-42051288" checked=""/><div class="controls bullet"><span class="by">fkyoureadthedoc</span><span>|</span><a href="#42051230">parent</a><span>|</span><a href="#42051255">next</a><span>|</span><label class="collapse" for="c-42051288">[-]</label><label class="expand" for="c-42051288">[1 more]</label></div><br/><div class="children"><div class="content">Coheed and Cambria were using ChatGPT this whole damn time, smh</div><br/></div></div><div id="42051255" class="c"><input type="checkbox" id="c-42051255" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#42051230">parent</a><span>|</span><a href="#42051288">prev</a><span>|</span><a href="#42056760">next</a><span>|</span><label class="collapse" for="c-42051255">[-]</label><label class="expand" for="c-42051255">[6 more]</label></div><br/><div class="children"><div class="content">ChatGPT does not have a watermark.</div><br/><div id="42051418" class="c"><input type="checkbox" id="c-42051418" checked=""/><div class="controls bullet"><span class="by">sunaookami</span><span>|</span><a href="#42051230">root</a><span>|</span><a href="#42051255">parent</a><span>|</span><a href="#42051281">next</a><span>|</span><label class="collapse" for="c-42051418">[-]</label><label class="expand" for="c-42051418">[3 more]</label></div><br/><div class="children"><div class="content">It has a rich tapestry of watermarks.</div><br/><div id="42055980" class="c"><input type="checkbox" id="c-42055980" checked=""/><div class="controls bullet"><span class="by">becquerel</span><span>|</span><a href="#42051230">root</a><span>|</span><a href="#42051418">parent</a><span>|</span><a href="#42051281">next</a><span>|</span><label class="collapse" for="c-42055980">[-]</label><label class="expand" for="c-42055980">[2 more]</label></div><br/><div class="children"><div class="content">A delicate dance of them, perhaps</div><br/><div id="42057038" class="c"><input type="checkbox" id="c-42057038" checked=""/><div class="controls bullet"><span class="by">mondobe</span><span>|</span><a href="#42051230">root</a><span>|</span><a href="#42055980">parent</a><span>|</span><a href="#42051281">next</a><span>|</span><label class="collapse" for="c-42057038">[-]</label><label class="expand" for="c-42057038">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like a symphony of watermarks, all playing in harmony.</div><br/></div></div></div></div></div></div><div id="42051281" class="c"><input type="checkbox" id="c-42051281" checked=""/><div class="controls bullet"><span class="by">jgalt212</span><span>|</span><a href="#42051230">root</a><span>|</span><a href="#42051255">parent</a><span>|</span><a href="#42051418">prev</a><span>|</span><a href="#42051284">next</a><span>|</span><label class="collapse" for="c-42051281">[-]</label><label class="expand" for="c-42051281">[1 more]</label></div><br/><div class="children"><div class="content">I suggest we &quot;delve&quot; deeper int this problem.</div><br/></div></div><div id="42051284" class="c"><input type="checkbox" id="c-42051284" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42051230">root</a><span>|</span><a href="#42051255">parent</a><span>|</span><a href="#42051281">prev</a><span>|</span><a href="#42056760">next</a><span>|</span><label class="collapse" for="c-42051284">[-]</label><label class="expand" for="c-42051284">[1 more]</label></div><br/><div class="children"><div class="content">What makes you sure about that?</div><br/></div></div></div></div></div></div><div id="42056760" class="c"><input type="checkbox" id="c-42056760" checked=""/><div class="controls bullet"><span class="by">fny</span><span>|</span><a href="#42051230">prev</a><span>|</span><a href="#42051274">next</a><span>|</span><label class="collapse" for="c-42056760">[-]</label><label class="expand" for="c-42056760">[3 more]</label></div><br/><div class="children"><div class="content">I think we just need to give up on this. What’s the harm? It’s not like some ground truth is fabricated.<p>I’m far, far more concerned about photo, video, and audio verification. We need a camera that can guarantee a recording is real.</div><br/><div id="42056812" class="c"><input type="checkbox" id="c-42056812" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#42056760">parent</a><span>|</span><a href="#42056833">next</a><span>|</span><label class="collapse" for="c-42056812">[-]</label><label class="expand" for="c-42056812">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been thinking about this for a while. Digital signatures can guarantee that a piece of data is authentic, if the author wishes to sign it.</div><br/></div></div><div id="42056833" class="c"><input type="checkbox" id="c-42056833" checked=""/><div class="controls bullet"><span class="by">foxglacier</span><span>|</span><a href="#42056760">parent</a><span>|</span><a href="#42056812">prev</a><span>|</span><a href="#42051274">next</a><span>|</span><label class="collapse" for="c-42056833">[-]</label><label class="expand" for="c-42056833">[1 more]</label></div><br/><div class="children"><div class="content">Why do we need that for photo, video and audio? If it&#x27;s about the general public believing something false, they&#x27;re not going to check the watermarks of random internet content or trust anyone who says they checked it. If they really want to know, they can go to the source and if they trust that person or organization, they can also trust the content they published. If it&#x27;s about use in court, we already have a system for that - the person who recorded it appears in court as a witness and promises that they didn&#x27;t alter it then if it turns out they did, they can go to prison.</div><br/></div></div></div></div><div id="42051274" class="c"><input type="checkbox" id="c-42051274" checked=""/><div class="controls bullet"><span class="by">mateus1</span><span>|</span><a href="#42056760">prev</a><span>|</span><a href="#42051337">next</a><span>|</span><label class="collapse" for="c-42051274">[-]</label><label class="expand" for="c-42051274">[7 more]</label></div><br/><div class="children"><div class="content">Google is branding this in a positive light but this is just AI text DRM.</div><br/><div id="42051339" class="c"><input type="checkbox" id="c-42051339" checked=""/><div class="controls bullet"><span class="by">sebstefan</span><span>|</span><a href="#42051274">parent</a><span>|</span><a href="#42051560">next</a><span>|</span><label class="collapse" for="c-42051339">[-]</label><label class="expand" for="c-42051339">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s likely more about preventing model incest than digital rights management</div><br/></div></div><div id="42051560" class="c"><input type="checkbox" id="c-42051560" checked=""/><div class="controls bullet"><span class="by">gwbas1c</span><span>|</span><a href="#42051274">parent</a><span>|</span><a href="#42051339">prev</a><span>|</span><a href="#42051329">next</a><span>|</span><label class="collapse" for="c-42051560">[-]</label><label class="expand" for="c-42051560">[2 more]</label></div><br/><div class="children"><div class="content">Like all things a computer can &#x2F; can&#x27;t do; DRM isn&#x27;t inherently bad: It&#x27;s how its used that&#x27;s a problem.<p>IE, DRM can&#x27;t change peoples&#x27; motivations. It&#x27;s useful for things like national security secrets and trade secrets, where the people who have access to the information have very clear motivations to protect that information, and very clear consequences for violating the rules that DRM is in place to protect.<p>In this case, the big question of if AI watermarking will work &#x2F; fail has more to do with peoples&#x27; motivations: Will the general public accept AI watermarking because it fits our motivations and the consequences we set up for AI masquerading as a real person, or AI being used for misinformation? That&#x27;s a big question that I can&#x27;t answer.</div><br/><div id="42051645" class="c"><input type="checkbox" id="c-42051645" checked=""/><div class="controls bullet"><span class="by">mateus1</span><span>|</span><a href="#42051274">root</a><span>|</span><a href="#42051560">parent</a><span>|</span><a href="#42051329">next</a><span>|</span><label class="collapse" for="c-42051645">[-]</label><label class="expand" for="c-42051645">[1 more]</label></div><br/><div class="children"><div class="content">This is not a “good deed for the public” done by Google, this is just a self serving tool to enforce their algorithms and digital property. There is nothing “bad” here for the public but it’s certainly not good either.</div><br/></div></div></div></div><div id="42051329" class="c"><input type="checkbox" id="c-42051329" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#42051274">parent</a><span>|</span><a href="#42051560">prev</a><span>|</span><a href="#42051337">next</a><span>|</span><label class="collapse" for="c-42051329">[-]</label><label class="expand" for="c-42051329">[3 more]</label></div><br/><div class="children"><div class="content">I for one am glad we might have a path forward to filtering out LLM-generated sludge.</div><br/><div id="42051447" class="c"><input type="checkbox" id="c-42051447" checked=""/><div class="controls bullet"><span class="by">pyrale</span><span>|</span><a href="#42051274">root</a><span>|</span><a href="#42051329">parent</a><span>|</span><a href="#42051337">next</a><span>|</span><label class="collapse" for="c-42051447">[-]</label><label class="expand" for="c-42051447">[2 more]</label></div><br/><div class="children"><div class="content">&gt; we<p>If by &quot;we&quot; you mean anyone else than Google and the select few other LLM provider they choose to associate with, I&#x27;m afraid you&#x27;re going to be disappointed.</div><br/><div id="42056958" class="c"><input type="checkbox" id="c-42056958" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#42051274">root</a><span>|</span><a href="#42051447">parent</a><span>|</span><a href="#42051337">next</a><span>|</span><label class="collapse" for="c-42056958">[-]</label><label class="expand" for="c-42056958">[1 more]</label></div><br/><div class="children"><div class="content">If there is a detectable fingerprint, we can detect it too. Probably don&#x27;t even need a Bletchley Park.</div><br/></div></div></div></div></div></div></div></div><div id="42051337" class="c"><input type="checkbox" id="c-42051337" checked=""/><div class="controls bullet"><span class="by">playingalong</span><span>|</span><a href="#42051274">prev</a><span>|</span><a href="#42051566">next</a><span>|</span><label class="collapse" for="c-42051337">[-]</label><label class="expand" for="c-42051337">[7 more]</label></div><br/><div class="children"><div class="content">&gt; the team tested it on 20 million prompts given to Gemini. Half of those prompts were routed to the SynthID-Text system and got a watermarked response, while the other half got the standard Gemini response. Judging by the “thumbs up” and “thumbs down” feedback from users, the watermarked responses were just as satisfactory to users as the standard ones.<p>Three comments here:<p>1. I wonder how many of the 20M prompts got a thumbs up or down. I don&#x27;t think people click that a lot. Unless the UI enforces it. I haven&#x27;t used Gemini, so I might be unaware.<p>2. Judging a single response might be not enough to tell if watermarking is acceptable or not. For instance,  imagine the watermarking is adding &quot;However,&quot; to the start of each paragraph. In a single GPT interaction you might not notice it. Once you get 3 or 4 responses it might stand out.<p>3. Since when Google is happy with measuring by self declared satisfaction? Aren&#x27;t they the kings of A&#x2F;B testing and high volume analysis of usage behavior?</div><br/><div id="42051363" class="c"><input type="checkbox" id="c-42051363" checked=""/><div class="controls bullet"><span class="by">varispeed</span><span>|</span><a href="#42051337">parent</a><span>|</span><a href="#42051566">next</a><span>|</span><label class="collapse" for="c-42051363">[-]</label><label class="expand" for="c-42051363">[6 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think people click that a lot.<p>I sometimes do, but I almost always give wrong answer or opposite answer where possible.</div><br/><div id="42056009" class="c"><input type="checkbox" id="c-42056009" checked=""/><div class="controls bullet"><span class="by">85392_school</span><span>|</span><a href="#42051337">root</a><span>|</span><a href="#42051363">parent</a><span>|</span><a href="#42051371">next</a><span>|</span><label class="collapse" for="c-42056009">[-]</label><label class="expand" for="c-42056009">[1 more]</label></div><br/><div class="children"><div class="content">I suspect your account&#x27;s feedback would be easily filtered out</div><br/></div></div><div id="42051371" class="c"><input type="checkbox" id="c-42051371" checked=""/><div class="controls bullet"><span class="by">froh</span><span>|</span><a href="#42051337">root</a><span>|</span><a href="#42051363">parent</a><span>|</span><a href="#42056009">prev</a><span>|</span><a href="#42051566">next</a><span>|</span><label class="collapse" for="c-42051371">[-]</label><label class="expand" for="c-42051371">[4 more]</label></div><br/><div class="children"><div class="content">but why?  what for?</div><br/><div id="42051416" class="c"><input type="checkbox" id="c-42051416" checked=""/><div class="controls bullet"><span class="by">thebruce87m</span><span>|</span><a href="#42051337">root</a><span>|</span><a href="#42051371">parent</a><span>|</span><a href="#42051442">next</a><span>|</span><label class="collapse" for="c-42051416">[-]</label><label class="expand" for="c-42051416">[1 more]</label></div><br/><div class="children"><div class="content">My timesheet SAAS constantly asks for feedback, which I give 0&#x2F;10 as constantly asking for feedback really annoys me.<p>They then contact me and ask me why, so I tell them then they say there is nothing they can do. A week later I’ll get a pop up asking for feedback and we go round the same loop again.</div><br/></div></div><div id="42051442" class="c"><input type="checkbox" id="c-42051442" checked=""/><div class="controls bullet"><span class="by">varispeed</span><span>|</span><a href="#42051337">root</a><span>|</span><a href="#42051371">parent</a><span>|</span><a href="#42051416">prev</a><span>|</span><a href="#42051566">next</a><span>|</span><label class="collapse" for="c-42051442">[-]</label><label class="expand" for="c-42051442">[2 more]</label></div><br/><div class="children"><div class="content">Because companies like Google are a cancer and I don&#x27;t want to give them data they didn&#x27;t pay for.</div><br/><div id="42054060" class="c"><input type="checkbox" id="c-42054060" checked=""/><div class="controls bullet"><span class="by">froh</span><span>|</span><a href="#42051337">root</a><span>|</span><a href="#42051442">parent</a><span>|</span><a href="#42051566">next</a><span>|</span><label class="collapse" for="c-42054060">[-]</label><label class="expand" for="c-42054060">[1 more]</label></div><br/><div class="children"><div class="content">hm<p>reminds me of &quot;what have the romans ever done for us?&quot;<p>but thx for elaborating.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42051566" class="c"><input type="checkbox" id="c-42051566" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#42051337">prev</a><span>|</span><a href="#42051252">next</a><span>|</span><label class="collapse" for="c-42051566">[-]</label><label class="expand" for="c-42051566">[3 more]</label></div><br/><div class="children"><div class="content">The academic paper: <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-024-08025-4" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-024-08025-4</a><p>They use the last N prefix tokens, hash them (with a keyed hash), and use the random value to sample the next token by doing an 8-wise tournament, by assigning random bits to each of the top 8 preferred tokens, making pairwise comparisons, and keeping the token with a larger bit. (Yes, it seems complicated, but apparently it increases the watermarking accuracy compared to a straightforward nucleus9 sampling.)<p>The negative of this approach is that you need to rerun the LLM, so you must keep all versions of all LLMs that you trained, forever.</div><br/><div id="42056417" class="c"><input type="checkbox" id="c-42056417" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42051566">parent</a><span>|</span><a href="#42056424">next</a><span>|</span><label class="collapse" for="c-42056417">[-]</label><label class="expand" for="c-42056417">[1 more]</label></div><br/><div class="children"><div class="content">They actually run 2^30-way tournament (they derive an equivalent form that doesn&#x27;t requires 2B operations). You do not need to run the LLM, it only depends on the tokenizer.</div><br/></div></div><div id="42056424" class="c"><input type="checkbox" id="c-42056424" checked=""/><div class="controls bullet"><span class="by">jkhdigital</span><span>|</span><a href="#42051566">parent</a><span>|</span><a href="#42056417">prev</a><span>|</span><a href="#42051252">next</a><span>|</span><label class="collapse" for="c-42056424">[-]</label><label class="expand" for="c-42056424">[1 more]</label></div><br/><div class="children"><div class="content">Why do you need to rerun the LLM? Watermark detection only requires the hash functions (equation (1) from the paper).</div><br/></div></div></div></div><div id="42051252" class="c"><input type="checkbox" id="c-42051252" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42051566">prev</a><span>|</span><a href="#42056915">next</a><span>|</span><label class="collapse" for="c-42051252">[-]</label><label class="expand" for="c-42051252">[16 more]</label></div><br/><div class="children"><div class="content">Correct me if I’m wrong, but wouldn’t it simply drive people to use LLMs that are not watermarking their content?</div><br/><div id="42051277" class="c"><input type="checkbox" id="c-42051277" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051291">next</a><span>|</span><label class="collapse" for="c-42051277">[-]</label><label class="expand" for="c-42051277">[1 more]</label></div><br/><div class="children"><div class="content">I think your idea is basically right, but there are two points to consider:<p>- Your hypothesis only holds if the alternative LLM is also &quot;sufficiently good&quot;. If Gemini does not stay competitive with other LLMs, Google&#x27;s AI plans have a <i>much more serious</i> problem.<p>- Your hypothesis assumes that many people will be capable of detecting the watermarks (both of Gemini and other LLMs) so that they can make a conscious choice for another LLM. But the idea behind good watermarking is that it is not that easy to detect.</div><br/></div></div><div id="42051291" class="c"><input type="checkbox" id="c-42051291" checked=""/><div class="controls bullet"><span class="by">kranner</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051277">prev</a><span>|</span><a href="#42051340">next</a><span>|</span><label class="collapse" for="c-42051291">[-]</label><label class="expand" for="c-42051291">[2 more]</label></div><br/><div class="children"><div class="content">According to the article, you can just have another LLM summarise Gemini&#x27;s watermarked output and that will &quot;likely&quot; defeat the watermark detection.</div><br/><div id="42051433" class="c"><input type="checkbox" id="c-42051433" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#42051252">root</a><span>|</span><a href="#42051291">parent</a><span>|</span><a href="#42051340">next</a><span>|</span><label class="collapse" for="c-42051433">[-]</label><label class="expand" for="c-42051433">[1 more]</label></div><br/><div class="children"><div class="content">But, if all the good models can only be trained by large mega corps with close connections to the government, it&#x27;s only a matter of time until that other LLM will just add its own watermark.</div><br/></div></div></div></div><div id="42051340" class="c"><input type="checkbox" id="c-42051340" checked=""/><div class="controls bullet"><span class="by">onion2k</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051291">prev</a><span>|</span><a href="#42051259">next</a><span>|</span><label class="collapse" for="c-42051340">[-]</label><label class="expand" for="c-42051340">[1 more]</label></div><br/><div class="children"><div class="content">People use Google Search despite it being littered with adverts and tracking. Maybe Google are counting on either being better than the competition despite watermarking, or simply accepting that people who don&#x27;t care are enough of a market that it&#x27;s still worth adding.</div><br/></div></div><div id="42051259" class="c"><input type="checkbox" id="c-42051259" checked=""/><div class="controls bullet"><span class="by">ndr</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051340">prev</a><span>|</span><a href="#42051505">next</a><span>|</span><label class="collapse" for="c-42051259">[-]</label><label class="expand" for="c-42051259">[1 more]</label></div><br/><div class="children"><div class="content">Agreed, that&#x27;s the obvious prediction.  They&#x27;re also going to perform worse on 3p benchmarks right?</div><br/></div></div><div id="42051505" class="c"><input type="checkbox" id="c-42051505" checked=""/><div class="controls bullet"><span class="by">dartharva</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051259">prev</a><span>|</span><a href="#42051358">next</a><span>|</span><label class="collapse" for="c-42051505">[-]</label><label class="expand" for="c-42051505">[1 more]</label></div><br/><div class="children"><div class="content">If Google locks in enterprise clients using Google Workspace to Gemini then they won&#x27;t really have a choice. It is selling it as an &quot;add-on&quot; already: <a href="https:&#x2F;&#x2F;workspace.google.com&#x2F;solutions&#x2F;ai&#x2F;#plan" rel="nofollow">https:&#x2F;&#x2F;workspace.google.com&#x2F;solutions&#x2F;ai&#x2F;#plan</a><p>Suffice to say it is evident that no other LLM will come close in integration with Google Docs and other Workspace apps as Gemini.</div><br/></div></div><div id="42051358" class="c"><input type="checkbox" id="c-42051358" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051505">prev</a><span>|</span><a href="#42051346">next</a><span>|</span><label class="collapse" for="c-42051358">[-]</label><label class="expand" for="c-42051358">[1 more]</label></div><br/><div class="children"><div class="content">Correct me if I&#x27;m wrong, but watermarking is only possible, if the model has a limited set of input you can provide (affects for the output) and a limited set of output it produces, and it should be completely deterministic. And you should pre-calculate all possible combinations.<p>And this should be also the case for every possible LLMs; then you can compare which LLMs could produce which outputs based on what inputs. Then there is some certainty that this output is produced by this LLM and this another LLM might produce it as well with these inputs.<p>So... impossible?</div><br/></div></div><div id="42051346" class="c"><input type="checkbox" id="c-42051346" checked=""/><div class="controls bullet"><span class="by">beepbooptheory</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051358">prev</a><span>|</span><a href="#42051676">next</a><span>|</span><label class="collapse" for="c-42051346">[-]</label><label class="expand" for="c-42051346">[6 more]</label></div><br/><div class="children"><div class="content">Why does the user care if its watermarked? Surely there are only some use cases for this stuff where it matters. Most of the time isn&#x27;t it just people having ephemeral chats where this wouldn&#x27;t matter?</div><br/><div id="42051357" class="c"><input type="checkbox" id="c-42051357" checked=""/><div class="controls bullet"><span class="by">ajdlinux</span><span>|</span><a href="#42051252">root</a><span>|</span><a href="#42051346">parent</a><span>|</span><a href="#42051486">next</a><span>|</span><label class="collapse" for="c-42051357">[-]</label><label class="expand" for="c-42051357">[3 more]</label></div><br/><div class="children"><div class="content">Using LLMs to write your essays and reports for school or uni, in a way that could get you punished if caught, is a reasonably big use case.</div><br/><div id="42052198" class="c"><input type="checkbox" id="c-42052198" checked=""/><div class="controls bullet"><span class="by">beepbooptheory</span><span>|</span><a href="#42051252">root</a><span>|</span><a href="#42051357">parent</a><span>|</span><a href="#42051472">next</a><span>|</span><label class="collapse" for="c-42052198">[-]</label><label class="expand" for="c-42052198">[1 more]</label></div><br/><div class="children"><div class="content">Agreed its probably a big use case in general, but like token per token I bet its relatively small! How many big papers do you have to write a semester? Even if its four, that&#x27;s nothing compared to the everyday use you will make of it.</div><br/></div></div><div id="42051472" class="c"><input type="checkbox" id="c-42051472" checked=""/><div class="controls bullet"><span class="by">highcountess</span><span>|</span><a href="#42051252">root</a><span>|</span><a href="#42051357">parent</a><span>|</span><a href="#42052198">prev</a><span>|</span><a href="#42051486">next</a><span>|</span><label class="collapse" for="c-42051472">[-]</label><label class="expand" for="c-42051472">[1 more]</label></div><br/><div class="children"><div class="content">I see no scenario where there won’t be an LLM that is deliberately tailored for that purpose, possibly even built by an “intel” agency for the very purpose of having blackmail over someone that may become useful later in their career.</div><br/></div></div></div></div><div id="42051486" class="c"><input type="checkbox" id="c-42051486" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42051252">root</a><span>|</span><a href="#42051346">parent</a><span>|</span><a href="#42051357">prev</a><span>|</span><a href="#42051676">next</a><span>|</span><label class="collapse" for="c-42051486">[-]</label><label class="expand" for="c-42051486">[2 more]</label></div><br/><div class="children"><div class="content">AIs and LLMs have an extremely uphill PR battle to fight right now. Anything that is deemed AI generated is assumed to be borderline trash (lots of exceptions, but you get the point). So, I can see that if someone uses LLM to generate text, they don’t want it to be marked as “low effort content”.</div><br/><div id="42052127" class="c"><input type="checkbox" id="c-42052127" checked=""/><div class="controls bullet"><span class="by">beepbooptheory</span><span>|</span><a href="#42051252">root</a><span>|</span><a href="#42051486">parent</a><span>|</span><a href="#42051676">next</a><span>|</span><label class="collapse" for="c-42052127">[-]</label><label class="expand" for="c-42052127">[1 more]</label></div><br/><div class="children"><div class="content">There are definitely exceptions, and that there are maybe proves that it is less Anti-AI prejudice at play and more just reacting to things that are indeed trashy. It just so happens a lot of it today is from AI I think (for, I hope, obvious reasons).<p>Just to say, maybe give it a little time, but a watermark like this is not going be thing that decides someone&#x27;s reaction in the near future, just what it says. (I am just betting here).<p>But its going to be an uphill battle either way if you are really getting the model to write everything, I do not envy that kind of project.</div><br/></div></div></div></div></div></div><div id="42051676" class="c"><input type="checkbox" id="c-42051676" checked=""/><div class="controls bullet"><span class="by">glenstein</span><span>|</span><a href="#42051252">parent</a><span>|</span><a href="#42051346">prev</a><span>|</span><a href="#42056915">next</a><span>|</span><label class="collapse" for="c-42051676">[-]</label><label class="expand" for="c-42051676">[2 more]</label></div><br/><div class="children"><div class="content">People made this same argument about DRM escalations, about increasing privacy violations in the browser, and about Google&#x27;s donations to support climate change misinformation. Even about Facebook interface redesigns. Every variation of &quot;people will be driven to do X&quot; I&#x27;ve ever heard assumes some coherence and unity of collective purpose that rarely matches the reality of how people behave.<p>There are counter examples, e.g. Unity. But catching that lightning in a bottle is rare and merits special explanation rather than being assumed.</div><br/><div id="42053898" class="c"><input type="checkbox" id="c-42053898" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42051252">root</a><span>|</span><a href="#42051676">parent</a><span>|</span><a href="#42056915">next</a><span>|</span><label class="collapse" for="c-42053898">[-]</label><label class="expand" for="c-42053898">[1 more]</label></div><br/><div class="children"><div class="content">Using LLMs in exams and homeworks has a different driver. Getting caught results in punishment, so using alternative would be better. None of the aforementioned examples have a “stick” aspect to it when you stick to Google.</div><br/></div></div></div></div></div></div><div id="42056915" class="c"><input type="checkbox" id="c-42056915" checked=""/><div class="controls bullet"><span class="by">harimau777</span><span>|</span><a href="#42051252">prev</a><span>|</span><a href="#42051332">next</a><span>|</span><label class="collapse" for="c-42056915">[-]</label><label class="expand" for="c-42056915">[1 more]</label></div><br/><div class="children"><div class="content">This strikes me as potentially a bad thing for regular people. For example, corporations call still use AI filtering to force job seekers to jump through hoops but job seekers won&#x27;t be able to use AI to generate the cover letters and resumes that those hoops demand.</div><br/></div></div><div id="42051332" class="c"><input type="checkbox" id="c-42051332" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#42056915">prev</a><span>|</span><a href="#42056174">next</a><span>|</span><label class="collapse" for="c-42051332">[-]</label><label class="expand" for="c-42051332">[3 more]</label></div><br/><div class="children"><div class="content">I think people are already doing that. I frequently hear people watermarking their speeches with phrases like &quot;are we aligned on this?&quot;, or &quot;let&#x27;s circle back&quot; and similar.</div><br/><div id="42051436" class="c"><input type="checkbox" id="c-42051436" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#42051332">parent</a><span>|</span><a href="#42056174">next</a><span>|</span><label class="collapse" for="c-42051436">[-]</label><label class="expand" for="c-42051436">[2 more]</label></div><br/><div class="children"><div class="content">I can’t tell if this is satire but that’s just corp-speak. I imagine those people also occasionally suggest “touching base” and “taking this offline”.<p>The phrases usually mean something useful, if one knows the meaning, but it is amusing how much people seem to stick with the same ones, even across companies.</div><br/><div id="42051567" class="c"><input type="checkbox" id="c-42051567" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#42051332">root</a><span>|</span><a href="#42051436">parent</a><span>|</span><a href="#42056174">next</a><span>|</span><label class="collapse" for="c-42051567">[-]</label><label class="expand" for="c-42051567">[1 more]</label></div><br/><div class="children"><div class="content">I am not sure whether it was satire. I personally don&#x27;t like corp speak - it feels like people talking like that are not humans. I am not sure I would welcome our AI overlords speaking like this, either.<p>But I find the idea that people will subconsciously start copying AI speech patterns (perhaps as a signal of submission) amusing. I think it&#x27;s gonna throw a wrench into the idea.<p>IMHO LLMs either should help us communicate more clearly and succinctly, or we can use them as tools for creativity (&quot;rephrase this in 18th century English&quot;). Watermarking speech sabotages both of these use cases.</div><br/></div></div></div></div></div></div><div id="42056174" class="c"><input type="checkbox" id="c-42056174" checked=""/><div class="controls bullet"><span class="by">ajwin</span><span>|</span><a href="#42051332">prev</a><span>|</span><a href="#42051314">next</a><span>|</span><label class="collapse" for="c-42056174">[-]</label><label class="expand" for="c-42056174">[4 more]</label></div><br/><div class="children"><div class="content">Do LLM&#x27;s always pick the most probable next word? I would have thought this would lead to having the same output for every input? How does this deal with the randomness that you get from prompting the same thing over and over?</div><br/><div id="42056344" class="c"><input type="checkbox" id="c-42056344" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42056174">parent</a><span>|</span><a href="#42056246">next</a><span>|</span><label class="collapse" for="c-42056344">[-]</label><label class="expand" for="c-42056344">[1 more]</label></div><br/><div class="children"><div class="content">It depends. If we use beam search we pick the most likely <i>sequence of tokens</i> rather than the most likely token at each point in time. This process is deterministic though.<p>We can also sample from the distribution, which introduces randomness. Basically, if word1 should be chosen 75% of the time and word2 25% of the time, it will do that.<p>The randomness you’re seeing can also be due to implementation details.<p><a href="https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;a-question-on-determinism&#x2F;8185&#x2F;9" rel="nofollow">https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;a-question-on-determinism&#x2F;818...</a></div><br/></div></div><div id="42056246" class="c"><input type="checkbox" id="c-42056246" checked=""/><div class="controls bullet"><span class="by">8note</span><span>|</span><a href="#42056174">parent</a><span>|</span><a href="#42056344">prev</a><span>|</span><a href="#42051314">next</a><span>|</span><label class="collapse" for="c-42056246">[-]</label><label class="expand" for="c-42056246">[2 more]</label></div><br/><div class="children"><div class="content">There is at least a parameter called Temperature which decides how much randomness to include in the output.<p>It doesn&#x27;t get you perfectly deterministic output to set it to 0 though, per <a href="https:&#x2F;&#x2F;medium.com&#x2F;google-cloud&#x2F;is-a-zero-temperature-deterministic-c4a7faef4d20" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;google-cloud&#x2F;is-a-zero-temperature-determ...</a> as you don&#x27;t have perfect control over what approximations are being made on your floating point operations</div><br/><div id="42056950" class="c"><input type="checkbox" id="c-42056950" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42056174">root</a><span>|</span><a href="#42056246">parent</a><span>|</span><a href="#42051314">next</a><span>|</span><label class="collapse" for="c-42056950">[-]</label><label class="expand" for="c-42056950">[1 more]</label></div><br/><div class="children"><div class="content">The most typical reason argmax (temp 0) is non-deterministic is that your request is running batched with other people requests. The number and size of these affects the matrix sizes and thus tiling decisions. Then you get different floating point order and thus different results.<p>Nvidia gives some guarantees about deterministic results of their kernels but that only applies when you have exact same input data and this is not the case when in-flight batching.</div><br/></div></div></div></div></div></div><div id="42051314" class="c"><input type="checkbox" id="c-42051314" checked=""/><div class="controls bullet"><span class="by">rany_</span><span>|</span><a href="#42056174">prev</a><span>|</span><a href="#42057253">next</a><span>|</span><label class="collapse" for="c-42051314">[-]</label><label class="expand" for="c-42051314">[2 more]</label></div><br/><div class="children"><div class="content">I really want to be able to try Gemini without the AI watermark. IIRC they&#x27;ve used SynthID from the start and it makes me wonder if it&#x27;s the source of all of Gemini&#x27;s issues.<p>Obviously Google claims that it doesn&#x27;t cause any issues but I&#x27;d think that OpenAI and other competitors would have something similar to SynthID if it didn&#x27;t impact performance.</div><br/><div id="42056205" class="c"><input type="checkbox" id="c-42056205" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42051314">parent</a><span>|</span><a href="#42057253">next</a><span>|</span><label class="collapse" for="c-42056205">[-]</label><label class="expand" for="c-42056205">[1 more]</label></div><br/><div class="children"><div class="content">&gt; IIRC they&#x27;ve used SynthID from the start<p>Is that not at odds with what&#x27;s presented in the article here?</div><br/></div></div></div></div><div id="42057253" class="c"><input type="checkbox" id="c-42057253" checked=""/><div class="controls bullet"><span class="by">matthewmorgan</span><span>|</span><a href="#42051314">prev</a><span>|</span><a href="#42057333">next</a><span>|</span><label class="collapse" for="c-42057253">[-]</label><label class="expand" for="c-42057253">[1 more]</label></div><br/><div class="children"><div class="content">Who is going to pay for watermarked output?</div><br/></div></div><div id="42057333" class="c"><input type="checkbox" id="c-42057333" checked=""/><div class="controls bullet"><span class="by">nprateem</span><span>|</span><a href="#42057253">prev</a><span>|</span><a href="#42051445">next</a><span>|</span><label class="collapse" for="c-42057333">[-]</label><label class="expand" for="c-42057333">[2 more]</label></div><br/><div class="children"><div class="content">Google are obviously pushing this as a way to root out AI blog spam.<p>If only they can get other providers to use it because of &#x27;safety&#x27; or something they won&#x27;t have to change their indexer much. Otherwise page rank is dead due to the ease of creating content farms.</div><br/><div id="42057395" class="c"><input type="checkbox" id="c-42057395" checked=""/><div class="controls bullet"><span class="by">riffraff</span><span>|</span><a href="#42057333">parent</a><span>|</span><a href="#42051445">next</a><span>|</span><label class="collapse" for="c-42057395">[-]</label><label class="expand" for="c-42057395">[1 more]</label></div><br/><div class="children"><div class="content">Not just them, openai is doing the same for the same reason: they need to avoid an Habsburg ai issue when the next half of their training material will be generated by themselves</div><br/></div></div></div></div><div id="42051445" class="c"><input type="checkbox" id="c-42051445" checked=""/><div class="controls bullet"><span class="by">tiffanyh</span><span>|</span><a href="#42057333">prev</a><span>|</span><a href="#42056173">next</a><span>|</span><label class="collapse" for="c-42051445">[-]</label><label class="expand" for="c-42051445">[1 more]</label></div><br/><div class="children"><div class="content">OT: The publication (Spectrum by IEEE) has some really good content.<p>It&#x27;s starting to become a common destination for when I want to read about interesting things.</div><br/></div></div><div id="42056173" class="c"><input type="checkbox" id="c-42056173" checked=""/><div class="controls bullet"><span class="by">cowmix</span><span>|</span><a href="#42051445">prev</a><span>|</span><a href="#42051276">next</a><span>|</span><label class="collapse" for="c-42056173">[-]</label><label class="expand" for="c-42056173">[1 more]</label></div><br/><div class="children"><div class="content">&quot;I hope this message finds you well.&quot; --- busted!</div><br/></div></div><div id="42051276" class="c"><input type="checkbox" id="c-42051276" checked=""/><div class="controls bullet"><span class="by">FilipSivak</span><span>|</span><a href="#42056173">prev</a><span>|</span><a href="#42051282">next</a><span>|</span><label class="collapse" for="c-42051276">[-]</label><label class="expand" for="c-42051276">[9 more]</label></div><br/><div class="children"><div class="content">How is this supposed to work? By inserting special unicode characters?<p>How can you watermark text?</div><br/><div id="42051335" class="c"><input type="checkbox" id="c-42051335" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#42051276">parent</a><span>|</span><a href="#42051294">next</a><span>|</span><label class="collapse" for="c-42051335">[-]</label><label class="expand" for="c-42051335">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t read how Google is doing it, but one way it could be done is to nudge which tokens get sampled. For example, every other token could have an odd numbered id (where each token is assigned an id from 0 to 32000 or however many it has). Then in order to detect the watermark you just tokenize the text and see if the pattern is there. A problem with this approach is that it harms the accuracy and coherency, for example if you ask &quot;What is 2+2&quot;, and the token &quot;4&quot; is token #102, and it has to pick an odd-numbered token, then it may respond with a wrong answer or yap on strangely due to its limited selection of tokens (like &quot;The accurate answer to your mathematical query is the number Four&quot;)</div><br/></div></div><div id="42051294" class="c"><input type="checkbox" id="c-42051294" checked=""/><div class="controls bullet"><span class="by">hiatus</span><span>|</span><a href="#42051276">parent</a><span>|</span><a href="#42051335">prev</a><span>|</span><a href="#42051303">next</a><span>|</span><label class="collapse" for="c-42051294">[-]</label><label class="expand" for="c-42051294">[2 more]</label></div><br/><div class="children"><div class="content">You can insert known spelling errors, choose certain phrasings, and more. It doesn&#x27;t have to be new characters added to the text. Government security services have done stuff like this for decades to weed out moles.</div><br/><div id="42051440" class="c"><input type="checkbox" id="c-42051440" checked=""/><div class="controls bullet"><span class="by">luigibosco</span><span>|</span><a href="#42051276">root</a><span>|</span><a href="#42051294">parent</a><span>|</span><a href="#42051303">next</a><span>|</span><label class="collapse" for="c-42051440">[-]</label><label class="expand" for="c-42051440">[1 more]</label></div><br/><div class="children"><div class="content">moles should know better than to utilize mountweazels!
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fictitious_entry" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fictitious_entry</a></div><br/></div></div></div></div><div id="42051303" class="c"><input type="checkbox" id="c-42051303" checked=""/><div class="controls bullet"><span class="by">zorked</span><span>|</span><a href="#42051276">parent</a><span>|</span><a href="#42051294">prev</a><span>|</span><a href="#42051374">next</a><span>|</span><label class="collapse" for="c-42051303">[-]</label><label class="expand" for="c-42051303">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve been studying unintentional watermarks for years.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stylometry" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stylometry</a></div><br/></div></div><div id="42051374" class="c"><input type="checkbox" id="c-42051374" checked=""/><div class="controls bullet"><span class="by">sumtechguy</span><span>|</span><a href="#42051276">parent</a><span>|</span><a href="#42051303">prev</a><span>|</span><a href="#42051327">next</a><span>|</span><label class="collapse" for="c-42051374">[-]</label><label class="expand" for="c-42051374">[1 more]</label></div><br/><div class="children"><div class="content">You do not even need extra characters (although they help).  You can use spaces, missing punctuation, upper&#x2F;lower case in particular cases, conjunction usage and not using it, word substitution, common misspellings, transposed letters, etc.  How many extra spaces&#x2F;tabs can you add to the end of a paragraph?  At the beginning?  Between sentences?  Inside them?  Then you have an AI agent design it and then train another one to detect it.</div><br/></div></div><div id="42051327" class="c"><input type="checkbox" id="c-42051327" checked=""/><div class="controls bullet"><span class="by">das_keyboard</span><span>|</span><a href="#42051276">parent</a><span>|</span><a href="#42051374">prev</a><span>|</span><a href="#42051403">next</a><span>|</span><label class="collapse" for="c-42051327">[-]</label><label class="expand" for="c-42051327">[1 more]</label></div><br/><div class="children"><div class="content">&gt; SynthID-Text works by discreetly interfering in the generation process: It alters some of the words that a chatbot outputs to the user in a way that’s invisible to humans but clear to a SynthID detector. “Such modifications introduce a statistical signature into the generated text,” [...] “During the watermark detection phase, the signature can be measured to determine whether the text was indeed generated by the watermarked LLM.”</div><br/></div></div><div id="42051403" class="c"><input type="checkbox" id="c-42051403" checked=""/><div class="controls bullet"><span class="by">voidUpdate</span><span>|</span><a href="#42051276">parent</a><span>|</span><a href="#42051327">prev</a><span>|</span><a href="#42051354">next</a><span>|</span><label class="collapse" for="c-42051403">[-]</label><label class="expand" for="c-42051403">[1 more]</label></div><br/><div class="children"><div class="content">As stated in the article, it alters the probabilities that the network produces in a predictable way so that a different (but still correct-sounding) word is picked. It subtly alters the wording from what it would have output normally in such a way that you can detect it, while still sounding correct to the user</div><br/></div></div><div id="42051354" class="c"><input type="checkbox" id="c-42051354" checked=""/><div class="controls bullet"><span class="by">sebstefan</span><span>|</span><a href="#42051276">parent</a><span>|</span><a href="#42051403">prev</a><span>|</span><a href="#42051282">next</a><span>|</span><label class="collapse" for="c-42051354">[-]</label><label class="expand" for="c-42051354">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an article from ieee that explains it:<p><a href="https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;watermark#:~:text=How%20Google%E2%80%99s%20Text%20Watermarks%20Work" rel="nofollow">https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;watermark#:~:text=How%20Google%E2%...</a></div><br/></div></div></div></div><div id="42051282" class="c"><input type="checkbox" id="c-42051282" checked=""/><div class="controls bullet"><span class="by">playingalong</span><span>|</span><a href="#42051276">prev</a><span>|</span><a href="#42051448">next</a><span>|</span><label class="collapse" for="c-42051282">[-]</label><label class="expand" for="c-42051282">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It has also open-sourced the tool and made it available to developers and businesses, allowing them to use the tool to determine whether text outputs have come from their own large language models (LLMs), the AI systems that power chatbots. However, only Google and those developers currently have access to the detector that checks for the watermark.<p>These two sentences next to each other don&#x27;t make much sense. Or are misleading.<p>Yeah. I know. Only the client is open source and it calls home.</div><br/><div id="42051344" class="c"><input type="checkbox" id="c-42051344" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#42051282">parent</a><span>|</span><a href="#42051448">next</a><span>|</span><label class="collapse" for="c-42051344">[-]</label><label class="expand" for="c-42051344">[1 more]</label></div><br/><div class="children"><div class="content">Is there significant throttling to prevent us from training a classification model against it?</div><br/></div></div></div></div><div id="42051448" class="c"><input type="checkbox" id="c-42051448" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#42051282">prev</a><span>|</span><a href="#42056708">next</a><span>|</span><label class="collapse" for="c-42051448">[-]</label><label class="expand" for="c-42051448">[3 more]</label></div><br/><div class="children"><div class="content">This is information-theoretically guaranteed to make LLM output worse.<p>My reasoning is simple: the only way to watermark text is to inject some relatively low-entropy signal into it, which can be detected later.  This has to a) work for &quot;all&quot; output for some values of all, and b) have a low false positive rate on the detection side.  The amount of signal involved cannot be subtle, for this reason.<p>That signal has a subtractive effect on the predictive-output signal.  The entropy of the output is fixed by the entropy of natural language, so this is a zero-sum game: the watermark signal will remove fidelity from the predictive output.<p>This is impossible to avoid or fix.</div><br/><div id="42056008" class="c"><input type="checkbox" id="c-42056008" checked=""/><div class="controls bullet"><span class="by">thornewolf</span><span>|</span><a href="#42051448">parent</a><span>|</span><a href="#42056471">next</a><span>|</span><label class="collapse" for="c-42056008">[-]</label><label class="expand" for="c-42056008">[1 more]</label></div><br/><div class="children"><div class="content">you are correct of we suppose we are at a global optimum. however, consider this example:<p>i have two hands<p>i have 2 hands<p>these sentences communicate the same thing but one could be a watermarked result. we can apply this equivalent meaning word&#x2F;phrase change many times over and be confident something is watermark while having avoided any semantic shifts.</div><br/></div></div><div id="42056471" class="c"><input type="checkbox" id="c-42056471" checked=""/><div class="controls bullet"><span class="by">jkhdigital</span><span>|</span><a href="#42051448">parent</a><span>|</span><a href="#42056008">prev</a><span>|</span><a href="#42056708">next</a><span>|</span><label class="collapse" for="c-42056471">[-]</label><label class="expand" for="c-42056471">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not wrong, but natural language has a lot of stylistic &quot;noise&quot; which can be utilized as a subliminal channel without noticeably degrading the semantic signal.</div><br/></div></div></div></div><div id="42056708" class="c"><input type="checkbox" id="c-42056708" checked=""/><div class="controls bullet"><span class="by">lowbloodsugar</span><span>|</span><a href="#42051448">prev</a><span>|</span><label class="collapse" for="c-42056708">[-]</label><label class="expand" for="c-42056708">[2 more]</label></div><br/><div class="children"><div class="content">I want AI to use just the right word when it’s writing for me. If it’s going to nerf itself to not choose the perfect word so it can be watermarked, then why would I use that product? I’ll go somewhere else. And if it does use just the right word, then how is that different from a great human writer?</div><br/><div id="42057265" class="c"><input type="checkbox" id="c-42057265" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#42056708">parent</a><span>|</span><label class="collapse" for="c-42057265">[-]</label><label class="expand" for="c-42057265">[1 more]</label></div><br/><div class="children"><div class="content">There is the &#x27;loser&#x27;s litigation&#x27; method of getting all of your non-watermarked competitors banned. Usually involving some combination of magical rights removing brain-hacks like national security or &#x27;the children&#x27;.</div><br/></div></div></div></div></div></div></div></div></div></body></html>