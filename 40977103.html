<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721206870077" as="style"/><link rel="stylesheet" href="styles.css?v=1721206870077"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://mistral.ai/news/codestral-mamba/">Codestral Mamba</a> <span class="domain">(<a href="https://mistral.ai">mistral.ai</a>)</span></div><div class="subtext"><span>tosh</span> | <span>125 comments</span></div><br/><div><div id="40983041" class="c"><input type="checkbox" id="c-40983041" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#40977885">next</a><span>|</span><label class="collapse" for="c-40983041">[-]</label><label class="expand" for="c-40983041">[7 more]</label></div><br/><div class="children"><div class="content">I kinda just want something that can keep up with the original version of Copilot. It was so much better than the crap they’re pumping out now (keeps messing up syntax and only completing a few characters at a time).</div><br/><div id="40983220" class="c"><input type="checkbox" id="c-40983220" checked=""/><div class="controls bullet"><span class="by">terhechte</span><span>|</span><a href="#40983041">parent</a><span>|</span><a href="#40983195">next</a><span>|</span><label class="collapse" for="c-40983220">[-]</label><label class="expand" for="c-40983220">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried supermaven? (<a href="https:&#x2F;&#x2F;supermaven.com" rel="nofollow">https:&#x2F;&#x2F;supermaven.com</a>). I find it much better than copilot. Using it daily.</div><br/></div></div><div id="40983195" class="c"><input type="checkbox" id="c-40983195" checked=""/><div class="controls bullet"><span class="by">heeton</span><span>|</span><a href="#40983041">parent</a><span>|</span><a href="#40983220">prev</a><span>|</span><a href="#40977885">next</a><span>|</span><label class="collapse" for="c-40983195">[-]</label><label class="expand" for="c-40983195">[5 more]</label></div><br/><div class="children"><div class="content">Have you tried supermaven? It replaced copilot for me a couple of months ago.</div><br/><div id="40983387" class="c"><input type="checkbox" id="c-40983387" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#40983041">root</a><span>|</span><a href="#40983195">parent</a><span>|</span><a href="#40977885">next</a><span>|</span><label class="collapse" for="c-40983387">[-]</label><label class="expand" for="c-40983387">[4 more]</label></div><br/><div class="children"><div class="content">I tried it, uses GPT-4o, the $10 sign up credit dissapeared in a few hours of intense coding, I&#x27;m not paying $500&#x2F;mo for a fancy autocomple. Manual instruct style chat about code with Claude-Sonnet-3.5 is the best price&#x2F;perf I&#x27;ve tried so far, through poe.com I use around 30k credits per day of coding of the 1M monthly allotment, I think it was $200&#x2F;y. It&#x27;s not available directly in my country. I&#x27;ve tried a bunch of local models too but Claude is just next level and inference is very cheap.</div><br/><div id="40983458" class="c"><input type="checkbox" id="c-40983458" checked=""/><div class="controls bullet"><span class="by">PufPufPuf</span><span>|</span><a href="#40983041">root</a><span>|</span><a href="#40983387">parent</a><span>|</span><a href="#40977885">next</a><span>|</span><label class="collapse" for="c-40983458">[-]</label><label class="expand" for="c-40983458">[3 more]</label></div><br/><div class="children"><div class="content">On the pricing page it says $10&#x2F;month, what you describe sounds like pay-as-you-go, are you sure it was this service you tried?</div><br/><div id="40983521" class="c"><input type="checkbox" id="c-40983521" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#40983041">root</a><span>|</span><a href="#40983458">parent</a><span>|</span><a href="#40983550">next</a><span>|</span><label class="collapse" for="c-40983521">[-]</label><label class="expand" for="c-40983521">[1 more]</label></div><br/><div class="children"><div class="content">Yes that the same, in the &quot;Your Account&quot; section of the service you have &quot;Chat Credits&quot; field, it was seeded with $10 initially and I got it down to $0.11 in one evening. You are right about the pricing page, it does say it&#x27;s $10&#x2F;month, but I believe it&#x27;s $10&#x2F;month up to some amount of queries in actuality, because they obviously can&#x27;t offer infinite inference for $10. Maybe I was &quot;holding it wrong&quot;, sending too much complete files as part of prompt and the token use and context window skyrocketed, but I absolutely used my initial $10 in one evening, unlike with Claude...</div><br/></div></div><div id="40983550" class="c"><input type="checkbox" id="c-40983550" checked=""/><div class="controls bullet"><span class="by">hobofan</span><span>|</span><a href="#40983041">root</a><span>|</span><a href="#40983458">parent</a><span>|</span><a href="#40983521">prev</a><span>|</span><a href="#40977885">next</a><span>|</span><label class="collapse" for="c-40983550">[-]</label><label class="expand" for="c-40983550">[1 more]</label></div><br/><div class="children"><div class="content">From what I can tell inside the product, the public pricing seems deceptive.<p>It appears that for the $10&#x2F;month you just get access to additional features (e.g. bigger context) + a budget of $5&#x2F;month of credits. The credits possibly translate 1:1 to the usage costs of the underlying models you chose.<p>I asked it to add some missing documentation with the Claude 3.5 Sonnet model to a medium-sized Python file (2k lines) as a first test and that used up 13 cents of the credits. If it were working in a really productive way (where I&#x27;d also include more files as context), I&#x27;d probably burn through $20-50&#x2F;day.<p>I&#x27;m wondering why they are not publicly showing some more transparent pricing. Yeah, this method will bump their signup numbers for investors, but it also absolutely wrecks their churn numbers, when people immediately cancel in the first hour.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40977885" class="c"><input type="checkbox" id="c-40977885" checked=""/><div class="controls bullet"><span class="by">bhouston</span><span>|</span><a href="#40983041">prev</a><span>|</span><a href="#40982897">next</a><span>|</span><label class="collapse" for="c-40977885">[-]</label><label class="expand" for="c-40977885">[28 more]</label></div><br/><div class="children"><div class="content">What are the steps required to get this running in VS Code?<p>If they had linked to the instructions in their post (or better yet a link to a one click install of a VS Code Extension), it would help a lot with adoption.<p>(BTW I consider it malpractice that they are at the top of hacker news with a model that is of great interest to a large portion of the users where and they do not have a monetizable call to action on the page featured.)</div><br/><div id="40978022" class="c"><input type="checkbox" id="c-40978022" checked=""/><div class="controls bullet"><span class="by">leourbina</span><span>|</span><a href="#40977885">parent</a><span>|</span><a href="#40983471">next</a><span>|</span><label class="collapse" for="c-40978022">[-]</label><label class="expand" for="c-40978022">[10 more]</label></div><br/><div class="children"><div class="content">If you can run this using ollama, then you should be able to use <a href="https:&#x2F;&#x2F;www.continue.dev&#x2F;">https:&#x2F;&#x2F;www.continue.dev&#x2F;</a> with both IntelliJ and VSCode. Haven’t tried this model yet - but overall this plugin works well.</div><br/><div id="40978224" class="c"><input type="checkbox" id="c-40978224" checked=""/><div class="controls bullet"><span class="by">scosman</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978022">parent</a><span>|</span><a href="#40979358">next</a><span>|</span><label class="collapse" for="c-40978224">[-]</label><label class="expand" for="c-40978224">[6 more]</label></div><br/><div class="children"><div class="content">They say no llama.cpp support yet, so no ollama yet (which uses llama.cpp)</div><br/><div id="40978371" class="c"><input type="checkbox" id="c-40978371" checked=""/><div class="controls bullet"><span class="by">HanClinto</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978224">parent</a><span>|</span><a href="#40978311">next</a><span>|</span><label class="collapse" for="c-40978371">[-]</label><label class="expand" for="c-40978371">[1 more]</label></div><br/><div class="children"><div class="content">Correct. The only back-end that Ollama uses is llama.cpp, and llama.cpp does not yet have Mamba2 support. The issues to track Mamba2 and Codestral Mamba support are here:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;8519">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;8519</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;7727">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;7727</a><p>Mamba support was added in March of this year:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;5328">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;5328</a><p>I have not yet seen a PR to address Mamba2.</div><br/></div></div><div id="40978273" class="c"><input type="checkbox" id="c-40978273" checked=""/><div class="controls bullet"><span class="by">sadeshmukh</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978224">parent</a><span>|</span><a href="#40978258">prev</a><span>|</span><a href="#40979358">next</a><span>|</span><label class="collapse" for="c-40978273">[-]</label><label class="expand" for="c-40978273">[2 more]</label></div><br/><div class="children"><div class="content">Ollama is supported: <a href="https:&#x2F;&#x2F;docs.continue.dev&#x2F;setup&#x2F;select-provider">https:&#x2F;&#x2F;docs.continue.dev&#x2F;setup&#x2F;select-provider</a></div><br/><div id="40978304" class="c"><input type="checkbox" id="c-40978304" checked=""/><div class="controls bullet"><span class="by">trsohmers</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978273">parent</a><span>|</span><a href="#40979358">next</a><span>|</span><label class="collapse" for="c-40978304">[-]</label><label class="expand" for="c-40978304">[1 more]</label></div><br/><div class="children"><div class="content">They meant that there is no support for Codestral Mamba for llama.cpp yet.</div><br/></div></div></div></div></div></div><div id="40979358" class="c"><input type="checkbox" id="c-40979358" checked=""/><div class="controls bullet"><span class="by">osmano807</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978022">parent</a><span>|</span><a href="#40978224">prev</a><span>|</span><a href="#40983471">next</a><span>|</span><label class="collapse" for="c-40979358">[-]</label><label class="expand" for="c-40979358">[3 more]</label></div><br/><div class="children"><div class="content">Unrelated, all my devices freeze when accessing this page, desktop Firefox and Chrome, mobile Firefox and Brave.
Is this the best alternative to access code ai helpers besides the GitHub Copilot and Google Gemini on VSCode?</div><br/><div id="40979538" class="c"><input type="checkbox" id="c-40979538" checked=""/><div class="controls bullet"><span class="by">raphaelj</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40979358">parent</a><span>|</span><a href="#40980170">next</a><span>|</span><label class="collapse" for="c-40979538">[-]</label><label class="expand" for="c-40979538">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using it for a few months (with Starcoder 2 for code, and GPT-4o for chat). I find the code completion actually better than Github Copilot.<p>My main complain is that the chat sometimes fails to correctly render some GPT-4o output (e.g. LaTeX expressions), but it&#x27;s mostly fixed with a custom system prompt. It also significantly reduces the battery life of my Macbook M1, but that&#x27;s expected.</div><br/></div></div><div id="40980170" class="c"><input type="checkbox" id="c-40980170" checked=""/><div class="controls bullet"><span class="by">oliverulerich</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40979358">parent</a><span>|</span><a href="#40979538">prev</a><span>|</span><a href="#40983471">next</a><span>|</span><label class="collapse" for="c-40980170">[-]</label><label class="expand" for="c-40980170">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m quite happy with Cody from Sourcegraph
<a href="https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=sourcegraph.cody-ai" rel="nofollow">https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=sourcegr...</a></div><br/></div></div></div></div></div></div><div id="40983471" class="c"><input type="checkbox" id="c-40983471" checked=""/><div class="controls bullet"><span class="by">PufPufPuf</span><span>|</span><a href="#40977885">parent</a><span>|</span><a href="#40978022">prev</a><span>|</span><a href="#40983067">next</a><span>|</span><label class="collapse" for="c-40983471">[-]</label><label class="expand" for="c-40983471">[1 more]</label></div><br/><div class="children"><div class="content">Currently the best (most user-friendly) way to run models locally is to use Ollama with Continue.dev. This one is not available yet, though: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;8519">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;8519</a></div><br/></div></div><div id="40983067" class="c"><input type="checkbox" id="c-40983067" checked=""/><div class="controls bullet"><span class="by">yogeshp</span><span>|</span><a href="#40977885">parent</a><span>|</span><a href="#40983471">prev</a><span>|</span><a href="#40978052">next</a><span>|</span><label class="collapse" for="c-40983067">[-]</label><label class="expand" for="c-40983067">[1 more]</label></div><br/><div class="children"><div class="content">Website codegpt.co also has a plugin for both VS Code and Intellij. When model becomes available in Ollama, you can connect plugin in VS code to local ollama instance.</div><br/></div></div><div id="40978052" class="c"><input type="checkbox" id="c-40978052" checked=""/><div class="controls bullet"><span class="by">sleepytimetea</span><span>|</span><a href="#40977885">parent</a><span>|</span><a href="#40983067">prev</a><span>|</span><a href="#40978164">next</a><span>|</span><label class="collapse" for="c-40978052">[-]</label><label class="expand" for="c-40978052">[3 more]</label></div><br/><div class="children"><div class="content">Looking through the Quickstart docs, they have an API that can generate code. However, I don&#x27;t think they have a way to do &quot;Day 2&quot; code editing.<p>Also, doesn&#x27;t seem to have a freemium tier...need to start paying even before trying it out ?<p>&quot;Our API is currently available through La Plateforme. You need to activate payments on your account to enable your API keys.&quot;</div><br/><div id="40978246" class="c"><input type="checkbox" id="c-40978246" checked=""/><div class="controls bullet"><span class="by">sv123</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978052">parent</a><span>|</span><a href="#40978164">next</a><span>|</span><label class="collapse" for="c-40978246">[-]</label><label class="expand" for="c-40978246">[2 more]</label></div><br/><div class="children"><div class="content">I signed up when codestral was first available and put my payment details in.  Been using it daily since then with continue.dev but my usage dashboard shows 0 tokens, and so far have not been billed for anything... Definitely not clear anywhere, but it seems to be free for now?  Or some sort of free limit that I am not hitting.</div><br/><div id="40978479" class="c"><input type="checkbox" id="c-40978479" checked=""/><div class="controls bullet"><span class="by">sunaookami</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978246">parent</a><span>|</span><a href="#40978164">next</a><span>|</span><label class="collapse" for="c-40978479">[-]</label><label class="expand" for="c-40978479">[1 more]</label></div><br/><div class="children"><div class="content">Through codestral.mistral.ai? It&#x27;s free until August 1st: <a href="https:&#x2F;&#x2F;docs.mistral.ai&#x2F;capabilities&#x2F;code_generation&#x2F;" rel="nofollow">https:&#x2F;&#x2F;docs.mistral.ai&#x2F;capabilities&#x2F;code_generation&#x2F;</a><p>&gt;Monthly subscription based, free until 1st of August</div><br/></div></div></div></div></div></div><div id="40978164" class="c"><input type="checkbox" id="c-40978164" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40977885">parent</a><span>|</span><a href="#40978052">prev</a><span>|</span><a href="#40982897">next</a><span>|</span><label class="collapse" for="c-40978164">[-]</label><label class="expand" for="c-40978164">[12 more]</label></div><br/><div class="children"><div class="content">&quot;All you need is users&quot; doesn&#x27;t seem optimal IMHO, Stability.ai providing an object lesson in that.<p>They just released weights, and being a for profit, need to optimize for making money, not eyeballs. It seems wise to guide people to the API offering.</div><br/><div id="40978374" class="c"><input type="checkbox" id="c-40978374" checked=""/><div class="controls bullet"><span class="by">bhouston</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978164">parent</a><span>|</span><a href="#40978435">next</a><span>|</span><label class="collapse" for="c-40978374">[-]</label><label class="expand" for="c-40978374">[7 more]</label></div><br/><div class="children"><div class="content">On top of Hacker News (the target demographic for coders) without an effective monetizable call to action?  What a missed opportunity.<p>Github Copilot makes +100M&#x2F;year, if not way way more.<p>Having a VS Code extension for Mistral would be a revenue stream if it was one-click and better or cheaper than Github Copilot.  It is malpractice in my mind to not be doing this if you are investing in creating coding models.</div><br/><div id="40981582" class="c"><input type="checkbox" id="c-40981582" checked=""/><div class="controls bullet"><span class="by">treyd</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978374">parent</a><span>|</span><a href="#40978556">next</a><span>|</span><label class="collapse" for="c-40981582">[-]</label><label class="expand" for="c-40981582">[5 more]</label></div><br/><div class="children"><div class="content">How the hell does Copilot make $100M&#x2F;yr?  That seems an order of magnitude higher than I would expect at the high end.</div><br/><div id="40982257" class="c"><input type="checkbox" id="c-40982257" checked=""/><div class="controls bullet"><span class="by">kcb</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40981582">parent</a><span>|</span><a href="#40983627">next</a><span>|</span><label class="collapse" for="c-40982257">[-]</label><label class="expand" for="c-40982257">[1 more]</label></div><br/><div class="children"><div class="content">I was thinking the opposite...remember there&#x27;s enterprise subscriptions and multi-million dollar contracts with single companies.</div><br/></div></div><div id="40983627" class="c"><input type="checkbox" id="c-40983627" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40981582">parent</a><span>|</span><a href="#40982257">prev</a><span>|</span><a href="#40981744">next</a><span>|</span><label class="collapse" for="c-40983627">[-]</label><label class="expand" for="c-40983627">[1 more]</label></div><br/><div class="children"><div class="content">yeah exactly only $100M&#x2F;yr? barely covers expenses</div><br/></div></div><div id="40981744" class="c"><input type="checkbox" id="c-40981744" checked=""/><div class="controls bullet"><span class="by">ketzo</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40981582">parent</a><span>|</span><a href="#40983627">prev</a><span>|</span><a href="#40982817">next</a><span>|</span><label class="collapse" for="c-40981744">[-]</label><label class="expand" for="c-40981744">[1 more]</label></div><br/><div class="children"><div class="content">if we’re talking individual subscriptions that’s ~1M paying subscribers. honestly that number would not totally shock me?<p>plus they’ve got some kinda enterprise&#x2F;team offering; assuming they charge extra there, I could easily see $100M ARR<p>but that’s pure conjecture, and generous at that; I don’t think we have any hard numbers</div><br/></div></div><div id="40982817" class="c"><input type="checkbox" id="c-40982817" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40981582">parent</a><span>|</span><a href="#40981744">prev</a><span>|</span><a href="#40978556">next</a><span>|</span><label class="collapse" for="c-40982817">[-]</label><label class="expand" for="c-40982817">[1 more]</label></div><br/><div class="children"><div class="content">???</div><br/></div></div></div></div><div id="40978556" class="c"><input type="checkbox" id="c-40978556" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978374">parent</a><span>|</span><a href="#40981582">prev</a><span>|</span><a href="#40978435">next</a><span>|</span><label class="collapse" for="c-40978556">[-]</label><label class="expand" for="c-40978556">[1 more]</label></div><br/><div class="children"><div class="content">I see, that makes sense: make an extension and charge for it.<p>I assumed they meant free x local. It doesn&#x27;t seem rational to make this one paid: its significantly smaller than their better model, and even more so than Copilot&#x27;s.</div><br/></div></div></div></div><div id="40978435" class="c"><input type="checkbox" id="c-40978435" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978164">parent</a><span>|</span><a href="#40978374">prev</a><span>|</span><a href="#40982897">next</a><span>|</span><label class="collapse" for="c-40978435">[-]</label><label class="expand" for="c-40978435">[4 more]</label></div><br/><div class="children"><div class="content">But they also signal competence in the space which means M&amp;A. Or big nation states in future would hire them to produce country models once the space matures as was Emad&#x27;s vision.</div><br/><div id="40981407" class="c"><input type="checkbox" id="c-40981407" checked=""/><div class="controls bullet"><span class="by">NotMichaelBay</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978435">parent</a><span>|</span><a href="#40978549">next</a><span>|</span><label class="collapse" for="c-40981407">[-]</label><label class="expand" for="c-40981407">[2 more]</label></div><br/><div class="children"><div class="content">What does a &quot;country model&quot; mean? Optimized for that country&#x27;s specific language, or with state propaganda or something else?</div><br/><div id="40981916" class="c"><input type="checkbox" id="c-40981916" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40981407">parent</a><span>|</span><a href="#40978549">next</a><span>|</span><label class="collapse" for="c-40981916">[-]</label><label class="expand" for="c-40981916">[1 more]</label></div><br/><div class="children"><div class="content">More or less; it was about as serious as your median Elon product tweet the last decade, or median coin nonsense.<p>Half-baked idea that obviously the models would need to be tuned for different languages &#x2F; for specific knowledge, therefore countries would pay to do that.<p>There were many ideas like that, none of them panned out, hence the defenestration. All love for the guy, he did a very, very good thing. It&#x27;s just meaningless to invoke it here, not only because it&#x27;s completely off-topic, if anything that&#x27;s already the play as the EU champion, and because the Stability gentleman was just thinking out loud, nothing more.</div><br/></div></div></div></div><div id="40978549" class="c"><input type="checkbox" id="c-40978549" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40977885">root</a><span>|</span><a href="#40978435">parent</a><span>|</span><a href="#40981407">prev</a><span>|</span><a href="#40982897">next</a><span>|</span><label class="collapse" for="c-40978549">[-]</label><label class="expand" for="c-40978549">[1 more]</label></div><br/><div class="children"><div class="content">Did Emad&#x27;s vision end up manifest? ex. did a nation-state end up paying Stability for a country model?<p>Would it help signal competency? They&#x27;re a small team focused on making models, not VS Code extensions.<p>Would they do M&amp;A? The founding team is ex-Googlers and has found significant attention in the MBA world via being an EU champion.</div><br/></div></div></div></div></div></div></div></div><div id="40982897" class="c"><input type="checkbox" id="c-40982897" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40977885">prev</a><span>|</span><a href="#40979105">next</a><span>|</span><label class="collapse" for="c-40982897">[-]</label><label class="expand" for="c-40982897">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Unlike Transformer models, Mamba models offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length<p>&gt; We have tested Codestral Mamba on in-context retrieval capabilities up to 256k tokens<p>Why only 256k tokens? Gemini&#x27;s context window is 1 million or more and it&#x27;s (probably) not even using Mamba.</div><br/><div id="40983409" class="c"><input type="checkbox" id="c-40983409" checked=""/><div class="controls bullet"><span class="by">rileyphone</span><span>|</span><a href="#40982897">parent</a><span>|</span><a href="#40979105">next</a><span>|</span><label class="collapse" for="c-40983409">[-]</label><label class="expand" for="c-40983409">[1 more]</label></div><br/><div class="children"><div class="content">Gemini is probably using ring attention. But scaling to that size requires more engineering effort in terms of interlink that goes beyond the purpose of this release from Mistral.</div><br/></div></div></div></div><div id="40979105" class="c"><input type="checkbox" id="c-40979105" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#40982897">prev</a><span>|</span><a href="#40978108">next</a><span>|</span><label class="collapse" for="c-40979105">[-]</label><label class="expand" for="c-40979105">[4 more]</label></div><br/><div class="children"><div class="content">Does anyone have a favorite FIM capable model? I&#x27;ve been using codellama-13b through ollama w&#x2F; a vim extension i wrote and it&#x27;s okay but not amazing, I definitely get better code most of the time out of Gemma-27b but no FIM (and for some reason codellama-34b has broken inference for me)</div><br/><div id="40983047" class="c"><input type="checkbox" id="c-40983047" checked=""/><div class="controls bullet"><span class="by">xoranth</span><span>|</span><a href="#40979105">parent</a><span>|</span><a href="#40978108">next</a><span>|</span><label class="collapse" for="c-40983047">[-]</label><label class="expand" for="c-40983047">[3 more]</label></div><br/><div class="children"><div class="content">Is the extension you wrote public?</div><br/><div id="40983108" class="c"><input type="checkbox" id="c-40983108" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#40979105">root</a><span>|</span><a href="#40983047">parent</a><span>|</span><a href="#40978108">next</a><span>|</span><label class="collapse" for="c-40983108">[-]</label><label class="expand" for="c-40983108">[2 more]</label></div><br/><div class="children"><div class="content">no but it&#x27;s super super janky and simple hodgepode of stack overflow and gemma:27b generated code, i&#x27;ll just put it in the comment here, you just need CURL on your path and vim that&#x27;s compiled with some specific flag<p><pre><code>    function! GetSurroundingLines(n)
        let l:current_line = line(&#x27;.&#x27;)
        let l:start_line = max([1, l:current_line - a:n])
        let l:end_line = min([line(&#x27;$&#x27;), l:current_line + a:n])
        
        let l:lines_before = getline(l:start_line, l:current_line - 1)
        let l:lines_after = getline(l:current_line + 1, l:end_line)
        
        return [l:lines_before, l:lines_after]
    endfunction
    
    function! AIComplete()
        let l:n = 256
        let [l:lines_before, l:lines_after] = GetSurroundingLines(l:n)
        
        let l:prompt = &#x27;&lt;PRE&gt;&#x27; . join(l:lines_before, &quot;\n&quot;) . &#x27; &lt;SUF&gt;&#x27; . join(l:lines_after, &quot;\n&quot;) . &#x27; &lt;MID&gt;&#x27;
        
        let l:json_data = json_encode({
            \ &#x27;model&#x27;: &#x27;codellama:13b-code-q6_K&#x27;,
            \ &#x27;keep_alive&#x27;: &#x27;30m&#x27;,
            \ &#x27;stream&#x27;: v:false,
            \ &#x27;prompt&#x27;: l:prompt
        \ })
        
        let l:response = system(&#x27;curl -s -X POST -H &quot;Content-Type: application&#x2F;json&quot; -d &#x27; . shellescape(l:json_data) . &#x27; http:&#x2F;&#x2F;localhost:11434&#x2F;api&#x2F;generate&#x27;)
    
        let l:completion = json_decode(l:response)[&#x27;response&#x27;]
        let l:paste_mode = &amp;paste
        set paste
        execute &quot;normal! a&quot; . l:completion
        let &amp;paste = l:paste_mode
    endfunction
    
    nnoremap &lt;leader&gt;c :call AIComplete()&lt;CR&gt;</code></pre></div><br/><div id="40983223" class="c"><input type="checkbox" id="c-40983223" checked=""/><div class="controls bullet"><span class="by">xoranth</span><span>|</span><a href="#40979105">root</a><span>|</span><a href="#40983108">parent</a><span>|</span><a href="#40978108">next</a><span>|</span><label class="collapse" for="c-40983223">[-]</label><label class="expand" for="c-40983223">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!</div><br/></div></div></div></div></div></div></div></div><div id="40978108" class="c"><input type="checkbox" id="c-40978108" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40979105">prev</a><span>|</span><a href="#40978044">next</a><span>|</span><label class="collapse" for="c-40978108">[-]</label><label class="expand" for="c-40978108">[6 more]</label></div><br/><div class="children"><div class="content">The MBPP column should bold DeepSeek as it has a better score than Codestral.</div><br/><div id="40978148" class="c"><input type="checkbox" id="c-40978148" checked=""/><div class="controls bullet"><span class="by">smith7018</span><span>|</span><a href="#40978108">parent</a><span>|</span><a href="#40982242">next</a><span>|</span><label class="collapse" for="c-40978148">[-]</label><label class="expand" for="c-40978148">[4 more]</label></div><br/><div class="children"><div class="content">Which means Codestral Mamba and DeepSeek both lead four benchmarks. Kinda takes the air out the announcement a bit.</div><br/><div id="40978276" class="c"><input type="checkbox" id="c-40978276" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#40978108">root</a><span>|</span><a href="#40978148">parent</a><span>|</span><a href="#40978291">next</a><span>|</span><label class="collapse" for="c-40978276">[-]</label><label class="expand" for="c-40978276">[2 more]</label></div><br/><div class="children"><div class="content">It should be corrected but the interesting aspect of this release is the architecture. To stay competitive while only needing linear inference time and supporting 256k context is pretty neat.</div><br/><div id="40978623" class="c"><input type="checkbox" id="c-40978623" checked=""/><div class="controls bullet"><span class="by">mbowcut2</span><span>|</span><a href="#40978108">root</a><span>|</span><a href="#40978276">parent</a><span>|</span><a href="#40978291">next</a><span>|</span><label class="collapse" for="c-40978623">[-]</label><label class="expand" for="c-40978623">[1 more]</label></div><br/><div class="children"><div class="content">THIS. People don&#x27;t realize the importance of Mamba competing on par with transformers.</div><br/></div></div></div></div><div id="40978291" class="c"><input type="checkbox" id="c-40978291" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#40978108">root</a><span>|</span><a href="#40978148">parent</a><span>|</span><a href="#40978276">prev</a><span>|</span><a href="#40982242">next</a><span>|</span><label class="collapse" for="c-40978291">[-]</label><label class="expand" for="c-40978291">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re in roughly the same class but totally different architectures<p>Deepseek uses a 4k sliding window compared to Codestral Mamba&#x27;s 256k+ tokens</div><br/></div></div></div></div><div id="40982242" class="c"><input type="checkbox" id="c-40982242" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#40978108">parent</a><span>|</span><a href="#40978148">prev</a><span>|</span><a href="#40978044">next</a><span>|</span><label class="collapse" for="c-40982242">[-]</label><label class="expand" for="c-40982242">[1 more]</label></div><br/><div class="children"><div class="content">codegeex4-all-9b beats them &quot;on paper&quot; so that&#x27;s why it&#x27;s not in the benchmarks.</div><br/></div></div></div></div><div id="40978044" class="c"><input type="checkbox" id="c-40978044" checked=""/><div class="controls bullet"><span class="by">magnio</span><span>|</span><a href="#40978108">prev</a><span>|</span><a href="#40977642">next</a><span>|</span><label class="collapse" for="c-40978044">[-]</label><label class="expand" for="c-40978044">[2 more]</label></div><br/><div class="children"><div class="content">They announce the model is on HuggingFace but don&#x27;t link to it. Here it is: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;mistralai&#x2F;mamba-codestral-7B-v0.1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;mistralai&#x2F;mamba-codestral-7B-v0.1</a></div><br/><div id="40978257" class="c"><input type="checkbox" id="c-40978257" checked=""/><div class="controls bullet"><span class="by">dvfjsdhgfv</span><span>|</span><a href="#40978044">parent</a><span>|</span><a href="#40977642">next</a><span>|</span><label class="collapse" for="c-40978257">[-]</label><label class="expand" for="c-40978257">[1 more]</label></div><br/><div class="children"><div class="content">The link is already there in the text, they probably just fixed it.</div><br/></div></div></div></div><div id="40977642" class="c"><input type="checkbox" id="c-40977642" checked=""/><div class="controls bullet"><span class="by">sa-code</span><span>|</span><a href="#40978044">prev</a><span>|</span><a href="#40981993">next</a><span>|</span><label class="collapse" for="c-40977642">[-]</label><label class="expand" for="c-40977642">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s great to see a high-profile model using Mamba2!</div><br/></div></div><div id="40981993" class="c"><input type="checkbox" id="c-40981993" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#40977642">prev</a><span>|</span><a href="#40980173">next</a><span>|</span><label class="collapse" for="c-40981993">[-]</label><label class="expand" for="c-40981993">[5 more]</label></div><br/><div class="children"><div class="content">I know it&#x27;s just a throwaway line, but the bit about Cleopatra at the top feels in poor taste. It&#x27;s completely inaccurate in that no one has ever attributed her death to a &quot;mamba&quot;, and even the asp that some sources claim has been disputed. But even aside from that, it just feels weird that a human being&#x27;s death has turned into a random reference you can make in a throwaway joke while advertising a product.<p>They&#x27;re certainly not the first to use Cleopatra this way nor the most egregious, but there are plenty of other random mamba jokes that could have filled in there and both made more sense and been less crass.</div><br/><div id="40983025" class="c"><input type="checkbox" id="c-40983025" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#40981993">parent</a><span>|</span><a href="#40982042">next</a><span>|</span><label class="collapse" for="c-40983025">[-]</label><label class="expand" for="c-40983025">[1 more]</label></div><br/><div class="children"><div class="content">I do agree, but there’s worse.<p>Ever heard of Patrice Lumumba? He was a congolese politician involved in its independence and democratization. He was shot with the involvement of western forces. 
There’s a drink named after him: Hot chocolate with a shot of rum.</div><br/></div></div><div id="40982042" class="c"><input type="checkbox" id="c-40982042" checked=""/><div class="controls bullet"><span class="by">virgildotcodes</span><span>|</span><a href="#40981993">parent</a><span>|</span><a href="#40983025">prev</a><span>|</span><a href="#40982037">next</a><span>|</span><label class="collapse" for="c-40982042">[-]</label><label class="expand" for="c-40982042">[2 more]</label></div><br/><div class="children"><div class="content">Too soon!</div><br/><div id="40982416" class="c"><input type="checkbox" id="c-40982416" checked=""/><div class="controls bullet"><span class="by">quantisan</span><span>|</span><a href="#40981993">root</a><span>|</span><a href="#40982042">parent</a><span>|</span><a href="#40982037">next</a><span>|</span><label class="collapse" for="c-40982416">[-]</label><label class="expand" for="c-40982416">[1 more]</label></div><br/><div class="children"><div class="content">Rick and Morty ref?<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;rYJP2YWtZl8?si=NIvyeVwCJaExq92U" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;rYJP2YWtZl8?si=NIvyeVwCJaExq92U</a></div><br/></div></div></div></div><div id="40982037" class="c"><input type="checkbox" id="c-40982037" checked=""/><div class="controls bullet"><span class="by">alluro2</span><span>|</span><a href="#40981993">parent</a><span>|</span><a href="#40982042">prev</a><span>|</span><a href="#40980173">next</a><span>|</span><label class="collapse" for="c-40982037">[-]</label><label class="expand" for="c-40982037">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it was generated :)</div><br/></div></div></div></div><div id="40980173" class="c"><input type="checkbox" id="c-40980173" checked=""/><div class="controls bullet"><span class="by">flakiness</span><span>|</span><a href="#40981993">prev</a><span>|</span><a href="#40982820">next</a><span>|</span><label class="collapse" for="c-40980173">[-]</label><label class="expand" for="c-40980173">[1 more]</label></div><br/><div class="children"><div class="content">So Mamba is supposed to be faster and the article claims that.
But they don&#x27;t have any latency numbers.<p>Has anyone tried this? And then, is it fast(er)?</div><br/></div></div><div id="40982820" class="c"><input type="checkbox" id="c-40982820" checked=""/><div class="controls bullet"><span class="by">zamalek</span><span>|</span><a href="#40980173">prev</a><span>|</span><a href="#40977915">next</a><span>|</span><label class="collapse" for="c-40982820">[-]</label><label class="expand" for="c-40982820">[1 more]</label></div><br/><div class="children"><div class="content">Is this the active Codestral model on Le Chat? I got quite some mixed results from it tonight.</div><br/></div></div><div id="40977915" class="c"><input type="checkbox" id="c-40977915" checked=""/><div class="controls bullet"><span class="by">monkeydust</span><span>|</span><a href="#40982820">prev</a><span>|</span><a href="#40978940">next</a><span>|</span><label class="collapse" for="c-40977915">[-]</label><label class="expand" for="c-40977915">[5 more]</label></div><br/><div class="children"><div class="content">Any recommended product primers to Mamba vs Transformers  - pros&#x2F;cons etc?</div><br/><div id="40978580" class="c"><input type="checkbox" id="c-40978580" checked=""/><div class="controls bullet"><span class="by">red2awn</span><span>|</span><a href="#40977915">parent</a><span>|</span><a href="#40981613">next</a><span>|</span><label class="collapse" for="c-40978580">[-]</label><label class="expand" for="c-40978580">[1 more]</label></div><br/><div class="children"><div class="content">A very good primer to state-space models (from which Mamba is based on) is The Annotated S4 [1]. If you want to dive into the code I wrote a minimal single-file implementation of Mamba-2 here [2].<p>[1]: <a href="https:&#x2F;&#x2F;srush.github.io&#x2F;annotated-s4&#x2F;" rel="nofollow">https:&#x2F;&#x2F;srush.github.io&#x2F;annotated-s4&#x2F;</a><p>[2]: <a href="https:&#x2F;&#x2F;github.com&#x2F;tommyip&#x2F;mamba2-minimal">https:&#x2F;&#x2F;github.com&#x2F;tommyip&#x2F;mamba2-minimal</a></div><br/></div></div><div id="40981613" class="c"><input type="checkbox" id="c-40981613" checked=""/><div class="controls bullet"><span class="by">flakiness</span><span>|</span><a href="#40977915">parent</a><span>|</span><a href="#40978580">prev</a><span>|</span><a href="#40977960">next</a><span>|</span><label class="collapse" for="c-40981613">[-]</label><label class="expand" for="c-40981613">[1 more]</label></div><br/><div class="children"><div class="content">For those who are text oriented: <a href="https:&#x2F;&#x2F;newsletter.maartengrootendorst.com&#x2F;p&#x2F;a-visual-guide-to-mamba-and-state" rel="nofollow">https:&#x2F;&#x2F;newsletter.maartengrootendorst.com&#x2F;p&#x2F;a-visual-guide-...</a><p>The paper author has a blog series but I don&#x27;t think it&#x27;s for general public
<a href="https:&#x2F;&#x2F;tridao.me&#x2F;blog&#x2F;2024&#x2F;mamba2-part1-model&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tridao.me&#x2F;blog&#x2F;2024&#x2F;mamba2-part1-model&#x2F;</a></div><br/></div></div><div id="40977960" class="c"><input type="checkbox" id="c-40977960" checked=""/><div class="controls bullet"><span class="by">bhouston</span><span>|</span><a href="#40977915">parent</a><span>|</span><a href="#40981613">prev</a><span>|</span><a href="#40977930">next</a><span>|</span><label class="collapse" for="c-40977960">[-]</label><label class="expand" for="c-40977960">[1 more]</label></div><br/><div class="children"><div class="content">This video is good: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=N6Piou4oYx8" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=N6Piou4oYx8</a>. As are the other videos on the same YouTube account.</div><br/></div></div><div id="40977930" class="c"><input type="checkbox" id="c-40977930" checked=""/><div class="controls bullet"><span class="by">ertgbnm</span><span>|</span><a href="#40977915">parent</a><span>|</span><a href="#40977960">prev</a><span>|</span><a href="#40978940">next</a><span>|</span><label class="collapse" for="c-40977930">[-]</label><label class="expand" for="c-40977930">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=X5F2X4tF9iM" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=X5F2X4tF9iM</a><p>This is what introduced me to them. May be a bit outdated at this point.</div><br/></div></div></div></div><div id="40978940" class="c"><input type="checkbox" id="c-40978940" checked=""/><div class="controls bullet"><span class="by">Kinrany</span><span>|</span><a href="#40977915">prev</a><span>|</span><a href="#40977760">next</a><span>|</span><label class="collapse" for="c-40978940">[-]</label><label class="expand" for="c-40978940">[3 more]</label></div><br/><div class="children"><div class="content">Is there a good explanation of the Mamba architecture?</div><br/><div id="40979396" class="c"><input type="checkbox" id="c-40979396" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#40978940">parent</a><span>|</span><a href="#40979077">next</a><span>|</span><label class="collapse" for="c-40979396">[-]</label><label class="expand" for="c-40979396">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;thegradient.pub&#x2F;mamba-explained&#x2F;" rel="nofollow">https:&#x2F;&#x2F;thegradient.pub&#x2F;mamba-explained&#x2F;</a><p><a href="https:&#x2F;&#x2F;jackcook.com&#x2F;2024&#x2F;02&#x2F;23&#x2F;mamba.html" rel="nofollow">https:&#x2F;&#x2F;jackcook.com&#x2F;2024&#x2F;02&#x2F;23&#x2F;mamba.html</a><p><a href="https:&#x2F;&#x2F;www.kolaayonrinde.com&#x2F;blog&#x2F;2024&#x2F;02&#x2F;11&#x2F;mamba.html" rel="nofollow">https:&#x2F;&#x2F;www.kolaayonrinde.com&#x2F;blog&#x2F;2024&#x2F;02&#x2F;11&#x2F;mamba.html</a></div><br/></div></div><div id="40979077" class="c"><input type="checkbox" id="c-40979077" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40978940">parent</a><span>|</span><a href="#40979396">prev</a><span>|</span><a href="#40977760">next</a><span>|</span><label class="collapse" for="c-40979077">[-]</label><label class="expand" for="c-40979077">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752</a><p>I haven&#x27;t seen any good non-paper explainers yet.</div><br/></div></div></div></div><div id="40977760" class="c"><input type="checkbox" id="c-40977760" checked=""/><div class="controls bullet"><span class="by">culopatin</span><span>|</span><a href="#40978940">prev</a><span>|</span><a href="#40980155">next</a><span>|</span><label class="collapse" for="c-40977760">[-]</label><label class="expand" for="c-40977760">[22 more]</label></div><br/><div class="children"><div class="content">Does anyone have a video or written article that would get one up to speed with a bit of the history&#x2F;progression and current products that are out there for one to try locally?<p>This is coming from someone that understands the general concepts of how LLMs work but only used the general publicly available tools like ChatGPT, Claude, etc.<p>I want to see if I have any hardware I can stress and run something locally, but don’t know where to start or even what are the available options.</div><br/><div id="40977808" class="c"><input type="checkbox" id="c-40977808" checked=""/><div class="controls bullet"><span class="by">Kydlaw</span><span>|</span><a href="#40977760">parent</a><span>|</span><a href="#40979807">next</a><span>|</span><label class="collapse" for="c-40977808">[-]</label><label class="expand" for="c-40977808">[4 more]</label></div><br/><div class="children"><div class="content">If I understand correctly what you are looking for, Ollama might be a solution (<a href="https:&#x2F;&#x2F;ollama.com&#x2F;">https:&#x2F;&#x2F;ollama.com&#x2F;</a>)?. I have no affiliation, but I lazily use this solution when I want to run a quick model locally.</div><br/><div id="40977951" class="c"><input type="checkbox" id="c-40977951" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40977808">parent</a><span>|</span><a href="#40979807">next</a><span>|</span><label class="collapse" for="c-40977951">[-]</label><label class="expand" for="c-40977951">[3 more]</label></div><br/><div class="children"><div class="content">Better yet install Open Web GUI and ollama at the same time via docker. Most people will want a familiar GUI rather than the terminal.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui">https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui</a><p>This will install ollama and open web GUI:<p>For GPU support run:<p>docker run -d -p 3000:8080 --gpus=all -v ollama:&#x2F;root&#x2F;.ollama -v open-webui:&#x2F;app&#x2F;backend&#x2F;data --name open-webui --restart always ghcr.io&#x2F;open-webui&#x2F;open-webui:ollama<p>Use for CPU only support:<p>docker run -d -p 3000:8080 -v ollama:&#x2F;root&#x2F;.ollama -v open-webui:&#x2F;app&#x2F;backend&#x2F;data --name open-webui --restart always ghcr.io&#x2F;open-webui&#x2F;open-webui:ollama</div><br/><div id="40978170" class="c"><input type="checkbox" id="c-40978170" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40977951">parent</a><span>|</span><a href="#40979807">next</a><span>|</span><label class="collapse" for="c-40978170">[-]</label><label class="expand" for="c-40978170">[2 more]</label></div><br/><div class="children"><div class="content">Why do people recommend this instead of the much better oobabooga text-gen-webui?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a><p>It&#x27;s like you hate settings, features, and access to many backends!</div><br/><div id="40978312" class="c"><input type="checkbox" id="c-40978312" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40978170">parent</a><span>|</span><a href="#40979807">next</a><span>|</span><label class="collapse" for="c-40978312">[-]</label><label class="expand" for="c-40978312">[1 more]</label></div><br/><div class="children"><div class="content">To each their own, how are you using these extra features? I personally am not looking to spend a bunch on API credits and don&#x27;t have the hardware to run models larger than 7-8b parameters. I use local llms almost exclusively for formatting notes and as a reading assistant&#x2F;summarizer and therefor don&#x27;t need these features.</div><br/></div></div></div></div></div></div></div></div><div id="40979807" class="c"><input type="checkbox" id="c-40979807" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#40977760">parent</a><span>|</span><a href="#40977808">prev</a><span>|</span><a href="#40977845">next</a><span>|</span><label class="collapse" for="c-40979807">[-]</label><label class="expand" for="c-40979807">[8 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a summary of what&#x27;s happened the past couple of years and what tools are out there.<p>After ChatGPT released, there was  a lot of hype in the space but open source was far behind. Iirc the best open foundation LLM that existed was GPT-2 but it was two generations behind.<p>Awhile later Meta released LLaMA[1], a well trained base foundation model, which brought an explosion to open source. It was soon implemented in the Hugging Face Transformers library[2] and the weights were spread across the Hugging Face website for anyone to use.<p>At first, it was difficult to run locally. Few developers had the system or money to run. It required too much RAM and iirc Meta&#x27;s original implementation didn&#x27;t support running on the CPU but developers soon came up with methods to make it smaller via quantization. The biggest project for this was Llama.cpp[3] which probably is still the biggest open source project today for running LLMs locally. Hugging Face Transformers also added quantization support through bitsandbytes[4].<p>Over the next months there was rapid development in open source. Quantization techniques improved which meant LLaMA was able to run with less and less RAM with greater and greater accuracy on more and more systems. Tools came out that were capable of finetuning LLaMA and there were hundreds of LLaMA finetunes that came out which finetuned LLaMA on instruction following, RLHF, and chat datasets which drastically increased accuracy even further. During this time, Stanford&#x27;s Alpaca, Lmsys&#x27;s Vicuna, Microsoft&#x27;s Wizard, 01ai&#x27;s Yi, Mistral, and a few others made their way onto the open LLM scene with some very good LLaMA finetunes.<p>A new inference engine (software for running LLMs like Llama.cpp, Transformers, etc) called vLLM[5] came out which was capable of running LLMs in a more efficient way than was previously possible in open source. Soon it would even get good AMD support, making it possible for those with AMD GPUs to run open LLMs locally and with relative efficiency.<p>Then Meta released Llama 2[6]. Llama 2 was by far the best open LLM for its time. Released with RLHF instruction finetunes for chat and with human evaluation data that put its open LLM leadership beyond doubt. Existing tools like Llama.cpp and Hugging Face Transformers quickly added support and users had access to the best LLM open source had to offer.<p>At this point in time, despite all the advancements, it was still difficult to run LLMs. Llama.cpp and Transformers were great engines for running LLMs but the setup process was difficult and required a lot of time. You had to find the best LLM, quantize it in the best way for your computer (or figure out how to identify and download one from Hugging Face), setup whatever engine you wanted, figure out how to use your quantized LLM with the engine, fix any bugs you made along the way, and finally figure out how to prompt your specific LLM in a chat-like format.<p>However, tools started coming out to make this process significantly easier. The first one of these that I remember was GPT4All[7]. GPT4All was a wrapper around Llama.cpp which made it easy to install, easy to select the LLM that you want (pre-quantized options for easy download from a download manager), and a chat UI which made LLMs easy to use. This significantly reduced the barrier to entry for those who were interested in using LLMs.<p>The second project that I remember was Ollama[8]. Also a wrapper around Llama.cpp, Ollama gave most of what GPT4All had to offer but in an even simpler way. Today, I believe Ollama is bigger than GPT4All although I think it&#x27;s missing some of the higher-level features of GPT4All.<p>Another important tool that came out during this time is called Exllama[9]. Exllama is an inference engine with a focus on modern consumer Nvidia GPUs and advanced quantization support based on GPTQ. It is probably the best inference engine for squeezing performance out of consumer Nvidia GPUs.<p>Months later, Nvidia came out with another new inference engine called TensorRT-LLM[10]. TensorRT-LLM is capable of running most LLMs and does so with extreme efficiency. It is the most efficient open source inferencing engine that exists for Nvidia GPUs. However, it also has the most difficult setup process of any inference engine and is made primarily for production use cases and Nvidia AI GPUs so don&#x27;t expect it to work on your personal computer.<p>With the rumors of GPT-4 being a Mixture of Experts LLM, research breakthroughs in MoE, and some small MoE LLMs coming out, interest in MoE LLMs was at an all-time high. The company Mistral had proven itself in the past with very impressive LLaMA finetunes, capitalized on this interest by releasing Mixtral 8x7b[11]. The best accuracy for its size LLM that the local LLM community had seen to date. Eventually MoE support was added to all inference engines and it was a very popular mid-to-large sized LLM.<p>Cohere released their own LLM as well called Command R+[12] built specifically for RAG-related tasks with a context length of 128k. It&#x27;s quite large and doesn&#x27;t have notable performance on many metrics, but it has some interesting RAG features no other LLM has.<p>More recently, Llama 3[13] was released which similar to previous Llama releases, blew every other open LLM out of the water. The smallest version of Llama 3 (Llama 3 8b) has the greatest accuracy for its size of any other open LLM and the largest version of Llama 3 released so far (Llama 3 70b) beats every other open LLM on almost every metric.<p>Less than a month ago, Google released Gemma 2[14], the largest of which, performs very well under human evaluation despite being less than half the size of Llama 3 70b, but performs only decently on automated benchmarks.<p>If you&#x27;re looking for a tool to get started running LLMs locally, I&#x27;d go with either Ollama or GPT4All. They make the process about as painless as possible. I believe GPT4All has more features like using your local documents for RAG, but you can also use something like Open WebUI[15] with Ollama to get the same functionality.<p>If you want to get into the weeds a bit and extract some more performance out of your machine, I&#x27;d go with using Llama.cpp, Exllama, or vLLM depending upon your system. If you have a normal, consumer Nvidia GPU, I&#x27;d go with Exllama. If you have an AMD GPU that supports ROCm 5.7 or 6.0, I&#x27;d go with vLLM. For anything else, including just running it on your CPU or M-series Mac, I&#x27;d go with Llama.cpp. TensorRT-LLM only makes sense if you have an AI Nvidia GPU like the A100, V100, A10, H100, etc.<p>[1] <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;large-language-model-llama-meta-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;large-language-model-llama-meta-ai&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a><p>[4] <a href="https:&#x2F;&#x2F;github.com&#x2F;bitsandbytes-foundation&#x2F;bitsandbytes">https:&#x2F;&#x2F;github.com&#x2F;bitsandbytes-foundation&#x2F;bitsandbytes</a><p>[5] <a href="https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm">https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm</a><p>[6] <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;llama-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;llama-2&#x2F;</a><p>[7] <a href="https:&#x2F;&#x2F;www.nomic.ai&#x2F;gpt4all" rel="nofollow">https:&#x2F;&#x2F;www.nomic.ai&#x2F;gpt4all</a><p>[8] <a href="http:&#x2F;&#x2F;ollama.ai&#x2F;" rel="nofollow">http:&#x2F;&#x2F;ollama.ai&#x2F;</a><p>[9] <a href="https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllamav2">https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllamav2</a><p>[10] <a href="https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;TensorRT-LLM">https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;TensorRT-LLM</a><p>[11] <a href="https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;mixtral-of-experts&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;mixtral-of-experts&#x2F;</a><p>[12] <a href="https:&#x2F;&#x2F;cohere.com&#x2F;blog&#x2F;command-r-plus-microsoft-azure" rel="nofollow">https:&#x2F;&#x2F;cohere.com&#x2F;blog&#x2F;command-r-plus-microsoft-azure</a><p>[13] <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;meta-llama-3&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;meta-llama-3&#x2F;</a><p>[14] <a href="https:&#x2F;&#x2F;blog.google&#x2F;technology&#x2F;developers&#x2F;google-gemma-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.google&#x2F;technology&#x2F;developers&#x2F;google-gemma-2&#x2F;</a><p>[15] <a href="https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui">https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui</a></div><br/><div id="40982952" class="c"><input type="checkbox" id="c-40982952" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40979807">parent</a><span>|</span><a href="#40980923">next</a><span>|</span><label class="collapse" for="c-40982952">[-]</label><label class="expand" for="c-40982952">[1 more]</label></div><br/><div class="children"><div class="content">Overall a good write up, but I have a few quips:<p>&gt; Awhile later Meta released LLaMA[1],<p>I think Stable Diffusion was first to release a SOTA model (August 2022) that worked locally, not in language but image generation, but it set the tone for Meta. LLaMA only came in February 2023.<p>&gt; The company Mistral had proven itself in the past with very impressive LLaMA finetunes<p>Mistal is not a finetune of LLaMA, it is a model trained from scratch. Also, Mistral was most of the time better than LLaMA during this period.<p>&gt; Quantization techniques improved which meant LLaMA was able to run with less and less RAM with greater and greater accuracy<p>Quantization does not improve accuracy, except if you trade off precision for longer context maybe, but not on similar prompts. It is like JPEG compression, the original is always better for a specific image, but for the same byte size you get more resolution from JPEG than say... a PNG.</div><br/></div></div><div id="40980923" class="c"><input type="checkbox" id="c-40980923" checked=""/><div class="controls bullet"><span class="by">psychoslave</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40979807">parent</a><span>|</span><a href="#40982952">prev</a><span>|</span><a href="#40980608">next</a><span>|</span><label class="collapse" for="c-40980923">[-]</label><label class="expand" for="c-40980923">[1 more]</label></div><br/><div class="children"><div class="content">This is one one the most useful and informative comment I ever faced on HN. Thank you very much.</div><br/></div></div><div id="40980608" class="c"><input type="checkbox" id="c-40980608" checked=""/><div class="controls bullet"><span class="by">iAmAPencilYo</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40979807">parent</a><span>|</span><a href="#40980923">prev</a><span>|</span><a href="#40980577">next</a><span>|</span><label class="collapse" for="c-40980608">[-]</label><label class="expand" for="c-40980608">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! Very helpful as a newbie coming in.</div><br/></div></div><div id="40980577" class="c"><input type="checkbox" id="c-40980577" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40979807">parent</a><span>|</span><a href="#40980608">prev</a><span>|</span><a href="#40977845">next</a><span>|</span><label class="collapse" for="c-40980577">[-]</label><label class="expand" for="c-40980577">[4 more]</label></div><br/><div class="children"><div class="content">Great info. Do you also know the state of the code assistants? Any thoughts on copilot versus others?</div><br/><div id="40980902" class="c"><input type="checkbox" id="c-40980902" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40980577">parent</a><span>|</span><a href="#40980921">next</a><span>|</span><label class="collapse" for="c-40980902">[-]</label><label class="expand" for="c-40980902">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been following the state of things, but I&#x27;m not sure which ones are the best. There&#x27;s Meta&#x27;s CodeLlama[1], Mistral&#x27;s Codestral[2], DeepSeek AI&#x27;s DeepSeek-Coder-V2-Instruct[3], CodeGemma[4], Alibaba&#x27;s CodeQwen[5], and Microsoft&#x27;s WizardCoder[6].<p>I&#x27;m pretty sure CodeLlama is out of date now. I&#x27;ve heard DeepSeek LLMs are good and DeepSeek-Coder-V2-Instruct was released recently. With the good reputation and its massive size (236b) I&#x27;d guess it is the best coding LLM, but if it&#x27;s not being trained efficiently, maybe Codestral and Codestral Mamba come close.<p>I don&#x27;t think the best coding LLMs are close to GitHub Copilot but I could be wrong since I&#x27;m just relaying information that I&#x27;ve heard secondhand.<p>[1] <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;code-llama-large-language-model-coding&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;code-llama-large-language-model-cod...</a><p>[2] <a href="https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;codestral&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;codestral&#x2F;</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-Coder-V2">https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-Coder-V2</a><p>[4] <a href="https:&#x2F;&#x2F;developers.googleblog.com&#x2F;en&#x2F;gemma-family-expands-with-models-tailored-for-developers-and-researchers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;developers.googleblog.com&#x2F;en&#x2F;gemma-family-expands-wi...</a><p>[5] <a href="https:&#x2F;&#x2F;qwenlm.github.io&#x2F;blog&#x2F;codeqwen1.5&#x2F;" rel="nofollow">https:&#x2F;&#x2F;qwenlm.github.io&#x2F;blog&#x2F;codeqwen1.5&#x2F;</a><p>[6] <a href="https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM">https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM</a></div><br/><div id="40982246" class="c"><input type="checkbox" id="c-40982246" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40980902">parent</a><span>|</span><a href="#40980921">next</a><span>|</span><label class="collapse" for="c-40982246">[-]</label><label class="expand" for="c-40982246">[1 more]</label></div><br/><div class="children"><div class="content">try THUDM&#x2F;codegeex4-all-9b</div><br/></div></div></div></div><div id="40980921" class="c"><input type="checkbox" id="c-40980921" checked=""/><div class="controls bullet"><span class="by">hobofan</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40980577">parent</a><span>|</span><a href="#40980902">prev</a><span>|</span><a href="#40977845">next</a><span>|</span><label class="collapse" for="c-40980921">[-]</label><label class="expand" for="c-40980921">[1 more]</label></div><br/><div class="children"><div class="content">All the main IDE-integrated ones seem very much on par (Copilot, Sourcegraph Cody, Continue.dev), with cursor.sh liked by some as it has code assistant-first UI.<p>I&#x27;ve personally went back to the browser with Claude 3.5 Sonnet (and the projects + artifacts feature), as it is one of the most industrious ones, and I really like the UX of artifacts + it integrates new code well into existing code you paste into it.<p>In the end I think it also often comes down to what languages&#x2F;frameworks you are using and how well the LLM&#x2F;product handles it, so I&#x27;d still recommend to test around. E.g. some of the main frameworks I&#x27;m working with on a daily basis went through big refactors&#x2F;interface changes 1-2 years ago, and I stopped using ChatGPT because it had a strong tendency to produce code based on the old interfaces&#x2F;paradigms.<p>Aider[0] is also quite interesting, especially when it comes to more significant refactorings in the codebase and has gotten quite good with that with the last few bigger model releases, but it takes same time to get used to and doesn&#x27;t have good IDE-integration.<p>[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider">https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider</a></div><br/></div></div></div></div></div></div><div id="40977845" class="c"><input type="checkbox" id="c-40977845" checked=""/><div class="controls bullet"><span class="by">currycurry16</span><span>|</span><a href="#40977760">parent</a><span>|</span><a href="#40979807">prev</a><span>|</span><a href="#40977908">next</a><span>|</span><label class="collapse" for="c-40977845">[-]</label><label class="expand" for="c-40977845">[1 more]</label></div><br/><div class="children"><div class="content">Find good models here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;open-llm-leaderboard&#x2F;open_llm_...</a><p>Check hardware requirements here: <a href="https:&#x2F;&#x2F;rahulschand.github.io&#x2F;gpu_poor&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rahulschand.github.io&#x2F;gpu_poor&#x2F;</a></div><br/></div></div><div id="40977908" class="c"><input type="checkbox" id="c-40977908" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#40977760">parent</a><span>|</span><a href="#40977845">prev</a><span>|</span><a href="#40977841">next</a><span>|</span><label class="collapse" for="c-40977908">[-]</label><label class="expand" for="c-40977908">[5 more]</label></div><br/><div class="children"><div class="content">Most the 7b instruct models are very bad outside very simple queries.<p>You can run a 7b on most modern hardware.How fast will vary.<p>To run 30-70b models you&#x27;re getting in the realm of needing 24gb or more of vRAM.</div><br/><div id="40979481" class="c"><input type="checkbox" id="c-40979481" checked=""/><div class="controls bullet"><span class="by">dTal</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40977908">parent</a><span>|</span><a href="#40978154">next</a><span>|</span><label class="collapse" for="c-40979481">[-]</label><label class="expand" for="c-40979481">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Most the 7b instruct models are very bad outside very simple queries.<p>I can&#x27;t agree with &quot;very bad&quot;. Maybe your standards are set by the best, largest models, but have a little perspective: a modern 7b model is a friggin <i>magical</i> piece of software. Fully in the realm of sci-fi until basically last Tuesday. It can reliably summarize documents, bash a 30 minute rambling voice note into a terse proposal, and give you social counseling at least on par with r&#x2F;Relationship_Advice. It might not always get facts exactly right but it is <i>smart</i> in a way that computers have never been before. And for all this capability, you can get it running on a computer a decade old, maybe even a Raspberry Pi or a smartphone.<p>To answer the parent: Download a &quot;gguf&quot; file (blob of weights) of a popular model like Mistral from HugginFace. Git pull and compile llama.cpp. Run .&#x2F;main -m path&#x2F;to&#x2F;gguf -p &quot;prompt&quot;</div><br/><div id="40983019" class="c"><input type="checkbox" id="c-40983019" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40979481">parent</a><span>|</span><a href="#40978154">next</a><span>|</span><label class="collapse" for="c-40983019">[-]</label><label class="expand" for="c-40983019">[1 more]</label></div><br/><div class="children"><div class="content">Even better, install ollama and then do &quot;ollama run llama3&quot;, it works like docker, pulls the model locally and starts a chat session right there in the terminal. No need to compile. Or just run the docker image &quot;ollama&#x2F;ollama&quot;.</div><br/></div></div></div></div><div id="40978154" class="c"><input type="checkbox" id="c-40978154" checked=""/><div class="controls bullet"><span class="by">Agentus</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40977908">parent</a><span>|</span><a href="#40979481">prev</a><span>|</span><a href="#40977841">next</a><span>|</span><label class="collapse" for="c-40978154">[-]</label><label class="expand" for="c-40978154">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m looking to run something on a 24gb GPU for the purpose of running wild with agentic use of LLMs.  Is there anything worth trying that would fit on that amount of vRAM?  Or are all the open-source PC-sized LLMs laughable still?</div><br/><div id="40978929" class="c"><input type="checkbox" id="c-40978929" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#40977760">root</a><span>|</span><a href="#40978154">parent</a><span>|</span><a href="#40977841">next</a><span>|</span><label class="collapse" for="c-40978929">[-]</label><label class="expand" for="c-40978929">[1 more]</label></div><br/><div class="children"><div class="content">You can run the llama 70b based models faster than 10 tkn&#x2F;s on 24gb vram. I&#x27;ve found that the quality of this class of LLMs is heavily swayed by your configuration and system prompting and results may vary. This Reddit post seems to have some input on the topic:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1cj4det&#x2F;llama_3_70b_instruct_works_surprisingly_well_on&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1cj4det&#x2F;llama_3...</a><p>I haven&#x27;t used any agent frameworks other than messing around with langchain a bit so I can&#x27;t speak to how that would effect things.</div><br/></div></div></div></div></div></div><div id="40977841" class="c"><input type="checkbox" id="c-40977841" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#40977760">parent</a><span>|</span><a href="#40977908">prev</a><span>|</span><a href="#40978043">next</a><span>|</span><label class="collapse" for="c-40977841">[-]</label><label class="expand" for="c-40977841">[1 more]</label></div><br/><div class="children"><div class="content">If you mean LLM in general, maybe try llamafile first<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile</a></div><br/></div></div><div id="40978043" class="c"><input type="checkbox" id="c-40978043" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#40977760">parent</a><span>|</span><a href="#40977841">prev</a><span>|</span><a href="#40977850">next</a><span>|</span><label class="collapse" for="c-40978043">[-]</label><label class="expand" for="c-40978043">[1 more]</label></div><br/><div class="children"><div class="content">For running LLMs, I think most people just dive into <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;</a> and start reading.<p>Not sure what the equivalent is for image generation; it&#x27;s either <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;</a> or one of the related subreddits it links to.<p>Sadly, I&#x27;ve yet to find anyone doing &quot;daily ML-hobbyist news&quot; content creation, summarizing the types of articles that appear on these subreddits. (Which is a surprise to me, as it&#x27;s really easy to find e.g. &quot;daily homelab news&quot; content creators. Please, someone, start a &quot;daily ML-hobbyist news&quot; blog&#x2F;channel! Given that the target audience would essentially be &quot;people who will get an itch to buy a better GPU soon&quot;, the CPM you&#x27;d earn on ad impressions would be really high...)<p>---<p>That being said, just to get you started, here&#x27;s a few things to know at present about &quot;what you can run locally&quot;:<p>1. Most models (of the architectures people care about today) will <i>probably</i> fit on a GPU which has something like 1.5x the VRAM of the model&#x27;s parameter-weights size. So e.g. a &quot;7B&quot; (7 billion parameter-weights) model, will fit on a GPU that has 12GB of VRAM. (You can potentially squeeze even tighter if you have a machine with integrated graphics + dedicated GPU, and you&#x27;re using the integrated graphics as graphics, leaving the GPU&#x27;s VRAM free to <i>only</i> hold the model.)<p>2. There are models that come in all sorts of sizes. Many open-source ML models are huge (70B, 120B, 144B — things you&#x27;d need datacenter-class GPUs to run), but then versions of these same models get released which have been heavily cut down (pruned and&#x2F;or quantized), to force them to fit into smaller VRAM sizes. There are 5B, 3B, 1B, even 0.5B models (although the last two are usually special-purpose models.)<p>3. Surprisingly, depending on your use-case, smaller models (or small quants of larger models) can &quot;mostly&quot; work perfectly well! They just have more edge-cases where something will send them off the rails spiralling into nonsense — so they&#x27;re less <i>reliable</i> than their larger cousins. You might have to give them more prompting, and try regenerating their output from the same prompt several times, to get good results.<p>4. Apple Silicon Macs have a GPU and TPU that read from&#x2F;write to the same unified memory that the CPU does. While this makes these devices <i>slower</i> for inference than &quot;real&quot; GPUs with dedicated VRAM, it means that if you happen to own a Mac with 16GB of RAM, then you own something that can run 7B models. AS Macs are, oddly enough, the &quot;cheapest&quot; things you can buy in terms of model-capacity-per-dollar. (Unlike a &quot;real&quot; GPU, they won&#x27;t be especially <i>quick</i> and won&#x27;t have any capacity for <i>concurrent</i> model inference, so you&#x27;d never use one as a server backing an Inference-as-a-Service business. But for home use? No real downsides.)</div><br/></div></div><div id="40977850" class="c"><input type="checkbox" id="c-40977850" checked=""/><div class="controls bullet"><span class="by">_kidlike</span><span>|</span><a href="#40977760">parent</a><span>|</span><a href="#40978043">prev</a><span>|</span><a href="#40980155">next</a><span>|</span><label class="collapse" for="c-40977850">[-]</label><label class="expand" for="c-40977850">[1 more]</label></div><br/><div class="children"><div class="content">not sure about the history&#x2F;progression part, but there&#x27;s ollama which makes it possible to run models locally. The UX of ollama is similar to docker.</div><br/></div></div></div></div><div id="40980155" class="c"><input type="checkbox" id="c-40980155" checked=""/><div class="controls bullet"><span class="by">tatsuya4</span><span>|</span><a href="#40977760">prev</a><span>|</span><a href="#40978003">next</a><span>|</span><label class="collapse" for="c-40980155">[-]</label><label class="expand" for="c-40980155">[1 more]</label></div><br/><div class="children"><div class="content">Just did a quick test in the <a href="https:&#x2F;&#x2F;model.box" rel="nofollow">https:&#x2F;&#x2F;model.box</a> playground, and it looks like the completion length is noticeably shorter than other models (e.g., gpt-4o). However, the response speed meets expectations..</div><br/></div></div><div id="40978003" class="c"><input type="checkbox" id="c-40978003" checked=""/><div class="controls bullet"><span class="by">rjurney</span><span>|</span><a href="#40980155">prev</a><span>|</span><a href="#40979727">next</a><span>|</span><label class="collapse" for="c-40978003">[-]</label><label class="expand" for="c-40978003">[19 more]</label></div><br/><div class="children"><div class="content">But I JUST switched from GPT4o to Claude! :( Kidding, but it isn&#x27;t clear how to use this thing, as others have pointed out.</div><br/><div id="40978263" class="c"><input type="checkbox" id="c-40978263" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#40978003">parent</a><span>|</span><a href="#40979727">next</a><span>|</span><label class="collapse" for="c-40978263">[-]</label><label class="expand" for="c-40978263">[18 more]</label></div><br/><div class="children"><div class="content">What made you switch?</div><br/><div id="40978806" class="c"><input type="checkbox" id="c-40978806" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978263">parent</a><span>|</span><a href="#40978524">next</a><span>|</span><label class="collapse" for="c-40978806">[-]</label><label class="expand" for="c-40978806">[6 more]</label></div><br/><div class="children"><div class="content">Claude Projects which allow attaching a bunch of files to fill up the 200k context. I wrote up a script to dump a bunch of code and documentation files to markdown as context and I add them to a bunch of Claude projects on a per topic basis.<p>For example, I&#x27;m currently working on a Rust&#x2F;Qt desktop app so I have a project with the whole Qt6 book attached to ask questions about Qt, a project with my SQL schema and ORM&#x2F;Sqlite docs to ask questions about the app&#x27;s data and generate models without dealing with hallucinations, a project with all my QML files and Rust QML element code, a project with a bunch of Rust crate docs, and so on and on.<p>GPTs allow attaching files too but Claude Projects dump the entire contents of the files into the context rather than trying to do some hacky RAG that never works like I want it to.</div><br/><div id="40979069" class="c"><input type="checkbox" id="c-40979069" checked=""/><div class="controls bullet"><span class="by">funnygiraffe</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978806">parent</a><span>|</span><a href="#40978524">next</a><span>|</span><label class="collapse" for="c-40979069">[-]</label><label class="expand" for="c-40979069">[5 more]</label></div><br/><div class="children"><div class="content">I was under the impression that with LLMs, in order to get high-quality answers, it&#x27;s always best to keep context short. Is that not the case anymore?
Does Claude under this usage paradigm not struggle with very long contexts in ways as for example described in the &quot;lost in the middle&quot; paper (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.03172" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.03172</a>)?</div><br/><div id="40980179" class="c"><input type="checkbox" id="c-40980179" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40979069">parent</a><span>|</span><a href="#40981201">next</a><span>|</span><label class="collapse" for="c-40980179">[-]</label><label class="expand" for="c-40980179">[1 more]</label></div><br/><div class="children"><div class="content">The conclusion you walked away with is the opposite of what usually works in practice.<p>The more context you give the llm, the better.<p>The key takeaway from that paper is to keep your instructions&#x2F;questions&#x2F;direction in the beginning or at the end of the context. Any information can go anywhere.<p>Not to be too dismissive, it&#x27;s a good paper, but we&#x27;re one year further and in practice this issue seems to have been tackled by training on better data.<p>This can differ a lot depending on what model you&#x27;re using, but in the case of claude sonnet 3.5, more relevant context is generally better for anything except for speed.<p>It does remain true that you need to keep your most important instructions at the beginning or at the end however.</div><br/></div></div><div id="40981201" class="c"><input type="checkbox" id="c-40981201" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40979069">parent</a><span>|</span><a href="#40980179">prev</a><span>|</span><a href="#40979337">next</a><span>|</span><label class="collapse" for="c-40981201">[-]</label><label class="expand" for="c-40981201">[1 more]</label></div><br/><div class="children"><div class="content">At the beginning it was true, the longer the context, the more the LLM was lost, but now, the new models can retrieve information anywhere in the context<p>c.f.<p><a href="https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;GH2NJMxbYAAcRL3?format=jpg&amp;name=medium" rel="nofollow">https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;GH2NJMxbYAAcRL3?format=jpg&amp;name=...</a></div><br/></div></div><div id="40979337" class="c"><input type="checkbox" id="c-40979337" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40979069">parent</a><span>|</span><a href="#40981201">prev</a><span>|</span><a href="#40979698">next</a><span>|</span><label class="collapse" for="c-40979337">[-]</label><label class="expand" for="c-40979337">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have the time to evaluate the effects of context length on my use cases so I have no idea. There might be some degradation when I attach the Qt book which is probably already in Claude&#x27;s training data but when using it against my private code base, it&#x27;s not like I have any other choice.<p>The UX of drag and dropping a few monolithic markdown files to include entire chunks of a large project outweighs the downsides of including irrelevant context in my experience.</div><br/></div></div><div id="40979698" class="c"><input type="checkbox" id="c-40979698" checked=""/><div class="controls bullet"><span class="by">inciampati</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40979069">parent</a><span>|</span><a href="#40979337">prev</a><span>|</span><a href="#40978524">next</a><span>|</span><label class="collapse" for="c-40979698">[-]</label><label class="expand" for="c-40979698">[1 more]</label></div><br/><div class="children"><div class="content">No, you need to provide as much information in context as possible. Otherwise you are sampling from the mode. &quot;Write me an essay about cows&quot; = garbage boring and probably 200 words. &quot;here are twenty papers about cow evolution, write me an overview of findings&quot; = yes</div><br/></div></div></div></div></div></div><div id="40978524" class="c"><input type="checkbox" id="c-40978524" checked=""/><div class="controls bullet"><span class="by">rjurney</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978263">parent</a><span>|</span><a href="#40978806">prev</a><span>|</span><a href="#40978313">next</a><span>|</span><label class="collapse" for="c-40978524">[-]</label><label class="expand" for="c-40978524">[6 more]</label></div><br/><div class="children"><div class="content">Claude is much better. Overwhelmingly better. It not only implements deep learning models for me, it has great suggestions on evolving them to actually work.</div><br/><div id="40978672" class="c"><input type="checkbox" id="c-40978672" checked=""/><div class="controls bullet"><span class="by">mountainriver</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978524">parent</a><span>|</span><a href="#40978313">next</a><span>|</span><label class="collapse" for="c-40978672">[-]</label><label class="expand" for="c-40978672">[5 more]</label></div><br/><div class="children"><div class="content">lol no it’s not, the benchmarks don’t show that at all. Both have issues in different ways</div><br/><div id="40978895" class="c"><input type="checkbox" id="c-40978895" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978672">parent</a><span>|</span><a href="#40981767">next</a><span>|</span><label class="collapse" for="c-40978895">[-]</label><label class="expand" for="c-40978895">[3 more]</label></div><br/><div class="children"><div class="content">Benchmarks are pretty flawed IMO, in particular their weakness here seems to be that they are poor at evaluating long-tail multiturn conversations. 4o often gives a great first response, then spirals into a repetition. Sonnet 3.5 is much better at seeing the big picture in a longer conversation IMO.</div><br/><div id="40980708" class="c"><input type="checkbox" id="c-40980708" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978895">parent</a><span>|</span><a href="#40979869">next</a><span>|</span><label class="collapse" for="c-40980708">[-]</label><label class="expand" for="c-40980708">[1 more]</label></div><br/><div class="children"><div class="content">I made a mobile app the other day using LLMs (I had never used React or TypeScript before, and I built an app with React Native). I was pretty disappointed, both Sonnet 3.5 and gpt-4-turbo performed pretty poorly, making mistakes like missing a closing bracket somewhere and meaning I had to revert, because I had no idea where they meant to put it.<p>Also they did the thing that junior developers tend to do, where you have a race condition of some sort, and they just work around it by adding some if checks. The app is at around 400 lines right now, it works but feels pretty brittle. Adding a tiny feature here or there breaks something else, and GPT does the wrong thing half the time.<p>All in all, I&#x27;m not complaining, because I made an app in two days, but it won&#x27;t replace a developer yet, no matter how much I want it to.</div><br/></div></div><div id="40979869" class="c"><input type="checkbox" id="c-40979869" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978895">parent</a><span>|</span><a href="#40980708">prev</a><span>|</span><a href="#40981767">next</a><span>|</span><label class="collapse" for="c-40979869">[-]</label><label class="expand" for="c-40979869">[1 more]</label></div><br/><div class="children"><div class="content">Repetition in multiturn conversations is actually Sonnet&#x27;s fatal flaw, both 3 and 3.5. 4o is also repetitive to an extent. Opus is <i>way</i> better than both at being non-repetitive.</div><br/></div></div></div></div><div id="40981767" class="c"><input type="checkbox" id="c-40981767" checked=""/><div class="controls bullet"><span class="by">treme</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978672">parent</a><span>|</span><a href="#40978895">prev</a><span>|</span><a href="#40978313">next</a><span>|</span><label class="collapse" for="c-40981767">[-]</label><label class="expand" for="c-40981767">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s the consensus of people that use both that claude.ai is superior for practical use despite benchmark results which are mostly one-shot based prompts.</div><br/></div></div></div></div></div></div><div id="40978313" class="c"><input type="checkbox" id="c-40978313" checked=""/><div class="controls bullet"><span class="by">pelagicAustral</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978263">parent</a><span>|</span><a href="#40978524">prev</a><span>|</span><a href="#40978382">next</a><span>|</span><label class="collapse" for="c-40978313">[-]</label><label class="expand" for="c-40978313">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using both, been doing that for months now. I can confidently assert that while Claude is getting better and better, GPT 4 and 4o seem the be getting dumbed down for some unexplained reason. Claude is now my go-to for anything code. (I do Ruby and C#, btw, other might have a different experience)</div><br/><div id="40979578" class="c"><input type="checkbox" id="c-40979578" checked=""/><div class="controls bullet"><span class="by">marcyb5st</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978313">parent</a><span>|</span><a href="#40978382">next</a><span>|</span><label class="collapse" for="c-40979578">[-]</label><label class="expand" for="c-40979578">[1 more]</label></div><br/><div class="children"><div class="content">I guess they are distilling the models so that they can save $$$ on serving.</div><br/></div></div></div></div><div id="40978382" class="c"><input type="checkbox" id="c-40978382" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978263">parent</a><span>|</span><a href="#40978313">prev</a><span>|</span><a href="#40979727">next</a><span>|</span><label class="collapse" for="c-40978382">[-]</label><label class="expand" for="c-40978382">[3 more]</label></div><br/><div class="children"><div class="content">GPT4o is way behind sonnet 3.5</div><br/><div id="40978677" class="c"><input type="checkbox" id="c-40978677" checked=""/><div class="controls bullet"><span class="by">mountainriver</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978382">parent</a><span>|</span><a href="#40979727">next</a><span>|</span><label class="collapse" for="c-40978677">[-]</label><label class="expand" for="c-40978677">[2 more]</label></div><br/><div class="children"><div class="content">Huh I guess all the benchmarks are wrong then</div><br/><div id="40978901" class="c"><input type="checkbox" id="c-40978901" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#40978003">root</a><span>|</span><a href="#40978677">parent</a><span>|</span><a href="#40979727">next</a><span>|</span><label class="collapse" for="c-40978901">[-]</label><label class="expand" for="c-40978901">[1 more]</label></div><br/><div class="children"><div class="content">Agreed.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40979727" class="c"><input type="checkbox" id="c-40979727" checked=""/><div class="controls bullet"><span class="by">taf2</span><span>|</span><a href="#40978003">prev</a><span>|</span><a href="#40978178">next</a><span>|</span><label class="collapse" for="c-40979727">[-]</label><label class="expand" for="c-40979727">[2 more]</label></div><br/><div class="children"><div class="content">How does this work in vim?</div><br/><div id="40981737" class="c"><input type="checkbox" id="c-40981737" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#40979727">parent</a><span>|</span><a href="#40978178">next</a><span>|</span><label class="collapse" for="c-40981737">[-]</label><label class="expand" for="c-40981737">[1 more]</label></div><br/><div class="children"><div class="content">Similarly, is there a way to use it with Kate or Sublime Text?</div><br/></div></div></div></div><div id="40978650" class="c"><input type="checkbox" id="c-40978650" checked=""/><div class="controls bullet"><span class="by">pzo</span><span>|</span><a href="#40978178">prev</a><span>|</span><a href="#40978156">next</a><span>|</span><label class="collapse" for="c-40978650">[-]</label><label class="expand" for="c-40978650">[3 more]</label></div><br/><div class="children"><div class="content">weird they compare to deepseek-coder v1.5 when we already have v2.0. Any advantage to use codestral mamba apart from that it&#x27;s lighter in weights?</div><br/><div id="40980477" class="c"><input type="checkbox" id="c-40980477" checked=""/><div class="controls bullet"><span class="by">kz919</span><span>|</span><a href="#40978650">parent</a><span>|</span><a href="#40982405">next</a><span>|</span><label class="collapse" for="c-40980477">[-]</label><label class="expand" for="c-40980477">[1 more]</label></div><br/><div class="children"><div class="content">obviously because they can&#x27;t beat it... There will be zero reason to use it when you have better transformer based models that can fit the existing infrastructure.</div><br/></div></div><div id="40982405" class="c"><input type="checkbox" id="c-40982405" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#40978650">parent</a><span>|</span><a href="#40980477">prev</a><span>|</span><a href="#40978156">next</a><span>|</span><label class="collapse" for="c-40982405">[-]</label><label class="expand" for="c-40982405">[1 more]</label></div><br/><div class="children"><div class="content">deepseek-coder 2.0 is big, even lite version.<p>I do wish they compare it to codegeex4-all-9b</div><br/></div></div></div></div><div id="40978156" class="c"><input type="checkbox" id="c-40978156" checked=""/><div class="controls bullet"><span class="by">localfirst</span><span>|</span><a href="#40978650">prev</a><span>|</span><a href="#40978873">next</a><span>|</span><label class="collapse" for="c-40978156">[-]</label><label class="expand" for="c-40978156">[1 more]</label></div><br/><div class="children"><div class="content">any sort of evals on how it compares to closed models like chat gpt 4 or open ones like WizardLLM ?</div><br/></div></div><div id="40978873" class="c"><input type="checkbox" id="c-40978873" checked=""/><div class="controls bullet"><span class="by">sam_goldman_</span><span>|</span><a href="#40978156">prev</a><span>|</span><a href="#40977917">next</a><span>|</span><label class="collapse" for="c-40978873">[-]</label><label class="expand" for="c-40978873">[2 more]</label></div><br/><div class="children"><div class="content">You can try this model out using OpenAI&#x27;s API format with this TypeScript SDK: <a href="https:&#x2F;&#x2F;github.com&#x2F;token-js&#x2F;token.js">https:&#x2F;&#x2F;github.com&#x2F;token-js&#x2F;token.js</a><p>You just need a Mistral API key: <a href="https:&#x2F;&#x2F;console.mistral.ai&#x2F;api-keys&#x2F;" rel="nofollow">https:&#x2F;&#x2F;console.mistral.ai&#x2F;api-keys&#x2F;</a></div><br/></div></div><div id="40977917" class="c"><input type="checkbox" id="c-40977917" checked=""/><div class="controls bullet"><span class="by">croemer</span><span>|</span><a href="#40978873">prev</a><span>|</span><label class="collapse" for="c-40977917">[-]</label><label class="expand" for="c-40977917">[8 more]</label></div><br/><div class="children"><div class="content">The first sentence is wrong. The website says:<p>&gt; As a tribute to Cleopatra, whose glorious destiny ended in tragic snake circumstances<p>but according to Wikipedia this is not true:<p>&gt; When Cleopatra learned that Octavian planned to bring her to his Roman triumphal procession, she killed herself by poisoning, contrary to the popular belief that she was bitten by an asp.</div><br/><div id="40978098" class="c"><input type="checkbox" id="c-40978098" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#40977917">parent</a><span>|</span><a href="#40978330">next</a><span>|</span><label class="collapse" for="c-40978098">[-]</label><label class="expand" for="c-40978098">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that seems to be a myth, but exact circumstances seem rather uncertain according to the Wikipedia article [1]:<p>&gt; [A]ccording to the Roman-era writers Strabo, Plutarch, and Cassius Dio, Cleopatra poisoned herself using either a toxic ointment or by introducing the poison with a sharp implement such as a hairpin. Modern scholars debate the validity of ancient reports involving snakebites as the cause of death and whether she was murdered. Some academics hypothesize that her Roman political rival Octavian forced her to kill herself in a manner of her choosing. The location of Cleopatra&#x27;s tomb is unknown. It was recorded that Octavian allowed for her and her husband, the Roman politician and general Mark Antony, who stabbed himself with a sword, to be buried together properly.<p>I think this rounds to “nobody really knows.”<p>The “glorious destiny” seems kind of shaky, too. It’s just a throwaway line anyway.<p>[1] <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Death_of_Cleopatra" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Death_of_Cleopatra</a></div><br/></div></div><div id="40978330" class="c"><input type="checkbox" id="c-40978330" checked=""/><div class="controls bullet"><span class="by">ljsprague</span><span>|</span><a href="#40977917">parent</a><span>|</span><a href="#40978098">prev</a><span>|</span><a href="#40978013">next</a><span>|</span><label class="collapse" for="c-40978330">[-]</label><label class="expand" for="c-40978330">[1 more]</label></div><br/><div class="children"><div class="content">What bothers me more is that the legend is that she was killed by an asp, not a mamba.</div><br/></div></div><div id="40978010" class="c"><input type="checkbox" id="c-40978010" checked=""/><div class="controls bullet"><span class="by">rjurney</span><span>|</span><a href="#40977917">parent</a><span>|</span><a href="#40978013">prev</a><span>|</span><a href="#40979473">next</a><span>|</span><label class="collapse" for="c-40978010">[-]</label><label class="expand" for="c-40978010">[2 more]</label></div><br/><div class="children"><div class="content">I believe this is in dispute among sources.</div><br/></div></div><div id="40979473" class="c"><input type="checkbox" id="c-40979473" checked=""/><div class="controls bullet"><span class="by">dghlsakjg</span><span>|</span><a href="#40977917">parent</a><span>|</span><a href="#40978010">prev</a><span>|</span><a href="#40977949">next</a><span>|</span><label class="collapse" for="c-40979473">[-]</label><label class="expand" for="c-40979473">[1 more]</label></div><br/><div class="children"><div class="content">Maybe Octavian was the snake?</div><br/></div></div></div></div></div></div></div></div></div></body></html>