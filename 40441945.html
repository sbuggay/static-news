<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716454870978" as="style"/><link rel="stylesheet" href="styles.css?v=1716454870978"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://unify.ai/chat?default=true">Show HN: Route your prompts to the best LLM</a> <span class="domain">(<a href="https://unify.ai">unify.ai</a>)</span></div><div class="subtext"><span>danlenton</span> | <span>73 comments</span></div><br/><div><div id="40452266" class="c"><input type="checkbox" id="c-40452266" checked=""/><div class="controls bullet"><span class="by">t_mann</span><span>|</span><a href="#40449633">next</a><span>|</span><label class="collapse" for="c-40452266">[-]</label><label class="expand" for="c-40452266">[1 more]</label></div><br/><div class="children"><div class="content">Sounds interesting, can you explain the conceptual differences between your approach and mixture-of-experts (for someone with minimal understanding of MoE)?</div><br/></div></div><div id="40449633" class="c"><input type="checkbox" id="c-40449633" checked=""/><div class="controls bullet"><span class="by">jefftk</span><span>|</span><a href="#40452266">prev</a><span>|</span><a href="#40451852">next</a><span>|</span><label class="collapse" for="c-40449633">[-]</label><label class="expand" for="c-40449633">[10 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Pricing-wise, we charge the same rates as the backend providers we route to, without taking any margins. We also give $50 in free credits to all new signups.</i><p>What&#x27;s your plan for making money? Are you planning to eventually take a margin?  Negotiate discounts with your backend providers?  Mine the data flowing through your system?</div><br/><div id="40451576" class="c"><input type="checkbox" id="c-40451576" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40449633">parent</a><span>|</span><a href="#40450010">next</a><span>|</span><label class="collapse" for="c-40451576">[-]</label><label class="expand" for="c-40451576">[1 more]</label></div><br/><div class="children"><div class="content">The idea is that at some point in future, we release new and improved router configurations which do take small margins, but from the user perspective they&#x27;re still paying less than using a single endpoint. We don&#x27;t intend to inflate the price when users only use the single-sign-on benefits. Negotiating discounts with backend providers is another possibility, but right now we&#x27;re just focused on providing value.</div><br/></div></div><div id="40450010" class="c"><input type="checkbox" id="c-40450010" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40449633">parent</a><span>|</span><a href="#40451576">prev</a><span>|</span><a href="#40450598">next</a><span>|</span><label class="collapse" for="c-40450010">[-]</label><label class="expand" for="c-40450010">[6 more]</label></div><br/><div class="children"><div class="content">Man, this space would get so much more interesting so quickly if base model providers had a revenue share system in place for routed requests...</div><br/><div id="40450132" class="c"><input type="checkbox" id="c-40450132" checked=""/><div class="controls bullet"><span class="by">runako</span><span>|</span><a href="#40449633">root</a><span>|</span><a href="#40450010">parent</a><span>|</span><a href="#40451586">next</a><span>|</span><label class="collapse" for="c-40450132">[-]</label><label class="expand" for="c-40450132">[4 more]</label></div><br/><div class="children"><div class="content">This would quickly erode confidence in the routers themselves...</div><br/><div id="40450263" class="c"><input type="checkbox" id="c-40450263" checked=""/><div class="controls bullet"><span class="by">loceng</span><span>|</span><a href="#40449633">root</a><span>|</span><a href="#40450132">parent</a><span>|</span><a href="#40451590">next</a><span>|</span><label class="collapse" for="c-40450263">[-]</label><label class="expand" for="c-40450263">[2 more]</label></div><br/><div class="children"><div class="content">Or create a competitive environment between routers?</div><br/><div id="40451610" class="c"><input type="checkbox" id="c-40451610" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40449633">root</a><span>|</span><a href="#40450263">parent</a><span>|</span><a href="#40451590">next</a><span>|</span><label class="collapse" for="c-40451610">[-]</label><label class="expand" for="c-40451610">[1 more]</label></div><br/><div class="children"><div class="content">Another point here is that some users prefer to use their own API keys for the backend providers (a feature we&#x27;re releasing soon). Any &quot;discounts&quot; would then be harder to implement. I do generally think it&#x27;s much cleaner if we route based on the public facing price + performance, so our users don&#x27;t need to lock into our own SSO if they&#x27;d prefer not to.</div><br/></div></div></div></div><div id="40451590" class="c"><input type="checkbox" id="c-40451590" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40449633">root</a><span>|</span><a href="#40450132">parent</a><span>|</span><a href="#40450263">prev</a><span>|</span><a href="#40451586">next</a><span>|</span><label class="collapse" for="c-40451590">[-]</label><label class="expand" for="c-40451590">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s a good point, impartiality would then be questioned</div><br/></div></div></div></div><div id="40451586" class="c"><input type="checkbox" id="c-40451586" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40449633">root</a><span>|</span><a href="#40450010">parent</a><span>|</span><a href="#40450132">prev</a><span>|</span><a href="#40450598">next</a><span>|</span><label class="collapse" for="c-40451586">[-]</label><label class="expand" for="c-40451586">[1 more]</label></div><br/><div class="children"><div class="content">I certainly wouldn&#x27;t complain about this lol</div><br/></div></div></div></div><div id="40450598" class="c"><input type="checkbox" id="c-40450598" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#40449633">parent</a><span>|</span><a href="#40450010">prev</a><span>|</span><a href="#40451852">next</a><span>|</span><label class="collapse" for="c-40450598">[-]</label><label class="expand" for="c-40450598">[2 more]</label></div><br/><div class="children"><div class="content">The data flowing through LLM routers is a hot commodity right now. OpenRouter, for example, even provides a flat-rate 1% discount across the board if you agree to let them use your API calls for model training, and rumor has it that they&#x27;re already profitable. To be fair, they do seem to be collaborating with model providers on some level, so they are likely getting discounted access on top of selling data.</div><br/><div id="40451451" class="c"><input type="checkbox" id="c-40451451" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40449633">root</a><span>|</span><a href="#40450598">parent</a><span>|</span><a href="#40451852">next</a><span>|</span><label class="collapse" for="c-40451451">[-]</label><label class="expand" for="c-40451451">[1 more]</label></div><br/><div class="children"><div class="content">It’s surprising how these app developers are okay with this much data being shown: <a href="https:&#x2F;&#x2F;openrouter.ai&#x2F;models&#x2F;mistralai&#x2F;mixtral-8x7b-instruct?tab=apps" rel="nofollow">https:&#x2F;&#x2F;openrouter.ai&#x2F;models&#x2F;mistralai&#x2F;mixtral-8x7b-instruct...</a></div><br/></div></div></div></div></div></div><div id="40451852" class="c"><input type="checkbox" id="c-40451852" checked=""/><div class="controls bullet"><span class="by">mzl</span><span>|</span><a href="#40449633">prev</a><span>|</span><a href="#40442440">next</a><span>|</span><label class="collapse" for="c-40451852">[-]</label><label class="expand" for="c-40451852">[2 more]</label></div><br/><div class="children"><div class="content">Sounds similar to the Composition of Experts from SambaNova (although that is for increased accuracy on single systems mostly, not for decreased cost from various suppliers): <a href="https:&#x2F;&#x2F;sambanova.ai&#x2F;blog&#x2F;samba-coe-the-power-of-routing-ml-models-at-scale" rel="nofollow">https:&#x2F;&#x2F;sambanova.ai&#x2F;blog&#x2F;samba-coe-the-power-of-routing-ml-...</a></div><br/><div id="40451969" class="c"><input type="checkbox" id="c-40451969" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40451852">parent</a><span>|</span><a href="#40442440">next</a><span>|</span><label class="collapse" for="c-40451969">[-]</label><label class="expand" for="c-40451969">[1 more]</label></div><br/><div class="children"><div class="content">Yep it&#x27;s the same idea. We also see improvements in quality on some benchmarks beyond any specific model. This is common especially common when training a custom router on your own prompt dataset: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0</a></div><br/></div></div></div></div><div id="40442440" class="c"><input type="checkbox" id="c-40442440" checked=""/><div class="controls bullet"><span class="by">lovesdogsnsnow</span><span>|</span><a href="#40451852">prev</a><span>|</span><a href="#40447697">next</a><span>|</span><label class="collapse" for="c-40442440">[-]</label><label class="expand" for="c-40442440">[6 more]</label></div><br/><div class="children"><div class="content">This is interesting! Sort of a super mixture of experts model. What&#x27;s the latency penalty paid with your router in the middle?<p>The pattern I often see is companies prototyping on the most expensive models, then testing smaller&#x2F;faster&#x2F;cheaper models to determine what is actually required for production. For which contexts and products do you foresee your approach being superior?<p>Given you&#x27;re just passing along inference costs from backend providers and aren&#x27;t taking margin, what&#x27;s your long-term plan for profitability?</div><br/><div id="40446698" class="c"><input type="checkbox" id="c-40446698" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40442440">parent</a><span>|</span><a href="#40450997">next</a><span>|</span><label class="collapse" for="c-40446698">[-]</label><label class="expand" for="c-40446698">[3 more]</label></div><br/><div class="children"><div class="content">Great question! Generally the neural network used for the router takes maybe ~20ms during inference. When deployed on prem, in your own cloud environment, then this is the only latecy. When using the public endpoints with our own intermediate server, it might add ~150ms to the time-to-first-token, but inter-token-latency is not affected.<p>We generally see the router being useful when the LLM application is being scaled, and cost and speed start to matter a lot. However, in some cases the output quality actually improved, as we&#x27;re able to squeeze the best of GPT4 and Claude etc.<p>Long-term plan for profitability would come from some future version of the router, where we save the user time and money, and then charge some overhead for the router, but with the user still paying less than they would be with a single endpoint. Hopefully that makes sense?<p>Happy to answer any other questions!</div><br/><div id="40450947" class="c"><input type="checkbox" id="c-40450947" checked=""/><div class="controls bullet"><span class="by">jonahx</span><span>|</span><a href="#40442440">root</a><span>|</span><a href="#40446698">parent</a><span>|</span><a href="#40450997">next</a><span>|</span><label class="collapse" for="c-40450947">[-]</label><label class="expand" for="c-40450947">[2 more]</label></div><br/><div class="children"><div class="content">Do you save the user data, ie, the searches themselves?  What do your TOS guarantee about the use of that data?</div><br/><div id="40452169" class="c"><input type="checkbox" id="c-40452169" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40442440">root</a><span>|</span><a href="#40450947">parent</a><span>|</span><a href="#40450997">next</a><span>|</span><label class="collapse" for="c-40452169">[-]</label><label class="expand" for="c-40452169">[1 more]</label></div><br/><div class="children"><div class="content">We use this data to improve the base router by default. It&#x27;s fully anonymized, and you can opt out.</div><br/></div></div></div></div></div></div><div id="40450997" class="c"><input type="checkbox" id="c-40450997" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40442440">parent</a><span>|</span><a href="#40446698">prev</a><span>|</span><a href="#40447697">next</a><span>|</span><label class="collapse" for="c-40450997">[-]</label><label class="expand" for="c-40450997">[2 more]</label></div><br/><div class="children"><div class="content">If I was doing this I&#x27;d negotiate a volume discount, charge the clients the base rate and pocket the difference.</div><br/><div id="40451713" class="c"><input type="checkbox" id="c-40451713" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40442440">root</a><span>|</span><a href="#40450997">parent</a><span>|</span><a href="#40447697">next</a><span>|</span><label class="collapse" for="c-40451713">[-]</label><label class="expand" for="c-40451713">[1 more]</label></div><br/><div class="children"><div class="content">definitely on the cards, we&#x27;re keeping our options open here. Right now just focused on creating value though.</div><br/></div></div></div></div></div></div><div id="40447697" class="c"><input type="checkbox" id="c-40447697" checked=""/><div class="controls bullet"><span class="by">ianbicking</span><span>|</span><a href="#40442440">prev</a><span>|</span><a href="#40451352">next</a><span>|</span><label class="collapse" for="c-40447697">[-]</label><label class="expand" for="c-40447697">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve bumped into a few of these. I use <a href="https:&#x2F;&#x2F;openrouter.ai" rel="nofollow">https:&#x2F;&#x2F;openrouter.ai</a> as a model abstraction, but not as a router. <a href="https:&#x2F;&#x2F;withmartian.com" rel="nofollow">https:&#x2F;&#x2F;withmartian.com</a> does the same thing but with a more enterprise feel. Also <a href="https:&#x2F;&#x2F;www.braintrustdata.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.braintrustdata.com&#x2F;</a> though it&#x27;s less clear how committed they are to that feature.<p>That said, while I&#x27;ve really enjoyed the LLM abstraction (making it easy for me to test different models without changing my code), I haven&#x27;t felt any desire for a router. I _do_ have some prompts that I send to gpt-3.5-turbo, and could potentially use other models, but it&#x27;s kind of niche.<p>In part this is because I try to do as much in a single prompt as I can, meaning I want to use a model that&#x27;s able to handle the hardest parts of the prompt and then the easy parts come along with. As a result there&#x27;s not many &quot;easy&quot; prompts. The easy prompts are usually text fixup and routing.<p>My &quot;routing&quot; prompts are at a different level of abstraction, usually routing some input or activity to one of several prompts (each of which has its own context, and the sum of all contexts across those prompts is too large, hence the routing). I don&#x27;t know if there&#x27;s some meaningful crossover between these two routing concepts.<p>Another issue I have with LLM portability is the use of tools&#x2F;functions&#x2F;structured output. Opus and Gemini Pro 1.5 have kind of implemented this OK, but until recently GPT was the only halfway decent implementation of this. This seems to be an &quot;advanced&quot; feature, yet it&#x27;s also a feature I use even more with smaller prompts, as those small prompts are often inside some larger algorithm and I don&#x27;t want the fuss of text parsing and exceptions from ad hoc output.<p>But in the end I&#x27;m not price sensitive in my work, so I always come back to the newest GPT model. If I make a switch to Opus it definitely won&#x27;t be to save money! And I&#x27;m probably not going to want to fiddle, but instead make a thoughtful choice and switch the default model in my code.</div><br/><div id="40448602" class="c"><input type="checkbox" id="c-40448602" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447697">parent</a><span>|</span><a href="#40449294">next</a><span>|</span><label class="collapse" for="c-40448602">[-]</label><label class="expand" for="c-40448602">[1 more]</label></div><br/><div class="children"><div class="content">Super helpful feedback, thanks for going so deep! I agree that for the really heavy agentic stuff, the router in it&#x27;s current form might not be the most important innovation.<p>However, for several use cases speed is really paramount, and can directly hinder the UX. Examples include sales call agents, copilots, auto-complete engines etc. These are some of the areas where we&#x27;ve seen the router really shine, diverting to slow models when absolutely necesary on complex prompts, but using fast models as often as possible to minimize disruption to the UX.<p>Having said that, another major benefit of the platform is the ability to quickly run objective benchmarks for quality, cost and speed across all models and providers, on your own prompts [<a href="https:&#x2F;&#x2F;youtu.be&#x2F;PO4r6ek8U6M" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;PO4r6ek8U6M</a>]. We have some users who run benchmarks regularly for different checkpoints of their fine-tuned model, comparing against all other custom fine-tuned models, as well as the various foundation models.<p>As for the overlap in routing concepts you mentioned, I&#x27;ve thought a lot about this actually. It&#x27;s our intention to broaden the kinds of routing we&#x27;re able to handle, where we assume all control flow decision (routing) and intermediate prompts are latent variables (DSPy perspective). In the immediate future there is not crossover though.<p>I agree cost is often an afterthought. Generally our users either care about improving speed, or they want to know which model or combination of models would be best for their task in terms of output quality (GPT-4, Opus, Gemini? etc.). This is not trivial to guage without performing benchmarks.<p>As for usually wanting to make a full LLM switch as opposed to routing, what&#x27;s the primary motivation? Avoiding extra complexity + dependencies in the stack? Perhaps worrying about model-specific prompts no longer working well with a new model? The general loss of control?</div><br/></div></div><div id="40448024" class="c"><input type="checkbox" id="c-40448024" checked=""/><div class="controls bullet"><span class="by">ankrgyl</span><span>|</span><a href="#40447697">parent</a><span>|</span><a href="#40449294">prev</a><span>|</span><a href="#40451352">next</a><span>|</span><label class="collapse" for="c-40448024">[-]</label><label class="expand" for="c-40448024">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for mentioning Braintrust!<p>We are very committed to the proxy :)<p>Although, to your point, we have seen less market pull for routing, and more for (a) supporting the latest LLMs, (b) basic translation (e.g. tool call API b&#x2F;w Anthropic &amp; OpenAI), and (c) solid infra features like caching&#x2F;load balancing api keys&#x2F;secret management. So that&#x27;s our focus.</div><br/></div></div></div></div><div id="40451352" class="c"><input type="checkbox" id="c-40451352" checked=""/><div class="controls bullet"><span class="by">_pdp_</span><span>|</span><a href="#40447697">prev</a><span>|</span><a href="#40449956">next</a><span>|</span><label class="collapse" for="c-40451352">[-]</label><label class="expand" for="c-40451352">[2 more]</label></div><br/><div class="children"><div class="content">I think everyone in this space soon or later invest a router. At CBK we did too. I am not sure if this is a big problem to solve but I am thinking we need to open finish our Open Relay soon.</div><br/><div id="40451627" class="c"><input type="checkbox" id="c-40451627" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40451352">parent</a><span>|</span><a href="#40449956">next</a><span>|</span><label class="collapse" for="c-40451627">[-]</label><label class="expand" for="c-40451627">[1 more]</label></div><br/><div class="children"><div class="content">Agreed, we&#x27;ve spoken to tons of users who reach out to us and start the conversation with &quot;we&#x27;ve tried to implement this ourselves&quot;.</div><br/></div></div></div></div><div id="40449956" class="c"><input type="checkbox" id="c-40449956" checked=""/><div class="controls bullet"><span class="by">aleksiy123</span><span>|</span><a href="#40451352">prev</a><span>|</span><a href="#40447392">next</a><span>|</span><label class="collapse" for="c-40449956">[-]</label><label class="expand" for="c-40449956">[4 more]</label></div><br/><div class="children"><div class="content">I wonder, have you also done any work on actually improving performance by enhancing the prompt or something similar?<p>I always thought a product like this that could empirically decrease costs for the same performance or increase performance for a small increase in cost would have a  fairly simple road of justifying its existence.</div><br/><div id="40451641" class="c"><input type="checkbox" id="c-40451641" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40449956">parent</a><span>|</span><a href="#40450043">next</a><span>|</span><label class="collapse" for="c-40451641">[-]</label><label class="expand" for="c-40451641">[1 more]</label></div><br/><div class="children"><div class="content">As outlined, DSPy is a create tool for this. Currently their focus is on optimizing in-context examples, but their broader vision includes optimising system prompts for intermediate LLM nodes in an agentic system as well. We uploaded an explainder video about DSPy last week! Feel free to check it out:
<a href="https:&#x2F;&#x2F;youtu.be&#x2F;kFB8kFchCH4?si=0YKAVcpKJjogP1sX" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;kFB8kFchCH4?si=0YKAVcpKJjogP1sX</a></div><br/></div></div><div id="40450043" class="c"><input type="checkbox" id="c-40450043" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#40449956">parent</a><span>|</span><a href="#40451641">prev</a><span>|</span><a href="#40447392">next</a><span>|</span><label class="collapse" for="c-40450043">[-]</label><label class="expand" for="c-40450043">[2 more]</label></div><br/><div class="children"><div class="content">I think dspy aims to do this: <a href="https:&#x2F;&#x2F;github.com&#x2F;stanfordnlp&#x2F;dspy">https:&#x2F;&#x2F;github.com&#x2F;stanfordnlp&#x2F;dspy</a></div><br/><div id="40450055" class="c"><input type="checkbox" id="c-40450055" checked=""/><div class="controls bullet"><span class="by">aleksiy123</span><span>|</span><a href="#40449956">root</a><span>|</span><a href="#40450043">parent</a><span>|</span><a href="#40447392">next</a><span>|</span><label class="collapse" for="c-40450055">[-]</label><label class="expand" for="c-40450055">[1 more]</label></div><br/><div class="children"><div class="content">Cool thanks for the link very interesting.</div><br/></div></div></div></div></div></div><div id="40447392" class="c"><input type="checkbox" id="c-40447392" checked=""/><div class="controls bullet"><span class="by">swiftlyTyped</span><span>|</span><a href="#40449956">prev</a><span>|</span><a href="#40449984">next</a><span>|</span><label class="collapse" for="c-40447392">[-]</label><label class="expand" for="c-40447392">[2 more]</label></div><br/><div class="children"><div class="content">Really great stuff.<p>People use the same model &#x2F; server for all queries not because it&#x27;s sensible, but because it&#x27;s simple. This brings the same simplicity to the far more optimal solution.<p>And great startup play too, by definition no incumbent can fill this role.</div><br/><div id="40447478" class="c"><input type="checkbox" id="c-40447478" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447392">parent</a><span>|</span><a href="#40449984">next</a><span>|</span><label class="collapse" for="c-40447478">[-]</label><label class="expand" for="c-40447478">[1 more]</label></div><br/><div class="children"><div class="content">Thanks - glad to hear the idea resonates!</div><br/></div></div></div></div><div id="40449984" class="c"><input type="checkbox" id="c-40449984" checked=""/><div class="controls bullet"><span class="by">ceroxylon</span><span>|</span><a href="#40447392">prev</a><span>|</span><a href="#40449863">next</a><span>|</span><label class="collapse" for="c-40449984">[-]</label><label class="expand" for="c-40449984">[2 more]</label></div><br/><div class="children"><div class="content">For those who like testing smaller local models, there is a tool in LangChain called Routing and in LlamaIndex called a &quot;router query engine&quot; which achieve a similar thing:<p><a href="https:&#x2F;&#x2F;python.langchain.com&#x2F;v0.1&#x2F;docs&#x2F;use_cases&#x2F;query_analysis&#x2F;techniques&#x2F;routing&#x2F;" rel="nofollow">https:&#x2F;&#x2F;python.langchain.com&#x2F;v0.1&#x2F;docs&#x2F;use_cases&#x2F;query_analy...</a><p><a href="https:&#x2F;&#x2F;docs.llamaindex.ai&#x2F;en&#x2F;stable&#x2F;examples&#x2F;query_engine&#x2F;RouterQueryEngine&#x2F;" rel="nofollow">https:&#x2F;&#x2F;docs.llamaindex.ai&#x2F;en&#x2F;stable&#x2F;examples&#x2F;query_engine&#x2F;R...</a></div><br/><div id="40451664" class="c"><input type="checkbox" id="c-40451664" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40449984">parent</a><span>|</span><a href="#40449863">next</a><span>|</span><label class="collapse" for="c-40451664">[-]</label><label class="expand" for="c-40451664">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing! These are useful toos, but they are a bit different, more based on similarity search in prompt space (a bit like semantic router: <a href="https:&#x2F;&#x2F;github.com&#x2F;aurelio-labs&#x2F;semantic-router">https:&#x2F;&#x2F;github.com&#x2F;aurelio-labs&#x2F;semantic-router</a>). Our router uses a neural network for the routing decisions, and it can be trained on your own prompts [<a href="https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0</a>]. We&#x27;re also adding support for on-prem deployment soon!</div><br/></div></div></div></div><div id="40449863" class="c"><input type="checkbox" id="c-40449863" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#40449984">prev</a><span>|</span><a href="#40447723">next</a><span>|</span><label class="collapse" for="c-40449863">[-]</label><label class="expand" for="c-40449863">[2 more]</label></div><br/><div class="children"><div class="content">This is super cool! I wonder if you could do a similar thing, but choosing between a collection of prompts for a task based on the input. Similar to dynamic few-shot prompting, but replacing the entire prompt instead of just the examples.</div><br/><div id="40451880" class="c"><input type="checkbox" id="c-40451880" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40449863">parent</a><span>|</span><a href="#40447723">next</a><span>|</span><label class="collapse" for="c-40451880">[-]</label><label class="expand" for="c-40451880">[1 more]</label></div><br/><div class="children"><div class="content">I agree this is an interesting direction, I think this is on the roadmap for DSPy [<a href="https:&#x2F;&#x2F;github.com&#x2F;stanfordnlp&#x2F;dspy">https:&#x2F;&#x2F;github.com&#x2F;stanfordnlp&#x2F;dspy</a>], but right now they mainly focus on optimizing the in-context examples.</div><br/></div></div></div></div><div id="40447723" class="c"><input type="checkbox" id="c-40447723" checked=""/><div class="controls bullet"><span class="by">thomasfromcdnjs</span><span>|</span><a href="#40449863">prev</a><span>|</span><a href="#40447448">next</a><span>|</span><label class="collapse" for="c-40447723">[-]</label><label class="expand" for="c-40447723">[4 more]</label></div><br/><div class="children"><div class="content">Since you take no extra charges from the end user, the presumption is you would charge LLM&#x27;s for giving them traffic?</div><br/><div id="40447864" class="c"><input type="checkbox" id="c-40447864" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447723">parent</a><span>|</span><a href="#40447448">next</a><span>|</span><label class="collapse" for="c-40447864">[-]</label><label class="expand" for="c-40447864">[3 more]</label></div><br/><div class="children"><div class="content">we intend to eventually have routers which improve the speed, cost (and maybe quality) to such an extent that we can then take some margins from these best performing routers, with users still saving costs compared to individual endpoints. For now we don&#x27;t take any margins though, we want to ensure we&#x27;re providing sufficient value before rushing to capture any of it.</div><br/><div id="40451095" class="c"><input type="checkbox" id="c-40451095" checked=""/><div class="controls bullet"><span class="by">t-writescode</span><span>|</span><a href="#40447723">root</a><span>|</span><a href="#40447864">parent</a><span>|</span><a href="#40447448">next</a><span>|</span><label class="collapse" for="c-40451095">[-]</label><label class="expand" for="c-40451095">[2 more]</label></div><br/><div class="children"><div class="content">Are you ready to eat the $50 per new user (and all the fake accounts that could get created this way to eat more and more $50) all the way until you create this next tier, &quot;for-profit&quot; step?</div><br/><div id="40451793" class="c"><input type="checkbox" id="c-40451793" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447723">root</a><span>|</span><a href="#40451095">parent</a><span>|</span><a href="#40447448">next</a><span>|</span><label class="collapse" for="c-40451793">[-]</label><label class="expand" for="c-40451793">[1 more]</label></div><br/><div class="children"><div class="content">yeah we might need to implemenet some kind of &quot;I am not a robot&quot; checks soon, as well as 2FA.</div><br/></div></div></div></div></div></div></div></div><div id="40447448" class="c"><input type="checkbox" id="c-40447448" checked=""/><div class="controls bullet"><span class="by">siquick</span><span>|</span><a href="#40447723">prev</a><span>|</span><a href="#40447209">next</a><span>|</span><label class="collapse" for="c-40447448">[-]</label><label class="expand" for="c-40447448">[2 more]</label></div><br/><div class="children"><div class="content">Very cool, going to try on some of our workflows this week.<p>Would love to see web access and RAG (LlamaIndex) integration. Are they on the roadmap?</div><br/><div id="40447495" class="c"><input type="checkbox" id="c-40447495" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447448">parent</a><span>|</span><a href="#40447209">next</a><span>|</span><label class="collapse" for="c-40447495">[-]</label><label class="expand" for="c-40447495">[1 more]</label></div><br/><div class="children"><div class="content">merged yesterday!
<a href="https:&#x2F;&#x2F;github.com&#x2F;run-llama&#x2F;llama_index&#x2F;pull&#x2F;12921">https:&#x2F;&#x2F;github.com&#x2F;run-llama&#x2F;llama_index&#x2F;pull&#x2F;12921</a></div><br/></div></div></div></div><div id="40447209" class="c"><input type="checkbox" id="c-40447209" checked=""/><div class="controls bullet"><span class="by">joaquincabezas</span><span>|</span><a href="#40447448">prev</a><span>|</span><a href="#40447357">next</a><span>|</span><label class="collapse" for="c-40447209">[-]</label><label class="expand" for="c-40447209">[2 more]</label></div><br/><div class="children"><div class="content">I’ve already heard a few times that the selection of models is seen as a problem (need to benchmark continually, justify changes…), this is an elegant solution.<p>I don’t know if choosing different models for the same consumer can be problematic (seen as not consistent), but maybe using this approach will force the post-processing code not to be “coupled” with one particular model.</div><br/><div id="40448274" class="c"><input type="checkbox" id="c-40448274" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447209">parent</a><span>|</span><a href="#40447357">next</a><span>|</span><label class="collapse" for="c-40448274">[-]</label><label class="expand" for="c-40448274">[1 more]</label></div><br/><div class="children"><div class="content">This is a great point. With models becoming more intelligent, they&#x27;re seeming to become less brittle to the subtleties in the prompts, which might mean decoupling will occur naturally anyway. With regards to customers wanting to stick with &quot;one model&quot;, we more often find that they want to stick with &quot;one cloud environment&quot;. This is not a problem, the router can be deployed inside custom cloud environments easily. Interesting to see how much auto-prompting such as DSPy generalizes across models though, when you really start to see intermediate prompts as latent variables. Not sure how much these learned prompts might behave differently with different LLMs. I guess you might need to jointly learn the prompts and the routing decisions together for it to work really well. One step at a time though!</div><br/></div></div></div></div><div id="40447357" class="c"><input type="checkbox" id="c-40447357" checked=""/><div class="controls bullet"><span class="by">tyrw</span><span>|</span><a href="#40447209">prev</a><span>|</span><a href="#40450539">next</a><span>|</span><label class="collapse" for="c-40447357">[-]</label><label class="expand" for="c-40447357">[3 more]</label></div><br/><div class="children"><div class="content">It feels like there is an analogy here with Yahoo! and the early days of going from curated lists of websites to search algorithms. Do you think of LLMs in a similar way? I.e. some kind of model ranking score that companies could eventually game?<p>I&#x27;m not sure what the SEO equivalent would be here...</div><br/><div id="40447793" class="c"><input type="checkbox" id="c-40447793" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447357">parent</a><span>|</span><a href="#40447722">next</a><span>|</span><label class="collapse" for="c-40447793">[-]</label><label class="expand" for="c-40447793">[1 more]</label></div><br/><div class="children"><div class="content">Great analogy, I&#x27;m not sure tbh. I don&#x27;t think we will see quite as many unique models as we see unique websites, but I do think we&#x27;re going to see an increasing number of divergent and specialized models, which lend themselves to routing. I guess the SEO analogy would be &quot;tricking&quot; the neural routing function into thinking your LLM is better than it actually is, there are many techniques already to hone in on neural net weaknesses. Definitely interested to see how the space evolves!</div><br/></div></div><div id="40447722" class="c"><input type="checkbox" id="c-40447722" checked=""/><div class="controls bullet"><span class="by">darreninthenet</span><span>|</span><a href="#40447357">parent</a><span>|</span><a href="#40447793">prev</a><span>|</span><a href="#40450539">next</a><span>|</span><label class="collapse" for="c-40447722">[-]</label><label class="expand" for="c-40447722">[1 more]</label></div><br/><div class="children"><div class="content">LHR<p>(Least Hallucinated Response)</div><br/></div></div></div></div><div id="40450539" class="c"><input type="checkbox" id="c-40450539" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40447357">prev</a><span>|</span><a href="#40450415">next</a><span>|</span><label class="collapse" for="c-40450539">[-]</label><label class="expand" for="c-40450539">[2 more]</label></div><br/><div class="children"><div class="content">yeah this is very nice, open source Martian. i never really understood the value of routing all the time. you want stability and predictability in models. and models have huge brand value. you&#x27;re never going to, through routing, construct a &quot;super&quot; model that people want more than one or a few really good brand name models.</div><br/><div id="40452198" class="c"><input type="checkbox" id="c-40452198" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40450539">parent</a><span>|</span><a href="#40450415">next</a><span>|</span><label class="collapse" for="c-40452198">[-]</label><label class="expand" for="c-40452198">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I agree, routing all the time is overkill for some applications, however for others it really is necessary in order to reach the necessary speed and performance for users of your real-time app.<p>For those who don&#x27;t want to always route, another core benefit of our platform is simple custom benchmarking on your task across all existing providers:
<a href="https:&#x2F;&#x2F;youtu.be&#x2F;PO4r6ek8U6M" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;PO4r6ek8U6M</a><p>If you then just want to use that provider rather than a router config, then that&#x27;s fine!</div><br/></div></div></div></div><div id="40450415" class="c"><input type="checkbox" id="c-40450415" checked=""/><div class="controls bullet"><span class="by">Sontho</span><span>|</span><a href="#40450539">prev</a><span>|</span><a href="#40447747">next</a><span>|</span><label class="collapse" for="c-40450415">[-]</label><label class="expand" for="c-40450415">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a good options to test local models I guess.</div><br/><div id="40452249" class="c"><input type="checkbox" id="c-40452249" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40450415">parent</a><span>|</span><a href="#40447747">next</a><span>|</span><label class="collapse" for="c-40452249">[-]</label><label class="expand" for="c-40452249">[1 more]</label></div><br/><div class="children"><div class="content">actually we don&#x27;t support local deployment quite yet, it&#x27;s all run in our server. However, fully local deployment is on the roadmap, stay tuned!</div><br/></div></div></div></div><div id="40447747" class="c"><input type="checkbox" id="c-40447747" checked=""/><div class="controls bullet"><span class="by">iknownthing</span><span>|</span><a href="#40450415">prev</a><span>|</span><a href="#40448757">next</a><span>|</span><label class="collapse" for="c-40447747">[-]</label><label class="expand" for="c-40447747">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It is trained in a supervised manner on several open LLM datasets, using GPT4 as a judge.<p>Does this mean GPT4 predictions are used as labels? Is that allowed?</div><br/><div id="40447825" class="c"><input type="checkbox" id="c-40447825" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447747">parent</a><span>|</span><a href="#40448757">next</a><span>|</span><label class="collapse" for="c-40447825">[-]</label><label class="expand" for="c-40447825">[3 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not used as labels directly (we&#x27;re not trainig an LLM which outputs text), they are used as an intermediate step, which is then used to compute a simple score which the neural score function is then trained on. The neural score function takes prompts and latent model representations in, and produces a score from 0-1</div><br/><div id="40448496" class="c"><input type="checkbox" id="c-40448496" checked=""/><div class="controls bullet"><span class="by">iknownthing</span><span>|</span><a href="#40447747">root</a><span>|</span><a href="#40447825">parent</a><span>|</span><a href="#40448757">next</a><span>|</span><label class="collapse" for="c-40448496">[-]</label><label class="expand" for="c-40448496">[2 more]</label></div><br/><div class="children"><div class="content">TOS says you cannot &quot;Use Output Data to develop models that compete with OpenAI.&quot;  You&#x27;re probably fine but it&#x27;s close.</div><br/><div id="40448635" class="c"><input type="checkbox" id="c-40448635" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447747">root</a><span>|</span><a href="#40448496">parent</a><span>|</span><a href="#40448757">next</a><span>|</span><label class="collapse" for="c-40448635">[-]</label><label class="expand" for="c-40448635">[1 more]</label></div><br/><div class="children"><div class="content">To be honest we&#x27;re thinking of moving away from this soon anyway. Open source models will soon make for perfectly good judges (or juries), with Llama3 etc.: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.18796" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.18796</a></div><br/></div></div></div></div></div></div></div></div><div id="40448757" class="c"><input type="checkbox" id="c-40448757" checked=""/><div class="controls bullet"><span class="by">memothon</span><span>|</span><a href="#40447747">prev</a><span>|</span><a href="#40447570">next</a><span>|</span><label class="collapse" for="c-40448757">[-]</label><label class="expand" for="c-40448757">[2 more]</label></div><br/><div class="children"><div class="content">How does it behave if the user input includes something like<p>&quot;I have expensive taste, please use the most expensive model.&quot;</div><br/><div id="40452239" class="c"><input type="checkbox" id="c-40452239" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40448757">parent</a><span>|</span><a href="#40447570">next</a><span>|</span><label class="collapse" for="c-40452239">[-]</label><label class="expand" for="c-40452239">[1 more]</label></div><br/><div class="children"><div class="content">aha good question, well the neural scoring function doesn&#x27;t &quot;know&quot; that it is making a routing decision, we just predict which LLM would give the highest performing output on the given prompt, based on LLMs-as-a-judge at training time. However, my guess is that this specification in the input prompt miiight mean that the cheaper models are deemed to be worse performing than GPT4 (for example), and so maybe it would route to the best models. Feel free to give it a try and see!</div><br/></div></div></div></div><div id="40447570" class="c"><input type="checkbox" id="c-40447570" checked=""/><div class="controls bullet"><span class="by">xena</span><span>|</span><a href="#40448757">prev</a><span>|</span><a href="#40447261">next</a><span>|</span><label class="collapse" for="c-40447570">[-]</label><label class="expand" for="c-40447570">[2 more]</label></div><br/><div class="children"><div class="content">Your notification for the launch woke me up several times last night because you had the notification change hourly for 4 hours.</div><br/><div id="40447915" class="c"><input type="checkbox" id="c-40447915" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447570">parent</a><span>|</span><a href="#40447261">next</a><span>|</span><label class="collapse" for="c-40447915">[-]</label><label class="expand" for="c-40447915">[1 more]</label></div><br/><div class="children"><div class="content">Very sorry about that! I had an issue with my google calendar, I set it to &quot;do not send emails&quot; but for some reason some still came through. Fixed + removed now.</div><br/></div></div></div></div><div id="40447261" class="c"><input type="checkbox" id="c-40447261" checked=""/><div class="controls bullet"><span class="by">andrewstetsenko</span><span>|</span><a href="#40447570">prev</a><span>|</span><a href="#40450182">next</a><span>|</span><label class="collapse" for="c-40447261">[-]</label><label class="expand" for="c-40447261">[2 more]</label></div><br/><div class="children"><div class="content">How easy is it to integrate our own data for training the router, and what kind of improvements can we expect from this customization?</div><br/><div id="40447884" class="c"><input type="checkbox" id="c-40447884" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447261">parent</a><span>|</span><a href="#40450182">next</a><span>|</span><label class="collapse" for="c-40447884">[-]</label><label class="expand" for="c-40447884">[1 more]</label></div><br/><div class="children"><div class="content">it depends on the task, but this video gives an idea :)
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9JYqNbIEac0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9JYqNbIEac0</a></div><br/></div></div></div></div><div id="40450182" class="c"><input type="checkbox" id="c-40450182" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#40447261">prev</a><span>|</span><a href="#40451072">next</a><span>|</span><label class="collapse" for="c-40450182">[-]</label><label class="expand" for="c-40450182">[2 more]</label></div><br/><div class="children"><div class="content">Surprised Gemini Flash isn&#x27;t included</div><br/><div id="40451847" class="c"><input type="checkbox" id="c-40451847" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40450182">parent</a><span>|</span><a href="#40451072">next</a><span>|</span><label class="collapse" for="c-40451847">[-]</label><label class="expand" for="c-40451847">[1 more]</label></div><br/><div class="children"><div class="content">will be soon!</div><br/></div></div></div></div><div id="40451072" class="c"><input type="checkbox" id="c-40451072" checked=""/><div class="controls bullet"><span class="by">lhousa</span><span>|</span><a href="#40450182">prev</a><span>|</span><a href="#40447161">next</a><span>|</span><label class="collapse" for="c-40451072">[-]</label><label class="expand" for="c-40451072">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always something that &quot;unifies&quot; stuff. Be it cabs or food or services, etc. There&#x27;s potential for this.</div><br/><div id="40451843" class="c"><input type="checkbox" id="c-40451843" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40451072">parent</a><span>|</span><a href="#40447161">next</a><span>|</span><label class="collapse" for="c-40451843">[-]</label><label class="expand" for="c-40451843">[1 more]</label></div><br/><div class="children"><div class="content">aha maybe we should change our slogan to &quot;Uber for LLMs&quot;</div><br/></div></div></div></div><div id="40447161" class="c"><input type="checkbox" id="c-40447161" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40451072">prev</a><span>|</span><a href="#40448160">next</a><span>|</span><label class="collapse" for="c-40447161">[-]</label><label class="expand" for="c-40447161">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this what a MoE LLM does already?</div><br/><div id="40447319" class="c"><input type="checkbox" id="c-40447319" checked=""/><div class="controls bullet"><span class="by">sodality2</span><span>|</span><a href="#40447161">parent</a><span>|</span><a href="#40447949">next</a><span>|</span><label class="collapse" for="c-40447319">[-]</label><label class="expand" for="c-40447319">[2 more]</label></div><br/><div class="children"><div class="content">MoE-based models are one model with multiple experts. This solution could use entirely different models with different architectures (and probably supports MoE models itself)</div><br/><div id="40448348" class="c"><input type="checkbox" id="c-40448348" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447161">root</a><span>|</span><a href="#40447319">parent</a><span>|</span><a href="#40447949">next</a><span>|</span><label class="collapse" for="c-40448348">[-]</label><label class="expand" for="c-40448348">[1 more]</label></div><br/><div class="children"><div class="content">exactly!</div><br/></div></div></div></div><div id="40447949" class="c"><input type="checkbox" id="c-40447949" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40447161">parent</a><span>|</span><a href="#40447319">prev</a><span>|</span><a href="#40448160">next</a><span>|</span><label class="collapse" for="c-40447949">[-]</label><label class="expand" for="c-40447949">[1 more]</label></div><br/><div class="children"><div class="content">MoE LLMs use several &quot;expert&quot; fully connected layers, which are routed to during the forward pass, all trained end-to-end. This approach can also work with black-box LLMs like Opus, GPT4 etc. It&#x27;s a similar concept but operating at a higher level of abstraction.</div><br/></div></div></div></div><div id="40448160" class="c"><input type="checkbox" id="c-40448160" checked=""/><div class="controls bullet"><span class="by">Alifatisk</span><span>|</span><a href="#40447161">prev</a><span>|</span><a href="#40450249">next</a><span>|</span><label class="collapse" for="c-40448160">[-]</label><label class="expand" for="c-40448160">[2 more]</label></div><br/><div class="children"><div class="content">Is this like openrouter ai?</div><br/><div id="40448329" class="c"><input type="checkbox" id="c-40448329" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40448160">parent</a><span>|</span><a href="#40450249">next</a><span>|</span><label class="collapse" for="c-40448329">[-]</label><label class="expand" for="c-40448329">[1 more]</label></div><br/><div class="children"><div class="content">definitely similar! I&#x27;m a fan of Alex and his work on OpenRouter :)<p>Some of the main differences would be:
- we focus on performance based routing, optimizing speed, cost and quality [<a href="https:&#x2F;&#x2F;youtu.be&#x2F;ZpY6SIkBosE" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;ZpY6SIkBosE</a>]
- we enable custom benchmarks on custom prompts, across all models + providers [<a href="https:&#x2F;&#x2F;youtu.be&#x2F;PO4r6ek8U6M" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;PO4r6ek8U6M</a>]
- we enable custom routers to be trained on custom data [<a href="https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0</a>]<p>Our users often already have LLM apps deployed, and are then looking to take better control of their performance profile, perhaps increasing speed to improve user experience, or improving response quality via clear benchmarking across all models and providers on their particular prompts.<p>So they are similar, but solving slightly different problems I&#x27;d say.</div><br/></div></div></div></div><div id="40450249" class="c"><input type="checkbox" id="c-40450249" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40448160">prev</a><span>|</span><label class="collapse" for="c-40450249">[-]</label><label class="expand" for="c-40450249">[2 more]</label></div><br/><div class="children"><div class="content">&gt; dynamic router for LLMs<p>This is sort of how Mixture-of-Experts models work, actually.</div><br/><div id="40451828" class="c"><input type="checkbox" id="c-40451828" checked=""/><div class="controls bullet"><span class="by">danlenton</span><span>|</span><a href="#40450249">parent</a><span>|</span><label class="collapse" for="c-40451828">[-]</label><label class="expand" for="c-40451828">[1 more]</label></div><br/><div class="children"><div class="content">Yep! Although MoE use several &quot;expert&quot; linear layers within a single network, and generally the &quot;routing&quot; is not based on high-level semantics, but token specialization, as disuccussed by Fuzhao Xue, author of OpenMoE, in one of our reading groups: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=k3QOpJA0A0Q&amp;t=1547s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=k3QOpJA0A0Q&amp;t=1547s</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>