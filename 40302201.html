<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715245271374" as="style"/><link rel="stylesheet" href="styles.css?v=1715245271374"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://hao-ai-lab.github.io/blogs/cllm/">Consistency LLM: converting LLMs to parallel decoders accelerates inference 3.5x</a> <span class="domain">(<a href="https://hao-ai-lab.github.io">hao-ai-lab.github.io</a>)</span></div><div class="subtext"><span>zhisbug</span> | <span>73 comments</span></div><br/><div><div id="40303428" class="c"><input type="checkbox" id="c-40303428" checked=""/><div class="controls bullet"><span class="by">DoctorOetker</span><span>|</span><a href="#40305627">next</a><span>|</span><label class="collapse" for="c-40303428">[-]</label><label class="expand" for="c-40303428">[10 more]</label></div><br/><div class="children"><div class="content">This mirrors what I experienced when I enrolled in &quot;free drawing&quot; (no teaching) classes:<p>While people considered me a good drawer since I was a child, I remember just repeating either similar detailed drawings I drew before, or otherwise just taking plenty of time to draw. I believe anyone with time and patience can make a nice drawing of a scene.<p>The &quot;free drawing&quot; class had no rules or lectures: you brought the materials you wanted to work with (some brought ink, others pencils, while I brought charcoal). The only thing determined was the timing between poses for the model: for each session the first few poses were very short (say a minute), and then the pose durations would progressively lengthen until say 5 minute poses. At all times you were free to tear your picture up and retry drawing the pose again.<p>My drawing skills improved considerably. The short &quot;warmups&quot; actually force you to get proportions and outlines correct on the first tries. Conventional wisdom says haste makes waste, but when learning or refining skills, it seems natural selection has hardcoded the sensation of haste as a stressor prompting attention and learning.<p>I am convinced I could have drawn similar quality drawings before enrolling in those classes, except they would have taken me easily 5 or 10 x as long to draw. Being forced not to beat around the bush and feeling the penalty of making a hasty mistake (further decreasing time left for the second try in the remaining time) does seem to work.<p>My only gripe is that the technique is termed &quot;Consistency&quot; whereas I would reserve such a term for an improvement in <i>performance</i> not inference speed, although I understand that they indicate &quot;consistency with what would ultimately have been generated one token at a time&quot;. I would rather dub it &quot;Proficiency LLM&quot;, where the same output is expected, only without the inhibition of stuttering to the same conclusion.</div><br/><div id="40303790" class="c"><input type="checkbox" id="c-40303790" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40303428">parent</a><span>|</span><a href="#40303791">next</a><span>|</span><label class="collapse" for="c-40303790">[-]</label><label class="expand" for="c-40303790">[5 more]</label></div><br/><div class="children"><div class="content">Hi we are CLLM authors and thanks for sharing your experience and insights! I can see this drawing skill refining process echoes with the training process in CLLM, the only thing is at this point stressor in CLLM training is not getting progressively demanding.<p>For example, while drawing, you can set very specific time limit on how long you are allowed to draw in each trial and make the time progressively shorter. In CLLM, maybe we can make this the learning process more and more difficult by mapping more and more distant states in Jacobi trajectory to its final state.<p>We are using the term &quot;consistency&quot; because we draw parallelism between consistency LLM and the consistency model in diffusion image generation where the training processes are analogous.</div><br/><div id="40304826" class="c"><input type="checkbox" id="c-40304826" checked=""/><div class="controls bullet"><span class="by">boroboro4</span><span>|</span><a href="#40303428">root</a><span>|</span><a href="#40303790">parent</a><span>|</span><a href="#40303877">next</a><span>|</span><label class="collapse" for="c-40304826">[-]</label><label class="expand" for="c-40304826">[2 more]</label></div><br/><div class="children"><div class="content">Do you use same dataset to train &#x2F; eval the model? Was the model used for example trained on GSM8K dataset for example?</div><br/><div id="40304853" class="c"><input type="checkbox" id="c-40304853" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40303428">root</a><span>|</span><a href="#40304826">parent</a><span>|</span><a href="#40303877">next</a><span>|</span><label class="collapse" for="c-40304853">[-]</label><label class="expand" for="c-40304853">[1 more]</label></div><br/><div class="children"><div class="content">Yes, we consider both domain-specific applications (spider for text2SQL, gsm8k for math, codesearchnet for python) as well as open-domain conversational applications (ShareGPT). We use test set from each application to evaluate CLLMs’ performance in our paper.<p>On the other hand, technically CLLM works on any kind of queries. But the speedup might vary. Feel free to try out our codebase for your use cases!</div><br/></div></div></div></div><div id="40303877" class="c"><input type="checkbox" id="c-40303877" checked=""/><div class="controls bullet"><span class="by">Quarrel</span><span>|</span><a href="#40303428">root</a><span>|</span><a href="#40303790">parent</a><span>|</span><a href="#40304826">prev</a><span>|</span><a href="#40303791">next</a><span>|</span><label class="collapse" for="c-40303877">[-]</label><label class="expand" for="c-40303877">[2 more]</label></div><br/><div class="children"><div class="content">Is it just me, or does this read like it was written by an LLM ... ?!</div><br/><div id="40303931" class="c"><input type="checkbox" id="c-40303931" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40303428">root</a><span>|</span><a href="#40303877">parent</a><span>|</span><a href="#40303791">next</a><span>|</span><label class="collapse" for="c-40303931">[-]</label><label class="expand" for="c-40303931">[1 more]</label></div><br/><div class="children"><div class="content">lol I take that as a compliment. Good try but sadly no LLM in this writing :)</div><br/></div></div></div></div></div></div><div id="40303791" class="c"><input type="checkbox" id="c-40303791" checked=""/><div class="controls bullet"><span class="by">aamargulies</span><span>|</span><a href="#40303428">parent</a><span>|</span><a href="#40303790">prev</a><span>|</span><a href="#40303506">next</a><span>|</span><label class="collapse" for="c-40303791">[-]</label><label class="expand" for="c-40303791">[2 more]</label></div><br/><div class="children"><div class="content">I had an interesting experience in an Invertebrate Zoology lab class one summer.<p>We students were brought into a lab, given specimens to draw, and the only instructions we received were &#x27;You have 30 minutes to draw this. Go.&#x27;<p>There was no &quot;here&#x27;s how to draw. here&#x27;s what to do and not to do&quot;. It was just basically &quot;We don&#x27;t care about any insecurities you might have. We don&#x27;t care if you think you can&#x27;t draw. No excuses, just fucking draw it. Now.&quot;<p>Not only did we draw, but we (all of us) improved enormously over the course of the class as more animals were brought in and the exercise was repeated over and over and over again throughout the summer.<p>What it taught us is that everyone, and I mean <i>everyone</i>, can draw. Our collective attitude shifted from &quot;don&#x27;t know if this is even possible&quot; to &quot;of course we can do this. this is easy. routine. trivial.&quot;<p>Highly recommended approach.<p>It was the most freeing and amazing class I had in college.</div><br/><div id="40305412" class="c"><input type="checkbox" id="c-40305412" checked=""/><div class="controls bullet"><span class="by">Version467</span><span>|</span><a href="#40303428">root</a><span>|</span><a href="#40303791">parent</a><span>|</span><a href="#40303506">next</a><span>|</span><label class="collapse" for="c-40305412">[-]</label><label class="expand" for="c-40305412">[1 more]</label></div><br/><div class="children"><div class="content">That sounds like a pretty awesome experience. Thanks for sharing.</div><br/></div></div></div></div><div id="40303506" class="c"><input type="checkbox" id="c-40303506" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#40303428">parent</a><span>|</span><a href="#40303791">prev</a><span>|</span><a href="#40305627">next</a><span>|</span><label class="collapse" for="c-40303506">[-]</label><label class="expand" for="c-40303506">[2 more]</label></div><br/><div class="children"><div class="content">Systems generally become more efficient when under stress. They are also forced into local optima - everything has upsides and downsides.</div><br/><div id="40304277" class="c"><input type="checkbox" id="c-40304277" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#40303428">root</a><span>|</span><a href="#40303506">parent</a><span>|</span><a href="#40305627">next</a><span>|</span><label class="collapse" for="c-40304277">[-]</label><label class="expand" for="c-40304277">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly - this is the idea behind Nassim Taleb’s book “Antifragile” and the concept of “anti-fragility”.<p>In essence, it promotes dynamic&#x2F;evolutionary&#x2F;always learning behaviour than performing the same set of steps every time, and in the process, becoming stronger than before.<p>An example he shares is: how the breakdown of muscle tissue through exercise leads to more muscle development and an increase in strength. I guess it’s similar to LLM training using error&#x2F;loss reducing functions (practice makes perfect) but dissimilar in the sense that training is a one—time action.</div><br/></div></div></div></div></div></div><div id="40305627" class="c"><input type="checkbox" id="c-40305627" checked=""/><div class="controls bullet"><span class="by">wangii</span><span>|</span><a href="#40303428">prev</a><span>|</span><a href="#40303071">next</a><span>|</span><label class="collapse" for="c-40305627">[-]</label><label class="expand" for="c-40305627">[6 more]</label></div><br/><div class="children"><div class="content">I feel it&#x27;s a pretty dangerous optimization before we REALLY understand what&#x27;s going on inside of the LLM. e.g. guys believe in the geometric interpretation will have something to say, and it would probably hurt if you are using &quot;filler&quot; tokens.<p>Besides, the assumption (not a universal fact) that &quot;forming complete sentences in mind before articulating word by word&quot; seems overly simplifies activities happens in our mind: do we really have a complete planning before start talking&#x2F;typing? as a Buddhist I lean towards it&#x27;s an illusion. further more, what about simultaneous thoughts? are we linear thinker in the sentence level?<p>anyway, pretty neat math!</div><br/><div id="40305816" class="c"><input type="checkbox" id="c-40305816" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#40305627">parent</a><span>|</span><a href="#40305679">next</a><span>|</span><label class="collapse" for="c-40305816">[-]</label><label class="expand" for="c-40305816">[4 more]</label></div><br/><div class="children"><div class="content">The optimization does not affect the result of LLM, it&#x27;s guaranteed to produce equivalent results as decoding directly. Let&#x27;s not treat that LLM as some magic that resembles our mind, it&#x27;s just another program that produces sentences that happens to make sense.</div><br/><div id="40306324" class="c"><input type="checkbox" id="c-40306324" checked=""/><div class="controls bullet"><span class="by">wangii</span><span>|</span><a href="#40305627">root</a><span>|</span><a href="#40305816">parent</a><span>|</span><a href="#40305885">next</a><span>|</span><label class="collapse" for="c-40306324">[-]</label><label class="expand" for="c-40306324">[1 more]</label></div><br/><div class="children"><div class="content">According to the original Jacobi decoding paper, it&#x27;s set in the machine translation tasks, with encoder + decoder, in which parallel algo applied only to the decoder part.</div><br/></div></div><div id="40305885" class="c"><input type="checkbox" id="c-40305885" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40305627">root</a><span>|</span><a href="#40305816">parent</a><span>|</span><a href="#40306324">prev</a><span>|</span><a href="#40305679">next</a><span>|</span><label class="collapse" for="c-40305885">[-]</label><label class="expand" for="c-40305885">[2 more]</label></div><br/><div class="children"><div class="content">Lets not treat our mind as something magical. It&#x27;s just another program that learned to speak by consuming lots of training input. The implementation might look slightly different from the outside, but from a mathematical perspective, artificial neural networks are proven to be at least as capable as the human mind.</div><br/><div id="40305903" class="c"><input type="checkbox" id="c-40305903" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#40305627">root</a><span>|</span><a href="#40305885">parent</a><span>|</span><a href="#40305679">next</a><span>|</span><label class="collapse" for="c-40305903">[-]</label><label class="expand" for="c-40305903">[1 more]</label></div><br/><div class="children"><div class="content">The best part is, your comment works both when sarcastic and completely serious.</div><br/></div></div></div></div></div></div><div id="40305679" class="c"><input type="checkbox" id="c-40305679" checked=""/><div class="controls bullet"><span class="by">Etheryte</span><span>|</span><a href="#40305627">parent</a><span>|</span><a href="#40305816">prev</a><span>|</span><a href="#40303071">next</a><span>|</span><label class="collapse" for="c-40305679">[-]</label><label class="expand" for="c-40305679">[1 more]</label></div><br/><div class="children"><div class="content">That assumption might be useful in this context, but I think it&#x27;s pretty clearly not true. Ask anyone to tell you about a complex past event with a lot of parallel branches and you&#x27;ll quickly see them add bits, pieces and tangents midsentence to cover the full range of events. I don&#x27;t think I&#x27;ve seen the sentence granularity hypothesis in any serious scientific context before.</div><br/></div></div></div></div><div id="40303071" class="c"><input type="checkbox" id="c-40303071" checked=""/><div class="controls bullet"><span class="by">miven</span><span>|</span><a href="#40305627">prev</a><span>|</span><a href="#40302689">next</a><span>|</span><label class="collapse" for="c-40303071">[-]</label><label class="expand" for="c-40303071">[3 more]</label></div><br/><div class="children"><div class="content">The authors mention that Jacobi decoding is equivalent to greedy autoregressive decoding, but in practice don&#x27;t we often want the sampling temperature to be above zero to avoid repetitions and excessively generic responses?<p>I&#x27;m completely unfamiliar with this decoding strategy so maybe I&#x27;m just missing a simple way to account for that.</div><br/><div id="40303825" class="c"><input type="checkbox" id="c-40303825" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40303071">parent</a><span>|</span><a href="#40303372">next</a><span>|</span><label class="collapse" for="c-40303825">[-]</label><label class="expand" for="c-40303825">[1 more]</label></div><br/><div class="children"><div class="content">Yes this is a great question! We are actively working on supporting other sampling strategies other than greedy sampling. In the context of CLLM training, instead of mapping to a static fixed point obtained from Jacobi decoding as the training ojbective, we term it dynamic fixed point. You can keep an eye on our github repo for new progress.</div><br/></div></div><div id="40303372" class="c"><input type="checkbox" id="c-40303372" checked=""/><div class="controls bullet"><span class="by">matheist</span><span>|</span><a href="#40303071">parent</a><span>|</span><a href="#40303825">prev</a><span>|</span><a href="#40302689">next</a><span>|</span><label class="collapse" for="c-40303372">[-]</label><label class="expand" for="c-40303372">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. It&#x27;s straightforward to check that a token was the argmax, but it seems difficult to check that a token appeared with the probability you wanted it to. You could still do the fine-tuning step I guess, where you train the trajectories to approach n-token completions with the statistics you want, but I can&#x27;t see how you can replace the &quot;check for a fixed point&quot; step. Maybe &quot;check the result was above this fixed threshold for likelihood&quot;.</div><br/></div></div></div></div><div id="40302689" class="c"><input type="checkbox" id="c-40302689" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#40303071">prev</a><span>|</span><a href="#40303379">next</a><span>|</span><label class="collapse" for="c-40302689">[-]</label><label class="expand" for="c-40302689">[3 more]</label></div><br/><div class="children"><div class="content">Wow, I&#x27;m mindblown this isn&#x27;t getting more attention. This seems like a clear win for inference. Fine tuning cost for this is reasonable (around 0.01% of the original pre-training cost). And the performance wins seem fairly consistent.</div><br/><div id="40303451" class="c"><input type="checkbox" id="c-40303451" checked=""/><div class="controls bullet"><span class="by">lopuhin</span><span>|</span><a href="#40302689">parent</a><span>|</span><a href="#40303890">next</a><span>|</span><label class="collapse" for="c-40303451">[-]</label><label class="expand" for="c-40303451">[1 more]</label></div><br/><div class="children"><div class="content">Similar or greater inference wins are achieved with speculative decoding which is already widely used, so while this is really interesting (and was tried before with less success AFAIK), it&#x27;s not yet clear how impactful it would be.</div><br/></div></div><div id="40303890" class="c"><input type="checkbox" id="c-40303890" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40302689">parent</a><span>|</span><a href="#40303451">prev</a><span>|</span><a href="#40303379">next</a><span>|</span><label class="collapse" for="c-40303890">[-]</label><label class="expand" for="c-40303890">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for interesting in our work! Yes we found training with consistency loss + AR loss on even a subset of a dataset results in significant speedup (0.01% pre-training cost). Training on more data permits even further speedup: the model is able to learn from more frequently-appearing collocations and phrases.<p>For more details, please check out our paper and you can also see speedup saturates as the size of training data grows.</div><br/></div></div></div></div><div id="40303379" class="c"><input type="checkbox" id="c-40303379" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#40302689">prev</a><span>|</span><a href="#40302569">next</a><span>|</span><label class="collapse" for="c-40303379">[-]</label><label class="expand" for="c-40303379">[5 more]</label></div><br/><div class="children"><div class="content">Interesting<p>I think soon we are going to realize that we don’t really need training the models<p>We just need good indexing and sampling<p>Essentially at some level any LLM is equivalent to a DB of the dataset, with a great NLP interface on top<p>Both are just different methods of navigating stored data</div><br/><div id="40303573" class="c"><input type="checkbox" id="c-40303573" checked=""/><div class="controls bullet"><span class="by">sdrg822</span><span>|</span><a href="#40303379">parent</a><span>|</span><a href="#40304015">next</a><span>|</span><label class="collapse" for="c-40303573">[-]</label><label class="expand" for="c-40303573">[1 more]</label></div><br/><div class="children"><div class="content">But indexing *is* training. It&#x27;s just not using end-to-end gradient descent.</div><br/></div></div><div id="40304015" class="c"><input type="checkbox" id="c-40304015" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#40303379">parent</a><span>|</span><a href="#40303573">prev</a><span>|</span><a href="#40304802">next</a><span>|</span><label class="collapse" for="c-40304015">[-]</label><label class="expand" for="c-40304015">[1 more]</label></div><br/><div class="children"><div class="content">LLMs can easily produce data not in training dataset.<p>LLMs do not navigate stored data. An LLM is not a DB of the training data.</div><br/></div></div><div id="40304802" class="c"><input type="checkbox" id="c-40304802" checked=""/><div class="controls bullet"><span class="by">JoannaWongs</span><span>|</span><a href="#40303379">parent</a><span>|</span><a href="#40304015">prev</a><span>|</span><a href="#40303559">next</a><span>|</span><label class="collapse" for="c-40304802">[-]</label><label class="expand" for="c-40304802">[1 more]</label></div><br/><div class="children"><div class="content">RAG (Retrieval-Augmented Generation) enhances Large Language Models (LLMs) by integrating them with specific domain knowledge or an enterprise&#x27;s internal databases, eliminating the need for model retraining. This method maintains the relevance, accuracy, and value of the outputs in a cost-effective way. However, the effectiveness of a RAG solution heavily depends on the quality of the datasets it uses. These datasets often come from varied sources and in different formats. For optimal performance, it&#x27;s crucial to normalize the various data types, clean and organize the data by removing unnecessary elements such as special characters, irrelevant metadata, or extraneous text.<p>HelloRAG.ai addresses this challenge by offering a no-code platform that allows you to easily transform multi-format datasets into structured formats like JSON. This tool helps you focus on developing the logic of RAG, streamlining the process of leveraging augmented data for enhanced model performance</div><br/></div></div><div id="40303559" class="c"><input type="checkbox" id="c-40303559" checked=""/><div class="controls bullet"><span class="by">nsagent</span><span>|</span><a href="#40303379">parent</a><span>|</span><a href="#40304802">prev</a><span>|</span><a href="#40302569">next</a><span>|</span><label class="collapse" for="c-40303559">[-]</label><label class="expand" for="c-40303559">[1 more]</label></div><br/><div class="children"><div class="content">You might like, the Infinigram paper then. It was discussed recently:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40266791">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40266791</a></div><br/></div></div></div></div><div id="40302569" class="c"><input type="checkbox" id="c-40302569" checked=""/><div class="controls bullet"><span class="by">andy12_</span><span>|</span><a href="#40303379">prev</a><span>|</span><a href="#40305470">next</a><span>|</span><label class="collapse" for="c-40302569">[-]</label><label class="expand" for="c-40302569">[2 more]</label></div><br/><div class="children"><div class="content">At first I thoght that this was another Medusa-like paper, simply using more unembed heads for guessing subsequent tokes, but damn, not at all. This is amazing. And it doesn&#x27;t even use extra parameters, it&#x27;s just an auxiliary training loss.</div><br/><div id="40304146" class="c"><input type="checkbox" id="c-40304146" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40302569">parent</a><span>|</span><a href="#40305470">next</a><span>|</span><label class="collapse" for="c-40304146">[-]</label><label class="expand" for="c-40304146">[1 more]</label></div><br/><div class="children"><div class="content">The only similarity between Medusa and CLLM is both train and adapt LLMs for fast inference. But they use completely different training technique, decoding technique and as you pointed out CLLMs don&#x27;t need extra parameters or configuring attention mask for tree-based verification.</div><br/></div></div></div></div><div id="40305470" class="c"><input type="checkbox" id="c-40305470" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#40302569">prev</a><span>|</span><a href="#40303311">next</a><span>|</span><label class="collapse" for="c-40305470">[-]</label><label class="expand" for="c-40305470">[3 more]</label></div><br/><div class="children"><div class="content">&gt; ... speculative decoding methods ... incurs extra memory cost during inference time.<p>Any detail on this? For speculative decoding you need a smaller model to generate &quot;branches&quot; which are fast but maybe inaccurate and verify these branches later with a larger model. However, only memory equivalent to a single token is needed for speculative decoding, and tokens in other branches are simply masked out during inference. With a context size of 1000 and ~30 branches for 5 tokens, the memory overhead would be 3% which is negligible. If your context size is much smaller compared to the number of branches - would someone who use a generative LLM with a context window of just 50 tokens care about generation speed?<p>Also, speculative decoding techniques are not restricted to greedy sampling - it&#x27;s expected to behave exactly the same as the original model and sample with the expected probabilities. Most literature on speculative decoding already reports 2.6x-3.5x speedup. The blog post here reports 2.4x-3.4x generation speed - which isn&#x27;t that much of an upgrade?<p>While I mentioned speculative decoding above and Medusa2 and Eagle seems to be the techniques that the author compares against, the core problem remains: whatever method you use to predict tokens ahead of time, there is a specific point where the previous tokens are absolutely needed before predicting the next token. It doesn&#x27;t depend on what your model is or what your techniques are, it&#x27;s just about what is mathematically achievable. How can you predict 5 tokens at once if the probability distribution of the 5th next token depends heavily on the previous 4 tokens? Speculative decoding, Jacobi decoding, multi-token parallel decoding, whatever.<p>If only greedy sampling is supported for this, then I wonder what are the advantages of this method, not to mention that other techniques already achieve the expected speedup. Comparing greedy sampling speedups to random sampling speedups is comparing apples to oranges, and I doubt if the speedup described by the method would remain after this method is adapted to random sampling (due to the core problem mentioned above).</div><br/><div id="40305596" class="c"><input type="checkbox" id="c-40305596" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#40305470">parent</a><span>|</span><a href="#40303311">next</a><span>|</span><label class="collapse" for="c-40305596">[-]</label><label class="expand" for="c-40305596">[2 more]</label></div><br/><div class="children"><div class="content">Speculative decoding requires you to load the smaller model into memory and run inference on it.</div><br/><div id="40305685" class="c"><input type="checkbox" id="c-40305685" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#40305470">root</a><span>|</span><a href="#40305596">parent</a><span>|</span><a href="#40303311">next</a><span>|</span><label class="collapse" for="c-40305685">[-]</label><label class="expand" for="c-40305685">[1 more]</label></div><br/><div class="children"><div class="content">I think the smaller model is at least 20 times smaller. If you do speculative decoding on a 70B model an 1B model would be appropriate.</div><br/></div></div></div></div></div></div><div id="40303311" class="c"><input type="checkbox" id="c-40303311" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#40305470">prev</a><span>|</span><a href="#40303845">next</a><span>|</span><label class="collapse" for="c-40303311">[-]</label><label class="expand" for="c-40303311">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no free lunch™, so from what I can tell there&#x27;s some pathway loss here. E.g. some Jacobi trajectories definitionally exclude higher temperature paths. Which might actually be a positive given data retrieval (but a negative if we want to maximize for creativity?).</div><br/><div id="40303749" class="c"><input type="checkbox" id="c-40303749" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40303311">parent</a><span>|</span><a href="#40303845">next</a><span>|</span><label class="collapse" for="c-40303749">[-]</label><label class="expand" for="c-40303749">[3 more]</label></div><br/><div class="children"><div class="content">There are better and worse algorithms. I&#x27;m not sure &quot;there is no free lunch&quot; always applies in a particularly meaningful way. Some things aren&#x27;t on the pareto frontier.</div><br/><div id="40303938" class="c"><input type="checkbox" id="c-40303938" checked=""/><div class="controls bullet"><span class="by">factormeta</span><span>|</span><a href="#40303311">root</a><span>|</span><a href="#40303749">parent</a><span>|</span><a href="#40303845">next</a><span>|</span><label class="collapse" for="c-40303938">[-]</label><label class="expand" for="c-40303938">[2 more]</label></div><br/><div class="children"><div class="content">Kinda like the aiff -&gt; mp3 conversion process. A lot of data is lost, but we human can really tell the too much of a difference?</div><br/><div id="40304265" class="c"><input type="checkbox" id="c-40304265" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40303311">root</a><span>|</span><a href="#40303938">parent</a><span>|</span><a href="#40303845">next</a><span>|</span><label class="collapse" for="c-40304265">[-]</label><label class="expand" for="c-40304265">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no reason to think the current next token prediction models are optimal for predicting sentences (they aren&#x27;t!)<p>&gt; An algorithm may outperform another on a problem when neither is specialized to the problem<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;No_free_lunch_in_search_and_optimization" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;No_free_lunch_in_search_and_...</a></div><br/></div></div></div></div></div></div></div></div><div id="40303845" class="c"><input type="checkbox" id="c-40303845" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#40303311">prev</a><span>|</span><a href="#40302564">next</a><span>|</span><label class="collapse" for="c-40303845">[-]</label><label class="expand" for="c-40303845">[7 more]</label></div><br/><div class="children"><div class="content">Anyone know somewhere someone dumb like me can &quot;Ask an AI expert&quot;?<p>I want to ask, for example, how is it that an LLM when given the same prompt does not respond in the same deterministic way?<p>I guess I want to learn this stuff and should maybe follow one of those &quot;write an LLM in an hour&quot; type videos on YouTube.</div><br/><div id="40304123" class="c"><input type="checkbox" id="c-40304123" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#40303845">parent</a><span>|</span><a href="#40303875">next</a><span>|</span><label class="collapse" for="c-40304123">[-]</label><label class="expand" for="c-40304123">[1 more]</label></div><br/><div class="children"><div class="content">&gt; how is it that an LLM when given the same prompt does not respond in the same deterministic way?<p>In software (not in the model) here&#x27;s literally a random number generator that picks from a weighted set of &quot;next-token&quot; choices that the model spits out.   The selection process can have a series of knobs to manipulate the responses.   If you want it to be deterministic (if you have direct access to the software) you can tell it to set &quot;top-k = 1&quot; or &quot;temperature = 0.0&quot; (depending on your software) and it will be deterministic.<p>Usually the default settings are not for determinism, because for whatever reason the quality of the results tends to not be that good when you go fully d.</div><br/></div></div><div id="40303875" class="c"><input type="checkbox" id="c-40303875" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#40303845">parent</a><span>|</span><a href="#40304123">prev</a><span>|</span><a href="#40303869">next</a><span>|</span><label class="collapse" for="c-40303875">[-]</label><label class="expand" for="c-40303875">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I want to ask, for example, how is it that an LLM when given the same prompt does not respond in the same deterministic way?<p>You can control that in most systems with an inference-set parameter called &quot;temperature&quot;.  But setting the temperature as low as possible tends to lead to very low-quality answers - the system can&#x27;t crawl out of some local optimum and ends up repeating itself over and over.  Such answers may be &quot;deterministic&quot; but they&#x27;re also not good.</div><br/></div></div><div id="40303869" class="c"><input type="checkbox" id="c-40303869" checked=""/><div class="controls bullet"><span class="by">8note</span><span>|</span><a href="#40303845">parent</a><span>|</span><a href="#40303875">prev</a><span>|</span><a href="#40303906">next</a><span>|</span><label class="collapse" for="c-40303869">[-]</label><label class="expand" for="c-40303869">[1 more]</label></div><br/><div class="children"><div class="content">For that answer, you can refer to the 3blue1brown videos<p>The llm model outputs a vector of probabilities for tokens, and the llm user picks a token from the most likely list using a random number</div><br/></div></div><div id="40303906" class="c"><input type="checkbox" id="c-40303906" checked=""/><div class="controls bullet"><span class="by">zipfcharge</span><span>|</span><a href="#40303845">parent</a><span>|</span><a href="#40303869">prev</a><span>|</span><a href="#40303884">next</a><span>|</span><label class="collapse" for="c-40303906">[-]</label><label class="expand" for="c-40303906">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because an LLM is essentially a probability matrix. You type a prompt, then it calculates what&#x27;s the probability of getting a next word and so on, eventually forming a sentence. The probability learned is based on the training data.<p>Because of the underlying probability model, it&#x27;s not going to be 100% deterministic. Plus a model like ChatGPT purposefully have &quot;temperature&quot; parameter that will further add randomisation to the whole process.<p>My answer is based on this paper if you&#x27;re interested to read more: The Matrix: A Bayesian learning model for LLMs, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.03175" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.03175</a></div><br/><div id="40305465" class="c"><input type="checkbox" id="c-40305465" checked=""/><div class="controls bullet"><span class="by">flopriore</span><span>|</span><a href="#40303845">root</a><span>|</span><a href="#40303906">parent</a><span>|</span><a href="#40303884">next</a><span>|</span><label class="collapse" for="c-40305465">[-]</label><label class="expand" for="c-40305465">[1 more]</label></div><br/><div class="children"><div class="content">Are there any ways to show the source of the information retrieved by the model? For instance, the LLM forms a sentence and it points to a stackoverflow answer with the same or similar content.</div><br/></div></div></div></div><div id="40303884" class="c"><input type="checkbox" id="c-40303884" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#40303845">parent</a><span>|</span><a href="#40303906">prev</a><span>|</span><a href="#40302564">next</a><span>|</span><label class="collapse" for="c-40303884">[-]</label><label class="expand" for="c-40303884">[1 more]</label></div><br/><div class="children"><div class="content">For this particular question, ask chatgpt how temperature affects llm softmax sampling.<p>For other things, study using Karpathy&#x27;s videos.</div><br/></div></div></div></div><div id="40302564" class="c"><input type="checkbox" id="c-40302564" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#40303845">prev</a><span>|</span><a href="#40303926">next</a><span>|</span><label class="collapse" for="c-40302564">[-]</label><label class="expand" for="c-40302564">[1 more]</label></div><br/><div class="children"><div class="content">Interesting stuff. I guess the idea has occurred to many but was well written and presented.</div><br/></div></div><div id="40303926" class="c"><input type="checkbox" id="c-40303926" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40302564">prev</a><span>|</span><a href="#40302970">next</a><span>|</span><label class="collapse" for="c-40303926">[-]</label><label class="expand" for="c-40303926">[2 more]</label></div><br/><div class="children"><div class="content">from CLLM authors:<p>Thank you guys for the great questions and insights! We have made a Twitter posts with some more details and we invite you to engage with us on Twitter as well.<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;haoailab&#x2F;status&#x2F;1788269848788869299" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;haoailab&#x2F;status&#x2F;1788269848788869299</a></div><br/></div></div><div id="40302970" class="c"><input type="checkbox" id="c-40302970" checked=""/><div class="controls bullet"><span class="by">paulclark</span><span>|</span><a href="#40303926">prev</a><span>|</span><a href="#40303128">next</a><span>|</span><label class="collapse" for="c-40302970">[-]</label><label class="expand" for="c-40302970">[5 more]</label></div><br/><div class="children"><div class="content">Is this how Groq (<a href="https:&#x2F;&#x2F;groq.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;groq.com&#x2F;</a>) is so fast, or are they doing something different?</div><br/><div id="40303050" class="c"><input type="checkbox" id="c-40303050" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#40302970">parent</a><span>|</span><a href="#40303767">next</a><span>|</span><label class="collapse" for="c-40303050">[-]</label><label class="expand" for="c-40303050">[2 more]</label></div><br/><div class="children"><div class="content">Groq is serving an LLM from (100s of chips worth of) SRAM, so the effective bandwidth thus token generation speed is an order of magnitude higher than HBM. This would 3.5x their speed as well, it is orthogonal.</div><br/><div id="40306262" class="c"><input type="checkbox" id="c-40306262" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#40302970">root</a><span>|</span><a href="#40303050">parent</a><span>|</span><a href="#40303767">next</a><span>|</span><label class="collapse" for="c-40306262">[-]</label><label class="expand" for="c-40306262">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised no one has done this for a GPU cluster yet - we used to do this for RNNs on GPUs &amp; FPGAs at Baidu:<p><a href="https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v48&#x2F;diamos16.pdf" rel="nofollow">https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v48&#x2F;diamos16.pdf</a><p>Or better yet - on Cerebras<p>Kudos to groq for writing that kernel</div><br/></div></div></div></div><div id="40303767" class="c"><input type="checkbox" id="c-40303767" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40302970">parent</a><span>|</span><a href="#40303050">prev</a><span>|</span><a href="#40304086">next</a><span>|</span><label class="collapse" for="c-40303767">[-]</label><label class="expand" for="c-40303767">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that theirs is a pure hardware solution. The hardware is flexible enough to model any current NN architecture.<p>(Incidentally, there are black box optimization algorithms, so a system as good as grok at inference might be useful for training even if it can&#x27;t support gradient descent)</div><br/></div></div><div id="40304086" class="c"><input type="checkbox" id="c-40304086" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#40302970">parent</a><span>|</span><a href="#40303767">prev</a><span>|</span><a href="#40303128">next</a><span>|</span><label class="collapse" for="c-40304086">[-]</label><label class="expand" for="c-40304086">[1 more]</label></div><br/><div class="children"><div class="content">According to someone I talked to at groq event I was invited to (I did not sign an nda), They are putting ~8 racks of hardware per llm.  Of course coordinating those racks to have exact timings between them to pull tokens through is definitely &quot;part of the hard part&quot;.</div><br/></div></div></div></div><div id="40303128" class="c"><input type="checkbox" id="c-40303128" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#40302970">prev</a><span>|</span><a href="#40303450">next</a><span>|</span><label class="collapse" for="c-40303128">[-]</label><label class="expand" for="c-40303128">[3 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t wait to see something like this merged into ollama (I&#x27;m sure there would be plenty of people fine-tuning models for it).</div><br/><div id="40303375" class="c"><input type="checkbox" id="c-40303375" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#40303128">parent</a><span>|</span><a href="#40303444">next</a><span>|</span><label class="collapse" for="c-40303375">[-]</label><label class="expand" for="c-40303375">[1 more]</label></div><br/><div class="children"><div class="content">Ollama doesn&#x27;t have their own inference engine, they just wrap llama.cpp. But yes, it will be awesome when it&#x27;s more generally available.</div><br/></div></div><div id="40303444" class="c"><input type="checkbox" id="c-40303444" checked=""/><div class="controls bullet"><span class="by">helloericsf</span><span>|</span><a href="#40303128">parent</a><span>|</span><a href="#40303375">prev</a><span>|</span><a href="#40303450">next</a><span>|</span><label class="collapse" for="c-40303444">[-]</label><label class="expand" for="c-40303444">[1 more]</label></div><br/><div class="children"><div class="content">The lab is tied to the vLLM project. I would say it might get picked up sooner by vLLM than other inference frameworks.</div><br/></div></div></div></div><div id="40303450" class="c"><input type="checkbox" id="c-40303450" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40303128">prev</a><span>|</span><a href="#40302584">next</a><span>|</span><label class="collapse" for="c-40303450">[-]</label><label class="expand" for="c-40303450">[4 more]</label></div><br/><div class="children"><div class="content">Could someone please explain the intuition around this technique in more lament terms?</div><br/><div id="40303569" class="c"><input type="checkbox" id="c-40303569" checked=""/><div class="controls bullet"><span class="by">TomatoCo</span><span>|</span><a href="#40303450">parent</a><span>|</span><a href="#40303514">next</a><span>|</span><label class="collapse" for="c-40303569">[-]</label><label class="expand" for="c-40303569">[2 more]</label></div><br/><div class="children"><div class="content">For all of these &quot;how can we batch predicting the next n tokens?&quot; the intuition is basically that it takes a buttload of math to predict <i>some</i> of the tokens, but that most tokens are actually easy to guess. For example, if I asked &quot;What was that phone number from that 80&#x27;s song?&quot; as soon as a model generates 867- it shouldn&#x27;t take that much math at all to finish predicting 5309.</div><br/><div id="40303970" class="c"><input type="checkbox" id="c-40303970" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40303450">root</a><span>|</span><a href="#40303569">parent</a><span>|</span><a href="#40303514">next</a><span>|</span><label class="collapse" for="c-40303970">[-]</label><label class="expand" for="c-40303970">[1 more]</label></div><br/><div class="children"><div class="content">A bit more intuition on how training works: in natural language processing, some phrases&#x2F;collocations, for example &quot;remind ... of ...&quot;, &quot;make a decision&quot;, &quot;learn a skill&quot; etc. are used together. We can ask LLMs to learn such collections &amp; frequently appearing n-grams. After learning, the model can use parallel decoding to predict many tokens that are frequently appear together in one forward pass.</div><br/></div></div></div></div></div></div><div id="40302584" class="c"><input type="checkbox" id="c-40302584" checked=""/><div class="controls bullet"><span class="by">fermuch</span><span>|</span><a href="#40303450">prev</a><span>|</span><a href="#40303122">next</a><span>|</span><label class="collapse" for="c-40302584">[-]</label><label class="expand" for="c-40302584">[2 more]</label></div><br/><div class="children"><div class="content">Would something like this apply to MAMBA&#x2F;JAMBA too?</div><br/><div id="40303800" class="c"><input type="checkbox" id="c-40303800" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40302584">parent</a><span>|</span><a href="#40303122">next</a><span>|</span><label class="collapse" for="c-40303800">[-]</label><label class="expand" for="c-40303800">[1 more]</label></div><br/><div class="children"><div class="content">I think any next token predictor will benefit. Iiuc mamba is a next token predictor.<p>I just skimmed the gradient article, but if their only change is swapping out the transformer block for the mamba block, I don&#x27;t think it&#x27;s already using this optimization</div><br/></div></div></div></div><div id="40303122" class="c"><input type="checkbox" id="c-40303122" checked=""/><div class="controls bullet"><span class="by">doctor_eval</span><span>|</span><a href="#40302584">prev</a><span>|</span><a href="#40303724">next</a><span>|</span><label class="collapse" for="c-40303122">[-]</label><label class="expand" for="c-40303122">[10 more]</label></div><br/><div class="children"><div class="content">&gt; Our research shows this process – mimicking human cognitive process of forming complete sentences in mind before articulating word by word<p>This is not how I work. Is there something wrong with me?</div><br/><div id="40303215" class="c"><input type="checkbox" id="c-40303215" checked=""/><div class="controls bullet"><span class="by">jerbear4328</span><span>|</span><a href="#40303122">parent</a><span>|</span><a href="#40303305">next</a><span>|</span><label class="collapse" for="c-40303215">[-]</label><label class="expand" for="c-40303215">[2 more]</label></div><br/><div class="children"><div class="content">Nor is it how I work, I think that&#x27;s normal enough. I do have an idea of what I&#x27;m going to say before I say it, I think that&#x27;s closer to what they meant. I think and speak in increments of ideas, not words.</div><br/><div id="40303334" class="c"><input type="checkbox" id="c-40303334" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#40303122">root</a><span>|</span><a href="#40303215">parent</a><span>|</span><a href="#40303305">next</a><span>|</span><label class="collapse" for="c-40303334">[-]</label><label class="expand" for="c-40303334">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think and speak in increments of ideas<p>extremely common among (but not unique to) people with ASD, those &quot;increments of ideas&quot; are called &quot;gestalts&quot;.<p><a href="https:&#x2F;&#x2F;kidtherapy.org&#x2F;helpful-articles&#x2F;what-is-gestalt-language-learning&#x2F;" rel="nofollow">https:&#x2F;&#x2F;kidtherapy.org&#x2F;helpful-articles&#x2F;what-is-gestalt-lang...</a></div><br/></div></div></div></div><div id="40303305" class="c"><input type="checkbox" id="c-40303305" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#40303122">parent</a><span>|</span><a href="#40303215">prev</a><span>|</span><a href="#40303239">next</a><span>|</span><label class="collapse" for="c-40303305">[-]</label><label class="expand" for="c-40303305">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Rem tene, verba sequentur&quot; (you hold the matter, then words come) is largely &quot;how it works&quot;.<p>You form logical ideas as you speak, as you speak your speech develops, so the translation is from ideas to sentences. It is not clear in which phase one would mentally form a complete sentence, nor why it should be relevant. You &quot;see something [that makes sense]&quot;, then you describe it - iteratively.</div><br/></div></div><div id="40303239" class="c"><input type="checkbox" id="c-40303239" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#40303122">parent</a><span>|</span><a href="#40303305">prev</a><span>|</span><a href="#40304101">next</a><span>|</span><label class="collapse" for="c-40303239">[-]</label><label class="expand" for="c-40303239">[2 more]</label></div><br/><div class="children"><div class="content">You might not have an internal monologue. A lot of us don&#x27;t, and the ones that do are equally shocked every time they find out. For what it&#x27;s worth, I&#x27;m in the same boat—<i>can</i> form sentences, but why would I? It&#x27;d slow me down.<p>People who don&#x27;t have inner monologues tend to assume that all that stuff is some form of analogy or metaphor. It&#x27;s not. It&#x27;s entirely literal.</div><br/><div id="40303316" class="c"><input type="checkbox" id="c-40303316" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#40303122">root</a><span>|</span><a href="#40303239">parent</a><span>|</span><a href="#40304101">next</a><span>|</span><label class="collapse" for="c-40303316">[-]</label><label class="expand" for="c-40303316">[1 more]</label></div><br/><div class="children"><div class="content">Do you mean in a real time conversation?<p>Because I definitely dont &quot;have an internal monologue about what I&#x27;m going to say&quot; in the 100ms between when someone asks a casual question and I respond to it.</div><br/></div></div></div></div><div id="40304101" class="c"><input type="checkbox" id="c-40304101" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#40303122">parent</a><span>|</span><a href="#40303239">prev</a><span>|</span><a href="#40304010">next</a><span>|</span><label class="collapse" for="c-40304101">[-]</label><label class="expand" for="c-40304101">[1 more]</label></div><br/><div class="children"><div class="content">Are you sure.  It might not be the whole sentence, but I would find it hard to believe that in practice the way you speak or write is like<p>hello
&lt;think&gt;
May
&lt;think&gt;
be
&lt;think&gt;
I&#x27;ll
&lt;think&gt;
go
&lt;think&gt;
get
&lt;think&gt;
break
&lt;think&gt;
fast</div><br/></div></div><div id="40304010" class="c"><input type="checkbox" id="c-40304010" checked=""/><div class="controls bullet"><span class="by">snyhlxde</span><span>|</span><a href="#40303122">parent</a><span>|</span><a href="#40304101">prev</a><span>|</span><a href="#40303255">next</a><span>|</span><label class="collapse" for="c-40304010">[-]</label><label class="expand" for="c-40304010">[1 more]</label></div><br/><div class="children"><div class="content">In some conversations, maybe it&#x27;s easier to form complete sentences. In some others, the best we can do is: have a rough draft about what to say in mind and then refine it word by word while speaking.</div><br/></div></div><div id="40303255" class="c"><input type="checkbox" id="c-40303255" checked=""/><div class="controls bullet"><span class="by">DrSiemer</span><span>|</span><a href="#40303122">parent</a><span>|</span><a href="#40304010">prev</a><span>|</span><a href="#40303452">next</a><span>|</span><label class="collapse" for="c-40303255">[-]</label><label class="expand" for="c-40303255">[1 more]</label></div><br/><div class="children"><div class="content">They probably do not mean people form entire sentences before expressing them, I am not aware of anybody doing that. I assume it refers to people first coming up with a global outline of what they want to say before they start speaking.</div><br/></div></div><div id="40303452" class="c"><input type="checkbox" id="c-40303452" checked=""/><div class="controls bullet"><span class="by">giardini</span><span>|</span><a href="#40303122">parent</a><span>|</span><a href="#40303255">prev</a><span>|</span><a href="#40303724">next</a><span>|</span><label class="collapse" for="c-40303452">[-]</label><label class="expand" for="c-40303452">[1 more]</label></div><br/><div class="children"><div class="content">Probably.</div><br/></div></div></div></div><div id="40303724" class="c"><input type="checkbox" id="c-40303724" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40303122">prev</a><span>|</span><a href="#40303643">next</a><span>|</span><label class="collapse" for="c-40303724">[-]</label><label class="expand" for="c-40303724">[1 more]</label></div><br/><div class="children"><div class="content">They can quickly try with one of the open source models, then show a side by side demo</div><br/></div></div></div></div></div></div></div></body></html>