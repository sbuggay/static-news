<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1723280461035" as="style"/><link rel="stylesheet" href="styles.css?v=1723280461035"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.thundercompute.com/">Show HN: Attaching to a virtual GPU over TCP</a> <span class="domain">(<a href="https://www.thundercompute.com">www.thundercompute.com</a>)</span></div><div class="subtext"><span>bmodel</span> | <span>91 comments</span></div><br/><div><div id="41208233" class="c"><input type="checkbox" id="c-41208233" checked=""/><div class="controls bullet"><span class="by">tamimio</span><span>|</span><a href="#41203918">next</a><span>|</span><label class="collapse" for="c-41208233">[-]</label><label class="expand" for="c-41208233">[1 more]</label></div><br/><div class="children"><div class="content">I’m more interested in using tools like hashcat, any benchmark on these? As the docs link returns error.</div><br/></div></div><div id="41203918" class="c"><input type="checkbox" id="c-41203918" checked=""/><div class="controls bullet"><span class="by">steelbrain</span><span>|</span><a href="#41208233">prev</a><span>|</span><a href="#41204617">next</a><span>|</span><label class="collapse" for="c-41203918">[-]</label><label class="expand" for="c-41203918">[10 more]</label></div><br/><div class="children"><div class="content">Ah this is quite interesting! I had a usecase where I needed a GPU-over-IP but only for transcoding videos. I had a not-so-powerful AMD GPU in my homelab server that somehow kept crashing the kernel any time I tried to encode videos with it and also an NVIDIA RTX 3080 in a gaming machine.<p>So I wrote <a href="https:&#x2F;&#x2F;github.com&#x2F;steelbrain&#x2F;ffmpeg-over-ip">https:&#x2F;&#x2F;github.com&#x2F;steelbrain&#x2F;ffmpeg-over-ip</a> and had the server running in the windows machine and the client in the media server (could be plex, emby, jellyfin etc) and it worked flawlessly.</div><br/><div id="41205253" class="c"><input type="checkbox" id="c-41205253" checked=""/><div class="controls bullet"><span class="by">toomuchtodo</span><span>|</span><a href="#41203918">parent</a><span>|</span><a href="#41204586">next</a><span>|</span><label class="collapse" for="c-41205253">[-]</label><label class="expand" for="c-41205253">[1 more]</label></div><br/><div class="children"><div class="content">Have you done a Show HN yet? If not, please consider doing so!<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;tzmartin&#x2F;88abb7ef63e41e27c2ec9a5ce5d9b5f9" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;tzmartin&#x2F;88abb7ef63e41e27c2ec9a5ce5d...</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;showhn.html">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;showhn.html</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22336638">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22336638</a></div><br/></div></div><div id="41204586" class="c"><input type="checkbox" id="c-41204586" checked=""/><div class="controls bullet"><span class="by">bhaney</span><span>|</span><a href="#41203918">parent</a><span>|</span><a href="#41205253">prev</a><span>|</span><a href="#41204329">next</a><span>|</span><label class="collapse" for="c-41204586">[-]</label><label class="expand" for="c-41204586">[6 more]</label></div><br/><div class="children"><div class="content">This is more or less what I was hoping for when I saw the submission title. Was disappointed to see that the submission wasn&#x27;t actually a useful generic tool but instead a paid cloud service. Of course the real content is in the comments.<p>As an aside, are there any uses for GPU-over-network other than video encoding? The increased latency seems like it would prohibit anything machine learning related or graphics intensive.</div><br/><div id="41206401" class="c"><input type="checkbox" id="c-41206401" checked=""/><div class="controls bullet"><span class="by">tommsy64</span><span>|</span><a href="#41203918">root</a><span>|</span><a href="#41204586">parent</a><span>|</span><a href="#41205686">next</a><span>|</span><label class="collapse" for="c-41206401">[-]</label><label class="expand" for="c-41206401">[1 more]</label></div><br/><div class="children"><div class="content">There is a GPU-over-network software called Juice [1]. I&#x27;ve used it on AWS for running CPU-intensive workloads that also happen to need some GPU without needing to use a huge GPU instance. I was able to use a small GPU instance, which had just 4 CPU cores, and stream its GPU to one with 128 CPU cores.<p>I found Juice to work decently for graphical applications too (e.g., games, CAD software). Latency was about what you&#x27;d expect for video encode + decode + network: 5-20ms on a LAN if I recall correctly.<p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;Juice-Labs&#x2F;Juice-Labs">https:&#x2F;&#x2F;github.com&#x2F;Juice-Labs&#x2F;Juice-Labs</a></div><br/></div></div><div id="41205686" class="c"><input type="checkbox" id="c-41205686" checked=""/><div class="controls bullet"><span class="by">trws</span><span>|</span><a href="#41203918">root</a><span>|</span><a href="#41204586">parent</a><span>|</span><a href="#41206401">prev</a><span>|</span><a href="#41207361">next</a><span>|</span><label class="collapse" for="c-41205686">[-]</label><label class="expand" for="c-41205686">[1 more]</label></div><br/><div class="children"><div class="content">Some computation tasks can tolerate the latency if they’re written with enough overlap and can keep enough of the data resident, but they usually need more performant networking than this. See older efforts like rcuda for remote cuda over infiniband as an example. It’s not ideal, but sometimes worth it. Usually the win is in taking a multi-GPU app and giving it 16 or 32 of them rather than a single remote GPU though.</div><br/></div></div><div id="41207361" class="c"><input type="checkbox" id="c-41207361" checked=""/><div class="controls bullet"><span class="by">Fnoord</span><span>|</span><a href="#41203918">root</a><span>|</span><a href="#41204586">parent</a><span>|</span><a href="#41205686">prev</a><span>|</span><a href="#41207533">next</a><span>|</span><label class="collapse" for="c-41207361">[-]</label><label class="expand" for="c-41207361">[1 more]</label></div><br/><div class="children"><div class="content">I mean, anything you use a GPU&#x2F;TPU for could benefit.<p>IPMI and such could use it. Like, for example, Proxmox could use it. Machine learning tasks (like Frigate) and hashcat could also use such. All in theory, of course. Many tasks use VNC right now, or SPICE. The ability to extract your GPU in the Unix way over TCP&#x2F;IP is powerful. Though Node.js would not be the way I&#x27;d want such to go.</div><br/></div></div><div id="41207533" class="c"><input type="checkbox" id="c-41207533" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#41203918">root</a><span>|</span><a href="#41204586">parent</a><span>|</span><a href="#41207361">prev</a><span>|</span><a href="#41204329">next</a><span>|</span><label class="collapse" for="c-41207533">[-]</label><label class="expand" for="c-41207533">[2 more]</label></div><br/><div class="children"><div class="content">How do you use it for video encoding&#x2F;decoding? Won&#x27;t the uncompressed video (input for encoding or output of decoding) be too large to transmit over network practically?</div><br/><div id="41208181" class="c"><input type="checkbox" id="c-41208181" checked=""/><div class="controls bullet"><span class="by">bhaney</span><span>|</span><a href="#41203918">root</a><span>|</span><a href="#41207533">parent</a><span>|</span><a href="#41204329">next</a><span>|</span><label class="collapse" for="c-41208181">[-]</label><label class="expand" for="c-41208181">[1 more]</label></div><br/><div class="children"><div class="content">Well, the ffmpeg-over-ip tool in the GP does it by just not sending uncompressed video. It&#x27;s more of an ffmpeg server where the server is implicitly expected to have access to a GPU that the client doesn&#x27;t have, and only compressed video is being sent back and forth in the form of video streams that would normally be the input and output of ffmpeg. It&#x27;s not a generic GPU server that tries to push a whole PCI bus over the network, which I personally think is a bit of a fool&#x27;s errand and doomed to never be particularly useful to existing generic workloads. It would work if you very carefully redesign the workload to not take advantage of a GPU&#x27;s typical high bandwidth and low latency, but if you have to do that then what&#x27;s the point of trying to abstract over the device layer? Better to work at a higher level of abstraction where you can optimize for your particular application, rather than a lower level that you can&#x27;t possibly implement well and then have to completely redo the higher levels anyway to work with it.</div><br/></div></div></div></div></div></div><div id="41204329" class="c"><input type="checkbox" id="c-41204329" checked=""/><div class="controls bullet"><span class="by">crishoj</span><span>|</span><a href="#41203918">parent</a><span>|</span><a href="#41204586">prev</a><span>|</span><a href="#41206820">next</a><span>|</span><label class="collapse" for="c-41204329">[-]</label><label class="expand" for="c-41204329">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. Do you know if your tool supports conversions resulting in multiple files, such as HLS and its myriad of timeslice files?</div><br/></div></div></div></div><div id="41204617" class="c"><input type="checkbox" id="c-41204617" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41203918">prev</a><span>|</span><a href="#41204517">next</a><span>|</span><label class="collapse" for="c-41204617">[-]</label><label class="expand" for="c-41204617">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m confused, if this operates at the CPU&#x2F;GPU boundary doesn&#x27;t it create a massive I&#x2F;O bottleneck for any dataset that doesn&#x27;t fit into VRAM? I&#x27;m probably misunderstanding how it works but if it intercepts GPU i&#x2F;o then it must stream your entire dataset on every epoch to a remote machine, which sounds wasteful, probably I&#x27;m not getting this right.</div><br/><div id="41204835" class="c"><input type="checkbox" id="c-41204835" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204617">parent</a><span>|</span><a href="#41204689">next</a><span>|</span><label class="collapse" for="c-41204835">[-]</label><label class="expand" for="c-41204835">[4 more]</label></div><br/><div class="children"><div class="content">That understanding of the system is correct. To make it practical we&#x27;ve implemented a bunch of optimizations to minimize I&#x2F;O cost. You can see how it performs on inference with BERT here: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;qsOBFQZtsFM?t=69" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;qsOBFQZtsFM?t=69</a>.<p>The overheads are larger for training compared to inference, and we are implementing more optimizations to approach native performance.</div><br/><div id="41206394" class="c"><input type="checkbox" id="c-41206394" checked=""/><div class="controls bullet"><span class="by">semitones</span><span>|</span><a href="#41204617">root</a><span>|</span><a href="#41204835">parent</a><span>|</span><a href="#41205006">next</a><span>|</span><label class="collapse" for="c-41206394">[-]</label><label class="expand" for="c-41206394">[1 more]</label></div><br/><div class="children"><div class="content">&gt; to approach native performance.<p>The same way one &quot;approaches the sun&quot; when they take the stairs?</div><br/></div></div><div id="41205006" class="c"><input type="checkbox" id="c-41205006" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41204617">root</a><span>|</span><a href="#41204835">parent</a><span>|</span><a href="#41206394">prev</a><span>|</span><a href="#41206729">next</a><span>|</span><label class="collapse" for="c-41205006">[-]</label><label class="expand" for="c-41205006">[1 more]</label></div><br/><div class="children"><div class="content">Aah ok thanks, that was my basic misunderstanding, my mind just jumped straight to my current training needs but for inference it makes a lot of sense. Thanks for the clarification.</div><br/></div></div><div id="41206729" class="c"><input type="checkbox" id="c-41206729" checked=""/><div class="controls bullet"><span class="by">ranger_danger</span><span>|</span><a href="#41204617">root</a><span>|</span><a href="#41204835">parent</a><span>|</span><a href="#41205006">prev</a><span>|</span><a href="#41204689">next</a><span>|</span><label class="collapse" for="c-41206729">[-]</label><label class="expand" for="c-41206729">[1 more]</label></div><br/><div class="children"><div class="content">Is DirectX support possible any time soon? This would be <i>huge</i> for Windows VMs on Linux...</div><br/></div></div></div></div></div></div><div id="41204517" class="c"><input type="checkbox" id="c-41204517" checked=""/><div class="controls bullet"><span class="by">dishsoap</span><span>|</span><a href="#41204617">prev</a><span>|</span><a href="#41204095">next</a><span>|</span><label class="collapse" for="c-41204517">[-]</label><label class="expand" for="c-41204517">[3 more]</label></div><br/><div class="children"><div class="content">For anyone curious about how this actually works, it looks like a library is injected into your process to hook these functions [1] in order to forward them to the service.<p>[1] <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;raw&#x2F;kCYmXr5A" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;raw&#x2F;kCYmXr5A</a></div><br/><div id="41206526" class="c"><input type="checkbox" id="c-41206526" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#41204517">parent</a><span>|</span><a href="#41204095">next</a><span>|</span><label class="collapse" for="c-41206526">[-]</label><label class="expand" for="c-41206526">[2 more]</label></div><br/><div class="children"><div class="content">How did you figure out these were hooked? I&#x27;m assuming some flag that tells ld&#x2F;ldd to tell you when some symbol is rebound? Also I thought a symbol has to be a weak symbol to be rebound and assuming nvidia doesn&#x27;t expose weak symbols (why would they) the implication is that their thing is basically LD_PRELOADed?</div><br/><div id="41206878" class="c"><input type="checkbox" id="c-41206878" checked=""/><div class="controls bullet"><span class="by">yarpen_z</span><span>|</span><a href="#41204517">root</a><span>|</span><a href="#41206526">parent</a><span>|</span><a href="#41204095">next</a><span>|</span><label class="collapse" for="c-41206878">[-]</label><label class="expand" for="c-41206878">[1 more]</label></div><br/><div class="children"><div class="content">Yes. While I don&#x27;t know what they do internally, API remoting has been used for GPUs since at least rCUDA - that&#x27;s over 10 years ago.<p>LD_PRELOAD trick allows you to intercept and virtualize calls to the CUDA runtime.</div><br/></div></div></div></div></div></div><div id="41204095" class="c"><input type="checkbox" id="c-41204095" checked=""/><div class="controls bullet"><span class="by">Cieric</span><span>|</span><a href="#41204517">prev</a><span>|</span><a href="#41205453">next</a><span>|</span><label class="collapse" for="c-41204095">[-]</label><label class="expand" for="c-41204095">[6 more]</label></div><br/><div class="children"><div class="content">This is interesting, but I&#x27;m more interested in self-hosting. I already have a lot of GPUs (some running some not.) Does this have a self-hosting option so I can use the GPUs I already have?</div><br/><div id="41204191" class="c"><input type="checkbox" id="c-41204191" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41204095">parent</a><span>|</span><a href="#41205228">next</a><span>|</span><label class="collapse" for="c-41204191">[-]</label><label class="expand" for="c-41204191">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t support self hosting yet but the same technology should work well here. Many of the same benefits apply in a self-hosted setting, namely efficient workload scheduling, GPU-sharing, and ease-of-use. Definitely open to this possibility in the future!</div><br/></div></div><div id="41205228" class="c"><input type="checkbox" id="c-41205228" checked=""/><div class="controls bullet"><span class="by">covi</span><span>|</span><a href="#41204095">parent</a><span>|</span><a href="#41204191">prev</a><span>|</span><a href="#41204971">next</a><span>|</span><label class="collapse" for="c-41205228">[-]</label><label class="expand" for="c-41205228">[1 more]</label></div><br/><div class="children"><div class="content">If you want to use your own GPUs or cloud accounts but with a great dev experience, see SkyPilot.</div><br/></div></div><div id="41204971" class="c"><input type="checkbox" id="c-41204971" checked=""/><div class="controls bullet"><span class="by">goku-goku</span><span>|</span><a href="#41204095">parent</a><span>|</span><a href="#41205228">prev</a><span>|</span><a href="#41205453">next</a><span>|</span><label class="collapse" for="c-41204971">[-]</label><label class="expand" for="c-41204971">[3 more]</label></div><br/><div class="children"><div class="content">Juice does have this ability today! :)<p>www.juicelabs.co</div><br/><div id="41205393" class="c"><input type="checkbox" id="c-41205393" checked=""/><div class="controls bullet"><span class="by">Cieric</span><span>|</span><a href="#41204095">root</a><span>|</span><a href="#41204971">parent</a><span>|</span><a href="#41206332">next</a><span>|</span><label class="collapse" for="c-41205393">[-]</label><label class="expand" for="c-41205393">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, but I&#x27;ve already evaluated JuiceLabs and it does not handle what I need it to. Plus with the whole project going commercial and the community edition being neglected I no longer have any interest in trying to support the project either.</div><br/></div></div><div id="41206332" class="c"><input type="checkbox" id="c-41206332" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#41204095">root</a><span>|</span><a href="#41204971">parent</a><span>|</span><a href="#41205393">prev</a><span>|</span><a href="#41205453">next</a><span>|</span><label class="collapse" for="c-41206332">[-]</label><label class="expand" for="c-41206332">[1 more]</label></div><br/><div class="children"><div class="content">Is it open source? The website seems to be one of those closed &quot;join a wait list&quot; sites</div><br/></div></div></div></div></div></div><div id="41205453" class="c"><input type="checkbox" id="c-41205453" checked=""/><div class="controls bullet"><span class="by">mmsc</span><span>|</span><a href="#41204095">prev</a><span>|</span><a href="#41207625">next</a><span>|</span><label class="collapse" for="c-41205453">[-]</label><label class="expand" for="c-41205453">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s it like to actually use this for any meaningful throughput? Can this be used for hash cracking? Every time I think about virtual GPUs over a network, I think about botnets. Specifically from <a href="https:&#x2F;&#x2F;www.hpcwire.com&#x2F;2012&#x2F;12&#x2F;06&#x2F;gpu_monster_shreds_password_hashes&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.hpcwire.com&#x2F;2012&#x2F;12&#x2F;06&#x2F;gpu_monster_shreds_passwo...</a> &quot;Gosney first had to convince Mosix co-creator Professor Amnon Barak that he was not going to “turn the world into a giant botnet.”&quot;</div><br/><div id="41205624" class="c"><input type="checkbox" id="c-41205624" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41205453">parent</a><span>|</span><a href="#41207625">next</a><span>|</span><label class="collapse" for="c-41205624">[-]</label><label class="expand" for="c-41205624">[1 more]</label></div><br/><div class="children"><div class="content">This is definitely an interesting thought experiment, however in practice our system is closer to AWS than a botnet, as the GPUs are not distributed. This technology does lend itself to some interesting applications with creating very flexible clusters within data centers that we are exploring.</div><br/></div></div></div></div><div id="41207625" class="c"><input type="checkbox" id="c-41207625" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#41205453">prev</a><span>|</span><a href="#41204005">next</a><span>|</span><label class="collapse" for="c-41207625">[-]</label><label class="expand" for="c-41207625">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s impressive that this is even possible, but I wonder what happens if the network connection goes down or is anything but 100% stable? In my experience drivers react badly to even a local GPU that isn&#x27;t behaving.</div><br/></div></div><div id="41204005" class="c"><input type="checkbox" id="c-41204005" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41207625">prev</a><span>|</span><a href="#41204021">next</a><span>|</span><label class="collapse" for="c-41204005">[-]</label><label class="expand" for="c-41204005">[2 more]</label></div><br/><div class="children"><div class="content">Given the interest here we decided to open up T4 instances for free. Would love for y&#x27;all to try it and let us know your thoughts!</div><br/><div id="41206860" class="c"><input type="checkbox" id="c-41206860" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#41204005">parent</a><span>|</span><a href="#41204021">next</a><span>|</span><label class="collapse" for="c-41206860">[-]</label><label class="expand" for="c-41206860">[1 more]</label></div><br/><div class="children"><div class="content">What is your A100 and H100 pricing?</div><br/></div></div></div></div><div id="41204021" class="c"><input type="checkbox" id="c-41204021" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41204005">prev</a><span>|</span><a href="#41203886">next</a><span>|</span><label class="collapse" for="c-41204021">[-]</label><label class="expand" for="c-41204021">[6 more]</label></div><br/><div class="children"><div class="content">This is neat. Were you able to get MIG or vGPUs working with it?</div><br/><div id="41204054" class="c"><input type="checkbox" id="c-41204054" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204021">parent</a><span>|</span><a href="#41203886">next</a><span>|</span><label class="collapse" for="c-41204054">[-]</label><label class="expand" for="c-41204054">[5 more]</label></div><br/><div class="children"><div class="content">We haven&#x27;t tested with MIG or vGPU, but I think it would work since it&#x27;s essentially physically partitioning the GPU.<p>One of our main goals for the near future is to allow GPU sharing. This would be better than MIG or vGPU since we&#x27;d allow users to use the entire GPU memory instead of restricting them to a fraction.</div><br/><div id="41204074" class="c"><input type="checkbox" id="c-41204074" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41204021">root</a><span>|</span><a href="#41204054">parent</a><span>|</span><a href="#41203886">next</a><span>|</span><label class="collapse" for="c-41204074">[-]</label><label class="expand" for="c-41204074">[4 more]</label></div><br/><div class="children"><div class="content">We had a hell of a time dealing with the licensing issues and ultimately just gave up and give people whole GPUs.<p>What are you doing to reset the GPU to clean state after a run? It&#x27;s surprisingly complicated to do this securely (we&#x27;re writing up a back-to-back sequence of audits we did with Atredis and Tetrel; should be publishing in a month or two).</div><br/><div id="41204227" class="c"><input type="checkbox" id="c-41204227" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204021">root</a><span>|</span><a href="#41204074">parent</a><span>|</span><a href="#41203886">next</a><span>|</span><label class="collapse" for="c-41204227">[-]</label><label class="expand" for="c-41204227">[3 more]</label></div><br/><div class="children"><div class="content">We kill the process to reset the GPU. Since we only store GPU state that&#x27;s the only clean up we need to do</div><br/><div id="41207485" class="c"><input type="checkbox" id="c-41207485" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#41204021">root</a><span>|</span><a href="#41204227">parent</a><span>|</span><a href="#41204618">next</a><span>|</span><label class="collapse" for="c-41207485">[-]</label><label class="expand" for="c-41207485">[1 more]</label></div><br/><div class="children"><div class="content">Won’t the VRAM still contain old bits?</div><br/></div></div><div id="41204618" class="c"><input type="checkbox" id="c-41204618" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41204021">root</a><span>|</span><a href="#41204227">parent</a><span>|</span><a href="#41207485">prev</a><span>|</span><a href="#41203886">next</a><span>|</span><label class="collapse" for="c-41204618">[-]</label><label class="expand" for="c-41204618">[1 more]</label></div><br/><div class="children"><div class="content">Hm. Ok. Well, this is all very cool! Congrats on shipping.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41203886" class="c"><input type="checkbox" id="c-41203886" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#41204021">prev</a><span>|</span><a href="#41204066">next</a><span>|</span><label class="collapse" for="c-41203886">[-]</label><label class="expand" for="c-41203886">[9 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get it. Why would I start an instance in ECS, to use your GPUs in ECS, when I could start an instance for the GPUs I want in ECS? Separately, why would I want half of Nitro, instead of real Nitro?</div><br/><div id="41203971" class="c"><input type="checkbox" id="c-41203971" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41203886">parent</a><span>|</span><a href="#41203970">next</a><span>|</span><label class="collapse" for="c-41203971">[-]</label><label class="expand" for="c-41203971">[5 more]</label></div><br/><div class="children"><div class="content">Great point, there are a few benefits:<p>1. If you&#x27;re actively developing and need a GPU then you typically would be paying the entire time the instance is running. Using Thunder means you only pay for the GPU while actively using it. Essentially, if you are running CPU only code you would not be paying for any GPU time. The alterative for this is to manually turn the instance on and off which can be annoying.<p>2. This allows you to easily scale the type and number of GPUs you&#x27;re using. For example, say you want to do development on a cheap T4 instance and run a full DL training job on a set of 8 A100. Instead of needing to swap instances and setup everything again, you can just run a command and then start running on the more powerful GPUs.</div><br/><div id="41205229" class="c"><input type="checkbox" id="c-41205229" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#41203886">root</a><span>|</span><a href="#41203971">parent</a><span>|</span><a href="#41203970">next</a><span>|</span><label class="collapse" for="c-41205229">[-]</label><label class="expand" for="c-41205229">[4 more]</label></div><br/><div class="children"><div class="content">Okay, but your GPUs are in ECS. Don&#x27;t I just want this feature from Amazon, not you, and natively via Nitro? Or even Google has TPU attachments.<p>&gt; 1. If you&#x27;re actively developing and need a GPU [for fractional amounts of time]...<p>Why would I need a GPU for a short amount of time during development? For testing?<p>I don&#x27;t get it - what would testing an H100 over a TCP connection tell me? It&#x27;s like, yeah, I can do that, but it doesn&#x27;t represent an environment I am going to use for real. Nobody runs applications to GPUs on buses virtualized over TCP connections, so what exactly would I be validating?</div><br/><div id="41205381" class="c"><input type="checkbox" id="c-41205381" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41203886">root</a><span>|</span><a href="#41205229">parent</a><span>|</span><a href="#41206291">next</a><span>|</span><label class="collapse" for="c-41205381">[-]</label><label class="expand" for="c-41205381">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe Nitro would allow you to access a GPU that&#x27;s not directly connected to the CPU that the VM is running on. So swapping between GPU type or scaling to multiple GPUs is still a problem.<p>From the developer perspective, you wouldn&#x27;t know that the H100 is across a network. The experience will be as if your computer is directly attached to an H100. The benefit here is that if you&#x27;re not actively using the H100 (such as when you&#x27;re setting up the instance or after the training job completes) you are not paying for the H100.</div><br/><div id="41205403" class="c"><input type="checkbox" id="c-41205403" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#41203886">root</a><span>|</span><a href="#41205381">parent</a><span>|</span><a href="#41206291">next</a><span>|</span><label class="collapse" for="c-41205403">[-]</label><label class="expand" for="c-41205403">[1 more]</label></div><br/><div class="children"><div class="content">Okay, a mock H100 object would also save me money. I could pretend a 3090 is an A100. “The experience would be that a 3090 is an A100.” Apples to oranges comparison? It’s using a GPU attached to the machine versus a GPU that crosses a VPC boundary. Do you see what I am saying?<p>I would never run a training job on a GPU virtualized over TCP connection. I would never run a training job that requires 80GB of VRAM on a 24GB VRAM device.<p>Whom is this for? Who needs to save kopecks on a single GPU who needs H100s?</div><br/></div></div></div></div><div id="41206291" class="c"><input type="checkbox" id="c-41206291" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#41203886">root</a><span>|</span><a href="#41205229">parent</a><span>|</span><a href="#41205381">prev</a><span>|</span><a href="#41203970">next</a><span>|</span><label class="collapse" for="c-41206291">[-]</label><label class="expand" for="c-41206291">[1 more]</label></div><br/><div class="children"><div class="content">I develop GPU accelerated web apps in an EC2 instance with a remote VSCode session. A lot of the time I’m just doing web dev and don’t need a GPU. I can save thousands per month by switching to this.</div><br/></div></div></div></div></div></div><div id="41203970" class="c"><input type="checkbox" id="c-41203970" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#41203886">parent</a><span>|</span><a href="#41203971">prev</a><span>|</span><a href="#41204066">next</a><span>|</span><label class="collapse" for="c-41203970">[-]</label><label class="expand" for="c-41203970">[3 more]</label></div><br/><div class="children"><div class="content">it&#x27;s more transparent to your system, for example, if you have a gui application that needs gpu acceleration on a thin client (Matlab, solidworks, blender), you can do so without setting up ECS. you can develop without any gpu, but suddenly have one when you need to run simulation. this will be way cheaper than AWS.<p>I think essentially this is solving the same problem Ray (<a href="https:&#x2F;&#x2F;www.ray.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ray.io&#x2F;</a>) is solving, but in a more generic way.<p>it potentially can have finer grained gpu sharing, like a half-gpu.<p>I&#x27;m very excited about this.</div><br/><div id="41204030" class="c"><input type="checkbox" id="c-41204030" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41203886">root</a><span>|</span><a href="#41203970">parent</a><span>|</span><a href="#41204066">next</a><span>|</span><label class="collapse" for="c-41204030">[-]</label><label class="expand" for="c-41204030">[2 more]</label></div><br/><div class="children"><div class="content">Exactly! The finer grain sharing is one of the key things on our radar right now</div><br/><div id="41204871" class="c"><input type="checkbox" id="c-41204871" checked=""/><div class="controls bullet"><span class="by">goku-goku</span><span>|</span><a href="#41203886">root</a><span>|</span><a href="#41204030">parent</a><span>|</span><a href="#41204066">next</a><span>|</span><label class="collapse" for="c-41204871">[-]</label><label class="expand" for="c-41204871">[1 more]</label></div><br/><div class="children"><div class="content">www.juicelabs.co does all this today, including the GPU sharing and fractionalization.</div><br/></div></div></div></div></div></div></div></div><div id="41204066" class="c"><input type="checkbox" id="c-41204066" checked=""/><div class="controls bullet"><span class="by">orsorna</span><span>|</span><a href="#41203886">prev</a><span>|</span><a href="#41204057">next</a><span>|</span><label class="collapse" for="c-41204066">[-]</label><label class="expand" for="c-41204066">[2 more]</label></div><br/><div class="children"><div class="content">So what exactly is the pricing model? Do I need a quote? Because otherwise I don&#x27;t see how to determine it without creating an account which is needlessly gatekeeping.</div><br/><div id="41204101" class="c"><input type="checkbox" id="c-41204101" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204066">parent</a><span>|</span><a href="#41204057">next</a><span>|</span><label class="collapse" for="c-41204101">[-]</label><label class="expand" for="c-41204101">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re still in our beta so it&#x27;s entirely free for now (we can&#x27;t promise a bug-free experience)! You have to make an account but it won&#x27;t require payment details.<p>Down the line we want to move to a pay-as-you-go model.</div><br/></div></div></div></div><div id="41204057" class="c"><input type="checkbox" id="c-41204057" checked=""/><div class="controls bullet"><span class="by">kawsper</span><span>|</span><a href="#41204066">prev</a><span>|</span><a href="#41204231">next</a><span>|</span><label class="collapse" for="c-41204057">[-]</label><label class="expand" for="c-41204057">[2 more]</label></div><br/><div class="children"><div class="content">Cool idea, nice product page!<p>Does anyone know if this is possible with USB?<p>I have a Davinci Resolve license USB-dongle I&#x27;d like to not plugging into my laptop.</div><br/><div id="41204106" class="c"><input type="checkbox" id="c-41204106" checked=""/><div class="controls bullet"><span class="by">kevmo314</span><span>|</span><a href="#41204057">parent</a><span>|</span><a href="#41204231">next</a><span>|</span><label class="collapse" for="c-41204106">[-]</label><label class="expand" for="c-41204106">[1 more]</label></div><br/><div class="children"><div class="content">You can do that with USB&#x2F;IP: <a href="https:&#x2F;&#x2F;usbip.sourceforge.net&#x2F;" rel="nofollow">https:&#x2F;&#x2F;usbip.sourceforge.net&#x2F;</a></div><br/></div></div></div></div><div id="41204231" class="c"><input type="checkbox" id="c-41204231" checked=""/><div class="controls bullet"><span class="by">the_reader</span><span>|</span><a href="#41204057">prev</a><span>|</span><a href="#41204154">next</a><span>|</span><label class="collapse" for="c-41204231">[-]</label><label class="expand" for="c-41204231">[5 more]</label></div><br/><div class="children"><div class="content">Would be possible to mix it with Blender?</div><br/><div id="41204248" class="c"><input type="checkbox" id="c-41204248" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204231">parent</a><span>|</span><a href="#41204154">next</a><span>|</span><label class="collapse" for="c-41204248">[-]</label><label class="expand" for="c-41204248">[4 more]</label></div><br/><div class="children"><div class="content">At the moment out tech is linux-only so it would not work with Blender.<p>Down the line, we could see this being used for batched render jobs (i.e. to replace a render farm).</div><br/><div id="41204299" class="c"><input type="checkbox" id="c-41204299" checked=""/><div class="controls bullet"><span class="by">comex</span><span>|</span><a href="#41204231">root</a><span>|</span><a href="#41204248">parent</a><span>|</span><a href="#41204154">next</a><span>|</span><label class="collapse" for="c-41204299">[-]</label><label class="expand" for="c-41204299">[3 more]</label></div><br/><div class="children"><div class="content">Blender can run on Linux…</div><br/><div id="41204402" class="c"><input type="checkbox" id="c-41204402" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204231">root</a><span>|</span><a href="#41204299">parent</a><span>|</span><a href="#41204154">next</a><span>|</span><label class="collapse" for="c-41204402">[-]</label><label class="expand" for="c-41204402">[2 more]</label></div><br/><div class="children"><div class="content">Oh nice, I didn&#x27;t know that! In that case it might work, you could try running `tnr run .&#x2F;blender` (replace the .&#x2F;blender with how you&#x27;d launch blender from the CLI) to see what happens. We haven&#x27;t tested it so I can&#x27;t make promises about performance or stability :)</div><br/><div id="41206982" class="c"><input type="checkbox" id="c-41206982" checked=""/><div class="controls bullet"><span class="by">chmod775</span><span>|</span><a href="#41204231">root</a><span>|</span><a href="#41204402">parent</a><span>|</span><a href="#41204154">next</a><span>|</span><label class="collapse" for="c-41206982">[-]</label><label class="expand" for="c-41206982">[1 more]</label></div><br/><div class="children"><div class="content"><i>Disclaimer: I only have a passing familiarity with Blender, so I might be wrong on some counts.</i><p>I think you&#x27;d want to run the blender GUI locally and only call out to a headless rendering server (&quot;render farm&quot;) that uses your service under the hood to get the actual render.<p>This separation is already something blender supports, and you could for instance use Blender on Windows despite your render farm using Linux servers.<p>Cloud rendering is adjacent to what you&#x27;re offering, and it should be trivial for you to expand into that space by just figuring out the setup and preparing a guide for users wishing to do that with your service.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41204154" class="c"><input type="checkbox" id="c-41204154" checked=""/><div class="controls bullet"><span class="by">rubatuga</span><span>|</span><a href="#41204231">prev</a><span>|</span><a href="#41204740">next</a><span>|</span><label class="collapse" for="c-41204154">[-]</label><label class="expand" for="c-41204154">[2 more]</label></div><br/><div class="children"><div class="content">What ML packages do you support? In the comments below it says you do not support Vulkan or OpenGL. Does this support AMD GPUs as well?</div><br/><div id="41204179" class="c"><input type="checkbox" id="c-41204179" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204154">parent</a><span>|</span><a href="#41204740">next</a><span>|</span><label class="collapse" for="c-41204179">[-]</label><label class="expand" for="c-41204179">[1 more]</label></div><br/><div class="children"><div class="content">We have tested this with pytorch and huggingface and it is mostly stable (we know there are issues with pycuda and jax). In theory this should work with any libraries, however we&#x27;re still actively developing this so bugs will show up</div><br/></div></div></div></div><div id="41204740" class="c"><input type="checkbox" id="c-41204740" checked=""/><div class="controls bullet"><span class="by">winecamera</span><span>|</span><a href="#41204154">prev</a><span>|</span><a href="#41203783">next</a><span>|</span><label class="collapse" for="c-41204740">[-]</label><label class="expand" for="c-41204740">[2 more]</label></div><br/><div class="children"><div class="content">I saw that in the tnr CLI, there are hints of an option to self-host a GPU. Is this going to be a released feature?</div><br/><div id="41204801" class="c"><input type="checkbox" id="c-41204801" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41204740">parent</a><span>|</span><a href="#41203783">next</a><span>|</span><label class="collapse" for="c-41204801">[-]</label><label class="expand" for="c-41204801">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t support self-hosting yet but are considering adding it in the future. We&#x27;re a small team working as hard as we can :)<p>Curious where you see this in the CLI, may be an oversight on our part. If you can join the Discord and point us to this bug we would really appreciate it!</div><br/></div></div></div></div><div id="41203783" class="c"><input type="checkbox" id="c-41203783" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#41204740">prev</a><span>|</span><a href="#41204145">next</a><span>|</span><label class="collapse" for="c-41203783">[-]</label><label class="expand" for="c-41203783">[5 more]</label></div><br/><div class="children"><div class="content">is this a remote nvapi?<p>this is awesome. can it do 3d rendering (vulkan&#x2F;opengl)</div><br/><div id="41203844" class="c"><input type="checkbox" id="c-41203844" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41203783">parent</a><span>|</span><a href="#41203838">next</a><span>|</span><label class="collapse" for="c-41203844">[-]</label><label class="expand" for="c-41203844">[2 more]</label></div><br/><div class="children"><div class="content">Thank you!<p>&gt; is this a remote nvapi<p>Essentially yes! Just to be clear, this covers the entire GPU not just the NVAPI (i.e. all of cuda). This functions like you have the physical card directly plugged into the machine.<p>Right now we don&#x27;t support vulkan or opengl since we&#x27;re mostly focusing on AI workloads, however we plan to support these in the future (especially if there is interest!)</div><br/><div id="41203879" class="c"><input type="checkbox" id="c-41203879" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#41203783">root</a><span>|</span><a href="#41203844">parent</a><span>|</span><a href="#41203838">next</a><span>|</span><label class="collapse" for="c-41203879">[-]</label><label class="expand" for="c-41203879">[1 more]</label></div><br/><div class="children"><div class="content">sorry, I didn&#x27;t mean nvapi, I meant rmapi.<p>I bet you saw this <a href="https:&#x2F;&#x2F;github.com&#x2F;mikex86&#x2F;LibreCuda">https:&#x2F;&#x2F;github.com&#x2F;mikex86&#x2F;LibreCuda</a><p>they implemented the cuda driver by calling into rmapi.<p>My understanding is if there is a remote rmapi, other user mode drivers should work out of the box?</div><br/></div></div></div></div><div id="41203838" class="c"><input type="checkbox" id="c-41203838" checked=""/><div class="controls bullet"><span class="by">czbond</span><span>|</span><a href="#41203783">parent</a><span>|</span><a href="#41203844">prev</a><span>|</span><a href="#41204145">next</a><span>|</span><label class="collapse" for="c-41203838">[-]</label><label class="expand" for="c-41203838">[2 more]</label></div><br/><div class="children"><div class="content">I am not in this &quot;space&quot;, but I second the &quot;this is cool to see&quot;, more stuff like this needed on HN.</div><br/><div id="41204232" class="c"><input type="checkbox" id="c-41204232" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41203783">root</a><span>|</span><a href="#41203838">parent</a><span>|</span><a href="#41204145">next</a><span>|</span><label class="collapse" for="c-41204232">[-]</label><label class="expand" for="c-41204232">[1 more]</label></div><br/><div class="children"><div class="content">Appreciate the praise!</div><br/></div></div></div></div></div></div><div id="41204145" class="c"><input type="checkbox" id="c-41204145" checked=""/><div class="controls bullet"><span class="by">throwaway888abc</span><span>|</span><a href="#41203783">prev</a><span>|</span><a href="#41204596">next</a><span>|</span><label class="collapse" for="c-41204145">[-]</label><label class="expand" for="c-41204145">[3 more]</label></div><br/><div class="children"><div class="content">Does it work for gaming on windows ? or even linux ?</div><br/><div id="41204161" class="c"><input type="checkbox" id="c-41204161" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41204145">parent</a><span>|</span><a href="#41207383">next</a><span>|</span><label class="collapse" for="c-41204161">[-]</label><label class="expand" for="c-41204161">[1 more]</label></div><br/><div class="children"><div class="content">In theory yes. In practice, however, latency between the CPU and remote GPU makes this impractical</div><br/></div></div><div id="41207383" class="c"><input type="checkbox" id="c-41207383" checked=""/><div class="controls bullet"><span class="by">boxerbk</span><span>|</span><a href="#41204145">parent</a><span>|</span><a href="#41204161">prev</a><span>|</span><a href="#41204596">next</a><span>|</span><label class="collapse" for="c-41207383">[-]</label><label class="expand" for="c-41207383">[1 more]</label></div><br/><div class="children"><div class="content">You could use a remote streaming protocol, like Parsec, for that. You&#x27;d need your own cloud account and connect directly to a GPU-enabled cloud machine. Otherwise, it would work to let you game.</div><br/></div></div></div></div><div id="41204596" class="c"><input type="checkbox" id="c-41204596" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#41204145">prev</a><span>|</span><a href="#41206233">next</a><span>|</span><label class="collapse" for="c-41204596">[-]</label><label class="expand" for="c-41204596">[3 more]</label></div><br/><div class="children"><div class="content">Reminds me of Plan9 :)</div><br/><div id="41205097" class="c"><input type="checkbox" id="c-41205097" checked=""/><div class="controls bullet"><span class="by">K0IN</span><span>|</span><a href="#41204596">parent</a><span>|</span><a href="#41206233">next</a><span>|</span><label class="collapse" for="c-41205097">[-]</label><label class="expand" for="c-41205097">[2 more]</label></div><br/><div class="children"><div class="content">can you elaborate a bit on why? (noob here)</div><br/><div id="41206223" class="c"><input type="checkbox" id="c-41206223" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#41204596">root</a><span>|</span><a href="#41205097">parent</a><span>|</span><a href="#41206233">next</a><span>|</span><label class="collapse" for="c-41206223">[-]</label><label class="expand" for="c-41206223">[1 more]</label></div><br/><div class="children"><div class="content">In Plan 9 everything is a file (for real this time). Remote file systems are accessible through the 9P protocol (still used in modern systems! I know it&#x27;s used in QEMU and WSL). Every process has its own view of the filesystem called a namespace. The implication of these three features is that remote resources can be transparently accessed as local resources by applications.</div><br/></div></div></div></div></div></div><div id="41206233" class="c"><input type="checkbox" id="c-41206233" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41204596">prev</a><span>|</span><a href="#41204957">next</a><span>|</span><label class="collapse" for="c-41206233">[-]</label><label class="expand" for="c-41206233">[2 more]</label></div><br/><div class="children"><div class="content">So won’t that make the network the prohibitive bottle neck? Your memory bandwidth is 1gbps max</div><br/><div id="41206273" class="c"><input type="checkbox" id="c-41206273" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#41206233">parent</a><span>|</span><a href="#41204957">next</a><span>|</span><label class="collapse" for="c-41206273">[-]</label><label class="expand" for="c-41206273">[1 more]</label></div><br/><div class="children"><div class="content">Cloud hosts will offer 10Gb&#x2F;s. Anyway, in my experience with training LoRAs and running DINOv2 inference you don’t need much bandwidth. We are usually sitting at around 10-30MB&#x2F;s per GPU.</div><br/></div></div></div></div><div id="41204234" class="c"><input type="checkbox" id="c-41204234" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#41204957">prev</a><span>|</span><a href="#41205188">next</a><span>|</span><label class="collapse" for="c-41204234">[-]</label><label class="expand" for="c-41204234">[3 more]</label></div><br/><div class="children"><div class="content">This could be perfect for us. We need very limited bandwidth but have high compute needs.</div><br/><div id="41204326" class="c"><input type="checkbox" id="c-41204326" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41204234">parent</a><span>|</span><a href="#41204983">next</a><span>|</span><label class="collapse" for="c-41204326">[-]</label><label class="expand" for="c-41204326">[1 more]</label></div><br/><div class="children"><div class="content">Awesome, we&#x27;d love to chat! You can reach us at founders@thundercompute.com or join the discord <a href="https:&#x2F;&#x2F;discord.gg&#x2F;nwuETS9jJK" rel="nofollow">https:&#x2F;&#x2F;discord.gg&#x2F;nwuETS9jJK</a>!</div><br/></div></div><div id="41204983" class="c"><input type="checkbox" id="c-41204983" checked=""/><div class="controls bullet"><span class="by">goku-goku</span><span>|</span><a href="#41204234">parent</a><span>|</span><a href="#41204326">prev</a><span>|</span><a href="#41205188">next</a><span>|</span><label class="collapse" for="c-41204983">[-]</label><label class="expand" for="c-41204983">[1 more]</label></div><br/><div class="children"><div class="content">Feel free to reach out www.juicelabs.co</div><br/></div></div></div></div><div id="41205188" class="c"><input type="checkbox" id="c-41205188" checked=""/><div class="controls bullet"><span class="by">test20240809</span><span>|</span><a href="#41204234">prev</a><span>|</span><a href="#41204281">next</a><span>|</span><label class="collapse" for="c-41205188">[-]</label><label class="expand" for="c-41205188">[2 more]</label></div><br/><div class="children"><div class="content">pocl (Portable Computing Language) [1] provides a remote backend [2] that allows for serialization and forwarding of OpenCL commands over a network.<p>Another solution is qCUDA [3] which is more specialized towards CUDA.<p>In addition to these solutions, various virtualization solutions today provide some sort of serialization mechanism for GPU commands, so they can be transferred to another host (or process). [4]<p>One example is the QEMU-based Android Emulator. It is using special translator libraries and a &quot;QEMU Pipe&quot; to efficiently communicate GPU commands from the virtualized Android OS to the host OS [5].<p>The new Cuttlefish Android emulator [6] uses Gallium3D for transport and the virglrenderer library [7].<p>I&#x27;d expect that the current virtio-gpu implementation in QEMU [8] might make this job even easier, because it includes the Android&#x27;s gfxstream [9] (formerly called &quot;Vulkan Cereal&quot;) that should already support communication over network sockets out of the box.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;pocl&#x2F;pocl">https:&#x2F;&#x2F;github.com&#x2F;pocl&#x2F;pocl</a><p>[2] <a href="https:&#x2F;&#x2F;portablecl.org&#x2F;docs&#x2F;html&#x2F;remote.html" rel="nofollow">https:&#x2F;&#x2F;portablecl.org&#x2F;docs&#x2F;html&#x2F;remote.html</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;coldfunction&#x2F;qCUDA">https:&#x2F;&#x2F;github.com&#x2F;coldfunction&#x2F;qCUDA</a><p>[4] <a href="https:&#x2F;&#x2F;www.linaro.org&#x2F;blog&#x2F;a-closer-look-at-virtio-and-gpu-virtualisation&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.linaro.org&#x2F;blog&#x2F;a-closer-look-at-virtio-and-gpu-...</a><p>[5] <a href="https:&#x2F;&#x2F;android.googlesource.com&#x2F;platform&#x2F;external&#x2F;qemu&#x2F;+&#x2F;emu-master-dev&#x2F;android&#x2F;android-emugl&#x2F;DESIGN" rel="nofollow">https:&#x2F;&#x2F;android.googlesource.com&#x2F;platform&#x2F;external&#x2F;qemu&#x2F;+&#x2F;em...</a><p>[6] <a href="https:&#x2F;&#x2F;source.android.com&#x2F;docs&#x2F;devices&#x2F;cuttlefish&#x2F;gpu" rel="nofollow">https:&#x2F;&#x2F;source.android.com&#x2F;docs&#x2F;devices&#x2F;cuttlefish&#x2F;gpu</a><p>[7] <a href="https:&#x2F;&#x2F;cs.android.com&#x2F;android&#x2F;platform&#x2F;superproject&#x2F;main&#x2F;+&#x2F;main:device&#x2F;generic&#x2F;opengl-transport&#x2F;host&#x2F;libs&#x2F;virglrenderer&#x2F;README.md" rel="nofollow">https:&#x2F;&#x2F;cs.android.com&#x2F;android&#x2F;platform&#x2F;superproject&#x2F;main&#x2F;+&#x2F;...</a><p>[8] <a href="https:&#x2F;&#x2F;www.qemu.org&#x2F;docs&#x2F;master&#x2F;system&#x2F;devices&#x2F;virtio-gpu.html" rel="nofollow">https:&#x2F;&#x2F;www.qemu.org&#x2F;docs&#x2F;master&#x2F;system&#x2F;devices&#x2F;virtio-gpu.h...</a><p>[9] <a href="https:&#x2F;&#x2F;android.googlesource.com&#x2F;platform&#x2F;hardware&#x2F;google&#x2F;gfxstream&#x2F;" rel="nofollow">https:&#x2F;&#x2F;android.googlesource.com&#x2F;platform&#x2F;hardware&#x2F;google&#x2F;gf...</a></div><br/><div id="41205269" class="c"><input type="checkbox" id="c-41205269" checked=""/><div class="controls bullet"><span class="by">fpoling</span><span>|</span><a href="#41205188">parent</a><span>|</span><a href="#41204281">next</a><span>|</span><label class="collapse" for="c-41205269">[-]</label><label class="expand" for="c-41205269">[1 more]</label></div><br/><div class="children"><div class="content">Zscaler uses a similar approach in their remote browser. WebGL in the local browser exposed as a GPU to a Chromium instance in the cloud.</div><br/></div></div></div></div><div id="41204281" class="c"><input type="checkbox" id="c-41204281" checked=""/><div class="controls bullet"><span class="by">bkitano19</span><span>|</span><a href="#41205188">prev</a><span>|</span><a href="#41203681">next</a><span>|</span><label class="collapse" for="c-41204281">[-]</label><label class="expand" for="c-41204281">[3 more]</label></div><br/><div class="children"><div class="content">this is nuts</div><br/><div id="41204330" class="c"><input type="checkbox" id="c-41204330" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41204281">parent</a><span>|</span><a href="#41204872">next</a><span>|</span><label class="collapse" for="c-41204330">[-]</label><label class="expand" for="c-41204330">[1 more]</label></div><br/><div class="children"><div class="content">We think so too, big things coming :)</div><br/></div></div><div id="41204872" class="c"><input type="checkbox" id="c-41204872" checked=""/><div class="controls bullet"><span class="by">goku-goku</span><span>|</span><a href="#41204281">parent</a><span>|</span><a href="#41204330">prev</a><span>|</span><a href="#41203681">next</a><span>|</span><label class="collapse" for="c-41204872">[-]</label><label class="expand" for="c-41204872">[1 more]</label></div><br/><div class="children"><div class="content">www.juicelabs.co</div><br/></div></div></div></div><div id="41203681" class="c"><input type="checkbox" id="c-41203681" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#41204281">prev</a><span>|</span><a href="#41204109">next</a><span>|</span><label class="collapse" for="c-41203681">[-]</label><label class="expand" for="c-41203681">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Access serverless GPUs through a simple CLI to run your existing code on the cloud while being billed precisely for usage<p>Hmm... well I just watched you run nvidia-smi in a Mac terminal, which is a platform it&#x27;s explicitly not supported on. My instant assumption is that your tool copies my code into a private server instance and communicates back and forth to run the commands.<p>Does this platform expose eGPU capabilities if my host machine supports it? Can I run raster workloads or network it with my own CUDA hardware? The actual way your tool and service connects isn&#x27;t very clear to me and I assume other developers will be confused too.</div><br/><div id="41203806" class="c"><input type="checkbox" id="c-41203806" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41203681">parent</a><span>|</span><a href="#41204109">next</a><span>|</span><label class="collapse" for="c-41203806">[-]</label><label class="expand" for="c-41203806">[7 more]</label></div><br/><div class="children"><div class="content">Great questions! To clarify the demo, we were ssh&#x27;d into a linux machine with no GPU.<p>Going into more details for how this works, we intercept communication between the CPU and the GPU so only GPU code and commands are sent across the network to a GPU that we are hosting. This way we are able to virtualize a remote GPU and make your computer think it&#x27;s directly attached to that GPU.<p>We are not copying your CPU code and running it on our machines. The CPU code runs entirely on your instance (meaning no files need to be copied over or packages installed on the GPU machine). One of the benefits of this approach is that you can easily scale to a more &#x2F; less powerful GPU without needing to setup a new server.</div><br/><div id="41203829" class="c"><input type="checkbox" id="c-41203829" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#41203681">root</a><span>|</span><a href="#41203806">parent</a><span>|</span><a href="#41204109">next</a><span>|</span><label class="collapse" for="c-41203829">[-]</label><label class="expand" for="c-41203829">[6 more]</label></div><br/><div class="children"><div class="content">does this mean you have a customized&#x2F;dummy kernel gpu driver?<p>will that cause system instability, say, if the network suddenly dropped?</div><br/><div id="41203853" class="c"><input type="checkbox" id="c-41203853" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41203681">root</a><span>|</span><a href="#41203829">parent</a><span>|</span><a href="#41204109">next</a><span>|</span><label class="collapse" for="c-41203853">[-]</label><label class="expand" for="c-41203853">[5 more]</label></div><br/><div class="children"><div class="content">We are not writing any kernel drivers, this runs entirely in userspace (this won&#x27;t result in a crowdstrike level crash haha).<p>Given that, if the network suddenly dropped then only the process using the GPU would fail.</div><br/><div id="41204067" class="c"><input type="checkbox" id="c-41204067" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#41203681">root</a><span>|</span><a href="#41203853">parent</a><span>|</span><a href="#41204109">next</a><span>|</span><label class="collapse" for="c-41204067">[-]</label><label class="expand" for="c-41204067">[4 more]</label></div><br/><div class="children"><div class="content">How do you do that exactly? Are you using eBPF or something else?<p>Also, for my ML workloads the most common bottleneck is GPU VRAM &lt;-&gt; RAM copies. Doesn&#x27;t this dramatically increase latency? Or is it more like it increases latency on first data transfer, but as long as you dump everything into VRAM all at once at the beginning you&#x27;re fine? I&#x27;d expect this wouldn&#x27;t play super well with stuff like PyTorch data loaders, but would be curious to hear how you&#x27;ve faired when testing.</div><br/><div id="41204168" class="c"><input type="checkbox" id="c-41204168" checked=""/><div class="controls bullet"><span class="by">bmodel</span><span>|</span><a href="#41203681">root</a><span>|</span><a href="#41204067">parent</a><span>|</span><a href="#41204109">next</a><span>|</span><label class="collapse" for="c-41204168">[-]</label><label class="expand" for="c-41204168">[3 more]</label></div><br/><div class="children"><div class="content">We intercept api calls and use our own implementation to forward them to a remote machine. No eBPF (which I believe need to run in the kernel).<p>As for latency, we&#x27;ve done a lot of work to minimize that as much as possible. You can see the performance we get running inference on BERT from huggingface here: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;qsOBFQZtsFM?t=64" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;qsOBFQZtsFM?t=64</a>. It&#x27;s still slower than local (mainly for training workloads) but not by as much as you&#x27;d expect. We&#x27;re aiming to reach near parity in the next few months!</div><br/><div id="41206345" class="c"><input type="checkbox" id="c-41206345" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#41203681">root</a><span>|</span><a href="#41204168">parent</a><span>|</span><a href="#41204979">next</a><span>|</span><label class="collapse" for="c-41206345">[-]</label><label class="expand" for="c-41206345">[1 more]</label></div><br/><div class="children"><div class="content">Got it. eBPF module run as part of the kernel, but they&#x27;re still user space programs.<p>I would would consider using a larger model for demonstrating inference performance as I have 7B models deployed on CPU at work, but GPU is still important training BERT size models.</div><br/></div></div><div id="41204979" class="c"><input type="checkbox" id="c-41204979" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#41203681">root</a><span>|</span><a href="#41204168">parent</a><span>|</span><a href="#41206345">prev</a><span>|</span><a href="#41204109">next</a><span>|</span><label class="collapse" for="c-41204979">[-]</label><label class="expand" for="c-41204979">[1 more]</label></div><br/><div class="children"><div class="content">When you release a self-host version, what would be really neat would be to see it across HFT focused NICs that have huge TCP buffers...<p><a href="https:&#x2F;&#x2F;www.arista.com&#x2F;assets&#x2F;data&#x2F;pdf&#x2F;HFT&#x2F;HFTTradingNetworkPrimer.pdf" rel="nofollow">https:&#x2F;&#x2F;www.arista.com&#x2F;assets&#x2F;data&#x2F;pdf&#x2F;HFT&#x2F;HFTTradingNetwork...</a><p>Basically taking into account the large buffers and super-time-sensitive nature of HFT networking optimizations, I wonder if your TCP&lt;--&gt;GPU might benefit from both the HW and the learnings of NFT stylings?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41204109" class="c"><input type="checkbox" id="c-41204109" checked=""/><div class="controls bullet"><span class="by">cpeterson42</span><span>|</span><a href="#41203681">prev</a><span>|</span><label class="collapse" for="c-41204109">[-]</label><label class="expand" for="c-41204109">[1 more]</label></div><br/><div class="children"><div class="content">We created a discord for the latest updates, bug reports, feature suggestions, and memes. We will try to respond to any issues and suggestions as quickly as we can! Feel free to join here: <a href="https:&#x2F;&#x2F;discord.gg&#x2F;nwuETS9jJK" rel="nofollow">https:&#x2F;&#x2F;discord.gg&#x2F;nwuETS9jJK</a></div><br/></div></div></div></div></div></div></div></body></html>