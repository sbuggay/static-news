<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1732266086309" as="style"/><link rel="stylesheet" href="styles.css?v=1732266086309"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/PaulPauls/llama3_interpretability_sae">Show HN: Llama 3.2 Interpretability with Sparse Autoencoders</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>PaulPauls</span> | <span>42 comments</span></div><br/><div><div id="42209839" class="c"><input type="checkbox" id="c-42209839" checked=""/><div class="controls bullet"><span class="by">foundry27</span><span>|</span><a href="#42210024">next</a><span>|</span><label class="collapse" for="c-42209839">[-]</label><label class="expand" for="c-42209839">[23 more]</label></div><br/><div class="children"><div class="content">For anyone who hasn’t seen this before, mechanistic interpretability solves a very common problem with LLMs: when you ask a model to explain itself, you’re playing a game of rhetoric where the model tries to “convince” you of a reason for what it did by generating a plausible-sounding answer based on patterns in its training data. But unlike most trends of benchmark numbers getting better as models improve, more powerful models often score worse on tests designed to self-detect “untruthfulness” because they have stronger rhetoric, and are therefore more compelling at justifying lies after the fact. The objective is coherence, not truth.<p>Rhetoric isn’t reasoning. True explainability, like what overfitted Sparse Autoencoders claim they offer, basically results in the causal sequence of “thoughts” the model went through as it produces an answer. It’s the same way you may have a bunch of ephemeral thoughts in different directions while you think about anything.</div><br/><div id="42209976" class="c"><input type="checkbox" id="c-42209976" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#42209839">parent</a><span>|</span><a href="#42211240">next</a><span>|</span><label class="collapse" for="c-42209976">[-]</label><label class="expand" for="c-42209976">[6 more]</label></div><br/><div class="children"><div class="content">I want to point out here that people do the same: a lot of the time we don&#x27;t know why we thought or did something, but we&#x27;ll confabulate plausible-sounding rhetoric after the fact.</div><br/><div id="42212158" class="c"><input type="checkbox" id="c-42212158" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42209976">parent</a><span>|</span><a href="#42211372">next</a><span>|</span><label class="collapse" for="c-42212158">[-]</label><label class="expand" for="c-42212158">[2 more]</label></div><br/><div class="children"><div class="content">&#x2F;Some&#x2F; people bullshit themselves stating the plausible; others check their hypotheses.<p>The difference is total in both humans and automated processes.</div><br/><div id="42212162" class="c"><input type="checkbox" id="c-42212162" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42212158">parent</a><span>|</span><a href="#42211372">next</a><span>|</span><label class="collapse" for="c-42212162">[-]</label><label class="expand" for="c-42212162">[1 more]</label></div><br/><div class="children"><div class="content">How are you going to check your hypotheses for why you preferred that jacket to that other jacket?</div><br/></div></div></div></div><div id="42211372" class="c"><input type="checkbox" id="c-42211372" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42209976">parent</a><span>|</span><a href="#42212158">prev</a><span>|</span><a href="#42211403">next</a><span>|</span><label class="collapse" for="c-42211372">[-]</label><label class="expand" for="c-42211372">[2 more]</label></div><br/><div class="children"><div class="content">Not in math.</div><br/><div id="42211546" class="c"><input type="checkbox" id="c-42211546" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42211372">parent</a><span>|</span><a href="#42211403">next</a><span>|</span><label class="collapse" for="c-42211546">[-]</label><label class="expand" for="c-42211546">[1 more]</label></div><br/><div class="children"><div class="content">Yes in math. Formalisms come <i>after</i> casual thoughts, at every step.</div><br/></div></div></div></div><div id="42211403" class="c"><input type="checkbox" id="c-42211403" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42209976">parent</a><span>|</span><a href="#42211372">prev</a><span>|</span><a href="#42211240">next</a><span>|</span><label class="collapse" for="c-42211403">[-]</label><label class="expand" for="c-42211403">[1 more]</label></div><br/><div class="children"><div class="content">The split-brain experiment is one of my favorites! <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wfYbgdo8e-8" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wfYbgdo8e-8</a></div><br/></div></div></div></div><div id="42211240" class="c"><input type="checkbox" id="c-42211240" checked=""/><div class="controls bullet"><span class="by">snthpy</span><span>|</span><a href="#42209839">parent</a><span>|</span><a href="#42209976">prev</a><span>|</span><a href="#42210551">next</a><span>|</span><label class="collapse" for="c-42211240">[-]</label><label class="expand" for="c-42211240">[7 more]</label></div><br/><div class="children"><div class="content">A{rt,I} imitating life<p>I believe that&#x27;s why humans reason too.  We make snap judgements and then use reason to try to convince others of our beliefs. Can&#x27;t recall the reference right now but they argued that it&#x27;s really a tool for social influence. That also explains why people who are good at it find it hard to admit when they are wrong - they&#x27;re not used to having to do it because they can usually out argue others. Prominent examples are easy to find - X marks de spot.</div><br/><div id="42212141" class="c"><input type="checkbox" id="c-42212141" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42211240">parent</a><span>|</span><a href="#42211658">next</a><span>|</span><label class="collapse" for="c-42212141">[-]</label><label class="expand" for="c-42212141">[1 more]</label></div><br/><div class="children"><div class="content">Already before Galileo we had experiments to determine whether ideas represented reality or not. And in crucial cases, long before that, it meant life or death. This will be clear to engineers.<p>«Reason» is part of that mechanism of vetting ideas. You experience massive failures without it.<p>So, no, trained judgement is a real thing, and the presence of innumerable incompetent do not prove an alleged absence of the competent.</div><br/></div></div><div id="42211658" class="c"><input type="checkbox" id="c-42211658" checked=""/><div class="controls bullet"><span class="by">omgwtfbyobbq</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42211240">parent</a><span>|</span><a href="#42212141">prev</a><span>|</span><a href="#42211705">next</a><span>|</span><label class="collapse" for="c-42211658">[-]</label><label class="expand" for="c-42211658">[1 more]</label></div><br/><div class="children"><div class="content">I think Robert Sapolsky&#x27;s lectures on yt cover this to some degree around 115.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;wLE71i4JJiM?feature=shared" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;wLE71i4JJiM?feature=shared</a><p>Sometimes our cortex is in charge, sometimes other parts of our brain are, and we can&#x27;t tell the difference. Regardless, if we try to justify it later, that justification isn&#x27;t always coherent because we&#x27;re not always using the part of our brain we consider to be rational.</div><br/></div></div><div id="42211705" class="c"><input type="checkbox" id="c-42211705" checked=""/><div class="controls bullet"><span class="by">shshshshs</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42211240">parent</a><span>|</span><a href="#42211658">prev</a><span>|</span><a href="#42211371">next</a><span>|</span><label class="collapse" for="c-42211705">[-]</label><label class="expand" for="c-42211705">[3 more]</label></div><br/><div class="children"><div class="content">People who are good at reasoning find it hard to admit that they were wrong?<p>That’s not my experience. People with reason are.. reasonable.<p>You mention X and that’s not where the reasoners are. That’s where the (wanna be) politicians are. Rhetoric is not all of reasoning.<p>I can agree that rationalizing snap judgements is <i>one</i> of our capabilities but I am totally unconvinced that it is the totality of our reasoning capabilities. Perhaps I misunderstood.</div><br/><div id="42212022" class="c"><input type="checkbox" id="c-42212022" checked=""/><div class="controls bullet"><span class="by">Hedepig</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42211705">parent</a><span>|</span><a href="#42211371">next</a><span>|</span><label class="collapse" for="c-42212022">[-]</label><label class="expand" for="c-42212022">[2 more]</label></div><br/><div class="children"><div class="content">This is not totally my experience, I&#x27;ve debated a successful engineer who by all accounts has good reasoning skills, but he will absolutely double down on unreasonable ideas he&#x27;s made on the fly he if can find what he considers a coherent argument behind them. Sometimes if I absolutely can prove him wrong he&#x27;ll change his mind.<p>But I think this is ego getting in the way, and our reluctance to change our minds.<p>We like to point to artificial intelligence and explain how it works differently and then say therefore it&#x27;s not &quot;true reasoning&quot;. I&#x27;m not sure that&#x27;s a good conclusion. We should look at the output and decide. As flawed as it is, I think it&#x27;s rather impressive</div><br/><div id="42212205" class="c"><input type="checkbox" id="c-42212205" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42212022">parent</a><span>|</span><a href="#42211371">next</a><span>|</span><label class="collapse" for="c-42212205">[-]</label><label class="expand" for="c-42212205">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>ego getting in the way</i><p>That thing which was in fact identified thousands of years ago as the evil to ditch.<p>&gt; <i>reluctance to change our minds</i><p>That is clumsiness in a general drive that makes sense and is recognized part of the Belief Change Theory: epistemic change is conservative. I.e., when you revise a body of knowledge you do not want to lose valid notion. But conversely, you do not want to be unable to see change or errors, so there is a balance.<p>&gt; <i>it&#x27;s not &quot;true reasoning&quot;</i><p>If it shows not to explicitly check its &quot;spontaneous&quot; ideas, then it is a correct formula to say &#x27;it&#x27;s not &quot;true reasoning&quot;&#x27;.</div><br/></div></div></div></div></div></div><div id="42211371" class="c"><input type="checkbox" id="c-42211371" checked=""/><div class="controls bullet"><span class="by">briffid</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42211240">parent</a><span>|</span><a href="#42211705">prev</a><span>|</span><a href="#42210551">next</a><span>|</span><label class="collapse" for="c-42211371">[-]</label><label class="expand" for="c-42211371">[1 more]</label></div><br/><div class="children"><div class="content">Jonathan Haidt&#x27;s The Righteous Mind describes this ín details.</div><br/></div></div></div></div><div id="42210551" class="c"><input type="checkbox" id="c-42210551" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42209839">parent</a><span>|</span><a href="#42211240">prev</a><span>|</span><a href="#42210223">next</a><span>|</span><label class="collapse" for="c-42210551">[-]</label><label class="expand" for="c-42210551">[4 more]</label></div><br/><div class="children"><div class="content">A lot of the mech interp stuff has seemed to me like a different kind of voodoo: the Integer Quantum Hall Effect? Overloading the term “Superposition” in a weird analogy not governed by serious group representation theory and some clear symmetry? You guys are reaching. And I’ve read all the papers. Spot the postdoc who decided to get paid.<p>But there is one thing in particular that I’ll acknowledge as a great insight and the beginnings of a very plausible research agenda: bounded near orthogonal vector spaces are wildly counterintuitive in high dimensions and there are existing results around it that create scope for rigor [1].<p>[1] <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstrauss_lemma" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstraus...</a></div><br/><div id="42210779" class="c"><input type="checkbox" id="c-42210779" checked=""/><div class="controls bullet"><span class="by">txnf</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42210551">parent</a><span>|</span><a href="#42210829">next</a><span>|</span><label class="collapse" for="c-42210779">[-]</label><label class="expand" for="c-42210779">[1 more]</label></div><br/><div class="children"><div class="content">Superposition code is a well known concept in information theory - I think there is certainly more to the story then described in the current works, but it does feel like they are going in the right direction</div><br/></div></div><div id="42210829" class="c"><input type="checkbox" id="c-42210829" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42210551">parent</a><span>|</span><a href="#42210779">prev</a><span>|</span><a href="#42210223">next</a><span>|</span><label class="collapse" for="c-42210829">[-]</label><label class="expand" for="c-42210829">[2 more]</label></div><br/><div class="children"><div class="content">Where are you seeing the integer quantum Hall effect mentioned? Or are you bringing it up rather than responding to it being brought up elsewhere? I don’t understand what the connection between IQHE and these SAE interpretability approaches is supposed to be.</div><br/><div id="42211243" class="c"><input type="checkbox" id="c-42211243" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42210829">parent</a><span>|</span><a href="#42210223">next</a><span>|</span><label class="collapse" for="c-42211243">[-]</label><label class="expand" for="c-42211243">[1 more]</label></div><br/><div class="children"><div class="content">Pardon me, the reference is to the fractional Hall effect.<p>&quot;But our results may also be of broader interest. We find preliminary evidence that superposition may be linked to adversarial examples and grokking, and might also suggest a theory for the performance of mixture of experts models. More broadly, the toy model we investigate has unexpectedly rich structure, exhibiting phase changes, a geometric structure based on uniform polytopes, &quot;energy level&quot;-like jumps during training, and a phenomenon which is qualitatively similar to the fractional quantum Hall effect in physics, among other striking phenomena. We originally investigated the subject to gain understanding of cleanly-interpretable neurons in larger models, but we&#x27;ve found these toy models to be surprisingly interesting in their own right.&quot;<p><a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2022&#x2F;toy_model&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2022&#x2F;toy_model&#x2F;index.html</a></div><br/></div></div></div></div></div></div><div id="42210223" class="c"><input type="checkbox" id="c-42210223" checked=""/><div class="controls bullet"><span class="by">Onavo</span><span>|</span><a href="#42209839">parent</a><span>|</span><a href="#42210551">prev</a><span>|</span><a href="#42210215">next</a><span>|</span><label class="collapse" for="c-42210223">[-]</label><label class="expand" for="c-42210223">[1 more]</label></div><br/><div class="children"><div class="content">How does the causality part work? Can it spit out a graphical model?</div><br/></div></div><div id="42210215" class="c"><input type="checkbox" id="c-42210215" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#42209839">parent</a><span>|</span><a href="#42210223">prev</a><span>|</span><a href="#42210024">next</a><span>|</span><label class="collapse" for="c-42210215">[-]</label><label class="expand" for="c-42210215">[4 more]</label></div><br/><div class="children"><div class="content">I stopped at: &quot;causal sequence of “thoughts” &quot;</div><br/><div id="42210349" class="c"><input type="checkbox" id="c-42210349" checked=""/><div class="controls bullet"><span class="by">benchmarkist</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42210215">parent</a><span>|</span><a href="#42210024">next</a><span>|</span><label class="collapse" for="c-42210349">[-]</label><label class="expand" for="c-42210349">[3 more]</label></div><br/><div class="children"><div class="content">Interpretability research is basically a projection of the original function implemented by the neural network onto a sub-space of &quot;explanatory&quot; functions that people consider to be more understandable. You&#x27;re right that the words they use to sell the research is completely nonsensical because the abstract process has nothing to do with anything causal.</div><br/><div id="42210754" class="c"><input type="checkbox" id="c-42210754" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42210349">parent</a><span>|</span><a href="#42210024">next</a><span>|</span><label class="collapse" for="c-42210754">[-]</label><label class="expand" for="c-42210754">[2 more]</label></div><br/><div class="children"><div class="content">All code is causal.</div><br/><div id="42210772" class="c"><input type="checkbox" id="c-42210772" checked=""/><div class="controls bullet"><span class="by">benchmarkist</span><span>|</span><a href="#42209839">root</a><span>|</span><a href="#42210754">parent</a><span>|</span><a href="#42210024">next</a><span>|</span><label class="collapse" for="c-42210772">[-]</label><label class="expand" for="c-42210772">[1 more]</label></div><br/><div class="children"><div class="content">Which makes it entirely irrelevant as a descriptive term.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42210024" class="c"><input type="checkbox" id="c-42210024" checked=""/><div class="controls bullet"><span class="by">jwuphysics</span><span>|</span><a href="#42209839">prev</a><span>|</span><a href="#42210668">next</a><span>|</span><label class="collapse" for="c-42210024">[-]</label><label class="expand" for="c-42210024">[2 more]</label></div><br/><div class="children"><div class="content">Incredible, well-documented work -- this is an amazing effort!<p>Two things that caught my eye were (i) your loss curves and (ii) the assessment of dead latents. Our team also studied SAEs -- trained to reconstruct dense embeddings of paper abstracts rather than individual tokens [1]. We observed a power-law scaling of the lower bound of loss curves, even when we varied the sparsity level and the dimensionality of the SAE latent space. We also were able to totally mitigate dead latents with an auxiliary loss, and we saw smooth sinusoidal patterns throughout training iterations. Not sure if these were due to the specific application we performed (over paper abstracts embeddings) or if they represent more general phenomena.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.00657" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.00657</a></div><br/><div id="42210325" class="c"><input type="checkbox" id="c-42210325" checked=""/><div class="controls bullet"><span class="by">PaulPauls</span><span>|</span><a href="#42210024">parent</a><span>|</span><a href="#42210668">next</a><span>|</span><label class="collapse" for="c-42210325">[-]</label><label class="expand" for="c-42210325">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m very happy you appreciate it - particularly the documentation. Writing the documentation was much harder for me than writing the code so I&#x27;m happy it is appreciated. I furthermore downloaded your paper and will read through it tomorrow morning - thank you for sharing it!</div><br/></div></div></div></div><div id="42210668" class="c"><input type="checkbox" id="c-42210668" checked=""/><div class="controls bullet"><span class="by">Eliezer</span><span>|</span><a href="#42210024">prev</a><span>|</span><a href="#42209072">next</a><span>|</span><label class="collapse" for="c-42210668">[-]</label><label class="expand" for="c-42210668">[1 more]</label></div><br/><div class="children"><div class="content">This seems like decent alignment-positive work on a glance, though I haven&#x27;t checked full details yet.  I probably can&#x27;t make it happen, but how much would someone need to pay you to make up your time, expense, and risk?</div><br/></div></div><div id="42209072" class="c"><input type="checkbox" id="c-42209072" checked=""/><div class="controls bullet"><span class="by">curious_cat_163</span><span>|</span><a href="#42210668">prev</a><span>|</span><a href="#42211493">next</a><span>|</span><label class="collapse" for="c-42209072">[-]</label><label class="expand" for="c-42209072">[3 more]</label></div><br/><div class="children"><div class="content">Hey - Thanks for sharing!<p>Will take a closer look later but if you are hanging around now, it might be worth asking this now. I read this blog post recently:<p><a href="https:&#x2F;&#x2F;adamkarvonen.github.io&#x2F;machine_learning&#x2F;2024&#x2F;06&#x2F;11&#x2F;sae-intuitions.html" rel="nofollow">https:&#x2F;&#x2F;adamkarvonen.github.io&#x2F;machine_learning&#x2F;2024&#x2F;06&#x2F;11&#x2F;s...</a><p>And the author talks about challenges with evaluating SAEs. I wonder how you tackled that and where to look inside your repo for understanding the your approach around that if possible.<p>Thanks again!</div><br/><div id="42210293" class="c"><input type="checkbox" id="c-42210293" checked=""/><div class="controls bullet"><span class="by">PaulPauls</span><span>|</span><a href="#42209072">parent</a><span>|</span><a href="#42211493">next</a><span>|</span><label class="collapse" for="c-42210293">[-]</label><label class="expand" for="c-42210293">[2 more]</label></div><br/><div class="children"><div class="content">So evaluating SAEs - determining which SAE is better at creating the most unique features while being as sparse as possible at the same time - is a very complex topic that is very much at the heart of the current research into LLM interpretability through SAEs.<p>Assuming you already solved the problem of finding multiple perfect SAE architectures and you trained them to perfection (very much an interesting ML engineering problem that this SAE project attempts to solve) then deciding on which SAE is better comes down to which SAE performs better on the metrics of your automated interpretability methodology. Particularly OpenAI&#x27;s methodology emphasizes this automated interpretability at scale utilizing a lot of technical metrics upon which the SAEs can be scored _and thereby evaluated_.<p>Since determining the best metrics and methodology is such an open research question that I could&#x27;ve experimented on for a few additional months, have I instead opted for a simple approach in this first release. I am talking about my and OpenAI&#x27;s methodology and the differences between both in chapter 4. Interpretability Analysis [1] in my Implementation Details &amp; Results section. I can also recommend reading the OpenAI paper directly or visiting Anthropics transformer-circuits.pub website that often publishes smaller blog posts on exactly this topic.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;PaulPauls&#x2F;llama3_interpretability_sae#4-interpretability-analysis">https:&#x2F;&#x2F;github.com&#x2F;PaulPauls&#x2F;llama3_interpretability_sae#4-i...</a>
[2] <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;</a></div><br/><div id="42211084" class="c"><input type="checkbox" id="c-42211084" checked=""/><div class="controls bullet"><span class="by">curious_cat_163</span><span>|</span><a href="#42209072">root</a><span>|</span><a href="#42210293">parent</a><span>|</span><a href="#42211493">next</a><span>|</span><label class="collapse" for="c-42211084">[-]</label><label class="expand" for="c-42211084">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!</div><br/></div></div></div></div></div></div><div id="42211493" class="c"><input type="checkbox" id="c-42211493" checked=""/><div class="controls bullet"><span class="by">vivekkalyan</span><span>|</span><a href="#42209072">prev</a><span>|</span><a href="#42210896">next</a><span>|</span><label class="collapse" for="c-42211493">[-]</label><label class="expand" for="c-42211493">[1 more]</label></div><br/><div class="children"><div class="content">This is great work! Mechanistic interpretability has tons of use cases, it&#x27;s great to see open research in that field.<p>You mentioned you spent your own time and money on it, would you be willing to share how much you spent? It would help others who might be considering independent research.</div><br/></div></div><div id="42210896" class="c"><input type="checkbox" id="c-42210896" checked=""/><div class="controls bullet"><span class="by">monkeycantype</span><span>|</span><a href="#42211493">prev</a><span>|</span><a href="#42211357">next</a><span>|</span><label class="collapse" for="c-42210896">[-]</label><label class="expand" for="c-42210896">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for posting this PaulPauls,<p>can I please ask a wacky question that I have about mech.interp. ?<p>we know that when we use a metric to measure humans, we game the metric, I wonder about future ai, gaming mech.interp.<p>for simplicity let me propose a 2d matrix to encode tokens, and hope my understanding of neural networks is good enough for this to be a valid question
I understand that higher dimensions have a lot more &#x27;space&#x27;, so accidental collisions are easily avoided, but 2d makes my example easier to explain.<p>if we had the following tokens mapped in 2d space<p><pre><code>  Apple 1a
  Pear  1b
  Donkey 2a
  Horse 2b

</code></pre>
it becomes impossible to understand if the neurons 1,2,a,b, all excited represents apple and horse or donkey and pear?<p>I wonder if a future, vastly more competent AI overseeing its own training might use this form of ambiguity as means to create a model in which the weights are selected so the potential for encoding collisions exist, so that it is possible to deceive an mech.int. observer as to the true thoughts of the model, essentially enabling the ai to think in euphemisms?</div><br/></div></div><div id="42211357" class="c"><input type="checkbox" id="c-42211357" checked=""/><div class="controls bullet"><span class="by">samstevens</span><span>|</span><a href="#42210896">prev</a><span>|</span><a href="#42211885">next</a><span>|</span><label class="collapse" for="c-42211357">[-]</label><label class="expand" for="c-42211357">[2 more]</label></div><br/><div class="children"><div class="content">I’m really excited to see some more open SAE work! The engineering effort is non trivial and I’m going to check out your dataloading code tomorrow. You might be interested in an currently in-progress project of mine to train SAEs on vision models: <a href="https:&#x2F;&#x2F;github.com&#x2F;samuelstevens&#x2F;saev">https:&#x2F;&#x2F;github.com&#x2F;samuelstevens&#x2F;saev</a></div><br/></div></div><div id="42211885" class="c"><input type="checkbox" id="c-42211885" checked=""/><div class="controls bullet"><span class="by">enterthedragon</span><span>|</span><a href="#42211357">prev</a><span>|</span><a href="#42208694">next</a><span>|</span><label class="collapse" for="c-42211885">[-]</label><label class="expand" for="c-42211885">[1 more]</label></div><br/><div class="children"><div class="content">This is amazing, the documentation is very well organized</div><br/></div></div><div id="42208694" class="c"><input type="checkbox" id="c-42208694" checked=""/><div class="controls bullet"><span class="by">jaykr_</span><span>|</span><a href="#42211885">prev</a><span>|</span><a href="#42209081">next</a><span>|</span><label class="collapse" for="c-42208694">[-]</label><label class="expand" for="c-42208694">[2 more]</label></div><br/><div class="children"><div class="content">This is awesome! I really appreciate the time you took to document everything!</div><br/><div id="42210147" class="c"><input type="checkbox" id="c-42210147" checked=""/><div class="controls bullet"><span class="by">PaulPauls</span><span>|</span><a href="#42208694">parent</a><span>|</span><a href="#42209081">next</a><span>|</span><label class="collapse" for="c-42210147">[-]</label><label class="expand" for="c-42210147">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for saying that! I have a much, much harder time documenting everything and writing out each decision in continuous text than actually writing the code. So it took a look time for me to write all of this down - so I&#x27;m happy you appreciate it! =)</div><br/></div></div></div></div><div id="42209081" class="c"><input type="checkbox" id="c-42209081" checked=""/><div class="controls bullet"><span class="by">JackYoustra</span><span>|</span><a href="#42208694">prev</a><span>|</span><a href="#42211228">next</a><span>|</span><label class="collapse" for="c-42209081">[-]</label><label class="expand" for="c-42209081">[2 more]</label></div><br/><div class="children"><div class="content">Very cool work! Any plans to integrate it with SAELens?</div><br/><div id="42210178" class="c"><input type="checkbox" id="c-42210178" checked=""/><div class="controls bullet"><span class="by">PaulPauls</span><span>|</span><a href="#42209081">parent</a><span>|</span><a href="#42211228">next</a><span>|</span><label class="collapse" for="c-42210178">[-]</label><label class="expand" for="c-42210178">[1 more]</label></div><br/><div class="children"><div class="content">Not sure yet to be honest. I&#x27;ll definitely consider it but I&#x27;ll reorient myself and what I plan to do next in the coming week. I also planned on maybe starting a simpler project and maybe showing people how to create the full model of a current Llama 3.2 implementation from scratch in pure PyTorch. I love building things from teh ground up and when I looked for documentation for the Llama 3.2 background section of this SAE project then the existing documentation I found was either too superficial or outdated and intended for Llama 1 or 2 - Documentation in ML gets outdated so quickly nowadays...</div><br/></div></div></div></div><div id="42211228" class="c"><input type="checkbox" id="c-42211228" checked=""/><div class="controls bullet"><span class="by">batterylake</span><span>|</span><a href="#42209081">prev</a><span>|</span><a href="#42211413">next</a><span>|</span><label class="collapse" for="c-42211228">[-]</label><label class="expand" for="c-42211228">[1 more]</label></div><br/><div class="children"><div class="content">This is incredible!<p>PaulPauls, how would you like us to cite your work?</div><br/></div></div><div id="42211413" class="c"><input type="checkbox" id="c-42211413" checked=""/><div class="controls bullet"><span class="by">Carrentt</span><span>|</span><a href="#42211228">prev</a><span>|</span><a href="#42211957">next</a><span>|</span><label class="collapse" for="c-42211413">[-]</label><label class="expand" for="c-42211413">[1 more]</label></div><br/><div class="children"><div class="content">Fantastic work! I absolutely love all the documentation.</div><br/></div></div></div></div></div></div></div></body></html>