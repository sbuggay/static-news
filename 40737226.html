<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719046861062" as="style"/><link rel="stylesheet" href="styles.css?v=1719046861062"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://img.ly/blog/browser-background-removal-using-onnx-runtime-webgpu/">20x Faster Background Removal in the Browser Using ONNX Runtime with WebGPU</a> <span class="domain">(<a href="https://img.ly">img.ly</a>)</span></div><div class="subtext"><span>buss_jan</span> | <span>24 comments</span></div><br/><div><div id="40756655" class="c"><input type="checkbox" id="c-40756655" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#40757382">next</a><span>|</span><label class="collapse" for="c-40756655">[-]</label><label class="expand" for="c-40756655">[6 more]</label></div><br/><div class="children"><div class="content">Background Removal can be thought of as Foreground Segmentation, inverted. That is no trivial feat; my undergraduate thesis was on segmentation, but using only “mechanical” approaches, no NNs, etc), hence my appreciation!<p>But here’s something I don’t understand: (And someone please correct me if I’m wrong!) - now I do understand that NNs are to software what FPGAs are to hardware, and the ability to pick any node and mess with it (delete, clone, more connections, less connections, link weights, swap-out the activation functions, etc) means they’re perfect for evolutionary-algorithms that mutate, spawn, and cull these NNs until they solve some problem (e.g. playing Super Mario on a NES (props to Tom7) or in this case, photo background segmentation.<p>…now, assuming the analogy to FPGAs still holds, with NNs being an incredibly inefficient way to encode and execute steps in a data-processing pipeline (but very efficient at evolving that pipeline) - doesn’t it then mean that whatever process is encoded in the NN, it should both be possible to represent in some more efficient representation (I.e. computer program code, even if it’s highly parallelised) <i>and</i> that “compiling” it down is essential for performance? And if so, then why are models&#x2F;systems like this being kept in NN form?<p>(I look forward to revisiting this post a decade from now and musing at my current misconceptions)</div><br/><div id="40756849" class="c"><input type="checkbox" id="c-40756849" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#40756655">parent</a><span>|</span><a href="#40757172">next</a><span>|</span><label class="collapse" for="c-40756849">[-]</label><label class="expand" for="c-40756849">[2 more]</label></div><br/><div class="children"><div class="content">For many tasks that neural networks can solve, there are traditional algorithms that are more compact (lines of source code vs size of neural network parameters), but they are not always faster and often produce results of lower quality. For a fair comparison, you have to compare the quality of result together with the computation time, which is not straightforward since those are two competing goals. That being said, neural networks perform quite well for two reasons:<p>1. They can produce approximate solutions which are often good enough in practice and faster than exact algorithmic solutions.<p>2. Neural networks benefit from billions of dollars of research into how to make them run faster, so even if they technically require more TFLOPs to compute, they are still faster than traditional algorithms that are not extremely well optimized.<p>Lastly, development time is also important. It is much easier to train a neural network on some large dataset than to come up with an algorithm that works for all kinds of edge cases. To be fair, neural networks might fail catastrophically when they encounter data that they have not been trained on, but maybe it is possible to collect more training data for this specific case.<p>I have not discussed any methods to compress and simplify already trained models here (model distillation, quantization, pruning, low-rank approximation, and probably many more that I&#x27;ve forgotten), but they all tip the scales in favor of neural networks.</div><br/><div id="40757036" class="c"><input type="checkbox" id="c-40757036" checked=""/><div class="controls bullet"><span class="by">salamo</span><span>|</span><a href="#40756655">root</a><span>|</span><a href="#40756849">parent</a><span>|</span><a href="#40757172">next</a><span>|</span><label class="collapse" for="c-40757036">[-]</label><label class="expand" for="c-40757036">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Neural networks are the second best way of doing just about anything.&quot; ~ John Denker<p>It&#x27;s an old quote that, although not 100% accurate anymore, still sums up my feelings quite nicely.</div><br/></div></div></div></div><div id="40757172" class="c"><input type="checkbox" id="c-40757172" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40756655">parent</a><span>|</span><a href="#40756849">prev</a><span>|</span><a href="#40756751">next</a><span>|</span><label class="collapse" for="c-40757172">[-]</label><label class="expand" for="c-40757172">[1 more]</label></div><br/><div class="children"><div class="content">NNs are, in a way, already &quot;compiled&quot;. If all you want to do is inference (forward pass), then you mostly do <i>a lot</i> of matrix multiplications. It&#x27;s the training pass that requires building up extra scaffolding to track gradients and such.<p>It occurred to me that NNs (&quot;AI&quot;) are indeed a bit like crypto, in the sense that both attempt to substitute compute for some human quality. Proof of Work and associated ideas try to substitute compute <i>for trust</i>[0]. Solving problems by feeding tons of data into a DNN is substituting compute <i>for understanding</i>. Specifically, for our understanding of the problem being solved.<p>It&#x27;s neat we can <i>just throw compute</i> at a problem to solve it well, but we then end up with a magic black box that&#x27;s even less comprehensible than the problem at hand.<p>It also occurs to me that stochastic gradient descent is better than evolutionary programming because it&#x27;s to evolution what closed-form analytical solutions are to running a simulation of interacting bodies - if you can get away with a formula that gives you what the simulation is trying to approximate, you&#x27;re better off with the formula. So in this sense, perhaps it&#x27;s worth to try harder to take a step back and reverse-engineer the problems solved by DNNs, try to gain that more theoretical understanding, because as fun as brute-forcing a solution is, analytical solutions are better.<p>--<p>[0] - Which I consider bad for reasons discussed many time before; it&#x27;s not where I want to go with this comment.</div><br/></div></div><div id="40756751" class="c"><input type="checkbox" id="c-40756751" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#40756655">parent</a><span>|</span><a href="#40757172">prev</a><span>|</span><a href="#40756742">next</a><span>|</span><label class="collapse" for="c-40756751">[-]</label><label class="expand" for="c-40756751">[1 more]</label></div><br/><div class="children"><div class="content">There is some work to convert NNs to decision trees.<p><a href="https:&#x2F;&#x2F;towardsdatascience.com&#x2F;neural-networks-as-decision-trees-89cd9fdcdf6a" rel="nofollow">https:&#x2F;&#x2F;towardsdatascience.com&#x2F;neural-networks-as-decision-t...</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.05189" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.05189</a><p>I haven&#x27;t reviewed any of it, I only know of it tangentially.<p><a href="https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;Converting-A-Trained-Neural-Network-To-a-Decision-Boz&#x2F;fb6172737873a69bd8d0117c88301121b5cabfa3" rel="nofollow">https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;Converting-A-Trained-N...</a><p>Distilling a Neural Network Into a Soft Decision Tree
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1711.09784" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1711.09784</a><p>GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.03515" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.03515</a></div><br/></div></div><div id="40756742" class="c"><input type="checkbox" id="c-40756742" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#40756655">parent</a><span>|</span><a href="#40756751">prev</a><span>|</span><a href="#40757382">next</a><span>|</span><label class="collapse" for="c-40756742">[-]</label><label class="expand" for="c-40756742">[1 more]</label></div><br/><div class="children"><div class="content">Neural networks are not trained with evolutionary algorithms because they are very slow, especially for the millions or billions of parameters that NNs have. Instead, stochastic gradient descent is used for training, which is much more efficient.</div><br/></div></div></div></div><div id="40757382" class="c"><input type="checkbox" id="c-40757382" checked=""/><div class="controls bullet"><span class="by">jvdvegt</span><span>|</span><a href="#40756655">prev</a><span>|</span><a href="#40757426">next</a><span>|</span><label class="collapse" for="c-40757382">[-]</label><label class="expand" for="c-40757382">[1 more]</label></div><br/><div class="children"><div class="content">MS teams does this already, right? (I assume they do, as it didn&#x27;t work in Firefox until recently)<p>Or do they do it server side?</div><br/></div></div><div id="40757426" class="c"><input type="checkbox" id="c-40757426" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#40757382">prev</a><span>|</span><a href="#40756553">next</a><span>|</span><label class="collapse" for="c-40757426">[-]</label><label class="expand" for="c-40757426">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, there’s also node version in &#x2F;packages.</div><br/></div></div><div id="40756553" class="c"><input type="checkbox" id="c-40756553" checked=""/><div class="controls bullet"><span class="by">tommek4077</span><span>|</span><a href="#40757426">prev</a><span>|</span><a href="#40756330">next</a><span>|</span><label class="collapse" for="c-40756553">[-]</label><label class="expand" for="c-40756553">[4 more]</label></div><br/><div class="children"><div class="content">If I run it in a browser on my client, why going to a website in the first place?</div><br/><div id="40756692" class="c"><input type="checkbox" id="c-40756692" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40756553">parent</a><span>|</span><a href="#40757097">prev</a><span>|</span><a href="#40756330">next</a><span>|</span><label class="collapse" for="c-40756692">[-]</label><label class="expand" for="c-40756692">[2 more]</label></div><br/><div class="children"><div class="content">to resolve a short url to a piece of software i guess</div><br/><div id="40756848" class="c"><input type="checkbox" id="c-40756848" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#40756553">root</a><span>|</span><a href="#40756692">parent</a><span>|</span><a href="#40756330">next</a><span>|</span><label class="collapse" for="c-40756848">[-]</label><label class="expand" for="c-40756848">[1 more]</label></div><br/><div class="children"><div class="content">This novel concept should have a catchy name…</div><br/></div></div></div></div></div></div><div id="40756330" class="c"><input type="checkbox" id="c-40756330" checked=""/><div class="controls bullet"><span class="by">tlarkworthy</span><span>|</span><a href="#40756553">prev</a><span>|</span><a href="#40756253">next</a><span>|</span><label class="collapse" for="c-40756330">[-]</label><label class="expand" for="c-40756330">[4 more]</label></div><br/><div class="children"><div class="content">Onnx is cool, the other option is tensorflow js which I have found quite nice as a usable matrix lib for JS with shockingly good perf.would love to know how well they compare</div><br/><div id="40756897" class="c"><input type="checkbox" id="c-40756897" checked=""/><div class="controls bullet"><span class="by">dleeftink</span><span>|</span><a href="#40756330">parent</a><span>|</span><a href="#40756253">next</a><span>|</span><label class="collapse" for="c-40756897">[-]</label><label class="expand" for="c-40756897">[3 more]</label></div><br/><div class="children"><div class="content">Also shout out to Taichi and GPU.js for alternatives in this space. I&#x27;ve also had success with Hamster.js, that &#x27;parallelizes&#x27; computations using Web workers instead of the GPU (who knows, in the future the two might be combined?).</div><br/><div id="40757064" class="c"><input type="checkbox" id="c-40757064" checked=""/><div class="controls bullet"><span class="by">salamo</span><span>|</span><a href="#40756330">root</a><span>|</span><a href="#40756897">parent</a><span>|</span><a href="#40756253">next</a><span>|</span><label class="collapse" for="c-40757064">[-]</label><label class="expand" for="c-40757064">[2 more]</label></div><br/><div class="children"><div class="content">They are probably two different use cases. Parallelizing with web workers could be faster for algorithms that do a lot of branching (minimax comes to mind) but if you can vectorize (matmuls for example) then GPU probably dominates.</div><br/><div id="40757231" class="c"><input type="checkbox" id="c-40757231" checked=""/><div class="controls bullet"><span class="by">dleeftink</span><span>|</span><a href="#40756330">root</a><span>|</span><a href="#40757064">parent</a><span>|</span><a href="#40756253">next</a><span>|</span><label class="collapse" for="c-40757231">[-]</label><label class="expand" for="c-40757231">[1 more]</label></div><br/><div class="children"><div class="content">It would be cool to implement some of these in either library to see how they stack up. In the Hamster.js case, I am envisioning each worker having access to a seperate GPU on your local machine..and having results come in asynchronously on the main thread. Massive in-browser simulations with access to  existing JS visualisation packages would make real-time prototyping more feasible.</div><br/></div></div></div></div></div></div></div></div><div id="40756253" class="c"><input type="checkbox" id="c-40756253" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#40756330">prev</a><span>|</span><a href="#40756525">next</a><span>|</span><label class="collapse" for="c-40756253">[-]</label><label class="expand" for="c-40756253">[3 more]</label></div><br/><div class="children"><div class="content">Worth noting that background removal is built in to Preview on Macos.</div><br/><div id="40756367" class="c"><input type="checkbox" id="c-40756367" checked=""/><div class="controls bullet"><span class="by">dagmx</span><span>|</span><a href="#40756253">parent</a><span>|</span><a href="#40756525">next</a><span>|</span><label class="collapse" for="c-40756367">[-]</label><label class="expand" for="c-40756367">[2 more]</label></div><br/><div class="children"><div class="content">It’s also built into Safari and Photos on all their platforms and available as an API that can be called by any app<p><a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;wwdc23&#x2F;10176" rel="nofollow">https:&#x2F;&#x2F;developer.apple.com&#x2F;wwdc23&#x2F;10176</a></div><br/><div id="40756917" class="c"><input type="checkbox" id="c-40756917" checked=""/><div class="controls bullet"><span class="by">oefrha</span><span>|</span><a href="#40756253">root</a><span>|</span><a href="#40756367">parent</a><span>|</span><a href="#40756525">next</a><span>|</span><label class="collapse" for="c-40756917">[-]</label><label class="expand" for="c-40756917">[1 more]</label></div><br/><div class="children"><div class="content">Huh, I&#x27;ve been copying background-removed subjects out of Preview and didn&#x27;t realize there&#x27;s a VisionKit API. Looks like it&#x27;s quite easy to use too, I put together a quick and dirty script in a couple minutes and it worked wonderfully:<p><pre><code>  import AppKit
  import VisionKit
  
  @main
  struct Script {
    static func main() async {
      let image = NSImage(contentsOfFile: &quot;input.heic&quot;)!
      let view = ImageAnalysisOverlayView()
      let analyzer = ImageAnalyzer()
      let configuration = ImageAnalyzer.Configuration(.visualLookUp)
      let analysis = try! await analyzer.analyze(image, orientation: .up, configuration: configuration)
      view.analysis = analysis
      let subjects = await view.subjects
      for (index, subject) in subjects.enumerated() {
        let subjectImage = try! await subject.image
        let pngData = NSBitmapImageRep(data: subjectImage.tiffRepresentation!)!.representation(
          using: .png, properties: [:])
        try! pngData?.write(to: URL(fileURLWithPath: &quot;subject-\(index).png&quot;))
        print(&quot;subject-\(index).png&quot;)
      }
    }
  }</code></pre></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>