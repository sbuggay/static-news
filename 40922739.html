<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720688470030" as="style"/><link rel="stylesheet" href="styles.css?v=1720688470030"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/lm-sys/RouteLLM">RouteLLM: A framework for serving and evaluating LLM routers</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>djhu9</span> | <span>41 comments</span></div><br/><div><div id="40927411" class="c"><input type="checkbox" id="c-40927411" checked=""/><div class="controls bullet"><span class="by">fbnbr</span><span>|</span><a href="#40923007">next</a><span>|</span><label class="collapse" for="c-40927411">[-]</label><label class="expand" for="c-40927411">[2 more]</label></div><br/><div class="children"><div class="content">This RouteLLM framework sounds really promising, especially for cost optimization. It reminds me of the KNN-router project ([<a href="https:&#x2F;&#x2F;github.com&#x2F;pulzeai-oss&#x2F;knn-router](https:&#x2F;&#x2F;github.com&#x2F;pulzeai-oss&#x2F;knn-router)">https:&#x2F;&#x2F;github.com&#x2F;pulzeai-oss&#x2F;knn-router](https:&#x2F;&#x2F;github.co...</a>), which uses a k-nearest neighbors approach to route queries to the most appropriate models.<p>What I like about these kinds of solutions is that they address the practical challenges of using multiple LLMs. Rate limits, cost per token, and even just choosing the right model for the job can be a real headache.<p>KNN-router, for example, lets you define your own logic for routing queries, so you can factor in things like model accuracy, response time, and cost. You can even set up fallback models for when your primary model is unavailable.<p>It&#x27;s cool to see these kinds of tools emerging because it shows that people are starting to think seriously about how to build robust, cost-effective LLM pipelines. This is going to be crucial as more and more companies start incorporating LLMs into their products and services.</div><br/><div id="40934043" class="c"><input type="checkbox" id="c-40934043" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#40927411">parent</a><span>|</span><a href="#40923007">next</a><span>|</span><label class="collapse" for="c-40934043">[-]</label><label class="expand" for="c-40934043">[1 more]</label></div><br/><div class="children"><div class="content">Cost is a plus but at least what I see is that getting good response time is even bigger. Something like OpenAI Azure instances are inconsistent and it is far too normal to get a 40sec lag with responses with gpt4-o.</div><br/></div></div></div></div><div id="40923007" class="c"><input type="checkbox" id="c-40923007" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#40927411">prev</a><span>|</span><a href="#40925017">next</a><span>|</span><label class="collapse" for="c-40923007">[-]</label><label class="expand" for="c-40923007">[11 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really get who these are for - do people use them in their projects?<p>I don&#x27;t find success just using a prompt against some other model without having some way to evaluate it and usually updating it for that model.</div><br/><div id="40923238" class="c"><input type="checkbox" id="c-40923238" checked=""/><div class="controls bullet"><span class="by">vatican_banker</span><span>|</span><a href="#40923007">parent</a><span>|</span><a href="#40924335">next</a><span>|</span><label class="collapse" for="c-40923238">[-]</label><label class="expand" for="c-40923238">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Trained routers are provided out of the box, which we have shown to reduce costs by up to 85%<p>The answer is here. This is a cost-saving tool.<p>All companies and their moms want to be in the GenAI game but have strict budgets. Tools like this help to keep GenAI projects within budget.</div><br/></div></div><div id="40924335" class="c"><input type="checkbox" id="c-40924335" checked=""/><div class="controls bullet"><span class="by">rodrigobahiense</span><span>|</span><a href="#40923007">parent</a><span>|</span><a href="#40923238">prev</a><span>|</span><a href="#40923097">next</a><span>|</span><label class="collapse" for="c-40924335">[-]</label><label class="expand" for="c-40924335">[1 more]</label></div><br/><div class="children"><div class="content">For the company I work for, one of the most important aspects is ensuring we can fallback to different models in case of content filtering since they are not equally sensitive&#x2F;restrict.</div><br/></div></div><div id="40923097" class="c"><input type="checkbox" id="c-40923097" checked=""/><div class="controls bullet"><span class="by">veb</span><span>|</span><a href="#40923007">parent</a><span>|</span><a href="#40924335">prev</a><span>|</span><a href="#40923095">next</a><span>|</span><label class="collapse" for="c-40923097">[-]</label><label class="expand" for="c-40923097">[6 more]</label></div><br/><div class="children"><div class="content">From what I understand, it&#x27;s from people using it in their workflows - say, Claude but keep hitting the rate limits, so they have to wait until Claude says &quot;you got 10 messages left until 9pm&quot;, so when they hit that, or before they switch to (maybe) ChatGPT manually.<p>With the router thingy, it keeps a record, so you know every query where you stand, and can switch to another model automatically instead of interrupting workflow?<p>I may be explaining this very badly, but I think that&#x27;s one use-case for how these LLM Routers help.</div><br/><div id="40925071" class="c"><input type="checkbox" id="c-40925071" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#40923007">root</a><span>|</span><a href="#40923097">parent</a><span>|</span><a href="#40923322">next</a><span>|</span><label class="collapse" for="c-40925071">[-]</label><label class="expand" for="c-40925071">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s a use case since you don&#x27;t get rate limited when using the API.</div><br/><div id="40926513" class="c"><input type="checkbox" id="c-40926513" checked=""/><div class="controls bullet"><span class="by">Onawa</span><span>|</span><a href="#40923007">root</a><span>|</span><a href="#40925071">parent</a><span>|</span><a href="#40927120">next</a><span>|</span><label class="collapse" for="c-40926513">[-]</label><label class="expand" for="c-40926513">[2 more]</label></div><br/><div class="children"><div class="content">We get rate limited when using Azure&#x27;s OpenAI API. As a gov contractor working with AI, I have limited means for getting access to frontier LLMs. So routing tools that can fail over to another model can be useful.</div><br/><div id="40927309" class="c"><input type="checkbox" id="c-40927309" checked=""/><div class="controls bullet"><span class="by">fkyoureadthedoc</span><span>|</span><a href="#40923007">root</a><span>|</span><a href="#40926513">parent</a><span>|</span><a href="#40927120">next</a><span>|</span><label class="collapse" for="c-40927309">[-]</label><label class="expand" for="c-40927309">[1 more]</label></div><br/><div class="children"><div class="content">Same. Initially we just load balanced between various regions, ultimately bought some PTUs.</div><br/></div></div></div></div><div id="40927120" class="c"><input type="checkbox" id="c-40927120" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#40923007">root</a><span>|</span><a href="#40925071">parent</a><span>|</span><a href="#40926513">prev</a><span>|</span><a href="#40923322">next</a><span>|</span><label class="collapse" for="c-40927120">[-]</label><label class="expand" for="c-40927120">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic Build Tier 4: 4,000 RPM, 400,000 TPM, 50,000,000 TPD for Claude 3.5 Sonnet</div><br/></div></div></div></div><div id="40923322" class="c"><input type="checkbox" id="c-40923322" checked=""/><div class="controls bullet"><span class="by">PiRho3141</span><span>|</span><a href="#40923007">root</a><span>|</span><a href="#40923097">parent</a><span>|</span><a href="#40925071">prev</a><span>|</span><a href="#40923095">next</a><span>|</span><label class="collapse" for="c-40923322">[-]</label><label class="expand" for="c-40923322">[1 more]</label></div><br/><div class="children"><div class="content">This is for applications that use LLMs or Chat GPT via API.</div><br/></div></div></div></div><div id="40923095" class="c"><input type="checkbox" id="c-40923095" checked=""/><div class="controls bullet"><span class="by">brandall10</span><span>|</span><a href="#40923007">parent</a><span>|</span><a href="#40923097">prev</a><span>|</span><a href="#40924303">next</a><span>|</span><label class="collapse" for="c-40923095">[-]</label><label class="expand" for="c-40923095">[1 more]</label></div><br/><div class="children"><div class="content">You may have a variety of model types&#x2F;sizes, fine tunes, etc, that serve different purposes - optimizing for cost&#x2F;speed&#x2F;specificity of task. At least that&#x27;s the general theory with routing. This one only seems to optimize for cost&#x2F;quality.</div><br/></div></div><div id="40924303" class="c"><input type="checkbox" id="c-40924303" checked=""/><div class="controls bullet"><span class="by">monarchwadia</span><span>|</span><a href="#40923007">parent</a><span>|</span><a href="#40923095">prev</a><span>|</span><a href="#40925017">next</a><span>|</span><label class="collapse" for="c-40924303">[-]</label><label class="expand" for="c-40924303">[1 more]</label></div><br/><div class="children"><div class="content">I think a lot of people are just interested in hitting the LLM without any bells or whistles, from Typescript. A low level connector lib would come in handy, yeah?  <a href="https:&#x2F;&#x2F;github.com&#x2F;monarchwadia&#x2F;ragged">https:&#x2F;&#x2F;github.com&#x2F;monarchwadia&#x2F;ragged</a></div><br/></div></div></div></div><div id="40925017" class="c"><input type="checkbox" id="c-40925017" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#40923007">prev</a><span>|</span><a href="#40928819">next</a><span>|</span><label class="collapse" for="c-40925017">[-]</label><label class="expand" for="c-40925017">[1 more]</label></div><br/><div class="children"><div class="content">Interesting that it is generalizable to other pairs. That implies some sort of prompt property or characteristic that could be widely used.<p>I don’t think using different models is the right approach though. They behave differently. Better to use a big and small one from same family. Or alternatively using this to drive whether to give the ai more “thinking time” via chain of thought or agents.</div><br/></div></div><div id="40928819" class="c"><input type="checkbox" id="c-40928819" checked=""/><div class="controls bullet"><span class="by">bangaladore</span><span>|</span><a href="#40925017">prev</a><span>|</span><a href="#40923002">next</a><span>|</span><label class="collapse" for="c-40928819">[-]</label><label class="expand" for="c-40928819">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using OpenRouter only for personal use, not for its router functionality, so I can use the API of various models (or open-source models) without signing up and prepaying&#x2F;paying a subscription on all their websites.<p>I believe OpenRouter also provides an API that does the same thing as RouteLLM. Again, you only have to pay OpenRouter, not every model&#x27;s service you use.</div><br/><div id="40931141" class="c"><input type="checkbox" id="c-40931141" checked=""/><div class="controls bullet"><span class="by">localfirst</span><span>|</span><a href="#40928819">parent</a><span>|</span><a href="#40923002">next</a><span>|</span><label class="collapse" for="c-40931141">[-]</label><label class="expand" for="c-40931141">[1 more]</label></div><br/><div class="children"><div class="content">OpenRouter is also interesting solution but I almost end up using like one or two LLMs and I rarely feel the need to switch between different LLMs so I ask why I am even using openrouter in the first place.</div><br/></div></div></div></div><div id="40923002" class="c"><input type="checkbox" id="c-40923002" checked=""/><div class="controls bullet"><span class="by">worstspotgain</span><span>|</span><a href="#40928819">prev</a><span>|</span><a href="#40923293">next</a><span>|</span><label class="collapse" for="c-40923002">[-]</label><label class="expand" for="c-40923002">[1 more]</label></div><br/><div class="children"><div class="content">I like their &quot;LLM isovalue&quot; graph, and the idea that different vendors can be forced to partake in the same synergy&#x2F;distillation scheme. Vendors dislike these schemes, but they&#x27;re probably OK with them as long as they&#x27;re niche.</div><br/></div></div><div id="40923293" class="c"><input type="checkbox" id="c-40923293" checked=""/><div class="controls bullet"><span class="by">tananaev</span><span>|</span><a href="#40923002">prev</a><span>|</span><a href="#40930953">next</a><span>|</span><label class="collapse" for="c-40923293">[-]</label><label class="expand" for="c-40923293">[6 more]</label></div><br/><div class="children"><div class="content">The problem is to understand how complex the request is, you have to use a smart enough model.</div><br/><div id="40927699" class="c"><input type="checkbox" id="c-40927699" checked=""/><div class="controls bullet"><span class="by">Fripplebubby</span><span>|</span><a href="#40923293">parent</a><span>|</span><a href="#40923730">next</a><span>|</span><label class="collapse" for="c-40927699">[-]</label><label class="expand" for="c-40927699">[1 more]</label></div><br/><div class="children"><div class="content">In this paper, they tried a couple different methods for determining how similar the incoming request is to requests that they have scored in their dataset. Actually, one of the best methods they used does not involve using a model at all to evaluate the incoming query (similarity-weighted ranking) although it _does_ use pre-trained embeddings.<p>Using this, they were able to produce quite good results applying this similarity measurement to unseen queries using a standard benchmark. The leap of faith here is assuming that the same query similarity method will continue to bear fruit when extended to queries that aren&#x27;t benchmarkable.</div><br/></div></div><div id="40923730" class="c"><input type="checkbox" id="c-40923730" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#40923293">parent</a><span>|</span><a href="#40927699">prev</a><span>|</span><a href="#40923327">next</a><span>|</span><label class="collapse" for="c-40923730">[-]</label><label class="expand" for="c-40923730">[1 more]</label></div><br/><div class="children"><div class="content">not true at all, you could have a efficient cheap model which is generally terrible at most things but has a savant like capacity for categorizing tasks by requirement and difficulty. Even easier when you dont need to support multiple languages and a truly staggering breadth of domains, like a conventional llm does. You could train a really small model to reject out of domain requests and partition the rest, running at a fraction of the cost of a more capable model.</div><br/></div></div><div id="40923327" class="c"><input type="checkbox" id="c-40923327" checked=""/><div class="controls bullet"><span class="by">ethegwo</span><span>|</span><a href="#40923293">parent</a><span>|</span><a href="#40923730">prev</a><span>|</span><a href="#40923312">next</a><span>|</span><label class="collapse" for="c-40923327">[-]</label><label class="expand" for="c-40923327">[1 more]</label></div><br/><div class="children"><div class="content">The weak-to-strong assumption is that it is easier to eval the result of a task than to generate it. If it is wrong, human can not make a stronger intelligence than us.</div><br/></div></div><div id="40923312" class="c"><input type="checkbox" id="c-40923312" checked=""/><div class="controls bullet"><span class="by">PiRho3141</span><span>|</span><a href="#40923293">parent</a><span>|</span><a href="#40923327">prev</a><span>|</span><a href="#40923315">next</a><span>|</span><label class="collapse" for="c-40923312">[-]</label><label class="expand" for="c-40923312">[1 more]</label></div><br/><div class="children"><div class="content">Not true. You can easily train a BERT single class classification model without having to train an LLM.</div><br/></div></div><div id="40923315" class="c"><input type="checkbox" id="c-40923315" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40923293">parent</a><span>|</span><a href="#40923312">prev</a><span>|</span><a href="#40930953">next</a><span>|</span><label class="collapse" for="c-40923315">[-]</label><label class="expand" for="c-40923315">[1 more]</label></div><br/><div class="children"><div class="content">You can distill evaluation ability</div><br/></div></div></div></div><div id="40930953" class="c"><input type="checkbox" id="c-40930953" checked=""/><div class="controls bullet"><span class="by">PetrBrzyBrzek</span><span>|</span><a href="#40923293">prev</a><span>|</span><a href="#40923246">next</a><span>|</span><label class="collapse" for="c-40930953">[-]</label><label class="expand" for="c-40930953">[1 more]</label></div><br/><div class="children"><div class="content">There is a similar project called NotDiamond, which is available on Hugging Face: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;notdiamond&#x2F;notdiamond-0001" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;notdiamond&#x2F;notdiamond-0001</a>.</div><br/></div></div><div id="40923246" class="c"><input type="checkbox" id="c-40923246" checked=""/><div class="controls bullet"><span class="by">vatican_banker</span><span>|</span><a href="#40930953">prev</a><span>|</span><a href="#40923279">next</a><span>|</span><label class="collapse" for="c-40923246">[-]</label><label class="expand" for="c-40923246">[5 more]</label></div><br/><div class="children"><div class="content">The tool currently allows only one set of strong and weak models.<p>I’d be really good to allow more than two models and change dynamically based on multiple constraints like latency, reasoning complexity, costs, etc.</div><br/><div id="40924164" class="c"><input type="checkbox" id="c-40924164" checked=""/><div class="controls bullet"><span class="by">voiper1</span><span>|</span><a href="#40923246">parent</a><span>|</span><a href="#40925146">next</a><span>|</span><label class="collapse" for="c-40924164">[-]</label><label class="expand" for="c-40924164">[2 more]</label></div><br/><div class="children"><div class="content">I think unify.ai (like openrouter) does that - it has several paramters you can choose from.<p>But the underlying &quot;how to choose a model that&#x27;s smart enough but not too smart&quot; seems difficult to understand.</div><br/><div id="40924714" class="c"><input type="checkbox" id="c-40924714" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#40923246">root</a><span>|</span><a href="#40924164">parent</a><span>|</span><a href="#40925146">next</a><span>|</span><label class="collapse" for="c-40924714">[-]</label><label class="expand" for="c-40924714">[1 more]</label></div><br/><div class="children"><div class="content">Its just sentiment analysis.</div><br/></div></div></div></div><div id="40925146" class="c"><input type="checkbox" id="c-40925146" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#40923246">parent</a><span>|</span><a href="#40924164">prev</a><span>|</span><a href="#40923300">next</a><span>|</span><label class="collapse" for="c-40925146">[-]</label><label class="expand" for="c-40925146">[1 more]</label></div><br/><div class="children"><div class="content">Portkey does that with configuration. You assign a base model, then add more models with weight to load-balance.</div><br/></div></div><div id="40923300" class="c"><input type="checkbox" id="c-40923300" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#40923246">parent</a><span>|</span><a href="#40925146">prev</a><span>|</span><a href="#40923279">next</a><span>|</span><label class="collapse" for="c-40923300">[-]</label><label class="expand" for="c-40923300">[1 more]</label></div><br/><div class="children"><div class="content">Some of that is already possible, since it can generate a difficulty score for a prompt that could be manually mapped between models based on ranges.</div><br/></div></div></div></div><div id="40933589" class="c"><input type="checkbox" id="c-40933589" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#40923279">prev</a><span>|</span><a href="#40924836">next</a><span>|</span><label class="collapse" for="c-40933589">[-]</label><label class="expand" for="c-40933589">[1 more]</label></div><br/><div class="children"><div class="content">Or just use a single LLM provider.<p>Problem solved, next.</div><br/></div></div><div id="40924836" class="c"><input type="checkbox" id="c-40924836" checked=""/><div class="controls bullet"><span class="by">daghamm</span><span>|</span><a href="#40933589">prev</a><span>|</span><a href="#40923167">next</a><span>|</span><label class="collapse" for="c-40924836">[-]</label><label class="expand" for="c-40924836">[2 more]</label></div><br/><div class="children"><div class="content">My take from this is that 85% of times we don&#x27;t need a powerfull LLM like 4o.<p>Or am I reading this wrong? :)</div><br/><div id="40924994" class="c"><input type="checkbox" id="c-40924994" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#40924836">parent</a><span>|</span><a href="#40923167">next</a><span>|</span><label class="collapse" for="c-40924994">[-]</label><label class="expand" for="c-40924994">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re reading it right. They have developed a system that automatically decides which model is sufficient, depending on your inputs, saving you costs even within one conversation stream.<p>The OpenAI-compatible API allows you to talk to the router like a regular GPT model.</div><br/></div></div></div></div><div id="40931101" class="c"><input type="checkbox" id="c-40931101" checked=""/><div class="controls bullet"><span class="by">localfirst</span><span>|</span><a href="#40923245">prev</a><span>|</span><a href="#40923248">next</a><span>|</span><label class="collapse" for="c-40931101">[-]</label><label class="expand" for="c-40931101">[1 more]</label></div><br/><div class="children"><div class="content">solution for a non-critical problem imho<p>im open to differing opinions but after dealing with langchain, premature optimization for non-critical problems is rampant in this space rn</div><br/></div></div></div></div></div></div></div></body></html>