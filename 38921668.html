<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704790865241" as="style"/><link rel="stylesheet" href="styles.css?v=1704790865241"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2401.04088">Mixtral 8x7B: A sparse Mixture of Experts language model</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>ignoramous</span> | <span>101 comments</span></div><br/><div><div id="38922177" class="c"><input type="checkbox" id="c-38922177" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#38921800">next</a><span>|</span><label class="collapse" for="c-38922177">[-]</label><label class="expand" for="c-38922177">[22 more]</label></div><br/><div class="children"><div class="content">This paper details the model that&#x27;s been in the wild for approximately a month now. Mixtral 8x7B is very, very good. It&#x27;s roughly sized at 13B, and ranked much, much higher than competitively sized models by, e.g. <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1916896&#x2F;llm_comparisontest_confirm_leaderboard_big_news&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1916896&#x2F;llm_com...</a>. Ravenwolf notes that the model slightly outperforms some of its benchmark testing, and this is my experience. It&#x27;s surprisingly good for a model of its size, and a very capable daily driver on a Mac for chat, code input and other uses.<p>Something that has come to light since the release of the weights, and not mentioned in this paper is that it looks like fairly likely that the 8 experts were all seeded by Mistral 7B and subsequently diverged. This has generated a lot of experimentation in the local LLM community with cloning models as a way to cheaply generate experts.<p>It was generally thought likely that training an 8x7B network would be as much work as training 8 7B networks, but this seems not to have been true for Mistral, which is super interesting.<p>There&#x27;s still a lot of rapid innovation happening in this space, with papers like Calm from DeepMind this week, and a lot of the adhoc experimental layer combining happening in the wild, (see, e.g. Goliath-120b), I think we&#x27;re likely to see some pretty interesting architectural improvements this year in the LLM space.<p>Calm seems to point the way to a next step after MoE, and models like Goliath seem to indicate that even a really really lazy version of Calm (no Linear layer combination, just literally alternating layers at full weights) can be very impactful. Overall I think we will see really, really strong models that are performant on consumer hardware in 2024, likely first half of this year.</div><br/><div id="38922300" class="c"><input type="checkbox" id="c-38922300" checked=""/><div class="controls bullet"><span class="by">Casteil</span><span>|</span><a href="#38922177">parent</a><span>|</span><a href="#38922933">next</a><span>|</span><label class="collapse" for="c-38922300">[-]</label><label class="expand" for="c-38922300">[15 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had excellent results with Mixtral too - it&#x27;s genuinely impressive. Only problem is that it&#x27;s a relatively big model that&#x27;s difficult to run with full GPU inference on consumer hardware (vs the 7b&#x2F;13b models people typically use).<p>So far, the main consumer platform capable of running it without &#x27;ruining&#x27; the quality of its output (with high levels of quantization) is the newer Apple Silicon Macs with unified memory - generally &gt;=48GB. It can apparently be done on 32 or 36GB, but there&#x27;s not much headroom.<p>Edit: As coder543 points out, yes - you can run it without more lossy levels of quantization on multi-GPU setups providing those have enough combined vram.</div><br/><div id="38922414" class="c"><input type="checkbox" id="c-38922414" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922300">parent</a><span>|</span><a href="#38922520">next</a><span>|</span><label class="collapse" for="c-38922414">[-]</label><label class="expand" for="c-38922414">[8 more]</label></div><br/><div class="children"><div class="content">Mixtral works great at 3-bit quantization. It fits onto a single RTX 3090 and runs at about 50 tokens&#x2F;s. The output quality is not &quot;ruined&quot; at all.<p>For the amount of money you&#x27;re talking about, you could also buy two 3090s (~$750 each on eBay) and have 48GB of VRAM to run with less quantization at full speed.<p>M-series Macs are surprisingly flexible platforms, but they&#x27;re not &quot;the only&quot; consumer platform that can do Mixtral.</div><br/><div id="38923092" class="c"><input type="checkbox" id="c-38923092" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922414">parent</a><span>|</span><a href="#38922440">next</a><span>|</span><label class="collapse" for="c-38923092">[-]</label><label class="expand" for="c-38923092">[2 more]</label></div><br/><div class="children"><div class="content">So you don&#x27;t see significantly worse performance on 3bit quantized models compared to 4? Every 7&#x2F;13b model I tried quantized gave much worse responses at 3 bit and below, whereas the differences from 4 bit to 6 or even 8 bit is more subtle.</div><br/><div id="38923270" class="c"><input type="checkbox" id="c-38923270" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38923092">parent</a><span>|</span><a href="#38922440">next</a><span>|</span><label class="collapse" for="c-38923270">[-]</label><label class="expand" for="c-38923270">[1 more]</label></div><br/><div class="children"><div class="content">Mixtral is a larger model, so maybe that makes it more tolerant of that level of quantization? I’ve been impressed with 3-bit Mixtral, but I haven’t done a ton of side by sides against 4-bit because I haven’t felt the need.</div><br/></div></div></div></div><div id="38922440" class="c"><input type="checkbox" id="c-38922440" checked=""/><div class="controls bullet"><span class="by">Casteil</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922414">parent</a><span>|</span><a href="#38923092">prev</a><span>|</span><a href="#38923579">next</a><span>|</span><label class="collapse" for="c-38922440">[-]</label><label class="expand" for="c-38922440">[4 more]</label></div><br/><div class="children"><div class="content">Fair enough. I did put &#x27;ruining&#x27; in quotes for a reason - I haven&#x27;t compared output between Q3 and Q4_K_M that I use, but you do generally sacrifice output quality at higher quantization levels.<p>And you&#x27;re right, you can run it on a multi-GPU setup if you&#x27;re so inclined.</div><br/><div id="38922459" class="c"><input type="checkbox" id="c-38922459" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922440">parent</a><span>|</span><a href="#38923579">next</a><span>|</span><label class="collapse" for="c-38922459">[-]</label><label class="expand" for="c-38922459">[3 more]</label></div><br/><div class="children"><div class="content">You can also choose to run at 4-bit quantization, offloading ~27 out of 33 layers to the GPU, and that runs at about 25 tokens&#x2F;s for me. I think that&#x27;s about the same speed as you get out of an M1 Max running at 4 bits? Although I&#x27;m not sure about the newer M2 or M3 Max chips. Googling around, I didn&#x27;t immediately see clear benchmarks for those.</div><br/><div id="38923382" class="c"><input type="checkbox" id="c-38923382" checked=""/><div class="controls bullet"><span class="by">patrakov</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922459">parent</a><span>|</span><a href="#38922521">next</a><span>|</span><label class="collapse" for="c-38923382">[-]</label><label class="expand" for="c-38923382">[1 more]</label></div><br/><div class="children"><div class="content">Just as another data point, a CPU-only setup with Q5_K_M would give you roughly 4 tokens per second on a Ryzen laptop (Dell Inspiron 7415 upgraded to 64 GB of RAM).</div><br/></div></div><div id="38922521" class="c"><input type="checkbox" id="c-38922521" checked=""/><div class="controls bullet"><span class="by">Casteil</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922459">parent</a><span>|</span><a href="#38923382">prev</a><span>|</span><a href="#38923579">next</a><span>|</span><label class="collapse" for="c-38922521">[-]</label><label class="expand" for="c-38922521">[1 more]</label></div><br/><div class="children"><div class="content">Nice - that&#x27;s still pretty solid.. although on a more typical 3060 or 3070 with less vram available, I probably wouldn&#x27;t expect numbers quite that good.<p>My 14&quot; M1 Max does around 30t&#x2F;s on Mixtral Q4_K_M.</div><br/></div></div></div></div></div></div><div id="38923579" class="c"><input type="checkbox" id="c-38923579" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922414">parent</a><span>|</span><a href="#38922440">prev</a><span>|</span><a href="#38922520">next</a><span>|</span><label class="collapse" for="c-38923579">[-]</label><label class="expand" for="c-38923579">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried the 2x 3090 setup? Using nvlink or SLI?</div><br/></div></div></div></div><div id="38922520" class="c"><input type="checkbox" id="c-38922520" checked=""/><div class="controls bullet"><span class="by">lithiumii</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922300">parent</a><span>|</span><a href="#38922414">prev</a><span>|</span><a href="#38922391">next</a><span>|</span><label class="collapse" for="c-38922520">[-]</label><label class="expand" for="c-38922520">[2 more]</label></div><br/><div class="children"><div class="content">Three 4060 Ti 16GB (there are single slot models) is around $1500. I think is possible to get a consumer system that&#x27;s cheaper than a 48GB Mac.</div><br/><div id="38922526" class="c"><input type="checkbox" id="c-38922526" checked=""/><div class="controls bullet"><span class="by">Casteil</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922520">parent</a><span>|</span><a href="#38922391">next</a><span>|</span><label class="collapse" for="c-38922526">[-]</label><label class="expand" for="c-38922526">[1 more]</label></div><br/><div class="children"><div class="content">Yep. Edited my post to reflect as much. The MBP makes a rather nice portable package though.</div><br/></div></div></div></div><div id="38922391" class="c"><input type="checkbox" id="c-38922391" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922300">parent</a><span>|</span><a href="#38922520">prev</a><span>|</span><a href="#38922933">next</a><span>|</span><label class="collapse" for="c-38922391">[-]</label><label class="expand" for="c-38922391">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>the newer Apple Silicon Macs with unified memory</i><p>Mixtral has been MLXd already? Write ups, if any?</div><br/><div id="38922697" class="c"><input type="checkbox" id="c-38922697" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922391">parent</a><span>|</span><a href="#38922421">next</a><span>|</span><label class="collapse" for="c-38922697">[-]</label><label class="expand" for="c-38922697">[1 more]</label></div><br/><div class="children"><div class="content">Yes it has, actually: <a href="https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx-examples">https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx-examples</a>. It&#x27;s right in the main repo. NB, I haven&#x27;t tried this, I&#x27;m using llama.cpp with a non-K-quant quantization on my MBP.</div><br/></div></div><div id="38922421" class="c"><input type="checkbox" id="c-38922421" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922391">parent</a><span>|</span><a href="#38922697">prev</a><span>|</span><a href="#38922933">next</a><span>|</span><label class="collapse" for="c-38922421">[-]</label><label class="expand" for="c-38922421">[2 more]</label></div><br/><div class="children"><div class="content">Not to my knowledge. But because the unified memory doubles as VRAM for the onboard GPU, normal GPU acceleration can access the entire model even if it&#x27;s 50+ GB. That&#x27;s why ASi Macs are currently the holy grail for at-home inferencing, and also why projects like llama.cpp focus so much on ASi above all else, and why so many UIs release for macOS first before other operating systems. Certain Mac models offer up to 192GB of unified memory.</div><br/><div id="38923414" class="c"><input type="checkbox" id="c-38923414" checked=""/><div class="controls bullet"><span class="by">mkesper</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922421">parent</a><span>|</span><a href="#38922933">next</a><span>|</span><label class="collapse" for="c-38923414">[-]</label><label class="expand" for="c-38923414">[1 more]</label></div><br/><div class="children"><div class="content">But that&#x27;s not a Macbook. And a Macbook M3Max with 128GB of RAM is almost 8000€.</div><br/></div></div></div></div></div></div></div></div><div id="38922933" class="c"><input type="checkbox" id="c-38922933" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#38922177">parent</a><span>|</span><a href="#38922300">prev</a><span>|</span><a href="#38922677">next</a><span>|</span><label class="collapse" for="c-38922933">[-]</label><label class="expand" for="c-38922933">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m looking forward to all the hardware announcements. It&#x27;s certainly looking like intentionally designed on device acceleration of LLMs for consumers is coming.</div><br/></div></div><div id="38922677" class="c"><input type="checkbox" id="c-38922677" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38922177">parent</a><span>|</span><a href="#38922933">prev</a><span>|</span><a href="#38923444">next</a><span>|</span><label class="collapse" for="c-38922677">[-]</label><label class="expand" for="c-38922677">[4 more]</label></div><br/><div class="children"><div class="content">Mixtral is good but those Ravenwolf benchmarks are meaningless. It’s like some random dude trying to reinvent MMLU without any rigor or consistency and in German. Dataset contamination is a problem, but not one that’s solved by folkloric evaluation of LLMs by people asking for tips on a subreddit.</div><br/><div id="38922742" class="c"><input type="checkbox" id="c-38922742" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922677">parent</a><span>|</span><a href="#38923856">next</a><span>|</span><label class="collapse" for="c-38922742">[-]</label><label class="expand" for="c-38922742">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think they&#x27;re meaningless; they have a few benefits:<p>1) He doesn&#x27;t have an ax to grind &#x2F; an LLM to pimp out, so he&#x27;s relatively even-handed<p>2) He uses the same (secret) test data for each model, so his testing is resistant to cherry-picking&#x2F;finetuning on tests<p>3) He likes weirdo role-play prompting, so he has a very good sense of the edges of refusal and alignment tuning<p>4) He picks up stuff well before it hits the only other fair testing I know of, the chat arena<p>5) I think asking stuff in German is at worst neutral, and at best useful for testing capacity in edge cases.<p>Practically speaking, his &#x27;preferred&#x27; non-giant models, Nous-Capybara-34B and Mixtral both are excellent in comparison with some of the others he looks at, and good recommendations.<p>That said, I&#x27;d like to see a test suite that GPT-4 fails at, or struggles at, at least. And, it would save him a lot of time if he could get something automated together, it&#x27;s clearly a lot of effort to hand test all those models.</div><br/><div id="38922803" class="c"><input type="checkbox" id="c-38922803" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922742">parent</a><span>|</span><a href="#38923856">next</a><span>|</span><label class="collapse" for="c-38922803">[-]</label><label class="expand" for="c-38922803">[1 more]</label></div><br/><div class="children"><div class="content">Any tests that are unfalsifiable and can’t be reproduced are meaningless when it comes to gauging the performance of LLMs (and most other things). I could also post on a subreddit and say I have a secret set of German tests that may or may not exist and and I like these models, but that does nothing to advance the science of evaluating these things. If you want to evaluate human preferences, you can use chatbot arena, which can be gamed, but at least reflects more than what one guy says to be true. And this is with me agreeing that Nous-Capybara is also a good model. But don’t take my word for it because it’s not worth very much!</div><br/></div></div></div></div><div id="38923856" class="c"><input type="checkbox" id="c-38923856" checked=""/><div class="controls bullet"><span class="by">epups</span><span>|</span><a href="#38922177">root</a><span>|</span><a href="#38922677">parent</a><span>|</span><a href="#38922742">prev</a><span>|</span><a href="#38923444">next</a><span>|</span><label class="collapse" for="c-38923856">[-]</label><label class="expand" for="c-38923856">[1 more]</label></div><br/><div class="children"><div class="content">I find it baffling that anyone would take these benchmarks seriously. The methodology is not transparent, and some of the tests completely unfair, like those in German or about specific niche German topics. The author readily acknowledges that these tests are his personal interests, which is totally fair. But that it would rise to the top of that subreddit and now HN as a general measure of quality of any kind is indicative of the lack of reliable benchmarks out there.</div><br/></div></div></div></div></div></div><div id="38921800" class="c"><input type="checkbox" id="c-38921800" checked=""/><div class="controls bullet"><span class="by">cuuupid</span><span>|</span><a href="#38922177">prev</a><span>|</span><a href="#38922180">next</a><span>|</span><label class="collapse" for="c-38921800">[-]</label><label class="expand" for="c-38921800">[37 more]</label></div><br/><div class="children"><div class="content">I’d like to note that this model’s parameter usage is low enough (13b) to run smoothly at high quality on a 3090 while beating GPT-3.5 on humaneval and sporting 32k context.<p>3090s are consumer grade and common on gaming rigs. I’m hoping game devs start experimenting with locally deployed Mixtral in their games. e.g. something like CIV but with each leader powered via LLM</div><br/><div id="38922202" class="c"><input type="checkbox" id="c-38922202" checked=""/><div class="controls bullet"><span class="by">snickell</span><span>|</span><a href="#38921800">parent</a><span>|</span><a href="#38921833">next</a><span>|</span><label class="collapse" for="c-38922202">[-]</label><label class="expand" for="c-38922202">[4 more]</label></div><br/><div class="children"><div class="content">You can also run Mixtral, at a decent token rate, on a post-2020 Apple Macbook Pro M1&#x2F;M2&#x2F;M3 with 32GB+ of RAM. 16GB RAM also works, sort of ok, which I suspect is the same quantization a 3090 is using, but I do notice a difference in the quantization. On my M2 Pro, the token rate and intelligence feels like GPT-3.5turbo. This is the first model I&#x27;ve started actually using (vs playing around with for the love of the tech) instead of GPT-3.5.<p>An Apple M2 Pro with 32GB of RAM is in the same price range as a gaming PC with a 3090, but its another example of normal people with moderately high performance systems &quot;accidentally&quot; being able to run a GPT-3.5 comparable model.<p>If you have an Apple meeting these specs and want to play around, LLM Studio is open source and has made it really easy to get started: <a href="https:&#x2F;&#x2F;lmstudio.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai&#x2F;</a><p>I hope to see a LOT more hobby hacking as a result of Mixtral and successors.</div><br/><div id="38923706" class="c"><input type="checkbox" id="c-38923706" checked=""/><div class="controls bullet"><span class="by">nraford</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38922202">parent</a><span>|</span><a href="#38922499">next</a><span>|</span><label class="collapse" for="c-38923706">[-]</label><label class="expand" for="c-38923706">[1 more]</label></div><br/><div class="children"><div class="content">How did you get Mixtral to run an a 32gb M1?<p>I tried using Ollama on my machine (same specs as above) and it told me I needed 49gb RAM minimum.</div><br/></div></div><div id="38922499" class="c"><input type="checkbox" id="c-38922499" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38922202">parent</a><span>|</span><a href="#38923706">prev</a><span>|</span><a href="#38921833">next</a><span>|</span><label class="collapse" for="c-38922499">[-]</label><label class="expand" for="c-38922499">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s true that LM Studio is open source. Maybe I&#x27;m missing something?</div><br/><div id="38923147" class="c"><input type="checkbox" id="c-38923147" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38922499">parent</a><span>|</span><a href="#38921833">next</a><span>|</span><label class="collapse" for="c-38923147">[-]</label><label class="expand" for="c-38923147">[1 more]</label></div><br/><div class="children"><div class="content">Lmstudio (that they linked) is definitely not open source, and doesn&#x27;t even offer a pricing model for business use.<p>Llmstudio is, but I suspect that was a typo in their comment. <a href="https:&#x2F;&#x2F;github.com&#x2F;TensorOpsAI&#x2F;LLMStudio">https:&#x2F;&#x2F;github.com&#x2F;TensorOpsAI&#x2F;LLMStudio</a></div><br/></div></div></div></div></div></div><div id="38921833" class="c"><input type="checkbox" id="c-38921833" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#38921800">parent</a><span>|</span><a href="#38922202">prev</a><span>|</span><a href="#38921843">next</a><span>|</span><label class="collapse" for="c-38921833">[-]</label><label class="expand" for="c-38921833">[15 more]</label></div><br/><div class="children"><div class="content">Google tells me that the RTX 3090 is priced between US$1,480 and $1,680.<p>You can buy a whole PC for that, I refuse to believe that a GPU priced that highly is &quot;consumer grade&quot; and &quot;common&quot;.<p>Are there any GPUs that are good for LLMs or other genAI that aren&#x27;t absurdly priced? Or ones specifically designed for AI rather than gaming graphics?</div><br/><div id="38922537" class="c"><input type="checkbox" id="c-38922537" checked=""/><div class="controls bullet"><span class="by">PrayagBhakar</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921833">parent</a><span>|</span><a href="#38921937">next</a><span>|</span><label class="collapse" for="c-38922537">[-]</label><label class="expand" for="c-38922537">[1 more]</label></div><br/><div class="children"><div class="content">[A used RTX 3090 goes for around $700.](<a href="https:&#x2F;&#x2F;prayag.bhakar.org&#x2F;apollo-ai-compute-cluster-for-the-gpu-poor#the-build" rel="nofollow">https:&#x2F;&#x2F;prayag.bhakar.org&#x2F;apollo-ai-compute-cluster-for-the-...</a>)</div><br/></div></div><div id="38921937" class="c"><input type="checkbox" id="c-38921937" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921833">parent</a><span>|</span><a href="#38922537">prev</a><span>|</span><a href="#38922083">next</a><span>|</span><label class="collapse" for="c-38921937">[-]</label><label class="expand" for="c-38921937">[8 more]</label></div><br/><div class="children"><div class="content">Gamers and LLM&#x2F;AI&#x2F;ML GPU users do not find that absurdly priced. Absurdly priced in our world is $15,000 so your perceptions are off by about a order of magnitude.</div><br/><div id="38921992" class="c"><input type="checkbox" id="c-38921992" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921937">parent</a><span>|</span><a href="#38922353">prev</a><span>|</span><a href="#38922083">next</a><span>|</span><label class="collapse" for="c-38921992">[-]</label><label class="expand" for="c-38921992">[6 more]</label></div><br/><div class="children"><div class="content">I can assure you a $1,500 graphics card is a big luxury for most gamers.<p><a href="https:&#x2F;&#x2F;store.steampowered.com&#x2F;hwsurvey&#x2F;Steam-Hardware-Software-Survey-Welcome-to-Steam" rel="nofollow">https:&#x2F;&#x2F;store.steampowered.com&#x2F;hwsurvey&#x2F;Steam-Hardware-Softw...</a><p>3090 isn&#x27;t even in the top 30.</div><br/><div id="38922384" class="c"><input type="checkbox" id="c-38922384" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921992">parent</a><span>|</span><a href="#38922970">next</a><span>|</span><label class="collapse" for="c-38922384">[-]</label><label class="expand" for="c-38922384">[1 more]</label></div><br/><div class="children"><div class="content">I would go even further - anytime I look at the hardware survey, I am surprised by the anemic and dated hardware people are running. Most people who game are not building a custom box anymore.</div><br/></div></div><div id="38922970" class="c"><input type="checkbox" id="c-38922970" checked=""/><div class="controls bullet"><span class="by">helloplanets</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921992">parent</a><span>|</span><a href="#38922384">prev</a><span>|</span><a href="#38923006">next</a><span>|</span><label class="collapse" for="c-38922970">[-]</label><label class="expand" for="c-38922970">[1 more]</label></div><br/><div class="children"><div class="content">Then again, the Apple II cost around $6.5k in today&#x27;s dollars. [0] My hunch is that people caring less for tricking out their computers for gaming is about people not being all that interested in being able to have the top of the line graphics settings enabled when playing AAA games. But I think the history of PCs and gaming very much proves that even normal consumers are willing to spend the big bucks on technology when it enables something truly new.<p>[0]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Apple_II" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Apple_II</a></div><br/></div></div><div id="38923006" class="c"><input type="checkbox" id="c-38923006" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921992">parent</a><span>|</span><a href="#38922970">prev</a><span>|</span><a href="#38922064">next</a><span>|</span><label class="collapse" for="c-38923006">[-]</label><label class="expand" for="c-38923006">[2 more]</label></div><br/><div class="children"><div class="content">Actually it should be #23 on that list but is split up into two items with roughly 0.6% each. Seems to be a bug.<p>Search the page for 3090 and see for yourself, it&#x27;s on the list twice.</div><br/><div id="38923114" class="c"><input type="checkbox" id="c-38923114" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38923006">parent</a><span>|</span><a href="#38922064">next</a><span>|</span><label class="collapse" for="c-38923114">[-]</label><label class="expand" for="c-38923114">[1 more]</label></div><br/><div class="children"><div class="content">To be honest I didn&#x27;t look that hard</div><br/></div></div></div></div><div id="38922064" class="c"><input type="checkbox" id="c-38922064" checked=""/><div class="controls bullet"><span class="by">ryanwaggoner</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921992">parent</a><span>|</span><a href="#38923006">prev</a><span>|</span><a href="#38922083">next</a><span>|</span><label class="collapse" for="c-38922064">[-]</label><label class="expand" for="c-38922064">[1 more]</label></div><br/><div class="children"><div class="content">Aren’t there others in that list above the 3090 that are even more expensive?</div><br/></div></div></div></div></div></div><div id="38922083" class="c"><input type="checkbox" id="c-38922083" checked=""/><div class="controls bullet"><span class="by">rfw300</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921833">parent</a><span>|</span><a href="#38921937">prev</a><span>|</span><a href="#38921954">next</a><span>|</span><label class="collapse" for="c-38922083">[-]</label><label class="expand" for="c-38922083">[1 more]</label></div><br/><div class="children"><div class="content">I recently purchased a 3090 on Reddit’s hardwareswap community for $550. New GPUs are pricey right now because of shortages, but if you look around a bit it can be affordable.</div><br/></div></div><div id="38921954" class="c"><input type="checkbox" id="c-38921954" checked=""/><div class="controls bullet"><span class="by">Conasg</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921833">parent</a><span>|</span><a href="#38922083">prev</a><span>|</span><a href="#38922014">next</a><span>|</span><label class="collapse" for="c-38921954">[-]</label><label class="expand" for="c-38921954">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, I got a card second hand to play with, and it was only £700 ($900~) and it came with manufacturer warranty. It was a bit of a gamble, but the 24GB VRAM has been a godsend for experimenting with LLMs. And playing video games at 4K!</div><br/></div></div><div id="38921953" class="c"><input type="checkbox" id="c-38921953" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921833">parent</a><span>|</span><a href="#38922014">prev</a><span>|</span><a href="#38921867">next</a><span>|</span><label class="collapse" for="c-38921953">[-]</label><label class="expand" for="c-38921953">[1 more]</label></div><br/><div class="children"><div class="content">Nah. It&#x27;s about half that. You can pick up a used 4090 for that much.</div><br/></div></div><div id="38921867" class="c"><input type="checkbox" id="c-38921867" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921833">parent</a><span>|</span><a href="#38921953">prev</a><span>|</span><a href="#38921843">next</a><span>|</span><label class="collapse" for="c-38921867">[-]</label><label class="expand" for="c-38921867">[1 more]</label></div><br/><div class="children"><div class="content">tl;dr, no, especially since AMD is lagging behind.<p>Apple is the one doing the best in terms of making consumer-friendly hardware that can perform AI&#x2F;ML tasks...but that involves a <i>different</i> problem regarding video games.</div><br/></div></div></div></div><div id="38921843" class="c"><input type="checkbox" id="c-38921843" checked=""/><div class="controls bullet"><span class="by">RandomBK</span><span>|</span><a href="#38921800">parent</a><span>|</span><a href="#38921833">prev</a><span>|</span><a href="#38922906">next</a><span>|</span><label class="collapse" for="c-38921843">[-]</label><label class="expand" for="c-38921843">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth noting that the 4bit quants can run on cpu at ~reading speed, which should unlock many use cases - especially if we can precompute some of the results async.</div><br/><div id="38921887" class="c"><input type="checkbox" id="c-38921887" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921843">parent</a><span>|</span><a href="#38922269">next</a><span>|</span><label class="collapse" for="c-38921887">[-]</label><label class="expand" for="c-38921887">[3 more]</label></div><br/><div class="children"><div class="content">That assumes full CPU utilization, with no other tasks heavily using the CPU.<p>In the case of high-end video games, that&#x27;s unlikely.</div><br/><div id="38921940" class="c"><input type="checkbox" id="c-38921940" checked=""/><div class="controls bullet"><span class="by">somnic</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921887">parent</a><span>|</span><a href="#38921931">next</a><span>|</span><label class="collapse" for="c-38921940">[-]</label><label class="expand" for="c-38921940">[1 more]</label></div><br/><div class="children"><div class="content">Resource constraints would be a concern, yeah, so if you were developing a game featuring LLMs (which would, at this point in their development and maturity, be a gimmick) you would keep that in mind and keep other demands on resources low.</div><br/></div></div><div id="38921931" class="c"><input type="checkbox" id="c-38921931" checked=""/><div class="controls bullet"><span class="by">RandomBK</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921887">parent</a><span>|</span><a href="#38921940">prev</a><span>|</span><a href="#38922269">next</a><span>|</span><label class="collapse" for="c-38921931">[-]</label><label class="expand" for="c-38921931">[1 more]</label></div><br/><div class="children"><div class="content">True, yet many games are effectively single-threaded anyways.<p>The bigger problem is memory capacity and bandwidth, but I suspect folks will eventually figure out some sort of QoS setup to let the system crunch LLMs using otherwise unused&#x2F;idle resources.</div><br/></div></div></div></div><div id="38922269" class="c"><input type="checkbox" id="c-38922269" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921843">parent</a><span>|</span><a href="#38921887">prev</a><span>|</span><a href="#38922906">next</a><span>|</span><label class="collapse" for="c-38922269">[-]</label><label class="expand" for="c-38922269">[1 more]</label></div><br/><div class="children"><div class="content">Although in my testing the 4bit reasoning was not nearly as good.</div><br/></div></div></div></div><div id="38922906" class="c"><input type="checkbox" id="c-38922906" checked=""/><div class="controls bullet"><span class="by">LanternLight83</span><span>|</span><a href="#38921800">parent</a><span>|</span><a href="#38921843">prev</a><span>|</span><a href="#38921813">next</a><span>|</span><label class="collapse" for="c-38922906">[-]</label><label class="expand" for="c-38922906">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been working with local models as agents, and anyone interested in trying this needs to know about llama.ccp&#x27;s &quot;gammers&quot; feature. You can force the model&#x27;s output to conform to a specific structure, which is not only useful for ensuring that you recieve eg. valid JSON output but more specific stuff like &quot;if you choose to do x, you must also provide y&quot;, which can be great for influencing it&#x27;s thinking (eg. an actor who&#x27;s planning ahead might be required to resond with three of any of the five W&#x27;s (it&#x27;s choice which three), but then it get&#x27;s to be free-form inside of the JSON string values, which can be used as context for a following selection from a restricted set of actions; or a model might have option of asking for more time to think at the end if it&#x27;s response, but if it doesn&#x27;t then it needs to specify it&#x27;s next action). This does&#x27;t impact generation speed AFAICT and can be used in vry creative ways, but results can still need re-generated if they&#x27;re truncated and I had to write a function to stop immediately when the valid JSON obj is closed (ie. at the end) or when more that like five newlines are generated it a row. This will vary by model.</div><br/></div></div><div id="38921813" class="c"><input type="checkbox" id="c-38921813" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38921800">parent</a><span>|</span><a href="#38922906">prev</a><span>|</span><a href="#38922163">next</a><span>|</span><label class="collapse" for="c-38921813">[-]</label><label class="expand" for="c-38921813">[7 more]</label></div><br/><div class="children"><div class="content">The average gamer doesn&#x27;t have a 3090-equivalent, or even an Nvidia GPU.<p>Running LLMs locally to create custom dialogue for games is still years away.</div><br/><div id="38923665" class="c"><input type="checkbox" id="c-38923665" checked=""/><div class="controls bullet"><span class="by">Mashimo</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921813">parent</a><span>|</span><a href="#38921849">next</a><span>|</span><label class="collapse" for="c-38923665">[-]</label><label class="expand" for="c-38923665">[1 more]</label></div><br/><div class="children"><div class="content">&gt; or even an Nvidia GPU.<p>What do you mean? Most gamers do have an nvidia GPU.</div><br/></div></div><div id="38921849" class="c"><input type="checkbox" id="c-38921849" checked=""/><div class="controls bullet"><span class="by">somnic</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921813">parent</a><span>|</span><a href="#38923665">prev</a><span>|</span><a href="#38922712">next</a><span>|</span><label class="collapse" for="c-38921849">[-]</label><label class="expand" for="c-38921849">[2 more]</label></div><br/><div class="children"><div class="content">VR isn&#x27;t pragmatically accessible to the average gamer due to hardware requirements and the necessity of setting up the right physical environment but there are still VR games.</div><br/><div id="38922002" class="c"><input type="checkbox" id="c-38922002" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921849">parent</a><span>|</span><a href="#38922712">next</a><span>|</span><label class="collapse" for="c-38922002">[-]</label><label class="expand" for="c-38922002">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a few after almost a decade of vr being a thing.</div><br/></div></div></div></div><div id="38922712" class="c"><input type="checkbox" id="c-38922712" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38921813">parent</a><span>|</span><a href="#38921849">prev</a><span>|</span><a href="#38921870">next</a><span>|</span><label class="collapse" for="c-38922712">[-]</label><label class="expand" for="c-38922712">[1 more]</label></div><br/><div class="children"><div class="content">Why couldn&#x27;t this be handled remotely and be handled as part of a subscription to the game?</div><br/></div></div></div></div><div id="38922163" class="c"><input type="checkbox" id="c-38922163" checked=""/><div class="controls bullet"><span class="by">sanjiwatsuki</span><span>|</span><a href="#38921800">parent</a><span>|</span><a href="#38921813">prev</a><span>|</span><a href="#38922225">next</a><span>|</span><label class="collapse" for="c-38922163">[-]</label><label class="expand" for="c-38922163">[3 more]</label></div><br/><div class="children"><div class="content">The VRAM usage is closer to a 47B model - although only 2 experts are used at a time for inference, all experts are needed to complete it.</div><br/><div id="38922479" class="c"><input type="checkbox" id="c-38922479" checked=""/><div class="controls bullet"><span class="by">discordance</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38922163">parent</a><span>|</span><a href="#38922225">next</a><span>|</span><label class="collapse" for="c-38922479">[-]</label><label class="expand" for="c-38922479">[2 more]</label></div><br/><div class="children"><div class="content">Confirmed. Currently running Mixtral 8x7B gguf (Q8_0) on a Macbook Pro M1 Max w 64GB ram, and RAM usage is sitting at 48.8 GB.</div><br/><div id="38923503" class="c"><input type="checkbox" id="c-38923503" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#38921800">root</a><span>|</span><a href="#38922479">parent</a><span>|</span><a href="#38922225">next</a><span>|</span><label class="collapse" for="c-38923503">[-]</label><label class="expand" for="c-38923503">[1 more]</label></div><br/><div class="children"><div class="content">How many t&#x2F;s?</div><br/></div></div></div></div></div></div><div id="38922225" class="c"><input type="checkbox" id="c-38922225" checked=""/><div class="controls bullet"><span class="by">fswd</span><span>|</span><a href="#38921800">parent</a><span>|</span><a href="#38922163">prev</a><span>|</span><a href="#38922180">next</a><span>|</span><label class="collapse" for="c-38922225">[-]</label><label class="expand" for="c-38922225">[1 more]</label></div><br/><div class="children"><div class="content">you cannot currently run mixtral with a 32k context on a 3090. Unless am I wrong? I think the largest context I was able to reproduce was around 1500 with 2 or 3 bit, I would have to look at my notes.</div><br/></div></div></div></div><div id="38922180" class="c"><input type="checkbox" id="c-38922180" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#38921800">prev</a><span>|</span><a href="#38922564">next</a><span>|</span><label class="collapse" for="c-38922180">[-]</label><label class="expand" for="c-38922180">[3 more]</label></div><br/><div class="children"><div class="content">If anyone wants to try out this model, I believe it&#x27;s one of the ones released as a Llamafile by Mozilla&#x2F;jart[0].<p>1) Download llamafile[1] (30.03 GB): <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;jartine&#x2F;Mixtral-8x7B-Instruct-v0.1-llamafile&#x2F;resolve&#x2F;main&#x2F;mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile?download=true" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;jartine&#x2F;Mixtral-8x7B-Instruct-v0.1-ll...</a><p>2) chmod +x mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile<p>3) .&#x2F;mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile<p>[0] <a href="https:&#x2F;&#x2F;hacks.mozilla.org&#x2F;2023&#x2F;11&#x2F;introducing-llamafile&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hacks.mozilla.org&#x2F;2023&#x2F;11&#x2F;introducing-llamafile&#x2F;</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile#quickstart">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile#quickstart</a></div><br/><div id="38922668" class="c"><input type="checkbox" id="c-38922668" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#38922180">parent</a><span>|</span><a href="#38922564">next</a><span>|</span><label class="collapse" for="c-38922668">[-]</label><label class="expand" for="c-38922668">[2 more]</label></div><br/><div class="children"><div class="content">I get that it&#x27;s clever, but consider making this person less popular. <a href="https:&#x2F;&#x2F;www.thedailybeast.com&#x2F;occupying-the-throne-justine-tunney-neoreactionaries-and-the-new-1" rel="nofollow">https:&#x2F;&#x2F;www.thedailybeast.com&#x2F;occupying-the-throne-justine-t...</a> <a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20140904232836&#x2F;http:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2014&#x2F;mar&#x2F;20&#x2F;occupy-founder-obama-eric-schmidt-ceo-america" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20140904232836&#x2F;http:&#x2F;&#x2F;www.thegua...</a></div><br/><div id="38923065" class="c"><input type="checkbox" id="c-38923065" checked=""/><div class="controls bullet"><span class="by">timschmidt</span><span>|</span><a href="#38922180">root</a><span>|</span><a href="#38922668">parent</a><span>|</span><a href="#38922564">next</a><span>|</span><label class="collapse" for="c-38923065">[-]</label><label class="expand" for="c-38923065">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s easy, just write better software than she does.</div><br/></div></div></div></div></div></div><div id="38922564" class="c"><input type="checkbox" id="c-38922564" checked=""/><div class="controls bullet"><span class="by">aunty_helen</span><span>|</span><a href="#38922180">prev</a><span>|</span><a href="#38921874">next</a><span>|</span><label class="collapse" for="c-38922564">[-]</label><label class="expand" for="c-38922564">[3 more]</label></div><br/><div class="children"><div class="content">On mac silicon:<p><a href="https:&#x2F;&#x2F;ollama.ai&#x2F;">https:&#x2F;&#x2F;ollama.ai&#x2F;</a><p>ollama pull mixtral<p>For a chatgpt-esk web ui<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ollama-webui&#x2F;ollama-webui">https:&#x2F;&#x2F;github.com&#x2F;ollama-webui&#x2F;ollama-webui</a><p>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v ollama-webui:&#x2F;app&#x2F;backend&#x2F;data --name ollama-webui --restart always ghcr.io&#x2F;ollama-webui&#x2F;ollama-webui:main<p>Navigate to <a href="http:&#x2F;&#x2F;localhost:3000" rel="nofollow">http:&#x2F;&#x2F;localhost:3000</a><p>You can also use ollama in langchain.</div><br/><div id="38923567" class="c"><input type="checkbox" id="c-38923567" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#38922564">parent</a><span>|</span><a href="#38922655">next</a><span>|</span><label class="collapse" for="c-38923567">[-]</label><label class="expand" for="c-38923567">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t use Ollama (or any LLM host) through docker on Mac m1 since as far as I&#x27;m aware there&#x27;s still no support for Metal.</div><br/></div></div><div id="38922655" class="c"><input type="checkbox" id="c-38922655" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#38922564">parent</a><span>|</span><a href="#38923567">prev</a><span>|</span><a href="#38921874">next</a><span>|</span><label class="collapse" for="c-38922655">[-]</label><label class="expand" for="c-38922655">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also some unlocked fine tunings available. Dolphin seems to be a very popular one. (Trained on more coding data) If you want to fit under 32gb, there&#x27;s <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;dolphin-mixtral:8x7b-v2.7-q3_K_M">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;dolphin-mixtral:8x7b-v2.7-q3_K_M</a></div><br/></div></div></div></div><div id="38921874" class="c"><input type="checkbox" id="c-38921874" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#38922564">prev</a><span>|</span><a href="#38922800">next</a><span>|</span><label class="collapse" for="c-38921874">[-]</label><label class="expand" for="c-38921874">[10 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks
</code></pre>
I&#x27;m interested in seeing how it does with mathematics. That has always seemed like a particular weakness that no one yet has effectively cracked.</div><br/><div id="38921899" class="c"><input type="checkbox" id="c-38921899" checked=""/><div class="controls bullet"><span class="by">justinl33</span><span>|</span><a href="#38921874">parent</a><span>|</span><a href="#38922800">next</a><span>|</span><label class="collapse" for="c-38921899">[-]</label><label class="expand" for="c-38921899">[9 more]</label></div><br/><div class="children"><div class="content">it&#x27;s sort of a weakness inherent to LLM&#x27;s... next word prediction isn&#x27;t really supposed to be good at math.<p>I doubt it will be ever &#x27;cracked&#x27; with better LLM&#x27;s, only multimodal ones with access to program execution and calculators.</div><br/><div id="38921990" class="c"><input type="checkbox" id="c-38921990" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38921899">parent</a><span>|</span><a href="#38922522">next</a><span>|</span><label class="collapse" for="c-38921990">[-]</label><label class="expand" for="c-38921990">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s been attempts to use different embeddings for numbers, which helps a lot. E.g., <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37936005">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37936005</a></div><br/></div></div><div id="38922522" class="c"><input type="checkbox" id="c-38922522" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38921899">parent</a><span>|</span><a href="#38921990">prev</a><span>|</span><a href="#38923254">next</a><span>|</span><label class="collapse" for="c-38922522">[-]</label><label class="expand" for="c-38922522">[3 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s sort of a weakness inherent to LLM&#x27;s... next word prediction isn&#x27;t really supposed to be good at math.<p>FWIW I don&#x27;t agree with this in a theoretical sense. The reason LLMs can do as much as they can is because next token prediction attempts to infer a world model for the processes that generated each next-token in the training set. I don&#x27;t see a reason that this would preclude learning arithmetic in order to better predict next tokens that require arithmetic.<p>I&#x27;d guess that arithmetic will become suddenly reliable with one of the next significant (e.g. 2-5x) jumps in parameter count.</div><br/><div id="38923588" class="c"><input type="checkbox" id="c-38923588" checked=""/><div class="controls bullet"><span class="by">justinl33</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38922522">parent</a><span>|</span><a href="#38923254">next</a><span>|</span><label class="collapse" for="c-38923588">[-]</label><label class="expand" for="c-38923588">[2 more]</label></div><br/><div class="children"><div class="content">I disagree; 1. the parameters are there to make the model better at learning textual patterns, not arithmetic patterns. 2. Next token prediction is a terrible way to perform arithmetic. 3. Perhaps most importantly, the loss function does not incentivise being good at arithmetic at all.<p>Any perceived arithmetic ability is just a textual coincidence.<p>I agree that a sufficiently intelligent LLM, like 300+ IQ would have an excellent model of how multiplication works. It may even assist in finding new theorems, but a calculator will always be better at 926*725.</div><br/><div id="38923622" class="c"><input type="checkbox" id="c-38923622" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38923588">parent</a><span>|</span><a href="#38923254">next</a><span>|</span><label class="collapse" for="c-38923622">[-]</label><label class="expand" for="c-38923622">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 1. the parameters are there to make the model better at learning textual patterns, not arithmetic patterns.<p>There&#x27;s nothing &quot;textual&quot; about the tokens. They are arbitrary identifiers.  There&#x27;s nothing &quot;textual&quot; about transformers!  The fact that e.g. GPT-4 can accept images as input, and that its textual performance improved as a result, and that transformers are also being used for text-to-speech models should have already communicated this.<p>&gt; 2. Next token prediction is a terrible way to perform arithmetic.<p>This is just attempting to resolve our disagreement with pure assertion. It&#x27;s certainly <i>less efficient</i> to use an artificial intelligence to do arithmetic.  But whether it&#x27;s efficient is a different question than how likely it is to be possible.<p>&gt; 3. Perhaps most importantly, the loss function does not incentivise being good at arithmetic at all.<p>This is blatantly untrue. The same argument would suggest that LLMs can&#x27;t do anything that wasn&#x27;t exactly in their training set already. But they can.</div><br/></div></div></div></div></div></div><div id="38923254" class="c"><input type="checkbox" id="c-38923254" checked=""/><div class="controls bullet"><span class="by">monkeydust</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38921899">parent</a><span>|</span><a href="#38922522">prev</a><span>|</span><a href="#38921968">next</a><span>|</span><label class="collapse" for="c-38923254">[-]</label><label class="expand" for="c-38923254">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need multimodals to access tools such as calculator. Check out PandasAI or Langchain Agent workflow. Rather than workout 451 * 995 for example the llm constructs the pandas query, runs it, returns result to user. Works pretty well.</div><br/></div></div><div id="38921968" class="c"><input type="checkbox" id="c-38921968" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38921899">parent</a><span>|</span><a href="#38923254">prev</a><span>|</span><a href="#38922590">next</a><span>|</span><label class="collapse" for="c-38921968">[-]</label><label class="expand" for="c-38921968">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. But they do call it out here and I&#x27;m interested in understanding why.</div><br/></div></div><div id="38922590" class="c"><input type="checkbox" id="c-38922590" checked=""/><div class="controls bullet"><span class="by">jakderrida</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38921899">parent</a><span>|</span><a href="#38921968">prev</a><span>|</span><a href="#38922800">next</a><span>|</span><label class="collapse" for="c-38922590">[-]</label><label class="expand" for="c-38922590">[2 more]</label></div><br/><div class="children"><div class="content">Annoyingly, Bard with Gemini will apparently use coding to answer every logical thinking question and get them all wrong. If you end with &quot;Do not use code&quot;, it will get them all right.</div><br/><div id="38922701" class="c"><input type="checkbox" id="c-38922701" checked=""/><div class="controls bullet"><span class="by">Kerbonut</span><span>|</span><a href="#38921874">root</a><span>|</span><a href="#38922590">parent</a><span>|</span><a href="#38922800">next</a><span>|</span><label class="collapse" for="c-38922701">[-]</label><label class="expand" for="c-38922701">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT-4 is starting to do this now all of the sudden, and it is also annoyingly returning erroneous information.</div><br/></div></div></div></div></div></div></div></div><div id="38922800" class="c"><input type="checkbox" id="c-38922800" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#38921874">prev</a><span>|</span><a href="#38923478">next</a><span>|</span><label class="collapse" for="c-38922800">[-]</label><label class="expand" for="c-38922800">[1 more]</label></div><br/><div class="children"><div class="content">Recent and related:<p><i>Mixtral of experts</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38598559">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38598559</a> - Dec 2023 (300 comments)<p><i>Mistral-8x7B-Chat</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38594578">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38594578</a> - Dec 2023 (69 comments)<p><i>Mistral &quot;Mixtral&quot; 8x7B 32k model [magnet]</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38570537">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38570537</a> - Dec 2023 (239 comments)</div><br/></div></div><div id="38923478" class="c"><input type="checkbox" id="c-38923478" checked=""/><div class="controls bullet"><span class="by">cgeier</span><span>|</span><a href="#38922800">prev</a><span>|</span><a href="#38923672">next</a><span>|</span><label class="collapse" for="c-38923478">[-]</label><label class="expand" for="c-38923478">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t read a lot of LLM papers, but I believe this is a rather weak paper low on details (note: not the results achieved of the LLM, but the paper itself).  If it had landed on my desk for a review, I probably would have sent it back just based on that.<p>For example, they never really say how they trained the experts or which dataset they used.<p>Is this the current standard in the field?</div><br/><div id="38923508" class="c"><input type="checkbox" id="c-38923508" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38923478">parent</a><span>|</span><a href="#38923672">next</a><span>|</span><label class="collapse" for="c-38923508">[-]</label><label class="expand" for="c-38923508">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is this the current standard in the field?<p>It’s becoming pretty common, yeah. The two things you mentioned: training particulars and dataset mixture are also basically the only competitive advantage companies have. Since the code&#x2F;architecture is trivial to reproduce, anyone with enough money can make a competing model “easily”.<p>OpenAI started this trend and cemented it with GPT4’s “technical report” which didn’t even specify the number of parameters in the model. They’ve been historically vague about their dataset for far longer than that though.</div><br/></div></div></div></div><div id="38923672" class="c"><input type="checkbox" id="c-38923672" checked=""/><div class="controls bullet"><span class="by">Mashimo</span><span>|</span><a href="#38923478">prev</a><span>|</span><a href="#38921947">next</a><span>|</span><label class="collapse" for="c-38923672">[-]</label><label class="expand" for="c-38923672">[1 more]</label></div><br/><div class="children"><div class="content">Anyone know of a decent model for coding assistant that can run on a 16GB vram rtx 4060ti?</div><br/></div></div><div id="38921947" class="c"><input type="checkbox" id="c-38921947" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38923672">prev</a><span>|</span><a href="#38922155">next</a><span>|</span><label class="collapse" for="c-38921947">[-]</label><label class="expand" for="c-38921947">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious when we&#x27;ll start to see open access multimodal models being released.<p>The advancement in text only models has been amazing, but a lot of the &#x27;emergent&#x27; behavior in GPT-4 may be because of multimodal training and not just MoE or parameter sizes.<p>I&#x27;ll be curious to see if multimodal smaller models see similar leaps.</div><br/><div id="38922050" class="c"><input type="checkbox" id="c-38922050" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38921947">parent</a><span>|</span><a href="#38922129">next</a><span>|</span><label class="collapse" for="c-38922050">[-]</label><label class="expand" for="c-38922050">[1 more]</label></div><br/><div class="children"><div class="content">LLaVA is open, although not the leap you are expecting: <a href="https:&#x2F;&#x2F;llava-vl.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llava-vl.github.io&#x2F;</a><p>Meta also released a (non-commercial) multimodal model among 6 modalities: <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;imagebind-six-modalities-binding-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;imagebind-six-modalities-binding-ai...</a></div><br/></div></div><div id="38922129" class="c"><input type="checkbox" id="c-38922129" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#38921947">parent</a><span>|</span><a href="#38922050">prev</a><span>|</span><a href="#38922032">next</a><span>|</span><label class="collapse" for="c-38922129">[-]</label><label class="expand" for="c-38922129">[2 more]</label></div><br/><div class="children"><div class="content">CogVLM is very good in my (brief) testing: <a href="https:&#x2F;&#x2F;github.com&#x2F;THUDM&#x2F;CogVLM">https:&#x2F;&#x2F;github.com&#x2F;THUDM&#x2F;CogVLM</a><p>The model weights seem to be under a non-commercial license, not true open source, but it is &quot;open access&quot; as you requested.<p>It would be nice if someone trained a CogVLM-compatible model from scratch under an open source license.</div><br/><div id="38922547" class="c"><input type="checkbox" id="c-38922547" checked=""/><div class="controls bullet"><span class="by">beoberha</span><span>|</span><a href="#38921947">root</a><span>|</span><a href="#38922129">parent</a><span>|</span><a href="#38922032">next</a><span>|</span><label class="collapse" for="c-38922547">[-]</label><label class="expand" for="c-38922547">[1 more]</label></div><br/><div class="children"><div class="content">When I tried last, it couldn’t be run on M-series Macs.</div><br/></div></div></div></div><div id="38922032" class="c"><input type="checkbox" id="c-38922032" checked=""/><div class="controls bullet"><span class="by">ijustlovemath</span><span>|</span><a href="#38921947">parent</a><span>|</span><a href="#38922129">prev</a><span>|</span><a href="#38922155">next</a><span>|</span><label class="collapse" for="c-38922032">[-]</label><label class="expand" for="c-38922032">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard that Google actually got the jump on OpenAI in this regard (just from people in a FAANG), and they&#x27;re playing a bit of catch-up. OpenAI still has a distinct advantage on the language side, though. This is all hearsay, of course.</div><br/></div></div></div></div><div id="38922155" class="c"><input type="checkbox" id="c-38922155" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#38921947">prev</a><span>|</span><a href="#38922042">next</a><span>|</span><label class="collapse" for="c-38922155">[-]</label><label class="expand" for="c-38922155">[4 more]</label></div><br/><div class="children"><div class="content">Is this a model that can be run using Simon Wilison&#x27;s LLM tool? I cannot find any mention of Mixtral in the issues nor in the discussions. Is there an easy way to play with this model from the command line other than that?</div><br/><div id="38922352" class="c"><input type="checkbox" id="c-38922352" checked=""/><div class="controls bullet"><span class="by">gsharma</span><span>|</span><a href="#38922155">parent</a><span>|</span><a href="#38922288">next</a><span>|</span><label class="collapse" for="c-38922352">[-]</label><label class="expand" for="c-38922352">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is probably the easiest way. <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral</a><p>LM Studio is another option.</div><br/></div></div><div id="38922288" class="c"><input type="checkbox" id="c-38922288" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#38922155">parent</a><span>|</span><a href="#38922352">prev</a><span>|</span><a href="#38922278">next</a><span>|</span><label class="collapse" for="c-38922288">[-]</label><label class="expand" for="c-38922288">[1 more]</label></div><br/><div class="children"><div class="content">Unsure what Simon Wilson&#x27;s program does, but you can pull models via many methods.<p>For CLI, Ollama: <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral</a><p>With UI, GPT4All: <a href="https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html</a> (doesn&#x27;t yet support <i>Mixtral</i>)<p>In-app, superagent.sh: <a href="https:&#x2F;&#x2F;github.com&#x2F;homanp&#x2F;superagent">https:&#x2F;&#x2F;github.com&#x2F;homanp&#x2F;superagent</a></div><br/></div></div><div id="38922278" class="c"><input type="checkbox" id="c-38922278" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38922155">parent</a><span>|</span><a href="#38922288">prev</a><span>|</span><a href="#38922042">next</a><span>|</span><label class="collapse" for="c-38922278">[-]</label><label class="expand" for="c-38922278">[1 more]</label></div><br/><div class="children"><div class="content">ollama or llama.cpp</div><br/></div></div></div></div><div id="38922042" class="c"><input type="checkbox" id="c-38922042" checked=""/><div class="controls bullet"><span class="by">invert_franklin</span><span>|</span><a href="#38922155">prev</a><span>|</span><a href="#38921717">next</a><span>|</span><label class="collapse" for="c-38922042">[-]</label><label class="expand" for="c-38922042">[2 more]</label></div><br/><div class="children"><div class="content">Does anyone know what Figure 8 at the end shows?<p>It looks like each expert is used interchangeably with no clear pattern. And earlier they say &quot;Surprisingly, we do not observe obvious patterns in the assignment of experts based on the topic.&quot;<p>So then, what is the point of the &quot;expert&quot;?<p>Could this extra performance just be through the 8 expert architectural design, and not be based on the underlying training material? For example, if all 8 experts were ArXiv papers, would the performance be different?</div><br/><div id="38922616" class="c"><input type="checkbox" id="c-38922616" checked=""/><div class="controls bullet"><span class="by">jakderrida</span><span>|</span><a href="#38922042">parent</a><span>|</span><a href="#38921717">next</a><span>|</span><label class="collapse" for="c-38922616">[-]</label><label class="expand" for="c-38922616">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if it answers your question, but the expertise of each expert arise from the training process and aren&#x27;t assigned by a human. Thus, they wouldn&#x27;t be discernable to a human. The choice of the model to use, I think, is something called like a &quot;gating network&quot;. That&#x27;s also trained to favor most appropriate model based on training.</div><br/></div></div></div></div><div id="38921717" class="c"><input type="checkbox" id="c-38921717" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#38922042">prev</a><span>|</span><a href="#38922778">next</a><span>|</span><label class="collapse" for="c-38921717">[-]</label><label class="expand" for="c-38921717">[6 more]</label></div><br/><div class="children"><div class="content">Wasn’t this what was released at the end of last year?</div><br/><div id="38921768" class="c"><input type="checkbox" id="c-38921768" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38921717">parent</a><span>|</span><a href="#38921740">next</a><span>|</span><label class="collapse" for="c-38921768">[-]</label><label class="expand" for="c-38921768">[2 more]</label></div><br/><div class="children"><div class="content">Yes, the Mixtral magnet link was tweeted on the 8th of December: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;mistralai&#x2F;status&#x2F;1733150512395038967" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;mistralai&#x2F;status&#x2F;1733150512395038967</a><p>The paper just came out today: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;dchaplot&#x2F;status&#x2F;1744547220983005478l" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;dchaplot&#x2F;status&#x2F;1744547220983005478l</a></div><br/><div id="38921850" class="c"><input type="checkbox" id="c-38921850" checked=""/><div class="controls bullet"><span class="by">grepfru_it</span><span>|</span><a href="#38921717">root</a><span>|</span><a href="#38921768">parent</a><span>|</span><a href="#38921740">next</a><span>|</span><label class="collapse" for="c-38921850">[-]</label><label class="expand" for="c-38921850">[1 more]</label></div><br/><div class="children"><div class="content">Also available on Ollama: <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral</a></div><br/></div></div></div></div><div id="38921740" class="c"><input type="checkbox" id="c-38921740" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#38921717">parent</a><span>|</span><a href="#38921768">prev</a><span>|</span><a href="#38921779">next</a><span>|</span><label class="collapse" for="c-38921740">[-]</label><label class="expand" for="c-38921740">[1 more]</label></div><br/><div class="children"><div class="content">The model itself was, but was the writeup? This submission is from yesterday.<p>I&#x27;ve not been following their releases too well but it seemed they were very much on the side of releasing models asap.</div><br/></div></div><div id="38921779" class="c"><input type="checkbox" id="c-38921779" checked=""/><div class="controls bullet"><span class="by">ricopags</span><span>|</span><a href="#38921717">parent</a><span>|</span><a href="#38921740">prev</a><span>|</span><a href="#38921742">next</a><span>|</span><label class="collapse" for="c-38921779">[-]</label><label class="expand" for="c-38921779">[1 more]</label></div><br/><div class="children"><div class="content">The model&#x27;s weights were released in a torrent, but this is the much anticipated paper detailing some of the work.</div><br/></div></div></div></div><div id="38922778" class="c"><input type="checkbox" id="c-38922778" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#38921717">prev</a><span>|</span><a href="#38923202">next</a><span>|</span><label class="collapse" for="c-38922778">[-]</label><label class="expand" for="c-38922778">[1 more]</label></div><br/><div class="children"><div class="content">questions running through my head:
Is there some magic with the number 8? Why not 6? 11?<p>Each of these 8 models were 7B models. What about using 80 x tinyllama 1B models?</div><br/></div></div><div id="38921818" class="c"><input type="checkbox" id="c-38921818" checked=""/><div class="controls bullet"><span class="by">justinl33</span><span>|</span><a href="#38923202">prev</a><span>|</span><label class="collapse" for="c-38921818">[-]</label><label class="expand" for="c-38921818">[2 more]</label></div><br/><div class="children"><div class="content">tldr;<p>- &#x27;sparse mixture of experts&#x27; means each layer contains 8 mini-neural-nets (experts), each trained to be good at certain data&#x2F;tasks<p>- a router network passes each token (word) to 2 experts which suit it best<p>- since only 2&#x2F;8 experts are utilized, the model effectively only uses 13&#x2F;47B parameters during inference (text generation)<p>- this expert mechanism makes it very efficient and effective (uses less params, is able to speciailize to tokens)<p>- beats llmana 70b and gpt-3.5, especially at math and coding and language<p>- has fine tuned model able to beat gemini pro, as well as llmana 70b and gpt-3.5</div><br/></div></div></div></div></div></div></div></body></html>