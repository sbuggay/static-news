<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1710666065876" as="style"/><link rel="stylesheet" href="styles.css?v=1710666065876"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/tspeterkim/flash-attention-minimal">Show HN: Flash Attention in ~100 lines of CUDA</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>tspeterkim</span> | <span>39 comments</span></div><br/><div><div id="39727161" class="c"><input type="checkbox" id="c-39727161" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39727520">next</a><span>|</span><label class="collapse" for="c-39727161">[-]</label><label class="expand" for="c-39727161">[14 more]</label></div><br/><div class="children"><div class="content">Pretty neat implementation. In general, for these sort of exercises (and even if the intention is to go to prod with custom kernels) I lean towards Triton to write the kernels themselves. It is much more easier to integrate to the tool chain, and allows a level of abstraction that doesn&#x27;t affect performance even a little bit while providing useful constructs.</div><br/><div id="39730758" class="c"><input type="checkbox" id="c-39730758" checked=""/><div class="controls bullet"><span class="by">queuebert</span><span>|</span><a href="#39727161">parent</a><span>|</span><a href="#39727718">next</a><span>|</span><label class="collapse" for="c-39730758">[-]</label><label class="expand" for="c-39730758">[2 more]</label></div><br/><div class="children"><div class="content">As a person who finds CUDA extremely easy to write and integrate, what does Triton have to offer?</div><br/><div id="39731325" class="c"><input type="checkbox" id="c-39731325" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39727161">root</a><span>|</span><a href="#39730758">parent</a><span>|</span><a href="#39727718">next</a><span>|</span><label class="collapse" for="c-39731325">[-]</label><label class="expand" for="c-39731325">[1 more]</label></div><br/><div class="children"><div class="content">block level rather than thread level programming, automatic optimization across hyperparameters, makes it much easier to write fast kernels</div><br/></div></div></div></div><div id="39727718" class="c"><input type="checkbox" id="c-39727718" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39727161">parent</a><span>|</span><a href="#39730758">prev</a><span>|</span><a href="#39727626">next</a><span>|</span><label class="collapse" for="c-39727718">[-]</label><label class="expand" for="c-39727718">[2 more]</label></div><br/><div class="children"><div class="content">yeah even the official flashattention is moving many implementations from cutlass to triton except for the main mha backward&#x2F;forward pass</div><br/><div id="39730033" class="c"><input type="checkbox" id="c-39730033" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39727161">root</a><span>|</span><a href="#39727718">parent</a><span>|</span><a href="#39727626">next</a><span>|</span><label class="collapse" for="c-39730033">[-]</label><label class="expand" for="c-39730033">[1 more]</label></div><br/><div class="children"><div class="content">It was written with cutlass? No wonder Peter Kim found it valuable and worthwhile to de-obfuscate. Adopting a new programming language invented by OpenAI doesn&#x27;t sound like a much better alternative. I&#x27;d be shocked if either of them were able to build code for AMD GPUs, where it&#x27;s easy to adapt CUDA code, but not if it&#x27;s buried in tens of thousands of lines of frameworks. I like open source code to have clarity so I can optimize it for my own production environment myself. When people distribute code they&#x27;ve productionized for themselves, it squeezes out all the alpha and informational value. Just because something&#x27;s open source doesn&#x27;t mean it&#x27;s open source. I think people mostly do it to lick the cookie without giving much away.</div><br/></div></div></div></div><div id="39727626" class="c"><input type="checkbox" id="c-39727626" checked=""/><div class="controls bullet"><span class="by">ixaxaar</span><span>|</span><a href="#39727161">parent</a><span>|</span><a href="#39727718">prev</a><span>|</span><a href="#39728029">next</a><span>|</span><label class="collapse" for="c-39727626">[-]</label><label class="expand" for="c-39727626">[4 more]</label></div><br/><div class="children"><div class="content">You mean triton the inference server or triton the DSL for cuda?</div><br/><div id="39727715" class="c"><input type="checkbox" id="c-39727715" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39727161">root</a><span>|</span><a href="#39727626">parent</a><span>|</span><a href="#39727713">next</a><span>|</span><label class="collapse" for="c-39727715">[-]</label><label class="expand" for="c-39727715">[1 more]</label></div><br/><div class="children"><div class="content">The DSL: <a href="https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;triton" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;triton</a></div><br/></div></div><div id="39727713" class="c"><input type="checkbox" id="c-39727713" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39727161">root</a><span>|</span><a href="#39727626">parent</a><span>|</span><a href="#39727715">prev</a><span>|</span><a href="#39728148">next</a><span>|</span><label class="collapse" for="c-39727713">[-]</label><label class="expand" for="c-39727713">[1 more]</label></div><br/><div class="children"><div class="content">they mean the dsl (not just necessarily for cuda)</div><br/></div></div><div id="39728148" class="c"><input type="checkbox" id="c-39728148" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39727161">root</a><span>|</span><a href="#39727626">parent</a><span>|</span><a href="#39727713">prev</a><span>|</span><a href="#39728029">next</a><span>|</span><label class="collapse" for="c-39728148">[-]</label><label class="expand" for="c-39728148">[1 more]</label></div><br/><div class="children"><div class="content">triton the DSL.</div><br/></div></div></div></div><div id="39728029" class="c"><input type="checkbox" id="c-39728029" checked=""/><div class="controls bullet"><span class="by">fpgamlirfanboy</span><span>|</span><a href="#39727161">parent</a><span>|</span><a href="#39727626">prev</a><span>|</span><a href="#39727520">next</a><span>|</span><label class="collapse" for="c-39728029">[-]</label><label class="expand" for="c-39728029">[5 more]</label></div><br/><div class="children"><div class="content">&gt; allows a level of abstraction that doesn&#x27;t affect performance even a little bit<p>The second part of this sentence is true because the first part is false.</div><br/><div id="39728155" class="c"><input type="checkbox" id="c-39728155" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39727161">root</a><span>|</span><a href="#39728029">parent</a><span>|</span><a href="#39727520">next</a><span>|</span><label class="collapse" for="c-39728155">[-]</label><label class="expand" for="c-39728155">[4 more]</label></div><br/><div class="children"><div class="content">zero cost abstractions exist. doesn&#x27;t mean all abstractions are zero-cost. or being zero-cost somehow invalidates their abstractness&#x2F;genericness. but maybe we differ on the definition of abstractions.</div><br/><div id="39728281" class="c"><input type="checkbox" id="c-39728281" checked=""/><div class="controls bullet"><span class="by">fpgamlirfanboy</span><span>|</span><a href="#39727161">root</a><span>|</span><a href="#39728155">parent</a><span>|</span><a href="#39728633">next</a><span>|</span><label class="collapse" for="c-39728281">[-]</label><label class="expand" for="c-39728281">[1 more]</label></div><br/><div class="children"><div class="content">&gt; zero cost abstractions exist<p>So does perpetual motion :shrug: but my point is Triton is not an abstraction in the least. Source: 1) I spent 6 months investigating targeting other backends 2) Phil himself said he doesn&#x27;t care to support other backends <a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;triton&#x2F;pull&#x2F;1797#issuecomment-1730112311">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;triton&#x2F;pull&#x2F;1797#issuecomment-1730...</a></div><br/></div></div></div></div></div></div></div></div><div id="39727520" class="c"><input type="checkbox" id="c-39727520" checked=""/><div class="controls bullet"><span class="by">araes</span><span>|</span><a href="#39727161">prev</a><span>|</span><a href="#39732772">next</a><span>|</span><label class="collapse" for="c-39727520">[-]</label><label class="expand" for="c-39727520">[10 more]</label></div><br/><div class="children"><div class="content">For those who have no idea what&#x27;s being discussed, quick background.<p>Discussing: Transformer [1] memory issues and approximate attention [2] in machine learning training.<p>Specifically: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. [3]<p>As a side comment, this entire industry is sorely in need of at least intros.  The entire space has moved so fast in the last year I need an entire new dictionary and thesaurus for all the terms they&#x27;ve created.  Notably, because of this, found out Google has a glossary of machine learning terms.  Actually somewhat handy.<p>[1] Google Machine Learning Glossary (Transformer): <a href="https:&#x2F;&#x2F;developers.google.com&#x2F;machine-learning&#x2F;glossary&#x2F;#transformer" rel="nofollow">https:&#x2F;&#x2F;developers.google.com&#x2F;machine-learning&#x2F;glossary&#x2F;#tra...</a><p>[2] Same (Attention): <a href="https:&#x2F;&#x2F;developers.google.com&#x2F;machine-learning&#x2F;glossary&#x2F;#attention" rel="nofollow">https:&#x2F;&#x2F;developers.google.com&#x2F;machine-learning&#x2F;glossary&#x2F;#att...</a><p>[2] arXiv: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14135" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14135</a></div><br/><div id="39727776" class="c"><input type="checkbox" id="c-39727776" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39727520">parent</a><span>|</span><a href="#39728159">next</a><span>|</span><label class="collapse" for="c-39727776">[-]</label><label class="expand" for="c-39727776">[7 more]</label></div><br/><div class="children"><div class="content">Regarding your comment about how fast the research and industry is moving, would HN readers be interested in relevant one or two paragraph summaries that are basically &quot;explain it like I am a machine learning engineer from 2020&quot; but also knows the power of these models from a perspective of using ChatGPT or MS Copilot?  That is, assume a fair amount of technical knowledge about the fundamentals, but don&#x27;t assume that the reader is paying any attention to have whitebox knowledge of the current state of the art.</div><br/><div id="39727845" class="c"><input type="checkbox" id="c-39727845" checked=""/><div class="controls bullet"><span class="by">jprete</span><span>|</span><a href="#39727520">root</a><span>|</span><a href="#39727776">parent</a><span>|</span><a href="#39730930">next</a><span>|</span><label class="collapse" for="c-39727845">[-]</label><label class="expand" for="c-39727845">[2 more]</label></div><br/><div class="children"><div class="content">I personally have been looking for &quot;explain it like I&#x27;m a CS PhD with lots of experience and the ability to look stuff up&quot;. But I suspect your summary would be pretty handy as well.</div><br/><div id="39730195" class="c"><input type="checkbox" id="c-39730195" checked=""/><div class="controls bullet"><span class="by">jhanoncomm</span><span>|</span><a href="#39727520">root</a><span>|</span><a href="#39727845">parent</a><span>|</span><a href="#39730930">next</a><span>|</span><label class="collapse" for="c-39730195">[-]</label><label class="expand" for="c-39730195">[1 more]</label></div><br/><div class="children"><div class="content">I reckon you need tacit knowledge. Experience. Luckily in the order
of 100 hours not 10000.<p>Build a GPT using Python and Pytorch. For a good course: Andrej Karpathy is your keyword. At $1000 his course is great value. But actually it is free which is even better ;-)<p>It wont take you to flash attention but will ramp you to the point you could probably read papers about it. I almost got that far then life lifed me. But I was able to implement changes to the architecture of GPT and do some âhey mum I am doing SOTA (2021) machine learningâ.</div><br/></div></div></div></div><div id="39730930" class="c"><input type="checkbox" id="c-39730930" checked=""/><div class="controls bullet"><span class="by">andoando</span><span>|</span><a href="#39727520">root</a><span>|</span><a href="#39727776">parent</a><span>|</span><a href="#39727845">prev</a><span>|</span><a href="#39728244">next</a><span>|</span><label class="collapse" for="c-39730930">[-]</label><label class="expand" for="c-39730930">[1 more]</label></div><br/><div class="children"><div class="content">I would love an explanation for software enginners &#x2F; CS majors who aren&#x27;t familiar with ML.<p>Last I studied ML was 2016 and that was stuff like decision trees, k nearest neighbors...</div><br/></div></div><div id="39728244" class="c"><input type="checkbox" id="c-39728244" checked=""/><div class="controls bullet"><span class="by">araes</span><span>|</span><a href="#39727520">root</a><span>|</span><a href="#39727776">parent</a><span>|</span><a href="#39730930">prev</a><span>|</span><a href="#39728181">next</a><span>|</span><label class="collapse" for="c-39728244">[-]</label><label class="expand" for="c-39728244">[2 more]</label></div><br/><div class="children"><div class="content">That sounds at least somewhat helpful.  Honestly, a gradient for some of this stuff would be nice.  Explain to me like I&#x27;m: &quot;five&quot;, &quot;a high schooler&quot;, &quot;a college grad (not CS&#x2F;ML&#x2F;Eng)&quot;, &quot;a CS&#x2F;Eng not ML&quot;.<p>Although in a couple years, kids in restaurants will probably telling me how they&#x27;re leveling up attention on their neuro-pet.  The singularity is steep.</div><br/><div id="39728526" class="c"><input type="checkbox" id="c-39728526" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#39727520">root</a><span>|</span><a href="#39728244">parent</a><span>|</span><a href="#39728181">next</a><span>|</span><label class="collapse" for="c-39728526">[-]</label><label class="expand" for="c-39728526">[1 more]</label></div><br/><div class="children"><div class="content">singularity implies AI increases exponentially, not human intelligence. Kids will not talk about neural nets any time soon.</div><br/></div></div></div></div><div id="39728181" class="c"><input type="checkbox" id="c-39728181" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39727520">root</a><span>|</span><a href="#39727776">parent</a><span>|</span><a href="#39728244">prev</a><span>|</span><a href="#39728159">next</a><span>|</span><label class="collapse" for="c-39728181">[-]</label><label class="expand" for="c-39728181">[1 more]</label></div><br/><div class="children"><div class="content">frankly i donât really feel like all that much has changed since 2020 except for scale</div><br/></div></div></div></div><div id="39728159" class="c"><input type="checkbox" id="c-39728159" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#39727520">parent</a><span>|</span><a href="#39727776">prev</a><span>|</span><a href="#39727775">next</a><span>|</span><label class="collapse" for="c-39728159">[-]</label><label class="expand" for="c-39728159">[1 more]</label></div><br/><div class="children"><div class="content">Zero shot is wrong, but that definition is commonly used.<p>Zero shot is testing out if distribution, not just &quot;a task&quot; not trained on. The later is ill defined.<p>The original definition comes from a few papers. But the classic example is a clarifier recognizing zebras but having never been trained in zebras (but may have been trained on horses). There&#x27;s are out of distribution. But importantly, out of the implicit distribution, not the target distribution.<p>The common improper usage usually confuses these two. A simple example might me training in 256x256 images and testing on 1024x1024. That&#x27;s still in the implicit distribution (as long as the classes are identical). A very common example is training on a large dataset like LAION and then testing on coco or image net 1k. This is not zero shot because the classes in ImageNet are in LAION (and in Coco). Basically, this is a useless definition because then any validation or test set is zero shot because those were never seen in the training data and thus out of the training distribution. But remember that data sets are proxies for larger distributions.<p>Where is can get sometimes tricky is tasks (emergence has entered the chat). For example, you may not intend to train a generative model to do clarification but you probably did (it&#x27;s very clear -- in the math -- if you&#x27;re training density models (KLD, score, etc)). This can get hairy because it&#x27;s very easy to train a model to do things that you aren&#x27;t realizing you are and later find out. Some people can get upset about this but it&#x27;s the nature of frameworks that have low interpretability. There&#x27;s still a lot of mathematics we need to learn and it tends not to be an explicit focus in ML but there are plenty in the community focused on this.</div><br/></div></div></div></div><div id="39732772" class="c"><input type="checkbox" id="c-39732772" checked=""/><div class="controls bullet"><span class="by">dcanelhas</span><span>|</span><a href="#39727520">prev</a><span>|</span><a href="#39731535">next</a><span>|</span><label class="collapse" for="c-39732772">[-]</label><label class="expand" for="c-39732772">[1 more]</label></div><br/><div class="children"><div class="content">If CPU&#x2F;GPU execution speed is the goal while simultaneously code golfing the source size, <a href="https:&#x2F;&#x2F;halide-lang.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;halide-lang.org&#x2F;</a> might have come in handy.</div><br/></div></div><div id="39731535" class="c"><input type="checkbox" id="c-39731535" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39732772">prev</a><span>|</span><a href="#39727770">next</a><span>|</span><label class="collapse" for="c-39731535">[-]</label><label class="expand" for="c-39731535">[1 more]</label></div><br/><div class="children"><div class="content">Fantastic work! Extremely neat and clear implementation! Interesting note on the backward pass - what do you think are the main blockers for a backward pass?</div><br/></div></div><div id="39727770" class="c"><input type="checkbox" id="c-39727770" checked=""/><div class="controls bullet"><span class="by">saiojd</span><span>|</span><a href="#39731535">prev</a><span>|</span><a href="#39729922">next</a><span>|</span><label class="collapse" for="c-39727770">[-]</label><label class="expand" for="c-39727770">[6 more]</label></div><br/><div class="children"><div class="content">What does __syncthreads() do here exactly? I&#x27;m new to CUDA, could get the overall idea of the FlashAttention paper but not the details.</div><br/><div id="39727804" class="c"><input type="checkbox" id="c-39727804" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#39727770">parent</a><span>|</span><a href="#39729922">next</a><span>|</span><label class="collapse" for="c-39727804">[-]</label><label class="expand" for="c-39727804">[5 more]</label></div><br/><div class="children"><div class="content">Causes every thread in the block to wait until they have reached this point. Worth reading a cuda primer for more details on blocks&#x2F;warps.<p>Since the threads are relying on each other to fill the SRAM with all needed data if you didnât wait then values would be missing.</div><br/><div id="39729333" class="c"><input type="checkbox" id="c-39729333" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#39727770">root</a><span>|</span><a href="#39727804">parent</a><span>|</span><a href="#39729922">next</a><span>|</span><label class="collapse" for="c-39729333">[-]</label><label class="expand" for="c-39729333">[4 more]</label></div><br/><div class="children"><div class="content">Any CUDA primer you recommend in particular? I had this same question.</div><br/><div id="39729842" class="c"><input type="checkbox" id="c-39729842" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#39727770">root</a><span>|</span><a href="#39729333">parent</a><span>|</span><a href="#39730231">next</a><span>|</span><label class="collapse" for="c-39729842">[-]</label><label class="expand" for="c-39729842">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s an article on syncing in CUDA via cooperative groups: <a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;cooperative-groups&#x2F;" rel="nofollow">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;cooperative-groups&#x2F;</a><p>There&#x27;s also explicit warp synchronization, i.e. __syncwarp().
More on warp primitives here: <a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;using-cuda-warp-level-primitives&#x2F;" rel="nofollow">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;using-cuda-warp-level-prim...</a></div><br/></div></div><div id="39730231" class="c"><input type="checkbox" id="c-39730231" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#39727770">root</a><span>|</span><a href="#39729333">parent</a><span>|</span><a href="#39729842">prev</a><span>|</span><a href="#39729922">next</a><span>|</span><label class="collapse" for="c-39730231">[-]</label><label class="expand" for="c-39730231">[2 more]</label></div><br/><div class="children"><div class="content">Probably <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=nOxKexn3iBo" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=nOxKexn3iBo</a> (or just skimming the attached colab).</div><br/><div id="39730435" class="c"><input type="checkbox" id="c-39730435" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#39727770">root</a><span>|</span><a href="#39730231">parent</a><span>|</span><a href="#39729922">next</a><span>|</span><label class="collapse" for="c-39730435">[-]</label><label class="expand" for="c-39730435">[1 more]</label></div><br/><div class="children"><div class="content">This is terrific, thanks!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39729922" class="c"><input type="checkbox" id="c-39729922" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#39727770">prev</a><span>|</span><a href="#39727873">next</a><span>|</span><label class="collapse" for="c-39729922">[-]</label><label class="expand" for="c-39729922">[2 more]</label></div><br/><div class="children"><div class="content">My GPU work is not in ML (deep or otherwise); but ...<p>1. &quot;100 lines of CUDA&quot; + PyTorch; maybe this is useful and maybe it isn&#x27;t, but counting lines of code on top of a huge codebase is not very meaningful.<p>2. Launching separate kernels, synchronously, on the default stream, for various operations, is typically not the right way to utilize a GPU.</div><br/><div id="39731163" class="c"><input type="checkbox" id="c-39731163" checked=""/><div class="controls bullet"><span class="by">chillee</span><span>|</span><a href="#39729922">parent</a><span>|</span><a href="#39727873">next</a><span>|</span><label class="collapse" for="c-39731163">[-]</label><label class="expand" for="c-39731163">[1 more]</label></div><br/><div class="children"><div class="content">&gt; maybe this is useful and maybe it isn&#x27;t, but counting lines of code on top of a huge codebase is not very meaningful.<p>In this case it&#x27;s pretty reasonable imo, since the kernel itself is fairly independent - the usage of torch is just for some bindings for the data structures.<p>&gt; Launching separate kernels, synchronously, on the default stream, for various operations, is typically not the right way to utilize a GPU.<p>This is actually the standard way to do things in ML. Assuming you&#x27;re from a HPC background (where this may seem quite strange), the biggest change is that &quot;More or less everything in ML runs on the GPU&quot;, so there is very rarely any device to host synchronizations. In addition, each individual kernel is typically run on fairly large chunks of data (a million elements would be on the smaller side), so maximizing occupancy with streams is not as necessary as in HPC.</div><br/></div></div></div></div><div id="39728210" class="c"><input type="checkbox" id="c-39728210" checked=""/><div class="controls bullet"><span class="by">zer0zzz</span><span>|</span><a href="#39727873">prev</a><span>|</span><label class="collapse" for="c-39728210">[-]</label><label class="expand" for="c-39728210">[3 more]</label></div><br/><div class="children"><div class="content">This is fantastic. I am just starting in the ML space (compile from compilers) and I love short kernels that I can use to understand things better with.</div><br/><div id="39728245" class="c"><input type="checkbox" id="c-39728245" checked=""/><div class="controls bullet"><span class="by">lagrange77</span><span>|</span><a href="#39728210">parent</a><span>|</span><label class="collapse" for="c-39728245">[-]</label><label class="expand" for="c-39728245">[2 more]</label></div><br/><div class="children"><div class="content">&gt; compile from compilers<p>What does that mean?</div><br/><div id="39729901" class="c"><input type="checkbox" id="c-39729901" checked=""/><div class="controls bullet"><span class="by">zer0zzz</span><span>|</span><a href="#39728210">root</a><span>|</span><a href="#39728245">parent</a><span>|</span><label class="collapse" for="c-39729901">[-]</label><label class="expand" for="c-39729901">[1 more]</label></div><br/><div class="children"><div class="content">Typo, meant to write âcoming from compilersâ</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>