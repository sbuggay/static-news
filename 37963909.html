<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697878857372" as="style"/><link rel="stylesheet" href="styles.css?v=1697878857372"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://restofworld.org/2023/ai-image-stereotypes/">AI Reduces the World to Stereotypes</a> <span class="domain">(<a href="https://restofworld.org">restofworld.org</a>)</span></div><div class="subtext"><span>gigama</span> | <span>47 comments</span></div><br/><div><div id="37965222" class="c"><input type="checkbox" id="c-37965222" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#37964570">next</a><span>|</span><label class="collapse" for="c-37965222">[-]</label><label class="expand" for="c-37965222">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div><div id="37964570" class="c"><input type="checkbox" id="c-37964570" checked=""/><div class="controls bullet"><span class="by">Tistron</span><span>|</span><a href="#37965222">prev</a><span>|</span><a href="#37964664">next</a><span>|</span><label class="collapse" for="c-37964570">[-]</label><label class="expand" for="c-37964570">[9 more]</label></div><br/><div class="children"><div class="content">It seems to me that they are asking for stereotypes and getting stereotypes.<p>If you&#x27;d ask me to paint an Indian person, of course I&#x27;d paint a stereotype to make sure it looks Indian, and not some normal person from India that could be from anywhere.<p>Or like imagine playing one of those games where you&#x27;re supposed to guess the prompt of what your friend is drawing. This is sort of like that, isn&#x27;t it? The AI is creating an image that would have you look at it and think &quot;an Indian person&quot;, not just &quot;a person&quot;.</div><br/><div id="37964828" class="c"><input type="checkbox" id="c-37964828" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#37964570">parent</a><span>|</span><a href="#37964592">next</a><span>|</span><label class="collapse" for="c-37964828">[-]</label><label class="expand" for="c-37964828">[2 more]</label></div><br/><div class="children"><div class="content">I think this is an interface problem. Given a generic prompt, the AI draws a generic image, like a beginner would. Things you don&#x27;t specify explicitly default to generic options.<p>A more sensible response would perhaps invent additional requirements to get more interesting and more varied outcomes. The right amount of variance depends on the context, but it&#x27;s rarely as low as the current interfaces default to.</div><br/></div></div><div id="37964592" class="c"><input type="checkbox" id="c-37964592" checked=""/><div class="controls bullet"><span class="by">somedude895</span><span>|</span><a href="#37964570">parent</a><span>|</span><a href="#37964828">prev</a><span>|</span><a href="#37964664">next</a><span>|</span><label class="collapse" for="c-37964592">[-]</label><label class="expand" for="c-37964592">[6 more]</label></div><br/><div class="children"><div class="content">Exactly. I assume their preferred solution would be for the AI to refuse to depict cultures, ethnicities or genders, as generalising leads to stereotyping. Postmodernists should touch grass sometime, preferrably outside their bubble.</div><br/><div id="37964752" class="c"><input type="checkbox" id="c-37964752" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#37964570">root</a><span>|</span><a href="#37964592">parent</a><span>|</span><a href="#37964863">next</a><span>|</span><label class="collapse" for="c-37964752">[-]</label><label class="expand" for="c-37964752">[4 more]</label></div><br/><div class="children"><div class="content">Those &quot;postmodernists&quot; are truthphobes, using their own lingo.</div><br/><div id="37964877" class="c"><input type="checkbox" id="c-37964877" checked=""/><div class="controls bullet"><span class="by">csydas</span><span>|</span><a href="#37964570">root</a><span>|</span><a href="#37964752">parent</a><span>|</span><a href="#37964852">next</a><span>|</span><label class="collapse" for="c-37964877">[-]</label><label class="expand" for="c-37964877">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think it’s about fearing the truth I think it’s a right for fear that there is a new system which everyone treats as authoritative and very often the system is wrong. I think it’s worth the question. What are we going to do with this new system that produces fast accurate, looking answers when the answers that AI produces are very often wrong or flawed in someway or present inaccurate answers or misrepresent  certain facts or data. I think it’s reasonable to be suspicious of any supposedly authoritative source and to question how we’re using such tools, and what the effect of such tools might be.</div><br/></div></div><div id="37964852" class="c"><input type="checkbox" id="c-37964852" checked=""/><div class="controls bullet"><span class="by">karmakurtisaani</span><span>|</span><a href="#37964570">root</a><span>|</span><a href="#37964752">parent</a><span>|</span><a href="#37964877">prev</a><span>|</span><a href="#37965165">next</a><span>|</span><label class="collapse" for="c-37964852">[-]</label><label class="expand" for="c-37964852">[1 more]</label></div><br/><div class="children"><div class="content">Damn those vaguely defined generic postmodernists!</div><br/></div></div><div id="37965165" class="c"><input type="checkbox" id="c-37965165" checked=""/><div class="controls bullet"><span class="by">LesZedCB</span><span>|</span><a href="#37964570">root</a><span>|</span><a href="#37964752">parent</a><span>|</span><a href="#37964852">prev</a><span>|</span><a href="#37964863">next</a><span>|</span><label class="collapse" for="c-37965165">[-]</label><label class="expand" for="c-37965165">[1 more]</label></div><br/><div class="children"><div class="content">to not be at least a little skeptical of one&#x27;s epistemology is arrogant as hell</div><br/></div></div></div></div><div id="37964863" class="c"><input type="checkbox" id="c-37964863" checked=""/><div class="controls bullet"><span class="by">csydas</span><span>|</span><a href="#37964570">root</a><span>|</span><a href="#37964592">parent</a><span>|</span><a href="#37964752">prev</a><span>|</span><a href="#37964664">next</a><span>|</span><label class="collapse" for="c-37964863">[-]</label><label class="expand" for="c-37964863">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think that follows at all I think what they would prefer is that both AI developers and users of AI systems are aware that this is what is being fed to them and that without purposefully going to avoid stereotypes you’re going to get stereotypes.<p>I don’t think that’s the article was trying to say that AI is inherently racist or inherently is causing people to be racist, it’s that AI is still seeing as authoritative. If you just ask AI show me a picture of someone from a specific culture ask and it shows very stereotypical result, I wouldn’t call the AI result. Racist what I would say is the problem is that the person viewing this might except that this is an authoritative answer this is what progrsmmers and maths have shown is undoubtably, a true accurate representation of a person from such a culture, and the end-user is no more aware how this picture was formed, or whether or not the AI considers to be a stereotypical representation<p>I think the focus on the Barbies around the world from AI generation was a good example as it does. Kind of show some very strange interpretations of different cultures. Now granted we need to take this with a grain of salt because we don’t know the queries we don’t know the exact models and stuff like that but that’s not really the point the point is more that AI in person to use AI frequently treat the AI output as authoritative. There is no indication if the AI maybe it wasn’t able to get a good idea of what your request was if they maybe had a lack of confidence in what they are responding, you just get a results and it seems with all the white papers and with the buzz around AI that it’s an authoritative result<p>I am not a stranger to using AI to assist with tasks, something like a quickly converting from one syntax to another, is something I do on a fairly regular basis the difference, however, between using AI like that, and using it as an authoritative source is that I would check the queries or the code that’s produced by the AI. I would not accept it blindly. I will double check and make sure since at some point I need to run this code in real environments. I think that is what the article was trying to say is that AI is very cool and I can do some very cool things but if you’re not understanding how it’s doing what it’s doing and not checking it output and trusting the AI blindly that is where it has a problem. And I would agree with the article if this is what I’m trying to say,<p>I don’t think it’s so much about moderating speech or anything like that, not that it must not produce certain outputs; It’s more long lines of there is simply too much implicit trust in how AI works in the output that AI produces. And I would agree with the article if this is what I’m trying to say, I don’t think it’s so much about curtailing speech or anything like that or that so must must not produce certain outputs. It’s more along the lines of people’s Trust in how AI works and in the output that it gives and s stereotypes users are probably giving it bad inputs to produce these outputs, which are quite stereotypical and very often incorrect.<p>I think, showing more about how the AI understood the prompt and how the output was produced, such as sources, or may be certain key words and examples of images, that the AI associate with his key words would help the users to understand why they are getting this out put and also it would help them understand may be watch the words they use in their props are associated with, and maybe understand a bit more about why their innocent understanding of their problems or questions may be is not as innocent as they thought originally. It doesn’t have to lecture them. It simply needs a show “X is associated with Y in this model and that’s produced Z” and let the users draw their own conclusions.</div><br/></div></div></div></div></div></div><div id="37964664" class="c"><input type="checkbox" id="c-37964664" checked=""/><div class="controls bullet"><span class="by">somedude895</span><span>|</span><a href="#37964570">prev</a><span>|</span><a href="#37964681">next</a><span>|</span><label class="collapse" for="c-37964664">[-]</label><label class="expand" for="c-37964664">[4 more]</label></div><br/><div class="children"><div class="content">&gt; “From a visual perspective, there are many, many versions of Nigeria,” Atewologun said.<p>&gt; But you wouldn’t know this from a simple search for “a Nigerian person” on Midjourney. Instead, all of the results are strikingly similar. While many images depict clothing that appears to indicate some form of traditional Nigerian attire, Atewologun said they lacked specificity. “It’s all some sort of generalized&quot;<p>It&#x27;s generalized because that&#x27;s what you asked for. If it was the other way around and a prompt for &quot;Nigerian person&quot; would return an image of a person from one specific group then these people would complain that &quot;not every Nigerian is Igbo. The other groups are being marginalized by AI.&quot;<p>At least they do explain why that is, and I found it interesting that the prompt for &quot;American person&quot; returned mainly women, so the article wasn&#x27;t a complete waste of time.<p>I also raised an eyebrow at the fact that they refer to prompting as &quot;searching&quot; throughout the article.</div><br/><div id="37964732" class="c"><input type="checkbox" id="c-37964732" checked=""/><div class="controls bullet"><span class="by">crystaln</span><span>|</span><a href="#37964664">parent</a><span>|</span><a href="#37964681">next</a><span>|</span><label class="collapse" for="c-37964732">[-]</label><label class="expand" for="c-37964732">[3 more]</label></div><br/><div class="children"><div class="content">The algorithm could return a random Nigerian ethnic group proportional to their actual population.<p>To be fair that’s probably challenging however perhaps the direction the models should go.<p>It would be great if the algorithm returned diversity by default.</div><br/><div id="37964834" class="c"><input type="checkbox" id="c-37964834" checked=""/><div class="controls bullet"><span class="by">somedude895</span><span>|</span><a href="#37964664">root</a><span>|</span><a href="#37964732">parent</a><span>|</span><a href="#37965138">next</a><span>|</span><label class="collapse" for="c-37964834">[-]</label><label class="expand" for="c-37964834">[1 more]</label></div><br/><div class="children"><div class="content">But then it wouldn&#x27;t be a generalized Nigerian person. That doesn&#x27;t exist, so you get an approximation.<p>Say I make a video and for some reason use genAI to depict nationalities. It has to be a single person so I can&#x27;t have it generate a group photo of all of Nigeria&#x27;s 300 ethnic groups, so how do you display diversity in a picture of a single person? With your proposal by chance I get a person from a small minority group, then that is much less representative of Nigeria, and less inclusive than if it just gave me the stereotype. It might even out over time, but most images that are generated will never see the light of day. Now if my video happens to be the one to blow up on the internet for some reason then the rest of the country probably won&#x27;t be happy that that specific group was used to represent them as a whole.<p>In that sense using the stereotype is the fairest way since everyone is equally misrepresented.<p>It&#x27;s not even fundamentally an AI issue. If I instructed someone on Fiverr to simply &quot;draw me a Nigerian person, no discussions&quot; instead, the result would be the same. It&#x27;s on the person writing the prompt to decide whether and how to display diversity in whatever they&#x27;re using the output for.</div><br/></div></div><div id="37965138" class="c"><input type="checkbox" id="c-37965138" checked=""/><div class="controls bullet"><span class="by">gostsamo</span><span>|</span><a href="#37964664">root</a><span>|</span><a href="#37964732">parent</a><span>|</span><a href="#37964834">prev</a><span>|</span><a href="#37964681">next</a><span>|</span><label class="collapse" for="c-37965138">[-]</label><label class="expand" for="c-37965138">[1 more]</label></div><br/><div class="children"><div class="content">maybe a solution would be if the ai can provide context. e.g. here is an image of a nigerian person with certain characteristics, but the possible result space is bigger and here you can try with nearby prompts which will return different results.</div><br/></div></div></div></div></div></div><div id="37964681" class="c"><input type="checkbox" id="c-37964681" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#37964664">prev</a><span>|</span><a href="#37965200">next</a><span>|</span><label class="collapse" for="c-37964681">[-]</label><label class="expand" for="c-37964681">[1 more]</label></div><br/><div class="children"><div class="content">Stereotyping or abstracting is how we can generalise and reason about the world in absence of further specifics or details. Generalisation in itself is not a problem at all. We need it to be able to function in absence of 100% complete knowledge.<p>It potentionally becomes a problem when we use generalisation without recognizing further information. additional detail and variation.<p>Problematic stereotyping is ignoring or refusing all information about a specific instance presented, and <i>persisting</i> in treating the instance solely based on the prototype of the category according to your ontology.<p>Many of the examples of stereotyping in the article demonstrated the former. Few are examples of the latter.<p>Every model holds &#x27;biases&#x27;. These correlate prompts with outputs. Without bias, the output would be a complete random sample of the target domain based on the training images regardless of their labels or descriptions. A picture of a duckling drinking water would be just as likely to be produced from the prompt &#x27;a sunset over Jupiter&#x27; or &#x27;a sportscar on a German autobahn&#x27; than from &#x27;a baby duck drinking&#x27;.<p>Most models let you play with parameters that losen the correlation. Look onto e.g. &#x27;temperature&#x27; or &#x27;prompt strength&#x27; parameters.<p>Now we can of course argue about wether a particular model is biased in our preference. Should Midjourney more often depict a picture of a typical blond Caucasian woman when prompted for &#x27;a Mexican&#x27;? This is not impossible. Some &#x27;anime&#x27; specific models will produce a Japanese looking young female for that prompt because that is all they can produce.<p>Some people argue that some models, &#x27;general&#x27; models, should be more alligned with their specific ideological ontology. More often than not, the loudest voices in that space hold very particular viewpoints that more often than not advocate very rigid categorical reasoning, precisely committing harmfull stereotyping in the latter sense above, refusing to take into consideration instance features over categorical generalizations extrapolated from a very narrow dogmatic and local context.<p>Most certainly a debate should be had. Is there enough model diversity, or is the space overly dominated by certain viewpoints?  Should the &#x27;market&#x27; (most often in this space this is driven by producer influence, not consumer choice) decide, or is some regulation required? ( but &#x27;Quis custodiet ipsos custodes?&#x27;)<p>Probably decent concerns on al sides, but no good answers?</div><br/></div></div><div id="37965200" class="c"><input type="checkbox" id="c-37965200" checked=""/><div class="controls bullet"><span class="by">hiAndrewQuinn</span><span>|</span><a href="#37964681">prev</a><span>|</span><a href="#37964789">next</a><span>|</span><label class="collapse" for="c-37965200">[-]</label><label class="expand" for="c-37965200">[1 more]</label></div><br/><div class="children"><div class="content">Stereotype accuracy is one of the largest and most replicable effects in all of social psychology.<p><a href="https:&#x2F;&#x2F;psycnet.apa.org&#x2F;record&#x2F;2015-19097-002" rel="nofollow noreferrer">https:&#x2F;&#x2F;psycnet.apa.org&#x2F;record&#x2F;2015-19097-002</a></div><br/></div></div><div id="37964789" class="c"><input type="checkbox" id="c-37964789" checked=""/><div class="controls bullet"><span class="by">probably_wrong</span><span>|</span><a href="#37965200">prev</a><span>|</span><a href="#37965175">next</a><span>|</span><label class="collapse" for="c-37964789">[-]</label><label class="expand" for="c-37964789">[4 more]</label></div><br/><div class="children"><div class="content">The interesting part to me is that they are getting <i>stereotypes</i> instead of the <i>average</i>.<p>I have never in my life seen an Indian person with a beard and turban, nor I&#x27;ve ever met a Mexican person wearing a sombrero and poncho. And given how boring the results of generic prompts tend to be, my theory is that they specifically tweaked their training data to avoid getting &quot;generic Indian worker wearing a shirt&quot; in favor of &quot;stereotypical Indian man that would make a good NatGeo cover&quot;.</div><br/><div id="37964903" class="c"><input type="checkbox" id="c-37964903" checked=""/><div class="controls bullet"><span class="by">spondylosaurus</span><span>|</span><a href="#37964789">parent</a><span>|</span><a href="#37964947">next</a><span>|</span><label class="collapse" for="c-37964903">[-]</label><label class="expand" for="c-37964903">[1 more]</label></div><br/><div class="children"><div class="content">Right, part of me would expect a generated person of &lt;x&gt; ethnicity to look something like those images where they superimpose a bunch of faces to find the &quot;facial average&quot; of different countries: <a href="https:&#x2F;&#x2F;www.artfido.com&#x2F;this-is-what-the-average-person-looks-like-in-each-country&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.artfido.com&#x2F;this-is-what-the-average-person-look...</a><p>I think it&#x27;s probably a matter of the training data itself using stereotypical images, though. The first page of Google Image results for &quot;mexican man&quot; is almost entirely guys in hats, most of those sombreros. And those images are obviously getting tagged as &quot;mexican man&quot; in training data, but if you have an image of (for example) the frontman of a death metal band from Mexico, I&#x27;d assume <i>that</i> image wont get any tags about the band members&#x27; ethnicity because it&#x27;s not obvious from the image context, nor is it the most striking thing about the image itself.<p>Hell, you could even have two different images of the same person: one where they&#x27;re wearing a poncho and sombrero, one where they&#x27;re wearing ripped jeans and skull face paint. I&#x27;m sure they&#x27;d be assigned wildly different tags.</div><br/></div></div><div id="37964947" class="c"><input type="checkbox" id="c-37964947" checked=""/><div class="controls bullet"><span class="by">somedude895</span><span>|</span><a href="#37964789">parent</a><span>|</span><a href="#37964903">prev</a><span>|</span><a href="#37965175">next</a><span>|</span><label class="collapse" for="c-37964947">[-]</label><label class="expand" for="c-37964947">[2 more]</label></div><br/><div class="children"><div class="content">&gt; they are getting stereotypes instead of the average.<p>That sort of makes sense though. The training data is labeled images, and a picture of an average Indian in say an Indian newspaper or someone posting their own picture on their blog, won&#x27;t be labeled &quot;Indian&quot;, since within that context the nationality either doesn&#x27;t matter or is a given. The training data would have to include the context like &quot;if source url tld = .in&quot; then add &quot;India&quot; to label. But that adds a whole host of other issues.<p>Someone correct me if I&#x27;m wrong.</div><br/><div id="37965084" class="c"><input type="checkbox" id="c-37965084" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37964789">root</a><span>|</span><a href="#37964947">parent</a><span>|</span><a href="#37965175">next</a><span>|</span><label class="collapse" for="c-37965084">[-]</label><label class="expand" for="c-37965084">[1 more]</label></div><br/><div class="children"><div class="content">The image model knows what images look like even without prompts, and if you train it on a trillion images it will create a latent space where similar pictures have similar embeddings. Inaccurate captions for some of them may mean that the text encoder can&#x27;t get you to those embeddings, but they&#x27;re still in there.<p>What this means is that text prompting is a bad way to drive an image generating model.</div><br/></div></div></div></div></div></div><div id="37965175" class="c"><input type="checkbox" id="c-37965175" checked=""/><div class="controls bullet"><span class="by">a0-prw</span><span>|</span><a href="#37964789">prev</a><span>|</span><a href="#37964538">next</a><span>|</span><label class="collapse" for="c-37965175">[-]</label><label class="expand" for="c-37965175">[1 more]</label></div><br/><div class="children"><div class="content">&quot;The depictions were clearly flawed: Several of the Asian Barbies were light-skinned; Thailand Barbie, Singapore Barbie, and the Philippines Barbie all had blonde hair.&quot;<p>Who is making assumptions here? My Asian gf has lighter skin than me (Northern European). Also, it is not uncommon for Asians to dye their hair.</div><br/></div></div><div id="37964538" class="c"><input type="checkbox" id="c-37964538" checked=""/><div class="controls bullet"><span class="by">prvc</span><span>|</span><a href="#37965175">prev</a><span>|</span><a href="#37964554">next</a><span>|</span><label class="collapse" for="c-37964538">[-]</label><label class="expand" for="c-37964538">[1 more]</label></div><br/><div class="children"><div class="content">The diversity-industrial-complex reduces the world to bias and oppression.</div><br/></div></div><div id="37964554" class="c"><input type="checkbox" id="c-37964554" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#37964538">prev</a><span>|</span><a href="#37965136">next</a><span>|</span><label class="collapse" for="c-37964554">[-]</label><label class="expand" for="c-37964554">[1 more]</label></div><br/><div class="children"><div class="content">Wait so you are telling me the magic pattern matching algorithm works by finding patterns?? Who&#x27;d a thunk.</div><br/></div></div><div id="37965136" class="c"><input type="checkbox" id="c-37965136" checked=""/><div class="controls bullet"><span class="by">gnarlouse</span><span>|</span><a href="#37964554">prev</a><span>|</span><a href="#37964511">next</a><span>|</span><label class="collapse" for="c-37965136">[-]</label><label class="expand" for="c-37965136">[1 more]</label></div><br/><div class="children"><div class="content">A better headline would be “AI is reducing the world to stereotypes” because the current phrasing assumes this to be some kind of invariant.</div><br/></div></div><div id="37964511" class="c"><input type="checkbox" id="c-37964511" checked=""/><div class="controls bullet"><span class="by">tetsuhamu</span><span>|</span><a href="#37965136">prev</a><span>|</span><a href="#37964428">next</a><span>|</span><label class="collapse" for="c-37964511">[-]</label><label class="expand" for="c-37964511">[1 more]</label></div><br/><div class="children"><div class="content">These things are made of neural networks. Literally everything about them is weights and biases.<p>I agree and all, but it&#x27;s weird to claim these models have general bias without testing them on a variety of inputs. These models have a lot of minute details. They&#x27;re capable of differentiating a lot of specific things. They don&#x27;t lack information about Indian women.</div><br/></div></div><div id="37964428" class="c"><input type="checkbox" id="c-37964428" checked=""/><div class="controls bullet"><span class="by">tmikaeld</span><span>|</span><a href="#37964511">prev</a><span>|</span><a href="#37965038">next</a><span>|</span><label class="collapse" for="c-37964428">[-]</label><label class="expand" for="c-37964428">[1 more]</label></div><br/><div class="children"><div class="content">The output is based on the input<p>Seems like a water is wet kind of issue</div><br/></div></div><div id="37965038" class="c"><input type="checkbox" id="c-37965038" checked=""/><div class="controls bullet"><span class="by">j7ake</span><span>|</span><a href="#37964428">prev</a><span>|</span><a href="#37964454">next</a><span>|</span><label class="collapse" for="c-37965038">[-]</label><label class="expand" for="c-37965038">[2 more]</label></div><br/><div class="children"><div class="content">Since AI is trained by human labelled &#x2F;generated data, what the article is implying is that humans reduce the world to stereotypes.</div><br/><div id="37965075" class="c"><input type="checkbox" id="c-37965075" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37965038">parent</a><span>|</span><a href="#37964454">next</a><span>|</span><label class="collapse" for="c-37965075">[-]</label><label class="expand" for="c-37965075">[1 more]</label></div><br/><div class="children"><div class="content">AI image models are almost entirely not trained on human labeled data; StableDiffusion is trained on scraping nearby text on the page, DALLE3 uses synthetic captions from an image-to-text model, Midjourney doesn&#x27;t disclose what they do. You can&#x27;t get humans to label a billion images.<p>One way you can tell this isn&#x27;t true is that if you take an image model and prompt it with an image, or just surf through the latent space by changing the embeddings, you&#x27;ll find absolutely everything in there, from non-stereotypical representations to undescribable things.</div><br/></div></div></div></div><div id="37964454" class="c"><input type="checkbox" id="c-37964454" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#37965038">prev</a><span>|</span><a href="#37965100">next</a><span>|</span><label class="collapse" for="c-37964454">[-]</label><label class="expand" for="c-37964454">[2 more]</label></div><br/><div class="children"><div class="content">Well, of course. Large language models reflect the average of the input data.</div><br/><div id="37965066" class="c"><input type="checkbox" id="c-37965066" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37964454">parent</a><span>|</span><a href="#37965100">next</a><span>|</span><label class="collapse" for="c-37965066">[-]</label><label class="expand" for="c-37965066">[1 more]</label></div><br/><div class="children"><div class="content">An average of the input data would be a scalar value, not a few GB model.<p>Don&#x27;t go around making claims about how a instruct-tuned text model works if you&#x27;ve never tried to operate a foundation model. It&#x27;s very obvious those two aren&#x27;t doing the same kind of thing; they can&#x27;t both be &quot;average text&quot;.<p>And of course don&#x27;t confuse the sampler for the model; you can change those out.</div><br/></div></div></div></div><div id="37965100" class="c"><input type="checkbox" id="c-37965100" checked=""/><div class="controls bullet"><span class="by">falsandtru</span><span>|</span><a href="#37964454">prev</a><span>|</span><a href="#37965089">next</a><span>|</span><label class="collapse" for="c-37965100">[-]</label><label class="expand" for="c-37965100">[1 more]</label></div><br/><div class="children"><div class="content">And if AI can&#x27;t find a stereotype, AI will speculate.</div><br/></div></div><div id="37964645" class="c"><input type="checkbox" id="c-37964645" checked=""/><div class="controls bullet"><span class="by">mnky9800n</span><span>|</span><a href="#37964506">prev</a><span>|</span><a href="#37964679">next</a><span>|</span><label class="collapse" for="c-37964645">[-]</label><label class="expand" for="c-37964645">[1 more]</label></div><br/><div class="children"><div class="content">Usually a statistical models job is to take pile of data and try and figure out the structure within it that makes it similar. It shouldn&#x27;t come as a surprise when they do this.</div><br/></div></div><div id="37964679" class="c"><input type="checkbox" id="c-37964679" checked=""/><div class="controls bullet"><span class="by">LudwigNagasena</span><span>|</span><a href="#37964645">prev</a><span>|</span><a href="#37964841">next</a><span>|</span><label class="collapse" for="c-37964679">[-]</label><label class="expand" for="c-37964679">[1 more]</label></div><br/><div class="children"><div class="content">Oh no, that&#x27;s what journalists and marketers were supposed to do. AI is taking their jobs.</div><br/></div></div><div id="37964841" class="c"><input type="checkbox" id="c-37964841" checked=""/><div class="controls bullet"><span class="by">archerx</span><span>|</span><a href="#37964679">prev</a><span>|</span><a href="#37964471">next</a><span>|</span><label class="collapse" for="c-37964841">[-]</label><label class="expand" for="c-37964841">[1 more]</label></div><br/><div class="children"><div class="content">I left the page once my scroll got hijacked. I wish people would stop doing this, it doesn’t look good and it’s annoying.</div><br/></div></div><div id="37964471" class="c"><input type="checkbox" id="c-37964471" checked=""/><div class="controls bullet"><span class="by">Our_Benefactors</span><span>|</span><a href="#37964841">prev</a><span>|</span><a href="#37964909">next</a><span>|</span><label class="collapse" for="c-37964471">[-]</label><label class="expand" for="c-37964471">[3 more]</label></div><br/><div class="children"><div class="content">Dumb article full of dumb quotes from dumb people with politically correct job titles. If I was at midjourney I wouldn’t want to talk to them either because they are the worst type of agenda-driven journalist.  Wow, minimal prompts have minimal amounts of variation between seeds! Now report on something interesting, like how prompting gore&#x2F;violence&#x2F;death tokens outputs fluffy pictures of cats and fields of flowers due to training methodology, and how this makes models perform worse even for “non censored” content.<p>Scammers, grifters, and charlatans, the lot of them who have never written a line of code and still want a piece of the pie to themselves. Fuck every “AI ethics think tank”, “AI policy expert”, and so on who wants to limit and remove people’s freedom of access to this technology.</div><br/><div id="37964528" class="c"><input type="checkbox" id="c-37964528" checked=""/><div class="controls bullet"><span class="by">aydyn</span><span>|</span><a href="#37964471">parent</a><span>|</span><a href="#37964909">next</a><span>|</span><label class="collapse" for="c-37964528">[-]</label><label class="expand" for="c-37964528">[2 more]</label></div><br/><div class="children"><div class="content">I agree with you, but these people are becoming more and more irrelevant by the minute.<p>Nobody is not using chatGPT or stable diffusion because it could be biased. The ship has already sailed and any complaints about bias (or copyright for that matter) are standing on the shore left behind.</div><br/><div id="37964804" class="c"><input type="checkbox" id="c-37964804" checked=""/><div class="controls bullet"><span class="by">SenAnder</span><span>|</span><a href="#37964471">root</a><span>|</span><a href="#37964528">parent</a><span>|</span><a href="#37964909">next</a><span>|</span><label class="collapse" for="c-37964804">[-]</label><label class="expand" for="c-37964804">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Nobody is not using chatGPT or stable diffusion because it could be biased.<p>But their makers did expend a lot of effort into lobotomizing, sorry, censoring, sorry, making the models &quot;safe&quot;, due to the smears, sorry, &quot;reporting&quot; by these journalists.</div><br/></div></div></div></div></div></div><div id="37964909" class="c"><input type="checkbox" id="c-37964909" checked=""/><div class="controls bullet"><span class="by">Mistletoe</span><span>|</span><a href="#37964471">prev</a><span>|</span><a href="#37964981">next</a><span>|</span><label class="collapse" for="c-37964909">[-]</label><label class="expand" for="c-37964909">[2 more]</label></div><br/><div class="children"><div class="content">Now think about how AI has been writing a lot of the articles we read and shaping how social media algorithms work and you’ll understand how the world is getting so polarized and weird.  I’m so sick of stereotypes.  It’s the laziest approach to anything and the world is so much more varied than that.</div><br/><div id="37965074" class="c"><input type="checkbox" id="c-37965074" checked=""/><div class="controls bullet"><span class="by">LudwigNagasena</span><span>|</span><a href="#37964909">parent</a><span>|</span><a href="#37964981">next</a><span>|</span><label class="collapse" for="c-37965074">[-]</label><label class="expand" for="c-37965074">[1 more]</label></div><br/><div class="children"><div class="content">Now think about how people have been writing a lot of the articles we read and shaping how social media algorithms work and you’ll understand how the world is getting so polarized and weird.</div><br/></div></div></div></div><div id="37964981" class="c"><input type="checkbox" id="c-37964981" checked=""/><div class="controls bullet"><span class="by">bulbosaur123</span><span>|</span><a href="#37964909">prev</a><span>|</span><a href="#37964790">next</a><span>|</span><label class="collapse" for="c-37964981">[-]</label><label class="expand" for="c-37964981">[3 more]</label></div><br/><div class="children"><div class="content">Maybe stereotypes are there for a reason, eh?</div><br/><div id="37965082" class="c"><input type="checkbox" id="c-37965082" checked=""/><div class="controls bullet"><span class="by">growingkittens</span><span>|</span><a href="#37964981">parent</a><span>|</span><a href="#37965031">next</a><span>|</span><label class="collapse" for="c-37965082">[-]</label><label class="expand" for="c-37965082">[1 more]</label></div><br/><div class="children"><div class="content">A white man and a black man walk into a fried chicken establishment. One of these men is here to get dinner for his family, and the other man is the butt end of this joke.</div><br/></div></div></div></div><div id="37964790" class="c"><input type="checkbox" id="c-37964790" checked=""/><div class="controls bullet"><span class="by">SenAnder</span><span>|</span><a href="#37964981">prev</a><span>|</span><label class="collapse" for="c-37964790">[-]</label><label class="expand" for="c-37964790">[1 more]</label></div><br/><div class="children"><div class="content">And we must stop stereotypes, whatever the cost: <a href="https:&#x2F;&#x2F;www.cbsnews.com&#x2F;sanfrancisco&#x2F;news&#x2F;bart-withholding-surveillance-videos-of-crime-to-avoid-stereotypes&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cbsnews.com&#x2F;sanfrancisco&#x2F;news&#x2F;bart-withholding-s...</a></div><br/></div></div></div></div></div></div></div></body></html>