<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720602064561" as="style"/><link rel="stylesheet" href="styles.css?v=1720602064561"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/facebookresearch/MobileLLM">MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>tosh</span> | <span>46 comments</span></div><br/><div><div id="40924956" class="c"><input type="checkbox" id="c-40924956" checked=""/><div class="controls bullet"><span class="by">pmontra</span><span>|</span><a href="#40916744">next</a><span>|</span><label class="collapse" for="c-40924956">[-]</label><label class="expand" for="c-40924956">[2 more]</label></div><br/><div class="children"><div class="content">Interesting research, but Meta do not have any device worth talking about (at least at scale,) unless they want to ship that as part of their apps.</div><br/><div id="40924990" class="c"><input type="checkbox" id="c-40924990" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40924956">parent</a><span>|</span><a href="#40916744">next</a><span>|</span><label class="collapse" for="c-40924990">[-]</label><label class="expand" for="c-40924990">[1 more]</label></div><br/><div class="children"><div class="content">They have Oculus.</div><br/></div></div></div></div><div id="40916744" class="c"><input type="checkbox" id="c-40916744" checked=""/><div class="controls bullet"><span class="by">mmastrac</span><span>|</span><a href="#40924956">prev</a><span>|</span><a href="#40916791">next</a><span>|</span><label class="collapse" for="c-40916744">[-]</label><label class="expand" for="c-40916744">[6 more]</label></div><br/><div class="children"><div class="content">&gt; MobileLLM-125M&#x2F;350M attains a remarkable 2.7%&#x2F;4.3% accuracy boost over preceding 125M&#x2F;350M SoTA models on zero-shot commonsense reasoning tasks<p>Small models, slightly improved, probably still not good enough for the same use as online models. Nothing wrong with incremental progress, however.<p>1.5B parameter model does seem to be a pretty decent step up, even beating larger models by a wide margin. I&#x27;m not sure why they didn&#x27;t go larger -- having a more efficient model that fits on hardware the size of the RPi could be a gamechanger (IIRC TinyLlama 7B does run, barely).</div><br/><div id="40917963" class="c"><input type="checkbox" id="c-40917963" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#40916744">parent</a><span>|</span><a href="#40921077">next</a><span>|</span><label class="collapse" for="c-40917963">[-]</label><label class="expand" for="c-40917963">[3 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Small models, slightly improved, probably still not good enough for the same use as online models. Nothing wrong with incremental progress, however.<p>An even smaller language model should still be useful as part of a speech-to-text system. These should benefit from using the language model to narrow down what word is spoken in the face of ambiguity or noise.</div><br/><div id="40923958" class="c"><input type="checkbox" id="c-40923958" checked=""/><div class="controls bullet"><span class="by">woodson</span><span>|</span><a href="#40916744">root</a><span>|</span><a href="#40917963">parent</a><span>|</span><a href="#40921077">next</a><span>|</span><label class="collapse" for="c-40923958">[-]</label><label class="expand" for="c-40923958">[2 more]</label></div><br/><div class="children"><div class="content">ASR systems already use language models during decoding, though mostly not large decoder-only LLMs. However, incorporating LLMs into ASR is currently at the center of a lot of research, e.g. using a speech encoder like wav2vec 2.0 or the whisper encoder with a Qformer etc. and a LoRA adapter on an LLM trained for ASR.</div><br/><div id="40924065" class="c"><input type="checkbox" id="c-40924065" checked=""/><div class="controls bullet"><span class="by">omarelb</span><span>|</span><a href="#40916744">root</a><span>|</span><a href="#40923958">parent</a><span>|</span><a href="#40921077">next</a><span>|</span><label class="collapse" for="c-40924065">[-]</label><label class="expand" for="c-40924065">[1 more]</label></div><br/><div class="children"><div class="content">Really interested in this! Do you know of some good reading in this area?</div><br/></div></div></div></div></div></div><div id="40921077" class="c"><input type="checkbox" id="c-40921077" checked=""/><div class="controls bullet"><span class="by">cjtrowbridge</span><span>|</span><a href="#40916744">parent</a><span>|</span><a href="#40917963">prev</a><span>|</span><a href="#40924000">next</a><span>|</span><label class="collapse" for="c-40921077">[-]</label><label class="expand" for="c-40921077">[1 more]</label></div><br/><div class="children"><div class="content">Llama-3-8b runs fine on raspberry pi</div><br/></div></div><div id="40924000" class="c"><input type="checkbox" id="c-40924000" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#40916744">parent</a><span>|</span><a href="#40921077">prev</a><span>|</span><a href="#40916791">next</a><span>|</span><label class="collapse" for="c-40924000">[-]</label><label class="expand" for="c-40924000">[1 more]</label></div><br/><div class="children"><div class="content">But imagine if these models were baked into your Instagram app and then used for ad targeting using your own compute. Then Facebook gets to look at tons of other data and for less cost (and much less litigation risk) to them.<p>In this application it’s unfair to compare tiny models to cloud models.  Moreover any incremental precision boosts to tiny models would be notable (and directly translate to revenue).</div><br/></div></div></div></div><div id="40916791" class="c"><input type="checkbox" id="c-40916791" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#40916744">prev</a><span>|</span><a href="#40917090">next</a><span>|</span><label class="collapse" for="c-40916791">[-]</label><label class="expand" for="c-40916791">[5 more]</label></div><br/><div class="children"><div class="content">What apps can one currently use to run them on say an iPhone? Only aware of the MLC one which has literally 3 old models only</div><br/><div id="40920305" class="c"><input type="checkbox" id="c-40920305" checked=""/><div class="controls bullet"><span class="by">pickettd</span><span>|</span><a href="#40916791">parent</a><span>|</span><a href="#40922380">next</a><span>|</span><label class="collapse" for="c-40920305">[-]</label><label class="expand" for="c-40920305">[1 more]</label></div><br/><div class="children"><div class="content">The Android apk for MLC is updated frequently with recent models built-in. And a Samsung S24+ can comfortably run 7-8B models at reasonable speeds (10ish tokens&#x2F;sec).<p><a href="https:&#x2F;&#x2F;llm.mlc.ai&#x2F;docs&#x2F;deploy&#x2F;android.html" rel="nofollow">https:&#x2F;&#x2F;llm.mlc.ai&#x2F;docs&#x2F;deploy&#x2F;android.html</a></div><br/></div></div><div id="40922380" class="c"><input type="checkbox" id="c-40922380" checked=""/><div class="controls bullet"><span class="by">woadwarrior01</span><span>|</span><a href="#40916791">parent</a><span>|</span><a href="#40920305">prev</a><span>|</span><a href="#40918062">next</a><span>|</span><label class="collapse" for="c-40922380">[-]</label><label class="expand" for="c-40922380">[1 more]</label></div><br/><div class="children"><div class="content">I have an (mlc-llm based) app on the App Store that supports over 2 dozen models, including some recent ones.</div><br/></div></div><div id="40918062" class="c"><input type="checkbox" id="c-40918062" checked=""/><div class="controls bullet"><span class="by">5cott0</span><span>|</span><a href="#40916791">parent</a><span>|</span><a href="#40922380">prev</a><span>|</span><a href="#40917090">next</a><span>|</span><label class="collapse" for="c-40918062">[-]</label><label class="expand" for="c-40918062">[2 more]</label></div><br/><div class="children"><div class="content">wat<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;mlc-ai" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;mlc-ai</a></div><br/><div id="40918819" class="c"><input type="checkbox" id="c-40918819" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#40916791">root</a><span>|</span><a href="#40918062">parent</a><span>|</span><a href="#40917090">next</a><span>|</span><label class="collapse" for="c-40918819">[-]</label><label class="expand" for="c-40918819">[1 more]</label></div><br/><div class="children"><div class="content">On my iphone there doesn’t seem to be an option to download more.<p>Vaguely recall there being a button initially but don’t see it anymore</div><br/></div></div></div></div></div></div><div id="40917090" class="c"><input type="checkbox" id="c-40917090" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#40916791">prev</a><span>|</span><a href="#40916968">next</a><span>|</span><label class="collapse" for="c-40917090">[-]</label><label class="expand" for="c-40917090">[3 more]</label></div><br/><div class="children"><div class="content">I wonder how much you can push the &quot;deeper and thinner&quot; part. At some point your entire FFN fits into your L2 cache, you&#x27;re bound to get some performance jumps.</div><br/><div id="40917171" class="c"><input type="checkbox" id="c-40917171" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40917090">parent</a><span>|</span><a href="#40923972">next</a><span>|</span><label class="collapse" for="c-40917171">[-]</label><label class="expand" for="c-40917171">[1 more]</label></div><br/><div class="children"><div class="content">Other research from Meta FAIR actually suggests that you should prune deeper layers if you want to improve performance while maintaining accuracy [1]. So there must be a cutoff point for smaller networks where this approach still works, otherwise the results are contradictory. Or we could drastically improve these new models even further.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2403.17887v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2403.17887v1</a></div><br/></div></div><div id="40923972" class="c"><input type="checkbox" id="c-40923972" checked=""/><div class="controls bullet"><span class="by">woodson</span><span>|</span><a href="#40917090">parent</a><span>|</span><a href="#40917171">prev</a><span>|</span><a href="#40916968">next</a><span>|</span><label class="collapse" for="c-40923972">[-]</label><label class="expand" for="c-40923972">[1 more]</label></div><br/><div class="children"><div class="content">That reminds me of the findings of Google’s paper on EfficientT5 (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2109.10686" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2109.10686</a>). They refer to it as “DeepNarrow”.</div><br/></div></div></div></div><div id="40916968" class="c"><input type="checkbox" id="c-40916968" checked=""/><div class="controls bullet"><span class="by">yshvrdhn</span><span>|</span><a href="#40917090">prev</a><span>|</span><a href="#40918746">next</a><span>|</span><label class="collapse" for="c-40916968">[-]</label><label class="expand" for="c-40916968">[2 more]</label></div><br/><div class="children"><div class="content">Am I missing something but can&#x27;t something like distillation help here ?</div><br/><div id="40917185" class="c"><input type="checkbox" id="c-40917185" checked=""/><div class="controls bullet"><span class="by">imurray</span><span>|</span><a href="#40916968">parent</a><span>|</span><a href="#40918746">next</a><span>|</span><label class="collapse" for="c-40917185">[-]</label><label class="expand" for="c-40917185">[1 more]</label></div><br/><div class="children"><div class="content">The paper says they tried that: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.14905" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.14905</a><p>Deep link to the relevant snippet in html version:
<a href="https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;2402.14905#S3.SS5" rel="nofollow">https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;2402.14905#S3.SS5</a><p><i>&quot;So far, we trained compact models from scratch using next tokens as hard labels. We explored Knowledge Distillation (KD)... Unfortunately KD increases training time (slowdown of 2.6−3.2×) and exhibits comparable or inferior accuracy to label-based training (details in appendix).&quot;</i></div><br/></div></div></div></div><div id="40918746" class="c"><input type="checkbox" id="c-40918746" checked=""/><div class="controls bullet"><span class="by">vhiremath4</span><span>|</span><a href="#40916968">prev</a><span>|</span><a href="#40921515">next</a><span>|</span><label class="collapse" for="c-40918746">[-]</label><label class="expand" for="c-40918746">[2 more]</label></div><br/><div class="children"><div class="content">It seems like the smaller models get the largest size decrease by embedding share&#x2F;weight tying between the linear head and token embeddings. Is there any research going into how to further reduce size from there?</div><br/><div id="40924706" class="c"><input type="checkbox" id="c-40924706" checked=""/><div class="controls bullet"><span class="by">cztomsik</span><span>|</span><a href="#40918746">parent</a><span>|</span><a href="#40921515">next</a><span>|</span><label class="collapse" for="c-40924706">[-]</label><label class="expand" for="c-40924706">[1 more]</label></div><br/><div class="children"><div class="content">If you mean that LM-head is just inverted embedding matrix then this was already done in GPT-2.<p>Unfortunately, the only thing I found out about this is that bigger models benefit from separate layer. But this was only mentioned somewhere in discord, so no paper to read and my personal hunch is that it should work for bigger models too. After all, GPT-3 was just scaled GPT-2.<p>From my personal experiments, models learn better if you give them harder task. And tied weights could be one of such things. Multi-token prediction could be another and bitnet could be also considered such... (and dropout too)</div><br/></div></div></div></div><div id="40921515" class="c"><input type="checkbox" id="c-40921515" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#40918746">prev</a><span>|</span><a href="#40923281">next</a><span>|</span><label class="collapse" for="c-40921515">[-]</label><label class="expand" for="c-40921515">[3 more]</label></div><br/><div class="children"><div class="content">Does it have to stay on mobile devices? Bit of niche but if its not a resource hog it could be handy for giving NPC&#x27;s in games more interesting dialogue without having use<p>Even better if it could be tuned in someway to allow dialogue to influence NPC behavior or actions.</div><br/><div id="40922060" class="c"><input type="checkbox" id="c-40922060" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#40921515">parent</a><span>|</span><a href="#40923947">next</a><span>|</span><label class="collapse" for="c-40922060">[-]</label><label class="expand" for="c-40922060">[1 more]</label></div><br/><div class="children"><div class="content">It would be fascinating if NPCs had more backstory to them and more complex behaviors. Although I would imagine it would be near impossible to test since anything could influence their behavior.</div><br/></div></div><div id="40923947" class="c"><input type="checkbox" id="c-40923947" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#40921515">parent</a><span>|</span><a href="#40922060">prev</a><span>|</span><a href="#40923281">next</a><span>|</span><label class="collapse" for="c-40923947">[-]</label><label class="expand" for="c-40923947">[1 more]</label></div><br/><div class="children"><div class="content">Would it be <i>interesting</i> dialogue? You could generate more dialogue, but would it have anything underpinning it of interest to the player? i.e. you could suddenly have townspeople that would talk about local scenery or their relationships with other NPCs, but none of that stuff they describe would actually <i>exist</i> in the game. I would personally be weirded out if NPCs started making stuff up.<p>I can imagine training some sort of LLM <i>on</i> your game data such that NPCs are able to actually describe the game world, but I can&#x27;t imagine what kind of scale you&#x27;d need to operate at for that to be cheaper than just paying someone to write the dialogue. Maybe at Ubisoft&#x27;s scale where your team sizes are in the thousands (AFAIK, they have been investigating using AI for writing, but it&#x27;s mostly for things like combat barks which are very repetitive and basically noise.)</div><br/></div></div></div></div><div id="40923281" class="c"><input type="checkbox" id="c-40923281" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#40921515">prev</a><span>|</span><a href="#40916727">next</a><span>|</span><label class="collapse" for="c-40923281">[-]</label><label class="expand" for="c-40923281">[1 more]</label></div><br/><div class="children"><div class="content">When Gemma 2 2b releases it would be interesting to compare its scaling with this</div><br/></div></div><div id="40916727" class="c"><input type="checkbox" id="c-40916727" checked=""/><div class="controls bullet"><span class="by">sourcecodeplz</span><span>|</span><a href="#40923281">prev</a><span>|</span><a href="#40916749">next</a><span>|</span><label class="collapse" for="c-40916727">[-]</label><label class="expand" for="c-40916727">[2 more]</label></div><br/><div class="children"><div class="content">Nice, could one use this to train models for Windows PCs also? I don&#x27;t have a lot of ram.</div><br/><div id="40917474" class="c"><input type="checkbox" id="c-40917474" checked=""/><div class="controls bullet"><span class="by">skiexperte</span><span>|</span><a href="#40916727">parent</a><span>|</span><a href="#40916749">next</a><span>|</span><label class="collapse" for="c-40917474">[-]</label><label class="expand" for="c-40917474">[1 more]</label></div><br/><div class="children"><div class="content">Training models is not OS dependend. RAM is dependend on the size and i would argue this should be a lot easier to finetune with less GPU Ram.<p>Nonetheless the endgoal will probably be downloading a model like this or paying for finetuning than downloading and using it through an optimized Neuralchip.<p>Its currently more a question of when this will happen. The newest Windows cert already requires some neuralchip and even my google pixel 8 pro can host small models (i know the pixel is not a cheap phone, but the coprocessor should still be much more affordable than a big GPU)</div><br/></div></div></div></div><div id="40916749" class="c"><input type="checkbox" id="c-40916749" checked=""/><div class="controls bullet"><span class="by">zurfer</span><span>|</span><a href="#40916727">prev</a><span>|</span><a href="#40921058">next</a><span>|</span><label class="collapse" for="c-40916749">[-]</label><label class="expand" for="c-40916749">[15 more]</label></div><br/><div class="children"><div class="content">While this is interesting, I wonder what the use case is, other than better autocomplete?</div><br/><div id="40917411" class="c"><input type="checkbox" id="c-40917411" checked=""/><div class="controls bullet"><span class="by">throwthrowuknow</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40917083">next</a><span>|</span><label class="collapse" for="c-40917411">[-]</label><label class="expand" for="c-40917411">[1 more]</label></div><br/><div class="children"><div class="content">You could possibly fine tune it for narrow domain tasks like they did with tiny-agent <a href="https:&#x2F;&#x2F;bair.berkeley.edu&#x2F;blog&#x2F;2024&#x2F;05&#x2F;29&#x2F;tiny-agent&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bair.berkeley.edu&#x2F;blog&#x2F;2024&#x2F;05&#x2F;29&#x2F;tiny-agent&#x2F;</a><p>I like the approach that Apple seems to be taking with fine tuned small models that handle routine tasks and then defer to larger off device models for things they can’t confidently do. I imagine you could construct a training set that contains examples that should produce low confidence answers where you could add an output that is essentially a “call for help” option so you could train it to choose that. Smaller models also means you could have more running in parallel and use another to route requests to the appropriate expert.</div><br/></div></div><div id="40917083" class="c"><input type="checkbox" id="c-40917083" checked=""/><div class="controls bullet"><span class="by">skiexperte</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40917411">prev</a><span>|</span><a href="#40916981">next</a><span>|</span><label class="collapse" for="c-40917083">[-]</label><label class="expand" for="c-40917083">[3 more]</label></div><br/><div class="children"><div class="content">Reading emails, replying to emails, scheduling tasks, using apis for services.<p>Basically everything which doesn&#x27;t need knowledge but actions.<p>&quot;Tell my wife i&#x27;m late&quot; and it will use some configured magic to talk to service xy and just does it.<p>Siri is very good in doing homeautomatistaion without the internet, the old google agent and alexa were absolutly not and i don&#x27;t think they were ever available offline.<p>This basically gives you a local (local-first!) good working assistent</div><br/><div id="40917797" class="c"><input type="checkbox" id="c-40917797" checked=""/><div class="controls bullet"><span class="by">Narhem</span><span>|</span><a href="#40916749">root</a><span>|</span><a href="#40917083">parent</a><span>|</span><a href="#40916981">next</a><span>|</span><label class="collapse" for="c-40917797">[-]</label><label class="expand" for="c-40917797">[2 more]</label></div><br/><div class="children"><div class="content">Would be very nice to have my schedule automatically managed by Siri. Already has a few nice things but I genuinely have trust issues, especially with AI.</div><br/><div id="40918434" class="c"><input type="checkbox" id="c-40918434" checked=""/><div class="controls bullet"><span class="by">lovethevoid</span><span>|</span><a href="#40916749">root</a><span>|</span><a href="#40917797">parent</a><span>|</span><a href="#40916981">next</a><span>|</span><label class="collapse" for="c-40918434">[-]</label><label class="expand" for="c-40918434">[1 more]</label></div><br/><div class="children"><div class="content">You can get very far with the Shortcuts app by the way. Some examples: using your current location to estimate when you should leave to get to your next meeting on your calendar, 
letting those included in the calendar event know you’re running late. Highly highly recommend it, the learning curve isn’t much, a bunch of drag and drop!</div><br/></div></div></div></div></div></div><div id="40916981" class="c"><input type="checkbox" id="c-40916981" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40917083">prev</a><span>|</span><a href="#40919262">next</a><span>|</span><label class="collapse" for="c-40916981">[-]</label><label class="expand" for="c-40916981">[1 more]</label></div><br/><div class="children"><div class="content">Local agent like siri that can do simple tasks, and route more complex requests.</div><br/></div></div><div id="40919262" class="c"><input type="checkbox" id="c-40919262" checked=""/><div class="controls bullet"><span class="by">barronli</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40916981">prev</a><span>|</span><a href="#40916847">next</a><span>|</span><label class="collapse" for="c-40919262">[-]</label><label class="expand" for="c-40919262">[1 more]</label></div><br/><div class="children"><div class="content">It can be fine tuned for device related actions. In other words, with all the capabilities of your device applications or services, the small model can virtually have the same capabilities. It can always dispatch a user request in way of “natural language” to those applications, and orchestrate the applications. It can dispatch user requests beyond the device capabilities to a cloud model. This is powerful since it changes how you interact with your devices.</div><br/></div></div><div id="40916847" class="c"><input type="checkbox" id="c-40916847" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40919262">prev</a><span>|</span><a href="#40920871">next</a><span>|</span><label class="collapse" for="c-40916847">[-]</label><label class="expand" for="c-40916847">[3 more]</label></div><br/><div class="children"><div class="content">It could power simple agents like Siri under the hood. Helping with natural language understanding, intent classification, retrieval, and other agent tasks.</div><br/><div id="40916885" class="c"><input type="checkbox" id="c-40916885" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#40916749">root</a><span>|</span><a href="#40916847">parent</a><span>|</span><a href="#40920871">next</a><span>|</span><label class="collapse" for="c-40916885">[-]</label><label class="expand" for="c-40916885">[2 more]</label></div><br/><div class="children"><div class="content">Like the Rabbit R1 or Humane AI Pin</div><br/></div></div></div></div><div id="40920871" class="c"><input type="checkbox" id="c-40920871" checked=""/><div class="controls bullet"><span class="by">syassami</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40916847">prev</a><span>|</span><a href="#40917907">next</a><span>|</span><label class="collapse" for="c-40920871">[-]</label><label class="expand" for="c-40920871">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.meta.com&#x2F;smart-glasses&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.meta.com&#x2F;smart-glasses&#x2F;</a></div><br/></div></div><div id="40917907" class="c"><input type="checkbox" id="c-40917907" checked=""/><div class="controls bullet"><span class="by">Narhem</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40920871">prev</a><span>|</span><a href="#40920940">next</a><span>|</span><label class="collapse" for="c-40917907">[-]</label><label class="expand" for="c-40917907">[1 more]</label></div><br/><div class="children"><div class="content">Probably hacking foreign intelligence codes.</div><br/></div></div><div id="40920940" class="c"><input type="checkbox" id="c-40920940" checked=""/><div class="controls bullet"><span class="by">simion314</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40917907">prev</a><span>|</span><a href="#40918144">next</a><span>|</span><label class="collapse" for="c-40920940">[-]</label><label class="expand" for="c-40920940">[1 more]</label></div><br/><div class="children"><div class="content">I tested the Google AI on my phone, I had the browser open and asked it to read the page to me and it responded that it does not have access to the internet.<p>So I would like an AI assistant that:<p>1 can understand english and my native language<p>2 that is aware that runs on Android(or KDE&#x2F;Linux) and can understand commands like &quot;open the Android Settings , Application section &quot;  or &quot;read the page that is opened in the browser&quot; or &quot;read the text in the popup that is now opened&quot;. Basically to be integrated with the OS via public and open APIs. Big AI companies could compete on selling us better assistants especially for multi lingual people.<p>3 the model should be small , it should not know geography, history, music bands etc, for  tasks where the user asks question there should be an option for the model to forward the question to a search engine or even an online LLM.</div><br/></div></div><div id="40918144" class="c"><input type="checkbox" id="c-40918144" checked=""/><div class="controls bullet"><span class="by">nsonha</span><span>|</span><a href="#40916749">parent</a><span>|</span><a href="#40920940">prev</a><span>|</span><a href="#40921058">next</a><span>|</span><label class="collapse" for="c-40918144">[-]</label><label class="expand" for="c-40918144">[2 more]</label></div><br/><div class="children"><div class="content">user cases are that of LLMs, from a mobile UI (so every AI use case there is), when you need privacy from big tech&#x27;s AI APIs.<p>I&#x27;m just so amazed by statements like &quot;LLMs can ONLY be used for autocomplete&quot;, like am I supposed to be impressed by the smirkiness?</div><br/><div id="40921963" class="c"><input type="checkbox" id="c-40921963" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40916749">root</a><span>|</span><a href="#40918144">parent</a><span>|</span><a href="#40921058">next</a><span>|</span><label class="collapse" for="c-40921963">[-]</label><label class="expand" for="c-40921963">[1 more]</label></div><br/><div class="children"><div class="content">The question was more about the capability and knowledge in a sub-1B LLM: at that size what is it capable to do beyond excellent autocompletion.</div><br/></div></div></div></div></div></div><div id="40921058" class="c"><input type="checkbox" id="c-40921058" checked=""/><div class="controls bullet"><span class="by">cjtrowbridge</span><span>|</span><a href="#40916749">prev</a><span>|</span><a href="#40917342">next</a><span>|</span><label class="collapse" for="c-40921058">[-]</label><label class="expand" for="c-40921058">[1 more]</label></div><br/><div class="children"><div class="content">Why no mmlu or gsm8k?</div><br/></div></div><div id="40917342" class="c"><input type="checkbox" id="c-40917342" checked=""/><div class="controls bullet"><span class="by">ejdhshsuwisjsh</span><span>|</span><a href="#40921058">prev</a><span>|</span><a href="#40917205">next</a><span>|</span><label class="collapse" for="c-40917342">[-]</label><label class="expand" for="c-40917342">[2 more]</label></div><br/><div class="children"><div class="content">Anyone is aware of custom mobile llms?<p>Optimizing and loading in your own voice, selecting your primary language and adding a little bit of personal knowledge like nicknames, location and stuff?<p>My pixel 8 apparently can use &#x2F; load local models but don&#x27;t have the time right now to follow that rabbit hole</div><br/><div id="40919339" class="c"><input type="checkbox" id="c-40919339" checked=""/><div class="controls bullet"><span class="by">euniceee3</span><span>|</span><a href="#40917342">parent</a><span>|</span><a href="#40917205">next</a><span>|</span><label class="collapse" for="c-40919339">[-]</label><label class="expand" for="c-40919339">[1 more]</label></div><br/><div class="children"><div class="content">Tensor chips are not open enough for an optimized mobile LLM to be ran on them.</div><br/></div></div></div></div></div></div></div></div></div></body></html>