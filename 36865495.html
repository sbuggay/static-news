<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690362058948" as="style"/><link rel="stylesheet" href="styles.css?v=1690362058948"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://replicate.com/blog/run-llama-locally">Guide to running Llama 2 locally</a> <span class="domain">(<a href="https://replicate.com">replicate.com</a>)</span></div><div class="subtext"><span>bfirsh</span> | <span>131 comments</span></div><br/><div><div id="36871730" class="c"><input type="checkbox" id="c-36871730" checked=""/><div class="controls bullet"><span class="by">shortrounddev2</span><span>|</span><a href="#36871535">next</a><span>|</span><label class="collapse" for="c-36871730">[-]</label><label class="expand" for="c-36871730">[3 more]</label></div><br/><div class="children"><div class="content">For my fellow Windows shills, here&#x27;s how you actually build it on windows:<p>Before steps:<p>1. (For Nvidia GPU users) Install cuda toolkit <a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cuda-downloads" rel="nofollow noreferrer">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cuda-downloads</a><p>2. Download the model somewhere: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-13B-chat-GGML&#x2F;resolve&#x2F;main&#x2F;llama-2-13b-chat.ggmlv3.q4_0.bin" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-13B-chat-GGML&#x2F;resolv...</a><p>In Windows Terminal with Powershell:<p><pre><code>    git clone https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp
    cd llama.cpp
    mkdir build
    cd build
    cmake .. -DLLAMA_CUBLAS=ON
    cmake --build . --config Release
    cd bin&#x2F;Release
    mkdir models
    mv Folder\Where\You\Downloaded\The\Model .\models
    .\main.exe -m .\models\llama-2-13b-chat.ggmlv3.q4_0.bin --color -p &quot;Hello, how are you, llama?&quot; 2&gt; $null
</code></pre>
`-DLLAMA_CUBLAS` uses cuda<p>`2&gt; $null` is to direct the debug messages printed to stderr to a null file so they don&#x27;t spam your terminal<p>Here&#x27;s a powershell function you can put in your $PSPROFILE so that you can just run prompts with `llama &quot;prompt goes here&quot;`:<p><pre><code>    function llama {
        .\main.exe -m .\models\llama-2-13b-chat.ggmlv3.q4_0.bin -p $args 2&gt; $null
    }
</code></pre>
adjust your paths as necessary. It has a tendency to talk to itself.</div><br/><div id="36872616" class="c"><input type="checkbox" id="c-36872616" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#36871730">parent</a><span>|</span><a href="#36871535">next</a><span>|</span><label class="collapse" for="c-36872616">[-]</label><label class="expand" for="c-36872616">[2 more]</label></div><br/><div class="children"><div class="content">&gt; adjust your paths as necessary. It has a tendency to talk to itself.<p>This is always a fun surprise. What I&#x27;ve seen help, especially with chat models, is to use a prompt template. Some tools (e.g. <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ollama.ai&#x2F;</a> – mentioned in the article) use a default, model-specific prompt template when you run the model. This is easier for users since they can just input your chat messages and get answers. The hard part is every model is trained (and behaves) differently.<p>With llama.cpp you&#x27;d need wrap your prompt text with the right template. For llama 2, the facebook developers&#x27; generation code wraps the system prompt and user prompts in specific tags (&lt;&lt;SYS&gt;&gt;{system prompt}&lt;&lt;&#x2F;SYS&gt;&gt; and [INST]{user prompt}[&#x2F;INST]) respectively): <a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;llama&#x2F;blob&#x2F;main&#x2F;llama&#x2F;generation.py#L44">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;llama&#x2F;blob&#x2F;main&#x2F;llama&#x2F;ge...</a>.<p>Worth noting that customizing prompt templates can be fun – you don&#x27;t <i>have</i> to use &quot;prescribed&quot; one – I&#x27;ve had a model generate a conversation between a few characters that talk to each other for example – it&#x27;s pretty entertaining!</div><br/><div id="36874448" class="c"><input type="checkbox" id="c-36874448" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36871730">root</a><span>|</span><a href="#36872616">parent</a><span>|</span><a href="#36871535">next</a><span>|</span><label class="collapse" for="c-36874448">[-]</label><label class="expand" for="c-36874448">[1 more]</label></div><br/><div class="children"><div class="content">As a followup on LLama2&#x27;s prompting, here&#x27;s a good thread with some more details: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;155po2p&#x2F;get_llama_2_prompt_format_right&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;155po2p&#x2F;get_lla...</a> (see also: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1561vn5&#x2F;here_is_a_practical_multiturn_llama2chat_prompt&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1561vn5&#x2F;here_is...</a> - it&#x27;s a bit complex and people are still wrapping their heads around it)<p>Note, it is possible to reinject &lt;&lt;SYS&gt;&gt; prompts during the conversation to keep the LLM on target: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;overlordayn&#x2F;status&#x2F;1681631554672513025" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;overlordayn&#x2F;status&#x2F;1681631554672513025</a> (but obviously this should be the first thing you filter and track if you are running an LLM that is accessible to end-users - rebuff is a lib w&#x2F; some good ideas to start with)<p>With new fine tunes coming out, it&#x27;s worth noting again that different datasets&#x2F;models all use slightly different prompt formats (many are using multiple datasets these days, so it&#x27;s hard to say just how much it matters now): <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13lwwux&#x2F;comment&#x2F;jksp37r&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13lwwux&#x2F;comment...</a><p>Ideally, fine tunes for LLama2 would regularize all datasets to the official tokens&#x2F;format and inference interfaces could standardize too (or there could be some metadata collected for what model uses what, but some of the low hanging fruit that&#x27;s out for the future I guess). One other thing to keep in mind is that all the benchmarks&#x2F;leaderboards don&#x27;t use instruct formatting at all, and don&#x27;t <i>really</i> represent capabilities for a model. IMO, ELO-style rankings against models for specific tasks would probably be more representative.</div><br/></div></div></div></div></div></div><div id="36871535" class="c"><input type="checkbox" id="c-36871535" checked=""/><div class="controls bullet"><span class="by">jawerty</span><span>|</span><a href="#36871730">prev</a><span>|</span><a href="#36867718">next</a><span>|</span><label class="collapse" for="c-36871535">[-]</label><label class="expand" for="c-36871535">[6 more]</label></div><br/><div class="children"><div class="content">Some you may have seen this but I have a Llama 2 finetuning live coding stream from 2 days ago where I walk through some fundamentals (like RLHF and Lora) and how to fine-tune LLama 2 using PEFT&#x2F;Lora on a Google Colab A100 GPU.<p>In the end with quantization and parameter efficient fine-tuning it only took up 13gb on a single GPU.<p>Check it out here if you&#x27;re interested: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TYgtG2Th6fI">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TYgtG2Th6fI</a></div><br/><div id="36874781" class="c"><input type="checkbox" id="c-36874781" checked=""/><div class="controls bullet"><span class="by">soultrees</span><span>|</span><a href="#36871535">parent</a><span>|</span><a href="#36874189">next</a><span>|</span><label class="collapse" for="c-36874781">[-]</label><label class="expand" for="c-36874781">[1 more]</label></div><br/><div class="children"><div class="content">Can you do more videos on preparing the data and scraping data? I noticed you seem to be proficient at it by your terminology and it’s something I want to dive into more.</div><br/></div></div><div id="36874189" class="c"><input type="checkbox" id="c-36874189" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#36871535">parent</a><span>|</span><a href="#36874781">prev</a><span>|</span><a href="#36871977">next</a><span>|</span><label class="collapse" for="c-36874189">[-]</label><label class="expand" for="c-36874189">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s special about Llama2?</div><br/></div></div><div id="36871977" class="c"><input type="checkbox" id="c-36871977" checked=""/><div class="controls bullet"><span class="by">aledalgrande</span><span>|</span><a href="#36871535">parent</a><span>|</span><a href="#36874189">prev</a><span>|</span><a href="#36872184">next</a><span>|</span><label class="collapse" for="c-36871977">[-]</label><label class="expand" for="c-36871977">[1 more]</label></div><br/><div class="children"><div class="content">Sick I&#x27;ve put it in my queue</div><br/></div></div><div id="36872184" class="c"><input type="checkbox" id="c-36872184" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#36871535">parent</a><span>|</span><a href="#36871977">prev</a><span>|</span><a href="#36867718">next</a><span>|</span><label class="collapse" for="c-36872184">[-]</label><label class="expand" for="c-36872184">[2 more]</label></div><br/><div class="children"><div class="content">which version of Llama 2?</div><br/><div id="36873072" class="c"><input type="checkbox" id="c-36873072" checked=""/><div class="controls bullet"><span class="by">jawerty</span><span>|</span><a href="#36871535">root</a><span>|</span><a href="#36872184">parent</a><span>|</span><a href="#36867718">next</a><span>|</span><label class="collapse" for="c-36873072">[-]</label><label class="expand" for="c-36873072">[1 more]</label></div><br/><div class="children"><div class="content">The 7b parameter one so not one of the larger ones</div><br/></div></div></div></div></div></div><div id="36867718" class="c"><input type="checkbox" id="c-36867718" checked=""/><div class="controls bullet"><span class="by">andreyk</span><span>|</span><a href="#36871535">prev</a><span>|</span><a href="#36870809">next</a><span>|</span><label class="collapse" for="c-36867718">[-]</label><label class="expand" for="c-36867718">[30 more]</label></div><br/><div class="children"><div class="content">This covers three things:
    Llama.cpp (Mac&#x2F;Windows&#x2F;Linux),
    Ollama (Mac),
    MLC LLM (iOS&#x2F;Android)<p>Which is not really comprehensive... If you have a linux machine with GPUs, i&#x27;d just use hugging face inference (<a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;text-generation-inference">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;text-generation-inference</a>). And I am sure there are other things that could be covered.</div><br/><div id="36867871" class="c"><input type="checkbox" id="c-36867871" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#36867718">parent</a><span>|</span><a href="#36868601">next</a><span>|</span><label class="collapse" for="c-36867871">[-]</label><label class="expand" for="c-36867871">[6 more]</label></div><br/><div class="children"><div class="content">Ollama works with Windows and Linux as well too, but doesn&#x27;t (yet) have GPU support for those platforms. You have to compile it yourself (it&#x27;s a simple `go build .`), but should work fine (albeit slow). The benefit is you can still pull the llama2 model really easily (with `ollama pull llama2`) and even use it with other runners.<p>DISCLAIMER: I&#x27;m one of the developers behind Ollama.</div><br/><div id="36868811" class="c"><input type="checkbox" id="c-36868811" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36867871">parent</a><span>|</span><a href="#36869870">next</a><span>|</span><label class="collapse" for="c-36868811">[-]</label><label class="expand" for="c-36868811">[2 more]</label></div><br/><div class="children"><div class="content">&gt; DISCLAIMER: I&#x27;m one of the developers behind Ollama.<p>I got a feature suggestion - would it be possible to have the ollama CLI automatically start up the GUI&#x2F;daemon if it&#x27;s not running? There&#x27;s only so much stuff one can keep in a Macbook Air&#x27;s auto start.</div><br/><div id="36869062" class="c"><input type="checkbox" id="c-36869062" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868811">parent</a><span>|</span><a href="#36869870">next</a><span>|</span><label class="collapse" for="c-36869062">[-]</label><label class="expand" for="c-36869062">[1 more]</label></div><br/><div class="children"><div class="content">Good suggestion! This is definitely on the radar, so that running `ollama` will start the server when it&#x27;s needed (instead of erroring!): <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;issues&#x2F;47">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;issues&#x2F;47</a></div><br/></div></div></div></div><div id="36869870" class="c"><input type="checkbox" id="c-36869870" checked=""/><div class="controls bullet"><span class="by">DennisP</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36867871">parent</a><span>|</span><a href="#36868811">prev</a><span>|</span><a href="#36871441">next</a><span>|</span><label class="collapse" for="c-36869870">[-]</label><label class="expand" for="c-36869870">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been wondering, is the M2&#x27;s neural engine usable for this?</div><br/><div id="36872404" class="c"><input type="checkbox" id="c-36872404" checked=""/><div class="controls bullet"><span class="by">npsomaratna</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36869870">parent</a><span>|</span><a href="#36871441">next</a><span>|</span><label class="collapse" for="c-36872404">[-]</label><label class="expand" for="c-36872404">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;d need to offload the model into CoreML to do so, right? My understanding is that none of the current popular inference frameworks do this (not yet, at least).</div><br/></div></div></div></div><div id="36871441" class="c"><input type="checkbox" id="c-36871441" checked=""/><div class="controls bullet"><span class="by">sanex</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36867871">parent</a><span>|</span><a href="#36869870">prev</a><span>|</span><a href="#36868601">next</a><span>|</span><label class="collapse" for="c-36871441">[-]</label><label class="expand" for="c-36871441">[1 more]</label></div><br/><div class="children"><div class="content">Matt Williams talk today at that conference was a great intro to Ollama.</div><br/></div></div></div></div><div id="36868601" class="c"><input type="checkbox" id="c-36868601" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#36867718">parent</a><span>|</span><a href="#36867871">prev</a><span>|</span><a href="#36870646">next</a><span>|</span><label class="collapse" for="c-36868601">[-]</label><label class="expand" for="c-36868601">[18 more]</label></div><br/><div class="children"><div class="content">Just a note that you have to have at least 12GB VRAM for it to be worth even trying to use your GPU for LLaMA 2.<p>The 7B model quantized to 4 bits can fit in 8GB VRAM with room for the context, but is pretty useless for getting good results in my experience. 13B is better but still not anything near as good as the 70B, which would require &gt;35GB VRAM to use at 4 bit quantization.<p>My solution for playing with this was just to upgrade my PC&#x27;s RAM to 64GB. It&#x27;s slower than the GPU, but it was way cheaper and I can run the 70B model easily.</div><br/><div id="36869609" class="c"><input type="checkbox" id="c-36869609" checked=""/><div class="controls bullet"><span class="by">ErneX</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868601">parent</a><span>|</span><a href="#36873549">next</a><span>|</span><label class="collapse" for="c-36869609">[-]</label><label class="expand" for="c-36869609">[2 more]</label></div><br/><div class="children"><div class="content">Apple Silicon Macs might not have great GPUs but they do have unified memory. I need to try this on mine I have 96GB of RAM on my M2 Max.</div><br/><div id="36874825" class="c"><input type="checkbox" id="c-36874825" checked=""/><div class="controls bullet"><span class="by">diffeomorphism</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36869609">parent</a><span>|</span><a href="#36873549">next</a><span>|</span><label class="collapse" for="c-36874825">[-]</label><label class="expand" for="c-36874825">[1 more]</label></div><br/><div class="children"><div class="content">What does &quot;unified&quot; actually mean and how much would that help?
It is still off the shelf LPDDR5‑6400, just with a better interconnnect (like a ps5).<p>How does this compare to non-unified ddr5 or hmb2e as on nvidia A100 cards?</div><br/></div></div></div></div><div id="36873549" class="c"><input type="checkbox" id="c-36873549" checked=""/><div class="controls bullet"><span class="by">creata</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868601">parent</a><span>|</span><a href="#36869609">prev</a><span>|</span><a href="#36873545">next</a><span>|</span><label class="collapse" for="c-36873549">[-]</label><label class="expand" for="c-36873549">[1 more]</label></div><br/><div class="children"><div class="content">Running just some of the layers on the GPU can still make things much faster, though.</div><br/></div></div><div id="36873545" class="c"><input type="checkbox" id="c-36873545" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868601">parent</a><span>|</span><a href="#36873549">prev</a><span>|</span><a href="#36869184">next</a><span>|</span><label class="collapse" for="c-36873545">[-]</label><label class="expand" for="c-36873545">[1 more]</label></div><br/><div class="children"><div class="content">Does that mean you have to run it on the CPU? Or can you use the GPU with system RAM?</div><br/></div></div><div id="36869184" class="c"><input type="checkbox" id="c-36869184" checked=""/><div class="controls bullet"><span class="by">dc443</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868601">parent</a><span>|</span><a href="#36873545">prev</a><span>|</span><a href="#36870062">next</a><span>|</span><label class="collapse" for="c-36869184">[-]</label><label class="expand" for="c-36869184">[5 more]</label></div><br/><div class="children"><div class="content">I have 2x 3090 do you know if it&#x27;s feasible to use that 48GB total for running this?</div><br/><div id="36869929" class="c"><input type="checkbox" id="c-36869929" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36869184">parent</a><span>|</span><a href="#36870062">next</a><span>|</span><label class="collapse" for="c-36869929">[-]</label><label class="expand" for="c-36869929">[4 more]</label></div><br/><div class="children"><div class="content">Yes, it runs totally fine. I ran it in Oobabooga&#x2F;text generation web ui. Nice thing about it is that it autodownloads all necessary gpu binaries on it&#x27;s own and creates a isolated conda env. I asked same questions on the official 70b demo and got same answers. I even got better answers with ooba, since the demo cuts text early<p>Ooobabooga: <a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a><p>Model: TheBloke_Llama-2-70B-chat-GPTQ from <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-70B-chat-GPTQ" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-70B-chat-GPTQ</a><p>ExLlama_HF loader gpu split 20,22, context size 2048<p>on the Chat Settings tab, choose Instruction template tab and pick Llama-v2 from the instruction template dropdown<p>Demo: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;llama2#demo" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;llama2#demo</a></div><br/><div id="36870580" class="c"><input type="checkbox" id="c-36870580" checked=""/><div class="controls bullet"><span class="by">zakki</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36869929">parent</a><span>|</span><a href="#36870062">next</a><span>|</span><label class="collapse" for="c-36870580">[-]</label><label class="expand" for="c-36870580">[3 more]</label></div><br/><div class="children"><div class="content">Is there any specific settings to make 2x3090 work together?</div><br/><div id="36874407" class="c"><input type="checkbox" id="c-36874407" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870580">parent</a><span>|</span><a href="#36872913">next</a><span>|</span><label class="collapse" for="c-36874407">[-]</label><label class="expand" for="c-36874407">[1 more]</label></div><br/><div class="children"><div class="content">Not really? I just got those cards in separate PCI slots and the Exllama_hf handles spreading the load internally. No NVLink bridge in particular. I use the &quot;20,22&quot; memory split so that the display card has some room for the framebuffer to handle display</div><br/></div></div><div id="36872913" class="c"><input type="checkbox" id="c-36872913" checked=""/><div class="controls bullet"><span class="by">kwerk</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870580">parent</a><span>|</span><a href="#36874407">prev</a><span>|</span><a href="#36870062">next</a><span>|</span><label class="collapse" for="c-36872913">[-]</label><label class="expand" for="c-36872913">[1 more]</label></div><br/><div class="children"><div class="content">Interested in this too</div><br/></div></div></div></div></div></div></div></div><div id="36870062" class="c"><input type="checkbox" id="c-36870062" checked=""/><div class="controls bullet"><span class="by">NoMoreNicksLeft</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868601">parent</a><span>|</span><a href="#36869184">prev</a><span>|</span><a href="#36871345">next</a><span>|</span><label class="collapse" for="c-36870062">[-]</label><label class="expand" for="c-36870062">[7 more]</label></div><br/><div class="children"><div class="content">Trying to figure out what hardware to convince my boss to spend on... if we were to get one of the A6000&#x2F;48gb cards, will that see significant performance improvements over just a 4090&#x2F;24gb? The primary limitation is vram, is it not?</div><br/><div id="36870781" class="c"><input type="checkbox" id="c-36870781" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870062">parent</a><span>|</span><a href="#36870540">next</a><span>|</span><label class="collapse" for="c-36870781">[-]</label><label class="expand" for="c-36870781">[3 more]</label></div><br/><div class="children"><div class="content">You might consider getting a Mac Studio (with as much RAM as you can afford up to 192GB) instead, since 192GB is more (unified) memory than you&#x27;re going to easily get to with GPUs.</div><br/><div id="36872462" class="c"><input type="checkbox" id="c-36872462" checked=""/><div class="controls bullet"><span class="by">abhibeckert</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870781">parent</a><span>|</span><a href="#36874221">next</a><span>|</span><label class="collapse" for="c-36872462">[-]</label><label class="expand" for="c-36872462">[1 more]</label></div><br/><div class="children"><div class="content">This. The main system memory on a Mac Studio is GPU memory and there&#x27;s a lot of it.<p>It also has the Neural Engine, which is specifically designed for this type of work - most software isn&#x27;t designed to take advantage of that yet, but presumably it will soon.</div><br/></div></div><div id="36874221" class="c"><input type="checkbox" id="c-36874221" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870781">parent</a><span>|</span><a href="#36872462">prev</a><span>|</span><a href="#36870540">next</a><span>|</span><label class="collapse" for="c-36874221">[-]</label><label class="expand" for="c-36874221">[1 more]</label></div><br/><div class="children"><div class="content">While on the surface, a 192GB Mac Studio seems like a great deal (it&#x27;s not much more than a 48GB A6000!), there are several reasons why this might not be a good idea:<p>* I assume most people have never used llama.cpp Metal w&#x2F; large models. It will drop to CPU speeds whenever the context window is full: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1730#issuecomment-1585580602">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1730#issuecomm...</a> - while sure this might be fixed in the future, it&#x27;s been an issue since Metal support was added, and is a significant problem if you are actually trying to actually use it for inferencing. With 192GB of memory, you could probably run larger models w&#x2F;o quantization, but I&#x27;ve never seen anyone post benchmarks of their experiences. Note that at that point, the limited memory bandwidth will be a big factor.<p>* If you are planning on using Apple Silicon for ML&#x2F;training, I&#x27;d also be wary. There are multi-year long open bugs in PyTorch[1], and most major LLM libs like deepspeed, bitsandbytes, etc don&#x27;t have Apple Silicon support[2][3].<p>You can see similar patterns w&#x2F; Stable Diffusion support [4][5] - support lagging by months, lots of problems and poor performance with inference, much less fine tuning. You can apply this to basically any ML application you want (srt, tts, video, etc)<p>Macs are fine to poke around with, but if you actually plan to do more than run a small LLM and say &quot;neat&quot;, especially for a business, recommending a Mac for anyone getting started w&#x2F; ML workloads is a bad take. (In general, for anyone getting started, unless you&#x27;re just burning budget, renting cloud GPU is going to be the best cost&#x2F;perf,  although on-prem&#x2F;local obviously has other advantages.)<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues?q=is%3Aissue+is%3Aopen+apple+metal+sort%3Acreated-asc+">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues?q=is%3Aissue+is%3A...</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;DeepSpeed&#x2F;issues&#x2F;1580">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;DeepSpeed&#x2F;issues&#x2F;1580</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes&#x2F;issues&#x2F;485">https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes&#x2F;issues&#x2F;485</a><p>[4] <a href="https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui&#x2F;discussions&#x2F;7453">https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui&#x2F;disc...</a><p>[5] <a href="https:&#x2F;&#x2F;forums.macrumors.com&#x2F;threads&#x2F;ai-generated-art-stable-diffusion-on-mac.2377762&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;forums.macrumors.com&#x2F;threads&#x2F;ai-generated-art-stable...</a></div><br/></div></div></div></div><div id="36870540" class="c"><input type="checkbox" id="c-36870540" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870062">parent</a><span>|</span><a href="#36870781">prev</a><span>|</span><a href="#36871345">next</a><span>|</span><label class="collapse" for="c-36870540">[-]</label><label class="expand" for="c-36870540">[3 more]</label></div><br/><div class="children"><div class="content">VRAM is what gets you up to the larger model sizes, and 24GB isn&#x27;t enough to load the full 70B even at 4 bits, you need at least 35 and some extra for the context. So it depends a lot on what you want to do—fine tuning will take even more as I understand it.<p>The card&#x27;s speed will affect your performance, but I don&#x27;t know enough about different graphics cards to tell you specifics.</div><br/><div id="36871852" class="c"><input type="checkbox" id="c-36871852" checked=""/><div class="controls bullet"><span class="by">ycombmehair</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870540">parent</a><span>|</span><a href="#36872065">next</a><span>|</span><label class="collapse" for="c-36871852">[-]</label><label class="expand" for="c-36871852">[1 more]</label></div><br/><div class="children"><div class="content">How would an APU, such as 5700g with up to 128gb of system ram perform when allocating it as vram?  Is this a cost effective way of using running this on a budget?</div><br/></div></div><div id="36872065" class="c"><input type="checkbox" id="c-36872065" checked=""/><div class="controls bullet"><span class="by">NoMoreNicksLeft</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36870540">parent</a><span>|</span><a href="#36871852">prev</a><span>|</span><a href="#36871345">next</a><span>|</span><label class="collapse" for="c-36872065">[-]</label><label class="expand" for="c-36872065">[1 more]</label></div><br/><div class="children"><div class="content">Well, 48gb is better than nothing at least. And it has the potential (if we get the build right) to drop a second A6000 card into it with the nvlink module (I think this does allow you to effectively have 96gb) later.</div><br/></div></div></div></div></div></div><div id="36871345" class="c"><input type="checkbox" id="c-36871345" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868601">parent</a><span>|</span><a href="#36870062">prev</a><span>|</span><a href="#36870646">next</a><span>|</span><label class="collapse" for="c-36871345">[-]</label><label class="expand" for="c-36871345">[1 more]</label></div><br/><div class="children"><div class="content">What is necessary to run 70B on CPU without quantization?</div><br/></div></div></div></div><div id="36870646" class="c"><input type="checkbox" id="c-36870646" checked=""/><div class="controls bullet"><span class="by">robotnikman</span><span>|</span><a href="#36867718">parent</a><span>|</span><a href="#36868601">prev</a><span>|</span><a href="#36868360">next</a><span>|</span><label class="collapse" for="c-36870646">[-]</label><label class="expand" for="c-36870646">[1 more]</label></div><br/><div class="children"><div class="content">Llama.cpp has been fun to experiment around with. I was suprised with how easy it was to set up, much easier than when I tried to set up a local llm almost a year ago.</div><br/></div></div><div id="36868360" class="c"><input type="checkbox" id="c-36868360" checked=""/><div class="controls bullet"><span class="by">krisoft</span><span>|</span><a href="#36867718">parent</a><span>|</span><a href="#36870646">prev</a><span>|</span><a href="#36870809">next</a><span>|</span><label class="collapse" for="c-36868360">[-]</label><label class="expand" for="c-36868360">[4 more]</label></div><br/><div class="children"><div class="content">&gt; If you have a linux machine with GPUs<p>How much VRAM one needs to run inference with llama 2 on a GPU approximately?</div><br/><div id="36868898" class="c"><input type="checkbox" id="c-36868898" checked=""/><div class="controls bullet"><span class="by">novaRom</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868360">parent</a><span>|</span><a href="#36868639">next</a><span>|</span><label class="collapse" for="c-36868898">[-]</label><label class="expand" for="c-36868898">[1 more]</label></div><br/><div class="children"><div class="content">16Gb is minimum to run 7B model with float16 weights; out of the box, with no further efforts.</div><br/></div></div><div id="36868639" class="c"><input type="checkbox" id="c-36868639" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868360">parent</a><span>|</span><a href="#36868898">prev</a><span>|</span><a href="#36871758">next</a><span>|</span><label class="collapse" for="c-36868639">[-]</label><label class="expand" for="c-36868639">[1 more]</label></div><br/><div class="children"><div class="content">Depends on which model. I haven&#x27;t bothered doing it on my 8GB because the only model that would fit is the 7B model quantized to 4 bits, and that model at that size is pretty bad for most things. I think you could have fun with 13B with 12GB VRAM. The full size model would require &gt;35GB even quantized.</div><br/></div></div><div id="36871758" class="c"><input type="checkbox" id="c-36871758" checked=""/><div class="controls bullet"><span class="by">zacmps</span><span>|</span><a href="#36867718">root</a><span>|</span><a href="#36868360">parent</a><span>|</span><a href="#36868639">prev</a><span>|</span><a href="#36870809">next</a><span>|</span><label class="collapse" for="c-36871758">[-]</label><label class="expand" for="c-36871758">[1 more]</label></div><br/><div class="children"><div class="content">70B at f16 needs about 120-160GB (4 A100 40GB or 2 A100 80GB).<p>Quantization still seems to have some issues for the 70B model.</div><br/></div></div></div></div></div></div><div id="36870809" class="c"><input type="checkbox" id="c-36870809" checked=""/><div class="controls bullet"><span class="by">krychu</span><span>|</span><a href="#36867718">prev</a><span>|</span><a href="#36869495">next</a><span>|</span><label class="collapse" for="c-36870809">[-]</label><label class="expand" for="c-36870809">[1 more]</label></div><br/><div class="children"><div class="content">Self-plug. Here’s a fork of the original llama 2 code adapted to run on the CPU or MPS (M1&#x2F;M2 GPU) if available:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;krychu&#x2F;llama">https:&#x2F;&#x2F;github.com&#x2F;krychu&#x2F;llama</a><p>It runs with the original weights, and gets you to ~4 tokens&#x2F;sec on MacBook Pro M1 with the 7B model.</div><br/></div></div><div id="36869495" class="c"><input type="checkbox" id="c-36869495" checked=""/><div class="controls bullet"><span class="by">thisisit</span><span>|</span><a href="#36870809">prev</a><span>|</span><a href="#36867752">next</a><span>|</span><label class="collapse" for="c-36869495">[-]</label><label class="expand" for="c-36869495">[1 more]</label></div><br/><div class="children"><div class="content">The easiest way I found was to use GPT4All. Just download and install, grab GGML version of Llama 2, copy to the models directory in the installation folder. Fire up GPT4All and run.</div><br/></div></div><div id="36867752" class="c"><input type="checkbox" id="c-36867752" checked=""/><div class="controls bullet"><span class="by">rootusrootus</span><span>|</span><a href="#36869495">prev</a><span>|</span><a href="#36869903">next</a><span>|</span><label class="collapse" for="c-36867752">[-]</label><label class="expand" for="c-36867752">[16 more]</label></div><br/><div class="children"><div class="content">For most people who just want to play around and are using MacOS or Windows, I&#x27;d just recommend lmstudio.ai.  Nice interface, with super easy searching and downloading of new models.</div><br/><div id="36870919" class="c"><input type="checkbox" id="c-36870919" checked=""/><div class="controls bullet"><span class="by">serf</span><span>|</span><a href="#36867752">parent</a><span>|</span><a href="#36872446">next</a><span>|</span><label class="collapse" for="c-36870919">[-]</label><label class="expand" for="c-36870919">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I&#x27;d just recommend lmstudio.ai.<p>on windows it&#x27;s an unsigned binary , backed by a website that only indicates a twitter&#x2F;discord&#x2F;github as far as explaining their organization, and the github doesn&#x27;t include source on the client itself, only models.<p>this must throw up some red flags for others, no?</div><br/></div></div><div id="36872446" class="c"><input type="checkbox" id="c-36872446" checked=""/><div class="controls bullet"><span class="by">trevor-e</span><span>|</span><a href="#36867752">parent</a><span>|</span><a href="#36870919">prev</a><span>|</span><a href="#36871196">next</a><span>|</span><label class="collapse" for="c-36872446">[-]</label><label class="expand" for="c-36872446">[1 more]</label></div><br/><div class="children"><div class="content">Does this not work with Stable Diffusion models? Not super familiar with all of this yet but I can&#x27;t find any from HuggingFace that are compatible.</div><br/></div></div><div id="36871196" class="c"><input type="checkbox" id="c-36871196" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#36867752">parent</a><span>|</span><a href="#36872446">prev</a><span>|</span><a href="#36870944">next</a><span>|</span><label class="collapse" for="c-36871196">[-]</label><label class="expand" for="c-36871196">[1 more]</label></div><br/><div class="children"><div class="content">This seems interesting. Does anyone know of an iOS app compatible with OpenAI API that you could use to talk to LM Studio over local network?</div><br/></div></div><div id="36870944" class="c"><input type="checkbox" id="c-36870944" checked=""/><div class="controls bullet"><span class="by">robotnikman</span><span>|</span><a href="#36867752">parent</a><span>|</span><a href="#36871196">prev</a><span>|</span><a href="#36868525">next</a><span>|</span><label class="collapse" for="c-36870944">[-]</label><label class="expand" for="c-36870944">[1 more]</label></div><br/><div class="children"><div class="content">Was taking a look into this. Is the source code open for lmstudio.ai?</div><br/></div></div><div id="36868525" class="c"><input type="checkbox" id="c-36868525" checked=""/><div class="controls bullet"><span class="by">dividedbyzero</span><span>|</span><a href="#36867752">parent</a><span>|</span><a href="#36870944">prev</a><span>|</span><a href="#36869903">next</a><span>|</span><label class="collapse" for="c-36868525">[-]</label><label class="expand" for="c-36868525">[11 more]</label></div><br/><div class="children"><div class="content">Does it make any sense to try this on a lower-end Mac (like a M2 Air)?</div><br/><div id="36868681" class="c"><input type="checkbox" id="c-36868681" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36868525">parent</a><span>|</span><a href="#36869903">next</a><span>|</span><label class="collapse" for="c-36868681">[-]</label><label class="expand" for="c-36868681">[10 more]</label></div><br/><div class="children"><div class="content">Yeah! How much memory do you have?<p>If by lower-end Macbook air, you mean with 8GB of memory, try the smaller models (Such as Orca Mini 3B). You can do this via LM Studio, Oogabooga&#x2F;text-generation-webui, KoboldCPP, GPT4all, ctransformers, and more.<p>I&#x27;m biased since I work on Ollama, and if you want to try it out:<p>1. Download <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;download" rel="nofollow noreferrer">https:&#x2F;&#x2F;ollama.ai&#x2F;download</a><p>2. `ollama run orca`<p>3. Enter your input to prompt<p>Note Ollama is open source, and you can compile it too from <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a></div><br/><div id="36874679" class="c"><input type="checkbox" id="c-36874679" checked=""/><div class="controls bullet"><span class="by">column</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36868681">parent</a><span>|</span><a href="#36868905">next</a><span>|</span><label class="collapse" for="c-36874679">[-]</label><label class="expand" for="c-36874679">[1 more]</label></div><br/><div class="children"><div class="content">Heads up for anyone else: clicking that link automatically starts the download (92MB)</div><br/></div></div><div id="36868905" class="c"><input type="checkbox" id="c-36868905" checked=""/><div class="controls bullet"><span class="by">bdavbdav</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36868681">parent</a><span>|</span><a href="#36874679">prev</a><span>|</span><a href="#36869693">next</a><span>|</span><label class="collapse" for="c-36868905">[-]</label><label class="expand" for="c-36868905">[6 more]</label></div><br/><div class="children"><div class="content">I’m deliberating on how much RAM to get on my new MBP. Is 32gb going to stand me in good stead?</div><br/><div id="36869664" class="c"><input type="checkbox" id="c-36869664" checked=""/><div class="controls bullet"><span class="by">rootusrootus</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36868905">parent</a><span>|</span><a href="#36869132">next</a><span>|</span><label class="collapse" for="c-36869664">[-]</label><label class="expand" for="c-36869664">[3 more]</label></div><br/><div class="children"><div class="content">32GB should be fine.  I went a little overboard and got a new MBP with M2 MAX and 96GB, but the hardware is really best suited at this point to a 30B model.  I can and do play around with 65B models, but at that point you&#x27;re making a fairly big tradeoff in generation speed for an incremental increase in quality.<p>As a datapoint, I have a 30B model [0] loaded right now and it&#x27;s using 23.44GB of RAM.  Getting around 9 tokens&#x2F;sec, which is very usable.  I also have the 65B version of the same model [1] and it&#x27;s good for around 3.6 tokens&#x2F;second, but it uses 44GB of RAM.  Not unusably slow, but more often than not I opt for the 30B because it&#x27;s good enough and a lot faster.<p>Haven&#x27;t tried the llama2 70B yet.<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;upstage-llama-30b-instruct-2048-GGML" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;upstage-llama-30b-instruct-2...</a>
[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Upstage-Llama1-65B-Instruct-GGML" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Upstage-Llama1-65B-Instruct-...</a></div><br/><div id="36873546" class="c"><input type="checkbox" id="c-36873546" checked=""/><div class="controls bullet"><span class="by">bdavbdav</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36869664">parent</a><span>|</span><a href="#36870191">next</a><span>|</span><label class="collapse" for="c-36873546">[-]</label><label class="expand" for="c-36873546">[1 more]</label></div><br/><div class="children"><div class="content">Thankyou that’s really helpful! The CTO lead times on Mac are huge here so it’s either the pro with 16 or the max with 32. Ideally I’d go pro with 64.</div><br/></div></div><div id="36870191" class="c"><input type="checkbox" id="c-36870191" checked=""/><div class="controls bullet"><span class="by">swader999</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36869664">parent</a><span>|</span><a href="#36873546">prev</a><span>|</span><a href="#36869132">next</a><span>|</span><label class="collapse" for="c-36870191">[-]</label><label class="expand" for="c-36870191">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s your use case for local if you don&#x27;t mind?</div><br/></div></div></div></div><div id="36869132" class="c"><input type="checkbox" id="c-36869132" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36868905">parent</a><span>|</span><a href="#36869664">prev</a><span>|</span><a href="#36869693">next</a><span>|</span><label class="collapse" for="c-36869132">[-]</label><label class="expand" for="c-36869132">[2 more]</label></div><br/><div class="children"><div class="content">Local memory management will definitely get better in the future.<p>For now:<p>You should have at least 8 GB of RAM to run the 3B models, 16 GB to run the 7B models, and 32 GB to run the 13B models.<p>My personal recommendation is to get as much memory as you can if you want to work with local models [including VRAM if you are planning to be executing on GPU]</div><br/><div id="36873534" class="c"><input type="checkbox" id="c-36873534" checked=""/><div class="controls bullet"><span class="by">bdavbdav</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36869132">parent</a><span>|</span><a href="#36869693">next</a><span>|</span><label class="collapse" for="c-36873534">[-]</label><label class="expand" for="c-36873534">[1 more]</label></div><br/><div class="children"><div class="content">Thanks - the issue I’m facing is the CTO lead times on Macs here!</div><br/></div></div></div></div></div></div><div id="36869693" class="c"><input type="checkbox" id="c-36869693" checked=""/><div class="controls bullet"><span class="by">dividedbyzero</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36868681">parent</a><span>|</span><a href="#36868905">prev</a><span>|</span><a href="#36872504">next</a><span>|</span><label class="collapse" for="c-36869693">[-]</label><label class="expand" for="c-36869693">[1 more]</label></div><br/><div class="children"><div class="content">By lower-end I meant that the Airs are quite low-end in general (compared to Pro&#x2F;Studio). I have the maxed-out 24gb, but 16gb may be more common among people who might use an Air for this kind of thing.</div><br/></div></div><div id="36872504" class="c"><input type="checkbox" id="c-36872504" checked=""/><div class="controls bullet"><span class="by">moneywoes</span><span>|</span><a href="#36867752">root</a><span>|</span><a href="#36868681">parent</a><span>|</span><a href="#36869693">prev</a><span>|</span><a href="#36869903">next</a><span>|</span><label class="collapse" for="c-36872504">[-]</label><label class="expand" for="c-36872504">[1 more]</label></div><br/><div class="children"><div class="content">what about a m1 with 16gb ram?</div><br/></div></div></div></div></div></div></div></div><div id="36869903" class="c"><input type="checkbox" id="c-36869903" checked=""/><div class="controls bullet"><span class="by">guy98238710</span><span>|</span><a href="#36867752">prev</a><span>|</span><a href="#36871997">next</a><span>|</span><label class="collapse" for="c-36869903">[-]</label><label class="expand" for="c-36869903">[16 more]</label></div><br/><div class="children"><div class="content">&gt; curl -L &quot;<a href="https:&#x2F;&#x2F;replicate.fyi&#x2F;install-llama-cpp" rel="nofollow noreferrer">https:&#x2F;&#x2F;replicate.fyi&#x2F;install-llama-cpp</a>&quot; | bash<p>Seriously? Pipe script from someone&#x27;s website directly to bash?</div><br/><div id="36870042" class="c"><input type="checkbox" id="c-36870042" checked=""/><div class="controls bullet"><span class="by">madars</span><span>|</span><a href="#36869903">parent</a><span>|</span><a href="#36870796">next</a><span>|</span><label class="collapse" for="c-36870042">[-]</label><label class="expand" for="c-36870042">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the recommended way to get Rust nightly too: <a href="https:&#x2F;&#x2F;rustup.rs&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;rustup.rs&#x2F;</a> But don&#x27;t look there, there is memory safety somewhere!</div><br/><div id="36873623" class="c"><input type="checkbox" id="c-36873623" checked=""/><div class="controls bullet"><span class="by">creata</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36870042">parent</a><span>|</span><a href="#36870105">next</a><span>|</span><label class="collapse" for="c-36873623">[-]</label><label class="expand" for="c-36873623">[1 more]</label></div><br/><div class="children"><div class="content">In rustup&#x27;s defense, if you&#x27;re already trusting them enough to run their executables, this isn&#x27;t <i>that</i> much worse, afaik.</div><br/></div></div><div id="36870105" class="c"><input type="checkbox" id="c-36870105" checked=""/><div class="controls bullet"><span class="by">raccolta</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36870042">parent</a><span>|</span><a href="#36873623">prev</a><span>|</span><a href="#36870796">next</a><span>|</span><label class="collapse" for="c-36870105">[-]</label><label class="expand" for="c-36870105">[1 more]</label></div><br/><div class="children"><div class="content">oh, this again.</div><br/></div></div></div></div><div id="36870796" class="c"><input type="checkbox" id="c-36870796" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#36869903">parent</a><span>|</span><a href="#36870042">prev</a><span>|</span><a href="#36870165">next</a><span>|</span><label class="collapse" for="c-36870796">[-]</label><label class="expand" for="c-36870796">[3 more]</label></div><br/><div class="children"><div class="content">Either you trust the TLS session to their website to deliver you software you&#x27;re going to run, or you don&#x27;t.</div><br/><div id="36871930" class="c"><input type="checkbox" id="c-36871930" checked=""/><div class="controls bullet"><span class="by">dharmab</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36870796">parent</a><span>|</span><a href="#36870165">next</a><span>|</span><label class="collapse" for="c-36871930">[-]</label><label class="expand" for="c-36871930">[2 more]</label></div><br/><div class="children"><div class="content">You can clone llama.cpp on GitHub and the models from HuggingFace. No need to trust this unrelated website.</div><br/><div id="36872765" class="c"><input type="checkbox" id="c-36872765" checked=""/><div class="controls bullet"><span class="by">ehnto</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36871930">parent</a><span>|</span><a href="#36870165">next</a><span>|</span><label class="collapse" for="c-36872765">[-]</label><label class="expand" for="c-36872765">[1 more]</label></div><br/><div class="children"><div class="content">But is you do trust it, very convenient.</div><br/></div></div></div></div></div></div><div id="36870165" class="c"><input type="checkbox" id="c-36870165" checked=""/><div class="controls bullet"><span class="by">gattilorenz</span><span>|</span><a href="#36869903">parent</a><span>|</span><a href="#36870796">prev</a><span>|</span><a href="#36872790">next</a><span>|</span><label class="collapse" for="c-36870165">[-]</label><label class="expand" for="c-36870165">[1 more]</label></div><br/><div class="children"><div class="content">Yes. If you are worried, you can redirect it to file and then sh it. It doesn’t get much easier to inspect than that…</div><br/></div></div><div id="36872790" class="c"><input type="checkbox" id="c-36872790" checked=""/><div class="controls bullet"><span class="by">mike_ivanov</span><span>|</span><a href="#36869903">parent</a><span>|</span><a href="#36870165">prev</a><span>|</span><a href="#36871882">next</a><span>|</span><label class="collapse" for="c-36872790">[-]</label><label class="expand" for="c-36872790">[1 more]</label></div><br/><div class="children"><div class="content">who doesn&#x27;t love surprises</div><br/></div></div><div id="36871882" class="c"><input type="checkbox" id="c-36871882" checked=""/><div class="controls bullet"><span class="by">dopidopHN</span><span>|</span><a href="#36869903">parent</a><span>|</span><a href="#36872790">prev</a><span>|</span><a href="#36871157">next</a><span>|</span><label class="collapse" for="c-36871882">[-]</label><label class="expand" for="c-36871882">[6 more]</label></div><br/><div class="children"><div class="content">Pretty common. You can inspect the script before piping it.</div><br/><div id="36872956" class="c"><input type="checkbox" id="c-36872956" checked=""/><div class="controls bullet"><span class="by">Evidlo</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36871882">parent</a><span>|</span><a href="#36871157">next</a><span>|</span><label class="collapse" for="c-36872956">[-]</label><label class="expand" for="c-36872956">[5 more]</label></div><br/><div class="children"><div class="content">Bad actors can detect if its being piped to bash and send different data.  Better to just download the script first if you&#x27;re concerned.</div><br/><div id="36873133" class="c"><input type="checkbox" id="c-36873133" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36872956">parent</a><span>|</span><a href="#36871157">next</a><span>|</span><label class="collapse" for="c-36873133">[-]</label><label class="expand" for="c-36873133">[4 more]</label></div><br/><div class="children"><div class="content">How can you detect where someone pipes the output of curl output to?</div><br/><div id="36873314" class="c"><input type="checkbox" id="c-36873314" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36873133">parent</a><span>|</span><a href="#36871157">next</a><span>|</span><label class="collapse" for="c-36873314">[-]</label><label class="expand" for="c-36873314">[3 more]</label></div><br/><div class="children"><div class="content">Basically, bash executes the script line by line as it is downloading - pausing the download while that line executes. By sending a sleep() command early in the script you can detect the delay in the next line beind downloaded.<p>Its a lot more complicated due to TCP buffers and trying to hide output from the user.<p>Original article below. It is giving me a certificate error though but its available through archives or a cache.<p><a href="https:&#x2F;&#x2F;www.idontplaydarts.com&#x2F;2016&#x2F;04&#x2F;detecting-curl-pipe-bash-server-side&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.idontplaydarts.com&#x2F;2016&#x2F;04&#x2F;detecting-curl-pipe-b...</a></div><br/><div id="36874692" class="c"><input type="checkbox" id="c-36874692" checked=""/><div class="controls bullet"><span class="by">column</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36873314">parent</a><span>|</span><a href="#36874089">next</a><span>|</span><label class="collapse" for="c-36874692">[-]</label><label class="expand" for="c-36874692">[1 more]</label></div><br/><div class="children"><div class="content">&quot;This Connection is Invalid. SSL certificate expired.&quot;</div><br/></div></div><div id="36874089" class="c"><input type="checkbox" id="c-36874089" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#36869903">root</a><span>|</span><a href="#36873314">parent</a><span>|</span><a href="#36874692">prev</a><span>|</span><a href="#36871157">next</a><span>|</span><label class="collapse" for="c-36874089">[-]</label><label class="expand" for="c-36874089">[1 more]</label></div><br/><div class="children"><div class="content">Amazing, thanks.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36871157" class="c"><input type="checkbox" id="c-36871157" checked=""/><div class="controls bullet"><span class="by">alexgartrell</span><span>|</span><a href="#36869903">parent</a><span>|</span><a href="#36871882">prev</a><span>|</span><a href="#36871997">next</a><span>|</span><label class="collapse" for="c-36871157">[-]</label><label class="expand" for="c-36871157">[1 more]</label></div><br/><div class="children"><div class="content">IMO this is equivalently scary to installing an arbitrary rpm.</div><br/></div></div></div></div><div id="36871997" class="c"><input type="checkbox" id="c-36871997" checked=""/><div class="controls bullet"><span class="by">aledalgrande</span><span>|</span><a href="#36869903">prev</a><span>|</span><a href="#36873790">next</a><span>|</span><label class="collapse" for="c-36871997">[-]</label><label class="expand" for="c-36871997">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t remember if the grammar has been merged in llama.cpp yet but it would be the first step to have Llama + Stable diffusion locally to output text + images and talk to each other. The only part I&#x27;m not sure is how Llama would interpret images back. At least it could use them though, to build e.g. a webpage.</div><br/><div id="36872153" class="c"><input type="checkbox" id="c-36872153" checked=""/><div class="controls bullet"><span class="by">jakedahn</span><span>|</span><a href="#36871997">parent</a><span>|</span><a href="#36873790">next</a><span>|</span><label class="collapse" for="c-36872153">[-]</label><label class="expand" for="c-36872153">[1 more]</label></div><br/><div class="children"><div class="content">It has merged! <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1773">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1773</a><p>I haven&#x27;t had a chance to try it yet, but I am :excitedllama:</div><br/></div></div></div></div><div id="36873790" class="c"><input type="checkbox" id="c-36873790" checked=""/><div class="controls bullet"><span class="by">jossclimb</span><span>|</span><a href="#36871997">prev</a><span>|</span><a href="#36872712">next</a><span>|</span><label class="collapse" for="c-36873790">[-]</label><label class="expand" for="c-36873790">[1 more]</label></div><br/><div class="children"><div class="content">Seems to be a better guide here (without the risk curl):<p><a href="https:&#x2F;&#x2F;www.stacklok.com&#x2F;post&#x2F;exploring-llama-2-on-a-apple-mac-m1-m2" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.stacklok.com&#x2F;post&#x2F;exploring-llama-2-on-a-apple-m...</a></div><br/></div></div><div id="36872712" class="c"><input type="checkbox" id="c-36872712" checked=""/><div class="controls bullet"><span class="by">ericHosick</span><span>|</span><a href="#36873790">prev</a><span>|</span><a href="#36870470">next</a><span>|</span><label class="collapse" for="c-36872712">[-]</label><label class="expand" for="c-36872712">[4 more]</label></div><br/><div class="children"><div class="content">The LLM is impressive (llama2:13b) but appears to have been greatly limited to what you are allowed to do with it.<p>I tried to get it to generate a JSON object about the movie The Matrix and the model refuses.</div><br/><div id="36872732" class="c"><input type="checkbox" id="c-36872732" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#36872712">parent</a><span>|</span><a href="#36870470">next</a><span>|</span><label class="collapse" for="c-36872732">[-]</label><label class="expand" for="c-36872732">[3 more]</label></div><br/><div class="children"><div class="content">In case you might be trying this with Ollama (given the model:tag syntax) – I haven&#x27;t added this to the readme yet (seeing some issues with the prompt template) but check out llama2-uncensored:<p><pre><code>  ollama run llama2-uncensored</code></pre></div><br/><div id="36872747" class="c"><input type="checkbox" id="c-36872747" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#36872712">root</a><span>|</span><a href="#36872732">parent</a><span>|</span><a href="#36870470">next</a><span>|</span><label class="collapse" for="c-36872747">[-]</label><label class="expand" for="c-36872747">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  % ollama run llama2-uncensored
  &gt;&gt;&gt; Generate a json object describing the most stunning part of the movie: &quot;The Matrix&quot;
  {   
    &quot;title&quot;: &quot;The Matrix&quot;,
    &quot;director&quot;: &quot;The Wachowskis&quot;,
    &quot;release_year&quot;: 1999,
    &quot;description&quot;: &quot;Set in a dystopian future, The Matrix follows the story of Neo (Keanu Reeves), a computer hacker who is drawn into an underground war against machines that have enslaved humanity. Along with his allies Morpheus (Laurence Fishburne) and Trinity (Carrie-Anne Moss), Neo learns to harness the power of the Matrix, a virtual reality system designed to keep humans obedient and ignorant. With the help of a rogue program named Agent Smith (Hugo Weaving), Neo discovers the true nature of the Matrix and sets out on a journey to free humankind from their digital prison.&quot;,
    &quot;stunning scenes&quot;: [
      &quot;bulletproof kung-fu fighting&quot;,
      &quot;bullet time&quot;,
      &quot;dream-like imagery&quot;,
      &quot;special effects&quot;,
    ]
  }
</code></pre>
edit: formatting</div><br/><div id="36873414" class="c"><input type="checkbox" id="c-36873414" checked=""/><div class="controls bullet"><span class="by">oidar</span><span>|</span><a href="#36872712">root</a><span>|</span><a href="#36872747">parent</a><span>|</span><a href="#36870470">next</a><span>|</span><label class="collapse" for="c-36873414">[-]</label><label class="expand" for="c-36873414">[1 more]</label></div><br/><div class="children"><div class="content">Are you going to add a &quot;stop&quot; argument to the API?</div><br/></div></div></div></div></div></div></div></div><div id="36870470" class="c"><input type="checkbox" id="c-36870470" checked=""/><div class="controls bullet"><span class="by">maxlin</span><span>|</span><a href="#36872712">prev</a><span>|</span><a href="#36869330">next</a><span>|</span><label class="collapse" for="c-36870470">[-]</label><label class="expand" for="c-36870470">[6 more]</label></div><br/><div class="children"><div class="content">I might be missing something. The article asks me to run a bash script on windows.<p>I assume this would still need to be run manually to access GPU resources etc, so can someone illuminate what is actually expected for a windows user to make this run?<p>I&#x27;m currently paying 15$ a month in a personal translation&#x2F;summarizer project&#x27;s ChatGPT queries. I run whisper (const.me&#x27;s GPU fork) locally and would love to get the LLM part local eventually too! The system generates 30k queries a month but is not super-affected by delay so lower token rates might work too.</div><br/><div id="36871080" class="c"><input type="checkbox" id="c-36871080" checked=""/><div class="controls bullet"><span class="by">Charlieholtz</span><span>|</span><a href="#36870470">parent</a><span>|</span><a href="#36870585">next</a><span>|</span><label class="collapse" for="c-36871080">[-]</label><label class="expand" for="c-36871080">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for pointing this out — we should&#x27;ve pointed out the script needs to be run on WSL. I added a note to the post to clarify (I work at Replicate).<p>Also, you don&#x27;t need a GPU to run this script! It builds and runs llama.cpp <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a></div><br/></div></div><div id="36870585" class="c"><input type="checkbox" id="c-36870585" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#36870470">parent</a><span>|</span><a href="#36871080">prev</a><span>|</span><a href="#36871543">next</a><span>|</span><label class="collapse" for="c-36870585">[-]</label><label class="expand" for="c-36870585">[2 more]</label></div><br/><div class="children"><div class="content">Windows has supported linux tools for some time now, using WSL: <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;wsl&#x2F;about" rel="nofollow noreferrer">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;wsl&#x2F;about</a><p>No idea if it will work, in this case, but it does with llama.cpp: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;103">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;103</a></div><br/><div id="36870645" class="c"><input type="checkbox" id="c-36870645" checked=""/><div class="controls bullet"><span class="by">maxlin</span><span>|</span><a href="#36870470">root</a><span>|</span><a href="#36870585">parent</a><span>|</span><a href="#36871543">next</a><span>|</span><label class="collapse" for="c-36870645">[-]</label><label class="expand" for="c-36870645">[1 more]</label></div><br/><div class="children"><div class="content">I know (should have included in my earlier response but editing would&#x27;ve felt weird) but I still assume one should run the result natively, so am asking if&#x2F;where there&#x27;s some jumping around required.<p>Last time I tried running an LLM I tried wsl&amp;native both on 2 machines and just got lovecraftian-tier errors so waiting if I&#x27;m missing something obvious before going down that route again</div><br/></div></div></div></div><div id="36871543" class="c"><input type="checkbox" id="c-36871543" checked=""/><div class="controls bullet"><span class="by">mattstir</span><span>|</span><a href="#36870470">parent</a><span>|</span><a href="#36870585">prev</a><span>|</span><a href="#36871736">next</a><span>|</span><label class="collapse" for="c-36871543">[-]</label><label class="expand" for="c-36871543">[1 more]</label></div><br/><div class="children"><div class="content">The bash script is downloading llama.cpp, a project which allows you to run LLaMA-based language models on your CPU. The bash script then downloads the 13 billion parameter GGML version of LLaMA 2. The GGML version is what will work with llama.cpp and uses CPU for inferencing. There are ways to run models using your GPU, but it depends on your setup whether it will be worth it.<p>I would highly recommend looking into the text-generation-webui project (<a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a>). It has a one-click installer and very comprehensive guides for getting models running locally and where to find models. The project also has an &quot;api&quot; command flag to let you use it like you might use a web-based service currently.</div><br/></div></div><div id="36871736" class="c"><input type="checkbox" id="c-36871736" checked=""/><div class="controls bullet"><span class="by">shortrounddev2</span><span>|</span><a href="#36870470">parent</a><span>|</span><a href="#36871543">prev</a><span>|</span><a href="#36869330">next</a><span>|</span><label class="collapse" for="c-36871736">[-]</label><label class="expand" for="c-36871736">[1 more]</label></div><br/><div class="children"><div class="content">check my comment elsewhere in the thread, I wrote build instructions for windows + Nvidia GPUs</div><br/></div></div></div></div><div id="36869330" class="c"><input type="checkbox" id="c-36869330" checked=""/><div class="controls bullet"><span class="by">oaththrowaway</span><span>|</span><a href="#36870470">prev</a><span>|</span><a href="#36871354">next</a><span>|</span><label class="collapse" for="c-36869330">[-]</label><label class="expand" for="c-36869330">[6 more]</label></div><br/><div class="children"><div class="content">Off topic: is there a way to use one of the LLMs and have it ingest data from a SQLite database and ask it questions about it?</div><br/><div id="36870367" class="c"><input type="checkbox" id="c-36870367" checked=""/><div class="controls bullet"><span class="by">siquick</span><span>|</span><a href="#36869330">parent</a><span>|</span><a href="#36869768">next</a><span>|</span><label class="collapse" for="c-36870367">[-]</label><label class="expand" for="c-36870367">[1 more]</label></div><br/><div class="children"><div class="content">You can migrate that data to a vector database (eg Pinecone or pgVector) and then query it. I didn’t write it but this guide has a good overview of concepts and some code. In your case your just replace the web crawler with database queries. All the libraries used also exist in Python.<p><a href="https:&#x2F;&#x2F;www.pinecone.io&#x2F;learn&#x2F;javascript-chatbot&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.pinecone.io&#x2F;learn&#x2F;javascript-chatbot&#x2F;</a><p>Edit: this might also be of use<p><a href="https:&#x2F;&#x2F;python.langchain.com&#x2F;docs&#x2F;modules&#x2F;chains&#x2F;popular&#x2F;sqlite" rel="nofollow noreferrer">https:&#x2F;&#x2F;python.langchain.com&#x2F;docs&#x2F;modules&#x2F;chains&#x2F;popular&#x2F;sql...</a></div><br/></div></div><div id="36869768" class="c"><input type="checkbox" id="c-36869768" checked=""/><div class="controls bullet"><span class="by">seanthemon</span><span>|</span><a href="#36869330">parent</a><span>|</span><a href="#36870367">prev</a><span>|</span><a href="#36870691">next</a><span>|</span><label class="collapse" for="c-36869768">[-]</label><label class="expand" for="c-36869768">[1 more]</label></div><br/><div class="children"><div class="content">You can, but as a crazy idea you can also ask chatgpt to write select queries using the functions parameter they added recently - you can also ask it to write jsonpath.<p>As long as it understands the schema and general idea of data, it does a fairly good job. Just be careful to do too much with one prompt, you can easily cause hallucinations</div><br/></div></div><div id="36870691" class="c"><input type="checkbox" id="c-36870691" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#36869330">parent</a><span>|</span><a href="#36869768">prev</a><span>|</span><a href="#36869791">next</a><span>|</span><label class="collapse" for="c-36870691">[-]</label><label class="expand" for="c-36870691">[1 more]</label></div><br/><div class="children"><div class="content">Have a look at this too, it&#x27;s just an integration which langchain can be good at : <a href="https:&#x2F;&#x2F;walkingtree.tech&#x2F;natural-language-to-query-your-sql-database-using-langchain-powered-by-llms&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;walkingtree.tech&#x2F;natural-language-to-query-your-sql-...</a></div><br/></div></div><div id="36869791" class="c"><input type="checkbox" id="c-36869791" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36869330">parent</a><span>|</span><a href="#36870691">prev</a><span>|</span><a href="#36869623">next</a><span>|</span><label class="collapse" for="c-36869791">[-]</label><label class="expand" for="c-36869791">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve experimented with that a bit.<p>Currently the absolutely best way to do that is to upload a SQLite database file to ChatGPT Code Interpreter.<p>I&#x27;m hoping that someone will fine-tune an openly licensed model for this at some point that can give results as good as Code Interpreter does.</div><br/></div></div><div id="36869623" class="c"><input type="checkbox" id="c-36869623" checked=""/><div class="controls bullet"><span class="by">thisisit</span><span>|</span><a href="#36869330">parent</a><span>|</span><a href="#36869791">prev</a><span>|</span><a href="#36871354">next</a><span>|</span><label class="collapse" for="c-36869623">[-]</label><label class="expand" for="c-36869623">[1 more]</label></div><br/><div class="children"><div class="content">You can but what you’ll end up trading precise answers while querying to a chance of hallucinations.</div><br/></div></div></div></div><div id="36871354" class="c"><input type="checkbox" id="c-36871354" checked=""/><div class="controls bullet"><span class="by">nravic</span><span>|</span><a href="#36869330">prev</a><span>|</span><a href="#36871629">next</a><span>|</span><label class="collapse" for="c-36871354">[-]</label><label class="expand" for="c-36871354">[2 more]</label></div><br/><div class="children"><div class="content">Self plug: run llama.cpp as an inference server on a spot instance anywhere: <a href="https:&#x2F;&#x2F;cedana.readthedocs.io&#x2F;en&#x2F;latest&#x2F;examples.html#running-llama-cpp-inference" rel="nofollow noreferrer">https:&#x2F;&#x2F;cedana.readthedocs.io&#x2F;en&#x2F;latest&#x2F;examples.html#runnin...</a></div><br/><div id="36872467" class="c"><input type="checkbox" id="c-36872467" checked=""/><div class="controls bullet"><span class="by">dabei</span><span>|</span><a href="#36871354">parent</a><span>|</span><a href="#36871629">next</a><span>|</span><label class="collapse" for="c-36872467">[-]</label><label class="expand" for="c-36872467">[1 more]</label></div><br/><div class="children"><div class="content">Looks cool, joined the waitlist.</div><br/></div></div></div></div><div id="36871629" class="c"><input type="checkbox" id="c-36871629" checked=""/><div class="controls bullet"><span class="by">nonethewiser</span><span>|</span><a href="#36871354">prev</a><span>|</span><a href="#36870843">next</a><span>|</span><label class="collapse" for="c-36871629">[-]</label><label class="expand" for="c-36871629">[4 more]</label></div><br/><div class="children"><div class="content">Maybe obvious to others, but the 1 line install command with curl is taking a long time. Must be the build step. Probably 40+ minutes now on an M2 max.</div><br/><div id="36871925" class="c"><input type="checkbox" id="c-36871925" checked=""/><div class="controls bullet"><span class="by">dharmab</span><span>|</span><a href="#36871629">parent</a><span>|</span><a href="#36870843">next</a><span>|</span><label class="collapse" for="c-36871925">[-]</label><label class="expand" for="c-36871925">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s odd, the build step only took me a few minutes on an 5900X on Linux.<p>EDIT: Timed a clean build at 30 seconds.</div><br/><div id="36872079" class="c"><input type="checkbox" id="c-36872079" checked=""/><div class="controls bullet"><span class="by">nonethewiser</span><span>|</span><a href="#36871629">root</a><span>|</span><a href="#36871925">parent</a><span>|</span><a href="#36870843">next</a><span>|</span><label class="collapse" for="c-36872079">[-]</label><label class="expand" for="c-36872079">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the info, wonder what the deal is</div><br/><div id="36872132" class="c"><input type="checkbox" id="c-36872132" checked=""/><div class="controls bullet"><span class="by">dharmab</span><span>|</span><a href="#36871629">root</a><span>|</span><a href="#36872079">parent</a><span>|</span><a href="#36870843">next</a><span>|</span><label class="collapse" for="c-36872132">[-]</label><label class="expand" for="c-36872132">[1 more]</label></div><br/><div class="children"><div class="content">I did manually clone from GitHub and build myself and download the model separately. I also noticed that many of the CLI flags the author chose are questionable after I read the docs and help text.</div><br/></div></div></div></div></div></div></div></div><div id="36870843" class="c"><input type="checkbox" id="c-36870843" checked=""/><div class="controls bullet"><span class="by">TheAceOfHearts</span><span>|</span><a href="#36871629">prev</a><span>|</span><a href="#36870594">next</a><span>|</span><label class="collapse" for="c-36870843">[-]</label><label class="expand" for="c-36870843">[2 more]</label></div><br/><div class="children"><div class="content">How do you decide what model variant to use? There&#x27;s a bunch of Quant method variations of Llama-2-13B-chat-GGML [0], how do you know which one to use? Reading the &quot;Explanation of the new k-quant methods&quot; is a bit opaque.<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-13B-chat-GGML" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-13B-chat-GGML</a></div><br/><div id="36871110" class="c"><input type="checkbox" id="c-36871110" checked=""/><div class="controls bullet"><span class="by">Charlieholtz</span><span>|</span><a href="#36870843">parent</a><span>|</span><a href="#36870594">next</a><span>|</span><label class="collapse" for="c-36871110">[-]</label><label class="expand" for="c-36871110">[1 more]</label></div><br/><div class="children"><div class="content">This is a great question. My best answer is that there&#x27;s a speed&#x2F;intelligence trade-off. The smaller weights (7B) will run faster and require less memory, but you won&#x27;t get the same quality responses as the 13B &#x2F; 70B model. I think there may be a Llama 30B variant coming soon too.</div><br/></div></div></div></div><div id="36870594" class="c"><input type="checkbox" id="c-36870594" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36870843">prev</a><span>|</span><a href="#36871455">next</a><span>|</span><label class="collapse" for="c-36870594">[-]</label><label class="expand" for="c-36870594">[1 more]</label></div><br/><div class="children"><div class="content">If you just want to do inference&#x2F;mess around with the model and have a 16GB GPU, then this[0] is enough to paste into a notebook. You need to have access to the HF models though.<p>0. <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;blog&#x2F;blob&#x2F;main&#x2F;llama2.md#using-transformers">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;blog&#x2F;blob&#x2F;main&#x2F;llama2.md#usin...</a></div><br/></div></div><div id="36871455" class="c"><input type="checkbox" id="c-36871455" checked=""/><div class="controls bullet"><span class="by">alvincodes</span><span>|</span><a href="#36870594">prev</a><span>|</span><a href="#36873505">next</a><span>|</span><label class="collapse" for="c-36871455">[-]</label><label class="expand" for="c-36871455">[2 more]</label></div><br/><div class="children"><div class="content">I appreciate their honesty when it&#x27;s in their interest that people use their API rather than run it locally.</div><br/><div id="36871627" class="c"><input type="checkbox" id="c-36871627" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36871455">parent</a><span>|</span><a href="#36873505">next</a><span>|</span><label class="collapse" for="c-36871627">[-]</label><label class="expand" for="c-36871627">[1 more]</label></div><br/><div class="children"><div class="content">This is classic blogging strategy. And the right way to do it. And few do. Which is why I ignore corporate blogs in general.</div><br/></div></div></div></div><div id="36873505" class="c"><input type="checkbox" id="c-36873505" checked=""/><div class="controls bullet"><span class="by">technological</span><span>|</span><a href="#36871455">prev</a><span>|</span><a href="#36869653">next</a><span>|</span><label class="collapse" for="c-36873505">[-]</label><label class="expand" for="c-36873505">[3 more]</label></div><br/><div class="children"><div class="content">Did anyone build pc for running these models and which one do you recommend</div><br/><div id="36874191" class="c"><input type="checkbox" id="c-36874191" checked=""/><div class="controls bullet"><span class="by">cfn</span><span>|</span><a href="#36873505">parent</a><span>|</span><a href="#36869653">next</a><span>|</span><label class="collapse" for="c-36874191">[-]</label><label class="expand" for="c-36874191">[2 more]</label></div><br/><div class="children"><div class="content">I assume you are talking about a Windows&#x2F;Linux PC. I have done that and got a Threadripper with 32 cores and 256Gb RAM. It runs any llama on the CPU although the 65&#x2F;70b are quite slow. I also added an A6000 (48Gb VRAM) that allows you to run the 65&#x2F;70b quantized with very good performance.<p>If you are going with the GPU and don&#x27;t care about loads of RAM then a 16 Zen CPU will do just fine (or Intel for that matter).<p>If you are only interested in llama only then an M1 Studio with 64Gb RAM is probably cheaper and will work just as well.</div><br/><div id="36874755" class="c"><input type="checkbox" id="c-36874755" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#36873505">root</a><span>|</span><a href="#36874191">parent</a><span>|</span><a href="#36869653">next</a><span>|</span><label class="collapse" for="c-36874755">[-]</label><label class="expand" for="c-36874755">[1 more]</label></div><br/><div class="children"><div class="content">Hi, so what are you using the models for?</div><br/></div></div></div></div></div></div><div id="36869653" class="c"><input type="checkbox" id="c-36869653" checked=""/><div class="controls bullet"><span class="by">handelaar</span><span>|</span><a href="#36873505">prev</a><span>|</span><a href="#36869039">next</a><span>|</span><label class="collapse" for="c-36869653">[-]</label><label class="expand" for="c-36869653">[5 more]</label></div><br/><div class="children"><div class="content">Idiot question:  if I have access to sentence-by-sentence professionally-translated text of foreign-language-to-English in gigantic quantities, and I fed the originals as prompts and the translations as completions...<p>... would I be likely to get anything useful if I then fed it new prompts in a similar style?  Or would it just generate gibberish?</div><br/><div id="36869708" class="c"><input type="checkbox" id="c-36869708" checked=""/><div class="controls bullet"><span class="by">seanthemon</span><span>|</span><a href="#36869653">parent</a><span>|</span><a href="#36870754">next</a><span>|</span><label class="collapse" for="c-36869708">[-]</label><label class="expand" for="c-36869708">[3 more]</label></div><br/><div class="children"><div class="content">Indeed, it sounds like you have what&#x27;s called fine tuned data (given an input, here&#x27;s the output), there&#x27;s loads of info both here on HN about fine tuning and on youtube&#x27;s huggingface channels<p>Note if you have sufficient data, look into existing models on huggingface, you may find a smaller, faster and more open (licencing-wise) model that you can fine tune to get the results you want - Llama is hot, but not a catch-all for all tasks (as no model should be)<p>Happy inferring!</div><br/><div id="36871497" class="c"><input type="checkbox" id="c-36871497" checked=""/><div class="controls bullet"><span class="by">zakki</span><span>|</span><a href="#36869653">root</a><span>|</span><a href="#36869708">parent</a><span>|</span><a href="#36870754">next</a><span>|</span><label class="collapse" for="c-36871497">[-]</label><label class="expand" for="c-36871497">[2 more]</label></div><br/><div class="children"><div class="content">How to know if the data is sufficient?</div><br/><div id="36873379" class="c"><input type="checkbox" id="c-36873379" checked=""/><div class="controls bullet"><span class="by">seanthemon</span><span>|</span><a href="#36869653">root</a><span>|</span><a href="#36871497">parent</a><span>|</span><a href="#36870754">next</a><span>|</span><label class="collapse" for="c-36873379">[-]</label><label class="expand" for="c-36873379">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more about quality vs sufficiency - you can have a relatively small but accurate and wide ranging dataset, this is better than an inaccurate huge dataset</div><br/></div></div></div></div></div></div><div id="36870754" class="c"><input type="checkbox" id="c-36870754" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36869653">parent</a><span>|</span><a href="#36869708">prev</a><span>|</span><a href="#36869039">next</a><span>|</span><label class="collapse" for="c-36870754">[-]</label><label class="expand" for="c-36870754">[1 more]</label></div><br/><div class="children"><div class="content">If you have that much data you can build your own model that can be much smaller and faster.<p>A simple version is a beginner tutorial: <a href="https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;translation_transformer.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;translation_transform...</a></div><br/></div></div></div></div><div id="36869039" class="c"><input type="checkbox" id="c-36869039" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36869653">prev</a><span>|</span><a href="#36868941">next</a><span>|</span><label class="collapse" for="c-36869039">[-]</label><label class="expand" for="c-36869039">[3 more]</label></div><br/><div class="children"><div class="content">The correct answer, as always, is the oogabooga text generation webUI, which supports all of the relevant backends: <a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a></div><br/><div id="36869583" class="c"><input type="checkbox" id="c-36869583" checked=""/><div class="controls bullet"><span class="by">cypress66</span><span>|</span><a href="#36869039">parent</a><span>|</span><a href="#36868941">next</a><span>|</span><label class="collapse" for="c-36869583">[-]</label><label class="expand" for="c-36869583">[2 more]</label></div><br/><div class="children"><div class="content">Yep. Use ooba. And people who like to RP often use ooba as a backend, and sillitavern as a frontend.</div><br/><div id="36870427" class="c"><input type="checkbox" id="c-36870427" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#36869039">root</a><span>|</span><a href="#36869583">parent</a><span>|</span><a href="#36868941">next</a><span>|</span><label class="collapse" for="c-36870427">[-]</label><label class="expand" for="c-36870427">[1 more]</label></div><br/><div class="children"><div class="content">Can it run onnx transformer models? I found optimised onnx models are at least twice the speed of vanilla pytorch on the CPU.</div><br/></div></div></div></div></div></div><div id="36868941" class="c"><input type="checkbox" id="c-36868941" checked=""/><div class="controls bullet"><span class="by">nomand</span><span>|</span><a href="#36869039">prev</a><span>|</span><a href="#36870387">next</a><span>|</span><label class="collapse" for="c-36868941">[-]</label><label class="expand" for="c-36868941">[9 more]</label></div><br/><div class="children"><div class="content">Is it possible for such local install to retain conversation history so if for example you&#x27;re working on a project and use it as your assistance across many days that you can continue conversations and for the model to keep track of what you and it already know?</div><br/><div id="36869801" class="c"><input type="checkbox" id="c-36869801" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36868941">parent</a><span>|</span><a href="#36869113">next</a><span>|</span><label class="collapse" for="c-36869801">[-]</label><label class="expand" for="c-36869801">[1 more]</label></div><br/><div class="children"><div class="content">My LLM command line tool can do that - it logs everything to a SQLite database and has an option to continue a conversation: <a href="https:&#x2F;&#x2F;llm.datasette.io" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.datasette.io</a></div><br/></div></div><div id="36869113" class="c"><input type="checkbox" id="c-36869113" checked=""/><div class="controls bullet"><span class="by">jmiskovic</span><span>|</span><a href="#36868941">parent</a><span>|</span><a href="#36869801">prev</a><span>|</span><a href="#36869043">next</a><span>|</span><label class="collapse" for="c-36869113">[-]</label><label class="expand" for="c-36869113">[6 more]</label></div><br/><div class="children"><div class="content">There is no fully built solution, only bits and pieces. I noticed that llama outputs tend to degrade with amount of text, the text becomes too repetitive and focused, and you have to raise the temperature to break the model out of loops.</div><br/><div id="36869182" class="c"><input type="checkbox" id="c-36869182" checked=""/><div class="controls bullet"><span class="by">nomand</span><span>|</span><a href="#36868941">root</a><span>|</span><a href="#36869113">parent</a><span>|</span><a href="#36869043">next</a><span>|</span><label class="collapse" for="c-36869182">[-]</label><label class="expand" for="c-36869182">[5 more]</label></div><br/><div class="children"><div class="content">Does what you&#x27;re saying mean you can only ask questions and get answers in a single step, and that having a long discussion where refinement of output is arrived at through conversation isn&#x27;t possible?</div><br/><div id="36869744" class="c"><input type="checkbox" id="c-36869744" checked=""/><div class="controls bullet"><span class="by">krisoft</span><span>|</span><a href="#36868941">root</a><span>|</span><a href="#36869182">parent</a><span>|</span><a href="#36869043">next</a><span>|</span><label class="collapse" for="c-36869744">[-]</label><label class="expand" for="c-36869744">[4 more]</label></div><br/><div class="children"><div class="content">My understanding is that at a high level you can look at this model as a black box which accepts a string and outputs a string.<p>If you want it to “remember” things you do that by appending all the previous conversations together and supply it in the input string.<p>In an ideal world this would work perfectly. It would read through the whole conversation and would provide the right output you expect, exactly as if it would “remember” the conversation. In reality there are all kind of issues which can crop up as the input grows longer and longer. One is that it takes more and more processing power and time for it to “read through” everything previously said. And there are things like what jmiskovic said that the output quality can also degrade in perhaps unexpected ways.<p>But that also doesn’t mean that “ refinement of output is arrived at through conversation isn&#x27;t possible”. It is not that black and white, just that you can run into troubles as the length of the discussion grows.<p>I don’t have direct experience with long conversations so I can’t tell you how long is definietly too long, and how long is still safe. Plus probably there are some tricks one can do to work around these. Probably there are things one can do if one unpacks that “black box” understanding of the process. But even without that you could imagine a “consolidation” process where the AI is instructed to write short notes about a given length of conversation and then those shorter notes would be copied in to the next input instead of the full previous conversation. All of these are possible, but you won’t have a turn-key solution for it just yet.</div><br/><div id="36870826" class="c"><input type="checkbox" id="c-36870826" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#36868941">root</a><span>|</span><a href="#36869744">parent</a><span>|</span><a href="#36869819">next</a><span>|</span><label class="collapse" for="c-36870826">[-]</label><label class="expand" for="c-36870826">[1 more]</label></div><br/><div class="children"><div class="content">The limit here is the &quot;context window&quot; length of the model, measured in tokens, which will quickly become too short to contain all of your previous conversations, which will mean it has to answer questions without access to all of that text.  And within a single conversation, it will mean that it starts forgetting the text from the start of the conversation, once the [conversation + new prompt] reaches the context length.<p>The kind of hacks that work around this are to train the model on the past conversations, and then rely on similarity in tensor space to pull the right (lossy) data back out of the model (or a separate database) later, based on its similarity to your question, and include it (or a summary of it, since summaries are smaller) within the context window for your new conversation, combined with your prompt.  This is what people are talking about when they use the term &quot;embeddings&quot;.</div><br/></div></div><div id="36869819" class="c"><input type="checkbox" id="c-36869819" checked=""/><div class="controls bullet"><span class="by">nomand</span><span>|</span><a href="#36868941">root</a><span>|</span><a href="#36869744">parent</a><span>|</span><a href="#36870826">prev</a><span>|</span><a href="#36869043">next</a><span>|</span><label class="collapse" for="c-36869819">[-]</label><label class="expand" for="c-36869819">[2 more]</label></div><br/><div class="children"><div class="content">My benchmark is having a peer programming session spanning days and dozens of queries with ChatGPT where we co-created a custom static site generator that works really well for my requirements. It was able to hold context for a while and not &quot;forget&quot; what code it provided me dozens of messages earlier, it was able to &quot;remember&quot; corrections and refactors that I gave it and overall was incredibly useful for working out things like recurrence for folder hierarchies and building data trees. This kind and similar use-cases where memory is important, when the model is used as a genuine assistant.</div><br/><div id="36870490" class="c"><input type="checkbox" id="c-36870490" checked=""/><div class="controls bullet"><span class="by">krisoft</span><span>|</span><a href="#36868941">root</a><span>|</span><a href="#36869819">parent</a><span>|</span><a href="#36869043">next</a><span>|</span><label class="collapse" for="c-36870490">[-]</label><label class="expand" for="c-36870490">[1 more]</label></div><br/><div class="children"><div class="content">Excelent! That sounds like a very usefull personal benchmark then. You could test llama v2 by copying in different lengths of snippets from that conversation and checking how usefull you find its outputs.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36869043" class="c"><input type="checkbox" id="c-36869043" checked=""/><div class="controls bullet"><span class="by">knodi123</span><span>|</span><a href="#36868941">parent</a><span>|</span><a href="#36869113">prev</a><span>|</span><a href="#36870387">next</a><span>|</span><label class="collapse" for="c-36869043">[-]</label><label class="expand" for="c-36869043">[1 more]</label></div><br/><div class="children"><div class="content">llama is just an input&#x2F;output engine.  It takes a big string as input, and gives a big string of output.<p>Save your outputs if you want, you can copy&#x2F;paste them into any editor.   Or make a shell script that mirrors outputs to a file and use <i>that</i> as your main interface.  It&#x27;s up to the user.</div><br/></div></div></div></div><div id="36870387" class="c"><input type="checkbox" id="c-36870387" checked=""/><div class="controls bullet"><span class="by">synaesthesisx</span><span>|</span><a href="#36868941">prev</a><span>|</span><a href="#36871615">next</a><span>|</span><label class="collapse" for="c-36870387">[-]</label><label class="expand" for="c-36870387">[1 more]</label></div><br/><div class="children"><div class="content">This is usable, but hopefully folks manage to tweak it a bit further for even higher tokens&#x2F;s. I’m running Llama.cpp locally on my M2 Max (32 GB) with decent performance but sticking to the 7B model for now.</div><br/></div></div><div id="36871615" class="c"><input type="checkbox" id="c-36871615" checked=""/><div class="controls bullet"><span class="by">theLiminator</span><span>|</span><a href="#36870387">prev</a><span>|</span><a href="#36870760">next</a><span>|</span><label class="collapse" for="c-36871615">[-]</label><label class="expand" for="c-36871615">[1 more]</label></div><br/><div class="children"><div class="content">Is it possible to do hybrid inference if I have a 24GB card with the 70B model? Ie. Offload some of it to my RAM?</div><br/></div></div><div id="36870760" class="c"><input type="checkbox" id="c-36870760" checked=""/><div class="controls bullet"><span class="by">RicoElectrico</span><span>|</span><a href="#36871615">prev</a><span>|</span><a href="#36870622">next</a><span>|</span><label class="collapse" for="c-36870760">[-]</label><label class="expand" for="c-36870760">[4 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    curl -L &quot;https:&#x2F;&#x2F;replicate.fyi&#x2F;windows-install-llama-cpp&quot;
</code></pre>
... returns 404 Not Found</div><br/><div id="36871079" class="c"><input type="checkbox" id="c-36871079" checked=""/><div class="controls bullet"><span class="by">skeletonjelly</span><span>|</span><a href="#36870760">parent</a><span>|</span><a href="#36871112">next</a><span>|</span><label class="collapse" for="c-36871079">[-]</label><label class="expand" for="c-36871079">[1 more]</label></div><br/><div class="children"><div class="content">Looks like it&#x27;s fixed now, I had the same 404 for a while</div><br/></div></div><div id="36871112" class="c"><input type="checkbox" id="c-36871112" checked=""/><div class="controls bullet"><span class="by">Charlieholtz</span><span>|</span><a href="#36870760">parent</a><span>|</span><a href="#36871079">prev</a><span>|</span><a href="#36870622">next</a><span>|</span><label class="collapse" for="c-36871112">[-]</label><label class="expand" for="c-36871112">[2 more]</label></div><br/><div class="children"><div class="content">Whoops! Fixed now</div><br/><div id="36871787" class="c"><input type="checkbox" id="c-36871787" checked=""/><div class="controls bullet"><span class="by">shortrounddev2</span><span>|</span><a href="#36870760">root</a><span>|</span><a href="#36871112">parent</a><span>|</span><a href="#36870622">next</a><span>|</span><label class="collapse" for="c-36871787">[-]</label><label class="expand" for="c-36871787">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need bash or WSL to build on windows; windows runs it perfectly fine with CUDA as well</div><br/></div></div></div></div></div></div><div id="36870622" class="c"><input type="checkbox" id="c-36870622" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#36870760">prev</a><span>|</span><label class="collapse" for="c-36870622">[-]</label><label class="expand" for="c-36870622">[1 more]</label></div><br/><div class="children"><div class="content">Llama.cpp can run on Android too.</div><br/></div></div></div></div></div></div></div></body></html>