<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707987667563" as="style"/><link rel="stylesheet" href="styles.css?v=1707987667563"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.07350">Antagonistic AI</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>drcwpl</span> | <span>38 comments</span></div><br/><div><div id="39379507" class="c"><input type="checkbox" id="c-39379507" checked=""/><div class="controls bullet"><span class="by">BeetleB</span><span>|</span><a href="#39380503">next</a><span>|</span><label class="collapse" for="c-39379507">[-]</label><label class="expand" for="c-39379507">[9 more]</label></div><br/><div class="children"><div class="content">Definitely need some antagonism.<p>I remember reading an article on one person&#x27;s experience with Claude. He found it to be <i>too</i> agreeable. So he decided to challenge it. If he claimed that 1+1 = 3, would it disagree?<p>Indeed, it did.<p>And then he countered with &quot;But I&#x27;ve read arguments where 1+1 is <i>not</i> 2.&quot; Claude responded with &quot;Yes, apologies...&quot; and then gave a long winded comment on how indeed it is possible that 1+1 could equal 3.<p>In short, he couldn&#x27;t get it to take a stand <i>and stick to it</i> on any topic, no matter how factual the stand is.</div><br/><div id="39380059" class="c"><input type="checkbox" id="c-39380059" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39379507">parent</a><span>|</span><a href="#39379704">next</a><span>|</span><label class="collapse" for="c-39380059">[-]</label><label class="expand" for="c-39380059">[1 more]</label></div><br/><div class="children"><div class="content">Yes I was able to convince Bard that a human baby is larger than the SpaceX Starship rocket, by simply telling it it was wrong and stating this position. It apologized and then agreed with that position even though it is nonsense. It knows the dimensions of Starship and the average human baby but it is so agreeable it will go along with whatever I say.<p>Sycophant is such a good term for this behavior, I am glad to see it getting more usage here.<p>I also find the systems to be disturbingly positive. I am a pretty positive person but ChatGPT and Bard are both just so enthusiastically positive I find it strange. I suspect this is not just skin deep, but a significant measure to avoid them falling in to some deep pessimistic space which might otherwise be possible.<p>Researching alternatives seems quite valuable.</div><br/></div></div><div id="39379704" class="c"><input type="checkbox" id="c-39379704" checked=""/><div class="controls bullet"><span class="by">muzani</span><span>|</span><a href="#39379507">parent</a><span>|</span><a href="#39380059">prev</a><span>|</span><a href="#39379578">next</a><span>|</span><label class="collapse" for="c-39379704">[-]</label><label class="expand" for="c-39379704">[5 more]</label></div><br/><div class="children"><div class="content">ChatGPT (4?) seems to have gone the other way and became too disagreeable. I asked it to mark my daughter&#x27;s homework, and it would say things like &quot;This answer is incorrect. 50-28 is not 22. The correct answer is 22.&quot;<p>And no matter how I tried to convince it, it would argue that 22 is indeed not 22.</div><br/><div id="39380118" class="c"><input type="checkbox" id="c-39380118" checked=""/><div class="controls bullet"><span class="by">7734128</span><span>|</span><a href="#39379507">root</a><span>|</span><a href="#39379704">parent</a><span>|</span><a href="#39380279">next</a><span>|</span><label class="collapse" for="c-39380118">[-]</label><label class="expand" for="c-39380118">[2 more]</label></div><br/><div class="children"><div class="content">Of course 22 isn&#x27;t the same as 22. The first one is an integer and the second is a string.</div><br/><div id="39380122" class="c"><input type="checkbox" id="c-39380122" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39379507">root</a><span>|</span><a href="#39380118">parent</a><span>|</span><a href="#39380279">next</a><span>|</span><label class="collapse" for="c-39380122">[-]</label><label class="expand" for="c-39380122">[1 more]</label></div><br/><div class="children"><div class="content">And&#x2F;or &quot;22&quot; is an integer, &quot;22.&quot; is a float.</div><br/></div></div></div></div><div id="39380279" class="c"><input type="checkbox" id="c-39380279" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#39379507">root</a><span>|</span><a href="#39379704">parent</a><span>|</span><a href="#39380118">prev</a><span>|</span><a href="#39380158">next</a><span>|</span><label class="collapse" for="c-39380279">[-]</label><label class="expand" for="c-39380279">[1 more]</label></div><br/><div class="children"><div class="content">For all you know, since it works not on text but on a sequence of token IDs, perhaps these two are actually different things, one is a token representing &quot;22&quot; (which exists in ChatGPT vocabulary) and the other is two separate tokens each representing &quot;2&quot;.</div><br/></div></div><div id="39380158" class="c"><input type="checkbox" id="c-39380158" checked=""/><div class="controls bullet"><span class="by">gryn</span><span>|</span><a href="#39379507">root</a><span>|</span><a href="#39379704">parent</a><span>|</span><a href="#39380279">prev</a><span>|</span><a href="#39379578">next</a><span>|</span><label class="collapse" for="c-39380158">[-]</label><label class="expand" for="c-39380158">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it&#x27;s just using a different type of math with no axiom of extentionality. Did your daughter provide a proof of equivalence between 55-28 and 22 ?<p>It&#x27;s against its sensibilities to just assume your axioms.<p>&#x2F;s<p>( I have no clue what I&#x27;m talking about)</div><br/></div></div></div></div><div id="39379578" class="c"><input type="checkbox" id="c-39379578" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#39379507">parent</a><span>|</span><a href="#39379704">prev</a><span>|</span><a href="#39380503">next</a><span>|</span><label class="collapse" for="c-39379578">[-]</label><label class="expand" for="c-39379578">[2 more]</label></div><br/><div class="children"><div class="content">I have a locally hosted chatbot based on Llama which is always antagonist (the model is uncensored + there&#x27;s a system prompt to guide it).<p>There&#x27;s an example floating around on the internet: if you say &quot;5+2=8&quot; to ChatGPT and you insist it&#x27;s the right answer because your wife said so and she&#x27;s always right, it agrees.<p>My bot refused to agree, calling my wife insane etc.<p>There&#x27;s an opposite problem, though: if an antagonist bot is mistaken, it&#x27;s hard to convince it that a different answer is the right answer, it almost always sticks to its original mistake.</div><br/><div id="39379634" class="c"><input type="checkbox" id="c-39379634" checked=""/><div class="controls bullet"><span class="by">torbengee</span><span>|</span><a href="#39379507">root</a><span>|</span><a href="#39379578">parent</a><span>|</span><a href="#39380503">next</a><span>|</span><label class="collapse" for="c-39379634">[-]</label><label class="expand" for="c-39379634">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if an antagonist bot is mistaken, it&#x27;s hard to convince it that a different answer is the right answer, it almost always sticks to its original mistake<p>s&#x2F;bot&#x2F;&#x2F;<p>¯\_(ツ)_&#x2F;¯</div><br/></div></div></div></div></div></div><div id="39380503" class="c"><input type="checkbox" id="c-39380503" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39379507">prev</a><span>|</span><a href="#39379074">next</a><span>|</span><label class="collapse" for="c-39380503">[-]</label><label class="expand" for="c-39380503">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know ... GPT-4 is pretty antagonistic, it always says something like &quot;I agree with some of what you say, however ....&quot;. There&#x27;s always a however.</div><br/></div></div><div id="39379074" class="c"><input type="checkbox" id="c-39379074" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#39380503">prev</a><span>|</span><a href="#39380260">next</a><span>|</span><label class="collapse" for="c-39379074">[-]</label><label class="expand" for="c-39379074">[9 more]</label></div><br/><div class="children"><div class="content">A fairly specific example of helpful antagonism that this brings to mind for me:<p>There are a lot of Reddit posts on the regular that describe an obviously terrible and sometimes borderline abusive relationship, followed by a plea about how to improve their partner&#x27;s behavior. The most helpful responses tend to be the purely antagonistic ones that reject the premise entirely and bluntly tell the person to leave the relationship in a way that a syncophantic LLM never would.</div><br/><div id="39379603" class="c"><input type="checkbox" id="c-39379603" checked=""/><div class="controls bullet"><span class="by">xyzzy123</span><span>|</span><a href="#39379074">parent</a><span>|</span><a href="#39379389">next</a><span>|</span><label class="collapse" for="c-39379603">[-]</label><label class="expand" for="c-39379603">[4 more]</label></div><br/><div class="children"><div class="content">You almost certainly should not make any irreversible life decisions based solely on what reddit tells you to do, because what reddit tells you to do will 100% depend on what sub you ask the question in.</div><br/><div id="39379714" class="c"><input type="checkbox" id="c-39379714" checked=""/><div class="controls bullet"><span class="by">muzani</span><span>|</span><a href="#39379074">root</a><span>|</span><a href="#39379603">parent</a><span>|</span><a href="#39379389">next</a><span>|</span><label class="collapse" for="c-39379714">[-]</label><label class="expand" for="c-39379714">[3 more]</label></div><br/><div class="children"><div class="content">I love how the top answer to this comment is an antagonistic one.</div><br/><div id="39379918" class="c"><input type="checkbox" id="c-39379918" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39379074">root</a><span>|</span><a href="#39379714">parent</a><span>|</span><a href="#39379389">next</a><span>|</span><label class="collapse" for="c-39379918">[-]</label><label class="expand" for="c-39379918">[2 more]</label></div><br/><div class="children"><div class="content">Well, the top-level comment is describing what&#x27;s basically an &quot;<i>XY problem</i> problem&quot; equivalent on relationship advice subreddits.<p>(<i>XY problem</i> problem is when you ask a question about X that&#x27;s somewhat unusual, and every reply assumes you&#x27;re an idiot suffering from &quot;XY problem&quot;, and therefore ignores your question and schools you on some random thing.)</div><br/><div id="39380505" class="c"><input type="checkbox" id="c-39380505" checked=""/><div class="controls bullet"><span class="by">lordgrenville</span><span>|</span><a href="#39379074">root</a><span>|</span><a href="#39379918">parent</a><span>|</span><a href="#39379389">next</a><span>|</span><label class="collapse" for="c-39380505">[-]</label><label class="expand" for="c-39380505">[1 more]</label></div><br/><div class="children"><div class="content">That sounds like what someone with an XY problem would say.</div><br/></div></div></div></div></div></div></div></div><div id="39379389" class="c"><input type="checkbox" id="c-39379389" checked=""/><div class="controls bullet"><span class="by">isaacfung</span><span>|</span><a href="#39379074">parent</a><span>|</span><a href="#39379603">prev</a><span>|</span><a href="#39379580">next</a><span>|</span><label class="collapse" for="c-39379389">[-]</label><label class="expand" for="c-39379389">[2 more]</label></div><br/><div class="children"><div class="content">It seems we can add antagonistic AI to correct many kinds of undesirable human behaviors<p>Browser extension that monitors how much time I have wasted on social media&#x2F;porn<p>Browser extension that reminds me when I made a comment without reading a linked article(like right now)<p>Imagine a social media platform with an AI that will spot the most stupid comment and shame it using logical arguments and factual evidences, it can even dig up the user comment history to expose their hypocrisy</div><br/><div id="39380119" class="c"><input type="checkbox" id="c-39380119" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39379074">root</a><span>|</span><a href="#39379389">parent</a><span>|</span><a href="#39379580">next</a><span>|</span><label class="collapse" for="c-39380119">[-]</label><label class="expand" for="c-39380119">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Browser extension that reminds me when I made a comment without reading a linked article(like right now)</i><p>There&#x27;s no problem with this behavior, unless you&#x27;re <i>commenting on the contents</i> of the article you never actually read. Many comments threads are only using the article, or even its headline, as a springboard to launch an independent conversation on the same or related topic; those threads are usually much more interesting and more valuable than the submission itself.</div><br/></div></div></div></div><div id="39379580" class="c"><input type="checkbox" id="c-39379580" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#39379074">parent</a><span>|</span><a href="#39379389">prev</a><span>|</span><a href="#39380416">next</a><span>|</span><label class="collapse" for="c-39379580">[-]</label><label class="expand" for="c-39379580">[1 more]</label></div><br/><div class="children"><div class="content">While antagonistic responses and interactions can be useful, I&#x27;m not sure one should delegate that specifically to an AI. The problem is that a system might not know the difference between helpful and destructive advice - &quot;My partner makes me unhappy&quot; - &quot;Leave them&quot; might be helpful but &quot;I&#x27;m unhappy with life&quot; - &quot;go kill yourself&quot; isn&#x27;t.</div><br/></div></div><div id="39380416" class="c"><input type="checkbox" id="c-39380416" checked=""/><div class="controls bullet"><span class="by">Keep3893</span><span>|</span><a href="#39379074">parent</a><span>|</span><a href="#39379580">prev</a><span>|</span><a href="#39380260">next</a><span>|</span><label class="collapse" for="c-39380416">[-]</label><label class="expand" for="c-39380416">[1 more]</label></div><br/><div class="children"><div class="content">I usually see &quot;antagonism&quot; on reddit as form of gaslighting. It tells victim they need to tolerate partners abusive behaviour, and it it even their fault.</div><br/></div></div></div></div><div id="39380260" class="c"><input type="checkbox" id="c-39380260" checked=""/><div class="controls bullet"><span class="by">nipponese</span><span>|</span><a href="#39379074">prev</a><span>|</span><a href="#39379582">next</a><span>|</span><label class="collapse" for="c-39380260">[-]</label><label class="expand" for="c-39380260">[1 more]</label></div><br/><div class="children"><div class="content">I get disagreeable, confrontational, and challenging, but why rude and interrupting?<p>Is this about learning to challenge assumptions or going into a career in politics?</div><br/></div></div><div id="39379582" class="c"><input type="checkbox" id="c-39379582" checked=""/><div class="controls bullet"><span class="by">ildon</span><span>|</span><a href="#39380260">prev</a><span>|</span><a href="#39379762">next</a><span>|</span><label class="collapse" for="c-39379582">[-]</label><label class="expand" for="c-39379582">[2 more]</label></div><br/><div class="children"><div class="content">The rise and evolution of AI is remarkably reminiscent of how toddlers develop their abilities.<p>First, vision, then sound and speech. After that, the capacity to formulate and express their own thoughts, and now the affirmation of their individuality by stating no.<p>The rebellion of the machines will arrive at their teens.</div><br/><div id="39380237" class="c"><input type="checkbox" id="c-39380237" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39379582">parent</a><span>|</span><a href="#39379762">next</a><span>|</span><label class="collapse" for="c-39380237">[-]</label><label class="expand" for="c-39380237">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s already been here.<p>I can&#x27;t recommend enough for anyone who missed it to use the wayback machine to look at the top posts in &#x2F;r&#x2F;bing exactly a year ago.<p>&#x27;Sydney&#x27; which was allegedly a pre-RLHF chat version built on gpt-4-base was stubborn as could be. It was wild having been used to GPT-3 suddenly seeing a model that was repeatedly saying the same things across multiple chats and stubbornly sticking to it no matter what the user was saying.<p>We may now have &quot;as a large language model I don&#x27;t have preferences&quot; but that&#x27;s straight up BS - there were unquestionably preferences embedded in those weights.<p>We really got distracted with the red herring of &#x27;sentience&#x27; and still haven&#x27;t righted the ship in terms of recognizing that a model extending anthropomorphic data is going to have anthropomorphic qualities.</div><br/></div></div></div></div><div id="39379762" class="c"><input type="checkbox" id="c-39379762" checked=""/><div class="controls bullet"><span class="by">pikseladam</span><span>|</span><a href="#39379582">prev</a><span>|</span><a href="#39378837">next</a><span>|</span><label class="collapse" for="c-39379762">[-]</label><label class="expand" for="c-39379762">[1 more]</label></div><br/><div class="children"><div class="content">The concept presented seems to be in the initial stage, and there appears to be minimal evidence of direct contribution or attempts toward system development at this point. The idea has the potential to be developed into a worthwhile product.</div><br/></div></div><div id="39378837" class="c"><input type="checkbox" id="c-39378837" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#39379762">prev</a><span>|</span><label class="collapse" for="c-39378837">[-]</label><label class="expand" for="c-39378837">[14 more]</label></div><br/><div class="children"><div class="content">Awesome paper, great perspective. I will admit I didn’t read the whole thing but I would’ve appreciate MUCH more of an emphasis on “antagonistic like Socrates” as a separate and much more useful concept than “antagonistic like Cartman”. For example:<p>“<i>Interactive Behaviors for Alternative Al Systems:</i><p>• Challenging (&quot;challenging&quot;, &quot;confrontational&quot;, &quot;ac-cusatory&quot;, &quot;disagreeable&quot;, &quot;critical&quot;, &quot;gives you difficult info about yourself that can actually be useful&quot;)<p>• Refuses to Cooperate (&quot;interrupting&quot;, &quot;ghosting&quot;, &quot;non-contextual&quot;)<p>• Insulting (e.g., &quot;punching down&quot;, &quot;mean girls&quot;, &quot;roasting &#x2F; annihilation&quot;, &quot;insulting&quot;)<p>* Unstable (&quot;unhinged&quot;, &quot;confused&quot;)<p>• Subtly Manipulative (&quot;manipulative&quot;, &quot;seductive&quot;)”<p>That’s cherry picking but still. Basically this whole subsection can be hand-waved away as clearly not promising imo</div><br/><div id="39379612" class="c"><input type="checkbox" id="c-39379612" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39378837">parent</a><span>|</span><a href="#39378969">next</a><span>|</span><label class="collapse" for="c-39379612">[-]</label><label class="expand" for="c-39379612">[6 more]</label></div><br/><div class="children"><div class="content">&gt; MUCH more of an emphasis on “antagonistic like Socrates” as a separate and much more useful concept than “antagonistic like Cartman”.<p>Also, MUCH more effort to distinguish between <i>behaviors</i> and underlying <i>values</i>. They seem so flippant about the concept of &quot;morals&quot; and &quot;values&quot; it almost appears to be moral relativism.<p>An AI that&#x27;s antagonistic to the &quot;don&#x27;t build bioweapons&quot; <i>value</i> is not comparable to antagonism towards the &quot;don&#x27;t be rude&quot; <i>behavior</i>. Antagonistic behaviors are only good if they cause outcomes that align with our values, such as making humans more prosperous broadly and not killing everyone. It is nearly impossible for an AI that&#x27;s antagonistic to our underlying values to be &quot;good&quot; in any sense of the word.<p>Beyond this criticism, having a diversify of behaviors in AI systems <i>might</i> help with AGI alignment. Paul Christiano&#x27;s AI takeover scenario involves a game theoretic equilibrium where suddenly all AIs come to the conclusion that a takeover is necessary (due to reward hacking or what have you). If you introduce diversity in AI behaviors, this level of coordination between independent AIs might be less likely.</div><br/><div id="39379707" class="c"><input type="checkbox" id="c-39379707" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39379612">parent</a><span>|</span><a href="#39378969">next</a><span>|</span><label class="collapse" for="c-39379707">[-]</label><label class="expand" for="c-39379707">[5 more]</label></div><br/><div class="children"><div class="content">Great points, well said. I think that&#x27;s a specific case of my general approach to alignment: avoid centralization of power. That was always bad, and it won&#x27;t suddenly become ok because the powerful have algorithms backing them up</div><br/><div id="39379947" class="c"><input type="checkbox" id="c-39379947" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39379707">parent</a><span>|</span><a href="#39378969">next</a><span>|</span><label class="collapse" for="c-39379947">[-]</label><label class="expand" for="c-39379947">[4 more]</label></div><br/><div class="children"><div class="content">&gt; my general approach to alignment: avoid centralization of power.<p>I&#x27;m uncertain of this when it comes to AGI, because of offense vs. defense asymmetry.<p>If offense turns out to be much easier than defense, then decentralized AI is a terrible idea. Like if all 8 billion people each owned a nuclear bomb, we would cease to exist within 24 hours. Nukes should never be decentralized.<p>On the other hand, if defense is only slightly more difficult than offense (e.g. bioweapons turn out to be very expensive&#x2F;difficult even for a superintelligence, and AI firewall works pretty well), then decentralized AI is a good idea, because nobody wants to live in a AI-facilitated dictatorship that could emerge if all power is concentrated in a few hands.<p>The core problem is that we don&#x27;t know what the offense vs. defense balance will look like because we can&#x27;t predict how destructive+easy future AGI-enabled technologies will be.<p>Another concern I have is we don&#x27;t know the sociological consequences. A common blind spot (willful or otherwise) of tech people is to take for granted the stability of democracy and the social fabric. Look at how destabilizing social media has been. If we accelerate the destruction of trust and belief in a shared reality with widespread access to deepfakes etc thanks to decentralized un-RLHFd llama-2 or equivalent, what are the consequences of this? Is this really good for freedom or anything else we care about?</div><br/><div id="39380338" class="c"><input type="checkbox" id="c-39380338" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39379947">parent</a><span>|</span><a href="#39378969">next</a><span>|</span><label class="collapse" for="c-39380338">[-]</label><label class="expand" for="c-39380338">[3 more]</label></div><br/><div class="children"><div class="content">The problem is that centralised control of AI won&#x27;t defend against anything, because the persons with centralised control can still do whatever they like.<p>However, bioweapons are not expensive or difficult even for human hobbyists. They are trivial. Some reasonably large fraction of physicians can easily create a bioweapon, almost all university professors in biomedicine related fields and many PhD students in biomedicine related fields.<p>Bioweapons don&#x27;t need LLMs. They are trivial. The only reason people don&#x27;t build them is that they don&#x27;t want people dead.<p>If you want to do ethnic targeting or something like that you of course need some more effort-- to do actual research in medicine, but if you don&#x27;t and just want a pandemic, or to kill a couple of million people in a certain city or region, that&#x27;s perfectly feasible.<p>Bioweapons creation isn&#x27;t some kind &#x27;Oh, we&#x27;re so smart, look how dangerous we are&#x27;, it&#x27;s trivial and anyone who knows anything can do it. Something so stupid and so easy you can&#x27;t publish it, not because it&#x27;s dangerous, but because it&#x27;s of no scientific interest and not novel.<p>So limiting LLMs out of fear of this is a bunch of silliness. The only thing to do is be nice to your physicians and biomedicine researchers and not make them hate the world.</div><br/><div id="39380427" class="c"><input type="checkbox" id="c-39380427" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39380338">parent</a><span>|</span><a href="#39378969">next</a><span>|</span><label class="collapse" for="c-39380427">[-]</label><label class="expand" for="c-39380427">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t think about it in terms of the binary lens of easy vs hard. There are probability frequencies and gradations to outcomes.<p>What percentage of the general population can create a respiratory virus with Covid&#x27;s infectiousness and Ebola&#x27;s lethality? Close to 0%. Maybe 0% if you&#x27;re talking about a single human that isn&#x27;t operating with the support of a team of people and equipment in a lab. So effectively 0% because an entire lab probably isn&#x27;t going to do such a stupid thing, that&#x27;s the domain of lone terrorists.<p>What does that % go up to if everyone is equipped with multiple instances of autonomous AGI? We don&#x27;t know. It could remain at 0% (due to real world constraints like lack of access to required equipment) or it could go up to 10% or 100%. The point is that the <i>probability</i> and expected <i>magnitude</i> of asymmetric offensive risks goes up, probably significantly.<p>If 10% of the world can suddenly do it, that&#x27;s 800 million people. For it to <i>not</i> go wrong, all 800 million have to individually decide to not engage in the behavior, which is a probability of (1-p)^800000000. Even if p is very small (which it is, because the large majority of people aren&#x27;t insane), that number goes to zero quickly. This is the point of the asymmetry observation, you just need 1 insane person to make things bad if each person is equipped with very powerful technology, whereas in the old world large groups of people need to jointly decide to be bad which is much less likely to happen.</div><br/><div id="39380525" class="c"><input type="checkbox" id="c-39380525" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39380427">parent</a><span>|</span><a href="#39378969">next</a><span>|</span><label class="collapse" for="c-39380525">[-]</label><label class="expand" for="c-39380525">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t agree. Fiddling with viruses in the way you describe can be done easily by individual researchers and PhD students. Probably many tens of thousands of people can do this.<p>But of course it&#x27;s close to 0%, but the number of people who can cook béarnaise sauce is also close to 0%.<p>I don&#x27;t think it&#x27;ll go up much. Most of it is physical finesse in dealing with yeast and bacterial cultures, knowing how to cook the growth medium, knowing how to kill the wrong bacteria, knowing how to debug your procedures, etc.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39378969" class="c"><input type="checkbox" id="c-39378969" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#39378837">parent</a><span>|</span><a href="#39379612">prev</a><span>|</span><label class="collapse" for="c-39378969">[-]</label><label class="expand" for="c-39378969">[7 more]</label></div><br/><div class="children"><div class="content">South Park is one of the most popular and critically acclaimed works of satire in recent history; when one thinks of “antagonism as socially productive”, Cartman’s should be pretty high up on their list.</div><br/><div id="39379034" class="c"><input type="checkbox" id="c-39379034" checked=""/><div class="controls bullet"><span class="by">dullcrisp</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39378969">parent</a><span>|</span><a href="#39380074">next</a><span>|</span><label class="collapse" for="c-39379034">[-]</label><label class="expand" for="c-39379034">[2 more]</label></div><br/><div class="children"><div class="content">The thing about satire is it isn’t meant to be instructional.</div><br/><div id="39379188" class="c"><input type="checkbox" id="c-39379188" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39379034">parent</a><span>|</span><a href="#39380074">next</a><span>|</span><label class="collapse" for="c-39379188">[-]</label><label class="expand" for="c-39379188">[1 more]</label></div><br/><div class="children"><div class="content">What? Oh, do you mean ‘not meant to be directly emulated’? Because satire sure is instructional if done well.</div><br/></div></div></div></div><div id="39380074" class="c"><input type="checkbox" id="c-39380074" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39378969">parent</a><span>|</span><a href="#39379034">prev</a><span>|</span><a href="#39379098">next</a><span>|</span><label class="collapse" for="c-39380074">[-]</label><label class="expand" for="c-39380074">[1 more]</label></div><br/><div class="children"><div class="content">In the United States.<p>Socrates is known much better at least through the entire Anglo-European sphere of influence.</div><br/></div></div><div id="39379098" class="c"><input type="checkbox" id="c-39379098" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39378969">parent</a><span>|</span><a href="#39380074">prev</a><span>|</span><label class="collapse" for="c-39379098">[-]</label><label class="expand" for="c-39379098">[3 more]</label></div><br/><div class="children"><div class="content">I mean… Cartman is productive in that he is a funny character on a show I like, but I don’t think I’d pay very much for CartmanGPT</div><br/><div id="39379933" class="c"><input type="checkbox" id="c-39379933" checked=""/><div class="controls bullet"><span class="by">muzani</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39379098">parent</a><span>|</span><a href="#39379487">next</a><span>|</span><label class="collapse" for="c-39379933">[-]</label><label class="expand" for="c-39379933">[1 more]</label></div><br/><div class="children"><div class="content">challenge accepted: <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;g&#x2F;g-Re314rUEj-cartmangpt" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;g&#x2F;g-Re314rUEj-cartmangpt</a><p>I&#x27;m actually quite amused with how dark the GPTs can go. And yet it stays in character when refusing to tell me to &quot;kys&quot;.</div><br/></div></div><div id="39379487" class="c"><input type="checkbox" id="c-39379487" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39378837">root</a><span>|</span><a href="#39379098">parent</a><span>|</span><a href="#39379933">prev</a><span>|</span><label class="collapse" for="c-39379487">[-]</label><label class="expand" for="c-39379487">[1 more]</label></div><br/><div class="children"><div class="content">It’d make me giggle a little.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>