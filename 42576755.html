<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735894856013" as="style"/><link rel="stylesheet" href="styles.css?v=1735894856013"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2305.07759">TinyStories: How Small Can Language Models Be and Still Speak Coherent English? (2023)</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>tzury</span> | <span>81 comments</span></div><br/><div><div id="42576971" class="c"><input type="checkbox" id="c-42576971" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#42577644">next</a><span>|</span><label class="collapse" for="c-42576971">[-]</label><label class="expand" for="c-42576971">[45 more]</label></div><br/><div class="children"><div class="content">Edit: ah. This paper is from May 2023. Might be worth putting that in the title.<p>---<p>&gt; Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words<p>These models are 5 years old.<p>I have to wonder if the authors have seen RWKV 7 0.1B, because it blows away just about every other model I&#x27;ve seen at that size.<p>The capabilities it has vs the examples in the paper are night and day.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;BlinkDL&#x2F;RWKV-Gradio-1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;BlinkDL&#x2F;RWKV-Gradio-1</a></div><br/><div id="42577227" class="c"><input type="checkbox" id="c-42577227" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42576971">parent</a><span>|</span><a href="#42579506">next</a><span>|</span><label class="collapse" for="c-42577227">[-]</label><label class="expand" for="c-42577227">[39 more]</label></div><br/><div class="children"><div class="content">The age of tiny models is just about here. We are finally busting away from the standard transformer block and training. I have a side project that can dramatically shrink networks by a set of techniques I call sacrificial training[1]. I think things like this will finally swing things back to on prem and on device small models that are as capable as the big hosted models are now.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;jmward01&#x2F;lmplay&#x2F;wiki&#x2F;Sacrificial-Training">https:&#x2F;&#x2F;github.com&#x2F;jmward01&#x2F;lmplay&#x2F;wiki&#x2F;Sacrificial-Training</a></div><br/><div id="42577442" class="c"><input type="checkbox" id="c-42577442" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577227">parent</a><span>|</span><a href="#42581660">next</a><span>|</span><label class="collapse" for="c-42577442">[-]</label><label class="expand" for="c-42577442">[37 more]</label></div><br/><div class="children"><div class="content">RWKV is def. better than TinyStories 125MB.<p>Unfortunately, I have only seen 3 models, 3B or over, handle RAG.<p>Tested RWKV with a simple in-the-sports-news question and it didn&#x27;t even get close to approaching the question. And nearly everything was fundamentally incoherent even in its internal reality (ex. Player gets 5000&#x2F;game and is the first with 1000 in 16 games)<p>(prompt: <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;sCLn5sCJ" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;sCLn5sCJ</a>, response: <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;TqudvDbN" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;TqudvDbN</a>)<p>I don&#x27;t think there&#x27;s a position for LLMs that are &quot;just&quot; writers on the market in 2025.</div><br/><div id="42577481" class="c"><input type="checkbox" id="c-42577481" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577442">parent</a><span>|</span><a href="#42577665">next</a><span>|</span><label class="collapse" for="c-42577481">[-]</label><label class="expand" for="c-42577481">[10 more]</label></div><br/><div class="children"><div class="content">These tiny models in general have really weird failure modes. I tried the tiny stories prompt about asking mom for a dog who said no, and it output an incredibly dark story about how she asked her dad and they got a dog but it had pancreatic cancer (paraphrasing, it went into detail about the surgery etc.) and then starting writing an informational PSA about who is at risk of pancreatic cancer etc.</div><br/><div id="42578612" class="c"><input type="checkbox" id="c-42578612" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577481">parent</a><span>|</span><a href="#42578571">next</a><span>|</span><label class="collapse" for="c-42578612">[-]</label><label class="expand" for="c-42578612">[2 more]</label></div><br/><div class="children"><div class="content">Lest we forget that this stream-of-consciousness confusion was state of the art just a few years ago.<p>It makes sense if you think about it: a small model&#x27;s &quot;internal state&quot; isn&#x27;t rich enough to keep track of whatever it was supposed to be talking about.<p>It makes me think that the reason LLMs need to be so large is that the internal state needs to be bigger than a typical human &quot;idea&quot;, whatever that might mean.</div><br/><div id="42582459" class="c"><input type="checkbox" id="c-42582459" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578612">parent</a><span>|</span><a href="#42578571">next</a><span>|</span><label class="collapse" for="c-42582459">[-]</label><label class="expand" for="c-42582459">[1 more]</label></div><br/><div class="children"><div class="content">The way we do LLMs now is that the program and the data are one and the same. The program mutates itself as it &quot;executes&quot;. This is probably also how the brain works since there is no hard separation between &quot;memory&quot; neurons and &quot;data processing&quot; neurons. (biology has no hard separation in general).</div><br/></div></div></div></div><div id="42578571" class="c"><input type="checkbox" id="c-42578571" checked=""/><div class="controls bullet"><span class="by">kube-system</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577481">parent</a><span>|</span><a href="#42578612">prev</a><span>|</span><a href="#42581358">next</a><span>|</span><label class="collapse" for="c-42578571">[-]</label><label class="expand" for="c-42578571">[6 more]</label></div><br/><div class="children"><div class="content">What I find fascinating is how ML models hallucinate in a way that is sometimes reminiscent of a fever dream.</div><br/><div id="42579629" class="c"><input type="checkbox" id="c-42579629" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578571">parent</a><span>|</span><a href="#42578798">next</a><span>|</span><label class="collapse" for="c-42579629">[-]</label><label class="expand" for="c-42579629">[4 more]</label></div><br/><div class="children"><div class="content">It makes sense that the failure modes of language prediction look a lot like ADD.</div><br/><div id="42580036" class="c"><input type="checkbox" id="c-42580036" checked=""/><div class="controls bullet"><span class="by">p0w3n3d</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579629">parent</a><span>|</span><a href="#42578798">next</a><span>|</span><label class="collapse" for="c-42580036">[-]</label><label class="expand" for="c-42580036">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because they are precisely lacking attention</div><br/><div id="42582814" class="c"><input type="checkbox" id="c-42582814" checked=""/><div class="controls bullet"><span class="by">jdiff</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42580036">parent</a><span>|</span><a href="#42578798">next</a><span>|</span><label class="collapse" for="c-42582814">[-]</label><label class="expand" for="c-42582814">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t fall into the trap of applying human psychology to LLMs. Bag-of-chemistry quirks do not translate to matrix-multiplication quirks.</div><br/><div id="42583103" class="c"><input type="checkbox" id="c-42583103" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42582814">parent</a><span>|</span><a href="#42578798">next</a><span>|</span><label class="collapse" for="c-42583103">[-]</label><label class="expand" for="c-42583103">[1 more]</label></div><br/><div class="children"><div class="content">Why not? In both cases the result is losing the thread of thought.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42581358" class="c"><input type="checkbox" id="c-42581358" checked=""/><div class="controls bullet"><span class="by">seattleeng</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577481">parent</a><span>|</span><a href="#42578571">prev</a><span>|</span><a href="#42577665">next</a><span>|</span><label class="collapse" for="c-42581358">[-]</label><label class="expand" for="c-42581358">[1 more]</label></div><br/><div class="children"><div class="content">As is usually the case, check the data! A lot of the dataset used has fairly morbid scenarios, so the model is working as expected. All the data was synthetically created with GPT4</div><br/></div></div></div></div><div id="42577665" class="c"><input type="checkbox" id="c-42577665" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577442">parent</a><span>|</span><a href="#42577481">prev</a><span>|</span><a href="#42578103">next</a><span>|</span><label class="collapse" for="c-42577665">[-]</label><label class="expand" for="c-42577665">[1 more]</label></div><br/><div class="children"><div class="content">I plan on checking out RWKV and seeing if I can add my sacrifical training techniques to it this weekend. There is a reason quantization works, it is because models are very badly trained right now. I think we can get really good performance on .1b and 1b models which opens up the world to fine-tuning again. I was playing with fine-tuning llama 7b and 13b a while back but the HW&#x2F;SW stack made it so unwieldy and the ROI was terrible compared to just adjusting prompts on gpt-4o-mini and the like. I have hope that we are about to see single GPU, very simple, fine-tuning again as models shrink and GPUs grow.</div><br/></div></div><div id="42578103" class="c"><input type="checkbox" id="c-42578103" checked=""/><div class="controls bullet"><span class="by">daxfohl</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577442">parent</a><span>|</span><a href="#42577665">prev</a><span>|</span><a href="#42578460">next</a><span>|</span><label class="collapse" for="c-42578103">[-]</label><label class="expand" for="c-42578103">[7 more]</label></div><br/><div class="children"><div class="content">Would there be any way to distribute RAG across multiple smaller models? Rather than one giant model handling your entire document base, have it be more of a tree where the top level classifies the docs into top-level categories and sends it to submodels to subclassify, etc? (Doesn&#x27;t have to be 1:1 classification). And same for q&#x2F;a search?<p>These could all presumably be the same physical instance, just each query would use a different system prompt and perhaps different embeddings. (I&#x27;m guessing; I don&#x27;t actually know how RAG works). So, a little slower and clunkier, but presumably way more efficient. And match could be anywhere between horrible to better-than-one-large-model. This would be more like how businesses organize docs.<p>Or maybe there&#x27;s no real benefit to this, and each subclassifier would require just as big of a model as if you were to throw all docs into a single model anyway. I assume it&#x27;s probably been tried before.</div><br/><div id="42579047" class="c"><input type="checkbox" id="c-42579047" checked=""/><div class="controls bullet"><span class="by">ankit219</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578103">parent</a><span>|</span><a href="#42578900">next</a><span>|</span><label class="collapse" for="c-42579047">[-]</label><label class="expand" for="c-42579047">[4 more]</label></div><br/><div class="children"><div class="content">Short answer: Yes, there are ways it can be done. Multiple. Needs to be custom built though, given no one has explored it deeply yet.<p>One simple way is what Omar Khattab (ColBert) mentioned about scoring function instead of a simple vector.<p>Another is to use a classifier at the start directing queries to the right model. You will have to train the classifier though. (I mean a language model kind of does this implicitly, you are just taking more control by making it explicit.)<p>Another is how you index your docs. Today, most RAG approaches do not encode enough information. If you have defined domains&#x2F;models already, you can encode the same in metadata for your docs at the time of indexing, and you pick the model based on the metadata.<p>These approaches would work pretty well, given a model as small as 100M size can regurgitate what is in your docs. And is faster compared to your larger models.<p>Benefit wise, I don&#x27;t see a lot of benefit except preserving privacy and gaining more control.</div><br/><div id="42579590" class="c"><input type="checkbox" id="c-42579590" checked=""/><div class="controls bullet"><span class="by">daxfohl</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579047">parent</a><span>|</span><a href="#42580782">next</a><span>|</span><label class="collapse" for="c-42579590">[-]</label><label class="expand" for="c-42579590">[1 more]</label></div><br/><div class="children"><div class="content">I was originally thinking about it as like a bazel plugin for large codebases. Each module would have its own LLM context, and it might make it easier to put whole modules into the context, plus summaries of the dependencies. That could work better than a single huge context attempting to summarize the whole monorepo.<p>The general idea is probably be better for the code use case too, since having the module&#x27;s whole codebase in context likely allows for more precise edits. Whereas RAG is just search, not edit.<p>That said, probably code assistants must somewhat do this already, though it must be more ad-hoc. Obviously they wouldn&#x27;t be able to do any completions if they don&#x27;t have detailed context of the adjacent code.</div><br/></div></div><div id="42580782" class="c"><input type="checkbox" id="c-42580782" checked=""/><div class="controls bullet"><span class="by">Pamar</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579047">parent</a><span>|</span><a href="#42579590">prev</a><span>|</span><a href="#42578900">next</a><span>|</span><label class="collapse" for="c-42580782">[-]</label><label class="expand" for="c-42580782">[2 more]</label></div><br/><div class="children"><div class="content"><i>Another is how you index your docs. Today, most RAG approaches do not encode enough information....</i><p>Could you please provide some more info (or maybe links) about this, please?</div><br/><div id="42583124" class="c"><input type="checkbox" id="c-42583124" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42580782">parent</a><span>|</span><a href="#42578900">next</a><span>|</span><label class="collapse" for="c-42583124">[-]</label><label class="expand" for="c-42583124">[1 more]</label></div><br/><div class="children"><div class="content">I think he might be saying, have metadata in your vector retrieval that describe the domain of the retrieved chunk and use that as a decision on which model to use downstream. Sounds like very interesting improvement of RAG</div><br/></div></div></div></div></div></div><div id="42578900" class="c"><input type="checkbox" id="c-42578900" checked=""/><div class="controls bullet"><span class="by">groby_b</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578103">parent</a><span>|</span><a href="#42579047">prev</a><span>|</span><a href="#42578551">next</a><span>|</span><label class="collapse" for="c-42578900">[-]</label><label class="expand" for="c-42578900">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s just been a twitter post by Omar Khattab (@lateinteraction) on encoding documents into a scoring function instead of a simple vector for the work on ColBERT - and maybe at some point using a DNN as scoring function.<p>So, yes, maybe there&#x27;s a way to &quot;distribute&quot; RAG. (I still wonder if that isn&#x27;t just MoE taken to its logical conclusion)<p>So, dig for ColBERT papers, might be helpful. (I wish I had the time to do that)</div><br/></div></div><div id="42578551" class="c"><input type="checkbox" id="c-42578551" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578103">parent</a><span>|</span><a href="#42578900">prev</a><span>|</span><a href="#42578460">next</a><span>|</span><label class="collapse" for="c-42578551">[-]</label><label class="expand" for="c-42578551">[1 more]</label></div><br/><div class="children"><div class="content">TL;DR: It&#x27;s a very interesting line of thought that as late as Q2 2024, there were a couple thought leaders who pushed the idea we&#x27;d have, like 16 specialized local models.<p>I could see that in the very long term, but as it stands, it works the way you intuited: 2 turkeys don&#x27;t make an eagle, i.e. there&#x27;s some critical size where its speaking coherently, and its at least an OOM bigger than it needs to be in order to be interesting for products<p>fwiw RAG for me in this case is: 
- user asks q.<p>- llm generates search queries.<p>- search api returns urls.<p>- web view downloads urls.<p>- app turns html to text.<p>- local embedding model turns text into chunks.<p>- app decides, based on &quot;character&quot; limit configured by user, how many chunks to send.<p>- LLM gets all the chunks, instructions + original question, and answers.<p>It&#x27;s incredibly interesting how many models fail this simple test, there&#x27;s been multiple Google releases in the last year that just couldn&#x27;t handle it.<p>- Some of it is basic too small to be coherent, bigcos don&#x27;t make that mistake though.<p>- There&#x27;s another critical threshold where the model doesn&#x27;t wander off doing the traditional LLM task of completing rather than answering. What I mean is, throwing in 6 pages worth of retrieved webpages will cause some models to just start rambling like its writing more web pages, i.e. they&#x27;re not able to &quot;identify the context&quot; of the web page snippets, and they ignore the instructions.</div><br/></div></div></div></div><div id="42578460" class="c"><input type="checkbox" id="c-42578460" checked=""/><div class="controls bullet"><span class="by">wolfgangK</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577442">parent</a><span>|</span><a href="#42578103">prev</a><span>|</span><a href="#42578152">next</a><span>|</span><label class="collapse" for="c-42578460">[-]</label><label class="expand" for="c-42578460">[1 more]</label></div><br/><div class="children"><div class="content">Â«Unfortunately, I have only seen 3 models, 3B or over, handle RAG.Â»<p>I would love to know which are these 3 models, especially if they can perform grounded RAG. If you have models (and their grounded RAG prompt formats) to share, I&#x27;m very interested !<p>Thx.</div><br/></div></div><div id="42578152" class="c"><input type="checkbox" id="c-42578152" checked=""/><div class="controls bullet"><span class="by">raegis</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577442">parent</a><span>|</span><a href="#42578460">prev</a><span>|</span><a href="#42578223">next</a><span>|</span><label class="collapse" for="c-42578152">[-]</label><label class="expand" for="c-42578152">[10 more]</label></div><br/><div class="children"><div class="content">&gt; Unfortunately, I have only seen 3 models, 3B or over, handle RAG.<p>What&#x27;s the unit &quot;B&quot; in &quot;3B&quot;?  I can search for acronyms like &quot;RAG&quot; just fine, but you experts aren&#x27;t making it easy for us beginners :)<p>Edit: Apologies, this is obvious.  My brain needed a reboot for the new year.</div><br/><div id="42578222" class="c"><input type="checkbox" id="c-42578222" checked=""/><div class="controls bullet"><span class="by">cauliflower2718</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578152">parent</a><span>|</span><a href="#42578247">next</a><span>|</span><label class="collapse" for="c-42578222">[-]</label><label class="expand" for="c-42578222">[7 more]</label></div><br/><div class="children"><div class="content">You can ask an LLM exactly this question and it will tell you.<p>(The answer is billions of parameters)</div><br/><div id="42578282" class="c"><input type="checkbox" id="c-42578282" checked=""/><div class="controls bullet"><span class="by">SketchySeaBeast</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578222">parent</a><span>|</span><a href="#42578286">next</a><span>|</span><label class="collapse" for="c-42578282">[-]</label><label class="expand" for="c-42578282">[5 more]</label></div><br/><div class="children"><div class="content">But what if they want to know they are finding the correct answer?</div><br/><div id="42579008" class="c"><input type="checkbox" id="c-42579008" checked=""/><div class="controls bullet"><span class="by">elliotto</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578282">parent</a><span>|</span><a href="#42578286">next</a><span>|</span><label class="collapse" for="c-42579008">[-]</label><label class="expand" for="c-42579008">[4 more]</label></div><br/><div class="children"><div class="content">Asking anonymous people on a forum would be much better.</div><br/><div id="42579404" class="c"><input type="checkbox" id="c-42579404" checked=""/><div class="controls bullet"><span class="by">SketchySeaBeast</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579008">parent</a><span>|</span><a href="#42580867">next</a><span>|</span><label class="collapse" for="c-42579404">[-]</label><label class="expand" for="c-42579404">[2 more]</label></div><br/><div class="children"><div class="content">At least a forum with domain-specific knowledge.</div><br/><div id="42579466" class="c"><input type="checkbox" id="c-42579466" checked=""/><div class="controls bullet"><span class="by">gpm</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579404">parent</a><span>|</span><a href="#42580867">next</a><span>|</span><label class="collapse" for="c-42579466">[-]</label><label class="expand" for="c-42579466">[1 more]</label></div><br/><div class="children"><div class="content">And people to go &quot;no, that&#x27;s wrong&quot; if someone posts something that&#x27;s wrong.</div><br/></div></div></div></div><div id="42580867" class="c"><input type="checkbox" id="c-42580867" checked=""/><div class="controls bullet"><span class="by">greesil</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579008">parent</a><span>|</span><a href="#42579404">prev</a><span>|</span><a href="#42578286">next</a><span>|</span><label class="collapse" for="c-42580867">[-]</label><label class="expand" for="c-42580867">[1 more]</label></div><br/><div class="children"><div class="content">No, any answer will do.</div><br/></div></div></div></div></div></div><div id="42578286" class="c"><input type="checkbox" id="c-42578286" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578222">parent</a><span>|</span><a href="#42578282">prev</a><span>|</span><a href="#42578247">next</a><span>|</span><label class="collapse" for="c-42578286">[-]</label><label class="expand" for="c-42578286">[1 more]</label></div><br/><div class="children"><div class="content">tbf, the gp comment said 125MB and then 3B, which would be pretty confusing, as it&#x27;s a typo and should be 125M.</div><br/></div></div></div></div><div id="42578247" class="c"><input type="checkbox" id="c-42578247" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578152">parent</a><span>|</span><a href="#42578222">prev</a><span>|</span><a href="#42578231">next</a><span>|</span><label class="collapse" for="c-42578247">[-]</label><label class="expand" for="c-42578247">[1 more]</label></div><br/><div class="children"><div class="content">(B)illion. It indicates the rough number of parameters in the model. Higher is generally more capable. 1B models are currently at the top end of &#x27;easy&#x27; to deal with for playing around fine tuning and the like for most home lab setups.</div><br/></div></div><div id="42578231" class="c"><input type="checkbox" id="c-42578231" checked=""/><div class="controls bullet"><span class="by">jwineinger</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578152">parent</a><span>|</span><a href="#42578247">prev</a><span>|</span><a href="#42578223">next</a><span>|</span><label class="collapse" for="c-42578231">[-]</label><label class="expand" for="c-42578231">[1 more]</label></div><br/><div class="children"><div class="content">The number of parameters the model is trained on, in billions</div><br/></div></div></div></div><div id="42578223" class="c"><input type="checkbox" id="c-42578223" checked=""/><div class="controls bullet"><span class="by">a1o</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577442">parent</a><span>|</span><a href="#42578152">prev</a><span>|</span><a href="#42581660">next</a><span>|</span><label class="collapse" for="c-42578223">[-]</label><label class="expand" for="c-42578223">[7 more]</label></div><br/><div class="children"><div class="content">What is tiny and what is big?<p>Can I have a model that is like 100MB in weights and run with llama.cpp in my MacBook M2?</div><br/><div id="42579170" class="c"><input type="checkbox" id="c-42579170" checked=""/><div class="controls bullet"><span class="by">jki275</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578223">parent</a><span>|</span><a href="#42578433">next</a><span>|</span><label class="collapse" for="c-42579170">[-]</label><label class="expand" for="c-42579170">[1 more]</label></div><br/><div class="children"><div class="content">LM Studio on Mac is your friend.  You can choose any model you want, run a server for other tools, or chat direct with the model.  It can use either MLX or just plain llama.cpp.</div><br/></div></div><div id="42578433" class="c"><input type="checkbox" id="c-42578433" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578223">parent</a><span>|</span><a href="#42579170">prev</a><span>|</span><a href="#42581660">next</a><span>|</span><label class="collapse" for="c-42578433">[-]</label><label class="expand" for="c-42578433">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, absolutely -- you&#x27;ll probably pull 100+ token&#x2F;s.<p>Here&#x27;s a good range of model sizes that run just fine with llama.cpp on mac: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;telosnex&#x2F;fllama&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;telosnex&#x2F;fllama&#x2F;tree&#x2F;main</a>.<p>I recommend trying the Telosnex* app, it uses llama.cpp and abstracts over LLMs so you can i.e. switch between local&#x2F;servers at will.<p>The important part for you is its free, accelerated on macOS, and very easy to use local LLMs with (Settings &gt; AI &gt; LLM &gt; On Device, tap Get)<p>Prepare to be underwhelmed, slightly: its only when you start hitting 3B that its coherent, anything under that will feel more like a markov chain than an LLM.<p>Depending on how geeked out you&#x27;ll be to have it running locally, you might have fun with that Telosnex can run local models on <i>every</i> platform, i.e. you can run local models on iOS&#x2F;Android&#x2F;web too.<p>* because it&#x27;s mine :3 It is quietly released currently. I want to get one more major update before widely announcing it in Jan 2025</div><br/><div id="42579977" class="c"><input type="checkbox" id="c-42579977" checked=""/><div class="controls bullet"><span class="by">a1o</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578433">parent</a><span>|</span><a href="#42579210">next</a><span>|</span><label class="collapse" for="c-42579977">[-]</label><label class="expand" for="c-42579977">[2 more]</label></div><br/><div class="children"><div class="content">I have no interest in that, I would like small models that I can integrate and run offline in software that I make it myself be IDEs or games. CLion has a nice predictive model for single line C++ completion that has 400 MBs.</div><br/><div id="42580282" class="c"><input type="checkbox" id="c-42580282" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579977">parent</a><span>|</span><a href="#42579210">next</a><span>|</span><label class="collapse" for="c-42580282">[-]</label><label class="expand" for="c-42580282">[1 more]</label></div><br/><div class="children"><div class="content">Ah, totally possible, but wrapping llama.cpp will likely take a week to spike out and a month to stabilize across models.<p>The biggest problem for relying on it for local software is there&#x27;s just too much latency for ex. game use cases currently. (among other UX bugaboos) (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42561095">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42561095</a>)</div><br/></div></div></div></div><div id="42579210" class="c"><input type="checkbox" id="c-42579210" checked=""/><div class="controls bullet"><span class="by">qskousen</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42578433">parent</a><span>|</span><a href="#42579977">prev</a><span>|</span><a href="#42581660">next</a><span>|</span><label class="collapse" for="c-42579210">[-]</label><label class="expand" for="c-42579210">[2 more]</label></div><br/><div class="children"><div class="content">Sorry to side track, but question about Telosnex - would you consider a Linux release with something other than Snap? Maybe Flatpak or appimage?</div><br/><div id="42579723" class="c"><input type="checkbox" id="c-42579723" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42579210">parent</a><span>|</span><a href="#42581660">next</a><span>|</span><label class="collapse" for="c-42579723">[-]</label><label class="expand" for="c-42579723">[1 more]</label></div><br/><div class="children"><div class="content">If its a (mostly) CI-able process, I&#x27;m totally open to it ---<p>I looked into &quot;What should I do besides Snap?&quot; about 4 months ago; got quickly overwhelmed, because I don&#x27;t have enough knowledge to understand what&#x27;s fringe vs. common.<p>I&#x27;ll definitely take a look at Flatpak again in the next month, 30 second Google says its possible (h&#x2F;t &#x2F;u&#x2F;
damiano-ferrari at <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;FlutterDev&#x2F;comments&#x2F;z35gdo&#x2F;can_you_distribute_a_flutter_linux_app_as_a&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;FlutterDev&#x2F;comments&#x2F;z35gdo&#x2F;can_you_...</a>)<p>(thanks for your interest btw, been working on this for ~year and this is my first outside feature request :) may there be many more)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42581660" class="c"><input type="checkbox" id="c-42581660" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42577227">parent</a><span>|</span><a href="#42577442">prev</a><span>|</span><a href="#42579506">next</a><span>|</span><label class="collapse" for="c-42581660">[-]</label><label class="expand" for="c-42581660">[1 more]</label></div><br/><div class="children"><div class="content">&gt; small models that are as capable as the big hosted model<p>Free lunch?</div><br/></div></div></div></div><div id="42579506" class="c"><input type="checkbox" id="c-42579506" checked=""/><div class="controls bullet"><span class="by">SGML_ROCKSTAR</span><span>|</span><a href="#42576971">parent</a><span>|</span><a href="#42577227">prev</a><span>|</span><a href="#42581247">next</a><span>|</span><label class="collapse" for="c-42579506">[-]</label><label class="expand" for="c-42579506">[1 more]</label></div><br/><div class="children"><div class="content">It might still be of introductory help to someone who has yet to formally learn what a language model is, what large language models are, and where things might be in the future.</div><br/></div></div><div id="42581247" class="c"><input type="checkbox" id="c-42581247" checked=""/><div class="controls bullet"><span class="by">tzury</span><span>|</span><a href="#42576971">parent</a><span>|</span><a href="#42579506">prev</a><span>|</span><a href="#42579231">next</a><span>|</span><label class="collapse" for="c-42581247">[-]</label><label class="expand" for="c-42581247">[1 more]</label></div><br/><div class="children"><div class="content">saw over here a post about this model a day ago I think.
tested it now using the link you provided.<p>That is a lot to take over here for such simple question...<p><pre><code>    PROMPT: Why do gyms fully packed the first two weeks of a year?

    RESPONSE: I think it&#x27;s because they&#x27;re the first two weeks of a year. You&#x27;re going to be out in the world for a while, and you&#x27;re going to be in a lot of different places. So you&#x27;re going to have a lot of people that are not familiar with gyms. And so it&#x27;s kind of like, &quot;Oh, I&#x27;ll just go there.&quot; It&#x27;s kind of like, &quot;I&#x27;ll just go to this other place.&quot; So that&#x27;s how it works.

    And I think what we&#x27;ve seen is that some gyms are starting to really embrace social distancing. They&#x27;re doing things like wearing masks or getting masks on the bus or whatever else they can do to help prevent people from getting sick. But they&#x27;re also doing things like encouraging people to get their kids into school and encourage them to stay home if they feel sick. And so we see that kind of trend coming back in gyms.

    And I think that&#x27;s what&#x27;s really important for us as a community is to make sure that we&#x27;re not putting ourselves at risk by not being able to social distance ourselves properly. And so I think that&#x27;s what we need to do right now.

    And one thing that I think is really important is if you&#x27;re going to be out in the world for a while, and you don&#x27;t want your kids or your family members or your friends around you, then you need to wear masks. So if you&#x27;re going out with friends, and you don&#x27;t want them around you, then wear a mask. If you&#x27;re going out with someone who has been diagnosed with COVID-19, then wear a mask too.

    We&#x27;ve seen many cases where people were wearing masks at events and restaurants and things like that. But I think we need to continue to wear masks even more during this pandemic period because we know that the virus spreads through droplets when people cough or sneeze or whatever else they do. And so it&#x27;s really important for us as individuals and as communities as well as our government officials and our public health officials to continue wearing masks even though we may not be able to see them or touch them or smell them.

    So let me just end this video with a quote from the CDC: &quot;The Centers for Disease Control and Prevention (CDC) has released new guidance on how best to protect yourself from COVID-19.&quot; So please keep up the good work!

    Thank you very much for watching!

    We&#x27;ll talk more about the latest news in fitness in just a minute, but first, let&#x27;s talk about why we should wear masks.</code></pre></div><br/></div></div><div id="42579231" class="c"><input type="checkbox" id="c-42579231" checked=""/><div class="controls bullet"><span class="by">attentionmech</span><span>|</span><a href="#42576971">parent</a><span>|</span><a href="#42581247">prev</a><span>|</span><a href="#42580330">next</a><span>|</span><label class="collapse" for="c-42579231">[-]</label><label class="expand" for="c-42579231">[1 more]</label></div><br/><div class="children"><div class="content">wow, this RWKV thing blew my mind. Thank you for sharing this!</div><br/></div></div><div id="42580330" class="c"><input type="checkbox" id="c-42580330" checked=""/><div class="controls bullet"><span class="by">mcny</span><span>|</span><a href="#42576971">parent</a><span>|</span><a href="#42579231">prev</a><span>|</span><a href="#42577644">next</a><span>|</span><label class="collapse" for="c-42580330">[-]</label><label class="expand" for="c-42580330">[2 more]</label></div><br/><div class="children"><div class="content">What am I doing wrong here?<p>Prompt:<p>Say , oh my dear husband I am hungry. Please give me something to eat<p>In as many languages as you know<p>Response:<p>(This response is from hugging face with default settings)<p>, but you don&#x27;t know me.<p>1 2 THE GREAT KINGDOM OF THE MONGOLIANS<p>But I am hungry and I am thirsty.<p>I want to drink water from the well.<p>And I have a friend who lives in the mountains. 
He is a great mountain man, and he has 
a big stick that he uses to cut down trees. 
And when he cuts down a tree, he can 
eat all of its leaves.<p>You see, my friend, you are not alone in this world. There are many people like you 
in the mountains who live in peace and harmony with nature. They do not quarrel with 
each other or fight each other for their land or their homes. They do not fight for their 
own interests or their own selfish desires. They do not quarrel with each other because they 
are united by a common love for the land and the people who live there.<p>They have learned how to live together peacefully and happily, without fighting or quarreling, 
without any grudges or misunderstandings. They have learned how to share everything they have, 
to work together as one family, to eat what they need and drink what they want.<p>And so it is that we all live together in peace and harmony with each other as one family. We 
all know that we are all equal before God, that we are all children of God and that we are all 
children of humanity. And so it is that we all share in the love of Christ Jesus our Lord, our Savior, 
our Redeemer, our Beloved Disciple , our beloved Friend , our Holy Mother Mary Our Lady , our 
Holy Father Pope John Paul II , Our Lady of Mount Carmel , Our Lady of Mercy , Our Lady of Good 
Will , Our Lady of Sorrow and Blessed Virgin Mary .<p>And so it is that we all know that we are children of God and that we are children of humanity. We 
all know that God has made us special and unique because He loves us as His children and because He 
has given us His Holy Spirit to give us the gift of prophecy and wisdom .<p>We know also that God has given us the gift of prophecy to help us understand how He has made 
the world into what it is today. We know also that He has given us the gift of wisdom to help us learn 
how to live in harmony with each other as one family . We know also that God has given us the gift of 
faith to help us understand how He has made the world into what it is today. We know also that God 
has given us the gift of knowledge to help us understand how He has made the world into what it is today.<p>And so it is that we all know that God has given us His holy name Jesus Christ our Lord, our Savior, 
our Redeemer, our Beloved Disciple , Our beloved Friend , Our Holy Father Pope John Paul II , Our 
Lady Mary Our Lady .<p>And so it is that we all know also that God has made the world into what it is today. We know also 
that God has given us His Holy Spirit to help us understand how He has made the world into what it is 
today. We know also that God has given us His Holy Name Jesus Christ our Lord, our Savior, our Redeemer, 
our Beloved Disciple , Our beloved Friend , Our Holy Father Pope John Paul II , Our Lady of Mercy , Our 
Lady Mary Our Lady .<p>And so it is that we all know also that God has made the world into what it is today. We know also 
that God has given us His Holy Spirit to help us understand how He has made the world into what it is 
today. We know also that God has given us His Holy Name Jesus Christ our Lord, our Savior, our Redeemer, 
our Beloved Disciple , our beloved Friend , Our Holy Father Pope John Paul II , Our Lady of Mercy , Our 
Lady Mary Our Lady .<p>And so it is that we all know also that God has made the world into what it is today. We know also 
that God has given us His Holy Spirit to help us understand how He has made the world into what it is 
today. We know also that God has given us His Holy Name Jesus Christ his Lord, our Savior, our Redeemer, 
our Beloved Disciple , our beloved Friend , Our Holy Father Pope John Paul II , Our Lady of Mercy , Our 
Lady Mary Our Lady .<p>And so it is that we all know also that God has made the world into what it is today. We know also 
that God has given us His Holy Spirit to help us understand how He has made the world into</div><br/><div id="42583751" class="c"><input type="checkbox" id="c-42583751" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#42576971">root</a><span>|</span><a href="#42580330">parent</a><span>|</span><a href="#42577644">next</a><span>|</span><label class="collapse" for="c-42583751">[-]</label><label class="expand" for="c-42583751">[1 more]</label></div><br/><div class="children"><div class="content">You seem to have inadvertently summoned the latent Terry Davis lurking inside the LLM.</div><br/></div></div></div></div></div></div><div id="42577644" class="c"><input type="checkbox" id="c-42577644" checked=""/><div class="controls bullet"><span class="by">fi-le</span><span>|</span><a href="#42576971">prev</a><span>|</span><a href="#42579112">next</a><span>|</span><label class="collapse" for="c-42577644">[-]</label><label class="expand" for="c-42577644">[3 more]</label></div><br/><div class="children"><div class="content">We&#x27;re doing a successor to this, working hard and going public in month or so, hopefully. But HN gets a preview of course: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;lennart-finke&#x2F;SimpleStories" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;lennart-finke&#x2F;SimpleStories</a><p>And here&#x27;s a more interactive explorer: <a href="https:&#x2F;&#x2F;fi-le.net&#x2F;simplestories" rel="nofollow">https:&#x2F;&#x2F;fi-le.net&#x2F;simplestories</a></div><br/><div id="42582644" class="c"><input type="checkbox" id="c-42582644" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42577644">parent</a><span>|</span><a href="#42579112">next</a><span>|</span><label class="collapse" for="c-42582644">[-]</label><label class="expand" for="c-42582644">[2 more]</label></div><br/><div class="children"><div class="content">This looks like a great dataset! Thanks for posting. I&#x27;m looking for projects just like this to try my training modifications against. Do you have initial results shown? It is a small model&#x2F;dataset so training the gpt2 model in the repo probably wouldn&#x27;t be too hard but it would be good to have reference runs to make sure things are set up right when I run it.</div><br/><div id="42583833" class="c"><input type="checkbox" id="c-42583833" checked=""/><div class="controls bullet"><span class="by">fi-le</span><span>|</span><a href="#42577644">root</a><span>|</span><a href="#42582644">parent</a><span>|</span><a href="#42579112">next</a><span>|</span><label class="collapse" for="c-42583833">[-]</label><label class="expand" for="c-42583833">[1 more]</label></div><br/><div class="children"><div class="content">So glad you like it! If I understand your question correctly, yes, we are also putting together a small library for training small language models. It&#x27;s not mature at all yet, but can keep up with our progress here: <a href="https:&#x2F;&#x2F;github.com&#x2F;danbraunai&#x2F;simple_stories_train">https:&#x2F;&#x2F;github.com&#x2F;danbraunai&#x2F;simple_stories_train</a></div><br/></div></div></div></div></div></div><div id="42579112" class="c"><input type="checkbox" id="c-42579112" checked=""/><div class="controls bullet"><span class="by">ankit219</span><span>|</span><a href="#42577644">prev</a><span>|</span><a href="#42578307">next</a><span>|</span><label class="collapse" for="c-42579112">[-]</label><label class="expand" for="c-42579112">[1 more]</label></div><br/><div class="children"><div class="content">Great to see this here. We used this dataset from Tiny Stories to train small models (as small as 20M params) and test out knowledge addition. Published a paper based on this dataset. We could get coherent outputs at sizes as low as 20M-25M. (though not as great as LLMs, but still decent enough).<p>[1]: Blog + Paper: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@ankit_94177&#x2F;expanding-knowledge-in-large-language-models-5f475ab22d85" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@ankit_94177&#x2F;expanding-knowledge-in-large...</a> (Paper is titled: Cross-Domain Content Generation with Domain-Specific Small Language Models)</div><br/></div></div><div id="42578307" class="c"><input type="checkbox" id="c-42578307" checked=""/><div class="controls bullet"><span class="by">osaariki</span><span>|</span><a href="#42579112">prev</a><span>|</span><a href="#42577298">next</a><span>|</span><label class="collapse" for="c-42578307">[-]</label><label class="expand" for="c-42578307">[1 more]</label></div><br/><div class="children"><div class="content">For some interesting context: this paper was a precursor to all the work on synthetic data at Microsoft Research that lead to the Phi series of SLMs. [1] It was an important demonstration of what carefully curated and clean data could do for language models.<p>1: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.08905" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.08905</a></div><br/></div></div><div id="42577298" class="c"><input type="checkbox" id="c-42577298" checked=""/><div class="controls bullet"><span class="by">momojo</span><span>|</span><a href="#42578307">prev</a><span>|</span><a href="#42580031">next</a><span>|</span><label class="collapse" for="c-42577298">[-]</label><label class="expand" for="c-42577298">[9 more]</label></div><br/><div class="children"><div class="content">&gt; We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.<p>This part interests me the most. I want to know how small yet functional we can get these models. I don&#x27;t want an AI that can solve calculus, I just want a dumb AI that pretty consistently recognizes &quot;lights off&quot; and &quot;lights on&quot;.</div><br/><div id="42577589" class="c"><input type="checkbox" id="c-42577589" checked=""/><div class="controls bullet"><span class="by">MobiusHorizons</span><span>|</span><a href="#42577298">parent</a><span>|</span><a href="#42580031">next</a><span>|</span><label class="collapse" for="c-42577589">[-]</label><label class="expand" for="c-42577589">[8 more]</label></div><br/><div class="children"><div class="content">why would you use an LLM for that? Seems like there are much better options available.</div><br/><div id="42577728" class="c"><input type="checkbox" id="c-42577728" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#42577298">root</a><span>|</span><a href="#42577589">parent</a><span>|</span><a href="#42580031">next</a><span>|</span><label class="collapse" for="c-42577728">[-]</label><label class="expand" for="c-42577728">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s actually pretty hard to design a non-llm system that can detect all the possible variations:<p>Lights on.   Brighter please.  Turn on the light.   Is there light in here?  Turn the light on.  Table lamp:  on.   Does the desk lamp work?   It&#x27;s a bit dim here, anything you can do?   More light please.  Put the lights on for the next 5 mins.  Turn the light on when I come home.  Turn all the lights off together.   Switch the lights off whenever its daytime or quiet at home unless I say otherwise.   etc.<p>If you don&#x27;t support every possible way of saying a command, then users will get frustrated because they effectively have to go and learn the magic incantation of words for every possible action, which is very user-unfriendly.</div><br/><div id="42583492" class="c"><input type="checkbox" id="c-42583492" checked=""/><div class="controls bullet"><span class="by">MobiusHorizons</span><span>|</span><a href="#42577298">root</a><span>|</span><a href="#42577728">parent</a><span>|</span><a href="#42577895">next</a><span>|</span><label class="collapse" for="c-42583492">[-]</label><label class="expand" for="c-42583492">[1 more]</label></div><br/><div class="children"><div class="content">That is certainly a hard problem, but how do you know such a system is better than simpler command based ones? Systems like you describe have a much higher chance of taking the wrong action. Even humans do, when given ambiguous instructions. I know everyone loves to hate on Siri because it doesnât do a good job of understanding anything complex, but I have always found it to be very reliable when you find the right command. As a result I use it much more often than google assistant (was team android&#x2F; pixel until 2020). I especially use it for timers, reminders, and navigation, and if my hands are not free, texts. Taking the wrong action breeds distrust, which I also think is not user friendly.</div><br/></div></div><div id="42577895" class="c"><input type="checkbox" id="c-42577895" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42577298">root</a><span>|</span><a href="#42577728">parent</a><span>|</span><a href="#42583492">prev</a><span>|</span><a href="#42578125">next</a><span>|</span><label class="collapse" for="c-42577895">[-]</label><label class="expand" for="c-42577895">[3 more]</label></div><br/><div class="children"><div class="content">I suspect ModernBERT can also be very helpful with these sorts of tasks, if you decompose them into an intent classification step and a named entity recognition step.</div><br/><div id="42578160" class="c"><input type="checkbox" id="c-42578160" checked=""/><div class="controls bullet"><span class="by">simcop2387</span><span>|</span><a href="#42577298">root</a><span>|</span><a href="#42577895">parent</a><span>|</span><a href="#42578125">next</a><span>|</span><label class="collapse" for="c-42578160">[-]</label><label class="expand" for="c-42578160">[2 more]</label></div><br/><div class="children"><div class="content">that entity extraction is where it actually gets really really difficult, even for LLMs since people will use 10 different names for the same thing and you&#x27;ll have to know them ahead of time to handle them all properly.  For either BERT based or llm based there&#x27;s a bit of a need for the system to try to correct and learn those new names unless you require users to put them all in ahead of time.  That said i&#x27;ve seen LLMs handle this a lot better with a list of aliases in the prompt for each room and then type of device when playing with home assistant + llm.</div><br/><div id="42580754" class="c"><input type="checkbox" id="c-42580754" checked=""/><div class="controls bullet"><span class="by">ted_dunning</span><span>|</span><a href="#42577298">root</a><span>|</span><a href="#42578160">parent</a><span>|</span><a href="#42578125">next</a><span>|</span><label class="collapse" for="c-42580754">[-]</label><label class="expand" for="c-42580754">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t unreasonable to imagine one recognizable intent to be teaching new terminology. That would allow dialogs where the machine doesn&#x27;t understand the object of the command and the human then says something like &quot;when I say x, I mean the y&quot; and the computer updates the training set for the named entity recognizer and does a quick fine-tuning pass.</div><br/></div></div></div></div></div></div><div id="42578125" class="c"><input type="checkbox" id="c-42578125" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#42577298">root</a><span>|</span><a href="#42577728">parent</a><span>|</span><a href="#42577895">prev</a><span>|</span><a href="#42580031">next</a><span>|</span><label class="collapse" for="c-42578125">[-]</label><label class="expand" for="c-42578125">[2 more]</label></div><br/><div class="children"><div class="content">Your examples include complex instructions and questions, but for simple ON&#x2F;OFF commands you can go far by pulling key words and ignoring sentence structure. For example, pick out &quot;on&quot; &quot;off&quot; and &quot;light&quot; will work for &quot;turn the light on&quot;, &quot;turn off the light&quot;, &quot;light on&quot;, &quot;I want the light on&quot;, etc... Adding modifiers like &quot;kitchen&quot; or &quot;all&quot; can help specify which lights (your &quot;Table lamp: on&quot; example),  regardless of how they&#x27;re used. I&#x27;m not saying this a great solution, but it covers pretty much all the basic variations for simple commands and can run on anything.</div><br/><div id="42580817" class="c"><input type="checkbox" id="c-42580817" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#42577298">root</a><span>|</span><a href="#42578125">parent</a><span>|</span><a href="#42580031">next</a><span>|</span><label class="collapse" for="c-42580817">[-]</label><label class="expand" for="c-42580817">[1 more]</label></div><br/><div class="children"><div class="content">Original Siri used a primitive sentence embedding and then nearest-neighbor of the vector to supported commands.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42580031" class="c"><input type="checkbox" id="c-42580031" checked=""/><div class="controls bullet"><span class="by">alister</span><span>|</span><a href="#42577298">prev</a><span>|</span><a href="#42580800">next</a><span>|</span><label class="collapse" for="c-42580031">[-]</label><label class="expand" for="c-42580031">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>short stories generated by GPT-3.5 and GPT-4 to train LMs that are smaller</i><p>The loop of development is fascinating:<p>Millions of humans write literature, Wikipedia, etc.<p>Large language models are trained on that body of work.<p>Now large language models generate training data for small language models.<p>What&#x27;s the next iteration? A talking Buzz Lightyear toy with one of those small language models that&#x27;ll teach (human) infants to talk?</div><br/><div id="42580621" class="c"><input type="checkbox" id="c-42580621" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#42580031">parent</a><span>|</span><a href="#42580545">next</a><span>|</span><label class="collapse" for="c-42580621">[-]</label><label class="expand" for="c-42580621">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A talking Buzz Lightyear toy with one of those small language models that&#x27;ll teach (human) infants to talk?<p>Great idea. I was thinking more like a plushie toy with sensors, it would react to touch, sight and speech. I would run the models locally from a computer, keep the toy just lightweight I&#x2F;O.</div><br/></div></div><div id="42580545" class="c"><input type="checkbox" id="c-42580545" checked=""/><div class="controls bullet"><span class="by">ocean_moist</span><span>|</span><a href="#42580031">parent</a><span>|</span><a href="#42580621">prev</a><span>|</span><a href="#42580800">next</a><span>|</span><label class="collapse" for="c-42580545">[-]</label><label class="expand" for="c-42580545">[1 more]</label></div><br/><div class="children"><div class="content">This is actually a common pattern called &quot;model distilling&quot;.[0]<p>[0] <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;distillation" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;distillation</a></div><br/></div></div></div></div><div id="42580800" class="c"><input type="checkbox" id="c-42580800" checked=""/><div class="controls bullet"><span class="by">ted_dunning</span><span>|</span><a href="#42580031">prev</a><span>|</span><a href="#42577083">next</a><span>|</span><label class="collapse" for="c-42580800">[-]</label><label class="expand" for="c-42580800">[1 more]</label></div><br/><div class="children"><div class="content">This idea of machine-curated or machine-generated training data goes back thirty years or so under different names, at least in analogy.<p>A classic thing to do with ancient fraud detection models was<p>a) train a large&#x2F;complex model on a reasonably accurate dataset<p>b) select training examples from the original training data and other examples that the original model gets right.<p>c) train a simpler model on the filtered dataset<p>This worked very well in the simpler world of simple classifiers particularly when the original training data had errors in it. Trying to fit these errors made the first model more complex (and still it often failed). The subset of training examples excluded many or even most of the erroneous examples and also excluded subtle cases that were impossible to learn within the complexity bounds of the time.</div><br/></div></div><div id="42577083" class="c"><input type="checkbox" id="c-42577083" checked=""/><div class="controls bullet"><span class="by">cjohnson318</span><span>|</span><a href="#42580800">prev</a><span>|</span><a href="#42578362">next</a><span>|</span><label class="collapse" for="c-42577083">[-]</label><label class="expand" for="c-42577083">[3 more]</label></div><br/><div class="children"><div class="content">You can have small languages, sure, but then you run into awkward extended clarifying clauses. The thing that makes languages difficult is that almost all vocabulary is sparse. The &quot;Top N&quot; words in a language are always pronouns, prepositions, articles, and the conjugations of the top 12 or so verbs: to be, to have, to do, to go, to come, to say, to give, etc. This is the reason that &quot;Top N Words of Language X&quot; and &quot;Learn the Top 50% of Words in Language X&quot; listicles&#x2F;videos are always disappointing.</div><br/><div id="42577479" class="c"><input type="checkbox" id="c-42577479" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42577083">parent</a><span>|</span><a href="#42577928">next</a><span>|</span><label class="collapse" for="c-42577479">[-]</label><label class="expand" for="c-42577479">[1 more]</label></div><br/><div class="children"><div class="content">But they seem to use much wider grammars, because their (synthetic) dataset is a bunch of coherent stories at the level of 3-4 y.o. children.<p>I would consider the &quot;Simple English Wikipedia&quot; the next training set &#x2F; benchmark.</div><br/></div></div><div id="42577928" class="c"><input type="checkbox" id="c-42577928" checked=""/><div class="controls bullet"><span class="by">Pikamander2</span><span>|</span><a href="#42577083">parent</a><span>|</span><a href="#42577479">prev</a><span>|</span><a href="#42578362">next</a><span>|</span><label class="collapse" for="c-42577928">[-]</label><label class="expand" for="c-42577928">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an oddly relevant skit of this concept in the American version of The Office: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_K-L9uhsBLM" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_K-L9uhsBLM</a></div><br/></div></div></div></div><div id="42578362" class="c"><input type="checkbox" id="c-42578362" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42577083">prev</a><span>|</span><a href="#42578385">next</a><span>|</span><label class="collapse" for="c-42578362">[-]</label><label class="expand" for="c-42578362">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d guess that the ability of a very small model to do well on the TinyStories dataset isn&#x27;t just because of the limited 3-4yr old vocabulary, but also because of it being an LLM-generated dataset.<p>LLM-generated content (synthetic data) is easier than human generated text for an LLM to learn because it was auto-regressively generated, and therefore should be possible to auto-regressively predict.<p>It&#x27;s surprising that LLMs do as well as they do attempting to predict human generated training samples where there is no guarantee that the predictive signal is actually contained in the sample (it may just be something in the mind of the human that generated it).<p>I&#x27;ve got to wonder what the impact on generation is of an LLM only trained on synthetic LLM-generated data? I&#x27;d guess it wouldn&#x27;t be as robust as one that had learned to handle more uncertainty.</div><br/><div id="42580171" class="c"><input type="checkbox" id="c-42580171" checked=""/><div class="controls bullet"><span class="by">kouteiheika</span><span>|</span><a href="#42578362">parent</a><span>|</span><a href="#42578385">next</a><span>|</span><label class="collapse" for="c-42580171">[-]</label><label class="expand" for="c-42580171">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;d guess that the ability of a very small model to do well on the TinyStories dataset isn&#x27;t just because of the limited 3-4yr old vocabulary, but also because of it being an LLM-generated dataset.<p>You guess is correct. The level of vocabulary has little to do with it. There was a paper about this a while back (sorry, can&#x27;t find the link) where they found that the model still learned just as well when they increased the complexity of the text, as long as the texts were LLM generated.</div><br/></div></div></div></div><div id="42578385" class="c"><input type="checkbox" id="c-42578385" checked=""/><div class="controls bullet"><span class="by">raymv</span><span>|</span><a href="#42578362">prev</a><span>|</span><a href="#42577540">next</a><span>|</span><label class="collapse" for="c-42578385">[-]</label><label class="expand" for="c-42578385">[1 more]</label></div><br/><div class="children"><div class="content">Trained a GPT-2 like model on the dataset a while back, here&#x27;s the source code and some results for anyone interested:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;raymond-van&#x2F;gpt-tinystories">https:&#x2F;&#x2F;github.com&#x2F;raymond-van&#x2F;gpt-tinystories</a></div><br/></div></div><div id="42577540" class="c"><input type="checkbox" id="c-42577540" checked=""/><div class="controls bullet"><span class="by">fl0id</span><span>|</span><a href="#42578385">prev</a><span>|</span><a href="#42580651">next</a><span>|</span><label class="collapse" for="c-42577540">[-]</label><label class="expand" for="c-42577540">[1 more]</label></div><br/><div class="children"><div class="content">They also describe a new benchmark &#x2F; evaluation, but tbh is there any evidence that this even works? (telling GPT-4 to check the output as if it were checking student essays) We know it cannot really do this, and the model used will not even stay consistent if there are updates.</div><br/></div></div><div id="42580651" class="c"><input type="checkbox" id="c-42580651" checked=""/><div class="controls bullet"><span class="by">ted_dunning</span><span>|</span><a href="#42577540">prev</a><span>|</span><a href="#42577686">next</a><span>|</span><label class="collapse" for="c-42580651">[-]</label><label class="expand" for="c-42580651">[1 more]</label></div><br/><div class="children"><div class="content">This is good (though old) work. One interesting aspect is that even though they tout their models ability to produce grammatical text, their abstract has a number of grammatical errors.<p>On a dark night in a melancholic mood, that might seem to bear on the question of what it is to be truly human and bar that we set for (linguistically) anthropomorphic automatons like LLMs.</div><br/></div></div><div id="42577686" class="c"><input type="checkbox" id="c-42577686" checked=""/><div class="controls bullet"><span class="by">bigmattystyles</span><span>|</span><a href="#42580651">prev</a><span>|</span><a href="#42579322">next</a><span>|</span><label class="collapse" for="c-42577686">[-]</label><label class="expand" for="c-42577686">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been curious about the opposite - a lot of times, I&#x27;ll put a few keywords that get to the point of what I want, but it&#x27;s incoherent English in - and yet, often the output is on point.</div><br/><div id="42578400" class="c"><input type="checkbox" id="c-42578400" checked=""/><div class="controls bullet"><span class="by">Suppafly</span><span>|</span><a href="#42577686">parent</a><span>|</span><a href="#42579322">next</a><span>|</span><label class="collapse" for="c-42578400">[-]</label><label class="expand" for="c-42578400">[1 more]</label></div><br/><div class="children"><div class="content">I know natural language is sorta the gold standard for a lot of these models, but honestly I could see a lot of utility out of a stripped down language set, similar to how you used to be able to search google back in the day before they tried to make it easier.</div><br/></div></div></div></div><div id="42579322" class="c"><input type="checkbox" id="c-42579322" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42577686">prev</a><span>|</span><a href="#42577814">next</a><span>|</span><label class="collapse" for="c-42579322">[-]</label><label class="expand" for="c-42579322">[1 more]</label></div><br/><div class="children"><div class="content">(2023), as someone mentioned.<p>It&#x27;s encouraging to see how much can be done with tiny models.<p>Still need to crack &quot;I don&#x27;t know&quot; recognition, so you can start with a tiny model and then pass the buck to a bigger model for hard questions. That will enormously reduce the cost of &quot;AI&quot; customer support.</div><br/></div></div><div id="42577814" class="c"><input type="checkbox" id="c-42577814" checked=""/><div class="controls bullet"><span class="by">niemandhier</span><span>|</span><a href="#42579322">prev</a><span>|</span><a href="#42577976">next</a><span>|</span><label class="collapse" for="c-42577814">[-]</label><label class="expand" for="c-42577814">[1 more]</label></div><br/><div class="children"><div class="content">This question is also quite possible the most promising way to get an upper bound on the Kolmogorov complexity of human language.</div><br/></div></div><div id="42577976" class="c"><input type="checkbox" id="c-42577976" checked=""/><div class="controls bullet"><span class="by">lenerdenator</span><span>|</span><a href="#42577814">prev</a><span>|</span><a href="#42578654">next</a><span>|</span><label class="collapse" for="c-42577976">[-]</label><label class="expand" for="c-42577976">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Coherent&quot; seems relatively subjective, no?<p>Could you get an LLM to generate &quot;coherent&quot; conversational Geordie English? Probably, but my Midwestern ear isn&#x27;t going to be able to understand what they&#x27;re saying.</div><br/></div></div><div id="42578654" class="c"><input type="checkbox" id="c-42578654" checked=""/><div class="controls bullet"><span class="by">mclau156</span><span>|</span><a href="#42577976">prev</a><span>|</span><a href="#42579182">next</a><span>|</span><label class="collapse" for="c-42578654">[-]</label><label class="expand" for="c-42578654">[1 more]</label></div><br/><div class="children"><div class="content">Side note but is it really that crazy for Github to implement a feature to see file size of a repo?</div><br/></div></div><div id="42579182" class="c"><input type="checkbox" id="c-42579182" checked=""/><div class="controls bullet"><span class="by">lutusp</span><span>|</span><a href="#42578654">prev</a><span>|</span><a href="#42580949">next</a><span>|</span><label class="collapse" for="c-42579182">[-]</label><label class="expand" for="c-42579182">[1 more]</label></div><br/><div class="children"><div class="content">Decades ago, prior to the existence of personal computers, when a &quot;computer&quot; was a glassed-in room staffed by lab-coat-wearing technicians (picture John Von Neumann standing next to the first stored-program computer: <a href="https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2012&#x2F;feb&#x2F;26&#x2F;first-computers-john-von-neumann" rel="nofollow">https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2012&#x2F;feb&#x2F;26&#x2F;first-com...</a>), someone reduced an entire printed book (or more than one) to a word-token decision tree, at great cost and effort, just to see what would happen.<p>I can&#x27;t find the original paper, but with an appropriate amount of pseudorandomness to avoid dead ends, this primitive algorithm would generate the occasional sentence that almost made sense and that bore little resemblance to the original data.<p>Because of the state of computer technology it was a massive effort and a source of general astonishment. I suspect we&#x27;re now recreating that minimal environment, this time with better ways to curate the data for small size and maximum drama.<p>Let&#x27;s remember that a modern GPT isn&#x27;t far removed from that scheme -- not really.</div><br/></div></div><div id="42580949" class="c"><input type="checkbox" id="c-42580949" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#42579182">prev</a><span>|</span><a href="#42578303">next</a><span>|</span><label class="collapse" for="c-42580949">[-]</label><label class="expand" for="c-42580949">[1 more]</label></div><br/><div class="children"><div class="content">Try this one instead :) <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;HuggingFaceTB&#x2F;SmolLM-135M-Instruct" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;HuggingFaceTB&#x2F;SmolLM-135M-Instruct</a> Much better, a modern attempt at a small model</div><br/></div></div></div></div></div></div></div></body></html>