<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687683650533" as="style"/><link rel="stylesheet" href="styles.css?v=1687683650533"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road">Many in the AI field think the bigger-is-better approach is running out of road</a> <span class="domain">(<a href="https://www.economist.com">www.economist.com</a>)</span></div><div class="subtext"><span>pseudolus</span> | <span>203 comments</span></div><br/><div><div id="36462587" class="c"><input type="checkbox" id="c-36462587" checked=""/><div class="controls bullet"><span class="by">CrzyLngPwd</span><span>|</span><a href="#36463564">next</a><span>|</span><label class="collapse" for="c-36462587">[-]</label><label class="expand" for="c-36462587">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;XwWTi" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.is&#x2F;XwWTi</a></div><br/></div></div><div id="36463564" class="c"><input type="checkbox" id="c-36463564" checked=""/><div class="controls bullet"><span class="by">michaelteter</span><span>|</span><a href="#36462587">prev</a><span>|</span><a href="#36464924">next</a><span>|</span><label class="collapse" for="c-36463564">[-]</label><label class="expand" for="c-36463564">[69 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the fundamental problem that LLM&#x27;s don&#x27;t actually understand anything (as greater concepts), but rather operate as complex probability machines?<p>My 2 month active experience with ChatGPT-4 gave me the following takeaways:<p>- when it&#x27;s right, it&#x27;s amazing; and when you, the operator, can recognize the niche use case where it performs really well, it can be a game-changer (although you could have programmed a tool to do the same narrow task)<p>- when it&#x27;s a little wrong, you (the expert) can fix the issue and move on without friction<p>- when it&#x27;s any amount of wrong and you are less than an expert, or specifically you are completely unfamiliar with the topic, you can waste an immense amount of time researching the output and&#x2F;or iterating with the system to refine the result<p>Initially I thought it was a 10:1 ratio of performance to effort.  But finally (as a developer) I settled on (1 to 1.5):1.  It basically just changed the game for me from doing the actual hard work to working out how to tease the system into producing a reasonable result.  And in the process (same as with co-pilot) I started to recognize how it was leading me to change my habits to avoid thinking and effort and instead rely on an external brain.  If you could have a reliable external brain always available, that would be a fair trade.  But when the external &quot;brain&quot; is unreliable and only available via certain interfaces, it&#x27;s better to train yourself to be proactive, voracious (with respect to documentation), and tolerant of learning&#x2F;producing cycles.<p>I once thought it would be a game-changer.  Now I realize it is a game-changer, but in a similar way that offshoring was... it didn&#x27;t improve or solve any problems, but it merely changed the work.</div><br/><div id="36464753" class="c"><input type="checkbox" id="c-36464753" checked=""/><div class="controls bullet"><span class="by">bemmu</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36464282">next</a><span>|</span><label class="collapse" for="c-36464753">[-]</label><label class="expand" for="c-36464753">[35 more]</label></div><br/><div class="children"><div class="content">I’m not convinced the language part of my brain isn’t just a complex probability machine, just with different trade-offs.</div><br/><div id="36465741" class="c"><input type="checkbox" id="c-36465741" checked=""/><div class="controls bullet"><span class="by">indigoabstract</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464753">parent</a><span>|</span><a href="#36465059">next</a><span>|</span><label class="collapse" for="c-36465741">[-]</label><label class="expand" for="c-36465741">[3 more]</label></div><br/><div class="children"><div class="content">I am pretty sure that my understanding of something (or lack of) is not encoded into words and probabilities. It&#x27;s more like a feeling of &quot;I got this figured out&quot; or &quot;I haven&#x27;t grasped this&quot;.<p>Words seem more like a protocol to express some internal model&#x2F;state in the brain and can never capture the entire actual state, only a small part of it. But since we&#x27;re not telepaths, we obviously need to use words to exchange information.</div><br/><div id="36466084" class="c"><input type="checkbox" id="c-36466084" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465741">parent</a><span>|</span><a href="#36465889">next</a><span>|</span><label class="collapse" for="c-36466084">[-]</label><label class="expand" for="c-36466084">[1 more]</label></div><br/><div class="children"><div class="content">If brains aren&#x27;t a complex probability machine, how is it possible that people inconsistently get the same sort of math problems wright and wrong in an inconsistent manner? Or mis-speak?<p>It is undeniable that human reasoning is a stochastic process. Otherwise it wouldn&#x27;t be reasonable for people to make mistakes after learning something. Especially inconsistent mistakes, like when we give people 10,000 addition problems to do in a row it&#x27;d be reasonable for them to get a few of them wrong.</div><br/></div></div><div id="36465889" class="c"><input type="checkbox" id="c-36465889" checked=""/><div class="controls bullet"><span class="by">Tenoke</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465741">parent</a><span>|</span><a href="#36466084">prev</a><span>|</span><a href="#36465059">next</a><span>|</span><label class="collapse" for="c-36465889">[-]</label><label class="expand" for="c-36465889">[1 more]</label></div><br/><div class="children"><div class="content">The feelings are just how the underlying probabilities are presented to the conscious part of your thinking.<p>You are clearly not consciously noting what your neurons are actually doing.</div><br/></div></div></div></div><div id="36465059" class="c"><input type="checkbox" id="c-36465059" checked=""/><div class="controls bullet"><span class="by">wzdd</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464753">parent</a><span>|</span><a href="#36465741">prev</a><span>|</span><a href="#36466211">next</a><span>|</span><label class="collapse" for="c-36465059">[-]</label><label class="expand" for="c-36465059">[1 more]</label></div><br/><div class="children"><div class="content">To expand on this, healthy human brains seem to be made of many interacting systems which function in different ways. It may be that in the future we view the search for one simple formula for &quot;intelligence&quot; &#x2F; &quot;understanding&quot; &#x2F; &quot;consciousness&quot; (pick your poorly-defined poison) the same way biologists view the concept of elan vital.</div><br/></div></div><div id="36466211" class="c"><input type="checkbox" id="c-36466211" checked=""/><div class="controls bullet"><span class="by">asddubs</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464753">parent</a><span>|</span><a href="#36465059">prev</a><span>|</span><a href="#36465636">next</a><span>|</span><label class="collapse" for="c-36466211">[-]</label><label class="expand" for="c-36466211">[1 more]</label></div><br/><div class="children"><div class="content">maybe, but the language part of your brain isn&#x27;t the only part of your brain</div><br/></div></div><div id="36465636" class="c"><input type="checkbox" id="c-36465636" checked=""/><div class="controls bullet"><span class="by">chaosjevil</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464753">parent</a><span>|</span><a href="#36466211">prev</a><span>|</span><a href="#36464787">next</a><span>|</span><label class="collapse" for="c-36465636">[-]</label><label class="expand" for="c-36465636">[4 more]</label></div><br/><div class="children"><div class="content">Nah. Human utterances convey purpose on a discursive level; including your comment or mine. We say stuff because we want to do something, like showing [dis]agreement or inform another speaker or change the actions of the other speaker. This is not just probabilistic - it&#x27;s a way to handle the world.<p>In the meantime those large language models simply predict the next word based on the preceding words.</div><br/><div id="36466168" class="c"><input type="checkbox" id="c-36466168" checked=""/><div class="controls bullet"><span class="by">MrScruff</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465636">parent</a><span>|</span><a href="#36465801">next</a><span>|</span><label class="collapse" for="c-36466168">[-]</label><label class="expand" for="c-36466168">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re able to do something analogous to reinforcement learning (take on new example data to update our &#x27;weights&#x27;).<p>Why do I spend time debating these ideas on Hacker News? Probably the underlying motivation is improving the reliability of my model of the world, which over my lifetime and the lifetimes of creatures before me has led to (somewhat indirectly) positive outcomes in survival and reproduction.<p>Is my model of the world that different to that of an LLM? I&#x27;m sure it is in many ways, but I expect their are similarities as well. An LLMs model encodes in a form a bunch of higher order relationships between concepts as defined by the word embedding. I think my brain encodes something similar, although the relationships are probably orders of magnitude more complex than the relationships encoded with GPT-4.</div><br/></div></div><div id="36465748" class="c"><input type="checkbox" id="c-36465748" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465636">parent</a><span>|</span><a href="#36465801">prev</a><span>|</span><a href="#36464787">next</a><span>|</span><label class="collapse" for="c-36465748">[-]</label><label class="expand" for="c-36465748">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We say stuff because we want to do something, like showing [dis]agreement or inform another speaker or change the actions of the other speaker.<p>My LLaMA instance is absolutely capable of this. ChatGPT shows a very, very narrow range of possible LLM behaviors.</div><br/></div></div></div></div><div id="36464787" class="c"><input type="checkbox" id="c-36464787" checked=""/><div class="controls bullet"><span class="by">thereare5lights</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464753">parent</a><span>|</span><a href="#36465636">prev</a><span>|</span><a href="#36465400">next</a><span>|</span><label class="collapse" for="c-36464787">[-]</label><label class="expand" for="c-36464787">[23 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no evidence for nondeterminism in the human brain</div><br/><div id="36464863" class="c"><input type="checkbox" id="c-36464863" checked=""/><div class="controls bullet"><span class="by">anikan_vader</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464787">parent</a><span>|</span><a href="#36465255">next</a><span>|</span><label class="collapse" for="c-36464863">[-]</label><label class="expand" for="c-36464863">[21 more]</label></div><br/><div class="children"><div class="content">There is plenty of evidence for non-determinism in matter, which the brain is notably made out of.</div><br/><div id="36464966" class="c"><input type="checkbox" id="c-36464966" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464863">parent</a><span>|</span><a href="#36465255">next</a><span>|</span><label class="collapse" for="c-36464966">[-]</label><label class="expand" for="c-36464966">[20 more]</label></div><br/><div class="children"><div class="content">Not necessarily. Everything is deterministic above the quantum level, and it&#x27;s possible that quantum non-determinism is the result of deterministic processes we can&#x27;t see.<p>Lots of deterministic processes (like PRNGs) look random from the outside - that&#x27;s what chaos theory is about. I think it&#x27;s likely that everything in the universe is deterministic.</div><br/><div id="36465482" class="c"><input type="checkbox" id="c-36465482" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464966">parent</a><span>|</span><a href="#36465124">next</a><span>|</span><label class="collapse" for="c-36465482">[-]</label><label class="expand" for="c-36465482">[1 more]</label></div><br/><div class="children"><div class="content">In most systems, if you simulate them for sufficiently long, macroscopic behavior depends on quantum effects. For example, if you simulate simple newtonian gravity on three bodies, your numeric precision quickly grows such that you need to know the position of the objects more accurately than Heisenberg allows.</div><br/></div></div><div id="36465124" class="c"><input type="checkbox" id="c-36465124" checked=""/><div class="controls bullet"><span class="by">kedean</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464966">parent</a><span>|</span><a href="#36465482">prev</a><span>|</span><a href="#36465132">next</a><span>|</span><label class="collapse" for="c-36465124">[-]</label><label class="expand" for="c-36465124">[8 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s not actually possible to simulate a system within the confines of physics, does it being deterministic actually matter outside of thought experiments?<p>I feel like a random system and a deterministic system which cannot be simulated are effectively the same thing.</div><br/><div id="36465519" class="c"><input type="checkbox" id="c-36465519" checked=""/><div class="controls bullet"><span class="by">zappchance</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465124">parent</a><span>|</span><a href="#36465132">next</a><span>|</span><label class="collapse" for="c-36465519">[-]</label><label class="expand" for="c-36465519">[7 more]</label></div><br/><div class="children"><div class="content">It only matters in matters of free will and ethics. One actual scenario where it&#x27;s relavant would be the discussion around the criminal justice system. If the universe is deterministic, how can punitive justice be justified?</div><br/><div id="36465696" class="c"><input type="checkbox" id="c-36465696" checked=""/><div class="controls bullet"><span class="by">tchaffee</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465519">parent</a><span>|</span><a href="#36465573">next</a><span>|</span><label class="collapse" for="c-36465696">[-]</label><label class="expand" for="c-36465696">[1 more]</label></div><br/><div class="children"><div class="content">Why would you need to justify a punishment that was already predetermined? If you are going to excuse crime with the no free will argument you can excuse the punishers with that argument too.</div><br/></div></div><div id="36465573" class="c"><input type="checkbox" id="c-36465573" checked=""/><div class="controls bullet"><span class="by">brutusborn</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465519">parent</a><span>|</span><a href="#36465696">prev</a><span>|</span><a href="#36465590">next</a><span>|</span><label class="collapse" for="c-36465573">[-]</label><label class="expand" for="c-36465573">[2 more]</label></div><br/><div class="children"><div class="content">It can be justified in a deterministic universe because it makes criminals less likely to commit crime in future.<p>As a recipient of punitive justice myself, being punished had a tangible effect on how I thought about crime and thus how I behaved post-punishment. Whether you believe that was deterministic or due to my own free will doesn’t change the outcome.</div><br/><div id="36465707" class="c"><input type="checkbox" id="c-36465707" checked=""/><div class="controls bullet"><span class="by">tchaffee</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465573">parent</a><span>|</span><a href="#36465590">next</a><span>|</span><label class="collapse" for="c-36465707">[-]</label><label class="expand" for="c-36465707">[1 more]</label></div><br/><div class="children"><div class="content">Punishment is not effective for most people in reducing crime. One thing that is effective is increasing the perceived risk of getting caught. People rarely commit crimes when they are sure they will be caught.</div><br/></div></div></div></div><div id="36465590" class="c"><input type="checkbox" id="c-36465590" checked=""/><div class="controls bullet"><span class="by">d-cc</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465519">parent</a><span>|</span><a href="#36465573">prev</a><span>|</span><a href="#36465628">next</a><span>|</span><label class="collapse" for="c-36465590">[-]</label><label class="expand" for="c-36465590">[1 more]</label></div><br/><div class="children"><div class="content">The free will &quot;debate&quot;, as most consider it, is an utter joke. A tumor or large amount of kinetic energy to the right parts of your, or anybody else&#x27;s, brain can turn them into an unredeemable monster.</div><br/></div></div><div id="36465628" class="c"><input type="checkbox" id="c-36465628" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465519">parent</a><span>|</span><a href="#36465590">prev</a><span>|</span><a href="#36465132">next</a><span>|</span><label class="collapse" for="c-36465628">[-]</label><label class="expand" for="c-36465628">[2 more]</label></div><br/><div class="children"><div class="content">The point of punishment is to discourage you (and other people) from doing that action in the future. You don&#x27;t need free will for that.</div><br/><div id="36465714" class="c"><input type="checkbox" id="c-36465714" checked=""/><div class="controls bullet"><span class="by">tchaffee</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465628">parent</a><span>|</span><a href="#36465132">next</a><span>|</span><label class="collapse" for="c-36465714">[-]</label><label class="expand" for="c-36465714">[1 more]</label></div><br/><div class="children"><div class="content">If you don&#x27;t have free will then how would something discourage you from doing something that is already determined you will do? That doesn&#x27;t make any sense.</div><br/></div></div></div></div></div></div></div></div><div id="36465132" class="c"><input type="checkbox" id="c-36465132" checked=""/><div class="controls bullet"><span class="by">aoeusnth1</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464966">parent</a><span>|</span><a href="#36465124">prev</a><span>|</span><a href="#36465306">next</a><span>|</span><label class="collapse" for="c-36465132">[-]</label><label class="expand" for="c-36465132">[1 more]</label></div><br/><div class="children"><div class="content">And how is a seeded RNG that an LLM uses any logically different from a deterministic brain? I’m not sure why any of this physics would be relevant to the functional behavior of the brain vs an LLM.</div><br/></div></div><div id="36465306" class="c"><input type="checkbox" id="c-36465306" checked=""/><div class="controls bullet"><span class="by">phreeza</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464966">parent</a><span>|</span><a href="#36465132">prev</a><span>|</span><a href="#36465237">next</a><span>|</span><label class="collapse" for="c-36465306">[-]</label><label class="expand" for="c-36465306">[6 more]</label></div><br/><div class="children"><div class="content">The problem is that any hidden variable resolving quantum indeterminacy would have to be non -local, i.e. able to propagate itself faster than light, which would also violate our understanding of the world quite a bit.</div><br/><div id="36465362" class="c"><input type="checkbox" id="c-36465362" checked=""/><div class="controls bullet"><span class="by">111111IIIIIII</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465306">parent</a><span>|</span><a href="#36465335">next</a><span>|</span><label class="collapse" for="c-36465362">[-]</label><label class="expand" for="c-36465362">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>which would also violate our understanding of the world quite a bit.</i><p>Are we discussing science or public relations?</div><br/></div></div><div id="36465335" class="c"><input type="checkbox" id="c-36465335" checked=""/><div class="controls bullet"><span class="by">llaolleh</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465306">parent</a><span>|</span><a href="#36465362">prev</a><span>|</span><a href="#36465358">next</a><span>|</span><label class="collapse" for="c-36465335">[-]</label><label class="expand" for="c-36465335">[2 more]</label></div><br/><div class="children"><div class="content">It won&#x27;t be the first time our understanding is flipped upside down if such a framework of thought arises.</div><br/><div id="36465592" class="c"><input type="checkbox" id="c-36465592" checked=""/><div class="controls bullet"><span class="by">phreeza</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465335">parent</a><span>|</span><a href="#36465358">next</a><span>|</span><label class="collapse" for="c-36465592">[-]</label><label class="expand" for="c-36465592">[1 more]</label></div><br/><div class="children"><div class="content">Yea I don&#x27;t disagree, just wanted to point out that the hidden variable would not mean that matter actually behaves like a classical mechanics machine.</div><br/></div></div></div></div><div id="36465358" class="c"><input type="checkbox" id="c-36465358" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465306">parent</a><span>|</span><a href="#36465335">prev</a><span>|</span><a href="#36465237">next</a><span>|</span><label class="collapse" for="c-36465358">[-]</label><label class="expand" for="c-36465358">[2 more]</label></div><br/><div class="children"><div class="content">Locality may also be an abstraction.<p>I find it very intriguing that a &quot;speed of light&quot; emerges automatically in Conway&#x27;s Game of Life. It&#x27;s not built into the system, but shows up from the convolutional update rule.</div><br/><div id="36465481" class="c"><input type="checkbox" id="c-36465481" checked=""/><div class="controls bullet"><span class="by">dgfl</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465358">parent</a><span>|</span><a href="#36465237">next</a><span>|</span><label class="collapse" for="c-36465481">[-]</label><label class="expand" for="c-36465481">[1 more]</label></div><br/><div class="children"><div class="content">Your comparison with Conway&#x27;s Game of Life is interesting, since that&#x27;s inherently local.<p>More importantly, I&#x27;m skeptical towards non-locality because it is an extremely strong assumption with very weak effects: the only place it really shows up is in post-correlations between measurements of previously entangled systems, which notably cannot transfer any information faster than light (in fact, they require classical communication to even be noticed).
Moreover, the only way to get entangled systems in the first place is through local interactions.<p>By believing in non-local hidden variables you get a deterministic universe with a mysterious, otherwise undetectable ether that instantaneously notifies quantum entities that they should update their behavior.
By not believing in them, you get rid of the only n
on-local &quot;phenomenon&quot; in physics (really, more of an interpretation) but you have to accept that some things are fundamentally random.<p>Easy choice, if you ask me (or most of the physics community).</div><br/></div></div></div></div></div></div><div id="36465237" class="c"><input type="checkbox" id="c-36465237" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464966">parent</a><span>|</span><a href="#36465306">prev</a><span>|</span><a href="#36465255">next</a><span>|</span><label class="collapse" for="c-36465237">[-]</label><label class="expand" for="c-36465237">[3 more]</label></div><br/><div class="children"><div class="content">Or the Many Worlds Interpretation is the correct understanding of quantum mechanics. The MWI people will say that indeterminism comes from the Copenhagen idea of there being a random collapse. But since measuring devices and human brains are also quantum systems, there&#x27;s no reason to propose a collapse. Decoherence would be the reason we only see one result.</div><br/><div id="36465490" class="c"><input type="checkbox" id="c-36465490" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465237">parent</a><span>|</span><a href="#36465255">next</a><span>|</span><label class="collapse" for="c-36465490">[-]</label><label class="expand" for="c-36465490">[2 more]</label></div><br/><div class="children"><div class="content">Maybe, but there&#x27;s no data to support one interpretation over another.<p>I believe that all quantum interpretations are incomplete and therefore wrong. Quantum mechanics is an abstraction over a deeper level of physics we can&#x27;t measure yet.</div><br/><div id="36465711" class="c"><input type="checkbox" id="c-36465711" checked=""/><div class="controls bullet"><span class="by">trashtester</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465490">parent</a><span>|</span><a href="#36465255">next</a><span>|</span><label class="collapse" for="c-36465711">[-]</label><label class="expand" for="c-36465711">[1 more]</label></div><br/><div class="children"><div class="content">Actually, there is a reason to support one interpretation over another. Specifically that it&#x27;s really hard to define what an &quot;observer&quot; is in the Copenhagen interpretation.<p>The MW&#x2F;Everett interpretation is what we have left if we remove the things we cannot define.<p>There are also reasons to believe other interpretations. For instance, if you&#x27;re religious, that could pull you towards the Bohr interpretation, since it may make it easier to assume that observation could be linked to an immortal soul.<p>In any case, it&#x27;s not natural to assume that below QM there exists a reality that is more similar to our instinctual world model than QM is. If anything, anything below it is likely to be even more abstract and hard to comprehend.<p>Or it could be that the principles of QM applies all the way down, just as we&#x27;ve seen for the pieces of the SM that we solved after QM was first introduced (strong force, electroweak force).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36465255" class="c"><input type="checkbox" id="c-36465255" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464787">parent</a><span>|</span><a href="#36464863">prev</a><span>|</span><a href="#36465400">next</a><span>|</span><label class="collapse" for="c-36465255">[-]</label><label class="expand" for="c-36465255">[1 more]</label></div><br/><div class="children"><div class="content">A recent discussion: <i>People can be convinced they committed a crime that never happened</i> <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36367147">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36367147</a></div><br/></div></div></div></div><div id="36465400" class="c"><input type="checkbox" id="c-36465400" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464753">parent</a><span>|</span><a href="#36464787">prev</a><span>|</span><a href="#36464282">next</a><span>|</span><label class="collapse" for="c-36465400">[-]</label><label class="expand" for="c-36465400">[2 more]</label></div><br/><div class="children"><div class="content">Yes. But modeling language doesn’t mean you’ve made a good model of anything else. Now, I think LLMs have incidentally modeled a ton of small simple systems very well. But those systems, and the ones LLMs haven’t mastered, would be better off modeled with models built for those systems specifically.</div><br/><div id="36465734" class="c"><input type="checkbox" id="c-36465734" checked=""/><div class="controls bullet"><span class="by">bemmu</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465400">parent</a><span>|</span><a href="#36464282">next</a><span>|</span><label class="collapse" for="c-36465734">[-]</label><label class="expand" for="c-36465734">[1 more]</label></div><br/><div class="children"><div class="content">Yup, while an LLM might know a cat in language contexts, I know what petting a cat feels like, both physically and emotionally. Can run little physics or visual simulations in my head etc., and have needs and drives which are obviously all still missing.</div><br/></div></div></div></div></div></div><div id="36464282" class="c"><input type="checkbox" id="c-36464282" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36464753">prev</a><span>|</span><a href="#36463859">next</a><span>|</span><label class="collapse" for="c-36464282">[-]</label><label class="expand" for="c-36464282">[3 more]</label></div><br/><div class="children"><div class="content">I like your takeaways and reflection, especially the &quot;changes the game&quot; idea. There is an analogy with pocket calculators and mental arithmetic. Personally I&#x27;m more comfortable reaching for the pocket calculator than offloading all thinking to an LLM. On the other hand, it&#x27;s not so long ago that manual calculation was a specialized occupation. I could maybe see software coding becoming automated just as calculation was -- except for the fact that calculations are <i>much</i> easier to specify than software.<p>I don&#x27;t quite get where you&#x27;re coming from with &quot;LLM&#x27;s don&#x27;t actually understand anything (as greater concepts)&quot;. I have heard the view coming from researchers that the larger models <i>do</i> form representations of higher-order information structures (&quot;concepts&quot;). Perhaps what you&#x27;re getting at is that current models don&#x27;t encode <i>enough</i> higher-order structure to deal accurately with your domain? Whether the models can be made to do so seems like an open question to me. The boosters say it will be here by next year.</div><br/><div id="36465525" class="c"><input type="checkbox" id="c-36465525" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464282">parent</a><span>|</span><a href="#36464735">next</a><span>|</span><label class="collapse" for="c-36465525">[-]</label><label class="expand" for="c-36465525">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t quite get where you&#x27;re coming from with &quot;LLM&#x27;s don&#x27;t actually understand anything<p>Well, Wikipedia doesn’t understand anything, despite having a lot of knowledge encoded in it. This is similar in that it approximates the output of someone who can generate the world’s written word, but there’s a big gap between the monkeys that wrote the original text and the machine that now regurgitates it. What we know is that our written word has enough encoded in it that we can generate new combinations. But one day into a truly novel problem, LLM’s would have no idea.<p>I kept trying to get ChatGPT 4 to generate code with an AWS API that was released in ~2020. Couldn’t do it. Kept predicting the earlier text because that was the bulk of what it saw. A year of text (to the 2021 cutoff) and it was still unable to let go of the old, now wrong, “knowledge”. Zero understanding, just parroting. No coder with a year of training on a new API would be that wrong. It would say things like “you’re right I must not do X” and then do X again, in the same output.</div><br/></div></div><div id="36464735" class="c"><input type="checkbox" id="c-36464735" checked=""/><div class="controls bullet"><span class="by">sumanthvepa</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464282">parent</a><span>|</span><a href="#36465525">prev</a><span>|</span><a href="#36463859">next</a><span>|</span><label class="collapse" for="c-36464735">[-]</label><label class="expand" for="c-36464735">[1 more]</label></div><br/><div class="children"><div class="content">This observation is spot on. I was initially euphoric about LLMs like ChatGPT, but it is increasingly becoming obvious that unless you are yourself an expert in the subject you are using the LLM for and can therefore easily verify its accuracy, the output is not reliable enough to use without extensive manual verification. More importantly it is difficult to incorporate its output into larger automated workflows.</div><br/></div></div></div></div><div id="36463859" class="c"><input type="checkbox" id="c-36463859" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36464282">prev</a><span>|</span><a href="#36465772">next</a><span>|</span><label class="collapse" for="c-36463859">[-]</label><label class="expand" for="c-36463859">[6 more]</label></div><br/><div class="children"><div class="content">&gt; <i>don&#x27;t actually understand anything (as greater concepts), but rather operate as complex probability machines?</i><p>But, things are defined by how they interact with the world around them.<p>A concept <i>is</i> its relations to other concepts.<p>Which does seem to be the general sort of thing that these models are trying to get at, even if they don&#x27;t seem to do a great job of it.</div><br/><div id="36464836" class="c"><input type="checkbox" id="c-36464836" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36463859">parent</a><span>|</span><a href="#36464754">next</a><span>|</span><label class="collapse" for="c-36464836">[-]</label><label class="expand" for="c-36464836">[1 more]</label></div><br/><div class="children"><div class="content"><i>A concept is its relations to other concepts.</i><p>No, that&#x27;s an <i>a priori</i> concept. <i>A posteriori</i> concepts comprise empirical knowledge which necessitates experience of the world [1].<p>Example: you can know <i>a priori</i> that &quot;all bachelors are unmarried.&quot; If I tell you &quot;Tom is a bachelor&quot; then you know that Tom is unmarried (assuming I tell the truth about Tom being a bachelor).<p>But if I say &quot;all bachelors are unhappy&quot; then you haven&#x27;t learned anything because knowledge about the happiness&#x2F;unhappiness of bachelors is an empirical question. To know whether or not I was telling the truth, you would need to conduct research about the real world, for example by conducting a survey of bachelors.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;A_priori_and_a_posteriori" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;A_priori_and_a_posteriori</a></div><br/></div></div><div id="36464754" class="c"><input type="checkbox" id="c-36464754" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36463859">parent</a><span>|</span><a href="#36464836">prev</a><span>|</span><a href="#36464630">next</a><span>|</span><label class="collapse" for="c-36464754">[-]</label><label class="expand" for="c-36464754">[2 more]</label></div><br/><div class="children"><div class="content">It’s using tokens not concepts.  ‘Apple’ the token could apply to a company or fruit, but tokens aren’t concepts.  It’s a fairly fundamental limitation on the approach that’s most noticeable while feeding it data it can’t basically copy  from it’s vast training data.<p>Thus the sharp drop off where it can for example guess the correct answer to some math problems yet get others that are conceptually identical completely wrong.</div><br/><div id="36466087" class="c"><input type="checkbox" id="c-36466087" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464754">parent</a><span>|</span><a href="#36464630">next</a><span>|</span><label class="collapse" for="c-36466087">[-]</label><label class="expand" for="c-36466087">[1 more]</label></div><br/><div class="children"><div class="content">They turn tokens into concepts internally.  Try asking ChatGPT &quot;can you eat Apple?&quot; and &quot;who is the CEO of Apple?&quot;.</div><br/></div></div></div></div><div id="36464630" class="c"><input type="checkbox" id="c-36464630" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36463859">parent</a><span>|</span><a href="#36464754">prev</a><span>|</span><a href="#36465772">next</a><span>|</span><label class="collapse" for="c-36464630">[-]</label><label class="expand" for="c-36464630">[2 more]</label></div><br/><div class="children"><div class="content">&gt; A concept is its relations to other concepts.<p>where does a concept begin? where does a concept end? what is a concept?</div><br/><div id="36465076" class="c"><input type="checkbox" id="c-36465076" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464630">parent</a><span>|</span><a href="#36465772">next</a><span>|</span><label class="collapse" for="c-36465076">[-]</label><label class="expand" for="c-36465076">[1 more]</label></div><br/><div class="children"><div class="content">A concept is an abstraction.<p>Abstractions are a way to characterize the behavior of complex systems. It&#x27;s not feasible to directly compute their behavior, but they still have predictable properties. Concepts let you handle emergence; you can manipulate an object as its own thing rather than a collection of atoms.<p>(And yes, I did just read Stephen Wolfram&#x27;s book and this idea is largely based on it. I think he has some delusions of grandeur but is also onto something.)</div><br/></div></div></div></div></div></div><div id="36465772" class="c"><input type="checkbox" id="c-36465772" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36463859">prev</a><span>|</span><a href="#36464952">next</a><span>|</span><label class="collapse" for="c-36465772">[-]</label><label class="expand" for="c-36465772">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLM&#x27;s don&#x27;t actually understand anything (as greater concepts), but rather operate as complex probability machines?<p>What does it mean to &quot;operate as a probability machine&quot;? And what does it mean to understand anything?<p>One recent example of understanding is that llms&#x2F;transformers learn to parse context free grammars via dynamic programming (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.02386" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.02386</a>). Basically they&#x27;ve understood what&#x27;s going on well enough to mold their neurons I to the optimal algorthm for parsing this kind of text.<p>I think they understand lots of things like this. Of course there&#x27;s other things they don&#x27;t understand or just pretend to understand.</div><br/></div></div><div id="36464952" class="c"><input type="checkbox" id="c-36464952" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36465772">prev</a><span>|</span><a href="#36464441">next</a><span>|</span><label class="collapse" for="c-36464952">[-]</label><label class="expand" for="c-36464952">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a game-changer in that it shows computers being able to climb levels of abstraction on their own.<p>In the past, if you wanted programs to work with a high-level idea, you had to explicitly hand it to them. Humans had to do the work of turning data into concepts. These generative AI systems are different - they can learn abstractions from data, and manipulate them in complex ways.<p>Is ChatGPT a good and accurate chatbot? Maybe not. But it&#x27;s a fundamental change in what computers are capable of.</div><br/></div></div><div id="36464441" class="c"><input type="checkbox" id="c-36464441" checked=""/><div class="controls bullet"><span class="by">syntheweave</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36464952">prev</a><span>|</span><a href="#36465198">next</a><span>|</span><label class="collapse" for="c-36464441">[-]</label><label class="expand" for="c-36464441">[2 more]</label></div><br/><div class="children"><div class="content">I do think there&#x27;s an element to it of &quot;uncanny valleyness&quot;. If you know absolutely nothing about a topic the authoritative tone and factualness of what it says is very appealing and even helpful in the same way that consulting an encyclopedia is helpful: it tells you of things you could investigate further that would never appear through a keyword search. But if you stop there, your knowledge is &quot;roughly encyclopedic&quot; which means it contains the hidden bias of some anonymous author, and not the harder-earned relationships of facts and logic.<p>If you use it to translate things between formal encodings(&quot;turn this into hexadecimal bytes, now role-play a lawyer arguing about why that is meaningful&quot;) it can produce occasionally useful aesthetic results and speed along tasks that would be challenging to model formally and don&#x27;t need a lot of rigor.<p>But once you start pushing it to be technically accurate in a narrow, measurable direction it flounders and the probabilistic element is revealed. Once, I asked it to translate a short string of Japanese characters and it confidently said that it was Kenshiro&#x27;s catch phrase from <i>Fist of the North Star</i>, &quot;Omae wa mou shinderu&quot; (you are already dead) which I could clearly see it wasn&#x27;t - not a single character matched. It&#x27;s just the thing if you need to learn some anime Japanese, though.</div><br/><div id="36464569" class="c"><input type="checkbox" id="c-36464569" checked=""/><div class="controls bullet"><span class="by">corey_moncure</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464441">parent</a><span>|</span><a href="#36465198">next</a><span>|</span><label class="collapse" for="c-36464569">[-]</label><label class="expand" for="c-36464569">[1 more]</label></div><br/><div class="children"><div class="content">This is very surprising to me.  I found ChatGPT&#x2F;4 to be extremely adept at translation between well asserted languages including English&#x2F;Japanese in which I am an expert in both. I&#x27;m curious how you managed to make it blow up.</div><br/></div></div></div></div><div id="36465198" class="c"><input type="checkbox" id="c-36465198" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36464441">prev</a><span>|</span><a href="#36465014">next</a><span>|</span><label class="collapse" for="c-36465198">[-]</label><label class="expand" for="c-36465198">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing that&#x27;s how people figured out it was getting &quot;nerfed&quot;. For Copilot, I kind of developed a relationship with the &quot;AI&quot;. I kind of expect what it is going to generate, and I use it as a shortcut to (find function names, write variable names, complete unit tests, etc...). On its own, it&#x27;s really bad to come up with the whole picture but as something that auto-completes you on steroid, it&#x27;s really perfect.<p>When it got nerfed, its perspective got completely out of whack and it&#x27;s making very different and inconsistent generation.</div><br/></div></div><div id="36465014" class="c"><input type="checkbox" id="c-36465014" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36465198">prev</a><span>|</span><a href="#36463687">next</a><span>|</span><label class="collapse" for="c-36465014">[-]</label><label class="expand" for="c-36465014">[1 more]</label></div><br/><div class="children"><div class="content">One field where it works rather well is in semantic data extraction. Do you remember the dream of semantic web? Having access to structured information from unstructured sources, auto-matching API functions and arguments to make interoperation easier. We can do that now.<p>Here is a sample where I used my own post (just copy pasted the raw text from the browser) and got this schema<p><pre><code>    comment:
      meta:
        points:
        author:
        time:
        action:
          parent:
          next:
          edit:
          delete:
        post:
      content:
      action:
        reply:
</code></pre>
It also guessed it was a Hacker News comment by the formatting of the meta section.</div><br/></div></div><div id="36463687" class="c"><input type="checkbox" id="c-36463687" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36465014">prev</a><span>|</span><a href="#36464886">next</a><span>|</span><label class="collapse" for="c-36463687">[-]</label><label class="expand" for="c-36463687">[11 more]</label></div><br/><div class="children"><div class="content">The next iteration will be trained on your own data where &quot;when it&#x27;s a little wrong, you (the expert) can fix the issue and move on without friction&quot; so that case will become &quot;when it&#x27;s right&quot; and some amount of &quot;when it&#x27;s any amount of wrong&quot; cases will become &quot;when it&#x27;s a little wrong&quot;. A few more cycles of this and we could be looking at GPT-10 which is a complete replacement for most tasks.</div><br/><div id="36464289" class="c"><input type="checkbox" id="c-36464289" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36463687">parent</a><span>|</span><a href="#36464113">next</a><span>|</span><label class="collapse" for="c-36464289">[-]</label><label class="expand" for="c-36464289">[8 more]</label></div><br/><div class="children"><div class="content">Better result from less data?<p>I doubt that.</div><br/><div id="36464455" class="c"><input type="checkbox" id="c-36464455" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464289">parent</a><span>|</span><a href="#36465806">next</a><span>|</span><label class="collapse" for="c-36464455">[-]</label><label class="expand" for="c-36464455">[4 more]</label></div><br/><div class="children"><div class="content">I mean &#x27;better result from less data&#x27; is at least a little bit possible. For example you can just clean out obviously bad data from the trillions of tokens data sets. It&#x27;s things like the subreddit where they are counting to a million or just like long lists of hash values in random cryptocurrency logs.<p>I agree that in the bigger picture this doesn&#x27;t matter, but it&#x27;s technically true that cleaning the data in some way would help.<p>A related project is TinyStories where they try to use good data for unlocking the LLM cognitive capabilities without requiring as many parameters or exaflops. Again, there is obviously a limit to this, and maybe the effort is better spent on just getting even more gigantic dataset instead of nitpicking the useless or redundant data in the dataset.</div><br/><div id="36465292" class="c"><input type="checkbox" id="c-36465292" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464455">parent</a><span>|</span><a href="#36465095">next</a><span>|</span><label class="collapse" for="c-36465292">[-]</label><label class="expand" for="c-36465292">[1 more]</label></div><br/><div class="children"><div class="content">I think TinyStories is a promising direction. I just wish we had an alternative to GPT-4 because supposedly you can&#x27;t use it to train other models.<p>Or maybe clarification of people publicly saying they are going to ignore that restriction.</div><br/></div></div><div id="36465095" class="c"><input type="checkbox" id="c-36465095" checked=""/><div class="controls bullet"><span class="by">hooande</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464455">parent</a><span>|</span><a href="#36465292">prev</a><span>|</span><a href="#36465806">next</a><span>|</span><label class="collapse" for="c-36465095">[-]</label><label class="expand" for="c-36465095">[2 more]</label></div><br/><div class="children"><div class="content">Data quality like you&#x27;re describing just doesn&#x27;t matter. GPT is trained on PEBIBYTES of data. Any individual reddit thread is an atom in a drop in a bucket. All of reddit is &lt; 2% of the total training data, regardless of quality.<p>Yes, the correct thing to do is get more data. Much more.</div><br/><div id="36465870" class="c"><input type="checkbox" id="c-36465870" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465095">parent</a><span>|</span><a href="#36465806">next</a><span>|</span><label class="collapse" for="c-36465870">[-]</label><label class="expand" for="c-36465870">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Data quality like you&#x27;re describing just doesn&#x27;t matter. GPT is trained on PEBIBYTES of data.<p>It matters a little bit, in a quantitative but not qualitative way. Probably with good data cleaning you could get as high quality result with only one pebibyte of data if it normally needs two pebibytes. If training time is proportional to dataset size then maybe it takes three months instead of six months to train. Maybe it would save hundreds of millions or a billion dollars which I guess would matter to someone. It probably wouldn&#x27;t matter qualitatively though.</div><br/></div></div></div></div></div></div><div id="36465806" class="c"><input type="checkbox" id="c-36465806" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464289">parent</a><span>|</span><a href="#36464455">prev</a><span>|</span><a href="#36464566">next</a><span>|</span><label class="collapse" for="c-36465806">[-]</label><label class="expand" for="c-36465806">[1 more]</label></div><br/><div class="children"><div class="content">? adding on ChatGPT data into existing data is not lessening the amount of data...</div><br/></div></div><div id="36464566" class="c"><input type="checkbox" id="c-36464566" checked=""/><div class="controls bullet"><span class="by">adg33</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464289">parent</a><span>|</span><a href="#36465806">prev</a><span>|</span><a href="#36464487">next</a><span>|</span><label class="collapse" for="c-36464566">[-]</label><label class="expand" for="c-36464566">[1 more]</label></div><br/><div class="children"><div class="content">This is actually possible - if you have a biased dataset, then more data is bad.<p>More data will fix variance problems, but not bias.</div><br/></div></div></div></div><div id="36464113" class="c"><input type="checkbox" id="c-36464113" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36463687">parent</a><span>|</span><a href="#36464289">prev</a><span>|</span><a href="#36464886">next</a><span>|</span><label class="collapse" for="c-36464113">[-]</label><label class="expand" for="c-36464113">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, wouldn’t this only be useful if you were solving the same problems over and over again, which does happen but say in the case of programming, I’d put the reusable pieces into a library, not a language model?<p>Like you’d train the model to give you the most accurate response based on your current problem space, but when they changes, you’d have to retrain on what you’re working on but by that stage it’s already out of date ?</div><br/><div id="36465531" class="c"><input type="checkbox" id="c-36465531" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36464113">parent</a><span>|</span><a href="#36464886">next</a><span>|</span><label class="collapse" for="c-36465531">[-]</label><label class="expand" for="c-36465531">[1 more]</label></div><br/><div class="children"><div class="content">the idea is it wld learn how to do reasoning better based on this</div><br/></div></div></div></div></div></div><div id="36464886" class="c"><input type="checkbox" id="c-36464886" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36463687">prev</a><span>|</span><a href="#36464275">next</a><span>|</span><label class="collapse" for="c-36464886">[-]</label><label class="expand" for="c-36464886">[1 more]</label></div><br/><div class="children"><div class="content">I still think LLMs are a game changer.  They are seemingly amazing at deducing meaning and intent from natural language.  That&#x27;s enough to be a game changer.  Is it a game changer like the internet is&#x2F;will-be?  No.  Is it on the level of the television or the telephone?  Probably not.  But you can still leverage it in many ways to add value or reduce human work.<p>If we&#x27;re talking about LLMs as general purpose AI or a tool to replace programmers... mostly a fail so far.  LLMs seem like they could eventually be a component of something bigger, but I don&#x27;t see a line between what they are and general purpose AI.</div><br/></div></div><div id="36464275" class="c"><input type="checkbox" id="c-36464275" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36464886">prev</a><span>|</span><a href="#36465364">next</a><span>|</span><label class="collapse" for="c-36464275">[-]</label><label class="expand" for="c-36464275">[1 more]</label></div><br/><div class="children"><div class="content">I have very similar observation, and while it is amazing at times at helping me with complex tasks, it just isn&#x27;t like any human. If we rank the tasks by difficulty for human and AI, it just has very small correlation. Also GPT 4 is idiot in conversation, compared to its problem solving skill. I would have expected completely opposite trajectory for AI. eg it&#x27;s hard for it to get it to ask good clarifying questions. There are lot of cheap tricks in prompting like asking it to act as expert, or prompt for chain of though that works.</div><br/></div></div><div id="36465364" class="c"><input type="checkbox" id="c-36465364" checked=""/><div class="controls bullet"><span class="by">mtsolitary</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36464275">prev</a><span>|</span><a href="#36465477">next</a><span>|</span><label class="collapse" for="c-36465364">[-]</label><label class="expand" for="c-36465364">[1 more]</label></div><br/><div class="children"><div class="content">Very well said, this has been precisely my experience too</div><br/></div></div><div id="36465477" class="c"><input type="checkbox" id="c-36465477" checked=""/><div class="controls bullet"><span class="by">tempodox</span><span>|</span><a href="#36463564">parent</a><span>|</span><a href="#36465364">prev</a><span>|</span><a href="#36464924">next</a><span>|</span><label class="collapse" for="c-36465477">[-]</label><label class="expand" for="c-36465477">[4 more]</label></div><br/><div class="children"><div class="content">Only you, a <i>human</i> developer can be truly creative.  An LLM can only ever reproduce what it has seen before.</div><br/><div id="36465660" class="c"><input type="checkbox" id="c-36465660" checked=""/><div class="controls bullet"><span class="by">throwaway2037</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465477">parent</a><span>|</span><a href="#36465782">next</a><span>|</span><label class="collapse" for="c-36465660">[-]</label><label class="expand" for="c-36465660">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    An LLM can only ever reproduce what it has seen before.
</code></pre>
I don&#x27;t believe it.  How do you explain Midjourney?  The art that it produces is incredible by any measurement.</div><br/></div></div><div id="36465782" class="c"><input type="checkbox" id="c-36465782" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465477">parent</a><span>|</span><a href="#36465660">prev</a><span>|</span><a href="#36465710">next</a><span>|</span><label class="collapse" for="c-36465782">[-]</label><label class="expand" for="c-36465782">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  An LLM can only ever reproduce what it has seen before.<p>Anyone who&#x27;s played around with these models know that at least some generalization is taking place.</div><br/></div></div><div id="36465710" class="c"><input type="checkbox" id="c-36465710" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#36463564">root</a><span>|</span><a href="#36465477">parent</a><span>|</span><a href="#36465782">prev</a><span>|</span><a href="#36464924">next</a><span>|</span><label class="collapse" for="c-36465710">[-]</label><label class="expand" for="c-36465710">[1 more]</label></div><br/><div class="children"><div class="content">Where, exactly, did the LLM see this epilogue to The Great Gatsby before?<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;tsimonite&#x2F;status&#x2F;1653065940463157248" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;tsimonite&#x2F;status&#x2F;1653065940463157248</a></div><br/></div></div></div></div></div></div><div id="36464924" class="c"><input type="checkbox" id="c-36464924" checked=""/><div class="controls bullet"><span class="by">goldenshale</span><span>|</span><a href="#36463564">prev</a><span>|</span><a href="#36462654">next</a><span>|</span><label class="collapse" for="c-36464924">[-]</label><label class="expand" for="c-36464924">[15 more]</label></div><br/><div class="children"><div class="content">As if anyone is good at predicting the future.  Please can we stop acting like expertise equates to fortune telling capabilities?!  Nobody has any clue what a 1000x sized GPT model could do, and anybody who makes strong claims is a charlatan.  In this age of paranoid AI risk cultists we need to cultivate humility and calm, a willingness to follow data rather than beliefs and predictions.</div><br/><div id="36465387" class="c"><input type="checkbox" id="c-36465387" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36464924">parent</a><span>|</span><a href="#36465314">next</a><span>|</span><label class="collapse" for="c-36465387">[-]</label><label class="expand" for="c-36465387">[8 more]</label></div><br/><div class="children"><div class="content">&gt; paranoid AI risk cultists<p>There is broad consensus among experts that a hypothetical strong AI would be a threat, and potentially an existential threat, to humanity. While not everyone agrees on details like timeline and alignment issues, the idea that AI is dangerous is not a cult, it&#x27;s the mainstream view.<p>Climate scientists cannot &quot;predict the future&quot; with certainty either. That doesn&#x27;t mean their warnings are hot air, and neither are the warnings from AI safety experts. It seems like the educated masses are currently in denial about AI in much the same way as the uneducated masses have been in denial about climate change for a while.<p>Risk assessment doesn&#x27;t require understanding. I don&#x27;t have to understand how a venomous snake senses prey in order to know that the snake is a potential threat to me. In fact, the <i>less</i> I know about the snake, the higher the assessed risk should be, since the uncertainty is higher as well.</div><br/><div id="36465472" class="c"><input type="checkbox" id="c-36465472" checked=""/><div class="controls bullet"><span class="by">goldenshale</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465387">parent</a><span>|</span><a href="#36465840">next</a><span>|</span><label class="collapse" for="c-36465472">[-]</label><label class="expand" for="c-36465472">[4 more]</label></div><br/><div class="children"><div class="content">What nonsense.  I&#x27;ve spent over a decade 100% focused on AI, and the broad consensus among everyone I&#x27;ve worked with is not to be that concerned at all.  The only consensus is that a small group of self proclaimed experts who make a lot of noise is that they get lots of press coverage if they scream and shout making predictions based on zero scientific evidence.<p>We can understand the physics of greenhouse gases and take measurements of earth systems to build evidence for models and theories.  (Many of which are nonetheless very inaccurate beyond short time horizons.)  Show me any evidence for AI risk today beyond people&#x27;s theories and beliefs?<p>The best predictor of the future is the past, not people&#x27;s wild ideas about what the future could be.  I&#x27;m not about to sit here feeling scared because there is more uncertainty that our matrix multiplies are about to go rogue.  There are no AGI experts or AI risk experts, because we don&#x27;t have any of these systems to study and analyze.  What we have is people forming beliefs about their own predictions about systems which are unknowable.</div><br/><div id="36465587" class="c"><input type="checkbox" id="c-36465587" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465472">parent</a><span>|</span><a href="#36465911">next</a><span>|</span><label class="collapse" for="c-36465587">[-]</label><label class="expand" for="c-36465587">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Show me any evidence for AI risk today beyond people&#x27;s theories and beliefs?<p>Deduction. Empirical evidence isn&#x27;t the only source of insight. You don&#x27;t have to conduct experiments in order to reasonably conclude that an entity that<p>1. outperforms humans at mental tasks<p>2. shares no evolutionary commonality with humans<p>3. does not necessarily have any goals that align with those of humans<p>is a potential threat to humans. This follows from very basic deductive analysis.<p>&gt; There are no AGI experts or AI risk experts, because we don&#x27;t have any of these systems to study and analyze.<p>Indeed. <i>Which increases the risk.</i> Unless you are claiming that AGI is actually impossible, the fact that its properties and behavior cannot be studied should make people <i>even more</i> worried.<p>Uncertainty and lack of knowledge are what risk is. How little we know about potential AGI is exactly why AGI represents such a big risk. If we completely understood it and were able to make reliable predictions, there would be zero risk by definition.</div><br/><div id="36465794" class="c"><input type="checkbox" id="c-36465794" checked=""/><div class="controls bullet"><span class="by">goldenshale</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465587">parent</a><span>|</span><a href="#36465911">next</a><span>|</span><label class="collapse" for="c-36465794">[-]</label><label class="expand" for="c-36465794">[1 more]</label></div><br/><div class="children"><div class="content">1) Computers, smart phones, and pocket calculators also outperform humans at mental tasks.  So do birds, dolphins, and dogs for that matter, at tasks for which they are specialized.<p>2) so?  What are you imagining this implies?  An infinity of possibilities does not a reason make, unless you are talking about arbitrary religious beliefs.<p>3) Right, no goals, no will, no purpose.  Just some matrix multiplies doing interesting things.<p>Deduction requires a premise which then leads to another premise or a conclusion due to accepted facts or reasons.  I&#x27;m genuinely curious why you think any of these properties automatically implies danger?<p>The future is uncertain.  The stock market, the economy, your health, your friendships and romances, are all unpredictable and uncertain.  Uncertainty is not a reason to freak out, although it might encourage us to find ways to become adaptable, anti-fragile, and wise.  I think AI will help us improve in these dimensions because it is already proving that it can with real evidence, not beliefs.</div><br/></div></div></div></div><div id="36465911" class="c"><input type="checkbox" id="c-36465911" checked=""/><div class="controls bullet"><span class="by">Tenoke</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465472">parent</a><span>|</span><a href="#36465587">prev</a><span>|</span><a href="#36465840">next</a><span>|</span><label class="collapse" for="c-36465911">[-]</label><label class="expand" for="c-36465911">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I&#x27;ve spent over a decade 100% focused on AI, and the broad consensus among everyone I&#x27;ve worked with is not to be that concerned at all.<p>I also work in AI and I don&#x27;t mention my concerns to colleagues who are so anti-AI risk as you. Perhaps your ideas about your colleagues&#x27; views are distorted.</div><br/></div></div></div></div><div id="36465840" class="c"><input type="checkbox" id="c-36465840" checked=""/><div class="controls bullet"><span class="by">civilized</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465387">parent</a><span>|</span><a href="#36465472">prev</a><span>|</span><a href="#36465314">next</a><span>|</span><label class="collapse" for="c-36465840">[-]</label><label class="expand" for="c-36465840">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Expertise&quot; in a speculative concept like AI risk is not remotely comparable to expertise in a scientific field like climate change.<p>There are two definitions of expertise:<p>1. Knowing more than most people about a topic. This is the type of expertise that wins the Quiz Bowl.<p>2. Actual mastery of a field, such that predictions and analyses generated by a person possessing such mastery are reliable. This is the type of expertise that fixes your home or car.<p>The first definition is easily verifiable, and due to the availability heuristic, it is often presented as a legitimate proxy for the second. But it isn&#x27;t really, not in general.<p>If I know more about horoscopes than most people, I am a horoscope expert. But it doesn&#x27;t mean I can be relied on to predict any of the things horoscopes supposedly predict. It&#x27;s the same with AI risk. Expertise in AI risk is not a basis for credibility because AI risk is not a real scientific field.<p>Climate change is a real field of science. AI risk is Nostradamic prognostication by people who know more than you.</div><br/><div id="36465909" class="c"><input type="checkbox" id="c-36465909" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465840">parent</a><span>|</span><a href="#36465314">next</a><span>|</span><label class="collapse" for="c-36465909">[-]</label><label class="expand" for="c-36465909">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Climate change is a real field of science. AI risk is Nostradamic prognostication by people who know more than you.<p>Any prediction of the future is necessarily based on modeling and extrapolation.<p>Five years ago AIs couldn&#x27;t pass a third-grade reading comprehension test. Today they pass in the top 10% of law, medical, and engineering exams for human professionals.<p>It is absolutely possible to extrapolate from such developments, and doing so is scientific, not &quot;Nostradamic&quot;. Many predictions of the potential impact of climate change also include speculative elements, such as societal effects, migration patterns, conflicts, etc., which cannot be modeled or forecast with any real certainty. That doesn&#x27;t make them unscientific.</div><br/><div id="36465962" class="c"><input type="checkbox" id="c-36465962" checked=""/><div class="controls bullet"><span class="by">civilized</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465909">parent</a><span>|</span><a href="#36465314">next</a><span>|</span><label class="collapse" for="c-36465962">[-]</label><label class="expand" for="c-36465962">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible to extrapolate from a horoscope too, it&#x27;s just not that useful. Let&#x27;s talk about the massive difference in the extrapolation being done here, climate change vs AI.<p>In climate change, we are analyzing historical climate data using weather models representing known physical processes. We try to predict the data using these models, and we are only able to do so if we include the forcing from greenhouse gases. From this we can constrain the range of impacts these gases could be having on temperature and forecast likely futures. The forecasts are heavily informed by a thoroughly validated base of prior knowledge, not just drawing lines through a log log plot.<p>None of this has any counterparts in AI. We don&#x27;t understand AI systems to anywhere near the level that physics affords understanding of physical systems. We don&#x27;t even understand them at a Moore&#x27;s Law level, where you can at least know what engineering innovations are in the pipeline and how far they could plausibly go. Predicting the sophistication of future AI is just Nostradamic prognostication.<p>Yann LeCun recently gave a presentation arguing that LLMs are a dead end and proposing a completely different approach. His arguments were extremely heuristic and unconvincing, but this at least shows that both sides have bigwigs with unconvincing heuristic arguments.</div><br/></div></div></div></div></div></div></div></div><div id="36465314" class="c"><input type="checkbox" id="c-36465314" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#36464924">parent</a><span>|</span><a href="#36465387">prev</a><span>|</span><a href="#36465290">next</a><span>|</span><label class="collapse" for="c-36465314">[-]</label><label class="expand" for="c-36465314">[4 more]</label></div><br/><div class="children"><div class="content">Warning people about potential extreme risks from advanced AI does not make you a cultist. It makes you a realist.<p>I love GPT and my whole life and plans are based on AI tools like it. But that doesn&#x27;t mean that if you make it say 50% smarter and 50 times faster that it can&#x27;t cause problems for people. Because all it takes is systems with superior reasoning capability to be given an overly broad goal.<p>In less than five years, these models may be thinking dozens of times faster than any human. Human input or activities will appear to be mostly frozen to them. The only way to keep up will be deploying your own models.<p>So to effectively lose control you don&#x27;t need the models to &quot;wake up&quot; and become living simulations of people or anything. You just need them to get somewhat smarter and much faster.<p>We have to expect them to get much, much faster. The models, software, and hardware for this specific application all have room for improvement. And there will be new paradigms&#x2F;approaches that are even more efficient for this application.<p>For hyperspeed AI to not come about would be a total break from computing history.</div><br/><div id="36465433" class="c"><input type="checkbox" id="c-36465433" checked=""/><div class="controls bullet"><span class="by">goldenshale</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465314">parent</a><span>|</span><a href="#36465290">next</a><span>|</span><label class="collapse" for="c-36465433">[-]</label><label class="expand" for="c-36465433">[3 more]</label></div><br/><div class="children"><div class="content">A realist is someone who accepts reality as it is, not as they might be able to anxiously envision that it could be.  Life is too short and attention too precious to fill the meme space with every dreamer&#x27;s deepest concerns.  None of these dramatic X-risk claims is based on anything but beliefs and conjecture.  &quot;Thinking dozens of times faster?&quot;  What do you even mean?  These are models executing matrix multiplies billions of times faster than our brains propagate information, and they represent knowledge in a manner which is unique and different from human brains.  They have no goals, no will, and no inner experience of us being frozen or fast or anything else.  We are so prone to anthropomorphize willy-nilly.  We evolved in a paradigm of resource competition so we have drives and impulses to protect, defend, devour, etc., of which AI models have zero.  Anyone who has investigated reinforcement learning knows that we are currently far away from understanding let alone implementing systems which can effectively deconstruct abstract goals into concrete sub-tasks, yet people are soooo sure that these models are somehow going to all of a sudden be an enormous risk.  Why don&#x27;t we wait until there is even the slightest glimmer of evidence before listening to these prophets of doom?<p>This pseudo-intellectual belief structure is very cult like.  Its an end of the world scenario that only an elite few can really understand, and they, our saviors, our band of reluctant nerd heroes, are screaming from the pulpit to warn us of utter destruction.  The actual end of days.  These &quot;black box&quot; (er, I mean, we engineered them that way after decades of research, but no, nobody really understands them, right?) shoggoths will be so incredibly brilliant that they will be able to dominate all of humanity.  They will understand humans so well as to manipulate us out of existence, yet they will be so utterly stupid as to pursue paper clips at all cost.<p>Maybe instead these models will just be really useful software tools to compress knowledge and make it available to humanity in myriad forms to develop a next level of civilization on top of?  People will become more educated and wise, the cost of goods and services will drop dramatically, thereby enriching all of humanity, and life will go on.  There are straighter paths from where we are today to this set of predictions than there are to many of the doomsday scenarios, yet it has become hip among the intelligentsia to be concerned about everything.  Being optimistic is somehow not real, (although the progress of civilization serves as great evidence that optimism is indeed rational) while being a loud mouthed scare mongerer or a quiet, very serious and concerned intellectual, is seen as respectable.  Forget that.  All the doomers can go rot in their depressive caves while the rest of us build a bad ass future for all of humanity.  Once hail bop has passed over I hope everyone feels welcome to come back to the party.</div><br/><div id="36465765" class="c"><input type="checkbox" id="c-36465765" checked=""/><div class="controls bullet"><span class="by">siilikuin</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465433">parent</a><span>|</span><a href="#36465290">next</a><span>|</span><label class="collapse" for="c-36465765">[-]</label><label class="expand" for="c-36465765">[2 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s try to rewrite this in a somewhat more dispassionate style:<p>A pragmatic perspective requires one to accept the present reality as it is, rather than hypothesize an exaggerated potential of what could be. Not all concerns surrounding existential risks in technology are necessarily grounded in empirical evidence. When it comes to artificial intelligence, for instance, current models operate at a speed vastly superior to human cognition. However, this does not equate to sentient consciousness or personal motivation. The projection of human traits onto these models may be misplaced, as AI systems do not possess inherently human drives or desires.<p>Many misconceptions about reinforcement learning and its capabilities abound. The development of systems that can translate abstract objectives into detailed subtasks remains a distant prospect. There seems to be a pervasive certainty about the risks associated with these models, yet concrete evidence of such dangers is still wanting.<p>This belief system, one might argue, shares certain characteristics with a doomsday cult. There is a narrative that portrays a small group of technologists as our only defense against a looming, catastrophic end. These artificial intelligence models, which were engineered after extensive research, are often misinterpreted as inscrutable entities capable of outsmarting and eradicating humanity, while simultaneously being so simplistic as to obsess over trivial tasks.<p>Alternatively, these AI models could be viewed as valuable tools for knowledge compression and distribution, enabling the advancement of civilization. As a result, societal education levels could improve, and the cost of goods and services might decrease, which could potentially enrich human life on a global scale. While there seems to be a tendency to worry about every potential hazard, optimism about the future is not unfounded given the trajectory of human progress.<p>There are certainly different perspectives on this issue. Some adhere to a more fatalistic viewpoint, while others are working towards a brighter future for humanity. Regardless, once the present fears subside, everyone is invited to participate in shaping our collective future.</div><br/><div id="36465811" class="c"><input type="checkbox" id="c-36465811" checked=""/><div class="controls bullet"><span class="by">goldenshale</span><span>|</span><a href="#36464924">root</a><span>|</span><a href="#36465765">parent</a><span>|</span><a href="#36465290">next</a><span>|</span><label class="collapse" for="c-36465811">[-]</label><label class="expand" for="c-36465811">[1 more]</label></div><br/><div class="children"><div class="content">Hahaha, thanks ChatGPT!  This is better said than my snarky, frustrated at the FUD version, and I can learn from the approach.</div><br/></div></div></div></div></div></div></div></div><div id="36465290" class="c"><input type="checkbox" id="c-36465290" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36464924">parent</a><span>|</span><a href="#36465314">prev</a><span>|</span><a href="#36465015">next</a><span>|</span><label class="collapse" for="c-36465290">[-]</label><label class="expand" for="c-36465290">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>As if anyone is good at predicting the future.</i><p>Whoever <i>predicts</i> the right direction, (and when the time is right) puts money where their mouth is, stands a shot at unseating... the <i>alt man</i>.<p><pre><code>  I think the way to use these big ideas is not to try to identify a precise point in the future and then ask yourself how to get from here to there, like the popular image of a visionary. You&#x27;ll be better off if you operate like Columbus and just head in a general westerly direction. Don&#x27;t try to construct the future like a building, because your current blueprint is almost certainly mistaken. Start with something you know works, and when you expand, expand westward.
  
  The popular image of the visionary is someone with a clear view of the future, but empirically it may be better to have a blurry one.
</code></pre>
paulgraham.com&#x2F;ambitious.html</div><br/></div></div><div id="36465015" class="c"><input type="checkbox" id="c-36465015" checked=""/><div class="controls bullet"><span class="by">JimtheCoder</span><span>|</span><a href="#36464924">parent</a><span>|</span><a href="#36465290">prev</a><span>|</span><a href="#36462654">next</a><span>|</span><label class="collapse" for="c-36465015">[-]</label><label class="expand" for="c-36465015">[1 more]</label></div><br/><div class="children"><div class="content">Sir, this is a discussion forum...</div><br/></div></div></div></div><div id="36462654" class="c"><input type="checkbox" id="c-36462654" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#36464924">prev</a><span>|</span><a href="#36462689">next</a><span>|</span><label class="collapse" for="c-36462654">[-]</label><label class="expand" for="c-36462654">[56 more]</label></div><br/><div class="children"><div class="content">We need a way to make tight little specialist models that don&#x27;t hallucinate and reliably report when they don&#x27;t know. Trying to cram all of the web into a LLM is a dead end.</div><br/><div id="36463065" class="c"><input type="checkbox" id="c-36463065" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36464346">next</a><span>|</span><label class="collapse" for="c-36463065">[-]</label><label class="expand" for="c-36463065">[5 more]</label></div><br/><div class="children"><div class="content">How much general &quot;thinking&quot;[0] would you want those &quot;tight little specialist models&quot; to retain? I think that cramming &quot;all of the web&quot; is actually crucial for this capability[1], so at least with LLM-style models, you likely can&#x27;t avoid it. The text in the training data set doesn&#x27;t encode <i>just</i> the object-level knowledge, but indirectly also higher-level, cross-domain and general concepts; cutting down on the size and breadth of the training data may cause the network to lose the ability to &quot;understand&quot;[0].<p>--<p>[0] - Or &quot;something very convincingly pretending to think by parroting stuff back&quot;, if you&#x27;re closer to the &quot;stochastic parrot&quot; view.<p>[1] - Per my hand-wavy hypothesis that the bulk of what we call thinking boils down to proximity search in extremely high-dimensional space.</div><br/><div id="36463759" class="c"><input type="checkbox" id="c-36463759" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463065">parent</a><span>|</span><a href="#36463772">next</a><span>|</span><label class="collapse" for="c-36463759">[-]</label><label class="expand" for="c-36463759">[1 more]</label></div><br/><div class="children"><div class="content">Even just for spell checking, having a ton of general knowledge helps. Knowing the correct spelling of product names, British vs American English, context-specific alternate spellings, etc…<p>Even GPT 3.5 has trouble following instructions, but I’ve found that GPT 4 is almost flawless. I can tell it the document uses Australian English but to preserve US spelling for product names and it’ll do it!<p>One quirk is that it’s almost <i>too</i> good at following instructions. You have to tell it to preserve product names, vendors names, place names, etc… otherwise it’ll “correct” the spelling of anything you forgot to list.</div><br/></div></div><div id="36463772" class="c"><input type="checkbox" id="c-36463772" checked=""/><div class="controls bullet"><span class="by">jstarfish</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463065">parent</a><span>|</span><a href="#36463759">prev</a><span>|</span><a href="#36465203">next</a><span>|</span><label class="collapse" for="c-36463772">[-]</label><label class="expand" for="c-36463772">[1 more]</label></div><br/><div class="children"><div class="content">There are boundaries that <i>can</i> be defined, but I&#x27;m unclear on what those <i>should</i> be. I&#x27;m thinking breaking it down by semantic domains, but not at the exclusion of others altogether-- just heavily weigh certain domains in favor of others. Kinda like how StackOverflow is broken down by domain-- each subdomain could be a LORA or something.<p>General purpose models containing significant overlap between Project Gutenberg and Github are unnecessary and don&#x27;t scale. Moby Dick has little to do with C++ unless you&#x27;re creating art for novelty&#x27;s sake. This is entirely speculative, but I&#x27;m convinced ChatGPT is faking the appearance of a single oracle while delegating requests to specialized models under the hood. It scales better and makes sense than trying to serve a 1T model to address everybody&#x27;s banal questions.<p>Like, at its core, for people who only want to write literature, give them a model with underweighed programming-related corpora. Writers don&#x27;t need it, will never use it, and that space could be filled with training content relevant to literature. Anything else results in expensive, unscalable solutions or jack-of-all-trades, master-of-none outcomes.<p>In recent usage, GPT3.5 helped me hack my way through writing Pester tests for Powershell scripts for the first time, and I mean hack-- there were a <i>lot</i> of assumptions it made and things it got wrong. GPT4 did a much better job, but I couldn&#x27;t help but think 3.5 probably has a ton of other training data in it that detracts from the specialization I needed from it in that context. For coding help, you don&#x27;t want to ask some random librarian who occasionally recommends resources that don&#x27;t exist; you ask someone who specializes in coding and trust they have familiarity with that domain.</div><br/></div></div><div id="36465203" class="c"><input type="checkbox" id="c-36465203" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463065">parent</a><span>|</span><a href="#36463772">prev</a><span>|</span><a href="#36463563">next</a><span>|</span><label class="collapse" for="c-36465203">[-]</label><label class="expand" for="c-36465203">[1 more]</label></div><br/><div class="children"><div class="content">They can&#x27;t reason yet, but they can extrapolate, but it cant check if its extrapolation is reasonable. Reasoning is not baked into the architecture of a GPT model. It seems it would be an entirely different type of model</div><br/></div></div><div id="36463563" class="c"><input type="checkbox" id="c-36463563" checked=""/><div class="controls bullet"><span class="by">nathan_compton</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463065">parent</a><span>|</span><a href="#36465203">prev</a><span>|</span><a href="#36464346">next</a><span>|</span><label class="collapse" for="c-36463563">[-]</label><label class="expand" for="c-36463563">[1 more]</label></div><br/><div class="children"><div class="content">This is basically my view as an extensive gpt4 user. It doesn&#x27;t think&#x2F;is very stupid, but has such fantastic recall for common patterns that its quite useful. Absolutely none of the smaller models even come close, including GPT3.5.</div><br/></div></div></div></div><div id="36464346" class="c"><input type="checkbox" id="c-36464346" checked=""/><div class="controls bullet"><span class="by">usrbinbash</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36463065">prev</a><span>|</span><a href="#36463207">next</a><span>|</span><label class="collapse" for="c-36464346">[-]</label><label class="expand" for="c-36464346">[6 more]</label></div><br/><div class="children"><div class="content">&gt; and reliably report when they don&#x27;t know.<p>Then we need a new system, because LMs, no matter if they are large or not, cannot do that, for a very simple reason:<p>A LM doesn&#x27;t understand &quot;truthfulness&quot;. It has no concept of a sequence being true or not, only of a sequence being probable.<p>And that probability cannot work as a standin for truthfulness, because the LM doesn&#x27;t produce improbable sequences to begin with...it&#x27;s output will always be the most (within heat settings) probable sequence. The LM simply has no way of knowing whether the sequence it just predicted is grounded in reality or not.</div><br/><div id="36465411" class="c"><input type="checkbox" id="c-36465411" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36464346">parent</a><span>|</span><a href="#36466111">next</a><span>|</span><label class="collapse" for="c-36465411">[-]</label><label class="expand" for="c-36465411">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A LM doesn&#x27;t understand &quot;truthfulness&quot;. It has no concept of a sequence being true or not, only of a sequence being probable.<p>I claim that the human brain doesn&#x27;t understand &quot;truthfulness&quot; either. It merely creates the impression that understanding is taking place, by adapting to social and environmental pressures. The brain has no &quot;concepts&quot; at all, it just generates output based on its input, its internal wiring, and a variety of essentially random factors, quite analogous to how LLMs operate.<p>Do you have any evidence that contradicts that claim?</div><br/></div></div><div id="36466111" class="c"><input type="checkbox" id="c-36466111" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36464346">parent</a><span>|</span><a href="#36465411">prev</a><span>|</span><a href="#36465254">next</a><span>|</span><label class="collapse" for="c-36466111">[-]</label><label class="expand" for="c-36466111">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s needed, ideally, is a checker. Something that takes the LLM&#x27;s output, can go back to the training material, and verify the output for consistency with it.<p>I don&#x27;t think those steps are out of the bounds of possibility, really.</div><br/></div></div><div id="36465254" class="c"><input type="checkbox" id="c-36465254" checked=""/><div class="controls bullet"><span class="by">exitb</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36464346">parent</a><span>|</span><a href="#36466111">prev</a><span>|</span><a href="#36464478">next</a><span>|</span><label class="collapse" for="c-36465254">[-]</label><label class="expand" for="c-36465254">[1 more]</label></div><br/><div class="children"><div class="content">If this is true, why is GPT-4 better in that regard than GPT-3.5? Or why do questions about Python yield much less hallucinations than questions about Rust, or other less popular tech?</div><br/></div></div><div id="36465069" class="c"><input type="checkbox" id="c-36465069" checked=""/><div class="controls bullet"><span class="by">acover</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36464346">parent</a><span>|</span><a href="#36464478">prev</a><span>|</span><a href="#36463207">next</a><span>|</span><label class="collapse" for="c-36465069">[-]</label><label class="expand" for="c-36465069">[1 more]</label></div><br/><div class="children"><div class="content">An llm can detect when an llm has outputted an inconsistent world.<p>It can reason. To an extent.</div><br/></div></div></div></div><div id="36463207" class="c"><input type="checkbox" id="c-36463207" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36464346">prev</a><span>|</span><a href="#36462763">next</a><span>|</span><label class="collapse" for="c-36463207">[-]</label><label class="expand" for="c-36463207">[8 more]</label></div><br/><div class="children"><div class="content">&gt; that don&#x27;t hallucinate<p>“Hallucination” is part of thought. Solving a new problem requires hallucinating new, non existing, possible outcomes and solutions, to find one that will work. It seems that eliminating the ability to interpolate and extrapolate (hallucinations) would make intelligence impossible. It would eliminate creativity, tying together new concepts, creation, etc.<p>Is the goal AI, or a nice database front end, to reference facts? Is intelligence facts, or is it the flexibility and the ability to handle and create the novel, things that are <i>new</i>?<p>The ability to have <i>confidence</i>, and know and respond to it, seems important, but that’s surely different than the elimination of hallucinations.<p>I’m probably misunderstanding something, and&#x2F;or don’t know what I’m talking about.</div><br/><div id="36464382" class="c"><input type="checkbox" id="c-36464382" checked=""/><div class="controls bullet"><span class="by">usrbinbash</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463207">parent</a><span>|</span><a href="#36464115">next</a><span>|</span><label class="collapse" for="c-36464382">[-]</label><label class="expand" for="c-36464382">[3 more]</label></div><br/><div class="children"><div class="content">The problem is, what we call &quot;hallucinating&quot; in LMs isn&#x27;t a way of creative thinking and coming up with novel solutions. It also has nothing to do with &quot;interpolate and extrapolate&quot;.<p>It&#x27;s simply when the predicted probable sequence isn&#x27;t grounded in reality.<p>When I ask an LLM to summarize the great water wars of 1999, and how the Trade Union was ultimately defeated by the Antarctic Coalitions hovercraft-fleet under Vice Admiral Zagalow, it isn&#x27;t &quot;extrapolating&quot; from knowledge of history, it is simply inventing a load of bollocks. But that bollocks will be dressed in fine language and probably mixed in with plausible-sounding references that have a somewhat-logical-sounding relation to the training data.<p>The problem is, <i>the LM doesn&#x27;t and cannot know when it produces bollocks.</i><p>All it can care about is if the sequences produced are probable according to it&#x27;s model.</div><br/><div id="36464991" class="c"><input type="checkbox" id="c-36464991" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36464382">parent</a><span>|</span><a href="#36465304">next</a><span>|</span><label class="collapse" for="c-36464991">[-]</label><label class="expand" for="c-36464991">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a great illusionist. But ultimately it cannot separate relevant information from simple word correlations.<p>&gt; What is heavier, a small floating passenger ferry or a two metric ton heavy rock that sinks to the bottom of the ocean.<p>&gt; A two metric ton heavy rock would be heavier than a small floating passenger ferry. The weight of the rock is two metric tons, which is equivalent to 2,000 kilograms or 4,409 pounds. The weight of the passenger ferry would depend on its specific design and construction materials, but it is unlikely to be heavier than two metric tons. Therefore, the heavy rock would have a greater weight than the small floating passenger ferry.<p>It completely relies on surface information such as &quot;small floating&quot; and ignores the deeper &quot;correlation&quot; that all ferries are heavy.</div><br/></div></div><div id="36465304" class="c"><input type="checkbox" id="c-36465304" checked=""/><div class="controls bullet"><span class="by">jlawson</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36464382">parent</a><span>|</span><a href="#36464991">prev</a><span>|</span><a href="#36464115">next</a><span>|</span><label class="collapse" for="c-36465304">[-]</label><label class="expand" for="c-36465304">[1 more]</label></div><br/><div class="children"><div class="content">I passed your query to GPT4 and this is what it said, see below. It seems like it can recognize bollocks, at least sometimes.<p>&quot;I&#x27;m sorry, but it appears there&#x27;s a misunderstanding. As of my knowledge cutoff in September 2021, there were no events known as the &quot;Great Water Wars of 1999&quot; involving a Trade Union being defeated by an Antarctic Coalition&#x27;s hovercraft fleet under Vice Admiral Zagalow. This might be part of a work of fiction, alternative history, or a future event beyond my last training cut-off.<p>My training includes real-world historical events and existing geopolitical structures, and as of 2021, Antarctica was governed by the Antarctic Treaty System, which prevents any military activity, mineral mining, nuclear testing, and nuclear waste disposal. It also supports scientific research and protects the continent&#x27;s ecozone.<p>Please provide more context if this information is from a book, a movie, or a game, or if it refers to something else that I may assist better with.&quot;</div><br/></div></div></div></div><div id="36464115" class="c"><input type="checkbox" id="c-36464115" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463207">parent</a><span>|</span><a href="#36464382">prev</a><span>|</span><a href="#36463345">next</a><span>|</span><label class="collapse" for="c-36464115">[-]</label><label class="expand" for="c-36464115">[1 more]</label></div><br/><div class="children"><div class="content"><i>“Hallucination” is part of thought.</i><p>Any evidence to support this claim or just commentary ?</div><br/></div></div><div id="36463345" class="c"><input type="checkbox" id="c-36463345" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463207">parent</a><span>|</span><a href="#36464115">prev</a><span>|</span><a href="#36462763">next</a><span>|</span><label class="collapse" for="c-36463345">[-]</label><label class="expand" for="c-36463345">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Is the goal AI, or a nice database front end, to reference facts?<p>The latter  given the kind of products that are currently being built with it. You don&#x27;t want your code completion or news aggregator to hallucinate for the same reason you don&#x27;t want your wrench to hallucinate, it&#x27;s a tool.<p>And as for hallucinations, that&#x27;s a PR friendly misnomer for &quot;it made **** up&quot;. Using the same phrase doesn&#x27;t mean it has functionally anything to do with the cognitive processes involved in human thought. In the same way a &#x27;artificial&#x27; neural net is really a metaphorical neural net, it has very few things in common with biological neurons.</div><br/><div id="36463718" class="c"><input type="checkbox" id="c-36463718" checked=""/><div class="controls bullet"><span class="by">mitchdoogle</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463345">parent</a><span>|</span><a href="#36462763">next</a><span>|</span><label class="collapse" for="c-36463718">[-]</label><label class="expand" for="c-36463718">[2 more]</label></div><br/><div class="children"><div class="content">&quot;hallucination&quot; just means the AI produced incorrect information. Humans produce incorrect information all the time. But we know we are fallible. I&#x27;ve seen lots of people treating these AIs as infallible, that they can&#x27;t get anything wrong. That&#x27;s a huge problem with people, not the AI.<p>Obviously it&#x27;s worth it to try and eliminate the incorrect information, but what grand-op is saying is we don&#x27;t want to do that if it takes away some valuable emergent properties.</div><br/><div id="36465428" class="c"><input type="checkbox" id="c-36465428" checked=""/><div class="controls bullet"><span class="by">zuminator</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463718">parent</a><span>|</span><a href="#36462763">next</a><span>|</span><label class="collapse" for="c-36465428">[-]</label><label class="expand" for="c-36465428">[1 more]</label></div><br/><div class="children"><div class="content">I think AI hallucinations are more than just incorrect information. They&#x27;re incorrect information coupled with an air of certitude. Sometimes with a whole backstory. And often oblivious to any inherent contradictions.  It&#x27;s like, if you ask a random person what years Lyndon B Johnson was president, they might say something like 1964-1968, shrugging with a bit of uncertainty. And both of those years would be incorrect but close. Whereas an AI (if hallucinating) would say something like Lyndon B Johnson was president from April 15, 1865 – March 4, 1869, after the shooting of John F. Kennedy in Ford&#x27;s theater. The dates are precise but completely wrong, as are the accompanying details. Then if you ask the AI when John F. Kennedy died, it might correctly respond November 22, 1963, in Dallas, TX, totally unaware how this information doesn&#x27;t match with the erroneous information it gave earlier.<p>I&#x27;ve been thinking there&#x27;s some parallels between how AIs hallucinate and how human toddlers do. If you ask a toddler&#x2F;young child a question about a fact they don&#x27;t know, they will usually say, &quot;iuno&quot; (even when they should), but depending on the child and the circumstances, they will sometimes just make up a story on the spot and sound as if they believe it. &quot;Who invented ice cream?&quot; &quot;Santa Claus! Mommy left him milk and cookies and he turned it into ice cream.&quot; It doesn&#x27;t make any real sense but it seems facially plausible in their universe.<p>But somewhere between first learning to speak and around 7ish, kids become markedly more accurate how they model the world, and their responses become correspondingly less fanciful. And they continue to improve beyond that point.<p>So how are kids doing what LLMs are currently incapable of? How do we teach ourselves not to hallucinate? Or do we, really? I mean, if I tell myself I&#x27;m going to make it through the intersection before the light turns red, but I end up running the red light, was I just mistaken, or was that a self-delusion, i.e., a mini-hallucination of sorts? Probably a self-driving car would be less likely to make that category of mistake, so maybe I shouldn&#x27;t be so smug about being grounded in reality.</div><br/></div></div></div></div></div></div></div></div><div id="36462763" class="c"><input type="checkbox" id="c-36462763" checked=""/><div class="controls bullet"><span class="by">rco8786</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36463207">prev</a><span>|</span><a href="#36463214">next</a><span>|</span><label class="collapse" for="c-36462763">[-]</label><label class="expand" for="c-36462763">[18 more]</label></div><br/><div class="children"><div class="content">Bingo. I&#x27;ve been beating this drum since the initial GPT-3 awe.. The future of AI is bespoke, purpose-driven models trained on a combination of public and (importantly) proprietary data.<p>Data is still king.</div><br/><div id="36465164" class="c"><input type="checkbox" id="c-36465164" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462763">parent</a><span>|</span><a href="#36463600">next</a><span>|</span><label class="collapse" for="c-36465164">[-]</label><label class="expand" for="c-36465164">[1 more]</label></div><br/><div class="children"><div class="content">You need sort of a &quot;primary education&quot; data set which gets the model up to roughly a high school education level. Then augment it with special-purpose models for specific areas.<p>But until &quot;I don&#x27;t know&quot; comes out, rather than hallucinations, we&#x27;re in trouble.</div><br/></div></div><div id="36463600" class="c"><input type="checkbox" id="c-36463600" checked=""/><div class="controls bullet"><span class="by">danielvaughn</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462763">parent</a><span>|</span><a href="#36465164">prev</a><span>|</span><a href="#36464319">next</a><span>|</span><label class="collapse" for="c-36463600">[-]</label><label class="expand" for="c-36463600">[2 more]</label></div><br/><div class="children"><div class="content">I’d be really interested to see an AI model built off of Sci Hub data.</div><br/><div id="36464384" class="c"><input type="checkbox" id="c-36464384" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463600">parent</a><span>|</span><a href="#36464319">next</a><span>|</span><label class="collapse" for="c-36464384">[-]</label><label class="expand" for="c-36464384">[1 more]</label></div><br/><div class="children"><div class="content">Facebook announced this a month or so before chatgpt took off, and got lambasted everywhere because of hallucinations.</div><br/></div></div></div></div><div id="36464319" class="c"><input type="checkbox" id="c-36464319" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462763">parent</a><span>|</span><a href="#36463600">prev</a><span>|</span><a href="#36462813">next</a><span>|</span><label class="collapse" for="c-36464319">[-]</label><label class="expand" for="c-36464319">[1 more]</label></div><br/><div class="children"><div class="content">This would only work if thinks like GPT were actually intelligent.<p>Train on dataset A to learn to think, use thinking on dataset B to become an export in B&#x27;s field.</div><br/></div></div><div id="36462813" class="c"><input type="checkbox" id="c-36462813" checked=""/><div class="controls bullet"><span class="by">JimtheCoder</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462763">parent</a><span>|</span><a href="#36464319">prev</a><span>|</span><a href="#36463185">next</a><span>|</span><label class="collapse" for="c-36462813">[-]</label><label class="expand" for="c-36462813">[10 more]</label></div><br/><div class="children"><div class="content">So, search isn&#x27;t dead after all...</div><br/><div id="36462876" class="c"><input type="checkbox" id="c-36462876" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462813">parent</a><span>|</span><a href="#36463185">next</a><span>|</span><label class="collapse" for="c-36462876">[-]</label><label class="expand" for="c-36462876">[9 more]</label></div><br/><div class="children"><div class="content">Search has definitely been dead since before LLMs, we just don&#x27;t have a replacement yet.</div><br/><div id="36462885" class="c"><input type="checkbox" id="c-36462885" checked=""/><div class="controls bullet"><span class="by">JimtheCoder</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462876">parent</a><span>|</span><a href="#36463174">next</a><span>|</span><label class="collapse" for="c-36462885">[-]</label><label class="expand" for="c-36462885">[2 more]</label></div><br/><div class="children"><div class="content">I mean more the concept of search, not the current implementation</div><br/><div id="36462891" class="c"><input type="checkbox" id="c-36462891" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462885">parent</a><span>|</span><a href="#36463174">next</a><span>|</span><label class="collapse" for="c-36462891">[-]</label><label class="expand" for="c-36462891">[1 more]</label></div><br/><div class="children"><div class="content">Oh, I don&#x27;t think we&#x27;ll ever stop wanting to search for things. Maybe not everything, but some things.</div><br/></div></div></div></div><div id="36463174" class="c"><input type="checkbox" id="c-36463174" checked=""/><div class="controls bullet"><span class="by">BolexNOLA</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462876">parent</a><span>|</span><a href="#36462885">prev</a><span>|</span><a href="#36463185">next</a><span>|</span><label class="collapse" for="c-36463174">[-]</label><label class="expand" for="c-36463174">[6 more]</label></div><br/><div class="children"><div class="content">For a while my replacement was “use google, add ‘reddit’ at the end.” Not sure how much longer that will work given even just this limited blackout impacted how effective that was lol</div><br/><div id="36463303" class="c"><input type="checkbox" id="c-36463303" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463174">parent</a><span>|</span><a href="#36463185">next</a><span>|</span><label class="collapse" for="c-36463303">[-]</label><label class="expand" for="c-36463303">[5 more]</label></div><br/><div class="children"><div class="content">That hasn’t worked since about three months after companies found out people do it.  It’s all astroturfing now days anyway and if it applies to products (which it for sure does) you can be sure that government actors caught on as well.</div><br/><div id="36463657" class="c"><input type="checkbox" id="c-36463657" checked=""/><div class="controls bullet"><span class="by">mitchdoogle</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463303">parent</a><span>|</span><a href="#36464491">next</a><span>|</span><label class="collapse" for="c-36463657">[-]</label><label class="expand" for="c-36463657">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not really sure what you&#x27;re saying. It seems to me the number of shills creating content is vastly outnumbered by normal people creating content, so the trick of adding &quot;reddit&quot; to the end of queries is still very much useful (blackout protests aside) since it&#x27;s usefulness derives from getting information from normal people and then having normal people upvote the &quot;best&quot; comments. Just the other day I tried this with data recovery software very quickly found some free software that was highly praised. If I had just searched &quot;data recovery software&quot; on Google I get whichever data recovery place as the best SEO, including many of them who claim that a novice should be trying it at all and should use their services pronto. Using reddit in this case gave me the exact information I wanted, where Google alone was next to useless.</div><br/><div id="36464645" class="c"><input type="checkbox" id="c-36464645" checked=""/><div class="controls bullet"><span class="by">BolexNOLA</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463657">parent</a><span>|</span><a href="#36464491">next</a><span>|</span><label class="collapse" for="c-36464645">[-]</label><label class="expand" for="c-36464645">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I don’t know their experience, but mine is when I add “Reddit” i often get 2-3 threads talking about the exact thing I’m looking up in the top 10 results.</div><br/></div></div></div></div><div id="36464491" class="c"><input type="checkbox" id="c-36464491" checked=""/><div class="controls bullet"><span class="by">quesera</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463303">parent</a><span>|</span><a href="#36463657">prev</a><span>|</span><a href="#36463185">next</a><span>|</span><label class="collapse" for="c-36464491">[-]</label><label class="expand" for="c-36464491">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure if it would help or restore the kinds of results you were seeing previously, but instead of adding &quot;reddit&quot;, you can add &quot;site:reddit.com&quot; to get only results from that site. (Originally a Google feature, but works on DuckDuckGo also. Not sure about others.)<p>Unless you mean that Reddit is astroturfed with the SEO garbage you&#x27;re trying to avoid, in which case this will definitely not help.<p>Is search on Reddit itself still useless?</div><br/><div id="36464642" class="c"><input type="checkbox" id="c-36464642" checked=""/><div class="controls bullet"><span class="by">BolexNOLA</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36464491">parent</a><span>|</span><a href="#36463185">next</a><span>|</span><label class="collapse" for="c-36464642">[-]</label><label class="expand" for="c-36464642">[1 more]</label></div><br/><div class="children"><div class="content">Search on Reddit several years ago went from “useless” to I don’t know…a C-? It <i>can</i> work.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36463185" class="c"><input type="checkbox" id="c-36463185" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462763">parent</a><span>|</span><a href="#36462813">prev</a><span>|</span><a href="#36463214">next</a><span>|</span><label class="collapse" for="c-36463185">[-]</label><label class="expand" for="c-36463185">[3 more]</label></div><br/><div class="children"><div class="content">It will be a terrific moment when we can browse a galaxy map or other lists of LLMs, and select &quot;context&quot; groups - so you can say from history, give me X</div><br/><div id="36463209" class="c"><input type="checkbox" id="c-36463209" checked=""/><div class="controls bullet"><span class="by">thelittleone</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463185">parent</a><span>|</span><a href="#36463214">next</a><span>|</span><label class="collapse" for="c-36463209">[-]</label><label class="expand" for="c-36463209">[2 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t sound ideal at all to me. It also sounds like a bottleneck. I want to ask AI once and it recommends or selects the best &quot;context&quot; group.</div><br/><div id="36463439" class="c"><input type="checkbox" id="c-36463439" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463209">parent</a><span>|</span><a href="#36463214">next</a><span>|</span><label class="collapse" for="c-36463439">[-]</label><label class="expand" for="c-36463439">[1 more]</label></div><br/><div class="children"><div class="content">It would bea good visual training tool for kids so they can get an understanding early on other than a block-box view.</div><br/></div></div></div></div></div></div></div></div><div id="36463214" class="c"><input type="checkbox" id="c-36463214" checked=""/><div class="controls bullet"><span class="by">4ndrewl</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36462763">prev</a><span>|</span><a href="#36463037">next</a><span>|</span><label class="collapse" for="c-36463214">[-]</label><label class="expand" for="c-36463214">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t know that they don&#x27;t know. It&#x27;s only hallucination from a human&#x27;s perspective. From the model&#x27;s perspective it&#x27;s _all_ hallucination.</div><br/></div></div><div id="36463037" class="c"><input type="checkbox" id="c-36463037" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36463214">prev</a><span>|</span><a href="#36465188">next</a><span>|</span><label class="collapse" for="c-36463037">[-]</label><label class="expand" for="c-36463037">[1 more]</label></div><br/><div class="children"><div class="content">Its complicated.<p>Training on something huge like &quot;the internet&quot; is what gives rise to those amazing emergent properties missing in smaller models (including the recent Pi model). And there are only so many datasets that huge.<p>But its also indeed a waste, as Pi proves.<p>There probably is some sweet spot (6B-40B?) for specialized, heavily focused models pre trained with high quality general data.</div><br/></div></div><div id="36465188" class="c"><input type="checkbox" id="c-36465188" checked=""/><div class="controls bullet"><span class="by">koochi10</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36463037">prev</a><span>|</span><a href="#36462939">next</a><span>|</span><label class="collapse" for="c-36465188">[-]</label><label class="expand" for="c-36465188">[3 more]</label></div><br/><div class="children"><div class="content">I disagree with this slightly, modern computer chips follow a general purpose architecture not special purpose ones. The reason for this is building a computer chip is expensive and difficult to do. Similarly building any useful language model requires tons of compute power, and very smart ML researchers. Most of the smaller open source one&#x27;s are just trained on GPT output.<p>By &quot;cramming all of the web&quot; on a model what is really going on is the hidden layers of that network are getting better at understanding language and logic. Imagine trying to teach a kid who doesn&#x27;t know how to read to learn about a Science by only giving them science textbooks. Chances are they won&#x27;t get very far.<p>Building little specialist model&#x27;s don&#x27;t really work either. It&#x27;s like trying to train a parrot to do science, sure it can repeat some of the phrases that you give it, but at the end of the day it&#x27;s not really making any new connections for you.</div><br/><div id="36465229" class="c"><input type="checkbox" id="c-36465229" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36465188">parent</a><span>|</span><a href="#36462939">next</a><span>|</span><label class="collapse" for="c-36465229">[-]</label><label class="expand" for="c-36465229">[2 more]</label></div><br/><div class="children"><div class="content">Since when did the world decide to shed the grammatically correct
&quot;computing power&quot; for this weird tech bro &quot;compute power&quot; phrase?</div><br/><div id="36465344" class="c"><input type="checkbox" id="c-36465344" checked=""/><div class="controls bullet"><span class="by">lukeschlather</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36465229">parent</a><span>|</span><a href="#36462939">next</a><span>|</span><label class="collapse" for="c-36465344">[-]</label><label class="expand" for="c-36465344">[1 more]</label></div><br/><div class="children"><div class="content">Definitions are a little fuzzy but &quot;compute&quot; is often used to distinguish between &quot;compute,&quot; as in CPU, memory, and disk. And GPU is a very specialized kind of compute power. There&#x27;s really no &quot;grammatically correct&quot; here these are different senses of the word. &quot;Computing power&quot; doesn&#x27;t exactly have the same sense of specifically referring to a CPU or GPU as &quot;compute power.&quot;</div><br/></div></div></div></div></div></div><div id="36462939" class="c"><input type="checkbox" id="c-36462939" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36465188">prev</a><span>|</span><a href="#36463390">next</a><span>|</span><label class="collapse" for="c-36462939">[-]</label><label class="expand" for="c-36462939">[1 more]</label></div><br/><div class="children"><div class="content">This is ultimately just very powerful semantic search though, is it not?<p>It seems that what we need to make a big leap forward is better reasoning. There is a lot of debate between the GPT-4 can&#x2F;can&#x27;t reason camps, but I haven&#x27;t seen anyone try to argue that it reasons <i>particularly</i> well.</div><br/></div></div><div id="36463390" class="c"><input type="checkbox" id="c-36463390" checked=""/><div class="controls bullet"><span class="by">twobitshifter</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36462939">prev</a><span>|</span><a href="#36462865">next</a><span>|</span><label class="collapse" for="c-36463390">[-]</label><label class="expand" for="c-36463390">[1 more]</label></div><br/><div class="children"><div class="content">I don’t know how a bunch of specialist models don’t combine into a super useful generalist model. Do we believe too much knowledge breaks an LLM?</div><br/></div></div><div id="36462865" class="c"><input type="checkbox" id="c-36462865" checked=""/><div class="controls bullet"><span class="by">causalmodels</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36463390">prev</a><span>|</span><a href="#36464309">next</a><span>|</span><label class="collapse" for="c-36462865">[-]</label><label class="expand" for="c-36462865">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not an either or. We&#x27;re going to leverage the web trained LLMs to bootstrap the specialist models via combination of training token quality classifiers and synthetic data generation. Phi-1 is a pretty good example of this.</div><br/></div></div><div id="36464309" class="c"><input type="checkbox" id="c-36464309" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36462865">prev</a><span>|</span><a href="#36463220">next</a><span>|</span><label class="collapse" for="c-36464309">[-]</label><label class="expand" for="c-36464309">[1 more]</label></div><br/><div class="children"><div class="content">They never know.<p>What we call hallucination is just when the resulting text is wrong but the underlying probabilities could be high.</div><br/></div></div><div id="36463220" class="c"><input type="checkbox" id="c-36463220" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36464309">prev</a><span>|</span><a href="#36462974">next</a><span>|</span><label class="collapse" for="c-36463220">[-]</label><label class="expand" for="c-36463220">[1 more]</label></div><br/><div class="children"><div class="content">The same way &quot;attention&quot; was a game changer, I&#x27;m not sure why they don&#x27;t invent some recursive self-maintenance algorithm that constantly improves the neural network within. Self directed attention, so to speak.</div><br/></div></div><div id="36462974" class="c"><input type="checkbox" id="c-36462974" checked=""/><div class="controls bullet"><span class="by">wilg</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36463220">prev</a><span>|</span><a href="#36462908">next</a><span>|</span><label class="collapse" for="c-36462974">[-]</label><label class="expand" for="c-36462974">[1 more]</label></div><br/><div class="children"><div class="content">How are we determining it&#x27;s a dead end here? Recently things like GPT-4 have come out which have improved on that technique. Why would specialization necessarily reduce hallucination and improve accuracy?</div><br/></div></div><div id="36462908" class="c"><input type="checkbox" id="c-36462908" checked=""/><div class="controls bullet"><span class="by">TX81Z</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36462974">prev</a><span>|</span><a href="#36462703">next</a><span>|</span><label class="collapse" for="c-36462908">[-]</label><label class="expand" for="c-36462908">[6 more]</label></div><br/><div class="children"><div class="content">GPT4 is really good at code and you can generally verify hallucination easily.<p>The other good use cases are using LLM to turn natural language prompts into API calls to real data.</div><br/><div id="36462945" class="c"><input type="checkbox" id="c-36462945" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462908">parent</a><span>|</span><a href="#36463119">next</a><span>|</span><label class="collapse" for="c-36462945">[-]</label><label class="expand" for="c-36462945">[2 more]</label></div><br/><div class="children"><div class="content">This is what I&#x27;ve been doing. GPT-4 to generate some data from some input, followed up by a 3.5T call to verify the output against the input to verify the content. You can feed the 3.5T output straight back into GPT-4 again and it will self-correct.<p>Doing this a couple of times gives me 100% accuracy for my use case that involves some level of summarization and reasoning.<p>Hallucinations are not as big of a deal at all IMO. Not enough that I&#x27;ll just sit there and wait for models that don&#x27;t hallucinate.</div><br/><div id="36463311" class="c"><input type="checkbox" id="c-36463311" checked=""/><div class="controls bullet"><span class="by">adriand</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462945">parent</a><span>|</span><a href="#36463119">next</a><span>|</span><label class="collapse" for="c-36463311">[-]</label><label class="expand" for="c-36463311">[1 more]</label></div><br/><div class="children"><div class="content">This sounds interesting, can you detail this data flow a bit more and maybe provide an example?</div><br/></div></div></div></div><div id="36463119" class="c"><input type="checkbox" id="c-36463119" checked=""/><div class="controls bullet"><span class="by">simion314</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36462908">parent</a><span>|</span><a href="#36462945">prev</a><span>|</span><a href="#36462703">next</a><span>|</span><label class="collapse" for="c-36463119">[-]</label><label class="expand" for="c-36463119">[3 more]</label></div><br/><div class="children"><div class="content">&gt;GPT4 is really good at code<p>for popular languages, though for JS it most of the time outputs obsolete syntax and code.<p>Today i tried to do a bit of scripting with my son in Garrys Mod, it uses Expression 2 for a Wiremod module. GPT hallucinated a lot of functions and the worst part it switched almost each time from e2 to lua.<p>It is good at solving homeworks for students, or solving popular problems in popular languages and libraries though it might give you an ugly solution and ugly code, it is probably trained on bad code too and it did not learn to prefer  good code over bad code.</div><br/><div id="36465630" class="c"><input type="checkbox" id="c-36465630" checked=""/><div class="controls bullet"><span class="by">TX81Z</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36463119">parent</a><span>|</span><a href="#36462703">next</a><span>|</span><label class="collapse" for="c-36465630">[-]</label><label class="expand" for="c-36465630">[2 more]</label></div><br/><div class="children"><div class="content">I’ve been writing JavaScript for 25 years and have always felt it has utterly trash syntax. I’m not sure that’s 100% on GPT.<p>(And I don’t mean to be rude - I just really hate that syntax!)</div><br/><div id="36466142" class="c"><input type="checkbox" id="c-36466142" checked=""/><div class="controls bullet"><span class="by">simion314</span><span>|</span><a href="#36462654">root</a><span>|</span><a href="#36465630">parent</a><span>|</span><a href="#36462703">next</a><span>|</span><label class="collapse" for="c-36466142">[-]</label><label class="expand" for="c-36466142">[1 more]</label></div><br/><div class="children"><div class="content">What I mean it will generate code using var instead of let or const
like instead of a nice
array.forEach it will create
for(var i<p>i mean use at least &quot;let&quot;</div><br/></div></div></div></div></div></div></div></div><div id="36462703" class="c"><input type="checkbox" id="c-36462703" checked=""/><div class="controls bullet"><span class="by">z3c0</span><span>|</span><a href="#36462654">parent</a><span>|</span><a href="#36462908">prev</a><span>|</span><a href="#36462689">next</a><span>|</span><label class="collapse" for="c-36462703">[-]</label><label class="expand" for="c-36462703">[1 more]</label></div><br/><div class="children"><div class="content">Logistic Regression is simple to implement, supports binomial, multinomial, and ordinal classification, and is a key layer in NN, as it&#x27;s often used as an actuator that sorts a probability into a discrete category. Very good for specialized problems, and easily trainable to sort unknown or nonsensical inputs into a noncategory.<p>Linear Regression is great for projections, and can even be fit to time series data using lagging.</div><br/></div></div></div></div><div id="36462689" class="c"><input type="checkbox" id="c-36462689" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#36462654">prev</a><span>|</span><a href="#36462728">next</a><span>|</span><label class="collapse" for="c-36462689">[-]</label><label class="expand" for="c-36462689">[5 more]</label></div><br/><div class="children"><div class="content">Another recent (but not called out in this article) is the &quot;Textbooks Are All You Need&quot; paper [1]; the results seem to suggest that careful curation and curriculums of training data can significantly improve model capabilities (when training domain specific, smaller models). Claiming a 10x smaller model can outperform competitors. (Eg. phi-1 vs. starcoder)<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.11644" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.11644</a></div><br/><div id="36464303" class="c"><input type="checkbox" id="c-36464303" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#36462689">parent</a><span>|</span><a href="#36462715">next</a><span>|</span><label class="collapse" for="c-36464303">[-]</label><label class="expand" for="c-36464303">[2 more]</label></div><br/><div class="children"><div class="content">TBH, it looks like metric manipulation to me. They have used GPT-3.5 to generate their data(and not use textbooks at all like the title suggests). And their dataset is very much like their benchmark data. While there was some filtering, but still it is very possible that lot of the benchmark questions were in training data.<p>We likely wouldn&#x27;t ever know how good the model is as it not only closed but they haven&#x27;t provided access to anyone.</div><br/><div id="36464409" class="c"><input type="checkbox" id="c-36464409" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#36462689">root</a><span>|</span><a href="#36464303">parent</a><span>|</span><a href="#36462715">next</a><span>|</span><label class="collapse" for="c-36464409">[-]</label><label class="expand" for="c-36464409">[1 more]</label></div><br/><div class="children"><div class="content">They seemed to be pretty mindful of this contamination, and call out that they agressively pruned some training dataset and still observed strong performance. That said, I agree, I really want to try it out myself and see how it feels, and if the scores really translate to day-to-day capabilities.<p>From section 5:<p><pre><code>    In Figure 2.1, we see that training on CodeExercises leads to a substantial boost in the performance of the
    model on the HumanEval benchmark. To investigate this boost, we propose to prune the CodeExercises
    dataset by removing files that are “similar” to those in HumanEval. This process can be viewed as
    a “strong form” of data decontamination. We then retrain our model on such pruned data, and still
    observe strong performance on HumanEval. In particular, even after aggressively pruning more than
    40% of the CodeExercises dataset (this even prunes files that are only vaguely similar to HumanEval, see
    Appendix C), the retrained phi-1 still outperforms StarCoder.</code></pre></div><br/></div></div></div></div><div id="36462715" class="c"><input type="checkbox" id="c-36462715" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#36462689">parent</a><span>|</span><a href="#36464303">prev</a><span>|</span><a href="#36462728">next</a><span>|</span><label class="collapse" for="c-36462715">[-]</label><label class="expand" for="c-36462715">[2 more]</label></div><br/><div class="children"><div class="content">I’ve still wondered if anyone has tried training with a large dataset of published books, like something from library genesis, or in the case of google using the full text from google books. There’s all this talk of finding quality text and I’ve not heard of text from print books being a major source beyond this textbooks paper?</div><br/><div id="36462753" class="c"><input type="checkbox" id="c-36462753" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#36462689">root</a><span>|</span><a href="#36462715">parent</a><span>|</span><a href="#36462728">next</a><span>|</span><label class="collapse" for="c-36462753">[-]</label><label class="expand" for="c-36462753">[1 more]</label></div><br/><div class="children"><div class="content">That’s how OpenAI was (is) doing it. Books downloaded from the Internet is a part of the dataset as per GPT3 model card. Right to read.</div><br/></div></div></div></div></div></div><div id="36462728" class="c"><input type="checkbox" id="c-36462728" checked=""/><div class="controls bullet"><span class="by">LeanderK</span><span>|</span><a href="#36462689">prev</a><span>|</span><a href="#36466095">next</a><span>|</span><label class="collapse" for="c-36462728">[-]</label><label class="expand" for="c-36462728">[2 more]</label></div><br/><div class="children"><div class="content">To the outsider it might seem that the only thing we&#x27;ve been doing is scaling up the neural networks, but that&#x27;s not true. A lot of innovation and changes happened, some enabled us to scale up more and others just improved performance. I am quite confident that innovation will continue.</div><br/><div id="36462783" class="c"><input type="checkbox" id="c-36462783" checked=""/><div class="controls bullet"><span class="by">jgalt212</span><span>|</span><a href="#36462728">parent</a><span>|</span><a href="#36466095">next</a><span>|</span><label class="collapse" for="c-36462783">[-]</label><label class="expand" for="c-36462783">[1 more]</label></div><br/><div class="children"><div class="content">Your statement is entirely fair, but the actual title is &quot;The bigger-is-better approach to AI is running out of road&quot;.  They are actually saying what you are saying, but your comment seems to contest the article.</div><br/></div></div></div></div><div id="36466095" class="c"><input type="checkbox" id="c-36466095" checked=""/><div class="controls bullet"><span class="by">digitcatphd</span><span>|</span><a href="#36462728">prev</a><span>|</span><a href="#36462605">next</a><span>|</span><label class="collapse" for="c-36466095">[-]</label><label class="expand" for="c-36466095">[1 more]</label></div><br/><div class="children"><div class="content">Mark Zuckerberg made a good point in his Lex Friedman podcast referring to LLAMA, that he foresees not one master model but a bunch of speciality purpose smaller models built on base models with fewer parameters</div><br/></div></div><div id="36462605" class="c"><input type="checkbox" id="c-36462605" checked=""/><div class="controls bullet"><span class="by">jug</span><span>|</span><a href="#36466095">prev</a><span>|</span><a href="#36464622">next</a><span>|</span><label class="collapse" for="c-36462605">[-]</label><label class="expand" for="c-36462605">[6 more]</label></div><br/><div class="children"><div class="content">I also heard this. I unfortunately forget which study it was but yes, their paper spoke of likely diminishing returns at least at around 400-500B parameters for current LLM&#x27;s. The recent news of GPT-4 running on 8x 220B LLM&#x27;s (which doesn&#x27;t equal a 8*220B size) fits that range and it&#x27;s also questionable how much further we can push LLM&#x27;s further by introducing multiple models like this, because this too eventually introduces problems due to granularity and picking the right model if I understood this correctly (from an earlier Hacker News discussion). (sorry for altogether no sources lol)</div><br/><div id="36462631" class="c"><input type="checkbox" id="c-36462631" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36462605">parent</a><span>|</span><a href="#36464622">next</a><span>|</span><label class="collapse" for="c-36462631">[-]</label><label class="expand" for="c-36462631">[5 more]</label></div><br/><div class="children"><div class="content">Have those reports from George Hotz been confirmed? It seems plausible to me, but also suggests to me that we have further to go by using that parameter budget for depth rather than for width.</div><br/><div id="36462651" class="c"><input type="checkbox" id="c-36462651" checked=""/><div class="controls bullet"><span class="by">woeirua</span><span>|</span><a href="#36462605">root</a><span>|</span><a href="#36462631">parent</a><span>|</span><a href="#36463708">next</a><span>|</span><label class="collapse" for="c-36462651">[-]</label><label class="expand" for="c-36462651">[3 more]</label></div><br/><div class="children"><div class="content">It seems consistent with the behavior we see when using GPT4 in chat mode. Every once in a while it will change its answer as it’s generating it, as though it’s switched which model it favors to produce the response. GPT3.5 doesn’t do that.</div><br/><div id="36462936" class="c"><input type="checkbox" id="c-36462936" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36462605">root</a><span>|</span><a href="#36462651">parent</a><span>|</span><a href="#36463708">next</a><span>|</span><label class="collapse" for="c-36462936">[-]</label><label class="expand" for="c-36462936">[2 more]</label></div><br/><div class="children"><div class="content">MoE models don&#x27;t work like that though</div><br/><div id="36464871" class="c"><input type="checkbox" id="c-36464871" checked=""/><div class="controls bullet"><span class="by">SgtBastard</span><span>|</span><a href="#36462605">root</a><span>|</span><a href="#36462936">parent</a><span>|</span><a href="#36463708">next</a><span>|</span><label class="collapse" for="c-36464871">[-]</label><label class="expand" for="c-36464871">[1 more]</label></div><br/><div class="children"><div class="content">Token-at-time MoE models could result in mid-output-stream directional change.<p>I agree with you that whole-output MoE models don’t.</div><br/></div></div></div></div></div></div><div id="36463708" class="c"><input type="checkbox" id="c-36463708" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#36462605">root</a><span>|</span><a href="#36462631">parent</a><span>|</span><a href="#36462651">prev</a><span>|</span><a href="#36464622">next</a><span>|</span><label class="collapse" for="c-36463708">[-]</label><label class="expand" for="c-36463708">[1 more]</label></div><br/><div class="children"><div class="content">Several people in the community have confirmed it so while we don&#x27;t &quot;know know&quot;, it does seem like the dominant plausible rumor going around</div><br/></div></div></div></div></div></div><div id="36464622" class="c"><input type="checkbox" id="c-36464622" checked=""/><div class="controls bullet"><span class="by">dwaltrip</span><span>|</span><a href="#36462605">prev</a><span>|</span><a href="#36466036">next</a><span>|</span><label class="collapse" for="c-36464622">[-]</label><label class="expand" for="c-36464622">[2 more]</label></div><br/><div class="children"><div class="content">The last paragraph in the article:<p>&gt; That such big performance increases can be extracted from relatively simple changes like rounding numbers or switching programming languages might seem surprising. But it reflects the breakneck speed with which llms have been developed. For many years they were research projects, and simply getting them to work well was more important than making them elegant. Only recently have they graduated to commercial, mass-market products. Most experts think there remains plenty of room for improvement. As Chris Manning, a computer scientist at Stanford University, put it: “There’s absolutely no reason to believe…that this is the ultimate neural architecture, and we will never find anything better.”</div><br/><div id="36465057" class="c"><input type="checkbox" id="c-36465057" checked=""/><div class="controls bullet"><span class="by">ruleryak</span><span>|</span><a href="#36464622">parent</a><span>|</span><a href="#36466036">next</a><span>|</span><label class="collapse" for="c-36465057">[-]</label><label class="expand" for="c-36465057">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an incredibly important point -<p>We have so many a-ha moments ahead of us in this field.  Seemingly minor changes yielding task speed multipliers, fresh eyes on foundational codebases saying now why the heck did they do it that way when xyz exists and works better, etc.  A recent graphics driver update took my local SD performance from almost 4 seconds per iteration to 2.7it&#x2F;s because someone somewhere had an a-ha moment.  We&#x27;re practically in the Commodore 64 era of this technology and there are only going to be more and more people putting their eyes and minds on these problems every day.</div><br/></div></div></div></div><div id="36466036" class="c"><input type="checkbox" id="c-36466036" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#36464622">prev</a><span>|</span><a href="#36462606">next</a><span>|</span><label class="collapse" for="c-36466036">[-]</label><label class="expand" for="c-36466036">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, many in the AI field never thought this approach would have worked at all, and were caught with their pants down when results started coming out.</div><br/></div></div><div id="36462606" class="c"><input type="checkbox" id="c-36462606" checked=""/><div class="controls bullet"><span class="by">awestroke</span><span>|</span><a href="#36466036">prev</a><span>|</span><a href="#36462623">next</a><span>|</span><label class="collapse" for="c-36462606">[-]</label><label class="expand" for="c-36462606">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been just a few months since GPT4. Calm down.</div><br/><div id="36462616" class="c"><input type="checkbox" id="c-36462616" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#36462606">parent</a><span>|</span><a href="#36462623">next</a><span>|</span><label class="collapse" for="c-36462616">[-]</label><label class="expand" for="c-36462616">[1 more]</label></div><br/><div class="children"><div class="content">Major AI winter #3. Let&#x27;s gooo!</div><br/></div></div></div></div><div id="36462623" class="c"><input type="checkbox" id="c-36462623" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36462606">prev</a><span>|</span><a href="#36464727">next</a><span>|</span><label class="collapse" for="c-36462623">[-]</label><label class="expand" for="c-36462623">[12 more]</label></div><br/><div class="children"><div class="content">Data requirements are overstated - you can train on longer and longer sequences and I am pretty sure most organizations are still using the “show the model the data only once“ approach which is just wasteful.<p>Compute challenges are more real, but we are seeing for the first time huge amounts of global capital being allocated to solve specifically these problems, so I am curious what fruit that will bear in a few years.<p>I mean already the stuff that some of these low level people are doing is absolutely nuts. Tim Dettmers work training with only 4 bits means only 16 possible values per weight and still getting great results.</div><br/><div id="36462708" class="c"><input type="checkbox" id="c-36462708" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#36462623">parent</a><span>|</span><a href="#36462759">next</a><span>|</span><label class="collapse" for="c-36462708">[-]</label><label class="expand" for="c-36462708">[8 more]</label></div><br/><div class="children"><div class="content">Yep. We are in the very early innings of capital being deployed to all this.</div><br/><div id="36463040" class="c"><input type="checkbox" id="c-36463040" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36462708">parent</a><span>|</span><a href="#36462759">next</a><span>|</span><label class="collapse" for="c-36463040">[-]</label><label class="expand" for="c-36463040">[7 more]</label></div><br/><div class="children"><div class="content">Ok - what&#x27;s the ROI on the $10bn (++) that OpenAI have had?<p>So far I reckon &lt;$10m in actual revenue.<p>This isn&#x27;t what VC&#x27;s (or microsoft) dream of.</div><br/><div id="36463104" class="c"><input type="checkbox" id="c-36463104" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36463040">parent</a><span>|</span><a href="#36463333">next</a><span>|</span><label class="collapse" for="c-36463104">[-]</label><label class="expand" for="c-36463104">[2 more]</label></div><br/><div class="children"><div class="content">I think Azure OpenAI service is growing at 1000% per quarter according to last earnings call</div><br/><div id="36463286" class="c"><input type="checkbox" id="c-36463286" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36463104">parent</a><span>|</span><a href="#36463333">next</a><span>|</span><label class="collapse" for="c-36463286">[-]</label><label class="expand" for="c-36463286">[1 more]</label></div><br/><div class="children"><div class="content">Indeed. Azure OpenAI service is how you get corporate-blessed ChatGPT that you can use with proprietary information, among other things. There&#x27;s a huge demand for it.</div><br/></div></div></div></div><div id="36463333" class="c"><input type="checkbox" id="c-36463333" checked=""/><div class="controls bullet"><span class="by">Quarrelsome</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36463040">parent</a><span>|</span><a href="#36463104">prev</a><span>|</span><a href="#36464806">next</a><span>|</span><label class="collapse" for="c-36463333">[-]</label><label class="expand" for="c-36463333">[1 more]</label></div><br/><div class="children"><div class="content">The level of exposure since chatGPT has&#x2F;will result in a lot of money turning up, especially for applications of the existing technology (whether they succeed or fail). That stats on usage demonstrate that the thundering herd has noticed and that attention can be extremely valuable.<p>I think its quite likely that OpenAI will make that money back and more, as both the industry leader and with the power of their brand (chatGPT).</div><br/></div></div><div id="36464749" class="c"><input type="checkbox" id="c-36464749" checked=""/><div class="controls bullet"><span class="by">justrealist</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36463040">parent</a><span>|</span><a href="#36464806">prev</a><span>|</span><a href="#36463719">next</a><span>|</span><label class="collapse" for="c-36464749">[-]</label><label class="expand" for="c-36464749">[1 more]</label></div><br/><div class="children"><div class="content">That would be 500,000 people buying 1 month of GPT-4.  I think they blew past that in the first week.</div><br/></div></div><div id="36463719" class="c"><input type="checkbox" id="c-36463719" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36463040">parent</a><span>|</span><a href="#36464749">prev</a><span>|</span><a href="#36462759">next</a><span>|</span><label class="collapse" for="c-36463719">[-]</label><label class="expand" for="c-36463719">[1 more]</label></div><br/><div class="children"><div class="content">OAI revenue is 100m+ at least right now iirc and probably gonna grow quite a bit</div><br/></div></div></div></div></div></div><div id="36462759" class="c"><input type="checkbox" id="c-36462759" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#36462623">parent</a><span>|</span><a href="#36462708">prev</a><span>|</span><a href="#36464727">next</a><span>|</span><label class="collapse" for="c-36462759">[-]</label><label class="expand" for="c-36462759">[3 more]</label></div><br/><div class="children"><div class="content">&gt; using the “show the model the data only once“ approach which is just wasteful.<p>According to the InstructGPT paper, that is not the case, showing the data multiple times results in overfitting.</div><br/><div id="36462792" class="c"><input type="checkbox" id="c-36462792" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36462759">parent</a><span>|</span><a href="#36464727">next</a><span>|</span><label class="collapse" for="c-36462792">[-]</label><label class="expand" for="c-36462792">[2 more]</label></div><br/><div class="children"><div class="content">1. You are just referring to fine tuning, I am referring to training the base model.<p>2. They still saw performance improvements which is why they did train on the data multiple times, you can see in the paper.<p>3. there was a recent paper demonstrating that reusing data still saw continued improvements in perplexity, i am on my ipad so cannot find it now</div><br/><div id="36462987" class="c"><input type="checkbox" id="c-36462987" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#36462623">root</a><span>|</span><a href="#36462792">parent</a><span>|</span><a href="#36464727">next</a><span>|</span><label class="collapse" for="c-36462987">[-]</label><label class="expand" for="c-36462987">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  1. You are just referring to fine tuning, I am referring to training the base model.<p>Ahh mb! Sorry.</div><br/></div></div></div></div></div></div></div></div><div id="36464727" class="c"><input type="checkbox" id="c-36464727" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36462623">prev</a><span>|</span><a href="#36464510">next</a><span>|</span><label class="collapse" for="c-36464727">[-]</label><label class="expand" for="c-36464727">[1 more]</label></div><br/><div class="children"><div class="content">There was a recent post here linking to the blog “The Secret Sauce behind 100K context window in LLMs: all tricks in one place” : <a href="https:&#x2F;&#x2F;blog.gopenai.com&#x2F;how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.gopenai.com&#x2F;how-to-speed-up-llms-and-use-100k-c...</a><p>My impression is that combining all of those, <i>plus</i> all of the post training quantisation and sparsity tricks into a new model with <i>more</i> training compute than GPT would yield an amazing improvement. Especially the price&#x2F;performance would be expected to dramatically improve. The current models are very wasteful during inference, there’s easily a factor of ten improvement available there.</div><br/></div></div><div id="36464510" class="c"><input type="checkbox" id="c-36464510" checked=""/><div class="controls bullet"><span class="by">_cs2017_</span><span>|</span><a href="#36464727">prev</a><span>|</span><a href="#36465642">next</a><span>|</span><label class="collapse" for="c-36464510">[-]</label><label class="expand" for="c-36464510">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Many in the AI field&quot;: who exactly? Shouldn&#x27;t the author quote a few famous people when making this claim?</div><br/></div></div><div id="36465642" class="c"><input type="checkbox" id="c-36465642" checked=""/><div class="controls bullet"><span class="by">SanderNL</span><span>|</span><a href="#36464510">prev</a><span>|</span><a href="#36464415">next</a><span>|</span><label class="collapse" for="c-36465642">[-]</label><label class="expand" for="c-36465642">[1 more]</label></div><br/><div class="children"><div class="content">GPT4 is better than its predecessor and quite impressively so. The (probable) param count reflects that.<p>It’s not just param count, because not all large models are good, but it clearly is part of the equation.<p>I always wondered why Altman said going bigger is a dead end. If it were true, saying it would  needlessly inform your competition. What’s the use in that? If it were false it might dissuade them from going down that path.. I think I got my answer.</div><br/></div></div><div id="36464415" class="c"><input type="checkbox" id="c-36464415" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36465642">prev</a><span>|</span><a href="#36463354">next</a><span>|</span><label class="collapse" for="c-36464415">[-]</label><label class="expand" for="c-36464415">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been following the LLMs and I don&#x27;t think it&#x27;s &#x27;running out of road&#x27;, except in the sense that it&#x27;s expensive. The model perplexity keeps decreasing when you scale up the parameters and exaflops and dataset sizes in the right proportions, and this decreasing perplexity is what has been unlocking so many amazing cognitive capabilities. I think the limitations are about regulatory deals not about any kind of technical roadblocks.</div><br/></div></div><div id="36463354" class="c"><input type="checkbox" id="c-36463354" checked=""/><div class="controls bullet"><span class="by">dpeckett</span><span>|</span><a href="#36464415">prev</a><span>|</span><a href="#36464528">next</a><span>|</span><label class="collapse" for="c-36463354">[-]</label><label class="expand" for="c-36463354">[1 more]</label></div><br/><div class="children"><div class="content">Sparse networks are the future, there&#x27;s definitely a few major algorithmic hurdles we&#x27;ll have to cross before they end up a real option but long term they will dominate (after all they already do in the living world).<p>All our current approaches rely on dense matrix multiplications. These approaches necessitate a tremendous amount of communication bandwidth (and low latency collectives). This is extremely challenging and expensive to scale O(n^2.3).<p>The constraints of physics and finance make significantly larger models out of reach for now.</div><br/></div></div><div id="36464528" class="c"><input type="checkbox" id="c-36464528" checked=""/><div class="controls bullet"><span class="by">jeswin</span><span>|</span><a href="#36463354">prev</a><span>|</span><a href="#36463313">next</a><span>|</span><label class="collapse" for="c-36464528">[-]</label><label class="expand" for="c-36464528">[1 more]</label></div><br/><div class="children"><div class="content">A big part of understanding concepts (and being able to reason) is correlating with patterns you already know. Even though there are errors, LLMs are doing fairly well at pattern recall. Which makes me believe that we&#x27;re perhaps one additive breakthrough away from another major leap in abilities.</div><br/></div></div><div id="36463313" class="c"><input type="checkbox" id="c-36463313" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#36464528">prev</a><span>|</span><a href="#36463670">next</a><span>|</span><label class="collapse" for="c-36463313">[-]</label><label class="expand" for="c-36463313">[4 more]</label></div><br/><div class="children"><div class="content">Sam Altman has been saying this for months. Nothing noteworthy here for someone following the industry closely.<p>A16z’s latest summary of the landscape was way more useful and relevant than this.</div><br/><div id="36466140" class="c"><input type="checkbox" id="c-36466140" checked=""/><div class="controls bullet"><span class="by">attemptone</span><span>|</span><a href="#36463313">parent</a><span>|</span><a href="#36464081">next</a><span>|</span><label class="collapse" for="c-36466140">[-]</label><label class="expand" for="c-36466140">[1 more]</label></div><br/><div class="children"><div class="content">Following research instead of the industry would this make a basic fact known for a while.</div><br/></div></div><div id="36464081" class="c"><input type="checkbox" id="c-36464081" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36463313">parent</a><span>|</span><a href="#36466140">prev</a><span>|</span><a href="#36463670">next</a><span>|</span><label class="collapse" for="c-36464081">[-]</label><label class="expand" for="c-36464081">[2 more]</label></div><br/><div class="children"><div class="content">Sam had been saying it but few people trust he is telling the truth imo. Maybe he was.</div><br/><div id="36464120" class="c"><input type="checkbox" id="c-36464120" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36463313">root</a><span>|</span><a href="#36464081">parent</a><span>|</span><a href="#36463670">next</a><span>|</span><label class="collapse" for="c-36464120">[-]</label><label class="expand" for="c-36464120">[1 more]</label></div><br/><div class="children"><div class="content">Sam is telling the truth I&#x27;m sure but he&#x27;s commenting on economic walls rather than necessarily capability ones.</div><br/></div></div></div></div></div></div><div id="36463670" class="c"><input type="checkbox" id="c-36463670" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#36463313">prev</a><span>|</span><a href="#36464776">next</a><span>|</span><label class="collapse" for="c-36463670">[-]</label><label class="expand" for="c-36463670">[1 more]</label></div><br/><div class="children"><div class="content">The optimal way right now seems to be to train a gigantic model to get strong capabilities and use distillation to make a much smaller model stronger. But you seem to still need that gigantic model and we still don&#x27;t know if we&#x27;re running out of space to improve capability just by scaling up the model (for reference GPT-4 is likely 1.76T parameters and we may be able to get much better just by scaling up)</div><br/></div></div><div id="36464776" class="c"><input type="checkbox" id="c-36464776" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#36463670">prev</a><span>|</span><a href="#36462814">next</a><span>|</span><label class="collapse" for="c-36464776">[-]</label><label class="expand" for="c-36464776">[1 more]</label></div><br/><div class="children"><div class="content">AI should not get better. We are playing with fire because the technology is moving too fast. We should consider the ethical implications of it before developing it, not after.</div><br/></div></div><div id="36462814" class="c"><input type="checkbox" id="c-36462814" checked=""/><div class="controls bullet"><span class="by">satellite2</span><span>|</span><a href="#36464776">prev</a><span>|</span><a href="#36462956">next</a><span>|</span><label class="collapse" for="c-36462814">[-]</label><label class="expand" for="c-36462814">[7 more]</label></div><br/><div class="children"><div class="content">How is the economist qualified to answer this question?</div><br/><div id="36463122" class="c"><input type="checkbox" id="c-36463122" checked=""/><div class="controls bullet"><span class="by">semiquaver</span><span>|</span><a href="#36462814">parent</a><span>|</span><a href="#36463800">next</a><span>|</span><label class="collapse" for="c-36463122">[-]</label><label class="expand" for="c-36463122">[2 more]</label></div><br/><div class="children"><div class="content">Not sure where I heard this but it’s apparently a common trope that many of the politicians and leaders that treat The Economist as close to holy writ are often horrified to learn that most the staff is actually a bunch of very precocious 20-somethings that are good at research and writing in an authoritative tone.<p>Actually, now that I think of it, not so different from LLMs…<p>(Full disclosure, I’ve been a subscriber for a couple of decades)</div><br/><div id="36465117" class="c"><input type="checkbox" id="c-36465117" checked=""/><div class="controls bullet"><span class="by">koboll</span><span>|</span><a href="#36462814">root</a><span>|</span><a href="#36463122">parent</a><span>|</span><a href="#36463800">next</a><span>|</span><label class="collapse" for="c-36465117">[-]</label><label class="expand" for="c-36465117">[1 more]</label></div><br/><div class="children"><div class="content">Also true about every single consulting firm</div><br/></div></div></div></div><div id="36463800" class="c"><input type="checkbox" id="c-36463800" checked=""/><div class="controls bullet"><span class="by">dtgriscom</span><span>|</span><a href="#36462814">parent</a><span>|</span><a href="#36463122">prev</a><span>|</span><a href="#36463115">next</a><span>|</span><label class="collapse" for="c-36463800">[-]</label><label class="expand" for="c-36463800">[1 more]</label></div><br/><div class="children"><div class="content">They periodically advertise for science journalism interns, sometimes with the proviso &quot;Our aim is more to discover writing talent in a science student or scientist than scientific aptitude in a budding journalist.&quot; So, I think they&#x27;re at least trying to hire the qualified.<p>(Long-time Economist subscriber.)</div><br/></div></div><div id="36463115" class="c"><input type="checkbox" id="c-36463115" checked=""/><div class="controls bullet"><span class="by">krona</span><span>|</span><a href="#36462814">parent</a><span>|</span><a href="#36463800">prev</a><span>|</span><a href="#36462826">next</a><span>|</span><label class="collapse" for="c-36463115">[-]</label><label class="expand" for="c-36463115">[1 more]</label></div><br/><div class="children"><div class="content">Usually its the most credentialed I&#x27;m least interested in listening to, especially when the value of those credentials are dependant upon the future looking a particular way.</div><br/></div></div><div id="36462826" class="c"><input type="checkbox" id="c-36462826" checked=""/><div class="controls bullet"><span class="by">JimtheCoder</span><span>|</span><a href="#36462814">parent</a><span>|</span><a href="#36463115">prev</a><span>|</span><a href="#36462956">next</a><span>|</span><label class="collapse" for="c-36462826">[-]</label><label class="expand" for="c-36462826">[2 more]</label></div><br/><div class="children"><div class="content">Using this logic, how are they qualified to answer 90% of the questions their articles deal with...</div><br/><div id="36463019" class="c"><input type="checkbox" id="c-36463019" checked=""/><div class="controls bullet"><span class="by">satellite2</span><span>|</span><a href="#36462814">root</a><span>|</span><a href="#36462826">parent</a><span>|</span><a href="#36462956">next</a><span>|</span><label class="collapse" for="c-36463019">[-]</label><label class="expand" for="c-36463019">[1 more]</label></div><br/><div class="children"><div class="content">Yes and no. Most questions they answers are more politically leaning, meaning that they are not optimisation problems but they answer the question of the kind of society we want to be.<p>This specific article seems to be reporting on a very technical issue on how to continue to scale LLM. Even scientific papers have a hard time answering those kind of questions because unless in very special circumstances where we can show with a good confidence that there are limitations (P vs NP for instance) the answer will simply be given by the most successful approach.</div><br/></div></div></div></div></div></div><div id="36462956" class="c"><input type="checkbox" id="c-36462956" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#36462814">prev</a><span>|</span><a href="#36464608">next</a><span>|</span><label class="collapse" for="c-36462956">[-]</label><label class="expand" for="c-36462956">[1 more]</label></div><br/><div class="children"><div class="content">I think the worst case scenario, and the one that I think is the most likely, is that we plateau at broad and shallow AI. Broad enough such that it can be used to replace many workers (it may not do a great job though), but narrow enough such that we don&#x27;t really see the kinds of productivity increases that would usher in the pseudo-utopia that many AI folks talk about.</div><br/></div></div><div id="36464608" class="c"><input type="checkbox" id="c-36464608" checked=""/><div class="controls bullet"><span class="by">willmadden</span><span>|</span><a href="#36462956">prev</a><span>|</span><a href="#36462615">next</a><span>|</span><label class="collapse" for="c-36464608">[-]</label><label class="expand" for="c-36464608">[2 more]</label></div><br/><div class="children"><div class="content">Define “running out of road”.<p>Road to control narratives or road to be useful?</div><br/><div id="36464989" class="c"><input type="checkbox" id="c-36464989" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#36464608">parent</a><span>|</span><a href="#36462615">next</a><span>|</span><label class="collapse" for="c-36464989">[-]</label><label class="expand" for="c-36464989">[1 more]</label></div><br/><div class="children"><div class="content">There are no datasets that are an order of magnitude bigger, and the latest model itself may be a mixture of experts; i.e., more of the same.</div><br/></div></div></div></div><div id="36462615" class="c"><input type="checkbox" id="c-36462615" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#36464608">prev</a><span>|</span><a href="#36464208">next</a><span>|</span><label class="collapse" for="c-36462615">[-]</label><label class="expand" for="c-36462615">[1 more]</label></div><br/><div class="children"><div class="content">Non paywall: <a href="https:&#x2F;&#x2F;archive.ph&#x2F;XwWTi" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.ph&#x2F;XwWTi</a><p>Imo, it is true that the current architecture is hitting the limit. We need a breakthrough on the scale of the transistor to get past this problem. We know it is possible though. Every single human is proof that high performance AI can be run with less energy than a laptop. We just need a dedicated architecture for their working mechanisms the same way a transistor is the embodiment of 1 and 0.<p>Unfortunately, in terms of understanding intelligence and how they work, I don&#x27;t think we have made any significant advance in the last few decades. Maybe with better tools at probing how the LLM work, we can get some new insights.</div><br/></div></div><div id="36464208" class="c"><input type="checkbox" id="c-36464208" checked=""/><div class="controls bullet"><span class="by">di456</span><span>|</span><a href="#36462615">prev</a><span>|</span><a href="#36462846">next</a><span>|</span><label class="collapse" for="c-36464208">[-]</label><label class="expand" for="c-36464208">[1 more]</label></div><br/><div class="children"><div class="content">No surprise.  Scale vertically first, then horizontally</div><br/></div></div><div id="36462846" class="c"><input type="checkbox" id="c-36462846" checked=""/><div class="controls bullet"><span class="by">jackmott42</span><span>|</span><a href="#36464208">prev</a><span>|</span><a href="#36462591">next</a><span>|</span><label class="collapse" for="c-36462846">[-]</label><label class="expand" for="c-36462846">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty excited by the possibilities. I am astounded by how much these language models can do with nothing but &quot;predict the next word&quot; as the core idea.  I imagine in the near future having collections of a hundred different models, physics models, grammar models, fact models, sentiment models, vision models, wired all together by coordination models, and wired up to math tools and databases to ground truth when possible. I think it can get pretty wild.<p>Just chatGPT wired up to Wolfram Alpha is already pretty creepy amazing.</div><br/></div></div><div id="36462870" class="c"><input type="checkbox" id="c-36462870" checked=""/><div class="controls bullet"><span class="by">xwdv</span><span>|</span><a href="#36462591">prev</a><span>|</span><a href="#36464704">next</a><span>|</span><label class="collapse" for="c-36462870">[-]</label><label class="expand" for="c-36462870">[3 more]</label></div><br/><div class="children"><div class="content">I remember how hyped people were seeing the progress from GPT3.5 to GPT4, people really felt like many jobs were going to be replaced very soon. The next big advancement was around the corner. I think the limitations of LLMs should be more salient to them by now.</div><br/><div id="36463726" class="c"><input type="checkbox" id="c-36463726" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#36462870">parent</a><span>|</span><a href="#36464704">next</a><span>|</span><label class="collapse" for="c-36463726">[-]</label><label class="expand" for="c-36463726">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been literally just 3 months since GPT-4 was released. I think you&#x27;re gonna see a lot of changes especially once GPT starts being trained on ChatGPT data</div><br/><div id="36465843" class="c"><input type="checkbox" id="c-36465843" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36462870">root</a><span>|</span><a href="#36463726">parent</a><span>|</span><a href="#36464704">next</a><span>|</span><label class="collapse" for="c-36465843">[-]</label><label class="expand" for="c-36465843">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI can just filter out text that they generated. Other models might be problematic.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>