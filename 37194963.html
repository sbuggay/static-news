<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692522055730" as="style"/><link rel="stylesheet" href="styles.css?v=1692522055730"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://wccftech.com/amd-radeon-7900-xtx-offers-higher-generative-ai-performance-per-dollar-than-nvidia-rtx-4080-after-990-speedup/">AMD&#x27;s 7900 XTX achieves better value for Stable Diffusion than Nvidia RTX 4080</a> <span class="domain">(<a href="https://wccftech.com">wccftech.com</a>)</span></div><div class="subtext"><span>mauricesvp</span> | <span>73 comments</span></div><br/><div><div id="37195466" class="c"><input type="checkbox" id="c-37195466" checked=""/><div class="controls bullet"><span class="by">klft</span><span>|</span><a href="#37196042">next</a><span>|</span><label class="collapse" for="c-37195466">[-]</label><label class="expand" for="c-37195466">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Using Microsoft Olive and DirectML instead of the PyTorch pathway results in the AMD 7900 XTX going form a measly 1.87 iterations per second to 18.59 iterations per second!<p>So the headline should be Microsoft Olive vs. PyTorch and not AMD vs. Nvidia.</div><br/><div id="37197305" class="c"><input type="checkbox" id="c-37197305" checked=""/><div class="controls bullet"><span class="by">mananaysiempre</span><span>|</span><a href="#37195466">parent</a><span>|</span><a href="#37196042">next</a><span>|</span><label class="collapse" for="c-37197305">[-]</label><label class="expand" for="c-37197305">[1 more]</label></div><br/><div class="children"><div class="content">The results of the usual benchmarks are inconclusive between the 7900 XTX and the 4080, Nvidia is only somewhat more expensive, yet CUDA is much more popular than anything AMD is allowed to support. So I’d say this makes sense as an AMD vs Nvidia comparison as well.</div><br/></div></div></div></div><div id="37196042" class="c"><input type="checkbox" id="c-37196042" checked=""/><div class="controls bullet"><span class="by">DarkmSparks</span><span>|</span><a href="#37195466">prev</a><span>|</span><a href="#37195478">next</a><span>|</span><label class="collapse" for="c-37196042">[-]</label><label class="expand" for="c-37196042">[37 more]</label></div><br/><div class="children"><div class="content">Been watching this quite closely.
As far as I summarise, the 7900XTX is the first (and only) desktop GPU from AMD that _might_ be worth buying. (They own the console gaming space, but thats a different story).<p>Not Nvidia beating due to the CUDA issue, but a massive leap in the right direction.<p>Intel is also making _some_ progress with its ARC range.<p>Its going to be happy days for us users if&#x2F;when AMD&#x2F;Intel are competitive, and cut some of that monopoly margin off Nvidias pricing, but a way to go yet.</div><br/><div id="37196383" class="c"><input type="checkbox" id="c-37196383" checked=""/><div class="controls bullet"><span class="by">EtienneK</span><span>|</span><a href="#37196042">parent</a><span>|</span><a href="#37196174">next</a><span>|</span><label class="collapse" for="c-37196383">[-]</label><label class="expand" for="c-37196383">[21 more]</label></div><br/><div class="children"><div class="content">This is not true at all. AMD GPUs have been constantly delivering better bang-for-buck for a while now.<p>Edit: of course I am talking about gaming here since you mentioned consoles</div><br/><div id="37196681" class="c"><input type="checkbox" id="c-37196681" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196383">parent</a><span>|</span><a href="#37196929">next</a><span>|</span><label class="collapse" for="c-37196681">[-]</label><label class="expand" for="c-37196681">[7 more]</label></div><br/><div class="children"><div class="content">For mid-tier gaming they are very competitive, but for consumer AI they were not even a player until very recently and still marginal at best.</div><br/><div id="37196916" class="c"><input type="checkbox" id="c-37196916" checked=""/><div class="controls bullet"><span class="by">mjan22640</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196681">parent</a><span>|</span><a href="#37196929">next</a><span>|</span><label class="collapse" for="c-37196916">[-]</label><label class="expand" for="c-37196916">[6 more]</label></div><br/><div class="children"><div class="content">What is the root cause of AMD not being good for AI?</div><br/><div id="37196937" class="c"><input type="checkbox" id="c-37196937" checked=""/><div class="controls bullet"><span class="by">arcanemachiner</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196916">parent</a><span>|</span><a href="#37196929">next</a><span>|</span><label class="collapse" for="c-37196937">[-]</label><label class="expand" for="c-37196937">[5 more]</label></div><br/><div class="children"><div class="content">“Nvidia&#x27;s SDK, CUDA, is the industry standard and they have built a huge range of tools on top of that for training large models, inference-serving, optimization, transfer learning, infrastructure management and operations automation, and a lot of vertical-specific applications.”<p><a href="https:&#x2F;&#x2F;www.google.com&#x2F;search?q=why%20is%20nvidia%20better%20for%20ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.google.com&#x2F;search?q=why%20is%20nvidia%20better%2...</a></div><br/><div id="37197058" class="c"><input type="checkbox" id="c-37197058" checked=""/><div class="controls bullet"><span class="by">mjan22640</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196937">parent</a><span>|</span><a href="#37196929">next</a><span>|</span><label class="collapse" for="c-37197058">[-]</label><label class="expand" for="c-37197058">[4 more]</label></div><br/><div class="children"><div class="content">So its not the hw, but an API issue? Is it not possible to port the tools to the AMD API? Or does the AMD API not allow to properly utilize the compute power of the hw?</div><br/><div id="37197233" class="c"><input type="checkbox" id="c-37197233" checked=""/><div class="controls bullet"><span class="by">dagw</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37197058">parent</a><span>|</span><a href="#37197274">next</a><span>|</span><label class="collapse" for="c-37197233">[-]</label><label class="expand" for="c-37197233">[1 more]</label></div><br/><div class="children"><div class="content">AMDs APIs have historically been way behind CUDA, both in features and ease of use, making it much harder to reach feature and performance parity with CUDA.<p>The second big problem is that AMD has a history of abandoning APIs. While Nvidia has been focusing on CUDA from the start, AMD is on its third or fourth API. A lot of developers got burned early on by putting in a lot of effort into supporting AMD cards, only to have their work become useless as AMD dropped old API.<p>So if you wanted to support AMD today you have to choose between using their latest API which is currently only supported on a handful of their latest top end cards, or using their previous API, which is much worse and may or may not be supported in 2 years time.</div><br/></div></div><div id="37197274" class="c"><input type="checkbox" id="c-37197274" checked=""/><div class="controls bullet"><span class="by">mananaysiempre</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37197058">parent</a><span>|</span><a href="#37197233">prev</a><span>|</span><a href="#37197106">next</a><span>|</span><label class="collapse" for="c-37197274">[-]</label><label class="expand" for="c-37197274">[1 more]</label></div><br/><div class="children"><div class="content">CUDA is proprietary to Nvidia, and it’s unlikely they would ever allow implementing it on other vendors’ hardware. DirectML mentioned here is proprietary to Microsoft but at least hardware-neutral. It is also much less popular. OpenCL is open but receives hardly any usage at all, unfortunately.<p>(To be fair, Nvidia were at the forefront of this GPGPU thing since before there were dedicated APIs for it, so they certainly deserve some credit, but at this point they seem in desperate need for someone to finally knock them down.)</div><br/></div></div><div id="37197106" class="c"><input type="checkbox" id="c-37197106" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37197058">parent</a><span>|</span><a href="#37197274">prev</a><span>|</span><a href="#37196929">next</a><span>|</span><label class="collapse" for="c-37197106">[-]</label><label class="expand" for="c-37197106">[1 more]</label></div><br/><div class="children"><div class="content">No CUDA is heavily integrated with how Nvidia hardware operates</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37196929" class="c"><input type="checkbox" id="c-37196929" checked=""/><div class="controls bullet"><span class="by">anshukg</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196383">parent</a><span>|</span><a href="#37196681">prev</a><span>|</span><a href="#37197101">next</a><span>|</span><label class="collapse" for="c-37196929">[-]</label><label class="expand" for="c-37196929">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know , I think it is still plenty of time before nvidia could have a serious competitor for value : <a href="https:&#x2F;&#x2F;medium.com&#x2F;@1kg&#x2F;nvidias-cuda-monopoly-6446f4ef7375" rel="nofollow noreferrer">https:&#x2F;&#x2F;medium.com&#x2F;@1kg&#x2F;nvidias-cuda-monopoly-6446f4ef7375</a></div><br/><div id="37197311" class="c"><input type="checkbox" id="c-37197311" checked=""/><div class="controls bullet"><span class="by">xcdzvyn</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196929">parent</a><span>|</span><a href="#37197017">next</a><span>|</span><label class="collapse" for="c-37197311">[-]</label><label class="expand" for="c-37197311">[1 more]</label></div><br/><div class="children"><div class="content">CUDA does not monopolise GPGPU. There are other offerings, they all just happen to suck.</div><br/></div></div><div id="37197017" class="c"><input type="checkbox" id="c-37197017" checked=""/><div class="controls bullet"><span class="by">EtienneK</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196929">parent</a><span>|</span><a href="#37197311">prev</a><span>|</span><a href="#37197101">next</a><span>|</span><label class="collapse" for="c-37197017">[-]</label><label class="expand" for="c-37197017">[3 more]</label></div><br/><div class="children"><div class="content">Again, I was referring to gaming and not AI since the comment I commented on mentioned consoles.</div><br/><div id="37197113" class="c"><input type="checkbox" id="c-37197113" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37197017">parent</a><span>|</span><a href="#37197101">next</a><span>|</span><label class="collapse" for="c-37197113">[-]</label><label class="expand" for="c-37197113">[2 more]</label></div><br/><div class="children"><div class="content">Why selectively apply your filter for gaming only when Nvidi does everything</div><br/><div id="37197289" class="c"><input type="checkbox" id="c-37197289" checked=""/><div class="controls bullet"><span class="by">jamespo</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37197113">parent</a><span>|</span><a href="#37197101">next</a><span>|</span><label class="collapse" for="c-37197289">[-]</label><label class="expand" for="c-37197289">[1 more]</label></div><br/><div class="children"><div class="content">because the two markets don&#x27;t really overlap?</div><br/></div></div></div></div></div></div></div></div><div id="37197101" class="c"><input type="checkbox" id="c-37197101" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196383">parent</a><span>|</span><a href="#37196929">prev</a><span>|</span><a href="#37196615">next</a><span>|</span><label class="collapse" for="c-37197101">[-]</label><label class="expand" for="c-37197101">[1 more]</label></div><br/><div class="children"><div class="content">Gaming is only a very small part of what you expect from GPUs these days</div><br/></div></div><div id="37196615" class="c"><input type="checkbox" id="c-37196615" checked=""/><div class="controls bullet"><span class="by">Iulioh</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196383">parent</a><span>|</span><a href="#37197101">prev</a><span>|</span><a href="#37196174">next</a><span>|</span><label class="collapse" for="c-37196615">[-]</label><label class="expand" for="c-37196615">[7 more]</label></div><br/><div class="children"><div class="content">Eh.<p>For me the problem is technology and not raw performance<p>DLSS and RT are massive for someone with a 4k screen but now 4k gaming hardware outside league of legends lol</div><br/><div id="37196670" class="c"><input type="checkbox" id="c-37196670" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196615">parent</a><span>|</span><a href="#37196174">next</a><span>|</span><label class="collapse" for="c-37196670">[-]</label><label class="expand" for="c-37196670">[6 more]</label></div><br/><div class="children"><div class="content">DLSS is only marginally better then FSR 2.0.<p>Sure, you can see the difference if you&#x27;re carefully looking for it in comparison videos. But when you&#x27;re actually playing it&#x27;s usually not noticeable to any meaningful degree. You&#x27;re gonna get occasional artefacts with both.<p>As for ray tracing - to be honest, playing at 4k with an RTX 3070 there&#x27;s very few games where I can turn it on without the game running unacceptably slow even with DLSS.<p>Ray traced shadows and AO are nice but hardly a deal breaker.<p>I think it&#x27;s more the desire to have the latest and greatest tech that makes Nvidia cards desirable, rather than any material difference that you&#x27;ll actually notice while gaming.<p>Some good comparison shots of DLSS 2 and FSR 2 in this video.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;1WM_w7TBbj0" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;1WM_w7TBbj0</a></div><br/><div id="37197225" class="c"><input type="checkbox" id="c-37197225" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196670">parent</a><span>|</span><a href="#37197070">next</a><span>|</span><label class="collapse" for="c-37197225">[-]</label><label class="expand" for="c-37197225">[1 more]</label></div><br/><div class="children"><div class="content">Seems like it&#x27;s still better to buy Nvidia then. Nvidia cards support both DLSS and FSR so you can choose the tech that works best with the particular game.<p>Meanwhile AMD only supports FSR and if the game looks better with DLSS then there&#x27;s nothing you can do.<p>AMD cards also aren&#x27;t that much cheaper so it doesn&#x27;t make much sense to buy worse tech that you&#x27;ll be using for years every day to save a few euros.</div><br/></div></div><div id="37197070" class="c"><input type="checkbox" id="c-37197070" checked=""/><div class="controls bullet"><span class="by">izacus</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196670">parent</a><span>|</span><a href="#37197225">prev</a><span>|</span><a href="#37197180">next</a><span>|</span><label class="collapse" for="c-37197070">[-]</label><label class="expand" for="c-37197070">[3 more]</label></div><br/><div class="children"><div class="content">I mean, you talk about &quot;comparison videos&quot;, when in reality the issue is that significant amount of games support DLSS and pretty much noone supports FSR 2.0.<p>You can do any comparison you want, but it won&#x27;t help you when your software won&#x27;t make use of the library your GPU supports.<p>(Also, we&#x27;re at DLSS 3.0 now with frame generation which moves the bar higher.)</div><br/><div id="37197136" class="c"><input type="checkbox" id="c-37197136" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37197070">parent</a><span>|</span><a href="#37197180">next</a><span>|</span><label class="collapse" for="c-37197136">[-]</label><label class="expand" for="c-37197136">[2 more]</label></div><br/><div class="children"><div class="content">Fsr 2 is supported out of tje box on Gamescope in Linux for all games</div><br/><div id="37197201" class="c"><input type="checkbox" id="c-37197201" checked=""/><div class="controls bullet"><span class="by">grey8</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37197136">parent</a><span>|</span><a href="#37197180">next</a><span>|</span><label class="collapse" for="c-37197201">[-]</label><label class="expand" for="c-37197201">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not FSR2, that&#x27;s FSR1.<p>FSR2 needs game specific integration (IIRC because it needs access to motion vectors) and cannot be applied globally.</div><br/></div></div></div></div></div></div><div id="37197180" class="c"><input type="checkbox" id="c-37197180" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196670">parent</a><span>|</span><a href="#37197070">prev</a><span>|</span><a href="#37196174">next</a><span>|</span><label class="collapse" for="c-37197180">[-]</label><label class="expand" for="c-37197180">[1 more]</label></div><br/><div class="children"><div class="content">Hardware Unboxed&#x27;s viewership draw is placating &quot;Team Red&quot;, or the the people who treat hardware like a sports competition, so when they say:<p>- the overwhelming conclusion is DLSS is the superior upscaling technology with near universally better results<p>- it was a brutal result for AMD<p>- DLSS gives NVIDIA a clear selling point<p>I&#x27;m not sure if it backs your suggestion...</div><br/></div></div></div></div></div></div></div></div><div id="37196174" class="c"><input type="checkbox" id="c-37196174" checked=""/><div class="controls bullet"><span class="by">throwaway2990</span><span>|</span><a href="#37196042">parent</a><span>|</span><a href="#37196383">prev</a><span>|</span><a href="#37196072">next</a><span>|</span><label class="collapse" for="c-37196174">[-]</label><label class="expand" for="c-37196174">[14 more]</label></div><br/><div class="children"><div class="content">&gt; the 7900XTX is the first (and only) desktop GPU from AMD that _might_ be worth buying.<p>Unless you need RT for gaming then most of AMDs cards are better value.</div><br/><div id="37196223" class="c"><input type="checkbox" id="c-37196223" checked=""/><div class="controls bullet"><span class="by">dralley</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196174">parent</a><span>|</span><a href="#37196755">next</a><span>|</span><label class="collapse" for="c-37196223">[-]</label><label class="expand" for="c-37196223">[12 more]</label></div><br/><div class="children"><div class="content">And if you use Linux and don&#x27;t need CUDA, it&#x27;s really no contest.  The AMD experience on Linux is vastly better than the Nvidia one.</div><br/><div id="37196763" class="c"><input type="checkbox" id="c-37196763" checked=""/><div class="controls bullet"><span class="by">KronisLV</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196223">parent</a><span>|</span><a href="#37196393">next</a><span>|</span><label class="collapse" for="c-37196763">[-]</label><label class="expand" for="c-37196763">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The AMD experience on Linux is vastly better than the Nvidia one.<p>I just wish we had an equivalent of AMD Software on Linux, so I could mess around with the settings more.<p>For example, I like to limit the GPU to 50-75% of it&#x27;s total power for ambient heat&#x2F;cooling reasons, or UPS&#x2F;PSU&#x2F;electricity bill reasons when specific games make it hard to cap framerates.<p>With AMD Software on Windows, it&#x27;s no big deal. On Linux, the best I found was CoreCtrl: <a href="https:&#x2F;&#x2F;gitlab.com&#x2F;corectrl&#x2F;corectrl" rel="nofollow noreferrer">https:&#x2F;&#x2F;gitlab.com&#x2F;corectrl&#x2F;corectrl</a><p>Sadly, it doesn&#x27;t seem to work all that well for my use case, which I mentioned in my blog post when using Linux instead of Windows as my daily driver at home too: <a href="https:&#x2F;&#x2F;blog.kronis.dev&#x2F;articles&#x2F;a-week-of-linux-instead-of-windows" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.kronis.dev&#x2F;articles&#x2F;a-week-of-linux-instead-of-...</a><p>&gt; You see, by default the card controls its own GPU and memory clock values, which means that when idle the GPU draws around 40 W of power. However, if I want to set a limit for how much W in total it can use, it also makes me set the GPU and memory clock values, which will them be fixed: so at idle the GPU will use about 60 W of power.<p>Oh also ROCm makes me want to pull my hair out. Still use almost all AMD hardware though, except for low power Celeron in netbook for notes.</div><br/><div id="37196900" class="c"><input type="checkbox" id="c-37196900" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196763">parent</a><span>|</span><a href="#37196947">next</a><span>|</span><label class="collapse" for="c-37196900">[-]</label><label class="expand" for="c-37196900">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried rocm-smi for controlling power limits? <a href="https:&#x2F;&#x2F;manpages.debian.org&#x2F;experimental&#x2F;rocm-smi&#x2F;rocm-smi.1.en.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;manpages.debian.org&#x2F;experimental&#x2F;rocm-smi&#x2F;rocm-smi.1...</a></div><br/></div></div><div id="37196947" class="c"><input type="checkbox" id="c-37196947" checked=""/><div class="controls bullet"><span class="by">mjan22640</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196763">parent</a><span>|</span><a href="#37196900">prev</a><span>|</span><a href="#37196393">next</a><span>|</span><label class="collapse" for="c-37196947">[-]</label><label class="expand" for="c-37196947">[1 more]</label></div><br/><div class="children"><div class="content">You got the CoreCtrl source. You can modify it to suit your needs.</div><br/></div></div></div></div><div id="37196393" class="c"><input type="checkbox" id="c-37196393" checked=""/><div class="controls bullet"><span class="by">cout</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196223">parent</a><span>|</span><a href="#37196763">prev</a><span>|</span><a href="#37196755">next</a><span>|</span><label class="collapse" for="c-37196393">[-]</label><label class="expand" for="c-37196393">[8 more]</label></div><br/><div class="children"><div class="content">Can you give a specific example of what&#x27;s better about the AMD experience on Linux?</div><br/><div id="37196968" class="c"><input type="checkbox" id="c-37196968" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196393">parent</a><span>|</span><a href="#37196639">next</a><span>|</span><label class="collapse" for="c-37196968">[-]</label><label class="expand" for="c-37196968">[1 more]</label></div><br/><div class="children"><div class="content">On Kubuntu 22.04 with an RTX 30*0, you have to downgrade NVIDIA drivers to version 515 or the system won&#x27;t boot. Has been like this since launch. This is especially annoying since if you want to install any other NVIDIA software, the installer will try to update the driver version and break the system again.<p>A friend of mine has a similar issue on Ubuntu where the system only boots 2 out of 3 times.<p>I also recall having similar issues almost every time when I installed systems with NVIDIA GPUs in the last decade.<p>Checking the forums, other people are having similar issues.<p><a href="https:&#x2F;&#x2F;forums.developer.nvidia.com&#x2F;t&#x2F;no-gui-or-display-after-install-the-nvidia-driver-535-on-ubuntu-20-04&#x2F;260521" rel="nofollow noreferrer">https:&#x2F;&#x2F;forums.developer.nvidia.com&#x2F;t&#x2F;no-gui-or-display-afte...</a><p><a href="https:&#x2F;&#x2F;forums.developer.nvidia.com&#x2F;t&#x2F;no-display-output-when-nvidia-driver-installed-ubuntu-22-04-2&#x2F;256756" rel="nofollow noreferrer">https:&#x2F;&#x2F;forums.developer.nvidia.com&#x2F;t&#x2F;no-display-output-when...</a></div><br/></div></div><div id="37196639" class="c"><input type="checkbox" id="c-37196639" checked=""/><div class="controls bullet"><span class="by">dralley</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196393">parent</a><span>|</span><a href="#37196968">prev</a><span>|</span><a href="#37196448">next</a><span>|</span><label class="collapse" for="c-37196639">[-]</label><label class="expand" for="c-37196639">[1 more]</label></div><br/><div class="children"><div class="content">The open source drivers for Nvidia have sucked for years, meaning you need to use the proprietary drivers.  The proprietary drivers frequently break on kernel updates and have historically been problematic to use in combination with Wayland.</div><br/></div></div><div id="37196448" class="c"><input type="checkbox" id="c-37196448" checked=""/><div class="controls bullet"><span class="by">neurostimulant</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196393">parent</a><span>|</span><a href="#37196639">prev</a><span>|</span><a href="#37196901">next</a><span>|</span><label class="collapse" for="c-37196448">[-]</label><label class="expand" for="c-37196448">[4 more]</label></div><br/><div class="children"><div class="content">Wayland is less buggy on AMD and Intel.</div><br/><div id="37196545" class="c"><input type="checkbox" id="c-37196545" checked=""/><div class="controls bullet"><span class="by">throwaway2990</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196448">parent</a><span>|</span><a href="#37196901">next</a><span>|</span><label class="collapse" for="c-37196545">[-]</label><label class="expand" for="c-37196545">[3 more]</label></div><br/><div class="children"><div class="content">This^ and also there’s always some driver issue on linux for Nvidia cards. It’s one of those things you set it up and then never want to touch it because if you update it. It’s flipping a coin on if you will spend the next few hours trying to figure out what’s wrong.</div><br/><div id="37196835" class="c"><input type="checkbox" id="c-37196835" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196545">parent</a><span>|</span><a href="#37196901">next</a><span>|</span><label class="collapse" for="c-37196835">[-]</label><label class="expand" for="c-37196835">[2 more]</label></div><br/><div class="children"><div class="content">I keep hearing complaints about NVIDIA driver issues under Linux, so they must be real.<p>Nevertheless, I have used for twenty years many different NVIDIA cards under Linux, both GeForce and Quadro models, on many different desktop and laptop computers.<p>I have never seen any problem with the NVIDIA drivers and libraries, everything has worked fine after installation, without needing any special action beyond choosing to install the NVIDIA software. The same was also true under FreeBSD.<p>The only exception to this was some years ago, on a laptop with NVIDIA Optimus switchable GPUs, where I have lost a couple of days until succeeding to configure how to select between the Intel GPU and the NVIDIA GPU.<p>I also have some AMD GPUs, older models which still had great FP64 performance, so they are used for computational applications, but with those I had greater problems under Linux, when used with many monitors.<p>Therefore whether problems with the Linux NVIDIA drivers are encountered must be dependent on complex combinations of hardware and software, so it is impossible to predict whether they will be encountered or not.<p>Unexpected problems may always happen with any piece of hardware under an open-source operating system, because the manufacturers typically do not provide technical documentation like they did before 1995 and most of them test their hardware only with Windows. NVIDIA certainly provides much better software support for Linux and FreeBSD than the majority of the hardware vendors.<p>So my point is that it is incorrect to attempt to discourage the Linux users to use NVIDIA due to supposed driver problems, because many of them, perhaps most of them will never encounter any problem.<p>Only the Intel GPUs can be said to have better Linux support, and they also have the best GPU technical documentation, much better than that of AMD. While NVIDIA has the worst documentation for their hardware, they have excellent documentation for the huge amount of software that they provide freely for their  GPUs under Linux.<p>What is wrong with NVIDIA is neither their software support for Linux nor the documentation for that software, but their pricing policy that always seems based on a greediness that is excessive even for a profit-oriented company.</div><br/><div id="37197041" class="c"><input type="checkbox" id="c-37197041" checked=""/><div class="controls bullet"><span class="by">butterknife</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196835">parent</a><span>|</span><a href="#37196901">next</a><span>|</span><label class="collapse" for="c-37197041">[-]</label><label class="expand" for="c-37197041">[1 more]</label></div><br/><div class="children"><div class="content">You used NVIDIA cards for twenty years and never run into an issue where the mainline driver stopped supporting your GPU and you had to manually download community hosted legacy driver family on every kernel upgrade? Gets old quick...</div><br/></div></div></div></div></div></div></div></div><div id="37196901" class="c"><input type="checkbox" id="c-37196901" checked=""/><div class="controls bullet"><span class="by">Karliss</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196393">parent</a><span>|</span><a href="#37196448">prev</a><span>|</span><a href="#37196755">next</a><span>|</span><label class="collapse" for="c-37196901">[-]</label><label class="expand" for="c-37196901">[1 more]</label></div><br/><div class="children"><div class="content">Note that my Nvidia experience is roughly 5 years old. Some things might have changed during that time.<p>When I was using Nvidia GPU my experience that 50% after a system update which included kernel update, the Nvidia kmod didn&#x27;t properly rebuild resulting in graphical interface completely non working next time I booted the system. In such situations I had to switch to terminal and enter a couple of commands to ensure the Nvidia driver is also properly updated and the glue layer between kernel and the driver rebuilt. Most of the time that helped but occasionally it didn&#x27;t and I had to stay on previous kernel version for a week until the issues got solved.<p>That was a couple of years ago with fedora. On a different distro with less frequent kernel updates and Nvidia drivers being repackaged by the maintainers of distro instead of third party repo like rpmfusion, or a bigger distro that Nvidia tests ahead of time, the experience might have been slightly more smooth.<p>Currently with AMD open source driver&#x27;s that&#x27;s not an issue since the drivers are part of kernel  so everything just works and always has compatible versions. I can install any recent distro and it will just work out of the box.<p>Last year Nvidia open sourced some parts of their driver, but not all of it (it still depends on large user space binary blobs), it might slightly help with the practical aspects of problem above. But I don&#x27;t know whether distros embraced it and did it actually help (I dropped Nvidia before that).<p>I don&#x27;t disagree that it&#x27;s at least partially more of the fault of Linux kernel for not having stable API, not so much AMD or Nvidia. But for the end user it doesn&#x27;t matter whose fault it is. AMD chose to play by the rules of Linux, which makes much better end user experience better.<p>All of this applies only to graphic driver part, GPU compute is a completely different story.<p>Another big part as others mentioned is wayland support. For a long time Nvidia drivers dindn&#x27;t support it at all due to not providing certain interface. From what I understand currently it works on the major desktop environments like Gnome and KDE, but due to them adding support for Nvidia specific interface not the other way around. So it still doesn&#x27;t work on more niche desktop environments which use the common interface supported by rest of drivers.<p>~12 years ago, before AMD made their official open source drivers situation was completely different. The choice then for AMD was between non official open source drivers which had 50% performance but worked correctly and the closed source AMD drivers that were 2x faster (when they worked) but otherwise buggy. Compared to that Nvidia closed source drivers worked.</div><br/></div></div></div></div></div></div><div id="37196755" class="c"><input type="checkbox" id="c-37196755" checked=""/><div class="controls bullet"><span class="by">nixass</span><span>|</span><a href="#37196042">root</a><span>|</span><a href="#37196174">parent</a><span>|</span><a href="#37196223">prev</a><span>|</span><a href="#37196072">next</a><span>|</span><label class="collapse" for="c-37196755">[-]</label><label class="expand" for="c-37196755">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Unless you need RT for gaming then most of AMDs cards are better value.<p>Nobody &quot;needs&quot; RT for gaming. It&#x27;s still in gimmicky phase and not worth neither performance hit nor the way it looks on screen.</div><br/></div></div></div></div><div id="37196072" class="c"><input type="checkbox" id="c-37196072" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#37196042">parent</a><span>|</span><a href="#37196174">prev</a><span>|</span><a href="#37195478">next</a><span>|</span><label class="collapse" for="c-37196072">[-]</label><label class="expand" for="c-37196072">[1 more]</label></div><br/><div class="children"><div class="content">For us users it has always been relatively fine, datacenters are the ones that are really milked by nvidia</div><br/></div></div></div></div><div id="37195478" class="c"><input type="checkbox" id="c-37195478" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37196042">prev</a><span>|</span><a href="#37195407">next</a><span>|</span><label class="collapse" for="c-37195478">[-]</label><label class="expand" for="c-37195478">[2 more]</label></div><br/><div class="children"><div class="content">Well the problem is that Automatic1111 is not fast...<p>Other diffusers based UIs with PyTorch Triton will net you 40%+ performance.<p>Facebook AITemplate inference in VoltaML will be at least twice as fast as A1111 on a 3080, with support for LORAs, controlnet and such. This supports AMD Instinct cards too.<p>What I am getting at is that people dont really care about A1111 performance on a 3080 because, for the most part, its fast enough.</div><br/><div id="37196745" class="c"><input type="checkbox" id="c-37196745" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#37195478">parent</a><span>|</span><a href="#37195407">next</a><span>|</span><label class="collapse" for="c-37196745">[-]</label><label class="expand" for="c-37196745">[1 more]</label></div><br/><div class="children"><div class="content">The extension ecosystem is what makes 1111 the winner for now. SegmentAnything, DreamBooth, ControlNet, OpenPose ... It&#x27;s <i>almost</i> easy</div><br/></div></div></div></div><div id="37195407" class="c"><input type="checkbox" id="c-37195407" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37195478">prev</a><span>|</span><a href="#37195543">next</a><span>|</span><label class="collapse" for="c-37195407">[-]</label><label class="expand" for="c-37195407">[9 more]</label></div><br/><div class="children"><div class="content">Nearly bought one thinking AMD will sort itself out shortly but hard to justify vs a second hand 3090 with 24gb and no cuda hassles</div><br/><div id="37195464" class="c"><input type="checkbox" id="c-37195464" checked=""/><div class="controls bullet"><span class="by">xigency</span><span>|</span><a href="#37195407">parent</a><span>|</span><a href="#37195543">next</a><span>|</span><label class="collapse" for="c-37195464">[-]</label><label class="expand" for="c-37195464">[8 more]</label></div><br/><div class="children"><div class="content">It makes a lot of sense to invest in a 24GB card for the right price.</div><br/><div id="37195600" class="c"><input type="checkbox" id="c-37195600" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#37195407">root</a><span>|</span><a href="#37195464">parent</a><span>|</span><a href="#37195543">next</a><span>|</span><label class="collapse" for="c-37195600">[-]</label><label class="expand" for="c-37195600">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still basking in my good fortune buying a hundred 3090s from crypto miners at rock bottom prices.</div><br/><div id="37195661" class="c"><input type="checkbox" id="c-37195661" checked=""/><div class="controls bullet"><span class="by">omgwtfbyobbq</span><span>|</span><a href="#37195407">root</a><span>|</span><a href="#37195600">parent</a><span>|</span><a href="#37195617">next</a><span>|</span><label class="collapse" for="c-37195661">[-]</label><label class="expand" for="c-37195661">[2 more]</label></div><br/><div class="children"><div class="content">How rock bottom?</div><br/><div id="37195892" class="c"><input type="checkbox" id="c-37195892" checked=""/><div class="controls bullet"><span class="by">wincy</span><span>|</span><a href="#37195407">root</a><span>|</span><a href="#37195661">parent</a><span>|</span><a href="#37195617">next</a><span>|</span><label class="collapse" for="c-37195892">[-]</label><label class="expand" for="c-37195892">[1 more]</label></div><br/><div class="children"><div class="content">They’re $650 on my local Facebook marketplace.</div><br/></div></div></div></div><div id="37195617" class="c"><input type="checkbox" id="c-37195617" checked=""/><div class="controls bullet"><span class="by">ufish235</span><span>|</span><a href="#37195407">root</a><span>|</span><a href="#37195600">parent</a><span>|</span><a href="#37195661">prev</a><span>|</span><a href="#37195662">next</a><span>|</span><label class="collapse" for="c-37195617">[-]</label><label class="expand" for="c-37195617">[1 more]</label></div><br/><div class="children"><div class="content">A hundred?</div><br/></div></div><div id="37195662" class="c"><input type="checkbox" id="c-37195662" checked=""/><div class="controls bullet"><span class="by">jquery</span><span>|</span><a href="#37195407">root</a><span>|</span><a href="#37195600">parent</a><span>|</span><a href="#37195617">prev</a><span>|</span><a href="#37195543">next</a><span>|</span><label class="collapse" for="c-37195662">[-]</label><label class="expand" for="c-37195662">[3 more]</label></div><br/><div class="children"><div class="content">Sheesh, and I thought I was going overboard with 2.</div><br/><div id="37195922" class="c"><input type="checkbox" id="c-37195922" checked=""/><div class="controls bullet"><span class="by">dannyw</span><span>|</span><a href="#37195407">root</a><span>|</span><a href="#37195662">parent</a><span>|</span><a href="#37195543">next</a><span>|</span><label class="collapse" for="c-37195922">[-]</label><label class="expand" for="c-37195922">[2 more]</label></div><br/><div class="children"><div class="content">Is it possible to link two and get 48GB VRAM and higher batch sizes?</div><br/><div id="37196010" class="c"><input type="checkbox" id="c-37196010" checked=""/><div class="controls bullet"><span class="by">berbec</span><span>|</span><a href="#37195407">root</a><span>|</span><a href="#37195922">parent</a><span>|</span><a href="#37195543">next</a><span>|</span><label class="collapse" for="c-37196010">[-]</label><label class="expand" for="c-37196010">[1 more]</label></div><br/><div class="children"><div class="content">The 3090 was the last card to get NVLink, which I believe allows you to do just that. None of the other 3000-series, nor any 4000-series, has it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37195543" class="c"><input type="checkbox" id="c-37195543" checked=""/><div class="controls bullet"><span class="by">cschmid</span><span>|</span><a href="#37195407">prev</a><span>|</span><a href="#37196650">next</a><span>|</span><label class="collapse" for="c-37195543">[-]</label><label class="expand" for="c-37195543">[5 more]</label></div><br/><div class="children"><div class="content">Can I also interpret this as: &#x27;AMD&#x27;s pytorch support is so abysmal that inference is 10x slower than it should be&#x27;?</div><br/><div id="37195770" class="c"><input type="checkbox" id="c-37195770" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#37195543">parent</a><span>|</span><a href="#37196650">next</a><span>|</span><label class="collapse" for="c-37195770">[-]</label><label class="expand" for="c-37195770">[4 more]</label></div><br/><div class="children"><div class="content">Should it not say PyTorch&#x27;s AMD support?</div><br/><div id="37195917" class="c"><input type="checkbox" id="c-37195917" checked=""/><div class="controls bullet"><span class="by">dannyw</span><span>|</span><a href="#37195543">root</a><span>|</span><a href="#37195770">parent</a><span>|</span><a href="#37196650">next</a><span>|</span><label class="collapse" for="c-37195917">[-]</label><label class="expand" for="c-37195917">[3 more]</label></div><br/><div class="children"><div class="content">It takes two to tango. AMD is always welcome to contribute patches.<p>You also have to keep in mind some latest gen AMD GPUs don’t even officially support ROCm on Linux. That’s absurd.<p>AMD has a choice to invest more staff into ML support, they’re choosing not to.</div><br/><div id="37196166" class="c"><input type="checkbox" id="c-37196166" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#37195543">root</a><span>|</span><a href="#37195917">parent</a><span>|</span><a href="#37196106">next</a><span>|</span><label class="collapse" for="c-37196166">[-]</label><label class="expand" for="c-37196166">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve harped on this on the past, the &quot;official&quot; hardware support list is tiny: <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;release&#x2F;gpu_os_support.html#linux-supported-gpus" rel="nofollow noreferrer">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;release&#x2F;gpu_os_support.h...</a><p>But, it&#x27;s worth noting there&#x27;s different levels of &quot;support.&quot; With ROCm 5.6, the 7900XT and 7900XTX RDNA3 cards, while not &quot;officially&quot; supported are gfx1100, which have rocBLAS&#x2F;MIOpen kernels and work w&#x2F;o jiggerpokery w&#x2F; PyTorch nightly and various HIPified inferencers I&#x27;ve tried like ExLlama.</div><br/></div></div><div id="37196106" class="c"><input type="checkbox" id="c-37196106" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#37195543">root</a><span>|</span><a href="#37195917">parent</a><span>|</span><a href="#37196166">prev</a><span>|</span><a href="#37196650">next</a><span>|</span><label class="collapse" for="c-37196106">[-]</label><label class="expand" for="c-37196106">[1 more]</label></div><br/><div class="children"><div class="content">From Geohotz&#x27;s investigation in the matter, it doesn&#x27;t look like it&#x27;s a manpower issue, it&#x27;s a quality&#x2F;culture issue. AMD&#x27;s firmware GPU division isn&#x27;t amazing.</div><br/></div></div></div></div></div></div></div></div><div id="37196650" class="c"><input type="checkbox" id="c-37196650" checked=""/><div class="controls bullet"><span class="by">laserbeam</span><span>|</span><a href="#37195543">prev</a><span>|</span><a href="#37195278">next</a><span>|</span><label class="collapse" for="c-37196650">[-]</label><label class="expand" for="c-37196650">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m actually curious if libraries like pytorch are even trying to move away from CUDA and if moving away from it is worth it. I get why newer ML toolchains would do that but do mainstream established MI frameeorks plan on sticking with nvidia exclusivity for now?</div><br/><div id="37197027" class="c"><input type="checkbox" id="c-37197027" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#37196650">parent</a><span>|</span><a href="#37195278">next</a><span>|</span><label class="collapse" for="c-37197027">[-]</label><label class="expand" for="c-37197027">[1 more]</label></div><br/><div class="children"><div class="content">Move to where?
AMD&#x27;s Rocm doesn&#x27;t even support their own cards.</div><br/></div></div></div></div><div id="37195278" class="c"><input type="checkbox" id="c-37195278" checked=""/><div class="controls bullet"><span class="by">lelandbatey</span><span>|</span><a href="#37196650">prev</a><span>|</span><a href="#37196551">next</a><span>|</span><label class="collapse" for="c-37195278">[-]</label><label class="expand" for="c-37195278">[1 more]</label></div><br/><div class="children"><div class="content">The comments point out that AMD in the table performing well required the use of Microsoft Olive, and someone in the article comments implies that if you use Microsoft Olive with Nvidia instead of Pytorch with Nvidia, then you&#x27;ll see the Nvidia jump in performance as well, largely rendering the supposed leap by AMD not relevant. Is that true? Can folks chime in?</div><br/></div></div><div id="37196551" class="c"><input type="checkbox" id="c-37196551" checked=""/><div class="controls bullet"><span class="by">tamrix</span><span>|</span><a href="#37195278">prev</a><span>|</span><a href="#37195259">next</a><span>|</span><label class="collapse" for="c-37196551">[-]</label><label class="expand" for="c-37196551">[1 more]</label></div><br/><div class="children"><div class="content">How does it compare to nvidias Jetson Orin?</div><br/></div></div><div id="37195259" class="c"><input type="checkbox" id="c-37195259" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37196551">prev</a><span>|</span><a href="#37195244">next</a><span>|</span><label class="collapse" for="c-37195259">[-]</label><label class="expand" for="c-37195259">[4 more]</label></div><br/><div class="children"><div class="content">Wait, why are they comparing Microsoft Olive on AMD to Pytorch on Nvidia? Nvidia supposedly shipped support for Olive recently, there should be no problem getting a head-to-head comparison: <a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;nvidia-geforce-driver-promises-doubled-stable-diffusion-performance" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;nvidia-geforce-driver-prom...</a><p>This is a very strange comparison.</div><br/><div id="37195947" class="c"><input type="checkbox" id="c-37195947" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37195259">parent</a><span>|</span><a href="#37195382">next</a><span>|</span><label class="collapse" for="c-37195947">[-]</label><label class="expand" for="c-37195947">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Nvidia supposedly shipped support for Olive recently<p>I mean, they announced it with a more than 2x speedup for SD in May:<p><a href="https:&#x2F;&#x2F;blogs.nvidia.com&#x2F;blog&#x2F;2023&#x2F;05&#x2F;23&#x2F;microsoft-build-nvidia-ai-windows-rtx&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blogs.nvidia.com&#x2F;blog&#x2F;2023&#x2F;05&#x2F;23&#x2F;microsoft-build-nvi...</a></div><br/></div></div><div id="37195382" class="c"><input type="checkbox" id="c-37195382" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#37195259">parent</a><span>|</span><a href="#37195947">prev</a><span>|</span><a href="#37195376">next</a><span>|</span><label class="collapse" for="c-37195382">[-]</label><label class="expand" for="c-37195382">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is Olive is a compressor, so comparing olive results to raw model is invalid.</div><br/></div></div><div id="37195376" class="c"><input type="checkbox" id="c-37195376" checked=""/><div class="controls bullet"><span class="by">asu_thomas</span><span>|</span><a href="#37195259">parent</a><span>|</span><a href="#37195382">prev</a><span>|</span><a href="#37195244">next</a><span>|</span><label class="collapse" for="c-37195376">[-]</label><label class="expand" for="c-37195376">[1 more]</label></div><br/><div class="children"><div class="content">A head-to-head comparison would render less ad views.</div><br/></div></div></div></div><div id="37195244" class="c"><input type="checkbox" id="c-37195244" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#37195259">prev</a><span>|</span><label class="collapse" for="c-37195244">[-]</label><label class="expand" for="c-37195244">[9 more]</label></div><br/><div class="children"><div class="content">No it doesn’t. AMD drivers don’t support all of the extensions, optimizations, and related in things like automatic1111. There’s always stuff that breaks on AMD and works perfectly in CUDA land.</div><br/><div id="37195360" class="c"><input type="checkbox" id="c-37195360" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#37195244">parent</a><span>|</span><a href="#37195318">next</a><span>|</span><label class="collapse" for="c-37195360">[-]</label><label class="expand" for="c-37195360">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, if they&#x27;re going to compare techniques breaking various features and extensions, I think it&#x27;s only fair to compare it to similarly incompatible techniques for nvidia.<p>In which case, I would recommend the AITemplate extension[0] for ComfyUI, which runing at a (hopefully) standardized 512x512 res, default euler A sampler, nets me about 23.8 it&#x2F;s on my 350w 3090.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;FizzleDorf&#x2F;AIT">https:&#x2F;&#x2F;github.com&#x2F;FizzleDorf&#x2F;AIT</a></div><br/></div></div><div id="37195318" class="c"><input type="checkbox" id="c-37195318" checked=""/><div class="controls bullet"><span class="by">FloatArtifact</span><span>|</span><a href="#37195244">parent</a><span>|</span><a href="#37195360">prev</a><span>|</span><a href="#37195971">next</a><span>|</span><label class="collapse" for="c-37195318">[-]</label><label class="expand" for="c-37195318">[6 more]</label></div><br/><div class="children"><div class="content">They explicitly use Automatic1111 to demonstrate those speeds...<p>Granted maybe not all extensions are compatible with DirectML in Automatic1111.</div><br/><div id="37195429" class="c"><input type="checkbox" id="c-37195429" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#37195244">root</a><span>|</span><a href="#37195318">parent</a><span>|</span><a href="#37195971">next</a><span>|</span><label class="collapse" for="c-37195429">[-]</label><label class="expand" for="c-37195429">[5 more]</label></div><br/><div class="children"><div class="content">I do think the original comment was wrong (if they tested in Automatic1111 then ... enough said about it working). However, they still have a point in warning people about AMD cards. There is more risk to these things than Nvidia:<p>- AMD&#x27;s support is flaky. The AMD 7900 XTX is only officially supported on Windows PCs (for ROCm). On Linux PCs in my experience there is a high risk of graphics card hard lockups.<p>- Historically AMD has dropped support for consumer cards really quickly (I don&#x27;t think they support any consumer cards from more than about 3 years ago). At some point that&#x27;ll stabilise, but it is hard to tell if this is the generation that will start working long term or the next.<p>- AMD generally gets second class support in return from the machine learning communities. There still seems to be some chance that you&#x27;ll be locked out of the latest and greatest if you go AMD.<p>At some point this will all probably come good and it&#x27;ll be like CPUs where there is really no difference between the vendors. There is too much interest in machine learning right now for anything else to happen ... but I&#x27;ve been buying AMD for more than a decade now and I&#x27;ve been thinking that for years already. If someone is on the edge they&#x27;d be better off not getting seduced by a minor performance improvement and sticking to Nvidia cards. It is uncertain when the long term will reach us. This year? 5 years? A decade? All options where AMDs software drivers are in play.<p>Although AMDs graphics drivers do seem great on Linux nowadays, I like things that just work. It is a pity they got so out of position vs CUDA.</div><br/><div id="37195657" class="c"><input type="checkbox" id="c-37195657" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#37195244">root</a><span>|</span><a href="#37195429">parent</a><span>|</span><a href="#37196536">next</a><span>|</span><label class="collapse" for="c-37195657">[-]</label><label class="expand" for="c-37195657">[3 more]</label></div><br/><div class="children"><div class="content">&gt; On Linux PCs in my experience there is a high risk of graphics card hard lockups.<p>What do you mean, does this become a crash &#x2F; kernel panic? Is this a common occurrence and is it due to bad kernel drivers?</div><br/><div id="37195848" class="c"><input type="checkbox" id="c-37195848" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#37195244">root</a><span>|</span><a href="#37195657">parent</a><span>|</span><a href="#37195906">next</a><span>|</span><label class="collapse" for="c-37195848">[-]</label><label class="expand" for="c-37195848">[1 more]</label></div><br/><div class="children"><div class="content">infamously, geohot encountered and documented a bunch of these kernel panics.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm&#x2F;issues&#x2F;2198">https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm&#x2F;issues&#x2F;2198</a><p><a href="https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-dive-into-amds-drivers.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;geohot.github.io&#x2F;blog&#x2F;jekyll&#x2F;update&#x2F;2023&#x2F;06&#x2F;07&#x2F;a-div...</a><p>they don&#x27;t occur in amdgpu-pro proprietary userland, but AMD apparently runs that on an internal release cycle and doesn&#x27;t make the current repo or build available to the public.  And AMD just historically hasn&#x27;t cared enough about the open userland to support that even in the officially-supported configurations.<p>I&#x27;ll also add to GP by saying that the windows support is all of about 2 weeks old at this point and it&#x27;s also the first time that consumer cards (as opposed to the CDNA compute cards and the workstation-branded Radeon Pro cards) have had official support.  Not that that is a guarantee it&#x27;ll actually run with AMD though, especially on the open-source userland.</div><br/></div></div><div id="37195906" class="c"><input type="checkbox" id="c-37195906" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#37195244">root</a><span>|</span><a href="#37195657">parent</a><span>|</span><a href="#37195848">prev</a><span>|</span><a href="#37196536">next</a><span>|</span><label class="collapse" for="c-37195906">[-]</label><label class="expand" for="c-37195906">[1 more]</label></div><br/><div class="children"><div class="content">It looks like a kernel panic to me, probably bad kernel drivers but I&#x27;ve never tried to figure out what is happening. My graphics card is more than 3 years old (ie, unsupported) so it isn&#x27;t worth reporting.</div><br/></div></div></div></div></div></div></div></div><div id="37195971" class="c"><input type="checkbox" id="c-37195971" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37195244">parent</a><span>|</span><a href="#37195318">prev</a><span>|</span><label class="collapse" for="c-37195971">[-]</label><label class="expand" for="c-37195971">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AMD drivers don’t support all of the extensions, optimizations, and related in things like automatic1111.<p>With olive in general, you need to rebuild all the models for it; and a whole lot of the extensions for A1111 are support for workflow components with new models (often, whole new classes of models). Unless Olive becomes a major platform for people <i>developing</i> models, this is always going to be lagging.</div><br/></div></div></div></div></div></div></div></div></div></body></html>