<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728378068696" as="style"/><link rel="stylesheet" href="styles.css?v=1728378068696"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/THUDM/LongWriter">Longwriter – Increase llama3.1 output to 10k words</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>taikon</span> | <span>22 comments</span></div><br/><div><div id="41767918" class="c"><input type="checkbox" id="c-41767918" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#41768274">next</a><span>|</span><label class="collapse" for="c-41767918">[-]</label><label class="expand" for="c-41767918">[5 more]</label></div><br/><div class="children"><div class="content">The sample output is interesting - it has highly suggestive chapter titles which read like pretty normal story beats. It seems like it&#x27;s guiding itself on these, then able to chunk out longer form writing per chapter.<p>For what it&#x27;s worth, the writing is .. bland. In the way that only an LLMs writing can be -- relatively grammatically sound, and totally soulless. I will never think of the love story of Elizabeth and Thomas again, despite having read the entire thing.<p>In early days of GPT-3, I experimented a lot with getting it respond <i>as</i> certain authors, and it was really quite excellent at that. This is one of the many things that seem likely to have been nerfed over time, I&#x27;d guess partly because human preference training just asks for bland responses, and partly because the injected prompts from OpenAI strongly discourage doing things related to real people, and those preferences are carried through, subtlely or not, into the augmented training data most open models tune on.</div><br/><div id="41770830" class="c"><input type="checkbox" id="c-41770830" checked=""/><div class="controls bullet"><span class="by">elfelf12</span><span>|</span><a href="#41767918">parent</a><span>|</span><a href="#41768274">next</a><span>|</span><label class="collapse" for="c-41770830">[-]</label><label class="expand" for="c-41770830">[4 more]</label></div><br/><div class="children"><div class="content">Is it a copyright problem or a capitalist problem or why do we only get nerfed dumb chatbots?<p>Would be interesting to really try hard and create a llm that can write novels in the style of an author. And skip the chat functionality!</div><br/><div id="41774860" class="c"><input type="checkbox" id="c-41774860" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#41767918">root</a><span>|</span><a href="#41770830">parent</a><span>|</span><a href="#41772810">next</a><span>|</span><label class="collapse" for="c-41774860">[-]</label><label class="expand" for="c-41774860">[1 more]</label></div><br/><div class="children"><div class="content">Copying writers is probably a copyright thing. But the experience with generative AI for images was that, at least for the early models, it was good to put things like &quot;masterpiece, highest quality&quot; in the prompt. The model biased towards average rather than trying to maintain a high standard. The more general problem here could easily be that people haven&#x27;t figured out how to prompt interesting writing from an LLM yet.<p>Although my personal theory would be that LLMs are just writing how someone without an ego or firsthand knowledge would write - it has a bunch of different angles it could take but has no particular reference to draw on to determine which is true. Great human writers are often cataloguing their extra-literary experiences. How is ChatGPT supposed to be inspired by a beautiful sunset to capture it in a way that has never been done before? It is capable of the writing part, but the inspiration part is a lot harder for it.</div><br/></div></div><div id="41772810" class="c"><input type="checkbox" id="c-41772810" checked=""/><div class="controls bullet"><span class="by">zobzu</span><span>|</span><a href="#41767918">root</a><span>|</span><a href="#41770830">parent</a><span>|</span><a href="#41774860">prev</a><span>|</span><a href="#41770937">next</a><span>|</span><label class="collapse" for="c-41772810">[-]</label><label class="expand" for="c-41772810">[1 more]</label></div><br/><div class="children"><div class="content">I believe this is neither. I believe this is purely a form of control - not to make money later or lose less money - rather, I believe many are very afraid of how people would use an un-nerfed LLM.<p>However, it&#x27;s inevitable.</div><br/></div></div><div id="41770937" class="c"><input type="checkbox" id="c-41770937" checked=""/><div class="controls bullet"><span class="by">sReinwald</span><span>|</span><a href="#41767918">root</a><span>|</span><a href="#41770830">parent</a><span>|</span><a href="#41772810">prev</a><span>|</span><a href="#41768274">next</a><span>|</span><label class="collapse" for="c-41770937">[-]</label><label class="expand" for="c-41770937">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps both. But I wonder if the incredible blandness of most chatbots is effectively just a regression towards the mean.<p>Most AI companies try to train their bots on vast amounts of different data, and I suspect it&#x27;s very difficult for that to result in very creative writing when you&#x27;re training on works of fiction, as well as cooking recipes, Reddit comments and technical documentation.</div><br/></div></div></div></div></div></div><div id="41768274" class="c"><input type="checkbox" id="c-41768274" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#41767918">prev</a><span>|</span><a href="#41767552">next</a><span>|</span><label class="collapse" for="c-41768274">[-]</label><label class="expand" for="c-41768274">[7 more]</label></div><br/><div class="children"><div class="content">What the difference between this and using chat history to concatenate outputs and prompting with something like “Now write the next section” repeatedly? I’ve done that with NotebookLM and it’ll write a complete fictional story based on sources, for example.</div><br/><div id="41768856" class="c"><input type="checkbox" id="c-41768856" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#41768274">parent</a><span>|</span><a href="#41774451">next</a><span>|</span><label class="collapse" for="c-41768856">[-]</label><label class="expand" for="c-41768856">[1 more]</label></div><br/><div class="children"><div class="content">In my testing, that often causes the model to &#x27;drift&#x27; and ramble wildly compared to just getting one long output from the very start.<p>The issue is probably that when you split it by just asking for the next section, you&#x27;re asking it to figure out how to continue from a block that wasn&#x27;t written with the awareness that it&#x27;d have to add on to it.<p>From the diagram on the repo, I guess this first plans out the structure for each block, and generates the blocks based on the plan.</div><br/></div></div><div id="41774451" class="c"><input type="checkbox" id="c-41774451" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#41768274">parent</a><span>|</span><a href="#41768856">prev</a><span>|</span><a href="#41768806">next</a><span>|</span><label class="collapse" for="c-41774451">[-]</label><label class="expand" for="c-41774451">[1 more]</label></div><br/><div class="children"><div class="content">My experience is that by default it&#x27;s like that game where you fold some paper, draw just up to the fold, pass the paper to the next player and they continue, and then you unfold it and see what came out: <a href="https:&#x2F;&#x2F;i.kym-cdn.com&#x2F;photos&#x2F;images&#x2F;facebook&#x2F;001&#x2F;782&#x2F;166&#x2F;960.jpg" rel="nofollow">https:&#x2F;&#x2F;i.kym-cdn.com&#x2F;photos&#x2F;images&#x2F;facebook&#x2F;001&#x2F;782&#x2F;166&#x2F;960...</a><p>With editing or lots of regeneration it can still work, of course.<p>(That said, I last tried this with the original ChatGPT…)</div><br/></div></div><div id="41768806" class="c"><input type="checkbox" id="c-41768806" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#41768274">parent</a><span>|</span><a href="#41774451">prev</a><span>|</span><a href="#41769504">next</a><span>|</span><label class="collapse" for="c-41768806">[-]</label><label class="expand" for="c-41768806">[2 more]</label></div><br/><div class="children"><div class="content">Most LLMs are trained to write &quot;complete&quot; outputs. So each section will end up being like a tiny self-contained short book. Without manual editing they will not create long narratives.</div><br/><div id="41772992" class="c"><input type="checkbox" id="c-41772992" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#41768274">root</a><span>|</span><a href="#41768806">parent</a><span>|</span><a href="#41769504">next</a><span>|</span><label class="collapse" for="c-41772992">[-]</label><label class="expand" for="c-41772992">[1 more]</label></div><br/><div class="children"><div class="content">The notion that an LLM will be used or can be used mostly for a one-shot request&#x2F;response has been one of the most idiosyncratic things about the first and second waves of this tech.<p>Like, not only can it not “make a complete app in 45 seconds,” I almost never even want that.</div><br/></div></div></div></div><div id="41769504" class="c"><input type="checkbox" id="c-41769504" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#41768274">parent</a><span>|</span><a href="#41768806">prev</a><span>|</span><a href="#41767552">next</a><span>|</span><label class="collapse" for="c-41769504">[-]</label><label class="expand" for="c-41769504">[2 more]</label></div><br/><div class="children"><div class="content">It would be the same if the model was &quot;raw&quot;, trained only on text completion.
But all models these days are RLHF&#x27;ed on (prompt, answer) pairs, so unfortunately they can get confused if the prompt already contains part of an answer.</div><br/><div id="41770773" class="c"><input type="checkbox" id="c-41770773" checked=""/><div class="controls bullet"><span class="by">elfelf12</span><span>|</span><a href="#41768274">root</a><span>|</span><a href="#41769504">parent</a><span>|</span><a href="#41767552">next</a><span>|</span><label class="collapse" for="c-41770773">[-]</label><label class="expand" for="c-41770773">[1 more]</label></div><br/><div class="children"><div class="content">I think base models are far superior to those boring instruct tuned models. I would rather have a good text completionist than a chat bot. But as far as i know i am in a minority there.</div><br/></div></div></div></div></div></div><div id="41767552" class="c"><input type="checkbox" id="c-41767552" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#41768274">prev</a><span>|</span><a href="#41772902">next</a><span>|</span><label class="collapse" for="c-41767552">[-]</label><label class="expand" for="c-41767552">[1 more]</label></div><br/><div class="children"><div class="content">Paper:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.07055" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.07055</a><p>The model is stock llama, fine tuned with a set of long documents to encourage longer outputs.<p>Most of the action seems to happen in an agent.</div><br/></div></div><div id="41772902" class="c"><input type="checkbox" id="c-41772902" checked=""/><div class="controls bullet"><span class="by">8bitsrule</span><span>|</span><a href="#41767552">prev</a><span>|</span><a href="#41767054">next</a><span>|</span><label class="collapse" for="c-41772902">[-]</label><label class="expand" for="c-41772902">[1 more]</label></div><br/><div class="children"><div class="content">Obviously there needs to be some oversight that limits distribution of machine-written articles &#x27;created&#x27; by people that aren&#x27;t already competent in the subject-area to know whether the content is trustworthy. Otherwise <i>a lot</i> of damage can be done in a very short time. (Pandora&#x27;s box and all that.)</div><br/></div></div><div id="41767054" class="c"><input type="checkbox" id="c-41767054" checked=""/><div class="controls bullet"><span class="by">danng87</span><span>|</span><a href="#41772902">prev</a><span>|</span><a href="#41768751">next</a><span>|</span><label class="collapse" for="c-41767054">[-]</label><label class="expand" for="c-41767054">[4 more]</label></div><br/><div class="children"><div class="content">Interesting project!<p>Does anyone know how LongWriter handles maintaining coherence and structure in longer outputs? Also, are there specific strategies or parameters recommended for fine-tuning LLaMA 3.1 with this setup to maximize the quality of generated text?</div><br/><div id="41767840" class="c"><input type="checkbox" id="c-41767840" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#41767054">parent</a><span>|</span><a href="#41768751">next</a><span>|</span><label class="collapse" for="c-41767840">[-]</label><label class="expand" for="c-41767840">[3 more]</label></div><br/><div class="children"><div class="content">How do people eval these very long outputs?<p>I&#x27;ve never figured that out (and no I can&#x27;t just... read all of them)</div><br/><div id="41769805" class="c"><input type="checkbox" id="c-41769805" checked=""/><div class="controls bullet"><span class="by">Multicomp</span><span>|</span><a href="#41767054">root</a><span>|</span><a href="#41767840">parent</a><span>|</span><a href="#41768751">next</a><span>|</span><label class="collapse" for="c-41769805">[-]</label><label class="expand" for="c-41769805">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know how to answer your question. But. I will say that I could see a future where one has a brainstormed setting &#x2F; plot outline &#x2F; concept and one could have the LLM output a first draft of whatever length, then make changes &#x2F; tweaks to the story &#x2F; copy over time.<p>The hardest part of writing for me is the first draft. Editing an existing copy to my own human artistic vision is much easier. No, this character doesn&#x27;t act like this, he acts like that.<p>Presuming you don&#x27;t have an allergic reaction to AI affected writing copy (even though the publishing houses are going to outsource their copyedits and style guide edits to LLMs, that is not hard to predict), an author could have the copy start with the souless and then hand edit until they like it from there.<p>Then it makes the copy go into hybrid world where AI was used to be a power tool, not the entire product. Copyright law may frustrate that for a time where if say over 5% of the final copy is AI-generated, it is ineligible for copyright protections, but otherwise, there will be stories and the best stories win.<p>1. Hand crafted on a fountain pen through all the edits, digitized to an opendoc (ok who are we kidding, .docx but I can dream for open file formats)<p>2. This story was started and is digital native through scrivener &#x2F; yWriter and eventually dumped to a .docx<p>3. This story started in an LLM chat response and edited muchly to match the artist&#x27;s human vision<p>All 3 stories will exist. and there will be a sea of slop that used (3) and then barely edited a thing, hoping to sell a book by SEO tag manipulation and an &#x27;eye-catching&#x27;&#x2F;lurid cover, just as there is now with (2) hastily thrown together rip-offs of others text.<p>But you can believe that I will be glad to go all Star Trek Holodeck on my idea concepts for books and tabletop campaigns.<p>Computer, give me a questline for a faction called the Silver Carders, there&#x27;s a catfolk named Marvin who is the adopted son of a human named Doug Alvaro and he is the old flame of the founder of the faction and there&#x27;s political intrigue that X Y and Z, please find a good mix-in for these 4-7 TV tropes links I like to play with, go.<p>Ok now swap out the absentminded professor gadgeteer with a cloud cuckoolander grandma mechanic.<p>Ok now find me a few entrypoints to this faction for my party characters who are currently A, B, and C.<p>Oh yeah, the max context this stuff will be useful for will be great.<p>Can I do that now with manual digital tools? Of course. But this lessens the activation energy&#x2F;boilerplate of typing this stuff up a lot.<p>Will long-term it make future generations unable to cope without the tool? Yes. Just like I cannot use a slide rule or do any geometry outside of my class, I have computer tools for that. LLMs will be a tool that after 20 years will be normalized enough.<p>Granted it will be odd when we have 3-book series come out covering a recent current events that captures the public&#x27;s imagination within weeks of the event, instead of the 3-years-later that usual entertainment media like books and movies take today.<p>Or odd when people can pay to have their own version of the story made, either inserting characters or &#x27;what if&#x27;ing the story where they can pay to alter a single plot point and see how the characters react and how that modifies the overall story.<p>We will all be more literarily conversant whether we want to or not, and I&#x27;m not sure whether I like that or I&#x27;m annoyed by it yet. Too soon to tell.</div><br/><div id="41771313" class="c"><input type="checkbox" id="c-41771313" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#41767054">root</a><span>|</span><a href="#41769805">parent</a><span>|</span><a href="#41768751">next</a><span>|</span><label class="collapse" for="c-41771313">[-]</label><label class="expand" for="c-41771313">[1 more]</label></div><br/><div class="children"><div class="content">I think some abstraction will need to occur, or it&#x27;s just too much information for us to ever take in and hold all at once... I think this goes past my problem of &quot;I can&#x27;t eval long outputs&quot; and your quest of pick-and-edit style. Code assistants are in the same boat right now too.<p>It looks like all these knowledge fields are converging into the same problem</div><br/></div></div></div></div></div></div></div></div><div id="41768751" class="c"><input type="checkbox" id="c-41768751" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#41767054">prev</a><span>|</span><a href="#41772612">next</a><span>|</span><label class="collapse" for="c-41768751">[-]</label><label class="expand" for="c-41768751">[1 more]</label></div><br/><div class="children"><div class="content">Really interesting. I wonder if you can do this on ollama too.</div><br/></div></div><div id="41772612" class="c"><input type="checkbox" id="c-41772612" checked=""/><div class="controls bullet"><span class="by">fitsumbelay</span><span>|</span><a href="#41768751">prev</a><span>|</span><a href="#41766974">next</a><span>|</span><label class="collapse" for="c-41772612">[-]</label><label class="expand" for="c-41772612">[1 more]</label></div><br/><div class="children"><div class="content">this looks cool. any plans to support 3.2?</div><br/></div></div><div id="41766974" class="c"><input type="checkbox" id="c-41766974" checked=""/><div class="controls bullet"><span class="by">alwinaugustin</span><span>|</span><a href="#41772612">prev</a><span>|</span><label class="collapse" for="c-41766974">[-]</label><label class="expand" for="c-41766974">[1 more]</label></div><br/><div class="children"><div class="content">How to use this with the local ollma setup</div><br/></div></div></div></div></div></div></div></body></html>