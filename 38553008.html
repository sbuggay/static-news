<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1701939658803" as="style"/><link rel="stylesheet" href="styles.css?v=1701939658803"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Tell HN: ZFS silent data corruption bugfix – my research results</a> </div><div class="subtext"><span>sandreas</span> | <span>37 comments</span></div><br/><div><div id="38553342" class="c"><input type="checkbox" id="c-38553342" checked=""/><div class="controls bullet"><span class="by">AndrewDavis</span><span>|</span><a href="#38554034">next</a><span>|</span><label class="collapse" for="c-38553342">[-]</label><label class="expand" for="c-38553342">[15 more]</label></div><br/><div class="children"><div class="content">This bug shouldn&#x27;t really scare people.  It&#x27;s requires such an incredibly specific workload to hit<p>Here&#x27;s a post by RobN (the dev who wrote the fix) on the ZFS On Linux mailing list<p>&gt; There&#x27;s a really important subtlety that a lot of people are missing in this. The bug is _not_ in reads. If you read data, its there. The bug is that sometimes, asking the filesystem &quot;is there data here?&quot; it says &quot;no&quot; when it should say &quot;yes&quot;. This distinction is important, because the vast majority of programs do not ask this - they just read.<p>&gt; Further, the answer only comes back &quot;no&quot; when it should be &quot;yes&quot; if there has been a write on that part of the file, where there was no data before (so overwriting data will not trip it), at the same moment from another thread, and at a time where the file is being synced out already, which means it had a change in the previous transaction and in this one.<p>&gt; And then, the gap you have to hit is in the tens of machine instructions.<p>&gt; This makes it very hard to suggest an actual probability, because this is a sequence and timing of events that basically doesn&#x27;t happen in real workloads, save for certain kinds of parallel build systems, which combine generated object files into a larger compiled program in very short amounts of time.<p>&gt; And even _then_, all this supposes that you do all this stuff, and don&#x27;t then use the destination file, because if you did, you would have noticed that its incomplete.<p>&gt; So while I would never say that no one has ever hit the problem unknowingly, I feel pretty confident that they haven&#x27;t. And if you&#x27;re not sure, ask yourself if you&#x27;ve ever had highly parallel workloads that involve writing and seeking the same files at the same moment.<p><a href="https:&#x2F;&#x2F;zfsonlinux.topicbox.com&#x2F;groups&#x2F;zfs-discuss&#x2F;Tcf27ae8f8cdd24ca-Md584bcc9d1f07edb6a6e042f" rel="nofollow noreferrer">https:&#x2F;&#x2F;zfsonlinux.topicbox.com&#x2F;groups&#x2F;zfs-discuss&#x2F;Tcf27ae8f...</a><p>Here&#x27;s another writeup by another ZFS dev Rincebrain  <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;rincebrain&#x2F;e23b4a39aba3fadc04db18574d30dc73" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;rincebrain&#x2F;e23b4a39aba3fadc04db18574...</a><p>I think the only reason this has gotten so much attention is because it came up as a block cloning bug (which it&#x27;s not) and that being a new feature created a massive scare that it&#x27;s widespread.  This isn&#x27;t the first or the last bug ZFS has had - it&#x27;s software.</div><br/><div id="38553426" class="c"><input type="checkbox" id="c-38553426" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#38553342">parent</a><span>|</span><a href="#38553479">next</a><span>|</span><label class="collapse" for="c-38553426">[-]</label><label class="expand" for="c-38553426">[13 more]</label></div><br/><div class="children"><div class="content">I feel like there has been kind of a weird concerted effort to push that zfs is bad due to this bug and how trust has been lost etcetera etcetera - super annoying when most other filesystems just corrupt your data and nobody will ever know it happened. I’ve experienced bad data corruption on xfs, btrfs, ext2, and ext4. So far zfs is been nothing but perfect.</div><br/><div id="38554044" class="c"><input type="checkbox" id="c-38554044" checked=""/><div class="controls bullet"><span class="by">3np</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553426">parent</a><span>|</span><a href="#38553506">next</a><span>|</span><label class="collapse" for="c-38554044">[-]</label><label class="expand" for="c-38554044">[2 more]</label></div><br/><div class="children"><div class="content">My experience has been aggravated by being simultaneously affected by <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;11893">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;issues&#x2F;11893</a>, which results in all zfs operations hanging (kill -9 is ineffective) until a power cycle. This round of upgrades was not fun.<p>Debian bookworm users: You need to enable backports for zfs. Both bugs are fixed in backports but still present in stable.<p>Let&#x27;s do away with the tribalism in both ways? Calling it perfect is quite a stretch. I&#x27;m not saying &quot;don&#x27;t use zfs&quot;, just &quot;you want to be prepared for the worst-case if your data matters&quot;. 3-2-1 and &quot;validate your backups&quot; still apply.</div><br/><div id="38554367" class="c"><input type="checkbox" id="c-38554367" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38554044">parent</a><span>|</span><a href="#38553506">next</a><span>|</span><label class="collapse" for="c-38554367">[-]</label><label class="expand" for="c-38554367">[1 more]</label></div><br/><div class="children"><div class="content">Perfect for me is not perfect for others of course. I have not run into the issue you linked, that sounds frustrating to deal with.<p>Backups is the only (near) 100% way to insure you data I agree.</div><br/></div></div></div></div><div id="38553506" class="c"><input type="checkbox" id="c-38553506" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553426">parent</a><span>|</span><a href="#38554044">prev</a><span>|</span><a href="#38554232">next</a><span>|</span><label class="collapse" for="c-38553506">[-]</label><label class="expand" for="c-38553506">[9 more]</label></div><br/><div class="children"><div class="content">The reason, and the difference, is that all these other filesystems have check and repair (and sometimes multiple) tools.<p>Please correct me, but ZFS has none.</div><br/><div id="38553769" class="c"><input type="checkbox" id="c-38553769" checked=""/><div class="controls bullet"><span class="by">chungy</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553506">parent</a><span>|</span><a href="#38553573">next</a><span>|</span><label class="collapse" for="c-38553769">[-]</label><label class="expand" for="c-38553769">[1 more]</label></div><br/><div class="children"><div class="content">What good would ZFS be without that?<p>The key differences are in two places:<p>1. Every administration task for ZFS is done online. You don&#x27;t need to take your pool offline just to check for errors and repair them (if possible). (Mind that on FreeBSD, UFS can usually have a fsck done in the background while the file system is in-use. Just about zero Linux-native file systems have this capability)<p>2. &quot;other filesytems&quot; can only hope to detect and repair inconsistencies in their metadata structures. If your files are corrupted, they can&#x27;t know and won&#x27;t tell you that they are. ZFS checksums <i>everything</i>, including regular file data. It will repair regular file data too, if possible.</div><br/></div></div><div id="38553573" class="c"><input type="checkbox" id="c-38553573" checked=""/><div class="controls bullet"><span class="by">gavinhoward</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553506">parent</a><span>|</span><a href="#38553769">prev</a><span>|</span><a href="#38553591">next</a><span>|</span><label class="collapse" for="c-38553573">[-]</label><label class="expand" for="c-38553573">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re completely wrong.<p>ZFS&#x27;s scrub is both a check and a repair tool. It&#x27;s already saved some of my data.</div><br/><div id="38554008" class="c"><input type="checkbox" id="c-38554008" checked=""/><div class="controls bullet"><span class="by">hdjdkdbdbe</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553573">parent</a><span>|</span><a href="#38553591">next</a><span>|</span><label class="collapse" for="c-38554008">[-]</label><label class="expand" for="c-38554008">[2 more]</label></div><br/><div class="children"><div class="content">You are partly right. Zfs scrub is a repair tool when it has parity &#x2F; mirrored copy of data to recreate it.</div><br/><div id="38554277" class="c"><input type="checkbox" id="c-38554277" checked=""/><div class="controls bullet"><span class="by">danparsonson</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38554008">parent</a><span>|</span><a href="#38553591">next</a><span>|</span><label class="collapse" for="c-38554277">[-]</label><label class="expand" for="c-38554277">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s safe to make the assumption that the tool isn&#x27;t magic.</div><br/></div></div></div></div></div></div><div id="38553591" class="c"><input type="checkbox" id="c-38553591" checked=""/><div class="controls bullet"><span class="by">nightfly</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553506">parent</a><span>|</span><a href="#38553573">prev</a><span>|</span><a href="#38553585">next</a><span>|</span><label class="collapse" for="c-38553591">[-]</label><label class="expand" for="c-38553591">[3 more]</label></div><br/><div class="children"><div class="content">Zfs has checksumming and scrubs, which can catch lots of data corruption that (most?) other filesystems can&#x27;t catch catch at all</div><br/><div id="38553786" class="c"><input type="checkbox" id="c-38553786" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553591">parent</a><span>|</span><a href="#38553585">next</a><span>|</span><label class="collapse" for="c-38553786">[-]</label><label class="expand" for="c-38553786">[2 more]</label></div><br/><div class="children"><div class="content">I guess I meant it as having a separate repair tool, but yes, you&#x27;re of course correct.<p>To me it was always weird to not have a separate tool for being able to do an offline ZFS repair.<p>With regards to data corruption, I mean, this is exactly why I moved to btrfs, because it&#x27;s able to catch bit rot same as ZFS.<p>I think ultimately this is why ZFS data corruption bugs always are such a big deal... It&#x27;s because in a lot of tech circles ZFS is put on this infallible pedestal where it can never do any harm to your data.</div><br/><div id="38553979" class="c"><input type="checkbox" id="c-38553979" checked=""/><div class="controls bullet"><span class="by">AndrewDavis</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553786">parent</a><span>|</span><a href="#38553585">next</a><span>|</span><label class="collapse" for="c-38553979">[-]</label><label class="expand" for="c-38553979">[1 more]</label></div><br/><div class="children"><div class="content">Could you please explain what you mean by a separate repair tool kit?</div><br/></div></div></div></div></div></div></div></div><div id="38554232" class="c"><input type="checkbox" id="c-38554232" checked=""/><div class="controls bullet"><span class="by">worthless-trash</span><span>|</span><a href="#38553342">root</a><span>|</span><a href="#38553426">parent</a><span>|</span><a href="#38553506">prev</a><span>|</span><a href="#38553479">next</a><span>|</span><label class="collapse" for="c-38554232">[-]</label><label class="expand" for="c-38554232">[1 more]</label></div><br/><div class="children"><div class="content">No, you&#x27;re just hearing about ZFS corruption like you&#x27;ve heard corruption in other filesystems, this wont be the first and last.<p>Its easy to feel like its being targeted with some kind of campaign, however the truth is rarely that exciting.  The software is being used by more people, which means it will expose more bugs.<p>No software is perfect, it just now starting to be abused enough to be important enough to be talked about.  This stage is entirely normal.</div><br/></div></div></div></div><div id="38553479" class="c"><input type="checkbox" id="c-38553479" checked=""/><div class="controls bullet"><span class="by">sandreas</span><span>|</span><a href="#38553342">parent</a><span>|</span><a href="#38553426">prev</a><span>|</span><a href="#38554034">next</a><span>|</span><label class="collapse" for="c-38553479">[-]</label><label class="expand" for="c-38553479">[1 more]</label></div><br/><div class="children"><div class="content">This is a great explanation, thank you.</div><br/></div></div></div></div><div id="38554034" class="c"><input type="checkbox" id="c-38554034" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#38553342">prev</a><span>|</span><a href="#38553187">next</a><span>|</span><label class="collapse" for="c-38554034">[-]</label><label class="expand" for="c-38554034">[1 more]</label></div><br/><div class="children"><div class="content">I hope you never look at the history of ext, or ntfs, or btrfs, or ufs, or xfs, or reiserfs, or… (stop me when you get the point).<p>One corner case data loss bug in 20 years?  Throw that baby out, the bath water is bad!</div><br/></div></div><div id="38553187" class="c"><input type="checkbox" id="c-38553187" checked=""/><div class="controls bullet"><span class="by">KyleSanderson</span><span>|</span><a href="#38554034">prev</a><span>|</span><a href="#38553337">next</a><span>|</span><label class="collapse" for="c-38553187">[-]</label><label class="expand" for="c-38553187">[11 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;15529#pullrequestreview-1732508242">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;15529#pullrequestreview-...</a><p>Honestly, ZFS is the best thing on the (Free)BSDs only... On Linux it doesn&#x27;t even use the page cache, and you conflict severely with L2ARC. I know there&#x27;s a variety of people who don&#x27;t care, but still for real users it&#x27;s not an actual option.</div><br/><div id="38553305" class="c"><input type="checkbox" id="c-38553305" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#38553187">parent</a><span>|</span><a href="#38553337">next</a><span>|</span><label class="collapse" for="c-38553305">[-]</label><label class="expand" for="c-38553305">[10 more]</label></div><br/><div class="children"><div class="content">For me it was the previous data corruption bug [1] that killed any enthusiasm for ZoL.  After that annoyances like the caching issues you mention and the constant kernel upgrades breaking DKMS on Fedora just stopped being worth it for me.<p>I finally moved to btrfs earlier this year, and so far I&#x27;m glad I did.<p>I run raid1 on my primary array, and raid5 on my off-site backup array at my mom&#x27;s apartment connecting with Tailscale.<p>Replication is with rsync and borg, not snapshots.<p>Yes, it&#x27;s more painful to replace disks after a failure, but once you get a bit used to it, it&#x27;s really no big deal.<p>On my main workstation&#x2F;laptop the dedupe and compression work much better than my experience with ZFS.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16797644">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16797644</a></div><br/><div id="38553489" class="c"><input type="checkbox" id="c-38553489" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553305">parent</a><span>|</span><a href="#38553473">next</a><span>|</span><label class="collapse" for="c-38553489">[-]</label><label class="expand" for="c-38553489">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Replication is with rsync and borg, not snapshots.<p>I backup my files with borg, but I still use snapshots during the Borg backup to ensure files are not modified during the process</div><br/><div id="38553529" class="c"><input type="checkbox" id="c-38553529" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553489">parent</a><span>|</span><a href="#38553473">next</a><span>|</span><label class="collapse" for="c-38553529">[-]</label><label class="expand" for="c-38553529">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, of course you should be doing that.  I was just trying to say that people shouldn&#x27;t pretend like having snapshot-based, filesystem-level replication alone (which can be a very efficient way of replicating data changes) is a good backup strategy.</div><br/></div></div></div></div><div id="38553473" class="c"><input type="checkbox" id="c-38553473" checked=""/><div class="controls bullet"><span class="by">sandreas</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553305">parent</a><span>|</span><a href="#38553489">prev</a><span>|</span><a href="#38553561">next</a><span>|</span><label class="collapse" for="c-38553473">[-]</label><label class="expand" for="c-38553473">[4 more]</label></div><br/><div class="children"><div class="content">Does btrfs support native encryption?</div><br/><div id="38553604" class="c"><input type="checkbox" id="c-38553604" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553473">parent</a><span>|</span><a href="#38553897">next</a><span>|</span><label class="collapse" for="c-38553604">[-]</label><label class="expand" for="c-38553604">[1 more]</label></div><br/><div class="children"><div class="content">Btrfs does not have native encryption.<p>I use LUKS &#x2F; dm-crypt on entire drive partitions.  Did that with ZFS too, and by the time ZFS got this feature I was already planning my migration to btrfs.<p>I like encrypting with LUKS, because I can have a drive configured with mine and my wife&#x27;s password.  Either one of us can take that drive, plug it in, and Gnome will put a nice graphical prompt asking for a password, then decrypt the drive and mount the filesystem.<p>If I get hit by a bus, LUKS makes it much easier for my family to get access to important data without having to have that data sit around somewhere in plaintext.</div><br/></div></div><div id="38553897" class="c"><input type="checkbox" id="c-38553897" checked=""/><div class="controls bullet"><span class="by">ikiris</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553473">parent</a><span>|</span><a href="#38553604">prev</a><span>|</span><a href="#38553561">next</a><span>|</span><label class="collapse" for="c-38553897">[-]</label><label class="expand" for="c-38553897">[2 more]</label></div><br/><div class="children"><div class="content">its also had <i>many</i> data loss &#x2F; corruption issues, and still looks to be scary for years to come.</div><br/><div id="38554011" class="c"><input type="checkbox" id="c-38554011" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553897">parent</a><span>|</span><a href="#38553561">next</a><span>|</span><label class="collapse" for="c-38554011">[-]</label><label class="expand" for="c-38554011">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never had data loss with either ZFS or btrfs.<p>For me on Fedora, uptime is better and maintenance overhead is lower on btrfs.</div><br/></div></div></div></div></div></div><div id="38553561" class="c"><input type="checkbox" id="c-38553561" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553305">parent</a><span>|</span><a href="#38553473">prev</a><span>|</span><a href="#38553337">next</a><span>|</span><label class="collapse" for="c-38553561">[-]</label><label class="expand" for="c-38553561">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t raid5 on raid5 perpetually broken and unsafe since inception?<p>Also a complicating factor with kernel upgrades is that while zfs release notes clearly delineate what kernel versions are supported that information doesn&#x27;t appear to be meaningfully encoded in package metadata so if you use new enough kernels compared to the version of zfs for your distro it is possible to front run support. For instance 2.2.2 supports up to 6.6 but you could very well for instance install 6.7 and it might not work.<p>The somewhat broken thing is not encoding known data like which kernel is supported to automatically do the right thing in the packaging system not the filesystem. The lazy fix is to just manually handle kernel updates. The lazier one is to grab the release notes and update if latest kernel is &lt;= supported.</div><br/><div id="38553688" class="c"><input type="checkbox" id="c-38553688" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553561">parent</a><span>|</span><a href="#38553337">next</a><span>|</span><label class="collapse" for="c-38553688">[-]</label><label class="expand" for="c-38553688">[2 more]</label></div><br/><div class="children"><div class="content">The packaging was a major reason for switching to btrfs.  I run sudo dnf upgrade and that&#x27;s it, my system is upgraded.  Zero issues ever.  With ZFS I had to pin to older kernel versions, not to mention a bunch of manual steps and cleanup after any major version upgrade (every 6-12 months).<p>Re btrfs raid 5&#x2F;6, yes everyone knows about this, and this is why I have it only on my backup system.  My primary data which holds 15TB of family photos and videos is on raid 1.  The offsite is there only for the time my house burns down.[1]<p>[1] I watched my neighbor&#x27;s house go up in flames 2 years ago, and it finally got me going on setting up remote backups.  The fire spread to 3 other houses, and everything happened very, very quickly.  No one got hurt, but multiple families got displaced for more than a year.  Besides having backups , it&#x27;s also a good reminder to have adequate insurance.  One neighbor did not.</div><br/><div id="38553939" class="c"><input type="checkbox" id="c-38553939" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#38553187">root</a><span>|</span><a href="#38553688">parent</a><span>|</span><a href="#38553337">next</a><span>|</span><label class="collapse" for="c-38553939">[-]</label><label class="expand" for="c-38553939">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if a metapackage that always depends on kernel &lt;= supported would resolve the issue by ensuring you don&#x27;t need to pin a specific version manually.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38553337" class="c"><input type="checkbox" id="c-38553337" checked=""/><div class="controls bullet"><span class="by">upon_drumhead</span><span>|</span><a href="#38553187">prev</a><span>|</span><a href="#38553404">next</a><span>|</span><label class="collapse" for="c-38553337">[-]</label><label class="expand" for="c-38553337">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;rincebrain&#x2F;e23b4a39aba3fadc04db18574d30dc73" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;rincebrain&#x2F;e23b4a39aba3fadc04db18574...</a> is a great writeup of the actual bug.</div><br/></div></div><div id="38553404" class="c"><input type="checkbox" id="c-38553404" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#38553337">prev</a><span>|</span><a href="#38553258">next</a><span>|</span><label class="collapse" for="c-38553404">[-]</label><label class="expand" for="c-38553404">[1 more]</label></div><br/><div class="children"><div class="content">See also the FreeBSD Errata Notice: <a href="https:&#x2F;&#x2F;www.freebsd.org&#x2F;security&#x2F;advisories&#x2F;FreeBSD-EN-23:16.openzfs.asc" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.freebsd.org&#x2F;security&#x2F;advisories&#x2F;FreeBSD-EN-23:16...</a></div><br/></div></div><div id="38553258" class="c"><input type="checkbox" id="c-38553258" checked=""/><div class="controls bullet"><span class="by">chungy</span><span>|</span><a href="#38553404">prev</a><span>|</span><a href="#38553344">next</a><span>|</span><label class="collapse" for="c-38553258">[-]</label><label class="expand" for="c-38553258">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  8. On linux, do a `sudo modinfo zfs | grep version` to see the version number<p>Even easier than that, type &quot;zfs version&quot;; it will report both the loaded kernel module and the userland version.</div><br/><div id="38553307" class="c"><input type="checkbox" id="c-38553307" checked=""/><div class="controls bullet"><span class="by">sandreas</span><span>|</span><a href="#38553258">parent</a><span>|</span><a href="#38553344">next</a><span>|</span><label class="collapse" for="c-38553307">[-]</label><label class="expand" for="c-38553307">[1 more]</label></div><br/><div class="children"><div class="content">thx, corrected :-)</div><br/></div></div></div></div><div id="38553344" class="c"><input type="checkbox" id="c-38553344" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38553258">prev</a><span>|</span><a href="#38553557">next</a><span>|</span><label class="collapse" for="c-38553344">[-]</label><label class="expand" for="c-38553344">[3 more]</label></div><br/><div class="children"><div class="content">Is there any good beginner friendly documentation for zfs? I&#x27;ve started using it as a testing&#x2F;learning NAS with a raspberry pi (cloning my google library to immich). It has not been a clearly easy process and errors are very much not clear. I recently extended a single drive to 2 and now I can&#x27;t import due to corrupted metadata and reports bad disk but smartctl reports all fine. Stack overflow is all over the place and reddit is... reddit. So is there a good goto place for these kinds of issues? I suspect this will be far from my last one.</div><br/><div id="38553852" class="c"><input type="checkbox" id="c-38553852" checked=""/><div class="controls bullet"><span class="by">denkmoon</span><span>|</span><a href="#38553344">parent</a><span>|</span><a href="#38553557">next</a><span>|</span><label class="collapse" for="c-38553852">[-]</label><label class="expand" for="c-38553852">[2 more]</label></div><br/><div class="children"><div class="content">Truenas forum. Level 1 Techs youtube channel. Lawrence Systems youtube channel.  the man pages of course.<p>First thing I&#x27;d suggest is really getting the terminology under your belt. Everything makes so much more sense when you use the correct terminology. For example, you don&#x27;t extend a drive. That doesn&#x27;t really make sense. I suspect what you mean is that you added a vdev to a zpool. Don&#x27;t think about things in terms of disks and extending them, think about things in terms of one or more disks making up a vdev, and a zpool is, well, a pool of those vdevs. Zfs then works out how best to write data across the vdevs in the pool. Not disks.</div><br/><div id="38553921" class="c"><input type="checkbox" id="c-38553921" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38553344">root</a><span>|</span><a href="#38553852">parent</a><span>|</span><a href="#38553557">next</a><span>|</span><label class="collapse" for="c-38553921">[-]</label><label class="expand" for="c-38553921">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. I&#x27;ll look into these. Seems like a rough entry, but I can totally get it being &quot;obvious&quot; after you get through. I&#x27;m a vim user so I can do this hahaha</div><br/></div></div></div></div></div></div><div id="38553557" class="c"><input type="checkbox" id="c-38553557" checked=""/><div class="controls bullet"><span class="by">mhio</span><span>|</span><a href="#38553344">prev</a><span>|</span><a href="#38553487">next</a><span>|</span><label class="collapse" for="c-38553557">[-]</label><label class="expand" for="c-38553557">[1 more]</label></div><br/><div class="children"><div class="content">What type of workload triggered the bug for you?</div><br/></div></div><div id="38553487" class="c"><input type="checkbox" id="c-38553487" checked=""/><div class="controls bullet"><span class="by">1letterunixname</span><span>|</span><a href="#38553557">prev</a><span>|</span><label class="collapse" for="c-38553487">[-]</label><label class="expand" for="c-38553487">[1 more]</label></div><br/><div class="children"><div class="content">I come from the GPFS, Lustre, and Panasas world of HPCC.<p>Personally, I used (past tense) ZoL in 2014-2017 on Ubuntu. The issue is that the array eventually entered an unrecoverable state where it could no longer be mounted RW. That wasn&#x27;t the end of the world, but the support from ZoL was to shrug at it. That was the end of that because without support and without pride, something that appears shiny is effectively useless.<p>Been running many XFS volumes over mdadm RAID10 near-2 arrays. Zero major problems in 5+ years with over 400 TiB online. SGI&#x27;s, Redhat&#x27;s, and more contributions to various Linux storage components are excellent.<p>My conclusion is that ZoL != Solaris ZFS. Once it left enterprise with controlled hardware and dedicated engineering &amp; support teams, it regressed and devolved. Beware of fanboys where passion and tribalism exceeds evidence and reliability.</div><br/></div></div></div></div></div></div></div></body></html>