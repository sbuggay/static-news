<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696928467233" as="style"/><link rel="stylesheet" href="styles.css?v=1696928467233"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://medium.com/@neum_ai/retrieval-augmented-generation-at-scale-building-a-distributed-system-for-synchronizing-and-eaa29162521">RAG at scale: Synchronizing and ingesting billions of text embeddings</a> <span class="domain">(<a href="https://medium.com">medium.com</a>)</span></div><div class="subtext"><span>picohen</span> | <span>45 comments</span></div><br/><div><div id="37826387" class="c"><input type="checkbox" id="c-37826387" checked=""/><div class="controls bullet"><span class="by">dluc</span><span>|</span><a href="#37827865">next</a><span>|</span><label class="collapse" for="c-37826387">[-]</label><label class="expand" for="c-37826387">[7 more]</label></div><br/><div class="children"><div class="content">We are also developing an open-source solution for those who would like to test it out and&#x2F;or contribute, it can be consumed as a web service, or embedded into .NET apps. The project is codenamed &quot;Semantic Memory&quot; (available in GitHub) and offers customizable external dependencies, such as using Azure Queues, RabbitMQ, or other alternatives, and options for Azure Cognitive Search, Qdrant (with plans to include Weaviate and more). The architecture is similar, with queues and pipelines.<p>We believe that enabling custom dependencies and logic, as well as the ability to add&#x2F;remove pipeline steps, is crucial. As of now, there is no definitive answer to the best chunk size or embedding model, so our project aims to provide the flexibility to inject and replace components and pipeline behavior.<p>Regarding Scalability, LLM text generators and GPUs remain a limiting factor also in this area, LLMs hold great potential for analyzing input data, and I believe the focus should be less on the speed of queues and storage and more on finding the optimal way to integrate LLMs into these pipelines.</div><br/><div id="37826799" class="c"><input type="checkbox" id="c-37826799" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37826387">parent</a><span>|</span><a href="#37828708">next</a><span>|</span><label class="collapse" for="c-37826799">[-]</label><label class="expand" for="c-37826799">[4 more]</label></div><br/><div class="children"><div class="content">The queues and storage are the foundation on which some of these other integrations can be built on top. Agree fully on the need for LLMs within the pipelines to help with data analysis.<p>Our current perspective has been on leveraging LLMs as part of async processes to help analyze data. This only really works when your data follows a template where I might be able to apply the analysis to a vast number of documents. Alternatively it becomes too expensive to do at a per document basis.<p>What types of analysis are you doing with LLMs? Have you started to integrate some of these into your existing solution?</div><br/><div id="37826876" class="c"><input type="checkbox" id="c-37826876" checked=""/><div class="controls bullet"><span class="by">dluc</span><span>|</span><a href="#37826387">root</a><span>|</span><a href="#37826799">parent</a><span>|</span><a href="#37828708">next</a><span>|</span><label class="collapse" for="c-37826876">[-]</label><label class="expand" for="c-37826876">[3 more]</label></div><br/><div class="children"><div class="content">Currently we use LLMs to generate a summary, used as an additional chunk. As you might guess, this can take time, so we postpone the summarization at the end (the current default pipeline is: extract, partition, gen embedding, save embeddings, summarize, gen embeddings (of the summary), save emb)<p>Initial tests though are showing that summaries are affecting the quality of answers, so we&#x27;ll probably remove it from the default flow and use it only for specific data types (e.g. chat logs).<p>There&#x27;s a bunch of synthetic data scenarios we want to leverage LLMs for. Without going too much into details, sometimes &quot;reading between the lines&quot;, and for some memory consolidation patterns (e.g. a &quot;dream phase&quot;), etc.</div><br/><div id="37826932" class="c"><input type="checkbox" id="c-37826932" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37826387">root</a><span>|</span><a href="#37826876">parent</a><span>|</span><a href="#37828708">next</a><span>|</span><label class="collapse" for="c-37826932">[-]</label><label class="expand" for="c-37826932">[2 more]</label></div><br/><div class="children"><div class="content">Makes sense. Interesting on the fact that summaries affect quality sometimes.<p>For synthetic data scenarios are you also thinking about synthetic queries over the data? (Try to predict which chunks might be more used than others)</div><br/><div id="37827977" class="c"><input type="checkbox" id="c-37827977" checked=""/><div class="controls bullet"><span class="by">dluc</span><span>|</span><a href="#37826387">root</a><span>|</span><a href="#37826932">parent</a><span>|</span><a href="#37828708">next</a><span>|</span><label class="collapse" for="c-37827977">[-]</label><label class="expand" for="c-37827977">[1 more]</label></div><br/><div class="children"><div class="content">yes, queries and also planning.<p>For instance, given the user &quot;ask&quot; (which could be any generic message in a copilot), decide how to query one or multiple storages. Ultimately, companies and users have different storages, and a few can be indexed with vectors (and additional fine tuned models). But there&#x27;s a lot of &quot;legacy&quot; structured data accessible only with SQL and similar languages, so a &quot;planner&quot; (in the SK sense of planners) could be useful to query vector indexes, text indexes and knowledge graphs, combining the result.</div><br/></div></div></div></div></div></div></div></div><div id="37828708" class="c"><input type="checkbox" id="c-37828708" checked=""/><div class="controls bullet"><span class="by">bradneuberg</span><span>|</span><a href="#37826387">parent</a><span>|</span><a href="#37826799">prev</a><span>|</span><a href="#37828054">next</a><span>|</span><label class="collapse" for="c-37828708">[-]</label><label class="expand" for="c-37828708">[1 more]</label></div><br/><div class="children"><div class="content">Really interesting library.<p>Is anyone aware of something similar but hooked into Google Cloud infra instead of Azure?</div><br/></div></div><div id="37828054" class="c"><input type="checkbox" id="c-37828054" checked=""/><div class="controls bullet"><span class="by">CharlieDigital</span><span>|</span><a href="#37826387">parent</a><span>|</span><a href="#37828708">prev</a><span>|</span><a href="#37827865">next</a><span>|</span><label class="collapse" for="c-37828054">[-]</label><label class="expand" for="c-37828054">[1 more]</label></div><br/><div class="children"><div class="content">Why .NET apps specifically?</div><br/></div></div></div></div><div id="37827865" class="c"><input type="checkbox" id="c-37827865" checked=""/><div class="controls bullet"><span class="by">dkatz23238</span><span>|</span><a href="#37826387">prev</a><span>|</span><a href="#37825731">next</a><span>|</span><label class="collapse" for="c-37827865">[-]</label><label class="expand" for="c-37827865">[1 more]</label></div><br/><div class="children"><div class="content">What statistics&#x2F;metrics are used to evaluate RAG systems? Is there any paper that systematically compares different RAG methods (chunkings, models, ect)? I would assume that such metric would be similar to something used for evaluating summarization or question and answering but I am curious to know if there are specific methods&#x2F;metrics used to evaluate RAG systems.</div><br/></div></div><div id="37825731" class="c"><input type="checkbox" id="c-37825731" checked=""/><div class="controls bullet"><span class="by">juxtaposicion</span><span>|</span><a href="#37827865">prev</a><span>|</span><a href="#37825321">next</a><span>|</span><label class="collapse" for="c-37825731">[-]</label><label class="expand" for="c-37825731">[9 more]</label></div><br/><div class="children"><div class="content">We’re also building billion-scale pipeline for indexing embeddings. Like the author, most of our pain has been scaling. If you only had to do millions, this whole pipeline would be a 100 LoC. but billions? Our system is at 20k LoC and growing.<p>The biggest surprise to me here is using Weavite at the scale of billions — my understanding was that this would require tremendous memory requirements (of order a TB in RAM) which are prohibitively expensive (10-50k&#x2F;m for that much memory).<p>Instead, we’ve been using Lance, which stores its vector index on disk instead of in memory.</div><br/><div id="37826043" class="c"><input type="checkbox" id="c-37826043" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37825731">parent</a><span>|</span><a href="#37826455">next</a><span>|</span><label class="collapse" for="c-37826043">[-]</label><label class="expand" for="c-37826043">[2 more]</label></div><br/><div class="children"><div class="content">Co-author of article here.<p>Yeah a ton of the time and effort has gone into building robustness and observability into the process. When dealing with millions of files, a failure half way through it is imperative to be able to recover.<p>RE: Weaviate: Yeah, we needed to use large amounts of memory with Weaviate which has been a drawback from a cost perspective, but that from a performance perspective delivers on the requirements of our customers. (on Weaviate we explored using product quantization. )<p>What type of performance have you gotten with Lance both on ingestion and retieval? Is disk retrieval fast enough?</div><br/><div id="37826925" class="c"><input type="checkbox" id="c-37826925" checked=""/><div class="controls bullet"><span class="by">juxtaposicion</span><span>|</span><a href="#37825731">root</a><span>|</span><a href="#37826043">parent</a><span>|</span><a href="#37826455">next</a><span>|</span><label class="collapse" for="c-37826925">[-]</label><label class="expand" for="c-37826925">[1 more]</label></div><br/><div class="children"><div class="content">Disk retrieval is definitely slower. In-memory retrieval typically can be ~1ms or less, whereas disk retrieval on a fast network drive is 50-100ms. But frankly, for any use case I can think of 50ms of latency is good enough. The best part is that the cost is driven by <i>disk</i> not <i>ram</i>, which means instead of $50k&#x2F;month for ~TB of RAM you&#x27;re talking about $1k&#x2F;mo for a fast NVMe on a fast link. That&#x27;s 50x cheaper, because disks are 50x cheaper. $50k&#x2F;mo for an extra 50ms latency is a pretty clear easy tradeoff.</div><br/></div></div></div></div><div id="37826455" class="c"><input type="checkbox" id="c-37826455" checked=""/><div class="controls bullet"><span class="by">bryan0</span><span>|</span><a href="#37825731">parent</a><span>|</span><a href="#37826043">prev</a><span>|</span><a href="#37827383">next</a><span>|</span><label class="collapse" for="c-37826455">[-]</label><label class="expand" for="c-37826455">[5 more]</label></div><br/><div class="children"><div class="content">we&#x27;ve been using pgvector at the 100M scale without any major problems so far, but I guess it depends on your specific use case. we&#x27;ve also been using elastic search dense vector fields which also seems to scale well, but of course its pricey but we already have it in our infra so works well.</div><br/><div id="37826744" class="c"><input type="checkbox" id="c-37826744" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37825731">root</a><span>|</span><a href="#37826455">parent</a><span>|</span><a href="#37827625">next</a><span>|</span><label class="collapse" for="c-37826744">[-]</label><label class="expand" for="c-37826744">[3 more]</label></div><br/><div class="children"><div class="content">What type of latency requirements are you dealing with? (i.e. look up time, ingestion time)<p>Were you using postgres already or migrated data into it?</div><br/><div id="37826963" class="c"><input type="checkbox" id="c-37826963" checked=""/><div class="controls bullet"><span class="by">juxtaposicion</span><span>|</span><a href="#37825731">root</a><span>|</span><a href="#37826744">parent</a><span>|</span><a href="#37827625">next</a><span>|</span><label class="collapse" for="c-37826963">[-]</label><label class="expand" for="c-37826963">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to know the answer here too!<p>I&#x27;ve ran a few tests on pg and retrieving 100 random indices from a billion-scale table -- without vectors, just a vanilla table with an int64 primary key -- easily took 700ms on beefy GCP instances. And that was without a vector index.<p>Entirely possibly my take was too cursory, would love to know what latencies you&#x27;re getting bryan0!</div><br/><div id="37827176" class="c"><input type="checkbox" id="c-37827176" checked=""/><div class="controls bullet"><span class="by">losteric</span><span>|</span><a href="#37825731">root</a><span>|</span><a href="#37826963">parent</a><span>|</span><a href="#37827625">next</a><span>|</span><label class="collapse" for="c-37827176">[-]</label><label class="expand" for="c-37827176">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  100 random indices from a billion-scale table -- without vectors, just a vanilla table with an int64 primary key -- easily took 700ms on beefy GCP instances.<p>Is there a write up of the analysis? Something seems very wrong with that taking 700ms</div><br/></div></div></div></div></div></div><div id="37827625" class="c"><input type="checkbox" id="c-37827625" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#37825731">root</a><span>|</span><a href="#37826455">parent</a><span>|</span><a href="#37826744">prev</a><span>|</span><a href="#37827383">next</a><span>|</span><label class="collapse" for="c-37827625">[-]</label><label class="expand" for="c-37827625">[1 more]</label></div><br/><div class="children"><div class="content">What size are your embeddings?</div><br/></div></div></div></div><div id="37827383" class="c"><input type="checkbox" id="c-37827383" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#37825731">parent</a><span>|</span><a href="#37826455">prev</a><span>|</span><a href="#37825321">next</a><span>|</span><label class="collapse" for="c-37827383">[-]</label><label class="expand" for="c-37827383">[1 more]</label></div><br/><div class="children"><div class="content">What kind of retrieval performance are you observing with Lance?</div><br/></div></div></div></div><div id="37825321" class="c"><input type="checkbox" id="c-37825321" checked=""/><div class="controls bullet"><span class="by">joewferrara</span><span>|</span><a href="#37825731">prev</a><span>|</span><a href="#37829524">next</a><span>|</span><label class="collapse" for="c-37825321">[-]</label><label class="expand" for="c-37825321">[7 more]</label></div><br/><div class="children"><div class="content">This is a great article about the technical difficulties of building a RAG system at scale from an engineering perspective. Performance is about speed and compute. A topic that is not addressed is how to evaluate a RAG system where performance is about whether the RAG system is retrieving the correct context and answering questions accurately. A RAG system should be built so that the different parts (retriever, embedder, etc) can easily be taken out and modified to improve the performance of the RAG system at answering questions accurately. Whether a RAG system is answering questions accurately should be assessed during development and then continuously monitored.</div><br/><div id="37825498" class="c"><input type="checkbox" id="c-37825498" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37825321">parent</a><span>|</span><a href="#37825469">next</a><span>|</span><label class="collapse" for="c-37825498">[-]</label><label class="expand" for="c-37825498">[5 more]</label></div><br/><div class="children"><div class="content">Co-author of the article here.<p>You are right. Retrieval accuracy is important as well. From an accuracy perspective, any tools you have found useful in helping validate retrieval accuracy?<p>In our current architecture, all the different pieces within the RAG ingestion pipeline are modifiable to be able to improve loading, chunking and embedding.<p>As part of our development process, we have started to enable other tools that we don&#x27;t talk as much in the article about including a pre processing and embeddings playground (<a href="https:&#x2F;&#x2F;www.neum.ai&#x2F;post&#x2F;pre-processing-playground">https:&#x2F;&#x2F;www.neum.ai&#x2F;post&#x2F;pre-processing-playground</a>) to be able to test different combinations of modules against a piece of text. The idea being that you can establish you ideal pipeline &#x2F; transformations that can then be scaled.</div><br/><div id="37825975" class="c"><input type="checkbox" id="c-37825975" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37825321">root</a><span>|</span><a href="#37825498">parent</a><span>|</span><a href="#37827444">next</a><span>|</span><label class="collapse" for="c-37825975">[-]</label><label class="expand" for="c-37825975">[3 more]</label></div><br/><div class="children"><div class="content">Did you consider pre-processing each chunk separately to generate useful information - summary, title, topics - that would enrich embeddings and aid retrieval? Embeddings only capture surface form. &quot;Third letter of second word&quot; won&#x27;t match embedding for letter &quot;t&quot;. Info has surface and depth. We get depth through chain-of-thought, but that requires first digesting raw text with an LLM.<p>Even LLMs are dumb during training but smart during inference. So to make more useful training examples, we need to first &quot;study&quot; them with a model, making the implicit explicit, before training. This allows training to benefit from inference-stage smarts.<p>Hopefully we avoid cases where &quot;A is B&quot; fails to recall &quot;B is A&quot; (the reversal curse). The reversal should be predicted during &quot;study&quot; and get added to the training set, reducing fragmentation.  Fragmented data in the dataset remains fragmented in the trained model. I believe many of the problems of RAG are related to data fragmentation and superficial presentation.<p>A RAG system should have an ingestion LLM step for retrieval augmentation and probably hierarchical summarisation up to a decent level. It will be adding insight into the system by processing the raw documents into a more useful form.</div><br/><div id="37826720" class="c"><input type="checkbox" id="c-37826720" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37825321">root</a><span>|</span><a href="#37825975">parent</a><span>|</span><a href="#37826366">next</a><span>|</span><label class="collapse" for="c-37826720">[-]</label><label class="expand" for="c-37826720">[1 more]</label></div><br/><div class="children"><div class="content">Not at scale. Currently we do some extraction for metadata, but pretty simple. Doing LLM based pre-processing of each chunk like this can be quite expensive especially with billions of them. Summarizing each document before ingestion could cost thousands of dollars when you have billions.<p>We have been experimenting with semantic chunking (<a href="https:&#x2F;&#x2F;www.neum.ai&#x2F;post&#x2F;contextually-splitting-documents">https:&#x2F;&#x2F;www.neum.ai&#x2F;post&#x2F;contextually-splitting-documents</a>) and semantic selectors (<a href="https:&#x2F;&#x2F;www.neum.ai&#x2F;post&#x2F;semantic-selectors-for-structured-data">https:&#x2F;&#x2F;www.neum.ai&#x2F;post&#x2F;semantic-selectors-for-structured-d...</a>) but from a scale perspective. For example, if we have 1 millions docs, but we know they are generally similar in format &#x2F; template, then we can bypass having to use an LLM to analyze them one by one and simply help create scripts to extract the right info.<p>We think there are clever approaches like this that can help improve RAG while still being scalable.</div><br/></div></div><div id="37826366" class="c"><input type="checkbox" id="c-37826366" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#37825321">root</a><span>|</span><a href="#37825975">parent</a><span>|</span><a href="#37826720">prev</a><span>|</span><a href="#37827444">next</a><span>|</span><label class="collapse" for="c-37826366">[-]</label><label class="expand" for="c-37826366">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any more resources on this topic? I’m currently very interested in scaling and verifying RAG systems.</div><br/></div></div></div></div><div id="37827444" class="c"><input type="checkbox" id="c-37827444" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#37825321">root</a><span>|</span><a href="#37825498">parent</a><span>|</span><a href="#37825975">prev</a><span>|</span><a href="#37825469">next</a><span>|</span><label class="collapse" for="c-37827444">[-]</label><label class="expand" for="c-37827444">[1 more]</label></div><br/><div class="children"><div class="content">&gt; From an accuracy perspective, any tools you have found useful in helping validate retrieval accuracy?<p>You’ll probably want to start with the standard rank-based metrics like MRR, nDCG, and precision&#x2F;recall@K.<p>Plus if you’re going to spend $$$ embedding tons of docs you’ll want to compare to a “dumb” baseline like bm25.</div><br/></div></div></div></div><div id="37825469" class="c"><input type="checkbox" id="c-37825469" checked=""/><div class="controls bullet"><span class="by">ac2u</span><span>|</span><a href="#37825321">parent</a><span>|</span><a href="#37825498">prev</a><span>|</span><a href="#37829524">next</a><span>|</span><label class="collapse" for="c-37825469">[-]</label><label class="expand" for="c-37825469">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, especially if you&#x27;re experimenting with training and applying a matrix to the embeddings generated by an off the shelf model to help it surface subtleties unique to your domain.</div><br/></div></div></div></div><div id="37829524" class="c"><input type="checkbox" id="c-37829524" checked=""/><div class="controls bullet"><span class="by">bluelightning2k</span><span>|</span><a href="#37825321">prev</a><span>|</span><a href="#37827449">next</a><span>|</span><label class="collapse" for="c-37829524">[-]</label><label class="expand" for="c-37829524">[3 more]</label></div><br/><div class="children"><div class="content">Good article BUT I can&#x27;t fathom that people would use a managed service to generate and store embeddings.<p>The openAI or replicate embeddings APIs are already a managed service... You would still need to self managing it all just into a different API.<p>And dealing with embeddings is the kind of fun work every engineer wants to do anyway.<p>Still a good article but very perplexing how the company can exist</div><br/><div id="37829615" class="c"><input type="checkbox" id="c-37829615" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37829524">parent</a><span>|</span><a href="#37829618">next</a><span>|</span><label class="collapse" for="c-37829615">[-]</label><label class="expand" for="c-37829615">[1 more]</label></div><br/><div class="children"><div class="content">Some engineers find it fun, other might not. Same as everything.<p>IMO the fun parts are actually prototyping and figuring out the right pattern I want to use for my solution. Once you have done that, scaling and dealing with robustness tends to be a bit less fun.</div><br/></div></div><div id="37829618" class="c"><input type="checkbox" id="c-37829618" checked=""/><div class="controls bullet"><span class="by">raverbashing</span><span>|</span><a href="#37829524">parent</a><span>|</span><a href="#37829615">prev</a><span>|</span><a href="#37827449">next</a><span>|</span><label class="collapse" for="c-37829618">[-]</label><label class="expand" for="c-37829618">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like the same people who use langchain&#x27;s &quot;Prompt replacement&quot; methods instead of, you know, just use string formatting<p><a href="https:&#x2F;&#x2F;python.langchain.com&#x2F;docs&#x2F;modules&#x2F;model_io&#x2F;prompts&#x2F;prompt_templates&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;python.langchain.com&#x2F;docs&#x2F;modules&#x2F;model_io&#x2F;prompts&#x2F;p...</a></div><br/></div></div></div></div><div id="37827449" class="c"><input type="checkbox" id="c-37827449" checked=""/><div class="controls bullet"><span class="by">typest</span><span>|</span><a href="#37829524">prev</a><span>|</span><a href="#37829373">next</a><span>|</span><label class="collapse" for="c-37827449">[-]</label><label class="expand" for="c-37827449">[6 more]</label></div><br/><div class="children"><div class="content">It seems to me that RAG is really search, and search is generally a hard problem without an easy one size fits all solution. E.g., as people push retrieval further and further in the context of LLM generation, they&#x27;re going to go further down the rabbit hole of how to build a good search system.<p>Is everyone currently reinventing search from first principles?</div><br/><div id="37829519" class="c"><input type="checkbox" id="c-37829519" checked=""/><div class="controls bullet"><span class="by">zby</span><span>|</span><a href="#37827449">parent</a><span>|</span><a href="#37828701">next</a><span>|</span><label class="collapse" for="c-37829519">[-]</label><label class="expand" for="c-37829519">[1 more]</label></div><br/><div class="children"><div class="content">I am convinced that we should teach the LLMs to use search as a tool instead of creating special search that is useful for LLMs. We now have a lot of search systems and LLMs can in theory use all kind of text interface, the only problem is with the limited context that LLMs can consume. But is is quite orthogonal to what kind of index we use for the search. In fact for humans it is also be useful that search returns limited chunks - we already have that with the &#x27;snippets&#x27; that for example Google shows - we just need it to tweak a bit for them to be maybe two kind of snippets - shorter as they are now and longer.<p>You can use LLMs to do semantic search using a keyword search - by telling the LLM to come up with a good search term that would include all the synonymes. But if vector search in embeddings really gives better results than keyword search - then we should start using it in all the other search tools used by humans.<p>LLMs are the more general tool - so adjusting them to the more restricted search technology should be easier and quicker to do instead of doing it the other way around.<p>By the way - this prompted me to create my Opinionated RAG wiki: <a href="https:&#x2F;&#x2F;github.com&#x2F;zby&#x2F;answerbot&#x2F;wiki">https:&#x2F;&#x2F;github.com&#x2F;zby&#x2F;answerbot&#x2F;wiki</a></div><br/></div></div><div id="37828701" class="c"><input type="checkbox" id="c-37828701" checked=""/><div class="controls bullet"><span class="by">isaacfung</span><span>|</span><a href="#37827449">parent</a><span>|</span><a href="#37829519">prev</a><span>|</span><a href="#37827742">next</a><span>|</span><label class="collapse" for="c-37828701">[-]</label><label class="expand" for="c-37828701">[1 more]</label></div><br/><div class="children"><div class="content">Depends on what you mean by search. Do you consider all Question Answering as search?<p>Some questions require multi-hop reasoning or have to be decomposed into simpler subproblems. When you google a question, often the answer is not trivially included in the retrieved text and you have to process(filter irrelevant information, resolve conflicting information, extrapolate to cases not covered, align the same entities referred to with two different names, etc), forumate an answer for the original question and maybe even predict your intent based on your history to personalize the result or customize the result in the format you like(markdown, json, csv, etc).<p>Researchers have developed many different techniques to solve the related problems. But as LLMs are getting hyped, many people try to tell you LLM+vector store is all you need.</div><br/></div></div><div id="37827742" class="c"><input type="checkbox" id="c-37827742" checked=""/><div class="controls bullet"><span class="by">mrfox321</span><span>|</span><a href="#37827449">parent</a><span>|</span><a href="#37828701">prev</a><span>|</span><a href="#37827930">next</a><span>|</span><label class="collapse" for="c-37827742">[-]</label><label class="expand" for="c-37827742">[1 more]</label></div><br/><div class="children"><div class="content">Your intuition on search being implemented is correct.<p>It&#x27;s still TBD on whether these new generations of language models will democratize search on bespoke corpuses.<p>There&#x27;s going to be a lot of arbitrary alchemy and tribal knowledge...</div><br/></div></div><div id="37827930" class="c"><input type="checkbox" id="c-37827930" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37827449">parent</a><span>|</span><a href="#37827742">prev</a><span>|</span><a href="#37829373">next</a><span>|</span><label class="collapse" for="c-37827930">[-]</label><label class="expand" for="c-37827930">[2 more]</label></div><br/><div class="children"><div class="content">To some degree. The amount of data that will be brought into search solutions will be enormous, seems like a good time to try to reimagine what that process might look like</div><br/><div id="37828046" class="c"><input type="checkbox" id="c-37828046" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#37827449">root</a><span>|</span><a href="#37827930">parent</a><span>|</span><a href="#37829373">next</a><span>|</span><label class="collapse" for="c-37828046">[-]</label><label class="expand" for="c-37828046">[1 more]</label></div><br/><div class="children"><div class="content">Also this is search for LLM not for humans so optimal solution will be different. Or even with models it is not that hard to imagine that Mistral-8b will need different results than GPT4 which has 1.76 trillion parameters.</div><br/></div></div></div></div></div></div><div id="37829373" class="c"><input type="checkbox" id="c-37829373" checked=""/><div class="controls bullet"><span class="by">joelthelion</span><span>|</span><a href="#37827449">prev</a><span>|</span><a href="#37828407">next</a><span>|</span><label class="collapse" for="c-37829373">[-]</label><label class="expand" for="c-37829373">[1 more]</label></div><br/><div class="children"><div class="content">Can anyone who has used such systems for some time comment on their usefulness? Is it something you can&#x27;t live with, a nice to have, or something you tend to forget is available after a while?</div><br/></div></div><div id="37828407" class="c"><input type="checkbox" id="c-37828407" checked=""/><div class="controls bullet"><span class="by">vimota</span><span>|</span><a href="#37829373">prev</a><span>|</span><a href="#37827129">next</a><span>|</span><label class="collapse" for="c-37828407">[-]</label><label class="expand" for="c-37828407">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for writing this up! I&#x27;m working on a very similar service (<a href="https:&#x2F;&#x2F;embeddingsync.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;embeddingsync.com&#x2F;</a>) and I implemented almost the same as you&#x27;ve described here, but using a poll-based stateful workflow model instead of queueing.<p>The biggest challenge - which I haven&#x27;t solved as seamlessly as I&#x27;d like - is supporting updates &#x2F; deletes in the source. You don&#x27;t seem to discuss it in this post, does Neum handle that?</div><br/><div id="37829598" class="c"><input type="checkbox" id="c-37829598" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37828407">parent</a><span>|</span><a href="#37827129">next</a><span>|</span><label class="collapse" for="c-37829598">[-]</label><label class="expand" for="c-37829598">[1 more]</label></div><br/><div class="children"><div class="content">Co-author of the article here.<p>We do support updates for some sources. Deletes not yet. For some sources we do polling which is then dumped on the queues. For other we have listeners that subscribe to changes.<p>What are the challenges you are facing in supporting this?</div><br/></div></div></div></div><div id="37827129" class="c"><input type="checkbox" id="c-37827129" checked=""/><div class="controls bullet"><span class="by">wanderingmind</span><span>|</span><a href="#37828407">prev</a><span>|</span><a href="#37828506">next</a><span>|</span><label class="collapse" for="c-37827129">[-]</label><label class="expand" for="c-37827129">[7 more]</label></div><br/><div class="children"><div class="content">Are there any good implementations of using RAG within postgresql ecosystem? I have seen blogposts from supabase[0] and timescale db[1] but not a full fledged project. The full text search is very good within postgres at the moment and having semantic search within the same ecosystem is quiet helpful atleast for simple usecases.<p>[0] <a href="https:&#x2F;&#x2F;supabase.com&#x2F;docs&#x2F;guides&#x2F;database&#x2F;extensions&#x2F;pgvector">https:&#x2F;&#x2F;supabase.com&#x2F;docs&#x2F;guides&#x2F;database&#x2F;extensions&#x2F;pgvecto...</a><p>[1] <a href="https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;postgresql-as-a-vector-database-create-store-and-query-openai-embeddings-with-pgvector&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.timescale.com&#x2F;blog&#x2F;postgresql-as-a-vector-databa...</a></div><br/><div id="37827150" class="c"><input type="checkbox" id="c-37827150" checked=""/><div class="controls bullet"><span class="by">losteric</span><span>|</span><a href="#37827129">parent</a><span>|</span><a href="#37828081">next</a><span>|</span><label class="collapse" for="c-37827150">[-]</label><label class="expand" for="c-37827150">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t RAG &quot;just&quot; dynamically injecting relevant text in a prompt? What more would one implement to achieve RAG, beyond using Postgres&#x27; built in full text or knn search?</div><br/><div id="37827225" class="c"><input type="checkbox" id="c-37827225" checked=""/><div class="controls bullet"><span class="by">wanderingmind</span><span>|</span><a href="#37827129">root</a><span>|</span><a href="#37827150">parent</a><span>|</span><a href="#37827982">next</a><span>|</span><label class="collapse" for="c-37827225">[-]</label><label class="expand" for="c-37827225">[2 more]</label></div><br/><div class="children"><div class="content">what i&#x27;m looking for is a neat python library (or equivalent) that integrates end to end say with postgres&#x2F;pgvector using sqlalchemy, enables parallel processing of large number of documents, create interfaces for embeddings using openai&#x2F;ollama etc. It looks like FastRAG [0] from intel looks close to what i&#x27;m envisioning but it doesnt appear to have integration to postgres ecosystem yet i guess.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;IntelLabs&#x2F;fastRAG">https:&#x2F;&#x2F;github.com&#x2F;IntelLabs&#x2F;fastRAG</a></div><br/><div id="37827941" class="c"><input type="checkbox" id="c-37827941" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37827129">root</a><span>|</span><a href="#37827225">parent</a><span>|</span><a href="#37827982">next</a><span>|</span><label class="collapse" for="c-37827941">[-]</label><label class="expand" for="c-37827941">[1 more]</label></div><br/><div class="children"><div class="content">Through the platform (Neum AI) we support the ability to do this with Postgres, it is just a cloud platform so not a python library.<p>Curious on what type of customization are you looking to add that you would want something like a library?</div><br/></div></div></div></div></div></div><div id="37828081" class="c"><input type="checkbox" id="c-37828081" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#37827129">parent</a><span>|</span><a href="#37827150">prev</a><span>|</span><a href="#37828506">next</a><span>|</span><label class="collapse" for="c-37828081">[-]</label><label class="expand" for="c-37828081">[2 more]</label></div><br/><div class="children"><div class="content">Or generally what are good vector dbs have tried LlaMaindex, pinecone and milvus but all kinda sucked different way.</div><br/><div id="37829605" class="c"><input type="checkbox" id="c-37829605" checked=""/><div class="controls bullet"><span class="by">ddematheu</span><span>|</span><a href="#37827129">root</a><span>|</span><a href="#37828081">parent</a><span>|</span><a href="#37828506">next</a><span>|</span><label class="collapse" for="c-37829605">[-]</label><label class="expand" for="c-37829605">[1 more]</label></div><br/><div class="children"><div class="content">What about then sucked?</div><br/></div></div></div></div></div></div><div id="37828506" class="c"><input type="checkbox" id="c-37828506" checked=""/><div class="controls bullet"><span class="by">vtuulos</span><span>|</span><a href="#37827129">prev</a><span>|</span><label class="collapse" for="c-37828506">[-]</label><label class="expand" for="c-37828506">[1 more]</label></div><br/><div class="children"><div class="content">here&#x27;s how we solved engineering challenges related to RAG using open-source Metaflow: <a href="https:&#x2F;&#x2F;outerbounds.com&#x2F;blog&#x2F;retrieval-augmented-generation&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;outerbounds.com&#x2F;blog&#x2F;retrieval-augmented-generation&#x2F;</a></div><br/></div></div></div></div></div></div></div></body></html>