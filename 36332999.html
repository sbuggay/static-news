<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686819663727" as="style"/><link rel="stylesheet" href="styles.css?v=1686819663727"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://inflecthealth.medium.com/im-an-er-doctor-here-s-how-i-m-already-using-chatgpt-to-help-treat-patients-a023615c65b6">I’m an ER doctor. Here’s how I’m already using ChatGPT to help treat patients</a> <span class="domain">(<a href="https://inflecthealth.medium.com">inflecthealth.medium.com</a>)</span></div><div class="subtext"><span>SLHamlet</span> | <span>148 comments</span></div><br/><div><div id="36334907" class="c"><input type="checkbox" id="c-36334907" checked=""/><div class="controls bullet"><span class="by">firebirdn99</span><span>|</span><a href="#36335424">next</a><span>|</span><label class="collapse" for="c-36334907">[-]</label><label class="expand" for="c-36334907">[46 more]</label></div><br/><div class="children"><div class="content">This really feels farcical, and kind of like vc bros hyping up a product. Considering this was a serious issue of life and death of a patient, it was surprising how it read like a prank or like the scene from Harold and Kumar, where Kumar plays a fake doctor and saves an ER patient through sheer luck.<p>I know some doctors are full of baloney but c&#x27;mon. I really hope maybe that we create better doctors that have a lot of empathy, and are able to wield technology to make themselves 10x better like we&#x27;re doing with programming. But, this is not the way.<p>Edit: I realized its fake...I think, it has to be. It doesn&#x27;t make sense, like others mention. You&#x27;re not reading a 5 para speech to a patient&#x27;s family while they are being treated. And also, in the article, they say to double check and verify what is being said. You better proofread that too! Or it might invent something absolutely crazy, and you may be liable.</div><br/><div id="36337958" class="c"><input type="checkbox" id="c-36337958" checked=""/><div class="controls bullet"><span class="by">raxxorraxor</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36335051">next</a><span>|</span><label class="collapse" for="c-36337958">[-]</label><label class="expand" for="c-36337958">[1 more]</label></div><br/><div class="children"><div class="content">Problem is with high empathy is that it affects you too. That might be pretty exhausting in a stressful hospital environment.</div><br/></div></div><div id="36335051" class="c"><input type="checkbox" id="c-36335051" checked=""/><div class="controls bullet"><span class="by">seabombs</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36337958">prev</a><span>|</span><a href="#36337642">next</a><span>|</span><label class="collapse" for="c-36335051">[-]</label><label class="expand" for="c-36335051">[2 more]</label></div><br/><div class="children"><div class="content">The author is a doctor, but also an executive at a health-focused VC fund, with several AI companies in it&#x27;s portfolio. Helpful context to have before reading his article I think.</div><br/><div id="36337208" class="c"><input type="checkbox" id="c-36337208" checked=""/><div class="controls bullet"><span class="by">mnky9800n</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335051">parent</a><span>|</span><a href="#36337642">next</a><span>|</span><label class="collapse" for="c-36337208">[-]</label><label class="expand" for="c-36337208">[1 more]</label></div><br/><div class="children"><div class="content">So literally it is impossible to know what, if anything, actually happened because this is just more ai Hypebeast trash.</div><br/></div></div></div></div><div id="36337642" class="c"><input type="checkbox" id="c-36337642" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36335051">prev</a><span>|</span><a href="#36336030">next</a><span>|</span><label class="collapse" for="c-36337642">[-]</label><label class="expand" for="c-36337642">[1 more]</label></div><br/><div class="children"><div class="content">Actual ER doctors disagree with you:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36336615">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36336615</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36337227">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36337227</a><p>Classic HN to confidently dismiss something in areas we&#x27;re actually clueless about.</div><br/></div></div><div id="36336030" class="c"><input type="checkbox" id="c-36336030" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36337642">prev</a><span>|</span><a href="#36335330">next</a><span>|</span><label class="collapse" for="c-36336030">[-]</label><label class="expand" for="c-36336030">[1 more]</label></div><br/><div class="children"><div class="content">Ironically, your comment about the state of doctors is quite lacking in empathy for the doctors themselves.<p>Regardless of whether or not this article is a fake, I think you don&#x27;t quite understand how overworked most doctors are, and how little time they have been given per patient to take care of what needs to be done.</div><br/></div></div><div id="36335330" class="c"><input type="checkbox" id="c-36335330" checked=""/><div class="controls bullet"><span class="by">heliodor</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36336030">prev</a><span>|</span><a href="#36335279">next</a><span>|</span><label class="collapse" for="c-36335330">[-]</label><label class="expand" for="c-36335330">[12 more]</label></div><br/><div class="children"><div class="content">The concept is right on point though:<p>ChatGPT is excellent at writing &quot;fluffy&quot; pieces full of empathy, compassion, PR talk, politician speech, and any other wordy flavor you need.<p>As an engineer who is unable to produce such writing, this tool is quite helpful!</div><br/><div id="36337583" class="c"><input type="checkbox" id="c-36337583" checked=""/><div class="controls bullet"><span class="by">rpastuszak</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335330">parent</a><span>|</span><a href="#36337407">next</a><span>|</span><label class="collapse" for="c-36337583">[-]</label><label class="expand" for="c-36337583">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ChatGPT is excellent at writing &quot;fluffy&quot; pieces full of empathy, compassion, PR talk, politician speech,<p>I think you&#x27;re putting a bunch of completely different things in the same basket.<p>Making the text more fluffy will not make it more empathetic, not only (but mainly) because there isn&#x27;t anyone to empathise with the respondent. Our bullshit receptors are good spotting dishonesty, so it&#x27;ll just sound cringe, putting it at the same level as PR talk. But that&#x27;s not empathy or compassion, just cheap and obvious packaging.<p>&gt; As an engineer who is unable to produce such writing, this tool is quite helpful!<p>I genuinely thought you were being sarcastic here. Of course you are and you don&#x27;t need to use more filler words for that.<p>I do think that there&#x27;s value in using GPT for training purposes here, that is learning how one could express themselves in a more context appropriate way. This is not much different from stylistic advice, e.g. in many languages a one word yes&#x2F;no response to a question is considered neutral, whereas in English it can be considered rude&#x2F;abrupt, so people use question tags more often.</div><br/></div></div><div id="36337407" class="c"><input type="checkbox" id="c-36337407" checked=""/><div class="controls bullet"><span class="by">basisword</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335330">parent</a><span>|</span><a href="#36337583">prev</a><span>|</span><a href="#36337691">next</a><span>|</span><label class="collapse" for="c-36337407">[-]</label><label class="expand" for="c-36337407">[1 more]</label></div><br/><div class="children"><div class="content">Helpful until someone reading your “writing” realises it’s AI and that you’re a robot who needs AI to write something with empathy or compassion. I’d much rather hear someone’s really voice in their writing than something false. It reminds me of people in certain places who are fake nice and fake positive 100% of the time. It’s draining to be around them.</div><br/></div></div><div id="36337691" class="c"><input type="checkbox" id="c-36337691" checked=""/><div class="controls bullet"><span class="by">rpastuszak</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335330">parent</a><span>|</span><a href="#36337407">prev</a><span>|</span><a href="#36336351">next</a><span>|</span><label class="collapse" for="c-36337691">[-]</label><label class="expand" for="c-36337691">[1 more]</label></div><br/><div class="children"><div class="content">Reading the comments I realised that:<p>- people are having two separate conversations: one about empathy, another about an empathetic-sounding style<p>- many commenters use different definitions of empathy and compassion<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Empathy" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Empathy</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Compassion" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Compassion</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sympathy" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sympathy</a></div><br/></div></div><div id="36336351" class="c"><input type="checkbox" id="c-36336351" checked=""/><div class="controls bullet"><span class="by">firebirdn99</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335330">parent</a><span>|</span><a href="#36337691">prev</a><span>|</span><a href="#36337350">next</a><span>|</span><label class="collapse" for="c-36336351">[-]</label><label class="expand" for="c-36336351">[7 more]</label></div><br/><div class="children"><div class="content">Many comments are saying it&#x27;s helpful. But what happens when it&#x27;s widespread, everyone crafting Hallmark type empathic messages to everyone. Or once everyone has bots, then most likely they will all be sent straight to spam or something. We will be inundated with such messaging.</div><br/><div id="36337332" class="c"><input type="checkbox" id="c-36337332" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336351">parent</a><span>|</span><a href="#36336598">next</a><span>|</span><label class="collapse" for="c-36337332">[-]</label><label class="expand" for="c-36337332">[2 more]</label></div><br/><div class="children"><div class="content">Then if you don&#x27;t want your messages to read like that, you&#x27;ll use bots to condense and extract the bits you care about.</div><br/><div id="36337738" class="c"><input type="checkbox" id="c-36337738" checked=""/><div class="controls bullet"><span class="by">tirpen</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36337332">parent</a><span>|</span><a href="#36336598">next</a><span>|</span><label class="collapse" for="c-36337738">[-]</label><label class="expand" for="c-36337738">[1 more]</label></div><br/><div class="children"><div class="content">And thus we will have invented a lossy rarefaction scheme for communication?<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rarefaction" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rarefaction</a></div><br/></div></div></div></div><div id="36336598" class="c"><input type="checkbox" id="c-36336598" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336351">parent</a><span>|</span><a href="#36337332">prev</a><span>|</span><a href="#36336702">next</a><span>|</span><label class="collapse" for="c-36336598">[-]</label><label class="expand" for="c-36336598">[1 more]</label></div><br/><div class="children"><div class="content">I look forward to AI-generated sappy Hallmark-like messages, because they&#x27;re bound to be better written than the same types of messages most humans write today.<p>The sentiment will be no less fake.  It&#x27;ll just be better written.</div><br/></div></div><div id="36336702" class="c"><input type="checkbox" id="c-36336702" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336351">parent</a><span>|</span><a href="#36336598">prev</a><span>|</span><a href="#36336474">next</a><span>|</span><label class="collapse" for="c-36336702">[-]</label><label class="expand" for="c-36336702">[1 more]</label></div><br/><div class="children"><div class="content">I presume the issue is detecting when the meaning is intended or not. Like if a doctor uses it to communicate their care vs a salesman uses it to get a deal?</div><br/></div></div><div id="36336474" class="c"><input type="checkbox" id="c-36336474" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336351">parent</a><span>|</span><a href="#36336702">prev</a><span>|</span><a href="#36337350">next</a><span>|</span><label class="collapse" for="c-36336474">[-]</label><label class="expand" for="c-36336474">[2 more]</label></div><br/><div class="children"><div class="content">And? What is wrong with being inundated with empathy?</div><br/><div id="36337334" class="c"><input type="checkbox" id="c-36337334" checked=""/><div class="controls bullet"><span class="by">hammyhavoc</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336474">parent</a><span>|</span><a href="#36337350">next</a><span>|</span><label class="collapse" for="c-36337334">[-]</label><label class="expand" for="c-36337334">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t necessarily sincere.</div><br/></div></div></div></div></div></div><div id="36337350" class="c"><input type="checkbox" id="c-36337350" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335330">parent</a><span>|</span><a href="#36336351">prev</a><span>|</span><a href="#36335279">next</a><span>|</span><label class="collapse" for="c-36337350">[-]</label><label class="expand" for="c-36337350">[1 more]</label></div><br/><div class="children"><div class="content">PR talk and political speech, I agree.  I wouldn&#x27;t call this fabricated shit &quot;empathy&quot; or &quot;compassion&quot;, for both, you need a little more then a LLM fine-tuned by SV lefties.  In fact, whenever ChatGPT tried to be empathic, it feels very pathetic.</div><br/></div></div></div></div><div id="36335279" class="c"><input type="checkbox" id="c-36335279" checked=""/><div class="controls bullet"><span class="by">protastus</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36335330">prev</a><span>|</span><a href="#36336492">next</a><span>|</span><label class="collapse" for="c-36335279">[-]</label><label class="expand" for="c-36335279">[10 more]</label></div><br/><div class="children"><div class="content">I believe it. I&#x27;ve used chatgpt for inspiration on how to craft difficult messages with empathy. I&#x27;ve been managing people for a long time and could&#x27;ve done it independently, but it&#x27;s good to get help. It can be tough to express empathy when exhausted after a long day of urgent demands and responsibilities.<p>This is no different than asking chatgpt to draft code that I could otherwise write from scratch. It&#x27;s like having an assistant.</div><br/><div id="36337654" class="c"><input type="checkbox" id="c-36337654" checked=""/><div class="controls bullet"><span class="by">rpastuszak</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335279">parent</a><span>|</span><a href="#36337254">next</a><span>|</span><label class="collapse" for="c-36337654">[-]</label><label class="expand" for="c-36337654">[1 more]</label></div><br/><div class="children"><div class="content">I get the approach of stylistic advice. I think that people in this thread have two separate conversations, one about an empathetic <i>style</i> and one about empathy.<p>Empathy means sharing (and often uncomfortable) emotion with another person.<p>If what the text represents is true, i.e. the model helped you express you really feel, but you&#x27;re too tired to write it, that&#x27;s sounds like a net positive.<p>Although <i>in my experience</i> if I&#x27;m too tired to sound empathetic on my own, I&#x27;d rather avoid messaging people at all or put whatever message I have for them in context, e.g.: &quot;Hey, It&#x27;s been a long day and I&#x27;m knackered, so I&#x27;ll be brief: ...&quot;. People appreciate being direct and honesty, but I don&#x27;t work much in places like large corps any more where communication is already very formalised and &quot;templated empathy&quot; just follows the existing practices. (not saying that you are, just giving an example here).</div><br/></div></div><div id="36337254" class="c"><input type="checkbox" id="c-36337254" checked=""/><div class="controls bullet"><span class="by">oxfordmale</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335279">parent</a><span>|</span><a href="#36337654">prev</a><span>|</span><a href="#36336342">next</a><span>|</span><label class="collapse" for="c-36337254">[-]</label><label class="expand" for="c-36337254">[1 more]</label></div><br/><div class="children"><div class="content">I only believe it as a cynical marketing piece from an executive at a health-focused VC fund with several AI companies.<p>This could spectacularly have backfired: &quot;&lt;expletives&gt;, what kind of doctor are you? Having to use ChatGPT to treat my mother. Do you even have a medical degree?&quot;<p>When communicating difficult news, you have to be concise. The ChatGPT-generated response is very fluffy.<p>ChatGPT can also hallucinate, so you must be extremely careful when using it at the end of your shift.</div><br/></div></div><div id="36336342" class="c"><input type="checkbox" id="c-36336342" checked=""/><div class="controls bullet"><span class="by">firebirdn99</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335279">parent</a><span>|</span><a href="#36337254">prev</a><span>|</span><a href="#36335345">next</a><span>|</span><label class="collapse" for="c-36336342">[-]</label><label class="expand" for="c-36336342">[5 more]</label></div><br/><div class="children"><div class="content">Now it feels kind of neat, and maybe works when others are reading well crafted messages. What happens when everyone has a bot, and a bot is talking to a bot, because I for one, don&#x27;t want to be sent or fall for emotionally manipulative messages, or that try to sound empathetic, maybe even are empathetic.<p>When this type of messaging is so widespread, I wonder what will happen. I think people will ignore most of this type of messaging. Maybe we&#x27;ll evolve to not be emotional anymore, when emotions are everywhere? I don&#x27;t know, but an interesting hypothetical.</div><br/><div id="36336570" class="c"><input type="checkbox" id="c-36336570" checked=""/><div class="controls bullet"><span class="by">protastus</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336342">parent</a><span>|</span><a href="#36336428">next</a><span>|</span><label class="collapse" for="c-36336570">[-]</label><label class="expand" for="c-36336570">[2 more]</label></div><br/><div class="children"><div class="content">The person sending the message is accountable for its content, no matter how it was authored.<p>Maybe I want to be tactful around a delicate issue, but am struggling to find the right words. An LLM can help me. Alternatively, someone who wants to be manipulative and deceitful might also optimize their message using an LLM.<p>The tool is of course not inherently good or evil. But one can use the tool for different moral outcomes.</div><br/><div id="36337495" class="c"><input type="checkbox" id="c-36337495" checked=""/><div class="controls bullet"><span class="by">flakeoil</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336570">parent</a><span>|</span><a href="#36336428">next</a><span>|</span><label class="collapse" for="c-36337495">[-]</label><label class="expand" for="c-36337495">[1 more]</label></div><br/><div class="children"><div class="content">But we can quite easily feel the difference between these ChatGPT messages which are too much and a genuine message.<p>We know corporate BS when we see it.<p>And in the example in this post I think the doctor said more or less the same thing to the relatives as the ChatGPT message, the biggest difference was that with the ChatGPT message he sat down with everyone around and reading it out load. Had he stopped and given a few seconds&#x2F;minutes with all relatives around him and talked with his own words he would reached the same effect.<p>The other effect is that it was written down. Written texts seems more authoritative than words that just come out of a mouth. So reading from a paper sounds more fact like than just speaking. He could have taken any paper. Or have prepared papers for the most common questions.</div><br/></div></div></div></div><div id="36336428" class="c"><input type="checkbox" id="c-36336428" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336342">parent</a><span>|</span><a href="#36336570">prev</a><span>|</span><a href="#36336867">next</a><span>|</span><label class="collapse" for="c-36336428">[-]</label><label class="expand" for="c-36336428">[1 more]</label></div><br/><div class="children"><div class="content">Will it really be a difference? Corporate speak is already a thing, people already feign emotions and sympathy. Sure, this is <i>more</i>, but it feels to me like more of the same; I don&#x27;t <i>think</i> it even magnifies the problem that much.</div><br/></div></div><div id="36336867" class="c"><input type="checkbox" id="c-36336867" checked=""/><div class="controls bullet"><span class="by">throwuwu</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336342">parent</a><span>|</span><a href="#36336428">prev</a><span>|</span><a href="#36335345">next</a><span>|</span><label class="collapse" for="c-36336867">[-]</label><label class="expand" for="c-36336867">[1 more]</label></div><br/><div class="children"><div class="content">I don’t see how these are different from any of the usual platitudes everyone commonly uses. I’d worry about everyone sounding the same but nevermind because we all use the same turns of phrase already.</div><br/></div></div></div></div><div id="36335345" class="c"><input type="checkbox" id="c-36335345" checked=""/><div class="controls bullet"><span class="by">johndhi</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335279">parent</a><span>|</span><a href="#36336342">prev</a><span>|</span><a href="#36335600">next</a><span>|</span><label class="collapse" for="c-36335345">[-]</label><label class="expand" for="c-36335345">[1 more]</label></div><br/><div class="children"><div class="content">I also believe it. When you&#x27;re focused on the best treatment it can feel like it takes too much effort to explain it well.</div><br/></div></div></div></div><div id="36336492" class="c"><input type="checkbox" id="c-36336492" checked=""/><div class="controls bullet"><span class="by">kodah</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36335279">prev</a><span>|</span><a href="#36335486">next</a><span>|</span><label class="collapse" for="c-36336492">[-]</label><label class="expand" for="c-36336492">[8 more]</label></div><br/><div class="children"><div class="content">Fake or not, people are incorporating ChatGPT into their work. We had a pipeline break today and when we looked at the PR that introduced the change it was in the PR notes that the tooling configuration was derived from a ChatGPT prompt. What do you even say or do about that?</div><br/><div id="36336542" class="c"><input type="checkbox" id="c-36336542" checked=""/><div class="controls bullet"><span class="by">nevi-me</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336492">parent</a><span>|</span><a href="#36337309">next</a><span>|</span><label class="collapse" for="c-36336542">[-]</label><label class="expand" for="c-36336542">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been embarrassed by a Copilot code snippet that I proofread to seem correct, but was incorrect due to some subtle comparison logic.<p>I can only imagine the risks in professions that aren&#x27;t dealing with logic that has a very finite list of options or where the answers AI produce require researched effort to validate.<p>It&#x27;s probably worse in law if you&#x27;re given a case reference as you&#x27;re professionally supposed to read the document in substance to not be muddled by selective quoting, then if it&#x27;s from a foreign jurisdiction you have to also check if it&#x27;s applicable to you.</div><br/><div id="36336715" class="c"><input type="checkbox" id="c-36336715" checked=""/><div class="controls bullet"><span class="by">licebmi__at__</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336542">parent</a><span>|</span><a href="#36337324">next</a><span>|</span><label class="collapse" for="c-36336715">[-]</label><label class="expand" for="c-36336715">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I can only imagine the risks in professions that aren&#x27;t dealing with logic that has a very finite list of options or where the answers AI produce require researched effort to validate<p>This can be done the same way as it’s done in the non ai-assisted process. Mistakes are made then the trained professionals correct the course of action. The problem is with people dealing and selling the problem like it’s a simple logical procedure. Then the layman will think that since a machine crafted the answer, and since machines are logical, the answer must be right.</div><br/></div></div><div id="36337324" class="c"><input type="checkbox" id="c-36337324" checked=""/><div class="controls bullet"><span class="by">weatherlite</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336542">parent</a><span>|</span><a href="#36336715">prev</a><span>|</span><a href="#36336826">next</a><span>|</span><label class="collapse" for="c-36337324">[-]</label><label class="expand" for="c-36337324">[1 more]</label></div><br/><div class="children"><div class="content">Some software mistakes can cost millions ... not saying it happens all the time but it can happen. We SHOULD try to be as accurate and bug free as we can in this business its no joke.
Yeah, sometimes its just an html div that looks funny (that can also cost the company depending on how embarrassing it looked and how much traffic was impacted). But sometimes its backend logic that screws up important data or who knows what with very real reputation or financial consequences to the business.</div><br/></div></div></div></div><div id="36337309" class="c"><input type="checkbox" id="c-36337309" checked=""/><div class="controls bullet"><span class="by">weatherlite</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336492">parent</a><span>|</span><a href="#36336542">prev</a><span>|</span><a href="#36337120">next</a><span>|</span><label class="collapse" for="c-36337309">[-]</label><label class="expand" for="c-36337309">[2 more]</label></div><br/><div class="children"><div class="content">&gt; What do you even say or do about that?<p>Nothing, you fix it. Since its a pipeline it was probably non trivial to somehow test it locally. So a pipeline broke, no biggie I think.</div><br/><div id="36337344" class="c"><input type="checkbox" id="c-36337344" checked=""/><div class="controls bullet"><span class="by">hammyhavoc</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36337309">parent</a><span>|</span><a href="#36337120">next</a><span>|</span><label class="collapse" for="c-36337344">[-]</label><label class="expand" for="c-36337344">[1 more]</label></div><br/><div class="children"><div class="content">Is fixing it not doing something about it?</div><br/></div></div></div></div><div id="36337120" class="c"><input type="checkbox" id="c-36337120" checked=""/><div class="controls bullet"><span class="by">Kwpolska</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336492">parent</a><span>|</span><a href="#36337309">prev</a><span>|</span><a href="#36335486">next</a><span>|</span><label class="collapse" for="c-36337120">[-]</label><label class="expand" for="c-36337120">[1 more]</label></div><br/><div class="children"><div class="content">You fire them for being an idiot and sharing details about your infrastructure with a third party.</div><br/></div></div></div></div><div id="36335486" class="c"><input type="checkbox" id="c-36335486" checked=""/><div class="controls bullet"><span class="by">Spooky23</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36336492">prev</a><span>|</span><a href="#36335072">next</a><span>|</span><label class="collapse" for="c-36335486">[-]</label><label class="expand" for="c-36335486">[6 more]</label></div><br/><div class="children"><div class="content">Some specialties attract different personalities. If you’ve ever been in a hospital setting around a lot of neuro residents, many of them will present… oddly. It’s a feature. Likewise, a lot of ER docs are kinda adrenaline people who are great at triage and quickly stabilizing you, but maybe not so much good at relating with people. That’s a feature — you don’t need a soft touch dealing with trauma.<p>If ChatGPT can help someone perform better in their area of weakness, great. If it can help super smart people get information out of their heads, great.</div><br/><div id="36335629" class="c"><input type="checkbox" id="c-36335629" checked=""/><div class="controls bullet"><span class="by">bohner</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335486">parent</a><span>|</span><a href="#36335072">next</a><span>|</span><label class="collapse" for="c-36335629">[-]</label><label class="expand" for="c-36335629">[5 more]</label></div><br/><div class="children"><div class="content">Have worked with many poor communicators and outright idiots but there is virtually no one graduating a decent medical school with native speaker skills that cant explain in simple terms why giving more fluids to someone that is overloaded with fluid is a bad idea (oh and without using any jargon like edema or diuresis, another thing missing in this whole farce, any 1st year let alone decent attending will explain clearly and simply what is being done. you’re not giving fluids then what the fuck <i>is</i> your plan doctor?).<p>I’ve yet to meet the combo of poor communicator&#x2F;idiot that would believe reading a canned “empathy” script would somehow be helpful.
Seriously is there anyone here that would feel confident in their doctor if they came reading a plan off a sheet like a teleprompter?<p>This guy is trying too hard and assumes most of his audience won’t realize how idiotic this sounds.</div><br/><div id="36336769" class="c"><input type="checkbox" id="c-36336769" checked=""/><div class="controls bullet"><span class="by">licebmi__at__</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335629">parent</a><span>|</span><a href="#36335072">next</a><span>|</span><label class="collapse" for="c-36336769">[-]</label><label class="expand" for="c-36336769">[4 more]</label></div><br/><div class="children"><div class="content">I disagree with the first part, mostly because I believe doctor-speak is a skill medical professionals use to deal with patients which in their desperation, think they know more than they do.<p>Yet I fully agree with the second paragraph. The idea of the teleprompter doctor working is so absurd it would be rejected immediately if not because chatgpt hype.</div><br/><div id="36337531" class="c"><input type="checkbox" id="c-36337531" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336769">parent</a><span>|</span><a href="#36335072">next</a><span>|</span><label class="collapse" for="c-36337531">[-]</label><label class="expand" for="c-36337531">[3 more]</label></div><br/><div class="children"><div class="content">The teleprompter bit makes no sense. Do you feel less confident in your doctor after learning that any kind of serious treatment usually involves <i>a checklist</i>?<p>Following a script isn&#x27;t a sign of lacking skills - it&#x27;s a sign of being wise enough to recognize human limitations. Medical care isn&#x27;t improv comedy.</div><br/><div id="36337602" class="c"><input type="checkbox" id="c-36337602" checked=""/><div class="controls bullet"><span class="by">epcoa</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36337531">parent</a><span>|</span><a href="#36335072">next</a><span>|</span><label class="collapse" for="c-36337602">[-]</label><label class="expand" for="c-36337602">[2 more]</label></div><br/><div class="children"><div class="content">Looking at notes or a paper is one thing and checklists and timeouts serve an important role in safety, but being unable to hold a conversation about a fundamental concept without a script is quite another, and it is lacking a skill by definition.</div><br/><div id="36337903" class="c"><input type="checkbox" id="c-36337903" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36337602">parent</a><span>|</span><a href="#36335072">next</a><span>|</span><label class="collapse" for="c-36337903">[-]</label><label class="expand" for="c-36337903">[1 more]</label></div><br/><div class="children"><div class="content">Really depends on the situation. I have no doubts a typical doctor could easily ELI5[0] you anything about the procedures or treatments they&#x27;re administering - once well fed, well rested, and somewhat relaxed. But if you&#x27;re catching them as they&#x27;re overworked and asking them to context-switch on the spot, well... I&#x27;d expect it work just as well as it would with software engineers. That is, if I was subject to random ELI5 requests during a busy work period, you&#x27;d bet I&#x27;d start preparing notes up front (and probably put them on a Wiki, and then give the people asking me a link to that wiki, and politely tell them to RTFM and GTFO).<p>--<p>[0] - &quot;Explain Like I&#x27;m 5 [years old]&quot;. It&#x27;s Reddit-speak, but describes the concept quite well, and there isn&#x27;t a proper single word alternative for it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36335072" class="c"><input type="checkbox" id="c-36335072" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#36334907">parent</a><span>|</span><a href="#36335486">prev</a><span>|</span><a href="#36335424">next</a><span>|</span><label class="collapse" for="c-36335072">[-]</label><label class="expand" for="c-36335072">[4 more]</label></div><br/><div class="children"><div class="content">Putting aside the question of if the linked is fake, I don&#x27;t necessarily fault medical workers for handling their work dryly.<p>Those doctors and nurses and technicians have to deal with all manners of disgust, biohazards, sadness, and most significantly death each and every single day. I cannot in good faith demand them to treat <i>everyone</i> with empathy, that might as well be psychological torture for the medical workers.<p>This isn&#x27;t to say they shouldn&#x27;t be courteous, professional, and kind to their patients, that should go without saying.</div><br/><div id="36336237" class="c"><input type="checkbox" id="c-36336237" checked=""/><div class="controls bullet"><span class="by">firebirdn99</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335072">parent</a><span>|</span><a href="#36337409">next</a><span>|</span><label class="collapse" for="c-36336237">[-]</label><label class="expand" for="c-36336237">[2 more]</label></div><br/><div class="children"><div class="content">I understand this point. Many doctors I see always being very social, and smiling in public even though they have seen horrors in the ER room, etc. I kind of get a bit of the callous, emotionally stable, and always cheery, positive nature but isn&#x27;t too emotionally deep (think of the doctor in movies, where they give the bad news with a straight face) they need to have to be able to operate like a robot, ironically enough.<p>But, maybe instead of what this article proposes, we do the opposite, we make our doctors more empathetic, and leave the robot to do the grunt work, machinery, surgery, etc. A lot of comments are saying they find the empathy useful. I&#x27;m not sure if I will be able to tell if someone sends me a crafted message, but I don&#x27;t like the idea of a message being sent by ChatGPT that is meant to artificially create empathy, to me it&#x27;s fake empathy.<p>This is all theory, but I don&#x27;t think robots creating fake empathy would resonate with humans once this is widespread. Maybe it creates a sort of disconnection, where people just blatantly avoid falling for text messages and paragraphs that sound empathetic.<p>There are some similarities too with the movie Big-Hero 6, and the Bayman robot that was created as a care robot. Initially, Hiro is very annoyed by it, because of it&#x27;s rote &quot;artificial empathy&quot; voice and messages. But it&#x27;s intelligence is what brings him around, when it understands things like contexts better.</div><br/><div id="36337590" class="c"><input type="checkbox" id="c-36337590" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36336237">parent</a><span>|</span><a href="#36337409">next</a><span>|</span><label class="collapse" for="c-36337590">[-]</label><label class="expand" for="c-36337590">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Maybe it creates a sort of disconnection, where people just blatantly avoid falling for text messages and paragraphs that sound empathetic.</i><p>That would be good to a degree. Right now, people are constantly falling for maliciously crafted empathetic&#x2F;emotional messaging, coming from the mounths and from under the pens of journalists, salesmen, politicians, advertisers, pundits, and social media influencers.<p>In some sense, it&#x27;s really saddening that people find issue with emotional text written by a bot, while they don&#x27;t seem to find any problem with being constantly subjected to <i>malicious</i> emotional messages from aforementioned ill-intentioned parties.</div><br/></div></div></div></div><div id="36337409" class="c"><input type="checkbox" id="c-36337409" checked=""/><div class="controls bullet"><span class="by">ThinkBeat</span><span>|</span><a href="#36334907">root</a><span>|</span><a href="#36335072">parent</a><span>|</span><a href="#36336237">prev</a><span>|</span><a href="#36335424">next</a><span>|</span><label class="collapse" for="c-36337409">[-]</label><label class="expand" for="c-36337409">[1 more]</label></div><br/><div class="children"><div class="content">ER nurses doing triage are some tough mother*ers. 
You walk in, bleeding and pretty sure you will die soon, 
The nurse takes one look at you and is not at all impressed.<p>&quot;Yeah, take a number, and keep pressure on the wound while you wait&quot; 
or
&quot;We are really busy right now.  You will have to wait for many hours. 
You dont really need a doctor. Just do ... ... ... and it will be fine. &quot;<p>One thing I have learned in life is that if you are at the ER
and you have to wait a long time. You are lucky.<p>It is when you are rushed into the back right away you know that
Whatever has happened, it is severe, and you should be scared.</div><br/></div></div></div></div></div></div><div id="36335424" class="c"><input type="checkbox" id="c-36335424" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#36334907">prev</a><span>|</span><a href="#36334492">next</a><span>|</span><label class="collapse" for="c-36335424">[-]</label><label class="expand" for="c-36335424">[17 more]</label></div><br/><div class="children"><div class="content">I&#x27;m having a hard time buying this anecdote. Not even the bit where its implied that multiple people read the same script multiple times and that worked for soothing people, but the actual content of the before and after; the gpt version doesn&#x27;t explain it any differently and the &#x27;compassionate&#x27; writing was absurd.<p>My most charitable interpretation is that the anecdote is true but the article itself was written by some one else and is more of an &#x27;inspired by real events&#x27; situation.<p>Side note; does anyone actually think that chat gpt succeeds at rewriting things &#x27;compassionately&#x27; or &#x27;more intelligently&#x27;? From what I have seen the actual output more resembles caricature than the requested results, in the same way that corporate communications frequently parody earnest communication.</div><br/><div id="36337778" class="c"><input type="checkbox" id="c-36337778" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36335424">parent</a><span>|</span><a href="#36335734">next</a><span>|</span><label class="collapse" for="c-36337778">[-]</label><label class="expand" for="c-36337778">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Side note; does anyone actually think that chat gpt succeeds at rewriting things &#x27;compassionately&#x27; or &#x27;more intelligently&#x27;?</i><p>Sure. There are some good examples in the replies already. For me, ChatGPT (specifically GPT-4, via API) does a better job than I do based on sheer breadth and command of vocabulary alone. I&#x27;ve used it a couple times to generate emotional texts and the results were quite good - though I&#x27;m not actually giving anyone the raw LLM output. The process always looked like:<p>1. Me describing in detail the kind of message I need, who the recipient is, and what the situational context is, and then asking ChatGPT to generate several variants.<p>2. LLM generating 2-3 variants, each of which is roughly 50% stellar, 50% meh.<p>3. Adding any missing details and trying again. And&#x2F;or, if one of the variants looks particularly good&#x2F;promising, asking the LLM to generate variations of that.<p>4. Taking the few best results, mixing them together, and blending with some of the text I independently wrote on my own.<p>In those few cases (and in more cases where I&#x27;d enlist GPT to write formal e-mails for me; the process is pretty much identical), the final outcome was a piece of text that&#x27;s a sentence or phrase-level blend, with 50-60% of the phrases being AI-authored and the rest my own, and 100% of it edited and reviewed by me.<p>It still takes some work to write a good message this way, but I resort to this, because that 50% of AI-sourced text I incorporate is <i>stellar</i> - all well chosen words and phrases (many of them I kind of forgot they exist). It&#x27;s something I <i>could</i> write completely on my own, but from my pre-GPT attempts I know that reaching this level would take me many hours of agonizing efforts, marked by strong feelings of self-doubt.</div><br/></div></div><div id="36335734" class="c"><input type="checkbox" id="c-36335734" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#36335424">parent</a><span>|</span><a href="#36337778">prev</a><span>|</span><a href="#36335491">next</a><span>|</span><label class="collapse" for="c-36335734">[-]</label><label class="expand" for="c-36335734">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Side note; does anyone actually think that chat gpt succeeds at rewriting things &#x27;compassionately&#x27; or &#x27;more intelligently&#x27;?&quot;<p>Often, sure.<p>A few days ago I asked ChatGPT about a situation that needed compassion and wisdom. I thought it did way better than the 5 or 6 people I had talked to about it.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;2dd26347-3fbd-483f-b567-21c040782050" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;2dd26347-3fbd-483f-b567-21c040...</a><p>(ignore the first question, which was silly)<p>FYI, we&#x27;re going to give the dog another chance.... but I was glad ChatGPT didn&#x27;t push for that. I think ChatGPT answered it pretty close to how someone well trained in both mental health therapy and dealing with dogs with severe behavior issues might handle it.</div><br/><div id="36335961" class="c"><input type="checkbox" id="c-36335961" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335734">parent</a><span>|</span><a href="#36335491">next</a><span>|</span><label class="collapse" for="c-36335961">[-]</label><label class="expand" for="c-36335961">[2 more]</label></div><br/><div class="children"><div class="content">Good luck with the dog, and thank you for sharing that. I agree that was a reasonable response but I may be biased because I recently had a conversation along these lines and the hit most of the same beats. It is certainly free of the defects I mentioned. Thank you so much for the counter example, its perfect.<p>I wonder if it is significant that you didn&#x27;t ask it for a specific kind of response?</div><br/><div id="36336238" class="c"><input type="checkbox" id="c-36336238" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335961">parent</a><span>|</span><a href="#36335491">next</a><span>|</span><label class="collapse" for="c-36336238">[-]</label><label class="expand" for="c-36336238">[1 more]</label></div><br/><div class="children"><div class="content">Likewise, thanks for scoring a point for humanity. :) It&#x27;s rare to get someone on the internet to appreciate a counterpoint.<p>Yes it probably helps that I didn&#x27;t ask it for advice, rather than just kind of talk to it as if it was a sounding board. I guess I&#x27;m weird that way, in that I feel it is important to talk to an AI like it is human and has feelings (saying everything from &quot;please&quot; to &quot;wow that was amazing, thanks&quot;), even if only for myself.<p>But I&#x27;ve become convinced it gets better responses. Which isn&#x27;t altogether surprising, if you think about how LLMs work.</div><br/></div></div></div></div></div></div><div id="36335491" class="c"><input type="checkbox" id="c-36335491" checked=""/><div class="controls bullet"><span class="by">chocolatkey</span><span>|</span><a href="#36335424">parent</a><span>|</span><a href="#36335734">prev</a><span>|</span><a href="#36337875">next</a><span>|</span><label class="collapse" for="c-36335491">[-]</label><label class="expand" for="c-36335491">[4 more]</label></div><br/><div class="children"><div class="content">&gt; does anyone actually think that chat gpt succeeds at rewriting things &#x27;compassionately&#x27; or &#x27;more intelligently&#x27;?<p>I&#x27;ve seen non-native English speakers (co-worker) use it to write lengthy apologies to customers, and I (native speaker) think it sounds over the top sincere and dramatic, especially when the same person tends to talk in a completely different manner.<p>I believe this story, because the customer in this case was satisfied too</div><br/><div id="36335828" class="c"><input type="checkbox" id="c-36335828" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335491">parent</a><span>|</span><a href="#36337875">next</a><span>|</span><label class="collapse" for="c-36335828">[-]</label><label class="expand" for="c-36335828">[3 more]</label></div><br/><div class="children"><div class="content">&quot;and I (native speaker) think it sounds over the top sincere and dramatic, especially when the same person tends to talk in a completely different manner.&quot;<p>Of course, you can actually tell ChatGPT to change its style and mannerisms. Here&#x27;s an example when I was experimenting with this. It went a bit over the top in the other direction, but only because I was so explicit about asking to to be casual.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;07f6f9aa-de02-4a0c-8eeb-574eef5722a5" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;07f6f9aa-de02-4a0c-8eeb-574eef...</a></div><br/><div id="36336540" class="c"><input type="checkbox" id="c-36336540" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335828">parent</a><span>|</span><a href="#36337487">next</a><span>|</span><label class="collapse" for="c-36336540">[-]</label><label class="expand" for="c-36336540">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s even pretty good at doing this multi-lingually. I live in Indonesia, and there is a sort of dialect used by various people from Jakarta that basically combines Indonesian and English in a way that is quite recognizeable when you hear it.<p>The other day I asked ChatGPT to answer a question I had in Indonesian, and the answer was very formal (Google Translate generally has the same problem – the translations it gives you are <i>way</i> too formal for most speech). So I asked it to rephrase in this Jakartan slang it and did very well.</div><br/></div></div><div id="36337487" class="c"><input type="checkbox" id="c-36337487" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335828">parent</a><span>|</span><a href="#36336540">prev</a><span>|</span><a href="#36337875">next</a><span>|</span><label class="collapse" for="c-36337487">[-]</label><label class="expand" for="c-36337487">[1 more]</label></div><br/><div class="children"><div class="content">This won&#x27;t necessarily help non-native speakers who are not able to tell if and which adjustments are needed.</div><br/></div></div></div></div></div></div><div id="36337875" class="c"><input type="checkbox" id="c-36337875" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#36335424">parent</a><span>|</span><a href="#36335491">prev</a><span>|</span><a href="#36335574">next</a><span>|</span><label class="collapse" for="c-36337875">[-]</label><label class="expand" for="c-36337875">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Side note; does anyone actually think that chat gpt succeeds at rewriting things &#x27;compassionately&#x27; or &#x27;more intelligently&#x27;?<p>Yes, I use it for this purpose all the time. I could have done better myself if I invested time in writing it, but it allows me to get a good enough result in a couple of seconds.</div><br/></div></div><div id="36335574" class="c"><input type="checkbox" id="c-36335574" checked=""/><div class="controls bullet"><span class="by">AmericanChopper</span><span>|</span><a href="#36335424">parent</a><span>|</span><a href="#36337875">prev</a><span>|</span><a href="#36334492">next</a><span>|</span><label class="collapse" for="c-36335574">[-]</label><label class="expand" for="c-36335574">[7 more]</label></div><br/><div class="children"><div class="content">I’m really not trying to be inflammatory when I say this, but highly technical professions that have intense training requirements for entry attract a disproportionate amount of people with social development issues, ranging from intense self-diagnosed “introversion” to full blown developmental disorders like autism&#x2F;aspergers. There’s plenty of this in software development, but medicine takes it to the next level. The demands of the curriculum and placement training basically ensure that a majority of the successful entrants to the profession are highly anti-social, as anybody with an interest in having a healthy social life would almost have to give it up to make it through that selection process. My thesis is that people with these personality traits legitimately find the synthesised “compassion” or “empathy” produced by LLMs to be legitimately insightful.</div><br/><div id="36336102" class="c"><input type="checkbox" id="c-36336102" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335574">parent</a><span>|</span><a href="#36336383">next</a><span>|</span><label class="collapse" for="c-36336102">[-]</label><label class="expand" for="c-36336102">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been in situations where reviewing a selection of sentiments as presented by hallmark cards could have improved my response so I can imagine something like this happening.<p>But this article specifically convinces me that it was not written by some one with those experiences.<p>Also, I&#x27;ve got a friend who started med school in their thirties, hearing about it from him I do not think your supposition is particularly inflammatory. I went through an engineering program which was time consuming and generally considered challenging and I still wonder at what sort of internal alchemy keeps proto-doctors in the program and not doing literally anything else with their time.</div><br/></div></div><div id="36336383" class="c"><input type="checkbox" id="c-36336383" checked=""/><div class="controls bullet"><span class="by">pcurve</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335574">parent</a><span>|</span><a href="#36336102">prev</a><span>|</span><a href="#36335786">next</a><span>|</span><label class="collapse" for="c-36336383">[-]</label><label class="expand" for="c-36336383">[1 more]</label></div><br/><div class="children"><div class="content">&quot;medicine takes it to the next level&quot;<p>I chuckled at this because I confirm this be true among my acquaintances and theirs who are doctors.<p>I hate to stereotype, and generally, they&#x27;re all decent people.  But almost all of them have undercurrent of arrogance that permeates through despite their best attempt to keep it in check.  It&#x27;s pretty funny.</div><br/></div></div><div id="36335786" class="c"><input type="checkbox" id="c-36335786" checked=""/><div class="controls bullet"><span class="by">bohner</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335574">parent</a><span>|</span><a href="#36336383">prev</a><span>|</span><a href="#36334492">next</a><span>|</span><label class="collapse" for="c-36335786">[-]</label><label class="expand" for="c-36335786">[4 more]</label></div><br/><div class="children"><div class="content">There is a kernel of truth to this, disproportionate sure, but I think majority is a bit over the top. There are plenty in medicine aware of this, antisocial behavior and toxicity in general, there’s a lot of crying that goes on from medical school throughout training. You can easily have a rich social life throughout training but the process doesn’t effectively filter out the trainwrecks either.</div><br/><div id="36335940" class="c"><input type="checkbox" id="c-36335940" checked=""/><div class="controls bullet"><span class="by">AmericanChopper</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335786">parent</a><span>|</span><a href="#36334492">next</a><span>|</span><label class="collapse" for="c-36335940">[-]</label><label class="expand" for="c-36335940">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You can easily have a rich social life throughout training<p>This is the complete opposite of my experience, and I’d be very surprised if there were any med schools where this was legitimately possible. When my wife was in med school she had about 40 hours of course work per week, and the curriculum was so dense that she spent nearly all the rest of her time studying. She worked (a job) 10 hours a week during med school and we’d be lucky to have a couple of hours of dedicated time together per week. Most of the time we spent together was when I’d help her with revising. During semester breaks she had mandatory placements to complete, and her school operated classes on all public holidays. It only eased for a couple of months between academic years (when she also had a non-trivial study workload to get through). During her residency this got worse when she had irregular 18+ hour shifts to get through, and just wanted to spend most of her time at home sleeping. This was the same experience that all of her peers had, so either you or my wife must have had an atypical academic experience, or you have a substantially different definition of “rich social life” than I do.<p>&gt; There are plenty in medicine aware of this, antisocial behavior and toxicity in general<p>To be clear I’m not referring to toxicity necessarily, or antisocial as in being destructive or disruptive. I mean averse to social settings, and not capable of or interested in participating in normal pro social behaviour. Nearly all of her peers at med school had these characteristics, if somebody told me that 90% of doctors had some form of autism, I really wouldn’t have a hard time believing it. This is very similar to the experience I had in all of my maths, physics and comp sci papers at college, and to my subsequent experience working in software development. Granted it’s not quite as severe in that setting, but the poorly socialised software engineer stereotype didn’t just materialise out of thin air.<p>People do find these highly contrived LLM outputs to be insightful for their “perspectives” on compassion or empathy, which is totally unrelatable for me, but this just seems like a reasonably plausible explanation.</div><br/><div id="36336565" class="c"><input type="checkbox" id="c-36336565" checked=""/><div class="controls bullet"><span class="by">bohner</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36335940">parent</a><span>|</span><a href="#36334492">next</a><span>|</span><label class="collapse" for="c-36336565">[-]</label><label class="expand" for="c-36336565">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This is the complete opposite of my experience, and I’d be very surprised if there were any med schools where this was legitimately possible.<p>Well then your experience is limited.<p>Most people don’t work a job in medical school, so that’s time right there. Also the format of US medical school has changed considerably in the last 20 years. There is plenty of free time in the preclinical years, very few attend lecture (preferring to use a number of commercial review&#x2F;prep product), there are usually only a few hours a week of mandatory team based learning groups and some hours of labs, this format is overwhelmingly common now in the US. Time management is also a large factor, some are better prepared to deal with the gauntlet.
Clinical years will have periods of intensity but usually for few week stretches, surgery clerkship might be intense, but psychiatry which is also at least 6 weeks will often be close to 9 to 5 with free weekends or even less. Time during residency training is extremely specialty and program dependent. Is your wife OB&#x2F;GYN or general surgery, that’s very different than medicine, anesthesia or dermatology.<p>&gt; Nearly all of her peers at med school had these characteristics, if somebody told me that 90% of doctors had some form of autism, I really wouldn’t have a hard time believing it.<p>Yeah whatever. There’s a lot of pricks and socially awkward people in medicine but a majority are primarily still interested in going into a service oriented industry working with people.<p>Just because some idiot with something to sell posts an apocryphal story about ChatGPT doesn’t prove your point.<p>Having worked in both tech and medicine your take is just selection bias and a limited view. Such a verbose response to say very little as well.</div><br/><div id="36337003" class="c"><input type="checkbox" id="c-36337003" checked=""/><div class="controls bullet"><span class="by">AmericanChopper</span><span>|</span><a href="#36335424">root</a><span>|</span><a href="#36336565">parent</a><span>|</span><a href="#36334492">next</a><span>|</span><label class="collapse" for="c-36337003">[-]</label><label class="expand" for="c-36337003">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also the format of US medical school has changed considerably in the last 20 years. There is plenty of free time in the preclinical years<p>You&#x27;re right about that, it&#x27;s gotten worse. The perspective you&#x27;re offering here contradicts basically all documented perspectives on the intensity and time demands of medical training.<p>&gt; Concerns about excessive content and curricular overcrowding in medical education have existed for more than a century. In 1910, Abraham Flexner noted that the packed medical school curriculum would “tax the memory, not the intellect.”1 The Rappleye Commission Report on Medical Education published in 1932 stated that “the almost frantic attempts to put into the medical course teaching in all phases of scientific and medical knowledge, and the tenacity with which traditional features of teaching are retained have been responsible for great rigidity, overcrowding, and a lack of proper balance in the training.”2 Since then, periodic concerns have been raised about this ongoing problem, notably in the Association of American Medical Colleges&#x27; Panel on the General Professional Education of the Physician (GPEP report) in 1984 and the Assessing Change in Medical Education (ACME-TRI) report in 1993, among others.3-6<p>&gt; The problem has only worsened in the years preceding the pandemic. New pharmaceutical agents seem to appear daily, while our understanding of pathophysiology of disease continues to expand. In addition, we have needed to consider new and vital subjects such as cultural competence, care of LGBTQ patients, teamwork and interprofessional care, health care quality and safety, medical humanities, narrative medicine, and even wellness curricula, that have added pressure to the preclerkship phase of medical school, as ever more content is added. And little has been removed.<p><a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC8463236&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC8463236&#x2F;</a><p>The relationship between empathy and burn out (especially in clinical practice) is also very well established. The whole system has a very strong selection against empathetic practitioners, whether through burning them out, or selecting a preference for their less empathetic peers.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36334492" class="c"><input type="checkbox" id="c-36334492" checked=""/><div class="controls bullet"><span class="by">amstan</span><span>|</span><a href="#36335424">prev</a><span>|</span><a href="#36337933">next</a><span>|</span><label class="collapse" for="c-36334492">[-]</label><label class="expand" for="c-36334492">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Satisfying HIPAA rules around patient privacy alone may take many years, or decades, to resolve themselves before we could even contemplate directly using programs like ChatGPT in a medical theater.<p>This is easily solved by not using someone else&#x27;s API.<p>Here&#x27;s $500 worth of hardware you can get on ebay running Vicuna-13B, locally:<p>&gt; IV fluids may not be the best treatment for someone with severe pulmonary edema and respiratory distress because it could make their condition worse. This is because when a person has severe pulmonary edema, their lungs are already filled with fluid, which makes it harder for them to breathe. Adding more fluid to their body could increase the pressure in their lungs and make it even harder for them to breathe. This could be life-threatening.<p>&gt; Instead, the best course of action would be to focus on treating the underlying cause of the pulmonary edema, such as heart failure or a lung infection. This may involve giving medications to help the heart pump more effectively or to reduce inflammation in the lungs. The patient may also need oxygen therapy to help them breathe more easily.<p>&gt; It&#x27;s important to remember that every case is unique, and the best course of treatment will depend on the individual patient&#x27;s condition. The healthcare team will do their best to provide the most appropriate care for the patient based on their specific needs.</div><br/><div id="36334911" class="c"><input type="checkbox" id="c-36334911" checked=""/><div class="controls bullet"><span class="by">doug_durham</span><span>|</span><a href="#36334492">parent</a><span>|</span><a href="#36336721">next</a><span>|</span><label class="collapse" for="c-36334911">[-]</label><label class="expand" for="c-36334911">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but this is adding in treatments not listed in the original prompt.  There was nothing mentioned about treating heart failure or reducing immflamuation. Vicuna is extrapolating here and not simplifying and making the message more compassionate.  It is an interesting application of local inference though.</div><br/></div></div><div id="36336721" class="c"><input type="checkbox" id="c-36336721" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#36334492">parent</a><span>|</span><a href="#36334911">prev</a><span>|</span><a href="#36335100">next</a><span>|</span><label class="collapse" for="c-36336721">[-]</label><label class="expand" for="c-36336721">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic announce HIPAA compliance yesterday.</div><br/></div></div><div id="36335100" class="c"><input type="checkbox" id="c-36335100" checked=""/><div class="controls bullet"><span class="by">aiappreciator</span><span>|</span><a href="#36334492">parent</a><span>|</span><a href="#36336721">prev</a><span>|</span><a href="#36334783">next</a><span>|</span><label class="collapse" for="c-36335100">[-]</label><label class="expand" for="c-36335100">[2 more]</label></div><br/><div class="children"><div class="content">You really, really don&#x27;t want to use inferior models where accuracy is a life or death issue.</div><br/><div id="36335267" class="c"><input type="checkbox" id="c-36335267" checked=""/><div class="controls bullet"><span class="by">Narkov</span><span>|</span><a href="#36334492">root</a><span>|</span><a href="#36335100">parent</a><span>|</span><a href="#36334783">next</a><span>|</span><label class="collapse" for="c-36335267">[-]</label><label class="expand" for="c-36335267">[1 more]</label></div><br/><div class="children"><div class="content">In this example, it is not a diagnosis tool. It is a communication tool with &quot;stakeholders&quot;.</div><br/></div></div></div></div><div id="36334783" class="c"><input type="checkbox" id="c-36334783" checked=""/><div class="controls bullet"><span class="by">cced</span><span>|</span><a href="#36334492">parent</a><span>|</span><a href="#36335100">prev</a><span>|</span><a href="#36337933">next</a><span>|</span><label class="collapse" for="c-36334783">[-]</label><label class="expand" for="c-36334783">[3 more]</label></div><br/><div class="children"><div class="content">How much time does this take to get produced locally?</div><br/><div id="36335639" class="c"><input type="checkbox" id="c-36335639" checked=""/><div class="controls bullet"><span class="by">thatcherc</span><span>|</span><a href="#36334492">root</a><span>|</span><a href="#36334783">parent</a><span>|</span><a href="#36334879">next</a><span>|</span><label class="collapse" for="c-36335639">[-]</label><label class="expand" for="c-36335639">[1 more]</label></div><br/><div class="children"><div class="content">Not OP, and I&#x27;d have to see the prompt to run it on my set up, but I but together a refurbished HP Chromebox with some extra RAM and an SSD for a total of $250 and it&#x27;ll run all the 13B models about as fast as I can type on my phone, just as a reference point. I&#x27;m consistently impressed by it even if it&#x27;s not blazing fast.</div><br/></div></div><div id="36334879" class="c"><input type="checkbox" id="c-36334879" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36334492">root</a><span>|</span><a href="#36334783">parent</a><span>|</span><a href="#36335639">prev</a><span>|</span><a href="#36337933">next</a><span>|</span><label class="collapse" for="c-36334879">[-]</label><label class="expand" for="c-36334879">[1 more]</label></div><br/><div class="children"><div class="content">I run a similar Vicuna bot on a free ARM VPS from Oracle. Inference takes ~20 seconds for ~200 tokens on the CPU, so it should stream results about half as fast as ChatGPT.<p>...<i>however</i>, that was on cheapo ARM hardware. On a hospital budget I bet you could beat OpenAI with a local model pre-mapped in GPU memory on a 3090.</div><br/></div></div></div></div></div></div><div id="36337933" class="c"><input type="checkbox" id="c-36337933" checked=""/><div class="controls bullet"><span class="by">locallost</span><span>|</span><a href="#36334492">prev</a><span>|</span><a href="#36334860">next</a><span>|</span><label class="collapse" for="c-36337933">[-]</label><label class="expand" for="c-36337933">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know if it&#x27;s fake, but I would not bet on it. There is imho a large crisis of trust in all types of institutions, professions etc. Being a doctor has been demystified to an extent, for various reasons (e.g. the realization I&#x27;ve heard from some people that doctors didn&#x27;t really know what was wrong and they just went from one guess to another, large amount of information being available giving people the wrong impression they know more than they do etc. etc.). It&#x27;s also my personal explanation for the rise in alternative medicine, homeopathy, ivermectin treatments for covid, but also more broadly various forms of conspiracy theories. It&#x27;s not far fetched that some people would trust &quot;AI&quot; more than a doctor who they don&#x27;t trust or respect at all anymore.<p>So fake or not, it definitely hits a nerve as something that is possible. On a tangent, I think this is actually quite interesting. If I was a sociologist, I would be interested in the root cause of this as much as a scientist or an engineer is interested in a bridge falling down. But I mostly see lamenting that the world has gone crazy.</div><br/></div></div><div id="36334860" class="c"><input type="checkbox" id="c-36334860" checked=""/><div class="controls bullet"><span class="by">sarchertech</span><span>|</span><a href="#36337933">prev</a><span>|</span><a href="#36334468">next</a><span>|</span><label class="collapse" for="c-36334860">[-]</label><label class="expand" for="c-36334860">[1 more]</label></div><br/><div class="children"><div class="content">There is no way it was faster to type this, verify it, and print it out than it was to just stop and think what you wanted to say.<p>I also very much doubt an agitated family member will be calmed down by a doctor reading a script to them.</div><br/></div></div><div id="36334468" class="c"><input type="checkbox" id="c-36334468" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#36334860">prev</a><span>|</span><a href="#36334843">next</a><span>|</span><label class="collapse" for="c-36334468">[-]</label><label class="expand" for="c-36334468">[11 more]</label></div><br/><div class="children"><div class="content">Kind of fascinating that the benefit here is not so much artificial intelligence as artificial empathy.</div><br/><div id="36334755" class="c"><input type="checkbox" id="c-36334755" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#36334468">parent</a><span>|</span><a href="#36334716">next</a><span>|</span><label class="collapse" for="c-36334755">[-]</label><label class="expand" for="c-36334755">[1 more]</label></div><br/><div class="children"><div class="content">ML models in general are not great when tasked with rigid stuff. Stable diffusion can make a cool picture, but getting it to make exactly the picture you had in mind is mostly impossible.<p>This is actually a great use case in the sense that the requirements are flexible, it just has to be something empathetic and directionally related. That bar is so much lower than a trustworthy diagnosis or even a document lookup, either of which can be right or wrong. When what you need is &quot;an example of x&quot;, this technology is great.</div><br/></div></div><div id="36334716" class="c"><input type="checkbox" id="c-36334716" checked=""/><div class="controls bullet"><span class="by">Nition</span><span>|</span><a href="#36334468">parent</a><span>|</span><a href="#36334755">prev</a><span>|</span><a href="#36335352">next</a><span>|</span><label class="collapse" for="c-36334716">[-]</label><label class="expand" for="c-36334716">[1 more]</label></div><br/><div class="children"><div class="content">I suppose really that&#x27;s one of the big benefits of LLMs for everything. They&#x27;re a friend who always has the time and willingness to try and help.</div><br/></div></div><div id="36335352" class="c"><input type="checkbox" id="c-36335352" checked=""/><div class="controls bullet"><span class="by">mutant_glofish</span><span>|</span><a href="#36334468">parent</a><span>|</span><a href="#36334716">prev</a><span>|</span><a href="#36334747">next</a><span>|</span><label class="collapse" for="c-36335352">[-]</label><label class="expand" for="c-36335352">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Kind of fascinating that the benefit here is not so much artificial intelligence as artificial empathy.<p>In their book &quot;The Elephant in the Brain&quot;, Kevin Simler and Robin Hanson argue that a large subset of health care is already something like that: conspicuous caring with little benefits other than social signaling and feeling that one&#x27;s being care for. [1]<p>[1]: <a href="https:&#x2F;&#x2F;johnathanbi.com&#x2F;book-notes-summaries&#x2F;notes-on-elephant-in-the-brain" rel="nofollow noreferrer">https:&#x2F;&#x2F;johnathanbi.com&#x2F;book-notes-summaries&#x2F;notes-on-elepha...</a></div><br/></div></div><div id="36334747" class="c"><input type="checkbox" id="c-36334747" checked=""/><div class="controls bullet"><span class="by">tshadley</span><span>|</span><a href="#36334468">parent</a><span>|</span><a href="#36335352">prev</a><span>|</span><a href="#36335002">next</a><span>|</span><label class="collapse" for="c-36334747">[-]</label><label class="expand" for="c-36334747">[4 more]</label></div><br/><div class="children"><div class="content">All cynicism aside, there&#x27;s vastly more in the collective writings of humans on empathy than medicine.</div><br/><div id="36334778" class="c"><input type="checkbox" id="c-36334778" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#36334468">root</a><span>|</span><a href="#36334747">parent</a><span>|</span><a href="#36335002">next</a><span>|</span><label class="collapse" for="c-36334778">[-]</label><label class="expand" for="c-36334778">[3 more]</label></div><br/><div class="children"><div class="content">See my sibling comment just posted, I don&#x27;t belive that&#x27;s relevant. You can always screw up facts, it&#x27;s a high bar. A loosely constrained example of an empathetic speech much easier. Despite the fact that medicine deals in facts (and this would hold true for an even more black and white discipline) so all you need in principle to learn from are a few examples, while empathy is much more subtle and would require a representation to be learned from many varied examples.</div><br/><div id="36334834" class="c"><input type="checkbox" id="c-36334834" checked=""/><div class="controls bullet"><span class="by">fsckboy</span><span>|</span><a href="#36334468">root</a><span>|</span><a href="#36334778">parent</a><span>|</span><a href="#36335002">next</a><span>|</span><label class="collapse" for="c-36334834">[-]</label><label class="expand" for="c-36334834">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>the fact that medicine deals in facts</i><p>medicine benefits from facts, but is largely based on experienced, expert <i>opinion</i></div><br/><div id="36335498" class="c"><input type="checkbox" id="c-36335498" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#36334468">root</a><span>|</span><a href="#36334834">parent</a><span>|</span><a href="#36335002">next</a><span>|</span><label class="collapse" for="c-36335498">[-]</label><label class="expand" for="c-36335498">[1 more]</label></div><br/><div class="children"><div class="content">And experience is the practical contact with and observation of facts, over an extended period.<p>Most English dictionaries explicitly separate out “formal judgment by an expert” as a distinct meaning for “opinion” from the more common meaning of personal subjective no-facts-needed belief.</div><br/></div></div></div></div></div></div></div></div><div id="36335002" class="c"><input type="checkbox" id="c-36335002" checked=""/><div class="controls bullet"><span class="by">msla</span><span>|</span><a href="#36334468">parent</a><span>|</span><a href="#36334747">prev</a><span>|</span><a href="#36334533">next</a><span>|</span><label class="collapse" for="c-36335002">[-]</label><label class="expand" for="c-36335002">[1 more]</label></div><br/><div class="children"><div class="content">So much for hyper-advanced robots knowing pi to a million places but not being able to process this thing you call &quot;love&quot;.<p>They&#x27;ve even got this thing we call &quot;bullshit&quot; down pat.</div><br/></div></div><div id="36334533" class="c"><input type="checkbox" id="c-36334533" checked=""/><div class="controls bullet"><span class="by">VWWHFSfQ</span><span>|</span><a href="#36334468">parent</a><span>|</span><a href="#36335002">prev</a><span>|</span><a href="#36335045">next</a><span>|</span><label class="collapse" for="c-36334533">[-]</label><label class="expand" for="c-36334533">[1 more]</label></div><br/><div class="children"><div class="content">&quot;here&#x27;s a squeeze ball.  go sit outside and squeeze it.&quot;</div><br/></div></div></div></div><div id="36334843" class="c"><input type="checkbox" id="c-36334843" checked=""/><div class="controls bullet"><span class="by">brigandish</span><span>|</span><a href="#36334468">prev</a><span>|</span><a href="#36336615">next</a><span>|</span><label class="collapse" for="c-36334843">[-]</label><label class="expand" for="c-36334843">[11 more]</label></div><br/><div class="children"><div class="content">Who is surprised that in a match between a doctor&#x27;s bedside manner and a machine&#x27;s that a machine wins?<p>The cold metal of a machine is nothing next to the coldness of a doctor&#x27;s heart! ;)</div><br/><div id="36336434" class="c"><input type="checkbox" id="c-36336434" checked=""/><div class="controls bullet"><span class="by">pcurve</span><span>|</span><a href="#36334843">parent</a><span>|</span><a href="#36335290">next</a><span>|</span><label class="collapse" for="c-36336434">[-]</label><label class="expand" for="c-36336434">[3 more]</label></div><br/><div class="children"><div class="content">Reminded me of this scene where Eric Dale was getting let go and HR was regurgitating canned script.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;zn2Xf9hAFcE?t=151" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;zn2Xf9hAFcE?t=151</a></div><br/><div id="36336595" class="c"><input type="checkbox" id="c-36336595" checked=""/><div class="controls bullet"><span class="by">noisy_boy</span><span>|</span><a href="#36334843">root</a><span>|</span><a href="#36336434">parent</a><span>|</span><a href="#36335290">next</a><span>|</span><label class="collapse" for="c-36336595">[-]</label><label class="expand" for="c-36336595">[2 more]</label></div><br/><div class="children"><div class="content">One of my favourite movies; we need more finance movies considering the outsized direct&#x2F;indirect impact financial industry has on our lives + hits closer to home too.</div><br/><div id="36336747" class="c"><input type="checkbox" id="c-36336747" checked=""/><div class="controls bullet"><span class="by">pcurve</span><span>|</span><a href="#36334843">root</a><span>|</span><a href="#36336595">parent</a><span>|</span><a href="#36335290">next</a><span>|</span><label class="collapse" for="c-36336747">[-]</label><label class="expand" for="c-36336747">[1 more]</label></div><br/><div class="children"><div class="content">mine too.  Must&#x27;ve watched it a dozen times.  Margin Call and The Big Short are fun to watch over and over for some reason.</div><br/></div></div></div></div></div></div><div id="36335290" class="c"><input type="checkbox" id="c-36335290" checked=""/><div class="controls bullet"><span class="by">protastus</span><span>|</span><a href="#36334843">parent</a><span>|</span><a href="#36336434">prev</a><span>|</span><a href="#36334952">next</a><span>|</span><label class="collapse" for="c-36335290">[-]</label><label class="expand" for="c-36335290">[2 more]</label></div><br/><div class="children"><div class="content">The machine doesn&#x27;t get tired after explaining the same thing 1000 times.</div><br/><div id="36337790" class="c"><input type="checkbox" id="c-36337790" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36334843">root</a><span>|</span><a href="#36335290">parent</a><span>|</span><a href="#36334952">next</a><span>|</span><label class="collapse" for="c-36337790">[-]</label><label class="expand" for="c-36337790">[1 more]</label></div><br/><div class="children"><div class="content">You haven&#x27;t met the Emergency Medical Hologram yet, have you?</div><br/></div></div></div></div><div id="36334952" class="c"><input type="checkbox" id="c-36334952" checked=""/><div class="controls bullet"><span class="by">kmoser</span><span>|</span><a href="#36334843">parent</a><span>|</span><a href="#36335290">prev</a><span>|</span><a href="#36335121">next</a><span>|</span><label class="collapse" for="c-36334952">[-]</label><label class="expand" for="c-36334952">[4 more]</label></div><br/><div class="children"><div class="content">My takeaway isn&#x27;t how useful ChatGPT is, but how useless the doctor is, and by extension, how poorly the the field of medicine is that it doesn&#x27;t adequately train doctors to have better bedside manner.<p>Regardless of my state of agitation, I would most definitely not want to be read a script prepared by an AI. In fact, were I to find out after the fact, I&#x27;d be even more incensed.</div><br/><div id="36335463" class="c"><input type="checkbox" id="c-36335463" checked=""/><div class="controls bullet"><span class="by">kaitai</span><span>|</span><a href="#36334843">root</a><span>|</span><a href="#36334952">parent</a><span>|</span><a href="#36335121">next</a><span>|</span><label class="collapse" for="c-36335463">[-]</label><label class="expand" for="c-36335463">[3 more]</label></div><br/><div class="children"><div class="content">I passed this article &amp; discussion on to physician spouse, who would specifically like me to reply to this comment: &quot;You are being very emotional.&quot;<p>Digging deeper (as the comment above alone isn&#x27;t worthy of HN, IMO): &quot;People believe what they want to believe because of small sample sizes. When there isn&#x27;t a black &amp; white answer people go back to their personal experience, which is limited. They will not be right in the way you want to be right if you&#x27;re practicing medicine.&quot; The family members are certainly exhibiting this. Are any of us, in this discussion?<p>Additional question: &quot;Who is the patient? Is it the anxious family members who are being treated, or the person with fluid in the lungs?&quot; The person with fluid in the lungs is the patient and the doc needs to be spending time on that person (the one who will die if not treated correctly). The family members are important but <i>not in need of medical care</i>. They can be handed off to an AI in the short term.<p>From my own vantage point, I&#x27;ve observed that the context switching between &quot;caring for patient&quot; and &quot;dealing with family&quot; can put a substantial drag on cognitive function for physicians. They do need to suck it up and deal with it since family members are the primary way in which patients receive post-release care in the US due to the non-existent safety net, and family member presence can really improve outcomes, but I&#x27;m not surprised that ChatGPT helps in producing the sort of bland prose that is well-received by family members.</div><br/><div id="36335872" class="c"><input type="checkbox" id="c-36335872" checked=""/><div class="controls bullet"><span class="by">average_hntist</span><span>|</span><a href="#36334843">root</a><span>|</span><a href="#36335463">parent</a><span>|</span><a href="#36335121">next</a><span>|</span><label class="collapse" for="c-36335872">[-]</label><label class="expand" for="c-36335872">[2 more]</label></div><br/><div class="children"><div class="content">This sort of response certainly seems indicative of a doctor with horrifically bad bedside manner. Families tend to be emotional when the health of their loved ones is at stake. I&#x27;m skeptical that a chatbot would help someone who can&#x27;t grasp that.</div><br/><div id="36337834" class="c"><input type="checkbox" id="c-36337834" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36334843">root</a><span>|</span><a href="#36335872">parent</a><span>|</span><a href="#36335121">next</a><span>|</span><label class="collapse" for="c-36337834">[-]</label><label class="expand" for="c-36337834">[1 more]</label></div><br/><div class="children"><div class="content">The response is pretty clear and 100% correct: family is important but <i>not in need of medical care</i>. Making doctors focus too much attention on the needs of third parties will worsen the outcomes for actual patients.<p>Doctors are, on average, quite empathetic people. But humans in general are what they are, and the politics and economics of medicine prevent the doctors from saying the right thing[0] to overbearing or emotionally distraught third parties, so I can easily see how inserting a chatbot in between would improve the outcomes of actual patients.<p>(Though I am surprised there isn&#x27;t dedicated staff for that yet. Or does it not work when people know they&#x27;re talking to a PR doctor instead of the one treating their loved one?)<p>--<p>[0] - &quot;Get the fuck out of my sickbay!&quot; - <a href="https:&#x2F;&#x2F;giphy.com&#x2F;gifs&#x2F;paramountplus-star-trek-lower-decks-eyk7bDJkp5Rr5jpVLt" rel="nofollow noreferrer">https:&#x2F;&#x2F;giphy.com&#x2F;gifs&#x2F;paramountplus-star-trek-lower-decks-e...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="36336615" class="c"><input type="checkbox" id="c-36336615" checked=""/><div class="controls bullet"><span class="by">hfo</span><span>|</span><a href="#36334843">prev</a><span>|</span><a href="#36337637">next</a><span>|</span><label class="collapse" for="c-36336615">[-]</label><label class="expand" for="c-36336615">[2 more]</label></div><br/><div class="children"><div class="content">Physician here with ER experience (and IT too). I think it&#x27;s quite realistic, except the relatives being in the ER room, giving orders to nurses. But that might be different in other hospitals and countries.<p>The reason for the edema is not given, perhaps cardiac insufficience, perhaps she didn’t take her pills (it is mentioned that she had dementia). Anyway, it’s possible that treatment has already been done but would take a few hours to fully set in, with the relatives impatiently waiting and seeing no improvement.<p>Explaining medical things to patients and relatives is difficult and a subject on its own in medschool. In extraordinary circumstances and without being able to do much on their own, people filter things they are told to what they want to hear. Sometimes you get feedback on what the patient understood, it&#x27;s surprising and also the potential base of a lawsuit. There is a reason that you have to document so many things in writing. So I think it’s a smart idea to use AI to generate simple-language messages that are easily understood.
It doesn’t matter if the medical content of the text is 100% correct, the message here is: ”Calm down, stand back, I handle this”.</div><br/><div id="36337227" class="c"><input type="checkbox" id="c-36337227" checked=""/><div class="controls bullet"><span class="by">rediguanayum</span><span>|</span><a href="#36336615">parent</a><span>|</span><a href="#36337637">next</a><span>|</span><label class="collapse" for="c-36337227">[-]</label><label class="expand" for="c-36337227">[1 more]</label></div><br/><div class="children"><div class="content">+1 Wife is an ER doc. She also agrees with the premise management that communicating medical facts to that patient and family is burdensome and time consuming, especially when there&#x27;s pressure on the staff to see a high volume of patients.  She has her own anecdotes similar to the story.  There needs to be improvements in communication though she hasn&#x27;t bought into ChatGPT being the right way to do this.</div><br/></div></div></div></div><div id="36334130" class="c"><input type="checkbox" id="c-36334130" checked=""/><div class="controls bullet"><span class="by">pedalpete</span><span>|</span><a href="#36337637">prev</a><span>|</span><a href="#36337443">next</a><span>|</span><label class="collapse" for="c-36334130">[-]</label><label class="expand" for="c-36334130">[14 more]</label></div><br/><div class="children"><div class="content">Will there be a Dr House (is there a more current TV doctor) video version which the family can interact with which would make them feel like they are getting the best care in the world from a famous doctor?<p>But also, if ChatGPT has a 50% chance of giving the wrong diagnosis, can you trust it more than 50% to give the correct reasoning to the family as to the activity of the medical staff?</div><br/><div id="36334736" class="c"><input type="checkbox" id="c-36334736" checked=""/><div class="controls bullet"><span class="by">givemeethekeys</span><span>|</span><a href="#36334130">parent</a><span>|</span><a href="#36334358">next</a><span>|</span><label class="collapse" for="c-36334736">[-]</label><label class="expand" for="c-36334736">[7 more]</label></div><br/><div class="children"><div class="content">&quot;Trust but verify&quot;. If you can&#x27;t verify, then don&#x27;t trust.<p>I&#x27;ve used ChatGPT for short scripts. I can read and test-run scripts to verify their correctness. On the other hand, asking it to do more complex programming is error-prone.</div><br/><div id="36334752" class="c"><input type="checkbox" id="c-36334752" checked=""/><div class="controls bullet"><span class="by">Madmallard</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36334736">parent</a><span>|</span><a href="#36334358">next</a><span>|</span><label class="collapse" for="c-36334752">[-]</label><label class="expand" for="c-36334752">[6 more]</label></div><br/><div class="children"><div class="content">If you have to verify then there is no trust. That&#x27;s not how trust works. I never liked the phrase.</div><br/><div id="36337665" class="c"><input type="checkbox" id="c-36337665" checked=""/><div class="controls bullet"><span class="by">givemeethekeys</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36334752">parent</a><span>|</span><a href="#36335169">next</a><span>|</span><label class="collapse" for="c-36337665">[-]</label><label class="expand" for="c-36337665">[1 more]</label></div><br/><div class="children"><div class="content">You can trust that ChatGPT will try its best to interpret your request and produce output to the best of its knowledge.<p>You must verify that the output meets your needs.<p>Trust, but verify, as you would when you work with any engineer ;).</div><br/></div></div><div id="36335169" class="c"><input type="checkbox" id="c-36335169" checked=""/><div class="controls bullet"><span class="by">justin_oaks</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36334752">parent</a><span>|</span><a href="#36337665">prev</a><span>|</span><a href="#36334974">next</a><span>|</span><label class="collapse" for="c-36335169">[-]</label><label class="expand" for="c-36335169">[3 more]</label></div><br/><div class="children"><div class="content">I agree. I&#x27;ve never liked the phrase either. Even so, I&#x27;ll take &quot;Trust but verify&quot; as a shorthand for &quot;Accept the output of a somewhat untrustworthy person&#x2F;process, but verify the output before making use of it.&quot;<p>Sometimes a short, inaccurate phrase wins out over the longer, accurate version.</div><br/><div id="36337860" class="c"><input type="checkbox" id="c-36337860" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36335169">parent</a><span>|</span><a href="#36335200">next</a><span>|</span><label class="collapse" for="c-36337860">[-]</label><label class="expand" for="c-36337860">[1 more]</label></div><br/><div class="children"><div class="content">There are bad and good uses of that phrase. I like the reading that goes &quot;trust the intentions and skills of the other person, but recognize they&#x27;re only human and can make mistakes, so if the thing is high-stakes, double-check anyway&quot;.</div><br/></div></div><div id="36335200" class="c"><input type="checkbox" id="c-36335200" checked=""/><div class="controls bullet"><span class="by">Madmallard</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36335169">parent</a><span>|</span><a href="#36337860">prev</a><span>|</span><a href="#36334974">next</a><span>|</span><label class="collapse" for="c-36335200">[-]</label><label class="expand" for="c-36335200">[1 more]</label></div><br/><div class="children"><div class="content">I would change &quot;accept&quot; to &quot;consider&quot; in your rephrasing.</div><br/></div></div></div></div><div id="36334974" class="c"><input type="checkbox" id="c-36334974" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36334752">parent</a><span>|</span><a href="#36335169">prev</a><span>|</span><a href="#36334358">next</a><span>|</span><label class="collapse" for="c-36334974">[-]</label><label class="expand" for="c-36334974">[1 more]</label></div><br/><div class="children"><div class="content">The world exists in a state of probability, it is rarely black and white.</div><br/></div></div></div></div></div></div><div id="36334358" class="c"><input type="checkbox" id="c-36334358" checked=""/><div class="controls bullet"><span class="by">tazjin</span><span>|</span><a href="#36334130">parent</a><span>|</span><a href="#36334736">prev</a><span>|</span><a href="#36334374">next</a><span>|</span><label class="collapse" for="c-36334358">[-]</label><label class="expand" for="c-36334358">[4 more]</label></div><br/><div class="children"><div class="content">&gt; if ChatGPT has a 50% chance of giving the wrong diagnosis<p>It depends on the average chance of the doctor giving the wrong diagnosies (I honestly have no idea?!)</div><br/><div id="36334765" class="c"><input type="checkbox" id="c-36334765" checked=""/><div class="controls bullet"><span class="by">EA-3167</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36334358">parent</a><span>|</span><a href="#36334484">next</a><span>|</span><label class="collapse" for="c-36334765">[-]</label><label class="expand" for="c-36334765">[2 more]</label></div><br/><div class="children"><div class="content">There are different figures, partly as a result of how this question is studied. The Mayo Clinic did a famous study on people who sought a second opinion, and they found that something like 12% of people seeking that second opinion were correctly diagnosed. Obviously this isn&#x27;t a fair sample since this is not only people seeking a second opinion, but one from a prestigious source.<p>This narrative review discusses the issue from a global perspective, concluding that most people will experience at least one misdiagnosis. <a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC5502242&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC5502242&#x2F;</a><p>Article on Mayo study, with link to study: <a href="https:&#x2F;&#x2F;newsnetwork.mayoclinic.org&#x2F;discussion&#x2F;mayo-clinic-researchers-demonstrate-value-of-second-opinions&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;newsnetwork.mayoclinic.org&#x2F;discussion&#x2F;mayo-clinic-re...</a></div><br/><div id="36337671" class="c"><input type="checkbox" id="c-36337671" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36334765">parent</a><span>|</span><a href="#36334484">next</a><span>|</span><label class="collapse" for="c-36337671">[-]</label><label class="expand" for="c-36337671">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t seeking a second opinion something you usually do when the first opinion sounds fishy, doesn&#x27;t seem to match the evidence, or when the doctor giving doesn&#x27;t seem to be doing his job seriously (e.g. dismissive, preoccupied etc)? Or in cases where it is known to be a difficult situation which makes you think multiple diagnoses are needed?</div><br/></div></div></div></div><div id="36334484" class="c"><input type="checkbox" id="c-36334484" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#36334130">root</a><span>|</span><a href="#36334358">parent</a><span>|</span><a href="#36334765">prev</a><span>|</span><a href="#36334374">next</a><span>|</span><label class="collapse" for="c-36334484">[-]</label><label class="expand" for="c-36334484">[1 more]</label></div><br/><div class="children"><div class="content">My experience is second-hand, but they seem to be pretty split between seeing exciting rare diagnoses everywhere (students, juniors primarily, or one&#x27;s own particular research interest) and biased against them, it&#x27;s ~never that, it&#x27;s the same boring thing we saw ten of yesterday (jaded specialists primarily).</div><br/></div></div></div></div><div id="36334374" class="c"><input type="checkbox" id="c-36334374" checked=""/><div class="controls bullet"><span class="by">weaksauce</span><span>|</span><a href="#36334130">parent</a><span>|</span><a href="#36334358">prev</a><span>|</span><a href="#36334424">next</a><span>|</span><label class="collapse" for="c-36334374">[-]</label><label class="expand" for="c-36334374">[1 more]</label></div><br/><div class="children"><div class="content">no, you still have to read it.<p>&gt; In each case, the output from the hungover intern&#x2F;ChatGPT needs to be carefully checked before it’s used. But in these scenarios, reviewing existing work is usually much faster than starting from scratch.</div><br/></div></div><div id="36334424" class="c"><input type="checkbox" id="c-36334424" checked=""/><div class="controls bullet"><span class="by">lockhouse</span><span>|</span><a href="#36334130">parent</a><span>|</span><a href="#36334374">prev</a><span>|</span><a href="#36337443">next</a><span>|</span><label class="collapse" for="c-36334424">[-]</label><label class="expand" for="c-36334424">[1 more]</label></div><br/><div class="children"><div class="content">There are a number of medical conditions that could be diagnosed very easily by some lab results, ChatGPT, and a deep fake.</div><br/></div></div></div></div><div id="36337443" class="c"><input type="checkbox" id="c-36337443" checked=""/><div class="controls bullet"><span class="by">gumballindie</span><span>|</span><a href="#36334130">prev</a><span>|</span><a href="#36335847">next</a><span>|</span><label class="collapse" for="c-36337443">[-]</label><label class="expand" for="c-36337443">[1 more]</label></div><br/><div class="children"><div class="content">&gt; using ChatGPT to help treat patients<p>He’s not using it to treat anyone. He’s using it for generating text that then is (weirdly) read to patients.<p>These ads for chatgpt are getting incredibly hilarious as of recently. But an improvement over the initial “chatgpt saved my dog’s life” type of nonsense.</div><br/></div></div><div id="36335847" class="c"><input type="checkbox" id="c-36335847" checked=""/><div class="controls bullet"><span class="by">stereoradonc</span><span>|</span><a href="#36337443">prev</a><span>|</span><a href="#36336995">next</a><span>|</span><label class="collapse" for="c-36335847">[-]</label><label class="expand" for="c-36335847">[1 more]</label></div><br/><div class="children"><div class="content">One of the worst write ups trying to drum up &quot;support&quot; for a product known to hallucinate. I couldn&#x27;t imagine this could land up here!</div><br/></div></div><div id="36336995" class="c"><input type="checkbox" id="c-36336995" checked=""/><div class="controls bullet"><span class="by">lovelearning</span><span>|</span><a href="#36335847">prev</a><span>|</span><a href="#36335358">next</a><span>|</span><label class="collapse" for="c-36336995">[-]</label><label class="expand" for="c-36336995">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you have any questions or concerns, don’t hesitate to reach out to the medical team.<p>But isn&#x27;t that what they were doing in the first place? More honest to suggest don&#x27;t hesitate to reach out to ChatGPT.</div><br/></div></div><div id="36335358" class="c"><input type="checkbox" id="c-36335358" checked=""/><div class="controls bullet"><span class="by">Pigalowda</span><span>|</span><a href="#36336995">prev</a><span>|</span><a href="#36334157">next</a><span>|</span><label class="collapse" for="c-36335358">[-]</label><label class="expand" for="c-36335358">[1 more]</label></div><br/><div class="children"><div class="content">This is silly and fake. Patient families are like this all the time. Just explain like a real big boy doctor</div><br/></div></div><div id="36334157" class="c"><input type="checkbox" id="c-36334157" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#36335358">prev</a><span>|</span><a href="#36337411">next</a><span>|</span><label class="collapse" for="c-36334157">[-]</label><label class="expand" for="c-36334157">[2 more]</label></div><br/><div class="children"><div class="content">Theoretically something like BioMedLM and&#x2F;or a vector search over clinical research and&#x2F;or GPT-4 with cited sources etc. could be useful in some diagnostic circumstances to at least give doctors ideas, right?<p>What he is talking about here is just an incredibly common scenario that the doctor and everyone is already very familiar with. Which is basically delaying the inevitable for people who are in some ways already gone.<p>Also my understanding is that the ChatGPT API does have some level of privacy now although maybe not compliant with the privacy laws which seems somewhat burdensome on the entire medical industry (overcompensating in some detrimental ways).<p>It seems like the fundamental problem is aging. Maybe a leading edge LLM approach can help keep people up to speed on aging research. Although probably that&#x27;s pushing it because you would need to get very deep into mitochondrial damage or whatever.</div><br/><div id="36335035" class="c"><input type="checkbox" id="c-36335035" checked=""/><div class="controls bullet"><span class="by">wolverine876</span><span>|</span><a href="#36334157">parent</a><span>|</span><a href="#36337411">next</a><span>|</span><label class="collapse" for="c-36335035">[-]</label><label class="expand" for="c-36335035">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Theoretically something like BioMedLM and&#x2F;or a vector search over clinical research and&#x2F;or GPT-4 with cited sources etc. could be useful in some diagnostic circumstances to at least give doctors ideas, right?<p>Anything is possible, but consider this doctor&#x27;s review of ChatGPT&#x27;s diagonistic output:<p>&gt; ChatGPT works fairly well as a diagnostic assistant — but only if you feed it perfect information, and the actual patient has a classic presentation, which is rarely the case. (And you don’t mind a 50% success rate that often misses life-threatening conditions.)<p>For any new technology to be useful, it needs to do more than just work; it must be significantly more productive than the alternative.</div><br/></div></div></div></div><div id="36337411" class="c"><input type="checkbox" id="c-36337411" checked=""/><div class="controls bullet"><span class="by">ChaitanyaSai</span><span>|</span><a href="#36334157">prev</a><span>|</span><a href="#36334889">next</a><span>|</span><label class="collapse" for="c-36337411">[-]</label><label class="expand" for="c-36337411">[1 more]</label></div><br/><div class="children"><div class="content">Human doctors using AI to sound more humane was not something I&#x27;d have guessed. Not just a one-off case <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;06&#x2F;12&#x2F;health&#x2F;doctors-chatgpt-artificial-intelligence.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;06&#x2F;12&#x2F;health&#x2F;doctors-chatgpt-ar...</a><p>And it is fun getting chatgpt to inject compassion into unexpected scenarios.
<a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;83a5b1f2-9b5a-4ebd-947c-b68fd26a1876" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;83a5b1f2-9b5a-4ebd-947c-b68fd2...</a></div><br/></div></div><div id="36334889" class="c"><input type="checkbox" id="c-36334889" checked=""/><div class="controls bullet"><span class="by">harshaw</span><span>|</span><a href="#36337411">prev</a><span>|</span><a href="#36334859">next</a><span>|</span><label class="collapse" for="c-36334889">[-]</label><label class="expand" for="c-36334889">[1 more]</label></div><br/><div class="children"><div class="content">The point here is that he is using chatGPT &#x2F;LLMs as an assistant - not making a diagnosis or something.  He easily could have come up with a similar script from working with another member of hospital staff (maybe a social worker??) but that takes time.<p>What I want (in a business context) is a LLM that can rewrite memos &#x2F; docs (I am an Amazonian) in exactly the style that someone expects and is trained on other successful docs. It&#x27;s not to cut out the hard work of getting the details right - its to cut down on the bullshit of misalignment from bad communication.</div><br/></div></div><div id="36335392" class="c"><input type="checkbox" id="c-36335392" checked=""/><div class="controls bullet"><span class="by">Tempest1981</span><span>|</span><a href="#36334859">prev</a><span>|</span><a href="#36333955">next</a><span>|</span><label class="collapse" for="c-36335392">[-]</label><label class="expand" for="c-36335392">[2 more]</label></div><br/><div class="children"><div class="content">Is there a name for this 5 paragraph response format, that feels so intrinsic to chatGPT?  (5 paragraphs:  General empathy, specific, empathy, technical explanation, establishing trust, reassurance)<p>Is the style a result of the default rules that OpenAI is using? Or is it derived automatically from the material chatGPT was trained on?</div><br/><div id="36335420" class="c"><input type="checkbox" id="c-36335420" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#36335392">parent</a><span>|</span><a href="#36333955">next</a><span>|</span><label class="collapse" for="c-36335420">[-]</label><label class="expand" for="c-36335420">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s called the five-paragraph essay [0] :) Or at least, it&#x27;s a variant of it, with more specific constraints on some of the paragraphs (e.g. &quot;establishing empathy&quot; in the introduction).<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Five-paragraph_essay" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Five-paragraph_essay</a></div><br/></div></div></div></div><div id="36333955" class="c"><input type="checkbox" id="c-36333955" checked=""/><div class="controls bullet"><span class="by">paulryanrogers</span><span>|</span><a href="#36335392">prev</a><span>|</span><a href="#36335709">next</a><span>|</span><label class="collapse" for="c-36333955">[-]</label><label class="expand" for="c-36333955">[10 more]</label></div><br/><div class="children"><div class="content">&gt; ...only if you feed it perfect information, and the actual patient has a classic presentation, which is rarely the case. (And you don’t mind a 50% success rate that often misses life-threatening conditions.)<p>This sounds like a step backwards?</div><br/><div id="36334114" class="c"><input type="checkbox" id="c-36334114" checked=""/><div class="controls bullet"><span class="by">danmaw</span><span>|</span><a href="#36333955">parent</a><span>|</span><a href="#36334159">next</a><span>|</span><label class="collapse" for="c-36334114">[-]</label><label class="expand" for="c-36334114">[6 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t a doctor, it&#x27;s trained on Reddit threads and other random shit off the internet not medical journals.</div><br/><div id="36334275" class="c"><input type="checkbox" id="c-36334275" checked=""/><div class="controls bullet"><span class="by">jprafael</span><span>|</span><a href="#36333955">root</a><span>|</span><a href="#36334114">parent</a><span>|</span><a href="#36335306">next</a><span>|</span><label class="collapse" for="c-36334275">[-]</label><label class="expand" for="c-36334275">[4 more]</label></div><br/><div class="children"><div class="content">Given that &quot;The Pile&quot; includes PubMed and NIH as data sources it would be unlikely to have GPT4 not use them at all. Even GPT3 uses Wikipedia which does have (mostly) factual data with cited sources.</div><br/><div id="36334364" class="c"><input type="checkbox" id="c-36334364" checked=""/><div class="controls bullet"><span class="by">tazjin</span><span>|</span><a href="#36333955">root</a><span>|</span><a href="#36334275">parent</a><span>|</span><a href="#36334493">next</a><span>|</span><label class="collapse" for="c-36334364">[-]</label><label class="expand" for="c-36334364">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Even GPT3 uses Wikipedia which does have (mostly) factual data with cited sources.<p>There&#x27;s a LOT of stuff on Wikipedia where the source is a link to some random, long article and it&#x27;s unclear where exactly the referred to information is coming from. Gets significantly worse for any &quot;hot&quot; topic.</div><br/></div></div><div id="36334493" class="c"><input type="checkbox" id="c-36334493" checked=""/><div class="controls bullet"><span class="by">kaliqt</span><span>|</span><a href="#36333955">root</a><span>|</span><a href="#36334275">parent</a><span>|</span><a href="#36334364">prev</a><span>|</span><a href="#36334351">next</a><span>|</span><label class="collapse" for="c-36334493">[-]</label><label class="expand" for="c-36334493">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s lossy though, it tries to remember everything and relate that to everything else. Performance would increase significantly if it were tuned for that use case.</div><br/></div></div><div id="36334351" class="c"><input type="checkbox" id="c-36334351" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#36333955">root</a><span>|</span><a href="#36334275">parent</a><span>|</span><a href="#36334493">prev</a><span>|</span><a href="#36335306">next</a><span>|</span><label class="collapse" for="c-36334351">[-]</label><label class="expand" for="c-36334351">[1 more]</label></div><br/><div class="children"><div class="content">Problem is half of the publications or even more is pure garbage that cannot be reproduced so there is that.</div><br/></div></div></div></div><div id="36335306" class="c"><input type="checkbox" id="c-36335306" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36333955">root</a><span>|</span><a href="#36334114">parent</a><span>|</span><a href="#36334275">prev</a><span>|</span><a href="#36334159">next</a><span>|</span><label class="collapse" for="c-36335306">[-]</label><label class="expand" for="c-36335306">[1 more]</label></div><br/><div class="children"><div class="content">You should try asking it detailed questions about medicine. It’s fairly deep and broad in its medical “knowledge.”  But it’s not good at deductive or inductive reasoning and has no agency, so is entirely unsurprising it’s not good at differential diagnosis. We actually have good differential diagnosis expert systems the challenge is getting the providers to input queries properly. I can imagine GPT4 acting as an intermediary between human natural language and expert systems to great effect. It seems rather rash to be judging these technologies based off a crappy web UI thrown on a chatbot a few months ago.</div><br/></div></div></div></div><div id="36334159" class="c"><input type="checkbox" id="c-36334159" checked=""/><div class="controls bullet"><span class="by">colechristensen</span><span>|</span><a href="#36333955">parent</a><span>|</span><a href="#36334114">prev</a><span>|</span><a href="#36335709">next</a><span>|</span><label class="collapse" for="c-36334159">[-]</label><label class="expand" for="c-36334159">[3 more]</label></div><br/><div class="children"><div class="content">Then you haven’t had much experience with doctors… you go in and you’re most likely getting either a cast, a broad spectrum antibiotic, or an antidepressant. The only things that get specific diagnoses reliably is when somebody finds a tumor.<p>In the last year I had to get a full antiparasite treatment course because a doctor misdiagnosed my partners psoriasis… that was fun.</div><br/><div id="36334505" class="c"><input type="checkbox" id="c-36334505" checked=""/><div class="controls bullet"><span class="by">wilonth</span><span>|</span><a href="#36333955">root</a><span>|</span><a href="#36334159">parent</a><span>|</span><a href="#36335709">next</a><span>|</span><label class="collapse" for="c-36334505">[-]</label><label class="expand" for="c-36334505">[2 more]</label></div><br/><div class="children"><div class="content">True, 90% of doctor diagnosis are garbage and maybe even worse than ChatGPT. The 10% is for when you have a visible problem that can be cured with a surgery, they are pretty good at that.
A doctor with GPT-4 could be way better than one without.</div><br/><div id="36334688" class="c"><input type="checkbox" id="c-36334688" checked=""/><div class="controls bullet"><span class="by">colechristensen</span><span>|</span><a href="#36333955">root</a><span>|</span><a href="#36334505">parent</a><span>|</span><a href="#36335709">next</a><span>|</span><label class="collapse" for="c-36334688">[-]</label><label class="expand" for="c-36334688">[1 more]</label></div><br/><div class="children"><div class="content">They already have diagnosis flowcharts where it’s a game it follow the symptom.  It would have to be considerably better than the tools they already have to be appropriate.</div><br/></div></div></div></div></div></div></div></div><div id="36335709" class="c"><input type="checkbox" id="c-36335709" checked=""/><div class="controls bullet"><span class="by">PlunderBunny</span><span>|</span><a href="#36333955">prev</a><span>|</span><a href="#36334269">next</a><span>|</span><label class="collapse" for="c-36335709">[-]</label><label class="expand" for="c-36335709">[1 more]</label></div><br/><div class="children"><div class="content">Could the explanation be simply that, when the doctor came back with the piece of paper, the family thought it was from a textbook, or a second-opinion from another doctor that had been printed out? I.e. that it carried more weight because of that.</div><br/></div></div><div id="36334269" class="c"><input type="checkbox" id="c-36334269" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#36335709">prev</a><span>|</span><a href="#36337224">next</a><span>|</span><label class="collapse" for="c-36334269">[-]</label><label class="expand" for="c-36334269">[3 more]</label></div><br/><div class="children"><div class="content">When you need blithering, ChatGPT is there for you.</div><br/><div id="36337872" class="c"><input type="checkbox" id="c-36337872" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36334269">parent</a><span>|</span><a href="#36335089">next</a><span>|</span><label class="collapse" for="c-36337872">[-]</label><label class="expand" for="c-36337872">[1 more]</label></div><br/><div class="children"><div class="content">In other words: ChatGPT is there for you to help with most human interaction.</div><br/></div></div><div id="36335089" class="c"><input type="checkbox" id="c-36335089" checked=""/><div class="controls bullet"><span class="by">wolverine876</span><span>|</span><a href="#36334269">parent</a><span>|</span><a href="#36337872">prev</a><span>|</span><a href="#36337224">next</a><span>|</span><label class="collapse" for="c-36335089">[-]</label><label class="expand" for="c-36335089">[1 more]</label></div><br/><div class="children"><div class="content">You are just in wishful denial about the technological disruption: ChatGPT will rapidly and dramatically increase productivity by displacing human workers.<p>ChatGPT will automate 99.9% of what&#x27;s on the Internet, which is blithering, displacing what most human labor is spent on these days, and leaving humans with no demand for their labor but going back to producing accurate, valuable knowledge.<p>The halcyon days are over. Post-truth technology has followed the typical maturation cycle, and now is at the point that we can automate it. (HN is dead. This post could be written by ChatGPT or a future LLM.)<p>;) :DDDDDDD</div><br/></div></div></div></div><div id="36337224" class="c"><input type="checkbox" id="c-36337224" checked=""/><div class="controls bullet"><span class="by">saargrin</span><span>|</span><a href="#36334269">prev</a><span>|</span><a href="#36335243">next</a><span>|</span><label class="collapse" for="c-36337224">[-]</label><label class="expand" for="c-36337224">[1 more]</label></div><br/><div class="children"><div class="content">so he outsourced the one uniquiely human task of explaining stuff to concerned family to an AI?</div><br/></div></div><div id="36335243" class="c"><input type="checkbox" id="c-36335243" checked=""/><div class="controls bullet"><span class="by">hartator</span><span>|</span><a href="#36337224">prev</a><span>|</span><a href="#36335097">next</a><span>|</span><label class="collapse" for="c-36335243">[-]</label><label class="expand" for="c-36335243">[1 more]</label></div><br/><div class="children"><div class="content">It is kind of counterintuitive that IV fluids are bad for the lungs somehow. You can’t really blame the family. Specially when the “bro” doctor decides it’s time for ChatGPT.</div><br/></div></div><div id="36335097" class="c"><input type="checkbox" id="c-36335097" checked=""/><div class="controls bullet"><span class="by">mvdtnz</span><span>|</span><a href="#36335243">prev</a><span>|</span><a href="#36335133">next</a><span>|</span><label class="collapse" for="c-36335097">[-]</label><label class="expand" for="c-36335097">[1 more]</label></div><br/><div class="children"><div class="content">If an ER doctor printed an AI-generated script and read it to me while I was in an extremely stressful situation I would punch him in the head. This is disgusting.</div><br/></div></div><div id="36335133" class="c"><input type="checkbox" id="c-36335133" checked=""/><div class="controls bullet"><span class="by">elforce002</span><span>|</span><a href="#36335097">prev</a><span>|</span><a href="#36337149">next</a><span>|</span><label class="collapse" for="c-36335133">[-]</label><label class="expand" for="c-36335133">[2 more]</label></div><br/><div class="children"><div class="content">Experience, Knowledge, Accountability, and Human touch. No amount of &quot;AI&quot; is going to change that and I&#x27;m saying that as a software engineer.</div><br/><div id="36335547" class="c"><input type="checkbox" id="c-36335547" checked=""/><div class="controls bullet"><span class="by">talldatethrow</span><span>|</span><a href="#36335133">parent</a><span>|</span><a href="#36337149">next</a><span>|</span><label class="collapse" for="c-36335547">[-]</label><label class="expand" for="c-36335547">[1 more]</label></div><br/><div class="children"><div class="content">As someone that knows several doctors very well (decades of friendship, weekly coffee or drinks), you are really over estimating the average patient doctor outside of basically surgeons.<p>A doctor might have everything you describe. But they don&#x27;t have the time, energy, or desire to spend much time discussing things with patients.<p>Similar to a car mechanic. They know a lot. And if it&#x27;s their car or their father&#x27;s question, they can probably walk you through rebuilding an engine over the phone. But try getting a real answer out of one during a visit.<p>The power of some of these AI bots is that you can query them to your hearts desire until you are really satisfied.</div><br/></div></div></div></div><div id="36337149" class="c"><input type="checkbox" id="c-36337149" checked=""/><div class="controls bullet"><span class="by">camillomiller</span><span>|</span><a href="#36335133">prev</a><span>|</span><a href="#36336511">next</a><span>|</span><label class="collapse" for="c-36337149">[-]</label><label class="expand" for="c-36337149">[1 more]</label></div><br/><div class="children"><div class="content">Another nice movie script written by Quentin Inventino</div><br/></div></div><div id="36336511" class="c"><input type="checkbox" id="c-36336511" checked=""/><div class="controls bullet"><span class="by">bastardoperator</span><span>|</span><a href="#36337149">prev</a><span>|</span><a href="#36336784">next</a><span>|</span><label class="collapse" for="c-36336511">[-]</label><label class="expand" for="c-36336511">[1 more]</label></div><br/><div class="children"><div class="content">TL;DR<p>ChatGPT: &quot;using empathy, tell the family why an iv is bad for someone with fluids in their lungs, keep it short and concise&quot;<p>&quot;I understand your concerns. It&#x27;s tough to see a loved one struggling. When there&#x27;s fluid in the lungs, adding more fluid to the body via IV can make the problem worse. It&#x27;s like adding more water to a cup that&#x27;s already overflowing. The body needs to remove excess fluid, not add more. Your loved one&#x27;s healthcare team is making decisions to help, not harm. Please keep asking questions – it&#x27;s important to understand the process. We&#x27;re all here with the same goal: to ensure the best possible care for your family member.&quot;</div><br/></div></div><div id="36336784" class="c"><input type="checkbox" id="c-36336784" checked=""/><div class="controls bullet"><span class="by">faangiq</span><span>|</span><a href="#36336511">prev</a><span>|</span><a href="#36334539">next</a><span>|</span><label class="collapse" for="c-36336784">[-]</label><label class="expand" for="c-36336784">[1 more]</label></div><br/><div class="children"><div class="content">Absolute nonsense.</div><br/></div></div></div></div></div></div></div></body></html>