<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700384457070" as="style"/><link rel="stylesheet" href="styles.css?v=1700384457070"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence">Meta disbanded its Responsible AI team</a> <span class="domain">(<a href="https://www.theverge.com">www.theverge.com</a>)</span></div><div class="subtext"><span>jo_beef</span> | <span>126 comments</span></div><br/><div><div id="38330073" class="c"><input type="checkbox" id="c-38330073" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38330166">next</a><span>|</span><label class="collapse" for="c-38330073">[-]</label><label class="expand" for="c-38330073">[6 more]</label></div><br/><div class="children"><div class="content">It never made any organizational sense for me to have a &quot;responsible AI team&quot; in the first place. Every team doing AI work should be responsible and should think about the ethical (and legal at a bare minimum baseline) dimension of what they are doing.  Having that concentrated in a single team means that team becomes a bottleneck where they have to vet all AI work everyone else does for responsibility and&#x2F;or everyone else gets a free pass to develop irresponsible AI which doesn&#x27;t sound great to me.<p>At some point AI becomes important enough to a company (and mature enough as a field) that there is a specific part of legal&#x2F;compliance in big companies that deals with the concrete elements of AI ethics and compliance and maybe trains everyone else, but everyone doing AI has to do responsible AI.  It can&#x27;t be a team.<p>For me this is exactly like how big Megacorps have an &quot;Innovation team&quot;[1] and convince themselves that makes them an innovative company. No - if you&#x27;re an innovative company then you foster innovation everywhere. If you have an &quot;innovation team&quot; that&#x27;s where innovation goes to die.<p>[1] In my experience they make a &quot;really cool&quot; floor with couches and everyone thinks it&#x27;s cool to draw on the glass walls of the conference rooms instead of whiteboards.</div><br/><div id="38330613" class="c"><input type="checkbox" id="c-38330613" checked=""/><div class="controls bullet"><span class="by">makeitdouble</span><span>|</span><a href="#38330073">parent</a><span>|</span><a href="#38330548">next</a><span>|</span><label class="collapse" for="c-38330613">[-]</label><label class="expand" for="c-38330613">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it the same as a legal team, another point you touch upon ?<p>I don&#x27;t think we solved the need for a specialized team dealing with legality, feels hard to expect companies to solve it for ethics.</div><br/></div></div><div id="38330548" class="c"><input type="checkbox" id="c-38330548" checked=""/><div class="controls bullet"><span class="by">timkam</span><span>|</span><a href="#38330073">parent</a><span>|</span><a href="#38330613">prev</a><span>|</span><a href="#38330433">next</a><span>|</span><label class="collapse" for="c-38330548">[-]</label><label class="expand" for="c-38330548">[1 more]</label></div><br/><div class="children"><div class="content">Fully agree. Central functions of these types do not scale. Even with more mundane objectives, like operational excellence, organizations have learned that centralization leads to ivory tower nothing-burgers. Most of the resources should go to where the actual work gets done, as little as possible should be managed centrally (perhaps a few ops and thought leadership fluff folks...).</div><br/></div></div><div id="38330433" class="c"><input type="checkbox" id="c-38330433" checked=""/><div class="controls bullet"><span class="by">anytime5704</span><span>|</span><a href="#38330073">parent</a><span>|</span><a href="#38330548">prev</a><span>|</span><a href="#38330529">next</a><span>|</span><label class="collapse" for="c-38330433">[-]</label><label class="expand" for="c-38330433">[1 more]</label></div><br/><div class="children"><div class="content">Is it really that far fetched? It sounds like a self-imposed regulatory group, which some companies&#x2F;industries operate proactively to avoid the ire of government agencies.<p>Yeah, product teams can&#x2F;should care about being responsible, but there’s an obvious conflict of interest.<p>To me, this story means Facebook dgaf about being responsible (big surprise).</div><br/></div></div><div id="38330529" class="c"><input type="checkbox" id="c-38330529" checked=""/><div class="controls bullet"><span class="by">watwut</span><span>|</span><a href="#38330073">parent</a><span>|</span><a href="#38330433">prev</a><span>|</span><a href="#38330306">next</a><span>|</span><label class="collapse" for="c-38330529">[-]</label><label class="expand" for="c-38330529">[1 more]</label></div><br/><div class="children"><div class="content">Everyone should think about it usually means no one will.</div><br/></div></div><div id="38330306" class="c"><input type="checkbox" id="c-38330306" checked=""/><div class="controls bullet"><span class="by">google234123</span><span>|</span><a href="#38330073">parent</a><span>|</span><a href="#38330529">prev</a><span>|</span><a href="#38330166">next</a><span>|</span><label class="collapse" for="c-38330306">[-]</label><label class="expand" for="c-38330306">[1 more]</label></div><br/><div class="children"><div class="content">Also, if you are on this team, you get promoted based on slowing down other work. Introduce a new review process, impact!</div><br/></div></div></div></div><div id="38330166" class="c"><input type="checkbox" id="c-38330166" checked=""/><div class="controls bullet"><span class="by">Simon_ORourke</span><span>|</span><a href="#38330073">prev</a><span>|</span><a href="#38328915">next</a><span>|</span><label class="collapse" for="c-38330166">[-]</label><label class="expand" for="c-38330166">[12 more]</label></div><br/><div class="children"><div class="content">AI is a tool, and there&#x27;s about as much point as having some team fretting about responsible usage as there is having similar notions in a Bazooka manufacturer. Whoever ultimately owns the AI (or the Bazooka) will always dictate how and where the particular tool is used.<p>Many of these AI Ethics foundations (e.g., DAIR), just seem to advocate rent seeking behavior, scraping out a role for themselves off the backs of others who do the actual technical (and indeed ethical) work. I&#x27;m sure the Meta Responsible AI team was staffed with similar semi-literate blowhards, all stance and no actual work.</div><br/><div id="38330396" class="c"><input type="checkbox" id="c-38330396" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#38330166">parent</a><span>|</span><a href="#38330267">next</a><span>|</span><label class="collapse" for="c-38330396">[-]</label><label class="expand" for="c-38330396">[6 more]</label></div><br/><div class="children"><div class="content">Fretting about the responsible use of bio weapons is a waste of time, it’s a weapon and like Bazooka manufactures we don’t need to worry about ethics…<p>See that’s the thing you can say A is like B, but that doesn’t actually make them the same thing.  AI has new implications because it’s a new thing, some of those are overblown, but others need to be carefully considered.  Companies are getting sued for their training data, chances are they’re going to win but lawsuits aren’t free.  Managing such risks ahead of time can be a lot cheaper than yelling yolo and forging ahead.</div><br/><div id="38330443" class="c"><input type="checkbox" id="c-38330443" checked=""/><div class="controls bullet"><span class="by">peyton</span><span>|</span><a href="#38330166">root</a><span>|</span><a href="#38330396">parent</a><span>|</span><a href="#38330267">next</a><span>|</span><label class="collapse" for="c-38330443">[-]</label><label class="expand" for="c-38330443">[5 more]</label></div><br/><div class="children"><div class="content">We’re talking about making sure chatbots don’t say “nigger,” not bioweapons. At some point you need to trust that the people using the tools are adults.</div><br/><div id="38330493" class="c"><input type="checkbox" id="c-38330493" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#38330166">root</a><span>|</span><a href="#38330443">parent</a><span>|</span><a href="#38330604">next</a><span>|</span><label class="collapse" for="c-38330493">[-]</label><label class="expand" for="c-38330493">[2 more]</label></div><br/><div class="children"><div class="content">Facebook has far bigger issues than that, such as peoples medical information getting released or potentially getting it wrong.  Privacy might not be well protected in the US but defamation lawsuits are no joke.  So training on people’s private chat history isn’t necessarily safe.<p>Even just the realization that ‘Logs from a chatbot conversation can go viral’ has actual real world implications.</div><br/></div></div><div id="38330604" class="c"><input type="checkbox" id="c-38330604" checked=""/><div class="controls bullet"><span class="by">fatherzine</span><span>|</span><a href="#38330166">root</a><span>|</span><a href="#38330443">parent</a><span>|</span><a href="#38330493">prev</a><span>|</span><a href="#38330608">next</a><span>|</span><label class="collapse" for="c-38330604">[-]</label><label class="expand" for="c-38330604">[1 more]</label></div><br/><div class="children"><div class="content">not sure it&#x27;s fair to trivialize ai risks. for just one example, the pen is mightier than the sword.</div><br/></div></div><div id="38330608" class="c"><input type="checkbox" id="c-38330608" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#38330166">root</a><span>|</span><a href="#38330443">parent</a><span>|</span><a href="#38330604">prev</a><span>|</span><a href="#38330267">next</a><span>|</span><label class="collapse" for="c-38330608">[-]</label><label class="expand" for="c-38330608">[1 more]</label></div><br/><div class="children"><div class="content">&gt; At some point you need to trust that the people using the tools are adults.<p>Ah yes. Let&#x27;s see:<p>- invasive and pervasive tracking<p>- social credit scores<p>- surveillance [1]<p>All done by adults, no need to worry<p>[1] Just one such story: <a href="https:&#x2F;&#x2F;www.404media.co&#x2F;fusus-ai-cameras-took-over-town-america&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.404media.co&#x2F;fusus-ai-cameras-took-over-town-amer...</a></div><br/></div></div></div></div></div></div><div id="38330267" class="c"><input type="checkbox" id="c-38330267" checked=""/><div class="controls bullet"><span class="by">sureglymop</span><span>|</span><a href="#38330166">parent</a><span>|</span><a href="#38330396">prev</a><span>|</span><a href="#38330555">next</a><span>|</span><label class="collapse" for="c-38330267">[-]</label><label class="expand" for="c-38330267">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not really the same as a bazooka.
These companies usually release AI models, for which the training phase is arguably more important than the usage phase when it comes to ethics.
It would be like if the manufacturer pre-calibrated the bazooka for a certain target. Sure, whoever uses it after may still use it in another, unethical way but the point is there is already a bias.
It is important to consider ethical implications of the training materials used, especially when scraping the internet for material. Now, is a whole team needed? Maybe not, but you can&#x27;t dismiss it that easily.</div><br/></div></div><div id="38330555" class="c"><input type="checkbox" id="c-38330555" checked=""/><div class="controls bullet"><span class="by">makeitdouble</span><span>|</span><a href="#38330166">parent</a><span>|</span><a href="#38330267">prev</a><span>|</span><a href="#38330554">next</a><span>|</span><label class="collapse" for="c-38330555">[-]</label><label class="expand" for="c-38330555">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Whoever ultimately owns the AI (or the Bazooka) will always dictate how and where the particular tool is used.<p>Your take confuses me, because in this case the owner is Meta. So yes, they have to think about what tools they make (&quot;should we design a bazooka&quot;) and how they&#x27;ll use what they made (&quot;what&#x27;s the target and when to pull the trigger ?&quot;)<p>They disbanded the team that was tasked with thinking about both.<p>From the article:<p>&gt; RAI was created to identify problems with its AI training approaches, including whether the company’s models are trained with adequately diverse information, with an eye toward preventing things like moderation issues on its platforms. Automated systems on Meta’s social platforms have led to problems like a Facebook translation issue that caused a false arrest</div><br/></div></div><div id="38330554" class="c"><input type="checkbox" id="c-38330554" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#38330166">parent</a><span>|</span><a href="#38330555">prev</a><span>|</span><a href="#38330229">next</a><span>|</span><label class="collapse" for="c-38330554">[-]</label><label class="expand" for="c-38330554">[2 more]</label></div><br/><div class="children"><div class="content">I dislike the weapon analogy, because it implies that proliferation of AI (ergo everyone running an LLM on their PCs for code completion or for the ability to speak to a home assistant) is akin to everybody having a cache of unlicensed firearms.<p>It has been the agenda of most FAANG corporations (with the notable exception of Apple) to turn the computers average people own into mere thin clients with all the computing resources.<p>Luckily, before the cloud era, the idea that people can and should own powerful personal computers was the normal. If PCs were invented today, I guess there would be people raising ethical concerns about regular citizens owning PCs that can hack into NASA.</div><br/><div id="38330643" class="c"><input type="checkbox" id="c-38330643" checked=""/><div class="controls bullet"><span class="by">JoshuaDavid</span><span>|</span><a href="#38330166">root</a><span>|</span><a href="#38330554">parent</a><span>|</span><a href="#38330229">next</a><span>|</span><label class="collapse" for="c-38330643">[-]</label><label class="expand" for="c-38330643">[1 more]</label></div><br/><div class="children"><div class="content">There were in fact concerns about normal people having access to strong cryptography <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Crypto_Wars" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Crypto_Wars</a></div><br/></div></div></div></div><div id="38330229" class="c"><input type="checkbox" id="c-38330229" checked=""/><div class="controls bullet"><span class="by">petters</span><span>|</span><a href="#38330166">parent</a><span>|</span><a href="#38330554">prev</a><span>|</span><a href="#38328915">next</a><span>|</span><label class="collapse" for="c-38330229">[-]</label><label class="expand" for="c-38330229">[1 more]</label></div><br/><div class="children"><div class="content">If the bazooka manufacturer only offers shooting rockets through an API where they can check the target before launching, they would be able to have some say about which targets are hit.<p>&gt; Whoever ultimately owns the AI (or the Bazooka)<p>This is not the user in most cases. So a responsible AI can make sense. I believe you don&#x27;t think AI can be dangerous, but some people do and from their point of view having a team for this makes sense.</div><br/></div></div></div></div><div id="38328915" class="c"><input type="checkbox" id="c-38328915" checked=""/><div class="controls bullet"><span class="by">RcouF1uZ4gsC</span><span>|</span><a href="#38330166">prev</a><span>|</span><a href="#38329372">next</a><span>|</span><label class="collapse" for="c-38328915">[-]</label><label class="expand" for="c-38328915">[15 more]</label></div><br/><div class="children"><div class="content">Because Meta is releasing their models to the public, I consider them the most ethical company doing AI at scale.<p>Keeping AI models closed under the guise of “ethics”, is I think the most unethical stance as it makes people more dependent on the arbitrary decisions, goals, and priorities of big companies, instead being allowed to define “alignment” for themselves.</div><br/><div id="38330374" class="c"><input type="checkbox" id="c-38330374" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#38328915">parent</a><span>|</span><a href="#38329341">next</a><span>|</span><label class="collapse" for="c-38330374">[-]</label><label class="expand" for="c-38330374">[7 more]</label></div><br/><div class="children"><div class="content">Kevin Esvelt says open source models could soon be used by terrorists to create bioweapons.<p><a href="https:&#x2F;&#x2F;nitter.net&#x2F;kesvelt&#x2F;status&#x2F;1720440451059335520" rel="nofollow noreferrer">https:&#x2F;&#x2F;nitter.net&#x2F;kesvelt&#x2F;status&#x2F;1720440451059335520</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kevin_M._Esvelt" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kevin_M._Esvelt</a></div><br/><div id="38330395" class="c"><input type="checkbox" id="c-38330395" checked=""/><div class="controls bullet"><span class="by">dmw_ng</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38330374">parent</a><span>|</span><a href="#38330441">next</a><span>|</span><label class="collapse" for="c-38330395">[-]</label><label class="expand" for="c-38330395">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s been instructions for manufacturing weapons useful for terrorism floating around since the BBS days, nothing new here</div><br/><div id="38330481" class="c"><input type="checkbox" id="c-38330481" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38330395">parent</a><span>|</span><a href="#38330441">next</a><span>|</span><label class="collapse" for="c-38330481">[-]</label><label class="expand" for="c-38330481">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a big difference when you have an expert which you can ask questions.</div><br/><div id="38330583" class="c"><input type="checkbox" id="c-38330583" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38330481">parent</a><span>|</span><a href="#38330441">next</a><span>|</span><label class="collapse" for="c-38330583">[-]</label><label class="expand" for="c-38330583">[2 more]</label></div><br/><div class="children"><div class="content">Terrorists already have all the information they need to build some heinous shit with ~no external guidance aside from what&#x27;s already on the internet.</div><br/><div id="38330622" class="c"><input type="checkbox" id="c-38330622" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38330583">parent</a><span>|</span><a href="#38330441">next</a><span>|</span><label class="collapse" for="c-38330622">[-]</label><label class="expand" for="c-38330622">[1 more]</label></div><br/><div class="children"><div class="content">Engineered viruses could cause far more deaths than conventional weapons. Even more than nuclear weapons, and they are easier to manufacture.</div><br/></div></div></div></div></div></div></div></div><div id="38330441" class="c"><input type="checkbox" id="c-38330441" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38330374">parent</a><span>|</span><a href="#38330395">prev</a><span>|</span><a href="#38329341">next</a><span>|</span><label class="collapse" for="c-38330441">[-]</label><label class="expand" for="c-38330441">[2 more]</label></div><br/><div class="children"><div class="content">Seriously? This is just silly. Everyone knows the barrier to terrorists using bio weapons is not specialist knowledge, but access to labs, equipment, reagents etc.<p>It&#x27;s the whole Guttenberg&#x27;s printing press argument. &quot;Whoaa hold on now, what do you mean you want knowledge to be freely available to the vulgar masses?&quot;<p>The only difference with LLMs is that you do not have to search for this knowledge by yourself, you get a very much hallucination prone AI to tell you the answers. If we extend this argument further why don&#x27;t we restrict access to public libraries, scientific research and neuter Google even more. And what about Wikipedia?</div><br/><div id="38330494" class="c"><input type="checkbox" id="c-38330494" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38330441">parent</a><span>|</span><a href="#38329341">next</a><span>|</span><label class="collapse" for="c-38330494">[-]</label><label class="expand" for="c-38330494">[1 more]</label></div><br/><div class="children"><div class="content">Neither Wikipedia nor public libraries allow instructions to make weapons of mass destruction.</div><br/></div></div></div></div></div></div><div id="38329341" class="c"><input type="checkbox" id="c-38329341" checked=""/><div class="controls bullet"><span class="by">wilsonnb3</span><span>|</span><a href="#38328915">parent</a><span>|</span><a href="#38330374">prev</a><span>|</span><a href="#38329230">next</a><span>|</span><label class="collapse" for="c-38329341">[-]</label><label class="expand" for="c-38329341">[2 more]</label></div><br/><div class="children"><div class="content">&gt; instead being allowed to define “alignment” for themselves.<p>Yeah, that is the whole point - not wanting bad actors to be able to define &quot;alignment&quot; for themselves.<p>Not sure how that is unethical.</div><br/></div></div><div id="38329230" class="c"><input type="checkbox" id="c-38329230" checked=""/><div class="controls bullet"><span class="by">o11c</span><span>|</span><a href="#38328915">parent</a><span>|</span><a href="#38329341">prev</a><span>|</span><a href="#38330410">next</a><span>|</span><label class="collapse" for="c-38329230">[-]</label><label class="expand" for="c-38329230">[2 more]</label></div><br/><div class="children"><div class="content">Exactly this.<p>There certainly needs to be regulation about <i>use</i> of AI to make decisions without sufficient human supervision (which has already proven a problem with prior systems), and someone will have to make a decision about copyright eventually, but closing the models off does absolutely <i>nothing</i> to protect anyone.</div><br/><div id="38330394" class="c"><input type="checkbox" id="c-38330394" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38329230">parent</a><span>|</span><a href="#38330410">next</a><span>|</span><label class="collapse" for="c-38330394">[-]</label><label class="expand" for="c-38330394">[1 more]</label></div><br/><div class="children"><div class="content">Exactly this.<p>There certainly needs to be regulation about use of bioweapons without sufficient human supervision (which has already proven a problem with prior systems), and someone will have to make a decision about synthetic viruses, but closing the gain of function labs does absolutely nothing to protect anyone.</div><br/></div></div></div></div><div id="38330410" class="c"><input type="checkbox" id="c-38330410" checked=""/><div class="controls bullet"><span class="by">unicornmama</span><span>|</span><a href="#38328915">parent</a><span>|</span><a href="#38329230">prev</a><span>|</span><a href="#38329468">next</a><span>|</span><label class="collapse" for="c-38330410">[-]</label><label class="expand" for="c-38330410">[2 more]</label></div><br/><div class="children"><div class="content">Meta’s products have damaged and continue to damage the mental health of hundreds of millions of people, including young children and teenagers.<p>Whatever their motivation to release models, it’s a for-profit business tactic first. Any ethical spin is varnish that was decided after the fact to promote Meta to its employees and the general public.</div><br/><div id="38330600" class="c"><input type="checkbox" id="c-38330600" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#38328915">root</a><span>|</span><a href="#38330410">parent</a><span>|</span><a href="#38329468">next</a><span>|</span><label class="collapse" for="c-38330600">[-]</label><label class="expand" for="c-38330600">[1 more]</label></div><br/><div class="children"><div class="content">pretty sure this comes down to bad parenting and social media being relatively new on the human timeline - teething pains are to be expected</div><br/></div></div></div></div><div id="38329468" class="c"><input type="checkbox" id="c-38329468" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#38328915">parent</a><span>|</span><a href="#38330410">prev</a><span>|</span><a href="#38329372">next</a><span>|</span><label class="collapse" for="c-38329468">[-]</label><label class="expand" for="c-38329468">[1 more]</label></div><br/><div class="children"><div class="content">Exactly.<p>I can&#x27;t speak about meta specifically, but from my exposure &quot;responsible ai&quot; are generally policy doomers with a heavy pro-control pro-limits perspective, or even worse-- psycho cultists that believe the only safe objective for AI work is the development of an electronic god to impose their own moral will on the world.<p>Either of those options are incompatible with actually ethical behavior, like assuring that the public has access instead of keeping it exclusive to a priesthood that hopes to weaponize the technology against the public &#x27;for the public&#x27;s own good&#x27;.</div><br/></div></div></div></div><div id="38329372" class="c"><input type="checkbox" id="c-38329372" checked=""/><div class="controls bullet"><span class="by">g96alqdm0x</span><span>|</span><a href="#38328915">prev</a><span>|</span><a href="#38329002">next</a><span>|</span><label class="collapse" for="c-38329372">[-]</label><label class="expand" for="c-38329372">[9 more]</label></div><br/><div class="children"><div class="content">How convenient! Turns out they don’t give the slightest damn about “Responsible AI” in the first place. It’s nice to roll out news like this while everyone else is distracted.</div><br/><div id="38329875" class="c"><input type="checkbox" id="c-38329875" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#38329372">parent</a><span>|</span><a href="#38329002">next</a><span>|</span><label class="collapse" for="c-38329875">[-]</label><label class="expand" for="c-38329875">[8 more]</label></div><br/><div class="children"><div class="content">Meta is probably the most ethical company in AI at the moment. Most importantly, their models are open source.</div><br/><div id="38330372" class="c"><input type="checkbox" id="c-38330372" checked=""/><div class="controls bullet"><span class="by">ikari_pl</span><span>|</span><a href="#38329372">root</a><span>|</span><a href="#38329875">parent</a><span>|</span><a href="#38330077">next</a><span>|</span><label class="collapse" for="c-38330372">[-]</label><label class="expand" for="c-38330372">[2 more]</label></div><br/><div class="children"><div class="content">are they an ethical company, though?</div><br/><div id="38330544" class="c"><input type="checkbox" id="c-38330544" checked=""/><div class="controls bullet"><span class="by">OezMaster</span><span>|</span><a href="#38329372">root</a><span>|</span><a href="#38330372">parent</a><span>|</span><a href="#38330077">next</a><span>|</span><label class="collapse" for="c-38330544">[-]</label><label class="expand" for="c-38330544">[1 more]</label></div><br/><div class="children"><div class="content">Is one division responsible for the crimes of another division, especially in a large corporation?</div><br/></div></div></div></div><div id="38330077" class="c"><input type="checkbox" id="c-38330077" checked=""/><div class="controls bullet"><span class="by">andrewedstrom</span><span>|</span><a href="#38329372">root</a><span>|</span><a href="#38329875">parent</a><span>|</span><a href="#38330372">prev</a><span>|</span><a href="#38329987">next</a><span>|</span><label class="collapse" for="c-38330077">[-]</label><label class="expand" for="c-38330077">[2 more]</label></div><br/><div class="children"><div class="content">You contradict yourself</div><br/><div id="38330528" class="c"><input type="checkbox" id="c-38330528" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#38329372">root</a><span>|</span><a href="#38330077">parent</a><span>|</span><a href="#38329987">next</a><span>|</span><label class="collapse" for="c-38330528">[-]</label><label class="expand" for="c-38330528">[1 more]</label></div><br/><div class="children"><div class="content">You think open sourcing your models isn&#x27;t ethical?</div><br/></div></div></div></div><div id="38329987" class="c"><input type="checkbox" id="c-38329987" checked=""/><div class="controls bullet"><span class="by">MattHeard</span><span>|</span><a href="#38329372">root</a><span>|</span><a href="#38329875">parent</a><span>|</span><a href="#38330077">prev</a><span>|</span><a href="#38329002">next</a><span>|</span><label class="collapse" for="c-38329987">[-]</label><label class="expand" for="c-38329987">[3 more]</label></div><br/><div class="children"><div class="content">Maybe this news should challenge your priors, then?</div><br/><div id="38330563" class="c"><input type="checkbox" id="c-38330563" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#38329372">root</a><span>|</span><a href="#38329987">parent</a><span>|</span><a href="#38330240">next</a><span>|</span><label class="collapse" for="c-38330563">[-]</label><label class="expand" for="c-38330563">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s assuming this division actually did something beneficial to begin with, and if they did, that they are the only ones responsible for &quot;responsible AI&quot; development at Meta. It is in all likelihood just a re-org being blown out of proportion.</div><br/></div></div></div></div></div></div></div></div><div id="38329002" class="c"><input type="checkbox" id="c-38329002" checked=""/><div class="controls bullet"><span class="by">luigi23</span><span>|</span><a href="#38329372">prev</a><span>|</span><a href="#38330287">next</a><span>|</span><label class="collapse" for="c-38329002">[-]</label><label class="expand" for="c-38329002">[1 more]</label></div><br/><div class="children"><div class="content">When moneys out and theres a fire going on (at openai), its the best moment to close departments that were solely for virtue signaling :&#x2F;</div><br/></div></div><div id="38330287" class="c"><input type="checkbox" id="c-38330287" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#38329002">prev</a><span>|</span><a href="#38329016">next</a><span>|</span><label class="collapse" for="c-38330287">[-]</label><label class="expand" for="c-38330287">[1 more]</label></div><br/><div class="children"><div class="content">Responsible AI should be team oriented in the first place, each project has very different security objective</div><br/></div></div><div id="38329016" class="c"><input type="checkbox" id="c-38329016" checked=""/><div class="controls bullet"><span class="by">speedylight</span><span>|</span><a href="#38330287">prev</a><span>|</span><a href="#38330365">next</a><span>|</span><label class="collapse" for="c-38329016">[-]</label><label class="expand" for="c-38329016">[30 more]</label></div><br/><div class="children"><div class="content">I honestly believe the best to make AI responsibly is to make it open source. That way no single entity has total control over it, and researchers can study them to better understand how they can be used nefariously as well as in a good way—doing that allows us to build defenses to minimize the risks, and reap the benefits. Meta is already doing that, but other companies and organizations should do that as well.</div><br/><div id="38330187" class="c"><input type="checkbox" id="c-38330187" checked=""/><div class="controls bullet"><span class="by">martindbp</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38329051">next</a><span>|</span><label class="collapse" for="c-38330187">[-]</label><label class="expand" for="c-38330187">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a doomer but I honestly don&#x27;t understand this argument. If releasing model as open source helps researchers determine if it&#x27;s safe, what about when it&#x27;s not deemed safe? Then it&#x27;s already out there, on the hard drives of half of 4chan. It&#x27;s much easier and cheaper to fine-tune a model, distil and quantize it and put it on a killer drone, than it is to train it from scratch.<p>On the other hand I totally relate with the idea that it could be preferable that everyday has access to advanced AI and not just large companies and nation states.</div><br/><div id="38330320" class="c"><input type="checkbox" id="c-38330320" checked=""/><div class="controls bullet"><span class="by">123yawaworht456</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38330187">parent</a><span>|</span><a href="#38329051">next</a><span>|</span><label class="collapse" for="c-38330320">[-]</label><label class="expand" for="c-38330320">[2 more]</label></div><br/><div class="children"><div class="content">what purpose does a LLM serve on a killing drone, exactly?</div><br/><div id="38330406" class="c"><input type="checkbox" id="c-38330406" checked=""/><div class="controls bullet"><span class="by">martindbp</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38330320">parent</a><span>|</span><a href="#38329051">next</a><span>|</span><label class="collapse" for="c-38330406">[-]</label><label class="expand" for="c-38330406">[1 more]</label></div><br/><div class="children"><div class="content">Open source models in general. Meta has for instance released DINO which is a self supervised transformer model. LLMs are also going multi modal (see LLaVA for instance). The name &quot;LLM&quot; has stuck but they should really be called Large Transformer Models. LeCun is working on self supervised visual world models (I-JEPA) which if successful and released could form the basis for killer drones.</div><br/></div></div></div></div></div></div><div id="38329051" class="c"><input type="checkbox" id="c-38329051" checked=""/><div class="controls bullet"><span class="by">Ericson2314</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38330187">prev</a><span>|</span><a href="#38329195">next</a><span>|</span><label class="collapse" for="c-38329051">[-]</label><label class="expand" for="c-38329051">[4 more]</label></div><br/><div class="children"><div class="content">Getting the results is nice but that&#x27;s &quot;shareware&quot; not &quot;free software&quot; (or, for a more modern example, that is like companies submitting firmware binary blobs into mainline Linux).<p>Free software means you have to be able to build the final binary from source. Having 10 TB of text is no problem, but having a data center of GPUs is. Until the training cost comes down there is no way to make it free software.</div><br/><div id="38329633" class="c"><input type="checkbox" id="c-38329633" checked=""/><div class="controls bullet"><span class="by">l33t7332273</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329051">parent</a><span>|</span><a href="#38329195">next</a><span>|</span><label class="collapse" for="c-38329633">[-]</label><label class="expand" for="c-38329633">[3 more]</label></div><br/><div class="children"><div class="content">If I publish a massive quantity of source code — to the point that it’s very expensive to compile — it’s still open source.<p>If the training data and model training code is available then it should be considered open, even if it’s hard to train.</div><br/><div id="38329990" class="c"><input type="checkbox" id="c-38329990" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329633">parent</a><span>|</span><a href="#38329195">next</a><span>|</span><label class="collapse" for="c-38329990">[-]</label><label class="expand" for="c-38329990">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the training data<p>This will never be fully open</div><br/><div id="38330132" class="c"><input type="checkbox" id="c-38330132" checked=""/><div class="controls bullet"><span class="by">l33t7332273</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329990">parent</a><span>|</span><a href="#38329195">next</a><span>|</span><label class="collapse" for="c-38330132">[-]</label><label class="expand" for="c-38330132">[1 more]</label></div><br/><div class="children"><div class="content">Maybe not for some closed models. That doesn’t mean truly open models can’t exist.</div><br/></div></div></div></div></div></div></div></div><div id="38329195" class="c"><input type="checkbox" id="c-38329195" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38329051">prev</a><span>|</span><a href="#38329363">next</a><span>|</span><label class="collapse" for="c-38329195">[-]</label><label class="expand" for="c-38329195">[6 more]</label></div><br/><div class="children"><div class="content">GNU&#x2F;Linux is open source. Is it being used responsibly?<p>What is the &quot;it&quot; that no single entity has control over?<p>You have absolutely no control of what your next door neighbor is doing with open source.<p>Hey, if we want alcohol to be made responsibly, everyone should have their own still, made from freely redistributed blueprints. That way no single entity has control.</div><br/><div id="38329635" class="c"><input type="checkbox" id="c-38329635" checked=""/><div class="controls bullet"><span class="by">JoshuaDavid</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329195">parent</a><span>|</span><a href="#38329915">next</a><span>|</span><label class="collapse" for="c-38329635">[-]</label><label class="expand" for="c-38329635">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Hey, if we want alcohol to be made responsibly, everyone should have their own still, made from freely redistributed blueprints.<p>Anyone who wants to can, in fact, find blueprints for making their own still. For example, <a href="https:&#x2F;&#x2F;moonshinestillplans.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;moonshinestillplans.com&#x2F;</a> contains plans for a variety of different types of stills and guidance on which type to build based on how you want to use it.<p>And in fact I think it&#x27;s good that this site exists, because it&#x27;s very easy to build a still that appears to work but actually leaves you with a high-methanol end product.</div><br/><div id="38330256" class="c"><input type="checkbox" id="c-38330256" checked=""/><div class="controls bullet"><span class="by">code_biologist</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329635">parent</a><span>|</span><a href="#38329915">next</a><span>|</span><label class="collapse" for="c-38330256">[-]</label><label class="expand" for="c-38330256">[1 more]</label></div><br/><div class="children"><div class="content"><i>it&#x27;s very easy to build a still that appears to work but actually leaves you with a high-methanol end product.</i><p>Is it? I&#x27;ve always seen concern about methanol in moonshine but I presume it came from intentional contamination from evil bootleggers. It&#x27;s difficult to get a wash containing enough methanol to meaningfully concentrate in the first place if you&#x27;re making whiskey or rum. Maybe with fruit wine and hard cider there&#x27;s a bit more.<p>The physics of distillation kind of have your back here too. The lower temperature fractions with acetone and methanol always come out first during distillation (the &quot;heads&quot;) and every resource and distiller will tell you to learn the taste and smell, then throw them out. The taste and smell of heads are really distinctive. A slow distillation to more effectively concentrate methanol also makes it easier to separate out. But even if you don&#x27;t separate the heads from the hearts, the methanol in any traditional wash is dilute enough that it&#x27;ll only give you a headache.<p>I think it&#x27;s extremely hard to build a still that appears to work but creates a high methanol end product.</div><br/></div></div></div></div><div id="38329915" class="c"><input type="checkbox" id="c-38329915" checked=""/><div class="controls bullet"><span class="by">didibus</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329195">parent</a><span>|</span><a href="#38329635">prev</a><span>|</span><a href="#38329788">next</a><span>|</span><label class="collapse" for="c-38329915">[-]</label><label class="expand" for="c-38329915">[1 more]</label></div><br/><div class="children"><div class="content">I think the question mark is if AI is more akin to the nuclear bomb of the Internet.<p>If you don&#x27;t put barriers, how quickly will AI bots take over people in online discourse, interaction and publication?<p>This isn&#x27;t just for the sake of keeping the Internet an interesting place free of bots and fraud and all that.<p>But I&#x27;ve also heard that it&#x27;s about improving AI itself. If AI starts to pollute the dataset we train AI on, the entire Internet, you get this weird feedback loop where the models could almost get worse over time, as they will start to unknowingly train on things their older versions produced.</div><br/></div></div><div id="38329788" class="c"><input type="checkbox" id="c-38329788" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329195">parent</a><span>|</span><a href="#38329915">prev</a><span>|</span><a href="#38329624">next</a><span>|</span><label class="collapse" for="c-38329788">[-]</label><label class="expand" for="c-38329788">[1 more]</label></div><br/><div class="children"><div class="content">Alcohol is probably the most open-source food product of all time.</div><br/></div></div><div id="38329624" class="c"><input type="checkbox" id="c-38329624" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329195">parent</a><span>|</span><a href="#38329788">prev</a><span>|</span><a href="#38329363">next</a><span>|</span><label class="collapse" for="c-38329624">[-]</label><label class="expand" for="c-38329624">[1 more]</label></div><br/><div class="children"><div class="content">&gt; GNU&#x2F;Linux is open source. Is it being used responsibly?<p>Great example!  Yes, linux being open source has been massively beneficial to society.  And this is true despite the fact that some bad guys use computers as well.</div><br/></div></div></div></div><div id="38329363" class="c"><input type="checkbox" id="c-38329363" checked=""/><div class="controls bullet"><span class="by">absrec</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38329195">prev</a><span>|</span><a href="#38330070">next</a><span>|</span><label class="collapse" for="c-38329363">[-]</label><label class="expand" for="c-38329363">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. The biggest question is why you would trust the single authority controlling the AI to be responsible. If there are enough random variables the good and the bad sort of cancel each other out to reach a happy neutral. But if an authority goes rogue what are you gonna do?<p>Making it open is the only way AI fulfills a power to the people goal. Without open source and locally trainable models AI is just more power to the big-tech industry&#x27;s authorities.</div><br/></div></div><div id="38330070" class="c"><input type="checkbox" id="c-38330070" checked=""/><div class="controls bullet"><span class="by">reocha</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38329363">prev</a><span>|</span><a href="#38329910">next</a><span>|</span><label class="collapse" for="c-38330070">[-]</label><label class="expand" for="c-38330070">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;j3s.sh&#x2F;thought&#x2F;drones-run-linux-free-software-isnt-enough.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;j3s.sh&#x2F;thought&#x2F;drones-run-linux-free-software-isnt-e...</a></div><br/></div></div><div id="38329910" class="c"><input type="checkbox" id="c-38329910" checked=""/><div class="controls bullet"><span class="by">jeffreygoesto</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38330070">prev</a><span>|</span><a href="#38329109">next</a><span>|</span><label class="collapse" for="c-38329910">[-]</label><label class="expand" for="c-38329910">[1 more]</label></div><br/><div class="children"><div class="content">If it really is A&quot;I&quot;, shouldn&#x27;t it figure out for itself and do it?</div><br/></div></div><div id="38329109" class="c"><input type="checkbox" id="c-38329109" checked=""/><div class="controls bullet"><span class="by">deanCommie</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38329910">prev</a><span>|</span><a href="#38329525">next</a><span>|</span><label class="collapse" for="c-38329109">[-]</label><label class="expand" for="c-38329109">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not necessarily true.<p>It&#x27;s entirely conceivable that even if AGI (or something comparably significant in terms of how impactful it would be to changing society or nation states) was achievable in our lifetime, it might be that:<p>1) Achieving it requires a critical mass of research talent in one place that perhaps currently exists at fewer than 5 companies - anecdotally only Google, Meta, and OpenAI. And a comparable number of world governments (At least in the US the best researchers in this field are at these companies, not in academia or government. China may be different.)<p>This makes it sound like a &quot;security by obscurity&quot; situation, and on a long enough timeline it may be. Without World War 2, without the Manhattan Project, and without the looming Cold War how long would it have taken for Humanity to construct a nuclear bomb? An extra 10 years? 20? 50? Hard to know. Regardless, there is a possibility that for things like AI, with extra time comes the ability to better understand and build those defenses <i>before</i> they&#x27;re needed.<p>2) It might also require an amount of computing capacity that only a dozen companies&#x2F;governments have.<p>If you open source all the work you remove the guard rails for the growth or what people focus investments on. It also means that hostile nations like Iran or North Korea who may not have the research talent but could acquire the raw compute could utilize it for unknown goals.<p>Not to mention that what nefarious parties on the internet would use it for. We only know about deep fake porn and generated vocal audio of family members for extortion. Things can get much much worse.</div><br/><div id="38329155" class="c"><input type="checkbox" id="c-38329155" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329109">parent</a><span>|</span><a href="#38329525">next</a><span>|</span><label class="collapse" for="c-38329155">[-]</label><label class="expand" for="c-38329155">[1 more]</label></div><br/><div class="children"><div class="content">&gt; there is a possibility that for things like AI, with extra time comes the ability to better understand and build those defenses before they&#x27;re needed.<p>Or not, and damaging wrongheaded ideas will become a self-reinforcing (because safety! humanity is at stake!) orthodoxy, leaving us completely butt-naked before actual risks once somebody makes a sudden clandestine breakthrough.<p><a href="https:&#x2F;&#x2F;bounded-regret.ghost.io&#x2F;ai-pause-will-likely-backfire-by-nora&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bounded-regret.ghost.io&#x2F;ai-pause-will-likely-backfir...</a><p>&gt; We don’t need to speculate about what would happen to AI alignment research during a pause—we can look at the historical record. Before the launch of GPT-3 in 2020, the alignment community had nothing even remotely like a general intelligence to empirically study, and spent its time doing theoretical research, engaging in philosophical arguments on LessWrong, and occasionally performing toy experiments in reinforcement learning.<p>&gt; The Machine Intelligence Research Institute (MIRI), which was at the forefront of theoretical AI safety research during this period, has since admitted that its efforts have utterly failed. Other agendas, such as “assistance games”, are still being actively pursued but have not been significantly integrated into modern deep learning systems— see Rohin Shah’s review here, as well as Alex Turner’s comments here. Finally, Nick Bostrom’s argument in Superintelligence, that value specification is the fundamental challenge to safety, seems dubious in light of LLM&#x27;s ability to perform commonsense reasoning.[2]<p>&gt; At best, these theory-first efforts did very little to improve our understanding of how to align powerful AI. And they may have been net negative, insofar as they propagated a variety of actively misleading ways of thinking both among alignment researchers and the broader public. Some examples include the now-debunked analogy from evolution, the false distinction between “inner” and “outer” alignment, and the idea that AIs will be rigid utility maximizing consequentialists (here, here, and here).<p>&gt; During an AI pause, I expect alignment research would enter another “winter” in which progress stalls, and plausible-sounding-but-false speculations become entrenched as orthodoxy without empirical evidence to falsify them. While some good work would of course get done, it’s not clear that the field would be better off as a whole. And even if a pause would be net positive for alignment research, it would likely be net negative for humanity’s future all things considered, due to the pause’s various unintended consequences. We’ll look at that in detail in the final section of the essay.</div><br/></div></div></div></div><div id="38329525" class="c"><input type="checkbox" id="c-38329525" checked=""/><div class="controls bullet"><span class="by">systemvoltage</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38329109">prev</a><span>|</span><a href="#38329075">next</a><span>|</span><label class="collapse" for="c-38329525">[-]</label><label class="expand" for="c-38329525">[2 more]</label></div><br/><div class="children"><div class="content">Is it just the model that needs to be open source?<p>I thought the big secret sauce is the sources of data that is used to train the models. Without this, the model itself is useless quite literally.</div><br/><div id="38329570" class="c"><input type="checkbox" id="c-38329570" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329525">parent</a><span>|</span><a href="#38329075">next</a><span>|</span><label class="collapse" for="c-38329570">[-]</label><label class="expand" for="c-38329570">[1 more]</label></div><br/><div class="children"><div class="content">No, the model is useful without the dataset, but its not functionally &quot;open source&quot;, because while you can tune it if you have the training code, you can&#x27;t replicate it or, more important, train it from scratch with a modified, but not completely <i>new</i>, dataset. (And, also, understanding the existing training data helps understand how to <i>structure</i> data to train that particular model, whether its with a new or modified data set from scratch, or for finetuning.)<p>At least, that&#x27;s my understanding.</div><br/></div></div></div></div><div id="38329075" class="c"><input type="checkbox" id="c-38329075" checked=""/><div class="controls bullet"><span class="by">gjsman-1000</span><span>|</span><a href="#38329016">parent</a><span>|</span><a href="#38329525">prev</a><span>|</span><a href="#38329060">next</a><span>|</span><label class="collapse" for="c-38329075">[-]</label><label class="expand" for="c-38329075">[8 more]</label></div><br/><div class="children"><div class="content">Great, Russia and China get the ability to use it or adapt it for any reason they want without any oversight.</div><br/><div id="38329201" class="c"><input type="checkbox" id="c-38329201" checked=""/><div class="controls bullet"><span class="by">bcherny</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329075">parent</a><span>|</span><a href="#38329257">next</a><span>|</span><label class="collapse" for="c-38329201">[-]</label><label class="expand" for="c-38329201">[2 more]</label></div><br/><div class="children"><div class="content">One could argue that open source won’t change much with regard to China and Russia.<p>Both countries have access to LLMs already. And if they didn’t, they would have built their own or gotten access through corporate espionage.<p>What open source does is it helps us better understand &amp; control the tech these countries use. And it helps level up our own homegrown tech. Both of these are good advantages to have.</div><br/><div id="38330333" class="c"><input type="checkbox" id="c-38330333" checked=""/><div class="controls bullet"><span class="by">mdhb</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329201">parent</a><span>|</span><a href="#38329257">next</a><span>|</span><label class="collapse" for="c-38330333">[-]</label><label class="expand" for="c-38330333">[1 more]</label></div><br/><div class="children"><div class="content">That last paragraph is an opinion you seem to have just formed as you typed it stated as a fact that doesn’t seem to hold up to even the lightest scrutiny.</div><br/></div></div></div></div><div id="38329257" class="c"><input type="checkbox" id="c-38329257" checked=""/><div class="controls bullet"><span class="by">malwrar</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329075">parent</a><span>|</span><a href="#38329201">prev</a><span>|</span><a href="#38329271">next</a><span>|</span><label class="collapse" for="c-38329257">[-]</label><label class="expand" for="c-38329257">[2 more]</label></div><br/><div class="children"><div class="content">There is no obvious reason they couldn&#x27;t just train one themselves, or merely steal existing weights given enough time.</div><br/><div id="38329816" class="c"><input type="checkbox" id="c-38329816" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329257">parent</a><span>|</span><a href="#38329271">next</a><span>|</span><label class="collapse" for="c-38329816">[-]</label><label class="expand" for="c-38329816">[1 more]</label></div><br/><div class="children"><div class="content">That is precious time that can be used to work on alignment.</div><br/></div></div></div></div><div id="38329535" class="c"><input type="checkbox" id="c-38329535" checked=""/><div class="controls bullet"><span class="by">beanjuiceII</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329075">parent</a><span>|</span><a href="#38329271">prev</a><span>|</span><a href="#38329326">next</a><span>|</span><label class="collapse" for="c-38329535">[-]</label><label class="expand" for="c-38329535">[1 more]</label></div><br/><div class="children"><div class="content">What are you talking about use what? It&#x27;s all in the open already anyway.. And someone like China even has more data to build from</div><br/></div></div><div id="38329326" class="c"><input type="checkbox" id="c-38329326" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#38329016">root</a><span>|</span><a href="#38329075">parent</a><span>|</span><a href="#38329535">prev</a><span>|</span><a href="#38329060">next</a><span>|</span><label class="collapse" for="c-38329326">[-]</label><label class="expand" for="c-38329326">[1 more]</label></div><br/><div class="children"><div class="content">They will get access to the good stuff anyway.  The only question is whether <i>you</i> get access to it.</div><br/></div></div></div></div></div></div><div id="38330365" class="c"><input type="checkbox" id="c-38330365" checked=""/><div class="controls bullet"><span class="by">jbirer</span><span>|</span><a href="#38329016">prev</a><span>|</span><a href="#38330449">next</a><span>|</span><label class="collapse" for="c-38330365">[-]</label><label class="expand" for="c-38330365">[1 more]</label></div><br/><div class="children"><div class="content">Looks like responsibility and ethics got in the way of profit.</div><br/></div></div><div id="38330449" class="c"><input type="checkbox" id="c-38330449" checked=""/><div class="controls bullet"><span class="by">camdenlock</span><span>|</span><a href="#38330365">prev</a><span>|</span><a href="#38330402">next</a><span>|</span><label class="collapse" for="c-38330449">[-]</label><label class="expand" for="c-38330449">[1 more]</label></div><br/><div class="children"><div class="content">Such teams are just panicked by the idea that these models might not exclusively push their preferred ideology (critical social justice). We probably shouldn’t shed a tear for their disbandment.</div><br/></div></div><div id="38330402" class="c"><input type="checkbox" id="c-38330402" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#38330449">prev</a><span>|</span><a href="#38330371">next</a><span>|</span><label class="collapse" for="c-38330402">[-]</label><label class="expand" for="c-38330402">[1 more]</label></div><br/><div class="children"><div class="content">It feels like to me the ai field is filled with Corp speak phrases that aren&#x27;t clear at all? Alignment, responsible, safety etc. These aren&#x27;t adjectives normal people use to describe things. What&#x27;s up with this?</div><br/></div></div><div id="38330371" class="c"><input type="checkbox" id="c-38330371" checked=""/><div class="controls bullet"><span class="by">unicornmama</span><span>|</span><a href="#38330402">prev</a><span>|</span><a href="#38329880">next</a><span>|</span><label class="collapse" for="c-38330371">[-]</label><label class="expand" for="c-38330371">[1 more]</label></div><br/><div class="children"><div class="content">Meta cannot be both referee and player on the field. Responsible schmenponsible. True oversight can only come from a an independent entity.<p>These internal committees are Kabuki theater.</div><br/></div></div><div id="38329880" class="c"><input type="checkbox" id="c-38329880" checked=""/><div class="controls bullet"><span class="by">baby</span><span>|</span><a href="#38330371">prev</a><span>|</span><a href="#38328929">next</a><span>|</span><label class="collapse" for="c-38329880">[-]</label><label class="expand" for="c-38329880">[13 more]</label></div><br/><div class="children"><div class="content">I really really hate what we did to LLMs. We throttled it so much that it&#x27;s not as useful as it used to be. I think everybody understands that the LLMs lie some % of the time, it&#x27;s just dumb to censor them. Good move on Meta.</div><br/><div id="38329955" class="c"><input type="checkbox" id="c-38329955" checked=""/><div class="controls bullet"><span class="by">hibernator149</span><span>|</span><a href="#38329880">parent</a><span>|</span><a href="#38330055">next</a><span>|</span><label class="collapse" for="c-38329955">[-]</label><label class="expand" for="c-38329955">[4 more]</label></div><br/><div class="children"><div class="content">What useful thing could the AI do that it can&#x27;t do any longer?</div><br/><div id="38330053" class="c"><input type="checkbox" id="c-38330053" checked=""/><div class="controls bullet"><span class="by">FirmwareBurner</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38329955">parent</a><span>|</span><a href="#38330197">next</a><span>|</span><label class="collapse" for="c-38330053">[-]</label><label class="expand" for="c-38330053">[1 more]</label></div><br/><div class="children"><div class="content">Make good jokes.</div><br/></div></div><div id="38330197" class="c"><input type="checkbox" id="c-38330197" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38329955">parent</a><span>|</span><a href="#38330053">prev</a><span>|</span><a href="#38330085">next</a><span>|</span><label class="collapse" for="c-38330197">[-]</label><label class="expand" for="c-38330197">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT has gotten noticeably worse at following directions, eg guidelines for writing an essay.<p>You used to be able to tell it to not include parts of the prompt or write in a certain style — and now it’ll ignore those guidelines.<p>I believe they did this to stop DAN jailbreaks, but now, it can no longer follow directions for composition at all.</div><br/></div></div><div id="38330085" class="c"><input type="checkbox" id="c-38330085" checked=""/><div class="controls bullet"><span class="by">anothernewdude</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38329955">parent</a><span>|</span><a href="#38330197">prev</a><span>|</span><a href="#38330055">next</a><span>|</span><label class="collapse" for="c-38330085">[-]</label><label class="expand" for="c-38330085">[1 more]</label></div><br/><div class="children"><div class="content">Give actual results rather than endlessly wasting tokens on useless apologies that it&#x27;s only an AI.</div><br/></div></div></div></div><div id="38330055" class="c"><input type="checkbox" id="c-38330055" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38329880">parent</a><span>|</span><a href="#38329955">prev</a><span>|</span><a href="#38328929">next</a><span>|</span><label class="collapse" for="c-38330055">[-]</label><label class="expand" for="c-38330055">[8 more]</label></div><br/><div class="children"><div class="content">Honestly after what happened with the OpenAI board, it&#x27;s kind of hard to take the AI safety people seriously. I think there are real problems with Gen AI systems including data privacy, copyright, the potential for convincing misinformation&#x2F;propaganda, etc, but I&#x27;m really not convinced a text generator is an existential threat to the species. We need to take these problems seriously and some of the AI safety discussion makes that really difficult.</div><br/><div id="38330082" class="c"><input type="checkbox" id="c-38330082" checked=""/><div class="controls bullet"><span class="by">Simon_ORourke</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38330055">parent</a><span>|</span><a href="#38330200">next</a><span>|</span><label class="collapse" for="c-38330082">[-]</label><label class="expand" for="c-38330082">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m really not convinced a text generator is an existential threat to the species.<p>Well said - there&#x27;s been too much &quot;Skynet going rogue&quot; sci-fi nonsense injected into this debate.</div><br/></div></div><div id="38330200" class="c"><input type="checkbox" id="c-38330200" checked=""/><div class="controls bullet"><span class="by">JanSt</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38330055">parent</a><span>|</span><a href="#38330082">prev</a><span>|</span><a href="#38330135">next</a><span>|</span><label class="collapse" for="c-38330200">[-]</label><label class="expand" for="c-38330200">[2 more]</label></div><br/><div class="children"><div class="content">&quot;&#x27;m really not convinced a text generator is an existential threat to the species&quot;<p>Except it&#x27;s not only a text generator. It now browses the web, runs code and calls functions.</div><br/><div id="38330644" class="c"><input type="checkbox" id="c-38330644" checked=""/><div class="controls bullet"><span class="by">nonrandomstring</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38330200">parent</a><span>|</span><a href="#38330135">next</a><span>|</span><label class="collapse" for="c-38330644">[-]</label><label class="expand" for="c-38330644">[1 more]</label></div><br/><div class="children"><div class="content">And yet we&#x27;ve survived Emacs.</div><br/></div></div></div></div><div id="38330135" class="c"><input type="checkbox" id="c-38330135" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38330055">parent</a><span>|</span><a href="#38330200">prev</a><span>|</span><a href="#38328929">next</a><span>|</span><label class="collapse" for="c-38330135">[-]</label><label class="expand" for="c-38330135">[4 more]</label></div><br/><div class="children"><div class="content">You’re thinking short term, applying safety to todays LLMs. No one is claiming todays tech poses and existential threat.<p>Others are looking at the trajectory and thinking about the future, where safety does start to become important.</div><br/><div id="38330259" class="c"><input type="checkbox" id="c-38330259" checked=""/><div class="controls bullet"><span class="by">123yawaworht456</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38330135">parent</a><span>|</span><a href="#38330213">next</a><span>|</span><label class="collapse" for="c-38330259">[-]</label><label class="expand" for="c-38330259">[1 more]</label></div><br/><div class="children"><div class="content">&gt;No one is claiming todays tech poses and existential threat.<p>really now, friend?</div><br/></div></div><div id="38330213" class="c"><input type="checkbox" id="c-38330213" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38330135">parent</a><span>|</span><a href="#38330259">prev</a><span>|</span><a href="#38328929">next</a><span>|</span><label class="collapse" for="c-38330213">[-]</label><label class="expand" for="c-38330213">[2 more]</label></div><br/><div class="children"><div class="content">All large scale human atrocities required a centralized government imposing on the public a technocratic agenda.<p>“AI safety” advocates are recreating that problem, now with AI spice.<p>How about we don’t create actual problems (technocrats imposing disasters on the public) because we’re fighting the scourge of hypothetical pixies?</div><br/><div id="38330294" class="c"><input type="checkbox" id="c-38330294" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#38329880">root</a><span>|</span><a href="#38330213">parent</a><span>|</span><a href="#38328929">next</a><span>|</span><label class="collapse" for="c-38330294">[-]</label><label class="expand" for="c-38330294">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Some large scale human atrocities<p>FTFY</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38328929" class="c"><input type="checkbox" id="c-38328929" checked=""/><div class="controls bullet"><span class="by">corethree</span><span>|</span><a href="#38329880">prev</a><span>|</span><a href="#38329581">next</a><span>|</span><label class="collapse" for="c-38328929">[-]</label><label class="expand" for="c-38328929">[13 more]</label></div><br/><div class="children"><div class="content">Safety for AI is like making safe bullets or safe swords or safe shotguns.<p>The reason why there&#x27;s so much emphasis on this is liability. That&#x27;s it. Otherwise there&#x27;s really no point.<p>It&#x27;s the psychological aspect of blame that influences the liability. If I wanted to make a dirty bomb it&#x27;s harder to blame google for it if I found the results through google, easier to blame AI for it if I found the results from an LLM. Mainly because the data was transferred from the servers directly to me when it&#x27;s an LLM. But the logical route of getting that information is essentially the same.<p>So because of this companies like Meta (who really don&#x27;t give a shit) spend so much time emphasizing on this safety bs. Now I&#x27;m not denigrating meta for not giving a shit, because I don&#x27;t give a shit either.<p>Kitchen knives can kill people folks. Nothing can stop it. And I don&#x27;t give a shit about people designing safety into kitchen knives anymore than I give a shit about people designing safety into AI. Pointless.</div><br/><div id="38329356" class="c"><input type="checkbox" id="c-38329356" checked=""/><div class="controls bullet"><span class="by">wilsonnb3</span><span>|</span><a href="#38328929">parent</a><span>|</span><a href="#38329693">next</a><span>|</span><label class="collapse" for="c-38329356">[-]</label><label class="expand" for="c-38329356">[3 more]</label></div><br/><div class="children"><div class="content">Just for the record, people put a lot of effort into making safe bullets and shotguns. Neither is going to go bang unless you make it go bang. Definitely not pointless.</div><br/><div id="38329424" class="c"><input type="checkbox" id="c-38329424" checked=""/><div class="controls bullet"><span class="by">corethree</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38329356">parent</a><span>|</span><a href="#38329693">next</a><span>|</span><label class="collapse" for="c-38329424">[-]</label><label class="expand" for="c-38329424">[2 more]</label></div><br/><div class="children"><div class="content">The safety is for accidental usage.  If the intent is to kill a safety isn&#x27;t going to stop anything.<p>All the unsafe things I can do with AI I can do with Google. No safety on Google. why? Liability is less of an issue.</div><br/><div id="38329894" class="c"><input type="checkbox" id="c-38329894" checked=""/><div class="controls bullet"><span class="by">baby</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38329424">parent</a><span>|</span><a href="#38329693">next</a><span>|</span><label class="collapse" for="c-38329894">[-]</label><label class="expand" for="c-38329894">[1 more]</label></div><br/><div class="children"><div class="content">Not sure why you&#x27;re getting downvoted. The safety around guns is indeed for the shooter, not for the shootee.</div><br/></div></div></div></div></div></div><div id="38329693" class="c"><input type="checkbox" id="c-38329693" checked=""/><div class="controls bullet"><span class="by">csydas</span><span>|</span><a href="#38328929">parent</a><span>|</span><a href="#38329356">prev</a><span>|</span><a href="#38328999">next</a><span>|</span><label class="collapse" for="c-38329693">[-]</label><label class="expand" for="c-38329693">[2 more]</label></div><br/><div class="children"><div class="content">i would disagree. i thjnk the safety concerns and conversations from the companies serving AI services are misguided simply because the companies know what they want to do with it (advertising based on user data and input) but they have no idea how to accurately predict all the unexpected or undesired responses from the AIs. they know there is likely some potential revenue there but they aren’t sure how to make the AI comply with regulations.<p>they already have processes for manipulating results and have a trained and likely tagged data set of “bad” things the AI shouldn’t return. if they don’t want the ai telling how to do illegal stuff they will just not include that in its dataset. if the ai “learns” this, that’s responsibility of the user likely in the clause. they will simply document how it was trained and true expected results, add clause on “if you don’t wanna see disturbing responses don’t ask disturbing questions for it to find he answer to”, and probably it will be enough unless the ai gets really combative and destructive.<p>i really don’t thjnk this about safety at all, it’s trying to seed the idea that the ai companies are at all concerned about violating existing privacy regulations that Meta et. al. already are bumping against.<p>obviously it’s supposition but i thjnk this is far likelier what they’re worried on and what all this “safety” talk is about.  they just want plausible deniability to be seeded before the first lawsuits come.</div><br/><div id="38330602" class="c"><input type="checkbox" id="c-38330602" checked=""/><div class="controls bullet"><span class="by">corethree</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38329693">parent</a><span>|</span><a href="#38328999">next</a><span>|</span><label class="collapse" for="c-38330602">[-]</label><label class="expand" for="c-38330602">[1 more]</label></div><br/><div class="children"><div class="content">Right. You and I are in agreement. Read my post carefully. I stated that it&#x27;s all about liability. That&#x27;s the only reason why they care.</div><br/></div></div></div></div><div id="38328999" class="c"><input type="checkbox" id="c-38328999" checked=""/><div class="controls bullet"><span class="by">cageface</span><span>|</span><a href="#38328929">parent</a><span>|</span><a href="#38329693">prev</a><span>|</span><a href="#38330049">next</a><span>|</span><label class="collapse" for="c-38328999">[-]</label><label class="expand" for="c-38328999">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more like making safe nukes. One person can do a lot more damage with AI than they can with a gun.</div><br/><div id="38329050" class="c"><input type="checkbox" id="c-38329050" checked=""/><div class="controls bullet"><span class="by">threadweaver34</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38328999">parent</a><span>|</span><a href="#38329045">next</a><span>|</span><label class="collapse" for="c-38329050">[-]</label><label class="expand" for="c-38329050">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure if it will actually be like that. In just a few years, AI will be so widespread we&#x27;ll just assume anything not from a source we trust is fake.</div><br/><div id="38329314" class="c"><input type="checkbox" id="c-38329314" checked=""/><div class="controls bullet"><span class="by">tiffanyg</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38329050">parent</a><span>|</span><a href="#38329045">next</a><span>|</span><label class="collapse" for="c-38329314">[-]</label><label class="expand" for="c-38329314">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, that sounds like a <i>great idea.</i><p>The US (in particular) has seen a significant decline in trust (think <i>community,</i> as in <i>union,</i> as in Federalist #10 etc.) in all manner of fundamentals of democracy and &#x27;modernity&#x27; (tech, science, etc.) in the past several decades. And, bear in mind that there are significant differences in the way people cope with these sorts of changes and the increasing instability* quite generally for many people as well as local and regional communities.<p>Fire departments, since the time of Ben Franklin, have mostly, to my knowledge, doused fires with &quot;extinguishers,&quot; not &quot;accelerants&quot;.**<p>* Especially economic - not in the sense of &quot;time for &#x27;entitlements&#x27;&quot;, ideally, in the sense of &quot;time to reconsider if trashing the &#x27;New Deal&#x27; starting ~ in the 70s might have been a bad idea&quot; ... for those not already thinking that way. Nothing better (socially) than to provide people with <i>meaningful</i> ways of &#x27;acquiring capital.&#x27;<p>** Outside of stories in books, anyway...</div><br/></div></div></div></div><div id="38329045" class="c"><input type="checkbox" id="c-38329045" checked=""/><div class="controls bullet"><span class="by">corethree</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38328999">parent</a><span>|</span><a href="#38329050">prev</a><span>|</span><a href="#38330049">next</a><span>|</span><label class="collapse" for="c-38329045">[-]</label><label class="expand" for="c-38329045">[2 more]</label></div><br/><div class="children"><div class="content">First off that&#x27;s theoretical. No damage of that scale has been done by an LLM yet. Second off nobody really believes this. That&#x27;s why there&#x27;s no age limit for LLM usage and there is for gun usage. Would you let your 10 year old kid play with a hand gun or chatGPT? Let&#x27;s be real.</div><br/><div id="38329381" class="c"><input type="checkbox" id="c-38329381" checked=""/><div class="controls bullet"><span class="by">dieselgate</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38329045">parent</a><span>|</span><a href="#38330049">next</a><span>|</span><label class="collapse" for="c-38329381">[-]</label><label class="expand" for="c-38329381">[1 more]</label></div><br/><div class="children"><div class="content">I agree with your pragmatic approach. LLMs are a &quot;high magnitude&quot; advancement but we can&#x27;t really correlate that with &quot;severe destruction&quot; in a physical way - maybe in a theoretical or abstract way.<p>Kind of reminds me of the whole &quot;dihydrogen monoxide kills so many people per year&quot; parody</div><br/></div></div></div></div></div></div><div id="38330049" class="c"><input type="checkbox" id="c-38330049" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#38328929">parent</a><span>|</span><a href="#38328999">prev</a><span>|</span><a href="#38329581">next</a><span>|</span><label class="collapse" for="c-38330049">[-]</label><label class="expand" for="c-38330049">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Safety for AI is like making safe bullets or safe swords or safe shotguns.<p>This seems like a very confused analogy for two reasons. One, there&#x27;s a reason you aren&#x27;t able to get your hands on a sword or shotgun in most places on earth, I&#x27;d prefer that not to be the case for AI.<p>Secondly, AI is a general purpose tool. Safety for AI is like safety for a car, or a phone, or the electrity grid. it&#x27;s going to be a ubiqutous background technology, not merely a tool to inflict damage. And I want safety and reliablity in a technology that&#x27;s going to power most stuff around me.</div><br/><div id="38330585" class="c"><input type="checkbox" id="c-38330585" checked=""/><div class="controls bullet"><span class="by">corethree</span><span>|</span><a href="#38328929">root</a><span>|</span><a href="#38330049">parent</a><span>|</span><a href="#38329581">next</a><span>|</span><label class="collapse" for="c-38330585">[-]</label><label class="expand" for="c-38330585">[1 more]</label></div><br/><div class="children"><div class="content">&gt;This seems like a very confused analogy for two reasons. One, there&#x27;s a reason you aren&#x27;t able to get your hands on a sword or shotgun in most places on earth, I&#x27;d prefer that not to be the case for AI.<p>In the US, I can get my hands on guns, knives and swords. In other countries you can get axes and knives. I think guns are mostly banned in other places.<p>&gt;Safety for AI is like safety for a car, or a phone<p>Your phone has a safety? What about your car? At best the car has air bags that prevent you from dying. Doesn&#x27;t prevent you from running other people over. The type of &quot;safety&quot; that big tech is talking about is safety to prevent people from using it malicious ways. They do this by making the AI LESS reliable.<p>For example chatGPT will refuse to help you do malicious things.<p>The big emphasis on this is pointless imo. If people aren&#x27;t using AI to look up malicious things, they&#x27;re going to be using google instead which has mostly the same information.</div><br/></div></div></div></div></div></div><div id="38329581" class="c"><input type="checkbox" id="c-38329581" checked=""/><div class="controls bullet"><span class="by">spangry</span><span>|</span><a href="#38328929">prev</a><span>|</span><a href="#38328886">next</a><span>|</span><label class="collapse" for="c-38329581">[-]</label><label class="expand" for="c-38329581">[4 more]</label></div><br/><div class="children"><div class="content">Does anyone know what this Responsible AI team did? Were they working on the AI alignment &#x2F; control issue, or was it more about curtailing politically undesirable model outputs? I feel like the conflation of these two things is unfortunate because the latter will cause people to turn off the former. It&#x27;s like a reverse motte and bailey.</div><br/><div id="38329900" class="c"><input type="checkbox" id="c-38329900" checked=""/><div class="controls bullet"><span class="by">baby</span><span>|</span><a href="#38329581">parent</a><span>|</span><a href="#38330469">next</a><span>|</span><label class="collapse" for="c-38329900">[-]</label><label class="expand" for="c-38329900">[1 more]</label></div><br/><div class="children"><div class="content">censor the AI with preprompts most likely. Or looking into the training data for bad apples.</div><br/></div></div><div id="38330469" class="c"><input type="checkbox" id="c-38330469" checked=""/><div class="controls bullet"><span class="by">camdenlock</span><span>|</span><a href="#38329581">parent</a><span>|</span><a href="#38329900">prev</a><span>|</span><a href="#38329757">next</a><span>|</span><label class="collapse" for="c-38330469">[-]</label><label class="expand" for="c-38330469">[1 more]</label></div><br/><div class="children"><div class="content">If this one was like any of the others, they were likely tasked with modelwashing LLMs to adhere to current academic fashions; i.e. tired feminist tropes and general social justice dogma.</div><br/></div></div></div></div><div id="38328886" class="c"><input type="checkbox" id="c-38328886" checked=""/><div class="controls bullet"><span class="by">asylteltine</span><span>|</span><a href="#38329581">prev</a><span>|</span><a href="#38328912">next</a><span>|</span><label class="collapse" for="c-38328886">[-]</label><label class="expand" for="c-38328886">[3 more]</label></div><br/><div class="children"><div class="content">I’m okay with this. They mostly complained about nonsense or nonexistent problems. Maybe they can stop “aligning” their models now</div><br/><div id="38329726" class="c"><input type="checkbox" id="c-38329726" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#38328886">parent</a><span>|</span><a href="#38328950">next</a><span>|</span><label class="collapse" for="c-38329726">[-]</label><label class="expand" for="c-38329726">[1 more]</label></div><br/><div class="children"><div class="content">You need alignment for it to do anything useful in the first place; base models are very hard to control. Alignment is just engineering.</div><br/></div></div></div></div><div id="38328912" class="c"><input type="checkbox" id="c-38328912" checked=""/><div class="controls bullet"><span class="by">ryanjshaw</span><span>|</span><a href="#38328886">prev</a><span>|</span><a href="#38329410">next</a><span>|</span><label class="collapse" for="c-38328912">[-]</label><label class="expand" for="c-38328912">[10 more]</label></div><br/><div class="children"><div class="content">Seems like something that should exist as a specialist knowledge team within an existing compliance team i.e. guided by legal concerns primarily.</div><br/><div id="38329194" class="c"><input type="checkbox" id="c-38329194" checked=""/><div class="controls bullet"><span class="by">kevinventullo</span><span>|</span><a href="#38328912">parent</a><span>|</span><a href="#38329534">next</a><span>|</span><label class="collapse" for="c-38329194">[-]</label><label class="expand" for="c-38329194">[6 more]</label></div><br/><div class="children"><div class="content">You might be surprised how often the tail wags the dog in these situations. Lawyers shrug and defer to the policy doomers because they ultimately don’t understand the tech.</div><br/><div id="38329396" class="c"><input type="checkbox" id="c-38329396" checked=""/><div class="controls bullet"><span class="by">pardoned_turkey</span><span>|</span><a href="#38328912">root</a><span>|</span><a href="#38329194">parent</a><span>|</span><a href="#38329597">next</a><span>|</span><label class="collapse" for="c-38329396">[-]</label><label class="expand" for="c-38329396">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s about understanding. Lawyers are pretty smart. But there&#x27;s no upside to you as a corporate lawyer if you advocate for taking risks. Even if you think you&#x27;re on solid legal footing, you&#x27;re going to miscalculate sooner or later, or run into a hostile regulator. And then, it&#x27;s on you.<p>Conversely, there&#x27;s no real downside to being too conservative, especially if engineers and leadership are entirely deferential to you because <i>they</i> don&#x27;t understand your field (or are too afraid to speak up.)<p>Although this is also somewhat true for security, privacy, and safety organizations, their remit tends to include &quot;enabling business.&quot; A safety team that defaults to &quot;you shouldn&#x27;t be doing this&quot; is not going to have much sway. A legal department might.</div><br/><div id="38329496" class="c"><input type="checkbox" id="c-38329496" checked=""/><div class="controls bullet"><span class="by">DannyBee</span><span>|</span><a href="#38328912">root</a><span>|</span><a href="#38329396">parent</a><span>|</span><a href="#38329479">next</a><span>|</span><label class="collapse" for="c-38329496">[-]</label><label class="expand" for="c-38329496">[1 more]</label></div><br/><div class="children"><div class="content">&quot;But there&#x27;s no upside to you as a corporate lawyer if you advocate for taking risks. &quot;<p>This is a great trope, but as anyone who ever worked with me or plenty of others would tell you, this is both totally wrong, and most good corporate lawyers don&#x27;t operate like this.<p>Effective corporations have legal departments who see their goal as enabling business as well, and that requires taking risks at times. because the legal world is not a particularly certain one either.<p>There are certainly plenty of ineffective corporate legal departments out there, but there are plenty of ineffective engineering, security, privacy, product managmenent, etc orgs out there too.</div><br/></div></div><div id="38329479" class="c"><input type="checkbox" id="c-38329479" checked=""/><div class="controls bullet"><span class="by">zooq_ai</span><span>|</span><a href="#38328912">root</a><span>|</span><a href="#38329396">parent</a><span>|</span><a href="#38329496">prev</a><span>|</span><a href="#38329597">next</a><span>|</span><label class="collapse" for="c-38329479">[-]</label><label class="expand" for="c-38329479">[2 more]</label></div><br/><div class="children"><div class="content">This is exactly how Elon Musk crushes competition.<p>His entire team including legal&#x2F;hr&#x2F;finance and not just engineering, has the culture of risk taking. Elon Musk is no genius, but his Material Science Engineering, risk taking and first-principle efficiency is unparalleled.<p>By focusing on Musk&#x27;s shitty personality, his critics always gets wrong about why he can still be successful despite Musk being a douchebag</div><br/><div id="38329587" class="c"><input type="checkbox" id="c-38329587" checked=""/><div class="controls bullet"><span class="by">mufti_menk</span><span>|</span><a href="#38328912">root</a><span>|</span><a href="#38329479">parent</a><span>|</span><a href="#38329597">next</a><span>|</span><label class="collapse" for="c-38329587">[-]</label><label class="expand" for="c-38329587">[1 more]</label></div><br/><div class="children"><div class="content">People equate likeability with how deserving someone is for their success, so they always say that Musk got lucky</div><br/></div></div></div></div></div></div><div id="38329597" class="c"><input type="checkbox" id="c-38329597" checked=""/><div class="controls bullet"><span class="by">daxfohl</span><span>|</span><a href="#38328912">root</a><span>|</span><a href="#38329194">parent</a><span>|</span><a href="#38329396">prev</a><span>|</span><a href="#38329534">next</a><span>|</span><label class="collapse" for="c-38329597">[-]</label><label class="expand" for="c-38329597">[1 more]</label></div><br/><div class="children"><div class="content">Or, moreover, legal safety policies are essentially written by the companies that produce the product, by their own definition of safety, and pushed by their own lobbyists to codify into law, effectively giving them monopoly power. Safety is just a vehicle to those ends.</div><br/></div></div></div></div><div id="38329534" class="c"><input type="checkbox" id="c-38329534" checked=""/><div class="controls bullet"><span class="by">justrealist</span><span>|</span><a href="#38328912">parent</a><span>|</span><a href="#38329194">prev</a><span>|</span><a href="#38329765">next</a><span>|</span><label class="collapse" for="c-38329534">[-]</label><label class="expand" for="c-38329534">[2 more]</label></div><br/><div class="children"><div class="content">Compliance is sorta widely-acknowledged to be useless paperpushing.</div><br/><div id="38329536" class="c"><input type="checkbox" id="c-38329536" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#38328912">root</a><span>|</span><a href="#38329534">parent</a><span>|</span><a href="#38329765">next</a><span>|</span><label class="collapse" for="c-38329536">[-]</label><label class="expand" for="c-38329536">[1 more]</label></div><br/><div class="children"><div class="content">Not at a bank</div><br/></div></div></div></div></div></div><div id="38329410" class="c"><input type="checkbox" id="c-38329410" checked=""/><div class="controls bullet"><span class="by">121789</span><span>|</span><a href="#38328912">prev</a><span>|</span><a href="#38329801">next</a><span>|</span><label class="collapse" for="c-38329410">[-]</label><label class="expand" for="c-38329410">[2 more]</label></div><br/><div class="children"><div class="content">These types of teams never last long</div><br/></div></div></div></div></div></div></div></body></html>