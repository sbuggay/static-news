<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689411653434" as="style"/><link rel="stylesheet" href="styles.css?v=1689411653434"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/Futrell/ziplm">Ziplm: Gzip-Backed Language Model</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>todsacerdoti</span> | <span>34 comments</span></div><br/><div><div id="36732982" class="c"><input type="checkbox" id="c-36732982" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#36733535">next</a><span>|</span><label class="collapse" for="c-36732982">[-]</label><label class="expand" for="c-36732982">[17 more]</label></div><br/><div class="children"><div class="content">Oh crud, I made the front page.<p>It should be obvious that this is junk quality as a language model. But it&#x27;s a cool example of the equivalence between compression codes and probability distributions, so I hope people find it interesting for that reason. You can also get a bit of an intuitive sense for the patterns that gzip and bzip2 pick up on in text (they like repetitive strings).</div><br/><div id="36733106" class="c"><input type="checkbox" id="c-36733106" checked=""/><div class="controls bullet"><span class="by">dleeftink</span><span>|</span><a href="#36732982">parent</a><span>|</span><a href="#36733316">next</a><span>|</span><label class="collapse" for="c-36733106">[-]</label><label class="expand" for="c-36733106">[1 more]</label></div><br/><div class="children"><div class="content">I think it is a cool &#x27;tangible&#x27; demo of how non-human readable data remains learnable for ML purposes. I&#x27;ve recently tried using Nilsimsa and sdeep hashes to reduce the input dimensionality of natural text, with some loss in accuracy on similarity tasks.<p>A combination of byte-pair encoding and local-sensitive hashing might prove a more stable combination. Throw some gzip function that way and we may be able to reduce input corpus file sizes immensely.</div><br/></div></div><div id="36733316" class="c"><input type="checkbox" id="c-36733316" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#36732982">parent</a><span>|</span><a href="#36733106">prev</a><span>|</span><a href="#36734704">next</a><span>|</span><label class="collapse" for="c-36733316">[-]</label><label class="expand" for="c-36733316">[3 more]</label></div><br/><div class="children"><div class="content">Maybe this was the inspiration for your project but I just saw a paper that applied compressors as part of a text classification system getting accuracy competitive with BERT. 14 lines of Python. <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426&#x2F;</a></div><br/><div id="36733344" class="c"><input type="checkbox" id="c-36733344" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733316">parent</a><span>|</span><a href="#36734704">next</a><span>|</span><label class="collapse" for="c-36733344">[-]</label><label class="expand" for="c-36733344">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, that paper made me wonder how well gzip would work as a language model so I typed up this thing in a coffee shop</div><br/><div id="36734356" class="c"><input type="checkbox" id="c-36734356" checked=""/><div class="controls bullet"><span class="by">sam_lowry_</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733344">parent</a><span>|</span><a href="#36734704">next</a><span>|</span><label class="collapse" for="c-36734356">[-]</label><label class="expand" for="c-36734356">[1 more]</label></div><br/><div class="children"><div class="content">Reminds immediately this famous article by Ted Chiang &quot;ChatGPT Is a Blurry JPEG of the Web
&quot; <a href="https:&#x2F;&#x2F;www.newyorker.com&#x2F;tech&#x2F;annals-of-technology&#x2F;chatgpt-is-a-blurry-jpeg-of-the-web" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.newyorker.com&#x2F;tech&#x2F;annals-of-technology&#x2F;chatgpt-...</a></div><br/></div></div></div></div></div></div><div id="36734704" class="c"><input type="checkbox" id="c-36734704" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#36732982">parent</a><span>|</span><a href="#36733316">prev</a><span>|</span><a href="#36733728">next</a><span>|</span><label class="collapse" for="c-36734704">[-]</label><label class="expand" for="c-36734704">[1 more]</label></div><br/><div class="children"><div class="content">A maybe dumb question for understanding: does a longer output imply lower probability of the input?</div><br/></div></div><div id="36733728" class="c"><input type="checkbox" id="c-36733728" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#36732982">parent</a><span>|</span><a href="#36734704">prev</a><span>|</span><a href="#36733183">next</a><span>|</span><label class="collapse" for="c-36733728">[-]</label><label class="expand" for="c-36733728">[4 more]</label></div><br/><div class="children"><div class="content">Repeating this experiment with a more optimal compression algorithm might be interesting too.</div><br/><div id="36734450" class="c"><input type="checkbox" id="c-36734450" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733728">parent</a><span>|</span><a href="#36734662">next</a><span>|</span><label class="collapse" for="c-36734450">[-]</label><label class="expand" for="c-36734450">[1 more]</label></div><br/><div class="children"><div class="content">Sure! The current best achieved compression for English text (well, specifically, Wikipedia articles) is Fabrice Bellard&#x27;s nncp, which uses... transformers. <a href="http:&#x2F;&#x2F;mattmahoney.net&#x2F;dc&#x2F;text.html#1085" rel="nofollow noreferrer">http:&#x2F;&#x2F;mattmahoney.net&#x2F;dc&#x2F;text.html#1085</a><p>Unfortunately I don&#x27;t think anyone has posted any trained models after running any inputs through it, which could be used to repeat the experiment. I would try it, but I don&#x27;t have a AVX2 CPU.<p>However, most research into reducing the model perplexity of text (increasing the compression ratio when the model is used for encoding) uses fixed language models which are learnt offline. Obviously, they use transformers too. You can see some of them here:<p><a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;language-modelling-on-wikitext-103" rel="nofollow noreferrer">https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;language-modelling-on-wikite...</a><p>However (token) perplexity is only comparable across models if the same tokenizer is used. The above page seems to compare papers using the GPT-2 tokenizer. I would assume that if you controlled for differences in tokenizers by just measuring bits-per-word&#x2F;byte, GPT-4 <i>without</i> RLHF would achieve the highest compression. It should be obvious that RLHF increases perplexity because it changes the training objective to something other than predicting text.</div><br/></div></div><div id="36734662" class="c"><input type="checkbox" id="c-36734662" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733728">parent</a><span>|</span><a href="#36734450">prev</a><span>|</span><a href="#36733820">next</a><span>|</span><label class="collapse" for="c-36734662">[-]</label><label class="expand" for="c-36734662">[1 more]</label></div><br/><div class="children"><div class="content">If I understand it correctly, this implementation re-compresses the prefix 256 times, once for each possible byte continuation. It should be possible to save a lot of work by saving the state of the compressor just before the predicted byte, and reusing it.<p>Also, since gzip works on a limited sliding context window, it&#x27;s probably pointless to give it a prefix much longer than the window... unless there&#x27;s something about the algorithm that I&#x27;m missing. Same with bzip2 and its block size.</div><br/></div></div><div id="36733820" class="c"><input type="checkbox" id="c-36733820" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733728">parent</a><span>|</span><a href="#36734662">prev</a><span>|</span><a href="#36733183">next</a><span>|</span><label class="collapse" for="c-36733820">[-]</label><label class="expand" for="c-36733820">[1 more]</label></div><br/><div class="children"><div class="content">In some sense, the feature space learned by a NN <i>is</i> a compression algorithm.</div><br/></div></div></div></div><div id="36733183" class="c"><input type="checkbox" id="c-36733183" checked=""/><div class="controls bullet"><span class="by">DarmokJalad1701</span><span>|</span><a href="#36732982">parent</a><span>|</span><a href="#36733728">prev</a><span>|</span><a href="#36733094">next</a><span>|</span><label class="collapse" for="c-36733183">[-]</label><label class="expand" for="c-36733183">[3 more]</label></div><br/><div class="children"><div class="content">I am not sure if it was intentional, but in your &quot;Moby Dick&quot; example, the &quot;alphabet&quot; vocab is missing the letter &#x27;f&#x27;.</div><br/><div id="36733273" class="c"><input type="checkbox" id="c-36733273" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733183">parent</a><span>|</span><a href="#36733094">next</a><span>|</span><label class="collapse" for="c-36733273">[-]</label><label class="expand" for="c-36733273">[2 more]</label></div><br/><div class="children"><div class="content">Ha, yeah. I was just running my finger along the keyboard to type out the string.<p>I just regenerated it, btw, and got a better looking result.</div><br/><div id="36733854" class="c"><input type="checkbox" id="c-36733854" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733273">parent</a><span>|</span><a href="#36733094">next</a><span>|</span><label class="collapse" for="c-36733854">[-]</label><label class="expand" for="c-36733854">[1 more]</label></div><br/><div class="children"><div class="content">i suggest you get all the unique characters from the moby dick text and use it as the alphabet if you&#x27;re generating from it, or truncate the text to just your hand selected characters else gzip cannot even approximate an optimal code if the alphabets don&#x27;t match.</div><br/></div></div></div></div></div></div><div id="36733094" class="c"><input type="checkbox" id="c-36733094" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#36732982">parent</a><span>|</span><a href="#36733183">prev</a><span>|</span><a href="#36733859">next</a><span>|</span><label class="collapse" for="c-36733094">[-]</label><label class="expand" for="c-36733094">[3 more]</label></div><br/><div class="children"><div class="content">I think you would get slightly better generations from the model if the input data were tokenized and token IDs were sampled instead.</div><br/><div id="36733151" class="c"><input type="checkbox" id="c-36733151" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733094">parent</a><span>|</span><a href="#36733859">next</a><span>|</span><label class="collapse" for="c-36733151">[-]</label><label class="expand" for="c-36733151">[2 more]</label></div><br/><div class="children"><div class="content">Possibly. It would definitely make it even slower though. To sample from the model, if you have a prompt&#x2F;training data c, you have to compute the compressed length of all strings cx for tokens x in your vocabulary. If you have a long prompt c (like 1 million lines of wikipedia) and a big vocabulary of tokens (like 100,000 different words), you&#x27;re going to have a bad time.</div><br/><div id="36734676" class="c"><input type="checkbox" id="c-36734676" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#36732982">root</a><span>|</span><a href="#36733151">parent</a><span>|</span><a href="#36733859">next</a><span>|</span><label class="collapse" for="c-36734676">[-]</label><label class="expand" for="c-36734676">[1 more]</label></div><br/><div class="children"><div class="content">It should be possible to save and reuse the state of the compressor right before the final byte.</div><br/></div></div></div></div></div></div><div id="36733859" class="c"><input type="checkbox" id="c-36733859" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36732982">parent</a><span>|</span><a href="#36733094">prev</a><span>|</span><a href="#36733535">next</a><span>|</span><label class="collapse" for="c-36733859">[-]</label><label class="expand" for="c-36733859">[1 more]</label></div><br/><div class="children"><div class="content">I think this would work better if you used a fixed compression dictionary trained over the corpus and used to compress the prompt. Unless you do that?  I wouldnât expect this to work well for such a small training set. But I do wonder if this is an excellent way to improve token resolution; especially in poorly represented languages like southeast Asian.</div><br/></div></div></div></div><div id="36733535" class="c"><input type="checkbox" id="c-36733535" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36732982">prev</a><span>|</span><a href="#36733227">next</a><span>|</span><label class="collapse" for="c-36733535">[-]</label><label class="expand" for="c-36733535">[2 more]</label></div><br/><div class="children"><div class="content">It should be noted that <i>Moby Dick</i> is (in)famous for its very large and unusual vocabulary. Some lighter reading might yield better results for demonstration purposes. Also, while fairly long for a single novel, the number of words is still minuscule compared to what other LMs are trained on. Using the entire Gutenberg library, or a Wikipedia dump, could improve the quality dramatically.<p>Of course, it doesn&#x27;t really matter, as the whole thing is obviously just a toy, but I still think that this approach should be able to produce much better output than the garbled <i>Moby Dick</i> example.</div><br/><div id="36734213" class="c"><input type="checkbox" id="c-36734213" checked=""/><div class="controls bullet"><span class="by">boberoni</span><span>|</span><a href="#36733535">parent</a><span>|</span><a href="#36733227">next</a><span>|</span><label class="collapse" for="c-36734213">[-]</label><label class="expand" for="c-36734213">[1 more]</label></div><br/><div class="children"><div class="content">&quot;An early version of a new project will sometimes be dismissed as a toy. It&#x27;s a good sign when people do this. That means it has everything a new idea needs except scale, and that tends to follow.&quot; [1]<p>[1] <a href="http:&#x2F;&#x2F;www.paulgraham.com&#x2F;greatwork.html" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.paulgraham.com&#x2F;greatwork.html</a></div><br/></div></div></div></div><div id="36733227" class="c"><input type="checkbox" id="c-36733227" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#36733535">prev</a><span>|</span><a href="#36733830">next</a><span>|</span><label class="collapse" for="c-36733227">[-]</label><label class="expand" for="c-36733227">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Transformers (GPT-3, Copilot ,) are built upon an expensive self-attention network. Instead or also FFT looks to be 7x GPU-cheaper.</i><p>&quot;Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs&quot; (2021) <a href="https:&#x2F;&#x2F;syncedreview.com&#x2F;2021&#x2F;05&#x2F;14&#x2F;deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;syncedreview.com&#x2F;2021&#x2F;05&#x2F;14&#x2F;deepmind-podracer-tpu-ba...</a><p>The next step of Conway&#x27;s Game can be calculated with FFT and also 2D Convolution.<p>Convolution &gt; 
Visual explanation: 
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convolution" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convolution</a></div><br/><div id="36733237" class="c"><input type="checkbox" id="c-36733237" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#36733227">parent</a><span>|</span><a href="#36733830">next</a><span>|</span><label class="collapse" for="c-36733237">[-]</label><label class="expand" for="c-36733237">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Convolutions &#x2F; Why X+Y in probability is a beautiful mess&quot; by 3blue1brown <a href="https:&#x2F;&#x2F;youtu.be&#x2F;IaSGqQa5O-M" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;IaSGqQa5O-M</a></div><br/></div></div></div></div><div id="36733830" class="c"><input type="checkbox" id="c-36733830" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#36733227">prev</a><span>|</span><a href="#36733907">next</a><span>|</span><label class="collapse" for="c-36733830">[-]</label><label class="expand" for="c-36733830">[3 more]</label></div><br/><div class="children"><div class="content">Hey author, would you mind explaining the conversion from compression length to probability please? Namely this line:<p>scipy.special.log_softmax(-code_lengths<i>self.conversion</i>(1&#x2F;temperature))<p>Your codes are K-ary but this doesn&#x27;t look like its taken into account ala the README. What is the log(256) conversion factor? What is 1&#x2F;temperature for?</div><br/><div id="36734047" class="c"><input type="checkbox" id="c-36734047" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#36733830">parent</a><span>|</span><a href="#36733907">next</a><span>|</span><label class="collapse" for="c-36734047">[-]</label><label class="expand" for="c-36734047">[2 more]</label></div><br/><div class="children"><div class="content">Iâm measuring the length of the gzipped string in bytes, so K=256.<p>The temperature parameter is there in case anyone wants to play around with it.</div><br/><div id="36734254" class="c"><input type="checkbox" id="c-36734254" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#36733830">root</a><span>|</span><a href="#36734047">parent</a><span>|</span><a href="#36733907">next</a><span>|</span><label class="collapse" for="c-36734254">[-]</label><label class="expand" for="c-36734254">[1 more]</label></div><br/><div class="children"><div class="content">For anyone else wondering how this works out:<p>1. You want: p(x) ~ K^(-|x|), where K=256.<p>2. log p(x) ~ log K^(-|x|) = -|x|<i>log K<p>3. he is using log(softmax) ~ log(e^x)<p>4. and log(e^(-|x|</i>log(K))) = -|x|*log K as required.</div><br/></div></div></div></div></div></div><div id="36733907" class="c"><input type="checkbox" id="c-36733907" checked=""/><div class="controls bullet"><span class="by">idbfs</span><span>|</span><a href="#36733830">prev</a><span>|</span><a href="#36734228">next</a><span>|</span><label class="collapse" for="c-36733907">[-]</label><label class="expand" for="c-36733907">[1 more]</label></div><br/><div class="children"><div class="content">Cool! I had been thinking about trying this as well, after reading about the idea in one of Cosma Shalizi&#x27;s notebooks [0]. I&#x27;d love to see how something like this performs when &quot;trained&quot; on a corpus the size of the web when given the same kind of computational resources used to train modern LLMs.<p>[0] <a href="http:&#x2F;&#x2F;bactra.org&#x2F;notebooks&#x2F;nn-attention-and-transformers.html#gllz" rel="nofollow noreferrer">http:&#x2F;&#x2F;bactra.org&#x2F;notebooks&#x2F;nn-attention-and-transformers.ht...</a></div><br/></div></div><div id="36734228" class="c"><input type="checkbox" id="c-36734228" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#36733907">prev</a><span>|</span><a href="#36734221">next</a><span>|</span><label class="collapse" for="c-36734228">[-]</label><label class="expand" for="c-36734228">[1 more]</label></div><br/><div class="children"><div class="content">Better write a pull request to have this added to LangChain :^) Eternal, VC money backed, fame awaits!</div><br/></div></div><div id="36734221" class="c"><input type="checkbox" id="c-36734221" checked=""/><div class="controls bullet"><span class="by">asim</span><span>|</span><a href="#36734228">prev</a><span>|</span><a href="#36734495">next</a><span>|</span><label class="collapse" for="c-36734221">[-]</label><label class="expand" for="c-36734221">[1 more]</label></div><br/><div class="children"><div class="content">I was curious as to whether this would work. Good to see people trying new things. Because the next is to feed it PGP or AES-256 encrypted data and hand it a symmetric key. Just sort of an attempt to secure what lives in the model itself.</div><br/></div></div><div id="36734495" class="c"><input type="checkbox" id="c-36734495" checked=""/><div class="controls bullet"><span class="by">djxfade</span><span>|</span><a href="#36734221">prev</a><span>|</span><a href="#36734508">next</a><span>|</span><label class="collapse" for="c-36734495">[-]</label><label class="expand" for="c-36734495">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps a stupid question, but is this kinda thing related to how Markov chains work?</div><br/><div id="36734926" class="c"><input type="checkbox" id="c-36734926" checked=""/><div class="controls bullet"><span class="by">OriPekelman</span><span>|</span><a href="#36734495">parent</a><span>|</span><a href="#36734508">next</a><span>|</span><label class="collapse" for="c-36734926">[-]</label><label class="expand" for="c-36734926">[1 more]</label></div><br/><div class="children"><div class="content">Basically &quot;Information theoretically&quot; compression is a measure of informational distance. So basically if text A and Text B contactenated together, compress better than text A and text C it means A and B repeat more patterns. and are closer. All we need are some distance functions .. in a way you can think about it like Levenhstein distance but that can take into account inputs with very different sozes, repetitions, changes in order big inserts etc...<p>Reminds me of a mostly  joke project ruby <a href="https:&#x2F;&#x2F;github.com&#x2F;oripekelman&#x2F;simple_similarity">https:&#x2F;&#x2F;github.com&#x2F;oripekelman&#x2F;simple_similarity</a></div><br/></div></div></div></div><div id="36734508" class="c"><input type="checkbox" id="c-36734508" checked=""/><div class="controls bullet"><span class="by">th0ma5</span><span>|</span><a href="#36734495">prev</a><span>|</span><a href="#36734370">next</a><span>|</span><label class="collapse" for="c-36734508">[-]</label><label class="expand" for="c-36734508">[1 more]</label></div><br/><div class="children"><div class="content">Previously, a similar work with great performance:
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36707193">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36707193</a></div><br/></div></div><div id="36734370" class="c"><input type="checkbox" id="c-36734370" checked=""/><div class="controls bullet"><span class="by">giuscri</span><span>|</span><a href="#36734508">prev</a><span>|</span><a href="#36734824">next</a><span>|</span><label class="collapse" for="c-36734370">[-]</label><label class="expand" for="c-36734370">[1 more]</label></div><br/><div class="children"><div class="content">Can someone explain?</div><br/></div></div></div></div></div></div></div></body></html>