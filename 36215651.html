<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686128463708" as="style"/><link rel="stylesheet" href="styles.css?v=1686128463708"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="http://ggml.ai">GGML – AI at the Edge</a> <span class="domain">(<a href="http://ggml.ai">ggml.ai</a>)</span></div><div class="subtext"><span>georgehill</span> | <span>183 comments</span></div><br/><div><div id="36216196" class="c"><input type="checkbox" id="c-36216196" checked=""/><div class="controls bullet"><span class="by">samwillis</span><span>|</span><a href="#36216376">next</a><span>|</span><label class="collapse" for="c-36216196">[-]</label><label class="expand" for="c-36216196">[50 more]</label></div><br/><div class="children"><div class="content">ggml and llama.cpp are such a good platform for local LLMs, having some financial backing to support development is brilliant. We should be concentrating as much as possible to do local inference (and training) based on privet data.<p>I want a <i>local</i> ChatGPT fine tuned on my personal data running on my own device, not in the cloud. Ideally open source too, llama.cpp is looking like the best bet to achieve that!</div><br/><div id="36217604" class="c"><input type="checkbox" id="c-36217604" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#36216196">parent</a><span>|</span><a href="#36217847">next</a><span>|</span><label class="collapse" for="c-36217604">[-]</label><label class="expand" for="c-36217604">[5 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m wrong, but I don&#x27;t think you want it fine-tuned on your data.<p>Pretty sure you might be looking for this: <a href="https:&#x2F;&#x2F;github.com&#x2F;SamurAIGPT&#x2F;privateGPT">https:&#x2F;&#x2F;github.com&#x2F;SamurAIGPT&#x2F;privateGPT</a><p>Fine-tuning is good for treating it how to act, but not great for reciting&#x2F;recalling data.</div><br/><div id="36219307" class="c"><input type="checkbox" id="c-36219307" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217604">parent</a><span>|</span><a href="#36220595">prev</a><span>|</span><a href="#36217847">next</a><span>|</span><label class="collapse" for="c-36219307">[-]</label><label class="expand" for="c-36219307">[3 more]</label></div><br/><div class="children"><div class="content">How does this work?</div><br/><div id="36219423" class="c"><input type="checkbox" id="c-36219423" checked=""/><div class="controls bullet"><span class="by">deet</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36219307">parent</a><span>|</span><a href="#36220553">next</a><span>|</span><label class="collapse" for="c-36219423">[-]</label><label class="expand" for="c-36219423">[1 more]</label></div><br/><div class="children"><div class="content">The parent is saying that &quot;fine tuning&quot;, which has a specific meaning related to actually retraining the model itself (or layers at its surface) on a specialized set of data, is not what the GP is actually looking for.<p>An alternative method is to index content in a database and then insert contextual hints into the LLM&#x27;s prompt that give it extra information and detail with which to respond with an answer on-the-fly.<p>That database can use semantic similarity (ie via a vector database), keyword search, or other ranking methods to decide what context to inject into the prompt.<p>PrivateGPT is doing this method, reading files, extracting their content, splitting the documents into small-enough-to-fit-into-prompt bits, and then indexing into a database. Then, at query time, it inserts context into the LLM prompt<p>The repo uses LangChain as boilerplate but it&#x27;s pretty easily to do manually or with other frameworks.<p>(PS if anyone wants this type of local LLM + document Q&#x2F;A and agents, it&#x27;s something I&#x27;m working on as supported product integrated into macOS, and using ggml; see profile)</div><br/></div></div><div id="36220553" class="c"><input type="checkbox" id="c-36220553" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36219307">parent</a><span>|</span><a href="#36219423">prev</a><span>|</span><a href="#36217847">next</a><span>|</span><label class="collapse" for="c-36220553">[-]</label><label class="expand" for="c-36220553">[1 more]</label></div><br/><div class="children"><div class="content">deet already gave a comprehensive answer, but I&#x27;ll add that the guts of privateGPT are pretty readable and only ~200 lines of code.<p>Core pieces: GPT4All (LLM interface&#x2F;bindings), Chroma (vector store), HuggingFaceEmbeddings (for embeddings), and Langchain to tie everything together.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;SamurAIGPT&#x2F;privateGPT&#x2F;blob&#x2F;main&#x2F;server&#x2F;privateGPT.py">https:&#x2F;&#x2F;github.com&#x2F;SamurAIGPT&#x2F;privateGPT&#x2F;blob&#x2F;main&#x2F;server&#x2F;pr...</a></div><br/></div></div></div></div></div></div><div id="36217847" class="c"><input type="checkbox" id="c-36217847" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36216196">parent</a><span>|</span><a href="#36217604">prev</a><span>|</span><a href="#36221973">next</a><span>|</span><label class="collapse" for="c-36217847">[-]</label><label class="expand" for="c-36217847">[7 more]</label></div><br/><div class="children"><div class="content">Can LLaMA be used for commerical purposes though (might limit external contributors)? I believe, FOSS alternatives like DataBricks <i>Dolly</i> &#x2F; Together <i>RedPajama</i> &#x2F; Eluether <i>GPT NeoX</i> (et al) is where the most progress is likely to be at.</div><br/><div id="36219223" class="c"><input type="checkbox" id="c-36219223" checked=""/><div class="controls bullet"><span class="by">detrites</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217847">parent</a><span>|</span><a href="#36217910">next</a><span>|</span><label class="collapse" for="c-36219223">[-]</label><label class="expand" for="c-36219223">[1 more]</label></div><br/><div class="children"><div class="content">May also be worth mentioning - UAE&#x27;s Falcon, which apparently performs well (leads?). Falcon recently had its royalty-based commercial license modified to be fully open for free private and commercial use, via Apache 2.0: <a href="https:&#x2F;&#x2F;falconllm.tii.ae&#x2F;" rel="nofollow">https:&#x2F;&#x2F;falconllm.tii.ae&#x2F;</a></div><br/></div></div><div id="36217910" class="c"><input type="checkbox" id="c-36217910" checked=""/><div class="controls bullet"><span class="by">samwillis</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217847">parent</a><span>|</span><a href="#36219223">prev</a><span>|</span><a href="#36219343">next</a><span>|</span><label class="collapse" for="c-36217910">[-]</label><label class="expand" for="c-36217910">[1 more]</label></div><br/><div class="children"><div class="content">Although llama.cpp started with the LLaMA model, it now supports many others.</div><br/></div></div><div id="36219343" class="c"><input type="checkbox" id="c-36219343" checked=""/><div class="controls bullet"><span class="by">digitallyfree</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217847">parent</a><span>|</span><a href="#36217910">prev</a><span>|</span><a href="#36218688">next</a><span>|</span><label class="collapse" for="c-36219343">[-]</label><label class="expand" for="c-36219343">[1 more]</label></div><br/><div class="children"><div class="content">OpenLLAMA will be released soon and it&#x27;s 100% compatible with the original LLAMA.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama">https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama</a></div><br/></div></div><div id="36218688" class="c"><input type="checkbox" id="c-36218688" checked=""/><div class="controls bullet"><span class="by">okhuman</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217847">parent</a><span>|</span><a href="#36219343">prev</a><span>|</span><a href="#36219290">next</a><span>|</span><label class="collapse" for="c-36218688">[-]</label><label class="expand" for="c-36218688">[1 more]</label></div><br/><div class="children"><div class="content">This is a very good question that will be interesting how this develops. thanks for posting the alternatives list.</div><br/></div></div><div id="36219290" class="c"><input type="checkbox" id="c-36219290" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217847">parent</a><span>|</span><a href="#36218688">prev</a><span>|</span><a href="#36221973">next</a><span>|</span><label class="collapse" for="c-36219290">[-]</label><label class="expand" for="c-36219290">[2 more]</label></div><br/><div class="children"><div class="content">Why is commercial necessary to run local models?</div><br/><div id="36219403" class="c"><input type="checkbox" id="c-36219403" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36219290">parent</a><span>|</span><a href="#36221973">next</a><span>|</span><label class="collapse" for="c-36219403">[-]</label><label class="expand" for="c-36219403">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t, but such models may eventually lag behind the FOSS ones.</div><br/></div></div></div></div></div></div><div id="36221973" class="c"><input type="checkbox" id="c-36221973" checked=""/><div class="controls bullet"><span class="by">shostack</span><span>|</span><a href="#36216196">parent</a><span>|</span><a href="#36217847">prev</a><span>|</span><a href="#36216377">next</a><span>|</span><label class="collapse" for="c-36221973">[-]</label><label class="expand" for="c-36221973">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been trying to figure out what I might need to do in order to turn my Obsidian vault into a dataset to fine tune against. I&#x27;d invest a lot more into it now if I thought it would be a key to an AI learning about my the way it does in the movie Her.</div><br/><div id="36222384" class="c"><input type="checkbox" id="c-36222384" checked=""/><div class="controls bullet"><span class="by">58x14</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36221973">parent</a><span>|</span><a href="#36216377">next</a><span>|</span><label class="collapse" for="c-36222384">[-]</label><label class="expand" for="c-36222384">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been working on this for awhile now and I&#x27;d love to chat. I&#x27;ll send you an email.</div><br/><div id="36222595" class="c"><input type="checkbox" id="c-36222595" checked=""/><div class="controls bullet"><span class="by">legendofbrando</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36222384">parent</a><span>|</span><a href="#36216377">next</a><span>|</span><label class="collapse" for="c-36222595">[-]</label><label class="expand" for="c-36222595">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m interested in this as well and have been exploring similarly. Would be super interesting to chat if you&#x27;re up for it as well. Sending you an email to say hello.</div><br/></div></div></div></div></div></div><div id="36216377" class="c"><input type="checkbox" id="c-36216377" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216196">parent</a><span>|</span><a href="#36221973">prev</a><span>|</span><a href="#36216465">next</a><span>|</span><label class="collapse" for="c-36216377">[-]</label><label class="expand" for="c-36216377">[12 more]</label></div><br/><div class="children"><div class="content">If MeZO gets implemented, we are basically there: <a href="https:&#x2F;&#x2F;github.com&#x2F;princeton-nlp&#x2F;MeZO">https:&#x2F;&#x2F;github.com&#x2F;princeton-nlp&#x2F;MeZO</a></div><br/><div id="36216988" class="c"><input type="checkbox" id="c-36216988" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216377">parent</a><span>|</span><a href="#36216465">next</a><span>|</span><label class="collapse" for="c-36216988">[-]</label><label class="expand" for="c-36216988">[11 more]</label></div><br/><div class="children"><div class="content">Basically there, with what kind of VRAM and processing requirements? I doubt anyone running on a CPU can fine tune in a time frame that doesn&#x27;t give them an obsolete model when they&#x27;re done.</div><br/><div id="36217136" class="c"><input type="checkbox" id="c-36217136" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216988">parent</a><span>|</span><a href="#36216465">next</a><span>|</span><label class="collapse" for="c-36217136">[-]</label><label class="expand" for="c-36217136">[10 more]</label></div><br/><div class="children"><div class="content">According to the paper it fine tunes at the speed of inference (!!)<p>This would make fine tuning a qantized 13B model achievable in ~0.3 seconds per training example on a CPU.</div><br/><div id="36217354" class="c"><input type="checkbox" id="c-36217354" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217136">parent</a><span>|</span><a href="#36218841">next</a><span>|</span><label class="collapse" for="c-36217354">[-]</label><label class="expand" for="c-36217354">[1 more]</label></div><br/><div class="children"><div class="content">MeZO assumes a smooth parameter space, so you probably won&#x27;t be able to do it with INT4&#x2F;8 quantization, probably needs fp8 or smoother.</div><br/></div></div><div id="36218841" class="c"><input type="checkbox" id="c-36218841" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217136">parent</a><span>|</span><a href="#36217354">prev</a><span>|</span><a href="#36218026">next</a><span>|</span><label class="collapse" for="c-36218841">[-]</label><label class="expand" for="c-36218841">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the same <i>memory footprint</i> as inference. It&#x27;s not that fast, and the paper mentions some optimizations that could still be done.</div><br/><div id="36220688" class="c"><input type="checkbox" id="c-36220688" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36218841">parent</a><span>|</span><a href="#36218026">next</a><span>|</span><label class="collapse" for="c-36220688">[-]</label><label class="expand" for="c-36220688">[1 more]</label></div><br/><div class="children"><div class="content">Yes you are right.<p>I completely misread that!</div><br/></div></div></div></div><div id="36218026" class="c"><input type="checkbox" id="c-36218026" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217136">parent</a><span>|</span><a href="#36218841">prev</a><span>|</span><a href="#36217827">next</a><span>|</span><label class="collapse" for="c-36218026">[-]</label><label class="expand" for="c-36218026">[1 more]</label></div><br/><div class="children"><div class="content">I cannot find any such numbers in the paper. What the paper says is that MeZO converges much slower than SGD, and each step needs two forward passes.<p>&quot;As a limitation, MeZO takes many steps in order to achieve strong performance.&quot;</div><br/></div></div><div id="36217827" class="c"><input type="checkbox" id="c-36217827" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217136">parent</a><span>|</span><a href="#36218026">prev</a><span>|</span><a href="#36217324">next</a><span>|</span><label class="collapse" for="c-36217827">[-]</label><label class="expand" for="c-36217827">[3 more]</label></div><br/><div class="children"><div class="content">If you go through the drudgery of integrating with all the existing channels (mail, Teams, discord, slack, traditional social media, texts, ...), such rapid finetuning speeds could enable an always up to date personality construct, modeled on you.<p>Which is my personal holy grail towards making myself unnecessary; it&#x27;d be amazing to be doing some light gardening while the bot handles my coworkers ;)</div><br/><div id="36221420" class="c"><input type="checkbox" id="c-36221420" checked=""/><div class="controls bullet"><span class="by">vgb2k18</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217827">parent</a><span>|</span><a href="#36217987">next</a><span>|</span><label class="collapse" for="c-36221420">[-]</label><label class="expand" for="c-36221420">[1 more]</label></div><br/><div class="children"><div class="content">&gt; while the bot handles my coworkers<p>Or it handles their bots ;)</div><br/></div></div></div></div><div id="36217324" class="c"><input type="checkbox" id="c-36217324" checked=""/><div class="controls bullet"><span class="by">valval</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217136">parent</a><span>|</span><a href="#36217827">prev</a><span>|</span><a href="#36217261">next</a><span>|</span><label class="collapse" for="c-36217324">[-]</label><label class="expand" for="c-36217324">[1 more]</label></div><br/><div class="children"><div class="content">I think more importantly, what would the fine tuning routine look like? It&#x27;s a non-trivial task to dump all of your personal data into any LLM architecture.</div><br/></div></div><div id="36217261" class="c"><input type="checkbox" id="c-36217261" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217136">parent</a><span>|</span><a href="#36217324">prev</a><span>|</span><a href="#36216465">next</a><span>|</span><label class="collapse" for="c-36217261">[-]</label><label class="expand" for="c-36217261">[1 more]</label></div><br/><div class="children"><div class="content">Wow if that&#x27;s true then it&#x27;s genuinely a complete gamechanger for LLMs as a whole. You probably mean more like 0.3s per token, not per example, but that&#x27;s still more like 1 or two minutes per training case, not like a day for 4 cases like it is now.</div><br/></div></div></div></div></div></div></div></div><div id="36216465" class="c"><input type="checkbox" id="c-36216465" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#36216196">parent</a><span>|</span><a href="#36216377">prev</a><span>|</span><a href="#36216508">next</a><span>|</span><label class="collapse" for="c-36216465">[-]</label><label class="expand" for="c-36216465">[10 more]</label></div><br/><div class="children"><div class="content">&gt; ggml and llama.cpp are such a good platform for local LLMs, having some financial backing to support development is brilliant<p>The problem is, this financial backing and support is via VCs, who will steer the project to close it all up again.<p>&gt; I want a local ChatGPT fine tuned on my personal data running on my own device, not in the cloud. Ideally open source too, llama.cpp is looking like the best bet to achieve that!<p>I think you are setting yourself up for disappointment in the future.</div><br/><div id="36219154" class="c"><input type="checkbox" id="c-36219154" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216465">parent</a><span>|</span><a href="#36216838">next</a><span>|</span><label class="collapse" for="c-36219154">[-]</label><label class="expand" for="c-36219154">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>The problem is, this financial backing and support is via VCs, who will steer the project to close it all up again.</i><p>A matter of <i>when</i>, not <i>if</i>. I mean, the website itself makes that much clear:<p><pre><code>  The ggml way
  
    ...
  
    Open Core

    The library and related projects are freely available under the MIT license... In the future we may choose to develop extensions that are licensed for commercial use
  
    Explore and have fun!

    ... Contributors are encouraged to try crazy ideas, build wild demos, and push the edge of what&#x27;s possible

</code></pre>
So, like many other &quot;open core&quot; devtools out there, they&#x27;d like to have their cake and eat it too. And they might just as well, like others before them.<p>Won&#x27;t blame anyone here though; because clearly, if you&#x27;re as good as Georgi Gerganov, why do it for free?</div><br/><div id="36223453" class="c"><input type="checkbox" id="c-36223453" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36219154">parent</a><span>|</span><a href="#36216838">next</a><span>|</span><label class="collapse" for="c-36223453">[-]</label><label class="expand" for="c-36223453">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like the SQLite model, which has been a net positive for the computing world.</div><br/></div></div></div></div><div id="36216838" class="c"><input type="checkbox" id="c-36216838" checked=""/><div class="controls bullet"><span class="by">ulchar</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216465">parent</a><span>|</span><a href="#36219154">prev</a><span>|</span><a href="#36217184">next</a><span>|</span><label class="collapse" for="c-36216838">[-]</label><label class="expand" for="c-36216838">[6 more]</label></div><br/><div class="children"><div class="content">&gt; The problem is, this financial backing and support is via VCs, who will steer the project to close it all up again.<p>How exactly could they meaningfully do that? Genuine question. The issue with the OpenAI business model is that the collaboration within academia and open source circles is creating innovations that are on track to out-pace the closed source approach. Does OpenAI have the pockets to buy the open source collaborators and researchers?<p>I&#x27;m truly cynical about many aspects of the tech industry but this is one of those fights that open source could win for the betterment of everybody.</div><br/><div id="36217454" class="c"><input type="checkbox" id="c-36217454" checked=""/><div class="controls bullet"><span class="by">yyyk</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216838">parent</a><span>|</span><a href="#36217177">next</a><span>|</span><label class="collapse" for="c-36217454">[-]</label><label class="expand" for="c-36217454">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been going on and on about this in HN: Open source can win this fight, but I think OSS is overconfident. We need to be clear there are serious challenges ahead - ClosedAI and other corporations also have a plan, a plan that has good chances unless properly countered:<p>A) Embed OpenAI (etc.) API everywhere. Make embedding easy and trivial. First to gain a small API&#x2F;install moat (user&#x2F;dev: &#x27;why install OSS model when OpenAI is already available with an OS API?&#x27;). If it&#x27;s easy to use OpenAI but not open source they have an advantage. Second to gain brand. But more importantly:<p>B) Gain a technical moat by having a permanent data advantage using the existing install base (see above). Retune constantly to keep it.<p>C) Combine with existing propriety data stores to increase local data advantage (e.g. easy access for all your Office 365&#x2F;GSuite documents, while OSS gets the scary permission prompts).<p>D) Combine with existing propriety moats to mutually reinforce.<p>E) Use selective copyright enforcement to increase data advantage.<p>F) Lobby legislators for limits that make competition (open or closed source) way harder.<p>TL;DR: OSS is probably catching up on algorithms. When it comes to good data and good integrations OSS is far behind and not yet catching up. It&#x27;s been argued that OpenAI&#x27;s entire performance advantage is due to having better data alone, and they intend to keep that advantage.</div><br/><div id="36218897" class="c"><input type="checkbox" id="c-36218897" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217454">parent</a><span>|</span><a href="#36217177">next</a><span>|</span><label class="collapse" for="c-36218897">[-]</label><label class="expand" for="c-36218897">[1 more]</label></div><br/><div class="children"><div class="content">Don’t forget chip shortages. That’s all centralized up through Nvidia, TSMC, and ASML</div><br/></div></div></div></div><div id="36217177" class="c"><input type="checkbox" id="c-36217177" checked=""/><div class="controls bullet"><span class="by">maxilevi</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216838">parent</a><span>|</span><a href="#36217454">prev</a><span>|</span><a href="#36217184">next</a><span>|</span><label class="collapse" for="c-36217177">[-]</label><label class="expand" for="c-36217177">[3 more]</label></div><br/><div class="children"><div class="content">I agree with the spirit but saying that open source is on track to outpace OpenAI in innovation is just not true. Open source models are being compared to GPT3.5, none yet even get close to GPT4 quality and they finished that last year.</div><br/><div id="36218569" class="c"><input type="checkbox" id="c-36218569" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217177">parent</a><span>|</span><a href="#36217184">next</a><span>|</span><label class="collapse" for="c-36218569">[-]</label><label class="expand" for="c-36218569">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;re basically surviving off the scraps companies like Facebook have been tossing off the table, like LLaMA. The fact that we&#x27;re even allowed and able to use these things ourselves, at all, is a tremendous victory.</div><br/><div id="36218687" class="c"><input type="checkbox" id="c-36218687" checked=""/><div class="controls bullet"><span class="by">maxilevi</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36218569">parent</a><span>|</span><a href="#36217184">next</a><span>|</span><label class="collapse" for="c-36218687">[-]</label><label class="expand" for="c-36218687">[1 more]</label></div><br/><div class="children"><div class="content">I agree</div><br/></div></div></div></div></div></div></div></div><div id="36217184" class="c"><input type="checkbox" id="c-36217184" checked=""/><div class="controls bullet"><span class="by">jdonaldson</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216465">parent</a><span>|</span><a href="#36216838">prev</a><span>|</span><a href="#36216508">next</a><span>|</span><label class="collapse" for="c-36217184">[-]</label><label class="expand" for="c-36217184">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think you are setting yourself up for disappointment in the future.<p>Why would you say that?</div><br/></div></div></div></div><div id="36216508" class="c"><input type="checkbox" id="c-36216508" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#36216196">parent</a><span>|</span><a href="#36216465">prev</a><span>|</span><a href="#36216376">next</a><span>|</span><label class="collapse" for="c-36216508">[-]</label><label class="expand" for="c-36216508">[12 more]</label></div><br/><div class="children"><div class="content">I wonder if ClosedAI and other companies use the findings of the open source community in their products. For example, do they use QLORA to reduce the costs of training and inference? Do they quantize their models to serve non-subscribing consumers?</div><br/><div id="36217149" class="c"><input type="checkbox" id="c-36217149" checked=""/><div class="controls bullet"><span class="by">jmoss20</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216508">parent</a><span>|</span><a href="#36216688">next</a><span>|</span><label class="collapse" for="c-36217149">[-]</label><label class="expand" for="c-36217149">[1 more]</label></div><br/><div class="children"><div class="content">Quantization is hardly a &quot;finding of the open source community&quot;. (IIRC the first TPU was int8! Though the tradition is much older than that.)</div><br/></div></div><div id="36216688" class="c"><input type="checkbox" id="c-36216688" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216508">parent</a><span>|</span><a href="#36217149">prev</a><span>|</span><a href="#36216376">next</a><span>|</span><label class="collapse" for="c-36216688">[-]</label><label class="expand" for="c-36216688">[10 more]</label></div><br/><div class="children"><div class="content">Not disagreeing with your points, but saying &quot;ClosedAI&quot; is about as clever as writing M$ for Microsoft back in the day, which is to say not very.</div><br/><div id="36218979" class="c"><input type="checkbox" id="c-36218979" checked=""/><div class="controls bullet"><span class="by">Miraste</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216688">parent</a><span>|</span><a href="#36216958">next</a><span>|</span><label class="collapse" for="c-36218979">[-]</label><label class="expand" for="c-36218979">[2 more]</label></div><br/><div class="children"><div class="content">M$ is a silly way to call Microsoft greedy. ClosedAI is somewhat better because OpenAI&#x27;s very name is a bald-faced lie, and they should be called on it. Are there more elegant ways to do that? Sure, but every time I see Altman in the news crying crocodile tears about the &quot;dangers&quot; of open anything I think we need all the forms of opposition we can find.</div><br/><div id="36220202" class="c"><input type="checkbox" id="c-36220202" checked=""/><div class="controls bullet"><span class="by">tanseydavid</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36218979">parent</a><span>|</span><a href="#36216958">next</a><span>|</span><label class="collapse" for="c-36220202">[-]</label><label class="expand" for="c-36220202">[1 more]</label></div><br/><div class="children"><div class="content">It is a colloquial spelling and they earned it, a long time ago.</div><br/></div></div></div></div><div id="36216958" class="c"><input type="checkbox" id="c-36216958" checked=""/><div class="controls bullet"><span class="by">loa_in_</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216688">parent</a><span>|</span><a href="#36218979">prev</a><span>|</span><a href="#36217145">next</a><span>|</span><label class="collapse" for="c-36216958">[-]</label><label class="expand" for="c-36216958">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say saying M$ makes it harder for M$ to find out I&#x27;m talking about them in them in the indexed web because it&#x27;s more ambiguous, that&#x27;s all I need to know.</div><br/><div id="36218186" class="c"><input type="checkbox" id="c-36218186" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216958">parent</a><span>|</span><a href="#36217145">next</a><span>|</span><label class="collapse" for="c-36218186">[-]</label><label class="expand" for="c-36218186">[1 more]</label></div><br/><div class="children"><div class="content">If we are talking about indexing, writing M$ is easier to find in an index because it is a such unique token. MS can mean many things (e.g. Miss), M$ is less ambiguous.</div><br/></div></div></div></div><div id="36217145" class="c"><input type="checkbox" id="c-36217145" checked=""/><div class="controls bullet"><span class="by">rafark</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216688">parent</a><span>|</span><a href="#36216958">prev</a><span>|</span><a href="#36218362">next</a><span>|</span><label class="collapse" for="c-36217145">[-]</label><label class="expand" for="c-36217145">[4 more]</label></div><br/><div class="children"><div class="content">I think it’s ironic that M$ made ClosedAI.</div><br/><div id="36218112" class="c"><input type="checkbox" id="c-36218112" checked=""/><div class="controls bullet"><span class="by">replygirl</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36217145">parent</a><span>|</span><a href="#36218362">next</a><span>|</span><label class="collapse" for="c-36218112">[-]</label><label class="expand" for="c-36218112">[3 more]</label></div><br/><div class="children"><div class="content">Pedantic but that&#x27;s not irony</div><br/><div id="36220087" class="c"><input type="checkbox" id="c-36220087" checked=""/><div class="controls bullet"><span class="by">rafark</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36218112">parent</a><span>|</span><a href="#36218362">next</a><span>|</span><label class="collapse" for="c-36220087">[-]</label><label class="expand" for="c-36220087">[2 more]</label></div><br/><div class="children"><div class="content">Why do you think so? According to the dictionary, ironic could be something paradoxical or weird.</div><br/><div id="36220713" class="c"><input type="checkbox" id="c-36220713" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36220087">parent</a><span>|</span><a href="#36218362">next</a><span>|</span><label class="collapse" for="c-36220713">[-]</label><label class="expand" for="c-36220713">[1 more]</label></div><br/><div class="children"><div class="content">Well it&#x27;s not paradoxical?<p>If one is the kind of person who writes M$ then it&#x27;s pretty much expected behaviour.</div><br/></div></div></div></div></div></div></div></div><div id="36218362" class="c"><input type="checkbox" id="c-36218362" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36216196">root</a><span>|</span><a href="#36216688">parent</a><span>|</span><a href="#36217145">prev</a><span>|</span><a href="#36216376">next</a><span>|</span><label class="collapse" for="c-36218362">[-]</label><label class="expand" for="c-36218362">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I think it feigns meaningful criticism. The &quot;Sleepy Joe&quot;-tier insults are ad-hominem enough that I don&#x27;t try to respond.</div><br/></div></div></div></div></div></div></div></div><div id="36216376" class="c"><input type="checkbox" id="c-36216376" checked=""/><div class="controls bullet"><span class="by">yukIttEft</span><span>|</span><a href="#36216196">prev</a><span>|</span><a href="#36216311">next</a><span>|</span><label class="collapse" for="c-36216376">[-]</label><label class="expand" for="c-36216376">[8 more]</label></div><br/><div class="children"><div class="content">Its graph execution is still full of busyloops, e.g.:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;44f906e8537fcec965e312d621c80556d6aa9bec&#x2F;ggml.c#L14575">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;44f906e8537fcec9...</a><p>I wonder how much more efficient it would be when Taskflow lib was used instead, or even inteltbb.</div><br/><div id="36218226" class="c"><input type="checkbox" id="c-36218226" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#36216376">parent</a><span>|</span><a href="#36217006">next</a><span>|</span><label class="collapse" for="c-36218226">[-]</label><label class="expand" for="c-36218226">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a very good library IMO.</div><br/><div id="36224140" class="c"><input type="checkbox" id="c-36224140" checked=""/><div class="controls bullet"><span class="by">dindresto</span><span>|</span><a href="#36216376">root</a><span>|</span><a href="#36218226">parent</a><span>|</span><a href="#36217006">next</a><span>|</span><label class="collapse" for="c-36224140">[-]</label><label class="expand" for="c-36224140">[1 more]</label></div><br/><div class="children"><div class="content">ggml or Intel TBB?</div><br/></div></div></div></div><div id="36217006" class="c"><input type="checkbox" id="c-36217006" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36216376">parent</a><span>|</span><a href="#36218226">prev</a><span>|</span><a href="#36217540">next</a><span>|</span><label class="collapse" for="c-36217006">[-]</label><label class="expand" for="c-36217006">[1 more]</label></div><br/><div class="children"><div class="content">Someone ought to be along with a PR eventually.</div><br/></div></div><div id="36217540" class="c"><input type="checkbox" id="c-36217540" checked=""/><div class="controls bullet"><span class="by">boywitharupee</span><span>|</span><a href="#36216376">parent</a><span>|</span><a href="#36217006">prev</a><span>|</span><a href="#36217840">next</a><span>|</span><label class="collapse" for="c-36217540">[-]</label><label class="expand" for="c-36217540">[2 more]</label></div><br/><div class="children"><div class="content">is graph execution used for training only or inference also?</div><br/><div id="36217851" class="c"><input type="checkbox" id="c-36217851" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36216376">root</a><span>|</span><a href="#36217540">parent</a><span>|</span><a href="#36217840">next</a><span>|</span><label class="collapse" for="c-36217851">[-]</label><label class="expand" for="c-36217851">[1 more]</label></div><br/><div class="children"><div class="content">Inference. It&#x27;s a big bottleneck for RWKV.cpp, second only to the matrix multiplies.</div><br/></div></div></div></div><div id="36217840" class="c"><input type="checkbox" id="c-36217840" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#36216376">parent</a><span>|</span><a href="#36217540">prev</a><span>|</span><a href="#36216311">next</a><span>|</span><label class="collapse" for="c-36217840">[-]</label><label class="expand" for="c-36217840">[2 more]</label></div><br/><div class="children"><div class="content">does tbb work with apple Silicon?</div><br/><div id="36217968" class="c"><input type="checkbox" id="c-36217968" checked=""/><div class="controls bullet"><span class="by">yukIttEft</span><span>|</span><a href="#36216376">root</a><span>|</span><a href="#36217840">parent</a><span>|</span><a href="#36216311">next</a><span>|</span><label class="collapse" for="c-36217968">[-]</label><label class="expand" for="c-36217968">[1 more]</label></div><br/><div class="children"><div class="content">I guess
<a href="https:&#x2F;&#x2F;formulae.brew.sh&#x2F;formula&#x2F;tbb" rel="nofollow">https:&#x2F;&#x2F;formulae.brew.sh&#x2F;formula&#x2F;tbb</a></div><br/></div></div></div></div></div></div><div id="36216311" class="c"><input type="checkbox" id="c-36216311" checked=""/><div class="controls bullet"><span class="by">kretaceous</span><span>|</span><a href="#36216376">prev</a><span>|</span><a href="#36222742">next</a><span>|</span><label class="collapse" for="c-36216311">[-]</label><label class="expand" for="c-36216311">[2 more]</label></div><br/><div class="children"><div class="content">Georgi&#x27;s Twitter announcement: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1666120568993730561" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1666120568993730561</a></div><br/><div id="36216686" class="c"><input type="checkbox" id="c-36216686" checked=""/><div class="controls bullet"><span class="by">jgrahamc</span><span>|</span><a href="#36216311">parent</a><span>|</span><a href="#36222742">next</a><span>|</span><label class="collapse" for="c-36216686">[-]</label><label class="expand" for="c-36216686">[1 more]</label></div><br/><div class="children"><div class="content">Cool. I&#x27;ve just started sponsoring him on GitHub.</div><br/></div></div></div></div><div id="36222742" class="c"><input type="checkbox" id="c-36222742" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#36216311">prev</a><span>|</span><a href="#36222913">next</a><span>|</span><label class="collapse" for="c-36222742">[-]</label><label class="expand" for="c-36222742">[2 more]</label></div><br/><div class="children"><div class="content">Georgi if you&#x27;re reading this, I&#x27;ve had a lot of fun with whisper.cpp llama.cpp because of you so thank you very much.</div><br/><div id="36224059" class="c"><input type="checkbox" id="c-36224059" checked=""/><div class="controls bullet"><span class="by">hanselot</span><span>|</span><a href="#36222742">parent</a><span>|</span><a href="#36222913">next</a><span>|</span><label class="collapse" for="c-36224059">[-]</label><label class="expand" for="c-36224059">[1 more]</label></div><br/><div class="children"><div class="content">I envy his drive and ambition. I can&#x27;t force myself to finish writing a simple alarm clock app for android, never mind pathing the literal road to the future of Open Source AI.<p>Would someone else have taken his place had he not been around? Maybe, but I&#x27;m insanely happy that he is around.<p>The amount of hours I&#x27;ve sunk into LLM&#x27;s is crazy, and it&#x27;s mostly thanks to his work that I can both run and download models in meaningful timeframes.<p>And yes, I have tested llama.cpp on my android and it works 100% on termux. (Your biggest enemy here will be Android process reaper when you hit the memory cap)</div><br/></div></div></div></div><div id="36222913" class="c"><input type="checkbox" id="c-36222913" checked=""/><div class="controls bullet"><span class="by">ankitg12</span><span>|</span><a href="#36222742">prev</a><span>|</span><a href="#36219204">next</a><span>|</span><label class="collapse" for="c-36222913">[-]</label><label class="expand" for="c-36222913">[1 more]</label></div><br/><div class="children"><div class="content">Quite impressive, able to run a LLM on my local mac<p><pre><code>    % .&#x2F;bin&#x2F;gpt-2 -m models&#x2F;gpt-2-117M&#x2F;ggml-model.bin -p &quot;Let&#x27;s talk about Machine Learning now&quot;
    main: seed = 1686112244
    gpt2_model_load: loading model from &#x27;models&#x2F;gpt-2-117M&#x2F;ggml-model.bin&#x27;
    gpt2_model_load: n_vocab = 50257
    gpt2_model_load: n_ctx   = 1024
    gpt2_model_load: n_embd  = 768
    gpt2_model_load: n_head  = 12
    gpt2_model_load: n_layer = 12
    gpt2_model_load: ftype   = 1
    gpt2_model_load: qntvr   = 0
    gpt2_model_load: ggml tensor size = 224 bytes
    gpt2_model_load: ggml ctx size = 384.77 MB
    gpt2_model_load: memory size =    72.00 MB, n_mem = 12288
    gpt2_model_load: model size  =   239.08 MB
    extract_tests_from_file : No test file found.
    test_gpt_tokenizer : 0 tests failed out of 0 tests.
    main: prompt: &#x27;Let&#x27;s talk about Machine Learning now&#x27;
    main: number of tokens in prompt = 7, first 8 tokens: 5756 338 1561 546 10850 18252 783

    Let&#x27;s talk about Machine Learning now.

    The first step is to get a good understanding of what machine learning is. This is where things get messy. What do you think is the most difficult aspect of machine learning?

    Machine learning is the process of transforming data into an understanding of its contents and its operations. For example, in the following diagram, you can see that we use a machine learning approach to model an object.

    The object is a piece of a puzzle with many different components and some of the problems it solves will be difficult to solve for humans.

    What do you think of machine learning as?

    Machine learning is one of the most important, because it can help us understand how our data are structured. You can understand the structure of the data as the object is represented in its representation.

    What about data structures? How do you find out where a data structure or a structure is located in your data?

    In a lot of fields, you can think of structures as

    main: mem per token =  2008284 bytes
    main:     load time =   366.33 ms
    main:   sample time =    39.59 ms
    main:  predict time =  3448.31 ms &#x2F; 16.74 ms per token
    main:    total time =  3894.15 ms</code></pre></div><br/></div></div><div id="36219204" class="c"><input type="checkbox" id="c-36219204" checked=""/><div class="controls bullet"><span class="by">iamflimflam1</span><span>|</span><a href="#36222913">prev</a><span>|</span><a href="#36218756">next</a><span>|</span><label class="collapse" for="c-36219204">[-]</label><label class="expand" for="c-36219204">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always thought on the edge to be IoT type stuff. So running on embedded devices. But maybe that not the case?</div><br/><div id="36219239" class="c"><input type="checkbox" id="c-36219239" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#36219204">parent</a><span>|</span><a href="#36219263">next</a><span>|</span><label class="collapse" for="c-36219239">[-]</label><label class="expand" for="c-36219239">[2 more]</label></div><br/><div class="children"><div class="content">Like any new term the (mis)usage broadens the meaning over time until it either it&#x27;s widely known, it&#x27;s unfashionable, or most likely; it becomes so broad as to be meaningless and hence it achieves buzzword apotheosis.<p>My old job title had &quot;edge&quot; in it, and I still don&#x27;t know what it&#x27;s supposed to mean, although &quot;not cloud&quot; is a good approximation.</div><br/><div id="36219335" class="c"><input type="checkbox" id="c-36219335" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#36219204">root</a><span>|</span><a href="#36219239">parent</a><span>|</span><a href="#36219263">next</a><span>|</span><label class="collapse" for="c-36219335">[-]</label><label class="expand" for="c-36219335">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like your job had a lot of velocity with lateral tragmorphicity in Q1, just in time for staff engineer optimization!<p>Nicely done. Here is ~$50 worth of stock.</div><br/></div></div></div></div><div id="36219263" class="c"><input type="checkbox" id="c-36219263" checked=""/><div class="controls bullet"><span class="by">timerol</span><span>|</span><a href="#36219204">parent</a><span>|</span><a href="#36219239">prev</a><span>|</span><a href="#36218756">next</a><span>|</span><label class="collapse" for="c-36219263">[-]</label><label class="expand" for="c-36219263">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Edge computing&quot; is a pretty vague term, and can encompass anything from a 8MHz ARM core that can barely talk compliant BLE, all the way to a multi-thousand dollar setup on something like a self-checkout machine, which may have more compute available than your average laptop. In that range are home assistants, which normally have some basic ML for wake word detection, and then send the next bit of audio to the cloud with a more advanced model for full speech-to-text (and response)</div><br/></div></div></div></div><div id="36218756" class="c"><input type="checkbox" id="c-36218756" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36219204">prev</a><span>|</span><a href="#36218585">next</a><span>|</span><label class="collapse" for="c-36218756">[-]</label><label class="expand" for="c-36218756">[4 more]</label></div><br/><div class="children"><div class="content">Really impressive work and I&#x27;ve asked this before, but is it really a good thing to have basically the whole library in a single 16k line file?</div><br/><div id="36219310" class="c"><input type="checkbox" id="c-36219310" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#36218756">parent</a><span>|</span><a href="#36220176">next</a><span>|</span><label class="collapse" for="c-36219310">[-]</label><label class="expand" for="c-36219310">[2 more]</label></div><br/><div class="children"><div class="content">It makes syncing between llama.cpp, whisper.cpp, and ggml itself quite straightforward.<p>I think the lesson here is that this setup has enabled some very high-speed project evolution or, at least, not got in its way. If that is surprising and you were expecting downsides, a) why; and b) where did they go?</div><br/><div id="36221083" class="c"><input type="checkbox" id="c-36221083" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#36218756">root</a><span>|</span><a href="#36219310">parent</a><span>|</span><a href="#36220176">next</a><span>|</span><label class="collapse" for="c-36221083">[-]</label><label class="expand" for="c-36221083">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;git-scm.com&#x2F;book&#x2F;en&#x2F;v2&#x2F;Git-Tools-Submodules" rel="nofollow">https:&#x2F;&#x2F;git-scm.com&#x2F;book&#x2F;en&#x2F;v2&#x2F;Git-Tools-Submodules</a></div><br/></div></div></div></div><div id="36220176" class="c"><input type="checkbox" id="c-36220176" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#36218756">parent</a><span>|</span><a href="#36219310">prev</a><span>|</span><a href="#36218585">next</a><span>|</span><label class="collapse" for="c-36220176">[-]</label><label class="expand" for="c-36220176">[1 more]</label></div><br/><div class="children"><div class="content">Yes.  Next question</div><br/></div></div></div></div><div id="36218585" class="c"><input type="checkbox" id="c-36218585" checked=""/><div class="controls bullet"><span class="by">KronisLV</span><span>|</span><a href="#36218756">prev</a><span>|</span><a href="#36218992">next</a><span>|</span><label class="collapse" for="c-36218585">[-]</label><label class="expand" for="c-36218585">[9 more]</label></div><br/><div class="children"><div class="content">Just today, I finished a blog post (also my latest submission, felt like could be useful to some) about how to get something like this working in a bundle of something to run models, as well as a web UI for more easy interaction - in my case that was koboldcpp, which can run GGML, both on the CPU (with OpenBLAS) and on the GPU (with CLBlast). Thanks to Hugging Face, getting Metharme, WizardLM or other models is also extremely easy, and the 4-bit quantized ones provide decent performance even on commodity hardware!<p>I tested it out both locally (6c&#x2F;12t CPU) and on a Hetzner CPX41 instance (8 AMD cores, 16 GB of RAM, no GPU), the latter of which costs about 25 EUR per month and still can generate decent responses in less than half a minute, my local machine needing approx. double that time. While not quite as good as one might expect (decent response times mean maxing out CPU for the single request, if you don&#x27;t have a compatible GPU with enough VRAM), the technology is definitely at a point where it&#x27;s possible for it to make people&#x27;s lives easier in select use cases with some supervision (e.g. customer support).<p>What an interesting time to be alive, I wonder where we&#x27;ll be in a decade.</div><br/><div id="36219214" class="c"><input type="checkbox" id="c-36219214" checked=""/><div class="controls bullet"><span class="by">digitallyfree</span><span>|</span><a href="#36218585">parent</a><span>|</span><a href="#36218947">next</a><span>|</span><label class="collapse" for="c-36219214">[-]</label><label class="expand" for="c-36219214">[1 more]</label></div><br/><div class="children"><div class="content">The fact that this is <i>commodity hardware</i> makes ggml extremely impressive and puts the tech in the hands of everyone. I recently reported my experience running 7B llama.cpp on a 15 year old Core 2 Quad [1] - when that machine came out it was a completely different world and I certainly never imagined how AI would look like today. This was around when the first iPhone was released and everyone began talking about how smartphones would become the next big thing. We saw what happened 15 years later...<p>Today with the new k-quants users are reporting that 30B models are working with 2-bit quantization on 16GB CPUs and GPUs [2]. That&#x27;s enabling access to millions of consumers and the optimizations will only improve from there.<p>[1] <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13q6hu8&#x2F;7b_performance_on_a_15_year_old_potato&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;13q6hu8&#x2F;7b_perf...</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684</a>, <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;141bdll&#x2F;moneros&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;141bdll&#x2F;moneros...</a></div><br/></div></div><div id="36218947" class="c"><input type="checkbox" id="c-36218947" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#36218585">parent</a><span>|</span><a href="#36219214">prev</a><span>|</span><a href="#36218767">next</a><span>|</span><label class="collapse" for="c-36218947">[-]</label><label class="expand" for="c-36218947">[1 more]</label></div><br/><div class="children"><div class="content">I wish everyone in tech had your perspective. That is what I see, as well.<p>There is a lull right now between gpt4 and gpt5 (literally and metaphorically). Consumer models are plateauing around 40B for a barely-reasonable RTX 3090 (ggml made this possible).<p>Now is the time to launch your ideas, all!</div><br/></div></div><div id="36218767" class="c"><input type="checkbox" id="c-36218767" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#36218585">parent</a><span>|</span><a href="#36218947">prev</a><span>|</span><a href="#36220027">next</a><span>|</span><label class="collapse" for="c-36218767">[-]</label><label class="expand" for="c-36218767">[3 more]</label></div><br/><div class="children"><div class="content">Seems like serverless is the way to go for fast output while remaining inexpensive.<p>e.g.<p><a href="https:&#x2F;&#x2F;replicate.com&#x2F;stability-ai&#x2F;stablelm-tuned-alpha-7b">https:&#x2F;&#x2F;replicate.com&#x2F;stability-ai&#x2F;stablelm-tuned-alpha-7b</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;runpod&#x2F;serverless-workers&#x2F;tree&#x2F;main&#x2F;workers&#x2F;gpt-neox">https:&#x2F;&#x2F;github.com&#x2F;runpod&#x2F;serverless-workers&#x2F;tree&#x2F;main&#x2F;worke...</a><p><a href="https:&#x2F;&#x2F;modal.com&#x2F;docs&#x2F;guide&#x2F;ex&#x2F;falcon_gptq" rel="nofollow">https:&#x2F;&#x2F;modal.com&#x2F;docs&#x2F;guide&#x2F;ex&#x2F;falcon_gptq</a></div><br/><div id="36219155" class="c"><input type="checkbox" id="c-36219155" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#36218585">root</a><span>|</span><a href="#36218767">parent</a><span>|</span><a href="#36220027">next</a><span>|</span><label class="collapse" for="c-36219155">[-]</label><label class="expand" for="c-36219155">[2 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s true if you&#x27;re doing minimal usage &#x2F; low utilization, otherwise a dedicated instance will be cheaper.</div><br/><div id="36220820" class="c"><input type="checkbox" id="c-36220820" checked=""/><div class="controls bullet"><span class="by">menzoic</span><span>|</span><a href="#36218585">root</a><span>|</span><a href="#36219155">parent</a><span>|</span><a href="#36220027">next</a><span>|</span><label class="collapse" for="c-36220820">[-]</label><label class="expand" for="c-36220820">[1 more]</label></div><br/><div class="children"><div class="content">You are correct. The pricing model guarantees this. Pay per compute vs pay for uptime (during which you could have more compute for cheaper)</div><br/></div></div></div></div></div></div><div id="36220027" class="c"><input type="checkbox" id="c-36220027" checked=""/><div class="controls bullet"><span class="by">c_o_n_v_e_x</span><span>|</span><a href="#36218585">parent</a><span>|</span><a href="#36218767">prev</a><span>|</span><a href="#36218992">next</a><span>|</span><label class="collapse" for="c-36220027">[-]</label><label class="expand" for="c-36220027">[3 more]</label></div><br/><div class="children"><div class="content">What do you mean by commodity hardware?  Single server single CPU socket x86&#x2F;ARM boxes?  Anything that does not have a GPU?</div><br/><div id="36222947" class="c"><input type="checkbox" id="c-36222947" checked=""/><div class="controls bullet"><span class="by">KronisLV</span><span>|</span><a href="#36218585">root</a><span>|</span><a href="#36220027">parent</a><span>|</span><a href="#36220104">next</a><span>|</span><label class="collapse" for="c-36222947">[-]</label><label class="expand" for="c-36222947">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What do you mean by commodity hardware?<p>In my case, my local workstation has a Ryzen 5 1600 desktop CPU from 2017 (first generation Zen, 14nm) and it still worked decently.<p>Of course, response times would grow with longer inputs and outputs or larger models, but getting a response in less than a minute when running off of purely CPU is encouraging in of itself.</div><br/></div></div></div></div></div></div><div id="36218992" class="c"><input type="checkbox" id="c-36218992" checked=""/><div class="controls bullet"><span class="by">boringuser2</span><span>|</span><a href="#36218585">prev</a><span>|</span><a href="#36215882">next</a><span>|</span><label class="collapse" for="c-36218992">[-]</label><label class="expand" for="c-36218992">[1 more]</label></div><br/><div class="children"><div class="content">Looking at the source of this kind of underlines the difference between machine learning scientist types and actual computer scientists.</div><br/></div></div><div id="36215882" class="c"><input type="checkbox" id="c-36215882" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#36218992">prev</a><span>|</span><a href="#36217295">next</a><span>|</span><label class="collapse" for="c-36215882">[-]</label><label class="expand" for="c-36215882">[3 more]</label></div><br/><div class="children"><div class="content">I believe ggml is the basis of llama.cpp (the OP says it&#x27;s &quot;used by llama.cpp&quot;)? I don&#x27;t know much about either, but when I read the llama.cpp code to see how it was created so quickly, I got the sense that the original project was ggml, given the amount of pasted code I saw. It seemed like quite an impressive library.</div><br/><div id="36215954" class="c"><input type="checkbox" id="c-36215954" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36215882">parent</a><span>|</span><a href="#36218722">next</a><span>|</span><label class="collapse" for="c-36215954">[-]</label><label class="expand" for="c-36215954">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33877893" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33877893</a><p>“OpenAI recently released a model for automatic speech recognition called Whisper. I decided to reimplement the inference of the model from scratch using C&#x2F;C++. To achieve this I implemented a minimalistic tensor library in C and ported the high-level architecture of the model in C++.”<p>That “minimalistic tensor library” was ggml.</div><br/></div></div><div id="36218722" class="c"><input type="checkbox" id="c-36218722" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#36215882">parent</a><span>|</span><a href="#36215954">prev</a><span>|</span><a href="#36217295">next</a><span>|</span><label class="collapse" for="c-36218722">[-]</label><label class="expand" for="c-36218722">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s the library used for tensor operations inside of llama.cpp, yes</div><br/></div></div></div></div><div id="36217295" class="c"><input type="checkbox" id="c-36217295" checked=""/><div class="controls bullet"><span class="by">aryamaan</span><span>|</span><a href="#36215882">prev</a><span>|</span><a href="#36218219">next</a><span>|</span><label class="collapse" for="c-36217295">[-]</label><label class="expand" for="c-36217295">[1 more]</label></div><br/><div class="children"><div class="content">Could someone at high level talk more about how one starts contributing to this kind of problems.<p>For the people who build solutions for data handling— ranging from crud to building highly scalable solutions— these things are alien concepts. (Or maybe I am just talking about it myself)</div><br/></div></div><div id="36218219" class="c"><input type="checkbox" id="c-36218219" checked=""/><div class="controls bullet"><span class="by">edfletcher_t137</span><span>|</span><a href="#36217295">prev</a><span>|</span><a href="#36216161">next</a><span>|</span><label class="collapse" for="c-36218219">[-]</label><label class="expand" for="c-36218219">[1 more]</label></div><br/><div class="children"><div class="content">This is a bang-up idea, you absolutely love to see capital investment on this type of open, commodity-hardware-focused foundational technology. Rock on GGMLers &amp; thank you!</div><br/></div></div><div id="36216161" class="c"><input type="checkbox" id="c-36216161" checked=""/><div class="controls bullet"><span class="by">world2vec</span><span>|</span><a href="#36218219">prev</a><span>|</span><a href="#36222814">next</a><span>|</span><label class="collapse" for="c-36216161">[-]</label><label class="expand" for="c-36216161">[13 more]</label></div><br/><div class="children"><div class="content">Might be a silly question but is GGML a similar&#x2F;competing library to George Hotz&#x27;s tinygrad [0]?<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;geohot&#x2F;tinygrad">https:&#x2F;&#x2F;github.com&#x2F;geohot&#x2F;tinygrad</a></div><br/><div id="36218539" class="c"><input type="checkbox" id="c-36218539" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#36216161">parent</a><span>|</span><a href="#36216187">next</a><span>|</span><label class="collapse" for="c-36218539">[-]</label><label class="expand" for="c-36218539">[3 more]</label></div><br/><div class="children"><div class="content">They are competing (although they are very different, tinygrad is full stack Python, ggml is focusing on a few very important models), but in my opinion George Hotz lost focus a bit by not working more on getting the low level optimizations perfect.</div><br/><div id="36219975" class="c"><input type="checkbox" id="c-36219975" checked=""/><div class="controls bullet"><span class="by">georgehotz</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36218539">parent</a><span>|</span><a href="#36216187">next</a><span>|</span><label class="collapse" for="c-36219975">[-]</label><label class="expand" for="c-36219975">[2 more]</label></div><br/><div class="children"><div class="content">Which low level optimizations specifically are you referring to?<p>I&#x27;m happy with most of the abstractions. We are pushing to assembly codegen. And if you meant things like matrix accelerators, that&#x27;s my next priority.<p>We are taking more a of breadth first approach. I think ggml is more depth first and application focused. (and I think Mojo is even more breadth first)</div><br/><div id="36222732" class="c"><input type="checkbox" id="c-36222732" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36219975">parent</a><span>|</span><a href="#36216187">next</a><span>|</span><label class="collapse" for="c-36222732">[-]</label><label class="expand" for="c-36222732">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;d love to see Tinygrad beat GGML in its own game (4 bit LLM support on M1 Mac GPU or Tensor cores) before adding more backends &#x2F; models.<p>It&#x27;s easy to debug because the generated kernels can be compared to GGML, and still gives something practical that we all can play with.<p>At this point breadth first is a bit boring,  because this way we don&#x27;t know how far tinygrad is from optimal generated output.</div><br/></div></div></div></div></div></div><div id="36216187" class="c"><input type="checkbox" id="c-36216187" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#36216161">parent</a><span>|</span><a href="#36218539">prev</a><span>|</span><a href="#36222814">next</a><span>|</span><label class="collapse" for="c-36216187">[-]</label><label class="expand" for="c-36216187">[9 more]</label></div><br/><div class="children"><div class="content">No, GGML is a CPU optimized library and quantized weight format that is closely linked to his other project llama.cpp</div><br/><div id="36216266" class="c"><input type="checkbox" id="c-36216266" checked=""/><div class="controls bullet"><span class="by">ggerganov</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216187">parent</a><span>|</span><a href="#36216244">next</a><span>|</span><label class="collapse" for="c-36216266">[-]</label><label class="expand" for="c-36216266">[4 more]</label></div><br/><div class="children"><div class="content">ggml started with focus on CPU inference, but lately we have been augmenting it with GPU support. Although still in development, it already has partial CUDA, OpenCL and Metal backend support</div><br/><div id="36216327" class="c"><input type="checkbox" id="c-36216327" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216266">parent</a><span>|</span><a href="#36219452">next</a><span>|</span><label class="collapse" for="c-36216327">[-]</label><label class="expand" for="c-36216327">[1 more]</label></div><br/><div class="children"><div class="content">Hi Georgi - thanks for all the work, have been following and using since the availability of Llama base layers!<p>Wasn’t implying it’s CPU only, just that it started as a CPU optimized library.</div><br/></div></div><div id="36219452" class="c"><input type="checkbox" id="c-36219452" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216266">parent</a><span>|</span><a href="#36216327">prev</a><span>|</span><a href="#36216442">next</a><span>|</span><label class="collapse" for="c-36219452">[-]</label><label class="expand" for="c-36219452">[1 more]</label></div><br/><div class="children"><div class="content">(a novice here who knows a couple of fancy terms)<p>&gt; <i>...lately we have been augmenting it with GPU support.</i><p>Would you say you&#x27;d then be building an equivalent to Google&#x27;s JAX?<p>Someone even asked if anyone would build a C++ to JAX transpiler [0]... I am wondering if that&#x27;s something you may implement? Thanks.<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35475675" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35475675</a></div><br/></div></div><div id="36216442" class="c"><input type="checkbox" id="c-36216442" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216266">parent</a><span>|</span><a href="#36219452">prev</a><span>|</span><a href="#36216244">next</a><span>|</span><label class="collapse" for="c-36216442">[-]</label><label class="expand" for="c-36216442">[1 more]</label></div><br/><div class="children"><div class="content">As a person burned by nvidia, I can&#x27;t thank you enough for the OpenCL support</div><br/></div></div></div></div><div id="36216244" class="c"><input type="checkbox" id="c-36216244" checked=""/><div class="controls bullet"><span class="by">stri8ed</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216187">parent</a><span>|</span><a href="#36216266">prev</a><span>|</span><a href="#36222814">next</a><span>|</span><label class="collapse" for="c-36216244">[-]</label><label class="expand" for="c-36216244">[4 more]</label></div><br/><div class="children"><div class="content">How does the quantization happen? Are the weights preprocessed before loading the model?</div><br/><div id="36216321" class="c"><input type="checkbox" id="c-36216321" checked=""/><div class="controls bullet"><span class="by">ggerganov</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216244">parent</a><span>|</span><a href="#36216303">next</a><span>|</span><label class="collapse" for="c-36216321">[-]</label><label class="expand" for="c-36216321">[1 more]</label></div><br/><div class="children"><div class="content">The weights are preprocessed into integer quants combined with scaling factors in various configurations (4, 5, 8-bits and recently more exotic 2, 3 and 6-bit quants). At runtime, we use efficient SIMD implementations to perform the matrix multiplication at integer level, carefully optimizing for both compute and memory bandwidth. Similar strategies are applied when running GPU inference - using custom kernels for fast Matrix x Vector multiplications</div><br/></div></div><div id="36216303" class="c"><input type="checkbox" id="c-36216303" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216244">parent</a><span>|</span><a href="#36216321">prev</a><span>|</span><a href="#36222814">next</a><span>|</span><label class="collapse" for="c-36216303">[-]</label><label class="expand" for="c-36216303">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but to my knowledge it doesn&#x27;t do any of the complicated optimization stuff that SOTA quantisation methods use. It basically is just doing a bunch of rounding.<p>There are advantages to simplicity, after all.</div><br/><div id="36216416" class="c"><input type="checkbox" id="c-36216416" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216161">root</a><span>|</span><a href="#36216303">parent</a><span>|</span><a href="#36222814">next</a><span>|</span><label class="collapse" for="c-36216416">[-]</label><label class="expand" for="c-36216416">[1 more]</label></div><br/><div class="children"><div class="content">Its not so simple anymore, see <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="36222814" class="c"><input type="checkbox" id="c-36222814" checked=""/><div class="controls bullet"><span class="by">legendofbrando</span><span>|</span><a href="#36216161">prev</a><span>|</span><a href="#36222313">next</a><span>|</span><label class="collapse" for="c-36222814">[-]</label><label class="expand" for="c-36222814">[2 more]</label></div><br/><div class="children"><div class="content">Running whisper locally on my iPhone back in December and watching perfect transcriptions pop out without sending anything to a server was a real lightbulb moment for me that set in motion a bunch of the work I’m doing now. Excited to see the new heights this unlocks!</div><br/><div id="36223623" class="c"><input type="checkbox" id="c-36223623" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#36222814">parent</a><span>|</span><a href="#36222313">next</a><span>|</span><label class="collapse" for="c-36223623">[-]</label><label class="expand" for="c-36223623">[1 more]</label></div><br/><div class="children"><div class="content">What are you using to run Whisper locally?</div><br/></div></div></div></div><div id="36222313" class="c"><input type="checkbox" id="c-36222313" checked=""/><div class="controls bullet"><span class="by">Tan-Aki</span><span>|</span><a href="#36222814">prev</a><span>|</span><a href="#36223128">next</a><span>|</span><label class="collapse" for="c-36222313">[-]</label><label class="expand" for="c-36222313">[3 more]</label></div><br/><div class="children"><div class="content">Can anyone explain to me, in simple terms, and at a high level, what the heck am I looking at?
What is this library for?
What does it mean &quot;it is used by lama.cpp and whisper.cpp&quot;? 
How is it revolutionary?
Thank you very much in advance!</div><br/><div id="36222597" class="c"><input type="checkbox" id="c-36222597" checked=""/><div class="controls bullet"><span class="by">orost</span><span>|</span><a href="#36222313">parent</a><span>|</span><a href="#36222346">next</a><span>|</span><label class="collapse" for="c-36222597">[-]</label><label class="expand" for="c-36222597">[1 more]</label></div><br/><div class="children"><div class="content">ggml is a library that provides operations for running machine learning models<p>llama.cpp is a project that uses ggml to run LLaMA, a large language model (like GPT) by Meta<p>whisper.cpp is a project that uses ggml to run Whisper, a speech recognition model by OpenAI<p>ggml&#x27;s distinguishing feature is efficient operation on CPU. Traditionally, this sort of work is done on GPU, but GPUs with large amounts of memory are specialized and extremely expensive hardware. ggml achieves acceptable speed on commodity hardware.</div><br/></div></div><div id="36222346" class="c"><input type="checkbox" id="c-36222346" checked=""/><div class="controls bullet"><span class="by">Tan-Aki</span><span>|</span><a href="#36222313">parent</a><span>|</span><a href="#36222597">prev</a><span>|</span><a href="#36223128">next</a><span>|</span><label class="collapse" for="c-36222346">[-]</label><label class="expand" for="c-36222346">[1 more]</label></div><br/><div class="children"><div class="content">With maybe a tiny bit of history as well? Pretty please? :  p</div><br/></div></div></div></div><div id="36223128" class="c"><input type="checkbox" id="c-36223128" checked=""/><div class="controls bullet"><span class="by">dangrover</span><span>|</span><a href="#36222313">prev</a><span>|</span><a href="#36222819">next</a><span>|</span><label class="collapse" for="c-36223128">[-]</label><label class="expand" for="c-36223128">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s so much potential for this as a tech powering all sorts of products, I hope it doesn&#x27;t just become some Tarsnap type thing. (judging from initial site)</div><br/><div id="36223613" class="c"><input type="checkbox" id="c-36223613" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#36223128">parent</a><span>|</span><a href="#36222819">next</a><span>|</span><label class="collapse" for="c-36223613">[-]</label><label class="expand" for="c-36223613">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s wrong with &quot;Tarsnap type things&quot;?</div><br/></div></div></div></div><div id="36222819" class="c"><input type="checkbox" id="c-36222819" checked=""/><div class="controls bullet"><span class="by">kyt</span><span>|</span><a href="#36223128">prev</a><span>|</span><a href="#36215876">next</a><span>|</span><label class="collapse" for="c-36222819">[-]</label><label class="expand" for="c-36222819">[1 more]</label></div><br/><div class="children"><div class="content">I used the GGML version of Whisper and I had to revert back to the PyTorch version released by OpenAI. The GGML version simply didn&#x27;t work well even for the same model. I am assuming it has to do with the quantization.</div><br/></div></div><div id="36215876" class="c"><input type="checkbox" id="c-36215876" checked=""/><div class="controls bullet"><span class="by">_20p0</span><span>|</span><a href="#36222819">prev</a><span>|</span><a href="#36216025">next</a><span>|</span><label class="collapse" for="c-36215876">[-]</label><label class="expand" for="c-36215876">[31 more]</label></div><br/><div class="children"><div class="content">This guy is damned good. I sponsored him on Github because his software is dope. I also like how when some controversy erupted on the project he just ejected the controversial people and moved on. Good stewardship. Great code.<p>I recall something like when he first ported it and it worked on my M1 Max he hadn&#x27;t even yet tested it on Apple Silicon since he didn&#x27;t have the hardware.<p>Honestly, with this and whisper, I am a huge fan. Good luck to him and the new company.</div><br/><div id="36216199" class="c"><input type="checkbox" id="c-36216199" checked=""/><div class="controls bullet"><span class="by">nchudleigh</span><span>|</span><a href="#36215876">parent</a><span>|</span><a href="#36216131">next</a><span>|</span><label class="collapse" for="c-36216199">[-]</label><label class="expand" for="c-36216199">[1 more]</label></div><br/><div class="children"><div class="content">he has been amazing to watch and has even helped me out with my app that uses his whisper.cpp project (<a href="https:&#x2F;&#x2F;superwhisper.com" rel="nofollow">https:&#x2F;&#x2F;superwhisper.com</a>)<p>Excited to see how his venture goes!</div><br/></div></div><div id="36216131" class="c"><input type="checkbox" id="c-36216131" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#36215876">parent</a><span>|</span><a href="#36216199">prev</a><span>|</span><a href="#36216264">next</a><span>|</span><label class="collapse" for="c-36216131">[-]</label><label class="expand" for="c-36216131">[11 more]</label></div><br/><div class="children"><div class="content">Another important detail about the ejections that I think is particularly classy is that the people he ejected are broadly considered to have world-class technical skills. In other words, he was very explicitly prioritizing collaborative potential &gt; technical skill. Maybe a future BDFL[1]!<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Benevolent_dictator_for_life" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Benevolent_dictator_for_life</a></div><br/><div id="36219666" class="c"><input type="checkbox" id="c-36219666" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216131">parent</a><span>|</span><a href="#36216264">next</a><span>|</span><label class="collapse" for="c-36219666">[-]</label><label class="expand" for="c-36219666">[10 more]</label></div><br/><div class="children"><div class="content">Gerganov was prioritizing collaboration with 4chan who raided his GitHub to demand a change written by a transgender woman be reverted. There was so much hate speech and immaturity thrown around (words like tranny troon cucking muh model) that it&#x27;s a real embarrassment (to those of us deeply want to see local models succeed) that one of the smartest guys working on the problem was taken in by all that. You can&#x27;t run a collaborative environment that&#x27;s open when you pander to hate, because hate subverts communities; it&#x27;s impossible to compromise with anonymous trolls who harass a public figure over physical traits about her body she can&#x27;t change.<p>You don&#x27;t have to take my word on it. Here are some archives of the 4chan threads where they coordinated the raid. It went on for like a month. <a href="https:&#x2F;&#x2F;archive.is&#x2F;EX7Fq" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;EX7Fq</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;enjpf" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;enjpf</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;Kbjtt" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;Kbjtt</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;HGwZm" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;HGwZm</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;pijMv" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;pijMv</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;M7hLJ" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;M7hLJ</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;4UxKP" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;4UxKP</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;IB9bv" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;IB9bv</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;p6Q2q" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;p6Q2q</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;phCGN" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;phCGN</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;M6AF1" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;M6AF1</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;mXoBs" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;mXoBs</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;68Ayg" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;68Ayg</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;DamPp" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;DamPp</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;DiQC2" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;DiQC2</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;DeX8Z" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;DeX8Z</a>
<a href="https:&#x2F;&#x2F;archive.is&#x2F;gStQ1" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;gStQ1</a><p>If you read these threads and see how nasty these little monsters are, you can probably imagine how Gerganov must have felt. He was probably scared they&#x27;d harass him too, since 4chan acts like he&#x27;s their boy. I wouldn&#x27;t even be surprised if he&#x27;s one of them. Plus it was weak leadership on his part to disappear for days, suddenly show up again to neutral knight the situation (<a href="https:&#x2F;&#x2F;justine.lol&#x2F;neutral-knight.png" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;neutral-knight.png</a>) by telling his team members they&#x27;re no longer welcome, and then going back and deleting his comment later. It goes to show that no matter how brilliant you are at hard technical skills, you can still be totally clueless about people.</div><br/><div id="36220053" class="c"><input type="checkbox" id="c-36220053" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36219666">parent</a><span>|</span><a href="#36220243">next</a><span>|</span><label class="collapse" for="c-36220053">[-]</label><label class="expand" for="c-36220053">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t want to not reply but I also didn&#x27;t want to be swept into a potentially fraught internet argument. So, I tried to edit my comment as a middle ground, but it looks like I can&#x27;t, I guess there must be a timeout. If I could edit it, I&#x27;d add the following:<p>&quot;I should point out that I wasn&#x27;t personally involved, haven&#x27;t looked into it in detail, and that there are many different perspectives that should be considered.&quot;</div><br/></div></div><div id="36220243" class="c"><input type="checkbox" id="c-36220243" checked=""/><div class="controls bullet"><span class="by">zo1</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36219666">parent</a><span>|</span><a href="#36220053">prev</a><span>|</span><a href="#36220738">next</a><span>|</span><label class="collapse" for="c-36220243">[-]</label><label class="expand" for="c-36220243">[4 more]</label></div><br/><div class="children"><div class="content">Really curious why you tried to rename the file format magic string to have your initials? Going from GGML (see Title of this post) to GGJT with JT being Justine Tunney? Seems quite unnecessary and bound to have rubbed a lot of people the wrong way.<p>Here is the official commit undoing the change:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;711&#x2F;files#diff-7696a3039a95b41e744a08272f14d2a4345fddbd06ac482deb37f03a3afad2b5R70">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;711&#x2F;files#diff-7...</a></div><br/><div id="36220508" class="c"><input type="checkbox" id="c-36220508" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36220243">parent</a><span>|</span><a href="#36220715">next</a><span>|</span><label class="collapse" for="c-36220508">[-]</label><label class="expand" for="c-36220508">[1 more]</label></div><br/><div class="children"><div class="content">Because the previous changes to the file format were done by changing the last two initials of the magic. Someone commented on the pull request suggesting using a version field instead, which hadn&#x27;t been documented, but Gerganov was so happy with the mmap() contribution that he asked me to keep the initials. So you should ask him why he wanted my initials to be there. I didn&#x27;t see anyone else raise concerns until later on when the 4chan raid happened. I guess I failed to consider that folks who hate trans women would feel uncomfortable needing to mark their files with the initials of one. Here&#x27;s the pull request: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;613">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;613</a></div><br/></div></div><div id="36220715" class="c"><input type="checkbox" id="c-36220715" checked=""/><div class="controls bullet"><span class="by">_20p0</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36220243">parent</a><span>|</span><a href="#36220508">prev</a><span>|</span><a href="#36220738">next</a><span>|</span><label class="collapse" for="c-36220715">[-]</label><label class="expand" for="c-36220715">[2 more]</label></div><br/><div class="children"><div class="content">Strange comment. This doesn&#x27;t sound like a legitimate criticism because of &quot;Here is the official commit undoing the change:&quot; being not a link to a commit to start with, and secondly being a declined pull request, and thirdly for the reality that `master` writes `ggjt` header.<p>Really looks like some axe-grinding here, if I&#x27;m being honest. Especially because it takes very little effort to find out what the present header is by someone who can write software.</div><br/><div id="36223367" class="c"><input type="checkbox" id="c-36223367" checked=""/><div class="controls bullet"><span class="by">zo1</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36220715">parent</a><span>|</span><a href="#36220738">next</a><span>|</span><label class="collapse" for="c-36223367">[-]</label><label class="expand" for="c-36223367">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a dev and know how to use git. Honestly that&#x27;s just the PR I could find whilst catching up on this story and was curious what this &quot;magic string&quot; was. The change is there and got reverted 100%, see the other commenter who was the one that made it. If there is a better link you&#x27;re welcome to post it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36216264" class="c"><input type="checkbox" id="c-36216264" checked=""/><div class="controls bullet"><span class="by">PrimeMcFly</span><span>|</span><a href="#36215876">parent</a><span>|</span><a href="#36216131">prev</a><span>|</span><a href="#36216191">next</a><span>|</span><label class="collapse" for="c-36216264">[-]</label><label class="expand" for="c-36216264">[15 more]</label></div><br/><div class="children"><div class="content">&gt; I also like how when some controversy erupted on the project he just ejected the controversial people and moved on. Good stewardship<p>Do you have more info on the controversy? I&#x27;m not sure ejecting developers just because of controversy is honestly good stewardship.</div><br/><div id="36216584" class="c"><input type="checkbox" id="c-36216584" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216264">parent</a><span>|</span><a href="#36218505">next</a><span>|</span><label class="collapse" for="c-36216584">[-]</label><label class="expand" for="c-36216584">[10 more]</label></div><br/><div class="children"><div class="content">Right.  More details needed to know if this is good stewardship (ejecting two toxic individuals) or laziness (ejecting a villain and a hero to get rid of the &quot;problem&quot; easily).  TikTok was using this method for a while by ejecting both bullies and victims, and it &quot;solved&quot; the problem but most people see the injustice there.<p>I&#x27;m not saying it was bad stewardship, I honestly don&#x27;t know.  I just agree that we shouldn&#x27;t make a judgment without more information.</div><br/><div id="36218398" class="c"><input type="checkbox" id="c-36218398" checked=""/><div class="controls bullet"><span class="by">jstarfish</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216584">parent</a><span>|</span><a href="#36218213">next</a><span>|</span><label class="collapse" for="c-36218398">[-]</label><label class="expand" for="c-36218398">[2 more]</label></div><br/><div class="children"><div class="content">&gt; More details needed to know if this is good stewardship (ejecting two toxic individuals) or laziness (ejecting a villain and a hero to get rid of the &quot;problem&quot; easily). TikTok was using this method for a while by ejecting both bullies and victims,<p>This is SOP for American schools. It&#x27;s laziness there, since education is supposed to be compulsory. They can&#x27;t be bothered to investigate (and with today&#x27;s hostile climate, I don&#x27;t blame them) so they consign both parties to independent-study programs.<p>For volunteer projects, throwing both overboard is unfortunate but necessary stewardship. The drama either attracts destabilizes the entire project, which only exists as long as it remains <i>fun</i> for the maintainer. It&#x27;s tragic, but victims who can&#x27;t recover gracefully are as toxic as their abusers.</div><br/><div id="36221044" class="c"><input type="checkbox" id="c-36221044" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36218398">parent</a><span>|</span><a href="#36218213">next</a><span>|</span><label class="collapse" for="c-36221044">[-]</label><label class="expand" for="c-36221044">[1 more]</label></div><br/><div class="children"><div class="content">Right, I for one would also prefer it if people who face harassment and hate would just accept it gracefully and move on. I mean, get over yourself, amiright?</div><br/></div></div></div></div><div id="36218213" class="c"><input type="checkbox" id="c-36218213" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216584">parent</a><span>|</span><a href="#36218398">prev</a><span>|</span><a href="#36216964">next</a><span>|</span><label class="collapse" for="c-36218213">[-]</label><label class="expand" for="c-36218213">[1 more]</label></div><br/><div class="children"><div class="content">&gt;justice<p>For an individual running a small open source project, there&#x27;s time enough for coding or detailed justice, but not both. When two parties start pointing fingers and raising hell and its not immediately clear who is in the right, ban both and let them fork it.</div><br/></div></div><div id="36216964" class="c"><input type="checkbox" id="c-36216964" checked=""/><div class="controls bullet"><span class="by">csmpltn</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216584">parent</a><span>|</span><a href="#36218213">prev</a><span>|</span><a href="#36218505">next</a><span>|</span><label class="collapse" for="c-36216964">[-]</label><label class="expand" for="c-36216964">[6 more]</label></div><br/><div class="children"><div class="content">&gt; More details needed to know if this is good stewardship (ejecting two toxic individuals) or laziness (ejecting a villain and a hero to get rid of the &quot;problem&quot; easily).<p>Man, nobody has time for this shit. Leave the games and the drama for the social justice warriors and the furries. People building shit ain&#x27;t got time for this - ejecting trouble makers is the right way to go regardless of which &quot;side&quot; they&#x27;re on.</div><br/><div id="36217227" class="c"><input type="checkbox" id="c-36217227" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216964">parent</a><span>|</span><a href="#36218052">next</a><span>|</span><label class="collapse" for="c-36217227">[-]</label><label class="expand" for="c-36217227">[1 more]</label></div><br/><div class="children"><div class="content">I would agree that there needs to be a balance because wasting time babysitting adults is dumb, but what if one person is a good and loved contributor, and the other is a social justice warrior new to the project that is picking fights with the contributor?  Your philosophy makes not only bad stewardship but an injustice.  I&#x27;m not suggesting this is the only scenario, just merely a hypothetical that I think illustrates my position.</div><br/></div></div><div id="36218052" class="c"><input type="checkbox" id="c-36218052" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216964">parent</a><span>|</span><a href="#36217227">prev</a><span>|</span><a href="#36218144">next</a><span>|</span><label class="collapse" for="c-36218052">[-]</label><label class="expand" for="c-36218052">[2 more]</label></div><br/><div class="children"><div class="content">And what do you do when every contributor to the project, including the founder, has been labeled a troublemaker?</div><br/><div id="36218229" class="c"><input type="checkbox" id="c-36218229" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36218052">parent</a><span>|</span><a href="#36218144">next</a><span>|</span><label class="collapse" for="c-36218229">[-]</label><label class="expand" for="c-36218229">[1 more]</label></div><br/><div class="children"><div class="content">Pick the fork that has devs who are focused on contributing code and not pursuing drama.</div><br/></div></div></div></div><div id="36218144" class="c"><input type="checkbox" id="c-36218144" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216964">parent</a><span>|</span><a href="#36218052">prev</a><span>|</span><a href="#36218505">next</a><span>|</span><label class="collapse" for="c-36218144">[-]</label><label class="expand" for="c-36218144">[2 more]</label></div><br/><div class="children"><div class="content">&gt; and the furries<p>Um, what?</div><br/><div id="36219391" class="c"><input type="checkbox" id="c-36219391" checked=""/><div class="controls bullet"><span class="by">camdenlock</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36218144">parent</a><span>|</span><a href="#36218505">next</a><span>|</span><label class="collapse" for="c-36219391">[-]</label><label class="expand" for="c-36219391">[1 more]</label></div><br/><div class="children"><div class="content">If you know, you know</div><br/></div></div></div></div></div></div></div></div><div id="36218505" class="c"><input type="checkbox" id="c-36218505" checked=""/><div class="controls bullet"><span class="by">infamouscow</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216264">parent</a><span>|</span><a href="#36216584">prev</a><span>|</span><a href="#36216191">next</a><span>|</span><label class="collapse" for="c-36218505">[-]</label><label class="expand" for="c-36218505">[4 more]</label></div><br/><div class="children"><div class="content">The code is MIT licensed. If you don&#x27;t agree with the direction the project is taking you can fork it and add whatever you want.<p>I don&#x27;t understand why this is so difficult for software developers with GitHub accounts to understand.</div><br/><div id="36218771" class="c"><input type="checkbox" id="c-36218771" checked=""/><div class="controls bullet"><span class="by">PrimeMcFly</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36218505">parent</a><span>|</span><a href="#36216191">next</a><span>|</span><label class="collapse" for="c-36218771">[-]</label><label class="expand" for="c-36218771">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve missed the point here more than I&#x27;ve seen anyone miss the point in a long time.</div><br/><div id="36219277" class="c"><input type="checkbox" id="c-36219277" checked=""/><div class="controls bullet"><span class="by">infamouscow</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36218771">parent</a><span>|</span><a href="#36216191">next</a><span>|</span><label class="collapse" for="c-36219277">[-]</label><label class="expand" for="c-36219277">[2 more]</label></div><br/><div class="children"><div class="content">Software stewardship is cringe.<p>The idea software licensed with a free software license can have a steward doesn&#x27;t even make sense.<p>How exactly does someone supervise or take care of intellectual property (read: code) when the author and original copyright holder explicitly licensed their work under the MIT license, granting anyone the following:<p>&gt; [T]o deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and&#x2F;or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions<p>The author was certainly a steward when they were working on it in private, or heck, even in public since copyright is implicit, but certainly not after adding the MIT license.<p>So when I think of software stewardship, all I see are self-appointed thought leaders and corporate vampires like Oracle chest beating about how important stewardship is.<p>Simply a way for those in positions of power&#x2F;status to remain in their positions elevated above everyone else. Depending on the situation and context that might be good or bad. What&#x27;s important is it&#x27;s not for these &quot;stewards&quot; to decide because the public always has the option to walk away.<p>I hope this detailed explanation illuminates the apparent misconceptions of how free software projects work. :)</div><br/><div id="36221946" class="c"><input type="checkbox" id="c-36221946" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36219277">parent</a><span>|</span><a href="#36216191">next</a><span>|</span><label class="collapse" for="c-36221946">[-]</label><label class="expand" for="c-36221946">[1 more]</label></div><br/><div class="children"><div class="content">Just because someone can&#x27;t wring your hand doesn&#x27;t mean they don&#x27;t deserve to influence a project. Releasing work under a license like MIT is an act of benevolence since it&#x27;s giving up leverage and relying on de facto leadership. It&#x27;s the hardest kind of leader to be, since de facto leaders are only followed on merit. It&#x27;s not unusual at all for folks who started projects to end up caving to political pressures and getting replaced by the sorts of people you&#x27;re talking about. But every now and again a Medici comes along.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36216191" class="c"><input type="checkbox" id="c-36216191" checked=""/><div class="controls bullet"><span class="by">evanwise</span><span>|</span><a href="#36215876">parent</a><span>|</span><a href="#36216264">prev</a><span>|</span><a href="#36216025">next</a><span>|</span><label class="collapse" for="c-36216191">[-]</label><label class="expand" for="c-36216191">[3 more]</label></div><br/><div class="children"><div class="content">What was the controversy?</div><br/><div id="36216585" class="c"><input type="checkbox" id="c-36216585" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216191">parent</a><span>|</span><a href="#36216394">next</a><span>|</span><label class="collapse" for="c-36216585">[-]</label><label class="expand" for="c-36216585">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35411909" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35411909</a></div><br/></div></div><div id="36216394" class="c"><input type="checkbox" id="c-36216394" checked=""/><div class="controls bullet"><span class="by">pubby</span><span>|</span><a href="#36215876">root</a><span>|</span><a href="#36216191">parent</a><span>|</span><a href="#36216585">prev</a><span>|</span><a href="#36216025">next</a><span>|</span><label class="collapse" for="c-36216394">[-]</label><label class="expand" for="c-36216394">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;711">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;711</a></div><br/></div></div></div></div></div></div><div id="36216025" class="c"><input type="checkbox" id="c-36216025" checked=""/><div class="controls bullet"><span class="by">danieljanes</span><span>|</span><a href="#36215876">prev</a><span>|</span><a href="#36216559">next</a><span>|</span><label class="collapse" for="c-36216025">[-]</label><label class="expand" for="c-36216025">[3 more]</label></div><br/><div class="children"><div class="content">Does GGML support training on the edge? We&#x27;re especially interested in training support for Android+iOS</div><br/><div id="36216179" class="c"><input type="checkbox" id="c-36216179" checked=""/><div class="controls bullet"><span class="by">svantana</span><span>|</span><a href="#36216025">parent</a><span>|</span><a href="#36216069">next</a><span>|</span><label class="collapse" for="c-36216179">[-]</label><label class="expand" for="c-36216179">[1 more]</label></div><br/><div class="children"><div class="content">Yes - look at the file tests&#x2F;test-opt.c. Unfortunately there&#x27;s almost no documentation about its training&#x2F;autodiff.</div><br/></div></div></div></div><div id="36216559" class="c"><input type="checkbox" id="c-36216559" checked=""/><div class="controls bullet"><span class="by">mliker</span><span>|</span><a href="#36216025">prev</a><span>|</span><a href="#36220486">next</a><span>|</span><label class="collapse" for="c-36216559">[-]</label><label class="expand" for="c-36216559">[1 more]</label></div><br/><div class="children"><div class="content">congrats! I was just listening to your changelog interview from months ago in which you said you were going to move on from this after you brush up the code a bit, but it seems the momentum is too great. Glad to see you carrying this amazing project(s) forward!</div><br/></div></div><div id="36220486" class="c"><input type="checkbox" id="c-36220486" checked=""/><div class="controls bullet"><span class="by">ex3ndr</span><span>|</span><a href="#36216559">prev</a><span>|</span><a href="#36217891">next</a><span>|</span><label class="collapse" for="c-36220486">[-]</label><label class="expand" for="c-36220486">[1 more]</label></div><br/><div class="children"><div class="content">so sad we still don&#x27;t have a perfect neural network for activation word to make home assistants complete</div><br/></div></div><div id="36217891" class="c"><input type="checkbox" id="c-36217891" checked=""/><div class="controls bullet"><span class="by">pawelduda</span><span>|</span><a href="#36220486">prev</a><span>|</span><a href="#36218127">next</a><span>|</span><label class="collapse" for="c-36217891">[-]</label><label class="expand" for="c-36217891">[5 more]</label></div><br/><div class="children"><div class="content">I happen to have RPi 4B with HomeAssistant. Is this something I could set up on it and integrate with HA to control it with speech, or is it overkill?</div><br/><div id="36218115" class="c"><input type="checkbox" id="c-36218115" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#36217891">parent</a><span>|</span><a href="#36221504">next</a><span>|</span><label class="collapse" for="c-36218115">[-]</label><label class="expand" for="c-36218115">[3 more]</label></div><br/><div class="children"><div class="content">I doubt it. I&#x27;m running 4-bit 30B and 65B models with 64GB ram, a 4080 and a 7900x. The 7B models are less demanding, but even so, You&#x27;ll need more than an rpi. Even then, it would be a <i>project</i> to get these to control something. This is more &#x27;first baby steps&#x27; toward the edge.</div><br/><div id="36218738" class="c"><input type="checkbox" id="c-36218738" checked=""/><div class="controls bullet"><span class="by">pawelduda</span><span>|</span><a href="#36217891">root</a><span>|</span><a href="#36218115">parent</a><span>|</span><a href="#36221504">next</a><span>|</span><label class="collapse" for="c-36218738">[-]</label><label class="expand" for="c-36218738">[2 more]</label></div><br/><div class="children"><div class="content">The article shows example running on RPI that recognizes colour names. I could just come up with keywords that would invoke certain commands and feed them to HA, which would match them to an automation (i.e. turn off kitchen, or just kitchen ) . I think a PoC is doable, but I&#x27;m aware I could run into limitations quickly. Idk might give it a try when I&#x27;m bored.<p>Would love voice assistant running locally but probably there are solutions out there - didn&#x27;t get to do the research yet</div><br/><div id="36222143" class="c"><input type="checkbox" id="c-36222143" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#36217891">root</a><span>|</span><a href="#36218738">parent</a><span>|</span><a href="#36221504">next</a><span>|</span><label class="collapse" for="c-36222143">[-]</label><label class="expand" for="c-36222143">[1 more]</label></div><br/><div class="children"><div class="content">Shameless plug, I&#x27;m the founder of Willow[0].<p>In short you can:<p>1) Run a local Willow Inference Server[1]. Supports CPU or CUDA, just about the fastest implementation of Whisper out there for &quot;real time&quot; speech.<p>2) Run local command detection on device. We pull your Home Assistant entities on setup and define basic grammar for them but any English commands (up to 400) are supported. They are recognized directly on the $50 ESP BOX device and sent to Home Assistant (or openHAB, or a REST endpoint, etc) for processing.<p>Whether WIS or local our performance target is 500ms from end of speech to command executed.<p>[0] - <a href="https:&#x2F;&#x2F;github.com&#x2F;toverainc&#x2F;willow">https:&#x2F;&#x2F;github.com&#x2F;toverainc&#x2F;willow</a><p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;toverainc&#x2F;willow-inference-server">https:&#x2F;&#x2F;github.com&#x2F;toverainc&#x2F;willow-inference-server</a></div><br/></div></div></div></div></div></div><div id="36221504" class="c"><input type="checkbox" id="c-36221504" checked=""/><div class="controls bullet"><span class="by">addandsubtract</span><span>|</span><a href="#36217891">parent</a><span>|</span><a href="#36218115">prev</a><span>|</span><a href="#36218127">next</a><span>|</span><label class="collapse" for="c-36221504">[-]</label><label class="expand" for="c-36221504">[1 more]</label></div><br/><div class="children"><div class="content">Home Assistant has been working on its own speech recognition solution. They&#x27;re calling 2023 &quot;The year of the voice&quot;: <a href="https:&#x2F;&#x2F;www.home-assistant.io&#x2F;blog&#x2F;2023&#x2F;04&#x2F;27&#x2F;year-of-the-voice-chapter-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.home-assistant.io&#x2F;blog&#x2F;2023&#x2F;04&#x2F;27&#x2F;year-of-the-vo...</a></div><br/></div></div></div></div><div id="36218127" class="c"><input type="checkbox" id="c-36218127" checked=""/><div class="controls bullet"><span class="by">huevosabio</span><span>|</span><a href="#36217891">prev</a><span>|</span><a href="#36216903">next</a><span>|</span><label class="collapse" for="c-36218127">[-]</label><label class="expand" for="c-36218127">[1 more]</label></div><br/><div class="children"><div class="content">Very exciting!<p>Now, we just need a post that benchmarks the different options (ggml, tvm, AItemplate, hippoml) and helps deciding which route to take.</div><br/></div></div><div id="36216903" class="c"><input type="checkbox" id="c-36216903" checked=""/><div class="controls bullet"><span class="by">conjecTech</span><span>|</span><a href="#36218127">prev</a><span>|</span><a href="#36217526">next</a><span>|</span><label class="collapse" for="c-36216903">[-]</label><label class="expand" for="c-36216903">[5 more]</label></div><br/><div class="children"><div class="content">Congratulations! How do you plan to make money?</div><br/><div id="36217079" class="c"><input type="checkbox" id="c-36217079" checked=""/><div class="controls bullet"><span class="by">ggerganov</span><span>|</span><a href="#36216903">parent</a><span>|</span><a href="#36217526">next</a><span>|</span><label class="collapse" for="c-36217079">[-]</label><label class="expand" for="c-36217079">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m planning to write code and have fun!</div><br/><div id="36219661" class="c"><input type="checkbox" id="c-36219661" checked=""/><div class="controls bullet"><span class="by">beardog</span><span>|</span><a href="#36216903">root</a><span>|</span><a href="#36217079">parent</a><span>|</span><a href="#36217727">next</a><span>|</span><label class="collapse" for="c-36219661">[-]</label><label class="expand" for="c-36219661">[1 more]</label></div><br/><div class="children"><div class="content">&gt;ggml.ai is a company founded by Georgi Gerganov to support the development of ggml. Nat Friedman and Daniel Gross provided the pre-seed funding.<p>Did you give them a different answer? It is okay if you can&#x27;t or don&#x27;t want to share, but I doubt the company is only planning to have fun. Regardless, best of luck to you and thank you for your efforts so far.</div><br/></div></div><div id="36217727" class="c"><input type="checkbox" id="c-36217727" checked=""/><div class="controls bullet"><span class="by">jgrahamc</span><span>|</span><a href="#36216903">root</a><span>|</span><a href="#36217079">parent</a><span>|</span><a href="#36219661">prev</a><span>|</span><a href="#36219393">next</a><span>|</span><label class="collapse" for="c-36217727">[-]</label><label class="expand" for="c-36217727">[1 more]</label></div><br/><div class="children"><div class="content">This is a good plan.</div><br/></div></div><div id="36219393" class="c"><input type="checkbox" id="c-36219393" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#36216903">root</a><span>|</span><a href="#36217079">parent</a><span>|</span><a href="#36217727">prev</a><span>|</span><a href="#36217526">next</a><span>|</span><label class="collapse" for="c-36219393">[-]</label><label class="expand" for="c-36219393">[1 more]</label></div><br/><div class="children"><div class="content">Have you thought about what your path looks like to get to the next phase? Are you taking on any more investors pre-seee?</div><br/></div></div></div></div></div></div><div id="36217526" class="c"><input type="checkbox" id="c-36217526" checked=""/><div class="controls bullet"><span class="by">doxeddaily</span><span>|</span><a href="#36216903">prev</a><span>|</span><a href="#36219584">next</a><span>|</span><label class="collapse" for="c-36217526">[-]</label><label class="expand" for="c-36217526">[1 more]</label></div><br/><div class="children"><div class="content">This scratches my itch for no dependencies.</div><br/></div></div><div id="36219584" class="c"><input type="checkbox" id="c-36219584" checked=""/><div class="controls bullet"><span class="by">s1k3s</span><span>|</span><a href="#36217526">prev</a><span>|</span><a href="#36218289">next</a><span>|</span><label class="collapse" for="c-36219584">[-]</label><label class="expand" for="c-36219584">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m out of the loop on this entire thing so call me an idiot if I get it wrong. Isn&#x27;t this whole movement based on a model leak from Meta? Aren&#x27;t licenses involved that prevent it from going commercial?</div><br/><div id="36219616" class="c"><input type="checkbox" id="c-36219616" checked=""/><div class="controls bullet"><span class="by">dimfeld</span><span>|</span><a href="#36219584">parent</a><span>|</span><a href="#36220025">next</a><span>|</span><label class="collapse" for="c-36219616">[-]</label><label class="expand" for="c-36219616">[1 more]</label></div><br/><div class="children"><div class="content">Only the weights themselves. There have been other models since then built on the same Llama architecture, but trained from scratch so they&#x27;re safe for commercial user. The GGML code and related projects (llama.cpp and so on) also support some other model types now such as Mosaic&#x27;s MPT series.</div><br/></div></div><div id="36220025" class="c"><input type="checkbox" id="c-36220025" checked=""/><div class="controls bullet"><span class="by">detrites</span><span>|</span><a href="#36219584">parent</a><span>|</span><a href="#36219616">prev</a><span>|</span><a href="#36221012">next</a><span>|</span><label class="collapse" for="c-36220025">[-]</label><label class="expand" for="c-36220025">[1 more]</label></div><br/><div class="children"><div class="content">GGML is essentially a library of lego pieces that can be put together to work with many LLM or other types of ML models.<p>Meta&#x27;s leaked model is one for which GGML has been applied to for fast, local inference.</div><br/></div></div><div id="36221012" class="c"><input type="checkbox" id="c-36221012" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#36219584">parent</a><span>|</span><a href="#36220025">prev</a><span>|</span><a href="#36218289">next</a><span>|</span><label class="collapse" for="c-36221012">[-]</label><label class="expand" for="c-36221012">[1 more]</label></div><br/><div class="children"><div class="content">It wasn&#x27;t a leak, LLaMa was released publicly under an open-ish license (the code is GPL, the model weights require registration and prohibit commercial use).</div><br/></div></div></div></div><div id="36218289" class="c"><input type="checkbox" id="c-36218289" checked=""/><div class="controls bullet"><span class="by">graycat</span><span>|</span><a href="#36219584">prev</a><span>|</span><a href="#36215875">next</a><span>|</span><label class="collapse" for="c-36218289">[-]</label><label class="expand" for="c-36218289">[1 more]</label></div><br/><div class="children"><div class="content">WOW!  They are using BFGS!  Haven&#x27;t heard of that in decades!  Had to think a little:  Yup, the full name is Broyden–Fletcher–Goldfarb–Shanno for iterative unconstrained non-linear optimization!<p>Some of the earlier descriptions of the optimization being used in the AI <i>learning</i> was about steepest descent, that is, just find the gradient of the function are trying to minimize and move some distance in that direction.  Just using the gradient was concerning since that method tends to <i>zig zag</i> where after, say, 100 iterations the distance moved in the 100 iterations might be several times farther than the distance from the starting point of the iterations to the final one.  Can visualize this <i>zig zag</i> already in just two dimensions, say, following a river, say, a river that curves, down a valley the river cut over a million years or so, that is, a valley with steep sides.  Then gradient descent may keep crossing the river and go maybe 10 feet for each foot downstream!<p>Right, if just trying to go downhill on a tilted flat plane, then the gradient will point in the steepest descent on the plane and gradient descent will go all way downhill in just one iteration.<p>In even moderately challenging problems, BFGS can a big improvement.</div><br/></div></div><div id="36216106" class="c"><input type="checkbox" id="c-36216106" checked=""/><div class="controls bullet"><span class="by">nivekney</span><span>|</span><a href="#36215875">prev</a><span>|</span><a href="#36215833">next</a><span>|</span><label class="collapse" for="c-36216106">[-]</label><label class="expand" for="c-36216106">[16 more]</label></div><br/><div class="children"><div class="content">On a similar thread, how does it compare to Hippoml?<p>Context: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36168666" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36168666</a></div><br/><div id="36216469" class="c"><input type="checkbox" id="c-36216469" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216106">parent</a><span>|</span><a href="#36215833">next</a><span>|</span><label class="collapse" for="c-36216469">[-]</label><label class="expand" for="c-36216469">[15 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t necessarily know... Hippo is closed source for now.<p>Its comparable to Apache TVM&#x27;s vulkan in speed on cuda, see <a href="https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm">https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm</a><p>But honestly, the biggest advantage of llama.cpp for me is being able to split a model so performantly. My puny 16GB laptop can <i>just barely</i>, but very practically, run LLaMA 30B at almost 3 tokens&#x2F;s, and do it right now. That is crazy!</div><br/><div id="36217701" class="c"><input type="checkbox" id="c-36217701" checked=""/><div class="controls bullet"><span class="by">smiley1437</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36216469">parent</a><span>|</span><a href="#36215833">next</a><span>|</span><label class="collapse" for="c-36217701">[-]</label><label class="expand" for="c-36217701">[14 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; run LLaMA 30B at almost 3 tokens&#x2F;s<p>Please tell me your config!  I have an i9-10900 with 32GB of ram that only gets .7 tokens&#x2F;s on a 30B model</div><br/><div id="36217992" class="c"><input type="checkbox" id="c-36217992" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36217701">parent</a><span>|</span><a href="#36217877">next</a><span>|</span><label class="collapse" for="c-36217992">[-]</label><label class="expand" for="c-36217992">[3 more]</label></div><br/><div class="children"><div class="content">With a single NVIDIA 3090 and the fastest inference branch of GPTQ-for-LLAMA <a href="https:&#x2F;&#x2F;github.com&#x2F;qwopqwop200&#x2F;GPTQ-for-LLaMa&#x2F;tree&#x2F;fastest-inference-4bit">https:&#x2F;&#x2F;github.com&#x2F;qwopqwop200&#x2F;GPTQ-for-LLaMa&#x2F;tree&#x2F;fastest-i...</a>, I get a healthy 10-15 tokens per second on the 30B models. IMO GGML is great (And I totally use it) but it&#x27;s still not as fast as running the models on GPU for now.</div><br/><div id="36219157" class="c"><input type="checkbox" id="c-36219157" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36217992">parent</a><span>|</span><a href="#36219874">next</a><span>|</span><label class="collapse" for="c-36219157">[-]</label><label class="expand" for="c-36219157">[1 more]</label></div><br/><div class="children"><div class="content">&gt; IMO GGML is great (And I totally use it) but it&#x27;s still not as fast as running the models on GPU for now.<p>I think it was originally designed to be easily embeddable—and most importantly, <i>native code</i> (i.e. not Python)—rather than competitive with GPUs.<p>I think it&#x27;s just starting to get into GPU support now, but carefully.</div><br/></div></div><div id="36219874" class="c"><input type="checkbox" id="c-36219874" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36217992">parent</a><span>|</span><a href="#36219157">prev</a><span>|</span><a href="#36217877">next</a><span>|</span><label class="collapse" for="c-36219874">[-]</label><label class="expand" for="c-36219874">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried the most recent cuda offload? A dev claims they are getting 26.2ms&#x2F;token (38 tokens per second) on 13B with a 4080.</div><br/></div></div></div></div><div id="36217877" class="c"><input type="checkbox" id="c-36217877" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36217701">parent</a><span>|</span><a href="#36217992">prev</a><span>|</span><a href="#36219745">next</a><span>|</span><label class="collapse" for="c-36217877">[-]</label><label class="expand" for="c-36217877">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Please tell me your config! I have an i9-10900 with 32GB of ram that only gets .7 tokens&#x2F;s on a 30B model<p>Have you quantized it?</div><br/><div id="36218570" class="c"><input type="checkbox" id="c-36218570" checked=""/><div class="controls bullet"><span class="by">smiley1437</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36217877">parent</a><span>|</span><a href="#36219745">next</a><span>|</span><label class="collapse" for="c-36218570">[-]</label><label class="expand" for="c-36218570">[6 more]</label></div><br/><div class="children"><div class="content">The model I have is q4_0 I think that&#x27;s 4 bit quantized<p>I&#x27;m running in Windows using koboldcpp, maybe it&#x27;s faster in Linux?</div><br/><div id="36219792" class="c"><input type="checkbox" id="c-36219792" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36218570">parent</a><span>|</span><a href="#36219174">next</a><span>|</span><label class="collapse" for="c-36219792">[-]</label><label class="expand" for="c-36219792">[3 more]</label></div><br/><div class="children"><div class="content">I am running linux with cublast offload, and I am using the new 3 bit quant that was just pulled in a day or two ago.</div><br/><div id="36220323" class="c"><input type="checkbox" id="c-36220323" checked=""/><div class="controls bullet"><span class="by">smiley1437</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36219792">parent</a><span>|</span><a href="#36222560">next</a><span>|</span><label class="collapse" for="c-36220323">[-]</label><label class="expand" for="c-36220323">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I&#x27;ll have to try the 3bit to see if that helps</div><br/></div></div><div id="36222560" class="c"><input type="checkbox" id="c-36222560" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36219792">parent</a><span>|</span><a href="#36220323">prev</a><span>|</span><a href="#36219174">next</a><span>|</span><label class="collapse" for="c-36222560">[-]</label><label class="expand" for="c-36222560">[1 more]</label></div><br/><div class="children"><div class="content">cuBLAS or CLBlast? There is no such thing as cublast</div><br/></div></div></div></div><div id="36219174" class="c"><input type="checkbox" id="c-36219174" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36218570">parent</a><span>|</span><a href="#36219792">prev</a><span>|</span><a href="#36219745">next</a><span>|</span><label class="collapse" for="c-36219174">[-]</label><label class="expand" for="c-36219174">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The model I have is q4_0 I think that&#x27;s 4 bit quantized<p>That&#x27;s correct, yeah. Q4_0 should be the smallest and fastest quantized model.<p>&gt; I&#x27;m running in Windows using koboldcpp, maybe it&#x27;s faster in Linux?<p>Possibly. You could try using WSL to test—I think both WSL1 and WSL2 are faster than Windows (but WSL1 should be faster than WSL2).</div><br/><div id="36220358" class="c"><input type="checkbox" id="c-36220358" checked=""/><div class="controls bullet"><span class="by">smiley1437</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36219174">parent</a><span>|</span><a href="#36219745">next</a><span>|</span><label class="collapse" for="c-36220358">[-]</label><label class="expand" for="c-36220358">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know what WSL was, but now I do, thanks for the tip!</div><br/></div></div></div></div></div></div></div></div><div id="36219745" class="c"><input type="checkbox" id="c-36219745" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36217701">parent</a><span>|</span><a href="#36217877">prev</a><span>|</span><a href="#36215833">next</a><span>|</span><label class="collapse" for="c-36219745">[-]</label><label class="expand" for="c-36219745">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;n on a Ryzen 4900HS laptop with a RTX 2060.<p>Like I said, very modest</div><br/><div id="36220337" class="c"><input type="checkbox" id="c-36220337" checked=""/><div class="controls bullet"><span class="by">smiley1437</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36219745">parent</a><span>|</span><a href="#36215833">next</a><span>|</span><label class="collapse" for="c-36220337">[-]</label><label class="expand" for="c-36220337">[2 more]</label></div><br/><div class="children"><div class="content">Are you offloading layers to the RTX2060?</div><br/><div id="36221349" class="c"><input type="checkbox" id="c-36221349" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216106">root</a><span>|</span><a href="#36220337">parent</a><span>|</span><a href="#36215833">next</a><span>|</span><label class="collapse" for="c-36221349">[-]</label><label class="expand" for="c-36221349">[1 more]</label></div><br/><div class="children"><div class="content">Some of them, yeah. 17 layers iirc.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36215833" class="c"><input type="checkbox" id="c-36215833" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#36216106">prev</a><span>|</span><label class="collapse" for="c-36215833">[-]</label><label class="expand" for="c-36215833">[5 more]</label></div><br/><div class="children"><div class="content">How common is avx on edge platforms?</div><br/><div id="36216269" class="c"><input type="checkbox" id="c-36216269" checked=""/><div class="controls bullet"><span class="by">svantana</span><span>|</span><a href="#36215833">parent</a><span>|</span><a href="#36217034">next</a><span>|</span><label class="collapse" for="c-36216269">[-]</label><label class="expand" for="c-36216269">[1 more]</label></div><br/><div class="children"><div class="content">Edge just means that the computing is done close to the I&#x2F;O data, so that includes PCs and such.</div><br/></div></div><div id="36217034" class="c"><input type="checkbox" id="c-36217034" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#36215833">parent</a><span>|</span><a href="#36216269">prev</a><span>|</span><label class="collapse" for="c-36217034">[-]</label><label class="expand" for="c-36217034">[3 more]</label></div><br/><div class="children"><div class="content">svantana is correct that PCs are edge, but if you meant &quot;mobile&quot;, then ARM in iOS and Android typically have NEON instructions for SIMD, not AVX: <a href="https:&#x2F;&#x2F;developer.arm.com&#x2F;Architectures&#x2F;Neon" rel="nofollow">https:&#x2F;&#x2F;developer.arm.com&#x2F;Architectures&#x2F;Neon</a></div><br/><div id="36217403" class="c"><input type="checkbox" id="c-36217403" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#36215833">root</a><span>|</span><a href="#36217034">parent</a><span>|</span><label class="collapse" for="c-36217403">[-]</label><label class="expand" for="c-36217403">[2 more]</label></div><br/><div class="children"><div class="content">I was thinking more edge in the distributed serverless sense, but I guess for this type of use the compute part is slow not the latency so question doesn&#x27;t make much sense in hindsight</div><br/><div id="36218877" class="c"><input type="checkbox" id="c-36218877" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#36215833">root</a><span>|</span><a href="#36217403">parent</a><span>|</span><label class="collapse" for="c-36218877">[-]</label><label class="expand" for="c-36218877">[1 more]</label></div><br/><div class="children"><div class="content">Compute <i>is</i> the latency for LLMs :)<p>And in general, your inference code will be compiled to a CPU&#x2F;Architecture target - so you can know ahead of time what instructions you&#x27;ll have access to when writing your code for that target.<p>For example in the case of AWS Lambda, you can choose graviton2 (ARM with NEON), or x86_64 (AVX).  The trick is that for some processors such as Xeon3+ there is AVX 512, and others you will top out at AVX 256.  You might be able to figure out what exact instruction set your serverless target supports.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>