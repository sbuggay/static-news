<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706086859910" as="style"/><link rel="stylesheet" href="styles.css?v=1706086859910"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html">Why is machine learning &#x27;hard&#x27;? (2016)</a> <span class="domain">(<a href="https://ai.stanford.edu">ai.stanford.edu</a>)</span></div><div class="subtext"><span>jxmorris12</span> | <span>97 comments</span></div><br/><div><div id="39112721" class="c"><input type="checkbox" id="c-39112721" checked=""/><div class="controls bullet"><span class="by">4death4</span><span>|</span><a href="#39111551">next</a><span>|</span><label class="collapse" for="c-39112721">[-]</label><label class="expand" for="c-39112721">[27 more]</label></div><br/><div class="children"><div class="content">I used to work on an ML research team. In addition to what the author mentions, there is an entirely separate issue: whether or not what you&#x27;re attempting to do is possible with the approach you&#x27;ve chosen. Consider making an iOS app. For the most part, an experienced software engineer can tell you if making a given app is possible, and they&#x27;ll have a relatively clear idea about the steps required to realize the idea. Compare this to ML problems: you don&#x27;t often know if your data or model selection can produce the results you want. On top of that, you don&#x27;t know if you&#x27;re not getting results because of a bug (i.e. the debugging issues mentioned by the author) or if because you have fundamental blocks elsewhere in the pipeline.</div><br/><div id="39112915" class="c"><input type="checkbox" id="c-39112915" checked=""/><div class="controls bullet"><span class="by">owlninja</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39114108">next</a><span>|</span><label class="collapse" for="c-39112915">[-]</label><label class="expand" for="c-39112915">[3 more]</label></div><br/><div class="children"><div class="content">You nailed where I currently stand. At my company I&#x27;ve been a jack of all trades but mostly software&#x2F;dba work. My boss and I were very excited about ML when the hype cycle was taking off several years ago and completed a successful project. Fast forward to today, I got loaned out to another team that lost their data scientist, and for the first time in my career I&#x27;m having to say - &quot;I don&#x27;t think we can do what you want.&quot; To me the &quot;science&quot; part really stands out. I have a decent grasp of methodologies and tools, but after weeks of dissecting the issue my conclusion is that they just don&#x27;t have enough useful data...</div><br/><div id="39113660" class="c"><input type="checkbox" id="c-39113660" checked=""/><div class="controls bullet"><span class="by">jxramos</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39112915">parent</a><span>|</span><a href="#39113142">next</a><span>|</span><label class="collapse" for="c-39113660">[-]</label><label class="expand" for="c-39113660">[1 more]</label></div><br/><div class="children"><div class="content">It can be a very empirical art. If you can&#x27;t generate more data at the time you can sometimes invest in reviewing the hand labeling ground truth to verify no false classifications slipped by.</div><br/></div></div><div id="39113142" class="c"><input type="checkbox" id="c-39113142" checked=""/><div class="controls bullet"><span class="by">throwaway8877</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39112915">parent</a><span>|</span><a href="#39113660">prev</a><span>|</span><a href="#39114108">next</a><span>|</span><label class="collapse" for="c-39113142">[-]</label><label class="expand" for="c-39113142">[1 more]</label></div><br/><div class="children"><div class="content">The situation is not bad then. Can they collect more data? Can they generate more data?</div><br/></div></div></div></div><div id="39114108" class="c"><input type="checkbox" id="c-39114108" checked=""/><div class="controls bullet"><span class="by">dchichkov</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39112915">prev</a><span>|</span><a href="#39114989">next</a><span>|</span><label class="collapse" for="c-39114108">[-]</label><label class="expand" for="c-39114108">[1 more]</label></div><br/><div class="children"><div class="content">On top of that, vast majority of engineers and researchers who had joined the field, only did it in the last few years.<p>While, like with many other fields, it takes decades to get to a level of a well-rounded expert.  One paper a day, one or two projects a year.  It just takes time.  No matter how brilliant or talented you are.<p>And then the research moves on. And more is different. A GFLOPS shift to TFLOPS and then PFLOPS over a single decade is a seismic shift.</div><br/></div></div><div id="39114989" class="c"><input type="checkbox" id="c-39114989" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39114108">prev</a><span>|</span><a href="#39113163">next</a><span>|</span><label class="collapse" for="c-39114989">[-]</label><label class="expand" for="c-39114989">[1 more]</label></div><br/><div class="children"><div class="content">I think one issue also, is that ML is so large as a field, it ecompasses huge subfields, or related fields (statistics, optimizatiob, etc...)</div><br/></div></div><div id="39113163" class="c"><input type="checkbox" id="c-39113163" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39114989">prev</a><span>|</span><a href="#39113747">next</a><span>|</span><label class="collapse" for="c-39113163">[-]</label><label class="expand" for="c-39113163">[9 more]</label></div><br/><div class="children"><div class="content">&gt; <i>you don&#x27;t often know if your data or model selection can produce the results you want.</i><p>Like, not knowing if your data set actually contains anything predictive of what you&#x27;re trying to predict?</div><br/><div id="39113326" class="c"><input type="checkbox" id="c-39113326" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113163">parent</a><span>|</span><a href="#39113582">next</a><span>|</span><label class="collapse" for="c-39113326">[-]</label><label class="expand" for="c-39113326">[6 more]</label></div><br/><div class="children"><div class="content">Here’s an example of something similar. Say you have a baseline model with an AUC of 0.8. There’s a cool feature you’d like to add. After a week or two of software engineering to add it, you get it into your pipeline.<p>AUC doesn’t budge. Is it because you added it in the wrong place? Is the feature too noisy? Is it because the feature is just a function of your existing features? Is it because your model isn’t big enough to learn the new feature? Is there a logical bug in your implementation?<p>All of these hypotheses will take on the order of days to check.</div><br/><div id="39114102" class="c"><input type="checkbox" id="c-39114102" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113326">parent</a><span>|</span><a href="#39114342">next</a><span>|</span><label class="collapse" for="c-39114102">[-]</label><label class="expand" for="c-39114102">[3 more]</label></div><br/><div class="children"><div class="content"><i>All of these hypotheses will take on the order of days to check.</i><p>OK, but you can check them, right? How is that different from a regular software bug?</div><br/><div id="39114156" class="c"><input type="checkbox" id="c-39114156" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39114102">parent</a><span>|</span><a href="#39114342">next</a><span>|</span><label class="collapse" for="c-39114156">[-]</label><label class="expand" for="c-39114156">[2 more]</label></div><br/><div class="children"><div class="content">In software engineering you can test things in something on the order of seconds to minutes. Functions have fixed contracts which can be unit tested.<p>In ML your turnaround time is days. That alone makes things harder.<p>Further, some of the problems I listed are open-ended which makes it very difficult to debug them.</div><br/><div id="39115134" class="c"><input type="checkbox" id="c-39115134" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39114156">parent</a><span>|</span><a href="#39114342">next</a><span>|</span><label class="collapse" for="c-39115134">[-]</label><label class="expand" for="c-39115134">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In software engineering you can test things in something on the order of seconds to minutes. Functions have fixed contracts which can be unit tested.<p>I think this only applies to a certain subset of software engineering, the one that rhymes with &quot;tine of christmas&quot;.<p>Implementing bitstream formats is an area I&#x27;m very familiar with, and I dance when an issue takes seconds to resolve. Sometimes you need to physically haul a vendor&#x27;s equipment to the lab. In broadcast we have this thing called &quot;Interops&quot; where tons of software and hardware vendors do just this, but in a more convention-esque style (actually is often done at actual conventions).</div><br/></div></div></div></div></div></div><div id="39114342" class="c"><input type="checkbox" id="c-39114342" checked=""/><div class="controls bullet"><span class="by">hcks</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113326">parent</a><span>|</span><a href="#39114102">prev</a><span>|</span><a href="#39113582">next</a><span>|</span><label class="collapse" for="c-39114342">[-]</label><label class="expand" for="c-39114342">[2 more]</label></div><br/><div class="children"><div class="content">Why do you spend weeks adding something instead of directly testing all the later hypotheses?</div><br/><div id="39114978" class="c"><input type="checkbox" id="c-39114978" checked=""/><div class="controls bullet"><span class="by">Chirono</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39114342">parent</a><span>|</span><a href="#39113582">next</a><span>|</span><label class="collapse" for="c-39114978">[-]</label><label class="expand" for="c-39114978">[1 more]</label></div><br/><div class="children"><div class="content">In some cases you can directly test hypotheses like that, but more often than not, there isn’t a way to test without just trying.</div><br/></div></div></div></div></div></div><div id="39113582" class="c"><input type="checkbox" id="c-39113582" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113163">parent</a><span>|</span><a href="#39113326">prev</a><span>|</span><a href="#39113747">next</a><span>|</span><label class="collapse" for="c-39113582">[-]</label><label class="expand" for="c-39113582">[2 more]</label></div><br/><div class="children"><div class="content">The Farmer is Turkey&#x27;s best friend. Turkey believes so because every day Farmer gives Turkey food, lots of food. Farmer also keeps the Turkey warm and safe from predators. Turkey predicts that the Farmer will keep on being his best friend also tomorrow. Until one day, around Thanksgiving, the prediction goes wrong, awfully wrong.</div><br/><div id="39113632" class="c"><input type="checkbox" id="c-39113632" checked=""/><div class="controls bullet"><span class="by">sanroot99</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113582">parent</a><span>|</span><a href="#39113747">next</a><span>|</span><label class="collapse" for="c-39113632">[-]</label><label class="expand" for="c-39113632">[1 more]</label></div><br/><div class="children"><div class="content">Three body problem, reference,yeh</div><br/></div></div></div></div></div></div><div id="39113747" class="c"><input type="checkbox" id="c-39113747" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39113163">prev</a><span>|</span><a href="#39113032">next</a><span>|</span><label class="collapse" for="c-39113747">[-]</label><label class="expand" for="c-39113747">[3 more]</label></div><br/><div class="children"><div class="content">I have the same sentiments about DIY electronic designs. If I take someone else&#x27;s designs and build it at home, I know it&#x27;s all on my build skills lacking if it doesn&#x27;t work as there is already working examples. If I design a device from the electronics to the software, I don&#x27;t know if the thing isn&#x27;t working because of bugs in the code, problems with the build of the electronics, or fundamental flaw in the design itself. At least not without a ton of time debugging it all.</div><br/><div id="39114004" class="c"><input type="checkbox" id="c-39114004" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113747">parent</a><span>|</span><a href="#39113032">next</a><span>|</span><label class="collapse" for="c-39114004">[-]</label><label class="expand" for="c-39114004">[2 more]</label></div><br/><div class="children"><div class="content">However, we now have techniques for debugging electronics. Electronics tends to be designed to be decomposable into subunits, with some way to do unit testing.
At least in the prototype, before it&#x27;s shrunk for production. 
Test gear can be expensive, but it exists, all the way down to the wafer if needed.<p>That wasn&#x27;t always the case. Electronics problems used to be more mysterious. The conquest of electronic design and debugging is what allows making really complex electronics that works.
It really is amazing that smartphones work at all, with all those radios in that little case.
That RF engineers can get a GPS receiver and a GSM transmitter to work a few centimeters apart is just amazing.<p>Machine learning isn&#x27;t that far along yet. When it doesn&#x27;t work, the tools for figuring out why are inadequate. It&#x27;s not even clear yet if this is a technique problem which can be fixed with tooling, or an inherent problem with having a huge matrix of weights.</div><br/><div id="39114106" class="c"><input type="checkbox" id="c-39114106" checked=""/><div class="controls bullet"><span class="by">mikrotikker</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39114004">parent</a><span>|</span><a href="#39113032">next</a><span>|</span><label class="collapse" for="c-39114106">[-]</label><label class="expand" for="c-39114106">[1 more]</label></div><br/><div class="children"><div class="content">I never understood the black magic behind things like 4g until I saw a teardown of some pole equipment and saw the the solid copper beam forming cavities inside. Blew my mind.</div><br/></div></div></div></div></div></div><div id="39113032" class="c"><input type="checkbox" id="c-39113032" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39113747">prev</a><span>|</span><a href="#39113242">next</a><span>|</span><label class="collapse" for="c-39113032">[-]</label><label class="expand" for="c-39113032">[5 more]</label></div><br/><div class="children"><div class="content">But aren&#x27;t all science basically like this? If you know your hypothsis works before you do the experiments, it&#x27;s not science anymore.</div><br/><div id="39114354" class="c"><input type="checkbox" id="c-39114354" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113032">parent</a><span>|</span><a href="#39113396">next</a><span>|</span><label class="collapse" for="c-39114354">[-]</label><label class="expand" for="c-39114354">[1 more]</label></div><br/><div class="children"><div class="content">You are right.  That&#x27;s why you want to avoid doing science, when you can.<p>Ideally, you want to be solving engineering problems instead.</div><br/></div></div><div id="39113396" class="c"><input type="checkbox" id="c-39113396" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113032">parent</a><span>|</span><a href="#39114354">prev</a><span>|</span><a href="#39113767">next</a><span>|</span><label class="collapse" for="c-39113396">[-]</label><label class="expand" for="c-39113396">[1 more]</label></div><br/><div class="children"><div class="content">Yes, and science is &quot;hard&quot; compared to software development in a lot of ways. Less certainty of success and poorly defined success criteria.</div><br/></div></div><div id="39113767" class="c"><input type="checkbox" id="c-39113767" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113032">parent</a><span>|</span><a href="#39113396">prev</a><span>|</span><a href="#39113402">next</a><span>|</span><label class="collapse" for="c-39113767">[-]</label><label class="expand" for="c-39113767">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes something you&#x27;ve done works, but you really don&#x27;t <i>know</i> why&#x2F;how. You then have to walk it back to figure out by experimenting what is causing it to work. I feel like this happen(ed|s) in chemistry a lot. Was it the fact that I stirred it counter clockwise this time, or that I got distracted and the temp went 5° hotter than intended, or that I didn&#x27;t quite clean my beaker properly and some residue contaminated this batch, or any number of other steps.</div><br/></div></div><div id="39113402" class="c"><input type="checkbox" id="c-39113402" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113032">parent</a><span>|</span><a href="#39113767">prev</a><span>|</span><a href="#39113242">next</a><span>|</span><label class="collapse" for="c-39113402">[-]</label><label class="expand" for="c-39113402">[1 more]</label></div><br/><div class="children"><div class="content">Not really. It’s easy to tell relative to existing methods whether the size of data will solve the problem. For example, if you’re trying to solve a classification problem with a large number of labels, but only have a small amount of training data for some (or all) of them, it will probably never work.</div><br/></div></div></div></div><div id="39113242" class="c"><input type="checkbox" id="c-39113242" checked=""/><div class="controls bullet"><span class="by">senthil_rajasek</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39113032">prev</a><span>|</span><a href="#39112985">next</a><span>|</span><label class="collapse" for="c-39113242">[-]</label><label class="expand" for="c-39113242">[3 more]</label></div><br/><div class="children"><div class="content">I have always thought of ML (not DL) as phenomena that can be modelled mathematically.<p>It turns out that not all problems have a great mathematical model like self driving cars for instance and so the search continues...</div><br/><div id="39114529" class="c"><input type="checkbox" id="c-39114529" checked=""/><div class="controls bullet"><span class="by">croutons</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113242">parent</a><span>|</span><a href="#39114348">next</a><span>|</span><label class="collapse" for="c-39114529">[-]</label><label class="expand" for="c-39114529">[1 more]</label></div><br/><div class="children"><div class="content">All of ML, including DL, are literally implemented using mathematical models. Alas, a model is just a model and doesn’t imply it works well or imply that it’s simple or easily discoverable.</div><br/></div></div><div id="39114348" class="c"><input type="checkbox" id="c-39114348" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39112721">root</a><span>|</span><a href="#39113242">parent</a><span>|</span><a href="#39114529">prev</a><span>|</span><a href="#39112985">next</a><span>|</span><label class="collapse" for="c-39114348">[-]</label><label class="expand" for="c-39114348">[1 more]</label></div><br/><div class="children"><div class="content">Why would self-driving cars not have a great mathematical model?<p>Or do you mean that the models are either black boxes (like deep learning) that we don&#x27;t understand, and the white box models are not good enough?</div><br/></div></div></div></div><div id="39112985" class="c"><input type="checkbox" id="c-39112985" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#39112721">parent</a><span>|</span><a href="#39113242">prev</a><span>|</span><a href="#39111551">next</a><span>|</span><label class="collapse" for="c-39112985">[-]</label><label class="expand" for="c-39112985">[1 more]</label></div><br/><div class="children"><div class="content">i.e. there a lot more unknown unknowns, which takes a lot more effort and intelligence to not stumble into haphazardly then most other fields.</div><br/></div></div></div></div><div id="39111551" class="c"><input type="checkbox" id="c-39111551" checked=""/><div class="controls bullet"><span class="by">sackfield</span><span>|</span><a href="#39112721">prev</a><span>|</span><a href="#39112542">next</a><span>|</span><label class="collapse" for="c-39111551">[-]</label><label class="expand" for="c-39111551">[8 more]</label></div><br/><div class="children"><div class="content">I think a big difference between ML and regular programming is how the components at scale make the systems viable. When I was learning computer science, it seemed quite intuitive to me that you would start out with assembly, then go to a C like compiler, then abstract that to a JIT&#x2F;dynamic type language, and go from that to the UI. I could see how each step in the layer added value and presented its tradeoffs.<p>Contrast that to ML and even though I have done a large amount of work in it (in both university and in industry), I still can&#x27;t fully appreciate how the building blocks interact to form an entire system. I find that I use intuition from other systems I have read about or implemented (e.g. decision trees and tabular data, ReLUs and images) to reason about the results in new systems and guess at better configurations and architectures.<p>Might say more about me, but I always found ML was a &quot;start big and go backwards&quot; deal whereas computer science was a &quot;start small and go forwards&quot; deal.</div><br/><div id="39111623" class="c"><input type="checkbox" id="c-39111623" checked=""/><div class="controls bullet"><span class="by">barelyauser</span><span>|</span><a href="#39111551">parent</a><span>|</span><a href="#39112542">next</a><span>|</span><label class="collapse" for="c-39111623">[-]</label><label class="expand" for="c-39111623">[7 more]</label></div><br/><div class="children"><div class="content">When building models it is useful to spend some time finding out what you already know about the problem. Things you yet don&#x27;t know you know. This kind of knowledge will greatly simplify the model.<p>I see newcomers making this mistake very often. In industrial vision, for example, the newcomers like to create very complicated models. I then show them that the &quot;box&quot; you trained a entire model to recognize will actually always be there in this position for the camera, because it sits in a conveyor belt which restricts its lateral movement. The problem can simply be solved with simple image processing. Stuff like that happens all the time.</div><br/><div id="39113541" class="c"><input type="checkbox" id="c-39113541" checked=""/><div class="controls bullet"><span class="by">shostack</span><span>|</span><a href="#39111551">root</a><span>|</span><a href="#39111623">parent</a><span>|</span><a href="#39112138">next</a><span>|</span><label class="collapse" for="c-39113541">[-]</label><label class="expand" for="c-39113541">[1 more]</label></div><br/><div class="children"><div class="content">When I was working on a pet project to teach myself how to build a scoring model based on analysis of images on mobile I went down a whole rabbit hole on how to detect where the image is in a photo to draw a box around it and then compress to 500x500.<p>In reality, if I&#x27;m using a phone,I can just create a square frame for the user to center the image in and then compress.<p>Sometimes the simplest solution is the one you don&#x27;t get to till after you slog through the harder approach. I&#x27;m glad I learned a bit about image processing with Pytorch along the way.</div><br/></div></div><div id="39112138" class="c"><input type="checkbox" id="c-39112138" checked=""/><div class="controls bullet"><span class="by">Cacti</span><span>|</span><a href="#39111551">root</a><span>|</span><a href="#39111623">parent</a><span>|</span><a href="#39113541">prev</a><span>|</span><a href="#39112542">next</a><span>|</span><label class="collapse" for="c-39112138">[-]</label><label class="expand" for="c-39112138">[5 more]</label></div><br/><div class="children"><div class="content">You let newcomers dick around for months with a net on an industrial vision problem? This stuff was solved two decades ago. Why didn’t you just tell them?</div><br/><div id="39112212" class="c"><input type="checkbox" id="c-39112212" checked=""/><div class="controls bullet"><span class="by">barelyauser</span><span>|</span><a href="#39111551">root</a><span>|</span><a href="#39112138">parent</a><span>|</span><a href="#39113322">next</a><span>|</span><label class="collapse" for="c-39112212">[-]</label><label class="expand" for="c-39112212">[2 more]</label></div><br/><div class="children"><div class="content">They usually don&#x27;t take months. Would be optimal if I could catch them at the get go. Not what happens most of the time. When that happen most of the times you will se a manager who is not technical leading a group selected by &quot;professional&quot; recruiters. There is a lot of waste out there.</div><br/><div id="39112458" class="c"><input type="checkbox" id="c-39112458" checked=""/><div class="controls bullet"><span class="by">Cacti</span><span>|</span><a href="#39111551">root</a><span>|</span><a href="#39112212">parent</a><span>|</span><a href="#39113322">next</a><span>|</span><label class="collapse" for="c-39112458">[-]</label><label class="expand" for="c-39112458">[1 more]</label></div><br/><div class="children"><div class="content">Oh. I understand. Carry on then lol<p>Been there. And you’re right.</div><br/></div></div></div></div><div id="39113322" class="c"><input type="checkbox" id="c-39113322" checked=""/><div class="controls bullet"><span class="by">wegfawefgawefg</span><span>|</span><a href="#39111551">root</a><span>|</span><a href="#39112138">parent</a><span>|</span><a href="#39112212">prev</a><span>|</span><a href="#39112542">next</a><span>|</span><label class="collapse" for="c-39113322">[-]</label><label class="expand" for="c-39113322">[2 more]</label></div><br/><div class="children"><div class="content">Sometimes its really hard to convince an organization that their strategy wont work. Especially if ego or higher ups are convinced.</div><br/><div id="39113820" class="c"><input type="checkbox" id="c-39113820" checked=""/><div class="controls bullet"><span class="by">chefandy</span><span>|</span><a href="#39111551">root</a><span>|</span><a href="#39113322">parent</a><span>|</span><a href="#39112542">next</a><span>|</span><label class="collapse" for="c-39113820">[-]</label><label class="expand" for="c-39113820">[1 more]</label></div><br/><div class="children"><div class="content">Many developers also mistakenly think their <i>mighty dev super brain genius powers</i> make them capable of accurately evaluating and critiquing any profession. I’ve had many developers try to explain design to me, even knowing I’m an experienced, educated professional designer. I can see why some might roll their eyes.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39112542" class="c"><input type="checkbox" id="c-39112542" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39111551">prev</a><span>|</span><a href="#39114036">next</a><span>|</span><label class="collapse" for="c-39112542">[-]</label><label class="expand" for="c-39112542">[6 more]</label></div><br/><div class="children"><div class="content">Machine learning isn&#x27;t comparable to software development. It is a statistical modelling exercise. This is like asking why advertising is hard - if a non-expert wades in to a different domain then they may find it has different challenges than what they are used to! This is just a specific case of the normal things that analysts routinely deal with.<p>The major challenges in this youthful field of machine learning are building appropriate hardware and making it work. That, so far, has kept it the domain of the software engineer. As the situation continues to stabilise this is going to become the playground of statisticians and analysts.<p>Or to put it another way - if you compare <i>any</i> field to software engineering, the problem is that other disciplines have a much harder time debugging things. Software is almost unique in that debugging is cheaper and quicker than building things right the first time.</div><br/><div id="39113350" class="c"><input type="checkbox" id="c-39113350" checked=""/><div class="controls bullet"><span class="by">wegfawefgawefg</span><span>|</span><a href="#39112542">parent</a><span>|</span><a href="#39113039">next</a><span>|</span><label class="collapse" for="c-39113350">[-]</label><label class="expand" for="c-39113350">[3 more]</label></div><br/><div class="children"><div class="content">I dont feel like im doing statistical modeling when i do ml. Usually feels more like pipe alignment, followed by tremendous amounts of debugging.</div><br/><div id="39113455" class="c"><input type="checkbox" id="c-39113455" checked=""/><div class="controls bullet"><span class="by">salty_biscuits</span><span>|</span><a href="#39112542">root</a><span>|</span><a href="#39113350">parent</a><span>|</span><a href="#39114162">next</a><span>|</span><label class="collapse" for="c-39113455">[-]</label><label class="expand" for="c-39113455">[1 more]</label></div><br/><div class="children"><div class="content">That is also what statistical modelling feels like. EDA and data cleaning.</div><br/></div></div><div id="39114162" class="c"><input type="checkbox" id="c-39114162" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39112542">root</a><span>|</span><a href="#39113350">parent</a><span>|</span><a href="#39113455">prev</a><span>|</span><a href="#39113039">next</a><span>|</span><label class="collapse" for="c-39114162">[-]</label><label class="expand" for="c-39114162">[1 more]</label></div><br/><div class="children"><div class="content">It depends on what is it you do when you “do ml”.</div><br/></div></div></div></div><div id="39113039" class="c"><input type="checkbox" id="c-39113039" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#39112542">parent</a><span>|</span><a href="#39113350">prev</a><span>|</span><a href="#39114036">next</a><span>|</span><label class="collapse" for="c-39113039">[-]</label><label class="expand" for="c-39113039">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Machine learning isn&#x27;t comparable to software development. It is a statistical modelling exercise.<p>It&#x27;s neither of the two. Machine learning isn&#x27;t comparable to any other human endeavor because in many cases, much more value comes out of the models than (seemingly) goes in.<p>LLMs for example are punching <i>way</i> above their weight. The ideas underlying their software implementations are extremely simple compared to the incredibly complex behavior they produce. Take some neural networks that can be explained to a bright high schooler, add a few more relatively basic ML concepts, then push an unfiltered dump of half the Internet into it, and suddenly you get a machine that talks like a human.<p>Obviously I&#x27;m simplifying here, but consider that state-of-the-art LLM architectures are still simple enough that they can be <i>completely</i> understood through a 10-hour online course, and can be implemented in a few hundred lines of PyTorch code. That&#x27;s absolutely bananas considering that the end result is something that can write a poem about airplanes in the style of Beowulf.</div><br/><div id="39114369" class="c"><input type="checkbox" id="c-39114369" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39112542">root</a><span>|</span><a href="#39113039">parent</a><span>|</span><a href="#39114036">next</a><span>|</span><label class="collapse" for="c-39114369">[-]</label><label class="expand" for="c-39114369">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure it&#x27;s as simple as you make it sound.<p>Lots of problems have very simple solutions.  And progress often means finding simpler solutions over time.<p>But coming up with those solutions, and debugging them, is what&#x27;s hard.<p>For a comparison, have a look at how pistols got simpler over the last two hundred years.  Have a look at the intricate mechanism of the P08 Luger <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9adOzT_qMq0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9adOzT_qMq0</a> and compare it to a modern pistol of your choice.  (And the P08 Luger is already pretty late invention.)<p>Or have a look at modern Ikea furniture, which can be assembled and understood by untrained members of the general public.  But designing new Ikea furniture is much harder.</div><br/></div></div></div></div></div></div><div id="39114036" class="c"><input type="checkbox" id="c-39114036" checked=""/><div class="controls bullet"><span class="by">hooande</span><span>|</span><a href="#39112542">prev</a><span>|</span><a href="#39110416">next</a><span>|</span><label class="collapse" for="c-39114036">[-]</label><label class="expand" for="c-39114036">[1 more]</label></div><br/><div class="children"><div class="content">Debugging is a problem. But the real problem I&#x27;m seeing is our expectations as software developers. We&#x27;re used to being able to fix any problem that we see. If a div is misaligned or a column of numbers is wrong we can open the file, find the offending lines of code and FIX it.<p>Machine learning is different because every implementation has a known error rate. If your application has a measured 80% accuracy then 20% of cases WILL have an error. You don&#x27;t know which 20% and you don&#x27;t get to choose. There&#x27;s no way to notice a problem and immediately fix it, like you can with almost every other kind of engineering. At best you can expand your dataset, incorporate new models, fix actual bugs in the code. Doing those things could increase the accuracy up to, say, 85%. This means there will be fewer errors overall, but the one that you happened to notice may or may not still be there. There&#x27;s no way to directly intervene.<p>I see a lot of people who are new to the field struggle with this. There are many ways to improve models and handle edge cases. But not being able to fix a problem that&#x27;s in front of you takes some getting used to.</div><br/></div></div><div id="39110416" class="c"><input type="checkbox" id="c-39110416" checked=""/><div class="controls bullet"><span class="by">doctorM</span><span>|</span><a href="#39114036">prev</a><span>|</span><a href="#39110766">next</a><span>|</span><label class="collapse" for="c-39110416">[-]</label><label class="expand" for="c-39110416">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a bit sceptical of the exponentially harder debugging claim.<p>First it looks polynomially harder for the given example :p.<p>Second other engineering domains arguably have additional dimensions which correspond to the machine learning ones mentioned in the article. The choice of which high level algorithm to implement is another dimension to traditional software engineering that seemingly exists and corresponds to the model dimension. This is often codified as &#x27;design&#x27;.<p>The data dimension often exists as well in standard learning software engineering. [Think of a system that is &#x27;downstream&#x27; of other].<p>It&#x27;s probably a lot simpler to deal with these dimensions in standard software engineering - but then this is what makes machine learning harder, not that there are simply &#x27;more dimensions&#x27;.<p>The delayed debugging cycles point seems a lot more valid.</div><br/><div id="39111722" class="c"><input type="checkbox" id="c-39111722" checked=""/><div class="controls bullet"><span class="by">jolt42</span><span>|</span><a href="#39110416">parent</a><span>|</span><a href="#39111142">next</a><span>|</span><label class="collapse" for="c-39111722">[-]</label><label class="expand" for="c-39111722">[2 more]</label></div><br/><div class="children"><div class="content">The article also pretends that there is only one correct answer, which seems atypical of the domain. The 1 green spot should extend somewhat fuzzily in each dimension in the ML case.</div><br/><div id="39113352" class="c"><input type="checkbox" id="c-39113352" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#39110416">root</a><span>|</span><a href="#39111722">parent</a><span>|</span><a href="#39111142">next</a><span>|</span><label class="collapse" for="c-39113352">[-]</label><label class="expand" for="c-39113352">[1 more]</label></div><br/><div class="children"><div class="content">There isn’t only one correct answer. Quite the opposite actually, many configurations give you local maximas. The difficulty is that it can be hard to explain from first principles why one local maxima is good.</div><br/></div></div></div></div><div id="39111142" class="c"><input type="checkbox" id="c-39111142" checked=""/><div class="controls bullet"><span class="by">pizzaknife</span><span>|</span><a href="#39110416">parent</a><span>|</span><a href="#39111722">prev</a><span>|</span><a href="#39110766">next</a><span>|</span><label class="collapse" for="c-39111142">[-]</label><label class="expand" for="c-39111142">[1 more]</label></div><br/><div class="children"><div class="content">i would subscribe to your newsletter if you offered one.</div><br/></div></div></div></div><div id="39110766" class="c"><input type="checkbox" id="c-39110766" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#39110416">prev</a><span>|</span><a href="#39112343">next</a><span>|</span><label class="collapse" for="c-39110766">[-]</label><label class="expand" for="c-39110766">[13 more]</label></div><br/><div class="children"><div class="content">The #1 thing that makes it ‘hard’ in real life is that nobody wants to make training and test sets.  So we have 50,000 papers on the NIST digits but no insight into ‘would this work for a different problem?’ (Ironically the latter might have been exactly what academics would have needed to understand why these algorithms work!)</div><br/><div id="39110884" class="c"><input type="checkbox" id="c-39110884" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#39110766">parent</a><span>|</span><a href="#39111506">next</a><span>|</span><label class="collapse" for="c-39110884">[-]</label><label class="expand" for="c-39110884">[2 more]</label></div><br/><div class="children"><div class="content">You’re not paying tribute to MNIST-1D and many other datasets (including the massive segmentation dataset released by Meta with SAM).   Read the literature before lecturing the community.</div><br/><div id="39111982" class="c"><input type="checkbox" id="c-39111982" checked=""/><div class="controls bullet"><span class="by">Mltrw</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39110884">parent</a><span>|</span><a href="#39111506">next</a><span>|</span><label class="collapse" for="c-39111982">[-]</label><label class="expand" for="c-39111982">[1 more]</label></div><br/><div class="children"><div class="content">We still don&#x27;t have enough data and people are still wasting their time with trying to extend algorithms instead of making better training data.<p>I&#x27;ve worked on a dozen ml projects two of them before Alex net came out and I&#x27;ve never gone wrong by spending 80% of my time creating a dataset specific to the problem and then using whatever algorithm is top dog right now.<p>Labelled data is king.</div><br/></div></div></div></div><div id="39111506" class="c"><input type="checkbox" id="c-39111506" checked=""/><div class="controls bullet"><span class="by">beckhamc</span><span>|</span><a href="#39110766">parent</a><span>|</span><a href="#39110884">prev</a><span>|</span><a href="#39111755">next</a><span>|</span><label class="collapse" for="c-39111506">[-]</label><label class="expand" for="c-39111506">[2 more]</label></div><br/><div class="children"><div class="content">The issue is the obsession with benchmark datasets and their flaky evaluation</div><br/><div id="39111534" class="c"><input type="checkbox" id="c-39111534" checked=""/><div class="controls bullet"><span class="by">graphe</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39111506">parent</a><span>|</span><a href="#39111755">next</a><span>|</span><label class="collapse" for="c-39111534">[-]</label><label class="expand" for="c-39111534">[1 more]</label></div><br/><div class="children"><div class="content">What else could you do to test it besides it works for me and this test said it&#x27;s good at talking?</div><br/></div></div></div></div><div id="39111755" class="c"><input type="checkbox" id="c-39111755" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#39110766">parent</a><span>|</span><a href="#39111506">prev</a><span>|</span><a href="#39110877">next</a><span>|</span><label class="collapse" for="c-39111755">[-]</label><label class="expand" for="c-39111755">[1 more]</label></div><br/><div class="children"><div class="content">no, this is routinely cited in introductory remarks these days, but ignores some practical aspects of the competitive context, among other things.</div><br/></div></div><div id="39110877" class="c"><input type="checkbox" id="c-39110877" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#39110766">parent</a><span>|</span><a href="#39111755">prev</a><span>|</span><a href="#39112343">next</a><span>|</span><label class="collapse" for="c-39110877">[-]</label><label class="expand" for="c-39110877">[7 more]</label></div><br/><div class="children"><div class="content">Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity.</div><br/><div id="39111456" class="c"><input type="checkbox" id="c-39111456" checked=""/><div class="controls bullet"><span class="by">MichaelRo</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39110877">parent</a><span>|</span><a href="#39111367">next</a><span>|</span><label class="collapse" for="c-39111456">[-]</label><label class="expand" for="c-39111456">[3 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Seems like a prime startup opportunity.<p>Sometimes it&#x27;s just ... hard. Apply some thought maybe before blindly parroting &quot;profit!&quot;<p>Reporter: &quot;Why is it hard to cure cancer?&quot;. Crowd: &quot;Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!&quot;<p>Reporter: &quot;Why is it hard to end World poverty?&quot;. Crowd: &quot;Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!&quot;<p>Reporter: &quot;Why is it hard to build a warp engine?&quot;. Crowd: &quot;Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!&quot;<p>Reporter: &quot;Why is it hard to wipe your ass using the left hand?&quot;. Crowd: &quot;Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!&quot;<p>You get the idea...</div><br/><div id="39113364" class="c"><input type="checkbox" id="c-39113364" checked=""/><div class="controls bullet"><span class="by">wegfawefgawefg</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39111456">parent</a><span>|</span><a href="#39112055">next</a><span>|</span><label class="collapse" for="c-39113364">[-]</label><label class="expand" for="c-39113364">[1 more]</label></div><br/><div class="children"><div class="content">A cure for cancer would be terribly profitable. For a while.</div><br/></div></div><div id="39112055" class="c"><input type="checkbox" id="c-39112055" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39111456">parent</a><span>|</span><a href="#39113364">prev</a><span>|</span><a href="#39111367">next</a><span>|</span><label class="collapse" for="c-39112055">[-]</label><label class="expand" for="c-39112055">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Reporter: &quot;Why is it hard to cure cancer?&quot;. Crowd: &quot;Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!&quot;<p>What you want to optimize for is the money amount that you make at some quantile of the probablity distribution of the profits; say, the profits that are guaranteed in the best, say, 3 %, 5 %, 10 % or even 20 % of all possible outcomes. With a probablity of 97 % (if you choose the best 3 % of the outcomes), you won&#x27;t make sufficient money if you attempt to cure cancer to be worth the risk, so the financial incentive is not there.<p>TLDR: Financial incentives <i>do</i> matter, but work differently from how many people think that they are structured.</div><br/></div></div></div></div><div id="39111367" class="c"><input type="checkbox" id="c-39111367" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39110877">parent</a><span>|</span><a href="#39111456">prev</a><span>|</span><a href="#39112030">next</a><span>|</span><label class="collapse" for="c-39111367">[-]</label><label class="expand" for="c-39111367">[1 more]</label></div><br/><div class="children"><div class="content">I was just thinking the same, but I&#x27;m skeptical.<p>When researchers want to publish a paper, are they going to pay extra money for extra difficulty in publishing their paper? No, they&#x27;ll just use whatever toy environment is free or already established and get that paper published!</div><br/></div></div><div id="39112030" class="c"><input type="checkbox" id="c-39112030" checked=""/><div class="controls bullet"><span class="by">Mltrw</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39110877">parent</a><span>|</span><a href="#39111367">prev</a><span>|</span><a href="#39111108">next</a><span>|</span><label class="collapse" for="c-39112030">[-]</label><label class="expand" for="c-39112030">[1 more]</label></div><br/><div class="children"><div class="content">There is plenty of money in it but you need to sell b2b and tp enterprise. That is not fun and as such no one is doing it.<p>Put another way,if I were trying to do a start up in this space I&#x27;d spend 50% of my budget on marketing 25% on a third world data labelling sweatshop, 20% on data pipeline engineering and 5% on sexy ml stuff.</div><br/></div></div><div id="39111108" class="c"><input type="checkbox" id="c-39111108" checked=""/><div class="controls bullet"><span class="by">rzzzz</span><span>|</span><a href="#39110766">root</a><span>|</span><a href="#39110877">parent</a><span>|</span><a href="#39112030">prev</a><span>|</span><a href="#39112343">next</a><span>|</span><label class="collapse" for="c-39111108">[-]</label><label class="expand" for="c-39111108">[1 more]</label></div><br/><div class="children"><div class="content">I believe that Scale.ai was founded to do exactly this.</div><br/></div></div></div></div></div></div><div id="39112343" class="c"><input type="checkbox" id="c-39112343" checked=""/><div class="controls bullet"><span class="by">yshvrdhn</span><span>|</span><a href="#39110766">prev</a><span>|</span><a href="#39114830">next</a><span>|</span><label class="collapse" for="c-39112343">[-]</label><label class="expand" for="c-39112343">[5 more]</label></div><br/><div class="children"><div class="content">I think one of the issues is that fixing a problem is a lot harder in ML than in software engineering. You know that the model fails on this particular data point. If you have identified a bug in the code and wrote a fix did a pull request as long you are able to test the code for conditions you failed on you would have solved the problem. With modern ml especially with nueral nets as long as you don&#x27;t have a way to spin up a data engine to track the problems you are facing and collect similar points you problem is not fixed.</div><br/><div id="39112865" class="c"><input type="checkbox" id="c-39112865" checked=""/><div class="controls bullet"><span class="by">nonethewiser</span><span>|</span><a href="#39112343">parent</a><span>|</span><a href="#39114252">next</a><span>|</span><label class="collapse" for="c-39112865">[-]</label><label class="expand" for="c-39112865">[1 more]</label></div><br/><div class="children"><div class="content">Certainly resonates with me. When the problem is something like a list being the wrong shape or two lists not maintaining a parallel order its basically invisible unless you load the mental model of the code and think deeply about it. Not like you’re going to notice your list of length 2,340,383 should start with [0.12, 1.67,  0.66 instead of [0.412, 0.567,  0.23</div><br/></div></div><div id="39114252" class="c"><input type="checkbox" id="c-39114252" checked=""/><div class="controls bullet"><span class="by">pbh101</span><span>|</span><a href="#39112343">parent</a><span>|</span><a href="#39112865">prev</a><span>|</span><a href="#39114830">next</a><span>|</span><label class="collapse" for="c-39114252">[-]</label><label class="expand" for="c-39114252">[3 more]</label></div><br/><div class="children"><div class="content">I remember running into a paper from Google circa 2017 IIRC discussing the maintainability issues with machine learning models but haven’t been able to track down since. Does anyone know which one this is and have a link?</div><br/><div id="39114278" class="c"><input type="checkbox" id="c-39114278" checked=""/><div class="controls bullet"><span class="by">hollerith</span><span>|</span><a href="#39112343">root</a><span>|</span><a href="#39114252">parent</a><span>|</span><a href="#39114830">next</a><span>|</span><label class="collapse" for="c-39114278">[-]</label><label class="expand" for="c-39114278">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;research.google&#x2F;pubs&#x2F;machine-learning-the-high-interest-credit-card-of-technical-debt&#x2F;" rel="nofollow">https:&#x2F;&#x2F;research.google&#x2F;pubs&#x2F;machine-learning-the-high-inter...</a></div><br/><div id="39114308" class="c"><input type="checkbox" id="c-39114308" checked=""/><div class="controls bullet"><span class="by">pbh101</span><span>|</span><a href="#39112343">root</a><span>|</span><a href="#39114278">parent</a><span>|</span><a href="#39114830">next</a><span>|</span><label class="collapse" for="c-39114308">[-]</label><label class="expand" for="c-39114308">[1 more]</label></div><br/><div class="children"><div class="content">Looks like I found a similar one just now too from the group. Thanks!<p><a href="https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2015&#x2F;file&#x2F;86df7dcfd896fcaf2674f757a2463eba-Paper.pdf" rel="nofollow">https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2015&#x2F;file&#x2F;8...</a></div><br/></div></div></div></div></div></div></div></div><div id="39114830" class="c"><input type="checkbox" id="c-39114830" checked=""/><div class="controls bullet"><span class="by">nazka</span><span>|</span><a href="#39112343">prev</a><span>|</span><a href="#39112293">next</a><span>|</span><label class="collapse" for="c-39114830">[-]</label><label class="expand" for="c-39114830">[1 more]</label></div><br/><div class="children"><div class="content">For me it’s 2 things: math is already hard even with a teacher and after you found a job it’s just you and your screen. The other part is research moves at a crazy speed it’s very hard to keep up I think while having a job.</div><br/></div></div><div id="39112293" class="c"><input type="checkbox" id="c-39112293" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#39114830">prev</a><span>|</span><a href="#39109584">next</a><span>|</span><label class="collapse" for="c-39112293">[-]</label><label class="expand" for="c-39112293">[1 more]</label></div><br/><div class="children"><div class="content">Great teachers(if you can find them): Andrej Karpathy, Andrew Gelman &amp; Ben Goodrich(Columbia), Subbarao Khambampati(ASU) to name a few I know of.<p>Go Where the hard problems are (or find someone who is doing it): If you don&#x27;t have a good intuition where to get good problems to practice on for a pay, choose a place to work where a Data Scientist is not just building dashboards&#x2F;analytics but the company&#x2F;team relies on them for answer to questions like: &quot;What goals should we set for the next half based on what you see&quot;?<p>ML practitioner (read: use ML tech to do&#x2F;debug X) different from ML Engineer (Read: Implement ML algorithm X e2e on data ) is different from Applied Statistician (think marketing sciences or powering experiments like A&#x2F;B tests): All three areas of work in different areas of ML in one form or another. But make sure what you want to work in is clear in your head and your expectations from it.<p>A <i>lot</i> of ML&#x2F;Stats can be not with big data and yet really intuitive: I would say look for a problem domain in social&#x2F;life&#x2F;pharma&#x2F;eco&#x2F;political&#x2F;survey&#x2F;edtech sciences. They are full of intuitive models that <i>need</i> to be explainable and are often debuggable. An example here is usage of Stan software for Multilevel&#x2F;Heirarchical Regression problems. Training here also makes you a great DS.</div><br/></div></div><div id="39109584" class="c"><input type="checkbox" id="c-39109584" checked=""/><div class="controls bullet"><span class="by">jruohonen</span><span>|</span><a href="#39112293">prev</a><span>|</span><a href="#39111665">next</a><span>|</span><label class="collapse" for="c-39109584">[-]</label><label class="expand" for="c-39109584">[3 more]</label></div><br/><div class="children"><div class="content">&quot;An aspect of this difficulty involves building an intuition for what tool should be leveraged to solve a problem.&quot;<p>While I agree with the good point about debugging, like many others, I am rather worried that we&#x27;re increasingly deploying AI&#x2F;ML where we shouldn&#x27;t be deploying it. Hence, the above quote.</div><br/><div id="39110856" class="c"><input type="checkbox" id="c-39110856" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39109584">parent</a><span>|</span><a href="#39111960">next</a><span>|</span><label class="collapse" for="c-39110856">[-]</label><label class="expand" for="c-39110856">[1 more]</label></div><br/><div class="children"><div class="content">I’m old enough to have learned that the secret to success is much less knowing the tool of the moment than picking the right tool for a job.<p>The right tool may in fact be the new one, and LLM do open a lot of doors with zero shot capabilities, but oftentimes they can underperform a well tuned heuristic. It’s the ability to pick the right tool that is key.</div><br/></div></div><div id="39111960" class="c"><input type="checkbox" id="c-39111960" checked=""/><div class="controls bullet"><span class="by">wwarner</span><span>|</span><a href="#39109584">parent</a><span>|</span><a href="#39110856">prev</a><span>|</span><a href="#39111665">next</a><span>|</span><label class="collapse" for="c-39111960">[-]</label><label class="expand" for="c-39111960">[1 more]</label></div><br/><div class="children"><div class="content">Want to agree with you, as so many ML apps seem to be solutions looking for problems. But I actually feel that we are rapidly deploying ML in a development context for vastly improved results. The way that good models are built relies on many ML steps, and when the results finally come together the result is superior to what could have been custom designed. Broad adoption of something like probabilistic programming is coming soon.</div><br/></div></div></div></div><div id="39111665" class="c"><input type="checkbox" id="c-39111665" checked=""/><div class="controls bullet"><span class="by">Aunche</span><span>|</span><a href="#39109584">prev</a><span>|</span><a href="#39111168">next</a><span>|</span><label class="collapse" for="c-39111665">[-]</label><label class="expand" for="c-39111665">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think machine learning is particularly hard. It just involves a lot of brute force work and most of the time, you get a middling result that isn&#x27;t particularly exciting. One of my friends had no CS background whatsoever but managed to make a basic Runescape mining bot by manually labeling hundreds of the correct colored rocks. His account got banned after a couple days though.</div><br/></div></div><div id="39111168" class="c"><input type="checkbox" id="c-39111168" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#39111665">prev</a><span>|</span><a href="#39114663">next</a><span>|</span><label class="collapse" for="c-39111168">[-]</label><label class="expand" for="c-39111168">[1 more]</label></div><br/><div class="children"><div class="content">With regard to model selection, one thing I learned a long time ago that provides powerful intuitive guidance on which model to use is the question: &quot;How could this be compressed further?&quot;<p>There are some deep connections between data compression and generalized learning, both at the statistical level and even lower at the algorithmic level (see Solomonoff induction).<p>For a specific example at the statistical level, suppose you fit a linear trendline to some data points using OLS. Now compute the standard deviation of the residual terms for each data point, and using the CDF of the normal distribution, for each residual, map its value into the interval [0, 1]. Sum together the logarithms of the trendline coefficients, the standard deviation, and the normalized residuals. This value is approximately proportional to &quot;sizeof(data | model) + sizeof(model)&quot;. It represents how well you compressed the data using the OLS model.<p>But now suppose you plot the distribution of the residuals and find out that they do not in fact resemble a Gaussian distribution. This is a problem because our model assumed the error terms were distributed normally, and since this is not the case, our compression is suboptimal.<p>So you back out some function f that closely maps between the uniform distribution on [0, 1] and the distribution that the residuals form and use this f to define a new model: yᵢ = m*xᵢ + b + εᵢ, with εᵢ distributed according to f(x;Θ), Θ being a parameter vector. When you sum the logarithms again, you will find that the new total is smaller than the original total obtained using OLS. The new trend line coefficients will slightly mess up the residual distribution again, so iterate on this process until you&#x27;ve converged on stable values for m, b, and Θ.<p>At the algorithmic level, the recommendation to use compression as a model selection guide applies even to LLMs, but it&#x27;s a bit harder to use in practice because &quot;sizeof(data | model) + sizeof(model)&quot; isn&#x27;t the entire story here. Suppose you had a &quot;perfect&quot; language model. In this case, you would achieve minimization of K(data | training data), where K is Kolmogorov complexity. In practice, what is being minimized with each new LLM version is &quot;sizeof(data | LLM model) + sizeof(training data | LLM model) + sizeof(LLM model)&quot;. You can assume &quot;sizeof(LLM model)&quot; is the smallest Turing machine equivalent to the LLM program.</div><br/></div></div><div id="39114663" class="c"><input type="checkbox" id="c-39114663" checked=""/><div class="controls bullet"><span class="by">nnevatie</span><span>|</span><a href="#39111168">prev</a><span>|</span><a href="#39112797">next</a><span>|</span><label class="collapse" for="c-39114663">[-]</label><label class="expand" for="c-39114663">[1 more]</label></div><br/><div class="children"><div class="content">I found the explanation quite convoluted (pun intended). Any system with billions of parameters is bound to be difficult to reason about.</div><br/></div></div><div id="39112797" class="c"><input type="checkbox" id="c-39112797" checked=""/><div class="controls bullet"><span class="by">epgui</span><span>|</span><a href="#39114663">prev</a><span>|</span><a href="#39109978">next</a><span>|</span><label class="collapse" for="c-39112797">[-]</label><label class="expand" for="c-39112797">[1 more]</label></div><br/><div class="children"><div class="content">I find it difficult to buy the argument that “it’s not difficult because of the math”, at least in the way the author meant.<p>I do however literally agree with the author: ML is not difficult because of the math. It’s difficult because for some reason people think the math is not important. But ML <i>is</i> math.</div><br/></div></div><div id="39109978" class="c"><input type="checkbox" id="c-39109978" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#39112797">prev</a><span>|</span><a href="#39114404">next</a><span>|</span><label class="collapse" for="c-39109978">[-]</label><label class="expand" for="c-39109978">[14 more]</label></div><br/><div class="children"><div class="content">Discussed at the time:<p><i>Why is machine learning ‘hard’?</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12936891">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12936891</a> - Nov 2016 (88 comments)</div><br/><div id="39110387" class="c"><input type="checkbox" id="c-39110387" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39109978">parent</a><span>|</span><a href="#39111599">next</a><span>|</span><label class="collapse" for="c-39110387">[-]</label><label class="expand" for="c-39110387">[12 more]</label></div><br/><div class="children"><div class="content">Love that thread.  The top comment is excellent:<p>&gt; Like picking hyperparamters - time and time again I&#x27;ve asked experts&#x2F;trainers&#x2F;colleagues: &quot;How do I know what type of model to use? How many layers? How many nodes per layer? Dropout or not?&quot; etc etc And the answer is always along the lines of &quot;just try a load of stuff and pick the one that works best&quot;.<p>&gt; To me, that feels weird and worrying. It&#x27;s like we don&#x27;t yet understand ML properly yet to definitively say, for a given data set, what sort of model we&#x27;ll need.<p>This embodies the very fundamental difference between science and engineering. With science, you make a discovery, but rarely do we ask &quot;what was the magical combination that let me find the needle in the haystack today?&quot; We instead just pass on the needle and show everyone we found it.<p>Should we work on finding out the magic behind hyperparameters?  In bioinformatics, the brilliant mathematician Lior Pachter once attacked the problem of sequence alignment using the tools of tropical algebra: what parameters to the alignment algorithms resulted in which regimes of solutions? It was beautiful.  It was great to understand. But I&#x27;m not sure if it even ever got published (though it likely did).  Having reasonable parameters is more important than understanding how to pick them from first principles, because even if you know all the possible output regimes for different segments of the hyper parameter space, really the only thing we care about is getting a functionally trained model at the end.<p>Sometimes deeper understanding provides deeper insights to the problems at hand.  But often, they don&#x27;t, even when the deeper understanding is beautiful. If the hammer works when you hold it a certain way, that&#x27;s great, but understanding all possible ways to hold a hammer doesn&#x27;t always help get the nail in better.</div><br/><div id="39111357" class="c"><input type="checkbox" id="c-39111357" checked=""/><div class="controls bullet"><span class="by">logtempo</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39110387">parent</a><span>|</span><a href="#39110897">next</a><span>|</span><label class="collapse" for="c-39111357">[-]</label><label class="expand" for="c-39111357">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Sometimes deeper understanding provides deeper insights to the problems at hand. But often, they don&#x27;t, even when the deeper understanding is beautiful. If the hammer works when you hold it a certain way, that&#x27;s great, but understanding all possible ways to hold a hammer doesn&#x27;t always help get the nail in better.<p>Is it true? I mean, in mathematics having a proof of something is way stronger than having a conjecture. And in engineering, proving that your solution is optimal is way stronger than saying &quot;hey look, I tried many things and finally it works!&quot;.<p>Worse, in statistics if you throw a bunch of tests and pick the one that &quot;works&quot; you might have false conclusions all the time. And AI is statistics.<p>Sure it works to test out 10 datatset and whatever number of different machine learning, but it takes time and money and might be suboptimal from an engineering POV.</div><br/></div></div><div id="39110897" class="c"><input type="checkbox" id="c-39110897" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39110387">parent</a><span>|</span><a href="#39111357">prev</a><span>|</span><a href="#39111188">next</a><span>|</span><label class="collapse" for="c-39110897">[-]</label><label class="expand" for="c-39110897">[3 more]</label></div><br/><div class="children"><div class="content">I do a lot of model tuning and I’m almost ashamed to say I tell GPT what performance I’m aiming for and have it generate the hyper parameters (as in just literally give me a code block). Then I see what works, tell GPT, and try again.<p>I’m deeply uncomfortable with such a method…but my models perform quite well. Note I spend a TON of time generating the right training data, so it’s not random.</div><br/><div id="39111247" class="c"><input type="checkbox" id="c-39111247" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39110897">parent</a><span>|</span><a href="#39111188">next</a><span>|</span><label class="collapse" for="c-39111247">[-]</label><label class="expand" for="c-39111247">[2 more]</label></div><br/><div class="children"><div class="content">1&#x2F;8th (soon to be 1&#x2F;2) of the working world:<p>&quot;I do a lot of X and I&#x27;m almost ashamed to say I tell GPT Y then I see if it works and try again&quot;.</div><br/><div id="39114384" class="c"><input type="checkbox" id="c-39114384" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39111247">parent</a><span>|</span><a href="#39111188">next</a><span>|</span><label class="collapse" for="c-39114384">[-]</label><label class="expand" for="c-39114384">[1 more]</label></div><br/><div class="children"><div class="content">Well, I do want to know more about how it works. Anything important I will teach myself, it’s just hard to justify the time investment during work hours when the robot does it.  Which I think is also important: these tools save time, but with downsides.</div><br/></div></div></div></div></div></div><div id="39111188" class="c"><input type="checkbox" id="c-39111188" checked=""/><div class="controls bullet"><span class="by">ummonk</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39110387">parent</a><span>|</span><a href="#39110897">prev</a><span>|</span><a href="#39111427">next</a><span>|</span><label class="collapse" for="c-39111188">[-]</label><label class="expand" for="c-39111188">[4 more]</label></div><br/><div class="children"><div class="content">&gt; This embodies the very fundamental difference between science and engineering.<p>Not really though. In engineering, you have heuristics, even if you don&#x27;t know why they work. In the case of deep learning &#x2F; AI, there seems to be very little in the way of built up heuristic knowledge - it&#x27;s just &quot;try stuff and see what works for every problem&quot;.</div><br/><div id="39112527" class="c"><input type="checkbox" id="c-39112527" checked=""/><div class="controls bullet"><span class="by">robotresearcher</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39111188">parent</a><span>|</span><a href="#39111526">next</a><span>|</span><label class="collapse" for="c-39112527">[-]</label><label class="expand" for="c-39112527">[1 more]</label></div><br/><div class="children"><div class="content">All the model topologies that have names are heuristics. The idea of a &#x27;layer&#x27; is a heuristic. And so on.<p>You don&#x27;t really just try stuff. You choose very few things to try from the space of models. And you choose craftily.<p>We have quite a lot of domain craftiness now, if you think about it that way.</div><br/></div></div><div id="39111526" class="c"><input type="checkbox" id="c-39111526" checked=""/><div class="controls bullet"><span class="by">liuxiansheng</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39111188">parent</a><span>|</span><a href="#39112527">prev</a><span>|</span><a href="#39111251">next</a><span>|</span><label class="collapse" for="c-39111526">[-]</label><label class="expand" for="c-39111526">[1 more]</label></div><br/><div class="children"><div class="content">I think if this was truly the case then wouldn&#x27;t ML algorithm development be a solved problem with AutoML? I don&#x27;t think AutoML is close to ubiquitous which means there must still be value in heuristics and a deeper understanding of our tools.</div><br/></div></div><div id="39111251" class="c"><input type="checkbox" id="c-39111251" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39111188">parent</a><span>|</span><a href="#39111526">prev</a><span>|</span><a href="#39111427">next</a><span>|</span><label class="collapse" for="c-39111251">[-]</label><label class="expand" for="c-39111251">[1 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s also the difference between science and engineering: has the tool&#x2F;technology been around enough to learn heuristics, or is everything still in the &quot;fuck around and find out&quot; phase?</div><br/></div></div></div></div><div id="39111427" class="c"><input type="checkbox" id="c-39111427" checked=""/><div class="controls bullet"><span class="by">logiduck</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39110387">parent</a><span>|</span><a href="#39111188">prev</a><span>|</span><a href="#39110734">next</a><span>|</span><label class="collapse" for="c-39111427">[-]</label><label class="expand" for="c-39111427">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this makes it very difficult to apply ML and RL in non-simulated scenarios.<p>With simulated scenarios you can just replay and &quot;sweep&quot; across hyperparameters to find the best one.<p>In a realworld scenario with limited information, fine tuning hyperparameters is much harder as you quickly find yourself in local maxima.</div><br/></div></div><div id="39110734" class="c"><input type="checkbox" id="c-39110734" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39110387">parent</a><span>|</span><a href="#39111427">prev</a><span>|</span><a href="#39111599">next</a><span>|</span><label class="collapse" for="c-39110734">[-]</label><label class="expand" for="c-39110734">[2 more]</label></div><br/><div class="children"><div class="content">The hammer analogy doesn&#x27;t make much sense because for a hammer we can actually use our scientific knowledge to compute the best possible way to hold the tool, and we can make instruments that are better than hammers, like pneumatic hammers, pile drivers, etc.<p>With your argument, we would be stuck with the good old, but basic hammer for the rest of time.</div><br/><div id="39111146" class="c"><input type="checkbox" id="c-39111146" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39109978">root</a><span>|</span><a href="#39110734">parent</a><span>|</span><a href="#39111599">next</a><span>|</span><label class="collapse" for="c-39111146">[-]</label><label class="expand" for="c-39111146">[1 more]</label></div><br/><div class="children"><div class="content">That seems like a different analogy; making better hammers is a different thing than understanding why holding a hammer a certain way works well. We did eventually invent enough physics to understand why we hold hammers where we do, but we got really far just experimenting without first principles. And even if we use first principles, we are going to discover a lot more by actually using the modified hand-held hammer and testing it, than necessarily hitting it out of the park with great physical modeling of the hammer and the biomechanics of the human body.<p>And in any case, I&#x27;m not saying we shouldn&#x27;t search for deep understanding of what hyperparameters work on a first try,  I&#x27;m just saying there&#x27;s a good chance that even if the principles are fully discovered, it may be that calculating using those principles is more expensive than a bunch of experimentation and won&#x27;t matter in the end.<p>That&#x27;s the trick about science, it&#x27;s more about finding the right question to answer than how to find answers, and often times the best questions only become apparent afterwards.</div><br/></div></div></div></div></div></div><div id="39111599" class="c"><input type="checkbox" id="c-39111599" checked=""/><div class="controls bullet"><span class="by">naveen99</span><span>|</span><a href="#39109978">parent</a><span>|</span><a href="#39110387">prev</a><span>|</span><a href="#39114404">next</a><span>|</span><label class="collapse" for="c-39111599">[-]</label><label class="expand" for="c-39111599">[1 more]</label></div><br/><div class="children"><div class="content">Similar:<p>Machine learning is still too hard for software engineers - 
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30432987">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30432987</a> - 151 comments<p>Machine learning is easier than it looks - 
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=6770785">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=6770785</a> 167 comments</div><br/></div></div></div></div><div id="39114404" class="c"><input type="checkbox" id="c-39114404" checked=""/><div class="controls bullet"><span class="by">crotchfire</span><span>|</span><a href="#39109978">prev</a><span>|</span><a href="#39110848">next</a><span>|</span><label class="collapse" for="c-39114404">[-]</label><label class="expand" for="c-39114404">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, chip design has both of these problems to a <i>much</i> greater degree than machine learning:<p>- Exponentially Difficult Debugging<p>- Delayed Debugging Cycles</div><br/></div></div><div id="39110848" class="c"><input type="checkbox" id="c-39110848" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#39114404">prev</a><span>|</span><a href="#39112655">next</a><span>|</span><label class="collapse" for="c-39110848">[-]</label><label class="expand" for="c-39110848">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; It becomes essential to build an intuition for where something went wrong based on the signals available.<p>This has always been my approach. I learned programming way before I had access to debuggers and other methods to dig in, set breakpoints and step through code to see where it was going wrong. As a result, when I got in the real world I kind of looked down on people using those tools (mostly because I hate tools actually). But then I saw people get to the root of problems that I don&#x27;t think I ever could have solved, and I started to appreciate those tools and the detail you could get to. My preference is still to have a great understanding of how algorithms work, how the code is written, and what the problem is, and noodle out what and where things may be going wrong. I only switch to detailed monitoring of the insides when &quot;thinking about it&quot; fails. Maybe I should have gone into this ML stuff ;-)</div><br/></div></div><div id="39112655" class="c"><input type="checkbox" id="c-39112655" checked=""/><div class="controls bullet"><span class="by">jvans</span><span>|</span><a href="#39110848">prev</a><span>|</span><a href="#39111099">next</a><span>|</span><label class="collapse" for="c-39112655">[-]</label><label class="expand" for="c-39112655">[2 more]</label></div><br/><div class="children"><div class="content">Machine learning systems are hard because your system can be badly under performing but still doing objectively great. I&#x27;ve seen systems that produce amazing improvements in core business metrics and then years later some subtle bug in the labeling process is accidentally uncovered and fixing it boosts performance by an additional 20%.<p>A ton of time in ML is spent on minimizing the surface area for bugs because it can be difficult to even know they exist.</div><br/><div id="39112718" class="c"><input type="checkbox" id="c-39112718" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#39112655">parent</a><span>|</span><a href="#39111099">next</a><span>|</span><label class="collapse" for="c-39112718">[-]</label><label class="expand" for="c-39112718">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s hard because there&#x27;s rarely one single correct solution, design, answer, etc. It&#x27;s hard in the same way as any other open-ended research work is hard. It will never not be hard in that sense.</div><br/></div></div></div></div><div id="39111850" class="c"><input type="checkbox" id="c-39111850" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#39111099">prev</a><span>|</span><a href="#39110784">next</a><span>|</span><label class="collapse" for="c-39111850">[-]</label><label class="expand" for="c-39111850">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s harder because there are no unifying theories. The ones that exist are seriously lacking.</div><br/></div></div><div id="39110784" class="c"><input type="checkbox" id="c-39110784" checked=""/><div class="controls bullet"><span class="by">Cacti</span><span>|</span><a href="#39111850">prev</a><span>|</span><a href="#39111262">next</a><span>|</span><label class="collapse" for="c-39110784">[-]</label><label class="expand" for="c-39110784">[1 more]</label></div><br/><div class="children"><div class="content">there are uncountable sets all over the place, and in practical terms, the repl loop may have a week long training lag after you hit enter.<p>also, the data is almost always complete shit.<p>lol. there’s no mystery why it’s hard.</div><br/></div></div><div id="39111262" class="c"><input type="checkbox" id="c-39111262" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#39110784">prev</a><span>|</span><label class="collapse" for="c-39111262">[-]</label><label class="expand" for="c-39111262">[2 more]</label></div><br/><div class="children"><div class="content">This article annoyed me. Not because I think it overestimates any of the difficulties, but it condescendingly compares against &quot;standard software engineers&quot; as if their problems are so much easier and lack additional dimensions beyond algorithm and implementation. It doesn&#x27;t help that they call N^4 &quot;exponential&quot;.<p>Long debugging cycles are not new. In fact, the field started out that way, when computers were slow enough that even a single edit-compile-run cycle could take hours (or days and some political capital, if you go back far enough). Even today, long debugging cycles are far from being restricted to ML, especially for failures that are only seen intermittently and in production.<p>Performance mattering is not unique to ML. Obviously.<p>Data issues are not unique to ML. Anytime you need to run against a &quot;representative workload&quot;, you&#x27;ll bump into them. Heck, anytime you run against a large test suite you&#x27;ll run into issues with the quality of the comparison data, especially if the tests are long in the tooth. Furthermore, anything you&#x27;re doing statistics on in general is going to bump into data issues—unless doing statistics is automatically ML?<p>ML lacks various other dimensions of difficulty. Distributed systems. Web browsers that need to execute arbitrary code that hasn&#x27;t been written yet. Backwards compatibility. Forwards compatibility. In-production databases. Cross platform development. Power usage optimization. Concurrency (ML can have this, but it doesn&#x27;t always need to, and it can be in the &quot;embarrassingly parallel&quot; bucket). Deep dependency graphs.<p>The author thinks ML development is hard because that&#x27;s what they work on. They are but a grasshopper.</div><br/></div></div></div></div></div></div></div></body></html>