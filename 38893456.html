<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704618054741" as="style"/><link rel="stylesheet" href="styles.css?v=1704618054741"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html">Chess-GPT&#x27;s Internal World Model</a> <span class="domain">(<a href="https://adamkarvonen.github.io">adamkarvonen.github.io</a>)</span></div><div class="subtext"><span>homarp</span> | <span>24 comments</span></div><br/><div><div id="38893484" class="c"><input type="checkbox" id="c-38893484" checked=""/><div class="controls bullet"><span class="by">homarp</span><span>|</span><a href="#38898693">next</a><span>|</span><label class="collapse" for="c-38893484">[-]</label><label class="expand" for="c-38893484">[1 more]</label></div><br/><div class="children"><div class="content">code here: <a href="https:&#x2F;&#x2F;github.com&#x2F;adamkarvonen&#x2F;chess_llm_interpretability">https:&#x2F;&#x2F;github.com&#x2F;adamkarvonen&#x2F;chess_llm_interpretability</a></div><br/></div></div><div id="38898693" class="c"><input type="checkbox" id="c-38898693" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#38893484">prev</a><span>|</span><a href="#38898998">next</a><span>|</span><label class="collapse" for="c-38898693">[-]</label><label class="expand" for="c-38898693">[5 more]</label></div><br/><div class="children"><div class="content">If you take a neural network that already knows the basic rules of chess and train it on chess games, you produce a chess engine.<p>From the Wikipedia page on one of the strongest ever[1]: &quot;Like Leela Zero and AlphaGo Zero, Leela Chess Zero starts with no intrinsic chess-specific knowledge other than the basic rules of the game. Leela Chess Zero then learns how to play chess by reinforcement learning from repeated self-play&quot;<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Leela_Chess_Zero" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Leela_Chess_Zero</a></div><br/><div id="38898899" class="c"><input type="checkbox" id="c-38898899" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#38898693">parent</a><span>|</span><a href="#38898998">next</a><span>|</span><label class="collapse" for="c-38898899">[-]</label><label class="expand" for="c-38898899">[4 more]</label></div><br/><div class="children"><div class="content">As described in the OP&#x27;s blog post <a href="https:&#x2F;&#x2F;adamkarvonen.github.io&#x2F;machine_learning&#x2F;2024&#x2F;01&#x2F;03&#x2F;chess-world-models.html" rel="nofollow">https:&#x2F;&#x2F;adamkarvonen.github.io&#x2F;machine_learning&#x2F;2024&#x2F;01&#x2F;03&#x2F;c...</a> - one of the incredible things here is that the standard GPT architecture, trained from scratch from PGN strings alone, can intuit the rules of the game from those examples, without any notion of the rules of chess or even that it is playing a game.<p>Leela, by contrast, requires a specialized structure of iterative tree searching to generate move recommendations: <a href="https:&#x2F;&#x2F;lczero.org&#x2F;dev&#x2F;wiki&#x2F;technical-explanation-of-leela-chess-zero&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lczero.org&#x2F;dev&#x2F;wiki&#x2F;technical-explanation-of-leela-c...</a><p>Which is not to diminish the work of the Leela team at all! But I find it fascinating that an unmodified GPT architecture can build up internal neural representations that correspond closely to board states, despite not having been designed for that task. As they say, attention may indeed be all you need.</div><br/><div id="38899677" class="c"><input type="checkbox" id="c-38899677" checked=""/><div class="controls bullet"><span class="by">bananapub</span><span>|</span><a href="#38898693">root</a><span>|</span><a href="#38898899">parent</a><span>|</span><a href="#38899235">next</a><span>|</span><label class="collapse" for="c-38899677">[-]</label><label class="expand" for="c-38899677">[1 more]</label></div><br/><div class="children"><div class="content">&gt; can intuit the rules of the game from those examples,<p>I am pretty sure a bunch of matrix multiplications can&#x27;t intuit anything.<p>naively, it doesn&#x27;t seem very surprising that enormous amounts of self play cause the internal structure to reflect the inputs and outputs?</div><br/></div></div><div id="38899235" class="c"><input type="checkbox" id="c-38899235" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#38898693">root</a><span>|</span><a href="#38898899">parent</a><span>|</span><a href="#38899677">prev</a><span>|</span><a href="#38898998">next</a><span>|</span><label class="collapse" for="c-38899235">[-]</label><label class="expand" for="c-38899235">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the strength of play for the GPT architecture? It&#x27;s impressive that it figures out the rules, but does it play strong chess?<p>&gt;&gt; As they say, attention may indeed be all you need.<p>I don&#x27;t think drawing general conclusions about intelligence from a board game is warranted. We didn&#x27;t evolve to play chess or Go.</div><br/><div id="38899428" class="c"><input type="checkbox" id="c-38899428" checked=""/><div class="controls bullet"><span class="by">jksk61</span><span>|</span><a href="#38898693">root</a><span>|</span><a href="#38899235">parent</a><span>|</span><a href="#38898998">next</a><span>|</span><label class="collapse" for="c-38899428">[-]</label><label class="expand" for="c-38899428">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What&#x27;s the strength of play for the GPT architecture? It&#x27;s impressive that it figures out the rules, but does it play strong chess?<p>sometimes it is not a matter of &quot;is it better? is it larger? is it more efficient?&quot;, but just a question.<p>mountains are mountains, men are men.</div><br/></div></div></div></div></div></div></div></div><div id="38898998" class="c"><input type="checkbox" id="c-38898998" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#38898693">prev</a><span>|</span><a href="#38897253">next</a><span>|</span><label class="collapse" for="c-38898998">[-]</label><label class="expand" for="c-38898998">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be curious to see if, in the 1-2% of cases where the linear probe fails to predict board occupancy, the LLM also predicts (or at least assigns non-trivial probability to) a corresponding illegal move. For example, if the linear probe incorrectly thinks there&#x27;s a bishop on b4, does the LLM give more probability to illegal bishop moves along the corresponding diagonals than to other illegal bishop moves?</div><br/></div></div><div id="38897253" class="c"><input type="checkbox" id="c-38897253" checked=""/><div class="controls bullet"><span class="by">ericbarrett</span><span>|</span><a href="#38898998">prev</a><span>|</span><a href="#38898820">next</a><span>|</span><label class="collapse" for="c-38897253">[-]</label><label class="expand" for="c-38897253">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for linking the actual post—it was a great read. I&#x27;m not an ML expert, but the author really made it easy to follow their experiment&#x27;s method and results.</div><br/></div></div><div id="38898820" class="c"><input type="checkbox" id="c-38898820" checked=""/><div class="controls bullet"><span class="by">MenhirMike</span><span>|</span><a href="#38897253">prev</a><span>|</span><a href="#38897637">next</a><span>|</span><label class="collapse" for="c-38898820">[-]</label><label class="expand" for="c-38898820">[1 more]</label></div><br/><div class="children"><div class="content">I hope it performs better than ChatGPT: <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;AnarchyChess&#x2F;comments&#x2F;10ydnbb&#x2F;i_placed_stockfish_white_against_chatgpt_black&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;AnarchyChess&#x2F;comments&#x2F;10ydnbb&#x2F;i_pla...</a><p>Though I will give it to ChatGPT, castling across the bishop was a genius move.</div><br/></div></div><div id="38897637" class="c"><input type="checkbox" id="c-38897637" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38898820">prev</a><span>|</span><a href="#38897720">next</a><span>|</span><label class="collapse" for="c-38897637">[-]</label><label class="expand" for="c-38897637">[5 more]</label></div><br/><div class="children"><div class="content">Is a linear probe part of observability&#x2F;interpretability?</div><br/><div id="38897658" class="c"><input type="checkbox" id="c-38897658" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#38897637">parent</a><span>|</span><a href="#38897720">next</a><span>|</span><label class="collapse" for="c-38897658">[-]</label><label class="expand" for="c-38897658">[4 more]</label></div><br/><div class="children"><div class="content">Yes, a pretty fundamental technique and one of the earliest. It lets you determine which layers contain what information among other things.</div><br/><div id="38897776" class="c"><input type="checkbox" id="c-38897776" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#38897637">root</a><span>|</span><a href="#38897658">parent</a><span>|</span><a href="#38897720">next</a><span>|</span><label class="collapse" for="c-38897776">[-]</label><label class="expand" for="c-38897776">[3 more]</label></div><br/><div class="children"><div class="content">The downside is that it&#x27;s a supervised technique, so you need to already know what you&#x27;re looking for. It would be nice to have an unsupervised tool that could list out all the things the network has learned.</div><br/><div id="38898201" class="c"><input type="checkbox" id="c-38898201" checked=""/><div class="controls bullet"><span class="by">JoshuaDavid</span><span>|</span><a href="#38897637">root</a><span>|</span><a href="#38897776">parent</a><span>|</span><a href="#38897720">next</a><span>|</span><label class="collapse" for="c-38898201">[-]</label><label class="expand" for="c-38898201">[2 more]</label></div><br/><div class="children"><div class="content">Anthropic has published some cool stuff in that direction: <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2023&#x2F;monosemantic-features" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2023&#x2F;monosemantic-features</a></div><br/><div id="38898649" class="c"><input type="checkbox" id="c-38898649" checked=""/><div class="controls bullet"><span class="by">firejake308</span><span>|</span><a href="#38897637">root</a><span>|</span><a href="#38898201">parent</a><span>|</span><a href="#38897720">next</a><span>|</span><label class="collapse" for="c-38898649">[-]</label><label class="expand" for="c-38898649">[1 more]</label></div><br/><div class="children"><div class="content">Whoa, this is super cool! I can imagine if we had something like this for ChatGPT, we could use it to do some serious prompt engineering. Imagine seeing what specific neurons you were activating with your prompt, and being able to identify which word in your prompt was triggering an undesired behavior. Super cool stuff, excited to see if it scales</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38897720" class="c"><input type="checkbox" id="c-38897720" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#38897637">prev</a><span>|</span><a href="#38897242">next</a><span>|</span><label class="collapse" for="c-38897720">[-]</label><label class="expand" for="c-38897720">[2 more]</label></div><br/><div class="children"><div class="content">I mean, this seems obvious to me. How would the model predict the next move WITHOUT calculating the board state first? Yes, by memorization, but memorization hypothesis is easily rejected by comparison to training dataset in this case.<p>It is possible the model calculates an approximate board state, which is different from the board state but equivalent for most games, but not all games. It would be interesting to train adversarial policy to check this. From KataGo attack we know this does happen for Go AIs: Go rules have a concept of liberty, but so called pseudoliberty is easier to calculate and equivalent for most cases (but not all cases). In fact, human programmers also used pseudoliberty to optimize their engines. Adversarial attack found Go AIs also use pseudoliberty.</div><br/><div id="38899503" class="c"><input type="checkbox" id="c-38899503" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38897720">parent</a><span>|</span><a href="#38897242">next</a><span>|</span><label class="collapse" for="c-38899503">[-]</label><label class="expand" for="c-38899503">[1 more]</label></div><br/><div class="children"><div class="content">It’s one thing to think it’s obvious, but quite another to prove it. I think this is the true value of this kind of work, is that it’s helping to decipher what these models are actually doing. Far too often we hear “NNs &#x2F; LLMs are black boxes” as if that’s the end of the conversation.</div><br/></div></div></div></div><div id="38897242" class="c"><input type="checkbox" id="c-38897242" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#38897720">prev</a><span>|</span><label class="collapse" for="c-38897242">[-]</label><label class="expand" for="c-38897242">[7 more]</label></div><br/><div class="children"><div class="content">World model might be a too big word here. When we talk of a world model (in the context of AI motels), we refer to its understanding of the world, at least in the context we trained it. But what I see is just a visualization of the output in a fashion similar to a chess board. A stronger evidence would be a for example a map of the next move, which will show whether it truly understood the game’s rules. If it show probability larger than zero on illegal board fields, it will show us why it sometimes makes illegal moves. And obviously, it didn’t fully understand the rules of the game.</div><br/><div id="38897695" class="c"><input type="checkbox" id="c-38897695" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#38897242">parent</a><span>|</span><a href="#38897380">next</a><span>|</span><label class="collapse" for="c-38897695">[-]</label><label class="expand" for="c-38897695">[5 more]</label></div><br/><div class="children"><div class="content">&gt; probability larger than zero<p>Strictly speaking, it should be a mistake to assign a probability equal to zero to any moves, even for illegal board moves, but especially for an AI that learns by example and self-play. It never gets taught the rules, it only gets shown the games -- there&#x27;s no reason that it should conclude that the probability of a rook moving diagonally is <i>exactly zero</i> just because it&#x27;s never seen it happen in the data, and gets penalized in training every time it tries it.<p>But even for a human, assigning probability of <i>exactly zero</i> is too strong. It would forbid any possibility that you misunderstand any rules, or forgot any special cases. It&#x27;s a good idea to always maintain at least a small amount of epistemic humility that you might be mistaken about the rules, so that sufficiently overwhelmingly strong evidence could convince you that a move you thought was illegal turns out to be legal.</div><br/><div id="38898028" class="c"><input type="checkbox" id="c-38898028" checked=""/><div class="controls bullet"><span class="by">redcobra762</span><span>|</span><a href="#38897242">root</a><span>|</span><a href="#38897695">parent</a><span>|</span><a href="#38899258">next</a><span>|</span><label class="collapse" for="c-38898028">[-]</label><label class="expand" for="c-38898028">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s got to be a probability cut-off, though.  LLMs don&#x27;t infinitely connect every token with every other token, some aren&#x27;t connected at all, even if some association is taught, right?</div><br/><div id="38898149" class="c"><input type="checkbox" id="c-38898149" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#38897242">root</a><span>|</span><a href="#38898028">parent</a><span>|</span><a href="#38899258">next</a><span>|</span><label class="collapse" for="c-38898149">[-]</label><label class="expand" for="c-38898149">[2 more]</label></div><br/><div class="children"><div class="content">The weights have finite precision which means they represent value-ranges &#x2F; have error bars. So even if the weight is exactly 0 it does not represent complete confidence in it never occurring.</div><br/><div id="38898993" class="c"><input type="checkbox" id="c-38898993" checked=""/><div class="controls bullet"><span class="by">redcobra762</span><span>|</span><a href="#38897242">root</a><span>|</span><a href="#38898149">parent</a><span>|</span><a href="#38899258">next</a><span>|</span><label class="collapse" for="c-38898993">[-]</label><label class="expand" for="c-38898993">[1 more]</label></div><br/><div class="children"><div class="content">A weight necessitates a relationship, but I’m arguing LLMs don’t create all relationships.  So a connection wouldn’t even exist.</div><br/></div></div></div></div></div></div><div id="38899258" class="c"><input type="checkbox" id="c-38899258" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#38897242">root</a><span>|</span><a href="#38897695">parent</a><span>|</span><a href="#38898028">prev</a><span>|</span><a href="#38897380">next</a><span>|</span><label class="collapse" for="c-38899258">[-]</label><label class="expand" for="c-38899258">[1 more]</label></div><br/><div class="children"><div class="content">The rules of chess are small and well known. For example, rooks can&#x27;t go diagonal no matter the situation. There&#x27;s no need for epistemic humility.</div><br/></div></div></div></div><div id="38897380" class="c"><input type="checkbox" id="c-38897380" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#38897242">parent</a><span>|</span><a href="#38897695">prev</a><span>|</span><label class="collapse" for="c-38897380">[-]</label><label class="expand" for="c-38897380">[1 more]</label></div><br/><div class="children"><div class="content">No, it is not a visualization of the output, it is a visualization of the information about pawn position contained in the model’s internal state.</div><br/></div></div></div></div></div></div></div></div></div></body></html>