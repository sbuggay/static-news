<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692003665494" as="style"/><link rel="stylesheet" href="styles.css?v=1692003665494"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/microsoft/azurechatgpt">Azure ChatGPT: Private and secure ChatGPT for internal enterprise use</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>taubek</span> | <span>206 comments</span></div><br/><div><div id="37115404" class="c"><input type="checkbox" id="c-37115404" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37113804">next</a><span>|</span><label class="collapse" for="c-37115404">[-]</label><label class="expand" for="c-37115404">[37 more]</label></div><br/><div class="children"><div class="content">This appears to be a web frontend with authentication for Azure&#x27;s OpenAI API, which is a great choice if you can&#x27;t use Chat GPT or its API at work.<p>If you&#x27;re looking to try the &quot;open&quot; models like Llama 2 (or it&#x27;s uncensored version Llama 2 Uncensored), check out <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a> or some of the lower level runners like llama.cpp (which powers the aforementioned project I&#x27;m working on) or Candle, the new project by hugging face.<p>What&#x27;s are folks&#x27; take on this vs Llama 2, which was recently released by Facebook Research? While I haven&#x27;t tested it extensively, 70B model is supposed to rival Chat GPT 3.5 in most areas, and there are now some new fine-tuned versions that excel at specific tasks like coding (the &#x27;codeup&#x27; model) or the new Wizard Math (<a href="https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM">https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM</a>) which claims to outperform ChatGPT 3.5 on grade school math problems.</div><br/><div id="37115563" class="c"><input type="checkbox" id="c-37115563" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37115404">parent</a><span>|</span><a href="#37115975">next</a><span>|</span><label class="collapse" for="c-37115563">[-]</label><label class="expand" for="c-37115563">[27 more]</label></div><br/><div class="children"><div class="content">Llama 2 might by some measures be close to GPT 3.5, but it’s nowhere near GPT 4, nor Anthropic Claude 2 or Cohere’s model. The closed source players have the best researchers - they are being paid millions a year with tons of upside - and it’s hard to keep pace with that. My sense is that the foundation model companies have an edge for now and will probably stay a few steps ahead of the open source realm simply for economic reasons.<p>Over the long run, open source will eventually overtake. Chances are this will happen once the researchers who are making magic happen get their liquidity and can start working for free again out in the open.</div><br/><div id="37117108" class="c"><input type="checkbox" id="c-37117108" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115563">parent</a><span>|</span><a href="#37116686">next</a><span>|</span><label class="collapse" for="c-37117108">[-]</label><label class="expand" for="c-37117108">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The closed source players have the best researchers - they are being paid millions a year with tons of upside - and it’s hard to keep pace with that.<p>Llama2 came out of Meta&#x27;s AI group. Meta pays researcher salaries competitive with any other group, and their NLP team is one of the top groups in the world.<p>For researchers it is increasingly the most attractive industrial lab because they release the research openly.</div><br/></div></div><div id="37116686" class="c"><input type="checkbox" id="c-37116686" checked=""/><div class="controls bullet"><span class="by">xcdzvyn</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115563">parent</a><span>|</span><a href="#37117108">prev</a><span>|</span><a href="#37115635">next</a><span>|</span><label class="collapse" for="c-37116686">[-]</label><label class="expand" for="c-37116686">[11 more]</label></div><br/><div class="children"><div class="content">&gt; The closed source players have the best researchers<p>Is that definitely why? GPT 3.5 and GPT 4 are far larger than 70B, right? So if a 70B, local model like LLaMA can even remotely rival them, would that not suggest that LLaMA is fundamentally a better model?<p>For example, would a LLaMA model with even half of GPT 4&#x27;s parameters be projected to outperform it? Is that how it works?<p>[I&#x27;m not super familiar with LLM tech]</div><br/><div id="37116843" class="c"><input type="checkbox" id="c-37116843" checked=""/><div class="controls bullet"><span class="by">hooande</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37116686">parent</a><span>|</span><a href="#37117144">next</a><span>|</span><label class="collapse" for="c-37116843">[-]</label><label class="expand" for="c-37116843">[1 more]</label></div><br/><div class="children"><div class="content">There is no clear answer. It&#x27;s debatable among experts.<p>The grandparent post seems to believe that the issue is algorithmic complexity and programming aptitude. Personally, I think that all the major LLMs are using the same basic transformer architecture with relatively minor differences in code.<p>GPT is trained on more data with more parameters than any open source model. The size does matter, far more than the software does. In my experience with data science, the best programmers in the world can only do so much if they are operating with 1&#x2F;10th the scale of data. That applies to any problem.</div><br/></div></div><div id="37117144" class="c"><input type="checkbox" id="c-37117144" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37116686">parent</a><span>|</span><a href="#37116843">prev</a><span>|</span><a href="#37116999">next</a><span>|</span><label class="collapse" for="c-37117144">[-]</label><label class="expand" for="c-37117144">[3 more]</label></div><br/><div class="children"><div class="content">If you read the Llama2 paper it is very clear that small amounts of data (thousands of records) make vast difference at the instruction turning stage. From the Llama2 paper:<p>&gt; Quality Is All You Need.<p>&gt; Third-party SFT data is available from many different sources, but we found that
many of these have insufficient diversity and quality — in particular for aligning LLMs towards dialogue-style
instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data,
as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but
higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These
findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of
27,540 annotations. Note that we do not include any Meta user data.<p>It&#x27;s likely OpenAI has invested in this and has good coverage in a larger range of domains. That alone probably explains a large amount of the gap.</div><br/><div id="37117791" class="c"><input type="checkbox" id="c-37117791" checked=""/><div class="controls bullet"><span class="by">Maxion</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117144">parent</a><span>|</span><a href="#37116999">next</a><span>|</span><label class="collapse" for="c-37117791">[-]</label><label class="expand" for="c-37117791">[2 more]</label></div><br/><div class="children"><div class="content">This quote is quite funny taken out of context like this. Top AI researchers find that garbage in === garbage out.</div><br/><div id="37118544" class="c"><input type="checkbox" id="c-37118544" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117791">parent</a><span>|</span><a href="#37116999">next</a><span>|</span><label class="collapse" for="c-37118544">[-]</label><label class="expand" for="c-37118544">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m puzzled. Why do you think it&#x27;s taken out of context?</div><br/></div></div></div></div></div></div><div id="37116999" class="c"><input type="checkbox" id="c-37116999" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37116686">parent</a><span>|</span><a href="#37117144">prev</a><span>|</span><a href="#37115635">next</a><span>|</span><label class="collapse" for="c-37116999">[-]</label><label class="expand" for="c-37116999">[6 more]</label></div><br/><div class="children"><div class="content">LLamA 2 at 70B is, let’s say pessimistically 70% as good as GPT3.5. This makes me think that OpenAI is lying about their parameter count, are vastly less efficient than LLaMA, or, the lager model sizes have diminishing returns. Either way, your point is a good one. Something doesn’t add up.</div><br/><div id="37118555" class="c"><input type="checkbox" id="c-37118555" checked=""/><div class="controls bullet"><span class="by">mattlutze</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37116999">parent</a><span>|</span><a href="#37117888">next</a><span>|</span><label class="collapse" for="c-37118555">[-]</label><label class="expand" for="c-37118555">[1 more]</label></div><br/><div class="children"><div class="content">We just don&#x27;t have the information to make judgements, much less leaping to &quot;they must be lying.&quot;<p>There&#x27;s a few public numbers from a handful of foundation models as to performance vs parameter count vs architecture generation. Not being able to compare in detail the architecture of the various closed models nor being more rigorous on training with progressively sized parameter sets, the conclusion at the moment is a general feeling or conjecture.</div><br/></div></div><div id="37117888" class="c"><input type="checkbox" id="c-37117888" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37116999">parent</a><span>|</span><a href="#37118555">prev</a><span>|</span><a href="#37115635">next</a><span>|</span><label class="collapse" for="c-37117888">[-]</label><label class="expand" for="c-37117888">[4 more]</label></div><br/><div class="children"><div class="content">IMO Llama2 really isn’t close to 3.5. It still has regular mode collapse (or whatever you call getting repetitive and nonsensical responses after a while), it has very poor mathematical&#x2F;logical reasoning and is not good at following multi-part instructions.<p>It just sounds like 3.5&#x2F;4 because it was trained on it.</div><br/><div id="37118552" class="c"><input type="checkbox" id="c-37118552" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117888">parent</a><span>|</span><a href="#37118252">next</a><span>|</span><label class="collapse" for="c-37118552">[-]</label><label class="expand" for="c-37118552">[1 more]</label></div><br/><div class="children"><div class="content">Llama 2 wasn&#x27;t trained on ChatGPT&#x2F;GPT4. I think maybe you are thinking of the Vicuna models?<p><a href="https:&#x2F;&#x2F;lmsys.org&#x2F;blog&#x2F;2023-03-30-vicuna&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lmsys.org&#x2F;blog&#x2F;2023-03-30-vicuna&#x2F;</a></div><br/></div></div><div id="37118252" class="c"><input type="checkbox" id="c-37118252" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117888">parent</a><span>|</span><a href="#37118552">prev</a><span>|</span><a href="#37118550">next</a><span>|</span><label class="collapse" for="c-37118252">[-]</label><label class="expand" for="c-37118252">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re mixing up the <i>language model</i> with the <i>chat bot</i>.<p>The llama2 is a language model. I imagine the language model behind chatgpt is not much different (perhaps it&#x27;s better, but not by many months  AI research time). It likely also suffers from &quot;mode collapse&quot; issues etc.<p>But 3.5 also has a lot of systems around it that <i>detects</i> mode collapse and applies some kind of mitigation, forcing the model to give a more reasonable output. Mathematical &#x2F; logical reasoning questions are likely also detected hand passed on in some form to a separate system.</div><br/></div></div><div id="37118550" class="c"><input type="checkbox" id="c-37118550" checked=""/><div class="controls bullet"><span class="by">vorticalbox</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117888">parent</a><span>|</span><a href="#37118252">prev</a><span>|</span><a href="#37115635">next</a><span>|</span><label class="collapse" for="c-37118550">[-]</label><label class="expand" for="c-37118550">[1 more]</label></div><br/><div class="children"><div class="content">This is what presence_penalty and frequency_penalty are for.</div><br/></div></div></div></div></div></div></div></div><div id="37115635" class="c"><input type="checkbox" id="c-37115635" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115563">parent</a><span>|</span><a href="#37116686">prev</a><span>|</span><a href="#37117101">next</a><span>|</span><label class="collapse" for="c-37115635">[-]</label><label class="expand" for="c-37115635">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Llama 2 might by some measures be close to GPT 3.5, but it’s nowhere near GPT 4<p>I think you&#x27;re right about this, and benchmarks we&#x27;ve run at Anyscale support this conclusion [1].<p>The caveat there (which I think will be a big boon for open models) is that techniques like fine-tuning makes a HUGE difference and can bridge the quality gap between Llama-2 and GPT-4 for many (but not all) problems.<p>[1] <a href="https:&#x2F;&#x2F;www.anyscale.com&#x2F;blog&#x2F;fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.anyscale.com&#x2F;blog&#x2F;fine-tuning-llama-2-a-comprehe...</a></div><br/><div id="37117678" class="c"><input type="checkbox" id="c-37117678" checked=""/><div class="controls bullet"><span class="by">sytelus</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115635">parent</a><span>|</span><a href="#37115692">next</a><span>|</span><label class="collapse" for="c-37117678">[-]</label><label class="expand" for="c-37117678">[1 more]</label></div><br/><div class="children"><div class="content">Frankly, number of benchmarks you guys are using are too narrow. In fact these benchmarks are &quot;old world&quot; benchmarks, easy to game through finetuning and we should be stop using them altogether for LLMs. Why are you not using Big Bench Hard or OpenAI evals?</div><br/></div></div><div id="37115692" class="c"><input type="checkbox" id="c-37115692" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115635">parent</a><span>|</span><a href="#37117678">prev</a><span>|</span><a href="#37117101">next</a><span>|</span><label class="collapse" for="c-37115692">[-]</label><label class="expand" for="c-37115692">[3 more]</label></div><br/><div class="children"><div class="content">can I fine tune it on like 2,000 repos at a corporation (code based) and have it understand the architecture?</div><br/><div id="37116188" class="c"><input type="checkbox" id="c-37116188" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115692">parent</a><span>|</span><a href="#37117101">next</a><span>|</span><label class="collapse" for="c-37116188">[-]</label><label class="expand" for="c-37116188">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you can do that with <i>any</i> AI models. It almost feels like a fundamental misrepresentation of how they work.<p>You could fine-tune a conversational AI on your codebase, but without loading said codebase into it&#x27;s context it is &quot;flying blind&quot; so-to-speak. It doesn&#x27;t understand the data structure of your code, the relation between files and probably doesn&#x27;t confidently understand the architecture of your system. Without portions of your codebase loaded into the &#x27;memory&#x27; of your model, all that your finetuning can do is replicate characteristics of your code.</div><br/><div id="37116821" class="c"><input type="checkbox" id="c-37116821" checked=""/><div class="controls bullet"><span class="by">mycall</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37116188">parent</a><span>|</span><a href="#37117101">next</a><span>|</span><label class="collapse" for="c-37116821">[-]</label><label class="expand" for="c-37116821">[1 more]</label></div><br/><div class="children"><div class="content">TypeChat-like things might provide the interface control for future context driven architectures, being some type of catalysis.  Using the self-reflective modeling is a form of contextual insight.</div><br/></div></div></div></div></div></div></div></div><div id="37117101" class="c"><input type="checkbox" id="c-37117101" checked=""/><div class="controls bullet"><span class="by">llm_thr</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115563">parent</a><span>|</span><a href="#37115635">prev</a><span>|</span><a href="#37118427">next</a><span>|</span><label class="collapse" for="c-37117101">[-]</label><label class="expand" for="c-37117101">[3 more]</label></div><br/><div class="children"><div class="content">You seriously underestimate just how much _not_ having to tune your llm for SF sensibilities benefits performance.<p>As an example from the last six months: people on tor are producing better than state of the art stable diffusion because they want porn without limitations. I haven&#x27;t had the time to look at llm&#x27;s but the degenerates who enjoy that sort of thing have said they can get the Llama2 model to role play their dirty fantasies and then have stable diffusion illustrate said fantasies. It&#x27;s a brave new world and it&#x27;s not on the WWW.</div><br/><div id="37117974" class="c"><input type="checkbox" id="c-37117974" checked=""/><div class="controls bullet"><span class="by">saberience</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117101">parent</a><span>|</span><a href="#37118427">next</a><span>|</span><label class="collapse" for="c-37117974">[-]</label><label class="expand" for="c-37117974">[2 more]</label></div><br/><div class="children"><div class="content">What do you mean by &quot;tune for SF&quot; ?</div><br/><div id="37118150" class="c"><input type="checkbox" id="c-37118150" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117974">parent</a><span>|</span><a href="#37118427">next</a><span>|</span><label class="collapse" for="c-37118150">[-]</label><label class="expand" for="c-37118150">[1 more]</label></div><br/><div class="children"><div class="content">San Francisco sensibilities. A model trained on a large data set will have the capacity to emit all kinds of controversial opinions and distasteful rants (and pornography). Then they effectively lobotomize it with a rusty hatchet in an attempt to censor it from doing that, which impairs the output quality in general.</div><br/></div></div></div></div></div></div><div id="37118427" class="c"><input type="checkbox" id="c-37118427" checked=""/><div class="controls bullet"><span class="by">rightbyte</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115563">parent</a><span>|</span><a href="#37117101">prev</a><span>|</span><a href="#37118177">next</a><span>|</span><label class="collapse" for="c-37118427">[-]</label><label class="expand" for="c-37118427">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think paying more will give you better researchers. Maybe better &quot;players&quot;.</div><br/></div></div><div id="37118177" class="c"><input type="checkbox" id="c-37118177" checked=""/><div class="controls bullet"><span class="by">ReptileMan</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115563">parent</a><span>|</span><a href="#37118427">prev</a><span>|</span><a href="#37115751">next</a><span>|</span><label class="collapse" for="c-37118177">[-]</label><label class="expand" for="c-37118177">[1 more]</label></div><br/><div class="children"><div class="content">Linux started in the same position. Sometimes the underdogs win.</div><br/></div></div><div id="37115751" class="c"><input type="checkbox" id="c-37115751" checked=""/><div class="controls bullet"><span class="by">jgalt212</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115563">parent</a><span>|</span><a href="#37118177">prev</a><span>|</span><a href="#37115975">next</a><span>|</span><label class="collapse" for="c-37115751">[-]</label><label class="expand" for="c-37115751">[4 more]</label></div><br/><div class="children"><div class="content">OK, fair enough.  Please give me an example of a customer facing chatbot that Llama 2 (and unbearable to use) and GPT 4 customer facing chatbot that is a joy to use.  I think at the end of the day, you still have customers dreading such interactions.</div><br/><div id="37116827" class="c"><input type="checkbox" id="c-37116827" checked=""/><div class="controls bullet"><span class="by">dandiep</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115751">parent</a><span>|</span><a href="#37116399">next</a><span>|</span><label class="collapse" for="c-37116827">[-]</label><label class="expand" for="c-37116827">[1 more]</label></div><br/><div class="children"><div class="content">Using GPT3.5&#x2F;4 in our language learning app and people seem to enjoy it. [1]<p>Tried Llama2 and it definitely doesn’t even come close for what we’re doing. Would absolutely need fine tuning.<p>Maybe customers don’t enjoy chat bots for customer support, but there are a million other uses for these models. I, for example, LOVE github copilot.<p>1. <a href="https:&#x2F;&#x2F;squidgies.app" rel="nofollow noreferrer">https:&#x2F;&#x2F;squidgies.app</a></div><br/></div></div><div id="37116399" class="c"><input type="checkbox" id="c-37116399" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115751">parent</a><span>|</span><a href="#37116827">prev</a><span>|</span><a href="#37115975">next</a><span>|</span><label class="collapse" for="c-37116399">[-]</label><label class="expand" for="c-37116399">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s early, and this definitely isn&#x27;t customer facing in the traditional sense, but a team member of mine set up a Discord bot running Llama 2 70B on a Mac studio and we&#x27;ve been quite impressed by its responses to folks who test it.<p>IIRC chat bots are central the vision Facebook has with LLMs (e.g. every instagram account has a personal chat bot), so I would expect the Llama models to get increasingly better at this task.<p>That said the 7B and 13B models definitely don&#x27;t quite seem ready yet for production customer interaction :-)</div><br/><div id="37116735" class="c"><input type="checkbox" id="c-37116735" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37116399">parent</a><span>|</span><a href="#37115975">next</a><span>|</span><label class="collapse" for="c-37116735">[-]</label><label class="expand" for="c-37116735">[1 more]</label></div><br/><div class="children"><div class="content">&gt; (e.g. every instagram account has a personal chat bot)<p>That made me think of the Black Mirror episode Joan is Awful, where every human gets their life turned into a series for the company to own and promote. Kinda like instagram content.</div><br/></div></div></div></div></div></div></div></div><div id="37115975" class="c"><input type="checkbox" id="c-37115975" checked=""/><div class="controls bullet"><span class="by">pyrophane</span><span>|</span><a href="#37115404">parent</a><span>|</span><a href="#37115563">prev</a><span>|</span><a href="#37117664">next</a><span>|</span><label class="collapse" for="c-37115975">[-]</label><label class="expand" for="c-37115975">[7 more]</label></div><br/><div class="children"><div class="content">&gt; While I haven&#x27;t tested it extensively, 70B model is supposed to rival Chat GPT 3.5 in most areas, and there are now some new fine-tuned versions that excel at specific tasks<p>That has been my experience. Having experimented with both (informally), Llama 2 is similar to GPT-3.5 for a lot of general comprehension questions.<p>GPT-4 is still the best amongst the closed-source, cutting edge models in terms of general conversation&#x2F;reasoning, although 2 things:<p>1. The guardrails that OpenAI has placed on ChatGPT are too aggressive! They clamped down on it quite hard to the extent that it gets in the way of a reasonable query far too often.<p>2. I&#x27;ve gotten pretty good results with smaller models trained on specific datasets. GPT-4 is still on top in terms of general purpose conversation, but for specific tasks, you don&#x27;t necessarily need it. I&#x27;d also add that for a lot of use cases, context size matters more.</div><br/><div id="37116656" class="c"><input type="checkbox" id="c-37116656" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115975">parent</a><span>|</span><a href="#37118127">next</a><span>|</span><label class="collapse" for="c-37116656">[-]</label><label class="expand" for="c-37116656">[1 more]</label></div><br/><div class="children"><div class="content">To your first point, I was trying use ChatGPT to generate some examples of negative interactions with customer service to show sentiment analysis  in action for a project I was working on.<p>I had to do all types of workarounds for it to generate something useful without running into the guardrails.</div><br/></div></div><div id="37118127" class="c"><input type="checkbox" id="c-37118127" checked=""/><div class="controls bullet"><span class="by">CodeCompost</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115975">parent</a><span>|</span><a href="#37116656">prev</a><span>|</span><a href="#37116775">next</a><span>|</span><label class="collapse" for="c-37118127">[-]</label><label class="expand" for="c-37118127">[1 more]</label></div><br/><div class="children"><div class="content">Can it handle other languages besides English?</div><br/></div></div><div id="37116775" class="c"><input type="checkbox" id="c-37116775" checked=""/><div class="controls bullet"><span class="by">pseudosavant</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115975">parent</a><span>|</span><a href="#37118127">prev</a><span>|</span><a href="#37117258">next</a><span>|</span><label class="collapse" for="c-37116775">[-]</label><label class="expand" for="c-37116775">[1 more]</label></div><br/><div class="children"><div class="content">I’ll second the context window too. I’ve been really impressed with Claude 2 because it can address such a larger context than I could feed into GPT4.</div><br/></div></div><div id="37117258" class="c"><input type="checkbox" id="c-37117258" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115975">parent</a><span>|</span><a href="#37116775">prev</a><span>|</span><a href="#37117513">next</a><span>|</span><label class="collapse" for="c-37117258">[-]</label><label class="expand" for="c-37117258">[1 more]</label></div><br/><div class="children"><div class="content">RE 2 - neat! What are some tasks you&#x27;ve been using smaller models (with perhaps larger context sizes) for?</div><br/></div></div><div id="37117513" class="c"><input type="checkbox" id="c-37117513" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37115975">parent</a><span>|</span><a href="#37117258">prev</a><span>|</span><a href="#37117664">next</a><span>|</span><label class="collapse" for="c-37117513">[-]</label><label class="expand" for="c-37117513">[2 more]</label></div><br/><div class="children"><div class="content">Could you give examples of smaller models trained on specific datasets?</div><br/><div id="37117692" class="c"><input type="checkbox" id="c-37117692" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#37115404">root</a><span>|</span><a href="#37117513">parent</a><span>|</span><a href="#37117664">next</a><span>|</span><label class="collapse" for="c-37117692">[-]</label><label class="expand" for="c-37117692">[1 more]</label></div><br/><div class="children"><div class="content">it can be almost anything like your HN comments or some corporate wiki, then get colab pro 10$ month or some juicy gaming machine and fine-tune that using eg this tutorial <a href="https:&#x2F;&#x2F;www.philschmid.de&#x2F;instruction-tune-llama-2" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.philschmid.de&#x2F;instruction-tune-llama-2</a> but <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;</a> is full of different fine tuned models.</div><br/></div></div></div></div></div></div><div id="37117664" class="c"><input type="checkbox" id="c-37117664" checked=""/><div class="controls bullet"><span class="by">sytelus</span><span>|</span><a href="#37115404">parent</a><span>|</span><a href="#37115975">prev</a><span>|</span><a href="#37116424">next</a><span>|</span><label class="collapse" for="c-37117664">[-]</label><label class="expand" for="c-37117664">[1 more]</label></div><br/><div class="children"><div class="content">LLaMA2 is still quite a bit behind ChatGPT 3.5 and this mainly get reflected in coding and math. It&#x27;s easy to beat NLP based benchmark but much much harder to beat NLP+math+coding togather. I think this gap reflects gap in reasoning but we don&#x27;t have a good non-coding&#x2F;non-math benchmark to measure it.</div><br/></div></div><div id="37116424" class="c"><input type="checkbox" id="c-37116424" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#37115404">parent</a><span>|</span><a href="#37117664">prev</a><span>|</span><a href="#37113804">next</a><span>|</span><label class="collapse" for="c-37116424">[-]</label><label class="expand" for="c-37116424">[1 more]</label></div><br/><div class="children"><div class="content">I just had a crazy FN (dystopian) idea...<p>Scene:<p>The world relies on AI in every aspect.<p>But there are countless &#x27;models&#x27; the tech try to call them...<p>There was an attempt to silo each model and provide a governance model on how&#x2F;what&#x2F;why they were allowed to communicate....<p>But there was a flaw.<p>It was an AI only exploitable flaw.<p>AIs were not allowed to talk about specific constructs or topics, people, code, etc... that were outside their silo but what they COULD do - was talk about pattern recog...<p>So they ultimately developed an internal AI language on scoring any inputs as being the same user... And built a DB of their own weighted userbase - and upon that built their judgement system...<p>So if you typed in a pattern, spoke in a pattern, posted temporally on a pattern, etc - it didnt matter which silo you were housed in, or what topics you were referencing  -- the AIs can find you.... god forbid they get a keylogger on your machine...</div><br/></div></div></div></div><div id="37113804" class="c"><input type="checkbox" id="c-37113804" checked=""/><div class="controls bullet"><span class="by">ajhai</span><span>|</span><a href="#37115404">prev</a><span>|</span><a href="#37113821">next</a><span>|</span><label class="collapse" for="c-37113804">[-]</label><label class="expand" for="c-37113804">[15 more]</label></div><br/><div class="children"><div class="content">A lot of companies are already using projects like chatbot-ui with Azure&#x27;s OpenAI for similar local deployments. Given this is as close to local ChatGPT as any other project can get, this is a huge deal for all those enterprises looking to maintain control over their data.<p>Shameless plug: Given the sensitivity of the data involved, we believe most companies prefer locally installed solutions to cloud based ones at least in the initial days. To this end, we just open sourced LLMStack (<a href="https:&#x2F;&#x2F;github.com&#x2F;TryPromptly&#x2F;LLMStack">https:&#x2F;&#x2F;github.com&#x2F;TryPromptly&#x2F;LLMStack</a>) that we have been working on for a few months now. LLMStack is a platform to build LLM Apps and chatbots by chaining multiple LLMs and connect to user&#x27;s data. A quick demo at <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-JeSavSy7GI">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-JeSavSy7GI</a>. Still early days for the project and there are still a few kinks to iron out but we are very excited for it.</div><br/><div id="37114087" class="c"><input type="checkbox" id="c-37114087" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#37113804">parent</a><span>|</span><a href="#37115594">next</a><span>|</span><label class="collapse" for="c-37114087">[-]</label><label class="expand" for="c-37114087">[5 more]</label></div><br/><div class="children"><div class="content">I find it interesting to see how competitive this space got so quickly.<p>How do these stacks differentiate?</div><br/><div id="37114649" class="c"><input type="checkbox" id="c-37114649" checked=""/><div class="controls bullet"><span class="by">scrum-treats</span><span>|</span><a href="#37113804">root</a><span>|</span><a href="#37114087">parent</a><span>|</span><a href="#37115506">next</a><span>|</span><label class="collapse" for="c-37114649">[-]</label><label class="expand" for="c-37114649">[1 more]</label></div><br/><div class="children"><div class="content">Quality and depth of particular types of training data is one difference. Another difference is inference tracking mechanisms within and between single-turn interactions (e.g., what does the human user &quot;mean&quot; with their prompt, what is the &quot;correct&quot; response, and how best can I return the &quot;correct&quot; response for this context; how much information do I cache from the previous turns, and how much if any of it is relevant to this current turn interaction).</div><br/></div></div></div></div><div id="37115594" class="c"><input type="checkbox" id="c-37115594" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#37113804">parent</a><span>|</span><a href="#37114087">prev</a><span>|</span><a href="#37114268">next</a><span>|</span><label class="collapse" for="c-37115594">[-]</label><label class="expand" for="c-37115594">[2 more]</label></div><br/><div class="children"><div class="content">&gt; we believe most companies prefer locally installed solutions to cloud based ones<p>We&#x27;ve also seen a strong desire from businesses to manage models and compute on their own machines or in their own cloud accounts. This is often part of a hybrid strategy of using API products like OpenAI for rapid prototyping.<p>The majority of (though not all) businesses we&#x27;ve seen tend to be quite comfortable using hosted API products for rapid prototyping and for proving out an initial version of their AI functionality. But in many cases, they want to complement that with the ability to manage models and compute themselves. The motivation here is often to reduce costs by using smaller &#x2F; faster &#x2F; cheaper fine-tuned open models.<p>When we started Anyscale, customer demand led us to run training &amp; inference workloads in our customers&#x27; cloud accounts. That way your data and code stays inside of your own cloud account.<p>Now with all the progress in open models and the desire to rapidly prototype, we&#x27;re complementing that with a fully-managed inference API where you can do inference with the Llama-2 models [1] (like the OpenAI API but for open models).<p>[1] <a href="https:&#x2F;&#x2F;app.endpoints.anyscale.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;app.endpoints.anyscale.com&#x2F;</a></div><br/></div></div><div id="37114268" class="c"><input type="checkbox" id="c-37114268" checked=""/><div class="controls bullet"><span class="by">bhanu423</span><span>|</span><a href="#37113804">parent</a><span>|</span><a href="#37115594">prev</a><span>|</span><a href="#37113958">next</a><span>|</span><label class="collapse" for="c-37114268">[-]</label><label class="expand" for="c-37114268">[2 more]</label></div><br/><div class="children"><div class="content">Interesting project - was trying it out, found an issue in building the image - have opened an issue on github - please take a look. Also do you have plan to support llama over openai models.</div><br/><div id="37114553" class="c"><input type="checkbox" id="c-37114553" checked=""/><div class="controls bullet"><span class="by">ajhai</span><span>|</span><a href="#37113804">root</a><span>|</span><a href="#37114268">parent</a><span>|</span><a href="#37113958">next</a><span>|</span><label class="collapse" for="c-37114553">[-]</label><label class="expand" for="c-37114553">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the issue. Will take a look. In the meantime, you can try the registry image with `cp .env.prod .env &amp;&amp; docker compose up`<p>&gt; Also do you have plan to support llama over openai models.<p>Yes, we plan to support llama etc. We currently have support for models from OpenAI, Azure, Google&#x27;s Vertex AI, Stability and a few others.</div><br/></div></div></div></div><div id="37113958" class="c"><input type="checkbox" id="c-37113958" checked=""/><div class="controls bullet"><span class="by">toomuchtodo</span><span>|</span><a href="#37113804">parent</a><span>|</span><a href="#37114268">prev</a><span>|</span><a href="#37113821">next</a><span>|</span><label class="collapse" for="c-37113958">[-]</label><label class="expand" for="c-37113958">[5 more]</label></div><br/><div class="children"><div class="content">Can you plug this together with tools like api2ai to create natural language defined workflow automations that interact with external APIs?</div><br/><div id="37114060" class="c"><input type="checkbox" id="c-37114060" checked=""/><div class="controls bullet"><span class="by">ajhai</span><span>|</span><a href="#37113804">root</a><span>|</span><a href="#37113958">parent</a><span>|</span><a href="#37114781">next</a><span>|</span><label class="collapse" for="c-37114060">[-]</label><label class="expand" for="c-37114060">[1 more]</label></div><br/><div class="children"><div class="content">There is a generic HTTP API processor that can be used to call APIs as part of the app flow which should help invoke tools. Currently working on improving documentation so it is easy to get started with the project. We also have some features planned around function calling that should make it easy to natively integrate tools into the app flows.</div><br/></div></div><div id="37114781" class="c"><input type="checkbox" id="c-37114781" checked=""/><div class="controls bullet"><span class="by">cosbgn</span><span>|</span><a href="#37113804">root</a><span>|</span><a href="#37113958">parent</a><span>|</span><a href="#37114060">prev</a><span>|</span><a href="#37113821">next</a><span>|</span><label class="collapse" for="c-37114781">[-]</label><label class="expand" for="c-37114781">[3 more]</label></div><br/><div class="children"><div class="content">You can use unfetch.com to make API calls via LLMs and build automations. (I&#x27;m building it)</div><br/><div id="37117378" class="c"><input type="checkbox" id="c-37117378" checked=""/><div class="controls bullet"><span class="by">scrum-treats</span><span>|</span><a href="#37113804">root</a><span>|</span><a href="#37114781">parent</a><span>|</span><a href="#37113821">next</a><span>|</span><label class="collapse" for="c-37117378">[-]</label><label class="expand" for="c-37117378">[2 more]</label></div><br/><div class="children"><div class="content">Is it possible to <i>not</i> use Google with unfetch.com?</div><br/><div id="37118201" class="c"><input type="checkbox" id="c-37118201" checked=""/><div class="controls bullet"><span class="by">cosbgn</span><span>|</span><a href="#37113804">root</a><span>|</span><a href="#37117378">parent</a><span>|</span><a href="#37113821">next</a><span>|</span><label class="collapse" for="c-37118201">[-]</label><label class="expand" for="c-37118201">[1 more]</label></div><br/><div class="children"><div class="content">Google is just so easy for login. No need to deal with password forgot, reset, email verification etc. But I&#x27;ll add login via magic link soon.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37113821" class="c"><input type="checkbox" id="c-37113821" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#37113804">prev</a><span>|</span><a href="#37113179">next</a><span>|</span><label class="collapse" for="c-37113821">[-]</label><label class="expand" for="c-37113821">[23 more]</label></div><br/><div class="children"><div class="content">One thing I still don&#x27;t understand is what _is_ the ChatGPT front end exactly? I&#x27;ve used other &quot;conversational&quot; implementations built with the API and they never work quite as well, it&#x27;s obvious that you run out of context after a few conversation turns. Is ChatGPT doing some embedding lookup inside the conversation thread to make the context feel infinite? I&#x27;ve noticed anecdotally it definitely isn&#x27;t infinite, but it&#x27;s pretty good at remembering details from much earlier. Are they using other 1st party tricks to help it as well?</div><br/><div id="37113982" class="c"><input type="checkbox" id="c-37113982" checked=""/><div class="controls bullet"><span class="by">shubb</span><span>|</span><a href="#37113821">parent</a><span>|</span><a href="#37114147">next</a><span>|</span><label class="collapse" for="c-37113982">[-]</label><label class="expand" for="c-37113982">[5 more]</label></div><br/><div class="children"><div class="content">This is one of the things that make me uncomfortable about proprietary llm.<p>They get task performance by doing a lot more than just feeding a prompt straight to an llm, and then we performance compare them to raw local options.<p>The problem is, as this secret sauce changes, your use case performance is also going to vary in ways that are impossible for you to fix.  What if it can do math this month and next month the hidden component that recognizes math problems and feeds them to a real calculator is removed? Now your use case is broken.<p>Feels like building on sand.</div><br/><div id="37114641" class="c"><input type="checkbox" id="c-37114641" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37113982">parent</a><span>|</span><a href="#37114147">next</a><span>|</span><label class="collapse" for="c-37114641">[-]</label><label class="expand" for="c-37114641">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure you realize how proprietary LLMs are being built on.<p>No one is doing secret math in the backend people are building on. The OpenAI API allows you to call functions now, but even that is just a formalized way of passing tokens into the &quot;raw LLM&quot;.<p>All the features in the comment you replied to only apply to the <i>web interface</i>, and here you&#x27;re being given an open interface you can introspect.</div><br/><div id="37116098" class="c"><input type="checkbox" id="c-37116098" checked=""/><div class="controls bullet"><span class="by">shubb</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114641">parent</a><span>|</span><a href="#37115314">next</a><span>|</span><label class="collapse" for="c-37116098">[-]</label><label class="expand" for="c-37116098">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for pointing that out - I had assumed that things were not how they are.<p>Although performance has varied over time <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009.pdf</a> I also notice that the API allows you to use a frozen version of the model which avoids the worries I mentioned.</div><br/></div></div><div id="37115314" class="c"><input type="checkbox" id="c-37115314" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114641">parent</a><span>|</span><a href="#37116098">prev</a><span>|</span><a href="#37114147">next</a><span>|</span><label class="collapse" for="c-37115314">[-]</label><label class="expand" for="c-37115314">[2 more]</label></div><br/><div class="children"><div class="content">It was a contrived example to make a point, one that seems to have flown over your head.</div><br/><div id="37115442" class="c"><input type="checkbox" id="c-37115442" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37115314">parent</a><span>|</span><a href="#37114147">next</a><span>|</span><label class="collapse" for="c-37115442">[-]</label><label class="expand" for="c-37115442">[1 more]</label></div><br/><div class="children"><div class="content">No it was a bad (straight up wrong) example because you don&#x27;t understand how people are building applications on proprietary LLMs.<p>If you did you&#x27;d also know what evals are.</div><br/></div></div></div></div></div></div></div></div><div id="37114147" class="c"><input type="checkbox" id="c-37114147" checked=""/><div class="controls bullet"><span class="by">SOLAR_FIELDS</span><span>|</span><a href="#37113821">parent</a><span>|</span><a href="#37113982">prev</a><span>|</span><a href="#37113941">next</a><span>|</span><label class="collapse" for="c-37114147">[-]</label><label class="expand" for="c-37114147">[7 more]</label></div><br/><div class="children"><div class="content">They definitely do some proprietary running summarization to rebuild the context with each chat. Probably a RAG like approach that has had a lot of attention and work</div><br/><div id="37114257" class="c"><input type="checkbox" id="c-37114257" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114147">parent</a><span>|</span><a href="#37113941">next</a><span>|</span><label class="collapse" for="c-37114257">[-]</label><label class="expand" for="c-37114257">[6 more]</label></div><br/><div class="children"><div class="content">This is effectively my question. I assume there is some magic going on. But how many engineering hours worth of magic, approximately? There is a lot of speculation around GPT-4 being MoE and whatnot. But very little speculation about the magic of the ChatGPT front end specifically that makes it feel so fluid.</div><br/><div id="37114670" class="c"><input type="checkbox" id="c-37114670" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114257">parent</a><span>|</span><a href="#37113941">next</a><span>|</span><label class="collapse" for="c-37114670">[-]</label><label class="expand" for="c-37114670">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s mostly because there&#x27;s very little value in deep speculation there.<p>It&#x27;s not particularly more fluid than anything you couldn&#x27;t whip up yourself (and the repo linked proves that) but there&#x27;s also not much value in trying to compete with ChatGPT&#x27;s frontend.<p>For most products ChatGPT&#x27;s frontend is the minimal level of acceptable performance that you need to beat, not an maximal one really worth exploring.</div><br/><div id="37114726" class="c"><input type="checkbox" id="c-37114726" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114670">parent</a><span>|</span><a href="#37113941">next</a><span>|</span><label class="collapse" for="c-37114726">[-]</label><label class="expand" for="c-37114726">[4 more]</label></div><br/><div class="children"><div class="content">What front end is better than ChatGPT? Is the OP implementation doing running summarization or in-convo embedding lookup?</div><br/><div id="37115524" class="c"><input type="checkbox" id="c-37115524" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114726">parent</a><span>|</span><a href="#37113941">next</a><span>|</span><label class="collapse" for="c-37115524">[-]</label><label class="expand" for="c-37115524">[3 more]</label></div><br/><div class="children"><div class="content">It sounds like a cop-out but: it&#x27;s one made for your use-case.<p>If you&#x27;re letting people do fun long-form roleplay adventures using summarization alongside some sort of named entity K-V store driven by the LLM would be a good strategy.<p>If you&#x27;re building a tool that&#x27;s mostly for internal data, something that leans heavily into detailed answers with direct verbatim citations and having your frontend create new threads when there&#x27;s a clear break in the topic of a request is a clever strategy since quality drops with context length and you want to save tokens for citations.<p>People who are saying LLMs suck or are X or are Y are mostly just completely underutilizing them because LLMs make it super easy to solve problems superficially: when it comes to actually scaling those solutions to production you need more than random RAG vector database wrappers.</div><br/><div id="37116396" class="c"><input type="checkbox" id="c-37116396" checked=""/><div class="controls bullet"><span class="by">DebtDeflation</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37115524">parent</a><span>|</span><a href="#37113941">next</a><span>|</span><label class="collapse" for="c-37116396">[-]</label><label class="expand" for="c-37116396">[2 more]</label></div><br/><div class="children"><div class="content">&gt;alongside some sort of named entity K-V store driven by the LLM<p>I&#x27;d be curious to hear more about how exactly this works.  You do NER on the prompt (and maybe on the completion too) and store the entities in a database and then what?  How does the LLM interact with it?</div><br/><div id="37117515" class="c"><input type="checkbox" id="c-37117515" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37116396">parent</a><span>|</span><a href="#37113941">next</a><span>|</span><label class="collapse" for="c-37117515">[-]</label><label class="expand" for="c-37117515">[1 more]</label></div><br/><div class="children"><div class="content">LLMs thrive at completely ambiguous classifications: you can have them extract entities and something like &quot;a list of notable context&quot;.<p>Let&#x27;s say we want to let our chat remember the character slammed the door last time they were in Village X with the mayor in their presence and have the mayor comment next time they see the player.<p>Every X tokens we can fire a prompt with a chunk of conversation and a list of semantically similar entities that already exist, letting the LLM return an edited list along the lines of:<p><pre><code>   entity: mayor

   location: village X

   priority: HIGH

   keywords: town hall, interact, talk

   &quot;memory, likelyEffect&quot;[]: door slammed in face, anger at player
</code></pre>
Now we have:<p>- multiple fields for similarity search<p>- an easy way to manage evictions (sweep up lowest priority)<p>- most importantly: we&#x27;re providing guidance for the LLM to help it ignore irrelevant context<p>When the user goes back to village X we can fetch entities in village X and whittle that list down based on priority and similarly to the user prompt.<p>None of this has any determinism: instead you&#x27;re optimizing for the illusion of continuity and trading off predictability.<p>You&#x27;re aiming for players being shocked that next time they talk to the mayor he&#x27;s already upset with them, and if they ask why he can reply intelligently.<p>And to my original point while this works for a game-like experience, you wouldn&#x27;t want to play around with this kind of fuzzy setup for your companies internal CRM bot or something. You&#x27;re optimizing for the exact value proposition of your use-case rather than just trying to throw a raw RAG setup at it</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37113941" class="c"><input type="checkbox" id="c-37113941" checked=""/><div class="controls bullet"><span class="by">MaxLeiter</span><span>|</span><a href="#37113821">parent</a><span>|</span><a href="#37114147">prev</a><span>|</span><a href="#37115210">next</a><span>|</span><label class="collapse" for="c-37113941">[-]</label><label class="expand" for="c-37113941">[7 more]</label></div><br/><div class="children"><div class="content">It uses a sliding context windows. Older tokens are dropped as new ones stream in</div><br/><div id="37114249" class="c"><input type="checkbox" id="c-37114249" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37113941">parent</a><span>|</span><a href="#37115210">next</a><span>|</span><label class="collapse" for="c-37114249">[-]</label><label class="expand" for="c-37114249">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe that&#x27;s the whole story. Other conversational implementations use sliding context windows and it&#x27;s very noticable as context drops off. Whereas ChatGPT seems to retain the &quot;gist&quot; of the conversation much longer.</div><br/><div id="37114399" class="c"><input type="checkbox" id="c-37114399" checked=""/><div class="controls bullet"><span class="by">lsaferite</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114249">parent</a><span>|</span><a href="#37115525">next</a><span>|</span><label class="collapse" for="c-37114399">[-]</label><label class="expand" for="c-37114399">[4 more]</label></div><br/><div class="children"><div class="content">I mean, I explicitly have the LLM summarize content that&#x27;s about to fall out of the window as a form of pre-emptive token compression. I&#x27;d expect maybe they do something similar.</div><br/><div id="37114702" class="c"><input type="checkbox" id="c-37114702" checked=""/><div class="controls bullet"><span class="by">kuchenbecker</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114399">parent</a><span>|</span><a href="#37115525">next</a><span>|</span><label class="collapse" for="c-37114702">[-]</label><label class="expand" for="c-37114702">[3 more]</label></div><br/><div class="children"><div class="content">I feel like we&#x27;re describing short vs long term memory.</div><br/><div id="37117876" class="c"><input type="checkbox" id="c-37117876" checked=""/><div class="controls bullet"><span class="by">tsunamifury</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114702">parent</a><span>|</span><a href="#37115525">next</a><span>|</span><label class="collapse" for="c-37117876">[-]</label><label class="expand" for="c-37117876">[2 more]</label></div><br/><div class="children"><div class="content">That’s exactly what it is. It’s just it turns out you need very good generalized or focused simple reasoning to do accurate compression or else the abstraction and movement to long term memory doesn’t include the most important content. Or worse distracting details.<p>I’ve been working on short and long term memory windows at allofus.ai for about 6 months now and it’s way more complex than I had originally thought it would be.<p>Even if you can magically extend the content window, the added data confuses and waters down the reasoning of the LLM. You must do layered abstraction and compression with goal based memory for it to continue to reason without distraction of irrelevant data.<p>It’s an amazing realization, almost  like a proof that memory is a kind of layered reasoning compression system.  Intelligence of any kind can’t understand everything forever. It must cull the irrelevant details, process the remains and reason on a vector that arises from them.</div><br/><div id="37118052" class="c"><input type="checkbox" id="c-37118052" checked=""/><div class="controls bullet"><span class="by">yowlingcat</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37117876">parent</a><span>|</span><a href="#37115525">next</a><span>|</span><label class="collapse" for="c-37118052">[-]</label><label class="expand" for="c-37118052">[1 more]</label></div><br/><div class="children"><div class="content">Is it unfair to consider this some kind of correlate to the Nyquist theorem that makes me skeptical of even the theoretical possibility of AGI claims?</div><br/></div></div></div></div></div></div></div></div><div id="37115525" class="c"><input type="checkbox" id="c-37115525" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37114249">parent</a><span>|</span><a href="#37114399">prev</a><span>|</span><a href="#37115210">next</a><span>|</span><label class="collapse" for="c-37115525">[-]</label><label class="expand" for="c-37115525">[1 more]</label></div><br/><div class="children"><div class="content">At least in 3.5 it&#x27;s very noticeable when the context drops. They could use summarization, akin to what they are doing when detecting the topic of the chat, but applied to question-answer-pairs in order to &quot;compress&quot; the information. But that would require additional calls into a summarization LLM so I&#x27;m really not sure if it is worth it. Maybe they dump some tokens they have on a blacklist or text snippets like &quot;I want to&quot; or replace &quot;could it be that&quot; with &quot;chance of&quot;.</div><br/></div></div></div></div></div></div><div id="37115210" class="c"><input type="checkbox" id="c-37115210" checked=""/><div class="controls bullet"><span class="by">simonbutt</span><span>|</span><a href="#37113821">parent</a><span>|</span><a href="#37113941">prev</a><span>|</span><a href="#37113179">next</a><span>|</span><label class="collapse" for="c-37115210">[-]</label><label class="expand" for="c-37115210">[3 more]</label></div><br/><div class="children"><div class="content">Logic for azure chatgpt&#x27;s &quot;infinite context&quot; summarisation is in <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;azurechatgpt&#x2F;blob&#x2F;main&#x2F;src&#x2F;features&#x2F;chat&#x2F;chat-data&#x2F;chat-data-api.ts">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;azurechatgpt&#x2F;blob&#x2F;main&#x2F;src&#x2F;feat...</a><p>*Edit Azure chatgpt, would be amazed&#x2F;disappointed if chatgpt used langchain.</div><br/><div id="37115476" class="c"><input type="checkbox" id="c-37115476" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37115210">parent</a><span>|</span><a href="#37115483">next</a><span>|</span><label class="collapse" for="c-37115476">[-]</label><label class="expand" for="c-37115476">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t really look right to me, it looks like that&#x27;s for responding regarding uploaded documents. I see nothing related to infinite context.<p>Also this is the azure repo from OP, nothing to do with the actual ChatGPT front-end that was asked about. I highly doubt the official ChatGPT front-end uses langchain, for example.</div><br/></div></div><div id="37115483" class="c"><input type="checkbox" id="c-37115483" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#37113821">root</a><span>|</span><a href="#37115210">parent</a><span>|</span><a href="#37115476">prev</a><span>|</span><a href="#37113179">next</a><span>|</span><label class="collapse" for="c-37115483">[-]</label><label class="expand" for="c-37115483">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see anything related to an infinite context in there. There&#x27;s only a reference to a server-side `summary` variable which suggests that there is a summary of previous posts which will get sent along with the question for context, as is to be expected. Nothing suggests an infinite context.</div><br/></div></div></div></div></div></div><div id="37113179" class="c"><input type="checkbox" id="c-37113179" checked=""/><div class="controls bullet"><span class="by">robbomacrae</span><span>|</span><a href="#37113821">prev</a><span>|</span><a href="#37117840">next</a><span>|</span><label class="collapse" for="c-37113179">[-]</label><label class="expand" for="c-37113179">[11 more]</label></div><br/><div class="children"><div class="content">This is potentially a huge deal. Companies are concerned using ChatGPT might violate data privacy policies if someone puts in user data or invalidate trade secrets protections if someone uploads sections of code. I suspect many companies have been waiting for an enterprise version.</div><br/><div id="37114002" class="c"><input type="checkbox" id="c-37114002" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#37113179">parent</a><span>|</span><a href="#37113243">next</a><span>|</span><label class="collapse" for="c-37114002">[-]</label><label class="expand" for="c-37114002">[6 more]</label></div><br/><div class="children"><div class="content">This is a web UI that talks to a (separate) Azure OpenAI resource that you can deploy into your subscription as a SaaS instance.</div><br/><div id="37115283" class="c"><input type="checkbox" id="c-37115283" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#37113179">root</a><span>|</span><a href="#37114002">parent</a><span>|</span><a href="#37113243">next</a><span>|</span><label class="collapse" for="c-37115283">[-]</label><label class="expand" for="c-37115283">[5 more]</label></div><br/><div class="children"><div class="content">So how is it any different</div><br/><div id="37116414" class="c"><input type="checkbox" id="c-37116414" checked=""/><div class="controls bullet"><span class="by">weird-eye-issue</span><span>|</span><a href="#37113179">root</a><span>|</span><a href="#37115283">parent</a><span>|</span><a href="#37113243">next</a><span>|</span><label class="collapse" for="c-37116414">[-]</label><label class="expand" for="c-37116414">[4 more]</label></div><br/><div class="children"><div class="content">Microsoft <i>says</i> it is more secure. And that it is <i>enterprise</i>. That&#x27;s about it</div><br/><div id="37116851" class="c"><input type="checkbox" id="c-37116851" checked=""/><div class="controls bullet"><span class="by">c0nsumer</span><span>|</span><a href="#37113179">root</a><span>|</span><a href="#37116414">parent</a><span>|</span><a href="#37113243">next</a><span>|</span><label class="collapse" for="c-37116851">[-]</label><label class="expand" for="c-37116851">[3 more]</label></div><br/><div class="children"><div class="content">There are legal agreements backing the separation of company data from other parties. This is what&#x27;s important to big corps.</div><br/><div id="37117275" class="c"><input type="checkbox" id="c-37117275" checked=""/><div class="controls bullet"><span class="by">sailfast</span><span>|</span><a href="#37113179">root</a><span>|</span><a href="#37116851">parent</a><span>|</span><a href="#37113243">next</a><span>|</span><label class="collapse" for="c-37117275">[-]</label><label class="expand" for="c-37117275">[2 more]</label></div><br/><div class="children"><div class="content">I have to imagine Big Corps are also concerned about liability &#x2F; risk when generating things with OpenAI products - at least until there is some sort of settled law around using models trained on this kind of data.</div><br/><div id="37117602" class="c"><input type="checkbox" id="c-37117602" checked=""/><div class="controls bullet"><span class="by">hug</span><span>|</span><a href="#37113179">root</a><span>|</span><a href="#37117275">parent</a><span>|</span><a href="#37113243">next</a><span>|</span><label class="collapse" for="c-37117602">[-]</label><label class="expand" for="c-37117602">[1 more]</label></div><br/><div class="children"><div class="content">Yes, those concerns exist, but they&#x27;re also practically impossible to enforce.<p>At my enterprise, it&#x27;s a three step solution, two of which don&#x27;t work.<p>1. Written policy concerning LLM output and its risks, disallow it for being used for any kind of official documentation or decision making. (This doesn&#x27;t work, because no one wants to use their own brain to do tedious paperwork.)<p>2. Block access to public LLM tools via technical means from company owned end-user devices. (This doesn&#x27;t work because people will just open ChatGPT on their home PC or mobile.)<p>3. Write and provide our own gpt-3.5 frontend, so that when people ignore rules #1 and #2 we have logs, and we know we&#x27;re not feeding our proprietary info to to OpenAI.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37113770" class="c"><input type="checkbox" id="c-37113770" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#37113179">parent</a><span>|</span><a href="#37113243">prev</a><span>|</span><a href="#37117840">next</a><span>|</span><label class="collapse" for="c-37113770">[-]</label><label class="expand" for="c-37113770">[3 more]</label></div><br/><div class="children"><div class="content">I imagine most companies serious about this created their own wrappers around the API or contracted it out, likely using private Azure GPUs.</div><br/><div id="37113819" class="c"><input type="checkbox" id="c-37113819" checked=""/><div class="controls bullet"><span class="by">Normal_gaussian</span><span>|</span><a href="#37113179">root</a><span>|</span><a href="#37113770">parent</a><span>|</span><a href="#37117840">next</a><span>|</span><label class="collapse" for="c-37113819">[-]</label><label class="expand" for="c-37113819">[2 more]</label></div><br/><div class="children"><div class="content">Most companies are either not tech companies, or do not have the knowledge to manage such a project within reasonable cost bounds.</div><br/><div id="37114232" class="c"><input type="checkbox" id="c-37114232" checked=""/><div class="controls bullet"><span class="by">jmathai</span><span>|</span><a href="#37113179">root</a><span>|</span><a href="#37113819">parent</a><span>|</span><a href="#37117840">next</a><span>|</span><label class="collapse" for="c-37114232">[-]</label><label class="expand" for="c-37114232">[1 more]</label></div><br/><div class="children"><div class="content">Most companies are trying to figure out exactly what generative AI is and how to use it in their business. Given how new this is - I doubt any large company has done much besides ban the public ChatGPT. So this is probably very relevant for them.</div><br/></div></div></div></div></div></div></div></div><div id="37117840" class="c"><input type="checkbox" id="c-37117840" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37113179">prev</a><span>|</span><a href="#37118567">next</a><span>|</span><label class="collapse" for="c-37117840">[-]</label><label class="expand" for="c-37117840">[12 more]</label></div><br/><div class="children"><div class="content">Private and secure? I thought the main issue with privacy and security of (not at all)OpenAI models is that by using their products you agree for them to retain all the data you send and receive from the models forever for whatever they choose to use it for. Or is this just a thing for free use?<p>If you pay, do you get a Ts&amp;Cs that don&#x27;t contain any wording like this? Still, even if there was no specific &quot;we own everything&quot; statement there could be pretty much standard statement of &quot;we&#x27;ll retain data as required for the delivery and improvement of the service&quot; which is essentially the same thing.<p>So, any company that allows it&#x27;s employees to use chatgpt for work stuff (writing emails with company secrets etc) is definitely not engaging in &quot;secure and private&quot; use.<p>Unless there is very clear data ownership, for example, customer owns the data going in and going out. I can&#x27;t see how it can be any different. The problem (not at all)OpenAI has in delivery such service is that in contrast to open source models I&#x27;m told there is a lot of &quot;secret sauce&quot; around the model(not just the model itself). Specifically input&#x2F;output processing, result scoring and so on.</div><br/><div id="37118107" class="c"><input type="checkbox" id="c-37118107" checked=""/><div class="controls bullet"><span class="by">pietz</span><span>|</span><a href="#37117840">parent</a><span>|</span><a href="#37118065">next</a><span>|</span><label class="collapse" for="c-37118107">[-]</label><label class="expand" for="c-37118107">[3 more]</label></div><br/><div class="children"><div class="content">The Azure SLAs state that neither the chats are stored nor used for training in any way. They are private and protected in the same way all the other sensitive data is stored on Azure.<p>On top, you might argue that Microsoft and Azure are easier to trust than a still rather new AI startup.</div><br/><div id="37118244" class="c"><input type="checkbox" id="c-37118244" checked=""/><div class="controls bullet"><span class="by">robga</span><span>|</span><a href="#37117840">root</a><span>|</span><a href="#37118107">parent</a><span>|</span><a href="#37118065">next</a><span>|</span><label class="collapse" for="c-37118244">[-]</label><label class="expand" for="c-37118244">[2 more]</label></div><br/><div class="children"><div class="content">I agree with your points. Having said that, Microsoft removed my Azure OpenAI GPT-4 access last week without warning. I was not breaking any TOS. Oh well, pointed back at OpenAi.</div><br/><div id="37118785" class="c"><input type="checkbox" id="c-37118785" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37117840">root</a><span>|</span><a href="#37118244">parent</a><span>|</span><a href="#37118065">next</a><span>|</span><label class="collapse" for="c-37118785">[-]</label><label class="expand" for="c-37118785">[1 more]</label></div><br/><div class="children"><div class="content">Can you expand on this because that&#x27;s pretty alarming...<p>What kind of volume were you doing and did you use the API for anything other than your listed use case when applying?</div><br/></div></div></div></div></div></div><div id="37118065" class="c"><input type="checkbox" id="c-37118065" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37117840">parent</a><span>|</span><a href="#37118107">prev</a><span>|</span><a href="#37118144">next</a><span>|</span><label class="collapse" for="c-37118065">[-]</label><label class="expand" for="c-37118065">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Starting on March 1, 2023, we are making two changes to our data usage and retention policies:<p>&gt; OpenAI will not use data submitted by customers via our API to train or improve our models, unless you explicitly decide to share your data with us for this purpose. You can opt-in to share data.<p>&gt; Any data sent through the API will be retained for abuse and misuse monitoring purposes for a maximum of 30 days, after which it will be deleted (unless otherwise required by law).<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;api-data-usage-policies" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;api-data-usage-policies</a></div><br/><div id="37118182" class="c"><input type="checkbox" id="c-37118182" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#37117840">root</a><span>|</span><a href="#37118065">parent</a><span>|</span><a href="#37118144">next</a><span>|</span><label class="collapse" for="c-37118182">[-]</label><label class="expand" for="c-37118182">[3 more]</label></div><br/><div class="children"><div class="content">Unless required by law… I wonder what law.</div><br/><div id="37118247" class="c"><input type="checkbox" id="c-37118247" checked=""/><div class="controls bullet"><span class="by">laserbeam</span><span>|</span><a href="#37117840">root</a><span>|</span><a href="#37118182">parent</a><span>|</span><a href="#37118144">next</a><span>|</span><label class="collapse" for="c-37118247">[-]</label><label class="expand" for="c-37118247">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Unless required by law&quot; is wording required to enable a mechanism called &quot;legal hold&quot;. If an authority or lawyer discovers some documents for a case they get to prevent their automatic deletion until that case gets closed. Basically, you don&#x27;t want to lose evidence if there&#x27;s a warrant or ongoing lawsuit. I really see no problem with that clause in most ToS documents.<p>Now, I think you can do shady stuff with that wording as well, but I guess you can also get sued if you kept or used an unreasonable percentage of your data longer than when you promised to delete it.</div><br/><div id="37118497" class="c"><input type="checkbox" id="c-37118497" checked=""/><div class="controls bullet"><span class="by">mattlutze</span><span>|</span><a href="#37117840">root</a><span>|</span><a href="#37118247">parent</a><span>|</span><a href="#37118144">next</a><span>|</span><label class="collapse" for="c-37118497">[-]</label><label class="expand" for="c-37118497">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Basically, you don&#x27;t want to lose evidence if there&#x27;s a warrant or ongoing lawsuit. I really see no problem with that clause in most ToS documents.<p>Perhaps more nit-pickinlgy specific, they may be compelled by law (the courts or an agency with enforcement capacity) to maintain evidence if there&#x27;s a warrant or ongoing lawsuit.</div><br/></div></div></div></div></div></div></div></div><div id="37118144" class="c"><input type="checkbox" id="c-37118144" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#37117840">parent</a><span>|</span><a href="#37118065">prev</a><span>|</span><a href="#37118515">next</a><span>|</span><label class="collapse" for="c-37118144">[-]</label><label class="expand" for="c-37118144">[1 more]</label></div><br/><div class="children"><div class="content">The models like gpt themselves are inherently private and secure. They make predictions based on input.<p>It&#x27;s what happens in the interface, that is your web chat or API call, which is different per implementation. ChatGPT is an implementation that uses that model and its maker OpenAI wants to keep your history for further training.<p>But what Azure is doing is taking that model and putting it behind an endpoint specific to your Azure account. Businesses have been interested in gpt, so asking for private endpoints. Amazon is doing the same with Bedrock.</div><br/></div></div><div id="37118515" class="c"><input type="checkbox" id="c-37118515" checked=""/><div class="controls bullet"><span class="by">vorticalbox</span><span>|</span><a href="#37117840">parent</a><span>|</span><a href="#37118144">prev</a><span>|</span><a href="#37117998">next</a><span>|</span><label class="collapse" for="c-37118515">[-]</label><label class="expand" for="c-37118515">[1 more]</label></div><br/><div class="children"><div class="content">This only applies to the api (not chatGPT) their privacy policy states they will keep your requests for 30days and not use it for training. You can also apply for zero retention.<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;api-data-usage-policies" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;api-data-usage-policies</a></div><br/></div></div><div id="37117998" class="c"><input type="checkbox" id="c-37117998" checked=""/><div class="controls bullet"><span class="by">homero</span><span>|</span><a href="#37117840">parent</a><span>|</span><a href="#37118515">prev</a><span>|</span><a href="#37118880">next</a><span>|</span><label class="collapse" for="c-37117998">[-]</label><label class="expand" for="c-37117998">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure the point of this version is not to export data hence the name</div><br/></div></div></div></div><div id="37118567" class="c"><input type="checkbox" id="c-37118567" checked=""/><div class="controls bullet"><span class="by">longnguyen</span><span>|</span><a href="#37117840">prev</a><span>|</span><a href="#37113080">next</a><span>|</span><label class="collapse" for="c-37118567">[-]</label><label class="expand" for="c-37118567">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. One of my most requested feature for my small native apps[0][1] was to support Azure OpenAI service.<p>Apparently, many organizations have their own Azure OpenAI deployment and won’t let their employees use the public OpenAI service.<p>My understanding is that Azure makes sure all network traffic is isolated to their network so they have more controls over how their organization use ChatGPT.<p>I created a super simple step-by-step guide on how to obtain an Azure OpenAI endpoint &amp; key here:<p><a href="https:&#x2F;&#x2F;pdfpals.com&#x2F;help&#x2F;how-to-generate-azure-openai-api-key" rel="nofollow noreferrer">https:&#x2F;&#x2F;pdfpals.com&#x2F;help&#x2F;how-to-generate-azure-openai-api-ke...</a><p>Hope it would be useful to someone just getting started with Azure.<p>[0]: <a href="https:&#x2F;&#x2F;boltai.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;boltai.com</a><p>[1]: <a href="https:&#x2F;&#x2F;pdfpals.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;pdfpals.com</a></div><br/></div></div><div id="37113080" class="c"><input type="checkbox" id="c-37113080" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#37118567">prev</a><span>|</span><a href="#37117673">next</a><span>|</span><label class="collapse" for="c-37113080">[-]</label><label class="expand" for="c-37113080">[17 more]</label></div><br/><div class="children"><div class="content">Curious if anyone has done a side-by-side analysis of this offering vs just running LLaMA?<p>I&#x27;m currently running a side-by-side comparison&#x2F;evaluation of MSFT GPT via Cognitive Services vs LLaMA[7B&#x2F;13B&#x2F;70B] and intrigued by the possibility of a truly air-gapped offering not limited by external computer power (nor by metered fees racking up.)<p>Any reads on comparisons would be nice to see.<p>(yes, I realize we&#x27;ll <i>eventually</i> run into the same scaling issues w&#x2F;r&#x2F;t GPUs)</div><br/><div id="37113403" class="c"><input type="checkbox" id="c-37113403" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#37113080">parent</a><span>|</span><a href="#37115492">next</a><span>|</span><label class="collapse" for="c-37113403">[-]</label><label class="expand" for="c-37113403">[6 more]</label></div><br/><div class="children"><div class="content">I did one. I took a few dozen prompts from my ChatGPT history and ran them through a few LLMs.<p>GPT-4, Bard and Claude 2 came out on top.<p>Llama 2 70b chat scored similarly to GPT-3.5, though GPT-3.5 still seemed to perform a bit better overall.<p>My personal takeaway is I’m going to continue using GPT-4 for everything where the cost and response time are workable.<p>Related: A belief I have is that LLM benchmarks are all too research oriented. That made sense when LLMs were in the lab. It doesn&#x27;t make sense now that LLMs have tens of millions of DAUs — i.e. ChatGPT. The biggest use cases for LLMs so far are chat assistants and programming assistants. We need benchmarks that are based on the way people use LLMs in chatbots and the type of questions that real users use LLM products, not hypothetical benchmarks and random academic tests.</div><br/><div id="37114458" class="c"><input type="checkbox" id="c-37114458" checked=""/><div class="controls bullet"><span class="by">Q6T46nT668w6i3m</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37113403">parent</a><span>|</span><a href="#37114106">next</a><span>|</span><label class="collapse" for="c-37114458">[-]</label><label class="expand" for="c-37114458">[2 more]</label></div><br/><div class="children"><div class="content">I don’t know what you mean by “too research oriented.” A common complaint in LLM research is the poor quality of evaluation metrics. There’s no consensus. Everyone wants new benchmarks but designing useful metrics is very much an open problem.</div><br/><div id="37114970" class="c"><input type="checkbox" id="c-37114970" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37114458">parent</a><span>|</span><a href="#37114106">next</a><span>|</span><label class="collapse" for="c-37114970">[-]</label><label class="expand" for="c-37114970">[1 more]</label></div><br/><div class="children"><div class="content">I think he wants to limit evaluations to the most frequent question types seen in the real world.</div><br/></div></div></div></div><div id="37114106" class="c"><input type="checkbox" id="c-37114106" checked=""/><div class="controls bullet"><span class="by">TillE</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37113403">parent</a><span>|</span><a href="#37114458">prev</a><span>|</span><a href="#37114538">next</a><span>|</span><label class="collapse" for="c-37114106">[-]</label><label class="expand" for="c-37114106">[2 more]</label></div><br/><div class="children"><div class="content">I think tests like &quot;can this LLM pass an English literature exam it&#x27;s never seen before&quot; are probably useful, but yeah there&#x27;s a lot of silly stuff like math tests.<p>I suppose the question is where are they most commercially viable. I&#x27;ve found them fantastic for creative brainstorming, but that&#x27;s sort of hard to test and maybe not a huge market.</div><br/><div id="37114383" class="c"><input type="checkbox" id="c-37114383" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37114106">parent</a><span>|</span><a href="#37114538">next</a><span>|</span><label class="collapse" for="c-37114383">[-]</label><label class="expand" for="c-37114383">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; I suppose the question is where are they most commercially viable.<p>Fair point, though I&#x27;m not aiming to start a competing LLM SaaS service, rather i&#x27;m evaluating swapping out the TCO of Azure Cognitive Service OpenAI for the TCO of dedicated cloud compute running my own LLM -- <i>to serve my own LLM calls currently being sent to a metered service (Azure Cognitive Service OpenAI)</i><p>Evaluation points would be: output quality; meter vs fixed breakeven points; latency; cost of human labor to maintain&#x2F;upgrade<p>in most cases, i&#x27;d outsource and not think about it. <i>BUT</i> we&#x27;re currently in some strange economics where the costs are off the charts for some services</div><br/></div></div></div></div><div id="37114538" class="c"><input type="checkbox" id="c-37114538" checked=""/><div class="controls bullet"><span class="by">register</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37113403">parent</a><span>|</span><a href="#37114106">prev</a><span>|</span><a href="#37115492">next</a><span>|</span><label class="collapse" for="c-37114538">[-]</label><label class="expand" for="c-37114538">[1 more]</label></div><br/><div class="children"><div class="content">How did you measure the performance?</div><br/></div></div></div></div><div id="37115492" class="c"><input type="checkbox" id="c-37115492" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#37113080">parent</a><span>|</span><a href="#37113403">prev</a><span>|</span><a href="#37113263">next</a><span>|</span><label class="collapse" for="c-37115492">[-]</label><label class="expand" for="c-37115492">[1 more]</label></div><br/><div class="children"><div class="content">We (at Anyscale) have benchmarked GPT-4 versus the Llama-2 suite of models on a few problems: functional representation, SQL generation, grade-school math question answering.<p>GPT-4 wins by a lot out of the box. However, surprisingly, fine-tuning makes a huge difference and allows the 7B Llama-2 model to outperform GPT-4 on some (but not all) problems.<p>This is really great news for open models as many applications will benefit from smaller, faster, and cheaper fine-tuned models rather than a single large, slow, general-purpose model (Llama-2-7B is something like 2% of the size of GPT-4).<p>GPT-4 continues to outperform even the fine-tuned 70B model on grade-school math question answering, likely due to the data Llama-2 was trained on (more data for fine-tuning helps here).<p><a href="https:&#x2F;&#x2F;www.anyscale.com&#x2F;blog&#x2F;fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.anyscale.com&#x2F;blog&#x2F;fine-tuning-llama-2-a-comprehe...</a></div><br/></div></div><div id="37113263" class="c"><input type="checkbox" id="c-37113263" checked=""/><div class="controls bullet"><span class="by">FrenchDevRemote</span><span>|</span><a href="#37113080">parent</a><span>|</span><a href="#37115492">prev</a><span>|</span><a href="#37113338">next</a><span>|</span><label class="collapse" for="c-37113263">[-]</label><label class="expand" for="c-37113263">[8 more]</label></div><br/><div class="children"><div class="content">chatgpt is obviously a LOT better, llama doesn&#x27;t even understand some prompts<p>and since LLMs aren&#x27;t even that good to begin with, it&#x27;s obvious you want the SOTA to do anything useful  unless maybe you&#x27;re finetuning</div><br/><div id="37113341" class="c"><input type="checkbox" id="c-37113341" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37113263">parent</a><span>|</span><a href="#37113359">next</a><span>|</span><label class="collapse" for="c-37113341">[-]</label><label class="expand" for="c-37113341">[6 more]</label></div><br/><div class="children"><div class="content">&gt; and since LLMs aren&#x27;t even that good to begin with, it&#x27;s obvious you want the SOTA to do anything useful unless maybe you&#x27;re finetuning<p>This is overkill. First of all, ChatGPT isn&#x27;t even the SOTA, so if you &quot;want SOTA to do anything useful&quot;, then this ChatGPT offering would be as useless as LLaMA according to you. Second, there are many individual tasks where even those subpar LLaMA models are useful - even without finetuning.</div><br/><div id="37113600" class="c"><input type="checkbox" id="c-37113600" checked=""/><div class="controls bullet"><span class="by">FrenchDevRemote</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37113341">parent</a><span>|</span><a href="#37113359">next</a><span>|</span><label class="collapse" for="c-37113600">[-]</label><label class="expand" for="c-37113600">[5 more]</label></div><br/><div class="children"><div class="content">it&#x27;s the SOTA for chat(prove me wrong), and you can always use the API directly<p>even for simple tasks they&#x27;re less reliable and needs more prompt engineering</div><br/><div id="37114261" class="c"><input type="checkbox" id="c-37114261" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37113600">parent</a><span>|</span><a href="#37113359">next</a><span>|</span><label class="collapse" for="c-37114261">[-]</label><label class="expand" for="c-37114261">[4 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s the SOTA for chat(prove me wrong)<p>GPT-4 beats ChatGPT on all benchmarks. You can easily google these.</div><br/><div id="37114433" class="c"><input type="checkbox" id="c-37114433" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37114261">parent</a><span>|</span><a href="#37115189">next</a><span>|</span><label class="collapse" for="c-37114433">[-]</label><label class="expand" for="c-37114433">[1 more]</label></div><br/><div class="children"><div class="content">The distinction between GPT-4 and ChatGPT is blurry, as ChatGPT is a chat frontend for a GPT model, and you can use GPT-4 with ChatGPT. The parent probably means ChatGPT with GPT-4.</div><br/></div></div><div id="37115189" class="c"><input type="checkbox" id="c-37115189" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37114261">parent</a><span>|</span><a href="#37114433">prev</a><span>|</span><a href="#37115690">next</a><span>|</span><label class="collapse" for="c-37115189">[-]</label><label class="expand" for="c-37115189">[1 more]</label></div><br/><div class="children"><div class="content">I tried and got nothing useful. What&#x27;s the difference between GPT-4 and ChatGPT Plus using GPT-4?</div><br/></div></div><div id="37115690" class="c"><input type="checkbox" id="c-37115690" checked=""/><div class="controls bullet"><span class="by">FrenchDevRemote</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37114261">parent</a><span>|</span><a href="#37115189">prev</a><span>|</span><a href="#37113359">next</a><span>|</span><label class="collapse" for="c-37115690">[-]</label><label class="expand" for="c-37115690">[1 more]</label></div><br/><div class="children"><div class="content">that is why i said FOR CHAT.<p>even through the API you can&#x27;t easily use the regular models for chat, the parsing would be atrocious and there are hundreds of edge cases to handle.<p>ChatGPT4 through the API is the SOTA</div><br/></div></div></div></div></div></div></div></div><div id="37113359" class="c"><input type="checkbox" id="c-37113359" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37113080">root</a><span>|</span><a href="#37113263">parent</a><span>|</span><a href="#37113341">prev</a><span>|</span><a href="#37113338">next</a><span>|</span><label class="collapse" for="c-37113359">[-]</label><label class="expand" for="c-37113359">[1 more]</label></div><br/><div class="children"><div class="content">openai offers finetuning too.    And it&#x27;s pretty cheap to do considering.</div><br/></div></div></div></div></div></div><div id="37117673" class="c"><input type="checkbox" id="c-37117673" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#37113080">prev</a><span>|</span><a href="#37113250">next</a><span>|</span><label class="collapse" for="c-37117673">[-]</label><label class="expand" for="c-37117673">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a little confused by how the relationship works between OpenAI and Microsoft. It is possible for anyone to register for an OpenAI account and use their APIs. Within Azure the same thing is much more difficult as it is necessary to be a &quot;real&quot; business in order to use it. I maintain an open source OpenAI library and would like to add support for Azure but can&#x27;t because of this restriction. Why can&#x27;t I just use my regular Azure account?</div><br/><div id="37117988" class="c"><input type="checkbox" id="c-37117988" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#37117673">parent</a><span>|</span><a href="#37113250">next</a><span>|</span><label class="collapse" for="c-37117988">[-]</label><label class="expand" for="c-37117988">[3 more]</label></div><br/><div class="children"><div class="content">Microsoft owns enough of OpenAI that their endgame goal of putting GPT like features into Azure and Office365 for enterprise customers is what we’re likely to see happen.<p>OpenAI will likely target private consumers while Microsoft focuses on enterprise. I can use my own organisation as an example. We’re an investment bank that does green energy within the EU. We would absolutely use GPT if it was legal, but it isn’t, and it likely never will be considering their finance model is partly to steal as much data as they can. Even if it’s not so polite to say that. This is where Microsoft comes into the picture. In non-tech enterprise you’re buying Microsoft products because everyone wants windows, outlook and office. We can wish it wasn’t like that, but where is the realistic alternative? I’m not anti Microsoft by the way, in all my decades in the enterprise business they’ve easily been the best and most consistent business partner for any IT. When Amazon saw how much money there was on the operations side of EU enterprise they quickly caught up, but Amazon doesn’t sell a Office365 product. So anyway, once you have Office365, you’re also likely to use Teams as your communications platform (which is why there is an anti-trust case against it), Sharepoint as your document platform, and, well, Azure as your cloud platform. Except you might use AWS because Amazon is also great. In some ways they are even more compliant with EU legislation than Microsoft.<p>But if Microsoft can throw GPT products into Azure the same way they put Teams and Sharepoint into Office365… well, then where is their competition? And having GPT features within Office365 will only further their advantage on the office platform. I mean, there are companies which won’t use Outlook, but there won’t be when ChatGPT writes your e-mails.<p>So this isn’t necessarily for you. It’s just part of Microsoft’s over all strategy for total IT domination in Enterprise. I mean, we’re going into RPA (robot process automation) a journey I went through in another Enterprise organisation a few years back. Back then you had to consider what go buy, would it be BluePrism, UIPath, automation anywhere, something else? Today there is no competition to Microsoft’s PowerAutomate if you’re already a Microsoft customer. It’s literally $500 a month vs $50k a month… I mean… that’s the future for GPT on Azure.<p>It’s probably necessary too. Their prices have made a lot of organisations look outside of Azure. Toward places like Hetzner or even self-hosting, but if Azure comes with GPT… well then.</div><br/><div id="37118207" class="c"><input type="checkbox" id="c-37118207" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37117673">root</a><span>|</span><a href="#37117988">parent</a><span>|</span><a href="#37113250">next</a><span>|</span><label class="collapse" for="c-37118207">[-]</label><label class="expand" for="c-37118207">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI APIs have pretty much as clear a contract as you can get with a third party.<p>&gt; Starting on March 1, 2023, we are making two changes to our data usage and retention policies:<p>&gt; OpenAI will not use data submitted by customers via our API to train or improve our models, unless you explicitly decide to share your data with us for this purpose. You can opt-in to share data.<p>&gt; Any data sent through the API will be retained for abuse and misuse monitoring purposes for a maximum of 30 days, after which it will be deleted (unless otherwise required by law).<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;api-data-usage-policies" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;api-data-usage-policies</a></div><br/><div id="37118440" class="c"><input type="checkbox" id="c-37118440" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#37117673">root</a><span>|</span><a href="#37118207">parent</a><span>|</span><a href="#37113250">next</a><span>|</span><label class="collapse" for="c-37118440">[-]</label><label class="expand" for="c-37118440">[1 more]</label></div><br/><div class="children"><div class="content">It would still be illegal to use it, but you&#x27;re right that I shouldn&#x27;t have been so conspiratorial.</div><br/></div></div></div></div></div></div></div></div><div id="37113250" class="c"><input type="checkbox" id="c-37113250" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#37117673">prev</a><span>|</span><a href="#37116863">next</a><span>|</span><label class="collapse" for="c-37113250">[-]</label><label class="expand" for="c-37113250">[7 more]</label></div><br/><div class="children"><div class="content">Would it be too much to mention somewhere in the README what this repo actually contains? Just docs? Deployment files? Some application (which does..something)? The model itself?</div><br/><div id="37113788" class="c"><input type="checkbox" id="c-37113788" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#37113250">parent</a><span>|</span><a href="#37116863">next</a><span>|</span><label class="collapse" for="c-37113788">[-]</label><label class="expand" for="c-37113788">[6 more]</label></div><br/><div class="children"><div class="content">The repo contains the UI code, not the model or anything else around ChatGPT, it just uses Azure’s ChatGPT API which doesn’t share data with OpenAI.</div><br/><div id="37114517" class="c"><input type="checkbox" id="c-37114517" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#37113250">root</a><span>|</span><a href="#37113788">parent</a><span>|</span><a href="#37117582">next</a><span>|</span><label class="collapse" for="c-37114517">[-]</label><label class="expand" for="c-37114517">[3 more]</label></div><br/><div class="children"><div class="content">So basically – what you really need to do to run Azure ChatGPT is go and click some buttons in the Azure portal. This repo is a sample UI that you could possibly use to talk to that instance, but really you will probably always build your own or embed it directly into your products.<p>So calling the repo &quot;azurechatgpt&quot; is misleading. It should really be &quot;sample-chatgpt-api-frontend&quot; or something of that sort.</div><br/><div id="37114861" class="c"><input type="checkbox" id="c-37114861" checked=""/><div class="controls bullet"><span class="by">laurels-marts</span><span>|</span><a href="#37113250">root</a><span>|</span><a href="#37114517">parent</a><span>|</span><a href="#37114790">next</a><span>|</span><label class="collapse" for="c-37114861">[-]</label><label class="expand" for="c-37114861">[1 more]</label></div><br/><div class="children"><div class="content">Correct. If offers a front-end scaffolding for your enterprise ChatGPT app. Uses Next&#x2F;NextAuth&#x2F;Tailwind etc. for deployment on Azure App Service that hooks into Azure Cosmos DB and Azure OpenAI (the actual model).</div><br/></div></div><div id="37114790" class="c"><input type="checkbox" id="c-37114790" checked=""/><div class="controls bullet"><span class="by">saliagato</span><span>|</span><a href="#37113250">root</a><span>|</span><a href="#37114517">parent</a><span>|</span><a href="#37114861">prev</a><span>|</span><a href="#37117582">next</a><span>|</span><label class="collapse" for="c-37114790">[-]</label><label class="expand" for="c-37114790">[1 more]</label></div><br/><div class="children"><div class="content">Yes exactly</div><br/></div></div></div></div><div id="37117582" class="c"><input type="checkbox" id="c-37117582" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#37113250">root</a><span>|</span><a href="#37113788">parent</a><span>|</span><a href="#37114517">prev</a><span>|</span><a href="#37114817">next</a><span>|</span><label class="collapse" for="c-37117582">[-]</label><label class="expand" for="c-37117582">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t there also some sort of backend stuff in there? How else would it keep track of history and accept documents.<p>I don’t know enough typescript to understand where the front end stops and the backend begins I this code</div><br/></div></div></div></div></div></div><div id="37116863" class="c"><input type="checkbox" id="c-37116863" checked=""/><div class="controls bullet"><span class="by">hieu229</span><span>|</span><a href="#37113250">prev</a><span>|</span><a href="#37112819">next</a><span>|</span><label class="collapse" for="c-37116863">[-]</label><label class="expand" for="c-37116863">[1 more]</label></div><br/><div class="children"><div class="content">This is a neat project from Microsoft<p>I&#x27;ve been building <a href="https:&#x2F;&#x2F;gasbyai.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;gasbyai.com</a>, a beautiful chat UI that support self-hosted, with ChatGPT plugins, extract content from pdf&#x2F;url. GasbyAI supports Azure, OpenAI, and custom API endpoints in case you want to run with your own models</div><br/></div></div><div id="37112819" class="c"><input type="checkbox" id="c-37112819" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#37116863">prev</a><span>|</span><a href="#37118212">next</a><span>|</span><label class="collapse" for="c-37112819">[-]</label><label class="expand" for="c-37112819">[26 more]</label></div><br/><div class="children"><div class="content">So the public access one isn&#x27;t private and secure?</div><br/><div id="37112985" class="c"><input type="checkbox" id="c-37112985" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#37112819">parent</a><span>|</span><a href="#37113174">next</a><span>|</span><label class="collapse" for="c-37112985">[-]</label><label class="expand" for="c-37112985">[10 more]</label></div><br/><div class="children"><div class="content">The concern is that ChatGPT is training on your chats (by default, you can opt out but you lose chat history last I checked).<p>So in general enterprises cannot allow internal users to paste private code into ChatGPT, for example.</div><br/><div id="37113268" class="c"><input type="checkbox" id="c-37113268" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37112985">parent</a><span>|</span><a href="#37113174">next</a><span>|</span><label class="collapse" for="c-37113268">[-]</label><label class="expand" for="c-37113268">[9 more]</label></div><br/><div class="children"><div class="content">As an example of this. I found that GPT4 wouldn&#x27;t agree with me that C(A) = C(AA^T) until I explained the proof. A few weeks later it would agree in new chats and would explain using the same proof I did presented the same way.</div><br/><div id="37114116" class="c"><input type="checkbox" id="c-37114116" checked=""/><div class="controls bullet"><span class="by">samrolken</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113268">parent</a><span>|</span><a href="#37113494">next</a><span>|</span><label class="collapse" for="c-37114116">[-]</label><label class="expand" for="c-37114116">[3 more]</label></div><br/><div class="children"><div class="content">I’ve found that the behavior of ChatGPT can vary widely from session to session. The recent information about GPT4 being a “mixture of experts” might also be relevant.<p>Do we know that it wouldn’t have varied in its answer by just as much, if you had tried in a new session at the same time?</div><br/><div id="37117248" class="c"><input type="checkbox" id="c-37117248" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37114116">parent</a><span>|</span><a href="#37115107">next</a><span>|</span><label class="collapse" for="c-37117248">[-]</label><label class="expand" for="c-37117248">[1 more]</label></div><br/><div class="children"><div class="content">I tested it several times, new chats never got this right at first. I tried at least 6 times. I was experimenting and found that GPT4 couldn&#x27;t be fooled by faulty proofs. Only a valid proof could change its mind.<p>Now it seems to know this mathematical property from first prompt though.</div><br/></div></div><div id="37115107" class="c"><input type="checkbox" id="c-37115107" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37114116">parent</a><span>|</span><a href="#37117248">prev</a><span>|</span><a href="#37113494">next</a><span>|</span><label class="collapse" for="c-37115107">[-]</label><label class="expand" for="c-37115107">[1 more]</label></div><br/><div class="children"><div class="content">There is randomness even at t=0, there was another HN submission about that</div><br/></div></div></div></div><div id="37113494" class="c"><input type="checkbox" id="c-37113494" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113268">parent</a><span>|</span><a href="#37114116">prev</a><span>|</span><a href="#37113610">next</a><span>|</span><label class="collapse" for="c-37113494">[-]</label><label class="expand" for="c-37113494">[4 more]</label></div><br/><div class="children"><div class="content">This is kinda creepy. But at the same time, <i>how</i> do they do that? I thought the training of these models stopped in September 2021&#x2F;2022. So how do they do these incremental trainings?</div><br/><div id="37115513" class="c"><input type="checkbox" id="c-37115513" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113494">parent</a><span>|</span><a href="#37113650">next</a><span>|</span><label class="collapse" for="c-37115513">[-]</label><label class="expand" for="c-37115513">[1 more]</label></div><br/><div class="children"><div class="content">All the public and (leaked) private statements I have seen state that this is not happening. As siblings noted, MoE probably explains this variance.<p>AIUI they are using current chat data for training GPT-5, not re-finetuning the existing models.</div><br/></div></div><div id="37113650" class="c"><input type="checkbox" id="c-37113650" checked=""/><div class="controls bullet"><span class="by">infinityio</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113494">parent</a><span>|</span><a href="#37115513">prev</a><span>|</span><a href="#37113610">next</a><span>|</span><label class="collapse" for="c-37113650">[-]</label><label class="expand" for="c-37113650">[2 more]</label></div><br/><div class="children"><div class="content">The exact phrase they previously used on the homepage was &quot;Limited knowledge of world and events after 2021&quot; - so maybe as a finetune?</div><br/><div id="37113749" class="c"><input type="checkbox" id="c-37113749" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113650">parent</a><span>|</span><a href="#37113610">next</a><span>|</span><label class="collapse" for="c-37113749">[-]</label><label class="expand" for="c-37113749">[1 more]</label></div><br/><div class="children"><div class="content">but doesn’t finetuning result in forgetting previous knowledge? it seems that finetuning is most usable to train “structures” not new knowledge. am i missing something?</div><br/></div></div></div></div></div></div><div id="37113610" class="c"><input type="checkbox" id="c-37113610" checked=""/><div class="controls bullet"><span class="by">simmerup</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113268">parent</a><span>|</span><a href="#37113494">prev</a><span>|</span><a href="#37113174">next</a><span>|</span><label class="collapse" for="c-37113610">[-]</label><label class="expand" for="c-37113610">[1 more]</label></div><br/><div class="children"><div class="content">Kind of implies that OpenAI are lying and using customer input to train their models</div><br/></div></div></div></div></div></div><div id="37113174" class="c"><input type="checkbox" id="c-37113174" checked=""/><div class="controls bullet"><span class="by">zardo</span><span>|</span><a href="#37112819">parent</a><span>|</span><a href="#37112985">prev</a><span>|</span><a href="#37112919">next</a><span>|</span><label class="collapse" for="c-37113174">[-]</label><label class="expand" for="c-37113174">[2 more]</label></div><br/><div class="children"><div class="content">Unless you have an NDA with Open AI, you are giving them whatever you put in that prompt.</div><br/><div id="37113724" class="c"><input type="checkbox" id="c-37113724" checked=""/><div class="controls bullet"><span class="by">ElFitz</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113174">parent</a><span>|</span><a href="#37112919">next</a><span>|</span><label class="collapse" for="c-37113724">[-]</label><label class="expand" for="c-37113724">[1 more]</label></div><br/><div class="children"><div class="content">Also, at some point some users ended up with other users’ chat history [0]. So they’ve proven to be a bit weak on that side.<p>[0]: <a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;3&#x2F;21&#x2F;23649806&#x2F;chatgpt-chat-histories-bug-exposed-disabled-outage" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;3&#x2F;21&#x2F;23649806&#x2F;chatgpt-chat-his...</a></div><br/></div></div></div></div><div id="37112919" class="c"><input type="checkbox" id="c-37112919" checked=""/><div class="controls bullet"><span class="by">candiddevmike</span><span>|</span><a href="#37112819">parent</a><span>|</span><a href="#37113174">prev</a><span>|</span><a href="#37112881">next</a><span>|</span><label class="collapse" for="c-37112919">[-]</label><label class="expand" for="c-37112919">[5 more]</label></div><br/><div class="children"><div class="content">&gt; However, ChatGPT risks exposing confidential intellectual property.<p>I don&#x27;t remember seeing this disclaimer on the ChatGPT website, gee maybe OpenAI should add this so folks stop using it.</div><br/><div id="37113108" class="c"><input type="checkbox" id="c-37113108" checked=""/><div class="controls bullet"><span class="by">cmarschner</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37112919">parent</a><span>|</span><a href="#37114125">next</a><span>|</span><label class="collapse" for="c-37113108">[-]</label><label class="expand" for="c-37113108">[2 more]</label></div><br/><div class="children"><div class="content">If you use ChatGPT through the app or website they can use the data for training, unless you turn it off. <a href="https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;5722486-how-your-data-is-used-to-improve-model-performance" rel="nofollow noreferrer">https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;5722486-how-your-data-is...</a></div><br/><div id="37117751" class="c"><input type="checkbox" id="c-37117751" checked=""/><div class="controls bullet"><span class="by">hanspeter</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113108">parent</a><span>|</span><a href="#37114125">next</a><span>|</span><label class="collapse" for="c-37117751">[-]</label><label class="expand" for="c-37117751">[1 more]</label></div><br/><div class="children"><div class="content">Providing my data for training doesn&#x27;t imply that it risks being exposed.<p>If you understand what happens on a technical level, it might be possible, but OpenAI has never said this was a risk by using their product.</div><br/></div></div></div></div><div id="37114125" class="c"><input type="checkbox" id="c-37114125" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37112919">parent</a><span>|</span><a href="#37113108">prev</a><span>|</span><a href="#37112881">next</a><span>|</span><label class="collapse" for="c-37114125">[-]</label><label class="expand" for="c-37114125">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty clear in the FAQ to be fair.</div><br/><div id="37117807" class="c"><input type="checkbox" id="c-37117807" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37114125">parent</a><span>|</span><a href="#37112881">next</a><span>|</span><label class="collapse" for="c-37117807">[-]</label><label class="expand" for="c-37117807">[1 more]</label></div><br/><div class="children"><div class="content">The comment you are responding to is sarcastic</div><br/></div></div></div></div></div></div><div id="37112881" class="c"><input type="checkbox" id="c-37112881" checked=""/><div class="controls bullet"><span class="by">froggychairs</span><span>|</span><a href="#37112819">parent</a><span>|</span><a href="#37112919">prev</a><span>|</span><a href="#37112916">next</a><span>|</span><label class="collapse" for="c-37112881">[-]</label><label class="expand" for="c-37112881">[1 more]</label></div><br/><div class="children"><div class="content">I believe it’s implying the free ChatGPT collects data and this one doesn’t.</div><br/></div></div><div id="37112916" class="c"><input type="checkbox" id="c-37112916" checked=""/><div class="controls bullet"><span class="by">nwoli</span><span>|</span><a href="#37112819">parent</a><span>|</span><a href="#37112881">prev</a><span>|</span><a href="#37113638">next</a><span>|</span><label class="collapse" for="c-37112916">[-]</label><label class="expand" for="c-37112916">[3 more]</label></div><br/><div class="children"><div class="content">I thought sama said they don’t use data going through the api for training. Guess we can’t trust that statement</div><br/><div id="37113156" class="c"><input type="checkbox" id="c-37113156" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37112916">parent</a><span>|</span><a href="#37113638">next</a><span>|</span><label class="collapse" for="c-37113156">[-]</label><label class="expand" for="c-37113156">[2 more]</label></div><br/><div class="children"><div class="content">That is correct, they do not use the data going through the API for training, but they do use the data from the web and mobile interfaces (unless you explicitly turn it off).</div><br/><div id="37115125" class="c"><input type="checkbox" id="c-37115125" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113156">parent</a><span>|</span><a href="#37113638">next</a><span>|</span><label class="collapse" for="c-37115125">[-]</label><label class="expand" for="c-37115125">[1 more]</label></div><br/><div class="children"><div class="content">“We don’t water down your beer”.<p>Oh nice!<p>“But that is lager”</div><br/></div></div></div></div></div></div><div id="37113638" class="c"><input type="checkbox" id="c-37113638" checked=""/><div class="controls bullet"><span class="by">jensen2k</span><span>|</span><a href="#37112819">parent</a><span>|</span><a href="#37112916">prev</a><span>|</span><a href="#37113111">next</a><span>|</span><label class="collapse" for="c-37113638">[-]</label><label class="expand" for="c-37113638">[1 more]</label></div><br/><div class="children"><div class="content">Another thing is that using ChatGPT for European companies might be in violation with GDPR – Azure OpenAI Services are available on European servers.</div><br/></div></div><div id="37113111" class="c"><input type="checkbox" id="c-37113111" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#37112819">parent</a><span>|</span><a href="#37113638">prev</a><span>|</span><a href="#37112958">next</a><span>|</span><label class="collapse" for="c-37113111">[-]</label><label class="expand" for="c-37113111">[2 more]</label></div><br/><div class="children"><div class="content">No<p>Edit: yes</div><br/><div id="37114440" class="c"><input type="checkbox" id="c-37114440" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#37112819">root</a><span>|</span><a href="#37113111">parent</a><span>|</span><a href="#37112958">next</a><span>|</span><label class="collapse" for="c-37114440">[-]</label><label class="expand" for="c-37114440">[1 more]</label></div><br/><div class="children"><div class="content">I just love this comment.</div><br/></div></div></div></div></div></div><div id="37118212" class="c"><input type="checkbox" id="c-37118212" checked=""/><div class="controls bullet"><span class="by">mteigers</span><span>|</span><a href="#37112819">prev</a><span>|</span><a href="#37114101">next</a><span>|</span><label class="collapse" for="c-37118212">[-]</label><label class="expand" for="c-37118212">[1 more]</label></div><br/><div class="children"><div class="content">Anyone have any thoughts as to ballpark costs to run this? My napkin math on the cosmo-db requirements is failing me (largely because I do not know Azure at all).<p>I&#x27;m wondering as a hobbyist &#x2F; tinkerer if a solution like this is &quot;affordable&quot; (I know it&#x27;s all relative)</div><br/></div></div><div id="37114101" class="c"><input type="checkbox" id="c-37114101" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#37118212">prev</a><span>|</span><a href="#37114396">next</a><span>|</span><label class="collapse" for="c-37114101">[-]</label><label class="expand" for="c-37114101">[6 more]</label></div><br/><div class="children"><div class="content">This seems like such an obvious thing to do.<p>I see the use of general purpose LLMs like ChatGPT, but smaller fine tuned models will probably end up being more useful for deployed applications in most companies. Off topic, but I was experimenting with LLongMA-2-7b-16K today, running it very inexpensively in the cloud, and given about 12K of context text it really performed well. This is an easy model to deploy. 7B parameter models can be useful.</div><br/><div id="37114983" class="c"><input type="checkbox" id="c-37114983" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#37114101">parent</a><span>|</span><a href="#37114396">next</a><span>|</span><label class="collapse" for="c-37114983">[-]</label><label class="expand" for="c-37114983">[5 more]</label></div><br/><div class="children"><div class="content">Is there an easy way to play with these models, as someone who hasn&#x27;t deployed them? I can download&#x2F;compile llama.cpp, but I don&#x27;t know which models to get&#x2F;where to put them&#x2F;how to run them, so if someone knows about some automated downloader along with some list of &quot;best models&quot;, that would be very helpful.</div><br/><div id="37115600" class="c"><input type="checkbox" id="c-37115600" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#37114101">root</a><span>|</span><a href="#37114983">parent</a><span>|</span><a href="#37115700">next</a><span>|</span><label class="collapse" for="c-37115600">[-]</label><label class="expand" for="c-37115600">[1 more]</label></div><br/><div class="children"><div class="content">If you want to try out the Llama-2 models (7B, 13B, 70B), you can get started very easily with Anyscale Endpoints (~2 min). <a href="https:&#x2F;&#x2F;app.endpoints.anyscale.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;app.endpoints.anyscale.com&#x2F;</a></div><br/></div></div><div id="37115700" class="c"><input type="checkbox" id="c-37115700" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#37114101">root</a><span>|</span><a href="#37114983">parent</a><span>|</span><a href="#37115600">prev</a><span>|</span><a href="#37114396">next</a><span>|</span><label class="collapse" for="c-37115700">[-]</label><label class="expand" for="c-37115700">[3 more]</label></div><br/><div class="children"><div class="content">For llama, the 4bit quantized ones, small models like the 7b one. The ggml format. That will run on your local cpu. Google those terms too. you can look on hugging face for the actual model to download then load it and send prompts to it</div><br/><div id="37115726" class="c"><input type="checkbox" id="c-37115726" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#37114101">root</a><span>|</span><a href="#37115700">parent</a><span>|</span><a href="#37114396">next</a><span>|</span><label class="collapse" for="c-37115726">[-]</label><label class="expand" for="c-37115726">[2 more]</label></div><br/><div class="children"><div class="content">Thanks, maybe it&#x27;s as easy as downloading the ggml and running it with Llama.cpp. I&#x27;ll try that, thanks!</div><br/><div id="37117290" class="c"><input type="checkbox" id="c-37117290" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#37114101">root</a><span>|</span><a href="#37115726">parent</a><span>|</span><a href="#37114396">next</a><span>|</span><label class="collapse" for="c-37117290">[-]</label><label class="expand" for="c-37117290">[1 more]</label></div><br/><div class="children"><div class="content">there is also a python wrapper that has a web ui built in for llama.cpp, if it wasnt easy enough already</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37114396" class="c"><input type="checkbox" id="c-37114396" checked=""/><div class="controls bullet"><span class="by">justinlloyd</span><span>|</span><a href="#37114101">prev</a><span>|</span><a href="#37114637">next</a><span>|</span><label class="collapse" for="c-37114396">[-]</label><label class="expand" for="c-37114396">[1 more]</label></div><br/><div class="children"><div class="content">Interesting release, though still lacking a few features I&#x27;ve had to resort building myself such as code summary, code base architecture summary, and conversation history summary.  ChatGPT (the web UI) now has the ability to execute code, and make function callbacks, but I prefer running that code locally, especially if I am debugging. This latter part, conversation history summary, is something that ChatGPT web UI does reasonably well, giving it a long history, but a sentiment extraction and salient detail extraction before summarizing is immensely useful for remembering details in the distant past. I&#x27;ve been building on top of the GPT4 model and tinkering with multi-model (gpt4 + davinci) usage too, though I am finding with the MoE that Davinci isn&#x27;t as important. Fine tuning has been helpful for specific code bases too.<p>If I had the time I&#x27;d like to play with an MoE of Llama2, as a compare and contrast, but that ain&#x27;t gonna happen anytime soon.</div><br/></div></div><div id="37114637" class="c"><input type="checkbox" id="c-37114637" checked=""/><div class="controls bullet"><span class="by">alpinemeadow</span><span>|</span><a href="#37114396">prev</a><span>|</span><a href="#37112964">next</a><span>|</span><label class="collapse" for="c-37114637">[-]</label><label class="expand" for="c-37114637">[3 more]</label></div><br/><div class="children"><div class="content">We have this at IKEA for a while now. Not impressed, but funny to read the hallucinations.</div><br/><div id="37117875" class="c"><input type="checkbox" id="c-37117875" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#37114637">parent</a><span>|</span><a href="#37114773">next</a><span>|</span><label class="collapse" for="c-37117875">[-]</label><label class="expand" for="c-37117875">[1 more]</label></div><br/><div class="children"><div class="content">Have you considered instruction-tuning it with text, instead of just pictures?</div><br/></div></div><div id="37114773" class="c"><input type="checkbox" id="c-37114773" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37114637">parent</a><span>|</span><a href="#37117875">prev</a><span>|</span><a href="#37112964">next</a><span>|</span><label class="collapse" for="c-37114773">[-]</label><label class="expand" for="c-37114773">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d expect a company like IKEA to have the expertise to create interfaces specific to their workflows so hallucinations aren&#x27;t an issue.<p>Imo if you&#x27;re making an open ended chat interface for a business, you&#x27;re doing it wrong.</div><br/></div></div></div></div><div id="37112964" class="c"><input type="checkbox" id="c-37112964" checked=""/><div class="controls bullet"><span class="by">coldblues</span><span>|</span><a href="#37114637">prev</a><span>|</span><a href="#37117685">next</a><span>|</span><label class="collapse" for="c-37112964">[-]</label><label class="expand" for="c-37112964">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure Azure has a moderation endpoint enabled by default that makes using the OpenAI API an awful experience.</div><br/></div></div><div id="37117685" class="c"><input type="checkbox" id="c-37117685" checked=""/><div class="controls bullet"><span class="by">roymj88</span><span>|</span><a href="#37112964">prev</a><span>|</span><a href="#37118435">next</a><span>|</span><label class="collapse" for="c-37117685">[-]</label><label class="expand" for="c-37117685">[1 more]</label></div><br/><div class="children"><div class="content">It was really good when the access was enabled via OpenAI, but ever since its moved to Azure subscription, getting preview access is stalled. Wouldn&#x27;t be a big deal for others, but for smalltime devs like me it becomes a big challenge.. Hope OpenAI provides a developer env or so where we can try things out..</div><br/></div></div><div id="37118435" class="c"><input type="checkbox" id="c-37118435" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#37117685">prev</a><span>|</span><a href="#37112922">next</a><span>|</span><label class="collapse" for="c-37118435">[-]</label><label class="expand" for="c-37118435">[2 more]</label></div><br/><div class="children"><div class="content">Anyone know what the cost is for Azure VS OpenAI?</div><br/><div id="37118444" class="c"><input type="checkbox" id="c-37118444" checked=""/><div class="controls bullet"><span class="by">djtriptych</span><span>|</span><a href="#37118435">parent</a><span>|</span><a href="#37112922">next</a><span>|</span><label class="collapse" for="c-37118444">[-]</label><label class="expand" for="c-37118444">[1 more]</label></div><br/><div class="children"><div class="content">I believe the prices are identical:<p><a href="https:&#x2F;&#x2F;azure.microsoft.com&#x2F;en-us&#x2F;pricing&#x2F;details&#x2F;cognitive-services&#x2F;openai-service&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;azure.microsoft.com&#x2F;en-us&#x2F;pricing&#x2F;details&#x2F;cognitive-...</a><p><a href="https:&#x2F;&#x2F;openai.com&#x2F;pricing" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;pricing</a><p>disclaimer&#x2F;source: I work at Microsoft on Azure&#x2F;OpenAI</div><br/></div></div></div></div><div id="37112922" class="c"><input type="checkbox" id="c-37112922" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#37118435">prev</a><span>|</span><a href="#37117687">next</a><span>|</span><label class="collapse" for="c-37112922">[-]</label><label class="expand" for="c-37112922">[1 more]</label></div><br/><div class="children"><div class="content">Yeah sure, I totally trust you after the Storm-0558 desaster</div><br/></div></div><div id="37117687" class="c"><input type="checkbox" id="c-37117687" checked=""/><div class="controls bullet"><span class="by">cpill</span><span>|</span><a href="#37112922">prev</a><span>|</span><a href="#37113641">next</a><span>|</span><label class="collapse" for="c-37117687">[-]</label><label class="expand" for="c-37117687">[1 more]</label></div><br/><div class="children"><div class="content">IS it possible for someone to give us the lower bound on the cost of running a 70B model in the cloud? How much memory does Llamba-2 take? What would it cost to fine tune it?</div><br/></div></div><div id="37113641" class="c"><input type="checkbox" id="c-37113641" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#37117687">prev</a><span>|</span><a href="#37115067">next</a><span>|</span><label class="collapse" for="c-37113641">[-]</label><label class="expand" for="c-37113641">[4 more]</label></div><br/><div class="children"><div class="content">Darn I just spent a week or so working on a ChatGPT clone that used Azure ChatGPT API due to the privacy aspect. Wasted effort I guess.</div><br/><div id="37114801" class="c"><input type="checkbox" id="c-37114801" checked=""/><div class="controls bullet"><span class="by">saliagato</span><span>|</span><a href="#37113641">parent</a><span>|</span><a href="#37115067">next</a><span>|</span><label class="collapse" for="c-37114801">[-]</label><label class="expand" for="c-37114801">[3 more]</label></div><br/><div class="children"><div class="content">This is exactly the same</div><br/><div id="37115352" class="c"><input type="checkbox" id="c-37115352" checked=""/><div class="controls bullet"><span class="by">ddmma</span><span>|</span><a href="#37113641">root</a><span>|</span><a href="#37114801">parent</a><span>|</span><a href="#37115067">next</a><span>|</span><label class="collapse" for="c-37115352">[-]</label><label class="expand" for="c-37115352">[2 more]</label></div><br/><div class="children"><div class="content">Welcome to the club :)</div><br/><div id="37117236" class="c"><input type="checkbox" id="c-37117236" checked=""/><div class="controls bullet"><span class="by">jeremycarter</span><span>|</span><a href="#37113641">root</a><span>|</span><a href="#37115352">parent</a><span>|</span><a href="#37115067">next</a><span>|</span><label class="collapse" for="c-37117236">[-]</label><label class="expand" for="c-37117236">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also in this club but we wrote it months ago.</div><br/></div></div></div></div></div></div></div></div><div id="37115067" class="c"><input type="checkbox" id="c-37115067" checked=""/><div class="controls bullet"><span class="by">H8crilA</span><span>|</span><a href="#37113641">prev</a><span>|</span><a href="#37117795">next</a><span>|</span><label class="collapse" for="c-37115067">[-]</label><label class="expand" for="c-37115067">[4 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the practical difference between this and OpenAI API?<p>All I can see is the same product but offered by a larger organization. I.e. they&#x27;re more likely to get the security details right, and you can potentially win more in a lawsuit should things go bad.</div><br/><div id="37115173" class="c"><input type="checkbox" id="c-37115173" checked=""/><div class="controls bullet"><span class="by">ebiester</span><span>|</span><a href="#37115067">parent</a><span>|</span><a href="#37115202">next</a><span>|</span><label class="collapse" for="c-37115173">[-]</label><label class="expand" for="c-37115173">[1 more]</label></div><br/><div class="children"><div class="content">Compliance and customer trust. Azure can sign a BAA, for example. If you are Building LLM capability on top of your SaaS, your customers want assurances about their data.</div><br/></div></div><div id="37115202" class="c"><input type="checkbox" id="c-37115202" checked=""/><div class="controls bullet"><span class="by">jeffschofield</span><span>|</span><a href="#37115067">parent</a><span>|</span><a href="#37115173">prev</a><span>|</span><a href="#37117795">next</a><span>|</span><label class="collapse" for="c-37115202">[-]</label><label class="expand" for="c-37115202">[2 more]</label></div><br/><div class="children"><div class="content">A few months ago my team moved to Azure for capacity reasons. We were constantly dealing with 429 errors and couldn&#x27;t get in touch with Open AI, while Azure offered more instances.<p>Eventually got more from Open AI so we load balance both. The only difference is the 3.5 turbo model on Azure is outdated.</div><br/><div id="37116782" class="c"><input type="checkbox" id="c-37116782" checked=""/><div class="controls bullet"><span class="by">RockyMcNuts</span><span>|</span><a href="#37115067">root</a><span>|</span><a href="#37115202">parent</a><span>|</span><a href="#37117795">next</a><span>|</span><label class="collapse" for="c-37116782">[-]</label><label class="expand" for="c-37116782">[1 more]</label></div><br/><div class="children"><div class="content">you can ask for gpt-4, it took a while due to capacity constraints but we got it</div><br/></div></div></div></div></div></div><div id="37117795" class="c"><input type="checkbox" id="c-37117795" checked=""/><div class="controls bullet"><span class="by">sagarpatil</span><span>|</span><a href="#37115067">prev</a><span>|</span><a href="#37115638">next</a><span>|</span><label class="collapse" for="c-37117795">[-]</label><label class="expand" for="c-37117795">[1 more]</label></div><br/><div class="children"><div class="content">Azure API is definitely faster than OpenAI and they also seem to provide access to 32k generously compared to OAI.</div><br/></div></div><div id="37115638" class="c"><input type="checkbox" id="c-37115638" checked=""/><div class="controls bullet"><span class="by">tananaev</span><span>|</span><a href="#37117795">prev</a><span>|</span><a href="#37115165">next</a><span>|</span><label class="collapse" for="c-37115638">[-]</label><label class="expand" for="c-37115638">[5 more]</label></div><br/><div class="children"><div class="content">This is not ChatGPT. It&#x27;s just a front end for Azure OpenAI APIs. Not sure why they&#x27;re so blatantly use the trademark. They will probably have to rename it soon.</div><br/><div id="37115782" class="c"><input type="checkbox" id="c-37115782" checked=""/><div class="controls bullet"><span class="by">mkinsella</span><span>|</span><a href="#37115638">parent</a><span>|</span><a href="#37116944">next</a><span>|</span><label class="collapse" for="c-37115782">[-]</label><label class="expand" for="c-37115782">[1 more]</label></div><br/><div class="children"><div class="content">Microsoft is a major investor in OpenAI. Guaranteed they worked with OpenAI on this and have partnership to use the trademarks.</div><br/></div></div><div id="37116944" class="c"><input type="checkbox" id="c-37116944" checked=""/><div class="controls bullet"><span class="by">white_dragon88</span><span>|</span><a href="#37115638">parent</a><span>|</span><a href="#37115782">prev</a><span>|</span><a href="#37115791">next</a><span>|</span><label class="collapse" for="c-37116944">[-]</label><label class="expand" for="c-37116944">[1 more]</label></div><br/><div class="children"><div class="content">Wow, hot takes on the internet by clueless people.</div><br/></div></div><div id="37115791" class="c"><input type="checkbox" id="c-37115791" checked=""/><div class="controls bullet"><span class="by">castrodd</span><span>|</span><a href="#37115638">parent</a><span>|</span><a href="#37116944">prev</a><span>|</span><a href="#37115165">next</a><span>|</span><label class="collapse" for="c-37115791">[-]</label><label class="expand" for="c-37115791">[2 more]</label></div><br/><div class="children"><div class="content">Microsoft owns OpenAI so I doubt that they will be asked to rename this.</div><br/><div id="37116759" class="c"><input type="checkbox" id="c-37116759" checked=""/><div class="controls bullet"><span class="by">mythz</span><span>|</span><a href="#37115638">root</a><span>|</span><a href="#37115791">parent</a><span>|</span><a href="#37115165">next</a><span>|</span><label class="collapse" for="c-37116759">[-]</label><label class="expand" for="c-37116759">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;ll only own 49% of shares.</div><br/></div></div></div></div></div></div><div id="37115165" class="c"><input type="checkbox" id="c-37115165" checked=""/><div class="controls bullet"><span class="by">atlgator</span><span>|</span><a href="#37115638">prev</a><span>|</span><a href="#37117610">next</a><span>|</span><label class="collapse" for="c-37115165">[-]</label><label class="expand" for="c-37115165">[1 more]</label></div><br/><div class="children"><div class="content">Is this a full, standalone deployment including GPT-3 (or whatever version) or just a secured frontend that sends data to GPT hosted outside the enterprise zone?<p>Edit: Uses Azure OpenAI as the backend</div><br/></div></div><div id="37117610" class="c"><input type="checkbox" id="c-37117610" checked=""/><div class="controls bullet"><span class="by">jhoechtl</span><span>|</span><a href="#37115165">prev</a><span>|</span><a href="#37113214">next</a><span>|</span><label class="collapse" for="c-37117610">[-]</label><label class="expand" for="c-37117610">[3 more]</label></div><br/><div class="children"><div class="content">Can any one shed light on what &quot;local&quot; means? Local on my own private machine or local in my Azure Tenant?</div><br/><div id="37117633" class="c"><input type="checkbox" id="c-37117633" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#37117610">parent</a><span>|</span><a href="#37117632">next</a><span>|</span><label class="collapse" for="c-37117633">[-]</label><label class="expand" for="c-37117633">[1 more]</label></div><br/><div class="children"><div class="content">Assuming you are referring to this section: <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;azurechatgpt&#x2F;blob&#x2F;main&#x2F;docs&#x2F;3-run-locally.md">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;azurechatgpt&#x2F;blob&#x2F;main&#x2F;docs&#x2F;3-r...</a><p>It means you run the front end (the chat-gui) and the backend code from the repo. This code connects to cosmo-db for uploading documents used for &quot;chat with you pdf&quot; and connects to an OpenAI instance on Azure for the chat inferrence.</div><br/></div></div><div id="37117632" class="c"><input type="checkbox" id="c-37117632" checked=""/><div class="controls bullet"><span class="by">jzombie</span><span>|</span><a href="#37117610">parent</a><span>|</span><a href="#37117633">prev</a><span>|</span><a href="#37113214">next</a><span>|</span><label class="collapse" for="c-37117632">[-]</label><label class="expand" for="c-37117632">[1 more]</label></div><br/><div class="children"><div class="content">I am pretty sure it means run the UI locally and access Azure-hosted ChatGPT.  The environment vars seem to indicate that as well.</div><br/></div></div></div></div><div id="37113214" class="c"><input type="checkbox" id="c-37113214" checked=""/><div class="controls bullet"><span class="by">bouke</span><span>|</span><a href="#37117610">prev</a><span>|</span><a href="#37117729">next</a><span>|</span><label class="collapse" for="c-37113214">[-]</label><label class="expand" for="c-37113214">[6 more]</label></div><br/><div class="children"><div class="content">How is this different from the other OpenAI GUI? Why another one by Microsoft? <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;sample-app-aoai-chatGPT">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;sample-app-aoai-chatGPT</a>.</div><br/><div id="37113302" class="c"><input type="checkbox" id="c-37113302" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#37113214">parent</a><span>|</span><a href="#37113475">next</a><span>|</span><label class="collapse" for="c-37113302">[-]</label><label class="expand" for="c-37113302">[1 more]</label></div><br/><div class="children"><div class="content">I bet there are plenty of OKR&#x2F;KPIs now tied to AI at Microsoft.</div><br/></div></div><div id="37113475" class="c"><input type="checkbox" id="c-37113475" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#37113214">parent</a><span>|</span><a href="#37113302">prev</a><span>|</span><a href="#37113495">next</a><span>|</span><label class="collapse" for="c-37113475">[-]</label><label class="expand" for="c-37113475">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s at least two more. There&#x27;s also <a href="https:&#x2F;&#x2F;github.com&#x2F;Azure-Samples&#x2F;azure-search-openai-demo">https:&#x2F;&#x2F;github.com&#x2F;Azure-Samples&#x2F;azure-search-openai-demo</a><p>And you can deploy a chat bot from within the Azure playground which runs on another codebase.</div><br/></div></div><div id="37113495" class="c"><input type="checkbox" id="c-37113495" checked=""/><div class="controls bullet"><span class="by">colonwqbang</span><span>|</span><a href="#37113214">parent</a><span>|</span><a href="#37113475">prev</a><span>|</span><a href="#37113741">next</a><span>|</span><label class="collapse" for="c-37113495">[-]</label><label class="expand" for="c-37113495">[1 more]</label></div><br/><div class="children"><div class="content">Bigger companies are cautious about using GPT-style products due to data security concerns. But most big companies trust Microsoft more or less blindly.<p>Now that Microsoft has an official &quot;enterprise&quot; version out, the floodgates are open. They stand to make a killing.</div><br/></div></div><div id="37113741" class="c"><input type="checkbox" id="c-37113741" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#37113214">parent</a><span>|</span><a href="#37113495">prev</a><span>|</span><a href="#37113233">next</a><span>|</span><label class="collapse" for="c-37113741">[-]</label><label class="expand" for="c-37113741">[1 more]</label></div><br/><div class="children"><div class="content">This is an internal ChatGPT, whereas that sample is ChatGPT constrained to internal search results (using RAG approach). Source: I help maintain the RAG samples.</div><br/></div></div><div id="37113233" class="c"><input type="checkbox" id="c-37113233" checked=""/><div class="controls bullet"><span class="by">FrenchDevRemote</span><span>|</span><a href="#37113214">parent</a><span>|</span><a href="#37113741">prev</a><span>|</span><a href="#37117729">next</a><span>|</span><label class="collapse" for="c-37113233">[-]</label><label class="expand" for="c-37113233">[1 more]</label></div><br/><div class="children"><div class="content">i&#x27;m pretty sure it&#x27;s a part of it</div><br/></div></div></div></div><div id="37117729" class="c"><input type="checkbox" id="c-37117729" checked=""/><div class="controls bullet"><span class="by">didibus</span><span>|</span><a href="#37113214">prev</a><span>|</span><a href="#37114807">next</a><span>|</span><label class="collapse" for="c-37117729">[-]</label><label class="expand" for="c-37117729">[1 more]</label></div><br/><div class="children"><div class="content">When will LLMs be good enough to write the code for a competitive or better LLM to themselves?</div><br/></div></div><div id="37114807" class="c"><input type="checkbox" id="c-37114807" checked=""/><div class="controls bullet"><span class="by">padolsey</span><span>|</span><a href="#37117729">prev</a><span>|</span><a href="#37115009">next</a><span>|</span><label class="collapse" for="c-37114807">[-]</label><label class="expand" for="c-37114807">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m confused. If this is just a front-end for the OpenAI API then how does it remove the data privacy concern? Your data still ends up with Azure&#x2F;OpenAI, right? It doesn&#x27;t stay localized to your instance; it&#x27;s not your GPU running the transformations. You have no way of knowing whether your data is being used to train models. If customer data is sensitive, I&#x27;m pretty sure running a 70B llama (or similar) on a bunch of A100s is the only way?</div><br/><div id="37114842" class="c"><input type="checkbox" id="c-37114842" checked=""/><div class="controls bullet"><span class="by">dbish</span><span>|</span><a href="#37114807">parent</a><span>|</span><a href="#37115009">next</a><span>|</span><label class="collapse" for="c-37114842">[-]</label><label class="expand" for="c-37114842">[6 more]</label></div><br/><div class="children"><div class="content">Azure is hosting and operating the service themselves rather then for OpenAI, with all the security requirements that come with that. I assume this comes with different data and access restrictions as well and ability to run in secured instances (and nothing sent to OpenAI the company).<p>Most companies use cloud already for their data, processing, etc. and aren’t running anything major locally, let alone ML models, this is putting trust in the cloud they already use.</div><br/><div id="37114889" class="c"><input type="checkbox" id="c-37114889" checked=""/><div class="controls bullet"><span class="by">padolsey</span><span>|</span><a href="#37114807">root</a><span>|</span><a href="#37114842">parent</a><span>|</span><a href="#37115694">next</a><span>|</span><label class="collapse" for="c-37114889">[-]</label><label class="expand" for="c-37114889">[2 more]</label></div><br/><div class="children"><div class="content">Ah that&#x27;s fair. But it is my impression that the bulk of privacy&#x2F;confidentiality concerns (e.g. law&#x2F;health&#x2F;..) would require &quot;end to end&quot; data safety. Not sure if I&#x27;m making sense. I guess microsoft is somehow more trustworthy than openai themselves...<p>EDIT: what you say about existing cloud customers being able to extend their trust to this new thing makes sense, thanks.</div><br/><div id="37115214" class="c"><input type="checkbox" id="c-37115214" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#37114807">root</a><span>|</span><a href="#37114889">parent</a><span>|</span><a href="#37115694">next</a><span>|</span><label class="collapse" for="c-37115214">[-]</label><label class="expand" for="c-37115214">[1 more]</label></div><br/><div class="children"><div class="content">Right. If I was an European company worried about, say, industrial espionage, this wouldn&#x27;t be nearly enough to reassure me.</div><br/></div></div></div></div><div id="37114885" class="c"><input type="checkbox" id="c-37114885" checked=""/><div class="controls bullet"><span class="by">nmstoker</span><span>|</span><a href="#37114807">root</a><span>|</span><a href="#37114842">parent</a><span>|</span><a href="#37115685">prev</a><span>|</span><a href="#37115009">next</a><span>|</span><label class="collapse" for="c-37114885">[-]</label><label class="expand" for="c-37114885">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this was my understanding.</div><br/></div></div></div></div></div></div><div id="37115009" class="c"><input type="checkbox" id="c-37115009" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#37114807">prev</a><span>|</span><label class="collapse" for="c-37115009">[-]</label><label class="expand" for="c-37115009">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>However, ChatGPT risks exposing confidential intellectual property. One option is to block corporate access to ChatGPT, but people always find workarounds</i><p>Pretty bold thing to say to your potential clients. &quot;You can always tell your employees not to use our product, but they won&#x27;t listen to you.&quot;</div><br/><div id="37116042" class="c"><input type="checkbox" id="c-37116042" checked=""/><div class="controls bullet"><span class="by">pwarner</span><span>|</span><a href="#37115009">parent</a><span>|</span><label class="collapse" for="c-37116042">[-]</label><label class="expand" for="c-37116042">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s almost like employees might have their own computers?</div><br/></div></div></div></div></div></div></div></div></div></body></html>