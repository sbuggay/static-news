<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708160453223" as="style"/><link rel="stylesheet" href="styles.css?v=1708160453223"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.09171">Automated Unit Test Improvement Using Large Language Models at Meta</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>mfiguiere</span> | <span>58 comments</span></div><br/><div><div id="39407128" class="c"><input type="checkbox" id="c-39407128" checked=""/><div class="controls bullet"><span class="by">hubraumhugo</span><span>|</span><a href="#39406816">next</a><span>|</span><label class="collapse" for="c-39407128">[-]</label><label class="expand" for="c-39407128">[20 more]</label></div><br/><div class="children"><div class="content">At a large insurance company I worked for, management set a target of 80% test coverage across our entire codebase.  
So people started writing stupid unit tests for getters and setters in Java DTOs to reach the goal. Of course devs also weren&#x27;t allowed to change the coverage measuring rules in Sonar.<p>As a young dev, it taught me that focusing only on KPIs can sometimes drive behaviors that don&#x27;t align with the intended goals. 
A few well-thought out E2E test scenarios would probably have had a better impact on the software quality.</div><br/><div id="39407557" class="c"><input type="checkbox" id="c-39407557" checked=""/><div class="controls bullet"><span class="by">wilgertvelinga</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407236">next</a><span>|</span><label class="collapse" for="c-39407557">[-]</label><label class="expand" for="c-39407557">[1 more]</label></div><br/><div class="children"><div class="content">The solution to that is mutation tests. They force your tests to actually verify the implementation instead of just running the code to fake coverage.
<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Mutation_testing" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Mutation_testing</a>
Tools and frameworks exist for almost all languages.
Some examples:<p>- stryker-mutator (C#, Typescript)<p>- pitest (Java)<p>- mutatest (Python)</div><br/></div></div><div id="39407236" class="c"><input type="checkbox" id="c-39407236" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407557">prev</a><span>|</span><a href="#39407321">next</a><span>|</span><label class="collapse" for="c-39407236">[-]</label><label class="expand" for="c-39407236">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Goodhart%27s_law" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Goodhart%27s_law</a></div><br/><div id="39407277" class="c"><input type="checkbox" id="c-39407277" checked=""/><div class="controls bullet"><span class="by">hubraumhugo</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407236">parent</a><span>|</span><a href="#39407321">next</a><span>|</span><label class="collapse" for="c-39407277">[-]</label><label class="expand" for="c-39407277">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, I was looking for that name.<p>Interesting examples:<p>&gt; San Francisco Declaration on Research Assessment – 2012 manifesto against using the journal impact factor to assess a scientist&#x27;s work. The statement denounces several problems in science and as Goodhart&#x27;s law explains, one of them is that measurement has become a target. The correlation between h-index and scientific awards is decreasing since widespread usage of h-index.<p>&gt; International Union for Conservation of Nature&#x27;s measure of extinction can be used to remove environmental protections, which resulted in IUCN becoming more conservative in labeling something as extinct</div><br/></div></div></div></div><div id="39407321" class="c"><input type="checkbox" id="c-39407321" checked=""/><div class="controls bullet"><span class="by">danielheath</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407236">prev</a><span>|</span><a href="#39407254">next</a><span>|</span><label class="collapse" for="c-39407321">[-]</label><label class="expand" for="c-39407321">[3 more]</label></div><br/><div class="children"><div class="content">IME, the only &quot;test coverage % rule&quot; that I&#x27;ve ever seen work was &quot;must not decrease the overall percentage of tested code&quot;. Once you get to 100%, that becomes &quot;All code must have a test&quot;.<p>Various people objected to this, pointing out that 100% test coverage tells you nothing about whether the tests are any good. Our lead (wisely, IMO) responded that they were correct - 100% tells you nothing - but that _any other percentage_ does tell you something.</div><br/><div id="39407536" class="c"><input type="checkbox" id="c-39407536" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407321">parent</a><span>|</span><a href="#39407659">next</a><span>|</span><label class="collapse" for="c-39407536">[-]</label><label class="expand" for="c-39407536">[1 more]</label></div><br/><div class="children"><div class="content">What tells you something is how many releases get rolled back as a % and how much rework you have. If your developers need 3 attempts to put something working in production that&#x27;s all you need to know about the quality of the reviews and tests. High or low coverage, what you need to look at is actual issues.<p>All you need is a bit hyperbolic, because you also need to quarantine flaky tests and other things, but coverage as a whole I think is useful only if you have an engineering organization that doesn&#x27;t see the point in tests - but that&#x27;s going to be an uphill battle.</div><br/></div></div><div id="39407659" class="c"><input type="checkbox" id="c-39407659" checked=""/><div class="controls bullet"><span class="by">ponector</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407321">parent</a><span>|</span><a href="#39407536">prev</a><span>|</span><a href="#39407254">next</a><span>|</span><label class="collapse" for="c-39407659">[-]</label><label class="expand" for="c-39407659">[1 more]</label></div><br/><div class="children"><div class="content">I had a colleague who wrote unit tests without any assertions. Perfect idea! 100% coverage and always green.<p>Another comment here mentioned mutation tests which could be a solution to increase quality of unit testing, but I&#x27;ve never seen anyone to actually use it in enterprise development. Same story with test driven development concept.</div><br/></div></div></div></div><div id="39407254" class="c"><input type="checkbox" id="c-39407254" checked=""/><div class="controls bullet"><span class="by">oneshtein</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407321">prev</a><span>|</span><a href="#39407371">next</a><span>|</span><label class="collapse" for="c-39407254">[-]</label><label class="expand" for="c-39407254">[6 more]</label></div><br/><div class="children"><div class="content">Is it hard to write ONE test case for ALL getters and setters using reflection?</div><br/><div id="39407280" class="c"><input type="checkbox" id="c-39407280" checked=""/><div class="controls bullet"><span class="by">reactordev</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407254">parent</a><span>|</span><a href="#39407304">next</a><span>|</span><label class="collapse" for="c-39407280">[-]</label><label class="expand" for="c-39407280">[3 more]</label></div><br/><div class="children"><div class="content">at that point what are you testing? allocation&#x2F;deallocation? Nothing is actually happening unless you depend on constructors&#x2F;destructors when you use reflection to gen a mock type or real value type to inject. Or do you mean just assert true that all getters are not null?<p>I agree that unit testing for the sake of KPI&#x27;s is the wrong approach and unit testing functionality (as a means of documenting it, proving it still works as intended) is far better.</div><br/><div id="39407593" class="c"><input type="checkbox" id="c-39407593" checked=""/><div class="controls bullet"><span class="by">oneshtein</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407280">parent</a><span>|</span><a href="#39407304">next</a><span>|</span><label class="collapse" for="c-39407593">[-]</label><label class="expand" for="c-39407593">[2 more]</label></div><br/><div class="children"><div class="content">&gt; at that point what are you testing?<p>Memory.<p>&gt; Nothing is actually happening unless you depend on constructors&#x2F;destructors when you use reflection to gen a mock type or real value type to inject.<p>Just get an object from IoC container, then test that getFoo() == getFoo(setFoo(getFoo())) for every Foo with get and set methods, so you will have 80% of coverage for those getters and setters. For read-only properties, just get value and throw it away.<p>&gt; I agree that unit testing for the sake of KPI&#x27;s is the wrong approach and unit testing functionality (as a means of documenting it, proving it still works as intended) is far better.<p>Unit tests are for proving correctness of logic in a unit of code. Data structures are not logic. IMHO, such trivial parts of program should be ignored by coverage tool, but topic starter said that they cannot change rules. :-&#x2F;</div><br/><div id="39407680" class="c"><input type="checkbox" id="c-39407680" checked=""/><div class="controls bullet"><span class="by">growse</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407593">parent</a><span>|</span><a href="#39407304">next</a><span>|</span><label class="collapse" for="c-39407680">[-]</label><label class="expand" for="c-39407680">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; at that point what are you testing?<p>&gt; Memory.<p>In unit testing, the &quot;memory&quot; is not the system under test though.</div><br/></div></div></div></div></div></div><div id="39407304" class="c"><input type="checkbox" id="c-39407304" checked=""/><div class="controls bullet"><span class="by">ildjarn</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407254">parent</a><span>|</span><a href="#39407280">prev</a><span>|</span><a href="#39407371">next</a><span>|</span><label class="collapse" for="c-39407304">[-]</label><label class="expand" for="c-39407304">[2 more]</label></div><br/><div class="children"><div class="content">Simplest solution? Don’t have any setters.</div><br/><div id="39407406" class="c"><input type="checkbox" id="c-39407406" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407304">parent</a><span>|</span><a href="#39407371">next</a><span>|</span><label class="collapse" for="c-39407406">[-]</label><label class="expand" for="c-39407406">[1 more]</label></div><br/><div class="children"><div class="content">Removing them would reduce your coverage percentage.</div><br/></div></div></div></div></div></div><div id="39407371" class="c"><input type="checkbox" id="c-39407371" checked=""/><div class="controls bullet"><span class="by">dexwiz</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407254">prev</a><span>|</span><a href="#39407159">next</a><span>|</span><label class="collapse" for="c-39407371">[-]</label><label class="expand" for="c-39407371">[1 more]</label></div><br/><div class="children"><div class="content">I’ve started getting pinged for this at my current job. I think it’s time to move on.</div><br/></div></div><div id="39407159" class="c"><input type="checkbox" id="c-39407159" checked=""/><div class="controls bullet"><span class="by">farhanhubble</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407371">prev</a><span>|</span><a href="#39407308">next</a><span>|</span><label class="collapse" for="c-39407159">[-]</label><label class="expand" for="c-39407159">[1 more]</label></div><br/><div class="children"><div class="content">True of any metric when it becomes the goal in itself.</div><br/></div></div><div id="39407308" class="c"><input type="checkbox" id="c-39407308" checked=""/><div class="controls bullet"><span class="by">raverbashing</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407159">prev</a><span>|</span><a href="#39407309">next</a><span>|</span><label class="collapse" for="c-39407308">[-]</label><label class="expand" for="c-39407308">[3 more]</label></div><br/><div class="children"><div class="content">Java is the only place where (non-automatic&#x2F;non-syntax-sugared) getters and setters are though as important and valuable<p>Only goes to confirm my view of them that the language is deficient</div><br/><div id="39407625" class="c"><input type="checkbox" id="c-39407625" checked=""/><div class="controls bullet"><span class="by">CraigJPerry</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407308">parent</a><span>|</span><a href="#39407313">next</a><span>|</span><label class="collapse" for="c-39407625">[-]</label><label class="expand" for="c-39407625">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think it’s accurate to label that as a language problem.<p>That is very squarely a people problem.<p>It’s not as bad today - many Java juniors don’t have the “bean” affliction burned into their brains so they don’t object to public fields on data carriers (today you’d just use a record) but even the bean generation (mostly people my age) can usually be won over these days by negotiating with them on cases where getters&#x2F;setters can be eliminated (e.g. start with value objects, then suggest maybe DTOs then you can go for the kill - 
 why do we need the stupid Java bean convention?)</div><br/></div></div><div id="39407313" class="c"><input type="checkbox" id="c-39407313" checked=""/><div class="controls bullet"><span class="by">hubraumhugo</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407308">parent</a><span>|</span><a href="#39407625">prev</a><span>|</span><a href="#39407309">next</a><span>|</span><label class="collapse" for="c-39407313">[-]</label><label class="expand" for="c-39407313">[1 more]</label></div><br/><div class="children"><div class="content">Record types solved this in JDK 14</div><br/></div></div></div></div><div id="39407309" class="c"><input type="checkbox" id="c-39407309" checked=""/><div class="controls bullet"><span class="by">dclowd9901</span><span>|</span><a href="#39407128">parent</a><span>|</span><a href="#39407308">prev</a><span>|</span><a href="#39406816">next</a><span>|</span><label class="collapse" for="c-39407309">[-]</label><label class="expand" for="c-39407309">[2 more]</label></div><br/><div class="children"><div class="content">What if…<p>They knew that people would write coverage tests for getters and setters, and calculated that eventuality into their minimums.</div><br/><div id="39407351" class="c"><input type="checkbox" id="c-39407351" checked=""/><div class="controls bullet"><span class="by">mrich</span><span>|</span><a href="#39407128">root</a><span>|</span><a href="#39407309">parent</a><span>|</span><a href="#39406816">next</a><span>|</span><label class="collapse" for="c-39407351">[-]</label><label class="expand" for="c-39407351">[1 more]</label></div><br/><div class="children"><div class="content">So you&#x27;re saying they knew engineers would be wasting their time doing useless things, but still went ahead? (instead of mandating 75% and spending 1&#x2F;100 of the wasted time to adjust the metric to filter out getter&#x2F;setter)</div><br/></div></div></div></div></div></div><div id="39406816" class="c"><input type="checkbox" id="c-39406816" checked=""/><div class="controls bullet"><span class="by">tivert</span><span>|</span><a href="#39407128">prev</a><span>|</span><a href="#39406901">next</a><span>|</span><label class="collapse" for="c-39406816">[-]</label><label class="expand" for="c-39406816">[10 more]</label></div><br/><div class="children"><div class="content">&gt; 75% of TestGen-LLM&#x27;s test cases built correctly, 57% passed reliably, and 25% increased coverage.<p>The problem I have with LLM generated tests is that it seems highly likely that they&#x27;d &quot;ratify&quot; buggy behavior, and I&#x27;d think that&#x27;d be especially likely if the code-base already had low test coverage.  One of the nice things about writing new tests by hand is you&#x27;ve got someone who can judge if it&#x27;s the system being stupid or if it&#x27;s the test.<p>At a minimum they should be segregated in a special test folder, so they can be treated with an appropriate level of suspicion.</div><br/><div id="39407380" class="c"><input type="checkbox" id="c-39407380" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#39406816">parent</a><span>|</span><a href="#39407208">next</a><span>|</span><label class="collapse" for="c-39407380">[-]</label><label class="expand" for="c-39407380">[2 more]</label></div><br/><div class="children"><div class="content">Writing tests is indeed a great opportunity for finding bugs.<p>But a codebase with good test coverage allows you to safely perform large scale refactorings without having regressions and that&#x27;s useful property even if you have bugs and the refactoring preserves them faithfully.<p>The risk of using a tool that generates tests designed to encode the current behaviour is that you may be lulled in a false sense of safety, while all you&#x27;ve done is to encode the current behaviour, as advertised.<p>Perhaps this problem can be just solved by not calling these tests &quot;tests&quot; but something like &quot;behavioural snapshots&quot; or something like that (cannot think of a better name, but the idea is to capture the idea that they were not meant to encode necessarily the correct behaviour but just the current behaviour)</div><br/><div id="39407601" class="c"><input type="checkbox" id="c-39407601" checked=""/><div class="controls bullet"><span class="by">drothlis</span><span>|</span><a href="#39406816">root</a><span>|</span><a href="#39407380">parent</a><span>|</span><a href="#39407208">next</a><span>|</span><label class="collapse" for="c-39407601">[-]</label><label class="expand" for="c-39407601">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Characterization_test" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Characterization_test</a><p>aka snapshot tests.</div><br/></div></div></div></div><div id="39407208" class="c"><input type="checkbox" id="c-39407208" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#39406816">parent</a><span>|</span><a href="#39407380">prev</a><span>|</span><a href="#39407409">next</a><span>|</span><label class="collapse" for="c-39407208">[-]</label><label class="expand" for="c-39407208">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  One of the nice things about writing new tests by hand is you&#x27;ve got someone who can judge if it&#x27;s the system being stupid or if it&#x27;s the test.<p>This is an instance of a more general problem, which I call &quot;the problem of unwanted change&quot;. If you have an automated system which can change itself, how do you know a change is actually intended&#x2F;correct or merely a symptom of a bug, failure or imperfect knowledge the automation has?<p>That&#x27;s why I think human supervision is always needed to an extent, to determine what scenario has occured.<p>This happens in all sorts of systems. And people tend to think they can solve this with just another layer of automation like here. Testing was originally invented as a way to check if the program works correctly. If you automate it, you will face the same problem, just with a bigger code (in the form of tests rather than assertions).</div><br/></div></div><div id="39407409" class="c"><input type="checkbox" id="c-39407409" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#39406816">parent</a><span>|</span><a href="#39407208">prev</a><span>|</span><a href="#39407009">next</a><span>|</span><label class="collapse" for="c-39407409">[-]</label><label class="expand" for="c-39407409">[1 more]</label></div><br/><div class="children"><div class="content">In sufficiently large systems there is some value in test that just detect changed behavior, even if the behavior is buggy. Parts of the code probably rely on the bugs and accidentally (or intentionally) fixing them can lead to more severe problems.<p>Of course these kinds of tests are no replacement for tests that check actual requirements.</div><br/></div></div><div id="39407009" class="c"><input type="checkbox" id="c-39407009" checked=""/><div class="controls bullet"><span class="by">chii</span><span>|</span><a href="#39406816">parent</a><span>|</span><a href="#39407409">prev</a><span>|</span><a href="#39406989">next</a><span>|</span><label class="collapse" for="c-39407009">[-]</label><label class="expand" for="c-39407009">[2 more]</label></div><br/><div class="children"><div class="content">&gt; you&#x27;ve got someone who can judge if it&#x27;s the system being stupid or if it&#x27;s the test.<p>but why couldn&#x27;t this be done even with the llm generated test cases?</div><br/><div id="39407099" class="c"><input type="checkbox" id="c-39407099" checked=""/><div class="controls bullet"><span class="by">anoopelias</span><span>|</span><a href="#39406816">root</a><span>|</span><a href="#39407009">parent</a><span>|</span><a href="#39406989">next</a><span>|</span><label class="collapse" for="c-39407099">[-]</label><label class="expand" for="c-39407099">[1 more]</label></div><br/><div class="children"><div class="content">In that case, I think the point is the difference between what is LLM &quot;assisted&quot; tests (like say Copilot) vs. LLM &quot;owned&quot; tests.</div><br/></div></div></div></div><div id="39406989" class="c"><input type="checkbox" id="c-39406989" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39406816">parent</a><span>|</span><a href="#39407009">prev</a><span>|</span><a href="#39407136">next</a><span>|</span><label class="collapse" for="c-39406989">[-]</label><label class="expand" for="c-39406989">[1 more]</label></div><br/><div class="children"><div class="content">Keeping them separated would also improve future training.</div><br/></div></div><div id="39407136" class="c"><input type="checkbox" id="c-39407136" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39406816">parent</a><span>|</span><a href="#39406989">prev</a><span>|</span><a href="#39407111">next</a><span>|</span><label class="collapse" for="c-39407136">[-]</label><label class="expand" for="c-39407136">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, but that means you can really live Hyrum&#x27;s Law.</div><br/></div></div><div id="39407111" class="c"><input type="checkbox" id="c-39407111" checked=""/><div class="controls bullet"><span class="by">christkv</span><span>|</span><a href="#39406816">parent</a><span>|</span><a href="#39407136">prev</a><span>|</span><a href="#39406901">next</a><span>|</span><label class="collapse" for="c-39407111">[-]</label><label class="expand" for="c-39407111">[1 more]</label></div><br/><div class="children"><div class="content">I mean if I can use it to generate coverage for a method then prune the tests it will still save me hours.<p>Might also be useful when you want to refactor a legacy system or try to figure out the input space for a module, library or method.</div><br/></div></div></div></div><div id="39406901" class="c"><input type="checkbox" id="c-39406901" checked=""/><div class="controls bullet"><span class="by">planetjones</span><span>|</span><a href="#39406816">prev</a><span>|</span><a href="#39407531">next</a><span>|</span><label class="collapse" for="c-39406901">[-]</label><label class="expand" for="c-39406901">[5 more]</label></div><br/><div class="children"><div class="content">From reading the PDF it seems that this ‘merely’ generates tests that will repeatedly pass i.e. that are not flaky. The main purpose is to create a regression test suite by having tests that pin the behaviour of existing code. This isn’t a replacement for developer written tests, which one would hope come with the knowledge of what the functional requirement is.<p>Almost 20 years ago the company I worked for trialled AgitarOne - its promise was automagically generating test cases for Java code that help explore its behaviour. But also Agitar could create passing tests more or less automatically, which you could then use as a regression suite. Personally I never liked it, as it just led to too much stuff and it was something management didn’t really understand - to them if the test coverage had gone up then the quality must have too. I wonder how much better the LLM approach FB talk about here is compared to that though…<p><a href="http:&#x2F;&#x2F;www.agitar.com&#x2F;solutions&#x2F;products&#x2F;agitarone.html" rel="nofollow">http:&#x2F;&#x2F;www.agitar.com&#x2F;solutions&#x2F;products&#x2F;agitarone.html</a></div><br/><div id="39406911" class="c"><input type="checkbox" id="c-39406911" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#39406901">parent</a><span>|</span><a href="#39407596">next</a><span>|</span><label class="collapse" for="c-39406911">[-]</label><label class="expand" for="c-39406911">[3 more]</label></div><br/><div class="children"><div class="content">A lot of unit tests generated that way will simply be change detectors (fail when code changes) rather than regression tests (fail when bug is re-introduced). Those are pretty big distinctions, I don’t see LML’s getting here until they can ascertain tear correctness without just assuming good tests pass or depending on an oracle (the prompt will have to include behavior expectations somehow).</div><br/><div id="39407379" class="c"><input type="checkbox" id="c-39407379" checked=""/><div class="controls bullet"><span class="by">sixstringtheory</span><span>|</span><a href="#39406901">root</a><span>|</span><a href="#39406911">parent</a><span>|</span><a href="#39406933">next</a><span>|</span><label class="collapse" for="c-39407379">[-]</label><label class="expand" for="c-39407379">[1 more]</label></div><br/><div class="children"><div class="content">This articulates the problem I’m having right now in an interesting way. I’m fine writing unit tests that validate business logic requirements or bug fixes, but writing tests that validate implementations to the point that they reimplement the same logic is a bit much.<p>I want to figure out how to count the number of times a test has had to change with updated requirements vs how many defects they’ve prevented (vs how much wall clock time &#x2F; compute resources they’ve consumed in running them).</div><br/></div></div><div id="39406933" class="c"><input type="checkbox" id="c-39406933" checked=""/><div class="controls bullet"><span class="by">planetjones</span><span>|</span><a href="#39406901">root</a><span>|</span><a href="#39406911">parent</a><span>|</span><a href="#39407379">prev</a><span>|</span><a href="#39407596">next</a><span>|</span><label class="collapse" for="c-39406933">[-]</label><label class="expand" for="c-39406933">[1 more]</label></div><br/><div class="children"><div class="content">That’s what I believe Facebook have created here, so you’re right ‘regression’ is a big word - the tests are more likely detecting change e.g. by asserting the existing behaviour of conditionals previously not executed.</div><br/></div></div></div></div><div id="39407596" class="c"><input type="checkbox" id="c-39407596" checked=""/><div class="controls bullet"><span class="by">Ma8ee</span><span>|</span><a href="#39406901">parent</a><span>|</span><a href="#39406911">prev</a><span>|</span><a href="#39407531">next</a><span>|</span><label class="collapse" for="c-39407596">[-]</label><label class="expand" for="c-39407596">[1 more]</label></div><br/><div class="children"><div class="content">And it will lock the system into behaviour that might just be accidental. The value of tests is to make sure that you don&#x27;t break anything that anyone cares about, not that the every little never used edge case behaviour, which might just an artefact of a specific implementation, is locked in forever.</div><br/></div></div></div></div><div id="39407531" class="c"><input type="checkbox" id="c-39407531" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#39406901">prev</a><span>|</span><a href="#39407700">next</a><span>|</span><label class="collapse" for="c-39407531">[-]</label><label class="expand" for="c-39407531">[1 more]</label></div><br/><div class="children"><div class="content">I want to go the other way. Let me feed acceptance criteria in, have it generate tests that check them, and only then generate code that passes the tests.<p>You can get close to this with Copilot, sometimes, in a fairly limited way, but why do I feel like nobody is focusing on doing it that way round?</div><br/></div></div><div id="39407700" class="c"><input type="checkbox" id="c-39407700" checked=""/><div class="controls bullet"><span class="by">oslac</span><span>|</span><a href="#39407531">prev</a><span>|</span><a href="#39407345">next</a><span>|</span><label class="collapse" for="c-39407700">[-]</label><label class="expand" for="c-39407700">[1 more]</label></div><br/><div class="children"><div class="content">Is there still no type theoretic answer to unit testing? Does not the type or the class generally contain all the necessary information to unit test itself, assuming its a unit? That is, we should not have to even write these &quot;theoretically&quot;. Just hit &quot;compiler --unit_test &lt;type&gt;&quot;</div><br/></div></div><div id="39407345" class="c"><input type="checkbox" id="c-39407345" checked=""/><div class="controls bullet"><span class="by">shardullavekar</span><span>|</span><a href="#39407700">prev</a><span>|</span><a href="#39406850">next</a><span>|</span><label class="collapse" for="c-39407345">[-]</label><label class="expand" for="c-39407345">[1 more]</label></div><br/><div class="children"><div class="content">At unlogged.io, for some time - our primary focus was to auto-generate junit tests. The approach didn&#x27;t take off for a few reasons:
1. A Lot of generated test code that no devs wanted to maintain.
2. The generated tests didn&#x27;t simulate real-world scenarios. 
3. Code coverage was a vanity metric. Devs worked around to reach their goals with scenarios that didn&#x27;t matter.<p>We are currently working on offering no-code replay tests that simulate all unique production scenarios and developers can replay locally while mocking external dependencies.<p>Disclaimer: I am a founder at unlogged.io</div><br/></div></div><div id="39406850" class="c"><input type="checkbox" id="c-39406850" checked=""/><div class="controls bullet"><span class="by">TheChaplain</span><span>|</span><a href="#39407345">prev</a><span>|</span><a href="#39407323">next</a><span>|</span><label class="collapse" for="c-39406850">[-]</label><label class="expand" for="c-39406850">[1 more]</label></div><br/><div class="children"><div class="content">In my experience writing tests is generally an outstanding method to determine code quality.<p>If a test is complicated or coverage is hard to achieve, it&#x27;s likely your tested code needs improvement.</div><br/></div></div><div id="39407323" class="c"><input type="checkbox" id="c-39407323" checked=""/><div class="controls bullet"><span class="by">curtis3389</span><span>|</span><a href="#39406850">prev</a><span>|</span><a href="#39406608">next</a><span>|</span><label class="collapse" for="c-39407323">[-]</label><label class="expand" for="c-39407323">[2 more]</label></div><br/><div class="children"><div class="content">TestGen-LLM is such a strange creation. I can see how it could be used as a first step in a refactoring or rewrite, but the emphasis on code coverage in the paper seems totally brain-broke. I suppose it&#x27;d be great if your org is already brain-broke and demanding high coverage, but TestGen-LLM won&#x27;t make your project&#x27;s code better in any way, and it&#x27;ll increase the friction involved in actually implementing improvements. It&#x27;d be much more useful to generate edge-case tests that may or may not be passing, but TestGen-LLM relies on compiler errors and failing tests to filter out LLM garbage. The lack of any examples of the generated tests in the paper makes me suspect that they&#x27;re the same as the rest of the LLM-generated code I have seen: amateurish.</div><br/><div id="39407422" class="c"><input type="checkbox" id="c-39407422" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#39407323">parent</a><span>|</span><a href="#39406608">next</a><span>|</span><label class="collapse" for="c-39407422">[-]</label><label class="expand" for="c-39407422">[1 more]</label></div><br/><div class="children"><div class="content">I recently had to refractory a project that had no tests what so ever.
Having LLMs automatically generate a first draft of tests was very helpful. Even just to understand what the code was supposed to do.</div><br/></div></div></div></div><div id="39406608" class="c"><input type="checkbox" id="c-39406608" checked=""/><div class="controls bullet"><span class="by">newzisforsukas</span><span>|</span><a href="#39407323">prev</a><span>|</span><a href="#39406631">next</a><span>|</span><label class="collapse" for="c-39406608">[-]</label><label class="expand" for="c-39406608">[8 more]</label></div><br/><div class="children"><div class="content">&gt; In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM&#x27;s test cases built correctly, 57% passed reliably, and 25% increased coverage.<p>That doesn&#x27;t seem great?</div><br/><div id="39406726" class="c"><input type="checkbox" id="c-39406726" checked=""/><div class="controls bullet"><span class="by">Jtsummers</span><span>|</span><a href="#39406608">parent</a><span>|</span><a href="#39406719">next</a><span>|</span><label class="collapse" for="c-39406726">[-]</label><label class="expand" for="c-39406726">[1 more]</label></div><br/><div class="children"><div class="content">Their abstract doesn&#x27;t match their actual paper contents. That&#x27;s unfortunate. Their summary indicates rates in terms of test cases (repeating your quote a bit):<p>&gt; 75% of test <i>cases</i> built correctly, 57% passed reliably [implying test cases by context], and 25% increased coverage [same implication]<p>The actual report talks about test <i>classes</i>, where each class has one or more test <i>cases</i>.<p>&gt; (1) 75% of test classes had at least one new test case that builds correctly.<p>&gt; (2) 57% of test classes had at least one test case that builds cor- rectly and passes reliably.<p>&gt; (3) 25% of test classes had at least one test case that builds cor- rectly, passes and increases line coverage compared to all other test classes that share the same build target.<p>Those are two very different statements. They even have a footnote acknowledging this:<p>&gt; For a given attempt to extend a test class, there can be many attempts to generate a test case, so the success rate per test case is typically considerably lower than that per test class.<p>But then in their conclusion they misrepresent their findings again, like the abstract:<p>&gt; When we use TestGen-LLM in its experimental mode (free from the confounding factors inherent in deployment), we found that the success rate per test case was 25% (See Section 3.3). However, line coverage is a stringent requirement for success. Were we to relax the requirement to require only that test cases build and pass, then the success rate rises to 57%.</div><br/></div></div><div id="39406719" class="c"><input type="checkbox" id="c-39406719" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39406608">parent</a><span>|</span><a href="#39406726">prev</a><span>|</span><a href="#39406661">next</a><span>|</span><label class="collapse" for="c-39406719">[-]</label><label class="expand" for="c-39406719">[2 more]</label></div><br/><div class="children"><div class="content">It looks like it only gets committed if passes all of those checks (i.e. no humans have to look at it unless it&#x27;s actually reliably passing and does increase code coverage). 25% code coverage improvement that passes reliably, for the cost of the electricity required for the GPUs, seems pretty cheap.<p>Of course, this doesn&#x27;t seem like it&#x27;s going to <i>replace</i> engineers, but it&#x27;ll help the organization out for relatively low cost.</div><br/><div id="39406836" class="c"><input type="checkbox" id="c-39406836" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39406608">root</a><span>|</span><a href="#39406719">parent</a><span>|</span><a href="#39406661">next</a><span>|</span><label class="collapse" for="c-39406836">[-]</label><label class="expand" for="c-39406836">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 25% code coverage improvement<p>The paper does not report code coverage improvement and it is probably not 25%. The paper does say this:<p>&gt; The median number of lines of code added by a TestGen-LLM test in the test-a-thon was 2.5.</div><br/></div></div></div></div><div id="39406661" class="c"><input type="checkbox" id="c-39406661" checked=""/><div class="controls bullet"><span class="by">sampli</span><span>|</span><a href="#39406608">parent</a><span>|</span><a href="#39406719">prev</a><span>|</span><a href="#39406885">next</a><span>|</span><label class="collapse" for="c-39406661">[-]</label><label class="expand" for="c-39406661">[3 more]</label></div><br/><div class="children"><div class="content">Considering human tests should already have a high coverage rate, if the 25% that increased coverage were actual good tests, I think it’s a useful tool</div><br/><div id="39406740" class="c"><input type="checkbox" id="c-39406740" checked=""/><div class="controls bullet"><span class="by">bornfreddy</span><span>|</span><a href="#39406608">root</a><span>|</span><a href="#39406661">parent</a><span>|</span><a href="#39406885">next</a><span>|</span><label class="collapse" for="c-39406740">[-]</label><label class="expand" for="c-39406740">[2 more]</label></div><br/><div class="children"><div class="content">I was actually surprised they had less than 75% coverage to start with.</div><br/><div id="39406788" class="c"><input type="checkbox" id="c-39406788" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39406608">root</a><span>|</span><a href="#39406740">parent</a><span>|</span><a href="#39406885">next</a><span>|</span><label class="collapse" for="c-39406788">[-]</label><label class="expand" for="c-39406788">[1 more]</label></div><br/><div class="children"><div class="content">25% of test cases increased coverage, not coverage was increased by 25%. For example, if they started with 90% coverage and each test case increased coverage by 0.1% and there were 100 test cases, final coverage was 92.5%.</div><br/></div></div></div></div></div></div><div id="39406885" class="c"><input type="checkbox" id="c-39406885" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#39406608">parent</a><span>|</span><a href="#39406661">prev</a><span>|</span><a href="#39406631">next</a><span>|</span><label class="collapse" for="c-39406885">[-]</label><label class="expand" for="c-39406885">[1 more]</label></div><br/><div class="children"><div class="content">All three are machine-verifiable, so you can easily filter out the ones that don&#x27;t work, right?</div><br/></div></div></div></div><div id="39406631" class="c"><input type="checkbox" id="c-39406631" checked=""/><div class="controls bullet"><span class="by">bilekas</span><span>|</span><a href="#39406608">prev</a><span>|</span><a href="#39407340">next</a><span>|</span><label class="collapse" for="c-39406631">[-]</label><label class="expand" for="c-39406631">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll admit its interesting, a 12 page paper by Meta employees to promote AI for developers. Even brought out the Sankey diagram.<p>I&#x27;m probably wrong but if its published in this way, shouldn&#x27;t the information be given to reproduce it ?<p>Edit: This is not tinfoil hat, I just don&#x27;t have the kind of data that meta has to learn from. So, maybe they released something ?</div><br/><div id="39406763" class="c"><input type="checkbox" id="c-39406763" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#39406631">parent</a><span>|</span><a href="#39406766">next</a><span>|</span><label class="collapse" for="c-39406763">[-]</label><label class="expand" for="c-39406763">[1 more]</label></div><br/><div class="children"><div class="content">It’s an FSE 2024 paper, so I’m guessing the artifacts need theory or formal evaluation.</div><br/></div></div><div id="39406766" class="c"><input type="checkbox" id="c-39406766" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39406631">parent</a><span>|</span><a href="#39406763">prev</a><span>|</span><a href="#39407340">next</a><span>|</span><label class="collapse" for="c-39406766">[-]</label><label class="expand" for="c-39406766">[1 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s anything like Google, it&#x27;s way too intimately tied to their infra and monorepo to release</div><br/></div></div></div></div><div id="39407340" class="c"><input type="checkbox" id="c-39407340" checked=""/><div class="controls bullet"><span class="by">Lio</span><span>|</span><a href="#39406631">prev</a><span>|</span><a href="#39406728">next</a><span>|</span><label class="collapse" for="c-39407340">[-]</label><label class="expand" for="c-39407340">[1 more]</label></div><br/><div class="children"><div class="content">It’s papers like this that will act as the justification of the next round of FAANG lAIoffs.  Regardless of how successful this approach is in the long term.</div><br/></div></div><div id="39406728" class="c"><input type="checkbox" id="c-39406728" checked=""/><div class="controls bullet"><span class="by">Roritharr</span><span>|</span><a href="#39407340">prev</a><span>|</span><a href="#39407398">next</a><span>|</span><label class="collapse" for="c-39406728">[-]</label><label class="expand" for="c-39406728">[1 more]</label></div><br/><div class="children"><div class="content">I am currently looking into building something like this for a client with large (several &gt; 3M LoCs) and old (started in 2001) Java Projects with low coverage.<p>Interesting to read how it would work it you already have good coverage (I assume).</div><br/></div></div><div id="39407398" class="c"><input type="checkbox" id="c-39407398" checked=""/><div class="controls bullet"><span class="by">pearjuice</span><span>|</span><a href="#39406728">prev</a><span>|</span><a href="#39406609">next</a><span>|</span><label class="collapse" for="c-39407398">[-]</label><label class="expand" for="c-39407398">[1 more]</label></div><br/><div class="children"><div class="content">So write unit tests automatically, change code later and then regenerate the unit tests? Now the code has a bug but the unit tests pass. I&#x27;m already seeing this today with devs using ChatGPT to quickly get the &quot;test boilerplate&quot; over and over.</div><br/></div></div><div id="39406609" class="c"><input type="checkbox" id="c-39406609" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#39407398">prev</a><span>|</span><label class="collapse" for="c-39406609">[-]</label><label class="expand" for="c-39406609">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM&#x27;s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta&#x27;s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers.
</code></pre>
…is that a good rate? I guess I have to read more and see if the unacceptable ones were silly mistakes like the ones that make us all do code review, or serious ones. I don’t think a human engineer with 25% failure rate would be very helpful, if it’s a certain kind of failure.<p><pre><code>  As part of our overall mission to automate unit test generation for Android code, we have developed an automated test class improver, TestGen-LLM.
</code></pre>
Is that a good mission? I feel like the TDD people are turning over in their graves, or at least in their beds at home. But again something tells me that they caveat this later</div><br/><div id="39407054" class="c"><input type="checkbox" id="c-39407054" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#39406609">parent</a><span>|</span><label class="collapse" for="c-39407054">[-]</label><label class="expand" for="c-39407054">[1 more]</label></div><br/><div class="children"><div class="content">There is a lot of testless code in Facebook, nobody gets PSC points for that.</div><br/></div></div></div></div></div></div></div></div></div></body></html>