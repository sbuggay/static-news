<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709888451440" as="style"/><link rel="stylesheet" href="styles.css?v=1709888451440"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://two-wrongs.com/statistical-process-control-a-practitioners-guide.html">Statistical Process Control: A Practitioner&#x27;s Guide (2022)</a> <span class="domain">(<a href="https://two-wrongs.com">two-wrongs.com</a>)</span></div><div class="subtext"><span>jpalomaki</span> | <span>16 comments</span></div><br/><div><div id="39638590" class="c"><input type="checkbox" id="c-39638590" checked=""/><div class="controls bullet"><span class="by">lifeisstillgood</span><span>|</span><a href="#39636571">next</a><span>|</span><label class="collapse" for="c-39638590">[-]</label><label class="expand" for="c-39638590">[1 more]</label></div><br/><div class="children"><div class="content">For interest:
<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Stable_process" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Stable_process</a><p>And<p>“Until we can define what a stable process is, we are doomed to argue forever all use of any statistical metric. For the love of a all science, please help!”<p><a href="https:&#x2F;&#x2F;www.isixsigma.com&#x2F;variation&#x2F;what-stable-process&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.isixsigma.com&#x2F;variation&#x2F;what-stable-process&#x2F;</a></div><br/></div></div><div id="39636571" class="c"><input type="checkbox" id="c-39636571" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39638590">prev</a><span>|</span><a href="#39636620">next</a><span>|</span><label class="collapse" for="c-39636571">[-]</label><label class="expand" for="c-39636571">[9 more]</label></div><br/><div class="children"><div class="content">A few related observations:<p>Software development is <i>not a stable process</i>. Either a team is always building new things in which case there isn&#x27;t a consistent process to measure, or there is a see-saw as people release new features, deal with the bugs in the features, then go back to building - that isn&#x27;t a controlled process, it is going to oscillate in statistically weird ways.<p>If SPC is applied to bugs, it will be monitoring the relevant manager&#x27;s habits. That is to say, if you show me a nice in-control timeseries of bug resolution, all that says is when a bug blows out horribly the manager splits it into 2x tickets or something similar. It isn&#x27;t necessarily a bad outcome (small tickets are happy tickets and gently stressing managers is a good idea) - but don&#x27;t expect the devs to behave differently.<p>It is good to have a grounding in SPC, just don&#x27;t try to apply it to every timeseries that you see. Bugs are a timeseries, but they aren&#x27;t expected to be a controlled process so SPC&#x27;s assumptions break down and the logic doesn&#x27;t work. If it does work, it is probably measuring something other than the software development aspect of the process.</div><br/><div id="39636616" class="c"><input type="checkbox" id="c-39636616" checked=""/><div class="controls bullet"><span class="by">jacques_chester</span><span>|</span><a href="#39636571">parent</a><span>|</span><a href="#39638491">next</a><span>|</span><label class="collapse" for="c-39636616">[-]</label><label class="expand" for="c-39636616">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Software development is not a stable process.</i><p>This causes confusion in my experience. &quot;Stable&quot; doesn&#x27;t mean what it does in casual usage. It means that you have a mean and variance that are not varying over time (and there are ways to alert if they are). A stable process can lurch around like a drunken sailor and still be considered stable for SPC purposes.<p>I think bugs <i>should</i> be stable if you normalize it by a size metric. So bugs&#x2F;SLOC or bugs&#x2F;story points.</div><br/><div id="39636672" class="c"><input type="checkbox" id="c-39636672" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39636571">root</a><span>|</span><a href="#39636616">parent</a><span>|</span><a href="#39638491">next</a><span>|</span><label class="collapse" for="c-39636672">[-]</label><label class="expand" for="c-39636672">[3 more]</label></div><br/><div class="children"><div class="content">They aren&#x27;t stable. Bugs, viewed as a timeseries, don&#x27;t have a steady mean and standard deviation. There is a feedback loop where those measures change depending on the maintenance and development strategy of the software, and priorities are going to shift suddenly. If there is a fix-small-bugs month for example, or a fix-this-client&#x27;s-bugs push, or a fix-the-bugs-in-the-new-feature campaign. The statistical properties of all these are not identical.<p>And once you&#x27;ve normalised for story points, what you&#x27;re measuring is stability of the story point allocations. Which is to say, different managers will get different means and standard deviations even if the devs change nothing about their work habits. It isn&#x27;t measuring the devs, software quality or even real number of bugs; it is measuring the ticket creation process.<p>As long as you know it isn&#x27;t measuring the software, all is fine. But people do get confused.</div><br/><div id="39637102" class="c"><input type="checkbox" id="c-39637102" checked=""/><div class="controls bullet"><span class="by">jacques_chester</span><span>|</span><a href="#39636571">root</a><span>|</span><a href="#39636672">parent</a><span>|</span><a href="#39638491">next</a><span>|</span><label class="collapse" for="c-39637102">[-]</label><label class="expand" for="c-39637102">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>If there is a fix-small-bugs month for example, or a fix-this-client&#x27;s-bugs push, or a fix-the-bugs-in-the-new-feature campaign. The statistical properties of all these are not identical.</i><p>For sure -- but these are surely <i>special</i> or <i>assignable</i> causes of variation. That&#x27;s one of the key insights in SPC: that some variations are ordinary and some out of the ordinary. A &quot;fix the bugs&quot; campaign is out of the ordinary.<p>&gt; <i>Which is to say, different managers will get different means and standard deviations even if the devs change nothing about their work habits.</i><p>If they have a bad measurement rubric, certainly. But that in turn goes to questions of gauge reliability, a topic about which SPC has a lot to say.<p>I should say by way of agreement that I&#x27;m not convinced that SPC can be safely and effectively applied to software development processes. But I&#x27;m also not convinced it <i>can&#x27;t</i>. I don&#x27;t have a firm, final position on the matter. I would need to apply it for a while and see for myself.</div><br/><div id="39637284" class="c"><input type="checkbox" id="c-39637284" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39636571">root</a><span>|</span><a href="#39637102">parent</a><span>|</span><a href="#39638491">next</a><span>|</span><label class="collapse" for="c-39637284">[-]</label><label class="expand" for="c-39637284">[1 more]</label></div><br/><div class="children"><div class="content">&gt; For sure -- but these are surely special or assignable causes of variation.<p>Well, for starters they are process changes. The team is working in a different way to normal and, as expected, getting a different result. So it is special variance, but because management has decided that the team should adopt different processes with different statistical characteristics. These aren&#x27;t one-offs, this is routine for how a well managed software team should perform. Priorities ought to change as the situation develops. And regardless, regular variations are what indicates that a process that isn&#x27;t under control. If a process <i>is</i> under control, variances should be remarkably rare.<p>Every month in software engineering is out of the ordinary. That is why SPC doesn&#x27;t work; there isn&#x27;t a repeatable process to model. There are lots of different processes chained together as vague specifications are continuously translated into logically formal specifications in an environment of continuous renegotiation. It might be possible to make a software team work in a controlled way, but it is pretty stupid - either the team won&#x27;t fix some bugs to make the metrics look good, or they have to fake bugs from time to time to make the metrics look reasonable. Both are inferior to non-statistical controls of prioritising bugs with reference to how challenging the fix seems likely to be.<p>Say we&#x27;re going to implement an application. The bugs aren&#x27;t going to appear as a stable process, there&#x27;ll be some sort of big wave up front, then successive waves as new features are identified. Eventually the application will be more or less finished and the engineering team will clean up the long tail of bugs. The mean and standard deviation of the work aren&#x27;t predictable in advance and aren&#x27;t in control as far as SPC is concerned, because those tools assume steady state. Smoothing that curve from a hill-shape to a flat line is not value-additive; it is bad management in its own right. Resources should be reallocated to fix bugs after a big release and then moved back to dev work.<p>Now there are going to be some situations where SPC isn&#x27;t crazy, but it is still a bad management tool for software because the SPC teams aren&#x27;t going to be agile in the face of change, because all their tools will scream blue murder at them for no reason.</div><br/></div></div></div></div></div></div></div></div><div id="39638491" class="c"><input type="checkbox" id="c-39638491" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#39636571">parent</a><span>|</span><a href="#39636616">prev</a><span>|</span><a href="#39636620">next</a><span>|</span><label class="collapse" for="c-39638491">[-]</label><label class="expand" for="c-39638491">[4 more]</label></div><br/><div class="children"><div class="content">Things in software development that have been stable in my experience:<p>- Weekly deploy count<p>- Weekly growth in lines of code<p>- Fraction of candidates hired<p>- Weekly number of bug fixes<p>- Weekly number of problems with a third party collaborator<p>- Many internal metrics generated by the software, reflecting usage etc.<p>- Weekly number of consultant hours required<p>- Monthly growth of feature flag count<p>- Length of standup<p>- Time required to complete &quot;small&quot; tasks (i.e. those that don&#x27;t involve novelty)<p>- Length of successful build in CI<p>- Proportion of CI builds that fail at least one test<p>- Growth of number of tasks in backlog<p>I could go on. The point is that while much of the value of product development comes from novelty and variation, there are many parts of the process that remain the same from week to week.<p>Figuring out what these parts are and getting variation out of them allows the developer to<p>(a) focus creativity on systematically solving process problems instead of doing it ad hoc, and<p>(b) let the process recede into the background and focus creativity on creating end-user value.</div><br/><div id="39638750" class="c"><input type="checkbox" id="c-39638750" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39636571">root</a><span>|</span><a href="#39638491">parent</a><span>|</span><a href="#39636620">next</a><span>|</span><label class="collapse" for="c-39638750">[-]</label><label class="expand" for="c-39638750">[3 more]</label></div><br/><div class="children"><div class="content">Your heartbeat and breaths&#x2F;week are also quite consistent. The issue here is that you have to have a reasonable theory of why variance in the metrics you&#x27;re tracking will destroy value. And that means actual value, not whinging that standup goes too long. I like a short standup as much as anyone, but if that is a material driver of value destruction then your organisation is not ready for statistical quality control. Plus it probably goes over time consistently.<p>And once you start looking at the value, the lesson of software is that high variance activity <i>is often the value add</i>. It is the week(s) where someone implements pivot tables in Excel that creates probably billions of dollars in value over all of humanity. If that turns up as a statistical anomaly in the metrics because their line manager didn&#x27;t bug them to fix bugs that week, that is a problem with the metrics not the programmer.<p>This isn&#x27;t a Toyota production line (if you&#x27;re interested in the history of this, that is no random example) where value is uniformly created with each car and optimising the daily process down to degree n creates value. This is software. The value isn&#x27;t created in the same way and these tools are not powerful in driving value add decisions. Variance is untidy but by no means an enemy. It must be managed case by case in context.</div><br/><div id="39639161" class="c"><input type="checkbox" id="c-39639161" checked=""/><div class="controls bullet"><span class="by">mjfisher</span><span>|</span><a href="#39636571">root</a><span>|</span><a href="#39638750">parent</a><span>|</span><a href="#39638974">next</a><span>|</span><label class="collapse" for="c-39639161">[-]</label><label class="expand" for="c-39639161">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a difference between input and output metrics to be considered too. Attempting to manage the output metrics directly rather than addressing the underlying causes is almost always the wrong thing to do.</div><br/></div></div><div id="39638974" class="c"><input type="checkbox" id="c-39638974" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#39636571">root</a><span>|</span><a href="#39638750">parent</a><span>|</span><a href="#39639161">prev</a><span>|</span><a href="#39636620">next</a><span>|</span><label class="collapse" for="c-39638974">[-]</label><label class="expand" for="c-39638974">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The issue here is that you have to have a reasonable theory of why variance in the metrics you&#x27;re tracking will destroy value.<p>Establishing this theory requires a stable process! Without a stable process, you cannot make deliberate, systematic changes and observe how it affects outcomes. That sort of observation is key to theory-building.<p>I agree with most of what you say about the value in product development coming from innovation which is literally unaccounted-for variance. I just don&#x27;t think that innovation happens in the length of standup meetings, which I think is better controlled statistically.</div><br/></div></div></div></div></div></div></div></div><div id="39636620" class="c"><input type="checkbox" id="c-39636620" checked=""/><div class="controls bullet"><span class="by">jacques_chester</span><span>|</span><a href="#39636571">prev</a><span>|</span><a href="#39638410">next</a><span>|</span><label class="collapse" for="c-39636620">[-]</label><label class="expand" for="c-39636620">[1 more]</label></div><br/><div class="children"><div class="content">This is one of my favorite introductions to this topic area, especially since it gets away from the dominance of manufacturing applications for SPC.</div><br/></div></div><div id="39638410" class="c"><input type="checkbox" id="c-39638410" checked=""/><div class="controls bullet"><span class="by">fjkdlsjflkds</span><span>|</span><a href="#39636620">prev</a><span>|</span><label class="collapse" for="c-39638410">[-]</label><label class="expand" for="c-39638410">[4 more]</label></div><br/><div class="children"><div class="content">I stopped reading at this point:<p>&gt; Roughly half of your measurements will be above average, and the other half below it.<p>This is simply not true for most definitions of &quot;average&quot; (i.e., for <i>all</i> definitions of &quot;average&quot; except the median).<p>Example:<p>&gt; (data &lt;- c(1:10,100))<p>[1]   1   2   3   4   5   6   7   8   9  10 100<p>&gt; sum(as.numeric(data &gt; mean(data)))&#x2F;length(data)<p>[1] 0.09090909<p>A 90&#x2F;10 split is hardly &quot;roughly half of your measurements&quot;.</div><br/><div id="39638452" class="c"><input type="checkbox" id="c-39638452" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#39638410">parent</a><span>|</span><label class="collapse" for="c-39638452">[-]</label><label class="expand" for="c-39638452">[3 more]</label></div><br/><div class="children"><div class="content">Would it help you get past that point if it had said &quot;of a stable process&quot;?</div><br/><div id="39638586" class="c"><input type="checkbox" id="c-39638586" checked=""/><div class="controls bullet"><span class="by">fjkdlsjflkds</span><span>|</span><a href="#39638410">root</a><span>|</span><a href="#39638452">parent</a><span>|</span><label class="collapse" for="c-39638586">[-]</label><label class="expand" for="c-39638586">[2 more]</label></div><br/><div class="children"><div class="content">Well... what do you mean by &quot;a stable process&quot; in this context?<p>Let&#x27;s try repeating the same example, but now drawing samples from a fixed distribution (in this case, a log-normal distribution):<p>&gt; data &lt;- exp(rnorm(100))<p>&gt; sum(as.numeric(data &gt; mean(data)))&#x2F;length(data)<p>[1] 0.32<p>So, again, quite far from a 50&#x2F;50 split, even though I am assuming a stable&#x2F;fixed data generation process.<p>In general, it would help if statistical subjects are not presented in a careless way (i.e., containing things which are obviously not true). I would suggest at least adding an &quot;assuming a symmetrical distribution&quot; (so that at least your claim is approximately correct under the arithmetic average and for bounded variance distributions).<p>EDIT: If by &quot;a stable process&quot; you mean &quot;a process following a stable distribution&quot;... then, no, it doesn&#x27;t help.<p>Here&#x27;s an example with samples drawn from a Lévy distribution (which is a stable distribution):<p>&gt; data &lt;- rmutil::rlevy(100)<p>&gt; sum(as.numeric(data &gt; mean(data)))&#x2F;length(data)<p>[1] 0.07</div><br/><div id="39639015" class="c"><input type="checkbox" id="c-39639015" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#39638410">root</a><span>|</span><a href="#39638586">parent</a><span>|</span><label class="collapse" for="c-39639015">[-]</label><label class="expand" for="c-39639015">[1 more]</label></div><br/><div class="children"><div class="content">In this context &quot;stable&quot; means the thing it means in statistical process control, i.e. the operational definition of no measurements outside of 2.66 times the mean consecutive difference between observations.<p>It is a problem -- particularly for software -- that SPC tools do not work with subexponential distributions, but it&#x27;s separate from the observation that when SPC determines that a process is stable, rougly half of measurements will lie above the average.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>