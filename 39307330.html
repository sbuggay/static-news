<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707469260071" as="style"/><link rel="stylesheet" href="styles.css?v=1707469260071"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ollama.ai/blog/openai-compatibility">OpenAI compatibility</a> <span class="domain">(<a href="https://ollama.ai">ollama.ai</a>)</span></div><div class="subtext"><span>Casteil</span> | <span>129 comments</span></div><br/><div><div id="39308619" class="c"><input type="checkbox" id="c-39308619" checked=""/><div class="controls bullet"><span class="by">ultrasaurus</span><span>|</span><a href="#39312845">next</a><span>|</span><label class="collapse" for="c-39308619">[-]</label><label class="expand" for="c-39308619">[20 more]</label></div><br/><div class="children"><div class="content">The improvements in ease of use for locally hosting LLMs over the last few months have been amazing.  I was ranting about how easy <a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile</a> is just a few hours ago [1]. Now I&#x27;m torn as to which one to use :)<p>1: Quite literally hours ago: <a href="https:&#x2F;&#x2F;euri.ca&#x2F;blog&#x2F;2024-llm-self-hosting-is-easy-now&#x2F;" rel="nofollow">https:&#x2F;&#x2F;euri.ca&#x2F;blog&#x2F;2024-llm-self-hosting-is-easy-now&#x2F;</a></div><br/><div id="39309809" class="c"><input type="checkbox" id="c-39309809" checked=""/><div class="controls bullet"><span class="by">vergessenmir</span><span>|</span><a href="#39308619">parent</a><span>|</span><a href="#39311360">next</a><span>|</span><label class="collapse" for="c-39309809">[-]</label><label class="expand" for="c-39309809">[1 more]</label></div><br/><div class="children"><div class="content">Personally I&#x27;d recommend Ollama, because they have a good model (dockeresque), the APIs are quite more widely supported<p>You can mix models in a single model file, it&#x27;s a feature I&#x27;ve been experimenting with lately<p>Note: you don&#x27;t have to rely on their model Library, you can use your own. Secondly, support for new  models is through their bindings with llama.cpp</div><br/></div></div><div id="39311360" class="c"><input type="checkbox" id="c-39311360" checked=""/><div class="controls bullet"><span class="by">xyc</span><span>|</span><a href="#39308619">parent</a><span>|</span><a href="#39309809">prev</a><span>|</span><a href="#39309403">next</a><span>|</span><label class="collapse" for="c-39311360">[-]</label><label class="expand" for="c-39311360">[1 more]</label></div><br/><div class="children"><div class="content">The pace of progress here is pretty amazing. I loved how easy it is to get llamafile up and running, but I missed feature complete chat interfaces, so I built one based off it: <a href="https:&#x2F;&#x2F;recurse.chat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;recurse.chat&#x2F;</a>.<p>I still need GPT-4 for some tasks, but in daily usage it&#x27;s replaced much of ChatGPT usage, especially since I can import all of my ChatGPT chat history. Also curious to learn about what people want to do with local AI.</div><br/></div></div><div id="39309403" class="c"><input type="checkbox" id="c-39309403" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39308619">parent</a><span>|</span><a href="#39311360">prev</a><span>|</span><a href="#39311349">next</a><span>|</span><label class="collapse" for="c-39309403">[-]</label><label class="expand" for="c-39309403">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always used `llamacpp -m &lt;model&gt; -p &lt;prompt&gt;`. Works great as my daily driver of Mixtral 8x7b + CodeLlama 70b on my MacBook. Do alternatives have any killer features over Llama.cpp? I don&#x27;t want to miss any cool developments.</div><br/><div id="39309887" class="c"><input type="checkbox" id="c-39309887" checked=""/><div class="controls bullet"><span class="by">Casteil</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39309403">parent</a><span>|</span><a href="#39309528">next</a><span>|</span><label class="collapse" for="c-39309887">[-]</label><label class="expand" for="c-39309887">[2 more]</label></div><br/><div class="children"><div class="content">70b is probably going to be a bit slow for most on M-series MBPs (even with enough RAM), but Mixtral 8x7b does really well. Very usable @ 25-30T&#x2F;s (64GB M1 Max), whereas 70b tends to run more like 3.5-5T&#x2F;s.<p>&#x27;llama.cpp-based&#x27; generally seems like the norm.<p>Ollama is just really easy to set up &amp; get going on MacOS. Integral support like this means one less thing to wire up or worry about when using a local LLM as a drop-in replacement for OpenAI&#x27;s remote API. Ollama also has a model library[1] you can browse &amp; easily retrieve models from.<p>Another project, Ollama-webui[2] is a nice webui&#x2F;frontend for local LLM models in Ollama - it supports the latest LLaVA for multimodal image&#x2F;prompt input, too.<p>[1] <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;mixtral</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama-webui&#x2F;ollama-webui">https:&#x2F;&#x2F;github.com&#x2F;ollama-webui&#x2F;ollama-webui</a></div><br/><div id="39311626" class="c"><input type="checkbox" id="c-39311626" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39309887">parent</a><span>|</span><a href="#39309528">next</a><span>|</span><label class="collapse" for="c-39311626">[-]</label><label class="expand" for="c-39311626">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, ollama-webui is an excellent front end and the team was responsive in fixing a bug I reported in a couple of days<p>It&#x27;s also possible to connect to OpenAI API and use GPT-4 on per token plan. I cancelled my chatGPT subscription since. But 90% of the usage for me is Mistral 7B fine-tunes, I rarely use OpenAI</div><br/></div></div></div></div><div id="39309528" class="c"><input type="checkbox" id="c-39309528" checked=""/><div class="controls bullet"><span class="by">ultrasaurus</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39309403">parent</a><span>|</span><a href="#39309887">prev</a><span>|</span><a href="#39309943">next</a><span>|</span><label class="collapse" for="c-39309528">[-]</label><label class="expand" for="c-39309528">[1 more]</label></div><br/><div class="children"><div class="content">Based on a day&#x27;s worth of kicking tires, I&#x27;d say no -- once you have a mix that supports your workflow the cool developments will probably be in new models.<p>I just played around with this tool and it works as advertised, which is cool but I&#x27;m up and running already.  (For anyone reading this though who, like me, doesn&#x27;t want to learn all the optimization work... I might see which one is faster on your machine)</div><br/></div></div><div id="39309943" class="c"><input type="checkbox" id="c-39309943" checked=""/><div class="controls bullet"><span class="by">skp1995</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39309403">parent</a><span>|</span><a href="#39309528">prev</a><span>|</span><a href="#39311349">next</a><span>|</span><label class="collapse" for="c-39309943">[-]</label><label class="expand" for="c-39309943">[2 more]</label></div><br/><div class="children"><div class="content">I have found deepseek coder 33B to be better than codellama 70B (personal opinion tho).. I think the best parts of deepseek are around the fact that it understands multi-file context the best.</div><br/><div id="39312434" class="c"><input type="checkbox" id="c-39312434" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39309943">parent</a><span>|</span><a href="#39311349">next</a><span>|</span><label class="collapse" for="c-39312434">[-]</label><label class="expand" for="c-39312434">[1 more]</label></div><br/><div class="children"><div class="content">Same here, I run deepseek coder 33b on my 64GB M1 Max at about 7-8t&#x2F;s and it blows all other models I&#x27;ve tried for coding. It feels like magic and cheating at the same time, getting these lenghty and in-depth answers with activity monitor showing 0 network IO.</div><br/></div></div></div></div></div></div><div id="39310319" class="c"><input type="checkbox" id="c-39310319" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39308619">parent</a><span>|</span><a href="#39311349">prev</a><span>|</span><a href="#39311818">next</a><span>|</span><label class="collapse" for="c-39310319">[-]</label><label class="expand" for="c-39310319">[8 more]</label></div><br/><div class="children"><div class="content">From the blog article:<p>&gt; A few pip install X’s and you’re off to the races with Llama 2! Well, maybe you are, my dev machine doesn’t have the resources to respond on even the smallest model in less than an hour.<p>I never tried to run these LLMs on my own machine -- is it this bad?<p>I guess if I only have a moderate GPU, say a 4060TI, there is no chance I can play with it, then?</div><br/><div id="39312685" class="c"><input type="checkbox" id="c-39312685" checked=""/><div class="controls bullet"><span class="by">jwr</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39310319">parent</a><span>|</span><a href="#39310583">next</a><span>|</span><label class="collapse" for="c-39312685">[-]</label><label class="expand" for="c-39312685">[1 more]</label></div><br/><div class="children"><div class="content">On an M3 MacBook Pro with 32GB of RAM, I can comfortably run 34B models like phind-codellama:34b-v2-q8_0.<p>Unfortunately, having tried this and a bunch of other models, they are all garbage compared to GPT-4.</div><br/></div></div><div id="39310583" class="c"><input type="checkbox" id="c-39310583" checked=""/><div class="controls bullet"><span class="by">pitched</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39310319">parent</a><span>|</span><a href="#39312685">prev</a><span>|</span><a href="#39310849">next</a><span>|</span><label class="collapse" for="c-39310583">[-]</label><label class="expand" for="c-39310583">[4 more]</label></div><br/><div class="children"><div class="content">I would expect that 4060ti to get about 20-25 tokens per second on Mixtral. I can read at roughly 10-15 tokens per second so above that is where I see diminishing returns for a chatbot. Generating whole blog articles might have you sit waiting for a minute or so though.</div><br/><div id="39311159" class="c"><input type="checkbox" id="c-39311159" checked=""/><div class="controls bullet"><span class="by">cellis</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39310583">parent</a><span>|</span><a href="#39310770">next</a><span>|</span><label class="collapse" for="c-39311159">[-]</label><label class="expand" for="c-39311159">[2 more]</label></div><br/><div class="children"><div class="content">It depends on the context window, but my 3090 gets ~60&#x2F;s on smaller windows.</div><br/><div id="39311617" class="c"><input type="checkbox" id="c-39311617" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39311159">parent</a><span>|</span><a href="#39310770">next</a><span>|</span><label class="collapse" for="c-39311617">[-]</label><label class="expand" for="c-39311617">[1 more]</label></div><br/><div class="children"><div class="content">I get 50-60t&#x2F;s on Mistral 7B on 2080 Ti</div><br/></div></div></div></div><div id="39310770" class="c"><input type="checkbox" id="c-39310770" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39310583">parent</a><span>|</span><a href="#39311159">prev</a><span>|</span><a href="#39310849">next</a><span>|</span><label class="collapse" for="c-39310770">[-]</label><label class="expand" for="c-39310770">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, that sounds more than tolerable than &quot;more than an hour&quot;!<p>I also have the 16GB version, which I assume would be a little bit better.</div><br/></div></div></div></div><div id="39310849" class="c"><input type="checkbox" id="c-39310849" checked=""/><div class="controls bullet"><span class="by">jsjohnst</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39310319">parent</a><span>|</span><a href="#39310583">prev</a><span>|</span><a href="#39310551">next</a><span>|</span><label class="collapse" for="c-39310849">[-]</label><label class="expand" for="c-39310849">[1 more]</label></div><br/><div class="children"><div class="content">The Apple M1 is very useable with ollama using 7B parameter models and is virtually as “fast” as ChatGPT in responding. Obviously not same quality, but still useful.</div><br/></div></div><div id="39310551" class="c"><input type="checkbox" id="c-39310551" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39310319">parent</a><span>|</span><a href="#39310849">prev</a><span>|</span><a href="#39311818">next</a><span>|</span><label class="collapse" for="c-39310551">[-]</label><label class="expand" for="c-39310551">[1 more]</label></div><br/><div class="children"><div class="content">You can load a 7B parameter model quantized at Q4_K_M as gguf. I don&#x27;t know ollama, but you can load it in koboldcpp -- use cuBLAS and gpu layers 100 context 2048 and it should fit it all into 8GB of VRAM. For quantized models look at TheBloke on huggingface -- Mistral 7B is a good one to try.</div><br/></div></div></div></div><div id="39311818" class="c"><input type="checkbox" id="c-39311818" checked=""/><div class="controls bullet"><span class="by">jondwillis</span><span>|</span><a href="#39308619">parent</a><span>|</span><a href="#39310319">prev</a><span>|</span><a href="#39312845">next</a><span>|</span><label class="collapse" for="c-39311818">[-]</label><label class="expand" for="c-39311818">[2 more]</label></div><br/><div class="children"><div class="content">I’ve been using Ollama with Mixtral-7B on my MBP for local development and it has been amazing.</div><br/><div id="39312731" class="c"><input type="checkbox" id="c-39312731" checked=""/><div class="controls bullet"><span class="by">gnicholas</span><span>|</span><a href="#39308619">root</a><span>|</span><a href="#39311818">parent</a><span>|</span><a href="#39312845">next</a><span>|</span><label class="collapse" for="c-39312731">[-]</label><label class="expand" for="c-39312731">[1 more]</label></div><br/><div class="children"><div class="content">I have used it too and am wondering why it starts responding so much faster than other similar-sized models I&#x27;ve tried. It doesn&#x27;t seem quite as good as some of the others, but it is nice that the responses start almost immediately (on my 2022 MBA with 16 GB RAM).<p>Does anyone know why this would be?</div><br/></div></div></div></div></div></div><div id="39312845" class="c"><input type="checkbox" id="c-39312845" checked=""/><div class="controls bullet"><span class="by">jhoechtl</span><span>|</span><a href="#39308619">prev</a><span>|</span><a href="#39311085">next</a><span>|</span><label class="collapse" for="c-39312845">[-]</label><label class="expand" for="c-39312845">[1 more]</label></div><br/><div class="children"><div class="content">How does ollama compare to H2o? We dabbled a bit with H2o and it looks very promising<p><a href="https:&#x2F;&#x2F;gpt.h2o.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gpt.h2o.ai&#x2F;</a></div><br/></div></div><div id="39311085" class="c"><input type="checkbox" id="c-39311085" checked=""/><div class="controls bullet"><span class="by">mrtimo</span><span>|</span><a href="#39312845">prev</a><span>|</span><a href="#39307809">next</a><span>|</span><label class="collapse" for="c-39311085">[-]</label><label class="expand" for="c-39311085">[1 more]</label></div><br/><div class="children"><div class="content">I am business prof. I wanted my students to try out ollama (with web-ui), so I built some directions for doing so on google cloud [1]. If you use a spot instance you can run it for 18 cents an hour.<p>[1] <a href="https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1OpZl4P3d0WKH9XtErUZib5_2EA_QTy50YjJM0kjX1g4&#x2F;edit" rel="nofollow">https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1OpZl4P3d0WKH9XtErUZib5_2...</a></div><br/></div></div><div id="39307809" class="c"><input type="checkbox" id="c-39307809" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39311085">prev</a><span>|</span><a href="#39307881">next</a><span>|</span><label class="collapse" for="c-39307809">[-]</label><label class="expand" for="c-39307809">[13 more]</label></div><br/><div class="children"><div class="content">I know a few people privately unhappy that openai api compatibility is becoming a community standard. Apart from some awkwardness around data.choices.text.response and such unnecessary defensive nesting in the schema, I don&#x27;t really have complaints.<p>wonder what pain points people have around the API becoming a standard, and if anyone has taken a crack at any alternative standards that people should consider.</div><br/><div id="39308864" class="c"><input type="checkbox" id="c-39308864" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39307809">parent</a><span>|</span><a href="#39308202">next</a><span>|</span><label class="collapse" for="c-39308864">[-]</label><label class="expand" for="c-39308864">[4 more]</label></div><br/><div class="children"><div class="content">I want it to be documented.<p>I&#x27;m fine with it emerging as a community standard if there&#x27;s a REALLY robust specification for what the community considers to be &quot;OpenAI API compatible&quot;.<p>Crucially, that standard needs to stay stable even if OpenAI have released a brand new feature this morning.<p>So I want the following:<p>- A very solid API specification, including error conditions<p>- A test suite that can be used to check that new implementations conform to that specification<p>- A name. I want to know what it means when software claims to be &quot;compatible with OpenAI-API-Spec v3&quot; (for example)<p>Right now telling me something is &quot;OpenAI API compatible&quot; really isn&#x27;t enough information. Which bits of that API? Which particular date-in-time was it created to match?</div><br/><div id="39309147" class="c"><input type="checkbox" id="c-39309147" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39307809">root</a><span>|</span><a href="#39308864">parent</a><span>|</span><a href="#39308202">next</a><span>|</span><label class="collapse" for="c-39309147">[-]</label><label class="expand" for="c-39309147">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a JSON API...   JSON API&#x27;s tend to be more...  &#x27;flexible&#x27;.<p>To consume them, just assume that every field is optional and extra fields might appear at any time.</div><br/><div id="39309300" class="c"><input type="checkbox" id="c-39309300" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39307809">root</a><span>|</span><a href="#39309147">parent</a><span>|</span><a href="#39308202">next</a><span>|</span><label class="collapse" for="c-39309300">[-]</label><label class="expand" for="c-39309300">[2 more]</label></div><br/><div class="children"><div class="content">and disappear at any time... was a leetle bit unsettled by the sudden deprecation of &quot;functions&quot; for &quot;tools&quot; with only minor apparante benefit</div><br/><div id="39309444" class="c"><input type="checkbox" id="c-39309444" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39307809">root</a><span>|</span><a href="#39309300">parent</a><span>|</span><a href="#39308202">next</a><span>|</span><label class="collapse" for="c-39309444">[-]</label><label class="expand" for="c-39309444">[1 more]</label></div><br/><div class="children"><div class="content">and what does `auto` even mean?</div><br/></div></div></div></div></div></div></div></div><div id="39308202" class="c"><input type="checkbox" id="c-39308202" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#39307809">parent</a><span>|</span><a href="#39308864">prev</a><span>|</span><a href="#39307842">next</a><span>|</span><label class="collapse" for="c-39308202">[-]</label><label class="expand" for="c-39308202">[4 more]</label></div><br/><div class="children"><div class="content">TBH, we debated about this a lot before adding it. It&#x27;s weird being beholden to someone else&#x27;s API which can dictate what features we should (or shouldn&#x27;t) be adding to our own project. If we add something cool&#x2F;new&#x2F;different to Ollama will people even be able to use it since there isn&#x27;t an equivalent thing in the OpenAI API?</div><br/><div id="39308350" class="c"><input type="checkbox" id="c-39308350" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39307809">root</a><span>|</span><a href="#39308202">parent</a><span>|</span><a href="#39308603">next</a><span>|</span><label class="collapse" for="c-39308350">[-]</label><label class="expand" for="c-39308350">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s more of a marketing problem than a technical problem. If there is indeed a novel use case with a good demo example that&#x27;s not present in OpenAI&#x27;s API, then people will use it. And if it&#x27;s <i>really</i> novel, OpenAI will copy it into their API and thus the problem is no longer an issue.<p>The power of open source!</div><br/><div id="39308554" class="c"><input type="checkbox" id="c-39308554" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#39307809">root</a><span>|</span><a href="#39308350">parent</a><span>|</span><a href="#39308603">next</a><span>|</span><label class="collapse" for="c-39308554">[-]</label><label class="expand" for="c-39308554">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right that it&#x27;s a marketing problem, but it&#x27;s also a technical problem. If tooling&#x2F;projects are built around the compat layer it makes it really difficult to consume those features without having to rewrite a lot of stuff. It also places a cognitive burden on developers to know which API to use. That might not sound like a lot, but one of the guiding principles around the project (and a big part of its success) is to keep the user experience as simple as possible.</div><br/></div></div></div></div><div id="39308603" class="c"><input type="checkbox" id="c-39308603" checked=""/><div class="controls bullet"><span class="by">satellite2</span><span>|</span><a href="#39307809">root</a><span>|</span><a href="#39308202">parent</a><span>|</span><a href="#39308350">prev</a><span>|</span><a href="#39307842">next</a><span>|</span><label class="collapse" for="c-39308603">[-]</label><label class="expand" for="c-39308603">[1 more]</label></div><br/><div class="children"><div class="content">At some point, (probably in a relatively close future), there will be the AI Consortium (AIC) to decide what enters the common API?</div><br/></div></div></div></div><div id="39307842" class="c"><input type="checkbox" id="c-39307842" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39307809">parent</a><span>|</span><a href="#39308202">prev</a><span>|</span><a href="#39309193">next</a><span>|</span><label class="collapse" for="c-39307842">[-]</label><label class="expand" for="c-39307842">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why it&#x27;s good as an <i>option</i> to minimize friction and reduce lock-in to OpenAI&#x27;s moat.</div><br/></div></div><div id="39309193" class="c"><input type="checkbox" id="c-39309193" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#39307809">parent</a><span>|</span><a href="#39307842">prev</a><span>|</span><a href="#39309220">next</a><span>|</span><label class="collapse" for="c-39309193">[-]</label><label class="expand" for="c-39309193">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s so trivially easy to create your own web server in your language of choice that calls directly into llama.cpp functions with the bindings for your language of choice it doesn&#x27;t really matter all that much. If you want more control you can get with just a little more work. You don&#x27;t really need these plug and play things.</div><br/></div></div><div id="39309220" class="c"><input type="checkbox" id="c-39309220" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#39307809">parent</a><span>|</span><a href="#39309193">prev</a><span>|</span><a href="#39307881">next</a><span>|</span><label class="collapse" for="c-39309220">[-]</label><label class="expand" for="c-39309220">[2 more]</label></div><br/><div class="children"><div class="content">I would take an imperfect standard over no standard any day!</div><br/><div id="39310474" class="c"><input type="checkbox" id="c-39310474" checked=""/><div class="controls bullet"><span class="by">dimask</span><span>|</span><a href="#39307809">root</a><span>|</span><a href="#39309220">parent</a><span>|</span><a href="#39307881">next</a><span>|</span><label class="collapse" for="c-39310474">[-]</label><label class="expand" for="c-39310474">[1 more]</label></div><br/><div class="children"><div class="content">There is a difference between a standard and a monopoly, though.</div><br/></div></div></div></div></div></div><div id="39307881" class="c"><input type="checkbox" id="c-39307881" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#39307809">prev</a><span>|</span><a href="#39312158">next</a><span>|</span><label class="collapse" for="c-39307881">[-]</label><label class="expand" for="c-39307881">[1 more]</label></div><br/><div class="children"><div class="content">Useful! At work we are building a better version of Copilot, and support bringing your own LLM. Recently I&#x27;ve been adding an &#x27;OpenAI compatible&#x27; backend, so that if you can provide any OpenAI compatible API endpoint, and just tell us which model to treat it as, then we can format prompts, stop sequences, respect max tokens, etc. according to the semantics of that model.<p>I&#x27;ve been needing something exactly like this to test against in local dev environments :) Ollama having this will make my life &#x2F; testing against the myriad of LLMs we need to support way, way easier.<p>Seems everyone is centralizing behind OpenAI API compatibility, e.g. there is OpenLLM and a few others which implement the same API as well.</div><br/></div></div><div id="39312158" class="c"><input type="checkbox" id="c-39312158" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#39307881">prev</a><span>|</span><a href="#39308276">next</a><span>|</span><label class="collapse" for="c-39312158">[-]</label><label class="expand" for="c-39312158">[1 more]</label></div><br/><div class="children"><div class="content">There has been a lot of progress with tools like llama.cpp and ollama, but despite slightly more difficult setup I prefer huggingface transformer based stuff(TGI for hosting, openllm proxy for (not at all)OpenAI compatibility). Why? Because you can bet the latest newest models are going to be supported in huggingface transformers library.<p>Llama.cpp is not far behind, but I find the well structured python code of transformers easy to modify and extend(with context free grammars, function calling etc) than just waiting for your favourite alternate runtime support a new model.</div><br/></div></div><div id="39308276" class="c"><input type="checkbox" id="c-39308276" checked=""/><div class="controls bullet"><span class="by">ptrhvns</span><span>|</span><a href="#39312158">prev</a><span>|</span><a href="#39307951">next</a><span>|</span><label class="collapse" for="c-39308276">[-]</label><label class="expand" for="c-39308276">[12 more]</label></div><br/><div class="children"><div class="content">FYI: the Linux installation script for Ollama works in the &quot;standard&quot; style for tooling these days:<p><pre><code>    curl https:&#x2F;&#x2F;ollama.ai&#x2F;install.sh | sh
</code></pre>
However, that script asks for root-level privileges via sudo the last time I checked. So, if you want the tool, you may want to download the script and have a look at it, or modify it depending on your needs.</div><br/><div id="39308372" class="c"><input type="checkbox" id="c-39308372" checked=""/><div class="controls bullet"><span class="by">riffic</span><span>|</span><a href="#39308276">parent</a><span>|</span><a href="#39307951">next</a><span>|</span><label class="collapse" for="c-39308372">[-]</label><label class="expand" for="c-39308372">[11 more]</label></div><br/><div class="children"><div class="content">we have package managers in this day and age, lol.</div><br/><div id="39308514" class="c"><input type="checkbox" id="c-39308514" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308372">parent</a><span>|</span><a href="#39311483">next</a><span>|</span><label class="collapse" for="c-39308514">[-]</label><label class="expand" for="c-39308514">[6 more]</label></div><br/><div class="children"><div class="content">do package managers make promises that they only distribute code that&#x27;s been audited to not pwn you? I&#x27;m not sure I see the difference if I decided I&#x27;m going to run someone&#x27;s software whether I install it with sudo apt install vs sudo curl | bash</div><br/><div id="39309907" class="c"><input type="checkbox" id="c-39309907" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308514">parent</a><span>|</span><a href="#39308638">next</a><span>|</span><label class="collapse" for="c-39309907">[-]</label><label class="expand" for="c-39309907">[1 more]</label></div><br/><div class="children"><div class="content">Well, there&#x27;s things like:<p><a href="https:&#x2F;&#x2F;www.debian.org&#x2F;doc&#x2F;debian-policy&#x2F;ch-archive.html#the-main-archive-area" rel="nofollow">https:&#x2F;&#x2F;www.debian.org&#x2F;doc&#x2F;debian-policy&#x2F;ch-archive.html#the...</a><p>The whole thing, actually:<p><a href="https:&#x2F;&#x2F;www.debian.org&#x2F;doc&#x2F;debian-policy&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;www.debian.org&#x2F;doc&#x2F;debian-policy&#x2F;index.html</a></div><br/></div></div><div id="39308638" class="c"><input type="checkbox" id="c-39308638" checked=""/><div class="controls bullet"><span class="by">n_plus_1_acc</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308514">parent</a><span>|</span><a href="#39309907">prev</a><span>|</span><a href="#39311483">next</a><span>|</span><label class="collapse" for="c-39308638">[-]</label><label class="expand" for="c-39308638">[4 more]</label></div><br/><div class="children"><div class="content">You are already trusting the maintainers of your distro by running Software they compiled, if you installed <i>anything</i> via the package manager. So it&#x27;s about the number of people.</div><br/><div id="39312040" class="c"><input type="checkbox" id="c-39312040" checked=""/><div class="controls bullet"><span class="by">sofixa</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308638">parent</a><span>|</span><a href="#39308932">next</a><span>|</span><label class="collapse" for="c-39312040">[-]</label><label class="expand" for="c-39312040">[2 more]</label></div><br/><div class="children"><div class="content">This only applies to software distributed by your distro. For something as novel as Ollama, I severely doubt it&#x27;s made it into anything other than the most bleeding edge(Arch and co). You&#x27;ll have to wait a few years to get it into mainline Debian, Ubuntu, Fedora, etc. and of course it will be at a set version.</div><br/><div id="39312711" class="c"><input type="checkbox" id="c-39312711" checked=""/><div class="controls bullet"><span class="by">n_plus_1_acc</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39312040">parent</a><span>|</span><a href="#39308932">next</a><span>|</span><label class="collapse" for="c-39312711">[-]</label><label class="expand" for="c-39312711">[1 more]</label></div><br/><div class="children"><div class="content">Debian considers this a feature. Choose a distro that fits your needs.</div><br/></div></div></div></div><div id="39308932" class="c"><input type="checkbox" id="c-39308932" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308638">parent</a><span>|</span><a href="#39312040">prev</a><span>|</span><a href="#39311483">next</a><span>|</span><label class="collapse" for="c-39308932">[-]</label><label class="expand" for="c-39308932">[1 more]</label></div><br/><div class="children"><div class="content">ok, so, I think i am trusting fewer people if I just run the bash script provided by the people whose software i want to run</div><br/></div></div></div></div></div></div><div id="39308496" class="c"><input type="checkbox" id="c-39308496" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308372">parent</a><span>|</span><a href="#39311483">prev</a><span>|</span><a href="#39307951">next</a><span>|</span><label class="collapse" for="c-39308496">[-]</label><label class="expand" for="c-39308496">[3 more]</label></div><br/><div class="children"><div class="content">Sadly most of them kinda suck, especially for packagers.</div><br/><div id="39310006" class="c"><input type="checkbox" id="c-39310006" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308496">parent</a><span>|</span><a href="#39309847">next</a><span>|</span><label class="collapse" for="c-39310006">[-]</label><label class="expand" for="c-39310006">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;jordansissel&#x2F;fpm">https:&#x2F;&#x2F;github.com&#x2F;jordansissel&#x2F;fpm</a></div><br/></div></div><div id="39309847" class="c"><input type="checkbox" id="c-39309847" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#39308276">root</a><span>|</span><a href="#39308496">parent</a><span>|</span><a href="#39310006">prev</a><span>|</span><a href="#39307951">next</a><span>|</span><label class="collapse" for="c-39309847">[-]</label><label class="expand" for="c-39309847">[1 more]</label></div><br/><div class="children"><div class="content">Wrap it in homebrew and have ruby call out to sudo. Problem solved &#x2F;s</div><br/></div></div></div></div></div></div></div></div><div id="39307951" class="c"><input type="checkbox" id="c-39307951" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#39308276">prev</a><span>|</span><a href="#39307966">next</a><span>|</span><label class="collapse" for="c-39307951">[-]</label><label class="expand" for="c-39307951">[6 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s a little misleading to say it&#x27;s compatible with OpenAI because I expect function or tool calling when you say that.<p>It&#x27;s nice that you have the role and content thing but that was always fairly trivial to implement.<p>When it gets to agents you do need to execute actions. In the agent hosting system I started, I included a scripting engine, which makes me think that maybe I need to set up security and permissions for the agent system and just let it run code. Which is what I started.<p>So I guess I am not sure I really need the function&#x2F;tool calling. But if I see a bunch of people actually am standardizing on tool calls then maybe I need it in my framework just because it will be expected. Even if I have arbitrary script execution.</div><br/><div id="39308019" class="c"><input type="checkbox" id="c-39308019" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39307951">parent</a><span>|</span><a href="#39308282">next</a><span>|</span><label class="collapse" for="c-39308019">[-]</label><label class="expand" for="c-39308019">[2 more]</label></div><br/><div class="children"><div class="content">The documentation is upfront about which features are excluded: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;openai.md">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;openai.md</a><p>Function calling&#x2F;tool choice is done at the application level and currently there&#x27;s no standard format, and the popular ones are essentually inefficient bespoke system prompts: <a href="https:&#x2F;&#x2F;github.com&#x2F;langchain-ai&#x2F;langchain&#x2F;blob&#x2F;master&#x2F;libs&#x2F;langchain&#x2F;langchain&#x2F;agents&#x2F;conversational_chat&#x2F;prompt.py">https:&#x2F;&#x2F;github.com&#x2F;langchain-ai&#x2F;langchain&#x2F;blob&#x2F;master&#x2F;libs&#x2F;l...</a></div><br/><div id="39309832" class="c"><input type="checkbox" id="c-39309832" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#39307951">root</a><span>|</span><a href="#39308019">parent</a><span>|</span><a href="#39308282">next</a><span>|</span><label class="collapse" for="c-39309832">[-]</label><label class="expand" for="c-39309832">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Function calling&#x2F;tool choice is done at the application level and currently there&#x27;s no standard format,<p>Is this true for open ai - or just everything else?</div><br/></div></div></div></div><div id="39308282" class="c"><input type="checkbox" id="c-39308282" checked=""/><div class="controls bullet"><span class="by">ianbicking</span><span>|</span><a href="#39307951">parent</a><span>|</span><a href="#39308019">prev</a><span>|</span><a href="#39308134">next</a><span>|</span><label class="collapse" for="c-39308282">[-]</label><label class="expand" for="c-39308282">[2 more]</label></div><br/><div class="children"><div class="content">I was drawn to Gemini Pro because it had function&#x2F;tool calling... but it works terribly. (I haven&#x27;t tried Gemini Ultra yet; unclear if it&#x27;s available by API?)<p>Anyway, probably best that they didn&#x27;t release support that doesn&#x27;t work.</div><br/><div id="39310264" class="c"><input type="checkbox" id="c-39310264" checked=""/><div class="controls bullet"><span class="by">williamstein</span><span>|</span><a href="#39307951">root</a><span>|</span><a href="#39308282">parent</a><span>|</span><a href="#39308134">next</a><span>|</span><label class="collapse" for="c-39310264">[-]</label><label class="expand" for="c-39310264">[1 more]</label></div><br/><div class="children"><div class="content">Gemini Ultra is not available via API yet, at least according to the Google reps we talked with today.  There&#x27;s a waiting list.   I suspect they are figuring out how to charge for API access, among other things.  The announcement today only seemed to have pricing for the &quot;$20&#x2F;month&quot; thing.</div><br/></div></div></div></div><div id="39308134" class="c"><input type="checkbox" id="c-39308134" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#39307951">parent</a><span>|</span><a href="#39308282">prev</a><span>|</span><a href="#39307966">next</a><span>|</span><label class="collapse" for="c-39308134">[-]</label><label class="expand" for="c-39308134">[1 more]</label></div><br/><div class="children"><div class="content">It makes obvious sense to anyone with experience with OpenAI APIs.</div><br/></div></div></div></div><div id="39307966" class="c"><input type="checkbox" id="c-39307966" checked=""/><div class="controls bullet"><span class="by">lolpanda</span><span>|</span><a href="#39307951">prev</a><span>|</span><a href="#39307728">next</a><span>|</span><label class="collapse" for="c-39307966">[-]</label><label class="expand" for="c-39307966">[5 more]</label></div><br/><div class="children"><div class="content">The compatibility layer can be also built in libraries. For example, Langchain has llm() which can work with multiple LLM backend. Which do you prefer?</div><br/><div id="39310974" class="c"><input type="checkbox" id="c-39310974" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#39307966">parent</a><span>|</span><a href="#39308620">next</a><span>|</span><label class="collapse" for="c-39310974">[-]</label><label class="expand" for="c-39310974">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d prefer it in library but there are a number of issues with that currently, the larger of it being that the landscape moves too fast and library wrappers aren&#x27;t keeping up. the other is, what if the world standardize on a terrible library like langchain we&#x27;d be stuck with it for a long time since maintenance cost of non uniform backend tend to kill possible runner ups. So for now the uniform api seems the choice of convenience.</div><br/></div></div><div id="39308620" class="c"><input type="checkbox" id="c-39308620" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#39307966">parent</a><span>|</span><a href="#39310974">prev</a><span>|</span><a href="#39308131">next</a><span>|</span><label class="collapse" for="c-39308620">[-]</label><label class="expand" for="c-39308620">[2 more]</label></div><br/><div class="children"><div class="content">but this means you need each library to support each llm, and I think this is the same issue what is with object storage where basically everyone support S3 compatible API<p>it&#x27;s great to have some standard API even if that&#x27;s isn&#x27;t perfect, but having second API that allows you to use full potential (like B2 for backblaze) is also fine<p>so there isn&#x27;t one model fits all, and if your model have different capabilities, then imo you should provide both options</div><br/><div id="39308887" class="c"><input type="checkbox" id="c-39308887" checked=""/><div class="controls bullet"><span class="by">SOLAR_FIELDS</span><span>|</span><a href="#39307966">root</a><span>|</span><a href="#39308620">parent</a><span>|</span><a href="#39308131">next</a><span>|</span><label class="collapse" for="c-39308887">[-]</label><label class="expand" for="c-39308887">[1 more]</label></div><br/><div class="children"><div class="content">This is hopefully much better than the s3 situation due to its simplicity. Many offerings that say “s3 compatible api” often mean “we support like 30% of api endpoints”. Granted often the most common stuff is supported and some stuff in the s3 api really only makes sense in AWS, but a good hunk of the s3 api is just hard or annoying to implement and a lot of vendors just don’t bother. Which ends up being rather annoying because you’ll pick some vendor and try to use an s3 client with it only to find out you can’t because of the 10% of the calls your client needs to make that are unsupported.</div><br/></div></div></div></div><div id="39308131" class="c"><input type="checkbox" id="c-39308131" checked=""/><div class="controls bullet"><span class="by">mise_en_place</span><span>|</span><a href="#39307966">parent</a><span>|</span><a href="#39308620">prev</a><span>|</span><a href="#39307728">next</a><span>|</span><label class="collapse" for="c-39308131">[-]</label><label class="expand" for="c-39308131">[1 more]</label></div><br/><div class="children"><div class="content">Before OpenAI released their app I was using langchain in a system that I built. It was a very simple SMS interface to LLMs. I preferred working with langchain&#x27;s abstractions over directly interfacing with the GPT4 API.</div><br/></div></div></div></div><div id="39307728" class="c"><input type="checkbox" id="c-39307728" checked=""/><div class="controls bullet"><span class="by">patelajay285</span><span>|</span><a href="#39307966">prev</a><span>|</span><a href="#39309239">next</a><span>|</span><label class="collapse" for="c-39307728">[-]</label><label class="expand" for="c-39307728">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve been working on a project that provides this sort of easy swapping between open source (via HF, VLLM) &amp; commercial models (OpenAI, Google, Anthropic, Together) in Python: <a href="https:&#x2F;&#x2F;github.com&#x2F;datadreamer-dev&#x2F;DataDreamer">https:&#x2F;&#x2F;github.com&#x2F;datadreamer-dev&#x2F;DataDreamer</a><p>It&#x27;s a little bit easier to use if you want to do this without an HTTP API, directly in Python.</div><br/></div></div><div id="39309239" class="c"><input type="checkbox" id="c-39309239" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39307728">prev</a><span>|</span><a href="#39307846">next</a><span>|</span><label class="collapse" for="c-39309239">[-]</label><label class="expand" for="c-39309239">[7 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the use case of Ollama? Why should I not use llama.cpp directly?</div><br/><div id="39310823" class="c"><input type="checkbox" id="c-39310823" checked=""/><div class="controls bullet"><span class="by">TheCoreh</span><span>|</span><a href="#39309239">parent</a><span>|</span><a href="#39309404">next</a><span>|</span><label class="collapse" for="c-39310823">[-]</label><label class="expand" for="c-39310823">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like a docker&#x2F;package manager for the LLMs. You can easily install them, discover new ones, update them via a standardized, simple CLI. It also auto updates effortlessly.</div><br/></div></div><div id="39309404" class="c"><input type="checkbox" id="c-39309404" checked=""/><div class="controls bullet"><span class="by">jpdus</span><span>|</span><a href="#39309239">parent</a><span>|</span><a href="#39310823">prev</a><span>|</span><a href="#39307846">next</a><span>|</span><label class="collapse" for="c-39309404">[-]</label><label class="expand" for="c-39309404">[5 more]</label></div><br/><div class="children"><div class="content">I have the same question. Noticed that Ollama got a lot of publicity and seems to be well received, but what exactly is the advantage over using llama.cpp (which also has a built-in server with OpenAI compatibility nowadays?) Directly?</div><br/><div id="39311666" class="c"><input type="checkbox" id="c-39311666" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39309239">root</a><span>|</span><a href="#39309404">parent</a><span>|</span><a href="#39307846">next</a><span>|</span><label class="collapse" for="c-39311666">[-]</label><label class="expand" for="c-39311666">[4 more]</label></div><br/><div class="children"><div class="content">ollama swaps models from the local library on the fly, based on the request args, so you can test against a bunch of models quickly</div><br/><div id="39311703" class="c"><input type="checkbox" id="c-39311703" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39309239">root</a><span>|</span><a href="#39311666">parent</a><span>|</span><a href="#39307846">next</a><span>|</span><label class="collapse" for="c-39311703">[-]</label><label class="expand" for="c-39311703">[3 more]</label></div><br/><div class="children"><div class="content">Once you&#x27;ve tested to your heart&#x27;s content, you&#x27;ll deploy your model in production. So, looks like this is really just a dev use case, not a production use case.</div><br/><div id="39311929" class="c"><input type="checkbox" id="c-39311929" checked=""/><div class="controls bullet"><span class="by">silverliver</span><span>|</span><a href="#39309239">root</a><span>|</span><a href="#39311703">parent</a><span>|</span><a href="#39307846">next</a><span>|</span><label class="collapse" for="c-39311929">[-]</label><label class="expand" for="c-39311929">[2 more]</label></div><br/><div class="children"><div class="content">In production, I&#x27;d be more concerned about the possibly of it going off on it&#x27;s own and autoupdating and causing regressions. FLOSS LLMs are interesting to me because I can precisely control the entire stack.<p>If Ollama doesn&#x27;t have a cli flag that disables auto updating and networking altogether, I&#x27;m not letting it anywhere near my production environments. Period.</div><br/><div id="39312162" class="c"><input type="checkbox" id="c-39312162" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#39309239">root</a><span>|</span><a href="#39311929">parent</a><span>|</span><a href="#39307846">next</a><span>|</span><label class="collapse" for="c-39312162">[-]</label><label class="expand" for="c-39312162">[1 more]</label></div><br/><div class="children"><div class="content">If you’re serious about production deployments vLLM is the best open source product out there. (I’m not affiliated with it)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39307846" class="c"><input type="checkbox" id="c-39307846" checked=""/><div class="controls bullet"><span class="by">shay_ker</span><span>|</span><a href="#39309239">prev</a><span>|</span><a href="#39311486">next</a><span>|</span><label class="collapse" for="c-39307846">[-]</label><label class="expand" for="c-39307846">[2 more]</label></div><br/><div class="children"><div class="content">Is Ollama effectively a dockerized HTTP server that calls llama.cpp directly? For the exception of this newly added OpenAI API ;)</div><br/><div id="39309577" class="c"><input type="checkbox" id="c-39309577" checked=""/><div class="controls bullet"><span class="by">okwhateverdude</span><span>|</span><a href="#39307846">parent</a><span>|</span><a href="#39311486">next</a><span>|</span><label class="collapse" for="c-39309577">[-]</label><label class="expand" for="c-39309577">[1 more]</label></div><br/><div class="children"><div class="content">More like an easy-mode llama.cpp that does a cgo wrapping of the lib (now; before they built patched llama.cpp runners and did IPC and managed child processes) and it does a few clever things to auto figure out layer splits (if you have meager GPU VRAM). The easy mode is that it will auto-load whatever model you&#x27;d like per request. They also implement docker-like layers for their representation of a model allowing you to overlay parameters of configuration and tag it. So far, it has been trivial to mix and match different models (or even the same models just with different parameters) for different tasks within the same application.</div><br/></div></div></div></div><div id="39311486" class="c"><input type="checkbox" id="c-39311486" checked=""/><div class="controls bullet"><span class="by">SamPatt</span><span>|</span><a href="#39307846">prev</a><span>|</span><a href="#39308946">next</a><span>|</span><label class="collapse" for="c-39311486">[-]</label><label class="expand" for="c-39311486">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is great. If you want a GUI, LMStudio and Jan are great too.<p>I&#x27;m building a React Native app to connect mobile devices to local LLM servers run with these programs.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;sampatt&#x2F;lookma">https:&#x2F;&#x2F;github.com&#x2F;sampatt&#x2F;lookma</a></div><br/></div></div><div id="39308946" class="c"><input type="checkbox" id="c-39308946" checked=""/><div class="controls bullet"><span class="by">init0</span><span>|</span><a href="#39311486">prev</a><span>|</span><a href="#39307671">next</a><span>|</span><label class="collapse" for="c-39308946">[-]</label><label class="expand" for="c-39308946">[2 more]</label></div><br/><div class="children"><div class="content">Trying to openai am I missing something?<p><pre><code>    import OpenAI from &#x27;openai&#x27;

    const openai = new OpenAI({
      baseURL: &#x27;http:&#x2F;&#x2F;localhost:11434&#x2F;v1&#x27;,
      apiKey: &#x27;ollama&#x27;, &#x2F;&#x2F; required but unused
    })

    const chatCompletion = await 
      openai.chat.completions.create({
      model: &#x27;llama2&#x27;,
      messages: [{ role: &#x27;user&#x27;, content: &#x27;Why is the sky blue?&#x27; }],
    })

    console.log(completion.choices[0].message.content)
</code></pre>
I am getting the below error:<p><pre><code>    return new NotFoundError(status, error, message, headers);
                   ^
    NotFoundError: 404 404 page not found</code></pre></div><br/><div id="39308959" class="c"><input type="checkbox" id="c-39308959" checked=""/><div class="controls bullet"><span class="by">xena</span><span>|</span><a href="#39308946">parent</a><span>|</span><a href="#39307671">next</a><span>|</span><label class="collapse" for="c-39308959">[-]</label><label class="expand" for="c-39308959">[1 more]</label></div><br/><div class="children"><div class="content">Remove the v1</div><br/></div></div></div></div><div id="39307671" class="c"><input type="checkbox" id="c-39307671" checked=""/><div class="controls bullet"><span class="by">theogravity</span><span>|</span><a href="#39308946">prev</a><span>|</span><a href="#39308987">next</a><span>|</span><label class="collapse" for="c-39307671">[-]</label><label class="expand" for="c-39307671">[11 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t LangChain supposed to provide abstractions that 3rd parties shouldn&#x27;t need to conform to OpenAI&#x27;s API contract?<p>I know not everyone uses LangChain, but I thought that was one of the primary use-cases for it.</div><br/><div id="39307884" class="c"><input type="checkbox" id="c-39307884" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39307671">parent</a><span>|</span><a href="#39308987">next</a><span>|</span><label class="collapse" for="c-39307884">[-]</label><label class="expand" for="c-39307884">[10 more]</label></div><br/><div class="children"><div class="content">Which just then creates lock-in for LangChain&#x27;s abstractions.</div><br/><div id="39307960" class="c"><input type="checkbox" id="c-39307960" checked=""/><div class="controls bullet"><span class="by">ludwik</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39307884">parent</a><span>|</span><a href="#39308987">next</a><span>|</span><label class="collapse" for="c-39307960">[-]</label><label class="expand" for="c-39307960">[9 more]</label></div><br/><div class="children"><div class="content">Which are pretty awful btw - every project at my job that started with LangChain openly regrets it - the abstractions, instead of making hard things easy, trend to make the way things hard (and hard to debug and maintain).</div><br/><div id="39309459" class="c"><input type="checkbox" id="c-39309459" checked=""/><div class="controls bullet"><span class="by">emilsedgh</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39307960">parent</a><span>|</span><a href="#39308069">next</a><span>|</span><label class="collapse" for="c-39309459">[-]</label><label class="expand" for="c-39309459">[1 more]</label></div><br/><div class="children"><div class="content">We use langchain and don&#x27;t regret it at all. As a matter of fact, it is likely that without lc we would&#x27;ve failed to deliver our product.<p>The main reason is langsmith. (But there are other reasons too). Because of langchain we got &quot;free&quot; (as in no development necessary) langsmith integration and now I can debug my llm.<p>Before that it was trying to make sense of whats happening inside my app within hundreds and hundreds of lines of text which was extremely painful and time consuming.<p>Also, lc people are extremely nice and very open&#x2F;quick to feedback.<p>The abstractions are too verbose, and make it difficult, but the value we&#x27;ve been getting from lc as a whole cannot be overstated.<p>other benefits:<p>* easy integrations with vector stores (we tried several until landing on one but switching was easy)<p>* easily adopting features like chat history, that would&#x27;ve taken us ages to determine correctly on our own<p>people that complain and say &quot;just call your llm directly&quot;: If your usecase is that simple, of course. using lc for that usecase is also almost equally simple.<p>But if you have more complex use cases, lc provides some verbose abstractions, but it&#x27;s very likely that you would&#x27;ve done the same.</div><br/></div></div><div id="39308015" class="c"><input type="checkbox" id="c-39308015" checked=""/><div class="controls bullet"><span class="by">phantompeace</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39307960">parent</a><span>|</span><a href="#39308069">prev</a><span>|</span><a href="#39308987">next</a><span>|</span><label class="collapse" for="c-39308015">[-]</label><label class="expand" for="c-39308015">[6 more]</label></div><br/><div class="children"><div class="content">What are some better options?</div><br/><div id="39310963" class="c"><input type="checkbox" id="c-39310963" checked=""/><div class="controls bullet"><span class="by">bdcs</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39308015">parent</a><span>|</span><a href="#39308279">next</a><span>|</span><label class="collapse" for="c-39310963">[-]</label><label class="expand" for="c-39310963">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.llamaindex.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.llamaindex.ai&#x2F;</a> is much better IMO, but it&#x27;s definitely a case of boilerplate-y, well-supported incumbent vs smaller, better, less supported (e.g. Java vs Python in the 00s or something like that). Depends on your team and your needs.<p>Also Autogen seems popular and well-ish liked <a href="https:&#x2F;&#x2F;microsoft.github.io&#x2F;autogen&#x2F;" rel="nofollow">https:&#x2F;&#x2F;microsoft.github.io&#x2F;autogen&#x2F;</a><p>LangChain definitely has the most market-&#x2F;mind- share. For example, GCP has a blog post on supporting it: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;ai-machine-learning&#x2F;deploy-langchain-on-cloud-run-with-langserve" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;ai-machine-learning&#x2F;d...</a></div><br/></div></div><div id="39308279" class="c"><input type="checkbox" id="c-39308279" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39308015">parent</a><span>|</span><a href="#39310963">prev</a><span>|</span><a href="#39308052">next</a><span>|</span><label class="collapse" for="c-39308279">[-]</label><label class="expand" for="c-39308279">[1 more]</label></div><br/><div class="children"><div class="content">Have a fairly thin layer than wraps the underlying LLM behind a common API (e.g., Ollama as being discussed here, Oobabooga, etc.) and leaves the application-level stuff for the application rather than a framework like LangChain.<p>(Better for certain use cases, that is, I’m not saying LangChain doesn&#x27;t have uses.)</div><br/></div></div><div id="39308052" class="c"><input type="checkbox" id="c-39308052" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39308015">parent</a><span>|</span><a href="#39308279">prev</a><span>|</span><a href="#39311332">next</a><span>|</span><label class="collapse" for="c-39308052">[-]</label><label class="expand" for="c-39308052">[1 more]</label></div><br/><div class="children"><div class="content">Not using an abstraction at all and avoiding the technical debt it causes.</div><br/></div></div><div id="39311332" class="c"><input type="checkbox" id="c-39311332" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39308015">parent</a><span>|</span><a href="#39308052">prev</a><span>|</span><a href="#39308061">next</a><span>|</span><label class="collapse" for="c-39311332">[-]</label><label class="expand" for="c-39311332">[1 more]</label></div><br/><div class="children"><div class="content">Haystack is much better option and way alot flexible, scalable</div><br/></div></div><div id="39308061" class="c"><input type="checkbox" id="c-39308061" checked=""/><div class="controls bullet"><span class="by">hospitalJail</span><span>|</span><a href="#39307671">root</a><span>|</span><a href="#39308015">parent</a><span>|</span><a href="#39311332">prev</a><span>|</span><a href="#39308987">next</a><span>|</span><label class="collapse" for="c-39308061">[-]</label><label class="expand" for="c-39308061">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t use langchain, just make the calls?<p>Its what I ended up doing.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39308987" class="c"><input type="checkbox" id="c-39308987" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39307671">prev</a><span>|</span><a href="#39312147">next</a><span>|</span><label class="collapse" for="c-39308987">[-]</label><label class="expand" for="c-39308987">[1 more]</label></div><br/><div class="children"><div class="content">I had trouble installing Ollama last time I tried, I&#x27;m going to try again tomorrow.<p>I&#x27;ve already got a web UI that &quot;should&quot; work with anything that matches OpenAI&#x27;s chat API, though I&#x27;m sure everyone here knows how reliable air-quotes like that are when a developer says them.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;BenWheatley&#x2F;YetAnotherChatUI">https:&#x2F;&#x2F;github.com&#x2F;BenWheatley&#x2F;YetAnotherChatUI</a></div><br/></div></div><div id="39312147" class="c"><input type="checkbox" id="c-39312147" checked=""/><div class="controls bullet"><span class="by">philprx</span><span>|</span><a href="#39308987">prev</a><span>|</span><a href="#39307860">next</a><span>|</span><label class="collapse" for="c-39312147">[-]</label><label class="expand" for="c-39312147">[1 more]</label></div><br/><div class="children"><div class="content">How does Ollama compare to LocalGPT ?</div><br/></div></div><div id="39307860" class="c"><input type="checkbox" id="c-39307860" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39312147">prev</a><span>|</span><a href="#39310268">next</a><span>|</span><label class="collapse" for="c-39307860">[-]</label><label class="expand" for="c-39307860">[1 more]</label></div><br/><div class="children"><div class="content">I wonder why ollama didn&#x27;t namespace the path (e.g. under &quot;&#x2F;openai&quot;) but in any case this is great for interoperability.</div><br/></div></div><div id="39310268" class="c"><input type="checkbox" id="c-39310268" checked=""/><div class="controls bullet"><span class="by">laingc</span><span>|</span><a href="#39307860">prev</a><span>|</span><a href="#39310929">next</a><span>|</span><label class="collapse" for="c-39310268">[-]</label><label class="expand" for="c-39310268">[4 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the current state-of-the-art in deploying large, &quot;self-hosted&quot; models to scalable infrastructure? (e.g. AWS or k8s)<p>Example use case would be to support a web application with, say, 100k DAU.</div><br/><div id="39310401" class="c"><input type="checkbox" id="c-39310401" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#39310268">parent</a><span>|</span><a href="#39310929">next</a><span>|</span><label class="collapse" for="c-39310401">[-]</label><label class="expand" for="c-39310401">[3 more]</label></div><br/><div class="children"><div class="content">Nvidia Triton Inference Server with the TensorRT-LLM backend:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;triton-inference-server&#x2F;tensorrtllm_backend">https:&#x2F;&#x2F;github.com&#x2F;triton-inference-server&#x2F;tensorrtllm_backe...</a><p>It’s used by Mistral, AWS, Cloudflare, and countless others.<p>vLLM, HF TGI, Rayserve, etc are certainly viable but Triton has many truly unique and very powerful features (not to mention performance).<p>100k DAU doesn’t mean much, you’d need to get a better understanding of the application, input tokens, generated output tokens, request rates, peaks, etc not to mention required time to first token, tokens per second, etc.<p>Anyway, the point is Triton is just about the only thing out there for use in this general range and up.</div><br/><div id="39312404" class="c"><input type="checkbox" id="c-39312404" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#39310268">root</a><span>|</span><a href="#39310401">parent</a><span>|</span><a href="#39312130">next</a><span>|</span><label class="collapse" for="c-39312404">[-]</label><label class="expand" for="c-39312404">[1 more]</label></div><br/><div class="children"><div class="content">Do you have a source on Mistral API, etc. being based on TensoRT-LLM? And what are the main distinguishing features?<p>What I like about vLLM is the following:<p>- It exposes AsyncLLMEngine, which can be easily wrapped in any API you&#x27;d like.<p>- It has a logit processor API making it simple to integrate custom sampling logic.<p>- It has decent support for interference of quantized models.</div><br/></div></div><div id="39312130" class="c"><input type="checkbox" id="c-39312130" checked=""/><div class="controls bullet"><span class="by">laingc</span><span>|</span><a href="#39310268">root</a><span>|</span><a href="#39310401">parent</a><span>|</span><a href="#39312404">prev</a><span>|</span><a href="#39310929">next</a><span>|</span><label class="collapse" for="c-39312130">[-]</label><label class="expand" for="c-39312130">[1 more]</label></div><br/><div class="children"><div class="content">Very helpful answer, thank you!</div><br/></div></div></div></div></div></div><div id="39310929" class="c"><input type="checkbox" id="c-39310929" checked=""/><div class="controls bullet"><span class="by">678j5367</span><span>|</span><a href="#39310268">prev</a><span>|</span><a href="#39307867">next</a><span>|</span><label class="collapse" for="c-39310929">[-]</label><label class="expand" for="c-39310929">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is very good and runs better than some of the other tooling I have tried. It also Just Works™. I ran Dolphin Mixtral 7b on a Raspberry pi 4 off a 32 gig SD card. Barely had room. I asked it for a cornbread recipe, stepped away for a few hours and it had generated two characters. I was surprised it got that far if I am being honest.</div><br/></div></div><div id="39307867" class="c"><input type="checkbox" id="c-39307867" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#39310929">prev</a><span>|</span><a href="#39308511">next</a><span>|</span><label class="collapse" for="c-39307867">[-]</label><label class="expand" for="c-39307867">[1 more]</label></div><br/><div class="children"><div class="content">Wow perfect timing. I personally love it. There’s so many projects out there that use OpenAI’s API whether you like it or not. I wanted to try this unit test writer notebook that OpenAI has but with Ollama. It was such a pain in the ass to fix it that I just didn’t bother cause it was just for fun. Now it should be 2 line of code change.</div><br/></div></div><div id="39308511" class="c"><input type="checkbox" id="c-39308511" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#39307867">prev</a><span>|</span><a href="#39307673">next</a><span>|</span><label class="collapse" for="c-39308511">[-]</label><label class="expand" for="c-39308511">[1 more]</label></div><br/><div class="children"><div class="content">Does ollama support loaders other than llamacpp? I&#x27;m using oobabooga with exllama2 to run exl2 quants on a dual NVIDIA gpu, and nothing else seems to beat performance of it.</div><br/></div></div><div id="39307673" class="c"><input type="checkbox" id="c-39307673" checked=""/><div class="controls bullet"><span class="by">Implicated</span><span>|</span><a href="#39308511">prev</a><span>|</span><a href="#39307849">next</a><span>|</span><label class="collapse" for="c-39307673">[-]</label><label class="expand" for="c-39307673">[1 more]</label></div><br/><div class="children"><div class="content">Love it! Ollama has been such a wonderful project (at least, for me).</div><br/></div></div><div id="39307849" class="c"><input type="checkbox" id="c-39307849" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39307673">prev</a><span>|</span><a href="#39309263">next</a><span>|</span><label class="collapse" for="c-39307849">[-]</label><label class="expand" for="c-39307849">[6 more]</label></div><br/><div class="children"><div class="content">ollama seems like taking a page from langchain book: develop something that&#x27;s open source but get it so popular that attracts VC money.<p>I never liked ollama, maybe because ollama builds on llama.cpp (a project I truly respect) but adds so much marketing bs.<p>For example, the @ollama account on twitter keeps shitposting on every possible thread to advertise ollama. The other day someone posted something about their Mac setup and @ollama said: &quot;You can run ollama on that Mac.&quot;<p>I don&#x27;t like it when +500 people are working tirelessly on llama.cpp and then guys like langchain, ollama, etc. rip off the benefits.</div><br/><div id="39308005" class="c"><input type="checkbox" id="c-39308005" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#39307849">parent</a><span>|</span><a href="#39312166">next</a><span>|</span><label class="collapse" for="c-39308005">[-]</label><label class="expand" for="c-39308005">[3 more]</label></div><br/><div class="children"><div class="content">Make something better, then. (I&#x27;m not being dismissive, I really genuinely mean it - please do)<p>I don&#x27;t know who is behind Ollama and don&#x27;t really care about them. I can agree with your disgust for VC &#x27;open source&#x27; projects. But there&#x27;s a reason they become popular and get investment: because they are valuable to people, and people use them.<p>If Ollama was just a wrapper over llama.cpp, then everyone would just use llama.cpp.<p>It&#x27;s not just marketing, either. Compare the README of llama.cpp to the Ollama homepage, notice the stark contrast of how difficult getting llama.cpp connected to some dumb JS app is compared to Ollama. That&#x27;s why it becomes valuable.<p>The same thing happened with Docker and we&#x27;re just now barely getting a viable alternative after Docker as a company imploded, Podman Desktop, and even then it still suffers from major instability on e.g. modern macs.<p>The sooner open source devs in general learn to make their projects usable by an average developer, the sooner it will be competitive with these VC-funded &#x27;open source&#x27; projects.</div><br/><div id="39308049" class="c"><input type="checkbox" id="c-39308049" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39307849">root</a><span>|</span><a href="#39308005">parent</a><span>|</span><a href="#39308677">next</a><span>|</span><label class="collapse" for="c-39308049">[-]</label><label class="expand" for="c-39308049">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp already has OpenAI compatible API.<p>It takes literally one line to install it (git clone and then make).<p>It takes one line to run the server as mentioned on their examples&#x2F;server README.<p><pre><code>    .&#x2F;server -m &lt;model&gt; &lt;any additional arguments like mmlock&gt;</code></pre></div><br/></div></div><div id="39308677" class="c"><input type="checkbox" id="c-39308677" checked=""/><div class="controls bullet"><span class="by">homarp</span><span>|</span><a href="#39307849">root</a><span>|</span><a href="#39308005">parent</a><span>|</span><a href="#39308049">prev</a><span>|</span><a href="#39312166">next</a><span>|</span><label class="collapse" for="c-39308677">[-]</label><label class="expand" for="c-39308677">[1 more]</label></div><br/><div class="children"><div class="content">&gt;notice the stark contrast of how difficult getting llama.cpp connected to some dumb JS app is compared to Ollama.<p>Sorry, I&#x27;m new to ollama &#x27;ecosystem&#x27;.<p>From llama.cpp readme, I ctrl-F-ed &quot;Node.js: withcatai&#x2F;node-llama-cpp&quot;
and from there, I got to <a href="https:&#x2F;&#x2F;withcatai.github.io&#x2F;node-llama-cpp&#x2F;guide&#x2F;" rel="nofollow">https:&#x2F;&#x2F;withcatai.github.io&#x2F;node-llama-cpp&#x2F;guide&#x2F;</a><p>Can you explain how ollama does it &#x27;easier&#x27; ?</div><br/></div></div></div></div><div id="39312166" class="c"><input type="checkbox" id="c-39312166" checked=""/><div class="controls bullet"><span class="by">udev4096</span><span>|</span><a href="#39307849">parent</a><span>|</span><a href="#39308005">prev</a><span>|</span><a href="#39308300">next</a><span>|</span><label class="collapse" for="c-39312166">[-]</label><label class="expand" for="c-39312166">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know ollama was VC funded</div><br/></div></div><div id="39308300" class="c"><input type="checkbox" id="c-39308300" checked=""/><div class="controls bullet"><span class="by">FanaHOVA</span><span>|</span><a href="#39307849">parent</a><span>|</span><a href="#39312166">prev</a><span>|</span><a href="#39309263">next</a><span>|</span><label class="collapse" for="c-39308300">[-]</label><label class="expand" for="c-39308300">[1 more]</label></div><br/><div class="children"><div class="content">ggml is also VC backed, so that has nothing to do with it.</div><br/></div></div></div></div><div id="39309263" class="c"><input type="checkbox" id="c-39309263" checked=""/><div class="controls bullet"><span class="by">LightMachine</span><span>|</span><a href="#39307849">prev</a><span>|</span><a href="#39308121">next</a><span>|</span><label class="collapse" for="c-39309263">[-]</label><label class="expand" for="c-39309263">[2 more]</label></div><br/><div class="children"><div class="content">Gemini Ultra release day, and a minor post on ollama OpenAI compatibility gets more points lol</div><br/><div id="39309699" class="c"><input type="checkbox" id="c-39309699" checked=""/><div class="controls bullet"><span class="by">subarctic</span><span>|</span><a href="#39309263">parent</a><span>|</span><a href="#39308121">next</a><span>|</span><label class="collapse" for="c-39309699">[-]</label><label class="expand" for="c-39309699">[1 more]</label></div><br/><div class="children"><div class="content">Who cares about another closed LLM that&#x27;s no better than GPT 4? I think there&#x27;s more exciting potential in open weights LLMs that you can run on your own machine and do whatever you want with.</div><br/></div></div></div></div><div id="39308121" class="c"><input type="checkbox" id="c-39308121" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#39309263">prev</a><span>|</span><a href="#39309698">next</a><span>|</span><label class="collapse" for="c-39308121">[-]</label><label class="expand" for="c-39308121">[5 more]</label></div><br/><div class="children"><div class="content">Smart. When they do come, will the embedding vectors be OpenAI compatible? I assume this is quite hard to do.</div><br/><div id="39308212" class="c"><input type="checkbox" id="c-39308212" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39308121">parent</a><span>|</span><a href="#39308162">next</a><span>|</span><label class="collapse" for="c-39308212">[-]</label><label class="expand" for="c-39308212">[3 more]</label></div><br/><div class="children"><div class="content">Embeddings as an I&#x2F;O schema are just text-in, a list of numbers out. There are very few embedding models which require enough preprocessing to warrant an abstraction. (A soft example is the new nomic-embed-text-v1, which requires adding prefix annotations: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;nomic-ai&#x2F;nomic-embed-text-v1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;nomic-ai&#x2F;nomic-embed-text-v1</a> )</div><br/><div id="39309145" class="c"><input type="checkbox" id="c-39309145" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#39308121">root</a><span>|</span><a href="#39308212">parent</a><span>|</span><a href="#39308162">next</a><span>|</span><label class="collapse" for="c-39309145">[-]</label><label class="expand" for="c-39309145">[2 more]</label></div><br/><div class="children"><div class="content">Yes of course (syntactically it is just float[] getEmbeddings(text)) but are the numbers close to what OpenAI would produce? I assume no.</div><br/><div id="39309215" class="c"><input type="checkbox" id="c-39309215" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39308121">root</a><span>|</span><a href="#39309145">parent</a><span>|</span><a href="#39308162">next</a><span>|</span><label class="collapse" for="c-39309215">[-]</label><label class="expand" for="c-39309215">[1 more]</label></div><br/><div class="children"><div class="content">This submission only about I&#x2F;O schema: the embeddings themselves are dependent on the model, and since OpenAI&#x27;s models are closed source no one can reproduce them.<p>No direct embedding model can be cross-compatable. (exception: constrastive learning models like CLIP)</div><br/></div></div></div></div></div></div><div id="39308162" class="c"><input type="checkbox" id="c-39308162" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#39308121">parent</a><span>|</span><a href="#39308212">prev</a><span>|</span><a href="#39309698">next</a><span>|</span><label class="collapse" for="c-39308162">[-]</label><label class="expand" for="c-39308162">[1 more]</label></div><br/><div class="children"><div class="content">Probably not, embedding vectors aren&#x27;t conpatible across different embedding models, and other tools presenting OAI-compatible APIs don&#x27;t use OAI-compatible embedding models (e.g., oobabooga lets you configure different embeddings models, but none of them produce compatible vectors to the OAI ones.)</div><br/></div></div></div></div><div id="39309698" class="c"><input type="checkbox" id="c-39309698" checked=""/><div class="controls bullet"><span class="by">arbuge</span><span>|</span><a href="#39308121">prev</a><span>|</span><a href="#39312127">next</a><span>|</span><label class="collapse" for="c-39309698">[-]</label><label class="expand" for="c-39309698">[6 more]</label></div><br/><div class="children"><div class="content">Genuinely curious to ask HN this: what are you using local models for?</div><br/><div id="39312842" class="c"><input type="checkbox" id="c-39312842" checked=""/><div class="controls bullet"><span class="by">RamblingCTO</span><span>|</span><a href="#39309698">parent</a><span>|</span><a href="#39309902">next</a><span>|</span><label class="collapse" for="c-39312842">[-]</label><label class="expand" for="c-39312842">[1 more]</label></div><br/><div class="children"><div class="content">I built myself a hacky alternative to the chat UI from openAI and implemented ollama to test different models locally. Also, openAI chat sucks, the API doesn&#x27;t seem to suck as much. Chat is just useless for coding at this point.<p>&#x2F;e: <a href="https:&#x2F;&#x2F;github.com&#x2F;ChristianSch&#x2F;theta">https:&#x2F;&#x2F;github.com&#x2F;ChristianSch&#x2F;theta</a></div><br/></div></div><div id="39309902" class="c"><input type="checkbox" id="c-39309902" checked=""/><div class="controls bullet"><span class="by">codazoda</span><span>|</span><a href="#39309698">parent</a><span>|</span><a href="#39312842">prev</a><span>|</span><a href="#39309750">next</a><span>|</span><label class="collapse" for="c-39309902">[-]</label><label class="expand" for="c-39309902">[1 more]</label></div><br/><div class="children"><div class="content">I got the most use out of it on an airplane with no wifi. It let me keep working on a coding solution without the internet because I could ask it quick questions. Magic.</div><br/></div></div><div id="39309750" class="c"><input type="checkbox" id="c-39309750" checked=""/><div class="controls bullet"><span class="by">mysteria</span><span>|</span><a href="#39309698">parent</a><span>|</span><a href="#39309902">prev</a><span>|</span><a href="#39309724">next</a><span>|</span><label class="collapse" for="c-39309750">[-]</label><label class="expand" for="c-39309750">[1 more]</label></div><br/><div class="children"><div class="content">I use it for personal entertainment, both writing and roleplaying. I put quite a bit of effort into my own responses and actively edit the output to get decent results out of the larger 30B and 70B models. Trying out different models and wrangling the LLM to write what you want is part of the fun.</div><br/></div></div><div id="39309724" class="c"><input type="checkbox" id="c-39309724" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#39309698">parent</a><span>|</span><a href="#39309750">prev</a><span>|</span><a href="#39310456">next</a><span>|</span><label class="collapse" for="c-39309724">[-]</label><label class="expand" for="c-39309724">[1 more]</label></div><br/><div class="children"><div class="content">Experimenting, as well as a cheaper alternative to cloud&#x2F;paid models. Local models don&#x27;t have the encyclopaedic knowledge as huge models such as GPT 3.5&#x2F;4, but they can perform tasks well.</div><br/></div></div><div id="39310456" class="c"><input type="checkbox" id="c-39310456" checked=""/><div class="controls bullet"><span class="by">dimask</span><span>|</span><a href="#39309698">parent</a><span>|</span><a href="#39309724">prev</a><span>|</span><a href="#39312127">next</a><span>|</span><label class="collapse" for="c-39310456">[-]</label><label class="expand" for="c-39310456">[1 more]</label></div><br/><div class="children"><div class="content">I used them to extract data from relatively unstructured reports into structured csv format. For privacy&#x2F;gdpr reasons it was not something I could use an online model for. Saved me from a lot of manual work, and it did not hallucinate stuff as far as I could see.</div><br/></div></div></div></div><div id="39312127" class="c"><input type="checkbox" id="c-39312127" checked=""/><div class="controls bullet"><span class="by">udev4096</span><span>|</span><a href="#39309698">prev</a><span>|</span><a href="#39312020">next</a><span>|</span><label class="collapse" for="c-39312127">[-]</label><label class="expand" for="c-39312127">[1 more]</label></div><br/><div class="children"><div class="content">Awesome!</div><br/></div></div><div id="39312020" class="c"><input type="checkbox" id="c-39312020" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#39312127">prev</a><span>|</span><a href="#39307741">next</a><span>|</span><label class="collapse" for="c-39312020">[-]</label><label class="expand" for="c-39312020">[1 more]</label></div><br/><div class="children"><div class="content">Does ollama support ROCm? It&#x27;s not clear from their github repo if it does.</div><br/></div></div><div id="39307741" class="c"><input type="checkbox" id="c-39307741" checked=""/><div class="controls bullet"><span class="by">thedangler</span><span>|</span><a href="#39312020">prev</a><span>|</span><label class="collapse" for="c-39307741">[-]</label><label class="expand" for="c-39307741">[12 more]</label></div><br/><div class="children"><div class="content">Is Ollama model I can use locally to use for my own project and keep my data secure?</div><br/><div id="39307781" class="c"><input type="checkbox" id="c-39307781" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#39307741">parent</a><span>|</span><a href="#39307763">next</a><span>|</span><label class="collapse" for="c-39307781">[-]</label><label class="expand" for="c-39307781">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is an easy way to run local models on Mac&#x2F;linux. See <a href="https:&#x2F;&#x2F;ollama.ai">https:&#x2F;&#x2F;ollama.ai</a> they have a web UI and a terminal&#x2F;server approach</div><br/></div></div><div id="39307763" class="c"><input type="checkbox" id="c-39307763" checked=""/><div class="controls bullet"><span class="by">MOARDONGZPLZ</span><span>|</span><a href="#39307741">parent</a><span>|</span><a href="#39307781">prev</a><span>|</span><label class="collapse" for="c-39307763">[-]</label><label class="expand" for="c-39307763">[10 more]</label></div><br/><div class="children"><div class="content">I would not explicitly count on that. I’m a big fan of Ollama and use it every day but they do have some dark patterns that make me question a usecase where data security is a requirement. So I don’t use it where that is something that’s important.</div><br/><div id="39309003" class="c"><input type="checkbox" id="c-39309003" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39307763">parent</a><span>|</span><a href="#39307769">next</a><span>|</span><label class="collapse" for="c-39309003">[-]</label><label class="expand" for="c-39309003">[1 more]</label></div><br/><div class="children"><div class="content">Ollama team are a few very down to earth, smart people. I really liked the folks I&#x27;ve met. I can&#x27;t imagine they are doing anything malicious and I&#x27;m sure would address any issues (log them on GitHub) &#x2F; entertain PRs to address any legitimate concerns</div><br/></div></div><div id="39307769" class="c"><input type="checkbox" id="c-39307769" checked=""/><div class="controls bullet"><span class="by">mbernstein</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39307763">parent</a><span>|</span><a href="#39309003">prev</a><span>|</span><a href="#39307787">next</a><span>|</span><label class="collapse" for="c-39307769">[-]</label><label class="expand" for="c-39307769">[1 more]</label></div><br/><div class="children"><div class="content">Examples?</div><br/></div></div><div id="39307787" class="c"><input type="checkbox" id="c-39307787" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39307763">parent</a><span>|</span><a href="#39307769">prev</a><span>|</span><a href="#39308095">next</a><span>|</span><label class="collapse" for="c-39307787">[-]</label><label class="expand" for="c-39307787">[6 more]</label></div><br/><div class="children"><div class="content">like what? If you&#x27;re gonna accuse a project of shady stuff, at least give examples :)</div><br/><div id="39308022" class="c"><input type="checkbox" id="c-39308022" checked=""/><div class="controls bullet"><span class="by">MOARDONGZPLZ</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39307787">parent</a><span>|</span><a href="#39308095">next</a><span>|</span><label class="collapse" for="c-39308022">[-]</label><label class="expand" for="c-39308022">[5 more]</label></div><br/><div class="children"><div class="content">The same examples given every time ollama is posted. Off the top of my head the installer silently adds login items with no way to opt out, spawns persistent processes in the background in addition to the application with unclear purposes, no info on install about the install itself, doesn’t let you back out of the installer when it requests admin access. Basically lots of dark patterns in the non-standard installer.<p>Reminds me of how Zoom got it start with the “growth hacking” of the installation. Not enough to keep me from using it, but enough for me to keep from using it for anything serious or secure.</div><br/><div id="39308931" class="c"><input type="checkbox" id="c-39308931" checked=""/><div class="controls bullet"><span class="by">Patrick_Devine</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39308022">parent</a><span>|</span><a href="#39308106">next</a><span>|</span><label class="collapse" for="c-39308931">[-]</label><label class="expand" for="c-39308931">[2 more]</label></div><br/><div class="children"><div class="content">These are some fair points. There definitely wasn&#x27;t an intention of &quot;growth hacking&quot;, but just trying to get a lot of things done with only a few people in a short period of time. Requiring admin access really sucks though and is something we&#x27;ve wanted to get rid of for a while.</div><br/><div id="39311798" class="c"><input type="checkbox" id="c-39311798" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39308931">parent</a><span>|</span><a href="#39308106">next</a><span>|</span><label class="collapse" for="c-39311798">[-]</label><label class="expand" for="c-39311798">[1 more]</label></div><br/><div class="children"><div class="content">I am running ollama in the CLI, under screen, and always disable the ollama daemon. It&#x27;s hard to configure, while CLI it is just adding a few env vars in front.</div><br/></div></div></div></div><div id="39308106" class="c"><input type="checkbox" id="c-39308106" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39308022">parent</a><span>|</span><a href="#39308931">prev</a><span>|</span><a href="#39308095">next</a><span>|</span><label class="collapse" for="c-39308106">[-]</label><label class="expand" for="c-39308106">[2 more]</label></div><br/><div class="children"><div class="content">Show me the code</div><br/><div id="39308371" class="c"><input type="checkbox" id="c-39308371" checked=""/><div class="controls bullet"><span class="by">MOARDONGZPLZ</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39308106">parent</a><span>|</span><a href="#39308095">next</a><span>|</span><label class="collapse" for="c-39308371">[-]</label><label class="expand" for="c-39308371">[1 more]</label></div><br/><div class="children"><div class="content">Install it on MacOs. Observe for yourself. This is a repeated problem mentioned in every thread. If you need help on the part about checking to see how many processes are running, let me know and I can assist. The rest are things you will observe, step by step, during the install process.</div><br/></div></div></div></div></div></div></div></div><div id="39308095" class="c"><input type="checkbox" id="c-39308095" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#39307741">root</a><span>|</span><a href="#39307763">parent</a><span>|</span><a href="#39307787">prev</a><span>|</span><label class="collapse" for="c-39308095">[-]</label><label class="expand" for="c-39308095">[1 more]</label></div><br/><div class="children"><div class="content">Opensource project so you can find evidence of foul play . Prove it or it is bs</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>