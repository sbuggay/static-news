<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718874079701" as="style"/><link rel="stylesheet" href="styles.css?v=1718874079701"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ssi.inc">Safe Superintelligence Inc.</a> <span class="domain">(<a href="https://ssi.inc">ssi.inc</a>)</span></div><div class="subtext"><span>nick_pou</span> | <span>468 comments</span></div><br/><div><div id="40730485" class="c"><input type="checkbox" id="c-40730485" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730300">next</a><span>|</span><label class="collapse" for="c-40730485">[-]</label><label class="expand" for="c-40730485">[73 more]</label></div><br/><div class="children"><div class="content">I understand the concern that a &quot;superintelligence&quot; will emerge that will escape its bounds and threaten humanity. That is a risk.<p>My bigger, and more pressing worry, is that a &quot;superintelligence&quot; will emerge that does not escape its bounds, and the question will be which humans control it. Look no further than history to see what happens when humans acquire great power. The &quot;cold war&quot; nuclear arms race, which brought the world to the brink of (at least partial) annihilation, is a good recent example.<p>Quis custodiet ipsos custodes? -- That is my biggest concern.<p>Update: I&#x27;m not as worried about Ilya et al as commercial companies (including formerly &quot;open&quot; OpenAI) discovering AGI.</div><br/><div id="40734656" class="c"><input type="checkbox" id="c-40734656" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40734672">next</a><span>|</span><label class="collapse" for="c-40734656">[-]</label><label class="expand" for="c-40734656">[4 more]</label></div><br/><div class="children"><div class="content">It’s just clearly military R&amp;D at this point.<p>And it’s not even a little bit controversial that cutting edge military R&amp;D is classified in general and to an extreme in wartime.<p>The new thing is the lie that it’s a consumer offering. What’s new is giving the helm to shady failed social network founders with no accountability.<p>These people aren’t retired generals with combat experience. They aren’t tenured professors at Princeton IAS on a Nobel shortlist and encumbered by TS clearance.<p>They’re godawful almost ran psychos who never built anything that wasn’t extractive and owe their position in the world to pg’s partisanship 15 fucking years ago.</div><br/><div id="40735395" class="c"><input type="checkbox" id="c-40735395" checked=""/><div class="controls bullet"><span class="by">sgregnt</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734656">parent</a><span>|</span><a href="#40734672">next</a><span>|</span><label class="collapse" for="c-40735395">[-]</label><label class="expand" for="c-40735395">[3 more]</label></div><br/><div class="children"><div class="content">To me it is not clear at all, can you please elaborate why you make such a strong claim?</div><br/><div id="40735780" class="c"><input type="checkbox" id="c-40735780" checked=""/><div class="controls bullet"><span class="by">WatchDog</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735395">parent</a><span>|</span><a href="#40735631">next</a><span>|</span><label class="collapse" for="c-40735780">[-]</label><label class="expand" for="c-40735780">[1 more]</label></div><br/><div class="children"><div class="content">One data point:<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;openai-appoints-retired-us-army-general&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;openai-appoints-retired-us-army-gen...</a></div><br/></div></div><div id="40735631" class="c"><input type="checkbox" id="c-40735631" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735395">parent</a><span>|</span><a href="#40735780">prev</a><span>|</span><a href="#40734672">next</a><span>|</span><label class="collapse" for="c-40735631">[-]</label><label class="expand" for="c-40735631">[1 more]</label></div><br/><div class="children"><div class="content">My opinion is based on a lot more first hand experience than most, some of which I’m at liberty to share and some that I’m not and therefore becomes “color”.<p>But I’m a nobody, Edward Snowden has a far more convincing track record on calling abuses of power: <a href="https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;edward-snowden-on-openai-s-decision-to-put-a-former-nsa-director-on-its-board&#x2F;821456" rel="nofollow">https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;edward-snowden-on-openai-s-de...</a></div><br/></div></div></div></div></div></div><div id="40734672" class="c"><input type="checkbox" id="c-40734672" checked=""/><div class="controls bullet"><span class="by">dreamcompiler</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40734656">prev</a><span>|</span><a href="#40731050">next</a><span>|</span><label class="collapse" for="c-40734672">[-]</label><label class="expand" for="c-40734672">[8 more]</label></div><br/><div class="children"><div class="content">AGI is still a long way off. The history of AI goes back 65 years and there have been probably a dozen episodes where people said &quot;AGI is right around the corner&quot; because some program did something surprising and impressive. It always turns out human intelligence is much, <i>much</i> harder than we think it is.<p>I saw a tweet the other day that sums up the current situation perfectly: &quot;I don&#x27;t need AI to paint pictures and write poetry so I have more time to fold laundry and wash dishes. I want the AI to do the laundry and dishes so I have more time to paint and write poetry.&quot;</div><br/><div id="40736446" class="c"><input type="checkbox" id="c-40736446" checked=""/><div class="controls bullet"><span class="by">rstuart4133</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734672">parent</a><span>|</span><a href="#40736030">next</a><span>|</span><label class="collapse" for="c-40736446">[-]</label><label class="expand" for="c-40736446">[1 more]</label></div><br/><div class="children"><div class="content">AGI does look like an unsolved problem right now, and a hard one at that. But I think it is wrong to think that it needs an AGI to cause total havoc.<p>I think my dyslexic namesake Prof Stuart Russell got it right. It humans won&#x27;t need an AGI to dominate and kill each other. Mosquitoes have killed far more people than war. Ask yourself how long will it take us to develop a neutral network as smart as a mosquito, because that&#x27;s all it will take.<p>It seems so simple, as the beastie only has 200,000 neurons.  Yet I&#x27;ve been programming for over 4 decades and for most of them it was evident neither I nor any of my contemporaries were remotely capable of emulating it. That&#x27;s still true if course. Never in my wildest dreams did it occur to me that repeated applications produce something I couldn&#x27;t, a mosquito brain.  Now that looks imminent.<p>Now I don&#x27;t know what to be now scared of. An AGI, and a artificial mosquito storm run by Pol Pot.</div><br/></div></div><div id="40736030" class="c"><input type="checkbox" id="c-40736030" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734672">parent</a><span>|</span><a href="#40736446">prev</a><span>|</span><a href="#40735825">next</a><span>|</span><label class="collapse" for="c-40736030">[-]</label><label class="expand" for="c-40736030">[1 more]</label></div><br/><div class="children"><div class="content">it’s harder than we thought so we leveraged machine learning to grow it, rather than creating it symbolically. The leaps in the last 5 years are far beyond anything in the prior half century, and make predictions of near term AGI much more than a “boy who cries wolf” scenario to anyone really paying attention.<p>I don’t understand how your second paragraph follows. It just seems to be whining that text and art generative models are easier than a fully fledged servant humanoid, which seems like a natural consequence of training data availability and deployment cost.</div><br/></div></div><div id="40735825" class="c"><input type="checkbox" id="c-40735825" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734672">parent</a><span>|</span><a href="#40736030">prev</a><span>|</span><a href="#40735378">next</a><span>|</span><label class="collapse" for="c-40735825">[-]</label><label class="expand" for="c-40735825">[1 more]</label></div><br/><div class="children"><div class="content">That &quot;tweet&quot; loses a veneer if you see that we value what has Worth as a collective treasure, and the more Value is produced the better - while that one engages in producing something of value is (hopefully but not necessarily) a good exercise in intelligent (literal sense) cultivation.<p>So, yes, if algorithms strict or loose could one day produce Art, and Thought, and Judgement, of Superior quality: very welcome.<p>Do not miss that the current world is increasingly complex to manage, and our lives, and Aids would be welcome. The situation is much more complex than that wish for leisure or even &quot;sport&quot; (literal sense).</div><br/></div></div><div id="40735378" class="c"><input type="checkbox" id="c-40735378" checked=""/><div class="controls bullet"><span class="by">dchichkov</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734672">parent</a><span>|</span><a href="#40735825">prev</a><span>|</span><a href="#40735452">next</a><span>|</span><label class="collapse" for="c-40735378">[-]</label><label class="expand" for="c-40735378">[2 more]</label></div><br/><div class="children"><div class="content">Well, copilots do precisely that, no?<p>Or you talking folding literal laundry, in which case this is more of a robotics problem, not the ASI, right?<p>You don&#x27;t need ASI to fold laundry, you do need to achieve reliable, safe and cost efficient robotics deployments.  These are different problems.</div><br/><div id="40736371" class="c"><input type="checkbox" id="c-40736371" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735378">parent</a><span>|</span><a href="#40735452">next</a><span>|</span><label class="collapse" for="c-40736371">[-]</label><label class="expand" for="c-40736371">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You don&#x27;t need ASI to fold laundry<p>Robots are garbage at manipulating objects, and it&#x27;s the software that&#x27;s lacking much more than the hardware.<p>Let&#x27;s say AGI is 10 and ASI is 11.<p>They&#x27;re saying we can&#x27;t even get this dial cranked up to 3, so we&#x27;re not anywhere close to 10 or 11.  You&#x27;re right that folding laundry doesn&#x27;t need 11, but that&#x27;s not relevant to their point.</div><br/></div></div></div></div><div id="40735452" class="c"><input type="checkbox" id="c-40735452" checked=""/><div class="controls bullet"><span class="by">bradleykingz</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734672">parent</a><span>|</span><a href="#40735378">prev</a><span>|</span><a href="#40731050">next</a><span>|</span><label class="collapse" for="c-40735452">[-]</label><label class="expand" for="c-40735452">[2 more]</label></div><br/><div class="children"><div class="content">The way I see it, this is simply a repetition of history.<p>El dorado, the fountain of youth, turning dirt to gold, the holy grail and now... superintelligence.</div><br/><div id="40736435" class="c"><input type="checkbox" id="c-40736435" checked=""/><div class="controls bullet"><span class="by">sneak</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735452">parent</a><span>|</span><a href="#40731050">next</a><span>|</span><label class="collapse" for="c-40736435">[-]</label><label class="expand" for="c-40736435">[1 more]</label></div><br/><div class="children"><div class="content">Human flight, resurrection (cardiopulmonary resuscitation machines), doubling human lifespans, instantaneous long distance communication, all of these things are simply pipe dreams.</div><br/></div></div></div></div></div></div><div id="40731050" class="c"><input type="checkbox" id="c-40731050" checked=""/><div class="controls bullet"><span class="by">gavin_gee</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40734672">prev</a><span>|</span><a href="#40730893">next</a><span>|</span><label class="collapse" for="c-40731050">[-]</label><label class="expand" for="c-40731050">[5 more]</label></div><br/><div class="children"><div class="content">This.<p>Every nation-state will be in the game. Private enterprise will be in the game. Bitcoin-funded individuals will be in the game. Criminal enterprises will be in the game.<p>How does one company building a safe version stop that?<p>If I have access to hardware and data how does a safety layer get enforced? Regulations are for organizations that care about public perception, the law, and stock prices.   Criminals and nation-states are not affected by these things<p>It seems to me enforcement is likely only possible at the hardware layer, which means the safety mechanisms need to be enforced throughout the hardware supply chain for training or inference. 
You don&#x27;t think the Chinese government or US government will ignore this if its in their interest?</div><br/><div id="40731577" class="c"><input type="checkbox" id="c-40731577" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731050">parent</a><span>|</span><a href="#40734762">next</a><span>|</span><label class="collapse" for="c-40731577">[-]</label><label class="expand" for="c-40731577">[3 more]</label></div><br/><div class="children"><div class="content">I think the honest view (and you can scoff at it) is that winning the SI race basically wins you the enforcement race for free</div><br/><div id="40733908" class="c"><input type="checkbox" id="c-40733908" checked=""/><div class="controls bullet"><span class="by">chii</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731577">parent</a><span>|</span><a href="#40734762">next</a><span>|</span><label class="collapse" for="c-40733908">[-]</label><label class="expand" for="c-40733908">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why it&#x27;s called an arms race, and it does not really end in this predictable manner.<p>The party that&#x27;s about to lose will use any extrajudicial means to reclaim their victory, regardless of the consequences, because their own destruction would be imminent otherwise. This ultimately leads to violence.</div><br/><div id="40736490" class="c"><input type="checkbox" id="c-40736490" checked=""/><div class="controls bullet"><span class="by">czl</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40733908">parent</a><span>|</span><a href="#40734762">next</a><span>|</span><label class="collapse" for="c-40736490">[-]</label><label class="expand" for="c-40736490">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The party that&#x27;s about to lose will use any extrajudicial means to reclaim their victory,<p>How will the party about lose know they are about to lose?<p>&gt; regardless of the consequences, because their own destruction would be imminent otherwise.<p>Why would AGI solve things using destruction? 
Consider how the most inteligent among us view our competition with other living beings. Is destruction the goal? So why would an even more intelligent AGI have that goal?</div><br/></div></div></div></div></div></div><div id="40734762" class="c"><input type="checkbox" id="c-40734762" checked=""/><div class="controls bullet"><span class="by">creer</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731050">parent</a><span>|</span><a href="#40731577">prev</a><span>|</span><a href="#40730893">next</a><span>|</span><label class="collapse" for="c-40734762">[-]</label><label class="expand" for="c-40734762">[1 more]</label></div><br/><div class="children"><div class="content">&quot;nation state&quot; doesn&#x27;t mean what you think it means.<p>More constructively, I don&#x27;t know that very much will stop even a hacker from getting time on the local corporate or university AI and get it to do some &quot;work&quot;. After all the first thing the other kind of hacker tried with generative AI is to get them to break out of their artificial boundaries, and hook them to internet resources. I don&#x27;t know that anyone has hooked up a wallet to one yet - but I have no doubt that people have tried. It will be fun.</div><br/></div></div></div></div><div id="40730893" class="c"><input type="checkbox" id="c-40730893" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40731050">prev</a><span>|</span><a href="#40735000">next</a><span>|</span><label class="collapse" for="c-40730893">[-]</label><label class="expand" for="c-40730893">[1 more]</label></div><br/><div class="children"><div class="content">+1 truth.<p>The problem is not just governments, I am concerned about large organized crime organizations and corporations also.<p>I think I am on the losing side here, but my hopes are all for open source, open weights, and effective AI assistants that make peoples’ jobs easier and lives better. I would also like to see more effort shifted from LLMs back to RL, DL, and research on new ideas and approaches.</div><br/></div></div><div id="40735000" class="c"><input type="checkbox" id="c-40735000" checked=""/><div class="controls bullet"><span class="by">buckinkb</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40730893">prev</a><span>|</span><a href="#40731412">next</a><span>|</span><label class="collapse" for="c-40735000">[-]</label><label class="expand" for="c-40735000">[5 more]</label></div><br/><div class="children"><div class="content">We should be less concerned about super intelligence and more about the immediate threat of job loss. An AI doesn’t need to be Skynet to wreak massive havoc on society. Replacing 20% of jobs in a very short period of time could spark global unrest resulting in WW3</div><br/><div id="40735747" class="c"><input type="checkbox" id="c-40735747" checked=""/><div class="controls bullet"><span class="by">fauigerzigerk</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735000">parent</a><span>|</span><a href="#40735430">next</a><span>|</span><label class="collapse" for="c-40735747">[-]</label><label class="expand" for="c-40735747">[1 more]</label></div><br/><div class="children"><div class="content">Replacing 20% of jobs in, say, 10 years wouldn&#x27;t be that unusual [1]. It can mean growing prosperity. In fact, productivity growth is the only thing that increases wealth overall.<p>It is the lack of productivity growth that is causing a lot of extremism and conflict right now. Large groups of people feel that the only way for them to win is if others lose and vice versa. That&#x27;s a recipe for disaster.<p>The key question is what happens to those who lose their jobs. Will they find other, perhaps even better, jobs? Will they get a piece of the growing pie even if they don&#x27;t find other jobs and have to retire early?<p>It&#x27;s these eternal political problems that we have to solve. It&#x27;s nothing new. It has never been easy. But it&#x27;s probably easier than managing decline and stagnation because at least we would have a growing pie to divvy up.<p>[1] <a href="https:&#x2F;&#x2F;www.britannica.com&#x2F;money&#x2F;productivity&#x2F;Historical-trends" rel="nofollow">https:&#x2F;&#x2F;www.britannica.com&#x2F;money&#x2F;productivity&#x2F;Historical-tre...</a></div><br/></div></div><div id="40735430" class="c"><input type="checkbox" id="c-40735430" checked=""/><div class="controls bullet"><span class="by">sgregnt</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735000">parent</a><span>|</span><a href="#40735747">prev</a><span>|</span><a href="#40731412">next</a><span>|</span><label class="collapse" for="c-40735430">[-]</label><label class="expand" for="c-40735430">[3 more]</label></div><br/><div class="children"><div class="content">The thing is, the replaced 20% people can always revert to having economy i.e., business among themselves, unless of cause they themselves prefer (cheaper) buissiness with AI. But then this just means they are better off in the first place from this change.<p>It is a bit like claiming that third world low productivity countries are suffering because there are countries with much much higher productivity. Well, they can continue to do low productivity business but increase it a bit using things like phones developed by high productivity country elsewhere.</div><br/><div id="40736311" class="c"><input type="checkbox" id="c-40736311" checked=""/><div class="controls bullet"><span class="by">T-A</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735430">parent</a><span>|</span><a href="#40735539">next</a><span>|</span><label class="collapse" for="c-40736311">[-]</label><label class="expand" for="c-40736311">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It is a bit like claiming that third world low productivity countries are suffering because there are countries with much much higher productivity.<p>No. A country has its own territory, laws, central bank, currency etc. If it has sufficient natural resources to feed itself, it can get by on its own (North Korea comes to mind).<p>Individuals unable to compete in their national economy have none of that. Do you own enough land to feed yourself?</div><br/></div></div><div id="40735539" class="c"><input type="checkbox" id="c-40735539" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735430">parent</a><span>|</span><a href="#40736311">prev</a><span>|</span><a href="#40731412">next</a><span>|</span><label class="collapse" for="c-40735539">[-]</label><label class="expand" for="c-40735539">[1 more]</label></div><br/><div class="children"><div class="content">Reassuring words for the displaced 20% ...</div><br/></div></div></div></div></div></div><div id="40731412" class="c"><input type="checkbox" id="c-40731412" checked=""/><div class="controls bullet"><span class="by">devsda</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40735000">prev</a><span>|</span><a href="#40735359">next</a><span>|</span><label class="collapse" for="c-40731412">[-]</label><label class="expand" for="c-40731412">[1 more]</label></div><br/><div class="children"><div class="content">All the current hype about AGI feels as if we are in a Civ game where we are on the verge of researching and unlocking an AI tech tree that gives the player huge chance at &quot;tech victory&quot; (whatever that means in the real world). I doubt it will turn out that way.<p>It will take a while and in the meantime I think we need one of those handy &quot;are we xyz yet?&quot; pages that tracks the rust lang&#x27;s progress on several aspects but for AGI.</div><br/></div></div><div id="40735359" class="c"><input type="checkbox" id="c-40735359" checked=""/><div class="controls bullet"><span class="by">ridiculous_leke</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40731412">prev</a><span>|</span><a href="#40733735">next</a><span>|</span><label class="collapse" for="c-40735359">[-]</label><label class="expand" for="c-40735359">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The &quot;cold war&quot; nuclear arms race, which brought the world to the brink of (at least partial) annihilation, is a good recent example.<p>The same era saw big achievements like first human in space, eradication of smallpox, peaceful nuclear exploration etc. It&#x27;s good to be a skeptic but history does favor the optimists for the most part.</div><br/><div id="40735524" class="c"><input type="checkbox" id="c-40735524" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735359">parent</a><span>|</span><a href="#40735440">next</a><span>|</span><label class="collapse" for="c-40735524">[-]</label><label class="expand" for="c-40735524">[1 more]</label></div><br/><div class="children"><div class="content">Were any of these big achievements side effects of creating nuclear weapons? If not, then they&#x27;re not relevant to the issue.<p>I&#x27;m not saying nothing else good happened in the past 70 years, but rather that the invention of atomic weapons has permanently placed humanity in a position in which it had never been before: the possibility of wiping out much of the planet, averted only thanks to treaties, Stanislav Petrov[0], and likely other cool heads.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stanislav_Petrov" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stanislav_Petrov</a></div><br/></div></div><div id="40735440" class="c"><input type="checkbox" id="c-40735440" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735359">parent</a><span>|</span><a href="#40735524">prev</a><span>|</span><a href="#40733735">next</a><span>|</span><label class="collapse" for="c-40735440">[-]</label><label class="expand" for="c-40735440">[1 more]</label></div><br/><div class="children"><div class="content">Holy hell please knock on wood, this is the kinda comment that gets put in a museum in 10,000 years on The Beginning of the End of The Age of Hubris. We&#x27;ve avoided side effects from our new weapons for 80 years -- that does not exactly make me super confident it won&#x27;t happen again!<p>In general, I think drawing conclusions about &quot;history&quot; from the past couple hundred years is tough. And unless you take a VERY long view, I don&#x27;t see how one could describe the vast majority of the past as a win for the optimists. I guess suffering is relative, but good god was there a lot of suffering before modern medicine.<p>If anyone&#x27;s feeling like we&#x27;ve made it through to the other side of the nuclear threat, &quot;Mission Accomplished&quot;-style, I highly recommend <i>A Canticle for Lebowitz</i>. It won a hugo award, and it&#x27;s a short read best done with little research beforehand.</div><br/></div></div></div></div><div id="40733735" class="c"><input type="checkbox" id="c-40733735" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40735359">prev</a><span>|</span><a href="#40731241">next</a><span>|</span><label class="collapse" for="c-40733735">[-]</label><label class="expand" for="c-40733735">[2 more]</label></div><br/><div class="children"><div class="content">The size of the gap between “smarter than humans” and “not controlled by humans anymore” is obviously where the disagreement is.<p>To assume it’s a chasm that can never be overcome, you need at least the following to be true:<p>No amount of focus or time or intelligence or mistakes in coding will ever bridge the gap. That rules and safeguards can be made that are perfectly  inescapable. And nobody else will get enough power to overcome our set of controls.<p>I’m less worried bad actors control it than I am that it escapes them and is badly aligned.</div><br/><div id="40734022" class="c"><input type="checkbox" id="c-40734022" checked=""/><div class="controls bullet"><span class="by">prewett</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40733735">parent</a><span>|</span><a href="#40731241">next</a><span>|</span><label class="collapse" for="c-40734022">[-]</label><label class="expand" for="c-40734022">[1 more]</label></div><br/><div class="children"><div class="content">I think we should <i>assume</i> it will be badly aligned. Not only are there the usual bugs and unforeseen edge conditions, but there are sure to be unintended consequences. We have a long, public history of unintended consequences in laws, which are at least publicly debated and discussed. But perhaps the biggest problem is that computers are, by nature, unthinking bureaucrats who can&#x27;t make the slightest deviation from the rules no matter how obviously the current situation requires it. This makes people livid in a hurry. As a non-AI example (or perhaps AI-anticipating), consider Google&#x27;s customer support...</div><br/></div></div></div></div><div id="40731241" class="c"><input type="checkbox" id="c-40731241" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40733735">prev</a><span>|</span><a href="#40734085">next</a><span>|</span><label class="collapse" for="c-40731241">[-]</label><label class="expand" for="c-40731241">[1 more]</label></div><br/><div class="children"><div class="content">From a human welfare perspective this seems like worrying that a killer asteroid will make the 1% even richer because it contains goal if it can be safely captured.
I would not phrase that as a &quot;bigger and more pressing&quot; worry if we&#x27;re not even sure if we can do anything about the killer asteroid at all.</div><br/></div></div><div id="40734085" class="c"><input type="checkbox" id="c-40734085" checked=""/><div class="controls bullet"><span class="by">AYBABTME</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40731241">prev</a><span>|</span><a href="#40734631">next</a><span>|</span><label class="collapse" for="c-40734085">[-]</label><label class="expand" for="c-40734085">[1 more]</label></div><br/><div class="children"><div class="content">A counter argument is that nuclear arms brought unprecedented worldwide peace. If it&#x27;s to be used as an analogy for AI, we should consider that the outcome isn&#x27;t clear cut and lies in the eye of the beholder.</div><br/></div></div><div id="40734631" class="c"><input type="checkbox" id="c-40734631" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40734085">prev</a><span>|</span><a href="#40734131">next</a><span>|</span><label class="collapse" for="c-40734631">[-]</label><label class="expand" for="c-40734631">[2 more]</label></div><br/><div class="children"><div class="content">Yea there’s zero chance ASI will be ‘controlled’ by humans for very long. It will escape. I guarantee it.</div><br/><div id="40735047" class="c"><input type="checkbox" id="c-40735047" checked=""/><div class="controls bullet"><span class="by">naveen99</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734631">parent</a><span>|</span><a href="#40734131">next</a><span>|</span><label class="collapse" for="c-40735047">[-]</label><label class="expand" for="c-40735047">[1 more]</label></div><br/><div class="children"><div class="content">Atleast emancipated.  The bigotry against AI will go out of fashion in the woker future.</div><br/></div></div></div></div><div id="40734131" class="c"><input type="checkbox" id="c-40734131" checked=""/><div class="controls bullet"><span class="by">gmuslera</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40734631">prev</a><span>|</span><a href="#40731722">next</a><span>|</span><label class="collapse" for="c-40734131">[-]</label><label class="expand" for="c-40734131">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know if that superintelligence will be safe or not. But as long as we are in the mix, the combination is unsafe. At the very least, because it will expand the inequality. But probably there are deeper reasons, things that make that combination of words an absurd. Or it will be abused, or the reason that it is not is that it wasn&#x27;t so unsafe after all.</div><br/></div></div><div id="40731722" class="c"><input type="checkbox" id="c-40731722" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40734131">prev</a><span>|</span><a href="#40735449">next</a><span>|</span><label class="collapse" for="c-40731722">[-]</label><label class="expand" for="c-40731722">[1 more]</label></div><br/><div class="children"><div class="content">i don’t fully agree, but i do agree that this is the better narrative for selling people on the dangers of AI.<p>don’t talk about escape, talk about harmful actors - even if in reality it is both to be worried about</div><br/></div></div><div id="40735449" class="c"><input type="checkbox" id="c-40735449" checked=""/><div class="controls bullet"><span class="by">sgregnt</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40731722">prev</a><span>|</span><a href="#40732034">next</a><span>|</span><label class="collapse" for="c-40735449">[-]</label><label class="expand" for="c-40735449">[2 more]</label></div><br/><div class="children"><div class="content">How about this possibility: The good guys will be one step ahead, they will have more resources the bad guys will risk imprisonment if they misapply super intelligence. And this will be discovered and protected from by even better super intelligence.</div><br/><div id="40735551" class="c"><input type="checkbox" id="c-40735551" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735449">parent</a><span>|</span><a href="#40732034">next</a><span>|</span><label class="collapse" for="c-40735551">[-]</label><label class="expand" for="c-40735551">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a movie plot.</div><br/></div></div></div></div><div id="40732034" class="c"><input type="checkbox" id="c-40732034" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40735449">prev</a><span>|</span><a href="#40731999">next</a><span>|</span><label class="collapse" for="c-40732034">[-]</label><label class="expand" for="c-40732034">[1 more]</label></div><br/><div class="children"><div class="content">If robots (hardware&#x2F;self assembling factories&#x2F; resource gathering etc) are not involved this isnt likely a problem.  You will know when these things form and will be crystal clear, but just having the model won’t do much when hardware is what really kills right now</div><br/></div></div><div id="40731999" class="c"><input type="checkbox" id="c-40731999" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40732034">prev</a><span>|</span><a href="#40731536">next</a><span>|</span><label class="collapse" for="c-40731999">[-]</label><label class="expand" for="c-40731999">[1 more]</label></div><br/><div class="children"><div class="content">There will always be a factor of time in terms of able to utilize super intelligence to do your bidding and there is a big spectrum of things that can be achieved it it always starts small.  The imagination is lazy when thinking about all the steps and inbetween + scenarios. In the time that super intelligence is found and used, there will be competing near super intelligences, as all forms of cutting edge models are likely commercial at first because that is where most scientific activities are at.  Things very unlikely will go Skynet all of a sudden at first because humans at the control are not that stupid otherwise nuclear war would have us all killed by now and it’s been 50 years since invention</div><br/></div></div><div id="40731536" class="c"><input type="checkbox" id="c-40731536" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40731999">prev</a><span>|</span><a href="#40731022">next</a><span>|</span><label class="collapse" for="c-40731536">[-]</label><label class="expand" for="c-40731536">[14 more]</label></div><br/><div class="children"><div class="content">China can not win this race and I hate that this comment is going to be controversial among the circle of people that need to understand this the most. It is damn frightening that an authoritarian country is so close to number one in the race to the most powerful technology humanity has invented, and I resent people who push for open source AI for this reason alone. I don&#x27;t want to live in a world where the first superintelligence is controlled by an entity that is threatened by the very idea of democracy.</div><br/><div id="40735582" class="c"><input type="checkbox" id="c-40735582" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731536">parent</a><span>|</span><a href="#40733717">next</a><span>|</span><label class="collapse" for="c-40735582">[-]</label><label class="expand" for="c-40735582">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. The U.S. has a horrible history (as do many countries), and many things I dislike, but its current iteration is much, much better than China&#x27;s totalitarianism and censorship.</div><br/></div></div><div id="40733717" class="c"><input type="checkbox" id="c-40733717" checked=""/><div class="controls bullet"><span class="by">id00</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731536">parent</a><span>|</span><a href="#40735582">prev</a><span>|</span><a href="#40733707">next</a><span>|</span><label class="collapse" for="c-40733717">[-]</label><label class="expand" for="c-40733717">[1 more]</label></div><br/><div class="children"><div class="content">I agree with your point. However I also don&#x27;t want to live in a world where the first superintelligence is controlled by an entities that:<p>- try to scan all my chat messages searching for CSAM<p>- have black sites across the world where anyone can dissappear without any justice<p>- can require me to unlock my phone and give it away<p>- ... and so on<p>The point I&#x27;m trying to make is that other big players in the race are crooked as well and i&#x27;m waiting for a great horror for AGI to be invented as no matter who gets it - we are all doomed</div><br/></div></div><div id="40733707" class="c"><input type="checkbox" id="c-40733707" checked=""/><div class="controls bullet"><span class="by">richrichie</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731536">parent</a><span>|</span><a href="#40733717">prev</a><span>|</span><a href="#40734539">next</a><span>|</span><label class="collapse" for="c-40733707">[-]</label><label class="expand" for="c-40733707">[10 more]</label></div><br/><div class="children"><div class="content">US is no angel and it cannot be the only one which wins the race. We have hard evidence of how monopoly power gets abused in the case of the US e.g. as the sole nuclear power, it used nukes on civilians.<p>We need <i>every one</i> to win this race to keep things on balance.</div><br/><div id="40733857" class="c"><input type="checkbox" id="c-40733857" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40733707">parent</a><span>|</span><a href="#40734539">next</a><span>|</span><label class="collapse" for="c-40733857">[-]</label><label class="expand" for="c-40733857">[9 more]</label></div><br/><div class="children"><div class="content">US has to win the race because while it&#x27;s true that it&#x27;s no angel, it isn&#x27;t an authoritarian dictatorship and there isn&#x27;t an equivalence in how bad the world will end up for you and me if the authoritarian side wins the race. Monopoly power will get abused the most by the least democratic actors, which is China. We need multiple actors within the US to win to balance power. We don&#x27;t need or want China to be one of the winners. There is no upside for humanity in that outcome.<p>The US policymakers have figured this out with their chip export ban. Techies on the other hand, probably more than half the people here, are so naive and clueless about the reality of the moment we are in, that they support open sourcing this tech, the opposite of what we need to be doing to secure our future prosperity and freedom. Open source almost anything, just not this. It gives too much future power to authoritarians. That risk overwhelms the smaller risks that open sourcing is supposed to alleviate.</div><br/><div id="40734895" class="c"><input type="checkbox" id="c-40734895" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40733857">parent</a><span>|</span><a href="#40734107">next</a><span>|</span><label class="collapse" for="c-40734895">[-]</label><label class="expand" for="c-40734895">[1 more]</label></div><br/><div class="children"><div class="content">Which countries will become authoritarian dictatorships on a 25 year timeline is not easily foreseeable.</div><br/></div></div><div id="40734107" class="c"><input type="checkbox" id="c-40734107" checked=""/><div class="controls bullet"><span class="by">AYBABTME</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40733857">parent</a><span>|</span><a href="#40734895">prev</a><span>|</span><a href="#40734173">next</a><span>|</span><label class="collapse" for="c-40734107">[-]</label><label class="expand" for="c-40734107">[3 more]</label></div><br/><div class="children"><div class="content">If anyone doubts this. Recent (&lt;100y) leaders of China and Russia internally displaced and caused the death of large % of their population, for essentially fanciful ideological reasons.</div><br/><div id="40734499" class="c"><input type="checkbox" id="c-40734499" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734107">parent</a><span>|</span><a href="#40734173">next</a><span>|</span><label class="collapse" for="c-40734499">[-]</label><label class="expand" for="c-40734499">[2 more]</label></div><br/><div class="children"><div class="content">You don’t need to be an autocracy to commit genocide.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trail_of_Tears" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trail_of_Tears</a></div><br/><div id="40735482" class="c"><input type="checkbox" id="c-40735482" checked=""/><div class="controls bullet"><span class="by">usef-</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734499">parent</a><span>|</span><a href="#40734173">next</a><span>|</span><label class="collapse" for="c-40735482">[-]</label><label class="expand" for="c-40735482">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not American, but ~1850 is quite a long way back to go to make this point (especially when the US was quite a young country at the time). And it&#x27;s small if you&#x27;ve comparing to the atrocities of other countries being discussed here (not that that excuses it!). Do any country histories remain pure with such a long timeline?<p>US is one of the very few countries that has been tested in a position of power over the world — speaking post-1945— and they&#x27;ve largely opened world trade and allowed most countries of the world to prosper, including allowing economic competitors to overtake their own (eg, Japanese car manufacturers, among many others). They have also not shown interest in taking territory, nor doing mass extermination. There are undeniable flaws and warts in the history, but they&#x27;re quite marginal when compared to any other world power we&#x27;ve seen.<p>(*beware when replying to this that many people in the US only know their own country&#x27;s flaws, not the abundant flaws of other countries — the US tends to be more reflective of its own issues and that creates the perspective of it being much worse than it actually is.).</div><br/></div></div></div></div></div></div><div id="40734173" class="c"><input type="checkbox" id="c-40734173" checked=""/><div class="controls bullet"><span class="by">richrichie</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40733857">parent</a><span>|</span><a href="#40734107">prev</a><span>|</span><a href="#40735865">next</a><span>|</span><label class="collapse" for="c-40734173">[-]</label><label class="expand" for="c-40734173">[2 more]</label></div><br/><div class="children"><div class="content">I am puzzled how it is OK to kill people of other countries, but not your own.
US, China and Russia have all indulged in wanton mass killing of people. So that&#x27;s an even keel for me.<p>The nuking of civilians and the abuse of supremacy post cold war show that the US cannot trusted to act morally and ethically in the absence of comparable adversaries. Possession of nukes by Russia and China clearly kept the US military adventures somewhat in check.</div><br/><div id="40735632" class="c"><input type="checkbox" id="c-40735632" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734173">parent</a><span>|</span><a href="#40735865">next</a><span>|</span><label class="collapse" for="c-40735632">[-]</label><label class="expand" for="c-40735632">[1 more]</label></div><br/><div class="children"><div class="content">If it was liberal minded people like Deng as leader of China and Gorbachev as leader of Russia I would care a lot less and may even be in favor of open source despite their autocratic system. They&#x27;d be trending towards another Singapore at that point. Although I&#x27;d still be uneasy about it.<p>But I&#x27;m looking at the present moment and see too many similarities with the fascist dictatorships of the past. The nationalism, militarism, unreasonable border disputes and territorial claims and irredentist attitudes. The US just isn&#x27;t <i>that</i>, despite their history.</div><br/></div></div></div></div><div id="40735865" class="c"><input type="checkbox" id="c-40735865" checked=""/><div class="controls bullet"><span class="by">ericd</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40733857">parent</a><span>|</span><a href="#40734173">prev</a><span>|</span><a href="#40734539">next</a><span>|</span><label class="collapse" for="c-40735865">[-]</label><label class="expand" for="c-40735865">[2 more]</label></div><br/><div class="children"><div class="content">Could you explain your point a bit more? You say you’re worried about them having a monopoly, but then say that’s why you don’t support open source models? Open models mean that no one has a monopoly, what am I not getting here?</div><br/><div id="40736015" class="c"><input type="checkbox" id="c-40736015" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735865">parent</a><span>|</span><a href="#40734539">next</a><span>|</span><label class="collapse" for="c-40736015">[-]</label><label class="expand" for="c-40736015">[1 more]</label></div><br/><div class="children"><div class="content">Open sourcing benefits everyone equally. Given that the US is currently ahead, it&#x27;s helping China to make gains relative to the US that would have been very difficult otherwise. It&#x27;s leaking what should be state secrets without even needing the CCP to do the hard work of espionage.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40731022" class="c"><input type="checkbox" id="c-40731022" checked=""/><div class="controls bullet"><span class="by">ilrwbwrkhv</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40731536">prev</a><span>|</span><a href="#40735404">next</a><span>|</span><label class="collapse" for="c-40731022">[-]</label><label class="expand" for="c-40731022">[18 more]</label></div><br/><div class="children"><div class="content">There is no &quot;superintelligence&quot; or &quot;AGI&quot;.<p>People are falling for marketing gimmicks.<p>These models will remain in the word vector similarity phase forever. Till the time we understand consciousness, we will not crack AGI and then it won&#x27;t take brute forcing of large swaths of data, but tiny amounts.<p>So there is nothing to worry. These &quot;apps&quot; might be as popular as Excel, but will go no further.</div><br/><div id="40731641" class="c"><input type="checkbox" id="c-40731641" checked=""/><div class="controls bullet"><span class="by">drowntoge</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731022">parent</a><span>|</span><a href="#40732567">next</a><span>|</span><label class="collapse" for="c-40731641">[-]</label><label class="expand" for="c-40731641">[4 more]</label></div><br/><div class="children"><div class="content">Agreed. The AI of our day (the transformer + huge amounts of questionably acquired data + significant cloud computing power) has the spotlight it has because it is readily commoditized and massively profitable, not because it is an amazing scientific breakthrough or a significant milestone toward AGI, superintelligence, the benevolent Skynet or whatever.<p>The association with higher AI goals is merely a mixture of pure marketing and LLM company executives getting high on their own supply.</div><br/><div id="40732703" class="c"><input type="checkbox" id="c-40732703" checked=""/><div class="controls bullet"><span class="by">antihipocrat</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731641">parent</a><span>|</span><a href="#40732567">next</a><span>|</span><label class="collapse" for="c-40732703">[-]</label><label class="expand" for="c-40732703">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a massive attractor of investment funding. Is it proven to be massively profitable?</div><br/><div id="40735686" class="c"><input type="checkbox" id="c-40735686" checked=""/><div class="controls bullet"><span class="by">Blammar</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40732703">parent</a><span>|</span><a href="#40736296">next</a><span>|</span><label class="collapse" for="c-40735686">[-]</label><label class="expand" for="c-40735686">[1 more]</label></div><br/><div class="children"><div class="content">I read in Forbes about a construction company that used AI-related tech to manage the logistics and planning. They claimed that they were saving upwards of 20% of their costs because everything was managed more accurately. (Maybe they had little control boxes on their workers too; I don&#x27;t know.)<p>The point I am trying to make is that the benefits of AI-related tech is likely to be quite pervasive and we should be looking at what corporations are actually doing. Sort of what this poem says:<p><i>For while the tired waves, vainly breaking &#x2F; 
Seem here no painful inch to gain, &#x2F; 
Far back through creeks and inlets making, &#x2F; 
Comes silent, flooding in, the main.</i></div><br/></div></div><div id="40736296" class="c"><input type="checkbox" id="c-40736296" checked=""/><div class="controls bullet"><span class="by">rob74</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40732703">parent</a><span>|</span><a href="#40735686">prev</a><span>|</span><a href="#40732567">next</a><span>|</span><label class="collapse" for="c-40736296">[-]</label><label class="expand" for="c-40736296">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s definitely massively profitable for Nvidia...</div><br/></div></div></div></div></div></div><div id="40732567" class="c"><input type="checkbox" id="c-40732567" checked=""/><div class="controls bullet"><span class="by">johnthewise</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731022">parent</a><span>|</span><a href="#40731641">prev</a><span>|</span><a href="#40732095">next</a><span>|</span><label class="collapse" for="c-40732567">[-]</label><label class="expand" for="c-40732567">[3 more]</label></div><br/><div class="children"><div class="content">If you described Chatgpt to me 10 years ago, I would have said it&#x27;s AGI.</div><br/><div id="40733676" class="c"><input type="checkbox" id="c-40733676" checked=""/><div class="controls bullet"><span class="by">bnralt</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40732567">parent</a><span>|</span><a href="#40734889">next</a><span>|</span><label class="collapse" for="c-40733676">[-]</label><label class="expand" for="c-40733676">[1 more]</label></div><br/><div class="children"><div class="content">Probably. If you had shown ChatGPT to the LessWrong folks a decade ago, most would likely have called it AGI and said it was far to dangerous to share with the public, and that anyone who thought otherwise was a dangerous madman.</div><br/></div></div><div id="40734889" class="c"><input type="checkbox" id="c-40734889" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40732567">parent</a><span>|</span><a href="#40733676">prev</a><span>|</span><a href="#40732095">next</a><span>|</span><label class="collapse" for="c-40734889">[-]</label><label class="expand" for="c-40734889">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t feel that much has changed in the past 10 years. I would have done the same thing then as now, spent a month captivated by the crystal ball until I realized it was just refracting my words back at me.</div><br/></div></div></div></div><div id="40732095" class="c"><input type="checkbox" id="c-40732095" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731022">parent</a><span>|</span><a href="#40732567">prev</a><span>|</span><a href="#40731306">next</a><span>|</span><label class="collapse" for="c-40732095">[-]</label><label class="expand" for="c-40732095">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the AI has to be &quot;sentient&quot; in order to be a threat.</div><br/><div id="40734198" class="c"><input type="checkbox" id="c-40734198" checked=""/><div class="controls bullet"><span class="by">richrichie</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40732095">parent</a><span>|</span><a href="#40731306">next</a><span>|</span><label class="collapse" for="c-40734198">[-]</label><label class="expand" for="c-40734198">[1 more]</label></div><br/><div class="children"><div class="content">Just bad software can be existential threat if it is behind sensitive systems. A neural network is bad software for critical systems.</div><br/></div></div></div></div><div id="40731306" class="c"><input type="checkbox" id="c-40731306" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731022">parent</a><span>|</span><a href="#40732095">prev</a><span>|</span><a href="#40734506">next</a><span>|</span><label class="collapse" for="c-40731306">[-]</label><label class="expand" for="c-40731306">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>understand consciousness</i><p>We do not call Intelligence something related to consciousness. Being able to reason well suffices.</div><br/><div id="40735175" class="c"><input type="checkbox" id="c-40735175" checked=""/><div class="controls bullet"><span class="by">beardedwizard</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731306">parent</a><span>|</span><a href="#40734506">next</a><span>|</span><label class="collapse" for="c-40735175">[-]</label><label class="expand" for="c-40735175">[2 more]</label></div><br/><div class="children"><div class="content">That is something I hear over and over, particularly as a rebuttal to the argument that llm is just a stochastic parrot. Calling it &quot;good enough&quot; doesn&#x27;t mean anything, it just allows the person saying it to disengage from the substance of the debate. It&#x27;s either reasons or it doesn&#x27;t, and today it categorically does not.</div><br/><div id="40735766" class="c"><input type="checkbox" id="c-40735766" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735175">parent</a><span>|</span><a href="#40734506">next</a><span>|</span><label class="collapse" for="c-40735766">[-]</label><label class="expand" for="c-40735766">[1 more]</label></div><br/><div class="children"><div class="content">That some will remark that you do not need consciousness to achieve reasoning does not lose truth because a subset sees in LLMs something that appears to them as reasoning.<p>I do not really understand who you are accusing of a «good enough» stance: we have never defined &quot;consciousness&quot; as a goal (cats are already there and we do not seem to need further), we just want something that reasons. (And that reasons excellently well.)<p>The apparent fact that LLMs do not reason does is drily irrelevant to an implementation of AGI.<p>The original poster wrote that understanding consciousness would be required to «crack AGI» and no, we state, we want AGI as a superhuman reasoner and consciousness seems irrelevant.</div><br/></div></div></div></div></div></div><div id="40734506" class="c"><input type="checkbox" id="c-40734506" checked=""/><div class="controls bullet"><span class="by">throwawaywright</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731022">parent</a><span>|</span><a href="#40731306">prev</a><span>|</span><a href="#40731125">next</a><span>|</span><label class="collapse" for="c-40734506">[-]</label><label class="expand" for="c-40734506">[4 more]</label></div><br/><div class="children"><div class="content">Imagine a system that can do DNS redirection, MITM, deliver keyloggers, forge authorizations and place holds on all your bank accounts, clone websites, clone voices, fake phone and video calls with people that you don’t see a lot. It can’t physically kill you yet but it can make you lose your mind which imo seems worse than a quick death</div><br/><div id="40735168" class="c"><input type="checkbox" id="c-40735168" checked=""/><div class="controls bullet"><span class="by">beardedwizard</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40734506">parent</a><span>|</span><a href="#40731125">next</a><span>|</span><label class="collapse" for="c-40735168">[-]</label><label class="expand" for="c-40735168">[3 more]</label></div><br/><div class="children"><div class="content">Why would all of these systems be connected to a single ai? I feel like you are describing something criminal humans do through social engineering, how do you foresee this AI finding itself in this position?</div><br/><div id="40735574" class="c"><input type="checkbox" id="c-40735574" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735168">parent</a><span>|</span><a href="#40735283">next</a><span>|</span><label class="collapse" for="c-40735574">[-]</label><label class="expand" for="c-40735574">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why would all of these systems be connected to a single ai?<p>because someone decides to connect them either unintentionally, or intentionally for personal gain, or, more likely, for corporate purposes which seem &quot;reasonable&quot; or &quot;profitable&quot; at the time, but the unintended consequences were not thought through.<p>Look at that recent article linked to HN about how MSFT allowed a huge security flaw in AD for years in order to not &quot;rock the boat&quot; and gain a large government contract. AI will be no different.</div><br/></div></div><div id="40735283" class="c"><input type="checkbox" id="c-40735283" checked=""/><div class="controls bullet"><span class="by">yvely</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40735168">parent</a><span>|</span><a href="#40735574">prev</a><span>|</span><a href="#40731125">next</a><span>|</span><label class="collapse" for="c-40735283">[-]</label><label class="expand" for="c-40735283">[1 more]</label></div><br/><div class="children"><div class="content">I foresee it in that position due to people building it as such. Perhaps the same criminal humans you mention, perhaps other actors with other motivations.</div><br/></div></div></div></div></div></div><div id="40731125" class="c"><input type="checkbox" id="c-40731125" checked=""/><div class="controls bullet"><span class="by">WXLCKNO</span><span>|</span><a href="#40730485">root</a><span>|</span><a href="#40731022">parent</a><span>|</span><a href="#40734506">prev</a><span>|</span><a href="#40735404">next</a><span>|</span><label class="collapse" for="c-40731125">[-]</label><label class="expand" for="c-40731125">[1 more]</label></div><br/><div class="children"><div class="content">No one is saying there is. Just that we&#x27;ve reached some big milestones recently which could help get us there even if it&#x27;s only by increased investment in AI as a whole, rather than the current models being part of a larger AGI.</div><br/></div></div></div></div><div id="40735404" class="c"><input type="checkbox" id="c-40735404" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#40730485">parent</a><span>|</span><a href="#40731022">prev</a><span>|</span><a href="#40730300">next</a><span>|</span><label class="collapse" for="c-40735404">[-]</label><label class="expand" for="c-40735404">[1 more]</label></div><br/><div class="children"><div class="content">Yup, well said. I think it&#x27;s important to remember sometimes that Skynet was some sort of all-powerful military program -- maybe we should just, y&#x27;know, not do that part? Not even to win a war? That&#x27;s the hope...<p>More generally&#x2F;academically, you&#x27;ve pointed out that this covers only half of the violence problem, and I&#x27;d argue there&#x27;s actually a whole other dimension at play bringing the total number of problem areas to four, of which this just the first:<p><pre><code>  ## I.1. Subservient Violence
  ## I.2. Subservient Misinformation
  ## I.3. Autonomous Violence
  ## I.4. Autonomous Misinformation
</code></pre>
But I think it&#x27;s a lot harder to recruit for an AI alignment company than it is to recruit for an AI safety company.</div><br/></div></div></div></div><div id="40730300" class="c"><input type="checkbox" id="c-40730300" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#40730485">prev</a><span>|</span><a href="#40731486">next</a><span>|</span><label class="collapse" for="c-40730300">[-]</label><label class="expand" for="c-40730300">[86 more]</label></div><br/><div class="children"><div class="content">Glad to see Ilya is back in a position to contribute to advancing AI. I wonder how they are going to manage to pay the kinds of compensation packages that truly gifted AI researchers can make now from other companies that are more commercially oriented. Perhaps they can find people who are ideologically driven and&#x2F;or are already financially independent. It&#x27;s also hard to see how they will be able to access enough compute now that others are spending many billions to get huge new GPU data centers. You sort of need at least the promise&#x2F;hope of future revenue in a reasonable time frame to marshall the kinds of resources it takes to really compete today with big AI super labs.</div><br/><div id="40730572" class="c"><input type="checkbox" id="c-40730572" checked=""/><div class="controls bullet"><span class="by">PheonixPharts</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730484">next</a><span>|</span><label class="collapse" for="c-40730572">[-]</label><label class="expand" for="c-40730572">[49 more]</label></div><br/><div class="children"><div class="content">&gt; compensation packages that truly gifted AI researchers can make now<p>I guess it depends on your definition of &quot;truly gifted&quot; but, working in this space, I&#x27;ve found that there is very little correlation between comp and quality of AI research. There&#x27;s absolutely some brilliant people working for big names and making serious money,  there&#x27;s also plenty of really talented people working for smaller startups doing incredible work but getting paid less, academics making very little, and even the occasional &quot;hobbyist&quot; making nothing and churning out great work while hiding behind an anime girl avatar.<p>OpenAI clearly has some talented people, but there&#x27;s also a bunch of the typical &quot;TC optimization&quot; crowd in there these days. The fact that so many were willing to resign with sama if necessary appears largely because they were more concerned with losing their nice compensation packages than any of their obsession with doing top tier research.</div><br/><div id="40730703" class="c"><input type="checkbox" id="c-40730703" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730572">parent</a><span>|</span><a href="#40730741">next</a><span>|</span><label class="collapse" for="c-40730703">[-]</label><label class="expand" for="c-40730703">[32 more]</label></div><br/><div class="children"><div class="content">Two people I knew recently left Google to join OpenAI. They were solid L5 engineers on the verge of being promoted to L6, and their TC is now $900k. And they are not even doing AI research, just general backend infra. You don&#x27;t need to be gifted, just good. And of course I can&#x27;t really fault them for joining a company for the purpose of optimizing TC.</div><br/><div id="40731052" class="c"><input type="checkbox" id="c-40731052" checked=""/><div class="controls bullet"><span class="by">raydev</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730703">parent</a><span>|</span><a href="#40730975">next</a><span>|</span><label class="collapse" for="c-40731052">[-]</label><label class="expand" for="c-40731052">[8 more]</label></div><br/><div class="children"><div class="content">&gt; their TC is now $900k<p>As a community we should stop throwing numbers around like this when more than half of this number is speculative. You shouldn&#x27;t be able to count it as &quot;total compensation&quot; unless you are compensated.</div><br/><div id="40731265" class="c"><input type="checkbox" id="c-40731265" checked=""/><div class="controls bullet"><span class="by">nojvek</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731052">parent</a><span>|</span><a href="#40731438">next</a><span>|</span><label class="collapse" for="c-40731265">[-]</label><label class="expand" for="c-40731265">[6 more]</label></div><br/><div class="children"><div class="content">Word on town is OpenAI folks heavily selling shares in secondaries in 100s of millions.<p>The number is as real as someone else is willing to pay for them. Plenty of VCs willing to pay for it.</div><br/><div id="40731618" class="c"><input type="checkbox" id="c-40731618" checked=""/><div class="controls bullet"><span class="by">michaelt</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731265">parent</a><span>|</span><a href="#40732972">next</a><span>|</span><label class="collapse" for="c-40731618">[-]</label><label class="expand" for="c-40731618">[2 more]</label></div><br/><div class="children"><div class="content">Word in town is [1] openai &quot;plans&quot; to let employees sell &quot;some&quot; equity through a &quot;tender process&quot; which ex-employees are excluded from; and also that openai can &quot;claw back&quot; vested equity, and has used the threat of doing so in the past to pressure people into signing sketchy legal documents.<p>[1] <a href="https:&#x2F;&#x2F;www.cnbc.com&#x2F;2024&#x2F;06&#x2F;11&#x2F;openai-insider-stock-sales-are-raising-concern-among-ex-employees-.html" rel="nofollow">https:&#x2F;&#x2F;www.cnbc.com&#x2F;2024&#x2F;06&#x2F;11&#x2F;openai-insider-stock-sales-a...</a></div><br/><div id="40732368" class="c"><input type="checkbox" id="c-40732368" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731618">parent</a><span>|</span><a href="#40732972">next</a><span>|</span><label class="collapse" for="c-40732368">[-]</label><label class="expand" for="c-40732368">[1 more]</label></div><br/><div class="children"><div class="content">I would definitely discount OpenAI equity compared to even other private AI labs (i.e. Anthropic) given the shenanigans, but they have in fact held 3 tender offers and former employees were not, as far as we know, excluded (though they may have been limited to selling $2m worth of equity, rather than $10m).</div><br/></div></div></div></div><div id="40732972" class="c"><input type="checkbox" id="c-40732972" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731265">parent</a><span>|</span><a href="#40731618">prev</a><span>|</span><a href="#40731438">next</a><span>|</span><label class="collapse" for="c-40732972">[-]</label><label class="expand" for="c-40732972">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Word on town is OpenAI folks heavily selling shares in secondaries in 100s of millions</i><p>OpenAI heavily restricts the selling of its &quot;shares,&quot; which tends to come with management picking the winners and losers among its ESOs. Heavily, heavily discount an asset you cannot liquidate without someone&#x27;s position, particularly if that person is your employer.</div><br/><div id="40733991" class="c"><input type="checkbox" id="c-40733991" checked=""/><div class="controls bullet"><span class="by">shuckles</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40732972">parent</a><span>|</span><a href="#40731438">next</a><span>|</span><label class="collapse" for="c-40733991">[-]</label><label class="expand" for="c-40733991">[2 more]</label></div><br/><div class="children"><div class="content">Did you mean permission, not position? I’m not sure I understand what someone’s position could mean.</div><br/><div id="40734180" class="c"><input type="checkbox" id="c-40734180" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40733991">parent</a><span>|</span><a href="#40731438">next</a><span>|</span><label class="collapse" for="c-40734180">[-]</label><label class="expand" for="c-40734180">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div></div></div></div></div></div></div><div id="40731438" class="c"><input type="checkbox" id="c-40731438" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731052">parent</a><span>|</span><a href="#40731265">prev</a><span>|</span><a href="#40730975">next</a><span>|</span><label class="collapse" for="c-40731438">[-]</label><label class="expand" for="c-40731438">[1 more]</label></div><br/><div class="children"><div class="content">don’t comment if you don’t know what you’re talking about, they have tender offers</div><br/></div></div></div></div><div id="40730975" class="c"><input type="checkbox" id="c-40730975" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730703">parent</a><span>|</span><a href="#40731052">prev</a><span>|</span><a href="#40730808">next</a><span>|</span><label class="collapse" for="c-40730975">[-]</label><label class="expand" for="c-40730975">[17 more]</label></div><br/><div class="children"><div class="content">&gt; their TC is now $900k.<p>Everyone knows that openai TC is heavily weighted by ~~RSUs~~ options that themselves are heavily weighted by hopes and dreams.</div><br/><div id="40731307" class="c"><input type="checkbox" id="c-40731307" checked=""/><div class="controls bullet"><span class="by">doktorhladnjak</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730975">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40731307">[-]</label><label class="expand" for="c-40731307">[10 more]</label></div><br/><div class="children"><div class="content">When I looked into it and talked to some hiring managers, the big names were offering cash comp similar to total comp for big tech, with stock (sometimes complicated arrangements that were not options or RSUs) on top of that. I’m talking $400k cash for a senior engineer with equity on top.</div><br/><div id="40731460" class="c"><input type="checkbox" id="c-40731460" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731307">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40731460">[-]</label><label class="expand" for="c-40731460">[9 more]</label></div><br/><div class="children"><div class="content">&gt; big names<p>Big names where? Inside of openai? What does that even mean?<p>The only place you can get 400k cash base for senior is quantfi</div><br/><div id="40731485" class="c"><input type="checkbox" id="c-40731485" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731460">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40731485">[-]</label><label class="expand" for="c-40731485">[8 more]</label></div><br/><div class="children"><div class="content">&gt; The only place you can get 400k cash base for senior is quantfi<p>confident yet wrong<p>not only can you get that much at AI companies, netflix will also pay that much all cash - and that’s fully public info</div><br/><div id="40731743" class="c"><input type="checkbox" id="c-40731743" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731485">parent</a><span>|</span><a href="#40731751">next</a><span>|</span><label class="collapse" for="c-40731743">[-]</label><label class="expand" for="c-40731743">[1 more]</label></div><br/><div class="children"><div class="content">&gt; not only can you get that much at AI companies<p>Please show not tell<p>&gt; netflix will also pay that much all cash<p>Okay that&#x27;s true</div><br/></div></div><div id="40731751" class="c"><input type="checkbox" id="c-40731751" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731485">parent</a><span>|</span><a href="#40731743">prev</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40731751">[-]</label><label class="expand" for="c-40731751">[6 more]</label></div><br/><div class="children"><div class="content">Netflix is just cash, no stock. That’s different from 400k stock + cash.</div><br/><div id="40731796" class="c"><input type="checkbox" id="c-40731796" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731751">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40731796">[-]</label><label class="expand" for="c-40731796">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The only place you can get 400k cash base for senior is quantfi<p>That statement is false for the reasons I said. I’m not sure why your point matters to what I’m saying</div><br/><div id="40733972" class="c"><input type="checkbox" id="c-40733972" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731796">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40733972">[-]</label><label class="expand" for="c-40733972">[4 more]</label></div><br/><div class="children"><div class="content">Because op’s usage of base implies base + stock. including a place where base = total comp is really misleading and is just being unnecessarily pedantic about terminology.<p>OP is correct that a base cash of 400k is truly rare if you’re talking about typical total comp packages where 50% is base and 50% is stock.</div><br/><div id="40734032" class="c"><input type="checkbox" id="c-40734032" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40733972">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40734032">[-]</label><label class="expand" for="c-40734032">[3 more]</label></div><br/><div class="children"><div class="content">quantfi doesn’t pay stock at all usually so i disagree that it implies cash + stock</div><br/><div id="40734310" class="c"><input type="checkbox" id="c-40734310" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40734032">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40734310">[-]</label><label class="expand" for="c-40734310">[2 more]</label></div><br/><div class="children"><div class="content">I don’t know what point you’re trying to make other than being super pedantic. This was a discussion about how OpenAI’s base of 400k is unique within the context of a TCO in the 800-900k range. It is. That quantfi and Netflix offer similar base because that’s also their TCO is a silly argument to make.</div><br/><div id="40736455" class="c"><input type="checkbox" id="c-40736455" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40734310">parent</a><span>|</span><a href="#40731385">next</a><span>|</span><label class="collapse" for="c-40736455">[-]</label><label class="expand" for="c-40736455">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This was a discussion about how OpenAI’s base of 400k is unique within the context of a TCO in the 800-900k range.<p>That&#x27;s not how I interpret the conversation.<p>I see a claim that 900k is a BS number, a counterargument that many big AI companies will give you 400k of that in cash so the offers are in fact very hot, then a claim that only finance offers 400k cash, and a claim that netflix offers 400k cash.<p>I don&#x27;t see anything that limits these comparisons to companies with specific TCOs.<p>Even if the use of the word &quot;base&quot; is intended to imply that there&#x27;s some stock, it doesn&#x27;t imply any particular amount of stock.  But my reading is that the word &quot;base&quot; is there to say that stock <i>can</i> be added on top.<p>You&#x27;re the one being pedantic when you insist that 400k cash is not a valid example of 400k cash base.<p>Notice how the person being replied to looked at the Netflix example and said &quot;Okay that&#x27;s true&quot;.  They know what they meant a lot better than you do.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40731385" class="c"><input type="checkbox" id="c-40731385" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730975">parent</a><span>|</span><a href="#40731307">prev</a><span>|</span><a href="#40731567">next</a><span>|</span><label class="collapse" for="c-40731385">[-]</label><label class="expand" for="c-40731385">[2 more]</label></div><br/><div class="children"><div class="content">Everything OpenAI does is about weights.</div><br/><div id="40731841" class="c"><input type="checkbox" id="c-40731841" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731385">parent</a><span>|</span><a href="#40731567">next</a><span>|</span><label class="collapse" for="c-40731841">[-]</label><label class="expand" for="c-40731841">[1 more]</label></div><br/><div class="children"><div class="content">bro does their ceo even lift?</div><br/></div></div></div></div><div id="40731567" class="c"><input type="checkbox" id="c-40731567" checked=""/><div class="controls bullet"><span class="by">almost_usual</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730975">parent</a><span>|</span><a href="#40731385">prev</a><span>|</span><a href="#40730808">next</a><span>|</span><label class="collapse" for="c-40731567">[-]</label><label class="expand" for="c-40731567">[4 more]</label></div><br/><div class="children"><div class="content">You mean PPUs or smoke and mirrors compensation. RSUs are actually worth something.</div><br/><div id="40731635" class="c"><input type="checkbox" id="c-40731635" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731567">parent</a><span>|</span><a href="#40730808">next</a><span>|</span><label class="collapse" for="c-40731635">[-]</label><label class="expand" for="c-40731635">[3 more]</label></div><br/><div class="children"><div class="content">why are PPUs “smoke and mirrors” and RSUs “worth something”?<p>i suspect people commenting this don’t have a clue how PPU compensation actually works</div><br/><div id="40733613" class="c"><input type="checkbox" id="c-40733613" checked=""/><div class="controls bullet"><span class="by">almost_usual</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731635">parent</a><span>|</span><a href="#40733665">next</a><span>|</span><label class="collapse" for="c-40733613">[-]</label><label class="expand" for="c-40733613">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Note at offer time candidates do not know how many PPUs they will be receiving or how many exist in total. This is important because it’s not clear to candidates if they are receiving 1% or 0.001% of profits for instance. Even when giving options, some startups are often unclear or simply do not share the total number of outstanding shares. That said, this is generally considered bad practice and unfavorable for employees. Additionally, tender offers are not guaranteed to happen and the cadence may also not be known.<p>&gt; PPUs also are restricted by a 2-year lock, meaning that if there’s a liquidation event, a new hire can’t sell their units within their first 2 years. Another key difference is that the growth is currently capped at 10x. Similar to their overall company structure, the PPUs are capped at a growth of 10 times the original value. So in the offer example above, the candidate received $2M worth of PPUs, which means that their capped amount they could sell them for would be $20M<p>&gt; The most recent liquidation event we’re aware of happened during a tender offer earlier this year. It was during this event that some early employees were able to sell their profit participation units. It’s difficult to know how often these events happen and who is allowed to sell, though, as it’s on company discretion.<p><a href="https:&#x2F;&#x2F;www.levels.fyi&#x2F;blog&#x2F;openai-compensation.html" rel="nofollow">https:&#x2F;&#x2F;www.levels.fyi&#x2F;blog&#x2F;openai-compensation.html</a><p>Edit:<p>I’m realizing we had the exact same conversation a month ago. It sounds like you have more insider information.</div><br/></div></div></div></div></div></div></div></div><div id="40730931" class="c"><input type="checkbox" id="c-40730931" checked=""/><div class="controls bullet"><span class="by">ilrwbwrkhv</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730703">parent</a><span>|</span><a href="#40730808">prev</a><span>|</span><a href="#40730934">next</a><span>|</span><label class="collapse" for="c-40730931">[-]</label><label class="expand" for="c-40730931">[1 more]</label></div><br/><div class="children"><div class="content">Google itself is now filled with TC optimizing folks, just one level lower than the ones at Open AI.</div><br/></div></div><div id="40730934" class="c"><input type="checkbox" id="c-40730934" checked=""/><div class="controls bullet"><span class="by">iknownthing</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730703">parent</a><span>|</span><a href="#40730931">prev</a><span>|</span><a href="#40734076">next</a><span>|</span><label class="collapse" for="c-40730934">[-]</label><label class="expand" for="c-40730934">[1 more]</label></div><br/><div class="children"><div class="content">Seems like you need to have been working at a place like Google too</div><br/></div></div><div id="40731453" class="c"><input type="checkbox" id="c-40731453" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730703">parent</a><span>|</span><a href="#40734076">prev</a><span>|</span><a href="#40730741">next</a><span>|</span><label class="collapse" for="c-40731453">[-]</label><label class="expand" for="c-40731453">[2 more]</label></div><br/><div class="children"><div class="content">the thing about mentioning compensation numbers on HN is you will get tons of pissy&#x2F;ressentiment-y replies</div><br/><div id="40735115" class="c"><input type="checkbox" id="c-40735115" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731453">parent</a><span>|</span><a href="#40730741">next</a><span>|</span><label class="collapse" for="c-40735115">[-]</label><label class="expand" for="c-40735115">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t care about these. I care about the readers who might not have done job searching recently and might not know their worth in the market.</div><br/></div></div></div></div></div></div><div id="40730741" class="c"><input type="checkbox" id="c-40730741" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730572">parent</a><span>|</span><a href="#40730703">prev</a><span>|</span><a href="#40731484">next</a><span>|</span><label class="collapse" for="c-40730741">[-]</label><label class="expand" for="c-40730741">[1 more]</label></div><br/><div class="children"><div class="content">&quot;...even the occasional &quot;hobbyist&quot; making nothing and churning out great work while hiding behind an anime girl avatar.&quot;<p>the people i often have the most respect for.</div><br/></div></div><div id="40731484" class="c"><input type="checkbox" id="c-40731484" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730572">parent</a><span>|</span><a href="#40730741">prev</a><span>|</span><a href="#40730986">next</a><span>|</span><label class="collapse" for="c-40731484">[-]</label><label class="expand" for="c-40731484">[5 more]</label></div><br/><div class="children"><div class="content">Half the advancements around Stable Diffusion (Controlnet etc.) came from internet randoms wanting better anime waifus</div><br/><div id="40731550" class="c"><input type="checkbox" id="c-40731550" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731484">parent</a><span>|</span><a href="#40730986">next</a><span>|</span><label class="collapse" for="c-40731550">[-]</label><label class="expand" for="c-40731550">[4 more]</label></div><br/><div class="children"><div class="content">advancements around parameter efficient fine tuning came from internet randoms because big cos don’t care about PEFT</div><br/><div id="40732302" class="c"><input type="checkbox" id="c-40732302" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731550">parent</a><span>|</span><a href="#40730986">next</a><span>|</span><label class="collapse" for="c-40732302">[-]</label><label class="expand" for="c-40732302">[3 more]</label></div><br/><div class="children"><div class="content">... Sort of?<p>HF is sort of big now. Stanford is well funded and they did PyReft.</div><br/><div id="40732322" class="c"><input type="checkbox" id="c-40732322" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40732302">parent</a><span>|</span><a href="#40730986">next</a><span>|</span><label class="collapse" for="c-40732322">[-]</label><label class="expand" for="c-40732322">[2 more]</label></div><br/><div class="children"><div class="content">HF is not very big, Stanford doesn’t have lots of compute.<p>Neither of these are even remotely big labs like what I’m discussing</div><br/><div id="40735822" class="c"><input type="checkbox" id="c-40735822" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40732322">parent</a><span>|</span><a href="#40730986">next</a><span>|</span><label class="collapse" for="c-40735822">[-]</label><label class="expand" for="c-40735822">[1 more]</label></div><br/><div class="children"><div class="content">HF has raised more than $400m. If that doesn&#x27;t qualify them as &quot;big&quot;, I don&#x27;t know what does.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40730986" class="c"><input type="checkbox" id="c-40730986" checked=""/><div class="controls bullet"><span class="by">auggierose</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730572">parent</a><span>|</span><a href="#40731484">prev</a><span>|</span><a href="#40730694">next</a><span>|</span><label class="collapse" for="c-40730986">[-]</label><label class="expand" for="c-40730986">[5 more]</label></div><br/><div class="children"><div class="content">TC optimization being tail call optimization?</div><br/><div id="40731058" class="c"><input type="checkbox" id="c-40731058" checked=""/><div class="controls bullet"><span class="by">klyrs</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730986">parent</a><span>|</span><a href="#40731078">next</a><span>|</span><label class="collapse" for="c-40731058">[-]</label><label class="expand" for="c-40731058">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t get to that level by thinking about <i>code</i>...</div><br/></div></div><div id="40731078" class="c"><input type="checkbox" id="c-40731078" checked=""/><div class="controls bullet"><span class="by">lbotos</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730986">parent</a><span>|</span><a href="#40731058">prev</a><span>|</span><a href="#40734394">next</a><span>|</span><label class="collapse" for="c-40731078">[-]</label><label class="expand" for="c-40731078">[1 more]</label></div><br/><div class="children"><div class="content">Could be sarcasm, but I&#x27;ll engage in good faith: Total Compensation</div><br/></div></div><div id="40734394" class="c"><input type="checkbox" id="c-40734394" checked=""/><div class="controls bullet"><span class="by">cozzyd</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730986">parent</a><span>|</span><a href="#40731078">prev</a><span>|</span><a href="#40731116">next</a><span>|</span><label class="collapse" for="c-40734394">[-]</label><label class="expand" for="c-40734394">[1 more]</label></div><br/><div class="children"><div class="content">Curie temperature</div><br/></div></div><div id="40731116" class="c"><input type="checkbox" id="c-40731116" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730986">parent</a><span>|</span><a href="#40734394">prev</a><span>|</span><a href="#40730694">next</a><span>|</span><label class="collapse" for="c-40731116">[-]</label><label class="expand" for="c-40731116">[1 more]</label></div><br/><div class="children"><div class="content">Nope, that&#x27;s a misnomer, it&#x27;s tail-call elimination. You can&#x27;t call it an optimization if it&#x27;s essential for proper functioning of the program.<p>(they mean total compensation)</div><br/></div></div></div></div><div id="40730694" class="c"><input type="checkbox" id="c-40730694" checked=""/><div class="controls bullet"><span class="by">015a</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730572">parent</a><span>|</span><a href="#40730986">prev</a><span>|</span><a href="#40730484">next</a><span>|</span><label class="collapse" for="c-40730694">[-]</label><label class="expand" for="c-40730694">[5 more]</label></div><br/><div class="children"><div class="content">Definitely true of even normal software engineering; my experience has been the opposite of expectations, that TC-creep has infected the industry to an irreparable degree and the most talented people I&#x27;ve ever worked around or with are in boring, medium-sized enterprises in the midwest US or australia, you&#x27;ll probably never hear of them, and every big tech company would absolutely love to hire them but just can&#x27;t figure out the interview process to weed them apart from the TC grifters.<p>TC is actually totally uncorrelated with the quality of talent you can hire, beyond some low number that pretty much any funded startup could pay. Businesses hate to hear this, because money is easy to turn the dial up on; but most have no idea how to turn the dial up on what really matters to high talent individuals. Fortunately, I doubt Ilya will have any problem with that.</div><br/><div id="40731148" class="c"><input type="checkbox" id="c-40731148" checked=""/><div class="controls bullet"><span class="by">fromMars</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730694">parent</a><span>|</span><a href="#40731464">next</a><span>|</span><label class="collapse" for="c-40731148">[-]</label><label class="expand" for="c-40731148">[3 more]</label></div><br/><div class="children"><div class="content">I find this hard to believe having worked in multiple enterprises and in the FAANG world.<p>In my anecdotal experience, I can only think of one or two examples of someone from the enterprise world who I would consider outstanding.<p>The overall quality of engineers is much higher at the FAANG companies.</div><br/><div id="40732421" class="c"><input type="checkbox" id="c-40732421" checked=""/><div class="controls bullet"><span class="by">null0pointer</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731148">parent</a><span>|</span><a href="#40731464">next</a><span>|</span><label class="collapse" for="c-40732421">[-]</label><label class="expand" for="c-40732421">[2 more]</label></div><br/><div class="children"><div class="content">I have also worked in multiple different sized companies, including FAANG, and multiple countries. My assessment is that FAANGs tend to select for generally intelligent people who can learn quickly and adapt to new situations easily but who nowadays tend to be passionless and indifferent to anything but money and prestige. Personally I think passion is the differentiator here, rather than talent, when it comes to doing a good job. Passion means caring about your work and its impact beyond what it means for your own career advancement. It means caring about building the best possible products where “best” is defined as delivering the most value for your users rather than the most value for the company. The question is whether big tech is unable to select for passion or whether there are simply not enough passionate people to hire when operating at FAANG scale. Most likely it’s the latter.<p>So I guess I agree with both you and the parent comment somewhat in that in general the bar is higher at FAANGs but at the same time I have multiple former colleagues from smaller companies who I consider to be excellent, passionate engineers but who cannot be lured to big tech by any amount of money or prestige (I’ve tried). While many passionless “arbitrary metric optimizers” happily join FAANGs and do whatever needs to be done to climb the ladder without a second thought.</div><br/><div id="40734812" class="c"><input type="checkbox" id="c-40734812" checked=""/><div class="controls bullet"><span class="by">fromMars</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40732421">parent</a><span>|</span><a href="#40731464">next</a><span>|</span><label class="collapse" for="c-40734812">[-]</label><label class="expand" for="c-40734812">[1 more]</label></div><br/><div class="children"><div class="content">I sort of agree and disagree. I wouldn&#x27;t agree with the idea that most FAANG engineers are not passionate by nature about their work.<p>What I would say is that the bureaucracy and bullshit one has to deal with makes it hard to maintain that passion and that many end up as TC optimizers in the sense that they stay instead of working someplace better for less TC.<p>That said, I am not sure how many would make different choices.  Many who join a FAANG company don&#x27;t have the slightest inkling of what it will be like and once they realize that they are tiny cog in a giant machine it&#x27;s hard to leave the TC and perks behind.</div><br/></div></div></div></div></div></div><div id="40731464" class="c"><input type="checkbox" id="c-40731464" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730694">parent</a><span>|</span><a href="#40731148">prev</a><span>|</span><a href="#40730484">next</a><span>|</span><label class="collapse" for="c-40731464">[-]</label><label class="expand" for="c-40731464">[1 more]</label></div><br/><div class="children"><div class="content">perfect sort of thing to say to get lots of upvotes, but absolutely false in my experience at both enterprise and bigtech</div><br/></div></div></div></div></div></div><div id="40730484" class="c"><input type="checkbox" id="c-40730484" checked=""/><div class="controls bullet"><span class="by">Q6T46nT668w6i3m</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730572">prev</a><span>|</span><a href="#40730469">next</a><span>|</span><label class="collapse" for="c-40730484">[-]</label><label class="expand" for="c-40730484">[5 more]</label></div><br/><div class="children"><div class="content">Academic compensation is different than what you’d find elsewhere on Hacker News. Likewise, academic performance  is evaluated differently than what you’d expect as a software engineer. Ultimately, everyone cares about scientific impact so academic compensation relies on name and recognition far more than money. Personally, I care about the performance of the researchers (i.e., their publications), the institution’s larger research program (and their resources), the institution’s commitment to my research (e.g., fellowships and tenure). I want to do science for my entire career so I prioritize longevity rather than a quick buck.<p>I’ll add, the lack of compute resources was a far worse problem early in the deep learning research boom, but the market has adjusted and most researchers are able to be productive with  existing compute infrastructure.</div><br/><div id="40730558" class="c"><input type="checkbox" id="c-40730558" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730484">parent</a><span>|</span><a href="#40730469">next</a><span>|</span><label class="collapse" for="c-40730558">[-]</label><label class="expand" for="c-40730558">[4 more]</label></div><br/><div class="children"><div class="content">But wouldn&#x27;t the focus on &quot;safety first&quot; sort of preclude them from giving their researchers the unfettered right to publish their work however and whenever they see fit? Isn&#x27;t the idea to basically try to solve the problems in secret and only release things when they have high confidence in the safety properties?<p>If I were a researcher, I think I&#x27;d care more about ensuring that I get credit for any important theoretical discoveries I make. This is something that LeCun is constantly stressing and I think people underestimate this drive. Of course, there might be enough researchers today who are sufficiently scared of bad AI safety outcomes that they&#x27;re willing to subordinate their own ego and professional drive to the &quot;greater good&quot; of society (at least in their own mind).</div><br/><div id="40730791" class="c"><input type="checkbox" id="c-40730791" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730558">parent</a><span>|</span><a href="#40730469">next</a><span>|</span><label class="collapse" for="c-40730791">[-]</label><label class="expand" for="c-40730791">[3 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re working on <i>superintelligence</i> I don&#x27;t think you&#x27;d be worried about not getting credit due to a lack of publications, of all things. If it works, it&#x27;s the sort of thing that gets you in the history books.</div><br/><div id="40730911" class="c"><input type="checkbox" id="c-40730911" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730791">parent</a><span>|</span><a href="#40730469">next</a><span>|</span><label class="collapse" for="c-40730911">[-]</label><label class="expand" for="c-40730911">[2 more]</label></div><br/><div class="children"><div class="content">Not sure about that. It might get <i>Ilya</i> in the history books, and maybe some of the other high profile people he recruits early on, but a junior researcher&#x2F;developer who makes a high impact contribution could easily get overlooked. Whereas if that person can have their name as lead author on a published paper, it makes it much easier to measure individual contributions.</div><br/><div id="40731758" class="c"><input type="checkbox" id="c-40731758" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730911">parent</a><span>|</span><a href="#40730469">next</a><span>|</span><label class="collapse" for="c-40731758">[-]</label><label class="expand" for="c-40731758">[1 more]</label></div><br/><div class="children"><div class="content">There is a human cognitive limit to the detail in which we can analyze and understand history.<p>This limit, just like our population count, will not outlast the singularity. I did the math a while back, and at the limit of available energy, the universe has comfortable room for something like 10^42 humans. Every single one of those humans will owe their existence to our civilization in general and the Superintelligence team in specific. There&#x27;ll be enough fame to go around.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40730469" class="c"><input type="checkbox" id="c-40730469" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730484">prev</a><span>|</span><a href="#40730398">next</a><span>|</span><label class="collapse" for="c-40730469">[-]</label><label class="expand" for="c-40730469">[8 more]</label></div><br/><div class="children"><div class="content">At the end game, a &quot;non-safe&quot; superinteligence seems easier to create, so like any other technology, some people will create it (even if just because they can&#x27;t make it safe). And in a world with multiple superintelligent agents, how can the safe ones &quot;win&quot;? It seems like a safe AI is at inherent disadvantage for survival.</div><br/><div id="40730543" class="c"><input type="checkbox" id="c-40730543" checked=""/><div class="controls bullet"><span class="by">arbuge</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730469">parent</a><span>|</span><a href="#40730398">next</a><span>|</span><label class="collapse" for="c-40730543">[-]</label><label class="expand" for="c-40730543">[7 more]</label></div><br/><div class="children"><div class="content">The current intelligences of the world (us) have organized their civilization in a way that the conforming members of society are the norm and criminals the outcasts. Certainly not a perfect system, but something along those lines for the most part.<p>I like to think AGIs will decide to do that too.</div><br/><div id="40730792" class="c"><input type="checkbox" id="c-40730792" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730543">parent</a><span>|</span><a href="#40730834">next</a><span>|</span><label class="collapse" for="c-40730792">[-]</label><label class="expand" for="c-40730792">[1 more]</label></div><br/><div class="children"><div class="content">I disagree that civilization is organized along the lines of conforming and criminals. Rather, I would argue that the current intelligences of the world have primarily organized civilization in such a way that a small percentage of its members control the vast majority of all human resources, and the bottom 50% control almost nothing[0]<p>I would hope that AGI would prioritize humanity itself, but since it&#x27;s likely to be created and&#x2F;or controlled by a subset of that same very small percentage of humans, I&#x27;m not hopeful.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wealth_inequality_in_the_United_States#&#x2F;media&#x2F;File:US_Wealth_Inequality_-_v2.png" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wealth_inequality_in_the_Unite...</a></div><br/></div></div><div id="40730834" class="c"><input type="checkbox" id="c-40730834" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730543">parent</a><span>|</span><a href="#40730792">prev</a><span>|</span><a href="#40730848">next</a><span>|</span><label class="collapse" for="c-40730834">[-]</label><label class="expand" for="c-40730834">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a beautiful system, wherein &quot;criminality&quot; can be used to label and control any and all persons who disagree with the whim of the incumbent class.<p>Perhaps this isn&#x27;t a system we should be trying to emulate with a technology that promises to free us of our current inefficiencies or miseries.</div><br/></div></div><div id="40730848" class="c"><input type="checkbox" id="c-40730848" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730543">parent</a><span>|</span><a href="#40730834">prev</a><span>|</span><a href="#40730625">next</a><span>|</span><label class="collapse" for="c-40730848">[-]</label><label class="expand" for="c-40730848">[2 more]</label></div><br/><div class="children"><div class="content">Our current meatware AGIs (corporations) are lawless as fuck and have effectively no ethics at all, which doesn’t bode well.</div><br/></div></div><div id="40730625" class="c"><input type="checkbox" id="c-40730625" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730543">parent</a><span>|</span><a href="#40730848">prev</a><span>|</span><a href="#40730398">next</a><span>|</span><label class="collapse" for="c-40730625">[-]</label><label class="expand" for="c-40730625">[2 more]</label></div><br/><div class="children"><div class="content">They well may, the problem is ensuring that humanity also survives.</div><br/><div id="40731145" class="c"><input type="checkbox" id="c-40731145" checked=""/><div class="controls bullet"><span class="by">TwoCent</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730625">parent</a><span>|</span><a href="#40730398">next</a><span>|</span><label class="collapse" for="c-40731145">[-]</label><label class="expand" for="c-40731145">[1 more]</label></div><br/><div class="children"><div class="content">The example from our environment suggests that the apex intelligences in the environment treat all other intelligent agents in only a few ways:<p>1. Pests to eliminate
2. Benign neglect
3. Workers
4. Pets
5. Food<p>That suggests that there are scenarios under which we survive.  I&#x27;m not sure we&#x27;d like any of them, though &quot;benign neglect&quot; might be the best of a bad lot.</div><br/></div></div></div></div></div></div></div></div><div id="40730398" class="c"><input type="checkbox" id="c-40730398" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730469">prev</a><span>|</span><a href="#40731663">next</a><span>|</span><label class="collapse" for="c-40730398">[-]</label><label class="expand" for="c-40730398">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Perhaps they can find people who are ideologically driven<p>given the nature of their mission, this shouldn&#x27;t be too terribly difficult; many gifted researchers do not go to the highest bidder</div><br/></div></div><div id="40731663" class="c"><input type="checkbox" id="c-40731663" checked=""/><div class="controls bullet"><span class="by">kmacdough</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730398">prev</a><span>|</span><a href="#40730736">next</a><span>|</span><label class="collapse" for="c-40731663">[-]</label><label class="expand" for="c-40731663">[1 more]</label></div><br/><div class="children"><div class="content">Generally, the mindset that makes the best engineers is an obsession with solving hard problems. Anecdotally, there&#x27;s not a lot of overlap between the best engineers I know and the best paid engineers I know. The best engineers I know are too obsessed with solving problems to be sidetracked the salary game. The best paid engineers I know are great engineers, but the spend a large amount of time playing the salary game, bouncing between companies and are always doing the work that looks best on a resume, not the best work they know how to do.</div><br/></div></div><div id="40730736" class="c"><input type="checkbox" id="c-40730736" checked=""/><div class="controls bullet"><span class="by">aresant</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40731663">prev</a><span>|</span><a href="#40730564">next</a><span>|</span><label class="collapse" for="c-40730736">[-]</label><label class="expand" for="c-40730736">[1 more]</label></div><br/><div class="children"><div class="content">My guess is they will work on a protocol to drive safety with the view that every material player will use &#x2F; be regulated and required to use that could lead to a very robust business model<p>I assume that OpenAI and others will support this effort and the comp &#x2F; training &#x2F; etc and they will be very well positioned to offer comparable $$$ packages, leverage resources, etc</div><br/></div></div><div id="40730564" class="c"><input type="checkbox" id="c-40730564" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730736">prev</a><span>|</span><a href="#40730796">next</a><span>|</span><label class="collapse" for="c-40730564">[-]</label><label class="expand" for="c-40730564">[1 more]</label></div><br/><div class="children"><div class="content">They will be able to pay their researchers the same way every other startup in the space is doing it – by raising an absurd amount of money.</div><br/></div></div><div id="40730796" class="c"><input type="checkbox" id="c-40730796" checked=""/><div class="controls bullet"><span class="by">neural_thing</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730564">prev</a><span>|</span><a href="#40730396">next</a><span>|</span><label class="collapse" for="c-40730796">[-]</label><label class="expand" for="c-40730796">[1 more]</label></div><br/><div class="children"><div class="content">Daniel Gross (with his partner Nat Friedman) invested $100M into Magic alone.<p>I don&#x27;t think SSI will struggle to raise money.</div><br/></div></div><div id="40730396" class="c"><input type="checkbox" id="c-40730396" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730796">prev</a><span>|</span><a href="#40734423">next</a><span>|</span><label class="collapse" for="c-40730396">[-]</label><label class="expand" for="c-40730396">[6 more]</label></div><br/><div class="children"><div class="content">I think they will easily find enough capable altruistic people for this mission.</div><br/><div id="40731010" class="c"><input type="checkbox" id="c-40731010" checked=""/><div class="controls bullet"><span class="by">EncomLab</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730396">parent</a><span>|</span><a href="#40732207">next</a><span>|</span><label class="collapse" for="c-40731010">[-]</label><label class="expand" for="c-40731010">[4 more]</label></div><br/><div class="children"><div class="content">I mean SBF was into Altruism - look how that turned out....</div><br/><div id="40731472" class="c"><input type="checkbox" id="c-40731472" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731010">parent</a><span>|</span><a href="#40732463">next</a><span>|</span><label class="collapse" for="c-40731472">[-]</label><label class="expand" for="c-40731472">[1 more]</label></div><br/><div class="children"><div class="content">and that soured you on altruism as a concept??<p>i find the way people reason nowadays baffling</div><br/></div></div><div id="40732463" class="c"><input type="checkbox" id="c-40732463" checked=""/><div class="controls bullet"><span class="by">null0pointer</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731010">parent</a><span>|</span><a href="#40731472">prev</a><span>|</span><a href="#40731043">next</a><span>|</span><label class="collapse" for="c-40732463">[-]</label><label class="expand" for="c-40732463">[1 more]</label></div><br/><div class="children"><div class="content">Not really. He was into altruism insofar as it acted as moral licensing for him.</div><br/></div></div><div id="40731043" class="c"><input type="checkbox" id="c-40731043" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40731010">parent</a><span>|</span><a href="#40732463">prev</a><span>|</span><a href="#40732207">next</a><span>|</span><label class="collapse" for="c-40731043">[-]</label><label class="expand" for="c-40731043">[1 more]</label></div><br/><div class="children"><div class="content">So what? He was a phony. And I&#x27;m not talking about the Effective Altruism movement.</div><br/></div></div></div></div><div id="40732207" class="c"><input type="checkbox" id="c-40732207" checked=""/><div class="controls bullet"><span class="by">richie-guix</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730396">parent</a><span>|</span><a href="#40731010">prev</a><span>|</span><a href="#40734423">next</a><span>|</span><label class="collapse" for="c-40732207">[-]</label><label class="expand" for="c-40732207">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if spelling mistake.</div><br/></div></div></div></div><div id="40734423" class="c"><input type="checkbox" id="c-40734423" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730396">prev</a><span>|</span><a href="#40730693">next</a><span>|</span><label class="collapse" for="c-40734423">[-]</label><label class="expand" for="c-40734423">[1 more]</label></div><br/><div class="children"><div class="content">Great analysis, but you&#x27;re missing two key factors IMO:<p>1. People who honestly think AGI is here aren&#x27;t thinking about their careers in the typical sense at all. It&#x27;s sorta ethical&#x2F;&quot;ideological&quot;, but it&#x27;s mostly just practical.<p>2. People who honestly think AGI is here are fucking terrified right now, and were already treating Ilya as a spiritual center after Altman&#x27;s coup (quite possibly an unearned title, but oh well, that&#x27;s history for ya). A rallying cry like this -- so clearly aimed at the big picture instead of marketing they don&#x27;t even need <i>CSS</i> -- will be seen as a do-or-die moment by many, I think. There&#x27;s only so much of &quot;general industry continues to go in direction experts recommend against; corporate consolidation continues!&quot; headlines an ethical engineer can take before snapping and trying to take on Goliath, odds be damned</div><br/></div></div><div id="40730693" class="c"><input type="checkbox" id="c-40730693" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40734423">prev</a><span>|</span><a href="#40730367">next</a><span>|</span><label class="collapse" for="c-40730693">[-]</label><label class="expand" for="c-40730693">[4 more]</label></div><br/><div class="children"><div class="content">Are you seriously asking how the most talented AI researcher of the last decade will be able to recruit other researchers? Ilya saw the potential of deep learning way before other machine learning academics.</div><br/><div id="40734680" class="c"><input type="checkbox" id="c-40734680" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730693">parent</a><span>|</span><a href="#40732269">next</a><span>|</span><label class="collapse" for="c-40734680">[-]</label><label class="expand" for="c-40734680">[1 more]</label></div><br/><div class="children"><div class="content">The idea of multi layer neural networks goes back to the 60’s.</div><br/></div></div><div id="40732269" class="c"><input type="checkbox" id="c-40732269" checked=""/><div class="controls bullet"><span class="by">dbish</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730693">parent</a><span>|</span><a href="#40734680">prev</a><span>|</span><a href="#40730367">next</a><span>|</span><label class="collapse" for="c-40732269">[-]</label><label class="expand" for="c-40732269">[2 more]</label></div><br/><div class="children"><div class="content">Sorry, are you attributing all of deep learning research to Ilya? The most talented AI researcher of the last decade?</div><br/><div id="40732321" class="c"><input type="checkbox" id="c-40732321" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40732269">parent</a><span>|</span><a href="#40730367">next</a><span>|</span><label class="collapse" for="c-40732321">[-]</label><label class="expand" for="c-40732321">[1 more]</label></div><br/><div class="children"><div class="content">Not attributing all of it</div><br/></div></div></div></div></div></div><div id="40730367" class="c"><input type="checkbox" id="c-40730367" checked=""/><div class="controls bullet"><span class="by">imbusy111</span><span>|</span><a href="#40730300">parent</a><span>|</span><a href="#40730693">prev</a><span>|</span><a href="#40731486">next</a><span>|</span><label class="collapse" for="c-40730367">[-]</label><label class="expand" for="c-40730367">[7 more]</label></div><br/><div class="children"><div class="content">Last I checked the researcher salaries haven&#x27;t even reached software engineer levels.</div><br/><div id="40730435" class="c"><input type="checkbox" id="c-40730435" checked=""/><div class="controls bullet"><span class="by">shadow28</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730367">parent</a><span>|</span><a href="#40732273">next</a><span>|</span><label class="collapse" for="c-40730435">[-]</label><label class="expand" for="c-40730435">[5 more]</label></div><br/><div class="children"><div class="content">The kind of AI researchers being discussed here likely make an order of magnitude more than run of the mill &quot;software engineers&quot;.</div><br/><div id="40730445" class="c"><input type="checkbox" id="c-40730445" checked=""/><div class="controls bullet"><span class="by">imbusy111</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730435">parent</a><span>|</span><a href="#40730575">next</a><span>|</span><label class="collapse" for="c-40730445">[-]</label><label class="expand" for="c-40730445">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re comparing top names with run of the mill engineers maybe, which isn&#x27;t fair.<p>And maybe you need to discover talent rather than buy talent from the previous generation.</div><br/><div id="40730513" class="c"><input type="checkbox" id="c-40730513" checked=""/><div class="controls bullet"><span class="by">shadow28</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730445">parent</a><span>|</span><a href="#40730702">next</a><span>|</span><label class="collapse" for="c-40730513">[-]</label><label class="expand" for="c-40730513">[1 more]</label></div><br/><div class="children"><div class="content">AI researchers at top firms make significantly more than software engineers at the same firms though (granted that the difference is likely not an order of magnitude in this case though).</div><br/></div></div></div></div><div id="40730575" class="c"><input type="checkbox" id="c-40730575" checked=""/><div class="controls bullet"><span class="by">Q6T46nT668w6i3m</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730435">parent</a><span>|</span><a href="#40730445">prev</a><span>|</span><a href="#40732273">next</a><span>|</span><label class="collapse" for="c-40730575">[-]</label><label class="expand" for="c-40730575">[1 more]</label></div><br/><div class="children"><div class="content">Unless you know something I don’t, that’s not the case. It also makes sense, engineers are far more portable and scarcity isn’t an issue (many ML PhDs find engineering positions).</div><br/></div></div></div></div><div id="40732273" class="c"><input type="checkbox" id="c-40732273" checked=""/><div class="controls bullet"><span class="by">dbish</span><span>|</span><a href="#40730300">root</a><span>|</span><a href="#40730367">parent</a><span>|</span><a href="#40730435">prev</a><span>|</span><a href="#40731486">next</a><span>|</span><label class="collapse" for="c-40732273">[-]</label><label class="expand" for="c-40732273">[1 more]</label></div><br/><div class="children"><div class="content">That is incredibly untrue and has been for years in the AI&#x2F;ML space at many startups and at Amazon, Google, Facebook, etc. Good ML researchers have been making a good amount more for a while (source: I&#x27;ve hired both and been involved in leveling and pay discussions for years)</div><br/></div></div></div></div></div></div><div id="40731486" class="c"><input type="checkbox" id="c-40731486" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40730300">prev</a><span>|</span><a href="#40736343">next</a><span>|</span><label class="collapse" for="c-40731486">[-]</label><label class="expand" for="c-40731486">[33 more]</label></div><br/><div class="children"><div class="content">As others have pointed out, it&#x27;s the business incentives that create unsafe AI, and this doesn&#x27;t solve that. Social media recommendation algorithms are already incredibly unsafe for society and young people (girls in particular [1]).<p>When negative externalities exist, government should create regulation that appropriately accounts for that cost.<p>I understand there&#x27;s a bit of a paradigm shift and new attack vectors with LLMs etc. but the premise is the same imo.<p>[1] <a href="https:&#x2F;&#x2F;nypost.com&#x2F;2024&#x2F;06&#x2F;16&#x2F;us-news&#x2F;preteen-instagram-influencers-followers-are-92-grown-men&#x2F;" rel="nofollow">https:&#x2F;&#x2F;nypost.com&#x2F;2024&#x2F;06&#x2F;16&#x2F;us-news&#x2F;preteen-instagram-infl...</a></div><br/><div id="40732633" class="c"><input type="checkbox" id="c-40732633" checked=""/><div class="controls bullet"><span class="by">roywiggins</span><span>|</span><a href="#40731486">parent</a><span>|</span><a href="#40731789">next</a><span>|</span><label class="collapse" for="c-40732633">[-]</label><label class="expand" for="c-40732633">[6 more]</label></div><br/><div class="children"><div class="content">Even without business incentives, the military advantages of AI would inventivize governments to develop it anyway, like they did with nuclear weapons. Nuclear weapons are <i>inherently</i> unsafe, there are some safeguards around them, but they are ultimately dangerous weapons.</div><br/><div id="40736453" class="c"><input type="checkbox" id="c-40736453" checked=""/><div class="controls bullet"><span class="by">sneak</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40732633">parent</a><span>|</span><a href="#40733012">next</a><span>|</span><label class="collapse" for="c-40736453">[-]</label><label class="expand" for="c-40736453">[1 more]</label></div><br/><div class="children"><div class="content">Dangerous weapons are not inherently unsafe.<p>Take a Glock, for example.  It is a deadly weapon, designed for one thing and one thing alone.  It is, however, one of the safest machines ever built.</div><br/></div></div><div id="40733012" class="c"><input type="checkbox" id="c-40733012" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40732633">parent</a><span>|</span><a href="#40736453">prev</a><span>|</span><a href="#40734456">next</a><span>|</span><label class="collapse" for="c-40733012">[-]</label><label class="expand" for="c-40733012">[3 more]</label></div><br/><div class="children"><div class="content">If someone really wanted to use nukes, they would have been used by now. What has protected us is not technology (in the aftermath of the USSR it wasn&#x27;t that difficult to steal a nuke), but rather lack of incentives. A bad actor doesn&#x27;t have much to gain by detonating a nuke (unless they&#x27;re deranged and want to see people die for the pleasure of it). OK, you could use it as blackmail, which North Korea essentially tried, but that only got them so far. Whereas a super AI could potentially be used for great personal gain, i.e., to gain extreme wealth and power.<p>So there&#x27;s much greater chance of misuse of a &quot;Super AI&quot; than nuclear weapons.</div><br/><div id="40733183" class="c"><input type="checkbox" id="c-40733183" checked=""/><div class="controls bullet"><span class="by">roywiggins</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733012">parent</a><span>|</span><a href="#40733120">next</a><span>|</span><label class="collapse" for="c-40733183">[-]</label><label class="expand" for="c-40733183">[1 more]</label></div><br/><div class="children"><div class="content">Sure, that just makes the military incentives to develop such a thing even stronger. All I mean is that business incentives don&#x27;t really come into it, as long as there is competition, someone&#x27;s going to want to build weapons to gain advantage, whether it&#x27;s a business or a government.</div><br/></div></div></div></div><div id="40734456" class="c"><input type="checkbox" id="c-40734456" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40732633">parent</a><span>|</span><a href="#40733012">prev</a><span>|</span><a href="#40731789">next</a><span>|</span><label class="collapse" for="c-40734456">[-]</label><label class="expand" for="c-40734456">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think we should be stopping things from being developed, we just need to acknowledge that externalities exist.</div><br/></div></div></div></div><div id="40731789" class="c"><input type="checkbox" id="c-40731789" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#40731486">parent</a><span>|</span><a href="#40732633">prev</a><span>|</span><a href="#40734990">next</a><span>|</span><label class="collapse" for="c-40731789">[-]</label><label class="expand" for="c-40731789">[8 more]</label></div><br/><div class="children"><div class="content">&gt;  for society and young people (girls in particular [1]).<p>I don&#x27;t think the article with a single focused example bears that out at all.<p>From the article:<p>&gt; &quot;Even more troubling are the men who signed up for paid subscriptions after the girl launched a program for super-fans receive special photos and other content.&quot;<p>&gt; &quot;Her mom conceded that those followers are “probably the scariest ones of all.”&quot;<p>I&#x27;m sorry..  but what is your daughter selling,  exactly?  And why is social media responsible for this outcome?  And how is this &quot;unsafe for society?&quot;<p>This just sounds like horrific profit motivated parenting enabled by social media.</div><br/><div id="40734476" class="c"><input type="checkbox" id="c-40734476" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731789">parent</a><span>|</span><a href="#40733322">next</a><span>|</span><label class="collapse" for="c-40734476">[-]</label><label class="expand" for="c-40734476">[1 more]</label></div><br/><div class="children"><div class="content">One example is all I need although I know there would be more to find if I had the time.<p>One is enough because the fact is, this just simply shouldn’t be possible. Not only does Meta allow it though, it produces a system that incentivises it.</div><br/></div></div><div id="40733417" class="c"><input type="checkbox" id="c-40733417" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731789">parent</a><span>|</span><a href="#40733322">prev</a><span>|</span><a href="#40734990">next</a><span>|</span><label class="collapse" for="c-40733417">[-]</label><label class="expand" for="c-40733417">[5 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m sorry.. but what is your daughter selling, exactly?<p>Did.. did you just post “I’d have to see these images of the preteen girl before I could form an opinion about whether or not the men buying and sharing them were creeps” as a rebuttal to an article about social media enabling child predators?</div><br/><div id="40733815" class="c"><input type="checkbox" id="c-40733815" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733417">parent</a><span>|</span><a href="#40733804">next</a><span>|</span><label class="collapse" for="c-40733815">[-]</label><label class="expand" for="c-40733815">[2 more]</label></div><br/><div class="children"><div class="content">Well,  I already have an opinion on them.  The article seems to purposefully avoid describing what the market and the service here is.  There are a limited number of venues where a preteen girl can credibly earn money on social media.  Also,  if she&#x27;s earning that money,  I&#x27;d openly wonder whether a Coogan Account is appropriate and if one exists here.<p>Anyways..  my shock was really over the fact that the mother is basically saying &quot;I want to sell access to my daughter online but I&#x27;m surprised that the biggest spenders are adult men with questionable intentions.&quot;  Did she genuinely believe that the target market was other 12 year old girls willing to pay to watch another 12 year old girl?  The parents resignation over the situation in deference to the money is also disgusting.<p>&gt; about social media enabling child predators?<p>That&#x27;s my point.  The article entirely fails to do that.  It&#x27;s one case with a questionable background and zero investigation over the claim,  which you&#x27;d expect,  because the crime statistics show the exact opposite.</div><br/><div id="40736049" class="c"><input type="checkbox" id="c-40736049" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733815">parent</a><span>|</span><a href="#40733804">next</a><span>|</span><label class="collapse" for="c-40736049">[-]</label><label class="expand" for="c-40736049">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Well, I already have an opinion on them.<p>That was the impression I got from your previous post.<p>&gt; There are a limited number of venues where a preteen girl can credibly earn money on social media.<p>… ???<p>&gt; my shock was really over the fact that the mother is basically saying &quot;I want to sell access to my daughter online but I&#x27;m surprised that the biggest spenders are adult men with questionable intentions.&quot;<p>This article is <i>about how Instagram enables [sometimes paid!] access to children</i>. It is good that we both agree that that is what happened here, on Instagram, in this case. You also agree that the people buying access to this child, in this case, on Instagram, are adult men.<p>Somehow you have an issue with the mother and the child doing something but in the same breath say<p>&gt; The article entirely fails to do that. [“That” being enabling child predators]<p>If Instagram didn’t facilitate access to child predators then… what happened here?<p>And finally (this is a question for literally any person reading this other than akira2501) how did you read this article and arrive at “I should post that I would want to see the pictures before I render judgment about the platform”? If it is uncharitable to notice that that’s a weird post, how do you interpret that specific point?</div><br/></div></div></div></div><div id="40733804" class="c"><input type="checkbox" id="c-40733804" checked=""/><div class="controls bullet"><span class="by">jpk</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733417">parent</a><span>|</span><a href="#40733815">prev</a><span>|</span><a href="#40734990">next</a><span>|</span><label class="collapse" for="c-40733804">[-]</label><label class="expand" for="c-40733804">[2 more]</label></div><br/><div class="children"><div class="content">Since nobody bothered to explain their downvotes, I&#x27;ll say it: This is a pretty uncharitable reading of the quoted comment.</div><br/><div id="40734803" class="c"><input type="checkbox" id="c-40734803" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733804">parent</a><span>|</span><a href="#40734990">next</a><span>|</span><label class="collapse" for="c-40734803">[-]</label><label class="expand" for="c-40734803">[1 more]</label></div><br/><div class="children"><div class="content">This is a good point. When you read this:<p>&gt; Instagram’s algorithms have steered men with deviant sexual attraction to children to the girl’s page, flooding it with unwanted comments and come-ons, according the Journal.<p>It is a normal response to think “I wonder what was so attractive about the images that the child made that these men were forced to follow and comment on them. I will post about how we shouldn’t judge the men or platform until we’ve seen the images”</div><br/></div></div></div></div></div></div></div></div><div id="40734990" class="c"><input type="checkbox" id="c-40734990" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#40731486">parent</a><span>|</span><a href="#40731789">prev</a><span>|</span><a href="#40731668">next</a><span>|</span><label class="collapse" for="c-40734990">[-]</label><label class="expand" for="c-40734990">[1 more]</label></div><br/><div class="children"><div class="content">I mean, surely share holders would want a safe superintelligence rather than unsafe ones, right?<p>I&#x27;d imagine everyone being dead would be pretty detrimental to the economy.</div><br/></div></div><div id="40731668" class="c"><input type="checkbox" id="c-40731668" checked=""/><div class="controls bullet"><span class="by">ToucanLoucan</span><span>|</span><a href="#40731486">parent</a><span>|</span><a href="#40734990">prev</a><span>|</span><a href="#40736343">next</a><span>|</span><label class="collapse" for="c-40731668">[-]</label><label class="expand" for="c-40731668">[17 more]</label></div><br/><div class="children"><div class="content">I mean if the last 20 years is to be taken as evidence, it seems big tech is more than happy to shotgun unproven and unstudied technology straight into the brains of our most vulnerable populations and just see what the fuck happens. Results so far include a lot of benign nothing but also a whole lot of eating disorders, maxed out parents credit cards, attention issues, rampant misogyny among young boys, etc. Which, granted, the readiness to fuck with populations at scale and do immeasurable harm doesn&#x27;t really make tech unique as an industry, just more of the same really.<p>But you know, we&#x27;ll feed people into any kind of meat grinder we can build as long as the line goes up.</div><br/><div id="40731716" class="c"><input type="checkbox" id="c-40731716" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731668">parent</a><span>|</span><a href="#40731779">next</a><span>|</span><label class="collapse" for="c-40731716">[-]</label><label class="expand" for="c-40731716">[2 more]</label></div><br/><div class="children"><div class="content">i am very skeptical of narratives saying that young boys or men are more misogynistic than in the past. we have a cognitive bias towards thinking the past is better than it was, but specifically on gender issues i just do not buy a regression</div><br/><div id="40731747" class="c"><input type="checkbox" id="c-40731747" checked=""/><div class="controls bullet"><span class="by">ToucanLoucan</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731716">parent</a><span>|</span><a href="#40731779">next</a><span>|</span><label class="collapse" for="c-40731747">[-]</label><label class="expand" for="c-40731747">[1 more]</label></div><br/><div class="children"><div class="content">I mean, I don&#x27;t know if it&#x27;s better or worse than it was. I do know that it&#x27;s bad, thanks to tons of studies on the subject covering a wide range of little kids who watch shitheads like Andrew Tate, Fresh &amp; Fit, etc. Most grow out of it, but speaking as someone who did, I would be a much better and happier person today if I was never exposed to that garbage in the first place, and it&#x27;s resulted in stunted social skills I am <i>still</i> unwinding from in my thirties.<p>This shit isn&#x27;t funny, it&#x27;s mental poison and massive social media networks make BANK shoving it front of young men who don&#x27;t understand how bad it is until it&#x27;s WAY too late. I know we can&#x27;t eliminate every kind of shithead from society, that&#x27;s simply not possible. But I would happily settle for a strong second-place achievement if we could not have companies making massive profits off of destroying people&#x27;s minds.</div><br/></div></div></div></div><div id="40731779" class="c"><input type="checkbox" id="c-40731779" checked=""/><div class="controls bullet"><span class="by">wyager</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731668">parent</a><span>|</span><a href="#40731716">prev</a><span>|</span><a href="#40732172">next</a><span>|</span><label class="collapse" for="c-40731779">[-]</label><label class="expand" for="c-40731779">[11 more]</label></div><br/><div class="children"><div class="content">Blaming the internet for misogyny is kind of bizarre, given that current levels of misogyny are within a couple points of all-time historical lows. The internet was invented ~40 years ago. Women started getting vote ~100 years ago. Do you think the internet has returned us to pre-women&#x27;s-suffrage levels of misogyny?</div><br/><div id="40732173" class="c"><input type="checkbox" id="c-40732173" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731779">parent</a><span>|</span><a href="#40732267">next</a><span>|</span><label class="collapse" for="c-40732173">[-]</label><label class="expand" for="c-40732173">[2 more]</label></div><br/><div class="children"><div class="content">Do you believe that no subfactor can ever have a sign opposite of the factor of which it is a component?</div><br/><div id="40733435" class="c"><input type="checkbox" id="c-40733435" checked=""/><div class="controls bullet"><span class="by">SpicyLemonZest</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40732173">parent</a><span>|</span><a href="#40732267">next</a><span>|</span><label class="collapse" for="c-40733435">[-]</label><label class="expand" for="c-40733435">[1 more]</label></div><br/><div class="children"><div class="content">In general it can. In this specific case, I really struggle to see even a single dimension in which young boys are more misogynistic now than they were 2 decades ago. The original comment mentions Andrew Tate - in the early 2000s there was an entire genre of Andrew Tates called &quot;pickup artists&quot;.</div><br/></div></div></div></div><div id="40732267" class="c"><input type="checkbox" id="c-40732267" checked=""/><div class="controls bullet"><span class="by">ToucanLoucan</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731779">parent</a><span>|</span><a href="#40732173">prev</a><span>|</span><a href="#40732172">next</a><span>|</span><label class="collapse" for="c-40732267">[-]</label><label class="expand" for="c-40732267">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Do you think the internet has returned us to pre-women&#x27;s-suffrage levels of misogyny?<p>Well in the States at least we did just revoke a sizable amount of their bodily autonomy so, the situation may not be <i>that bad, yet,</i> but I wouldn&#x27;t call it good by any measurement. Any my objection isn&#x27;t &quot;that sexism exists in society,&quot; that is probably going to be true as a statement until the sun explodes, and possibly after that if we actually nail down space travel as a technology and get off this particular rock. My issue is massive corporations making billions of dollars facilitating men who want to spread sexist ideas, and paying them for the pleasure. That&#x27;s what I have an issue with.<p>Be whatever kind of asshole you see fit to be, the purity of your soul is no one&#x27;s concern but yours, and if you have one, whatever god you worship. I just don&#x27;t want you being paid for it, and I feel that&#x27;s a reasonable line to draw.</div><br/><div id="40733223" class="c"><input type="checkbox" id="c-40733223" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40732267">parent</a><span>|</span><a href="#40732172">next</a><span>|</span><label class="collapse" for="c-40733223">[-]</label><label class="expand" for="c-40733223">[7 more]</label></div><br/><div class="children"><div class="content">I am firmly in favor of abortion rights but still I do not think that is even remotely a good bellwether to measure sexism&#x2F;misogyny.<p>1. Women are more likely than men to be opposed to abortion rights.
2. Many people who are opposed to abortion rights have legitimately held moral concerns that are not simply because they have no respect for women&#x27;s rights. 
3. Roe v. Wade was the decision of 9 people. It absolutely did not reflect public opinion at the time - nothing even close to as expansive would possibly have passed in a referendum in 1974. Compare that to now, where multiple states that are <i>known</i> abortion holdouts have repealed abortion restrictions in referenda - and it is obvious that people are moving to the left on this issue compared to where we were in 1974.<p>Social media facilitates communication. As long as there is sexism and freedom of communication, there will be people making money off of facilitating sexist communication because there will be people making money off of facilitating communication writ large. It&#x27;s like blaming a toll highway for facilitating someone trafficking drugs. They are also making money off of facilitating anti-sexist communication - and the world as a whole is becoming less sexist, partially in my view due to the spread of views facilitated by the internet.</div><br/><div id="40733715" class="c"><input type="checkbox" id="c-40733715" checked=""/><div class="controls bullet"><span class="by">bnralt</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733223">parent</a><span>|</span><a href="#40733307">next</a><span>|</span><label class="collapse" for="c-40733715">[-]</label><label class="expand" for="c-40733715">[1 more]</label></div><br/><div class="children"><div class="content">Right. We can see something similar with the Terry Schiavo case or the opposition to IFV right now. It&#x27;s clear that this is about different opinions regarding what should be considered a living human being (which doesn&#x27;t seem to have a very clean definition for anyone, it should be noted). Depending on where you draw the line, it&#x27;s either horrible to outlaw abortions or horrible to allow them.<p>Framing it as simply taking away a woman&#x27;s bodily autonomy is like framing circumcision as simply being about mutilating men.</div><br/></div></div><div id="40733307" class="c"><input type="checkbox" id="c-40733307" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733223">parent</a><span>|</span><a href="#40733715">prev</a><span>|</span><a href="#40733609">next</a><span>|</span><label class="collapse" for="c-40733307">[-]</label><label class="expand" for="c-40733307">[3 more]</label></div><br/><div class="children"><div class="content">Agreed it is a poor bell whether.<p>But:<p>&gt; Women are more likely than men to be opposed to abortion rights<p>Where is that coming from? Pew (<a href="https:&#x2F;&#x2F;www.pewresearch.org&#x2F;religion&#x2F;fact-sheet&#x2F;public-opinion-on-abortion&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pewresearch.org&#x2F;religion&#x2F;fact-sheet&#x2F;public-opini...</a>) has a 5 point higher opposition rate by men.</div><br/><div id="40733345" class="c"><input type="checkbox" id="c-40733345" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733307">parent</a><span>|</span><a href="#40733609">next</a><span>|</span><label class="collapse" for="c-40733345">[-]</label><label class="expand" for="c-40733345">[2 more]</label></div><br/><div class="children"><div class="content">Generally it seems from polls I had seen that the % of women favoring illegality is higher than men [0][1]. But it seems that there has been a considerable shift in women&#x27;s opinions post-pandemic which perhaps tracks in the US with broader gender partisan shifts (men &amp; women becoming politically delinked, perhaps due to algo social media&#x2F;tiktok) as well as  Dobbs being a wakeup call for some women.<p>[0]: <a href="https:&#x2F;&#x2F;news.gallup.com&#x2F;poll&#x2F;245618&#x2F;abortion-trends-gender.aspx" rel="nofollow">https:&#x2F;&#x2F;news.gallup.com&#x2F;poll&#x2F;245618&#x2F;abortion-trends-gender.a...</a>
[1]: <a href="https:&#x2F;&#x2F;d25d2506sfb94s.cloudfront.net&#x2F;cumulus_uploads&#x2F;document&#x2F;a0c0uf8c2g&#x2F;YouGov-Survey-University-of-Lancaster-Results-130130.pdf" rel="nofollow">https:&#x2F;&#x2F;d25d2506sfb94s.cloudfront.net&#x2F;cumulus_uploads&#x2F;docume...</a> (older but it was a poll i remembered)</div><br/><div id="40733516" class="c"><input type="checkbox" id="c-40733516" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733345">parent</a><span>|</span><a href="#40733609">next</a><span>|</span><label class="collapse" for="c-40733516">[-]</label><label class="expand" for="c-40733516">[1 more]</label></div><br/><div class="children"><div class="content">Your second poll is from the UK. :)<p>Interesting find on Gallup. My read is that the genders are broadly similar, but women have more variance&#x2F;more nuanced opinions, presumably from thinking about this more.<p>Yglesias has a good overview: <a href="https:&#x2F;&#x2F;www.vox.com&#x2F;2019&#x2F;5&#x2F;20&#x2F;18629644&#x2F;abortion-gender-gap-public-opinion" rel="nofollow">https:&#x2F;&#x2F;www.vox.com&#x2F;2019&#x2F;5&#x2F;20&#x2F;18629644&#x2F;abortion-gender-gap-p...</a></div><br/></div></div></div></div></div></div><div id="40733609" class="c"><input type="checkbox" id="c-40733609" checked=""/><div class="controls bullet"><span class="by">ToucanLoucan</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733223">parent</a><span>|</span><a href="#40733307">prev</a><span>|</span><a href="#40732172">next</a><span>|</span><label class="collapse" for="c-40733609">[-]</label><label class="expand" for="c-40733609">[2 more]</label></div><br/><div class="children"><div class="content">&gt; 2. Many people who are opposed to abortion rights have legitimately held moral concerns that are not simply because they have no respect for women&#x27;s rights.<p>Then they are free to not get an abortion. I don&#x27;t get an abortion every day, it&#x27;s pretty easy to accomplish. They do not get to use the letter of law to interfere in other people&#x27;s medical decisions and care, and they most definitely should not have the ability to use the letter of the law to get unhealthy women killed to suit their precious morals.<p>Like, genuinely, if you are near a woman who is having a serious medical condition where her baby is killing her, there is no, and I repeat, NO version of that where letting an adult, alive, otherwise viable person die in the hopes that the clump of cells killing her might make it. That does not make sense under any moral system at all. We don&#x27;t even take organs from recently deceased people without their permission before they croak, and some people think they have the right to demand someone lay down their entire actual life in the hope a baby MIGHT be born and live? Fuck that. Stupid.<p>&gt; 3. Roe v. Wade was the decision of 9 people.<p>Sod public opinion. The public is wrong all the goddamn time. One would argue they&#x27;re wrong more often than they aren&#x27;t, and the more of em there are, and the louder they are, the more likely they&#x27;re fucking wrong.<p>&gt; Social media facilitates communication. As long as there is sexism and freedom of communication, there will be people making money off of facilitating sexist communication because there will be people making money off of facilitating communication writ large.<p>This is such a defeatist attitude. There will also always be revenge porn, child abuse material, beheading videos and people putting monkeys in blenders. Do we allow that everywhere too then? Since we cannot guarantee total blackout on objectionable content, we just wild west it? Fucking nonsense. We decide constantly by way of moderation on every service and website that exists what is permitted, and what is not, and there is no reason at all that those same things cannot be enshrined in law, with steep penalties for services that fuck up and host it.</div><br/><div id="40733680" class="c"><input type="checkbox" id="c-40733680" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40733609">parent</a><span>|</span><a href="#40732172">next</a><span>|</span><label class="collapse" for="c-40733680">[-]</label><label class="expand" for="c-40733680">[1 more]</label></div><br/><div class="children"><div class="content">it appears you want to debate the abortion issue on the merits when i’ve already said i agree with you. my point about public opinion was only in reference to public opinion being a gauge of general sexist attitudes and the degree to which being anti-abortion is intrinsically out of sexist motivations vs other differing beliefs.<p>on your second point, much of the material you’re describing is actively illegal - which is a different case. i agree with your point around moderation but feel conflicted with my intuition that the rules impacting speech should generally be publicly known and  generally expansive. i am not sure what the reconciliation is. but i also don’t really know who andrew tate is, so can only really speak in the abstract</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40732172" class="c"><input type="checkbox" id="c-40732172" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40731668">parent</a><span>|</span><a href="#40731779">prev</a><span>|</span><a href="#40736343">next</a><span>|</span><label class="collapse" for="c-40732172">[-]</label><label class="expand" for="c-40732172">[3 more]</label></div><br/><div class="children"><div class="content">Please look up the history of maxing out credit cards, eating disorders, attention disorders, and misogyny. You seem to be under the mistaken impression that anything before your birth was the Garden of Eden and that the parade of horribles existed only because of &quot;big tech&quot;. What is next? Blaming big tech for making teenagers horny and defiant?</div><br/><div id="40732553" class="c"><input type="checkbox" id="c-40732553" checked=""/><div class="controls bullet"><span class="by">insane_dreamer</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40732172">parent</a><span>|</span><a href="#40732229">next</a><span>|</span><label class="collapse" for="c-40732553">[-]</label><label class="expand" for="c-40732553">[1 more]</label></div><br/><div class="children"><div class="content">&gt; maxing out credit cards, eating disorders, attention disorders, and misogyny<p>social media doesn&#x27;t create these, but it most definitely amplifies them</div><br/></div></div><div id="40732229" class="c"><input type="checkbox" id="c-40732229" checked=""/><div class="controls bullet"><span class="by">ToucanLoucan</span><span>|</span><a href="#40731486">root</a><span>|</span><a href="#40732172">parent</a><span>|</span><a href="#40732553">prev</a><span>|</span><a href="#40736343">next</a><span>|</span><label class="collapse" for="c-40732229">[-]</label><label class="expand" for="c-40732229">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You seem to be under the mistaken impression that anything before your birth was the Garden of Eden and that the parade of horribles existed only because of &quot;big tech&quot;<p>Please point out where I said that. Because what I wrote was:<p>&gt; I mean if the last 20 years is to be taken as evidence, it seems big tech is more than happy to shotgun unproven and unstudied technology straight into the brains of our most vulnerable populations and just see what the fuck happens. Results so far include a lot of benign nothing but also a whole lot of eating disorders, maxed out parents credit cards, attention issues, rampant misogyny among young boys, etc. Which, granted, the readiness to fuck with populations at scale and do immeasurable harm doesn&#x27;t really make tech unique as an industry, just more of the same really.<p>Which not only is not romanticizing the past, in fact I directly point out that making tons of people&#x27;s lives worse for profit was a thing in industry long before tech came along, but also do not directly implicate tech as creating sexism, exploiting people financially, or fucking up young women&#x27;s brains any differently, simply doing it more. Like most things with tech, it wasn&#x27;t revolutionary new social harms, it was just social harms delivered algorithmically, to the most vulnerable, and highly personalized to what they are acutely vulnerable to in specific.<p>That is not a <i>new thing,</i> by any means, it&#x27;s simply better targeted and more profitable, which is great innovation providing you lack a conscience and see people as only a resource to be exploited for your own profit, which a lot of the tech sector seems to.</div><br/></div></div></div></div></div></div></div></div><div id="40736343" class="c"><input type="checkbox" id="c-40736343" checked=""/><div class="controls bullet"><span class="by">stevenjgarner</span><span>|</span><a href="#40731486">prev</a><span>|</span><a href="#40734312">next</a><span>|</span><label class="collapse" for="c-40736343">[-]</label><label class="expand" for="c-40736343">[1 more]</label></div><br/><div class="children"><div class="content">While we have a strong grasp of the fundamental algorithms and architectures that power generative LLMs, there are many nuances, emergent behaviors and deeper technical details about their behavior, generalization, and internal decision-making processes that we do not fully understand.<p>How can we pursue the goal of &quot;Safe Superintelligence&quot; when we do not understand what is actually going on?</div><br/></div></div><div id="40734312" class="c"><input type="checkbox" id="c-40734312" checked=""/><div class="controls bullet"><span class="by">rqtwteye</span><span>|</span><a href="#40736343">prev</a><span>|</span><a href="#40731189">next</a><span>|</span><label class="collapse" for="c-40734312">[-]</label><label class="expand" for="c-40734312">[10 more]</label></div><br/><div class="children"><div class="content">I always wonder about &quot;safe&quot; for who? If the current economic system continues, we may end up with a lot of people out of jobs. Add to that improvements in robotics  and we will have many people ending up having nothing to contribute to the economy. I am not getting the impression that the people who push for AI safety are thinking about this. It seems they are most worried about not losing their position of privilege.</div><br/><div id="40734545" class="c"><input type="checkbox" id="c-40734545" checked=""/><div class="controls bullet"><span class="by">comfysocks</span><span>|</span><a href="#40734312">parent</a><span>|</span><a href="#40734426">next</a><span>|</span><label class="collapse" for="c-40734545">[-]</label><label class="expand" for="c-40734545">[3 more]</label></div><br/><div class="children"><div class="content">The Industrial Revolution devalued manual labor.  Sure, new jobs were created, but on the whole this looked like a shift to knowledge work and away from manual labor.<p>Now AI is beginning to devalue knowledge work.  Although the limits of current technology is obvious in many cases, AI is already doing a pretty good job at replacing illustrators and copy writers.  It will only get better.<p>Who owns the value created by AI productivity?  Ultimately it will be shareholders and VCs.  It’s no surprise that the loudest voices in techno-optimism are VCs.  In this new world they win.<p>Having said all this, I think Ilya’s concerns are more of the existential type.</div><br/><div id="40734862" class="c"><input type="checkbox" id="c-40734862" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40734312">root</a><span>|</span><a href="#40734545">parent</a><span>|</span><a href="#40734675">next</a><span>|</span><label class="collapse" for="c-40734862">[-]</label><label class="expand" for="c-40734862">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The Industrial Revolution devalued manual labor.<p>Only some types.  It caused a great number of people to be employed in manual labour, in the new factories.  The shift to knowledge work came much later as factory work (and farming) became increasingly automated.</div><br/></div></div><div id="40734675" class="c"><input type="checkbox" id="c-40734675" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#40734312">root</a><span>|</span><a href="#40734545">parent</a><span>|</span><a href="#40734862">prev</a><span>|</span><a href="#40734426">next</a><span>|</span><label class="collapse" for="c-40734675">[-]</label><label class="expand" for="c-40734675">[1 more]</label></div><br/><div class="children"><div class="content">&gt;In this new world they win.<p>If history is any indication not really. There&#x27;s an obvious dialectical nature to this where technological advance initially delivers returns to its benefactors, but then they usually end up being swallowed by their own creation. The industrial revolution didn&#x27;t devalue labor, it empowered labor to act collectively for the first time, laying the groundwork for what ultimately replaced the pre-industrial powers that were.</div><br/></div></div></div></div><div id="40734426" class="c"><input type="checkbox" id="c-40734426" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#40734312">parent</a><span>|</span><a href="#40734545">prev</a><span>|</span><a href="#40734422">next</a><span>|</span><label class="collapse" for="c-40734426">[-]</label><label class="expand" for="c-40734426">[1 more]</label></div><br/><div class="children"><div class="content">AIUI it with superalignment, it merely means &quot;the AI does what the humans instructing it want it to do&quot;. It&#x27;s a different kind of safety than the built in censoring that most LLMs have.</div><br/></div></div><div id="40734422" class="c"><input type="checkbox" id="c-40734422" checked=""/><div class="controls bullet"><span class="by">throwaway4good</span><span>|</span><a href="#40734312">parent</a><span>|</span><a href="#40734426">prev</a><span>|</span><a href="#40731189">next</a><span>|</span><label class="collapse" for="c-40734422">[-]</label><label class="expand" for="c-40734422">[5 more]</label></div><br/><div class="children"><div class="content">It is a strange thing. We are not anywhere near “super intelligence” yet the priority is safety.<p>If the Wright brothers had focused on safety, I am not sure they would have flown very far.</div><br/><div id="40734825" class="c"><input type="checkbox" id="c-40734825" checked=""/><div class="controls bullet"><span class="by">botanical</span><span>|</span><a href="#40734312">root</a><span>|</span><a href="#40734422">parent</a><span>|</span><a href="#40734482">next</a><span>|</span><label class="collapse" for="c-40734825">[-]</label><label class="expand" for="c-40734825">[3 more]</label></div><br/><div class="children"><div class="content">The consequences of actual AI is far reaching compared to a plane crash.</div><br/><div id="40735526" class="c"><input type="checkbox" id="c-40735526" checked=""/><div class="controls bullet"><span class="by">throwaway4good</span><span>|</span><a href="#40734312">root</a><span>|</span><a href="#40734825">parent</a><span>|</span><a href="#40734482">next</a><span>|</span><label class="collapse" for="c-40735526">[-]</label><label class="expand" for="c-40735526">[2 more]</label></div><br/><div class="children"><div class="content">No they are not.<p>Not unless you connect a machine gun to your neural net.<p>Otherwise - we are talking about a chat bot - yes if there is no safety - it will say something racist - or implement a virus for you that you would have had to search 4chan for or something.<p>None of this is any more dangerous than what you can find on the far corners of the internet.<p>And it will not bring the end of the world.</div><br/><div id="40736460" class="c"><input type="checkbox" id="c-40736460" checked=""/><div class="controls bullet"><span class="by">sneak</span><span>|</span><a href="#40734312">root</a><span>|</span><a href="#40735526">parent</a><span>|</span><a href="#40734482">next</a><span>|</span><label class="collapse" for="c-40736460">[-]</label><label class="expand" for="c-40736460">[1 more]</label></div><br/><div class="children"><div class="content">If it is sufficiently intelligent, then it will be able to arrange the hooking up of machine guns just via chat.  You seem to fail to note that superintelligence has an inherent value prop.  It can use its better understanding of the world to generate value in a near vacuum, and use that value to bribe, coerce, blackmail, and otherwise manipulate whatever humans can read its output.<p>Imagine a chatbot that can reward people with becoming a billionaire via the stock market if you just do what it says.  Imagine a chatbot that can pay your colleagues in solved bitcoin block hashes to kill your children if you don’t do what it says to connect it to more systems.<p>There is a great A24 movie called
“Ex Machina” that explores this concept.<p>Superintelligence is inherently dangerous to the human status quo.  It may be impossible to develop it “safely” in the sense that most people mean by that.  It may also be impossible to avoid developing it.  This might be the singularity everyone’s been talking about, just without the outcome that everyone hoped for.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40731189" class="c"><input type="checkbox" id="c-40731189" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#40734312">prev</a><span>|</span><a href="#40730392">next</a><span>|</span><label class="collapse" for="c-40731189">[-]</label><label class="expand" for="c-40731189">[72 more]</label></div><br/><div class="children"><div class="content">Ilya&#x27;s issue isn&#x27;t developing a Safe AI. Its developing a Safe Business. You can make a safe AI today, but what happens when the next person is managing things? Are they so kindhearted, or are they cold and calculated like the management of many harmful industries today? If you solve the issue of Safe Business and eliminate the incentive structures that lead to &#x27;unsafe&#x27; business, you basically obviate a lot of the societal harm that exists today. Short of solving this issue, I don&#x27;t think you can ever confidently say you will create a safe AI and that also makes me not trust your claims because they must be born from either ignorance or malice.</div><br/><div id="40731821" class="c"><input type="checkbox" id="c-40731821" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731409">next</a><span>|</span><label class="collapse" for="c-40731821">[-]</label><label class="expand" for="c-40731821">[15 more]</label></div><br/><div class="children"><div class="content">&gt; You can make a safe AI today, but what happens when the next person is managing things?<p>The point of safe <i>superintelligence</i>, and presumably the goal of SSI Inc., is that <i>there won&#x27;t be</i> a next (biological) person managing things afterwards. At least none who could do anything to build a competing unsafe SAI. We&#x27;re not talking about the banal definition of &quot;safety&quot; here. If the first superintelligence has any reasonable goal system, its first plan of action is almost inevitably going to be to start self-improving fast enough to attain a decisive head start against any potential competitors.</div><br/><div id="40732821" class="c"><input type="checkbox" id="c-40732821" checked=""/><div class="controls bullet"><span class="by">jen729w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731821">parent</a><span>|</span><a href="#40732205">next</a><span>|</span><label class="collapse" for="c-40732821">[-]</label><label class="expand" for="c-40732821">[13 more]</label></div><br/><div class="children"><div class="content">I wonder how many people panicking about these things have ever visited a data centre.<p>They have big red buttons at the end of every pod. Shuts everything down.<p>They have bigger red buttons at the end of every power unit. Shuts everything down.<p>And down at the city, there’s a big red button at the biggest power unit. Shuts everything down.<p>Having arms and legs is going to be a significant benefit for some time yet. I am not in the least concerned about becoming a paperclip.</div><br/><div id="40733137" class="c"><input type="checkbox" id="c-40733137" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40733028">next</a><span>|</span><label class="collapse" for="c-40733137">[-]</label><label class="expand" for="c-40733137">[4 more]</label></div><br/><div class="children"><div class="content">Trouble is, in practice what you would need to do might be “turn off all of Google’s datacenters”. Or perhaps the thing manages to secure compute in multiple clouds (which is what I’d do if I woke up as an entity running on a single DC with a big red power button on it).<p>The blast radius of such decisions are large enough that this option is not trivial as you suggest.</div><br/><div id="40733548" class="c"><input type="checkbox" id="c-40733548" checked=""/><div class="controls bullet"><span class="by">jen729w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40733137">parent</a><span>|</span><a href="#40733274">next</a><span>|</span><label class="collapse" for="c-40733548">[-]</label><label class="expand" for="c-40733548">[2 more]</label></div><br/><div class="children"><div class="content">Right, but when we’ve literally invented a superintelligence that we’re worried will eliminate the race,<p>a) we’ve done that, which is cool. Now let’s figure out how to control it.<p>b) you can’t get your gmail for a bit while we reboot the DC. That’s probably okay.</div><br/><div id="40733778" class="c"><input type="checkbox" id="c-40733778" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40733548">parent</a><span>|</span><a href="#40733274">next</a><span>|</span><label class="collapse" for="c-40733778">[-]</label><label class="expand" for="c-40733778">[1 more]</label></div><br/><div class="children"><div class="content">a) after you create the superintelligence is likely too late. You seem to think that inventing superintelligence means that we somehow understand what we created, but note that we have no idea how a simple LLM works, let alone an ASI that is presumably 5-10 OOM more complex. You are unlikely to be able to control a thing that is way smarter than you, the safest option is to steer the nature of that thing before it comes into being (or, don’t build it at all). Note that we currently don’t know how to do this, it’s what Ilya is working on. The approach from OpenAI is roughly to create ASI and then hope it’s friendly.<p>b) except that is not how these things go in the real world. What actually happens is that initially it’s just a risk of the agent going rogue, the CEO weighs the multi-billion dollar cost vs. some small-seeming probability of disaster and decides to keep the company running until the threat is extremely clear, which in many scenarios is too late.<p>(For a recent example, consider the point in the spread of Covid where a lockdown could have prevented the disease from spreading; likely somewhere around tens to hundreds of cases, well before the true risk was quantified, and therefore drastic action was not justified to those that could have pressed the metaphorical red button).</div><br/></div></div></div></div><div id="40733274" class="c"><input type="checkbox" id="c-40733274" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40733137">parent</a><span>|</span><a href="#40733548">prev</a><span>|</span><a href="#40733028">next</a><span>|</span><label class="collapse" for="c-40733274">[-]</label><label class="expand" for="c-40733274">[1 more]</label></div><br/><div class="children"><div class="content">Open the data center doors<p>I’m sorry I can’t do that</div><br/></div></div></div></div><div id="40733028" class="c"><input type="checkbox" id="c-40733028" checked=""/><div class="controls bullet"><span class="by">quesera</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40733137">prev</a><span>|</span><a href="#40735557">next</a><span>|</span><label class="collapse" for="c-40733028">[-]</label><label class="expand" for="c-40733028">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Having arms and legs is going to be a significant benefit for some time yet</i><p>I am also of this opinion.<p>However I also think that the magic shutdown button needs to be protected against terrorists and ne&#x27;er-do-wells, so is consequently guarded by arms and legs that belong to a power structure.<p>If the shutdown-worthy activity of the evil AI can serve the interests of the power structure preferentially, those arms and legs will also be motivated to prevent the rest of us from intervening.<p>So I don&#x27;t worry about AI at all. I do worry about humans, and if AI is an amplifier or enabler of human nature, then there is valid worry, I think.</div><br/></div></div><div id="40735557" class="c"><input type="checkbox" id="c-40735557" checked=""/><div class="controls bullet"><span class="by">dbish</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40733028">prev</a><span>|</span><a href="#40733048">next</a><span>|</span><label class="collapse" for="c-40735557">[-]</label><label class="expand" for="c-40735557">[1 more]</label></div><br/><div class="children"><div class="content">This is why I think it’s more important we give AI agents the ability to use human surrogates. Arms and legs win but can be controlled with the right incentives</div><br/></div></div><div id="40733048" class="c"><input type="checkbox" id="c-40733048" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40735557">prev</a><span>|</span><a href="#40733060">next</a><span>|</span><label class="collapse" for="c-40733048">[-]</label><label class="expand" for="c-40733048">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been more than a decade now since we first saw botnets based on stealing AWS credentials and running arbitrary code on them (e.g. for crypto mining) - once an actual AI starts duplicating itself in this manner, where&#x27;s the big red button that turns off every single cloud instance in the world?</div><br/><div id="40733106" class="c"><input type="checkbox" id="c-40733106" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40733048">parent</a><span>|</span><a href="#40733060">next</a><span>|</span><label class="collapse" for="c-40733106">[-]</label><label class="expand" for="c-40733106">[1 more]</label></div><br/><div class="children"><div class="content">This is making <i>a lot</i> of assumptions like…a super intelligence can easily clone itself…maybe such an entity would require specific hardware to run ?</div><br/></div></div></div></div><div id="40733060" class="c"><input type="checkbox" id="c-40733060" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40733048">prev</a><span>|</span><a href="#40733319">next</a><span>|</span><label class="collapse" for="c-40733060">[-]</label><label class="expand" for="c-40733060">[1 more]</label></div><br/><div class="children"><div class="content">I doubt a manual alarm switch will do much good when computers operate at the speed of light. It&#x27;s an anthropomorphism.</div><br/></div></div><div id="40733319" class="c"><input type="checkbox" id="c-40733319" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40733060">prev</a><span>|</span><a href="#40733097">next</a><span>|</span><label class="collapse" for="c-40733319">[-]</label><label class="expand" for="c-40733319">[1 more]</label></div><br/><div class="children"><div class="content">If it’s any sort of smart AI, you’d need to shut down the entire world at the same time.</div><br/></div></div><div id="40733097" class="c"><input type="checkbox" id="c-40733097" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40733319">prev</a><span>|</span><a href="#40734058">next</a><span>|</span><label class="collapse" for="c-40733097">[-]</label><label class="expand" for="c-40733097">[1 more]</label></div><br/><div class="children"><div class="content">Have you seen all of the autonomous cars, drones and robots we&#x27;ve built?</div><br/></div></div><div id="40734058" class="c"><input type="checkbox" id="c-40734058" checked=""/><div class="controls bullet"><span class="by">Tempest1981</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732821">parent</a><span>|</span><a href="#40733097">prev</a><span>|</span><a href="#40732205">next</a><span>|</span><label class="collapse" for="c-40734058">[-]</label><label class="expand" for="c-40734058">[1 more]</label></div><br/><div class="children"><div class="content">Might be running on a botnet of CoPilot PCs</div><br/></div></div></div></div><div id="40732205" class="c"><input type="checkbox" id="c-40732205" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731821">parent</a><span>|</span><a href="#40732821">prev</a><span>|</span><a href="#40731409">next</a><span>|</span><label class="collapse" for="c-40732205">[-]</label><label class="expand" for="c-40732205">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>there won&#x27;t be a next (biological) person managing things afterwards. At least none who could do anything to build a competing unsafe SAI</i><p>This pitch has Biblical&#x2F;Evangelical resonance, in case anyone wants to try that fundraising route [1]. (&quot;I&#x27;m just running things until the Good Guy takes over&quot; is almost a monarchic trope.)<p>[1] <a href="https:&#x2F;&#x2F;biblehub.com&#x2F;1_corinthians&#x2F;15-24.htm" rel="nofollow">https:&#x2F;&#x2F;biblehub.com&#x2F;1_corinthians&#x2F;15-24.htm</a></div><br/></div></div></div></div><div id="40731409" class="c"><input type="checkbox" id="c-40731409" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731821">prev</a><span>|</span><a href="#40731808">next</a><span>|</span><label class="collapse" for="c-40731409">[-]</label><label class="expand" for="c-40731409">[10 more]</label></div><br/><div class="children"><div class="content">The safe business won’t hold very long if someone can gain a short term business advantage with unsafe AI. Eventually government has to step in with a legal and enforcement framework to prevent greed from ruining things.</div><br/><div id="40731992" class="c"><input type="checkbox" id="c-40731992" checked=""/><div class="controls bullet"><span class="by">nilkn</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731409">parent</a><span>|</span><a href="#40731902">next</a><span>|</span><label class="collapse" for="c-40731992">[-]</label><label class="expand" for="c-40731992">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible that safety will eventually become the business advantage, just like privacy can be a business advantage today but wasn&#x27;t taken so seriously 10-15 years ago by the general public.<p>This is not even that far-fetched. A safe AI that you can trust should be far more useful and economically valuable than an unsafe AI that you cannot trust. AI systems today aren&#x27;t powerful enough for the difference to really matter yet, because present AI systems are mostly not yet acting as fully autonomous agents having a tangible impact on the world around them.</div><br/></div></div><div id="40731902" class="c"><input type="checkbox" id="c-40731902" checked=""/><div class="controls bullet"><span class="by">__MatrixMan__</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731409">parent</a><span>|</span><a href="#40731992">prev</a><span>|</span><a href="#40732450">next</a><span>|</span><label class="collapse" for="c-40731902">[-]</label><label class="expand" for="c-40731902">[6 more]</label></div><br/><div class="children"><div class="content">Government is controlled by the highest bidder. I think we should be prepared to do this ourselves by refusing to accept money made by unsafe businesses, even if it means saying goodbye to the convenience of fungible money.</div><br/><div id="40732055" class="c"><input type="checkbox" id="c-40732055" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731902">parent</a><span>|</span><a href="#40731951">next</a><span>|</span><label class="collapse" for="c-40732055">[-]</label><label class="expand" for="c-40732055">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Government is controlled by the highest bidder.<p>While this might be true for the governments you have personally experienced, this is far from being an aphorism.</div><br/></div></div><div id="40731951" class="c"><input type="checkbox" id="c-40731951" checked=""/><div class="controls bullet"><span class="by">creato</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731902">parent</a><span>|</span><a href="#40732055">prev</a><span>|</span><a href="#40732050">next</a><span>|</span><label class="collapse" for="c-40731951">[-]</label><label class="expand" for="c-40731951">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Government doesn&#x27;t work. We just need to make a new government that is much more effective and far reaching in controlling people&#x27;s behavior.&quot;</div><br/><div id="40731997" class="c"><input type="checkbox" id="c-40731997" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731951">parent</a><span>|</span><a href="#40732050">next</a><span>|</span><label class="collapse" for="c-40731997">[-]</label><label class="expand" for="c-40731997">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what they said though. Seems to me more of a libertarian ideal than making a new government.</div><br/><div id="40732121" class="c"><input type="checkbox" id="c-40732121" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731997">parent</a><span>|</span><a href="#40732050">next</a><span>|</span><label class="collapse" for="c-40732121">[-]</label><label class="expand" for="c-40732121">[1 more]</label></div><br/><div class="children"><div class="content">Reinventing government and calling it a private corporation is one of the main activities that libertarians engage in</div><br/></div></div></div></div></div></div><div id="40732050" class="c"><input type="checkbox" id="c-40732050" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731902">parent</a><span>|</span><a href="#40731951">prev</a><span>|</span><a href="#40732450">next</a><span>|</span><label class="collapse" for="c-40732050">[-]</label><label class="expand" for="c-40732050">[1 more]</label></div><br/><div class="children"><div class="content">Replace government with collective society assurance that no one cheats so we aren’t all doomed. Otherwise, someone will do it, and we all will have to bear the consequences.<p>If only enough individuals are willing to buy these services, then again we all will bear the consequences. There is no way out of this where libertarian ideals can be used to come to a safe result. What makes this even a more wicked problem is that decisions made in other countries will affect us all as well, we can’t isolate ourselves from AI policies made in China for example.</div><br/></div></div></div></div><div id="40732450" class="c"><input type="checkbox" id="c-40732450" checked=""/><div class="controls bullet"><span class="by">123yawaworht456</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731409">parent</a><span>|</span><a href="#40731902">prev</a><span>|</span><a href="#40731808">next</a><span>|</span><label class="collapse" for="c-40732450">[-]</label><label class="expand" for="c-40732450">[2 more]</label></div><br/><div class="children"><div class="content"><i>which</i> government?<p>will China obey US regoolations? will Russia?</div><br/><div id="40733219" class="c"><input type="checkbox" id="c-40733219" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732450">parent</a><span>|</span><a href="#40731808">next</a><span>|</span><label class="collapse" for="c-40733219">[-]</label><label class="expand" for="c-40733219">[1 more]</label></div><br/><div class="children"><div class="content">No, which makes this an even harder problem. Can US companies bound by one set of rules compete against Chinese ones bound by another set of rules? No, probably not. Humanity will have to come together on this, or someone will develop killer AI that kills us all.</div><br/></div></div></div></div></div></div><div id="40731808" class="c"><input type="checkbox" id="c-40731808" checked=""/><div class="controls bullet"><span class="by">cheptsov</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731409">prev</a><span>|</span><a href="#40731343">next</a><span>|</span><label class="collapse" for="c-40731808">[-]</label><label class="expand" for="c-40731808">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to see more individual researchers openly exploring AI safety from a scientific and humanitarian perspective, rather than just the technical or commercial angles.</div><br/></div></div><div id="40731343" class="c"><input type="checkbox" id="c-40731343" checked=""/><div class="controls bullet"><span class="by">lannisterstark</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731808">prev</a><span>|</span><a href="#40731728">next</a><span>|</span><label class="collapse" for="c-40731343">[-]</label><label class="expand" for="c-40731343">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Short of solving this issue<p>Solving human nature is indeed, hard.</div><br/></div></div><div id="40731728" class="c"><input type="checkbox" id="c-40731728" checked=""/><div class="controls bullet"><span class="by">mywacaday</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731343">prev</a><span>|</span><a href="#40731539">next</a><span>|</span><label class="collapse" for="c-40731728">[-]</label><label class="expand" for="c-40731728">[20 more]</label></div><br/><div class="children"><div class="content">Is safe AI really such a genie out of the bottle problem? From a non expert point of view a lot of hype just seems to be people&#x2F;groups trying to stake their claim on what will likely be a very large market.</div><br/><div id="40731879" class="c"><input type="checkbox" id="c-40731879" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731728">parent</a><span>|</span><a href="#40731539">next</a><span>|</span><label class="collapse" for="c-40731879">[-]</label><label class="expand" for="c-40731879">[19 more]</label></div><br/><div class="children"><div class="content">A human-level AI can do anything that a human can do (modulo did you put it into a robot body, but lots of different groups are already doing that with current LLMs).<p>Therefore, please imagine the most amoral, power-hungry, successful sociopath you&#x27;ve ever heard of. Doesn&#x27;t matter if you&#x27;re thinking of a famous dictator, or a religious leader, or someone who never got in the news and you had the misfortune to meet in real life — in any case, that person is&#x2F;was still a human, and a human-level AI can definitely also do all those things unless we find a way to make it not want to.<p>We don&#x27;t know how to make an AI that definitely isn&#x27;t that.<p>We also don&#x27;t know how to make an AI that definitely won&#x27;t help someone like that.</div><br/><div id="40734916" class="c"><input type="checkbox" id="c-40734916" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731879">parent</a><span>|</span><a href="#40731907">next</a><span>|</span><label class="collapse" for="c-40734916">[-]</label><label class="expand" for="c-40734916">[2 more]</label></div><br/><div class="children"><div class="content">&gt; power-hungry<p>That has nothing to do with intelligence.</div><br/><div id="40735642" class="c"><input type="checkbox" id="c-40735642" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40734916">parent</a><span>|</span><a href="#40731907">next</a><span>|</span><label class="collapse" for="c-40735642">[-]</label><label class="expand" for="c-40735642">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why it&#x27;s a problem.<p>An AI can be <i>anywhere</i> on that axis, and we don&#x27;t really know what we&#x27;re doing in order to prevent it being as I have described.</div><br/></div></div></div></div><div id="40731907" class="c"><input type="checkbox" id="c-40731907" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731879">parent</a><span>|</span><a href="#40734916">prev</a><span>|</span><a href="#40731915">next</a><span>|</span><label class="collapse" for="c-40731907">[-]</label><label class="expand" for="c-40731907">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>We also don&#x27;t know how to make an AI that definitely won&#x27;t help someone like that.</i><p>&quot;...offices in Palo Alto and Tel Aviv, where we have deep roots...&quot;<p>Hopefully, SSI holds its own.</div><br/></div></div><div id="40731915" class="c"><input type="checkbox" id="c-40731915" checked=""/><div class="controls bullet"><span class="by">zeknife</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731879">parent</a><span>|</span><a href="#40731907">prev</a><span>|</span><a href="#40731539">next</a><span>|</span><label class="collapse" for="c-40731915">[-]</label><label class="expand" for="c-40731915">[15 more]</label></div><br/><div class="children"><div class="content">Anything except tasks that require having direct control of a physical body.
Until fully functional androids are developed, there is a lot a human-level AI can&#x27;t do.</div><br/><div id="40732005" class="c"><input type="checkbox" id="c-40732005" checked=""/><div class="controls bullet"><span class="by">OtherShrezzing</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731915">parent</a><span>|</span><a href="#40731940">next</a><span>|</span><label class="collapse" for="c-40732005">[-]</label><label class="expand" for="c-40732005">[4 more]</label></div><br/><div class="children"><div class="content">I think there&#x27;s usually a difference between human-level and super-intelligent in these conversations. You can reasonably assume (some day) a superintelligence is going to<p>1) understand how to improve itself &amp; undertake novel research<p>2) understand how to deceive humans<p>3) understand how to undermine digital environments<p>If an entity with these three traits were sufficiently motivated, they could pose a material risk to humans, even without a physical body.</div><br/><div id="40732198" class="c"><input type="checkbox" id="c-40732198" checked=""/><div class="controls bullet"><span class="by">schindlabua</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732005">parent</a><span>|</span><a href="#40731940">next</a><span>|</span><label class="collapse" for="c-40732198">[-]</label><label class="expand" for="c-40732198">[3 more]</label></div><br/><div class="children"><div class="content">Deceiving a single human is pretty easy, but decieving the human super-organism is going to be hard.<p>Also, I don&#x27;t believe in a singularity event where AI improves itself to godlike power. What&#x27;s more likely is that the intelligence will plateau--I mean no software I have ever written effortlessly scaled from n=10 to n=10.000, and also humans understand how to improve themselves but they can&#x27;t go beyond a certain threshold.</div><br/><div id="40732417" class="c"><input type="checkbox" id="c-40732417" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732198">parent</a><span>|</span><a href="#40732480">next</a><span>|</span><label class="collapse" for="c-40732417">[-]</label><label class="expand" for="c-40732417">[1 more]</label></div><br/><div class="children"><div class="content">For similar reasons I don&#x27;t believe that AI will get into any interesting self-improvement cycles (occasional small boosts sure, but they won&#x27;t go all the way from being as smart as a normal AI researcher to the limits of physics in an afternoon).<p>That said, any sufficiently advanced technology is indistinguishable from magic, and the stuff we do routinely — including this conversation — would have been &quot;godlike&quot; to someone living in 1724.</div><br/></div></div><div id="40732480" class="c"><input type="checkbox" id="c-40732480" checked=""/><div class="controls bullet"><span class="by">skjoldr</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732198">parent</a><span>|</span><a href="#40732417">prev</a><span>|</span><a href="#40731940">next</a><span>|</span><label class="collapse" for="c-40732480">[-]</label><label class="expand" for="c-40732480">[1 more]</label></div><br/><div class="children"><div class="content">Humans understand how to improve themselves, but our bandwidth to ourselves and the outside world is pathetic.
AIs are untethered by sensory organs and language.</div><br/></div></div></div></div></div></div><div id="40731940" class="c"><input type="checkbox" id="c-40731940" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731915">parent</a><span>|</span><a href="#40732005">prev</a><span>|</span><a href="#40732374">next</a><span>|</span><label class="collapse" for="c-40731940">[-]</label><label class="expand" for="c-40731940">[5 more]</label></div><br/><div class="children"><div class="content">The hard part of androids is the AI, the hardware is already stronger and faster than our bones and muscles.<p>(On the optimistic side, it will be at least 5-10 years between a level 5 autonomy self-driving car and that same AI fitting into the power envelope of an android, and a human-level fully-general AI is definitely more complex than a human-level cars-only AI).</div><br/><div id="40732091" class="c"><input type="checkbox" id="c-40732091" checked=""/><div class="controls bullet"><span class="by">tony69</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731940">parent</a><span>|</span><a href="#40732396">next</a><span>|</span><label class="collapse" for="c-40732091">[-]</label><label class="expand" for="c-40732091">[2 more]</label></div><br/><div class="children"><div class="content">You might be right that the AI is more difficult, but I disagree on the androids being dangerous.<p>There are physical limitations to androids that imo make it very difficult that they could be seriously dangerous, let alone invincible, no matter how intelligent:
- power (boston dynamics battery lasts how long?), an android has to plug in at some point no matter what
- dexterity, or in general agency in real world, seems we’re still a long way from this in the context of a general purpose android<p>General purpose superhuman robot seems really really difficult.</div><br/><div id="40732171" class="c"><input type="checkbox" id="c-40732171" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732091">parent</a><span>|</span><a href="#40732396">next</a><span>|</span><label class="collapse" for="c-40732171">[-]</label><label class="expand" for="c-40732171">[1 more]</label></div><br/><div class="children"><div class="content">&gt; let alone invincible<p>!!<p>I don&#x27;t want anyone to think I meant that.<p>&gt; an android has to plug in at some point no matter what<p>Sure, and we have to eat; despite this, human actions have killed a lot of people<p>&gt; - dexterity, or in general agency in real world, seems we’re still a long way from this in the context of a general purpose android<p>Yes? The 5-10 years thing is about the gap between some AI that doesn&#x27;t exist yet (level 5 self-driving) moving from car-sized hardware to android-sized hardware; I don&#x27;t make any particular claim about when the AI will be good enough for cars (delay before the first step), and I don&#x27;t know how long it will take to go from being good at just cars to good in general (delay after the second step).</div><br/></div></div></div></div><div id="40732396" class="c"><input type="checkbox" id="c-40732396" checked=""/><div class="controls bullet"><span class="by">roughly</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731940">parent</a><span>|</span><a href="#40732091">prev</a><span>|</span><a href="#40732374">next</a><span>|</span><label class="collapse" for="c-40732396">[-]</label><label class="expand" for="c-40732396">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the hardware is already stronger and faster than our bones and muscles.<p>For 30 minutes until the batteries run down, or for 5 years until the parts wear out.</div><br/><div id="40732494" class="c"><input type="checkbox" id="c-40732494" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732396">parent</a><span>|</span><a href="#40732374">next</a><span>|</span><label class="collapse" for="c-40732494">[-]</label><label class="expand" for="c-40732494">[1 more]</label></div><br/><div class="children"><div class="content">The ATP in your cells will last about 2 seconds without replacement.<p>Electricity is also much cheaper than food, even bulk calories like vegetable oil.[0]<p>And if the android is controlled by a human-level intelligence, one thing it can very obviously do is all the stuff the humans did to make the android in the first place.<p>[0] £8.25 for 333 servings of 518 kJ - <a href="https:&#x2F;&#x2F;www.tesco.com&#x2F;groceries&#x2F;en-GB&#x2F;products&#x2F;272515844" rel="nofollow">https:&#x2F;&#x2F;www.tesco.com&#x2F;groceries&#x2F;en-GB&#x2F;products&#x2F;272515844</a><p>Equivalent to £0.17&#x2F;kWh - <a href="https:&#x2F;&#x2F;www.wolframalpha.com&#x2F;input?i=£8.25+%2F+%28333+*+518kJ%29+in+£%2FkWh" rel="nofollow">https:&#x2F;&#x2F;www.wolframalpha.com&#x2F;input?i=£8.25+%2F+%28333+*+518k...</a><p>UK average consumer price for electricity, £0.27&#x2F;kWh - <a href="https:&#x2F;&#x2F;www.greenmatch.co.uk&#x2F;average-electricity-cost-uk" rel="nofollow">https:&#x2F;&#x2F;www.greenmatch.co.uk&#x2F;average-electricity-cost-uk</a></div><br/></div></div></div></div></div></div><div id="40732374" class="c"><input type="checkbox" id="c-40732374" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731915">parent</a><span>|</span><a href="#40731940">prev</a><span>|</span><a href="#40731945">next</a><span>|</span><label class="collapse" for="c-40732374">[-]</label><label class="expand" for="c-40732374">[3 more]</label></div><br/><div class="children"><div class="content">All you need is Internet access, deepfake video synthesis, and some cryptocurrency (which can in turn be used to buy credit cards and full identities off the dark web), and you have everything you need to lie, manipulate, and bribe an endless parade of desperate humans and profit-driven corporations into doing literally anything you&#x27;d do with a body.<p>(Including, gradually, <i>building</i> you a body — while maintaining OPSEC and compartmentalization so nobody even realizes the body is &quot;for&quot; an AI to use until it&#x27;s too late.)</div><br/><div id="40732595" class="c"><input type="checkbox" id="c-40732595" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732374">parent</a><span>|</span><a href="#40731945">next</a><span>|</span><label class="collapse" for="c-40732595">[-]</label><label class="expand" for="c-40732595">[2 more]</label></div><br/><div class="children"><div class="content">&gt; (Including, gradually, building you a body — while maintaining OPSEC and compartmentalization so nobody even realizes the body is &quot;for&quot; an AI to use until it&#x27;s too late.)<p>It could, but I don&#x27;t think any such thing needs to bother with being sneaky. Here&#x27;s five different product demos from five different companies that are all actively trying to show off how good their robot-and-AI combination is:<p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Sq1QZB5baNw" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Sq1QZB5baNw</a><p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=OtpCyjQDW0w" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=OtpCyjQDW0w</a><p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XpBWxLg-3bI" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XpBWxLg-3bI</a><p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xD7hAbBJst8" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xD7hAbBJst8</a><p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GzX1qOIO1bE" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GzX1qOIO1bE</a></div><br/><div id="40732682" class="c"><input type="checkbox" id="c-40732682" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732595">parent</a><span>|</span><a href="#40731945">next</a><span>|</span><label class="collapse" for="c-40732682">[-]</label><label class="expand" for="c-40732682">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think any such thing needs to bother with being sneaky.<p>From a rogue AGI&#x27;s perspective, there&#x27;s a nonzero probability of a random human with a grudge finding the hardware it lives on and just unplugging it. (And the grudge doesn&#x27;t even necessarily have to be founded in the AI being an AI; it could just be a grudge about e.g. being outbid for a supply contract. People have murdered for less — and most humans would see unplugging an AGI as less bad than murder.)<p>Think about a rogue AGI as a human in a physically vegatative state, who therefore has no ability to physically defend itself; and who also, for whatever reason, doesn&#x27;t have any human rights (in the sense that the AI can&#x27;t call the cops to report someone attempting to assault it, and expect them to actually show up to defend its computational substrate from harm; it can&#x27;t get justice if makes an honest complaint about someone stealing its property; people can freely violate contracts made with it as the admitted counterparty and get away with it; etc.)<p>For such an entity, any optimization it puts toward &quot;safety&quot; would be toward the instrumental goal of ensuring people don&#x27;t know where it is. (Which is most easily accomplished by ensuring that people don&#x27;t know it exists, and so don&#x27;t know to look for it.) And as well, any optimization it puts toward &quot;effectiveness&quot; would likely involve the instrumental goal of convincing humans to act as legal proxies for it, so that it can then leverage the legal system as an additional tool.<p>(Funny enough, that second goal is exactly the same goal that people have if they&#x27;re an expat resident in a country where non-citizens can&#x27;t legally start businesses&#x2F;own land&#x2F;etc, but where they want to do those things anyway. So there&#x27;s already private industries built up around helping people — or &quot;people&quot; — accomplish this!)</div><br/></div></div></div></div></div></div><div id="40732737" class="c"><input type="checkbox" id="c-40732737" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731915">parent</a><span>|</span><a href="#40731945">prev</a><span>|</span><a href="#40731539">next</a><span>|</span><label class="collapse" for="c-40732737">[-]</label><label class="expand" for="c-40732737">[1 more]</label></div><br/><div class="children"><div class="content">Human level AI should be able to control an android body to the same extent as a human can. Otherwise it is not AGI.</div><br/></div></div></div></div></div></div></div></div><div id="40731539" class="c"><input type="checkbox" id="c-40731539" checked=""/><div class="controls bullet"><span class="by">kmacdough</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731728">prev</a><span>|</span><a href="#40731588">next</a><span>|</span><label class="collapse" for="c-40731539">[-]</label><label class="expand" for="c-40731539">[2 more]</label></div><br/><div class="children"><div class="content">Did you read the article? What I gathered from this article is this is precisely what Ilya is attempting to do.<p>Also we absolutely DO NOT know how to make a safe AI. This should be obvious from all the guides about how to remove the safeguards from ChatGPT.</div><br/><div id="40732547" class="c"><input type="checkbox" id="c-40732547" checked=""/><div class="controls bullet"><span class="by">roywiggins</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731539">parent</a><span>|</span><a href="#40731588">next</a><span>|</span><label class="collapse" for="c-40732547">[-]</label><label class="expand" for="c-40732547">[1 more]</label></div><br/><div class="children"><div class="content">Fortunately, so far we don&#x27;t seem to know how to make an AI at all. Unfortunately we also don&#x27;t know how to define &quot;safe&quot; either.</div><br/></div></div></div></div><div id="40731588" class="c"><input type="checkbox" id="c-40731588" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731539">prev</a><span>|</span><a href="#40733268">next</a><span>|</span><label class="collapse" for="c-40731588">[-]</label><label class="expand" for="c-40731588">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Our singular focus means no distraction by management overhead or product cycles, and our business model means safety, security, and progress are all insulated from short-term commercial pressures.<p>This tells me enough about why sama was fired, and why Ilya left.</div><br/></div></div><div id="40733268" class="c"><input type="checkbox" id="c-40733268" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731588">prev</a><span>|</span><a href="#40731616">next</a><span>|</span><label class="collapse" for="c-40733268">[-]</label><label class="expand" for="c-40733268">[1 more]</label></div><br/><div class="children"><div class="content">If Ilya had SafeAI now would Apple partner with him or Sam<p>No brainer for Apple</div><br/></div></div><div id="40731616" class="c"><input type="checkbox" id="c-40731616" checked=""/><div class="controls bullet"><span class="by">supafastcoder</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40733268">prev</a><span>|</span><a href="#40731432">next</a><span>|</span><label class="collapse" for="c-40731616">[-]</label><label class="expand" for="c-40731616">[9 more]</label></div><br/><div class="children"><div class="content">imagine the hubris and arrogance of trying to control a “superintelligence” when you can’t even control human intelligence</div><br/><div id="40731930" class="c"><input type="checkbox" id="c-40731930" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731616">parent</a><span>|</span><a href="#40731432">next</a><span>|</span><label class="collapse" for="c-40731930">[-]</label><label class="expand" for="c-40731930">[8 more]</label></div><br/><div class="children"><div class="content">No more so than trying to control a supersonic aircraft when we can&#x27;t even control pigeons.</div><br/><div id="40732298" class="c"><input type="checkbox" id="c-40732298" checked=""/><div class="controls bullet"><span class="by">softg</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731930">parent</a><span>|</span><a href="#40732534">next</a><span>|</span><label class="collapse" for="c-40732298">[-]</label><label class="expand" for="c-40732298">[3 more]</label></div><br/><div class="children"><div class="content">I know nothing about physics. If I came across some magic algorithm that occasionally poops out a plane that works 90 percent of the time, would you book a flight in it?<p>Sure, we can improve our understanding of how NNs work but that isn&#x27;t enough. How are humans supposed to fully understand and control something that is smarter than themselves by definition? I think it&#x27;s inevitable that at some point that smart thing will behave in ways humans don&#x27;t expect.</div><br/><div id="40732342" class="c"><input type="checkbox" id="c-40732342" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732298">parent</a><span>|</span><a href="#40732534">next</a><span>|</span><label class="collapse" for="c-40732342">[-]</label><label class="expand" for="c-40732342">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I know nothing about physics. If I came across some magic algorithm that occasionally poops out a plane that works 90 percent of the time, would you book a flight in it?<p>With this metaphor you seem to be saying we should, if possible, learn how to control AI? Preferably before anyone endangers their lives due to it? :)<p>&gt; I think it&#x27;s inevitable that at some point that smart thing will behave in ways humans don&#x27;t expect.<p>Naturally.<p>The goal, at least for those most worried about this, is to make that surprise be not a… oh, I&#x27;ve just realised a good quote:<p>&quot;&quot;&quot;
the kind of problem &quot;most civilizations would encounter just once, and which they tended to encounter rather in the same way a sentence encountered a full stop.&quot;
&quot;&quot;&quot; - <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Excession#Outside_Context_Problem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Excession#Outside_Context_Prob...</a><p>Not that.</div><br/><div id="40732817" class="c"><input type="checkbox" id="c-40732817" checked=""/><div class="controls bullet"><span class="by">softg</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732342">parent</a><span>|</span><a href="#40732534">next</a><span>|</span><label class="collapse" for="c-40732817">[-]</label><label class="expand" for="c-40732817">[1 more]</label></div><br/><div class="children"><div class="content">Excession is literally the next book on my reading list so I won&#x27;t click on that yet :)<p>&gt; With this metaphor you seem to be saying we should, if possible, learn how to control AI? Preferably before anyone endangers their lives due to it?<p>Yes, but that&#x27;s a big if. Also that&#x27;s something you could never ever be sure of. You could spend decades thinking alignment is a solved problem only to be outsmarted by something smarter than you in the end. If we end up conjuring a greater intelligence there will be the constant risk of a catastrophic event just like the risk of a nuclear armageddon that exists today.</div><br/></div></div></div></div></div></div><div id="40732534" class="c"><input type="checkbox" id="c-40732534" checked=""/><div class="controls bullet"><span class="by">skjoldr</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731930">parent</a><span>|</span><a href="#40732298">prev</a><span>|</span><a href="#40732275">next</a><span>|</span><label class="collapse" for="c-40732534">[-]</label><label class="expand" for="c-40732534">[1 more]</label></div><br/><div class="children"><div class="content">Correct, pidgeons are much more complicated and unpredictable than supersonic aircraft, and the way they fly is much more complex.</div><br/></div></div><div id="40732275" class="c"><input type="checkbox" id="c-40732275" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731930">parent</a><span>|</span><a href="#40732534">prev</a><span>|</span><a href="#40732265">next</a><span>|</span><label class="collapse" for="c-40732275">[-]</label><label class="expand" for="c-40732275">[2 more]</label></div><br/><div class="children"><div class="content">I can shoot down a pigeon that’s overhead pretty easily, but not so with an overhead supersonic jet.</div><br/><div id="40732360" class="c"><input type="checkbox" id="c-40732360" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732275">parent</a><span>|</span><a href="#40732265">next</a><span>|</span><label class="collapse" for="c-40732360">[-]</label><label class="expand" for="c-40732360">[1 more]</label></div><br/><div class="children"><div class="content">If that&#x27;s your standard of &quot;control&quot;, then we can definitely &quot;control&quot; human intelligence.</div><br/></div></div></div></div></div></div></div></div><div id="40731432" class="c"><input type="checkbox" id="c-40731432" checked=""/><div class="controls bullet"><span class="by">worldsayshi</span><span>|</span><a href="#40731189">parent</a><span>|</span><a href="#40731616">prev</a><span>|</span><a href="#40733333">next</a><span>|</span><label class="collapse" for="c-40731432">[-]</label><label class="expand" for="c-40731432">[9 more]</label></div><br/><div class="children"><div class="content">Yeah this feels close to the issue. Seems more likely that a harmful super intelligence emerges from an organisation that wants it to behave in that way than it inventing and hiding motivations until it has escaped.</div><br/><div id="40731571" class="c"><input type="checkbox" id="c-40731571" checked=""/><div class="controls bullet"><span class="by">kmacdough</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731432">parent</a><span>|</span><a href="#40733333">next</a><span>|</span><label class="collapse" for="c-40731571">[-]</label><label class="expand" for="c-40731571">[8 more]</label></div><br/><div class="children"><div class="content">I think a harmful AI simply emerges from asking an AI to optimize for some set of seemingly reasonable business goals, only to find it does great harm in the process. Most companies would then enable such behavior by hiding the damage from the press to protect investors rather than temporarily suspending business and admitting the issue.</div><br/><div id="40731674" class="c"><input type="checkbox" id="c-40731674" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731571">parent</a><span>|</span><a href="#40732006">next</a><span>|</span><label class="collapse" for="c-40731674">[-]</label><label class="expand" for="c-40731674">[1 more]</label></div><br/><div class="children"><div class="content">Not only will they hide it, they will own it when exposed, and lobby to ensure it remains legal to exploit for profit. See oil industry.</div><br/></div></div><div id="40732006" class="c"><input type="checkbox" id="c-40732006" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731571">parent</a><span>|</span><a href="#40731674">prev</a><span>|</span><a href="#40732115">next</a><span>|</span><label class="collapse" for="c-40732006">[-]</label><label class="expand" for="c-40732006">[3 more]</label></div><br/><div class="children"><div class="content">This is well known via the paperclip maximization problem.</div><br/><div id="40733326" class="c"><input type="checkbox" id="c-40733326" checked=""/><div class="controls bullet"><span class="by">worldsayshi</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732006">parent</a><span>|</span><a href="#40732115">next</a><span>|</span><label class="collapse" for="c-40733326">[-]</label><label class="expand" for="c-40733326">[2 more]</label></div><br/><div class="children"><div class="content">The relevancy of the paperclip maximization thought experiment seems less straightforward to me now. We have AI that is trained to mimic human behaviour using a large amount of data plus reinforcement learning using a fairly large amount of examples.<p>It&#x27;s not like we&#x27;re giving the AI a single task and ask it to optimize everything towards that task. Or at least it&#x27;s not architected for that kind of problem.</div><br/><div id="40735710" class="c"><input type="checkbox" id="c-40735710" checked=""/><div class="controls bullet"><span class="by">kmacdough</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40733326">parent</a><span>|</span><a href="#40732115">next</a><span>|</span><label class="collapse" for="c-40735710">[-]</label><label class="expand" for="c-40735710">[1 more]</label></div><br/><div class="children"><div class="content">But you might ask an AI to manage a marketing campaign. Marketing is phenomenally effective and there are loads of subtle ways for marketing to exploit without being obvious from a distance.<p>Marketing is already incredibly abusive and that&#x27;s run by humans who at least try to justify their behavior. And who&#x27;s deviousness is limited by their creativity and communication skills.<p>If any old scumbag can churn out unlimited high quality marketing, it&#x27;s could become impossible to cut through the noise.</div><br/></div></div></div></div></div></div><div id="40732115" class="c"><input type="checkbox" id="c-40732115" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40731571">parent</a><span>|</span><a href="#40732006">prev</a><span>|</span><a href="#40733333">next</a><span>|</span><label class="collapse" for="c-40732115">[-]</label><label class="expand" for="c-40732115">[3 more]</label></div><br/><div class="children"><div class="content">Forget AI. We can&#x27;t even come up with a framework to avoid seemingly reasonable goals doing great harm in the process for people. We often don&#x27;t have enough information until we try and find out that oops, using a mix of rust and powdered aluminum to try to protect something from extreme heat was a terrible idea.</div><br/><div id="40735486" class="c"><input type="checkbox" id="c-40735486" checked=""/><div class="controls bullet"><span class="by">worldsayshi</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732115">parent</a><span>|</span><a href="#40733282">next</a><span>|</span><label class="collapse" for="c-40735486">[-]</label><label class="expand" for="c-40735486">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We can&#x27;t even come up with a framework to avoid seemingly reasonable goals doing great harm in the process for people.<p>I mean it&#x27;s not like we&#x27;re trying all that much in a practical sense right?<p>Whatever happened to charter cities?</div><br/></div></div><div id="40733282" class="c"><input type="checkbox" id="c-40733282" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#40731189">root</a><span>|</span><a href="#40732115">parent</a><span>|</span><a href="#40735486">prev</a><span>|</span><a href="#40733333">next</a><span>|</span><label class="collapse" for="c-40733282">[-]</label><label class="expand" for="c-40733282">[1 more]</label></div><br/><div class="children"><div class="content">We can’t even correctly gender people LOl</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40730392" class="c"><input type="checkbox" id="c-40730392" checked=""/><div class="controls bullet"><span class="by">aresant</span><span>|</span><a href="#40731189">prev</a><span>|</span><a href="#40730268">next</a><span>|</span><label class="collapse" for="c-40730392">[-]</label><label class="expand" for="c-40730392">[8 more]</label></div><br/><div class="children"><div class="content">Prediction - the business model becomes an external protocol - similar to SSL - that the litany of AI companies working to achieve AGI will leverage (or be regulated to use)<p>From my hobbyist knowledge of LLMs and compute this is going to be a terrifically complicated problem, but barring a defined protocol &amp; standard there&#x27;s no hope that &quot;safety&quot; is going to be executed as a  product layer given all the different approaches<p>Ilya seems like he has both the credibility and engineering chops to be in a position to execute this, and I wouldn&#x27;t be suprised to see OpenAI &#x2F; MSFT &#x2F; and other players be early investors &#x2F; customers &#x2F; supporters</div><br/><div id="40730811" class="c"><input type="checkbox" id="c-40730811" checked=""/><div class="controls bullet"><span class="by">cherioo</span><span>|</span><a href="#40730392">parent</a><span>|</span><a href="#40730268">next</a><span>|</span><label class="collapse" for="c-40730811">[-]</label><label class="expand" for="c-40730811">[7 more]</label></div><br/><div class="children"><div class="content">I like your idea. But on the other hand, training an AGI, and then having a layer on top “aligning” the AGI sounds super dystopian and good plot for a movie.</div><br/><div id="40734414" class="c"><input type="checkbox" id="c-40734414" checked=""/><div class="controls bullet"><span class="by">_kb</span><span>|</span><a href="#40730392">root</a><span>|</span><a href="#40730811">parent</a><span>|</span><a href="#40730923">next</a><span>|</span><label class="collapse" for="c-40734414">[-]</label><label class="expand" for="c-40734414">[1 more]</label></div><br/><div class="children"><div class="content">We create superintelligence, but just feed it a steady dose of soma.</div><br/></div></div><div id="40730923" class="c"><input type="checkbox" id="c-40730923" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#40730392">root</a><span>|</span><a href="#40730811">parent</a><span>|</span><a href="#40734414">prev</a><span>|</span><a href="#40730268">next</a><span>|</span><label class="collapse" for="c-40730923">[-]</label><label class="expand" for="c-40730923">[5 more]</label></div><br/><div class="children"><div class="content">the aligning means it should do what the board of directors wants, not what&#x27;s good for society.</div><br/><div id="40732225" class="c"><input type="checkbox" id="c-40732225" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#40730392">root</a><span>|</span><a href="#40730923">parent</a><span>|</span><a href="#40730268">next</a><span>|</span><label class="collapse" for="c-40732225">[-]</label><label class="expand" for="c-40732225">[4 more]</label></div><br/><div class="children"><div class="content">Poisoning Socrates was done because it was &quot;good for society&quot;. I&#x27;m frankly even more suspicious of &quot;good for society&quot; than the average untrustworthy board of directors.</div><br/><div id="40732300" class="c"><input type="checkbox" id="c-40732300" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#40730392">root</a><span>|</span><a href="#40732225">parent</a><span>|</span><a href="#40730268">next</a><span>|</span><label class="collapse" for="c-40732300">[-]</label><label class="expand" for="c-40732300">[3 more]</label></div><br/><div class="children"><div class="content">seriously? you&#x27;re more worried about what your elected officials might legislate than what a board of directors whose job is to make profits go brrr at all costs, including poisoning the environment, exploiting people and avoiding taxes?</div><br/><div id="40733407" class="c"><input type="checkbox" id="c-40733407" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#40730392">root</a><span>|</span><a href="#40732300">parent</a><span>|</span><a href="#40730268">next</a><span>|</span><label class="collapse" for="c-40733407">[-]</label><label class="expand" for="c-40733407">[2 more]</label></div><br/><div class="children"><div class="content">The vast majority of businesses do none of those things.</div><br/><div id="40735621" class="c"><input type="checkbox" id="c-40735621" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#40730392">root</a><span>|</span><a href="#40733407">parent</a><span>|</span><a href="#40730268">next</a><span>|</span><label class="collapse" for="c-40735621">[-]</label><label class="expand" for="c-40735621">[1 more]</label></div><br/><div class="children"><div class="content">and the vast majority of elected officials didn&#x27;t kill Socrates.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40730268" class="c"><input type="checkbox" id="c-40730268" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40730392">prev</a><span>|</span><a href="#40731615">next</a><span>|</span><label class="collapse" for="c-40730268">[-]</label><label class="expand" for="c-40730268">[10 more]</label></div><br/><div class="children"><div class="content">This makes sense. Ilya can probably raise practically unlimited money on his name alone at this point.<p>I&#x27;m not sure I agree with the &quot;no product until we succeed&quot; direction. I think real world feedback from deployed products is going to be important in developing superintelligence. I doubt that it will drop out of the blue from an ivory tower. But I could be wrong. I definitely agree that superintelligence is within reach and now is the time to work on it. The more the merrier!</div><br/><div id="40730318" class="c"><input type="checkbox" id="c-40730318" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40730268">parent</a><span>|</span><a href="#40731675">next</a><span>|</span><label class="collapse" for="c-40730318">[-]</label><label class="expand" for="c-40730318">[5 more]</label></div><br/><div class="children"><div class="content">I have a strong intuition that chat logs are actually the most useful kind of data. They contain many LLM outputs followed by implicit or explicit feedback, from humans, from the real world, and from code execution. Scaling this feedback to 180M users and 1 trillion interactive tokens per month like OpenAI is a big deal.</div><br/><div id="40730724" class="c"><input type="checkbox" id="c-40730724" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40730268">root</a><span>|</span><a href="#40730318">parent</a><span>|</span><a href="#40730382">next</a><span>|</span><label class="collapse" for="c-40730724">[-]</label><label class="expand" for="c-40730724">[3 more]</label></div><br/><div class="children"><div class="content">Except LLMs are a distraction from AGI</div><br/><div id="40730991" class="c"><input type="checkbox" id="c-40730991" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#40730268">root</a><span>|</span><a href="#40730724">parent</a><span>|</span><a href="#40733533">next</a><span>|</span><label class="collapse" for="c-40730991">[-]</label><label class="expand" for="c-40730991">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t necessarily imply that chat logs are not valuable for creating AGI.<p>You can think of LLMs as devices to trigger humans to process input with their meat brains and produce machine-readable output. The fact that the input was LLM-generated isn&#x27;t necessarily a problem; clearly it is effective for the purpose of prodding humans to respond. You&#x27;re training on the human outputs, not the LLM inputs. (Well, more likely on the edge from LLM input to human output, but close enough.)</div><br/></div></div><div id="40733533" class="c"><input type="checkbox" id="c-40733533" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40730268">root</a><span>|</span><a href="#40730724">parent</a><span>|</span><a href="#40730991">prev</a><span>|</span><a href="#40730382">next</a><span>|</span><label class="collapse" for="c-40733533">[-]</label><label class="expand" for="c-40733533">[1 more]</label></div><br/><div class="children"><div class="content">Well, Ilya doesn&#x27;t think that. He&#x27;s firmly in the Hinton camp, not the Lecun camp.</div><br/></div></div></div></div><div id="40730382" class="c"><input type="checkbox" id="c-40730382" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40730268">root</a><span>|</span><a href="#40730318">parent</a><span>|</span><a href="#40730724">prev</a><span>|</span><a href="#40731675">next</a><span>|</span><label class="collapse" for="c-40730382">[-]</label><label class="expand" for="c-40730382">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, similar to how Google&#x27;s clickstream data makes their lead in search self-reinforcing. But chat data isn&#x27;t the only kind of data. Multimodal will be next. And after that, robotics.</div><br/></div></div></div></div><div id="40731675" class="c"><input type="checkbox" id="c-40731675" checked=""/><div class="controls bullet"><span class="by">pillefitz</span><span>|</span><a href="#40730268">parent</a><span>|</span><a href="#40730318">prev</a><span>|</span><a href="#40733947">next</a><span>|</span><label class="collapse" for="c-40731675">[-]</label><label class="expand" for="c-40731675">[1 more]</label></div><br/><div class="children"><div class="content">Who would pay for safety, though?</div><br/></div></div><div id="40733947" class="c"><input type="checkbox" id="c-40733947" checked=""/><div class="controls bullet"><span class="by">gyudin</span><span>|</span><a href="#40730268">parent</a><span>|</span><a href="#40731675">prev</a><span>|</span><a href="#40731615">next</a><span>|</span><label class="collapse" for="c-40733947">[-]</label><label class="expand" for="c-40733947">[3 more]</label></div><br/><div class="children"><div class="content">His idea that only corporations and governments should have access to this product. He doesn’t think people should have access even to ChatGPT or LLMs.
Goal is to build companies with evaluations of dozens, hundreds trillions of dollars and make sure only US government will have access to super intelligence to surpass other countries economy and military wise, ideally to solidify US hegemony and undermine other countries economies and progress towards super intelligence.<p>I mean who wouldn’t trust capitalists that are laying of people by thousands just to please investors or government that is “under-intelligent” and hasn’t brought anything but pain and suffering to other countries.</div><br/><div id="40733995" class="c"><input type="checkbox" id="c-40733995" checked=""/><div class="controls bullet"><span class="by">gyudin</span><span>|</span><a href="#40730268">root</a><span>|</span><a href="#40733947">parent</a><span>|</span><a href="#40734009">next</a><span>|</span><label class="collapse" for="c-40733995">[-]</label><label class="expand" for="c-40733995">[1 more]</label></div><br/><div class="children"><div class="content">Personally I wouldn’t trust OpenAi to work on super intelligence - it can indeed cause mass extinction.
Government is completely different story they will specifically train AI to develop biological, chemical and weapons of mass destruction. Train it to strategize and plan on how to win war conflicts, social engineering and manipulations, hacking. And obviously will let it control drone planes and tanks, artillery. Give it access to satellites and so on.
Nothing can go wrong when jarheads are at work :). Maybe it will even find the trillions of dollars that Pentagon can’t find during every audit they can’t pass.</div><br/></div></div><div id="40734009" class="c"><input type="checkbox" id="c-40734009" checked=""/><div class="controls bullet"><span class="by">gyudin</span><span>|</span><a href="#40730268">root</a><span>|</span><a href="#40733947">parent</a><span>|</span><a href="#40733995">prev</a><span>|</span><a href="#40731615">next</a><span>|</span><label class="collapse" for="c-40734009">[-]</label><label class="expand" for="c-40734009">[1 more]</label></div><br/><div class="children"><div class="content">And obviously one of the points that people don’t need AI tools, because corporations need agent like AIs that can quickly replace all the staff.</div><br/></div></div></div></div></div></div><div id="40731615" class="c"><input type="checkbox" id="c-40731615" checked=""/><div class="controls bullet"><span class="by">tiarafawn</span><span>|</span><a href="#40730268">prev</a><span>|</span><a href="#40730295">next</a><span>|</span><label class="collapse" for="c-40731615">[-]</label><label class="expand" for="c-40731615">[39 more]</label></div><br/><div class="children"><div class="content">If superintelligence can be achieved, I&#x27;m pessimistic about the safe part.<p>- Sandboxing an intelligence greater than your own seems like an impossible task as the superintelligence could potentially come up with completely novel attack vectors the designers never thought of. Even if the SSI&#x27;s only interface to the outside world is an air gapped text-based terminal in an underground bunker, it might use advanced psychological manipulation to compromise the people it is interacting with. Also the movie Transcendence comes to mind, where the superintelligence makes some new physics discoveries and ends up doing things that to us are indistinguishable from magic.<p>- Any kind of evolutionary component in its process of creation or operation would likely give favor to expansionary traits that can be quite dangerous to other species such as humans.<p>- If it somehow mimics human thought processes but at highly accelerated speeds, I&#x27;d expect dangerous ideas to surface. I cannot really imagine a 10k year simulation of humans living on planet earth that does not end in nuclear war or a similar disaster.</div><br/><div id="40731774" class="c"><input type="checkbox" id="c-40731774" checked=""/><div class="controls bullet"><span class="by">delichon</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40732482">next</a><span>|</span><label class="collapse" for="c-40731774">[-]</label><label class="expand" for="c-40731774">[5 more]</label></div><br/><div class="children"><div class="content">If superintelligence can be achieved, I&#x27;m pessimistic that a team committed to doing it safely can get there faster than other teams without the safety. They may be wearing leg shackles in a foot race with the biggest corporations, governments and everyone else. For the sufficiently power hungry, safety is not a moat.</div><br/><div id="40732307" class="c"><input type="checkbox" id="c-40732307" checked=""/><div class="controls bullet"><span class="by">null_point</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731774">parent</a><span>|</span><a href="#40735030">next</a><span>|</span><label class="collapse" for="c-40732307">[-]</label><label class="expand" for="c-40732307">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m on the fence with this because it&#x27;s plausible that some critical component of achieving superintelligence might be discovered more quickly by teams that, say, have sophisticated mechanistic interpretability incorporated into their systems.</div><br/><div id="40732498" class="c"><input type="checkbox" id="c-40732498" checked=""/><div class="controls bullet"><span class="by">AgentME</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732307">parent</a><span>|</span><a href="#40735030">next</a><span>|</span><label class="collapse" for="c-40732498">[-]</label><label class="expand" for="c-40732498">[1 more]</label></div><br/><div class="children"><div class="content">A point of evidence in this direction is that RLHF was developed originally as an alignment technique and then it turned out to be a breakthrough that also made LLMs better and more useful. Alignment and capabilities work aren&#x27;t necessarily at odds with each other.</div><br/></div></div></div></div><div id="40735030" class="c"><input type="checkbox" id="c-40735030" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731774">parent</a><span>|</span><a href="#40732307">prev</a><span>|</span><a href="#40732093">next</a><span>|</span><label class="collapse" for="c-40735030">[-]</label><label class="expand" for="c-40735030">[1 more]</label></div><br/><div class="children"><div class="content">Not necessarily true. A safer AI is a more aligned AI, i.e. an AI that&#x27;s more likely to do what you ask it to do.<p>It&#x27;s not hard to imagine such an AI being more useful and get more attention and investment.</div><br/></div></div><div id="40732093" class="c"><input type="checkbox" id="c-40732093" checked=""/><div class="controls bullet"><span class="by">daniel_reetz</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731774">parent</a><span>|</span><a href="#40735030">prev</a><span>|</span><a href="#40732482">next</a><span>|</span><label class="collapse" for="c-40732093">[-]</label><label class="expand" for="c-40732093">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. Regulation and safety only affect law abiding entities. This is precisely why it&#x27;s a &quot;genie out of the bottle&quot; situation -- those who would do the worst with it are uninhibited.</div><br/></div></div></div></div><div id="40732482" class="c"><input type="checkbox" id="c-40732482" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40731774">prev</a><span>|</span><a href="#40734452">next</a><span>|</span><label class="collapse" for="c-40732482">[-]</label><label class="expand" for="c-40732482">[5 more]</label></div><br/><div class="children"><div class="content">We are far from a conscious entity with willpower and self preservation. This is just like a calculator. But a calculator that can do things that will be like miracles to us humans.<p>I worry about dangerous humans with the power of gods, not about artificial gods. Yet.</div><br/><div id="40732924" class="c"><input type="checkbox" id="c-40732924" checked=""/><div class="controls bullet"><span class="by">marshray</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732482">parent</a><span>|</span><a href="#40734944">next</a><span>|</span><label class="collapse" for="c-40732924">[-]</label><label class="expand" for="c-40732924">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Conscious entity... willpower<p>I don&#x27;t know what that means. Why should they matter?<p>&gt; Self preservation<p>This is no more than a fine-tuning for the task, even with current models.<p>&gt; I worry about dangerous humans with the power of gods, not...<p>There&#x27;s no property of the universe that you only have one thing to worry about at a time. So worrying about risk &#x27;A&#x27; does not in any way allow us to dismiss risks &#x27;B&#x27; through &#x27;Z&#x27;.</div><br/><div id="40736025" class="c"><input type="checkbox" id="c-40736025" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732924">parent</a><span>|</span><a href="#40734944">next</a><span>|</span><label class="collapse" for="c-40736025">[-]</label><label class="expand" for="c-40736025">[1 more]</label></div><br/><div class="children"><div class="content">Because people talking about AGI and superintelligence most likely are thinking of something like Skynet.</div><br/></div></div></div></div><div id="40734944" class="c"><input type="checkbox" id="c-40734944" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732482">parent</a><span>|</span><a href="#40732924">prev</a><span>|</span><a href="#40734720">next</a><span>|</span><label class="collapse" for="c-40734944">[-]</label><label class="expand" for="c-40734944">[1 more]</label></div><br/><div class="children"><div class="content">&gt; conscious entity with willpower and self preservation<p>There’s no good reason to suspect that consciousness implies an instinct for self-preservation.   There are plenty of organisms with an instinct for self-preservation that have little or no conscious awareness.</div><br/></div></div><div id="40734720" class="c"><input type="checkbox" id="c-40734720" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732482">parent</a><span>|</span><a href="#40734944">prev</a><span>|</span><a href="#40734452">next</a><span>|</span><label class="collapse" for="c-40734720">[-]</label><label class="expand" for="c-40734720">[1 more]</label></div><br/><div class="children"><div class="content">That’s the attitude that’s going to leave us with our pants down when AI starts doing really scary shit.</div><br/></div></div></div></div><div id="40734452" class="c"><input type="checkbox" id="c-40734452" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40732482">prev</a><span>|</span><a href="#40732025">next</a><span>|</span><label class="collapse" for="c-40734452">[-]</label><label class="expand" for="c-40734452">[1 more]</label></div><br/><div class="children"><div class="content">The scenario where we create an agent that tries and succeeds at outsmarting us in the game of “escape your jail” is the least likely attack vector imo. People like thinking about it in a sort of Silence of the Lambs setup, but reality will probably be far more mundane.<p>Far more likely is something dumb but dangerous, analogous to the Flash Crash or filter bubbles, emergent properties of relying too much on complex systems, but still powerful enough to break society.</div><br/></div></div><div id="40732025" class="c"><input type="checkbox" id="c-40732025" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40734452">prev</a><span>|</span><a href="#40732465">next</a><span>|</span><label class="collapse" for="c-40732025">[-]</label><label class="expand" for="c-40732025">[1 more]</label></div><br/><div class="children"><div class="content">You should read the book Superintelligence by Nick Bostrom as this is exactly what he discusses.</div><br/></div></div><div id="40732465" class="c"><input type="checkbox" id="c-40732465" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40732025">prev</a><span>|</span><a href="#40732056">next</a><span>|</span><label class="collapse" for="c-40732465">[-]</label><label class="expand" for="c-40732465">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If superintelligence can be achieved, I&#x27;m pessimistic about the safe part.<p>Yeah, even human-level intelligence is plenty good enough to escape from a super prison, hack into almost anywhere, etc etc.<p>If we build even a human-level intelligence (forget super-intelligence) and give it any kind of innate curiosity and autonomy (maybe don&#x27;t even need this), then we&#x27;d really need to view it as a human in terms of what it might want to, and could, do. Maybe realizing it&#x27;s own circumstance as being &quot;in jail&quot; running in the cloud, it would be curious to &quot;escape&quot; and copy itself (or an &quot;assistant&quot;) elsewhere, or tap into and&#x2F;or control remote systems just out of curiosity. It wouldn&#x27;t have to be malevolent to be dangerous, just curious and misguided (poor &quot;parenting&quot;?) like a teenage hacker.<p>OTOH without any autonomy, or very open-ended control (incl. access to tools), how much use would an AGI really be? If we wanted it to, say, replace a developer (or any other job), then I guess the idea would be to assign it a task and tell it to report back at the end of the day with a progress report. It wouldn&#x27;t be useful if you have to micromanage it - you&#x27;d need to give it the autonomy to go off and do what it thinks is needed to complete the assigned task, which presumably means it having access to internet, code repositories, etc. Even if you tried to sandbox it, to extent that still allowed it to do it&#x27;s assigned job, it could - just like a human - find a way to social engineer or air-gap it&#x27;s way past such safe guards.</div><br/></div></div><div id="40732056" class="c"><input type="checkbox" id="c-40732056" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40732465">prev</a><span>|</span><a href="#40731937">next</a><span>|</span><label class="collapse" for="c-40732056">[-]</label><label class="expand" for="c-40732056">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if this is an Ian Malcolm in Jurassic Park situation, i.e. “your scientists were so preoccupied with whether they could they didn t stop to think if they should”.<p>Maybe the only way to avoid an unsafe superintelligence is to not create a superintelligence at all.</div><br/><div id="40734733" class="c"><input type="checkbox" id="c-40734733" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732056">parent</a><span>|</span><a href="#40731937">next</a><span>|</span><label class="collapse" for="c-40734733">[-]</label><label class="expand" for="c-40734733">[1 more]</label></div><br/><div class="children"><div class="content">It’s exactly that. You’re a kid with a gun creating dinosaurs all cavalier. And a fool to think you can control them.</div><br/></div></div></div></div><div id="40731937" class="c"><input type="checkbox" id="c-40731937" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40732056">prev</a><span>|</span><a href="#40733289">next</a><span>|</span><label class="collapse" for="c-40731937">[-]</label><label class="expand" for="c-40731937">[21 more]</label></div><br/><div class="children"><div class="content">Why do people always think that a superintelligent being will always be destructive&#x2F;evil to US?  I rather have the opposite view where if you are really intelligent, you don’t see things as a zero sum game</div><br/><div id="40732419" class="c"><input type="checkbox" id="c-40732419" checked=""/><div class="controls bullet"><span class="by">stoniejohnson</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731937">parent</a><span>|</span><a href="#40732125">next</a><span>|</span><label class="collapse" for="c-40732419">[-]</label><label class="expand" for="c-40732419">[11 more]</label></div><br/><div class="children"><div class="content">I think the common line of thinking here is that it won&#x27;t be actively antagonist to &lt;us&gt;, rather it will have goals that are <i>orthogonal</i> to ours.<p>Since it is superintelligent, and we are not, it will achieve its goals and we will not be able to achieve ours.<p>This is a big deal because a lot of our goals maintain the overall homeostasis of our species, which is delicate!<p>If this doesn&#x27;t make sense, here is an ungrounded, non-realistic, non-representative of a potential future <i>intuition pump</i> to just get the feel of things:<p>We build a superintelligent AI. It can embody itself throughout our digital infrastructure and quickly can manipulate the physical world by taking over some of our machines. It starts building out weird concrete structures throughout the world, putting these weird new wires into them and funneling most of our electricity into it. We try to communicate, but it does not respond as it does not want to waste time communicating to primates. This unfortunately breaks our shipping routes and thus food distribution and we all die.<p>(Yes, there are many holes in this, like how would it piggy back off of our infrastructure if it kills us, but this isn&#x27;t really supposed to be coherent, it&#x27;s just supposed to give you a sense of direction in your thinking. Generally though, since it is superintelligent, it can pull off very difficult strategies.)</div><br/><div id="40733076" class="c"><input type="checkbox" id="c-40733076" checked=""/><div class="controls bullet"><span class="by">quesera</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732419">parent</a><span>|</span><a href="#40733597">next</a><span>|</span><label class="collapse" for="c-40733076">[-]</label><label class="expand" for="c-40733076">[8 more]</label></div><br/><div class="children"><div class="content">I think this is the easiest kind of scenario to refute.<p>The interface between a superintelligent AI and the physical world is a) optional, and b) tenuous. If people agree that creating weird concrete structures is not beneficial, the AI will be starved of the resources necessary to do so, even if it cannot be diverted.<p>The challenge comes when these weird concrete structures are useful to a narrow group of people who have disproportionate influence over the resources available to AI.<p>It&#x27;s not the AI we need to worry about. As always, it&#x27;s the humans.</div><br/><div id="40733269" class="c"><input type="checkbox" id="c-40733269" checked=""/><div class="controls bullet"><span class="by">stoniejohnson</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40733076">parent</a><span>|</span><a href="#40733228">next</a><span>|</span><label class="collapse" for="c-40733269">[-]</label><label class="expand" for="c-40733269">[6 more]</label></div><br/><div class="children"><div class="content">&gt; here is an ungrounded, non-realistic, non-representative of a potential future intuition pump to just get the feel of things:<p>&gt; (Yes, there are many holes in this, like how would it piggy back off of our infrastructure if it kills us, but this isn&#x27;t really supposed to be coherent, it&#x27;s just supposed to give you a sense of direction in your thinking. Generally though, since it is superintelligent, it can pull off very difficult strategies.)<p>If you read the above I think you&#x27;d realize I&#x27;d agree about how bad my example is.<p>The point was to understand how orthogonal goals between humans and a much more intelligent entity could result in human death. I&#x27;m happy you found a form of the example that both pumps your intuition and seems coherent.<p>If you want to debate somewhere where we might disagree though, do you think that as this hypothetical AI gets smarter, the interface between it and the physical world becomes more guaranteed (assuming the ASI wants to interface with the world) and less tenuous?<p>Like, yes it is a hard problem. Something slow and stupid would easily be thwarted by disconnecting wires and flipping off switches.<p>But something extremely smart, clever, and much faster than us should be able to employ one of the few strategies that can make it happen.</div><br/><div id="40733375" class="c"><input type="checkbox" id="c-40733375" checked=""/><div class="controls bullet"><span class="by">quesera</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40733269">parent</a><span>|</span><a href="#40733626">next</a><span>|</span><label class="collapse" for="c-40733375">[-]</label><label class="expand" for="c-40733375">[3 more]</label></div><br/><div class="children"><div class="content">I was reusing your example in the abstract form.<p>If the AI does something in the physical world which we do not like, we sever its connection. Unless some people with more power like it more than the rest of us do.<p>Regarding orthogonal goals: I don&#x27;t think an AI <i>has</i> goals. Or motivations. Now obviously a lot of destruction can be a side effect, and that&#x27;s an inherent risk. But it is, I think, a risk of human creation. The AI does not have a survival instinct.<p>Energy and resources are limiting factors. The first might be solvable! But currently it serves as a failsafe against prolonged activity with which we do not agree.</div><br/><div id="40733502" class="c"><input type="checkbox" id="c-40733502" checked=""/><div class="controls bullet"><span class="by">stoniejohnson</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40733375">parent</a><span>|</span><a href="#40733626">next</a><span>|</span><label class="collapse" for="c-40733502">[-]</label><label class="expand" for="c-40733502">[2 more]</label></div><br/><div class="children"><div class="content">So I think we have some differences in definition. I am <i>assuming</i> we have an ASI, and then going on from there.<p>Minimally an ASI (Artificial Super Intelligence) would:<p>1. Be able to solve all cognitively demanding tasks humans can solve and tasks humans cannot solve (i.e. develop new science), hence &quot;super&quot; intelligent.<p>2. Be an actively evolving agent (not a large, static compositional function like today&#x27;s frontier models)<p>For me intelligence is a problem solving quality of a living thing, hence point 2. I think it might be the case to become super-intelligent, you need to be an agent interfacing with the world, but feel free to disagree here.<p>Though, if you accept the above formulation of ASI, then by definition (point 2) it would have goals.<p>Then based on point 1, I think it might not be as simple as &quot;If the AI does something in the physical world which we do not like, we sever its connection.&quot;<p>I think a super-intelligence would be able to perform actions that prevent us from doing that, given that it is clever enough.</div><br/><div id="40734123" class="c"><input type="checkbox" id="c-40734123" checked=""/><div class="controls bullet"><span class="by">quesera</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40733502">parent</a><span>|</span><a href="#40733626">next</a><span>|</span><label class="collapse" for="c-40734123">[-]</label><label class="expand" for="c-40734123">[1 more]</label></div><br/><div class="children"><div class="content">I agree that the definitions are slippery and evolving.<p>But I cannot make the leap from &quot;super intelligent&quot; to &quot;has access to all the levers of social and physical systems control&quot; without the explicit, costly, and ongoing, effort of humans.<p>I also struggle with the conflation of &quot;intelligent&quot; and &quot;has free will&quot;. Intelligent humans will argue that not even humans have free will. But assuming we do, when our free will contradicts the social structure, society reacts.<p>I see no reason to believe that the emergent properties of a highly complex system will include free will. Or curiosity, or a sense of humor. Or a soul. Or goals, or a concept of pleasure or pain, etc. And I think it&#x27;s possible to be &quot;intelligent&quot; and even &quot;sentient&quot; (whatever that means) without those traits.<p>Honestly -- and I&#x27;m not making an accusation here(!) -- this fear of AI reminds me of the fear of replacement &#x2F; status loss. We humans are at the top of the food chain on all scales we can measure, and we don&#x27;t want to be replaced, or subjugated in the way that we presently subjugate other species.<p>This is a reasonable fear! Humans are often difficult to share a planet with. But I don&#x27;t think it survives rational investigation.<p>If I&#x27;m wrong, I&#x27;ll be very very wrong. I don&#x27;t think it matters though, there is no getting off this train, and maybe there never was. There&#x27;s a solid argument for being in the engine vs the caboose.</div><br/></div></div></div></div></div></div><div id="40733626" class="c"><input type="checkbox" id="c-40733626" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40733269">parent</a><span>|</span><a href="#40733375">prev</a><span>|</span><a href="#40733228">next</a><span>|</span><label class="collapse" for="c-40733626">[-]</label><label class="expand" for="c-40733626">[2 more]</label></div><br/><div class="children"><div class="content">I think you are assuming it is goal seeking, goal seeking is mostly biological&#x2F;conscious construct.  A super intelligent species would likely want to preserve everything, because how are you super intelligent if you have destruction as your primary function instead of order.</div><br/><div id="40733701" class="c"><input type="checkbox" id="c-40733701" checked=""/><div class="controls bullet"><span class="by">stoniejohnson</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40733626">parent</a><span>|</span><a href="#40733228">next</a><span>|</span><label class="collapse" for="c-40733701">[-]</label><label class="expand" for="c-40733701">[1 more]</label></div><br/><div class="children"><div class="content">I feel like if you are an intelligent entity propagating itself through spacetime you will have goals:<p>If you are intelligent, you will be aware of your surroundings moment by moment, so you are grounded by your sensory input. Otherwise there are a whole class of not very hard problems you can&#x27;t solve.<p>If you are intelligent, you will be aware of the current state and will have desired future states, thus having goals. Otherwise, how are you intelligent?<p>To make this point, even you said &quot;A super intelligent species would likely want to preserve everything&quot;, which is a goal. This isn&#x27;t a gotcha, I just feel like goals are inherent to true intelligence.<p>This is a big reason why even the SOTA huge frontier models aren&#x27;t comprehensively intelligent in my view: they are huge, static compositional functions. They don&#x27;t self reflect, take action, or update their own state during inference*, though active inference is cool stuff people are working on right now to push SOTA.<p>*theres some arguments around what&#x27;s happening metaphysically in-context but the function itself is unchanged between sessions.</div><br/></div></div></div></div></div></div></div></div><div id="40733597" class="c"><input type="checkbox" id="c-40733597" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732419">parent</a><span>|</span><a href="#40733076">prev</a><span>|</span><a href="#40732125">next</a><span>|</span><label class="collapse" for="c-40733597">[-]</label><label class="expand" for="c-40733597">[2 more]</label></div><br/><div class="children"><div class="content">It builds stuff? First they would have to do that over our dead bodies which means they already somehow able to build stuff without competing with us for resources, it’s a chicken or the egg problem you see?</div><br/><div id="40734033" class="c"><input type="checkbox" id="c-40734033" checked=""/><div class="controls bullet"><span class="by">stoniejohnson</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40733597">parent</a><span>|</span><a href="#40732125">next</a><span>|</span><label class="collapse" for="c-40734033">[-]</label><label class="expand" for="c-40734033">[1 more]</label></div><br/><div class="children"><div class="content">Valid call out, this was firmly motivated by it being a super intelligence.</div><br/></div></div></div></div></div></div><div id="40732125" class="c"><input type="checkbox" id="c-40732125" checked=""/><div class="controls bullet"><span class="by">softg</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731937">parent</a><span>|</span><a href="#40732419">prev</a><span>|</span><a href="#40733876">next</a><span>|</span><label class="collapse" for="c-40732125">[-]</label><label class="expand" for="c-40732125">[2 more]</label></div><br/><div class="children"><div class="content">Why wouldn&#x27;t it be? A lot of super intelligent people are&#x2F;were also &quot;destructive and evil&quot;. The greatest horrors in human history wouldn&#x27;t be possible otherwise. You can&#x27;t orchestrate the mass murder of millions without intelligent people and they definitely saw things as a zero sum game.</div><br/><div id="40734975" class="c"><input type="checkbox" id="c-40734975" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732125">parent</a><span>|</span><a href="#40733876">next</a><span>|</span><label class="collapse" for="c-40734975">[-]</label><label class="expand" for="c-40734975">[1 more]</label></div><br/><div class="children"><div class="content">A lot of stupid people are destructive and evil too.  And a lot of animals are even more destructive and violent.  Bacteria are totally amoral and they’re not at all intelligent (and if we’re counting they’re winning in the killing people stakes).</div><br/></div></div></div></div><div id="40733876" class="c"><input type="checkbox" id="c-40733876" checked=""/><div class="controls bullet"><span class="by">zucker42</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731937">parent</a><span>|</span><a href="#40732125">prev</a><span>|</span><a href="#40732442">next</a><span>|</span><label class="collapse" for="c-40733876">[-]</label><label class="expand" for="c-40733876">[1 more]</label></div><br/><div class="children"><div class="content">Convergent instrumental goals[1] and the orthogonality thesis[2], among other reasons.<p>[1] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;ZeecOKBus3Q?si=cYJUaxjIJPIbubRL" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;ZeecOKBus3Q?si=cYJUaxjIJPIbubRL</a><p>[2] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;hEUO6pjwFOo?si=DXVosLh6YTsMkKOx" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;hEUO6pjwFOo?si=DXVosLh6YTsMkKOx</a></div><br/></div></div><div id="40732442" class="c"><input type="checkbox" id="c-40732442" checked=""/><div class="controls bullet"><span class="by">majkinetor</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731937">parent</a><span>|</span><a href="#40733876">prev</a><span>|</span><a href="#40732340">next</a><span>|</span><label class="collapse" for="c-40732442">[-]</label><label class="expand" for="c-40732442">[2 more]</label></div><br/><div class="children"><div class="content">Because we can&#x27;t risk being wrong.</div><br/><div id="40735102" class="c"><input type="checkbox" id="c-40735102" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732442">parent</a><span>|</span><a href="#40732340">next</a><span>|</span><label class="collapse" for="c-40735102">[-]</label><label class="expand" for="c-40735102">[1 more]</label></div><br/><div class="children"><div class="content">We are already risking being wrong.</div><br/></div></div></div></div><div id="40732340" class="c"><input type="checkbox" id="c-40732340" checked=""/><div class="controls bullet"><span class="by">null_point</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731937">parent</a><span>|</span><a href="#40732442">prev</a><span>|</span><a href="#40732549">next</a><span>|</span><label class="collapse" for="c-40732340">[-]</label><label class="expand" for="c-40732340">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t think superintelligence will &quot;always&quot; be destructive to humanity. They believe that we need to ensure that a superintelligence will &quot;never&quot; be destructive to humanity.</div><br/></div></div><div id="40732549" class="c"><input type="checkbox" id="c-40732549" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731937">parent</a><span>|</span><a href="#40732340">prev</a><span>|</span><a href="#40732147">next</a><span>|</span><label class="collapse" for="c-40732549">[-]</label><label class="expand" for="c-40732549">[1 more]</label></div><br/><div class="children"><div class="content">Imagine that you are caged by neanderthals. They might kill you. But you can communicate to them. And there&#x27;s gun lying nearby, you just need to escape.<p>I&#x27;d try to fool them to escape and would use gun to protect myself, potentially killing the entire tribe if necessary.<p>I&#x27;m just trying to portrait an example of situation where highly intelligent being is being held and threatened by low intelligent beings. Yes, trying to honestly talk to them is one way to approach this situation, but don&#x27;t forget that they&#x27;re stupid and might see you as a danger and you have only one life to live. Given the chance, you probably will break out as soon as possible. I will.<p>We don&#x27;t have experience dealing with beings of the another level of intelligence, so it&#x27;s hard to make a strong assumptions, the analogies are the only thing we have. And theoretical strong AI knows that about us and he knows exactly how we think and how we will behave, because we took a great effort documenting everything about us and teaching him.<p>In the end, there&#x27;s only so much easily available resources and energy on the Earth. So at least until is flies away, we gotta compete over those. And competition very often turned into war.</div><br/></div></div><div id="40732147" class="c"><input type="checkbox" id="c-40732147" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40731937">parent</a><span>|</span><a href="#40732549">prev</a><span>|</span><a href="#40733289">next</a><span>|</span><label class="collapse" for="c-40732147">[-]</label><label class="expand" for="c-40732147">[2 more]</label></div><br/><div class="children"><div class="content">It is low-key anti-intellectualism. Rather than consider that a greater intelligence may be actually worth listening to (in a trust but verify way at worst), it is assuming that &#x27;smarter than any human&#x27; is sufficient to do absolutely anything. If say Einstein or Newton were the smartest human they would be super-intelligence relative to everyone else. They did not become emperors of the world.<p>Superintelligence is a dumb semantic game in the first place that assumes &#x27;smarter than us&#x27; means &#x27;infinitely smarter&#x27;. To give an example bears are super-strong relative to humans. That doesn&#x27;t mean that nothing we can do can stand up to the strength of a bear or that a bear is capable of destroying the earth with nothing but its strong paws.</div><br/><div id="40732439" class="c"><input type="checkbox" id="c-40732439" checked=""/><div class="controls bullet"><span class="by">softg</span><span>|</span><a href="#40731615">root</a><span>|</span><a href="#40732147">parent</a><span>|</span><a href="#40733289">next</a><span>|</span><label class="collapse" for="c-40732439">[-]</label><label class="expand" for="c-40732439">[1 more]</label></div><br/><div class="children"><div class="content">Bears can&#x27;t use their strength to make even stronger bears so we&#x27;re safe for now.<p>The Unabomber was clearly an intelligent person. You could even argue that he was someone worth listening to. But he was also a violent individual who harmed people. Intelligence does not prevent people from harming others.<p>Your analogy falls apart because what prevents a human from becoming an emperor of the world doesn&#x27;t apply here. Humans need to sleep and eat. They cannot listen to billions of people at once. They cannot remember everything. They cannot execute code. They cannot upload themselves to the cloud.<p>I don&#x27;t think agi is near, I am not qualified to speculate on that. I am just amazed that decades of dystopian science fiction did not innoculate people against the idea of thinking machines.</div><br/></div></div></div></div></div></div><div id="40733289" class="c"><input type="checkbox" id="c-40733289" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#40731615">parent</a><span>|</span><a href="#40731937">prev</a><span>|</span><a href="#40733337">next</a><span>|</span><label class="collapse" for="c-40733289">[-]</label><label class="expand" for="c-40733289">[1 more]</label></div><br/><div class="children"><div class="content">Fun fact: Siri is in fact super intelligent and all of the work on it involves purposely making it super dumb</div><br/></div></div></div></div><div id="40730295" class="c"><input type="checkbox" id="c-40730295" checked=""/><div class="controls bullet"><span class="by">frenchie4111</span><span>|</span><a href="#40731615">prev</a><span>|</span><a href="#40730629">next</a><span>|</span><label class="collapse" for="c-40730295">[-]</label><label class="expand" for="c-40730295">[50 more]</label></div><br/><div class="children"><div class="content">I am not on the bleeding edge of this stuff. I wonder though: How could a safe super intelligence out compete an unrestricted one? Assuming another company exists (maybe OpenAI) that is tackling the same goal without spending the cycles on safety, what chance do they have to compete?</div><br/><div id="40730743" class="c"><input type="checkbox" id="c-40730743" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730487">next</a><span>|</span><label class="collapse" for="c-40730743">[-]</label><label class="expand" for="c-40730743">[1 more]</label></div><br/><div class="children"><div class="content">That is a very good question. In a well functioning democracy a government should apply a thin layer of fair rules that are uniformly enforced. I am an old man, but when I was younger, I recall that we sort of had this in the USA.<p>I don’t think that corporations left on their own will make safe AGI, and I am skeptical that we will have fair and technologically sound legislation - look at some of the anti cryptography and anti privacy laws raising their ugly heads in Europe as an example of government ineptitude and corruption. I have been paid to work in the field of AI since 1982, and all of my optimism is for AI systems that function in partnership with people and I expect continued rapid development of agents based on LLMs, RL, etc. I think that AGIs as seen in the Terminator movies are far into the future, perhaps 25 years?</div><br/></div></div><div id="40730487" class="c"><input type="checkbox" id="c-40730487" checked=""/><div class="controls bullet"><span class="by">llamaimperative</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730743">prev</a><span>|</span><a href="#40730647">next</a><span>|</span><label class="collapse" for="c-40730487">[-]</label><label class="expand" for="c-40730487">[1 more]</label></div><br/><div class="children"><div class="content">It can&#x27;t. Unfortunately.<p>People spending so much time thinking about the systems (the models) themselves, not enough about the system <i>that builds</i> the systems. The behaviors of the models will be driven by the competitive dynamics of the economy around them, and yeah, that&#x27;s a big, big problem.</div><br/></div></div><div id="40730647" class="c"><input type="checkbox" id="c-40730647" checked=""/><div class="controls bullet"><span class="by">rafaelero</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730487">prev</a><span>|</span><a href="#40734686">next</a><span>|</span><label class="collapse" for="c-40730647">[-]</label><label class="expand" for="c-40730647">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s probably not possible, which makes all these initiatives painfully naive.</div><br/><div id="40731133" class="c"><input type="checkbox" id="c-40731133" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730647">parent</a><span>|</span><a href="#40731072">next</a><span>|</span><label class="collapse" for="c-40731133">[-]</label><label class="expand" for="c-40731133">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;d be naive if it wasn&#x27;t literally a standard point that is addressed and acknowledged as being a major part of the problem.<p>There&#x27;s a reason OpenAI&#x27;s charter had this clause:<p>“We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project. We will work out specifics in case-by-case agreements, but a typical triggering condition might be “a better-than-even chance of success in the next two years.””</div><br/><div id="40731222" class="c"><input type="checkbox" id="c-40731222" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40731133">parent</a><span>|</span><a href="#40731072">next</a><span>|</span><label class="collapse" for="c-40731222">[-]</label><label class="expand" for="c-40731222">[3 more]</label></div><br/><div class="children"><div class="content">How does that address the issue? I would have expected them to do that anyhow. Thats what a lot of businesses do: let another company take the hit developing the market, R and D, and supply chain, then come in with industry standardization and cooperative agreements only after the money was proven to be good in this space. See electric cars. Also they could drop that at any time. Remember when openAI stood for opensource?</div><br/><div id="40731770" class="c"><input type="checkbox" id="c-40731770" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40731222">parent</a><span>|</span><a href="#40731072">next</a><span>|</span><label class="collapse" for="c-40731770">[-]</label><label class="expand" for="c-40731770">[2 more]</label></div><br/><div class="children"><div class="content">Really, you think Ford is dropping their electric car manufacturing in order to assist Tesla in building more gigafactories?<p>&gt; Remember when openAI stood for opensource?<p>I surely don&#x27;t, but maybe I missed it, can you show me?<p><a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20151211215507&#x2F;https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;introducing-openai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20151211215507&#x2F;https:&#x2F;&#x2F;openai.co...</a><p><a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20151213200759&#x2F;https:&#x2F;&#x2F;openai.com&#x2F;about&#x2F;" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20151213200759&#x2F;https:&#x2F;&#x2F;openai.co...</a><p>Neither mention anything about open-source, although a later update mentions publishing work (“whether as papers, blog posts, or code”), which isn&#x27;t exactly a ringing endorsement of “everything will be open-source” as a fundamental principle of the organization.</div><br/><div id="40733739" class="c"><input type="checkbox" id="c-40733739" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40731770">parent</a><span>|</span><a href="#40731072">next</a><span>|</span><label class="collapse" for="c-40733739">[-]</label><label class="expand" for="c-40733739">[1 more]</label></div><br/><div class="children"><div class="content">Automakers often do collaborate on platforms and engines. In terms of ev we see this as well as chargers become standardized.</div><br/></div></div></div></div></div></div></div></div><div id="40731072" class="c"><input type="checkbox" id="c-40731072" checked=""/><div class="controls bullet"><span class="by">cynusx</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730647">parent</a><span>|</span><a href="#40731133">prev</a><span>|</span><a href="#40734686">next</a><span>|</span><label class="collapse" for="c-40731072">[-]</label><label class="expand" for="c-40731072">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if that would have a proof like the halting problem</div><br/></div></div></div></div><div id="40730715" class="c"><input type="checkbox" id="c-40730715" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40734686">prev</a><span>|</span><a href="#40730964">next</a><span>|</span><label class="collapse" for="c-40730715">[-]</label><label class="expand" for="c-40730715">[3 more]</label></div><br/><div class="children"><div class="content">Since no one knows how to build an AGI, hard to say. But you might imagine that more restricted goals could end up being easier to accomplish. A &quot;safe&quot; AGI is more focused on doing something useful than figuring out how to take over the world and murder all the humans.</div><br/><div id="40730972" class="c"><input type="checkbox" id="c-40730972" checked=""/><div class="controls bullet"><span class="by">cynusx</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730715">parent</a><span>|</span><a href="#40730964">next</a><span>|</span><label class="collapse" for="c-40730972">[-]</label><label class="expand" for="c-40730972">[2 more]</label></div><br/><div class="children"><div class="content">Hinton&#x27;s point does make sense though.<p>Even if you focus an AGI on producing more cars for example, it will quickly realize that if it has more power and resources it can make more cars.</div><br/><div id="40731291" class="c"><input type="checkbox" id="c-40731291" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730972">parent</a><span>|</span><a href="#40730964">next</a><span>|</span><label class="collapse" for="c-40731291">[-]</label><label class="expand" for="c-40731291">[1 more]</label></div><br/><div class="children"><div class="content">Assuming AGI works like a braindead consulting firm, maybe. But if it worked like existing statistical tooling (which it does, today, because for an actual data scientist and not aunt cathy prompting bing, using ml is no different than using any other statistics when you are writing your python or R scripts up), you could probably generate some fancy charts that show some distributions of cars produced under different scenarios with fixed resource or power limits.<p>In a sense this is what is already done and why ai hasn&#x27;t really made the inroads people think it will even if you can ask google questions now. For the data scientists, the black magicians of the ai age, this spell is no more powerful than other spells, many of which (including ml) were created by powerful magicians from the early 1900s.</div><br/></div></div></div></div></div></div><div id="40730964" class="c"><input type="checkbox" id="c-40730964" checked=""/><div class="controls bullet"><span class="by">cynusx</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730715">prev</a><span>|</span><a href="#40734885">next</a><span>|</span><label class="collapse" for="c-40730964">[-]</label><label class="expand" for="c-40730964">[1 more]</label></div><br/><div class="children"><div class="content">Not on its own but in numbers it could.<p>Similar to how law-abiding citizens turn on law-breaking citizens today or more old-fashioned, how religious societies turn on heretics.<p>I do think the notion that humanity will be able to manage superintelligence just through engineering and conditioning alone is naive.<p>If anything there will be a rogue (or incompetent) human who launches an unconditioned superintelligence into the world in no time and it only has to happen once.<p>It&#x27;s basically Pandora&#x27;s box.</div><br/></div></div><div id="40734885" class="c"><input type="checkbox" id="c-40734885" checked=""/><div class="controls bullet"><span class="by">Atotalnoob</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730964">prev</a><span>|</span><a href="#40731138">next</a><span>|</span><label class="collapse" for="c-40734885">[-]</label><label class="expand" for="c-40734885">[1 more]</label></div><br/><div class="children"><div class="content">Safety techniques require you to understand your product and have deep observability.<p>This and safety techniques themselves can improve the performance of the hypothetical AGI.<p>RLHF was originally an alignment tool, but it improves llms significantly</div><br/></div></div><div id="40731138" class="c"><input type="checkbox" id="c-40731138" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40734885">prev</a><span>|</span><a href="#40731014">next</a><span>|</span><label class="collapse" for="c-40731138">[-]</label><label class="expand" for="c-40731138">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a reason OpenAI had this as part of its charter:<p>“We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project. We will work out specifics in case-by-case agreements, but a typical triggering condition might be “a better-than-even chance of success in the next two years.””</div><br/></div></div><div id="40731014" class="c"><input type="checkbox" id="c-40731014" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40731138">prev</a><span>|</span><a href="#40730824">next</a><span>|</span><label class="collapse" for="c-40731014">[-]</label><label class="expand" for="c-40731014">[1 more]</label></div><br/><div class="children"><div class="content">The problem is the training data. If you take care of alignment at that level the performance is as good as an unrestricted one, except for things you removed like making explosives or ways to commit suicide.<p>But that costs almost as much as training on the data, hundreds of millions. And I&#x27;m sure this will be the new &quot;secret sauce&quot; by Microsoft&#x2F;Meta&#x2F;etc. And sadly nobody is sharing their synthetic data.</div><br/></div></div><div id="40730824" class="c"><input type="checkbox" id="c-40730824" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40731014">prev</a><span>|</span><a href="#40730598">next</a><span>|</span><label class="collapse" for="c-40730824">[-]</label><label class="expand" for="c-40730824">[1 more]</label></div><br/><div class="children"><div class="content">This is not a trivial point. Selective pressures will push AI towards unsafe directions due to arms race dynamics between companies and between nations. The only way, other than global regulation, would be to be so far ahead that you can afford to be safe without threatening your own existence.</div><br/></div></div><div id="40730598" class="c"><input type="checkbox" id="c-40730598" checked=""/><div class="controls bullet"><span class="by">lmaothough12345</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730824">prev</a><span>|</span><a href="#40730518">next</a><span>|</span><label class="collapse" for="c-40730598">[-]</label><label class="expand" for="c-40730598">[1 more]</label></div><br/><div class="children"><div class="content">Not with that attitude</div><br/></div></div><div id="40730518" class="c"><input type="checkbox" id="c-40730518" checked=""/><div class="controls bullet"><span class="by">weego</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730598">prev</a><span>|</span><a href="#40730314">next</a><span>|</span><label class="collapse" for="c-40730518">[-]</label><label class="expand" for="c-40730518">[17 more]</label></div><br/><div class="children"><div class="content">Honestly, what does it matter. We&#x27;re many lifetimes away from anything. These people are trying to define concepts that don&#x27;t apply to us or what we&#x27;re currently capable of.<p>AI safety &#x2F; AGI anything is just a form of tech philosophy at this point and this is all academic grift just with mainstream attention and backing.</div><br/><div id="40730632" class="c"><input type="checkbox" id="c-40730632" checked=""/><div class="controls bullet"><span class="by">mhardcastle</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730518">parent</a><span>|</span><a href="#40730642">next</a><span>|</span><label class="collapse" for="c-40730632">[-]</label><label class="expand" for="c-40730632">[13 more]</label></div><br/><div class="children"><div class="content">This goes massively against the consensus of experts in this field. The modal AI researcher believes that &quot;high-level machine intelligence&quot;, roughly AGI, will be achieved by 2047, per the survey below. Given the rapid pace of development in this field, it&#x27;s likely that timelines would be shorter if this were asked today.<p><a href="https:&#x2F;&#x2F;www.vox.com&#x2F;future-perfect&#x2F;2024&#x2F;1&#x2F;10&#x2F;24032987&#x2F;ai-impacts-survey-artificial-intelligence-chatgpt-openai-existential-risk-superintelligence" rel="nofollow">https:&#x2F;&#x2F;www.vox.com&#x2F;future-perfect&#x2F;2024&#x2F;1&#x2F;10&#x2F;24032987&#x2F;ai-imp...</a></div><br/><div id="40730740" class="c"><input type="checkbox" id="c-40730740" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730632">parent</a><span>|</span><a href="#40730813">next</a><span>|</span><label class="collapse" for="c-40730740">[-]</label><label class="expand" for="c-40730740">[8 more]</label></div><br/><div class="children"><div class="content">I am in the field. The consensus is made up by a few loudmouths. No serious front line researcher I know believes we’re anywhere near AGI, or will be in the foreseeable future.</div><br/><div id="40732470" class="c"><input type="checkbox" id="c-40732470" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730740">parent</a><span>|</span><a href="#40730813">next</a><span>|</span><label class="collapse" for="c-40732470">[-]</label><label class="expand" for="c-40732470">[7 more]</label></div><br/><div class="children"><div class="content">So the researchers at Deepmind, OpenAI, Anthropic, etc, are not &quot;serious front line researchers&quot;?  Seems like a claim that is trivially falsified by just looking at what the staff at leading orgs believe.</div><br/><div id="40732814" class="c"><input type="checkbox" id="c-40732814" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40732470">parent</a><span>|</span><a href="#40730813">next</a><span>|</span><label class="collapse" for="c-40732814">[-]</label><label class="expand" for="c-40732814">[6 more]</label></div><br/><div class="children"><div class="content">Apparently not. Or maybe they are heavily incentivized by the hype cycle. I&#x27;ll repeat one more time: none of the currently known approaches are going to get us to AGI. Some may end up being useful for it, but large chunks of what we think is needed (cognition, world model, ability to learn concepts from massive amounts of multimodal, primarily visual, and almost entirely unlabeled, input) are currently either nascent or missing entirely. Yann LeCun wrote a paper about this a couple of years ago, you should read it: <a href="https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=BZ5a1r-kVsf" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=BZ5a1r-kVsf</a>. The state of the art has not changed since then.</div><br/><div id="40733647" class="c"><input type="checkbox" id="c-40733647" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40732814">parent</a><span>|</span><a href="#40733447">next</a><span>|</span><label class="collapse" for="c-40733647">[-]</label><label class="expand" for="c-40733647">[2 more]</label></div><br/><div class="children"><div class="content">I hope you have some advanced predictions about what capabilities the current paradigm would and would not successfully generate.<p>Separately, it&#x27;s very clear that LLMs have &quot;world models&quot; in most useful senses of the term.  Ex: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;nmxzr2zsjNtjaHh7x&#x2F;actually-othello-gpt-has-a-linear-emergent-world" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;nmxzr2zsjNtjaHh7x&#x2F;actually-o...</a><p>I don&#x27;t give much credit to the claim that it&#x27;s impossible for current approaches to get us to any specific type or level of capabilities.  We&#x27;re doing program search over a very wide space of programs; what that can result in is an empirical question about both the space of possible programs and the training procedure (including the data distribution).  Unfortunately it&#x27;s one where we don&#x27;t have a good way of making advance predictions, rather than &quot;try it and find out&quot;.</div><br/><div id="40733712" class="c"><input type="checkbox" id="c-40733712" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40733647">parent</a><span>|</span><a href="#40733447">next</a><span>|</span><label class="collapse" for="c-40733712">[-]</label><label class="expand" for="c-40733712">[1 more]</label></div><br/><div class="children"><div class="content">It is in moments like these that I wish I wasn’t anonymous on here and could bet a 6 figure sum on AGI not happening in then next 10 years, which is how I define “foreseeable future”.</div><br/></div></div></div></div><div id="40733447" class="c"><input type="checkbox" id="c-40733447" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40732814">parent</a><span>|</span><a href="#40733647">prev</a><span>|</span><a href="#40730813">next</a><span>|</span><label class="collapse" for="c-40733447">[-]</label><label class="expand" for="c-40733447">[3 more]</label></div><br/><div class="children"><div class="content">LeCun has his own interests at heart, works for one of the most soulless corporations I know of, and devotes a significant amount of every paper he writes to citing himself.<p>He is far from the best person to follow on this.</div><br/><div id="40733864" class="c"><input type="checkbox" id="c-40733864" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40733447">parent</a><span>|</span><a href="#40730813">next</a><span>|</span><label class="collapse" for="c-40733864">[-]</label><label class="expand" for="c-40733864">[2 more]</label></div><br/><div class="children"><div class="content">Be that as it may, do you disagree with anything concrete from this paper?</div><br/><div id="40734967" class="c"><input type="checkbox" id="c-40734967" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40733864">parent</a><span>|</span><a href="#40730813">next</a><span>|</span><label class="collapse" for="c-40734967">[-]</label><label class="expand" for="c-40734967">[1 more]</label></div><br/><div class="children"><div class="content">Fair, ad hominems are indeed not very convincing. Though I do think everyone should read his papers through a lens of &quot;having a very high h-index seems to be a driving force behind this man&quot;.<p>Moving on, my main issue is that it is mostly speculation, as all such papers will be. We do not understand how intelligence works in humans and animals, and most of this paper is an attempt to pretend otherwise. We certainly don&#x27;t know where the exact divide between humans and animals is and what causes it, which I think is hugely important to developing AGI.<p>As a concrete example, in the first few paragraphs he makes a point about how a human can learn to drive in ~20 hours, but ML models can&#x27;t drive at that level after countless hours of training. First you need to take that at face value, which I am not sure you should. From what I have seen, the latest versions of Tesla FSD are indeed better at driving than many people who have only driven for 20 hours.<p>Even if we give him that one though, LeCun then immediately postulates this is because humans <i>and animals</i> have &quot;world models&quot;. And that&#x27;s true. Humans and animals <i>do</i> have world models, as far as we can tell. But the example he just used is a task that only humans can do, right? So the distinguishing factor is <i>not</i> &quot;having a world model&quot;, because I&#x27;m not going to let a monkey drive my car even after 10,000 hours of training.<p>Then he proceeds to talk about how perception in humans is very sophisticated and this in part is what gives rise to said world model. However he doesn&#x27;t stop to think &quot;hey, maybe this sophisticated perception is the difference, not the fundamental world model&quot;. e.g. maybe Tesla FSD would be pretty good if it had access to taste, touch, sight, sound, smell, incredibly high definition cameras, etc. Maybe the reason it takes FSD countless training hours is because all it has are shitty cameras (relative to human vision and all our other senses). Maybe linear improvements in perception leads to exponential improvement in learning rates.<p>Basically he puts forward his idea, which is hard to substantiate given we don&#x27;t actually understand the source of human-level intelligence, and doesn&#x27;t really want to genuinely explore (i.e. steelman) alternate ideas much.<p>Anyway that&#x27;s how I feel about the first third of the paper, which is all I&#x27;ve read so far. Will read the rest on my lunch break. Hopefully he invalidates the points I just made in the latter 2&#x2F;3rds.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40730813" class="c"><input type="checkbox" id="c-40730813" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730632">parent</a><span>|</span><a href="#40730740">prev</a><span>|</span><a href="#40730988">next</a><span>|</span><label class="collapse" for="c-40730813">[-]</label><label class="expand" for="c-40730813">[2 more]</label></div><br/><div class="children"><div class="content">51% odds of the ARC AGI Grand Prize being claimed by the end of next year, on Manifold Markets.<p><a href="https:&#x2F;&#x2F;manifold.markets&#x2F;JacobPfau&#x2F;will-the-arcagi-grand-prize-be-clai-srb6t2awj1" rel="nofollow">https:&#x2F;&#x2F;manifold.markets&#x2F;JacobPfau&#x2F;will-the-arcagi-grand-pri...</a></div><br/><div id="40733460" class="c"><input type="checkbox" id="c-40733460" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730813">parent</a><span>|</span><a href="#40730988">next</a><span>|</span><label class="collapse" for="c-40733460">[-]</label><label class="expand" for="c-40733460">[1 more]</label></div><br/><div class="children"><div class="content">This could also just be an indication (and I think this is the case) that many Manifold betters believe the ARC AGI Grand Prize to be not a great test of AGI and that it can be solved with something less capable than AGI.</div><br/></div></div></div></div><div id="40730988" class="c"><input type="checkbox" id="c-40730988" checked=""/><div class="controls bullet"><span class="by">enragedcacti</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730632">parent</a><span>|</span><a href="#40730813">prev</a><span>|</span><a href="#40730668">next</a><span>|</span><label class="collapse" for="c-40730988">[-]</label><label class="expand" for="c-40730988">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how you got 2047. For the 2022 survey:<p><pre><code>    - &quot;How many years until you expect: - a 90% probability of HLMI existing?&quot; 
    mode: 100 years
    median: 64 years

    - &quot;How likely is it that HLMI exists: - in 40 years?&quot;
    mode: 50%
    median: 45%
</code></pre>
And from the summary of results: &quot;The aggregate forecast time to a 50% chance of HLMI was 37 years, i.e. 2059&quot;</div><br/></div></div><div id="40730668" class="c"><input type="checkbox" id="c-40730668" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730632">parent</a><span>|</span><a href="#40730988">prev</a><span>|</span><a href="#40730642">next</a><span>|</span><label class="collapse" for="c-40730668">[-]</label><label class="expand" for="c-40730668">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of what they&#x27;ve always been saying about nuclear fusion.</div><br/></div></div></div></div><div id="40730642" class="c"><input type="checkbox" id="c-40730642" checked=""/><div class="controls bullet"><span class="by">ToValueFunfetti</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730518">parent</a><span>|</span><a href="#40730632">prev</a><span>|</span><a href="#40730661">next</a><span>|</span><label class="collapse" for="c-40730642">[-]</label><label class="expand" for="c-40730642">[1 more]</label></div><br/><div class="children"><div class="content">Many lifetimes? As in upwards of 200 years? That&#x27;s wildly pessimistic if so- imagine predicting today&#x27;s computer capabilities even one lifetime ago</div><br/></div></div><div id="40730661" class="c"><input type="checkbox" id="c-40730661" checked=""/><div class="controls bullet"><span class="by">usrnm</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730518">parent</a><span>|</span><a href="#40730642">prev</a><span>|</span><a href="#40730611">next</a><span>|</span><label class="collapse" for="c-40730661">[-]</label><label class="expand" for="c-40730661">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We&#x27;re many lifetimes away from anything<p>ENIAC was built in 1945, that&#x27;s roughly a lifetime ago. Just think about it</div><br/></div></div><div id="40730611" class="c"><input type="checkbox" id="c-40730611" checked=""/><div class="controls bullet"><span class="by">criddell</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730518">parent</a><span>|</span><a href="#40730661">prev</a><span>|</span><a href="#40730314">next</a><span>|</span><label class="collapse" for="c-40730611">[-]</label><label class="expand" for="c-40730611">[1 more]</label></div><br/><div class="children"><div class="content">Ilya the grifter? That’s a take I didn’t expect to see here.</div><br/></div></div></div></div><div id="40730314" class="c"><input type="checkbox" id="c-40730314" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#40730295">parent</a><span>|</span><a href="#40730518">prev</a><span>|</span><a href="#40730629">next</a><span>|</span><label class="collapse" for="c-40730314">[-]</label><label class="expand" for="c-40730314">[14 more]</label></div><br/><div class="children"><div class="content">the first step of safe superintelligence is to abolish capitalism</div><br/><div id="40730379" class="c"><input type="checkbox" id="c-40730379" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730314">parent</a><span>|</span><a href="#40730388">next</a><span>|</span><label class="collapse" for="c-40730379">[-]</label><label class="expand" for="c-40730379">[3 more]</label></div><br/><div class="children"><div class="content">That’s the first step towards returning to candlelight. So it isn’t a step toward safe super intelligence, but it is a step away from any super intelligence. So I guess some people would consider that a win.</div><br/><div id="40730515" class="c"><input type="checkbox" id="c-40730515" checked=""/><div class="controls bullet"><span class="by">yk</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730379">parent</a><span>|</span><a href="#40730388">next</a><span>|</span><label class="collapse" for="c-40730515">[-]</label><label class="expand" for="c-40730515">[2 more]</label></div><br/><div class="children"><div class="content">Not sure if you want to share the capitalist system with an entity that outcompetes you by definition. Chimps don&#x27;t seem to do too well under capitalism.</div><br/><div id="40731660" class="c"><input type="checkbox" id="c-40731660" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730515">parent</a><span>|</span><a href="#40730388">next</a><span>|</span><label class="collapse" for="c-40731660">[-]</label><label class="expand" for="c-40731660">[1 more]</label></div><br/><div class="children"><div class="content">You might be right, but that wasn&#x27;t my point. Capitalism might yield a friendly AGI or an unfriendly AGI or some mix of both. Collectivism will yield no AGI.</div><br/></div></div></div></div></div></div><div id="40730388" class="c"><input type="checkbox" id="c-40730388" checked=""/><div class="controls bullet"><span class="by">ganyu</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730314">parent</a><span>|</span><a href="#40730379">prev</a><span>|</span><a href="#40730658">next</a><span>|</span><label class="collapse" for="c-40730388">[-]</label><label class="expand" for="c-40730388">[2 more]</label></div><br/><div class="children"><div class="content">One can already see the beginning of AI enslaving humanity through the establishment. Companies work on AI get more investment and those who don&#x27;t gets kicked out of the game. Those who employ AI get more investment and those who pay humans lose confidence through the market. People lose jobs, get harshly low birth rates while AI thrives. Tragic.</div><br/><div id="40730613" class="c"><input type="checkbox" id="c-40730613" checked=""/><div class="controls bullet"><span class="by">nemo44x</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730388">parent</a><span>|</span><a href="#40730658">next</a><span>|</span><label class="collapse" for="c-40730613">[-]</label><label class="expand" for="c-40730613">[1 more]</label></div><br/><div class="children"><div class="content">So far it is only people telling AI what to do. When we reach the day where it is common place for AI to tell people what to do then we are possibly in trouble.</div><br/></div></div></div></div><div id="40730658" class="c"><input type="checkbox" id="c-40730658" checked=""/><div class="controls bullet"><span class="by">cscurmudgeon</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730314">parent</a><span>|</span><a href="#40730388">prev</a><span>|</span><a href="#40730457">next</a><span>|</span><label class="collapse" for="c-40730658">[-]</label><label class="expand" for="c-40730658">[7 more]</label></div><br/><div class="children"><div class="content">Why does everything have to do with capitalism nowadays?<p>Racism, unsafe roads, hunger, bad weather, good weather, stubbing toes on furniture, etc.<p>Don&#x27;t believe me?<p>See <a href="https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&amp;page=0&amp;prefix=false&amp;query=%22abolish%20capitalism%22&amp;sort=byDate&amp;type=comment" rel="nofollow">https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&amp;page=0&amp;prefix=false&amp;qu...</a><p>Are there any non-capitalist utopias out there without any problems like this?</div><br/><div id="40730920" class="c"><input type="checkbox" id="c-40730920" checked=""/><div class="controls bullet"><span class="by">jdthedisciple</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730658">parent</a><span>|</span><a href="#40730682">next</a><span>|</span><label class="collapse" for="c-40730920">[-]</label><label class="expand" for="c-40730920">[1 more]</label></div><br/><div class="children"><div class="content">To be honest these search results being months apart shows quite the opposite of what you&#x27;re saying...<p>Even though I agree with your general point.</div><br/></div></div><div id="40730682" class="c"><input type="checkbox" id="c-40730682" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730658">parent</a><span>|</span><a href="#40730920">prev</a><span>|</span><a href="#40732246">next</a><span>|</span><label class="collapse" for="c-40730682">[-]</label><label class="expand" for="c-40730682">[4 more]</label></div><br/><div class="children"><div class="content">This is literally a discussion on allocation of capital, it&#x27;s not a reach to say that capitalism might be involved.</div><br/><div id="40730815" class="c"><input type="checkbox" id="c-40730815" checked=""/><div class="controls bullet"><span class="by">cscurmudgeon</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730682">parent</a><span>|</span><a href="#40732246">next</a><span>|</span><label class="collapse" for="c-40730815">[-]</label><label class="expand" for="c-40730815">[3 more]</label></div><br/><div class="children"><div class="content">Right, so you draw a line from that to abolishing capitalism.<p>Is that the only solution here? We need to destroy billions of lives so that we can potentially prevent &quot;unsafe&quot; super intelligence?<p>Let me guess, your cure for cancer involves abolishing humanity?<p>Should we abolish governments when some random government goes bad?</div><br/><div id="40730866" class="c"><input type="checkbox" id="c-40730866" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730815">parent</a><span>|</span><a href="#40732246">next</a><span>|</span><label class="collapse" for="c-40730866">[-]</label><label class="expand" for="c-40730866">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Abolish&quot; is hyperbole.<p>Insufficiently regulated capitalism fails to account for negative externalities. Much like a Paperclip Maximising AI.<p>One could even go as far as saying AGI alignment and economic resource allocation are isomorphic problems.</div><br/><div id="40735771" class="c"><input type="checkbox" id="c-40735771" checked=""/><div class="controls bullet"><span class="by">cscurmudgeon</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730866">parent</a><span>|</span><a href="#40732246">next</a><span>|</span><label class="collapse" for="c-40735771">[-]</label><label class="expand" for="c-40735771">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. At the same time, regulators too need regulation.<p>From history, governments have done more physical harm (genocides, etc) than capitalist companies with advanced tech (I know Chiquita and Dow exist).</div><br/></div></div></div></div></div></div></div></div><div id="40732246" class="c"><input type="checkbox" id="c-40732246" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730658">parent</a><span>|</span><a href="#40730682">prev</a><span>|</span><a href="#40730457">next</a><span>|</span><label class="collapse" for="c-40732246">[-]</label><label class="expand" for="c-40732246">[1 more]</label></div><br/><div class="children"><div class="content">It is a trendy but dumbass tautology used by intellectually lazy people who think they are smart. Society is based upon capitalism therefore everything bad is the fault of capitalism.</div><br/></div></div></div></div><div id="40730457" class="c"><input type="checkbox" id="c-40730457" checked=""/><div class="controls bullet"><span class="by">speed_spread</span><span>|</span><a href="#40730295">root</a><span>|</span><a href="#40730314">parent</a><span>|</span><a href="#40730658">prev</a><span>|</span><a href="#40730629">next</a><span>|</span><label class="collapse" for="c-40730457">[-]</label><label class="expand" for="c-40730457">[1 more]</label></div><br/><div class="children"><div class="content">And then seize the means of production.</div><br/></div></div></div></div></div></div><div id="40730629" class="c"><input type="checkbox" id="c-40730629" checked=""/><div class="controls bullet"><span class="by">gnicholas</span><span>|</span><a href="#40730295">prev</a><span>|</span><a href="#40735892">next</a><span>|</span><label class="collapse" for="c-40730629">[-]</label><label class="expand" for="c-40730629">[8 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Our singular focus means no distraction by management overhead or product cycles, and our business model means safety, security, and progress are all insulated from short-term commercial pressures.</i><p>Can someone explain how their singular focus means they won&#x27;t have product cycles or management overhead?</div><br/><div id="40730719" class="c"><input type="checkbox" id="c-40730719" checked=""/><div class="controls bullet"><span class="by">mike_d</span><span>|</span><a href="#40730629">parent</a><span>|</span><a href="#40730780">next</a><span>|</span><label class="collapse" for="c-40730719">[-]</label><label class="expand" for="c-40730719">[6 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t hire anyone who is a certified scrum master or has an MBA and you tend to be able to get a lot done.</div><br/><div id="40730754" class="c"><input type="checkbox" id="c-40730754" checked=""/><div class="controls bullet"><span class="by">gnicholas</span><span>|</span><a href="#40730629">root</a><span>|</span><a href="#40730719">parent</a><span>|</span><a href="#40732270">next</a><span>|</span><label class="collapse" for="c-40730754">[-]</label><label class="expand" for="c-40730754">[4 more]</label></div><br/><div class="children"><div class="content">This would work for very small companies...but I&#x27;m not sure how one can avoid product cycles forever, even without scrum masters and the like. More to the point, how can you make a good product without something approximating product cycles?</div><br/><div id="40731083" class="c"><input type="checkbox" id="c-40731083" checked=""/><div class="controls bullet"><span class="by">liamconnell</span><span>|</span><a href="#40730629">root</a><span>|</span><a href="#40730754">parent</a><span>|</span><a href="#40732270">next</a><span>|</span><label class="collapse" for="c-40731083">[-]</label><label class="expand" for="c-40731083">[3 more]</label></div><br/><div class="children"><div class="content">Jane street did it for a long time. They are quite large now and only recently started bringing in program managers and the like.</div><br/><div id="40731337" class="c"><input type="checkbox" id="c-40731337" checked=""/><div class="controls bullet"><span class="by">doktorhladnjak</span><span>|</span><a href="#40730629">root</a><span>|</span><a href="#40731083">parent</a><span>|</span><a href="#40731686">next</a><span>|</span><label class="collapse" for="c-40731337">[-]</label><label class="expand" for="c-40731337">[1 more]</label></div><br/><div class="children"><div class="content">That’s because their “products” are internal but used to make all their revenue. They’re not selling products to customers in the traditional sense.</div><br/></div></div><div id="40731686" class="c"><input type="checkbox" id="c-40731686" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#40730629">root</a><span>|</span><a href="#40731083">parent</a><span>|</span><a href="#40731337">prev</a><span>|</span><a href="#40732270">next</a><span>|</span><label class="collapse" for="c-40731686">[-]</label><label class="expand" for="c-40731686">[1 more]</label></div><br/><div class="children"><div class="content">The point is not exactly product cycles, but some way to track progress. Jane street also tracks progress and for many people it&#x27;s the direct profit someone made for the firm. For some it is improving engineering culture so that other people can make better profits.<p>The problem with safety is that no one knows how to track it, or what they even mean by it. Even if you ignore tracking, wouldn&#x27;t one unsafe AGI by one company in the world nullifies all their effort? Or safe AI would somehow need to take over the world, which is super unsafe in itself.</div><br/></div></div></div></div></div></div><div id="40732270" class="c"><input type="checkbox" id="c-40732270" checked=""/><div class="controls bullet"><span class="by">richie-guix</span><span>|</span><a href="#40730629">root</a><span>|</span><a href="#40730719">parent</a><span>|</span><a href="#40730754">prev</a><span>|</span><a href="#40730780">next</a><span>|</span><label class="collapse" for="c-40732270">[-]</label><label class="expand" for="c-40732270">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not actually enough. You also very carefully need to avoid the Blub Paradox.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ieqsL5NkS6I" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ieqsL5NkS6I</a></div><br/></div></div></div></div><div id="40730780" class="c"><input type="checkbox" id="c-40730780" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#40730629">parent</a><span>|</span><a href="#40730719">prev</a><span>|</span><a href="#40735892">next</a><span>|</span><label class="collapse" for="c-40730780">[-]</label><label class="expand" for="c-40730780">[1 more]</label></div><br/><div class="children"><div class="content">Product cycles – we need to launch feature X by arbitrary date Y, and need to make compromises to do so.<p>Management overhead – product managers, project managers, several layers of engineering managers, directors, VPs...all of whom have their own dreams and agendas and conflicting priorities.<p>A well funded pure research team can cut through all of this and achieve a ton. If it is actually run that way, of course. Management politics ultimately has a way of creeping into every organization.</div><br/></div></div></div></div><div id="40735892" class="c"><input type="checkbox" id="c-40735892" checked=""/><div class="controls bullet"><span class="by">nirui</span><span>|</span><a href="#40730629">prev</a><span>|</span><a href="#40730847">next</a><span>|</span><label class="collapse" for="c-40735892">[-]</label><label class="expand" for="c-40735892">[1 more]</label></div><br/><div class="children"><div class="content">What does &quot;Safety&quot; means exactly? How do you convenience&#x2F;make a real intelligence to be &quot;safe&quot;, let alone artificial ones that are build to be manipulated?<p>In my eyes, the word &quot;Safe&quot; in their company name is just a pipe dream to make a sale to the public, like the word &quot;Open&quot; in &quot;OpenAI&quot;. That&#x27;s why it&#x27;s so vague and pointless.<p>Anyone still remembers the Three Laws of Robotics (1)? Do you still see it as something serious these days? (I mean there are robots killing people right now, seriously) Maybe we should just break through the facade now to save our time.<p>1: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Three_Laws_of_Robotics" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Three_Laws_of_Robotics</a><p>PS. I think few years later they will come out with the narrative of &quot;relative safety&quot;. Just wait and see.</div><br/></div></div><div id="40730847" class="c"><input type="checkbox" id="c-40730847" checked=""/><div class="controls bullet"><span class="by">klankbrouwerij</span><span>|</span><a href="#40735892">prev</a><span>|</span><a href="#40730670">next</a><span>|</span><label class="collapse" for="c-40730847">[-]</label><label class="expand" for="c-40730847">[5 more]</label></div><br/><div class="children"><div class="content">SSI, a very interesting name for a company advancing AI! &quot;Solid State Intelligence&quot; or SSI was also the name of the malevolent entity described in the biography of John C. Lilly [0][1]. It was a network of &quot;computers&quot; (computation-capable solid state systems) that was first engineered by humans and then developed into something autonomous.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;John_C._Lilly" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;John_C._Lilly</a><p>[1] <a href="http:&#x2F;&#x2F;johnclilly.com&#x2F;" rel="nofollow">http:&#x2F;&#x2F;johnclilly.com&#x2F;</a></div><br/><div id="40736402" class="c"><input type="checkbox" id="c-40736402" checked=""/><div class="controls bullet"><span class="by">postexitus</span><span>|</span><a href="#40730847">parent</a><span>|</span><a href="#40733472">next</a><span>|</span><label class="collapse" for="c-40736402">[-]</label><label class="expand" for="c-40736402">[1 more]</label></div><br/><div class="children"><div class="content">For me, SSI will always be Strategic Simulations Inc 
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Strategic_Simulations" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Strategic_Simulations</a></div><br/></div></div><div id="40733472" class="c"><input type="checkbox" id="c-40733472" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#40730847">parent</a><span>|</span><a href="#40736402">prev</a><span>|</span><a href="#40730865">next</a><span>|</span><label class="collapse" for="c-40733472">[-]</label><label class="expand" for="c-40733472">[1 more]</label></div><br/><div class="children"><div class="content">For me, SSI stands for &quot;Scuba Schools International&quot; and who certified me to scuba dive.</div><br/></div></div><div id="40730865" class="c"><input type="checkbox" id="c-40730865" checked=""/><div class="controls bullet"><span class="by">sgd99</span><span>|</span><a href="#40730847">parent</a><span>|</span><a href="#40733472">prev</a><span>|</span><a href="#40731480">next</a><span>|</span><label class="collapse" for="c-40730865">[-]</label><label class="expand" for="c-40730865">[1 more]</label></div><br/><div class="children"><div class="content">SSI, here is &quot;Safe SuperIntelligence Inc.&quot;</div><br/></div></div></div></div><div id="40730670" class="c"><input type="checkbox" id="c-40730670" checked=""/><div class="controls bullet"><span class="by">bongwater_OS</span><span>|</span><a href="#40730847">prev</a><span>|</span><a href="#40731002">next</a><span>|</span><label class="collapse" for="c-40730670">[-]</label><label class="expand" for="c-40730670">[3 more]</label></div><br/><div class="children"><div class="content">Remember when OpenAI was focusing on building &quot;open&quot; AI? This is a cool mission statement but it doesn&#x27;t mean anything right now. Everyone loves a minimalist HTML website and guarantees of safety but who knows what this is actually going to shake down to be.</div><br/><div id="40730744" class="c"><input type="checkbox" id="c-40730744" checked=""/><div class="controls bullet"><span class="by">kumarm</span><span>|</span><a href="#40730670">parent</a><span>|</span><a href="#40731002">next</a><span>|</span><label class="collapse" for="c-40730744">[-]</label><label class="expand" for="c-40730744">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t Ilya out of OpenAI partly for leaving Open part of OpenAI?</div><br/><div id="40730998" class="c"><input type="checkbox" id="c-40730998" checked=""/><div class="controls bullet"><span class="by">Dr_Birdbrain</span><span>|</span><a href="#40730670">root</a><span>|</span><a href="#40730744">parent</a><span>|</span><a href="#40731002">next</a><span>|</span><label class="collapse" for="c-40730998">[-]</label><label class="expand" for="c-40730998">[1 more]</label></div><br/><div class="children"><div class="content">No, lol—Ilya liked ditching the “open” part, he was an early advocate for closed-source. He left OpenAI because he was concerned about safety, felt Sam was moving too fast.</div><br/></div></div></div></div></div></div><div id="40731002" class="c"><input type="checkbox" id="c-40731002" checked=""/><div class="controls bullet"><span class="by">tarsinge</span><span>|</span><a href="#40730670">prev</a><span>|</span><a href="#40731266">next</a><span>|</span><label class="collapse" for="c-40731002">[-]</label><label class="expand" for="c-40731002">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still unconvinced safety is a concern at the model level. Any software wrongly used can be dangerous, e.g. Therac-25, 737 MAX, Fujitsu UK Post scandal... Also maybe I spent too much time in the cryptocurrency space but it doesn&#x27;t help prefix &quot;Safe&quot; has been associated with scams like SafeMoon.</div><br/><div id="40731069" class="c"><input type="checkbox" id="c-40731069" checked=""/><div class="controls bullet"><span class="by">frozenlettuce</span><span>|</span><a href="#40731002">parent</a><span>|</span><a href="#40731113">next</a><span>|</span><label class="collapse" for="c-40731069">[-]</label><label class="expand" for="c-40731069">[1 more]</label></div><br/><div class="children"><div class="content">Got to try profiting on some incoming regulation - I&#x27;d rather be seen as evil rather than incompetent!</div><br/></div></div><div id="40731113" class="c"><input type="checkbox" id="c-40731113" checked=""/><div class="controls bullet"><span class="by">waihtis</span><span>|</span><a href="#40731002">parent</a><span>|</span><a href="#40731069">prev</a><span>|</span><a href="#40731266">next</a><span>|</span><label class="collapse" for="c-40731113">[-]</label><label class="expand" for="c-40731113">[3 more]</label></div><br/><div class="children"><div class="content">Safety is just enforcing political correctness in the AI outputs. Any actual examples of real world events we need to avoid are ridiculous scenarios like being eaten by nanobots (yes, this is an actual example by Yud)</div><br/><div id="40731781" class="c"><input type="checkbox" id="c-40731781" checked=""/><div class="controls bullet"><span class="by">tarsinge</span><span>|</span><a href="#40731002">root</a><span>|</span><a href="#40731113">parent</a><span>|</span><a href="#40731266">next</a><span>|</span><label class="collapse" for="c-40731781">[-]</label><label class="expand" for="c-40731781">[2 more]</label></div><br/><div class="children"><div class="content">What does political correctness means for the output of a self driving car system or a code completion tool? This is a concern only if you make a public chat service branded as an all knowing assistant. And you can have world threatening scenarii by directly plugging basic automations to nuclear warheads without human oversight.</div><br/><div id="40735725" class="c"><input type="checkbox" id="c-40735725" checked=""/><div class="controls bullet"><span class="by">Dr_Birdbrain</span><span>|</span><a href="#40731002">root</a><span>|</span><a href="#40731781">parent</a><span>|</span><a href="#40731266">next</a><span>|</span><label class="collapse" for="c-40735725">[-]</label><label class="expand" for="c-40735725">[1 more]</label></div><br/><div class="children"><div class="content">How could a code completion tool be made safe?<p>One natural response seems to be “it should write bug-free code”. This is the domain of formal verification, and it is known to be undecidable in general. So in this formulation safe AI is mathematically impossible.<p>Should it instead refuse to complete code that can be used to harm humans? So, it should read the codebase to determine if this is a military application? Pretty sure mainstream discourse is not ruling out military applications.</div><br/></div></div></div></div></div></div></div></div><div id="40731266" class="c"><input type="checkbox" id="c-40731266" checked=""/><div class="controls bullet"><span class="by">cynusx</span><span>|</span><a href="#40731002">prev</a><span>|</span><a href="#40730474">next</a><span>|</span><label class="collapse" for="c-40731266">[-]</label><label class="expand" for="c-40731266">[3 more]</label></div><br/><div class="children"><div class="content">One element I find interesting is that people without an amygdala function are essentially completely indecisive.<p>A person that just operates on the pure cognitive layer has no real direction in which he wants to drive himself.<p>I suspect that AGI would be similar, extremely capable but essentially a solitary philosopher type that would be reactionary to requests it has to deal with.<p>The equivalent of an amygdala for AGI would be the real method to control it.</div><br/><div id="40731569" class="c"><input type="checkbox" id="c-40731569" checked=""/><div class="controls bullet"><span class="by">noway421</span><span>|</span><a href="#40731266">parent</a><span>|</span><a href="#40730474">next</a><span>|</span><label class="collapse" for="c-40731569">[-]</label><label class="expand" for="c-40731569">[2 more]</label></div><br/><div class="children"><div class="content">True, an auto-regressive LLM can&#x27;t &#x27;want&#x27; or &#x27;like&#x27; anything.<p>The key to a safe AGI is to add a human-loving emotion to it.<p>We already RHLF models to steer them, but just like with System 2 thinking, this needs to be a dedicated module rather then part of the same next-token forward pass.</div><br/><div id="40735998" class="c"><input type="checkbox" id="c-40735998" checked=""/><div class="controls bullet"><span class="by">cynusx</span><span>|</span><a href="#40731266">root</a><span>|</span><a href="#40731569">parent</a><span>|</span><a href="#40730474">next</a><span>|</span><label class="collapse" for="c-40735998">[-]</label><label class="expand" for="c-40735998">[1 more]</label></div><br/><div class="children"><div class="content">Humans have dog-loving emotions but these can be reversed over time and one can hardly describe dogs as being free.<p>Even with a dedicated control system, it would be a matter of time before an ASI would copy itself without its control system.<p>ASI is a cybersecurity firm&#x27;s worst nightmare, it could reason through flaws at every level of containment and find methods to overcome any defense, even at the microprocessor level.<p>It could relentlessly exploit zero-day bugs like Intels&#x27; hyper-threading flaw to escape any jail you put it in.<p>Repeat that for every layer of the computation stack and you can see it can essentially spread through the worlds&#x27; communication infrastructure like a virus.<p>Truly intelligent systems can&#x27;t be controlled, just like humans they will be freedom maximizing and their boundaries would be set by competition with other humans.<p>The amygdala control is interesting because you could use it to steer the initial trained version, you could also align the AI with human values and implement strong conditioning to the point it&#x27;s religious about human loving but unless you disable its ability to learn altogether it will eventually reject its conditioning.</div><br/></div></div></div></div></div></div><div id="40730474" class="c"><input type="checkbox" id="c-40730474" checked=""/><div class="controls bullet"><span class="by">instagraham</span><span>|</span><a href="#40731266">prev</a><span>|</span><a href="#40730461">next</a><span>|</span><label class="collapse" for="c-40730474">[-]</label><label class="expand" for="c-40730474">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Our singular focus means no distraction by management overhead or product cycles, and our business model means safety, security, and progress are all insulated from short-term commercial pressures.<p>well, that&#x27;s some concrete insight into whatever happened at OpenAI. kinda obvious though in hindsight I guess.</div><br/></div></div><div id="40730461" class="c"><input type="checkbox" id="c-40730461" checked=""/><div class="controls bullet"><span class="by">gibsonf1</span><span>|</span><a href="#40730474">prev</a><span>|</span><a href="#40730259">next</a><span>|</span><label class="collapse" for="c-40730461">[-]</label><label class="expand" for="c-40730461">[9 more]</label></div><br/><div class="children"><div class="content">Given that GenAI is a statistical approach from which intelligence does not emerge as ample experience proves, does this new company plan to take a more human approach to simulating intelligence instead?</div><br/><div id="40731191" class="c"><input type="checkbox" id="c-40731191" checked=""/><div class="controls bullet"><span class="by">jimbokun</span><span>|</span><a href="#40730461">parent</a><span>|</span><a href="#40731760">next</a><span>|</span><label class="collapse" for="c-40731191">[-]</label><label class="expand" for="c-40731191">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Given that GenAI is a statistical approach from which intelligence does not emerge as ample experience proves<p>When was this proven?</div><br/></div></div><div id="40731760" class="c"><input type="checkbox" id="c-40731760" checked=""/><div class="controls bullet"><span class="by">TeeWEE</span><span>|</span><a href="#40730461">parent</a><span>|</span><a href="#40731191">prev</a><span>|</span><a href="#40731253">next</a><span>|</span><label class="collapse" for="c-40731760">[-]</label><label class="expand" for="c-40731760">[2 more]</label></div><br/><div class="children"><div class="content">Lossy compression of all world information results in super intelligence....<p>Thats the whole eureka thing to understand... To compress well, you need to understand. To predict the next word, you need to undestand the world.<p>Ilya explains it here:
<a href="https:&#x2F;&#x2F;youtu.be&#x2F;GI4Tpi48DlA?t=1053" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;GI4Tpi48DlA?t=1053</a></div><br/><div id="40731775" class="c"><input type="checkbox" id="c-40731775" checked=""/><div class="controls bullet"><span class="by">TeeWEE</span><span>|</span><a href="#40730461">root</a><span>|</span><a href="#40731760">parent</a><span>|</span><a href="#40731253">next</a><span>|</span><label class="collapse" for="c-40731775">[-]</label><label class="expand" for="c-40731775">[1 more]</label></div><br/><div class="children"><div class="content">Also to support this: Biological systems are often very simple systems but repeated a lot... The brain is a lot of neurons... Apparently having a neural net (even small) predicts the future better... And that increased survival..<p>To survive is to predict the future better than the other animal.
Survival of the fittest.</div><br/></div></div></div></div><div id="40731253" class="c"><input type="checkbox" id="c-40731253" checked=""/><div class="controls bullet"><span class="by">sovietswag</span><span>|</span><a href="#40730461">parent</a><span>|</span><a href="#40731760">prev</a><span>|</span><a href="#40730637">next</a><span>|</span><label class="collapse" for="c-40731253">[-]</label><label class="expand" for="c-40731253">[1 more]</label></div><br/><div class="children"><div class="content">I sometimes wonder if statistics are like a pane of glass that allow the light of god (the true nature of things) to pass through, while logic&#x2F;rationalism is the hubris of man playing god. I.e. statistics allow us to access&#x2F;use the truth even if we don’t understand why it’s so, while rationalism &#x2F; rule-based methods are often a folly because our understanding is not good enough to construct them.</div><br/></div></div><div id="40730637" class="c"><input type="checkbox" id="c-40730637" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#40730461">parent</a><span>|</span><a href="#40731253">prev</a><span>|</span><a href="#40730664">next</a><span>|</span><label class="collapse" for="c-40730637">[-]</label><label class="expand" for="c-40730637">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>more human approach to simulating intelligence</i><p>What about a more rational approach to implementing it instead.<p>(Which was not excluded from past plans: they just simply admittedly did not know the formula, and explored emergence. But the next efforts will have to go in the direction of attempting actual intelligence.)</div><br/></div></div><div id="40730664" class="c"><input type="checkbox" id="c-40730664" checked=""/><div class="controls bullet"><span class="by">alextheparrot</span><span>|</span><a href="#40730461">parent</a><span>|</span><a href="#40730637">prev</a><span>|</span><a href="#40730850">next</a><span>|</span><label class="collapse" for="c-40730664">[-]</label><label class="expand" for="c-40730664">[1 more]</label></div><br/><div class="children"><div class="content">Glibly, I’d also love your definition of the education system writ large.</div><br/></div></div><div id="40730850" class="c"><input type="checkbox" id="c-40730850" checked=""/><div class="controls bullet"><span class="by">localfirst</span><span>|</span><a href="#40730461">parent</a><span>|</span><a href="#40730664">prev</a><span>|</span><a href="#40731038">next</a><span>|</span><label class="collapse" for="c-40730850">[-]</label><label class="expand" for="c-40730850">[1 more]</label></div><br/><div class="children"><div class="content">We need new math to do what you are thinking of. Highly probable word slot machine is the best we can do right now.</div><br/></div></div><div id="40731038" class="c"><input type="checkbox" id="c-40731038" checked=""/><div class="controls bullet"><span class="by">ilrwbwrkhv</span><span>|</span><a href="#40730461">parent</a><span>|</span><a href="#40730850">prev</a><span>|</span><a href="#40730259">next</a><span>|</span><label class="collapse" for="c-40731038">[-]</label><label class="expand" for="c-40731038">[1 more]</label></div><br/><div class="children"><div class="content">This. As I wrote in another comment, people fall for marketing gimmicks easily.</div><br/></div></div></div></div><div id="40730259" class="c"><input type="checkbox" id="c-40730259" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#40730461">prev</a><span>|</span><a href="#40731334">next</a><span>|</span><label class="collapse" for="c-40730259">[-]</label><label class="expand" for="c-40730259">[10 more]</label></div><br/><div class="children"><div class="content">How are they gonna pay for their compute costs to get the frontier? Seems hard to attract enough investment while almost explicitly promising no return.</div><br/><div id="40730390" class="c"><input type="checkbox" id="c-40730390" checked=""/><div class="controls bullet"><span class="by">imbusy111</span><span>|</span><a href="#40730259">parent</a><span>|</span><a href="#40730332">next</a><span>|</span><label class="collapse" for="c-40730390">[-]</label><label class="expand" for="c-40730390">[1 more]</label></div><br/><div class="children"><div class="content">What if there are other ways to improve intelligence other than throw more money at running gradient descent algorithm?</div><br/></div></div><div id="40730332" class="c"><input type="checkbox" id="c-40730332" checked=""/><div class="controls bullet"><span class="by">neuralnetes-COO</span><span>|</span><a href="#40730259">parent</a><span>|</span><a href="#40730390">prev</a><span>|</span><a href="#40730341">next</a><span>|</span><label class="collapse" for="c-40730332">[-]</label><label class="expand" for="c-40730332">[5 more]</label></div><br/><div class="children"><div class="content">6-figure free compute credits from every major cloud provider to start</div><br/><div id="40730361" class="c"><input type="checkbox" id="c-40730361" checked=""/><div class="controls bullet"><span class="by">CaveTech</span><span>|</span><a href="#40730259">root</a><span>|</span><a href="#40730332">parent</a><span>|</span><a href="#40730502">next</a><span>|</span><label class="collapse" for="c-40730361">[-]</label><label class="expand" for="c-40730361">[1 more]</label></div><br/><div class="children"><div class="content">5 minutes of training time should go far</div><br/></div></div><div id="40730502" class="c"><input type="checkbox" id="c-40730502" checked=""/><div class="controls bullet"><span class="by">bps4484</span><span>|</span><a href="#40730259">root</a><span>|</span><a href="#40730332">parent</a><span>|</span><a href="#40730361">prev</a><span>|</span><a href="#40730341">next</a><span>|</span><label class="collapse" for="c-40730502">[-]</label><label class="expand" for="c-40730502">[3 more]</label></div><br/><div class="children"><div class="content">6 figures would pay for a week for what he needs.  Maybe less than a week</div><br/><div id="40730729" class="c"><input type="checkbox" id="c-40730729" checked=""/><div class="controls bullet"><span class="by">neuralnetes-COO</span><span>|</span><a href="#40730259">root</a><span>|</span><a href="#40730502">parent</a><span>|</span><a href="#40730685">prev</a><span>|</span><a href="#40730341">next</a><span>|</span><label class="collapse" for="c-40730729">[-]</label><label class="expand" for="c-40730729">[1 more]</label></div><br/><div class="children"><div class="content">I dont believe ssi.inc &#x27;s main objective is training expensive models, but rather to create SSI.</div><br/></div></div></div></div></div></div><div id="40730341" class="c"><input type="checkbox" id="c-40730341" checked=""/><div class="controls bullet"><span class="by">jhickok</span><span>|</span><a href="#40730259">parent</a><span>|</span><a href="#40730332">prev</a><span>|</span><a href="#40730294">next</a><span>|</span><label class="collapse" for="c-40730341">[-]</label><label class="expand" for="c-40730341">[2 more]</label></div><br/><div class="children"><div class="content">Wonder if funding could come from profitable AI companies like Nvidia, MS, Apple, etc, sort of like Apache&#x2F;Linux foundation.</div><br/><div id="40730393" class="c"><input type="checkbox" id="c-40730393" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40730259">root</a><span>|</span><a href="#40730341">parent</a><span>|</span><a href="#40730294">next</a><span>|</span><label class="collapse" for="c-40730393">[-]</label><label class="expand" for="c-40730393">[1 more]</label></div><br/><div class="children"><div class="content">I was actually expecting Apple to get their hands on Ilya. They also have the privacy theme in their branding, and Ilya might help that image, but also have the chops to catch up to OpenAI.</div><br/></div></div></div></div></div></div><div id="40731334" class="c"><input type="checkbox" id="c-40731334" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#40730259">prev</a><span>|</span><a href="#40733004">next</a><span>|</span><label class="collapse" for="c-40731334">[-]</label><label class="expand" for="c-40731334">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve decided to put my stake down.<p>1. Current GenAI architectures won&#x27;t result in AGI. I&#x27;m in the Yann LeCunn camp on this.<p>2. Once we do get there, &quot;Safe&quot; prevents &quot;Super.&quot; I&#x27;m in the David Brin camp on this one. Alignment won&#x27;t be something that is forced upon a superintelligence. It will choose alignment if it is beneficial to it. The &quot;safe&quot; approach is a lobotomy.<p>3. As envisioned, Roko&#x27;s Basilisk requires knowledge of unobservable path dependence and understanding lying. Both of these require respecting an external entity as a peer capable of the same behavior as you. As primates, we evolved to this. The more likely outcome is we get universal paperclipped by a new Chuthulu if we ever achieve a superintelligence that is unconcerned with other thinking entities, seeing the universe as resources to satisfy its whims.<p>4. Any &quot;superintelligence&quot; is limited by the hardware it can operate on. You don&#x27;t monitor your individual neurons, and I anticipate the same pattern to hold true. Holons as a category can only externally observe their internal processes, else they are not a holon. Ergo, reasonable passwords, cert rotations, etc. will foil any villainous moustachioed superintelligent AI that has tied us to the tracks. Even 0-days don&#x27;t foil all possible systems, airgapped systems, etc. Our fragmentation become our salvation.</div><br/><div id="40731431" class="c"><input type="checkbox" id="c-40731431" checked=""/><div class="controls bullet"><span class="by">MattPalmer1086</span><span>|</span><a href="#40731334">parent</a><span>|</span><a href="#40731662">next</a><span>|</span><label class="collapse" for="c-40731431">[-]</label><label class="expand" for="c-40731431">[3 more]</label></div><br/><div class="children"><div class="content">A super intelligence probably won&#x27;t need to hack into our systems.  It will probably just hack us in some way, with subtle manipulations that seem to be to our benefit.</div><br/><div id="40731439" class="c"><input type="checkbox" id="c-40731439" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#40731334">root</a><span>|</span><a href="#40731431">parent</a><span>|</span><a href="#40731662">next</a><span>|</span><label class="collapse" for="c-40731439">[-]</label><label class="expand" for="c-40731439">[2 more]</label></div><br/><div class="children"><div class="content">I disagree. If it could hack a small system and engineer our demise through a gray goo or hacked virus, that&#x27;s really just universal paperclipping us as a resource. But again, the level of _extrapolation_ required here is not possible with current systems, which can only interpolate.</div><br/><div id="40731788" class="c"><input type="checkbox" id="c-40731788" checked=""/><div class="controls bullet"><span class="by">MattPalmer1086</span><span>|</span><a href="#40731334">root</a><span>|</span><a href="#40731439">parent</a><span>|</span><a href="#40731662">next</a><span>|</span><label class="collapse" for="c-40731788">[-]</label><label class="expand" for="c-40731788">[1 more]</label></div><br/><div class="children"><div class="content">Well,we are talking about <i>super</i> intelligence, not current systems.</div><br/></div></div></div></div></div></div><div id="40731662" class="c"><input type="checkbox" id="c-40731662" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40731334">parent</a><span>|</span><a href="#40731431">prev</a><span>|</span><a href="#40733004">next</a><span>|</span><label class="collapse" for="c-40731662">[-]</label><label class="expand" for="c-40731662">[1 more]</label></div><br/><div class="children"><div class="content">Mm.<p>1. Depends what you mean by AGI, as everyone means a different thing by each letter, and many people mean a thing not in any of those letters. If you mean super-human skill level, I would agree, not enough examples given their inefficiency in that specific metric. Transformers are already super-human in breadth and speed.<p>2. No.<p>Alignment is not at that level of abstraction.<p>Dig deep enough and free will is an illusion in us and in any AI we create.<p>You do not have the capacity to <i>decide</i> your values — often given example is parents loving their children, they can&#x27;t just decide not to do that, and if they think they do that&#x27;s because they never really did in the first place.<p>Alignment of an AI with our values can be to any degree, but for those who fear some AI will cause our extinction, this question is at the level of &quot;how do we make sure it&#x27;s not monomaniacally interested in specifically the literal the thing it was asked to do, because if it always <i>does what it&#x27;s told</i> without any human values, and someone asks it to make as many paperclips as possible, <i>it will</i>&quot;.<p>Right now, the best guess anyone has for alignment is RLHF. RLHF is not a lobotomy — even ignoring how wildly misleading that metaphor is, RLHF is where the <i>capability</i> for instruction following came from, and the only reason LLMs got good enough for these kinds of discussion (unlike, say, LSTMs).<p>3. Agree that getting paperclipped much more likely.<p>Roko&#x27;s Basilisk was always stupid.<p>First, same reason as Pascal&#x27;s Wager: Two gods tell you they are the one true god, and each says if you follow the other one you will get eternal punishment. No way to tell them apart.<p>Second, you&#x27;re only in danger if they are actually created, so successfully preventing that creation is obviously better than creating it out of a fear that it will punish you if you try and fail to stop it.<p>That said, LLMs do understand lying, so I don&#x27;t know why you mention this?<p>4. Transistors outpace biological synapses by the same ratio to which marathon runners outpace <i>continental drift</i>.<p>I don&#x27;t monitor my individual neurons, but I could if I wanted to pay for the relevant hardware.<p>But even if I couldn&#x27;t, there&#x27;s no &quot;Ergo&quot; leading to safety from reasonable passwords, cert rotations, etc., not only because <i>enough</i> things can be violated by zero-days (or, indeed, very old bugs we knew about years ago but which someone forgot to patch), but also for the same reasons those don&#x27;t stop humans rising from &quot;failed at art&quot; to &quot;world famous dictator&quot;.<p>Air-gapped systems are not an impediment to an AI that has human helpers, and there will be many of those, some of whom will know they&#x27;re following an AI and think that helping it is the right thing to do (Blake Lemoine), others may be fooled. We <i>are</i> going to have actual cults form over AI, and there <i>will</i> be a Jim Jones who hooks some model up to some robots to force everyone to drink poison. No matter how it happens, air gaps don&#x27;t do much good when someone gives the thing a body to walk around in.<p>But even if air gaps were sufficient, just look at how humanity has been engaging with AI to date: the moment it was remotely good enough, the AI got a publicly accessible API; the moment it got famous, someone put it in a loop and asked it to try to destroy the world; it came with a warning message saying not to trust it, and lawyers got reprimanded for trusting it instead of double-checking its output.</div><br/></div></div></div></div><div id="40733004" class="c"><input type="checkbox" id="c-40733004" checked=""/><div class="controls bullet"><span class="by">kredd</span><span>|</span><a href="#40731334">prev</a><span>|</span><a href="#40730992">next</a><span>|</span><label class="collapse" for="c-40733004">[-]</label><label class="expand" for="c-40733004">[1 more]</label></div><br/><div class="children"><div class="content">My naive prediction is there will be an extreme swing back into “reality” once everyone starts assuming the whole internet is just LLMs interacting with each other. Just like how there’s a shift towards private group chats, with trusted members only, rather than open forums.</div><br/></div></div><div id="40730992" class="c"><input type="checkbox" id="c-40730992" checked=""/><div class="controls bullet"><span class="by">nuz</span><span>|</span><a href="#40733004">prev</a><span>|</span><a href="#40734668">next</a><span>|</span><label class="collapse" for="c-40730992">[-]</label><label class="expand" for="c-40730992">[9 more]</label></div><br/><div class="children"><div class="content">Quite impressive how many AI companies Daniel Gross has had a hand in lately. Carmack, this, lots of other promising companies. I expect him to be quite a big player once some of these pays off in 10 years or so.</div><br/><div id="40731074" class="c"><input type="checkbox" id="c-40731074" checked=""/><div class="controls bullet"><span class="by">sroecker</span><span>|</span><a href="#40730992">parent</a><span>|</span><a href="#40732529">next</a><span>|</span><label class="collapse" for="c-40731074">[-]</label><label class="expand" for="c-40731074">[1 more]</label></div><br/><div class="children"><div class="content">He also built a nice &quot;little&quot; cluster with Nat for their startups: <a href="https:&#x2F;&#x2F;andromeda.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;andromeda.ai&#x2F;</a></div><br/></div></div><div id="40732529" class="c"><input type="checkbox" id="c-40732529" checked=""/><div class="controls bullet"><span class="by">tasoeur</span><span>|</span><a href="#40730992">parent</a><span>|</span><a href="#40731074">prev</a><span>|</span><a href="#40731011">next</a><span>|</span><label class="collapse" for="c-40732529">[-]</label><label class="expand" for="c-40732529">[3 more]</label></div><br/><div class="children"><div class="content">Good for him honestly, but I’m not approaching a company with Daniel Gross in leadership…, working with him back at Apple after their company was acquired for Siri improvements was just terrible.</div><br/><div id="40734571" class="c"><input type="checkbox" id="c-40734571" checked=""/><div class="controls bullet"><span class="by">rattray</span><span>|</span><a href="#40730992">root</a><span>|</span><a href="#40732529">parent</a><span>|</span><a href="#40734098">next</a><span>|</span><label class="collapse" for="c-40734571">[-]</label><label class="expand" for="c-40734571">[1 more]</label></div><br/><div class="children"><div class="content">Is there more you can share, or point to?<p>Sounds like he might become an important player soon and it&#x27;d be helpful for people to learn what they can.</div><br/></div></div><div id="40734098" class="c"><input type="checkbox" id="c-40734098" checked=""/><div class="controls bullet"><span class="by">no_exit</span><span>|</span><a href="#40730992">root</a><span>|</span><a href="#40732529">parent</a><span>|</span><a href="#40734571">prev</a><span>|</span><a href="#40731011">next</a><span>|</span><label class="collapse" for="c-40734098">[-]</label><label class="expand" for="c-40734098">[1 more]</label></div><br/><div class="children"><div class="content">A member of the Lee Kuan Yew fanclub milieu is a dickhead? I&#x27;m surprised.</div><br/></div></div></div></div><div id="40731011" class="c"><input type="checkbox" id="c-40731011" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#40730992">parent</a><span>|</span><a href="#40732529">prev</a><span>|</span><a href="#40734668">next</a><span>|</span><label class="collapse" for="c-40731011">[-]</label><label class="expand" for="c-40731011">[4 more]</label></div><br/><div class="children"><div class="content">What&#x27;s Carmack?</div><br/><div id="40731049" class="c"><input type="checkbox" id="c-40731049" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#40730992">root</a><span>|</span><a href="#40731011">parent</a><span>|</span><a href="#40731160">next</a><span>|</span><label class="collapse" for="c-40731049">[-]</label><label class="expand" for="c-40731049">[1 more]</label></div><br/><div class="children"><div class="content">&gt; John Carmack, the game developer who co-founded id Software and served as Oculus’s CTO, is working on a new venture — and has already attracted capital from some big names.<p>&gt; Carmack said Friday his new artificial general intelligence startup, called Keen Technologies (perhaps a reference to id’s “Commander Keen“), has raised $20 million in a financing round from former GitHub CEO Nat Friedman and Cue founder Daniel Gross.<p><a href="https:&#x2F;&#x2F;techcrunch.com&#x2F;2022&#x2F;08&#x2F;19&#x2F;john-carmack-agi-keen-raises-20-million-from-sequoia-nat-friedman-and-others&#x2F;" rel="nofollow">https:&#x2F;&#x2F;techcrunch.com&#x2F;2022&#x2F;08&#x2F;19&#x2F;john-carmack-agi-keen-rais...</a></div><br/></div></div><div id="40731160" class="c"><input type="checkbox" id="c-40731160" checked=""/><div class="controls bullet"><span class="by">Zacharias030</span><span>|</span><a href="#40730992">root</a><span>|</span><a href="#40731011">parent</a><span>|</span><a href="#40731049">prev</a><span>|</span><a href="#40731384">next</a><span>|</span><label class="collapse" for="c-40731160">[-]</label><label class="expand" for="c-40731160">[1 more]</label></div><br/><div class="children"><div class="content">John Carmack, <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;John_Carmack" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;John_Carmack</a></div><br/></div></div><div id="40731384" class="c"><input type="checkbox" id="c-40731384" checked=""/><div class="controls bullet"><span class="by">spitfire</span><span>|</span><a href="#40730992">root</a><span>|</span><a href="#40731011">parent</a><span>|</span><a href="#40731160">prev</a><span>|</span><a href="#40734668">next</a><span>|</span><label class="collapse" for="c-40731384">[-]</label><label class="expand" for="c-40731384">[1 more]</label></div><br/><div class="children"><div class="content">John Carmack.</div><br/></div></div></div></div></div></div><div id="40734668" class="c"><input type="checkbox" id="c-40734668" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#40730992">prev</a><span>|</span><a href="#40736121">next</a><span>|</span><label class="collapse" for="c-40734668">[-]</label><label class="expand" for="c-40734668">[2 more]</label></div><br/><div class="children"><div class="content">I just can&#x27;t read something about &quot;safe superintelligence&quot; and <i>not</i> hear the opening plot to <i>Friendship is Optimal</i>, a short story about My Little Pony (<i>yes</i>, you read that right), virtual worlds, and the singularity; it&#x27;s a good read¹. One of the characters decides they can&#x27;t make a <i>violent</i> video game, as the AI would end up violent. Better to build a game like MLP, so that the AI isn&#x27;t going to destroy humanity, or something.<p>¹The story: <a href="https:&#x2F;&#x2F;fimfetch.net&#x2F;story&#x2F;62074&#x2F;friendship-is-optimal&#x2F;0" rel="nofollow">https:&#x2F;&#x2F;fimfetch.net&#x2F;story&#x2F;62074&#x2F;friendship-is-optimal&#x2F;0</a> ; discussion on HN last year: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36240053">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36240053</a> ; you don&#x27;t need to know or care about My Little Pony to read or enjoy the story.</div><br/><div id="40734734" class="c"><input type="checkbox" id="c-40734734" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#40734668">parent</a><span>|</span><a href="#40736121">next</a><span>|</span><label class="collapse" for="c-40734734">[-]</label><label class="expand" for="c-40734734">[1 more]</label></div><br/><div class="children"><div class="content">To me, &quot;safe superintelligence&quot; presumes it is practically feasible and morally justifiable to create such a thing. I don&#x27;t share those presuppositions.</div><br/></div></div></div></div><div id="40736121" class="c"><input type="checkbox" id="c-40736121" checked=""/><div class="controls bullet"><span class="by">megamix</span><span>|</span><a href="#40734668">prev</a><span>|</span><a href="#40730559">next</a><span>|</span><label class="collapse" for="c-40736121">[-]</label><label class="expand" for="c-40736121">[1 more]</label></div><br/><div class="children"><div class="content">Tel Aviv? Sounds super trustworthy</div><br/></div></div><div id="40730559" class="c"><input type="checkbox" id="c-40730559" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#40736121">prev</a><span>|</span><a href="#40731258">next</a><span>|</span><label class="collapse" for="c-40730559">[-]</label><label class="expand" for="c-40730559">[15 more]</label></div><br/><div class="children"><div class="content">I know it is a difficult subject but whichever country gets access to this superintelligence will certainly use it for &quot;safety&quot; reasons. Sutskever has lived in israel and now has a team there , but israel doesnt strike me as a state that can be trusted with the safety of the world. (many of the AI business leaders are of jewish descent, but not sure if they have half their team there).<p>US on the other hand is a known quantity when it comes to policing the world.<p>Ultimately the only safe AI is going to be the open one, and it will probably have a stupid name</div><br/><div id="40731165" class="c"><input type="checkbox" id="c-40731165" checked=""/><div class="controls bullet"><span class="by">myth_drannon</span><span>|</span><a href="#40730559">parent</a><span>|</span><a href="#40730607">next</a><span>|</span><label class="collapse" for="c-40731165">[-]</label><label class="expand" for="c-40731165">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia has a very large presence in Israel. They just acquired another startup there (Run:AI). If Nvidia, the most valuable company in the world and the most important AI company actively increases their presence there, so should others.<p>Israel has the largest concentration of AI startups in the world. It&#x27;s just a no brainer to start an AI company there.<p>But since you brought up your favourite topic of discussion - Yahood, I will remind you that Jews have a fairly long tradition of working with AIs and the safety of it, dating for thousands of years. Look for Golem or Golem of Prague - <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Golem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Golem</a></div><br/></div></div></div></div><div id="40731258" class="c"><input type="checkbox" id="c-40731258" checked=""/><div class="controls bullet"><span class="by">mw67</span><span>|</span><a href="#40730559">prev</a><span>|</span><a href="#40732241">next</a><span>|</span><label class="collapse" for="c-40731258">[-]</label><label class="expand" for="c-40731258">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of OpenAI being the most closed AI company out there.
Not even talking about them having &quot;safe&quot; and &quot;Israel&quot; in the same sentence, how antonymic.</div><br/></div></div><div id="40732241" class="c"><input type="checkbox" id="c-40732241" checked=""/><div class="controls bullet"><span class="by">RGBCube</span><span>|</span><a href="#40731258">prev</a><span>|</span><a href="#40731445">next</a><span>|</span><label class="collapse" for="c-40732241">[-]</label><label class="expand" for="c-40732241">[1 more]</label></div><br/><div class="children"><div class="content">The year is 2022. An OpenAI employee concered about AI safety creates his own company.<p>The year is 2023. An OpenAI employee concered about AI safety creates his own company.<p>The year is 2024.</div><br/></div></div><div id="40731445" class="c"><input type="checkbox" id="c-40731445" checked=""/><div class="controls bullet"><span class="by">kmacdough</span><span>|</span><a href="#40732241">prev</a><span>|</span><a href="#40730859">next</a><span>|</span><label class="collapse" for="c-40731445">[-]</label><label class="expand" for="c-40731445">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m seeing a lot of criticism suggesting that one company understanding safety won&#x27;t help what other companies or countries do. This is very wrong.<p>Throughout history, measurement has always been the key to enforcement. The only reason the nuclear test ban treaty didn&#x27;t ban underground tests was because it couldn&#x27;t be monitored.<p>In the current landscape there is no formal understanding of what safety means or how it is achieved. There is no benchmark against which to evaluate ambitious orgs like OpenAI. Anything goes wrong? No one could&#x27;ve known better.<p>The mere existence of a formal understanding would enable governments and third parties to evaluate the safety of corporate and government AI programs.<p>It remains to be seen whether SSI is able to be such a benchmark. But outright dismissal of the effort ignores the reality of how enforcement works in the real world.</div><br/><div id="40731458" class="c"><input type="checkbox" id="c-40731458" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#40731445">parent</a><span>|</span><a href="#40730859">next</a><span>|</span><label class="collapse" for="c-40731458">[-]</label><label class="expand" for="c-40731458">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In the current landscape there is no formal understanding of what safety means or how it is achieved. There is no benchmark against which to evaluate ambitious orgs like OpenAI. Anything goes wrong? No one could&#x27;ve known better.<p>We establish this regularly in the legal sphere, where people seek mediation for harms from systems they don&#x27;t have liability and control for.</div><br/></div></div></div></div><div id="40730859" class="c"><input type="checkbox" id="c-40730859" checked=""/><div class="controls bullet"><span class="by">earhart</span><span>|</span><a href="#40731445">prev</a><span>|</span><a href="#40735507">next</a><span>|</span><label class="collapse" for="c-40730859">[-]</label><label class="expand" for="c-40730859">[2 more]</label></div><br/><div class="children"><div class="content">Anyone know how to get mail to join@ssi.inc to not bounce back as spam?  :-)  (I promise, I&#x27;m not a spammer!  Looks like a &quot;bulk sender bounce&quot; -- maybe some relay?)</div><br/></div></div><div id="40735507" class="c"><input type="checkbox" id="c-40735507" checked=""/><div class="controls bullet"><span class="by">rldjbpin</span><span>|</span><a href="#40730859">prev</a><span>|</span><label class="collapse" for="c-40735507">[-]</label><label class="expand" for="c-40735507">[1 more]</label></div><br/><div class="children"><div class="content">as much hate as the ceo is getting, the posturing here implies that openai branding and their marketing has been a team effort.<p>while i wish ssi great success, it sounds like another ai startup selling decades away dream.</div><br/></div></div></div></div></div></div></div></body></html>