<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719219674315" as="style"/><link rel="stylesheet" href="styles.css?v=1719219674315"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a> <span class="domain">(<a href="https://www.nature.com">www.nature.com</a>)</span></div><div class="subtext"><span>Tomte</span> | <span>88 comments</span></div><br/><div><div id="40772262" class="c"><input type="checkbox" id="c-40772262" checked=""/><div class="controls bullet"><span class="by">badrunaway</span><span>|</span><a href="#40772035">next</a><span>|</span><label class="collapse" for="c-40772262">[-]</label><label class="expand" for="c-40772262">[4 more]</label></div><br/><div class="children"><div class="content">Current architecture of LLMs focus mainly on the retrieval part and the weights learned are just converged to get best outcome for next token prediction. Whereas,  ability to put this data into a logical system should also have been a training goal IMO. Next token prediction + Formal Verification of knowledge during training phase itself = that would give LLM ability to keep consistency in it&#x27;s knowledge generation and see the right hallucinations (which I like to call imagination)<p>The process can look like-<p>1. Use existing large models to convert the same previous dataset they were trained on into formal logical relationships. Let them generate multiple solutions<p>2. Take this enriched dataset and train a new LLM which not only outputs next token but also a the formal relationships between previous knowledge and the new generated text<p>3. Network can optimize weights until the generated formal code get high accuracy on proof checker along with the token generation accuracy function<p>In my own mind I feel language is secondary - it&#x27;s not the base of my intelligence. Base seems more like a dreamy simulation where things are consistent with each other and language is just what i use to describe it.</div><br/><div id="40772924" class="c"><input type="checkbox" id="c-40772924" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#40772262">parent</a><span>|</span><a href="#40773615">next</a><span>|</span><label class="collapse" for="c-40772924">[-]</label><label class="expand" for="c-40772924">[2 more]</label></div><br/><div class="children"><div class="content">But the problem is with the new stuff it hasn&#x27;t seen, and questions humans don&#x27;t know the answers to. It feels like this whole hallucinations thing is just the halting problem with extra steps. Maybe we should ask ChatGPT whether P=NP :)</div><br/><div id="40773294" class="c"><input type="checkbox" id="c-40773294" checked=""/><div class="controls bullet"><span class="by">wizardforhire</span><span>|</span><a href="#40772262">root</a><span>|</span><a href="#40772924">parent</a><span>|</span><a href="#40773615">next</a><span>|</span><label class="collapse" for="c-40773294">[-]</label><label class="expand" for="c-40773294">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but when you come to halting problems on that level of complexity multi-hierarchical-emergent phenomena occur aperiodically and chaotically that is to say in the frequency domain the aperiodicity is fractal like, discreet and mappable.</div><br/></div></div></div></div><div id="40773615" class="c"><input type="checkbox" id="c-40773615" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#40772262">parent</a><span>|</span><a href="#40772924">prev</a><span>|</span><a href="#40772035">next</a><span>|</span><label class="collapse" for="c-40773615">[-]</label><label class="expand" for="c-40773615">[1 more]</label></div><br/><div class="children"><div class="content">What is the formal logical system?<p>Eg, KGs (RDF, PGs, ...) are logical, but in automated construction, are not semantic in the sense of the ground domain of NLP, and in manual construction, tiny ontology. Conversely, fancy powerful logics like modal ones are even less semantic in NLP domains. Code is more expressive, but brings its own issues.</div><br/></div></div></div></div><div id="40772035" class="c"><input type="checkbox" id="c-40772035" checked=""/><div class="controls bullet"><span class="by">jasonlfunk</span><span>|</span><a href="#40772262">prev</a><span>|</span><a href="#40771098">next</a><span>|</span><label class="collapse" for="c-40772035">[-]</label><label class="expand" for="c-40772035">[28 more]</label></div><br/><div class="children"><div class="content">Isn’t it true that the only thing that LLM’s do is “hallucinate”?<p>The only way to know if it did “hallucinate” is to already know the correct answer. If you can make a system that knows when an answer is right or not, you no longer need the LLM!</div><br/><div id="40772617" class="c"><input type="checkbox" id="c-40772617" checked=""/><div class="controls bullet"><span class="by">pvillano</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40772927">next</a><span>|</span><label class="collapse" for="c-40772617">[-]</label><label class="expand" for="c-40772617">[6 more]</label></div><br/><div class="children"><div class="content">Hallucination implies a failure of an otherwise sound mind. What current LLMs do is better described as bullshitting. As the bullshitting improves, it happens to be correct a greater and greater percentage of the time</div><br/><div id="40773460" class="c"><input type="checkbox" id="c-40773460" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772617">parent</a><span>|</span><a href="#40773376">next</a><span>|</span><label class="collapse" for="c-40773460">[-]</label><label class="expand" for="c-40773460">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes when I am narrating a story I don&#x27;t care that much about trivial details but focus on the connection between those details. Is there LLM counterpart to such a behaviour? In this case, one can say I was bullshitting on the trivial details.</div><br/></div></div><div id="40773376" class="c"><input type="checkbox" id="c-40773376" checked=""/><div class="controls bullet"><span class="by">idle_zealot</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772617">parent</a><span>|</span><a href="#40773460">prev</a><span>|</span><a href="#40772927">next</a><span>|</span><label class="collapse" for="c-40773376">[-]</label><label class="expand" for="c-40773376">[4 more]</label></div><br/><div class="children"><div class="content">At what ratio of correctness:nonsense does it cease to be bullshitting? Or is there no tipping point so long as the source is a generative model?</div><br/><div id="40773470" class="c"><input type="checkbox" id="c-40773470" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40773376">parent</a><span>|</span><a href="#40772927">next</a><span>|</span><label class="collapse" for="c-40773470">[-]</label><label class="expand" for="c-40773470">[3 more]</label></div><br/><div class="children"><div class="content">It has nothing to do with ratio and to do with intent. Bullshitting is what we say you do when you just spin a story with no care for the truth, just make up stuff that sound plausible. That is what LLMs do today, and what they will always do as long as we don&#x27;t train them to care about the truth.<p>You can have a generative model that cares about the truth when it tries to generate responses, its just the current LLMs don&#x27;t.</div><br/><div id="40773659" class="c"><input type="checkbox" id="c-40773659" checked=""/><div class="controls bullet"><span class="by">Ma8ee</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40773470">parent</a><span>|</span><a href="#40772927">next</a><span>|</span><label class="collapse" for="c-40773659">[-]</label><label class="expand" for="c-40773659">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You can have a generative model that cares about the truth when it tries to generate responses, its just the current LLMs don&#x27;t.<p>How would you do that, when they don’t have any concept of truth to start with (or any concepts at all).</div><br/><div id="40773753" class="c"><input type="checkbox" id="c-40773753" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40773659">parent</a><span>|</span><a href="#40772927">next</a><span>|</span><label class="collapse" for="c-40773753">[-]</label><label class="expand" for="c-40773753">[1 more]</label></div><br/><div class="children"><div class="content">You can program a concept of truth into them, or maybe punishing it for making mistakes instead of just rewarding it for replicating text. Nobody knows how to do that in a way that get intelligent results today, but we know how to code things that outputs or checks truths in other contexts, like wolfram alpha is capable of solving tons of things and isn&#x27;t wrong.<p>&gt; (or any concepts at all).<p>Nobody here said that, that is your interpretation. Not everyone who is skeptical of current LLM architectures future potential as AGI thinks that computers are unable to solve these things. Most here who argues against LLM don&#x27;t think the problems are unsolvable, just not solvable by the current style of LLMs.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40772927" class="c"><input type="checkbox" id="c-40772927" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40772617">prev</a><span>|</span><a href="#40772660">next</a><span>|</span><label class="collapse" for="c-40772927">[-]</label><label class="expand" for="c-40772927">[1 more]</label></div><br/><div class="children"><div class="content">I had this perfect mosquito repellent - all you had to do was catch the mosquito and spray the solution into his eyes blinding him immediately.</div><br/></div></div><div id="40772660" class="c"><input type="checkbox" id="c-40772660" checked=""/><div class="controls bullet"><span class="by">mistercow</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40772927">prev</a><span>|</span><a href="#40773651">next</a><span>|</span><label class="collapse" for="c-40772660">[-]</label><label class="expand" for="c-40772660">[7 more]</label></div><br/><div class="children"><div class="content">Does every thread about this topic have to have someone quibbling about the word “hallucination”, which is already an established term of art with a well understood meaning? It’s getting exhausting.</div><br/><div id="40773718" class="c"><input type="checkbox" id="c-40773718" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772660">parent</a><span>|</span><a href="#40772937">next</a><span>|</span><label class="collapse" for="c-40773718">[-]</label><label class="expand" for="c-40773718">[1 more]</label></div><br/><div class="children"><div class="content">you stole a term which means something else in an established domain and now assert that the ship has sailed, whereas a perfectly valid term in both domains exists. don&#x27;t be a lazy smartass.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Confabulation" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Confabulation</a></div><br/></div></div><div id="40772937" class="c"><input type="checkbox" id="c-40772937" checked=""/><div class="controls bullet"><span class="by">keiferski</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772660">parent</a><span>|</span><a href="#40773718">prev</a><span>|</span><a href="#40773792">next</a><span>|</span><label class="collapse" for="c-40772937">[-]</label><label class="expand" for="c-40772937">[3 more]</label></div><br/><div class="children"><div class="content">The term <i>hallucination</i> is a fundamental misunderstanding of how LLMs work, and continuing to use it will ultimately result in a confused picture of what AI and AGI are and what is &quot;actually happening&quot; under the hood.<p>Wanting to use accurate language isn&#x27;t exhausting, it&#x27;s a requirement if you want to think about and discuss problems clearly.</div><br/><div id="40773344" class="c"><input type="checkbox" id="c-40773344" checked=""/><div class="controls bullet"><span class="by">phist_mcgee</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772937">parent</a><span>|</span><a href="#40773792">next</a><span>|</span><label class="collapse" for="c-40773344">[-]</label><label class="expand" for="c-40773344">[2 more]</label></div><br/><div class="children"><div class="content">Arguing about semantics rarely keeps topics on track, e.g, my reply to your comment.</div><br/><div id="40773445" class="c"><input type="checkbox" id="c-40773445" checked=""/><div class="controls bullet"><span class="by">keiferski</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40773344">parent</a><span>|</span><a href="#40773792">next</a><span>|</span><label class="collapse" for="c-40773445">[-]</label><label class="expand" for="c-40773445">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Arguing about semantics&quot; implies that there is no real difference between calling something A vs. calling it B.<p>I don&#x27;t think that&#x27;s the case here: there is a very real difference between describing something with a model that implies one (false) thing vs. a model that doesn&#x27;t have that flaw.<p>If you don&#x27;t find that convincing, then consider this: by taking the time to properly define things at the beginning, you&#x27;ll save yourself a ton of time later on down the line – as you don&#x27;t need to untangle the mess that resulted from being sloppy with definitions at the start.<p>This is all a long way of saying that <i>aiming to clarify your thoughts</i> is not the same as <i>arguing pointlessly over definitions.</i></div><br/></div></div></div></div></div></div><div id="40773792" class="c"><input type="checkbox" id="c-40773792" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772660">parent</a><span>|</span><a href="#40772937">prev</a><span>|</span><a href="#40773024">next</a><span>|</span><label class="collapse" for="c-40773792">[-]</label><label class="expand" for="c-40773792">[1 more]</label></div><br/><div class="children"><div class="content">The paper itself talks about this, so yes?</div><br/></div></div><div id="40773024" class="c"><input type="checkbox" id="c-40773024" checked=""/><div class="controls bullet"><span class="by">DidYaWipe</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772660">parent</a><span>|</span><a href="#40773792">prev</a><span>|</span><a href="#40773651">next</a><span>|</span><label class="collapse" for="c-40773024">[-]</label><label class="expand" for="c-40773024">[1 more]</label></div><br/><div class="children"><div class="content">Does every completely legitimate condemnation of erroneous language have to be whined about by some apologist for linguistic erosion?</div><br/></div></div></div></div><div id="40773651" class="c"><input type="checkbox" id="c-40773651" checked=""/><div class="controls bullet"><span class="by">shiandow</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40772660">prev</a><span>|</span><a href="#40772130">next</a><span>|</span><label class="collapse" for="c-40773651">[-]</label><label class="expand" for="c-40773651">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;d read the aticle you might have noticed that generating answers with the LLM is very much part of the fact-checking process.</div><br/></div></div><div id="40772130" class="c"><input type="checkbox" id="c-40772130" checked=""/><div class="controls bullet"><span class="by">stoniejohnson</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40773651">prev</a><span>|</span><a href="#40772321">next</a><span>|</span><label class="collapse" for="c-40772130">[-]</label><label class="expand" for="c-40772130">[9 more]</label></div><br/><div class="children"><div class="content">All people do is confabulate too.<p>Sometimes it is coherent (grounded in physical and social dynamics) and sometimes it is not.<p>We need systems that <i>try to be coherent</i>, not systems that try to be unequivocally right, which wouldn&#x27;t be possible.</div><br/><div id="40772531" class="c"><input type="checkbox" id="c-40772531" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772130">parent</a><span>|</span><a href="#40772149">next</a><span>|</span><label class="collapse" for="c-40772531">[-]</label><label class="expand" for="c-40772531">[6 more]</label></div><br/><div class="children"><div class="content">&gt; We need systems that try to be coherent, not systems that try to be unequivocally right, which wouldn&#x27;t be possible.<p>The fact that it isn&#x27;t possible to be right about 100% of things doesn&#x27;t mean that you shouldn&#x27;t try to be right.<p>Humans generally try to be right, these models don&#x27;t, that is a massive difference you can&#x27;t ignore. The fact that humans often fails to be right doesn&#x27;t mean that these models shouldn&#x27;t even try to be right.</div><br/><div id="40772682" class="c"><input type="checkbox" id="c-40772682" checked=""/><div class="controls bullet"><span class="by">mrtesthah</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772531">parent</a><span>|</span><a href="#40772149">next</a><span>|</span><label class="collapse" for="c-40772682">[-]</label><label class="expand" for="c-40772682">[5 more]</label></div><br/><div class="children"><div class="content">By their nature, the models don’t ‘try’ to do anything at all—they’re just weights applied during inference, and the semantic features that are most prevalent in the training set will be most likely to be asserted as truth.</div><br/><div id="40772751" class="c"><input type="checkbox" id="c-40772751" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772682">parent</a><span>|</span><a href="#40773514">next</a><span>|</span><label class="collapse" for="c-40772751">[-]</label><label class="expand" for="c-40772751">[3 more]</label></div><br/><div class="children"><div class="content">They are trained to predict next word that is similar to the text they have seen, I call that what they &quot;try&quot; to do here. A chess AI tries to win since that is what it was encouraged to do during training, current LLM try to predict the next word since that is what they are trained to do, there is nothing wrong using that word.<p>This is an accurate usage of try, ML models at their core tries to maximize a score, so what that score represents is what they try to do. And there is no concept of truth in LLM training, just sequences of words, they have no score for true or false.<p>Edit: Humans are punished as kids for being wrong all throughout school and in most homes, that makes human try to be right. That is very different from these models that are just rewarded for mimicking regardless if it is right or wrong.</div><br/><div id="40773397" class="c"><input type="checkbox" id="c-40773397" checked=""/><div class="controls bullet"><span class="by">idle_zealot</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772751">parent</a><span>|</span><a href="#40773514">next</a><span>|</span><label class="collapse" for="c-40773397">[-]</label><label class="expand" for="c-40773397">[2 more]</label></div><br/><div class="children"><div class="content">&gt; That is very different from these models that are just rewarded for mimicking regardless if it is right or wrong<p>That&#x27;s not a totally accurate characterization. The base models are just trained to predict plausible text, but then the models are fine-tuned on instruct or chat training data that encourages a certain &quot;attitude&quot; and correctness. It&#x27;s far from perfect, but an attempt is certainly made to train them to be right.</div><br/><div id="40773505" class="c"><input type="checkbox" id="c-40773505" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40773397">parent</a><span>|</span><a href="#40773514">next</a><span>|</span><label class="collapse" for="c-40773505">[-]</label><label class="expand" for="c-40773505">[1 more]</label></div><br/><div class="children"><div class="content">They are trained to replicate text semantically and then given a lot of correct statements to replicate, that is very different from being trained to be correct. That makes them more useful and less incorrect, but they still don&#x27;t have a concept of correctness trained into them.</div><br/></div></div></div></div></div></div><div id="40773514" class="c"><input type="checkbox" id="c-40773514" checked=""/><div class="controls bullet"><span class="by">shinycode</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772682">parent</a><span>|</span><a href="#40772751">prev</a><span>|</span><a href="#40772149">next</a><span>|</span><label class="collapse" for="c-40773514">[-]</label><label class="expand" for="c-40773514">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, if a massive data poisoning would happen, will the AI be able to know what’s the truth is there is as much new false information than there is real one ? It won’t be able to reason about it</div><br/></div></div></div></div></div></div><div id="40772149" class="c"><input type="checkbox" id="c-40772149" checked=""/><div class="controls bullet"><span class="by">android521</span><span>|</span><a href="#40772035">root</a><span>|</span><a href="#40772130">parent</a><span>|</span><a href="#40772531">prev</a><span>|</span><a href="#40772321">next</a><span>|</span><label class="collapse" for="c-40772149">[-]</label><label class="expand" for="c-40772149">[2 more]</label></div><br/><div class="children"><div class="content">It is an unsolved problem for humans .</div><br/></div></div></div></div><div id="40772321" class="c"><input type="checkbox" id="c-40772321" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40772130">prev</a><span>|</span><a href="#40772989">next</a><span>|</span><label class="collapse" for="c-40772321">[-]</label><label class="expand" for="c-40772321">[1 more]</label></div><br/><div class="children"><div class="content">This isn’t true in the way many np problems are difficult to solve but easy to verify.</div><br/></div></div><div id="40772989" class="c"><input type="checkbox" id="c-40772989" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40772321">prev</a><span>|</span><a href="#40772065">next</a><span>|</span><label class="collapse" for="c-40772989">[-]</label><label class="expand" for="c-40772989">[1 more]</label></div><br/><div class="children"><div class="content">The idea behind this research is to generate answer few times and if results are semantically vastly different from each other then probably they are wrong.</div><br/></div></div><div id="40772065" class="c"><input type="checkbox" id="c-40772065" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#40772035">parent</a><span>|</span><a href="#40772989">prev</a><span>|</span><a href="#40771098">next</a><span>|</span><label class="collapse" for="c-40772065">[-]</label><label class="expand" for="c-40772065">[1 more]</label></div><br/><div class="children"><div class="content">profound but disagree<p>the fact checker doesn’t synthesize the facts or the topic</div><br/></div></div></div></div><div id="40771098" class="c"><input type="checkbox" id="c-40771098" checked=""/><div class="controls bullet"><span class="by">MikeGale</span><span>|</span><a href="#40772035">prev</a><span>|</span><a href="#40772729">next</a><span>|</span><label class="collapse" for="c-40771098">[-]</label><label class="expand" for="c-40771098">[26 more]</label></div><br/><div class="children"><div class="content">One formulation is that these are hallucinations.  Another is that these systems are &quot;orthogonal to truth&quot;.  They have nothing to do with truth or falsity.<p>One expression of that idea is in this paper: <a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10676-024-09775-5" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10676-024-09775-5</a></div><br/><div id="40771886" class="c"><input type="checkbox" id="c-40771886" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#40771098">parent</a><span>|</span><a href="#40771192">next</a><span>|</span><label class="collapse" for="c-40771886">[-]</label><label class="expand" for="c-40771886">[3 more]</label></div><br/><div class="children"><div class="content">The linked paper is about detecting when the LLM is choosing randomly versus consistently at the level of factoids. Procedurally-generated randomness can be great for some things like brainstorming, while consistency suggests that it&#x27;s repeating something that also appeared fairly consistently in the training material. So it might be true or false, but it&#x27;s more likely to have gotten it from somewhere.<p>Knowing how random the information is seems like a small step forward.</div><br/><div id="40772293" class="c"><input type="checkbox" id="c-40772293" checked=""/><div class="controls bullet"><span class="by">caseyy</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771886">parent</a><span>|</span><a href="#40771192">next</a><span>|</span><label class="collapse" for="c-40772293">[-]</label><label class="expand" for="c-40772293">[2 more]</label></div><br/><div class="children"><div class="content">I don’t know. It could be a misleading step.<p>Take social media like Reddit for example. It has a filtering mechanism for content that elevates low-entropy thoughts people commonly express and agree with. And I don’t think that necessarily equates such popular ideas there to the truth.</div><br/><div id="40772502" class="c"><input type="checkbox" id="c-40772502" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40772293">parent</a><span>|</span><a href="#40771192">next</a><span>|</span><label class="collapse" for="c-40772502">[-]</label><label class="expand" for="c-40772502">[1 more]</label></div><br/><div class="children"><div class="content">The conversations about people being misled by LLM&#x27;s remind me of when the Internet was new (not safe!), when Wikipedia was new (not safe!) and social media was new (still not safe!)<p>And they&#x27;re right, it&#x27;s not safe! Yes, people will certainly be misled. The Internet is not safe for gullible people, and LLM&#x27;s are very gullible too.<p>With some work, eventually they might get LLM&#x27;s to be about as accurate as Wikipedia. People will likely trust it too much, but the same is true of Wikipedia.<p>I think it&#x27;s best to treat LLM&#x27;s as a fairly accurate hint provider. A source of good hints can be a very useful component of a larger system, if there&#x27;s something else doing the vetting.<p>But if you want to know whether something is true, you need some other way of checking it. An LLM cannot check anything for you - that&#x27;s up to you. If you have no way of checking its hints, you&#x27;re in trouble.</div><br/></div></div></div></div></div></div><div id="40771192" class="c"><input type="checkbox" id="c-40771192" checked=""/><div class="controls bullet"><span class="by">soist</span><span>|</span><a href="#40771098">parent</a><span>|</span><a href="#40771886">prev</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40771192">[-]</label><label class="expand" for="c-40771192">[11 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like asking if a probability distribution is truthful or a liar. It&#x27;s a category error to speak about algorithms as if they had personal characteristics.</div><br/><div id="40771563" class="c"><input type="checkbox" id="c-40771563" checked=""/><div class="controls bullet"><span class="by">thwarted</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771192">parent</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40771563">[-]</label><label class="expand" for="c-40771563">[10 more]</label></div><br/><div class="children"><div class="content">The lie occurs when information which is known to be false or its truthfulness can not be assessed is presented as useful or truthful.</div><br/><div id="40772111" class="c"><input type="checkbox" id="c-40772111" checked=""/><div class="controls bullet"><span class="by">parineum</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771563">parent</a><span>|</span><a href="#40771903">next</a><span>|</span><label class="collapse" for="c-40772111">[-]</label><label class="expand" for="c-40772111">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  is presented as useful or truthful.<p>LLMs are incapable of presenting things as truth.</div><br/></div></div><div id="40771821" class="c"><input type="checkbox" id="c-40771821" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771563">parent</a><span>|</span><a href="#40771903">prev</a><span>|</span><a href="#40771907">next</a><span>|</span><label class="collapse" for="c-40771821">[-]</label><label class="expand" for="c-40771821">[1 more]</label></div><br/><div class="children"><div class="content">This seems a bit ironic...you&#x27;re claiming something needs to be true to be useful?</div><br/></div></div><div id="40771907" class="c"><input type="checkbox" id="c-40771907" checked=""/><div class="controls bullet"><span class="by">soloist11</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771563">parent</a><span>|</span><a href="#40771821">prev</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40771907">[-]</label><label class="expand" for="c-40771907">[6 more]</label></div><br/><div class="children"><div class="content">Lying is intentional, algorithms and computers do not have intentions. People can lie, computers can only execute their programmed instructions. Much of AI discourse is extremely confusing and confused because people keep attributing needs and intentions to computers and algorithms.<p>The social media gurus don&#x27;t help with these issues by claiming that non-intentional objects are going to cause humanity&#x27;s demise when there are much more pertinent issues to be concerned about like global warming, corporate malfeasance, and the general plundering of the biosphere. Algorithms that lie are not even in the top 100 list of things that people should be concerned about.</div><br/><div id="40772698" class="c"><input type="checkbox" id="c-40772698" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771907">parent</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40772698">[-]</label><label class="expand" for="c-40772698">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Lying is intentional, algorithms and computers do not have intentions. People can lie, computers can only execute their programmed instructions. Much of AI discourse is extremely confusing and confused because people keep attributing needs and intentions to computers and algorithms.<p>How do you know whether something has “intentions”? How can you know that humans have them but computer programs (including LLMs) don’t or can’t?<p>If one is a materialist&#x2F;physicalist, one has to say that human intentions (assuming one agrees they exist, <i>contra</i> eliminativism) have to be reducible to or emergent from physical processes in the brain. If intentions can be reducible to&#x2F;emergent from physical processes in the brain, why can’t they also be reducible to&#x2F;emergent from a computer program, which is also ultimately a physical process (calculations on a CPU&#x2F;GPU&#x2F;etc)?<p>What if one is a non-materialist&#x2F;non-physicalist? I don’t think that makes the question any easier to answer. For example, a substance dualist will insist that intentionality is inherently immaterial, and hence requires an immaterial soul. And yet, if one believes that, one has to say those immaterial souls somehow get attached to material human brains - why couldn’t one then be attached to an LLM (or the physical hardware it executes on), hence giving it the same intentionality that humans have?<p>I think this is one of those questions where if someone thinks the answer is obvious, that’s a sign they likely know far less about the topic than they think they do.</div><br/><div id="40772874" class="c"><input type="checkbox" id="c-40772874" checked=""/><div class="controls bullet"><span class="by">soist</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40772698">parent</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40772874">[-]</label><label class="expand" for="c-40772874">[4 more]</label></div><br/><div class="children"><div class="content">You&#x27;re using circular logic. You are assuming all physical processes are computational and then concluding that the brain is a computer even though that&#x27;s exactly what you assumed to begin with. I don&#x27;t find this argument convincing because I don&#x27;t think that everything in the universe is a computer or a computation. The computational assumption is a totalizing ontology and metaphysics which leaves no room for further progress other than the construction of larger data centers and faster computers.</div><br/><div id="40773547" class="c"><input type="checkbox" id="c-40773547" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40772874">parent</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40773547">[-]</label><label class="expand" for="c-40773547">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You&#x27;re using circular logic. You are assuming all physical processes are computational and then concluding that the brain is a computer even though that&#x27;s exactly what you assumed to begin with.<p>No, I never assumed “all physical processes are computational”. I never said that in my comment and nothing I said in my comment relies on such an assumption.<p>What I’m claiming is (1) we lack consensus on what “intentionality” is (2) we lack consensus on how we can determine whether something has it. Neither claim depends on any assumptions about “physical processes are computational”<p>If one assumes materialism&#x2F;physicalism - and I personally don’t, but given most people do, I’ll assume it for the sake of the argument - intentionality must ultimately be physical. But I never said it must ultimately be computational. Computers are also (assuming physicalism) ultimately physical, so if both human brains and computers are ultimately physical, if the former have (ultimately physical) intentionality - why can’t the latter? That argument hinges on the idea both brains and computers are ultimately physical, not on any claim that the physical is computational.<p>Suppose, hypothetically, that intentionality while ultimately physical, involves some extra-special quantum mechanical process - as suggested by Penrose and Hameroff’s extremely controversial and speculative “orchestrated objective reduction” theory [0]. Well, in that case, a program&#x2F;LLM running on a classical computer couldn’t have intentionality, but maybe one running on a quantum computer could, depending on exactly how this “extra-special quantum mechanical process” works. Maybe, a standard quantum computer would lack the “extra-special” part, but one could design a special kind of quantum computer that did have it.<p>But, my point is, we don’t actually know whether that theory is true or false. I think the majority of expert opinion in relevant disciplines doubts it is true, but nobody claims to be able to disprove it. In its current form, it is too vague to be disproven.<p>[0] <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Orchestrated_objective_reduction" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Orchestrated_objective_reduc...</a></div><br/><div id="40773643" class="c"><input type="checkbox" id="c-40773643" checked=""/><div class="controls bullet"><span class="by">soist</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40773547">parent</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40773643">[-]</label><label class="expand" for="c-40773643">[2 more]</label></div><br/><div class="children"><div class="content">Intentions are not reducible to computational implementation because intentions are not algorithms that can be implemented with digital circuits. What can be implemented with computers and digital circuits are deterministic signal processors which always produce consistent outputs for indistinguishable inputs.<p>You seem to be saying that because we have no clear cut way of determining whether people have intentions then that means, by physical reductionism, algorithms could also have intentions. The limiting case of this kind of semantic hair splitting is that I can say this about anything. There is no way to determine if something is dead or alive, there is no definition that works in all cases and no test to determine whether something is truly dead or alive so it must be the case that algorithms might or might not be alive but because we can&#x27;t tell then me might as well assume there will be a way to make algorithms that are alive.<p>It&#x27;s possible to reach any nonsensical conclusion using your logic because I can always ask for a more stringent definition and a way to test whether some object or attribute satisfies all the requirements.<p>I don&#x27;t know anything about theories of consciousness but that&#x27;s another example of something which does not have an algorithmic implementation unless one uses circular logic and assumes that the brain is a computer and consciousness is just software.</div><br/><div id="40773841" class="c"><input type="checkbox" id="c-40773841" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40773643">parent</a><span>|</span><a href="#40771609">next</a><span>|</span><label class="collapse" for="c-40773841">[-]</label><label class="expand" for="c-40773841">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Intentions are not reducible to computational implementation because intentions are not algorithms that can be implemented with digital circuits.<p>What is an &quot;intention&quot;? Do we all agree on what it even is?<p>&gt; What can be implemented with computers and digital circuits are deterministic signal processors which always produce consistent outputs for indistinguishable inputs.<p>We don&#x27;t actually know whether humans are ultimately deterministic or not. It is exceedingly difficult, even impossible, to distinguish the apparent indeterminism of a sufficiently complex&#x2F;chaotic deterministic system, from genuinely irreducible indeterminism. It is often assumed that classical systems have merely apparent indeterminism (pseudorandomness) whereas quantum systems have genuine indeterminism (true randomness), but we don&#x27;t actually know that for sure – if many-worlds or hidden variables are true, then quantum indeterminism is ultimately deterministic too. Orchestrated objective reduction (OOR) assumes that QM is ultimately indeterministic, and there is some neuronal mechanism (microtubules are commonly suggested) which permits this quantum indeterminism to influence the operations of the brain.<p>However, if you provide your computer with a quantum noise input, then whether the results of computations relying on that noise input are deterministic depends on whether quantum randomness itself is deterministic. So, if OOR is correct in claiming that QM is ultimately indeterministic, and quantum indeterminism plays an important role in human intentionality, why couldn&#x27;t an LLM sampled using a quantum random number generator also have that intentionality?<p>&gt; You seem to be saying that because we have no clear cut way of determining whether people have intentions then that means, by physical reductionism, algorithms could also have intentions.<p>Personally, I&#x27;m a subjective idealist, who believes that intentionality is an irreducible aspect of reality. So no, I don&#x27;t believe in physical reductionism, nor do I believe that algorithms can have intentions by way of physical reductionism.<p>However, while I personally believe that subjective idealism is true, it is an extremely controversial philosophical position, which the clear majority of people reject (at least in the contemporary West) – so I can&#x27;t claim &quot;we know&quot; it is true. Which is my whole point – we, collectively speaking, don&#x27;t know much at all about intentionality, because we lack the consensus on what it is and what determines whether it is present.<p>&gt; The limiting case of this kind of semantic hair splitting is that I can say this about anything. There is no way to determine if something is dead or alive, there is no definition that works in all cases and no test to determine whether something is truly dead or alive so it must be the case that algorithms might or might not be alive.<p>We have a reasonably clear consensus that animals and plants are alive, whereas ore deposits are not. (Although ore deposits, at least on Earth, may contain microscopic life–but the question is whether the ore deposit in itself is alive, as opposed being the home of lifeforms which are distinct from it.) However, there is genuine debate among biologists about whether viruses and prions should be classified as alive, not alive, or in some intermediate category. And more speculatively, there is also semantic debate about whether ecosystems are alive (as a kind of superorganism which is a living being beyond the mere sum of the life of its members) and also about whether artificial life is possible (and if so, how to determine whether any putative case of artificial life actually is alive or not). So, I think alive-vs-dead is actually rather similar to the question of intentionality – most people agree humans and some animals have intentionality, most people would agree that uranium ore deposits don&#x27;t, but other questions are much more disputed (e.g. could AIs have intentionality? do plants have intentionality?)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40771609" class="c"><input type="checkbox" id="c-40771609" checked=""/><div class="controls bullet"><span class="by">TheBlight</span><span>|</span><a href="#40771098">parent</a><span>|</span><a href="#40771192">prev</a><span>|</span><a href="#40773009">next</a><span>|</span><label class="collapse" for="c-40771609">[-]</label><label class="expand" for="c-40771609">[2 more]</label></div><br/><div class="children"><div class="content">My suspicion is shared reality will end up bending to accommodate LLMs not vice-versa. Whatever the computer says will be &quot;truth.&quot;</div><br/><div id="40771935" class="c"><input type="checkbox" id="c-40771935" checked=""/><div class="controls bullet"><span class="by">EForEndeavour</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771609">parent</a><span>|</span><a href="#40773009">next</a><span>|</span><label class="collapse" for="c-40771935">[-]</label><label class="expand" for="c-40771935">[1 more]</label></div><br/><div class="children"><div class="content">The botulinum that developed in this person&#x27;s[1] garlic and olive oil mixture wouldn&#x27;t particularly care to alter its toxicity to make Gemini&#x27;s recommendation look better.<p>[1] <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;1diljf2&#x2F;google_gemini_tried_to_kill_me&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;1diljf2&#x2F;google_gem...</a></div><br/></div></div></div></div><div id="40773009" class="c"><input type="checkbox" id="c-40773009" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#40771098">parent</a><span>|</span><a href="#40771609">prev</a><span>|</span><a href="#40771219">next</a><span>|</span><label class="collapse" for="c-40773009">[-]</label><label class="expand" for="c-40773009">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are trained with the objective: “no matter what, always have at least three paragraphs of response”. and that response is always preferred to silence or “unfriendly” responses like: “what are you talking about?”<p>Then yes, it is being taught to bullshit.<p>Similar to how an improv class teaches you to keep a conversation interesting and “never to say no” to your acting partner.</div><br/></div></div><div id="40771219" class="c"><input type="checkbox" id="c-40771219" checked=""/><div class="controls bullet"><span class="by">kreeben</span><span>|</span><a href="#40771098">parent</a><span>|</span><a href="#40773009">prev</a><span>|</span><a href="#40771416">next</a><span>|</span><label class="collapse" for="c-40771219">[-]</label><label class="expand" for="c-40771219">[7 more]</label></div><br/><div class="children"><div class="content">Your linked paper suffers from the same anthropomorphisation as does all papers who uses the word &quot;hallucination&quot;.</div><br/><div id="40773635" class="c"><input type="checkbox" id="c-40773635" checked=""/><div class="controls bullet"><span class="by">Karellen</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771219">parent</a><span>|</span><a href="#40771427">next</a><span>|</span><label class="collapse" for="c-40773635">[-]</label><label class="expand" for="c-40773635">[1 more]</label></div><br/><div class="children"><div class="content">Maybe another way of looking at it is - the paper is attempting to explain what LLMs are actually doing to people who have already anthropomorphised them.<p>Sometimes, to lead people out of a wrong belief or worldview, you have to meet them where they currently are first.</div><br/></div></div><div id="40771427" class="c"><input type="checkbox" id="c-40771427" checked=""/><div class="controls bullet"><span class="by">mordechai9000</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771219">parent</a><span>|</span><a href="#40773635">prev</a><span>|</span><a href="#40771675">next</a><span>|</span><label class="collapse" for="c-40771427">[-]</label><label class="expand" for="c-40771427">[3 more]</label></div><br/><div class="children"><div class="content">It seems like a useful adaptation of the term to a new usage, but I can understand if your objection is that it promotes anthropomorphizing these types of models. What do you think we should call this kind output, instead of hallucination?</div><br/><div id="40771523" class="c"><input type="checkbox" id="c-40771523" checked=""/><div class="controls bullet"><span class="by">isidor3</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771427">parent</a><span>|</span><a href="#40771675">next</a><span>|</span><label class="collapse" for="c-40771523">[-]</label><label class="expand" for="c-40771523">[2 more]</label></div><br/><div class="children"><div class="content">An author at Ars Technica has been trying to push the term &quot;confabulation&quot; for this</div><br/><div id="40771680" class="c"><input type="checkbox" id="c-40771680" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771523">parent</a><span>|</span><a href="#40771675">next</a><span>|</span><label class="collapse" for="c-40771680">[-]</label><label class="expand" for="c-40771680">[1 more]</label></div><br/><div class="children"><div class="content">I think Geoff Hinton made this suggestion first.</div><br/></div></div></div></div></div></div><div id="40771675" class="c"><input type="checkbox" id="c-40771675" checked=""/><div class="controls bullet"><span class="by">nerevarthelame</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771219">parent</a><span>|</span><a href="#40771427">prev</a><span>|</span><a href="#40772292">next</a><span>|</span><label class="collapse" for="c-40771675">[-]</label><label class="expand" for="c-40771675">[1 more]</label></div><br/><div class="children"><div class="content">The criticism that people shouldn&#x27;t anthropomorphize AI models that are deliberately and specifically replicating human behavior is already so tired. I think we need to accept that human traits will no longer be unique to humans (if they ever were, if you expand the analysis to non-human species), and that attributing these emergent traits to non-humans is justified.
&quot;Hallucination&quot; may not be the optimal metaphor for LLM falsehoods, but some humans absolutely regularly spout bullshit in the same way that LLMs do - the same sort of inaccurate responses generated from the same loose past associations.</div><br/></div></div><div id="40772292" class="c"><input type="checkbox" id="c-40772292" checked=""/><div class="controls bullet"><span class="by">fouc</span><span>|</span><a href="#40771098">root</a><span>|</span><a href="#40771219">parent</a><span>|</span><a href="#40771675">prev</a><span>|</span><a href="#40771416">next</a><span>|</span><label class="collapse" for="c-40772292">[-]</label><label class="expand" for="c-40772292">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In this paper, we argue against the view that when ChatGPT and the like produce false claims they are lying or even hallucinating, and in favour of the position that the activity they are engaged in is bullshitting, in the Frankfurtian sense (Frankfurt, 2002, 2005). Because these programs cannot themselves be concerned with truth, and because they are designed to produce text that looks truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.<p>&gt; We think that this is worth paying attention to. Descriptions of new technology, including metaphorical ones, guide policymakers’ and the public’s understanding of new technology; they also inform applications of the new technology. They tell us what the technology is for and what it can be expected to do. Currently, false statements by ChatGPT and other large language models are described as “hallucinations”, which give policymakers and the public the idea that these systems are misrepresenting the world, and describing what they “see”. We argue that this is an inapt metaphor which will misinform the public, policymakers, and other interested parties.</div><br/></div></div></div></div><div id="40771416" class="c"><input type="checkbox" id="c-40771416" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#40771098">parent</a><span>|</span><a href="#40771219">prev</a><span>|</span><a href="#40772729">next</a><span>|</span><label class="collapse" for="c-40771416">[-]</label><label class="expand" for="c-40771416">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s unnecessarily negative. A better question is what the answer to a prompt is grounded in. And sometimes the answer is &quot;nothing&quot;.</div><br/></div></div></div></div><div id="40772729" class="c"><input type="checkbox" id="c-40772729" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#40771098">prev</a><span>|</span><a href="#40772455">next</a><span>|</span><label class="collapse" for="c-40772729">[-]</label><label class="expand" for="c-40772729">[1 more]</label></div><br/><div class="children"><div class="content"><i>&quot;We show how to detect confabulations by developing a quantitative measure of when an input is likely to cause an LLM to generate arbitrary and ungrounded answers. ... Intuitively, our method works by sampling several possible answers to each question and clustering them algorithmically into answers that have similar meanings.&quot;</i><p>That&#x27;s reasonable for questions with a single objective answer. It probably won&#x27;t help when multiple, equally valid answers are possible.<p>However, that&#x27;s good enough for search engine applications.</div><br/></div></div><div id="40772455" class="c"><input type="checkbox" id="c-40772455" checked=""/><div class="controls bullet"><span class="by">curious_cat_163</span><span>|</span><a href="#40772729">prev</a><span>|</span><a href="#40772327">next</a><span>|</span><label class="collapse" for="c-40772455">[-]</label><label class="expand" for="c-40772455">[1 more]</label></div><br/><div class="children"><div class="content">It’s a pretty clever idea: “check” if the model answers “differently” when asked the same question again and again and again.<p>“checking” is being done with another model.<p>“differently” is being measured with entropy.</div><br/></div></div><div id="40772327" class="c"><input type="checkbox" id="c-40772327" checked=""/><div class="controls bullet"><span class="by">caseyy</span><span>|</span><a href="#40772455">prev</a><span>|</span><a href="#40771324">next</a><span>|</span><label class="collapse" for="c-40772327">[-]</label><label class="expand" for="c-40772327">[6 more]</label></div><br/><div class="children"><div class="content">Maybe for the moment it would be better if the AI companies simply presented their chatbots as slightly-steered text generation tools. Then people could use them appropriately.<p>Yes, there seems to be a little bit of grokking and the models can be made to approximate step-by-step reasoning a little bit. But 95% of the function of these black boxes is text generation. Not fact generation, not knowledge generation. They are more like improv partners than encyclopedias and everyone in tech knows it.<p>I don’t know if LLMs misleading people needs a clever answer entropy solution. And it is a very interesting solution that really seems like it would improve things — effectively putting certainty scores to statements. But what if we just stopped marketing machine learning text generators as near-AGI, which they are not? Wouldn’t that undo most of the damage, and arguably help us much more?</div><br/><div id="40772491" class="c"><input type="checkbox" id="c-40772491" checked=""/><div class="controls bullet"><span class="by">signatoremo</span><span>|</span><a href="#40772327">parent</a><span>|</span><a href="#40771324">next</a><span>|</span><label class="collapse" for="c-40772491">[-]</label><label class="expand" for="c-40772491">[5 more]</label></div><br/><div class="children"><div class="content">I’m working with a LLM right this moment to build some front end with react and redux, the technologies that I have almost no knowledge of. I posed questions and the LLM gave me the answers along with JavaScript code, a language that I’m also very rusty with. All of the code compiled, and most of them worked as expected. There were errors, some of them I had no idea what they were about. LLM was able to explained the issues and gave me revised code that worked.<p>All in all it’s been a great experience, it’s like working with a mentor along the way. It must have saved me a great deal of time, given how rookie I am. I do need to verify the result.<p>Where did you get the 95% figure? And whether what it does is text generation or fact or knowledge generation is irrelevant. It’s really a valuable tool and is way above anything I’ve used.</div><br/><div id="40772680" class="c"><input type="checkbox" id="c-40772680" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40772327">root</a><span>|</span><a href="#40772491">parent</a><span>|</span><a href="#40771324">next</a><span>|</span><label class="collapse" for="c-40772680">[-]</label><label class="expand" for="c-40772680">[4 more]</label></div><br/><div class="children"><div class="content">The last 6 weeks there&#x27;s been a pronounced uptick in comments, motivated by tiredness of seeing &quot;AI&quot;, manifested as a fever dream of them not being useful at all, and swindling the unwashed masses who just haven&#x27;t used them enough yet to know their true danger.<p>I&#x27;ve started calling it what it is: lashing out in confusion at why they&#x27;re not going away, given a prior that theres no point in using them<p>I have a feeling there&#x27;ll be near-religious holdouts in tech for some time to come. We attract a certain personality type, and they tend to be wedded to the idea of things being absolute and correct in a way things never are.</div><br/><div id="40773068" class="c"><input type="checkbox" id="c-40773068" checked=""/><div class="controls bullet"><span class="by">hatefulmoron</span><span>|</span><a href="#40772327">root</a><span>|</span><a href="#40772680">parent</a><span>|</span><a href="#40771324">next</a><span>|</span><label class="collapse" for="c-40773068">[-]</label><label class="expand" for="c-40773068">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also fair to say there&#x27;s a personality type that becomes fully bought into the newest emerging technologies, insisting that everyone else is either bought into their refusal or &quot;just doesn&#x27;t get it.&quot;<p>Look, I&#x27;m not against LLMs making me super-human (or at least super-me) in terms of productivity. It just isn&#x27;t there yet, or maybe it won&#x27;t be. Maybe whatever approach after current LLMs will be.<p>I think it&#x27;s just a little funny that you started by accusing people of dismissing others as &quot;unwashed masses&quot;, only to conclude that the people who disagree with you are being unreasonable, near-religious, and simply lashing out.</div><br/><div id="40773375" class="c"><input type="checkbox" id="c-40773375" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40772327">root</a><span>|</span><a href="#40773068">parent</a><span>|</span><a href="#40771324">next</a><span>|</span><label class="collapse" for="c-40773375">[-]</label><label class="expand" for="c-40773375">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t describe disagreeing with anyone, nor do I describe the people making these comments as near-religious, or simply lashing out, nor do I describe anyone as unreasonable<p>I reject simplistic binaries and They-ing altogether, it&#x27;s incredibly boring and waste of everyones time.<p>An old-fashioned breakdown for your troubles:<p>&gt; It&#x27;s also fair to say<p>Did anyone say it isn&#x27;t fair?<p>&gt; there&#x27;s a personality type that becomes fully bought into the newest emerging technologies<p>Who are you referring to? Why is this group relevant?<p>&gt; insisting that everyone else is either bought into their refusal or &quot;just doesn&#x27;t get it.&quot;<p>Who?<p>What does insisting mean to you?<p>What does &quot;bought into refusal&quot; mean? I tried googling, but there&#x27;s 0 results for both &#x27;bought into refusal&#x27; and &#x27;bought into their refusal&#x27;<p>Who are you quoting when you introduce this &quot;just doesn&#x27;t get it&quot; quote?<p>&gt; Look, I&#x27;m not against LLMs making me super-human (or at least super-me) in terms of productivity.<p>Who is invoking super humans? Who said you were against it?<p>&gt; It just isn&#x27;t there yet, or maybe it won&#x27;t be.<p>Given the language you use below, I&#x27;m just extremely curious how you&#x27;d describe me telling the person I was replying to that their lived experience was incorrect. Would that be accusing them of exaggerating? Dismissing them? Almost like calling them part of an unwashed mass?<p>&gt; Maybe whatever approach after current LLMs will be.<p>You&#x27;re blithely doing a stream of consciousness deconstructing a strawman and now you get to the interesting part? And just left it here? Darn! I was really excited to hear some specifics on this.<p>&gt; I think it&#x27;s just a little funny that you started by accusing people of dismissing others as &quot;unwashed masses&quot;,<p>Thats quite charged language from the reasonable referee! Accusing, dismissing, funny...my.<p>&gt; only to conclude that the people who disagree with you are being unreasonable, near-religious, and simply lashing out.<p>Source? Are you sure I didn&#x27;t separate the paragraphs on purpose? Paragraph breaks are commonly used to separate ideas and topics. Is it possible I intended to do that? I could claim I did, but it seems you expect me to wait for your explanation for what I&#x27;m thinking.</div><br/><div id="40773522" class="c"><input type="checkbox" id="c-40773522" checked=""/><div class="controls bullet"><span class="by">hatefulmoron</span><span>|</span><a href="#40772327">root</a><span>|</span><a href="#40773375">parent</a><span>|</span><a href="#40771324">next</a><span>|</span><label class="collapse" for="c-40773522">[-]</label><label class="expand" for="c-40773522">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; It&#x27;s also fair to say<p>&gt; Did anyone say it isn&#x27;t fair?<p>No. I don&#x27;t think I said you did, either. One might call this a turn of phrase.<p>&gt;&gt; there&#x27;s a personality type that becomes fully bought into the newest emerging technologies<p>&gt; Who? Why is this group relevant?<p>What do you mean &#x27;who&#x27;? Do you want names? It&#x27;s relevant because it&#x27;s the opposite, but also incorrect mirror image of the technology denier that you describe.<p>&gt;&gt; Look, I&#x27;m not against LLMs making me super-human (or at least super-me) in terms of productivity.<p>&gt; Who is invoking super humans? Who said you were against it?<p>... I am? And I didn&#x27;t say you thought I was against it? I feel like this might be a common issue for you (see paragraph 1.) I&#x27;m just saying that I&#x27;d like to be able to use LLMs to make myself more productive! Forgive me!<p>&gt;&gt; It just isn&#x27;t there yet, or maybe it won&#x27;t be.<p>&gt; Strawman<p>Of what?? I&#x27;m simply expressing my own opinion of something, detached from what you think. It&#x27;s not there yet. That&#x27;s it.<p>&gt;&gt; Maybe whatever approach after current LLMs will be.<p>&gt; Darn! I was really excited to hear some specifics on this.<p>I don&#x27;t know what will be after LLMs, I don&#x27;t recall expressing some belief that I did.<p>&gt; Thats quite charged language from the reasonable referee! Accusing, dismissing, funny...my.<p>I could use the word &#x27;describing&#x27; if you think the word &#x27;accusing&#x27; is too painful for your ears. Let me know.<p>&gt; Source? Are you sure I didn&#x27;t separate the paragraphs on purpose? Paragraph breaks are commonly used to separate ideas and topics. Is it possible I intended to do that? I could claim I did, but it seems you expect me to wait for your explanation for what I&#x27;m thinking.<p>Could you rephrase this in a different way? The rambling questions are obscuring your point.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40771324" class="c"><input type="checkbox" id="c-40771324" checked=""/><div class="controls bullet"><span class="by">jostmey</span><span>|</span><a href="#40772327">prev</a><span>|</span><a href="#40772263">next</a><span>|</span><label class="collapse" for="c-40771324">[-]</label><label class="expand" for="c-40771324">[3 more]</label></div><br/><div class="children"><div class="content">So, I can understand how their semantic entropy (which seems to require a LLM trained to detect semantic equivalence) might be better at catching hallucinations. However, I don&#x27;t see how semantic equivalence directly tackles the problem of hallucinations. Currently, I naively suspect it is just a heuristic for catching hallucinations. Furthermore, the requirement of a second LLM trained at detecting semantic equivalence to catch these events seems like an unnecessary complication. If I had a dataset of semantic equivalence to train a second LLM, I would directly incorporate this into the training process of my primary LLM</div><br/><div id="40773128" class="c"><input type="checkbox" id="c-40773128" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#40771324">parent</a><span>|</span><a href="#40771538">next</a><span>|</span><label class="collapse" for="c-40773128">[-]</label><label class="expand" for="c-40773128">[1 more]</label></div><br/><div class="children"><div class="content">Catching &quot;hallucinations&quot; is quite useful for many applications. I&#x27;m doing some research in mitigating effects of factual errors in LLM generated answers for public agencies, where giving a factually wrong answer may be illegal. If those could be detected (with sufficient accuracy), the system could simply decline to give an answer and ask the user to contact the agency.<p>Training the models not to give wrong answers (or giving them less) in the first place would of course be even better.<p>Unnecessary complications come also from the use of pre-trained commercial black-box LLMs through APIs, which is (sadly) the way LLMs are used in applications in vast majority of times. These could perhaps be fine tuned through the APIs too, but it tends to be rather fiddly and limited and very expensive to do for large synthetic datasets like would be used here.<p>P.S. I found it quite difficult to figure out from the article how the &quot;semantic entropy&quot; (actually multiple different entropies) is concretely computed. If somebody is interested in this, it&#x27;s a lot easier to figure out from the code: <a href="https:&#x2F;&#x2F;github.com&#x2F;jlko&#x2F;semantic_uncertainty&#x2F;blob&#x2F;master&#x2F;semantic_uncertainty&#x2F;uncertainty&#x2F;uncertainty_measures&#x2F;semantic_entropy.py">https:&#x2F;&#x2F;github.com&#x2F;jlko&#x2F;semantic_uncertainty&#x2F;blob&#x2F;master&#x2F;sem...</a></div><br/></div></div><div id="40771538" class="c"><input type="checkbox" id="c-40771538" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#40771324">parent</a><span>|</span><a href="#40773128">prev</a><span>|</span><a href="#40772263">next</a><span>|</span><label class="collapse" for="c-40771538">[-]</label><label class="expand" for="c-40771538">[1 more]</label></div><br/><div class="children"><div class="content">I haven’t really grokked this work yet well enough to critique it, but to answer your question:<p>Yes you could incorporate a semantic equivalence dataset into your training but:<p>1) when you have a bunch of ‘clear-cut’ functions (“achieve good AUC on semantics”) and you mix them to compensate for the weaknesses of a complicated model with an unknown perceptual objective, things are still kinda weird. You don’t know if you’re mixing them well, to start, and you also don’t know if they introduce unpredictable consequences or hazards or biases in the learning.<p>2) on a kinda narrowly defined task like: “can you determine semantic equivalence”, you can build a good model with less risk of unknown unknowns (than when there are myriad unpredictable interactions with other goal scoring measures)<p>3) if you can apply that model in a relatively clear cut way, you also have fewer unknowns unknowns.<p>Thus, carving a path to a particular reasonable heuristic using two slightly biased estimators can be MUCH safer and more general than mixing that data into a preexisting unholy brew and expecting its contribution to be predictable.</div><br/></div></div></div></div><div id="40772263" class="c"><input type="checkbox" id="c-40772263" checked=""/><div class="controls bullet"><span class="by">caseyy</span><span>|</span><a href="#40771324">prev</a><span>|</span><a href="#40772649">next</a><span>|</span><label class="collapse" for="c-40772263">[-]</label><label class="expand" for="c-40772263">[1 more]</label></div><br/><div class="children"><div class="content">This makes sense. Low semantic entropy probably means the answer was more represented in the unsupervised learning training data, or in later tuning. And I understand this is a tool to indirectly measure how much it was represented?<p>It’s an interesting idea to measure certainty this way. The problem remains that the model can be certain in this way and wrong. But the author did say this was a partial solution.<p>Still, wouldn’t we be able to already produce a confidence score at the model level like this? Instead of a “post-processor”?</div><br/></div></div><div id="40772649" class="c"><input type="checkbox" id="c-40772649" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#40772263">prev</a><span>|</span><a href="#40771874">next</a><span>|</span><label class="collapse" for="c-40772649">[-]</label><label class="expand" for="c-40772649">[2 more]</label></div><br/><div class="children"><div class="content">This seems to do the same as this paper from last year but getting more press.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.08896" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.08896</a></div><br/><div id="40773561" class="c"><input type="checkbox" id="c-40773561" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#40772649">parent</a><span>|</span><a href="#40771874">next</a><span>|</span><label class="collapse" for="c-40773561">[-]</label><label class="expand" for="c-40773561">[1 more]</label></div><br/><div class="children"><div class="content">That does indeed sound very similar.</div><br/></div></div></div></div><div id="40771874" class="c"><input type="checkbox" id="c-40771874" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#40772649">prev</a><span>|</span><a href="#40771670">next</a><span>|</span><label class="collapse" for="c-40771874">[-]</label><label class="expand" for="c-40771874">[1 more]</label></div><br/><div class="children"><div class="content">The semantic equivalence of possible outputs is already encoded in the model. While it is not necessarily recoverable from the logits of a particular sampling rollout it exists throughout prior layers.<p>So this is basically saying we shouldn&#x27;t try to estimate entropy over logits, but should be able to learn a function from activations earlier in the network to a degree of uncertainty that would signal (aka be classifiable as) confabulation.</div><br/></div></div><div id="40771670" class="c"><input type="checkbox" id="c-40771670" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#40771874">prev</a><span>|</span><a href="#40772259">next</a><span>|</span><label class="collapse" for="c-40771670">[-]</label><label class="expand" for="c-40771670">[3 more]</label></div><br/><div class="children"><div class="content">Won’t this catch creativity too? ie write me a story about a horse. LLMs freestyle that sort of thing quite hard so won’t that look the same under the hood?</div><br/><div id="40772524" class="c"><input type="checkbox" id="c-40772524" checked=""/><div class="controls bullet"><span class="by">Def_Os</span><span>|</span><a href="#40771670">parent</a><span>|</span><a href="#40772259">next</a><span>|</span><label class="collapse" for="c-40772524">[-]</label><label class="expand" for="c-40772524">[2 more]</label></div><br/><div class="children"><div class="content">This is a good point. If you&#x27;re worried about factuality, entropy is generally bad. But creative uses might thrive on it.</div><br/><div id="40773013" class="c"><input type="checkbox" id="c-40773013" checked=""/><div class="controls bullet"><span class="by">surfingdino</span><span>|</span><a href="#40771670">root</a><span>|</span><a href="#40772524">parent</a><span>|</span><a href="#40772259">next</a><span>|</span><label class="collapse" for="c-40773013">[-]</label><label class="expand" for="c-40773013">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.storycubes.com&#x2F;en&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.storycubes.com&#x2F;en&#x2F;</a> have been around for a while and do not require a huge data centre to create random ideas.</div><br/></div></div></div></div></div></div><div id="40772259" class="c"><input type="checkbox" id="c-40772259" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#40771670">prev</a><span>|</span><a href="#40771349">next</a><span>|</span><label class="collapse" for="c-40772259">[-]</label><label class="expand" for="c-40772259">[2 more]</label></div><br/><div class="children"><div class="content">The intersection into epistemology is very interesting</div><br/><div id="40772285" class="c"><input type="checkbox" id="c-40772285" checked=""/><div class="controls bullet"><span class="by">caseyy</span><span>|</span><a href="#40772259">parent</a><span>|</span><a href="#40771349">next</a><span>|</span><label class="collapse" for="c-40772285">[-]</label><label class="expand" for="c-40772285">[1 more]</label></div><br/><div class="children"><div class="content">Yes… is knowledge with lower entropy in society more true? That sounds to me like saying ideas big echo chambers like Reddit or X hold are more true. They kind of have a similar low entropy = higher visibility principle. But I don’t think many commonly agreed upon ideas on social media are necessarily true.</div><br/></div></div></div></div><div id="40771349" class="c"><input type="checkbox" id="c-40771349" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40772259">prev</a><span>|</span><a href="#40771856">next</a><span>|</span><label class="collapse" for="c-40771349">[-]</label><label class="expand" for="c-40771349">[2 more]</label></div><br/><div class="children"><div class="content">I skimmed through the paper, but don&#x27;t LLMs most of the time guess, sometimes these guesses contains noise that might be on point or not. I wonder if  &quot;confabulation&quot; had a more formal definition.</div><br/><div id="40771959" class="c"><input type="checkbox" id="c-40771959" checked=""/><div class="controls bullet"><span class="by">sn41</span><span>|</span><a href="#40771349">parent</a><span>|</span><a href="#40771856">next</a><span>|</span><label class="collapse" for="c-40771959">[-]</label><label class="expand" for="c-40771959">[1 more]</label></div><br/><div class="children"><div class="content">There seems to be an article on confabulations - seems to be a concept from neuroscience. From the abstract of the article:<p>&quot;Confabulations are inaccurate or false narratives purporting to convey information about world or self. It
is the received view that they are uttered by subjects intent on ‘covering up’ for a putative memory deficit.&quot;<p>It seems that there is a clear memory deficit about the incident, so the subject &quot;makes stuff up&quot;, knowingly or unknowingly.<p>--<p>cited from:<p>German E. Berrios, &quot;Confabulations: A Conceptual History&quot;,  
Journal of the History of the Neurosciences, Volume 7, 1998 - Issue 3<p><a href="https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;abs&#x2F;10.1076&#x2F;jhin.7.3.225.1855" rel="nofollow">https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;abs&#x2F;10.1076&#x2F;jhin.7.3.225.185...</a><p>DOI: 10.1076&#x2F;jhin.7.3.225.1855</div><br/></div></div></div></div><div id="40771856" class="c"><input type="checkbox" id="c-40771856" checked=""/><div class="controls bullet"><span class="by">lopkeny12ko</span><span>|</span><a href="#40771349">prev</a><span>|</span><a href="#40771221">next</a><span>|</span><label class="collapse" for="c-40771856">[-]</label><label class="expand" for="c-40771856">[2 more]</label></div><br/><div class="children"><div class="content">The best way to detect if something was written by an LLM, which has not failed me to date, is to check for any ocurrences of the word &quot;delve.&quot;</div><br/><div id="40771859" class="c"><input type="checkbox" id="c-40771859" checked=""/><div class="controls bullet"><span class="by">slater</span><span>|</span><a href="#40771856">parent</a><span>|</span><a href="#40771221">next</a><span>|</span><label class="collapse" for="c-40771859">[-]</label><label class="expand" for="c-40771859">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;1bzv071&#x2F;apparently_the_word_delve_is_the_biggest&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;1bzv071&#x2F;apparently...</a></div><br/></div></div></div></div><div id="40771221" class="c"><input type="checkbox" id="c-40771221" checked=""/><div class="controls bullet"><span class="by">more_corn</span><span>|</span><a href="#40771856">prev</a><span>|</span><a href="#40771890">next</a><span>|</span><label class="collapse" for="c-40771221">[-]</label><label class="expand" for="c-40771221">[2 more]</label></div><br/><div class="children"><div class="content">This is huge though not a hundred percent there.</div><br/></div></div><div id="40771890" class="c"><input type="checkbox" id="c-40771890" checked=""/><div class="controls bullet"><span class="by">imchillyb</span><span>|</span><a href="#40771221">prev</a><span>|</span><a href="#40773028">next</a><span>|</span><label class="collapse" for="c-40771890">[-]</label><label class="expand" for="c-40771890">[2 more]</label></div><br/><div class="children"><div class="content">Lies lie at the center of common discourse.<p>The trick isn&#x27;t in how to spot the lies, but how to properly apply them.  We cannot teach the AI how not to lie, without first teaching it when it must lie, and then how to apply the lie properly.<p>&quot;AI, tell me, do these jeans make me look fat?&quot;<p>AI:  NO.  You are fat.  The jeans are fine.<p>Is not an acceptable discourse.  Learning when and how to apply semantical truth stretching is imperative.<p>They must first understand where and when, then how, and finally why.<p>It&#x27;s how we teach our young.  Isn&#x27;t it?</div><br/><div id="40772268" class="c"><input type="checkbox" id="c-40772268" checked=""/><div class="controls bullet"><span class="by">alliao</span><span>|</span><a href="#40771890">parent</a><span>|</span><a href="#40773028">next</a><span>|</span><label class="collapse" for="c-40772268">[-]</label><label class="expand" for="c-40772268">[1 more]</label></div><br/><div class="children"><div class="content">but this thing doesn&#x27;t die, and why should it imitate our young?</div><br/></div></div></div></div></div></div></div></div></div></body></html>