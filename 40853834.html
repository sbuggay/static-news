<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720083676209" as="style"/><link rel="stylesheet" href="styles.css?v=1720083676209"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://hallofdreams.org/posts/trackmania-1/">The History of Machine Learning in Trackmania</a> <span class="domain">(<a href="https://hallofdreams.org">hallofdreams.org</a>)</span></div><div class="subtext"><span>Philpax</span> | <span>11 comments</span></div><br/><div><div id="40873136" class="c"><input type="checkbox" id="c-40873136" checked=""/><div class="controls bullet"><span class="by">Macuyiko</span><span>|</span><a href="#40871534">next</a><span>|</span><label class="collapse" for="c-40873136">[-]</label><label class="expand" for="c-40873136">[1 more]</label></div><br/><div class="children"><div class="content">I follow RL from the sides (I have dabbled with it myself), and have seen some of the cool videos the article also lists. I think one of the key points (and a bit of a personal nitpick) the article makes is this:<p>&gt; Thus far, every attempt at training a Trackmania-playing program has trained the program on one map at a time. As a result, no matter how well the network did on one track, it would have to be retrained - probably significantly retrained<p>This is a crucial aspect when talking about RL. Most of the Trackmania AI attempts focuses on a track at a time, which is not really a problem since they want to, given an individual track, outperform the best human racers.<p>However, it is this nuance that a lot of more business oriented users don&#x27;t get when being sold on some fancy new RL project. In the real world (think self-driving cars), we typically want agents to be way more able to generalize.<p>Most of the RL techniques we have to rather well in these kinds of constrained environments (in a sense they eventually start overfitting on the given environment), but making them behave well in more varied environments is way harder. A lot of beginner RL tutorials also fail to make this very explicit, and will e.g. show how to train an agent to find the exit in a maze without ever trying it on a newly generated maze :).</div><br/></div></div><div id="40871534" class="c"><input type="checkbox" id="c-40871534" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#40873136">prev</a><span>|</span><a href="#40872972">next</a><span>|</span><label class="collapse" for="c-40871534">[-]</label><label class="expand" for="c-40871534">[1 more]</label></div><br/><div class="children"><div class="content">I just got into Trackmania recently. Very difficult game, especially on a keyboard, but fun! It&#x27;s crazy to see how dedicated the pros are. I got into it after watching the streamer Wirtual try and beat the hardest map the game has seen (Deep Dip 2), for a prizepool of something like $30,000. It&#x27;s an insanely hard tower climb map, where if you fall, you have to start completely over. A 1-2 hour run could just disappear. Anyway, over just a few weeks, Wirtual put several hundred hours into the map, with over 1,500 falls... and then gave up, understandably :P</div><br/></div></div><div id="40872972" class="c"><input type="checkbox" id="c-40872972" checked=""/><div class="controls bullet"><span class="by">yuriks</span><span>|</span><a href="#40871534">prev</a><span>|</span><a href="#40872860">next</a><span>|</span><label class="collapse" for="c-40872972">[-]</label><label class="expand" for="c-40872972">[1 more]</label></div><br/><div class="children"><div class="content">Wanted to point out that Linesight, the final project described in the article, has since released a new update last month, and it now beats world records in about a dozen maps between official and user made ones: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cUojVsCJ51I" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cUojVsCJ51I</a>
It&#x27;s some really impressive stuff.</div><br/></div></div><div id="40872860" class="c"><input type="checkbox" id="c-40872860" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#40872972">prev</a><span>|</span><a href="#40872038">next</a><span>|</span><label class="collapse" for="c-40872860">[-]</label><label class="expand" for="c-40872860">[1 more]</label></div><br/><div class="children"><div class="content">For those who only read the comments, be sure to check out the videos by Yosh.  They&#x27;re amazing, and do a great job explaining how reinforcement learning works in practice:<p><a href="https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=Dw3BZ6O_8LY" rel="nofollow">https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=Dw3BZ6O_8LY</a><p><a href="https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=kojH8a7BW04" rel="nofollow">https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=kojH8a7BW04</a></div><br/></div></div><div id="40872038" class="c"><input type="checkbox" id="c-40872038" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#40872860">prev</a><span>|</span><a href="#40871548">next</a><span>|</span><label class="collapse" for="c-40872038">[-]</label><label class="expand" for="c-40872038">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Nienders concluded that this was due to the difference in the information available. Sophy had information about the track curvature of the upcoming 6 seconds of track, based on the current speed. TMRL, however, only had distance measurements from the LIDAR. While the TMRL program could plan for the next turn, it could not plan two turns ahead, and this fundamentally limited the program to mere safe driving, avoiding walls and crashes, but never optimizing.<p>I think that point is an important one. ML algorithms work better when they are given better context. Especially in programming, it is clear the models are trained on code, rather than repositories. They know about files and repositories, but i always get the impression that they are totally clueless about whole programs.<p>What could be done better in code, is provide in training more data about where each function is located in the project, some other files where similar functions are defined or called and so on. In general before each code is fed into the training, to do a little bit of data mining in the project like the tree-hugger project [1] enables. Tree-hugger however is a little bit older code, and tree-sitter has advanced a lot the last 4 years.<p>In my opinion 5x to 10x in code, is within reach, with no need to increase GPU compute or electricity.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;autosoft-dev&#x2F;tree-hugger">https:&#x2F;&#x2F;github.com&#x2F;autosoft-dev&#x2F;tree-hugger</a></div><br/></div></div><div id="40871548" class="c"><input type="checkbox" id="c-40871548" checked=""/><div class="controls bullet"><span class="by">programd</span><span>|</span><a href="#40872038">prev</a><span>|</span><a href="#40871306">next</a><span>|</span><label class="collapse" for="c-40871548">[-]</label><label class="expand" for="c-40871548">[2 more]</label></div><br/><div class="children"><div class="content">Make sure to read the followup post linked at the bottom of this one. It&#x27;s vastly entertaining in watching an open source train wreck kind of way. You have to admire the persistance.<p>Tangentially related, is anybody besides the autonomous car folks developing games or virtual environments designed from the ground up for exporting machine learning APIs? By this I mean exporting game state and accepting game controls through the network without going through adapter contortions.</div><br/><div id="40871589" class="c"><input type="checkbox" id="c-40871589" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#40871548">parent</a><span>|</span><a href="#40871306">next</a><span>|</span><label class="collapse" for="c-40871589">[-]</label><label class="expand" for="c-40871589">[1 more]</label></div><br/><div class="children"><div class="content">I think BeamNG Drive would fall under that category</div><br/></div></div></div></div><div id="40871306" class="c"><input type="checkbox" id="c-40871306" checked=""/><div class="controls bullet"><span class="by">msephton</span><span>|</span><a href="#40871548">prev</a><span>|</span><a href="#40872011">next</a><span>|</span><label class="collapse" for="c-40871306">[-]</label><label class="expand" for="c-40871306">[2 more]</label></div><br/><div class="children"><div class="content">Nice work! An enjoyable read. Edit: And the newer post!<p>I do love Trackmania. I&#x27;d like to play 2020 but alas I do not have a compatible computer. I mostly play the Wii version.</div><br/><div id="40872347" class="c"><input type="checkbox" id="c-40872347" checked=""/><div class="controls bullet"><span class="by">dr_kiszonka</span><span>|</span><a href="#40871306">parent</a><span>|</span><a href="#40872011">next</a><span>|</span><label class="collapse" for="c-40872347">[-]</label><label class="expand" for="c-40872347">[1 more]</label></div><br/><div class="children"><div class="content">That part about motivation was outstanding. I am pasting it below in case folks missed it.<p>&quot;Of course, we come into this with modest ambitions: make humans obsolete in the process of racing Trackmania. Perhaps we’ll be featured in the cover of a scientific journal to show how AI now dominates all racing sports. The kind of thing we can reasonably expect when entering a project with no directly relevant experience.<p>Obviously, logically, we know that this kind of oversized ambition isn’t realistic. No one at Google is going to see this and say “They really do need eight billion dollars in cloud credits for the noble task of racing emulated cars.” But having ludicrous ambitions is a key aspect for avoiding the thing that kills most projects: ennui. For us, if a project is merely aiming to be “kind of good” then it’s not going to go anywhere, because being “kind of good” is not exciting. So we trick our brains: we envision the realistic best case scenario (a pretty good racing program) and multiply it by a thousand. Is it grounded? No. But when you’re spending eight hours trying to figure out how to configure virtual desktops over ssh, you need something to keep you going. When every block you put down is a step towards a towering edifice, setbacks feel smaller, and wins feel like they’re worth something more.<p>So, that’s always been how we approach the early days of these projects, with unrealistic goals that we “know” are impossible. Once its potential starts to materialize, once you have the shape of the work laid down, then you can start to discard the grandiose for the grounded, and then you can start to take those castles in the clouds and boil them down into the bricks of the house. Projects like this are a long haul. We’ve already spent four full months trying to get TM2020 and OpenPlanet both installed. If you don’t have a good goal, you won’t have a reason to make it off the starting line.&quot;</div><br/></div></div></div></div><div id="40872011" class="c"><input type="checkbox" id="c-40872011" checked=""/><div class="controls bullet"><span class="by">budududuroiu</span><span>|</span><a href="#40871306">prev</a><span>|</span><label class="collapse" for="c-40872011">[-]</label><label class="expand" for="c-40872011">[1 more]</label></div><br/><div class="children"><div class="content">Always wondered if something similar would be possible with milsim games like DCS Worlds.<p>E.g. can you improve or replicate missile intercept algorithms</div><br/></div></div></div></div></div></div></div></body></html>