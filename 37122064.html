<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692090071808" as="style"/><link rel="stylesheet" href="styles.css?v=1692090071808"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/epsilla-cloud/vectordb">Show HN: Epsilla – Open-source vector database with low query latency</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>songrenchu</span> | <span>25 comments</span></div><br/><div><div id="37124123" class="c"><input type="checkbox" id="c-37124123" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#37129578">next</a><span>|</span><label class="collapse" for="c-37124123">[-]</label><label class="expand" for="c-37124123">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious about your approach on where you draw the line for database features; I don&#x27;t have a perspective on what&#x27;s right, just trying to get informed.<p>There are a bunch of possible areas to circle or ignore when making an ML-capable database of some sort. In rough order of data complexity:<p>1. Embeddings (context-free vectors, just an ID and the vector)<p>2. Metadata + Embedding (source data, JSON)<p>3. Binary Data + Metadata + Embedding (add documents)<p>Then there are tooling questions: in this matrix you&#x27;d want to decide if you&#x27;re going to allow inference, and if so, will it be arbitrary, service-based, etc. against the documents, and if so, how will you store the results?<p>I&#x27;m curious how you&#x27;re thinking about the design space. The embedding-only route is conceptually appealing because it&#x27;s simple. In a larger engineering project, there&#x27;s a tension between &quot;where do I keep all this data,&quot; &quot;how do I process and reprocess all this data&quot;, and &quot;where do I keep the results of all the processing&quot;, and to me there aren&#x27;t clear bright-line architectures that seem &quot;best of&quot;.<p>Put another way, 15 years ago, we went memcached -&gt; redis 1 -&gt; redis (whatever it is now), and at the same time, we went mysql&#x2F;postgres&#x2F;oracle -&gt; nosql json stores; today all of these have relatively well-defined use cases, (and for most of them sqlite is the best choice, obviously).<p>How are you seeing the ML db scene playing out, and where do you think the sqlite of this space will land on architecture?</div><br/><div id="37126444" class="c"><input type="checkbox" id="c-37126444" checked=""/><div class="controls bullet"><span class="by">songrenchu</span><span>|</span><a href="#37124123">parent</a><span>|</span><a href="#37129578">next</a><span>|</span><label class="collapse" for="c-37126444">[-]</label><label class="expand" for="c-37126444">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the insightful topic! By reading the question itself drive me think a lot.<p>For the database perspective, instead of dividing the table schema into 3 parts: id, metadata, embedding, we designed in a way closer to SQL, treat vector as another data type, and let user to define any number of fields in a table. ID is just an annotation of a field (composite key might be overkilling for now). There will be another debate on whether schemaful or schemaless is the right approach, we can leave it here for now<p>With this foundation, we already covers 1 and 2. And in our roadmap we also plan to cover 3, with multi-modal data type support. We think the real big advantage of embedding is on unstructured data (documents, images, video, audio, etc), and storing the embedding of multi-modal data and connect them through semantic relevance will open up big opportunities. And this fits with the table and fields-based design for introducing cross table embedding index on connecting different shape data.<p>And from the multi-modal data perspective comes the problem where do we store those data? One way is we provide a generic binary data type that let users put anything. Another way which most enterprise will do is integrate us with a larger data warehouse&#x2F;data lake system. And this opens up the requirement for us on supporting data streaming in&#x2F;out with kafka connector, spark connector, etc.<p>And totally agree that SQLite works so well in huge amount of scenarios, now there is DuckDB. We also see some other players like LanceDB taking this approach to be Vector DB space&#x27;s SQLite. We are also pretty close to announce our Python in-process package support, so docker &#x2F; a separate server is not a must have anymore.<p>For inference, this is a broader direction for us for now. We are open to explore this space and see if the serverless architecture on cloud can provide extra efficiency benefit to the market</div><br/></div></div></div></div><div id="37129578" class="c"><input type="checkbox" id="c-37129578" checked=""/><div class="controls bullet"><span class="by">chandureddyvari</span><span>|</span><a href="#37124123">prev</a><span>|</span><a href="#37124989">next</a><span>|</span><label class="collapse" for="c-37129578">[-]</label><label class="expand" for="c-37129578">[1 more]</label></div><br/><div class="children"><div class="content">Interesting project. How does it fare on scaling? We were evaluating vector dbs and since we were into b2b saas, we are keen on the sharding, scaling and multi tenancy features. currently we are more inclined towards milvus. <a href="https:&#x2F;&#x2F;zilliz.com&#x2F;comparison&#x2F;qdrant-vs-milvus" rel="nofollow noreferrer">https:&#x2F;&#x2F;zilliz.com&#x2F;comparison&#x2F;qdrant-vs-milvus</a></div><br/></div></div><div id="37124989" class="c"><input type="checkbox" id="c-37124989" checked=""/><div class="controls bullet"><span class="by">vosper</span><span>|</span><a href="#37129578">prev</a><span>|</span><a href="#37128058">next</a><span>|</span><label class="collapse" for="c-37124989">[-]</label><label class="expand" for="c-37124989">[5 more]</label></div><br/><div class="children"><div class="content">Vector databases seem to be a dime a dozen, now, as well as being built into Elasticsearch and available as Postgres extensions.<p>As far as I know they’re all relatively undifferentiated in performance and features.<p>Is there a viable long-term business here?</div><br/><div id="37125536" class="c"><input type="checkbox" id="c-37125536" checked=""/><div class="controls bullet"><span class="by">snordgren</span><span>|</span><a href="#37124989">parent</a><span>|</span><a href="#37128058">next</a><span>|</span><label class="collapse" for="c-37125536">[-]</label><label class="expand" for="c-37125536">[4 more]</label></div><br/><div class="children"><div class="content">This one has more reason to exist than most vector DBs since it&#x27;s not just a wrapper around hnswlib.</div><br/><div id="37129664" class="c"><input type="checkbox" id="c-37129664" checked=""/><div class="controls bullet"><span class="by">blast_one</span><span>|</span><a href="#37124989">root</a><span>|</span><a href="#37125536">parent</a><span>|</span><a href="#37128058">next</a><span>|</span><label class="collapse" for="c-37129664">[-]</label><label class="expand" for="c-37129664">[3 more]</label></div><br/><div class="children"><div class="content">Well, hnswlib is actually faster than epsilla according to benchmarks (compare their own vs ann benchmark), especially in terms of throughput</div><br/><div id="37129902" class="c"><input type="checkbox" id="c-37129902" checked=""/><div class="controls bullet"><span class="by">riedel</span><span>|</span><a href="#37124989">root</a><span>|</span><a href="#37129664">parent</a><span>|</span><a href="#37128058">next</a><span>|</span><label class="collapse" for="c-37129902">[-]</label><label class="expand" for="c-37129902">[2 more]</label></div><br/><div class="children"><div class="content">They claim that they are 10x faster given the high accuracy target ( no clue what that means in practice for the AI use case, probably less tokens for the LLM). Can you elaborate why you think hnswlib is still faster?  Can  you link the benchmark you mention.</div><br/><div id="37130628" class="c"><input type="checkbox" id="c-37130628" checked=""/><div class="controls bullet"><span class="by">blast_one</span><span>|</span><a href="#37124989">root</a><span>|</span><a href="#37129902">parent</a><span>|</span><a href="#37128058">next</a><span>|</span><label class="collapse" for="c-37130628">[-]</label><label class="expand" for="c-37130628">[1 more]</label></div><br/><div class="children"><div class="content">Sure. The benchmark from Epsilla <a href="https:&#x2F;&#x2F;miro.medium.com&#x2F;v2&#x2F;resize:fit:1400&#x2F;format:webp&#x2F;1*dDywzlTorC23ZuDDBjTN-g.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;miro.medium.com&#x2F;v2&#x2F;resize:fit:1400&#x2F;format:webp&#x2F;1*dDy...</a>, the benchmark for the same dataset and same K (10) from ann benchmark <a href="https:&#x2F;&#x2F;ann-benchmarks.com&#x2F;gist-960-euclidean_10_euclidean.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;ann-benchmarks.com&#x2F;gist-960-euclidean_10_euclidean.h...</a>
At fixed recall (which is the mentioned accuracy) 0.95 Epsilla gets 200 QPS using multiple intra-threads and a single inter-thread. Hnswlib gets more than 370 QPS at higher 0.97 recall and both single intra- and inter- threads, which is much faster and uses less CPU.<p>Because hnswlib does not use intra-threads it will scale much better in terms of full throughput, probably close to 7X-8X with 16 threads on 16 vCPUs (compared to Epsilla which saturates with 2.2X improvement from  multiple threads).
The main premise of Epsilla&#x27;s solution is <i>trading</i> throughput for latency, which is probably legit but would not work for all.<p>Note that even though the hardware between the benchmarks is not controlled (Epsilla only says it is some AWS EC2 
16C32G, ann benchmark uses AWS r6i.16xlarge), it does not matter that much since the single threaded cpu speeds are pretty stagnant over the years, so ann benchmark single-thread results can be transferred (unless Epsilla is using non-x64 hardware, which would be a weird choice).
There is a constant overhead from communication between the nodes in Epsilla, but it is constant and should not affect the speed at high recalls (for which the hnswlib is also faster).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37128058" class="c"><input type="checkbox" id="c-37128058" checked=""/><div class="controls bullet"><span class="by">xfalcox</span><span>|</span><a href="#37124989">prev</a><span>|</span><a href="#37123737">next</a><span>|</span><label class="collapse" for="c-37128058">[-]</label><label class="expand" for="c-37128058">[1 more]</label></div><br/><div class="children"><div class="content">I know this isn&#x27;t your focus, but could this be added as an index type to pgvector ? They recently added HNSW, and ship with IVFlat for a while, so I wonder if that would be possible.</div><br/></div></div><div id="37123737" class="c"><input type="checkbox" id="c-37123737" checked=""/><div class="controls bullet"><span class="by">social_quotient</span><span>|</span><a href="#37128058">prev</a><span>|</span><a href="#37124412">next</a><span>|</span><label class="collapse" for="c-37123737">[-]</label><label class="expand" for="c-37123737">[2 more]</label></div><br/><div class="children"><div class="content">Regarding the embedding vectors - is there a maximum limit to their dimensionality? Also, can you share insights into how the precision remains consistent at 99.9% even with high-dimension vectors?</div><br/><div id="37125832" class="c"><input type="checkbox" id="c-37125832" checked=""/><div class="controls bullet"><span class="by">songrenchu</span><span>|</span><a href="#37123737">parent</a><span>|</span><a href="#37124412">next</a><span>|</span><label class="collapse" for="c-37125832">[-]</label><label class="expand" for="c-37125832">[1 more]</label></div><br/><div class="children"><div class="content">For now we didn&#x27;t put a limit on the dimension of the vectors, so the machine can fit as much as #vector * #dimension * sizeof(float) into memory. For now we just support dense vector, and in the future we will work on sparse vector support for much higher dimension. I think you are referring to the &quot;Curse of dimensionality&quot; problem. Here is my thoughts: in a graph-based index such as SpeedANN, or HNSW, each vector is treated as a node in the graph, and the index is a nearest neighbor graph. Different from spatial partition-based indices, the topology quality of the nearest neighbor graph is independent from the dimensionality of the vectors. Our benchmark is on 960 dimension vector, but we will do more experiments in sparse vectors in the future</div><br/></div></div></div></div><div id="37124412" class="c"><input type="checkbox" id="c-37124412" checked=""/><div class="controls bullet"><span class="by">VoVAllen</span><span>|</span><a href="#37123737">prev</a><span>|</span><a href="#37123626">next</a><span>|</span><label class="collapse" for="c-37124412">[-]</label><label class="expand" for="c-37124412">[3 more]</label></div><br/><div class="children"><div class="content">Why did you choose SpeedANN instead of other new indexes such as DiskANN? And you changed the color of epsilla in every benchmark figure, which is quite confusing</div><br/><div id="37125638" class="c"><input type="checkbox" id="c-37125638" checked=""/><div class="controls bullet"><span class="by">songrenchu</span><span>|</span><a href="#37124412">parent</a><span>|</span><a href="#37124435">next</a><span>|</span><label class="collapse" for="c-37125638">[-]</label><label class="expand" for="c-37125638">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing! DiskANN was published in 2019 and SpeedANN in 2022. DiskANN is specialized in disk based ANNS solution, and it&#x27;s focus on the scenario where the vectors don&#x27;t fit into memory. SpeedANN is in-memory solution and specialize in low latency query, which is the scenario we want to tackle for now. We can further extend our engine to support DiskANN and other index algorithms based on our customer&#x27;s requirements
Thanks for pointing out the benchmark figure, we just fixed it to have consistent colors</div><br/></div></div></div></div><div id="37123626" class="c"><input type="checkbox" id="c-37123626" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37124412">prev</a><span>|</span><a href="#37124499">next</a><span>|</span><label class="collapse" for="c-37123626">[-]</label><label class="expand" for="c-37123626">[3 more]</label></div><br/><div class="children"><div class="content">How long until you sell it to BigCompany? I just don’t get why there are numerous vector databases all with similar functionality.</div><br/><div id="37124685" class="c"><input type="checkbox" id="c-37124685" checked=""/><div class="controls bullet"><span class="by">songrenchu</span><span>|</span><a href="#37123626">parent</a><span>|</span><a href="#37123726">next</a><span>|</span><label class="collapse" for="c-37124685">[-]</label><label class="expand" for="c-37124685">[1 more]</label></div><br/><div class="children"><div class="content">You are right, there are numerous vector databases in the market. Most of them (including us) are still pretty early and a lot of enterprise readiness features to build. Including role based &#x2F; privilege based access control, authN&#x2F;Z integration, data versioning and backup &#x2F; restore,  fault tolerance, data streaming in&#x2F;out, etc. We have first hand experience on the enterprise level product development and sales in our previous job at a series D graph database startup, and we will apply our learnings there to make Epsilla enterprise ready in the next few months</div><br/></div></div><div id="37123726" class="c"><input type="checkbox" id="c-37123726" checked=""/><div class="controls bullet"><span class="by">theolivenbaum</span><span>|</span><a href="#37123626">parent</a><span>|</span><a href="#37124685">prev</a><span>|</span><a href="#37124499">next</a><span>|</span><label class="collapse" for="c-37123726">[-]</label><label class="expand" for="c-37123726">[1 more]</label></div><br/><div class="children"><div class="content">Because wrapping HNSW is just that easy. Same with all the ChatGPT-based tools popping around a dozen a week, it&#x27;s just  easy to throw something together and see if it sticks.</div><br/></div></div></div></div><div id="37124499" class="c"><input type="checkbox" id="c-37124499" checked=""/><div class="controls bullet"><span class="by">itake</span><span>|</span><a href="#37123626">prev</a><span>|</span><a href="#37127884">next</a><span>|</span><label class="collapse" for="c-37124499">[-]</label><label class="expand" for="c-37124499">[2 more]</label></div><br/><div class="children"><div class="content">imho, vectorDbs need to scale horizontally. Simply running on a single host doesn&#x27;t cut it anymore.</div><br/><div id="37125699" class="c"><input type="checkbox" id="c-37125699" checked=""/><div class="controls bullet"><span class="by">songrenchu</span><span>|</span><a href="#37124499">parent</a><span>|</span><a href="#37127884">next</a><span>|</span><label class="collapse" for="c-37125699">[-]</label><label class="expand" for="c-37125699">[1 more]</label></div><br/><div class="children"><div class="content">You are right. We designed our storage in a segment-based way, with configurable segment size, so it can horizontally scale in the future cross multiple workers in one machine, and cross multiple machine cluster. And the search will become a two-stage search: find top K in each segment, then a global merger (can also be horizontally scaled) to merge the results from all segments</div><br/></div></div></div></div><div id="37127884" class="c"><input type="checkbox" id="c-37127884" checked=""/><div class="controls bullet"><span class="by">yding</span><span>|</span><a href="#37124499">prev</a><span>|</span><a href="#37127483">next</a><span>|</span><label class="collapse" for="c-37127884">[-]</label><label class="expand" for="c-37127884">[1 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch!</div><br/></div></div><div id="37127483" class="c"><input type="checkbox" id="c-37127483" checked=""/><div class="controls bullet"><span class="by">sidhantgandhi</span><span>|</span><a href="#37127884">prev</a><span>|</span><a href="#37127887">next</a><span>|</span><label class="collapse" for="c-37127483">[-]</label><label class="expand" for="c-37127483">[3 more]</label></div><br/><div class="children"><div class="content">“Hippocampus of AI” in a readme is a yellow flag</div><br/><div id="37131455" class="c"><input type="checkbox" id="c-37131455" checked=""/><div class="controls bullet"><span class="by">VoVAllen</span><span>|</span><a href="#37127483">parent</a><span>|</span><a href="#37127548">next</a><span>|</span><label class="collapse" for="c-37131455">[-]</label><label class="expand" for="c-37131455">[1 more]</label></div><br/><div class="children"><div class="content">Why it&#x27;s a yellow flag?</div><br/></div></div><div id="37127548" class="c"><input type="checkbox" id="c-37127548" checked=""/><div class="controls bullet"><span class="by">songrenchu</span><span>|</span><a href="#37127483">parent</a><span>|</span><a href="#37131455">prev</a><span>|</span><a href="#37127887">next</a><span>|</span><label class="collapse" for="c-37127548">[-]</label><label class="expand" for="c-37127548">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for pointing it out. We just removed it from README</div><br/></div></div></div></div><div id="37127887" class="c"><input type="checkbox" id="c-37127887" checked=""/><div class="controls bullet"><span class="by">fireysausage</span><span>|</span><a href="#37127483">prev</a><span>|</span><label class="collapse" for="c-37127887">[-]</label><label class="expand" for="c-37127887">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how you compete against the 50 other providers. You just implemented a speedup on the graph search (maybe credit the authors of the paper you are ripping off in your github?). However this speedup trades the overall throughput for latency. And even in the paper, its not exact.<p>Maybe drop the disingenuous marketing and find something else to work on. The 50 other vector dbs will implement this trivial addition and you&#x27;ll be left with nothing to show.<p>Sources:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2201.13007" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2201.13007</a>
<a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3572848.3577527" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3572848.3577527</a></div><br/></div></div></div></div></div></div></div></body></html>