<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714813258525" as="style"/><link rel="stylesheet" href="styles.css?v=1714813258525"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/google-deepmind/torax">TORAX is a differentiable tokamak core transport simulator</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>yeldarb</span> | <span>15 comments</span></div><br/><div><div id="40253830" class="c"><input type="checkbox" id="c-40253830" checked=""/><div class="controls bullet"><span class="by">heisenzombie</span><span>|</span><a href="#40252042">next</a><span>|</span><label class="collapse" for="c-40253830">[-]</label><label class="expand" for="c-40253830">[5 more]</label></div><br/><div class="children"><div class="content">I recently started using JAX for some ion-optics work in accelerator physics. I have found it very very good. The autodiff stuff is magical for doing optimisation work, but even just as a compiled-numpy, I have found it very easy to get highly performant code. For reference, I previously tried roughly the same thing in “numba”, and wasn’t able to get anywhere near the same performance as JAX, even running on the CPU, which I understand is JAX’s weakest backend. By and large I have just written basically idiomatic Python&#x2F;numpy code — sprinkled a few “vmap”s and “scan”s around, and got great results. I’m very pleased with JAX.</div><br/><div id="40253861" class="c"><input type="checkbox" id="c-40253861" checked=""/><div class="controls bullet"><span class="by">Iwan-Zotow</span><span>|</span><a href="#40253830">parent</a><span>|</span><a href="#40252042">next</a><span>|</span><label class="collapse" for="c-40253861">[-]</label><label class="expand" for="c-40253861">[4 more]</label></div><br/><div class="children"><div class="content">have any code on github? remembering my acc background, would be interesting to see...</div><br/><div id="40254105" class="c"><input type="checkbox" id="c-40254105" checked=""/><div class="controls bullet"><span class="by">heisenzombie</span><span>|</span><a href="#40253830">root</a><span>|</span><a href="#40253861">parent</a><span>|</span><a href="#40252042">next</a><span>|</span><label class="collapse" for="c-40254105">[-]</label><label class="expand" for="c-40254105">[3 more]</label></div><br/><div class="children"><div class="content">Unfortunately it’s for my work so not currently public. One chunk of it which I would love to open source sometime, is basically just TRANSPORT but modern Python. Despite being a million years old, just second order matrix-style beam optics is actually still a really useful place to start for a lot of stuff. And with it all differentiable, one can do way more kinds of interesting optimisations.<p>What’s interesting is that the old Fortran source code for TRANSPORT is still out there if one digs, and most of it is taken up by thousands of lines of code that (1) implement a bunch of matrix multiplication that is now just one call to “jnp.einsum()“, and (2) implementing a tedious by-hand differentiation of those same calculations, in a few parameters, which is now basically inferior to just one call to “jax.grad()”. It’s amazing how modern tools can make hard stuff truly trivial.</div><br/><div id="40254214" class="c"><input type="checkbox" id="c-40254214" checked=""/><div class="controls bullet"><span class="by">Iwan-Zotow</span><span>|</span><a href="#40253830">root</a><span>|</span><a href="#40254105">parent</a><span>|</span><a href="#40252042">next</a><span>|</span><label class="collapse" for="c-40254214">[-]</label><label class="expand" for="c-40254214">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Despite being a million years old, just second order matrix-style beam optics is actually still a really useful place to start for a lot of stuff<p>ha, you bet<p>TRANSPORT and MAD(X)</div><br/><div id="40254967" class="c"><input type="checkbox" id="c-40254967" checked=""/><div class="controls bullet"><span class="by">heisenzombie</span><span>|</span><a href="#40253830">root</a><span>|</span><a href="#40254214">parent</a><span>|</span><a href="#40252042">next</a><span>|</span><label class="collapse" for="c-40254967">[-]</label><label class="expand" for="c-40254967">[1 more]</label></div><br/><div class="children"><div class="content">Yup! Still used all the time! And both (IMHO) super un-ergonomic.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40252042" class="c"><input type="checkbox" id="c-40252042" checked=""/><div class="controls bullet"><span class="by">aqme28</span><span>|</span><a href="#40253830">prev</a><span>|</span><a href="#40252908">next</a><span>|</span><label class="collapse" for="c-40252042">[-]</label><label class="expand" for="c-40252042">[5 more]</label></div><br/><div class="children"><div class="content">Very interesting that it&#x27;s coming from Google. I did my masters in tokamak simulation, so my first question is about performance. Python is very rarely used in this space just for performance reasons. Even though Python can call out to BLAS or whatever, it&#x27;s still usually worth it to code in Fortran or C or maybe Julia.</div><br/><div id="40252358" class="c"><input type="checkbox" id="c-40252358" checked=""/><div class="controls bullet"><span class="by">nestorD</span><span>|</span><a href="#40252042">parent</a><span>|</span><a href="#40252084">next</a><span>|</span><label class="collapse" for="c-40252358">[-]</label><label class="expand" for="c-40252358">[1 more]</label></div><br/><div class="children"><div class="content">I am doing quite a bit of work with JAX (the Python library used here) in a high-performance numerical computing context.<p>On GPU&#x2F;TPU, it is not going to reach perfect 100% hardware usage, but it is going to get close enough (far above vanilla Python performance) and be significantly more productive than alternatives.<p>That makes it a sweet spot for research (where you will want to tweak things as you go) and extremely complex codes (where you already need to put your full focus on the correctness of the code). I highly recommend it to domain experts who need performance for their research project.</div><br/></div></div><div id="40252084" class="c"><input type="checkbox" id="c-40252084" checked=""/><div class="controls bullet"><span class="by">cokernel_hacker</span><span>|</span><a href="#40252042">parent</a><span>|</span><a href="#40252358">prev</a><span>|</span><a href="#40252206">next</a><span>|</span><label class="collapse" for="c-40252084">[-]</label><label class="expand" for="c-40252084">[2 more]</label></div><br/><div class="children"><div class="content">This python actually builds a graph under the hood which then gets JIT compiled for CPU&#x2F;GPU&#x2F;TPU.</div><br/><div id="40252885" class="c"><input type="checkbox" id="c-40252885" checked=""/><div class="controls bullet"><span class="by">senseiV</span><span>|</span><a href="#40252042">root</a><span>|</span><a href="#40252084">parent</a><span>|</span><a href="#40252206">next</a><span>|</span><label class="collapse" for="c-40252885">[-]</label><label class="expand" for="c-40252885">[1 more]</label></div><br/><div class="children"><div class="content">Does a TPU have XLA-graph for GPUs Cuda-graphs? Not sure on TPU theory</div><br/></div></div></div></div><div id="40252206" class="c"><input type="checkbox" id="c-40252206" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#40252042">parent</a><span>|</span><a href="#40252084">prev</a><span>|</span><a href="#40252908">next</a><span>|</span><label class="collapse" for="c-40252206">[-]</label><label class="expand" for="c-40252206">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s built on JAX, not vanilla Python.<p>The metric being optimized is not just performance, but also the ability to build reasonably performant workflows with arbitrary differentiable (i.e., ML) inputs and outputs.</div><br/></div></div></div></div><div id="40252908" class="c"><input type="checkbox" id="c-40252908" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#40252042">prev</a><span>|</span><a href="#40251799">next</a><span>|</span><label class="collapse" for="c-40252908">[-]</label><label class="expand" for="c-40252908">[1 more]</label></div><br/><div class="children"><div class="content">I just noticed this podcast episode on Deep RL for fusion reactors was recently published, if anyone likes this stuff. I have not listened yet, but this podcast in general is great.<p><a href="https:&#x2F;&#x2F;twimlai.com&#x2F;podcast&#x2F;twimlai&#x2F;controlling-fusion-reactor-instability-with-deep-reinforcement-learning&#x2F;" rel="nofollow">https:&#x2F;&#x2F;twimlai.com&#x2F;podcast&#x2F;twimlai&#x2F;controlling-fusion-react...</a></div><br/></div></div><div id="40251799" class="c"><input type="checkbox" id="c-40251799" checked=""/><div class="controls bullet"><span class="by">yeldarb</span><span>|</span><a href="#40252908">prev</a><span>|</span><a href="#40254427">next</a><span>|</span><label class="collapse" for="c-40251799">[-]</label><label class="expand" for="c-40251799">[2 more]</label></div><br/><div class="children"><div class="content">Found this really cool; I didn’t even know Deepmind was working on Fusion research <a href="https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;deepmind-ai-nuclear-fusion&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;deepmind-ai-nuclear-fusion&#x2F;</a></div><br/><div id="40252191" class="c"><input type="checkbox" id="c-40252191" checked=""/><div class="controls bullet"><span class="by">soggybread</span><span>|</span><a href="#40251799">parent</a><span>|</span><a href="#40254427">next</a><span>|</span><label class="collapse" for="c-40252191">[-]</label><label class="expand" for="c-40252191">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s an archive.org link: <a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20220217012159&#x2F;https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;deepmind-ai-nuclear-fusion&#x2F;" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20220217012159&#x2F;https:&#x2F;&#x2F;www.wired...</a></div><br/></div></div></div></div><div id="40254427" class="c"><input type="checkbox" id="c-40254427" checked=""/><div class="controls bullet"><span class="by">parentheses</span><span>|</span><a href="#40251799">prev</a><span>|</span><label class="collapse" for="c-40254427">[-]</label><label class="expand" for="c-40254427">[1 more]</label></div><br/><div class="children"><div class="content">Cool project. I would love to explore simulation projects like this but often don&#x27;t know where to begin. It&#x27;s partly because the domains are so foreign to me.</div><br/></div></div></div></div></div></div></div></body></html>