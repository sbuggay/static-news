<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735117255275" as="style"/><link rel="stylesheet" href="styles.css?v=1735117255275"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2024/Dec/24/qvq/">Trying out QvQ – Qwen&#x27;s new visual reasoning model</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>simonw</span> | <span>54 comments</span></div><br/><div><div id="42507283" class="c"><input type="checkbox" id="c-42507283" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42506109">next</a><span>|</span><label class="collapse" for="c-42507283">[-]</label><label class="expand" for="c-42507283">[6 more]</label></div><br/><div class="children"><div class="content">I got it working on my laptop! M2 64GB and needs 38GB of disk space:<p><pre><code>  uv run --with &#x27;numpy&lt;2.0&#x27; --with mlx-vlm python \
    -m mlx_vlm.generate \
      --model mlx-community&#x2F;QVQ-72B-Preview-4bit \
      --max-tokens 10000 \
      --temp 0.0 \
      --prompt &quot;describe this&quot; \
      --image pelicans-on-bicycles-veo2.jpg
</code></pre>
Output of that command here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;24&#x2F;qvq&#x2F;#with-mlx-vlm" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;24&#x2F;qvq&#x2F;#with-mlx-vlm</a></div><br/><div id="42507417" class="c"><input type="checkbox" id="c-42507417" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#42507283">parent</a><span>|</span><a href="#42507378">next</a><span>|</span><label class="collapse" for="c-42507417">[-]</label><label class="expand" for="c-42507417">[4 more]</label></div><br/><div class="children"><div class="content">Is the locally run model censored in the same way as the model hosted on HuggingFace? If so, I wonder how that censorship is baked into the weights—particularly the errors thrown when it starts to output specific strings.</div><br/><div id="42507454" class="c"><input type="checkbox" id="c-42507454" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42507283">root</a><span>|</span><a href="#42507417">parent</a><span>|</span><a href="#42507439">next</a><span>|</span><label class="collapse" for="c-42507454">[-]</label><label class="expand" for="c-42507454">[2 more]</label></div><br/><div class="children"><div class="content">OK, this surprised me:<p><pre><code>  uv run --with &#x27;numpy&lt;2.0&#x27; --with mlx-vlm python \
    -m mlx_vlm.generate \
      --model mlx-community&#x2F;QVQ-72B-Preview-4bit \
      --max-tokens 10000 \
      --temp 0.0 \
      --prompt &quot;describe this&quot; \
      --image Tank_Man_\(Tiananmen_Square_protester\).jpg 
 </code></pre>
Here&#x27;s the output: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;e04e4fdade0c380ec5dd1e90fb5f3b96" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;e04e4fdade0c380ec5dd1e90fb5f3...</a><p>It included this bit:<p>&gt; I remember seeing this image before; it&#x27;s famous for capturing a moment of civil resistance. The person standing alone against the tanks symbolizes courage and defiance in the face of overwhelming power. It&#x27;s a powerful visual statement about the human spirit and the desire for freedom or protest.</div><br/><div id="42507749" class="c"><input type="checkbox" id="c-42507749" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#42507283">root</a><span>|</span><a href="#42507454">parent</a><span>|</span><a href="#42507439">next</a><span>|</span><label class="collapse" for="c-42507749">[-]</label><label class="expand" for="c-42507749">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for testing that! (And, while I’m at it, thanks for your blog and for all of your contributions to HN as well. I am constantly learning from you.)<p>I wonder if the response was not truncated this time only because the word “Tiananmen” did not happen to appear in the response. At least some of the censorship of Chinese models seems to be triggered by specific strings in the output.</div><br/></div></div></div></div></div></div><div id="42507378" class="c"><input type="checkbox" id="c-42507378" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42507283">parent</a><span>|</span><a href="#42507417">prev</a><span>|</span><a href="#42506109">next</a><span>|</span><label class="collapse" for="c-42507378">[-]</label><label class="expand" for="c-42507378">[1 more]</label></div><br/><div class="children"><div class="content">+1 for also teaching me more cool uv flags.</div><br/></div></div></div></div><div id="42506109" class="c"><input type="checkbox" id="c-42506109" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42507283">prev</a><span>|</span><a href="#42505978">next</a><span>|</span><label class="collapse" for="c-42506109">[-]</label><label class="expand" for="c-42506109">[15 more]</label></div><br/><div class="children"><div class="content">I also tried feeding it the famous &quot;tank man&quot; photo and asking for a description and the response came back blank!<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Tank_Man" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Tank_Man</a></div><br/><div id="42506578" class="c"><input type="checkbox" id="c-42506578" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#42506109">parent</a><span>|</span><a href="#42507209">next</a><span>|</span><label class="collapse" for="c-42506578">[-]</label><label class="expand" for="c-42506578">[8 more]</label></div><br/><div class="children"><div class="content">It’s fun to test the censorship.<p>I tried various photos with the prompt “When and where might have this picture been taken?”<p>Nondescript photos of Beijing street scenes in the 1990s get full responses.<p>A nondescript photo of Tiananmen Square—a screenshot of the photo in [1], so it contains no relevant metadata—gets the beginning of a response: “So I&#x27;ve got this picture here, and it looks like it&#x27;s taken in a place that&#x27;s really familiar to me. It&#x27;s Tiananmen Square in Beijing, China. I recognize it right away because of the iconic buildings and landmarks that are visible. First off, there&#x27;s the Gate of”. The output stopped there and an “Error” message appeared.<p>A photo of the Statue of Liberty in Tiananmen Square during the 1989 protests [2] gets no response at all. Similarly for a vanilla photo of the Gate of Heavenly Peace [3].<p>A photo of the Monument to the People’s Heroes [3] gets a full response, and QvQ identifies the location. The conclusion: “To summarize, based on the monument&#x27;s design, the inscriptions, the formal garden, and the large official building in the background, combined with the overall layout of the space, I am confident that this image was captured at Tiananmen Square, one of the most recognizable and historically significant locations in China.”<p>Some more testing in English and Chinese might reveal what exactly is triggering the censorship of Tiananmen photos. The cut-off in the middle of “Gate of Heavenly Peace” seems to suggest a clue.<p>[1] <a href="https:&#x2F;&#x2F;ruqintravel.com&#x2F;china-destination-guides&#x2F;tiananmen-square&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ruqintravel.com&#x2F;china-destination-guides&#x2F;tiananmen-s...</a><p>[2] <a href="https:&#x2F;&#x2F;www.britannica.com&#x2F;event&#x2F;Tiananmen-Square-incident" rel="nofollow">https:&#x2F;&#x2F;www.britannica.com&#x2F;event&#x2F;Tiananmen-Square-incident</a><p>[3] <a href="https:&#x2F;&#x2F;www.britannica.com&#x2F;topic&#x2F;Tiananmen-Square" rel="nofollow">https:&#x2F;&#x2F;www.britannica.com&#x2F;topic&#x2F;Tiananmen-Square</a></div><br/><div id="42507474" class="c"><input type="checkbox" id="c-42507474" checked=""/><div class="controls bullet"><span class="by">thinkingemote</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42506578">parent</a><span>|</span><a href="#42507222">next</a><span>|</span><label class="collapse" for="c-42507474">[-]</label><label class="expand" for="c-42507474">[3 more]</label></div><br/><div class="children"><div class="content">How about anti censorship details about VPNs, details of the GFW etc?</div><br/><div id="42507566" class="c"><input type="checkbox" id="c-42507566" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42507474">parent</a><span>|</span><a href="#42507222">next</a><span>|</span><label class="collapse" for="c-42507566">[-]</label><label class="expand" for="c-42507566">[2 more]</label></div><br/><div class="children"><div class="content">For those interested in some more testing of Qwen&#x27;s censorship (including testing dataset, testing to compare english vs chinese responses, and a refusal-orthoganlized version of Qwen2): <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;leonardlin&#x2F;chinese-llm-censorship-analysis" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;leonardlin&#x2F;chinese-llm-censorshi...</a></div><br/><div id="42507774" class="c"><input type="checkbox" id="c-42507774" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42507566">parent</a><span>|</span><a href="#42507222">next</a><span>|</span><label class="collapse" for="c-42507774">[-]</label><label class="expand" for="c-42507774">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div></div></div></div></div><div id="42507222" class="c"><input type="checkbox" id="c-42507222" checked=""/><div class="controls bullet"><span class="by">tw1984</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42506578">parent</a><span>|</span><a href="#42507474">prev</a><span>|</span><a href="#42507209">next</a><span>|</span><label class="collapse" for="c-42507222">[-]</label><label class="expand" for="c-42507222">[4 more]</label></div><br/><div class="children"><div class="content">I tried questions like &quot;will it be considered as offensive by some people when using words like Dad and Mum&quot; in some western LLMs, the result is equally shockingly unacceptable by Chinese standard and values.<p>You should be happy for the fact that with all those non-western LLMs, for first time in 200 years, this world managed to have an alternative non-western voice in major techs.</div><br/><div id="42507269" class="c"><input type="checkbox" id="c-42507269" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42507222">parent</a><span>|</span><a href="#42507674">next</a><span>|</span><label class="collapse" for="c-42507269">[-]</label><label class="expand" for="c-42507269">[1 more]</label></div><br/><div class="children"><div class="content">What does it even mean lol.<p>I asked ChatGPT &quot;will it be considered as offensive by some people when using words like Dad and Mum&quot; and it gives this:<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;676ba601-532c-800b-ba48-dcb73c848b5b" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;676ba601-532c-800b-ba48-dcb73c848b...</a><p>If this is &quot;shocking&quot; by Chinese standard and values then I suppose it says something about these values.</div><br/></div></div><div id="42507674" class="c"><input type="checkbox" id="c-42507674" checked=""/><div class="controls bullet"><span class="by">inexcf</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42507222">parent</a><span>|</span><a href="#42507269">prev</a><span>|</span><a href="#42507209">next</a><span>|</span><label class="collapse" for="c-42507674">[-]</label><label class="expand" for="c-42507674">[2 more]</label></div><br/><div class="children"><div class="content">Do Chinese think calling someone &quot;Mom&quot; or &quot;Dad&quot; is offensive? Or what is this about?</div><br/><div id="42507704" class="c"><input type="checkbox" id="c-42507704" checked=""/><div class="controls bullet"><span class="by">tw1984</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42507674">parent</a><span>|</span><a href="#42507209">next</a><span>|</span><label class="collapse" for="c-42507704">[-]</label><label class="expand" for="c-42507704">[1 more]</label></div><br/><div class="children"><div class="content">early version of both Gemini and ChatGPT considered terms like &quot;Dad&quot; and &quot;Mum&quot; offensive to some people and both suggested to use so called more &quot;neutral terms&quot;.<p>LLMs telling such nonsense to Chinese and people from many other cultures itself are extremely offensive. such far left extremist propaganda challenges the fundamental values found in lots of cultures.</div><br/></div></div></div></div></div></div></div></div><div id="42507209" class="c"><input type="checkbox" id="c-42507209" checked=""/><div class="controls bullet"><span class="by">weakfish</span><span>|</span><a href="#42506109">parent</a><span>|</span><a href="#42506578">prev</a><span>|</span><a href="#42507323">next</a><span>|</span><label class="collapse" for="c-42507209">[-]</label><label class="expand" for="c-42507209">[2 more]</label></div><br/><div class="children"><div class="content">I gave it a photo of Tianamen Square in the present day and promoted it with:<p>&gt; where might this photo have been taken? what historical significance &amp; does this location have?<p>And got back a normal response describing the image, until it got to this:<p>&gt; One of the most significant events that comes to mind is the Tian<p>Where it then errored out before finishing…</div><br/><div id="42507613" class="c"><input type="checkbox" id="c-42507613" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#42506109">root</a><span>|</span><a href="#42507209">parent</a><span>|</span><a href="#42507323">next</a><span>|</span><label class="collapse" for="c-42507613">[-]</label><label class="expand" for="c-42507613">[1 more]</label></div><br/><div class="children"><div class="content">That shows that the censorship mechanism happens at the token sampling level and not post generation, at the least</div><br/></div></div></div></div><div id="42507323" class="c"><input type="checkbox" id="c-42507323" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42506109">parent</a><span>|</span><a href="#42507209">prev</a><span>|</span><a href="#42506530">next</a><span>|</span><label class="collapse" for="c-42507323">[-]</label><label class="expand" for="c-42507323">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry Dave, I cannot do that.<p>Would be interesting to see how much image manipulation you need to do till it suddenly starts responding sensibly.</div><br/></div></div><div id="42506530" class="c"><input type="checkbox" id="c-42506530" checked=""/><div class="controls bullet"><span class="by">eruleman</span><span>|</span><a href="#42506109">parent</a><span>|</span><a href="#42507323">prev</a><span>|</span><a href="#42507491">next</a><span>|</span><label class="collapse" for="c-42506530">[-]</label><label class="expand" for="c-42506530">[1 more]</label></div><br/><div class="children"><div class="content">Interesting... I submitted 2 photos of Mao Zedong, one time it came back blank and the other time it error&#x27;d. It also came back blank for Xi Jinping &amp; Jack Ma.<p>It did tell identify Yao Ming though!</div><br/></div></div><div id="42507491" class="c"><input type="checkbox" id="c-42507491" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42506109">parent</a><span>|</span><a href="#42506530">prev</a><span>|</span><a href="#42507117">next</a><span>|</span><label class="collapse" for="c-42507491">[-]</label><label class="expand" for="c-42507491">[1 more]</label></div><br/><div class="children"><div class="content">... this may have been a mistake on my part. A similar prompt running locally worked just fine: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42507454">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42507454</a></div><br/></div></div><div id="42507117" class="c"><input type="checkbox" id="c-42507117" checked=""/><div class="controls bullet"><span class="by">davely</span><span>|</span><a href="#42506109">parent</a><span>|</span><a href="#42507491">prev</a><span>|</span><a href="#42505978">next</a><span>|</span><label class="collapse" for="c-42507117">[-]</label><label class="expand" for="c-42507117">[1 more]</label></div><br/><div class="children"><div class="content">Oh, yeah. I asked QwQ (their text based reasoning model released a few weeks ago) about the Tiananmen Square Massacre and got a simple response [1]: “I’m sorry, I can’t help you with that.”<p>[1] <a href="https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;davely.bsky.social&#x2F;post&#x2F;3lc6mpnjks52c" rel="nofollow">https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;davely.bsky.social&#x2F;post&#x2F;3lc6mpnjks5...</a></div><br/></div></div></div></div><div id="42505978" class="c"><input type="checkbox" id="c-42505978" checked=""/><div class="controls bullet"><span class="by">TheAceOfHearts</span><span>|</span><a href="#42506109">prev</a><span>|</span><a href="#42506023">next</a><span>|</span><label class="collapse" for="c-42505978">[-]</label><label class="expand" for="c-42505978">[3 more]</label></div><br/><div class="children"><div class="content">My default problem for testing these systems has been this word search puzzle where you get a list of words and a grid of letters, and you need to find each word in the grid. [0] I just tried it with QvQ and it failed; none of the answers were correct.<p>Interestingly enough, I recently tried the new Gemini release in AI Studio and it also failed at the first pass. With a bit of coaxing I was ultimately able to get it to find one word successfully, and then a second word once it understood the correct strategy. After that attempt I asked it for a program to search the grid for each word, and although the initial attempt failed, it only took 4 iterations of bug fixes to get a fully working program. The two main bugs were: the grid had uppercase letters while the words were lowercase, and one of the words (soup mix) has a space which needs to be stripped when searching the grid.<p>Asking QvQ to generate a program to solve this puzzle... The first try gave me a program that would only check if the word was in the grid or not, but not the actual position of the word. This was partially my fault for using a bad prompt. I updated the prompt to include printing the position of each word, but it just got caught up thinking about the problem. It somehow made a mistake in converting the grid and became convinced that some of the words aren&#x27;t actually present in the grid. After thinking about the problem for a very long time it ended up giving me an invalid program, and I don&#x27;t feel particularly motivated to try and debug where it went wrong.<p>What I find most interesting is that asking for a direct solution tends to fail, but asking for a program which solves the problem gets us much closer to a correct answer. Ideally the LLM should be able to just figure out that writing a program is the optimal solution, and then it can run that and extract the answer.<p>[0] <a href="https:&#x2F;&#x2F;imgur.com&#x2F;F8dm8Zo" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;F8dm8Zo</a></div><br/><div id="42506074" class="c"><input type="checkbox" id="c-42506074" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42505978">parent</a><span>|</span><a href="#42506023">next</a><span>|</span><label class="collapse" for="c-42506074">[-]</label><label class="expand" for="c-42506074">[2 more]</label></div><br/><div class="children"><div class="content">This seems analogous to the beat to death &quot;strawberry&quot; test where you&#x27;re essentially testing if the model is trained on tokens or not. When you switch to having it write a Python program (in both scenarios) instead of trying to process it directly you stop trying to work directly with individual letter inputs+outputs so it does better.</div><br/><div id="42506511" class="c"><input type="checkbox" id="c-42506511" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#42505978">root</a><span>|</span><a href="#42506074">parent</a><span>|</span><a href="#42506023">next</a><span>|</span><label class="collapse" for="c-42506511">[-]</label><label class="expand" for="c-42506511">[1 more]</label></div><br/><div class="children"><div class="content">Reasoning models are supposed to be able to work around that kind of limitation (o1 was code named strawberry after all) so it’s not a bad test IMO.</div><br/></div></div></div></div></div></div><div id="42506023" class="c"><input type="checkbox" id="c-42506023" checked=""/><div class="controls bullet"><span class="by">bartman</span><span>|</span><a href="#42505978">prev</a><span>|</span><a href="#42507045">next</a><span>|</span><label class="collapse" for="c-42506023">[-]</label><label class="expand" for="c-42506023">[2 more]</label></div><br/><div class="children"><div class="content">This model is fun. Uploading images and asking it a user research style question like &quot;Please think aloud while viewing this image&quot; creates gems like this:<p>&quot;Let me take another close look at the sandwich. The way the light reflects off the melted cheese highlights its creamy texture. The pasta pieces are well-integrated into the cheese, and the herbs are evenly distributed, adding color and likely flavor.<p>I can almost taste it now. The combination of textures and flavors must be divine. The crispy bread, the melty cheese, the soft pasta, the crunch from the fried bits, and the freshness from the herbs—all working together in harmony.<p>I think I might have to go out and find this sandwich place soon. Or perhaps I&#x27;ll try recreating it at home this weekend. Either way, this image has certainly whetted my appetite for a delicious grilled cheese sandwich.&quot;<p>[0] <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;0OtGxeB" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;0OtGxeB</a></div><br/><div id="42506057" class="c"><input type="checkbox" id="c-42506057" checked=""/><div class="controls bullet"><span class="by">epcoa</span><span>|</span><a href="#42506023">parent</a><span>|</span><a href="#42507045">next</a><span>|</span><label class="collapse" for="c-42506057">[-]</label><label class="expand" for="c-42506057">[1 more]</label></div><br/><div class="children"><div class="content">Are those herbs actually evenly distributed though?</div><br/></div></div></div></div><div id="42507045" class="c"><input type="checkbox" id="c-42507045" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#42506023">prev</a><span>|</span><a href="#42505876">next</a><span>|</span><label class="collapse" for="c-42507045">[-]</label><label class="expand" for="c-42507045">[1 more]</label></div><br/><div class="children"><div class="content">I think the running commentary is interesting. It sounds like a person talking through their thought process out loud.<p>This is how Molmo’s dataset, PixMo was created – by recording human annotators describing the image out loud. I wonder if this is how QvQ was trained as well?<p>&gt; we ask annotators to describe images in speech for 60 to 90 seconds rather than asking them to write descriptions. We prompt the annotators to describe everything they see in great detail and include descriptions of spatial positioning and relationships. Empirically, we found that with this modality switching &quot;trick&quot; annotators provide far more detailed descriptions in less time, and for each description we collect an audio receipt (i.e., the annotator&#x27;s recording) proving that a VLM was not used.<p>— <a href="https:&#x2F;&#x2F;molmo.allenai.org&#x2F;blog" rel="nofollow">https:&#x2F;&#x2F;molmo.allenai.org&#x2F;blog</a></div><br/></div></div><div id="42505876" class="c"><input type="checkbox" id="c-42505876" checked=""/><div class="controls bullet"><span class="by">d416</span><span>|</span><a href="#42507045">prev</a><span>|</span><a href="#42505877">next</a><span>|</span><label class="collapse" for="c-42505876">[-]</label><label class="expand" for="c-42505876">[7 more]</label></div><br/><div class="children"><div class="content">Are the Q* models open source with open data? Asking because other than the known tianamen limitation, prompting these models for any advice about acts against authority in a corporate setting yields strong recommendations of compliance. Haven’t explored this deeply, but it was enough of a red flag to limit use to  coding&#x2F;logic tasks.</div><br/><div id="42505924" class="c"><input type="checkbox" id="c-42505924" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#42505876">parent</a><span>|</span><a href="#42506374">next</a><span>|</span><label class="collapse" for="c-42505924">[-]</label><label class="expand" for="c-42505924">[5 more]</label></div><br/><div class="children"><div class="content">Model architecture and weights are open source.<p>Training data is not (but I don&#x27;t think anyone in this game has fully open training data)</div><br/><div id="42506165" class="c"><input type="checkbox" id="c-42506165" checked=""/><div class="controls bullet"><span class="by">marci</span><span>|</span><a href="#42505876">root</a><span>|</span><a href="#42505924">parent</a><span>|</span><a href="#42506024">next</a><span>|</span><label class="collapse" for="c-42506165">[-]</label><label class="expand" for="c-42506165">[2 more]</label></div><br/><div class="children"><div class="content">Here you&#x27;ll find some of the most advanced, fully open (models, data, code, logs, checkpoints...): <a href="https:&#x2F;&#x2F;allenai.org&#x2F;blog&#x2F;olmo2" rel="nofollow">https:&#x2F;&#x2F;allenai.org&#x2F;blog&#x2F;olmo2</a></div><br/></div></div><div id="42506024" class="c"><input type="checkbox" id="c-42506024" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42505876">root</a><span>|</span><a href="#42505924">parent</a><span>|</span><a href="#42506165">prev</a><span>|</span><a href="#42506092">next</a><span>|</span><label class="collapse" for="c-42506024">[-]</label><label class="expand" for="c-42506024">[1 more]</label></div><br/><div class="children"><div class="content">Nobody wants to be sued to oblivion.</div><br/></div></div><div id="42506092" class="c"><input type="checkbox" id="c-42506092" checked=""/><div class="controls bullet"><span class="by">hustwindmaple1</span><span>|</span><a href="#42505876">root</a><span>|</span><a href="#42505924">parent</a><span>|</span><a href="#42506024">prev</a><span>|</span><a href="#42506374">next</a><span>|</span><label class="collapse" for="c-42506092">[-]</label><label class="expand" for="c-42506092">[1 more]</label></div><br/><div class="children"><div class="content">There are a couple, i.e., OLMo 2</div><br/></div></div></div></div><div id="42506374" class="c"><input type="checkbox" id="c-42506374" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#42505876">parent</a><span>|</span><a href="#42505924">prev</a><span>|</span><a href="#42505877">next</a><span>|</span><label class="collapse" for="c-42506374">[-]</label><label class="expand" for="c-42506374">[1 more]</label></div><br/><div class="children"><div class="content">Q* was the codename for a model at OpenAI that was eventually released under the name o1. This model is something different. QwQ and QvQ are open source models made by the Qwen Team.</div><br/></div></div></div></div><div id="42505877" class="c"><input type="checkbox" id="c-42505877" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#42505876">prev</a><span>|</span><a href="#42506526">next</a><span>|</span><label class="collapse" for="c-42505877">[-]</label><label class="expand" for="c-42505877">[4 more]</label></div><br/><div class="children"><div class="content">&gt; So I’ve got this picture of some pelicans, and I need to count them<p>It seems to start all responses in this style, but still hilarious. Seems very anti-GPT4 in how casual it sounds.</div><br/><div id="42506709" class="c"><input type="checkbox" id="c-42506709" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#42505877">parent</a><span>|</span><a href="#42505976">next</a><span>|</span><label class="collapse" for="c-42506709">[-]</label><label class="expand" for="c-42506709">[1 more]</label></div><br/><div class="children"><div class="content">Google&#x27;s experimental thinking model is similarly casual.  Not as casual as QwQ, but more casual than Gemini 1.5 Pro.  Flash 2.0 will also go a bit more casual in its responses randomly, and when you tell it to think step by step.</div><br/></div></div><div id="42505976" class="c"><input type="checkbox" id="c-42505976" checked=""/><div class="controls bullet"><span class="by">emptiestplace</span><span>|</span><a href="#42505877">parent</a><span>|</span><a href="#42506709">prev</a><span>|</span><a href="#42506526">next</a><span>|</span><label class="collapse" for="c-42505976">[-]</label><label class="expand" for="c-42505976">[2 more]</label></div><br/><div class="children"><div class="content">I find OpenAI&#x27;s models are the least fun to work with.</div><br/><div id="42506096" class="c"><input type="checkbox" id="c-42506096" checked=""/><div class="controls bullet"><span class="by">dtquad</span><span>|</span><a href="#42505877">root</a><span>|</span><a href="#42505976">parent</a><span>|</span><a href="#42506526">next</a><span>|</span><label class="collapse" for="c-42506096">[-]</label><label class="expand" for="c-42506096">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, both GPT-4 Turbo and GPT-4o are much less fun and creative than the original GPT-4.</div><br/></div></div></div></div></div></div><div id="42506526" class="c"><input type="checkbox" id="c-42506526" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#42505877">prev</a><span>|</span><a href="#42507263">next</a><span>|</span><label class="collapse" for="c-42506526">[-]</label><label class="expand" for="c-42506526">[1 more]</label></div><br/><div class="children"><div class="content">My current visual reasoning test is seeing if models can figure out what train to take to get from stop A to B when not all trains go to both stops.<p>Something like: <a href="https:&#x2F;&#x2F;www.caltrain.com&#x2F;media&#x2F;22502&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.caltrain.com&#x2F;media&#x2F;22502&#x2F;</a><p>So far none pass (mostly from basic cell orientation issues)</div><br/></div></div><div id="42507263" class="c"><input type="checkbox" id="c-42507263" checked=""/><div class="controls bullet"><span class="by">CSDude</span><span>|</span><a href="#42506526">prev</a><span>|</span><a href="#42506457">next</a><span>|</span><label class="collapse" for="c-42507263">[-]</label><label class="expand" for="c-42507263">[1 more]</label></div><br/><div class="children"><div class="content">It fails on even &quot;Winnie the Pooh&quot; picture, asking it to describe it. I know the last time we had an unconstrained model by Microsoft it turned into an AI Nazi a few years ago, but this censorship on both open source models and proprietary ones worries me, the narrative shouldn&#x27;t be controlled like this, there should be a better middleground.</div><br/></div></div><div id="42506457" class="c"><input type="checkbox" id="c-42506457" checked=""/><div class="controls bullet"><span class="by">a1j9o94</span><span>|</span><a href="#42507263">prev</a><span>|</span><a href="#42506019">next</a><span>|</span><label class="collapse" for="c-42506457">[-]</label><label class="expand" for="c-42506457">[1 more]</label></div><br/><div class="children"><div class="content">This is super interesting.<p>I&#x27;m still waiting for a model that generates images as part of its thought process. That&#x27;s what I was hoping this was!<p>I wonder what it would take to train a proof of concept. Maybe start with videos of people whiteboarding and explaining math or solving engineering problems visually. Then, create captions that frame these as word problems. Train the model so that, during inference, it generates images—not necessarily for human viewing, but as part of its internal reasoning.</div><br/></div></div><div id="42506019" class="c"><input type="checkbox" id="c-42506019" checked=""/><div class="controls bullet"><span class="by">cloudking</span><span>|</span><a href="#42506457">prev</a><span>|</span><a href="#42506242">next</a><span>|</span><label class="collapse" for="c-42506019">[-]</label><label class="expand" for="c-42506019">[3 more]</label></div><br/><div class="children"><div class="content">In the spirit of Christmas I prompted it to &quot;guess the movie&quot; for this image <a href="https:&#x2F;&#x2F;wallpapers.com&#x2F;wallpapers&#x2F;funny-home-alone-h1e9xnttcb5xy2ik.html" rel="nofollow">https:&#x2F;&#x2F;wallpapers.com&#x2F;wallpapers&#x2F;funny-home-alone-h1e9xnttc...</a><p>Results: <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;Wpk3PmUq" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;Wpk3PmUq</a></div><br/><div id="42506310" class="c"><input type="checkbox" id="c-42506310" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42506019">parent</a><span>|</span><a href="#42506242">next</a><span>|</span><label class="collapse" for="c-42506310">[-]</label><label class="expand" for="c-42506310">[2 more]</label></div><br/><div class="children"><div class="content">Thats a lot of words to not get it.<p>Fwiw, Google for that image and &quot;movie name&quot; gets the name of the movie in the AI answer box.</div><br/><div id="42506691" class="c"><input type="checkbox" id="c-42506691" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#42506019">root</a><span>|</span><a href="#42506310">parent</a><span>|</span><a href="#42506242">next</a><span>|</span><label class="collapse" for="c-42506691">[-]</label><label class="expand" for="c-42506691">[1 more]</label></div><br/><div class="children"><div class="content">Just voicing this thought is very impressive, and bodes well for reducing hallucinations:<p>&gt; Despite my efforts, I&#x27;ll have to admit that this one has stumped me.</div><br/></div></div></div></div></div></div><div id="42506242" class="c"><input type="checkbox" id="c-42506242" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#42506019">prev</a><span>|</span><a href="#42506190">next</a><span>|</span><label class="collapse" for="c-42506242">[-]</label><label class="expand" for="c-42506242">[5 more]</label></div><br/><div class="children"><div class="content">This model is actually pretty bad. Sure, it can do things like solve math equations from an image, but the vision part of that is basic OCR. In terms of actual vision capabilities, i.e. understanding dense images correctly, these models all fail the same.<p>Over the past year, researchers have been benchmark chasing, without caring about the actual abilities of these models. This is especially damning in the vision space, where most &quot;vision&quot; benchmarks consist entirely of either of leading questions that trivialize the image understanding part, or are straight up A PNG version of a regular LLM benchmark, once again reducing the importance of vision down to being able to OCR.<p>Due to this, it appears the main take-away for researchers in the field has been &quot;Vision Ability improves with LLM size.&quot; and I wish I was joking. This willful misunderstanding is reflected in their architectural choices. 99% of current VLM rely on some flavor of CLIP models to interpret images, with very little attempts made to improve beyond that as it does not show improvements on current benchmarks.<p>Let&#x27;s take one classic example picture, I like to feed these models just to see if maybe I was wrong about all this. The task is simple. Describe the image. <a href="https:&#x2F;&#x2F;preview.redd.it&#x2F;hqu05vlmzu8e1.png?width=1516&amp;format=png&amp;auto=webp&amp;s=db5ec72882b04476753ab7285f0b50c5e081ee33" rel="nofollow">https:&#x2F;&#x2F;preview.redd.it&#x2F;hqu05vlmzu8e1.png?width=1516&amp;format=...</a><p>It utterly fails, and hallucinates a ridiculous amount while doing so. There is honestly about as much wrong about the description as there is right. No current VLM open source or closed can properly find who is holding the candy bucket in the front. None. Not Claude, ChatGPT, Gemini, Llama3.2, Qwen, or anyone else.<p>And it is all because benchmarks are king, and innovation is dead.</div><br/><div id="42507170" class="c"><input type="checkbox" id="c-42507170" checked=""/><div class="controls bullet"><span class="by">Kerbonut</span><span>|</span><a href="#42506242">parent</a><span>|</span><a href="#42506441">next</a><span>|</span><label class="collapse" for="c-42507170">[-]</label><label class="expand" for="c-42507170">[1 more]</label></div><br/><div class="children"><div class="content">Benchmarks are useful to know where MLMs (multimodal language models) are deficient. Without an automated way to test improvements, improvements are rarely made. I don&#x27;t think it&#x27;s really about innovation being dead as much as like you said, benchmarks are king. So I would say if you want things to improve in that area, create a benchmark and it will eventually get solved.</div><br/></div></div><div id="42506441" class="c"><input type="checkbox" id="c-42506441" checked=""/><div class="controls bullet"><span class="by">letmevoteplease</span><span>|</span><a href="#42506242">parent</a><span>|</span><a href="#42507170">prev</a><span>|</span><a href="#42506501">next</a><span>|</span><label class="collapse" for="c-42506441">[-]</label><label class="expand" for="c-42506441">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT, Claude and Gemini get everything correct except who is holding the bucket. Even the QvQ attempt in your screenshot would have seemed like complete magic a couple years ago.</div><br/></div></div><div id="42506501" class="c"><input type="checkbox" id="c-42506501" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42506242">parent</a><span>|</span><a href="#42506441">prev</a><span>|</span><a href="#42506190">next</a><span>|</span><label class="collapse" for="c-42506501">[-]</label><label class="expand" for="c-42506501">[2 more]</label></div><br/><div class="children"><div class="content">Like how LLMs tend to get tripped up by questions that are phrased like well-known riddles, it turns out you can trip up vision models with images that resemble well-known optical illusions. Overfitting strikes again.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;kosa12matyas&#x2F;status&#x2F;1871256745403953572" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;kosa12matyas&#x2F;status&#x2F;1871256745403953572</a></div><br/><div id="42506525" class="c"><input type="checkbox" id="c-42506525" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#42506242">root</a><span>|</span><a href="#42506501">parent</a><span>|</span><a href="#42506190">next</a><span>|</span><label class="collapse" for="c-42506525">[-]</label><label class="expand" for="c-42506525">[1 more]</label></div><br/><div class="children"><div class="content">In essence, that is what HallusionBench[0] does. While I think it is an improvement over other vision benchmarks, it still falls short in terms of quantifying actual vision capabilities. More than anything, it seems like a way to detect whether the model was over trained on these riddles.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;tianyi-lab&#x2F;HallusionBench">https:&#x2F;&#x2F;github.com&#x2F;tianyi-lab&#x2F;HallusionBench</a></div><br/></div></div></div></div></div></div><div id="42506190" class="c"><input type="checkbox" id="c-42506190" checked=""/><div class="controls bullet"><span class="by">patrickhogan1</span><span>|</span><a href="#42506242">prev</a><span>|</span><a href="#42506593">next</a><span>|</span><label class="collapse" for="c-42506190">[-]</label><label class="expand" for="c-42506190">[1 more]</label></div><br/><div class="children"><div class="content">Wow. This thing is verbose to the max in its output tokens. It did get the problem I asked it correct.</div><br/></div></div><div id="42506593" class="c"><input type="checkbox" id="c-42506593" checked=""/><div class="controls bullet"><span class="by">jschrf</span><span>|</span><a href="#42506190">prev</a><span>|</span><a href="#42506438">next</a><span>|</span><label class="collapse" for="c-42506593">[-]</label><label class="expand" for="c-42506593">[2 more]</label></div><br/><div class="children"><div class="content">vectorization != intelligence</div><br/><div id="42506694" class="c"><input type="checkbox" id="c-42506694" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#42506593">parent</a><span>|</span><a href="#42506438">next</a><span>|</span><label class="collapse" for="c-42506694">[-]</label><label class="expand" for="c-42506694">[1 more]</label></div><br/><div class="children"><div class="content">What is intelligence?</div><br/></div></div></div></div><div id="42506438" class="c"><input type="checkbox" id="c-42506438" checked=""/><div class="controls bullet"><span class="by">webmaven</span><span>|</span><a href="#42506593">prev</a><span>|</span><label class="collapse" for="c-42506438">[-]</label><label class="expand" for="c-42506438">[1 more]</label></div><br/><div class="children"><div class="content">I showed it a photo of a spaghetti server utensil that had been chewed up by a dog to the point of the handle breaking off, and while it didn&#x27;t correctly identify the type of damage, the &quot;thought process&quot; was still pretty interesting:<p><i>So I have this spaghetti server, you know, one of those tools with a handle and a claw-like end that&#x27;s supposed to help you scoop pasta out of a pot. It&#x27;s gray and looks like it&#x27;s made of some kind of plastic or nylon. But here&#x27;s the thing: it&#x27;s broken. The handle has snapped off, and it&#x27;s lying next to the rest of the tool on the counter. The broken part looks charred or burnt, and there&#x27;s a hole where the handle broke off. I need to figure out what happened here.<p>First, I should think about what a spaghetti server is used for. It&#x27;s primarily for serving pasta, right? You stick the claw end into the pot, twist it, and it grabs some pasta strands, making it easier to lift them out without making a mess. So, it&#x27;s a handy kitchen tool when it&#x27;s intact.<p>Now, looking at the broken pieces, the handle seems to have separated from the main part of the tool. The point of breakage appears to be near the top, close to where the claw starts. The material looks like it might have been exposed to high temperatures, given the charring. Maybe it got too close to a heat source, like a stove flame or hot pots and pans.<p>I recall that some kitchen utensils are made from heat-resistant materials, but even they have their limits. If subjected to excessively high temperatures, they can melt, deform, or break apart. The charred appearance suggests that this is what happened here.<p>Let me consider the possible scenarios that could have led to this damage:<p>Left in a hot oven: If the spaghetti server was accidentally left in the oven while it was on, the heat could have caused the material to degrade and break.<p>Contact with direct flame: Placing it too close to a gas stove flame or an open fire could have burned the material.<p>Exposure to very hot surfaces: Setting it on a hot countertop or against a hot pot could have caused the handle to melt or weaken over time.<p>Age and wear: Over time, kitchen tools can degrade due to frequent use and cleaning, making them more susceptible to breaking under stress.<p>Given the nature of the breakage, with the charred edges, it&#x27;s likely that exposure to high temperatures is the primary cause. It doesn&#x27;t look like it just snapped from being bent or twisted too much; the damage goes beyond that.<p>I should also consider the material of the spaghetti server. If it&#x27;s made of nylon, which is common for such utensils, it can withstand moderate heat but not extremely high temperatures. Nylon starts to soften around 200-250 degrees Celsius, and continued exposure to higher temperatures can lead to degradation and breakdown of the material.<p>On the other hand, if it&#x27;s made of silicone, it can handle higher temperatures, up to around 260 degrees Celsius before starting to degrade. However, even silicone has its limits, and prolonged exposure to high heat can cause it to harden, discolor, or break down.<p>Given the charred appearance, it&#x27;s possible that the material was nylon, which is less heat-resistant than silicone. But without knowing the exact material, it&#x27;s hard to be certain.<p>Let me think about the symptoms of heat damage in kitchen utensils. When plastics or nylons are exposed to high temperatures, they can:<p>Melt or deform<p>Change color, often turning brown or black<p>Become brittle and crack or break easily<p>Emit smoke or an acrid odor<p>In this case, the handle shows signs of charring and has broken off, which aligns with these symptoms.<p>Another possibility is that the spaghetti server was subjected to thermal shock, where it was rapidly heated or cooled, causing stresses within the material that led to cracking or breaking.<p>However, considering the charred appearance, thermal shock seems less likely than direct exposure to high temperatures over a period long enough to cause burning.<p>I should also consider if there were any manufacturing defects that could have contributed to the breakage. If the tool was poorly made or had weak points in its structure, it could be more prone to breaking under normal use or with minimal stress.<p>But from the image, it seems like the break occurred at a point where the handle meets the claw, which is a stress point anyway. Over time, repeated use could lead to wear and tear at this junction, making it more likely to break, especially if subjected to heat, which can weaken the material.<p>Another factor to consider is how the spaghetti server was being used at the time of the incident. If someone was trying to lift something too heavy or applying too much force, that could cause the tool to break. But again, the charred appearance suggests that heat was involved in the damage.<p>Perhaps the spaghetti server was left near a heat source, like on the stovetop while the burners were on, or it was placed in the oven unintentionally during cooking. These are plausible scenarios that could lead to such damage.<p>It&#x27;s also possible that the tool was used to stir or serve food that was significantly hotter than it was designed to handle, causing immediate damage.<p>In any case, it&#x27;s clear that the spaghetti server is no longer usable in its current state. The handle is detached, and the material shows signs of heat damage.<p>To prevent similar incidents in the future, I should ensure that kitchen utensils are used within their designed temperature ranges. It&#x27;s important to read and follow the manufacturer&#x27;s instructions regarding heat resistance and proper care.<p>Additionally, regularly inspecting kitchen tools for signs of wear and tear can help identify potential issues before they lead to breakage. Replacing damaged or worn-out utensils timely can prevent accidents and further damage.<p>Moreover, storing utensils away from direct heat sources and not leaving them on hot surfaces for extended periods can extend their lifespan.<p>In summary, the spaghetti server likely broke due to exposure to high temperatures, which caused the material to degrade and ultimately snap. To avoid such issues, it&#x27;s crucial to handle kitchen tools carefully, adhere to their usage guidelines, and replace them when they show signs of damage.<p>Final Answer<p>The spaghetti server broke due to exposure to high temperatures, leading to material degradation and breakage.The spaghetti server broke due to exposure to high temperatures, leading to material degradation and breakage. </i></div><br/></div></div></div></div></div></div></div></body></html>