<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714035655433" as="style"/><link rel="stylesheet" href="styles.css?v=1714035655433"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://medium.com/@sumandas0/fine-tune-llama3-on-million-scale-dataset-in-consumer-gpu-using-qlora-deepspeed-3ae8ad75299a">Fine tune LLAMA3 on million scale dataset in consumer GPU using QLora, DeepSpeed</a> <span class="domain">(<a href="https://medium.com">medium.com</a>)</span></div><div class="subtext"><span>mehulashah</span> | <span>17 comments</span></div><br/><div><div id="40152843" class="c"><input type="checkbox" id="c-40152843" checked=""/><div class="controls bullet"><span class="by">unraveller</span><span>|</span><a href="#40153600">next</a><span>|</span><label class="collapse" for="c-40152843">[-]</label><label class="expand" for="c-40152843">[13 more]</label></div><br/><div class="children"><div class="content">This is a thorough &quot;how to&quot; but it is missing a &quot;why for&quot; about any of the chosen starting elements.<p>I don&#x27;t understand why you would use an old dataset that worked for llama2 and just fine-tune llama3 on it. Isn&#x27;t it most likely that the new model has covered off everything it missed last time around and now the last dataset is only valuable for the last gen.</div><br/><div id="40153240" class="c"><input type="checkbox" id="c-40153240" checked=""/><div class="controls bullet"><span class="by">factorymoo</span><span>|</span><a href="#40152843">parent</a><span>|</span><a href="#40153495">next</a><span>|</span><label class="collapse" for="c-40153240">[-]</label><label class="expand" for="c-40153240">[3 more]</label></div><br/><div class="children"><div class="content">This might be an unfair statement but it really feels like all of these blogs don&#x27;t know why. They copy&#x2F;paste each other (you often seem the same errors in multiple notebooks&#x2F;blogs) and I have a feeling no one really deeply understands what they&#x27;re doing.</div><br/><div id="40153887" class="c"><input type="checkbox" id="c-40153887" checked=""/><div class="controls bullet"><span class="by">jackblemming</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153240">parent</a><span>|</span><a href="#40153495">next</a><span>|</span><label class="collapse" for="c-40153887">[-]</label><label class="expand" for="c-40153887">[2 more]</label></div><br/><div class="children"><div class="content">Most of the entire field of machine learning is “try shit and see what works”. So it seems like they’re par for the course.</div><br/><div id="40154794" class="c"><input type="checkbox" id="c-40154794" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153887">parent</a><span>|</span><a href="#40153495">next</a><span>|</span><label class="collapse" for="c-40154794">[-]</label><label class="expand" for="c-40154794">[1 more]</label></div><br/><div class="children"><div class="content">Same as software engineering field too.</div><br/></div></div></div></div></div></div><div id="40153495" class="c"><input type="checkbox" id="c-40153495" checked=""/><div class="controls bullet"><span class="by">sa-code</span><span>|</span><a href="#40152843">parent</a><span>|</span><a href="#40153240">prev</a><span>|</span><a href="#40153562">next</a><span>|</span><label class="collapse" for="c-40153495">[-]</label><label class="expand" for="c-40153495">[3 more]</label></div><br/><div class="children"><div class="content">Thank you for saying this! The number of people that would need to fine tune vs just using RAG is really small. People that are not familiar with the source often jump to fine tuning as an option</div><br/><div id="40153688" class="c"><input type="checkbox" id="c-40153688" checked=""/><div class="controls bullet"><span class="by">Foobar8568</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153495">parent</a><span>|</span><a href="#40153562">next</a><span>|</span><label class="collapse" for="c-40153688">[-]</label><label class="expand" for="c-40153688">[2 more]</label></div><br/><div class="children"><div class="content">I am still unsure where to stand on this fine tuning vs rag. I feel that for live data, rag would be preferable but for daily&#x2F;weekly updated one, then fine tuning.<p>Another aspect where I am unsure is multi user for a model e.g. can we have concurrency for a model or the queries have to be queued.</div><br/><div id="40154530" class="c"><input type="checkbox" id="c-40154530" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153688">parent</a><span>|</span><a href="#40153562">next</a><span>|</span><label class="collapse" for="c-40154530">[-]</label><label class="expand" for="c-40154530">[1 more]</label></div><br/><div class="children"><div class="content">Fine tuning doesn’t ’add content’ the way RAG does though. They’re not really comparable in that way.</div><br/></div></div></div></div></div></div><div id="40153562" class="c"><input type="checkbox" id="c-40153562" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40152843">parent</a><span>|</span><a href="#40153495">prev</a><span>|</span><a href="#40153119">next</a><span>|</span><label class="collapse" for="c-40153562">[-]</label><label class="expand" for="c-40153562">[2 more]</label></div><br/><div class="children"><div class="content">The &quot;why for&quot; is usually learning&#x2F;gaining experience&#x2F;FOMO.</div><br/><div id="40153593" class="c"><input type="checkbox" id="c-40153593" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153562">parent</a><span>|</span><a href="#40153119">next</a><span>|</span><label class="collapse" for="c-40153593">[-]</label><label class="expand" for="c-40153593">[1 more]</label></div><br/><div class="children"><div class="content">For the human or the LLM?</div><br/></div></div></div></div><div id="40153119" class="c"><input type="checkbox" id="c-40153119" checked=""/><div class="controls bullet"><span class="by">blackoil</span><span>|</span><a href="#40152843">parent</a><span>|</span><a href="#40153562">prev</a><span>|</span><a href="#40153600">next</a><span>|</span><label class="collapse" for="c-40153119">[-]</label><label class="expand" for="c-40153119">[4 more]</label></div><br/><div class="children"><div class="content">Dataset may not be public. All large companies have millions of internal documents. Internal LLM can be trained on them.</div><br/><div id="40153177" class="c"><input type="checkbox" id="c-40153177" checked=""/><div class="controls bullet"><span class="by">bradfox2</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153119">parent</a><span>|</span><a href="#40153600">next</a><span>|</span><label class="collapse" for="c-40153177">[-]</label><label class="expand" for="c-40153177">[3 more]</label></div><br/><div class="children"><div class="content">Qlora won&#x27;t work well to add knowledge via private data.<p>Parameter efficient methods are not useful for these cases at the 8b scale without a more complex training procedure that periodically merges back adapters. Maybe at the 70B scale.</div><br/><div id="40153306" class="c"><input type="checkbox" id="c-40153306" checked=""/><div class="controls bullet"><span class="by">tpurves</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153177">parent</a><span>|</span><a href="#40153600">next</a><span>|</span><label class="collapse" for="c-40153306">[-]</label><label class="expand" for="c-40153306">[2 more]</label></div><br/><div class="children"><div class="content">What scale of company do you need to be to actually be able afford and get return on investment on retraining base models with your own proprietary knowledge and docs? Considering also the implications of continually retraining?</div><br/><div id="40153423" class="c"><input type="checkbox" id="c-40153423" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#40152843">root</a><span>|</span><a href="#40153306">parent</a><span>|</span><a href="#40153600">next</a><span>|</span><label class="collapse" for="c-40153423">[-]</label><label class="expand" for="c-40153423">[1 more]</label></div><br/><div class="children"><div class="content">I was under the impression that you wouldn&#x27;t.  If you want access to proprietary knowledge, you would use RAG + LLM.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40153600" class="c"><input type="checkbox" id="c-40153600" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#40152843">prev</a><span>|</span><a href="#40153639">next</a><span>|</span><label class="collapse" for="c-40153600">[-]</label><label class="expand" for="c-40153600">[2 more]</label></div><br/><div class="children"><div class="content">With unsloth&#x27;s optimizations you can do llama-3-8b&#x27;s QLoRA fine-tuning on your 8GB card(mine&#x27;s a 2070S) with 900MB to spare with BS of 4.</div><br/></div></div><div id="40153639" class="c"><input type="checkbox" id="c-40153639" checked=""/><div class="controls bullet"><span class="by">SunlitCat</span><span>|</span><a href="#40153600">prev</a><span>|</span><label class="collapse" for="c-40153639">[-]</label><label class="expand" for="c-40153639">[1 more]</label></div><br/><div class="children"><div class="content">Since the crypto (currency) craze of 2017, every time I hear &quot;consumer GPU&quot; somewhere in a story that has nothing to do with gaming, it sends a chill down my spine.</div><br/></div></div></div></div></div></div></div></body></html>