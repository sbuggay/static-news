<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735030877304" as="style"/><link rel="stylesheet" href="styles.css?v=1735030877304"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://xenaproject.wordpress.com/2024/12/22/can-ai-do-maths-yet-thoughts-from-a-mathematician/">Can AI do maths yet? Thoughts from a mathematician</a> <span class="domain">(<a href="https://xenaproject.wordpress.com">xenaproject.wordpress.com</a>)</span></div><div class="subtext"><span>mathgenius</span> | <span>244 comments</span></div><br/><div><div id="42499713" class="c"><input type="checkbox" id="c-42499713" checked=""/><div class="controls bullet"><span class="by">Syzygies</span><span>|</span><a href="#42493979">next</a><span>|</span><label class="collapse" for="c-42499713">[-]</label><label class="expand" for="c-42499713">[17 more]</label></div><br/><div class="children"><div class="content">&quot;Can AI do math for us&quot; is the canonical wrong question. People want self-driving cars so they can drink and watch TV. We should crave tools that enhance our abilities, as tools have done since prehistoric times.<p>I&#x27;m a research mathematician. In the 1980&#x27;s I&#x27;d ask everyone I knew a question, and flip through the hard bound library volumes of Mathematical Reviews, hoping to recognize something. If I was lucky, I&#x27;d get a hit in three weeks.<p>Internet search has shortened this turnaround. One instead needs to guess what someone else might call an idea. &quot;Broken circuits?&quot; Score! Still, time consuming.<p>I went all in on ChatGPT after hearing that Terry Tao had learned the Lean 4 proof assistant in a matter of weeks, relying heavily on AI advice. It&#x27;s clumsy, but a very fast way to get suggestions.<p>Now, one can hold involved conversations with ChatGPT or Claude, exploring mathematical ideas. AI is often wrong, never knows when it&#x27;s wrong, but people are like this too. Read how the insurance incidents for self-driving taxis are well below the human incident rates? Talking to fellow mathematicians can be frustrating, and so is talking with AI, but AI conversations go faster and can take place in the middle of the night.<p>I don&#x27;t want AI to prove theorems for me, those theorems will be as boring as most of the dreck published by humans. I want AI to inspire bursts of creativity in humans.</div><br/><div id="42500066" class="c"><input type="checkbox" id="c-42500066" checked=""/><div class="controls bullet"><span class="by">amanda99</span><span>|</span><a href="#42499713">parent</a><span>|</span><a href="#42500269">next</a><span>|</span><label class="collapse" for="c-42500066">[-]</label><label class="expand" for="c-42500066">[4 more]</label></div><br/><div class="children"><div class="content">&gt; AI is often wrong, never knows when it&#x27;s wrong, but people are like this too.<p>When talking with various models of ChatGPT about research math, my biggest gripe is that it&#x27;s either confidently right (10% of my work) or confidently wrong (90%). A human researcher would be right 15% of the time, unsure 50% of the time, and give helpful ideas that are right&#x2F;helpful (25%) or wrong&#x2F;a red herring (10%). And only 5% of the time would a good researcher be confidently wrong in a way that ChatGPT is often.<p>In other words, ChatGPT completely lacks the meta-layer of &quot;having a feeling&#x2F;knowing how confident it is&quot;, which is so useful in research.</div><br/><div id="42500479" class="c"><input type="checkbox" id="c-42500479" checked=""/><div class="controls bullet"><span class="by">halayli</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42500066">parent</a><span>|</span><a href="#42500129">next</a><span>|</span><label class="collapse" for="c-42500479">[-]</label><label class="expand" for="c-42500479">[1 more]</label></div><br/><div class="children"><div class="content">these numbers are just your perception. The way you ask the question will  very much influence the output and certain topics more than others. I get much better results when I share my certainty levels in my questions and say things like &quot;if at all&quot;, &quot;if any&quot; etc.</div><br/></div></div><div id="42500129" class="c"><input type="checkbox" id="c-42500129" checked=""/><div class="controls bullet"><span class="by">portaouflop</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42500066">parent</a><span>|</span><a href="#42500479">prev</a><span>|</span><a href="#42500382">next</a><span>|</span><label class="collapse" for="c-42500129">[-]</label><label class="expand" for="c-42500129">[1 more]</label></div><br/><div class="children"><div class="content">A human researcher that is basically right 40%-95% of the time would probably an Einstein level genius.<p>Just assume that the LLM is wrong and test their assumptions - math is one of the few disciplines where you can do that easily</div><br/></div></div><div id="42500382" class="c"><input type="checkbox" id="c-42500382" checked=""/><div class="controls bullet"><span class="by">eleveriven</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42500066">parent</a><span>|</span><a href="#42500129">prev</a><span>|</span><a href="#42500269">next</a><span>|</span><label class="collapse" for="c-42500382">[-]</label><label class="expand" for="c-42500382">[1 more]</label></div><br/><div class="children"><div class="content">Do you think there’s potential for AI to develop a kind of probabilistic reasoning?</div><br/></div></div></div></div><div id="42500269" class="c"><input type="checkbox" id="c-42500269" checked=""/><div class="controls bullet"><span class="by">heresie-dabord</span><span>|</span><a href="#42499713">parent</a><span>|</span><a href="#42500066">prev</a><span>|</span><a href="#42500376">next</a><span>|</span><label class="collapse" for="c-42500269">[-]</label><label class="expand" for="c-42500269">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Talking to fellow &lt;humans&gt; can be frustrating, and so is talking with AI, but AI conversations go faster and can take place in the middle of the night.<p>I made a slight change to generalise your statement, I think you have summarised the actual marketing opportunity.</div><br/></div></div><div id="42500376" class="c"><input type="checkbox" id="c-42500376" checked=""/><div class="controls bullet"><span class="by">eleveriven</span><span>|</span><a href="#42499713">parent</a><span>|</span><a href="#42500269">prev</a><span>|</span><a href="#42500386">next</a><span>|</span><label class="collapse" for="c-42500376">[-]</label><label class="expand" for="c-42500376">[1 more]</label></div><br/><div class="children"><div class="content">The analogy with self-driving cars is spot on</div><br/></div></div><div id="42499799" class="c"><input type="checkbox" id="c-42499799" checked=""/><div class="controls bullet"><span class="by">didibus</span><span>|</span><a href="#42499713">parent</a><span>|</span><a href="#42500386">prev</a><span>|</span><a href="#42499728">next</a><span>|</span><label class="collapse" for="c-42499799">[-]</label><label class="expand" for="c-42499799">[6 more]</label></div><br/><div class="children"><div class="content">I think I&#x27;m missing your point? You still want to enjoy doing math yourself? Is that what you are saying? So you equate &quot;Can AI do math in my place?&quot; with &quot;Can AI drink and watch TV in my place?&quot;</div><br/><div id="42500017" class="c"><input type="checkbox" id="c-42500017" checked=""/><div class="controls bullet"><span class="by">elbear</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42499799">parent</a><span>|</span><a href="#42499837">next</a><span>|</span><label class="collapse" for="c-42500017">[-]</label><label class="expand" for="c-42500017">[1 more]</label></div><br/><div class="children"><div class="content">In a way, AI is part of the process, but it&#x27;s a collaborative process. It
doesn&#x27;t do all the work.</div><br/></div></div><div id="42499837" class="c"><input type="checkbox" id="c-42499837" checked=""/><div class="controls bullet"><span class="by">bubble12345</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42499799">parent</a><span>|</span><a href="#42500017">prev</a><span>|</span><a href="#42499728">next</a><span>|</span><label class="collapse" for="c-42499837">[-]</label><label class="expand" for="c-42499837">[4 more]</label></div><br/><div class="children"><div class="content">AI will not do math for us, but maybe eventually it will lead to another mainstream tool for mathematicians. Along with R, Matlab, Sage, GAP, Magma, ...<p>It would be interesting if in the future mathematicians are just as fluent in some (possibly AI-powered) proof verifying tool, as they are with LaTeX today.</div><br/><div id="42499954" class="c"><input type="checkbox" id="c-42499954" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42499837">parent</a><span>|</span><a href="#42499728">next</a><span>|</span><label class="collapse" for="c-42499954">[-]</label><label class="expand" for="c-42499954">[3 more]</label></div><br/><div class="children"><div class="content">AI can already do a bunch of math. So &quot;AI will not do math for us&quot; is just factually wrong.</div><br/><div id="42500028" class="c"><input type="checkbox" id="c-42500028" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42499954">parent</a><span>|</span><a href="#42500015">next</a><span>|</span><label class="collapse" for="c-42500028">[-]</label><label class="expand" for="c-42500028">[1 more]</label></div><br/><div class="children"><div class="content">Can AI solve “toy” math problems that computers have not been able to do? Yes. Can AI produce novel math research? No, it hasn’t yet. So “AI will not do math for us” is only factually wrong if you take the weaker definition of “doing math for us”. The stronger definition is not factually wrong yet.<p>More problematic with that statement is that a timeline isn’t specified. 1 year? Probably not. 10 years? Probably. 20 years? Very likely. 100 years? None of us here will be alive to be proven wrong but I’ll venture that that’s a certainty.</div><br/></div></div><div id="42500015" class="c"><input type="checkbox" id="c-42500015" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42499954">parent</a><span>|</span><a href="#42500028">prev</a><span>|</span><a href="#42499728">next</a><span>|</span><label class="collapse" for="c-42500015">[-]</label><label class="expand" for="c-42500015">[1 more]</label></div><br/><div class="children"><div class="content">Your idea of ‘do math’ is a bit different from this context.<p>Here it means do math research or better, find new math.</div><br/></div></div></div></div></div></div></div></div><div id="42499728" class="c"><input type="checkbox" id="c-42499728" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42499713">parent</a><span>|</span><a href="#42499799">prev</a><span>|</span><a href="#42493979">next</a><span>|</span><label class="collapse" for="c-42499728">[-]</label><label class="expand" for="c-42499728">[3 more]</label></div><br/><div class="children"><div class="content">Your optimism should be tempered with the downside of progress meaning that AI in the near future may not only inspire creativity in humans, but it can replace human creativity all together.<p>Why do I need to hire an artist for my movie&#x2F;video game&#x2F;advertisement when AI can replicate all the creativity I need.</div><br/><div id="42499756" class="c"><input type="checkbox" id="c-42499756" checked=""/><div class="controls bullet"><span class="by">wnc3141</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42499728">parent</a><span>|</span><a href="#42499944">next</a><span>|</span><label class="collapse" for="c-42499756">[-]</label><label class="expand" for="c-42499756">[1 more]</label></div><br/><div class="children"><div class="content">There is research on AI limiting creative output in completive arenas. Essentially it breaks expectancy therefore deteriorates iteration.<p><a href="https:&#x2F;&#x2F;direct.mit.edu&#x2F;rest&#x2F;article-abstract&#x2F;102&#x2F;3&#x2F;583&#x2F;96779&#x2F;Creativity-Under-Fire-The-Effects-of-Competition?redirectedFrom=fulltext" rel="nofollow">https:&#x2F;&#x2F;direct.mit.edu&#x2F;rest&#x2F;article-abstract&#x2F;102&#x2F;3&#x2F;583&#x2F;96779...</a></div><br/></div></div><div id="42499944" class="c"><input type="checkbox" id="c-42499944" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#42499713">root</a><span>|</span><a href="#42499728">parent</a><span>|</span><a href="#42499756">prev</a><span>|</span><a href="#42493979">next</a><span>|</span><label class="collapse" for="c-42499944">[-]</label><label class="expand" for="c-42499944">[1 more]</label></div><br/><div class="children"><div class="content">This was about mathematics.</div><br/></div></div></div></div></div></div><div id="42493979" class="c"><input type="checkbox" id="c-42493979" checked=""/><div class="controls bullet"><span class="by">nebulous1</span><span>|</span><a href="#42499713">prev</a><span>|</span><a href="#42493864">next</a><span>|</span><label class="collapse" for="c-42493979">[-]</label><label class="expand" for="c-42493979">[9 more]</label></div><br/><div class="children"><div class="content">There was a little more information in that reddit thread.  Of the three difficulty tiers, 25% are T1 (easiest) and 50% are T2.  Of the five public problems that the author looked at, two were T1 and two were T2.  Glazer on reddit described T1 as &quot;IMO&#x2F;undergraduate problems&quot;, but the article author says that they don&#x27;t consider them to be undergraduate problems.  So the LLM is <i>already</i> doing what the author says they would be surprised about.<p>Also Glazer seemed to regret calling T1 &quot;IMO&#x2F;undergraduate&quot;, and not only because of the disparity between IMO and typical undergraduate.  He said that &quot;We bump problems down a tier if we feel the difficulty comes too heavily from applying a major result, even in an advanced field, as a black box, since that makes a problem vulnerable to naive attacks from models&quot;<p>Also, all of the problems shows to Tao were T3</div><br/><div id="42496854" class="c"><input type="checkbox" id="c-42496854" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#42493979">parent</a><span>|</span><a href="#42496707">next</a><span>|</span><label class="collapse" for="c-42496854">[-]</label><label class="expand" for="c-42496854">[7 more]</label></div><br/><div class="children"><div class="content">The reddit thread is ... interesting (direct link[1]). It seems to be a debate among mathematicians some of whom do have access to the secret set. But they&#x27;re debating publicly and so naturally avoiding any concrete examples that would give the set away so wind-up with fuzzy-fiddly language for the qualities of the problem tiers.<p>The &quot;reality&quot; of keeping this stuff secret &#x27;cause someone would train on it is itself bizarre and certainly shouldn&#x27;t be above questioning.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;OpenAI&#x2F;comments&#x2F;1hiq4yv&#x2F;comment&#x2F;m30yfqp&#x2F;?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;OpenAI&#x2F;comments&#x2F;1hiq4yv&#x2F;comment&#x2F;m30...</a></div><br/><div id="42496903" class="c"><input type="checkbox" id="c-42496903" checked=""/><div class="controls bullet"><span class="by">obastani</span><span>|</span><a href="#42493979">root</a><span>|</span><a href="#42496854">parent</a><span>|</span><a href="#42497402">next</a><span>|</span><label class="collapse" for="c-42496903">[-]</label><label class="expand" for="c-42496903">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not about training directly on the test set, it&#x27;s about people discussing questions in the test set online (e.g., in forums), and then this data is swept up into the training set. That&#x27;s what makes test set contamination so difficult to avoid.</div><br/><div id="42498299" class="c"><input type="checkbox" id="c-42498299" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#42493979">root</a><span>|</span><a href="#42496903">parent</a><span>|</span><a href="#42497278">next</a><span>|</span><label class="collapse" for="c-42498299">[-]</label><label class="expand" for="c-42498299">[2 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; It&#x27;s not about training directly on the test set, it&#x27;s about people discussing questions in the test set online<p>Don&#x27;t kid yourself. There are 10&#x27;s of billions of dollars going into AI. Some of the humans involved would happily cheat on comparative tests to boost investment.</div><br/><div id="42499667" class="c"><input type="checkbox" id="c-42499667" checked=""/><div class="controls bullet"><span class="by">xmprt</span><span>|</span><a href="#42493979">root</a><span>|</span><a href="#42498299">parent</a><span>|</span><a href="#42497278">next</a><span>|</span><label class="collapse" for="c-42499667">[-]</label><label class="expand" for="c-42499667">[1 more]</label></div><br/><div class="children"><div class="content">The incentives are definitely there, but even CEOs and VCs know that if they cheat the tests just to get more investment, they&#x27;re only cheating themselves. No one is liquidating within the next 5 years so either they end up getting caught and lose everything or they spent all this energy trying to cheat while having a subpar model which results in them losing to competitors who actually invested in good technology.<p>Having a higher valuation could help with attracting better talent or more funding to invest in GPUs and actual model improvements but I don&#x27;t think that outweighs the risks unless you&#x27;re a tiny startup with nothing to show (but then you wouldn&#x27;t have the money to bribe anyone).</div><br/></div></div></div></div><div id="42497278" class="c"><input type="checkbox" id="c-42497278" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#42493979">root</a><span>|</span><a href="#42496903">parent</a><span>|</span><a href="#42498299">prev</a><span>|</span><a href="#42498420">next</a><span>|</span><label class="collapse" for="c-42497278">[-]</label><label class="expand" for="c-42497278">[1 more]</label></div><br/><div class="children"><div class="content">Yes,<p>That is the &quot;reality&quot; - that because companies can train their models on the whole Internet, companies will train their (base) models on the entire Internet.<p>And in this situation, &quot;having heard the problem&quot; actually serves as a barrier to understanding of these harder problems since any variation of known problem will receive a standard &quot;half-assed guestimate&quot;.<p>And these companies &quot;can&#x27;t not&quot; use these base models since they&#x27;re resigned to the &quot;bitter lesson&quot; (better the &quot;bitter lesson viewpoint&quot; imo) that they need large scale heuristics for the start of their process and only then can they start symbolic&#x2F;reasoning manipulations.<p>But hold-up! Why couldn&#x27;t an organization freeze their training set and their problems and release both to the public? That would give us an idea where the research stands. Ah, the answer comes out, &#x27;cause they don&#x27;t own the training set and the result they want to train is a commercial product that needs every drop of data to be the best. As Yan LeCun has said, <i>this isn&#x27;t research, this is product development</i>.</div><br/></div></div></div></div><div id="42497402" class="c"><input type="checkbox" id="c-42497402" checked=""/><div class="controls bullet"><span class="by">zifpanachr23</span><span>|</span><a href="#42493979">root</a><span>|</span><a href="#42496854">parent</a><span>|</span><a href="#42496903">prev</a><span>|</span><a href="#42496707">next</a><span>|</span><label class="collapse" for="c-42497402">[-]</label><label class="expand" for="c-42497402">[1 more]</label></div><br/><div class="children"><div class="content">Not having access to the dataset really makes the whole thing seem incredibly shady. Totally valid questions you are raising</div><br/></div></div></div></div><div id="42496707" class="c"><input type="checkbox" id="c-42496707" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42493979">parent</a><span>|</span><a href="#42496854">prev</a><span>|</span><a href="#42493864">next</a><span>|</span><label class="collapse" for="c-42496707">[-]</label><label class="expand" for="c-42496707">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So the LLM is already doing what the author says they would be surprised about.<p>that&#x27;s if you unconditionally believe in result without any proofreading, confirmation, reproducability and even barely any details (we are given only one slide).</div><br/></div></div></div></div><div id="42493864" class="c"><input type="checkbox" id="c-42493864" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#42493979">prev</a><span>|</span><a href="#42494417">next</a><span>|</span><label class="collapse" for="c-42493864">[-]</label><label class="expand" for="c-42493864">[42 more]</label></div><br/><div class="children"><div class="content">I just spent a few days trying to figure out some linear algebra with the help of ChatGPT. It&#x27;s very useful for finding conceptual information from literature (which for a not-professional-mathematician at least can be really hard to find and decipher). But in the actual math it constantly makes very silly errors. E.g. indexing a vector beyond its dimension, trying to do matrix decomposition for scalars and insisting on multiplying matrices with mismatching dimensions.<p>O1 is a lot better at spotting its errors than 4o but it too still makes a lot of really stupid mistakes. It seems to be quite far from producing results itself consistently without at least a somewhat clueful human doing hand-holding.</div><br/><div id="42498348" class="c"><input type="checkbox" id="c-42498348" checked=""/><div class="controls bullet"><span class="by">mseri</span><span>|</span><a href="#42493864">parent</a><span>|</span><a href="#42493935">next</a><span>|</span><label class="collapse" for="c-42498348">[-]</label><label class="expand" for="c-42498348">[1 more]</label></div><br/><div class="children"><div class="content">It reliably fails also basic real analysis proofs, but I think this is not too surprising since those require a mix of logic and computation that is likely hard to just infer from statistical likelihood of tokens</div><br/></div></div><div id="42493935" class="c"><input type="checkbox" id="c-42493935" checked=""/><div class="controls bullet"><span class="by">glimshe</span><span>|</span><a href="#42493864">parent</a><span>|</span><a href="#42498348">prev</a><span>|</span><a href="#42494272">next</a><span>|</span><label class="collapse" for="c-42493935">[-]</label><label class="expand" for="c-42493935">[23 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t Wolfram Alpha a better &quot;ChatGPT of Math&quot;?</div><br/><div id="42494019" class="c"><input type="checkbox" id="c-42494019" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42493935">parent</a><span>|</span><a href="#42495594">next</a><span>|</span><label class="collapse" for="c-42494019">[-]</label><label class="expand" for="c-42494019">[11 more]</label></div><br/><div class="children"><div class="content">Wolfram Alpha is better at actually doing math, but far worse at explaining  what it’s doing, and why.</div><br/><div id="42494170" class="c"><input type="checkbox" id="c-42494170" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494019">parent</a><span>|</span><a href="#42495398">next</a><span>|</span><label class="collapse" for="c-42494170">[-]</label><label class="expand" for="c-42494170">[7 more]</label></div><br/><div class="children"><div class="content">What’s worse about it?<p>It never tells you the wrong thing, at the very least.</div><br/><div id="42494237" class="c"><input type="checkbox" id="c-42494237" checked=""/><div class="controls bullet"><span class="by">jvanderbot</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494170">parent</a><span>|</span><a href="#42494229">next</a><span>|</span><label class="collapse" for="c-42494237">[-]</label><label class="expand" for="c-42494237">[4 more]</label></div><br/><div class="children"><div class="content">When you give it a large math problem and the answer is &quot;seven point one three five ... &quot;, and it shows a plot of the result v some randomly selected domain, well there could be more I&#x27;d like to know.<p>You can unlock a full derivation of the solution, for cases where you say &quot;Solve&quot; or &quot;Simplify&quot;, but what I (and I suspect GP) might want, is to know why a few of the key steps might work.<p>It&#x27;s a fantastic tool that helped get me through my (engineering) grad work, but ultimately the breakthrough inequalities that helped me write some of my best stuff were out of a book I bought in desperation that basically cataloged linear algebra known inequalities and simplifications.<p>When I try that kind of thing with the best LLM I can use (as of a few months ago, albeit), the results can get incorrect pretty quickly.</div><br/><div id="42500113" class="c"><input type="checkbox" id="c-42500113" checked=""/><div class="controls bullet"><span class="by">PeeMcGee</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494237">parent</a><span>|</span><a href="#42499427">next</a><span>|</span><label class="collapse" for="c-42500113">[-]</label><label class="expand" for="c-42500113">[1 more]</label></div><br/><div class="children"><div class="content">&gt; [...], but what I (and I suspect GP) might want, is to know why a few of the key steps might work.<p>It&#x27;s been some time since I&#x27;ve used the step-by-step explainer, and it was for calculus or intro physics problems at best, but IIRC the pro subscription will at least mention the method used to solve each step and link to reference materials (e.g., a clickable tag labeled &quot;integration by parts&quot;).  
Doesn&#x27;t exactly explain <i>why</i> but does provide useful keywords in a sequence that can be used to derive the why.</div><br/></div></div><div id="42499427" class="c"><input type="checkbox" id="c-42499427" checked=""/><div class="controls bullet"><span class="by">kens</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494237">parent</a><span>|</span><a href="#42500113">prev</a><span>|</span><a href="#42499955">next</a><span>|</span><label class="collapse" for="c-42499427">[-]</label><label class="expand" for="c-42499427">[1 more]</label></div><br/><div class="children"><div class="content">What book was it that you found helpful?</div><br/></div></div><div id="42499955" class="c"><input type="checkbox" id="c-42499955" checked=""/><div class="controls bullet"><span class="by">seattleeng</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494237">parent</a><span>|</span><a href="#42499427">prev</a><span>|</span><a href="#42494229">next</a><span>|</span><label class="collapse" for="c-42499955">[-]</label><label class="expand" for="c-42499955">[1 more]</label></div><br/><div class="children"><div class="content">Im reviewing linear algebra now and would also love to know that book!</div><br/></div></div></div></div><div id="42494229" class="c"><input type="checkbox" id="c-42494229" checked=""/><div class="controls bullet"><span class="by">fn-mote</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494170">parent</a><span>|</span><a href="#42494237">prev</a><span>|</span><a href="#42495398">next</a><span>|</span><label class="collapse" for="c-42494229">[-]</label><label class="expand" for="c-42494229">[2 more]</label></div><br/><div class="children"><div class="content">Its understanding of problems was very bad last time I used it. Meaning it was difficult to communicate what you wanted it to do. Usually I try to write in the Mathematica language, but even that is not foolproof.<p>Hopefully they have incorporated more modern LLM since then, but it hasn’t been that long.</div><br/><div id="42494384" class="c"><input type="checkbox" id="c-42494384" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494229">parent</a><span>|</span><a href="#42495398">next</a><span>|</span><label class="collapse" for="c-42494384">[-]</label><label class="expand" for="c-42494384">[1 more]</label></div><br/><div class="children"><div class="content">Wolfram Alpha&#x27;s &quot;smartness&quot; is often Clippy level enraging. E.g. it makes assumptions of symbols based on their names (e.g. a is assumed to be a constant, derivatives are taken w.r.t. x). Even with Mathematica syntax it tends to make such assumptions and refuses to lift them even when explicitly directed. Quite often one has to change the variable symbols used to try to make Alpha to do what&#x27;s meant.</div><br/></div></div></div></div></div></div><div id="42495398" class="c"><input type="checkbox" id="c-42495398" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494019">parent</a><span>|</span><a href="#42494170">prev</a><span>|</span><a href="#42496915">next</a><span>|</span><label class="collapse" for="c-42495398">[-]</label><label class="expand" for="c-42495398">[2 more]</label></div><br/><div class="children"><div class="content">I wish there was a way to tell Chatgpt where it has made a mistake, with a single mouse click.</div><br/><div id="42498368" class="c"><input type="checkbox" id="c-42498368" checked=""/><div class="controls bullet"><span class="by">akoboldfrying</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42495398">parent</a><span>|</span><a href="#42496915">next</a><span>|</span><label class="collapse" for="c-42498368">[-]</label><label class="expand" for="c-42498368">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s surprising to me is that this would surely be in OpenAI&#x27;s interests, too -- free RLHF!<p>Of course there would be the risk of adversaries giving bogus feedback, but my gut says it&#x27;s relatively straightforward to filter out most of this muck.</div><br/></div></div></div></div><div id="42496915" class="c"><input type="checkbox" id="c-42496915" checked=""/><div class="controls bullet"><span class="by">a3w</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494019">parent</a><span>|</span><a href="#42495398">prev</a><span>|</span><a href="#42495594">next</a><span>|</span><label class="collapse" for="c-42496915">[-]</label><label class="expand" for="c-42496915">[1 more]</label></div><br/><div class="children"><div class="content">Is the explanation a pro feature? At the very end it says &quot;step by step? Pay here&quot;</div><br/></div></div></div></div><div id="42495594" class="c"><input type="checkbox" id="c-42495594" checked=""/><div class="controls bullet"><span class="by">GuB-42</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42493935">parent</a><span>|</span><a href="#42494019">prev</a><span>|</span><a href="#42494031">next</a><span>|</span><label class="collapse" for="c-42495594">[-]</label><label class="expand" for="c-42495594">[9 more]</label></div><br/><div class="children"><div class="content">Wolfram Alpha can solve equations well, but it is terrible at understanding natural language.<p>For example I asked Wolfram Alpha &quot;How heavy a rocket has to be to launch 5 tons to LEO with a specific impulse of 400s&quot;, which is a straightforward application of the Tsiolkovsky rocket equation. Wolfram Alpha gave me some nonsense about particle physics (result: 95 MeV&#x2F;c^2), GPT-4o did it right (result: 53.45 tons).<p>Wolfram alpha knows about the Tsiolkovsky rocket equation, it knows about LEO (low earth orbit), but I found no way to get a delta-v out of it, again, more nonsense. It tells me about Delta airlines, mentions satellites that it knows are not in LEO. The &quot;natural language&quot; part is a joke. It is more like an advanced calculator, and for that, it is great.</div><br/><div id="42496187" class="c"><input type="checkbox" id="c-42496187" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42495594">parent</a><span>|</span><a href="#42494031">next</a><span>|</span><label class="collapse" for="c-42496187">[-]</label><label class="expand" for="c-42496187">[8 more]</label></div><br/><div class="children"><div class="content">You&#x27;re using it wrong, you can use natural language in your equation, but afaik it&#x27;s not supposed to be able to do what you&#x27;re asking of it.</div><br/><div id="42497065" class="c"><input type="checkbox" id="c-42497065" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42496187">parent</a><span>|</span><a href="#42494031">next</a><span>|</span><label class="collapse" for="c-42497065">[-]</label><label class="expand" for="c-42497065">[7 more]</label></div><br/><div class="children"><div class="content">You know, &quot;You&#x27;re using it wrong&quot; is usually meant to carry an ironic or sarcastic tone, right?<p>It dates back to Steve Jobs blaming an iPhone 4 user for &quot;holding it wrong&quot; rather than acknowledging a flawed antenna design that was causing dropped calls.  The closest Apple ever came to admitting that it was their problem was when they subsequently ran an employment ad to hire a new antenna engineering lead.  Maybe it&#x27;s time for Wolfram to hire a new language-model lead.</div><br/><div id="42498992" class="c"><input type="checkbox" id="c-42498992" checked=""/><div class="controls bullet"><span class="by">kortilla</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42497065">parent</a><span>|</span><a href="#42497855">next</a><span>|</span><label class="collapse" for="c-42498992">[-]</label><label class="expand" for="c-42498992">[2 more]</label></div><br/><div class="children"><div class="content">No, “holding it wrong” is the sarcastic version. “You’re using it wrong” is a super common way to tell people they are literally using something wrong.</div><br/><div id="42499745" class="c"><input type="checkbox" id="c-42499745" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42498992">parent</a><span>|</span><a href="#42497855">next</a><span>|</span><label class="collapse" for="c-42499745">[-]</label><label class="expand" for="c-42499745">[1 more]</label></div><br/><div class="children"><div class="content">But they&#x27;re not using it wrong.  They are using it as advertised by Wolfram themselves (read: himself).<p>The GP&#x27;s rocket equation question is <i>exactly</i> the sort of use case for which Alpha has been touted for years.</div><br/></div></div></div></div><div id="42497855" class="c"><input type="checkbox" id="c-42497855" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42497065">parent</a><span>|</span><a href="#42498992">prev</a><span>|</span><a href="#42494031">next</a><span>|</span><label class="collapse" for="c-42497855">[-]</label><label class="expand" for="c-42497855">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not an LLM. You&#x27;re simply asking too much of it. It doesn&#x27;t work the way you want it to, sorry.</div><br/><div id="42498473" class="c"><input type="checkbox" id="c-42498473" checked=""/><div class="controls bullet"><span class="by">edflsafoiewq</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42497855">parent</a><span>|</span><a href="#42498269">next</a><span>|</span><label class="collapse" for="c-42498473">[-]</label><label class="expand" for="c-42498473">[1 more]</label></div><br/><div class="children"><div class="content">Correct, so it isn&#x27;t a &quot;ChatGPT of Math&quot;, which was the point.</div><br/></div></div><div id="42498269" class="c"><input type="checkbox" id="c-42498269" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42497855">parent</a><span>|</span><a href="#42498473">prev</a><span>|</span><a href="#42494031">next</a><span>|</span><label class="collapse" for="c-42498269">[-]</label><label class="expand" for="c-42498269">[2 more]</label></div><br/><div class="children"><div class="content">Tell Wolfram.  They&#x27;re the ones who&#x27;ve been advertising it for years, well before LLMs were a thing, using English-language prompts like these examples: <a href="https:&#x2F;&#x2F;www.pcmag.com&#x2F;news&#x2F;23-cool-non-math-things-you-can-do-with-wolfram-alpha" rel="nofollow">https:&#x2F;&#x2F;www.pcmag.com&#x2F;news&#x2F;23-cool-non-math-things-you-can-d...</a><p>The problem has always been that you only get good answers if you happen to stumble on a specific question that it can handle.  Combining Alpha with an LLM could actually be pretty awesome, but I&#x27;m sure it&#x27;s easier said than done.</div><br/><div id="42498354" class="c"><input type="checkbox" id="c-42498354" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42498269">parent</a><span>|</span><a href="#42494031">next</a><span>|</span><label class="collapse" for="c-42498354">[-]</label><label class="expand" for="c-42498354">[1 more]</label></div><br/><div class="children"><div class="content">Before LLMs exploded nobody really <i>expected</i> WA to perform well at natural language comprehension. The expectations were at the level of &quot;an ELIZA that knows math&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42494031" class="c"><input type="checkbox" id="c-42494031" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42493935">parent</a><span>|</span><a href="#42495594">prev</a><span>|</span><a href="#42494799">next</a><span>|</span><label class="collapse" for="c-42494031">[-]</label><label class="expand" for="c-42494031">[1 more]</label></div><br/><div class="children"><div class="content">Wolfram Alpha is mostly for &quot;trivia&quot; type problems. Or giving solutions to equations.<p>I was figuring out some mode decomposition methods such as ESPRIT and Prony and how to potentially extend&#x2F;customize them. Wolfram Alpha doesn&#x27;t seem to have a clue about such.</div><br/></div></div><div id="42494799" class="c"><input type="checkbox" id="c-42494799" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42493935">parent</a><span>|</span><a href="#42494031">prev</a><span>|</span><a href="#42494272">next</a><span>|</span><label class="collapse" for="c-42494799">[-]</label><label class="expand" for="c-42494799">[1 more]</label></div><br/><div class="children"><div class="content">No. Wolfram Alpha can&#x27;t solve anything that isn&#x27;t a function evaluation or equation. And it can&#x27;t do modular arithmetic to save its unlife.<p>WolframOne&#x2F;Mathematica is better, but that requires the user (or ChatGPT!)to write complicated code, not natural language queries.</div><br/></div></div></div></div><div id="42494272" class="c"><input type="checkbox" id="c-42494272" checked=""/><div class="controls bullet"><span class="by">spacemanspiff01</span><span>|</span><a href="#42493864">parent</a><span>|</span><a href="#42493935">prev</a><span>|</span><a href="#42494474">next</a><span>|</span><label class="collapse" for="c-42494272">[-]</label><label class="expand" for="c-42494272">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if these are tokenization issues? I really am curious about metas byte tokenization scheme...</div><br/><div id="42494357" class="c"><input type="checkbox" id="c-42494357" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494272">parent</a><span>|</span><a href="#42494474">next</a><span>|</span><label class="collapse" for="c-42494357">[-]</label><label class="expand" for="c-42494357">[1 more]</label></div><br/><div class="children"><div class="content">Probably mostly not. The errors tend to be logical&#x2F;conceptual. E.g. mixing up scalars and matrices is unlikely to be from tokenization. Especially if using spaces between the variables and operators, as AFAIK GPTs don&#x27;t form tokens over spaces (although tokens may start or end with them).</div><br/></div></div></div></div><div id="42494474" class="c"><input type="checkbox" id="c-42494474" checked=""/><div class="controls bullet"><span class="by">lordnacho</span><span>|</span><a href="#42493864">parent</a><span>|</span><a href="#42494272">prev</a><span>|</span><a href="#42494524">next</a><span>|</span><label class="collapse" for="c-42494474">[-]</label><label class="expand" for="c-42494474">[9 more]</label></div><br/><div class="children"><div class="content">The only thing I&#x27;ve consistently had issues with while using AI is graphs. If I ask it to put some simple function, it produces a really weird image that has nothing to do with the graph I want. It will be a weird swirl of lines and words, and it never corrects itself no matter what I say to it.<p>Has anyone had any luck with this? It seems like the only thing that it just can&#x27;t do.</div><br/><div id="42494548" class="c"><input type="checkbox" id="c-42494548" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494474">parent</a><span>|</span><a href="#42494691">next</a><span>|</span><label class="collapse" for="c-42494548">[-]</label><label class="expand" for="c-42494548">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re doing it wrong. It can&#x27;t produce proper graphs with it&#x27;s diffusion style image generation.<p>Ask it to produce graphs with python and matplotlib. That will work.</div><br/><div id="42498231" class="c"><input type="checkbox" id="c-42498231" checked=""/><div class="controls bullet"><span class="by">lanstin</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494548">parent</a><span>|</span><a href="#42494691">next</a><span>|</span><label class="collapse" for="c-42498231">[-]</label><label class="expand" for="c-42498231">[1 more]</label></div><br/><div class="children"><div class="content">And works very well - it made me a nice general &quot;draw successively accurate Fourier series approximations given this lambda for coefficients and this lambda for the constant term&quot;. PNG output, no real programming errors (I wouldn&#x27;t remember if it had some stupid error, I&#x27;m a python programmer). Even TikZ in LaTeX isn&#x27;t hopeless (although I did ending up reading the tikz manual)</div><br/></div></div></div></div><div id="42494691" class="c"><input type="checkbox" id="c-42494691" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494474">parent</a><span>|</span><a href="#42494548">prev</a><span>|</span><a href="#42496621">next</a><span>|</span><label class="collapse" for="c-42494691">[-]</label><label class="expand" for="c-42494691">[5 more]</label></div><br/><div class="children"><div class="content">Ask it to plot the graph with python plotting utilities. Not using its image generator. I think you need a ChatGPT subscription though for it to be able to run python code.</div><br/><div id="42494778" class="c"><input type="checkbox" id="c-42494778" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494691">parent</a><span>|</span><a href="#42496049">next</a><span>|</span><label class="collapse" for="c-42494778">[-]</label><label class="expand" for="c-42494778">[2 more]</label></div><br/><div class="children"><div class="content">You seem to get 2(?) free Python program runs per week(?) as part of the 01 preview.<p>When you visit chatgpt on the free account it automatically gives you the best model and then disables it after some amount of work and says to come back later or upgrade.</div><br/><div id="42495381" class="c"><input type="checkbox" id="c-42495381" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494778">parent</a><span>|</span><a href="#42496049">next</a><span>|</span><label class="collapse" for="c-42495381">[-]</label><label class="expand" for="c-42495381">[1 more]</label></div><br/><div class="children"><div class="content">Just install Python locally, and copy paste the code.</div><br/></div></div></div></div><div id="42496049" class="c"><input type="checkbox" id="c-42496049" checked=""/><div class="controls bullet"><span class="by">xienze</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494691">parent</a><span>|</span><a href="#42494778">prev</a><span>|</span><a href="#42496621">next</a><span>|</span><label class="collapse" for="c-42496049">[-]</label><label class="expand" for="c-42496049">[2 more]</label></div><br/><div class="children"><div class="content">Shouldn’t ChatGPT be smart enough to know to do this automatically, based on context?</div><br/><div id="42497056" class="c"><input type="checkbox" id="c-42497056" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42496049">parent</a><span>|</span><a href="#42496621">next</a><span>|</span><label class="collapse" for="c-42497056">[-]</label><label class="expand" for="c-42497056">[1 more]</label></div><br/><div class="children"><div class="content">It was, for a while.  I think this is an area where there may have been some regression.  It can still write code to solve problems that are a poor fit for the language model, but you may need to ask it to do that explicitly.</div><br/></div></div></div></div></div></div><div id="42496621" class="c"><input type="checkbox" id="c-42496621" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42494474">parent</a><span>|</span><a href="#42494691">prev</a><span>|</span><a href="#42494524">next</a><span>|</span><label class="collapse" for="c-42496621">[-]</label><label class="expand" for="c-42496621">[1 more]</label></div><br/><div class="children"><div class="content">The agentic reasoning models should be able to fix this if they have the ability to run code instead of giving each task to itself. &quot;I need to make a graph&quot; &quot;LLMs have difficulty graphing novel functions&quot; &quot;Call python instead&quot; is a line of reasoning I would expect after seeing what O1 has come up with on other problems.<p>Giving AI the ability to execute code is the safety peoples nightmare though, wonder if we&#x27;ll hear anything from them as this is surely coming</div><br/></div></div></div></div><div id="42495371" class="c"><input type="checkbox" id="c-42495371" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42493864">parent</a><span>|</span><a href="#42494524">prev</a><span>|</span><a href="#42494417">next</a><span>|</span><label class="collapse" for="c-42495371">[-]</label><label class="expand" for="c-42495371">[5 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t most mathematical papers contain at least one such error?</div><br/><div id="42495592" class="c"><input type="checkbox" id="c-42495592" checked=""/><div class="controls bullet"><span class="by">aiono</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42495371">parent</a><span>|</span><a href="#42497041">next</a><span>|</span><label class="collapse" for="c-42495592">[-]</label><label class="expand" for="c-42495592">[3 more]</label></div><br/><div class="children"><div class="content">Where is this data from?</div><br/><div id="42496895" class="c"><input type="checkbox" id="c-42496895" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42495592">parent</a><span>|</span><a href="#42497041">next</a><span>|</span><label class="collapse" for="c-42496895">[-]</label><label class="expand" for="c-42496895">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a question, and to be fair to AI it should actually refer to papers before review.</div><br/><div id="42498429" class="c"><input type="checkbox" id="c-42498429" checked=""/><div class="controls bullet"><span class="by">monktastic1</span><span>|</span><a href="#42493864">root</a><span>|</span><a href="#42496895">parent</a><span>|</span><a href="#42497041">next</a><span>|</span><label class="collapse" for="c-42498429">[-]</label><label class="expand" for="c-42498429">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s a question, but you haven&#x27;t answered what you read that makes you suspect so.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42494417" class="c"><input type="checkbox" id="c-42494417" checked=""/><div class="controls bullet"><span class="by">ivan_ah</span><span>|</span><a href="#42493864">prev</a><span>|</span><a href="#42495668">next</a><span>|</span><label class="collapse" for="c-42494417">[-]</label><label class="expand" for="c-42494417">[2 more]</label></div><br/><div class="children"><div class="content">Yesterday, I saw a thought provoking talk about the future of of &quot;math jobs&quot; assuming automated theory proving becomes more prevalent in the future.<p>[ (Re)imagining mathematics in a world of reasoning machines by Akshay Venkatesh]<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vYCT7cw0ycw" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vYCT7cw0ycw</a>  [54min]<p>Abstract: In the coming decades, developments in automated reasoning will likely transform the way that research mathematics is conceptualized and carried out. I will discuss some ways we might think about this. The talk will not be about current or potential abilities of computers to do mathematics—rather I will look at topics such as the history of automation and mathematics, and related philosophical questions.<p>See discussion at <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42465907">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42465907</a></div><br/><div id="42500547" class="c"><input type="checkbox" id="c-42500547" checked=""/><div class="controls bullet"><span class="by">qnleigh</span><span>|</span><a href="#42494417">parent</a><span>|</span><a href="#42495668">next</a><span>|</span><label class="collapse" for="c-42500547">[-]</label><label class="expand" for="c-42500547">[1 more]</label></div><br/><div class="children"><div class="content">That was wonderful, thank you for linking it. For the benefit of anyone who doesn&#x27;t have time to watch the whole thing, here are a few really nice quotes that convey some main points.<p>&quot;We might put the axioms into a reasoning apparatus like the logical machinery of Stanley Jevons, and see all geometry come out of it. That process of reasoning are replaced by symbols and formulas... may seem artificial and puerile; and it is needless to point out how disastrous it would be in teaching and how hurtful to the mental development; how deadening it would be for investigators, whose originality it would nip in the bud. But as used by Professor Hilbert, it explains and justifies itself if one remembers the end pursued.&quot; Poincare on the value of reasoning machines, but the analogy to mathematics once we have theorem-proving AI is clear (that the tools and the lie direct outputs are not the ends. Human understanding is).<p>&quot;Even if such a machine produced largely incomprehensible proofs, I would imagine that we would place much less value on proofs as a goal of math. I don&#x27;t think humans will stop doing mathematics... I&#x27;m not saying there will be jobs for them, but I don&#x27;t think we&#x27;ll stop doing math.&quot;<p>&quot;Mathematics is the study of reproducible mental objects.&quot; This definition is human (&quot;mental&quot;) and social (it implies reproducing among individuals). &quot;Maybe in this world, mathematics would involve a broader range of inquiry... We need to renegotiate the basic goals and values of the discipline.&quot; And he gives some examples of deep questions we may tackle beyond just proving theorems.</div><br/></div></div></div></div><div id="42495668" class="c"><input type="checkbox" id="c-42495668" checked=""/><div class="controls bullet"><span class="by">busyant</span><span>|</span><a href="#42494417">prev</a><span>|</span><a href="#42494198">next</a><span>|</span><label class="collapse" for="c-42495668">[-]</label><label class="expand" for="c-42495668">[37 more]</label></div><br/><div class="children"><div class="content">As someone who has a 18 yo son who wants to study math, this has me (and him) 
 ... worried ... about becoming obsolete?<p>But I&#x27;m wondering what other people think of this analogy.<p>I used to be a bench scientist (molecular genetics).<p>There were world class researchers who were more creative than I was. I even had a Nobel Laureate once tell me that my research was simply &quot;dotting &#x27;i&#x27;s and crossing &#x27;t&#x27;s&quot;.<p>Nevertheless, I still moved the field forward in my own small ways. I still did respectable work.<p>So, will these LLMs make us <i>completely</i> obsolete? Or will there still be room for those of us who can dot the &quot;i&quot;?--if only for the fact that LLMs don&#x27;t have infinite time&#x2F;resources to solve &quot;everything.&quot;<p>I don&#x27;t know. Maybe I&#x27;m whistling past the graveyard.</div><br/><div id="42495887" class="c"><input type="checkbox" id="c-42495887" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42497776">next</a><span>|</span><label class="collapse" for="c-42495887">[-]</label><label class="expand" for="c-42495887">[12 more]</label></div><br/><div class="children"><div class="content">I was just thinking about this. I already posted a comment here, but I will say that as a mathematician (PhD in number theory), that for me, AI signficantly takes away the beauty of doing mathematics within a realm in which AI is used.<p>The best part of math (again, just for me) is that it was a journey that was done by hand with only the human intellect that computers didn&#x27;t understand. The beauty of the subject was precisely that it was a journey of human intellect.<p>As I said elsewhere, my friends used to ask me why something was true and it was fun to explain it to them, or ask them and have them explain it to me. Now most will just use some AI.<p>Soulless, in my opinion. Pure mathematics should be about the art of the thing, not producing results on an assembly line like it will be with AI. Of course, the best mathematicians are going into this because it helps their current careers, not because it helps the future of the subject. Math done with AI will be a lot like Olympic running done with performance-enhancing drugs.<p>Yes, we will get a few more results, faster. But the results will be entirely boring.</div><br/><div id="42497714" class="c"><input type="checkbox" id="c-42497714" checked=""/><div class="controls bullet"><span class="by">hn3er1q</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42495887">parent</a><span>|</span><a href="#42499373">next</a><span>|</span><label class="collapse" for="c-42497714">[-]</label><label class="expand" for="c-42497714">[4 more]</label></div><br/><div class="children"><div class="content">There are many similarities in your comment to how grandmasters discuss engines. I have a hunch the arc of AI in math will be very similar to the arc of engines in chess.<p><a href="https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;defeated-chess-champ-garry-kasparov-made-peace-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;defeated-chess-champ-garry-kaspa...</a></div><br/><div id="42497764" class="c"><input type="checkbox" id="c-42497764" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497714">parent</a><span>|</span><a href="#42499373">next</a><span>|</span><label class="collapse" for="c-42497764">[-]</label><label class="expand" for="c-42497764">[3 more]</label></div><br/><div class="children"><div class="content">I agree with that, in the sense that math will become more about who can use AI the fastest to generate the most theories, which sort of side-steps the whole point of math.</div><br/><div id="42497811" class="c"><input type="checkbox" id="c-42497811" checked=""/><div class="controls bullet"><span class="by">hn3er1q</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497764">parent</a><span>|</span><a href="#42499373">next</a><span>|</span><label class="collapse" for="c-42497811">[-]</label><label class="expand" for="c-42497811">[2 more]</label></div><br/><div class="children"><div class="content">As a chess aficionado and a former tournament player, who didn’t get very far, I can see pros &amp; cons.  They helped me train and get significantly better than I would’ve gotten without them.  On the other hand, so did the competition. :) The average level of the game is so much higher than when I was a kid (30+ years ago) and new ways of playing that were unthinkable before are possible now.  On the other hand cheating (online anyway) is rampant and all the memorization required to begin to be competitive can be daunting, and that sucks.</div><br/><div id="42498063" class="c"><input type="checkbox" id="c-42498063" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497811">parent</a><span>|</span><a href="#42499373">next</a><span>|</span><label class="collapse" for="c-42498063">[-]</label><label class="expand" for="c-42498063">[1 more]</label></div><br/><div class="children"><div class="content">Hey I play chess too. Not a very good player though. But to be honest, I enjoy playing with people who are not serious because I do think an overabundance of knowledge makes the game too mechanical. Just my personal experience, but I think the risk of cheaters who use programs and the overmechanization of chess is not worth becoming a better player. (And in fact, I think MOST people can gain satisfaction by improving just by studying books and playing. But I do think that  a few who don&#x27;t have access to opponents benefit from a chess-playing computer).</div><br/></div></div></div></div></div></div></div></div><div id="42499373" class="c"><input type="checkbox" id="c-42499373" checked=""/><div class="controls bullet"><span class="by">_jayhack_</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42495887">parent</a><span>|</span><a href="#42497714">prev</a><span>|</span><a href="#42499640">next</a><span>|</span><label class="collapse" for="c-42499373">[-]</label><label class="expand" for="c-42499373">[1 more]</label></div><br/><div class="children"><div class="content">If you think the purpose of pure math is to provide employment and entertainment to mathematicians, this is a dark day.<p>If you believe the purpose of pure math is to shed light on patterns in nature, pave the way for the sciences, etc., this is fantastic news.</div><br/></div></div><div id="42499640" class="c"><input type="checkbox" id="c-42499640" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42495887">parent</a><span>|</span><a href="#42499373">prev</a><span>|</span><a href="#42499853">next</a><span>|</span><label class="collapse" for="c-42499640">[-]</label><label class="expand" for="c-42499640">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Now most will just use some AI.<p>Do people with PhD in math really ask AI to explain math concepts to them?</div><br/></div></div><div id="42499853" class="c"><input type="checkbox" id="c-42499853" checked=""/><div class="controls bullet"><span class="by">agentultra</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42495887">parent</a><span>|</span><a href="#42499640">prev</a><span>|</span><a href="#42496046">next</a><span>|</span><label class="collapse" for="c-42499853">[-]</label><label class="expand" for="c-42499853">[1 more]</label></div><br/><div class="children"><div class="content">I think it will become apparent how bad they are at it. They’re algorithms and not sentient beings. They do not think of themselves, their place in the world, and do not fathom the contents of the minds of others. They do no care what others think of them.<p>Whatever they write only happens to contain some truth by virtue of the model and the training data. An algorithm doesn’t know what truth is or why we value it. It’s a bullshitter of the highest calibre.<p>Then comes the question: will they write proofs that we will consider beautiful and elegant, that we will remember and pass down?<p>Or will they generate what they’ve been asked to and nothing less? That would be utterly boring to read.</div><br/></div></div><div id="42496046" class="c"><input type="checkbox" id="c-42496046" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42495887">parent</a><span>|</span><a href="#42499853">prev</a><span>|</span><a href="#42497776">next</a><span>|</span><label class="collapse" for="c-42496046">[-]</label><label class="expand" for="c-42496046">[4 more]</label></div><br/><div class="children"><div class="content">Presumably people who get into math going forward will feel differently.<p>For myself, chasing lemmas was always boring — and there’s little interest in doing the busywork of fleshing out a theory. For me, LLMs are a great way to do the fun parts (conceptual architecture) without the boring parts.<p>And I expect we’ll such much the same change as with physics: computers increase the complexity of the objects we study, which tend to be rather simple when done by hand — eg, people don’t investigate patterns in the diagrams of group(oids) because drawing million element diagrams isn’t tractable by hand. And you only notice the patterns in them when you see examples of the diagrams at scale.</div><br/><div id="42497756" class="c"><input type="checkbox" id="c-42497756" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496046">parent</a><span>|</span><a href="#42496208">next</a><span>|</span><label class="collapse" for="c-42497756">[-]</label><label class="expand" for="c-42497756">[2 more]</label></div><br/><div class="children"><div class="content">Just a counterpoint, but I wonder how much you&#x27;ll really understand if you can&#x27;t even prove the whole thing yourself. Personally, I learn by proving but I guess everyone is different.</div><br/><div id="42498994" class="c"><input type="checkbox" id="c-42498994" checked=""/><div class="controls bullet"><span class="by">daxfohl</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497756">parent</a><span>|</span><a href="#42496208">next</a><span>|</span><label class="collapse" for="c-42498994">[-]</label><label class="expand" for="c-42498994">[1 more]</label></div><br/><div class="children"><div class="content">My hunch is it won&#x27;t be much different, even when we can simply ask a machine that doesn&#x27;t have a cached proof, &quot;prove riemann hypothesis&quot; and it thinks for ten seconds and spits out a fully correct proof.<p>As Erdos(I think?) said, great math is not about the answers, it&#x27;s about the questions. Or maybe it was someone else, and maybe &quot;great mathematicians&quot; rather than &quot;great math&quot;. But, gist is the same.<p>&quot;What happens when you invent a thing that makes a function continuous (aka limit point)&quot;? &quot;What happens when you split the area under a curve into infinitesimal pieces and sum them up&quot;? &quot;What happens when you take the middle third out of an interval recursively&quot;? &quot;Can we define a set of axioms that underlie all mathematics&quot;? &quot;Is the graph of how many repetitions it takes for a complex number to diverge interesting&quot;? I have a hard time imagining computers would ever have a strong enough understanding of the human experience with mathematics to even begin pondering such questions unprompted, let alone answer them and grok the implications.<p>Ultimately the truths of mathematics, the answers, soon to be proved primarily by computers, already exist. Proving a truth does not create the truth; the truth exists independent of whether it has been proved or not. So fundamentally math is closer to archeology than it may appear. As such, AI is just a tool to help us dig with greater efficiency. But it should not be considered or feared as a replacement for mathematicians. AI can never take away the enlightenment of discovering something new, even if it does all the hard work itself.</div><br/></div></div></div></div><div id="42496208" class="c"><input type="checkbox" id="c-42496208" checked=""/><div class="controls bullet"><span class="by">ndriscoll</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496046">parent</a><span>|</span><a href="#42497756">prev</a><span>|</span><a href="#42497776">next</a><span>|</span><label class="collapse" for="c-42496208">[-]</label><label class="expand" for="c-42496208">[1 more]</label></div><br/><div class="children"><div class="content">Even current people will feel differently. I don&#x27;t bemoan the fact that Lean&#x2F;Mathlib has `simp` and `linarith` to automate trivial computations. A &quot;copilot for Lean&quot; that can turn &quot;by induction, X&quot; or &quot;evidently Y&quot; into a formal proof sounds great.<p>The the trick is teaching the thing how high powered of theorems to use or how to factor out details or not depending on the user&#x27;s level of understanding. We&#x27;ll have to find a pedagogical balance (e.g. you don&#x27;t give `linarith` to someone practicing basic proofs), but I&#x27;m sure it will be a great tool to aid human understanding.<p>A tool to help translate natural language to formal propositions&#x2F;types also sounds great, and could help more people to use more formal methods, which could make for more robust software.</div><br/></div></div></div></div></div></div><div id="42497776" class="c"><input type="checkbox" id="c-42497776" checked=""/><div class="controls bullet"><span class="by">peterbonney</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42495887">prev</a><span>|</span><a href="#42500506">next</a><span>|</span><label class="collapse" for="c-42497776">[-]</label><label class="expand" for="c-42497776">[3 more]</label></div><br/><div class="children"><div class="content">If you looked at how the average accountant spent their time before the arrival of the digital spreadsheet, you might have predicted that automated calculation would make the profession obsolete. But it didn&#x27;t.<p>This time could be different, of course. But I&#x27;ll need a lot more evidence before I start telling people to base their major life decisions on projected technological change.<p>That&#x27;s before we even consider that only a very slim minority of the people who study math (or physics or statistics or biology or literature or...) go on to work in the field of math (or physics or statistics or biology or literature or...). AI could completely take over math research and still have next to impact on the value of the skills one acquires from studying math.<p>Or if you want to be more fatalistic about it: if AI is going to put everyone out of work then it doesn&#x27;t really matter what you do now to prepare for it. Might as well follow your interests in the meantime.</div><br/><div id="42498009" class="c"><input type="checkbox" id="c-42498009" checked=""/><div class="controls bullet"><span class="by">blagie</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497776">parent</a><span>|</span><a href="#42500506">next</a><span>|</span><label class="collapse" for="c-42498009">[-]</label><label class="expand" for="c-42498009">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s important to base life decisions on very real technological change. We don&#x27;t know what the change will be, but it&#x27;s coming. At the very least, that suggests more diverse skills.<p>We&#x27;re all usually (but not always) better off, with more productivity, eventually, but in the meantime, jobs do disappear. Robotics did not fully displace machinists and factory workers, but single-skilled people in Detroit did not do well. The loom, the steam engine... all of them displaced often highly-trained often low-skilled artisans.</div><br/><div id="42498204" class="c"><input type="checkbox" id="c-42498204" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42498009">parent</a><span>|</span><a href="#42500506">next</a><span>|</span><label class="collapse" for="c-42498204">[-]</label><label class="expand" for="c-42498204">[1 more]</label></div><br/><div class="children"><div class="content">If AI reaches this level socioeconomic impact is going to be so immense, that choosing what subject you study will have no impact on your outcome - no matter what it is - so it&#x27;s a pointless consideration.</div><br/></div></div></div></div></div></div><div id="42500506" class="c"><input type="checkbox" id="c-42500506" checked=""/><div class="controls bullet"><span class="by">ykonstant</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42497776">prev</a><span>|</span><a href="#42496094">next</a><span>|</span><label class="collapse" for="c-42500506">[-]</label><label class="expand" for="c-42500506">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I even had a Nobel Laureate once tell me that my research was simply &quot;dotting &#x27;i&#x27;s and crossing &#x27;t&#x27;s&quot;.<p>(｡•́︿•̀｡)</div><br/></div></div><div id="42496094" class="c"><input type="checkbox" id="c-42496094" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42500506">prev</a><span>|</span><a href="#42498974">next</a><span>|</span><label class="collapse" for="c-42496094">[-]</label><label class="expand" for="c-42496094">[3 more]</label></div><br/><div class="children"><div class="content">What LLMs can do is limited, they are superior to wet-wear in some tasks like finding and matching patterns in higher dimensional space, they are still fundamentally limited to a tiny class of problems outside of that pattern finding and matching.<p>LLMs will be tools for some math needs and even if we ever get quantum computers will be limited in what they can do.<p>LLMs, without pattern matching, can only do up to about integer division, and while they can calculate parity, they can&#x27;t use it in their calculations.<p>There are several groups sitting on what are known limitations of LLMs, waiting to take advantage of those who don&#x27;t understand the fundamental limitations, simplicity bias etc...<p>The hype will meet reality soon and we will figure out where they work and where they are problematic over the next few years.<p>But even the most celebrated achievements like proof finding with Lean, heavily depends on smart people producing hints that machines can use.<p>Basically lots of the fundamental hints of the limits of computation still hold.<p>Model logic may be an accessable way to approach the limits of statistical inference if you want to know one path yourself.<p>A lot of what is in this article relates to some the known fundamental limitations.<p>Remember that for all the amazing progress, one of the core founders of the perceptron, Pitts  drank him self to death in the 50s because it was shown that they were insufficient to accurately model biological neurons.<p>Optimism is high, but reality will hit soon.<p>So think of it as new tools that will be available to your child, not a replacement.</div><br/><div id="42496225" class="c"><input type="checkbox" id="c-42496225" checked=""/><div class="controls bullet"><span class="by">ComplexSystems</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496094">parent</a><span>|</span><a href="#42498974">next</a><span>|</span><label class="collapse" for="c-42496225">[-]</label><label class="expand" for="c-42496225">[2 more]</label></div><br/><div class="children"><div class="content">&quot;LLMs, without pattern matching, can only do up to about integer division, and while they can calculate parity, they can&#x27;t use it in their calculations.&quot; - what do you mean by this? Counting the number of 1&#x27;s in a bitstring and determining if it&#x27;s even or odd?</div><br/><div id="42496880" class="c"><input type="checkbox" id="c-42496880" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496225">parent</a><span>|</span><a href="#42498974">next</a><span>|</span><label class="collapse" for="c-42496880">[-]</label><label class="expand" for="c-42496880">[1 more]</label></div><br/><div class="children"><div class="content">Yes, in this case PARITY is determining if the number of 1s in a binary input is odd or even<p>It is an effect of the complex to unpack descriptive complexity class DLOGTIME-uniform TC0, which has AND, OR and MAJORITY gates.<p><a href="http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.13629" rel="nofollow">http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.13629</a><p>The point being that the ability to use parity gates is different than being able to calculate it, which is where the union of the typically ram machine DLOGTIME with the circuit complexity of uniform TC0 comes into play.<p>PARITY, MAJ, AND, and OR are all symmetric, and are in TCO, but PARITY is not in DLOGTIME-uniform TC0, which is first-order logic with Majority quantifiers.<p>Another path, if you think about symantic properties and Rice&#x27;s theorem, this may make sense especially as PAC learning even depth 2 nets is equivalent to the approximate SVP.<p>PAC-learning even depth-2 threshold circuits is NP-hard.<p><a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;~klivans&#x2F;crypto-hs.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;~klivans&#x2F;crypto-hs.pdf</a><p>For me thinking about how ZFC was structured so we can keep the niceties of the law of the excluded middle, and how statistics pretty much depends on it for the central limit and law of large numbers, IID etc...<p>But that path runs the risk of reliving the Brouwer–Hilbert controversy.</div><br/></div></div></div></div></div></div><div id="42498974" class="c"><input type="checkbox" id="c-42498974" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42496094">prev</a><span>|</span><a href="#42495879">next</a><span>|</span><label class="collapse" for="c-42498974">[-]</label><label class="expand" for="c-42498974">[1 more]</label></div><br/><div class="children"><div class="content">I doubt it.<p>Most likely AI will be good at some things and not others, and mathematicians will just move to whatever AI isn&#x27;t good at.<p>Alternatively, if AI is able to do all math at a level above PhDs, then its going to be a brave new world and basically the singularity. Everything will change so much that speculating about it will probably be useless.</div><br/></div></div><div id="42495879" class="c"><input type="checkbox" id="c-42495879" checked=""/><div class="controls bullet"><span class="by">pfisherman</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42498974">prev</a><span>|</span><a href="#42497136">next</a><span>|</span><label class="collapse" for="c-42495879">[-]</label><label class="expand" for="c-42495879">[2 more]</label></div><br/><div class="children"><div class="content">I used to do bench top work too; and was blessed with “the golden hands” in that I could  almost always get protocols working.  To me this always felt more like intuition than deductive reasoning. And it made me a terrible TA.  My advice to students in lab was always something along the lines of “just mess around with it, and see how it works.”  Not very helpful for the stressed and struggling student -_-<p>Digression aside, my point is that I don’t think we know exactly what makes or defines “the golden hands”. And if that is the case, can we optimize for it?<p>Another point is that scalable fine tuning only works for verifiable stuff.  Think a priori knowledge. To me that seems to be at the opposite end of the spectrum from “mess with it and see what happens”.</div><br/><div id="42496924" class="c"><input type="checkbox" id="c-42496924" checked=""/><div class="controls bullet"><span class="by">busyant</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42495879">parent</a><span>|</span><a href="#42497136">next</a><span>|</span><label class="collapse" for="c-42496924">[-]</label><label class="expand" for="c-42496924">[1 more]</label></div><br/><div class="children"><div class="content">&gt; blessed with “the golden hands” in that I could almost always get protocols working.<p>Very funny. My friends and I never used the phrase &quot;golden hands&quot; but we used to say something similar: &quot;so-and-so has &#x27;great hands&#x27;&quot;.<p>But it meant the same thing.<p>I, myself, did not have great hands, but my comment was more about the intellectual process of conducting research.<p>I guess my point was that:<p>* I&#x27;ve already dealt with more talented researchers, but I still contributed meaningfully.<p>* Hopefully, the &quot;AI&quot; will simply add another layer of talent, but the rest of us lesser mortals will still be able to contribute.<p>But I don&#x27;t know if I&#x27;m correct.</div><br/></div></div></div></div><div id="42497136" class="c"><input type="checkbox" id="c-42497136" checked=""/><div class="controls bullet"><span class="by">hyhconito</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42495879">prev</a><span>|</span><a href="#42496971">next</a><span>|</span><label class="collapse" for="c-42497136">[-]</label><label class="expand" for="c-42497136">[4 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s put it this way, from another mathematician, and I&#x27;m sure I&#x27;ll probably be shot for this one.<p>Every LLM release moves half of the remaining way to the minimum viable goal of replacing a third class undergrad. If your business or research initiative is fine with that level of competence then you will find utility.<p>The problem is that I don&#x27;t know anyone who would find that useful. Nor does it fit within any existing working methodology we have. And on top of that the verification of any output can take considerably longer than just doing it yourself in the first place, particularly where it goes off the rails, which it does all the time. I mean it was 3 months ago I was arguing with a model over it not understanding place-value systems properly, something we teach 7 year olds here?<p>But the abstract problem is at a higher level. If it doesn&#x27;t become a general utility for people outside of mathematics, which is very very evident at the moment by the poor overall adoption and very public criticism of the poor result quality, then the funding will dry up. Models cost lots of money to train and if you don&#x27;t have customers it&#x27;s not happening and no one is going to lend you the money any more. And then it&#x27;s moot.</div><br/><div id="42499708" class="c"><input type="checkbox" id="c-42499708" checked=""/><div class="controls bullet"><span class="by">meroes</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497136">parent</a><span>|</span><a href="#42497214">next</a><span>|</span><label class="collapse" for="c-42499708">[-]</label><label class="expand" for="c-42499708">[1 more]</label></div><br/><div class="children"><div class="content">Well said. As someone with only a math undergrad and as a math RLHF’er, this speaks to my experience the most.<p>That craving for an understanding an elegant proof is nowhere to be found with verifying an LLM’s proof.<p>Like sure, you could put together a car by first building an airplane, disassembling all of it minus the two front seats, and having zero elegance and still get a car at the end. But if you do all that and don’t provide novelty in results or useful techniques, there’s no business.<p>Hell, I can’t even get a model to calculate compound interest for me (save for the technicality of prompt engineering a python function to do it). What do I expect?</div><br/></div></div><div id="42497214" class="c"><input type="checkbox" id="c-42497214" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497136">parent</a><span>|</span><a href="#42499708">prev</a><span>|</span><a href="#42496971">next</a><span>|</span><label class="collapse" for="c-42497214">[-]</label><label class="expand" for="c-42497214">[2 more]</label></div><br/><div class="children"><div class="content">This is a great point that nobody will shoot you over :)<p>But the main question is still: assuming you replace an undergrad with a model, who checks the work?  If you have a good process around that already, and find utility as an augmented system, then get you’ll get value - but I still think it’s better for the undergrad to still have the job and be at the wheel, and does things faster and better when leveraging a powerful tool.</div><br/><div id="42497563" class="c"><input type="checkbox" id="c-42497563" checked=""/><div class="controls bullet"><span class="by">hyhconito</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42497214">parent</a><span>|</span><a href="#42496971">next</a><span>|</span><label class="collapse" for="c-42497563">[-]</label><label class="expand" for="c-42497563">[1 more]</label></div><br/><div class="children"><div class="content">Shot already for criticising the shiny thing (happened with crypto and blockchain already...)<p>Well to be fair no one checks what the graduates do properly, even if we hired KPMG in. That is until we get sued. But at least we have someone to blame then. What we don&#x27;t want is something for the graduate to blame. The buck stops at someone corporeal because that&#x27;s what the customers want and the regulators require.<p>That&#x27;s the reality and it&#x27;s not quite as shiny and happy as the tech industry loves to promote itself.<p>My main point, probably cleared up with a simple point: no one gives a shit about this either way.</div><br/></div></div></div></div></div></div><div id="42496971" class="c"><input type="checkbox" id="c-42496971" checked=""/><div class="controls bullet"><span class="by">TheRealPomax</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42497136">prev</a><span>|</span><a href="#42495791">next</a><span>|</span><label class="collapse" for="c-42496971">[-]</label><label class="expand" for="c-42496971">[2 more]</label></div><br/><div class="children"><div class="content">What part do you think is going to become obsolete? Because Math isn&#x27;t about &quot;working out the math&quot;, it&#x27;s about finding the relations between seemingly unrelated things to bust open a problem. Short of AGI, there is no amount of neural net that&#x27;s going to realize that a seemingly impossible probabilistic problem is actually equivalent to a projection of an easy to work with 4D geometry. &quot;Doing the math&quot; is what we have computers for, and the better they get, the easier the tedious parts of the job become, but &quot;doing math&quot; is still very much a human game.</div><br/><div id="42497121" class="c"><input type="checkbox" id="c-42497121" checked=""/><div class="controls bullet"><span class="by">busyant</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496971">parent</a><span>|</span><a href="#42495791">next</a><span>|</span><label class="collapse" for="c-42497121">[-]</label><label class="expand" for="c-42497121">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What part do you think is going to become obsolete?<p>Thank you for the question.<p>I guess what I&#x27;m saying is:<p>Will LLMs (or whatever comes after them) be _so_ good and _so_ pervasive that we will simply be able to say, &quot;Hey ChatGPT-9000, I&#x27;d like to see if the xyz conjecture is correct.&quot; And then ChatGPT-9000 just does the work without us contributing beyond asking a question.<p>Or will the technology be limited&#x2F;bound in some way such that we will still be able to use ChatGPT-9000 as a tool of our own intellectual augmentation and&#x2F;or we could still contribute to research even without it.<p>Hopefully, my comment clarifies my original post.<p>Also, writing this stuff has helped me think about it more. I don&#x27;t have any grand insight, but the more I write, the more I lean toward the outcome that these machines will allow us to augment our research.</div><br/></div></div></div></div><div id="42495791" class="c"><input type="checkbox" id="c-42495791" checked=""/><div class="controls bullet"><span class="by">deepsun</span><span>|</span><a href="#42495668">parent</a><span>|</span><a href="#42496971">prev</a><span>|</span><a href="#42494198">next</a><span>|</span><label class="collapse" for="c-42495791">[-]</label><label class="expand" for="c-42495791">[8 more]</label></div><br/><div class="children"><div class="content">By the way, don&#x27;t trust Nobel laureates or even winners. E.g. Linus Pauling was talking absolute garbage, harmful and evil, after winning the Nobel.</div><br/><div id="42496038" class="c"><input type="checkbox" id="c-42496038" checked=""/><div class="controls bullet"><span class="by">Radim</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42495791">parent</a><span>|</span><a href="#42494198">next</a><span>|</span><label class="collapse" for="c-42496038">[-]</label><label class="expand" for="c-42496038">[7 more]</label></div><br/><div class="children"><div class="content">&gt; <i>don&#x27;t trust Nobel laureates or even winners</i><p>Nobel laureate and winner are the same thing.<p>&gt; <i>Linus Pauling was talking absolute garbage, harmful and evil, after winning the Nobel.</i><p>Can you be more specific, what garbage? And which Nobel prize do you mean – Pauling got two, one for chemistry and one for peace.</div><br/><div id="42496188" class="c"><input type="checkbox" id="c-42496188" checked=""/><div class="controls bullet"><span class="by">deepsun</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496038">parent</a><span>|</span><a href="#42496171">next</a><span>|</span><label class="collapse" for="c-42496188">[-]</label><label class="expand" for="c-42496188">[2 more]</label></div><br/><div class="children"><div class="content">Thank you, my bad.<p>I was referring to Linus&#x27;s harmful and evil promotion of Vitamin C as the cure for everything and cancer. I don&#x27;t think Linus was attaching that garbage to any particular Nobel prize. But people did say to their doctors: &quot;Are you a Nobel winner, doctor?&quot;. Don&#x27;t think they cared about particular prize either.</div><br/><div id="42499816" class="c"><input type="checkbox" id="c-42499816" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496188">parent</a><span>|</span><a href="#42496171">next</a><span>|</span><label class="collapse" for="c-42499816">[-]</label><label class="expand" for="c-42499816">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Linus&#x27;s harmful and evil promotion of Vitamin C<p>Which is &quot;harmful and evil&quot; thanks to your afterknowledge. He had based his books on the research that failed to replicate. But given low toxicity of vitamin C it&#x27;s not that &quot;evil&quot; to recommend treatment even if probabilistic estimation of positive effects is not that high.<p>Sloppy, but not exceptionally bad. At least it was instrumental in teaching me to not expect marvels coming from dietary research.</div><br/></div></div></div></div><div id="42496171" class="c"><input type="checkbox" id="c-42496171" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496038">parent</a><span>|</span><a href="#42496188">prev</a><span>|</span><a href="#42494198">next</a><span>|</span><label class="collapse" for="c-42496171">[-]</label><label class="expand" for="c-42496171">[4 more]</label></div><br/><div class="children"><div class="content">Eugenics and vitamin C as a cure all.</div><br/><div id="42496853" class="c"><input type="checkbox" id="c-42496853" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496171">parent</a><span>|</span><a href="#42494198">next</a><span>|</span><label class="collapse" for="c-42496853">[-]</label><label class="expand" for="c-42496853">[3 more]</label></div><br/><div class="children"><div class="content">If Pauling&#x27;s eugenics policies were bad, then the laws against incest that are currently on the books in many states (which are also eugenics policies that use the same mechanism) are also bad. There are different forms of eugenics policies, and Pauling&#x27;s proposal to restrict the mating choices of people carrying certain recessive genes so their children don&#x27;t suffer is ethically different from Hitler exterminating people with certain genes and also ethically different from other governments sterilizing people with certain genes. He later supported voluntary abortion with genetic testing, which is now standard practice in the US today, though no longer in a few states with ethically questionable laws restricting abortion. This again is ethically different from forced abortion.<p><a href="https:&#x2F;&#x2F;scarc.library.oregonstate.edu&#x2F;coll&#x2F;pauling&#x2F;blood&#x2F;narrative&#x2F;page35.html" rel="nofollow">https:&#x2F;&#x2F;scarc.library.oregonstate.edu&#x2F;coll&#x2F;pauling&#x2F;blood&#x2F;nar...</a></div><br/><div id="42499605" class="c"><input type="checkbox" id="c-42499605" checked=""/><div class="controls bullet"><span class="by">voltaireodactyl</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496853">parent</a><span>|</span><a href="#42498654">next</a><span>|</span><label class="collapse" for="c-42499605">[-]</label><label class="expand" for="c-42499605">[1 more]</label></div><br/><div class="children"><div class="content">FWIW my understanding is that the policies against incest you mention actually have much less to do with controlling genetic reproduction and are more directed at combating familial rape&#x2F;grooming&#x2F;etc.<p>Not a fun thing to discuss, but apparently a significant issue, which I guess should be unsurprising given some of the laws allowing underage marriage if the family signs off.<p>Mentioning only to draw attention to the fact that theoretical policy is often undeniable in a vacuum, but runs aground when faced with real world conditions.</div><br/></div></div><div id="42498654" class="c"><input type="checkbox" id="c-42498654" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42495668">root</a><span>|</span><a href="#42496853">parent</a><span>|</span><a href="#42499605">prev</a><span>|</span><a href="#42494198">next</a><span>|</span><label class="collapse" for="c-42498654">[-]</label><label class="expand" for="c-42498654">[1 more]</label></div><br/><div class="children"><div class="content">From what I remember, he wanted to mark people with tattoos or something.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42494198" class="c"><input type="checkbox" id="c-42494198" checked=""/><div class="controls bullet"><span class="by">voidhorse</span><span>|</span><a href="#42495668">prev</a><span>|</span><a href="#42494042">next</a><span>|</span><label class="collapse" for="c-42494198">[-]</label><label class="expand" for="c-42494198">[24 more]</label></div><br/><div class="children"><div class="content">Eventually we may produce a collection of problems exhaustive enough that these tools can solve almost any problem that isn&#x27;t novel in practice, but I doubt that they will ever become general problem solvers capable of what we consider to be reasoning in humans.<p>Historically, the claim that neural nets were actual models of the human brain and human thinking was always epistemically dubious. It still is. Even as the <i>practical</i> problems of producing better and better algorithms, architectures, and output have been solved, there is no reason to believe a connection between the mechanical model and what happens in organisms has been established. The most important point, in my view, is that all of the representation and interpretation still has to happen outside the computational units. Without human interpreters, none of the AI outputs have any meaning. Unless you believe in determinism and an overseeing god, the story for human beings is much different. AI will not be capable of reason until, like humans, it can develop socio-rational collectivities of meaning that are <i>independent</i> of the human being.<p>Researchers seemed to have a decent grasp on this in the 90s, but today, everyone seems all too ready to make the same ridiculous leaps as the original creators of neural nets. They did not show, as they claimed, that thinking is reducible to computation. All they showed was that a neural net can realize a <i>boolean function</i>—which is not even logic, since, again, the entire semantic interpretive side of the logic is ignored.</div><br/><div id="42494454" class="c"><input type="checkbox" id="c-42494454" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#42494198">parent</a><span>|</span><a href="#42494403">next</a><span>|</span><label class="collapse" for="c-42494454">[-]</label><label class="expand" for="c-42494454">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Unless you believe in determinism and an overseeing god<p>Or perhaps, determinism and mechanistic materialism - which in STEM-adjacent circles has a relatively prevalent adherence.<p>Worldviews which strip a human being of agency in the sense you invoke crop up quite a lot today in such spaces. If you start of adopting a view like this, you have a deflationary sword which can cut down most any notion that&#x27;s not mechanistic in terms of mechanistic parts. &quot;Meaning? Well that&#x27;s just an emergent phenomenon of the influence of such and such causal factors in the unrolling of a deterministic physical system.&quot;<p>Similar for reasoning, etc.<p>Now obviously large swathes of people don&#x27;t really subscribe to this - but it is prevalent and ties in well with utopian progress stories. If something is amenable to mechanistic dissection, possibly it&#x27;s amenable to mechanistic control. And that&#x27;s what our education is really good at teaching us. So such stories end up having intoxicating &quot;hype&quot; effects and drive fundraising, and so we get where we are.<p>For one, I wish people were just excited about making computers do things they couldn&#x27;t do before, without needing to dress it up as something more than it is. &quot;This model can prove a set of theorems in this format with such and such limits and efficiency&quot;</div><br/><div id="42495827" class="c"><input type="checkbox" id="c-42495827" checked=""/><div class="controls bullet"><span class="by">exprofmaddy</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42494454">parent</a><span>|</span><a href="#42494403">next</a><span>|</span><label class="collapse" for="c-42495827">[-]</label><label class="expand" for="c-42495827">[4 more]</label></div><br/><div class="children"><div class="content">Agreed. If someone believes the world is purely mechanistic, then it follows that a sufficiently large computing machine can model the world---like Leibniz&#x27;s Ratiocinator. The intoxication may stem from the potential for predictability and control.<p>The irony is: why would someone want control if they don&#x27;t have true choice? Unfortunately, such a question rarely pierces the intoxicated mind when this mind is preoccupied with pass the class, get an A, get a job, buy a house, raise funds, sell the product, win clients, gain status, eat right, exercise, check insta, watch the game, binge the show, post on Reddit, etc.</div><br/><div id="42497037" class="c"><input type="checkbox" id="c-42497037" checked=""/><div class="controls bullet"><span class="by">Quekid5</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495827">parent</a><span>|</span><a href="#42497234">next</a><span>|</span><label class="collapse" for="c-42497037">[-]</label><label class="expand" for="c-42497037">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If someone believes the world is purely mechanistic, then it follows that a sufficiently large computing machine can model the world<p>Is this controversial in some way? The problem is that to simulate a universe you need a bigger universe -- which doesn&#x27;t exist (or is certainly out of reach due to information theoretical limits)<p>&gt; ---like Leibniz&#x27;s Ratiocinator. The intoxication may stem from the potential for predictability and control.<p>I really don&#x27;t understand the &#x27;control&#x27; angle here. It seems pretty obvious that even in a purely mechanistic view of the universe, information theory forbids using the universe to simulate itself. Limited simulations, sure... but that leaves lots of gaps wherein you lose determinism (and control, whatever that means).</div><br/><div id="42497606" class="c"><input type="checkbox" id="c-42497606" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42497037">parent</a><span>|</span><a href="#42497234">next</a><span>|</span><label class="collapse" for="c-42497606">[-]</label><label class="expand" for="c-42497606">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is this controversial in some way?<p>It’s not “controversial”, it’s just not a given that the universe is to be thought a deterministic machine. Not to everyone, at least.</div><br/></div></div></div></div><div id="42497234" class="c"><input type="checkbox" id="c-42497234" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495827">parent</a><span>|</span><a href="#42497037">prev</a><span>|</span><a href="#42494403">next</a><span>|</span><label class="collapse" for="c-42497234">[-]</label><label class="expand" for="c-42497234">[1 more]</label></div><br/><div class="children"><div class="content">Choice is over rated. This gets to an issue Ive long had with Nozicks experience machine. Not only would I happily spend my days in such a machine, Im pretty sure most other people would too. Maybe they say they wouldnt but if you let them try it out and then offered them the question again I think theyd say yes. The real conclusion of the experience machine is that the unknown is scary.</div><br/></div></div></div></div></div></div><div id="42494403" class="c"><input type="checkbox" id="c-42494403" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42494198">parent</a><span>|</span><a href="#42494454">prev</a><span>|</span><a href="#42495681">next</a><span>|</span><label class="collapse" for="c-42494403">[-]</label><label class="expand" for="c-42494403">[12 more]</label></div><br/><div class="children"><div class="content">&gt;  there is no reason to believe a connection between the mechanical model and what happens in organisms has been established<p>The universal approximation theorem. And that&#x27;s basically it. The rest is empirical.<p>No matter which physical processes happen inside the human brain, a sufficiently large neural network can approximate them. Barring unknowns like super-Turing computational processes in the brain.</div><br/><div id="42495870" class="c"><input type="checkbox" id="c-42495870" checked=""/><div class="controls bullet"><span class="by">exprofmaddy</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42494403">parent</a><span>|</span><a href="#42495444">next</a><span>|</span><label class="collapse" for="c-42495870">[-]</label><label class="expand" for="c-42495870">[6 more]</label></div><br/><div class="children"><div class="content">The universal approximation theorem is set in a precise mathematical context; I encourage you to limit its applicability to that context despite the marketing label &quot;universal&quot; (which it isn&#x27;t). Consider your concession about empiricism. There&#x27;s no empirical way to prove (i.e. there&#x27;s no experiment that can demonstrate beyond doubt) that all brain or other organic processes are deterministic and can be represented completely as functions.</div><br/><div id="42495943" class="c"><input type="checkbox" id="c-42495943" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495870">parent</a><span>|</span><a href="#42495444">next</a><span>|</span><label class="collapse" for="c-42495943">[-]</label><label class="expand" for="c-42495943">[5 more]</label></div><br/><div class="children"><div class="content">Function is the most general way of describing relations. Non-deterministic processes can be represented as functions with a probability distribution codomain. Physics seems to require only continuous functions.<p>Sorry, but there&#x27;s not much evidence that can support human exceptionalism.</div><br/><div id="42499900" class="c"><input type="checkbox" id="c-42499900" checked=""/><div class="controls bullet"><span class="by">voidhorse</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495943">parent</a><span>|</span><a href="#42496335">next</a><span>|</span><label class="collapse" for="c-42499900">[-]</label><label class="expand" for="c-42499900">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand your point here. A (logical) relation is, by definition, a more general way of describing relations than a function, and it is telling that we still suck at using and developing truly <i>relational</i> models that are not univalent (i.e. functions). Only a few old logicians really took the calculus of relations proper seriously (Pierce, for one). We use functions precisely because they are less general, they are rigid, and simpler to work with. I do not think anyone is working under the impression that a function is a high fidelity means to model the world as it is experienced and actually exists. It is necessarily reductionistic (and abstract). Any truth we achieve through functional models is necessarily a general, abstracted, truth, which in many ways proves to be extremely useful but in others (e.g. when an essential piece of information in the <i>particular</i> is not accounted for in the <i>general reductive model</i>) can be disastrous.</div><br/><div id="42500027" class="c"><input type="checkbox" id="c-42500027" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42499900">parent</a><span>|</span><a href="#42496335">next</a><span>|</span><label class="collapse" for="c-42500027">[-]</label><label class="expand" for="c-42500027">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a big fan of philosophy. The epistemology you are talking about is another abstraction on top of the physical world. But the evolution of the physical world as far as we know can be described as a function of time (at least, in a weak gravitational field when energies involved are well below the grand unification energy level, that is for the objects like brains).<p>The brain is a physical system, so whatever it does (including philosophy) can be replicated by modelling (a (vastly) simplified version of) underlying physics.<p>Anyway, I am not especially interested in discussing possible impossibility of an LLM-based AGI. It might be resolved empirically soon enough.</div><br/></div></div></div></div><div id="42496335" class="c"><input type="checkbox" id="c-42496335" checked=""/><div class="controls bullet"><span class="by">exprofmaddy</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495943">parent</a><span>|</span><a href="#42499900">prev</a><span>|</span><a href="#42495444">next</a><span>|</span><label class="collapse" for="c-42496335">[-]</label><label class="expand" for="c-42496335">[2 more]</label></div><br/><div class="children"><div class="content">Some differential equations that model physics admit singularities and multiple solutions. Therefore, functions are not the most general way of describing relations. Functions are a subset of relations.<p>Although &quot;non-deterministic&quot; and &quot;stochastic&quot; are often used interchangeably, they are not equivalent. Probability is applied analysis whose objects are distributions. Analysis is a form of deductive, i.e. mechanical, reasoning. Therefore, it&#x27;s more accurate (philosophically) to identify mathematical probability with determinism. Probability is a model for our experience. That doesn&#x27;t mean our experience is truly probabilistic.<p>Humans aren&#x27;t exceptional. Math modeling and reasoning are human activities.</div><br/><div id="42500638" class="c"><input type="checkbox" id="c-42500638" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42496335">parent</a><span>|</span><a href="#42495444">next</a><span>|</span><label class="collapse" for="c-42500638">[-]</label><label class="expand" for="c-42500638">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Some differential equations that model physics admit singularities and multiple solutions.<p>And physicists regard those as unphysical: the theory breaks down, we need better one.</div><br/></div></div></div></div></div></div></div></div><div id="42495444" class="c"><input type="checkbox" id="c-42495444" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42494403">parent</a><span>|</span><a href="#42495870">prev</a><span>|</span><a href="#42495681">next</a><span>|</span><label class="collapse" for="c-42495444">[-]</label><label class="expand" for="c-42495444">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not useful by itself, because &quot;anything cam model anything else&quot; doesn&#x27;t put any upper bound on emulation cost, which for one small task could be larger than the total energy available in the entire Universem</div><br/><div id="42496129" class="c"><input type="checkbox" id="c-42496129" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495444">parent</a><span>|</span><a href="#42495677">next</a><span>|</span><label class="collapse" for="c-42496129">[-]</label><label class="expand" for="c-42496129">[1 more]</label></div><br/><div class="children"><div class="content">Either the brain violates the physical Church-Turing thesis or it&#x27;s not.<p>If it does, well, it will take more time to incorporate those physical mechanisms into computers to get them on par with the brain.<p>I leave the possibility that it&#x27;s &quot;magic&quot;[1] aside. It&#x27;s just impossible to predict, because it will violate everything we know about our physical world.<p>[1] One example of &quot;magic&quot;: we live in a simulation and the brain is not fully simulated by the physics engine, but creators of the simulation for some reason gave it access to computational resources that are impossible to harness using the standard physics of the simulated world. Another example: interactionistic soul.</div><br/></div></div><div id="42495677" class="c"><input type="checkbox" id="c-42495677" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495444">parent</a><span>|</span><a href="#42496129">prev</a><span>|</span><a href="#42495681">next</a><span>|</span><label class="collapse" for="c-42495677">[-]</label><label class="expand" for="c-42495677">[3 more]</label></div><br/><div class="children"><div class="content">I mean, that is why they mention super-Turning processes like quantum based computing.</div><br/><div id="42496304" class="c"><input type="checkbox" id="c-42496304" checked=""/><div class="controls bullet"><span class="by">dinosaurdynasty</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495677">parent</a><span>|</span><a href="#42495681">next</a><span>|</span><label class="collapse" for="c-42496304">[-]</label><label class="expand" for="c-42496304">[2 more]</label></div><br/><div class="children"><div class="content">Quantum computing actually isn&#x27;t super-Turing, it &quot;just&quot; computes some things faster. (Strictly speaking it&#x27;s somewhere between a standard Turing machine and a nondeterministic Turing machine in speed, and the first can emulate the second.)</div><br/><div id="42498447" class="c"><input type="checkbox" id="c-42498447" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42496304">parent</a><span>|</span><a href="#42495681">next</a><span>|</span><label class="collapse" for="c-42498447">[-]</label><label class="expand" for="c-42498447">[1 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re nitpicking: quantum computing algorithms could (if implemented) compute <i>certain things</i> faster <i>than the best classical algorithms we know</i>. We don&#x27;t know any quantum algorithms that are provably faster than <i>all possible</i> classical algorithms.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42495681" class="c"><input type="checkbox" id="c-42495681" checked=""/><div class="controls bullet"><span class="by">exprofmaddy</span><span>|</span><a href="#42494198">parent</a><span>|</span><a href="#42494403">prev</a><span>|</span><a href="#42494530">next</a><span>|</span><label class="collapse" for="c-42495681">[-]</label><label class="expand" for="c-42495681">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m with you. Interpreting a problem as a problem requires a human (1) to recognize the problem and (2) to convince other humans that it&#x27;s a problem worth solving. Both involve value, and value has no computational or mechanistic description (other than &quot;given&quot; or &quot;illusion&quot;). Once humans have identified a problem, they might employ a tool to find the solution. The tool has no sense that the problem is important or even hard; such values are imposed by the tool&#x27;s users.<p>It&#x27;s worth considering why &quot;everyone seems all too ready to make ... leaps ...&quot; &quot;Neural&quot;, &quot;intelligence&quot;, &quot;learning&quot;, and others are metaphors that have performed very well as marketing slogans. Behind the marketing slogans are deep-pocketed, platformed corporate and government (i.e. socio-rational collective) interests. Educational institutions (another socio-rational collective) and their leaders have on the whole postured as trainers and preparers for the &quot;real world&quot; (i.e. a job), which means they accept, support, and promote the corporate narratives about techno-utopia. Which institutions are left to check the narratives? Who has time to ask questions given the need to learn all the technobabble (by paying hundreds of thousands for 120 university credits) to become a competitive job candidate?<p>I&#x27;ve found there are many voices speaking against the hype---indeed, even (rightly) questioning the epistemic underpinnings of AI. But they&#x27;re ignored and out-shouted by tech marketing, fundraising politicians, and engagement-driven media.</div><br/></div></div><div id="42494530" class="c"><input type="checkbox" id="c-42494530" checked=""/><div class="controls bullet"><span class="by">gmadsen</span><span>|</span><a href="#42494198">parent</a><span>|</span><a href="#42495681">prev</a><span>|</span><a href="#42494238">next</a><span>|</span><label class="collapse" for="c-42494530">[-]</label><label class="expand" for="c-42494530">[4 more]</label></div><br/><div class="children"><div class="content">I hear these arguments a lot from law and philosophy students, never from those trained in mathematics. It seems to me, &quot;literary&quot; people will still be discussing these theoretical hypotheticals as technology passes them by building it.</div><br/><div id="42495557" class="c"><input type="checkbox" id="c-42495557" checked=""/><div class="controls bullet"><span class="by">voidhorse</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42494530">parent</a><span>|</span><a href="#42494238">next</a><span>|</span><label class="collapse" for="c-42495557">[-]</label><label class="expand" for="c-42495557">[3 more]</label></div><br/><div class="children"><div class="content">I straddle both worlds. Consider that using the lens of mathematical reasoning to understand everything is a bit like trying to use a single mathematical theory (eg that of groups) to comprehend mathematics as a whole. You will almost always benefit and enrich your own understanding by daring to incorporate outside perspectives.<p>Consider also that even as digital technology and the ratiomathimatical understanding of the world has advanced it is still rife with  dynamics and problems that require a humanistic approach. In particular, a mathematical conception cannot resolve <i>teleological</i> problems which require the establishment of consensus and the actual determination of what we, as a species, want the world to look like. Climate change and general economic imbalance are already evidence of the kind of disasters that mount when you limit yourself to a reductionistic, overly mathematical and technological understanding of life and existence. Being is not a solely technical problem.</div><br/><div id="42497309" class="c"><input type="checkbox" id="c-42497309" checked=""/><div class="controls bullet"><span class="by">gmadsen</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42495557">parent</a><span>|</span><a href="#42494238">next</a><span>|</span><label class="collapse" for="c-42497309">[-]</label><label class="expand" for="c-42497309">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree, I just don&#x27;t think it is done well or at least as seriously as it used to. In modern philosophy, there are many mathematically specious arguments, that just make clear how large the mathematical gap has become e.g. improper application of Godel&#x27;s incompleteness theorems. Yet Godel was a philosopher himself, who would disagree with its current hand-wavy usage.<p>19th&#x2F;20th was a golden era of philosophy with a coherent and rigorous mathematical lens to apply with other lenses. Russel, Turing, Godel, etc. However this just doesn&#x27;t exist anymore</div><br/><div id="42499803" class="c"><input type="checkbox" id="c-42499803" checked=""/><div class="controls bullet"><span class="by">voidhorse</span><span>|</span><a href="#42494198">root</a><span>|</span><a href="#42497309">parent</a><span>|</span><a href="#42494238">next</a><span>|</span><label class="collapse" for="c-42499803">[-]</label><label class="expand" for="c-42499803">[1 more]</label></div><br/><div class="children"><div class="content">While I agree that these are titans of 20th c. philosophy, particularly of the philosophy of mathematics and logic, the overarching school they belonged to (logical positivism) has been thoroughly and rightly criticized, and it is informative to read these criticisms to understand why a view of life that is <i>overly</i> mathematical is in many ways inadequate. Your comment still argues from a very limited perspective. There is no reason that correct application of Gödel s theorem should be any indication of the richness of someone&#x27;s philosophical views <i>unless</i> you are already a staunchly committed reductionist who values mathematical arguments above all else (why? can maths help you explain and understand the phenomena of love in a way that will actually help you <i>experience love</i>? this is just one example domain where it does not make much sense), <i>or</i> unless they are specifically attempting a philosophy of mathematics. The question of whether or not we can effectively model cognition and human mental function using mathematical models is not a question of mathematical philosophy, but rather one of <i>epistemology</i>. If you really want to head a spurious argument, read McCulloch and Pitts. They essentially present an argument of two premises, the brain is finite, and we can create a machine of formal &quot;neurons&quot; (which are not even complete models of real neurons) that computes a boolean function, they then <i>conclude</i> that they must have a model of cognition, that cognition must be nothing more than computation, and that the brain must basically be a Turing machine.<p>The relevance of mathematics to the cognitive problem must be decided <i>outside of</i> mathematics. As another poster said, even if you buy the theorems, it is still an <i>empirical question</i> as to whether or not they really <i>model</i> what they claim to model, and whether or not that model is of a fidelity that we find acceptable for a definition of general intelligence. Often, people reach claims of adequacy today <i>not</i> by producing really fantastic models but instead by <i>lowering the bar enormously</i>. They claim that these models approximate humans by severely reducing the idea of what it means to be an intelligent human to the specific talents their tech happens to excel at (e.g. apparently being a language parrot is all that intelligence is, ignoring all the very nuanced views and definitions of intelligence we have come up with over the course of history. A machine that is not embodied ina skeletal structure and cannot even <i>experience</i>, let alone solve, the vast number of physical, anatomical problems we contend with on a daily basis is, in my view, still very far from anything I would call general intelligence).</div><br/></div></div></div></div></div></div></div></div><div id="42494238" class="c"><input type="checkbox" id="c-42494238" checked=""/><div class="controls bullet"><span class="by">nmca</span><span>|</span><a href="#42494198">parent</a><span>|</span><a href="#42494530">prev</a><span>|</span><a href="#42494042">next</a><span>|</span><label class="collapse" for="c-42494238">[-]</label><label class="expand" for="c-42494238">[1 more]</label></div><br/><div class="children"><div class="content">Can you define what you mean by novel here?</div><br/></div></div></div></div><div id="42494042" class="c"><input type="checkbox" id="c-42494042" checked=""/><div class="controls bullet"><span class="by">seafoamteal</span><span>|</span><a href="#42494198">prev</a><span>|</span><a href="#42494217">next</a><span>|</span><label class="collapse" for="c-42494042">[-]</label><label class="expand" for="c-42494042">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have much to opine from an advanced maths perspective, but I&#x27;d like to point out a couple examples of where ChatGPT made basic errors in questions I asked it as an undergrad CS student.<p>1. I asked it to show me the derivation of a formula for the efficiency of Stop-and-Wait ARQ and it seemed to do it, but a day later, I realised that in one of the steps, it just made a term vanish to get to the next step. Obviously, I should have verified more carefully, but when I asked it to spot the mistake in that step, it did the same thing twice more with bs explanations of how the term is absorbed.<p>2. I asked it to provide me syllogisms that I could practice proving. An overwhelming number of the syllogisms it gave me were inconsistent and did not hold. This surprised me more because syllogisms are about the most structured arguments you can find, having been formalized centuries ago and discussed extensively since then. In this case, asking it to walk step-by-step actually fixed the issue.<p>Both of these were done on the free plan of ChatGPT, but I can remember if it was 4o or 4.</div><br/><div id="42497196" class="c"><input type="checkbox" id="c-42497196" checked=""/><div class="controls bullet"><span class="by">voiper1</span><span>|</span><a href="#42494042">parent</a><span>|</span><a href="#42494217">next</a><span>|</span><label class="collapse" for="c-42497196">[-]</label><label class="expand" for="c-42497196">[1 more]</label></div><br/><div class="children"><div class="content">The first question is always: which model? Which fortunately you at least addressed: 
&gt;free plan of ChatGPT, but I can remember if it was 4o or 4.<p>Since chatgpt-4o, there has been o1-preview, and o1 (full) is out. They just announced o3 got a 25% on frontiermath which is what this article is a reaction to. So, any tests on 4o are at least TWO (or three) AI releases with new capabilities.</div><br/></div></div></div></div><div id="42494217" class="c"><input type="checkbox" id="c-42494217" checked=""/><div class="controls bullet"><span class="by">upghost</span><span>|</span><a href="#42494042">prev</a><span>|</span><a href="#42493879">next</a><span>|</span><label class="collapse" for="c-42494217">[-]</label><label class="expand" for="c-42494217">[10 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t see anyone else ask this but.. isn&#x27;t the FrontierMath dataset compromised now? At the very least OpenAI now knows the questions if not the answers. I would expect that the next iteration will &quot;magically&quot; get over 80% on the FrontierMath test. I imagine that experiment was pretty closely monitored.</div><br/><div id="42494255" class="c"><input type="checkbox" id="c-42494255" checked=""/><div class="controls bullet"><span class="by">jvanderbot</span><span>|</span><a href="#42494217">parent</a><span>|</span><a href="#42494788">next</a><span>|</span><label class="collapse" for="c-42494255">[-]</label><label class="expand" for="c-42494255">[5 more]</label></div><br/><div class="children"><div class="content">I figured their model was independently evaluated against the questions&#x2F;answers. That&#x27;s not to say it&#x27;s not compromised by &quot;Here&#x27;s a bag of money&quot; type methods, but I don&#x27;t even think it&#x27;d be a reasonable test if they just handed over the dataset.</div><br/><div id="42494285" class="c"><input type="checkbox" id="c-42494285" checked=""/><div class="controls bullet"><span class="by">upghost</span><span>|</span><a href="#42494217">root</a><span>|</span><a href="#42494255">parent</a><span>|</span><a href="#42494788">next</a><span>|</span><label class="collapse" for="c-42494285">[-]</label><label class="expand" for="c-42494285">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure it was independently evaluated, but I&#x27;m sure the folks running the test were not given an on-prem installation of ChatGPT to mess with. It was still done via API calls, presumably through the chat interface UI.<p>That means the questions went over the fence to OpenAI.<p>I&#x27;m quite certain they are aware of that, and it would be pretty foolish not to take advantage of at least knowing what the questions are.</div><br/><div id="42498972" class="c"><input type="checkbox" id="c-42498972" checked=""/><div class="controls bullet"><span class="by">ls612</span><span>|</span><a href="#42494217">root</a><span>|</span><a href="#42494285">parent</a><span>|</span><a href="#42494336">next</a><span>|</span><label class="collapse" for="c-42498972">[-]</label><label class="expand" for="c-42498972">[2 more]</label></div><br/><div class="children"><div class="content">Depending on the plan the researchers used they may have contractual protections against OpenAI training on their inputs.</div><br/><div id="42499553" class="c"><input type="checkbox" id="c-42499553" checked=""/><div class="controls bullet"><span class="by">upghost</span><span>|</span><a href="#42494217">root</a><span>|</span><a href="#42498972">parent</a><span>|</span><a href="#42494336">next</a><span>|</span><label class="collapse" for="c-42499553">[-]</label><label class="expand" for="c-42499553">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but given the resourcing at OpenAI, it would not be hard to clean[1] the inputs. I&#x27;m just trying to be realistic here, there are plenty of ways around contractual obligations and a significant incentive to do so.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Clean-room_design" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Clean-room_design</a></div><br/></div></div></div></div><div id="42494336" class="c"><input type="checkbox" id="c-42494336" checked=""/><div class="controls bullet"><span class="by">jvanderbot</span><span>|</span><a href="#42494217">root</a><span>|</span><a href="#42494285">parent</a><span>|</span><a href="#42498972">prev</a><span>|</span><a href="#42494788">next</a><span>|</span><label class="collapse" for="c-42494336">[-]</label><label class="expand" for="c-42494336">[1 more]</label></div><br/><div class="children"><div class="content">Now that you put it that way, it is laughably easy.</div><br/></div></div></div></div></div></div><div id="42494788" class="c"><input type="checkbox" id="c-42494788" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#42494217">parent</a><span>|</span><a href="#42494255">prev</a><span>|</span><a href="#42493879">next</a><span>|</span><label class="collapse" for="c-42494788">[-]</label><label class="expand" for="c-42494788">[4 more]</label></div><br/><div class="children"><div class="content">This was my first thought when I saw the results:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42473470">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42473470</a></div><br/><div id="42494913" class="c"><input type="checkbox" id="c-42494913" checked=""/><div class="controls bullet"><span class="by">upghost</span><span>|</span><a href="#42494217">root</a><span>|</span><a href="#42494788">parent</a><span>|</span><a href="#42493879">next</a><span>|</span><label class="collapse" for="c-42494913">[-]</label><label class="expand" for="c-42494913">[3 more]</label></div><br/><div class="children"><div class="content">Insightful comment. The thing that&#x27;s extremely frustrating is look at all the energy poured into this conversation around benchmarks. There is a fundamental assumption of honesty and integrity in the benchmarking process by at least some people. But when the dataset is compromised and generation N+1 has miraculous performance gains, how can we see this as anything other than a ploy to pump up valuations? Some people have millions of dollars at stake here and they don&#x27;t care about the naysayers in the peanut gallery like us.</div><br/><div id="42495201" class="c"><input type="checkbox" id="c-42495201" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#42494217">root</a><span>|</span><a href="#42494913">parent</a><span>|</span><a href="#42493879">next</a><span>|</span><label class="collapse" for="c-42495201">[-]</label><label class="expand" for="c-42495201">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s sadly inevitable that when billions in funding and industry hype are tied to performance on a handful of benchmarks, scores will somehow, magically, continue to go up.<p>Needless to say, it doesn&#x27;t bring us any closer to AGI.<p>The only solution I see here is people crafting their own, private benchmarks that the big players don&#x27;t care about enough to train on. That, at least, gives you a clearer view of the field.</div><br/><div id="42495455" class="c"><input type="checkbox" id="c-42495455" checked=""/><div class="controls bullet"><span class="by">upghost</span><span>|</span><a href="#42494217">root</a><span>|</span><a href="#42495201">parent</a><span>|</span><a href="#42493879">next</a><span>|</span><label class="collapse" for="c-42495455">[-]</label><label class="expand" for="c-42495455">[1 more]</label></div><br/><div class="children"><div class="content">Not sure why your comment was downvoted, but it certainly shows the pressure going against people who point out fundamental flaws. This is pushing us towards &quot;AVI&quot; rather than AGI-- &quot;Artificially Valued Intelligence&quot;. The optimization function here is around the market.<p>I&#x27;m being completely serious. You are correct, despite the downvotes, that this could not be pushing us towards AGI because if the dataset is leaked you can&#x27;t claim the G-- generalizability.<p>The point of the benchmark is to lead is to believe that this is a substantial breakthrough. But a reasonable person would be forced to conclude that the results are misleading to due to optimizing around the training data.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42493879" class="c"><input type="checkbox" id="c-42493879" checked=""/><div class="controls bullet"><span class="by">LittleTimothy</span><span>|</span><a href="#42494217">prev</a><span>|</span><a href="#42494065">next</a><span>|</span><label class="collapse" for="c-42493879">[-]</label><label class="expand" for="c-42493879">[30 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fascinating that this has run into the exact same problem as the Quantum research. Ie, in the quantum research to demonstrate any valuable forward progress you must compute something that is impossible to do with a traditional computer. If you can&#x27;t do it with a traditional computer, it suddenly becomes difficult to verify correctness (ie, you can&#x27;t just check it was matching the traditional computer&#x27;s answer.<p>In the same way ChatGPT scores 25% on this and the question is &quot;How close were those 25% to questions in the training set&quot;. Or to put it another way we want to answer the question &quot;Is ChatGPT getting better at applying it&#x27;s reasoning to out-of-set problems or is it pulling more data into it&#x27;s training set&quot;. Or &quot;Is the test leaking into the training&quot;.<p>Maybe the whole question is academic and it doesn&#x27;t matter, we solve the entire problem by pulling all human knowledge into the training set and that&#x27;s a massive benefit. But maybe it implies a limit to how far it can push human knowledge forward.</div><br/><div id="42493978" class="c"><input type="checkbox" id="c-42493978" checked=""/><div class="controls bullet"><span class="by">newpavlov</span><span>|</span><a href="#42493879">parent</a><span>|</span><a href="#42494001">next</a><span>|</span><label class="collapse" for="c-42493978">[-]</label><label class="expand" for="c-42493978">[12 more]</label></div><br/><div class="children"><div class="content">&gt;in the quantum research to demonstrate any valuable forward progress you must compute something that is impossible to do with a traditional computer<p>This is factually wrong. The most interesting problems motivating the quantum computing research are hard to solve, but easy to verify on classical computers. The factorization problem is the most classical example.<p>The problem is that existing quantum computers are not powerful enough to solve the interesting problems, so researchers have to invent semi-artificial problems to demonstrate &quot;quantum advantage&quot; to keep the funding flowing.<p>There is a plethora of opportunities for LLMs to show their worth. For example, finding interesting links between different areas of research or being a proof assistant in a math&#x2F;programming formal verification system. There is a lot of ongoing work in this area, but at the moment signal-to-noise ratio of such tools is too low for them to be practical.</div><br/><div id="42494399" class="c"><input type="checkbox" id="c-42494399" checked=""/><div class="controls bullet"><span class="by">bondarchuk</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493978">parent</a><span>|</span><a href="#42494621">next</a><span>|</span><label class="collapse" for="c-42494399">[-]</label><label class="expand" for="c-42494399">[7 more]</label></div><br/><div class="children"><div class="content">No, it is factually right, at least if Scott Aaronson is to be believed:<p>&gt; <i>Having said that, the biggest caveat to the “10^25 years” result is one to which I fear Google drew insufficient attention. Namely, for the exact same reason why (as far as anyone knows) this quantum computation would take ~10^25 years for a classical computer to simulate, it would also take ~10^25 years for a classical computer to directly verify the quantum computer’s results!! (For example, by computing the “Linear Cross-Entropy” score of the outputs.) For this reason, all validation of Google’s new supremacy experiment is indirect, based on extrapolations from smaller circuits, ones for which a classical computer can feasibly check the results. To be clear, I personally see no reason to doubt those extrapolations. But for anyone who wonders why I’ve been obsessing for years about the need to design efficiently verifiable near-term quantum supremacy experiments: well, this is why! We’re now deeply into the unverifiable regime that I warned about.</i><p><a href="https:&#x2F;&#x2F;scottaaronson.blog&#x2F;?p=8525" rel="nofollow">https:&#x2F;&#x2F;scottaaronson.blog&#x2F;?p=8525</a></div><br/><div id="42494562" class="c"><input type="checkbox" id="c-42494562" checked=""/><div class="controls bullet"><span class="by">newpavlov</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494399">parent</a><span>|</span><a href="#42498838">next</a><span>|</span><label class="collapse" for="c-42494562">[-]</label><label class="expand" for="c-42494562">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a property of the &quot;semi-artificial&quot; problem chosen by Google. If anything, it means that we should heavily discount this claim of &quot;quantum advantage&quot;, especially in the light of inherent probabilistic nature of quantum computations.<p>Note that the OP wrote &quot;you MUST compute something that is impossible to do with a traditional computer&quot;. I demonstrated a simple counter-example to this statement: you CAN demonstrate forward progress by factorizing big numbers, but the problem is that no one can do it despite billions of investments.</div><br/><div id="42494606" class="c"><input type="checkbox" id="c-42494606" checked=""/><div class="controls bullet"><span class="by">bondarchuk</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494562">parent</a><span>|</span><a href="#42498838">next</a><span>|</span><label class="collapse" for="c-42494606">[-]</label><label class="expand" for="c-42494606">[3 more]</label></div><br/><div class="children"><div class="content">Apparently they can&#x27;t, right now, as you admit. Anyway this is turning into a stupid semantic argument, have a nice day.</div><br/><div id="42495909" class="c"><input type="checkbox" id="c-42495909" checked=""/><div class="controls bullet"><span class="by">joshuaissac</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494606">parent</a><span>|</span><a href="#42498838">next</a><span>|</span><label class="collapse" for="c-42495909">[-]</label><label class="expand" for="c-42495909">[2 more]</label></div><br/><div class="children"><div class="content">If they can&#x27;t, then is it really quantum supremacy?<p>They claimed it last time in 2019 with Sycamore, which could perform in 200 seconds a calculation that Google claimed would take a classical supercomputer 10,000 years.<p>That was debunked when a team of scientists replicated the same thing on an ordinary computer in 15 hours with a large number of GPUs. Scott Aaronson said that on a supercomputer, the same technique would have solved the problem in seconds.[1]<p>So if they now come up with another problem which they say cannot even be verified by a classical computer and uses it to claim quantum advantage, then it is right to be suspicious of that claim.<p>1. <a href="https:&#x2F;&#x2F;www.science.org&#x2F;content&#x2F;article&#x2F;ordinary-computers-can-beat-google-s-quantum-computer-after-all" rel="nofollow">https:&#x2F;&#x2F;www.science.org&#x2F;content&#x2F;article&#x2F;ordinary-computers-c...</a></div><br/><div id="42499276" class="c"><input type="checkbox" id="c-42499276" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42495909">parent</a><span>|</span><a href="#42498838">next</a><span>|</span><label class="collapse" for="c-42499276">[-]</label><label class="expand" for="c-42499276">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If they can&#x27;t, then is it really quantum supremacy?<p>Yes, quantum supremacy on an artificial problem is quantum supremacy (even if it&#x27;s &quot;this quantum computer can simulate itself faster than a classical computer&quot;). Quantum supremacy on problems that are easy to verify would of course be nicer, but unfortunately not all problems happen to have an easy verification.</div><br/></div></div></div></div></div></div></div></div><div id="42498838" class="c"><input type="checkbox" id="c-42498838" checked=""/><div class="controls bullet"><span class="by">cowl</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494399">parent</a><span>|</span><a href="#42494562">prev</a><span>|</span><a href="#42495986">next</a><span>|</span><label class="collapse" for="c-42498838">[-]</label><label class="expand" for="c-42498838">[1 more]</label></div><br/><div class="children"><div class="content">that applies specifically to this artificial problem google created to be hard for classical computers and in fact in the end it turned out it was not so much. IBM came up with a method to do what google said it would take 10.000 years on a classical computers in just 2 days. I would not be surprised if a similar reduction happened also to their second attempt if anyone was motivated enough to look at it.<p>In general we have thousands of optimisations problems that are hard to solve but immediate to verify.</div><br/></div></div><div id="42495986" class="c"><input type="checkbox" id="c-42495986" checked=""/><div class="controls bullet"><span class="by">noqc</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494399">parent</a><span>|</span><a href="#42498838">prev</a><span>|</span><a href="#42494621">next</a><span>|</span><label class="collapse" for="c-42495986">[-]</label><label class="expand" for="c-42495986">[1 more]</label></div><br/><div class="children"><div class="content">the unverifiable regime is a <i>great</i> way to extract funding.</div><br/></div></div></div></div><div id="42494621" class="c"><input type="checkbox" id="c-42494621" checked=""/><div class="controls bullet"><span class="by">derangedHorse</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493978">parent</a><span>|</span><a href="#42494399">prev</a><span>|</span><a href="#42494153">next</a><span>|</span><label class="collapse" for="c-42494621">[-]</label><label class="expand" for="c-42494621">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This is factually wrong.<p>What&#x27;s factually wrong about it? OP said &quot;you must compute something that is impossible to do with a traditional computer&quot; which is true, regardless of the output produced. Verifying an output is very different from verifying the proper execution of a program. The difference between testing a program and seeing its code.<p>What is being computed is fundamentally different from classical computers, therefore the verification methods of proper adherence to instructions becomes increasingly complex.</div><br/><div id="42494721" class="c"><input type="checkbox" id="c-42494721" checked=""/><div class="controls bullet"><span class="by">ajmurmann</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494621">parent</a><span>|</span><a href="#42494153">next</a><span>|</span><label class="collapse" for="c-42494721">[-]</label><label class="expand" for="c-42494721">[1 more]</label></div><br/><div class="children"><div class="content">They left out the key part which was incorrect and the sentence right after &quot;If you can&#x27;t do it with a traditional computer, it suddenly becomes difficult to verify correctness&quot;<p>The point stands that for actually interesting problems verifying correctness of the results is trivial. I don&#x27;t know if &quot;adherence to instructions&quot; transudates at all to quantum computing.</div><br/></div></div></div></div><div id="42494153" class="c"><input type="checkbox" id="c-42494153" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493978">parent</a><span>|</span><a href="#42494621">prev</a><span>|</span><a href="#42494001">next</a><span>|</span><label class="collapse" for="c-42494153">[-]</label><label class="expand" for="c-42494153">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This is factually wrong. The most interesting problems motivating the quantum computing research are hard to solve, but easy to verify on classical computers.<p>You parent did not talk about quantum <i>computers</i>. I guess he rather had predictions of novel quantum-field theories or theories of quantum gravity in the back of his mind.</div><br/><div id="42494195" class="c"><input type="checkbox" id="c-42494195" checked=""/><div class="controls bullet"><span class="by">newpavlov</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494153">parent</a><span>|</span><a href="#42494001">next</a><span>|</span><label class="collapse" for="c-42494195">[-]</label><label class="expand" for="c-42494195">[1 more]</label></div><br/><div class="children"><div class="content">Then his comment makes even less sense.</div><br/></div></div></div></div></div></div><div id="42494001" class="c"><input type="checkbox" id="c-42494001" checked=""/><div class="controls bullet"><span class="by">0xfffafaCrash</span><span>|</span><a href="#42493879">parent</a><span>|</span><a href="#42493978">prev</a><span>|</span><a href="#42493960">next</a><span>|</span><label class="collapse" for="c-42494001">[-]</label><label class="expand" for="c-42494001">[2 more]</label></div><br/><div class="children"><div class="content">I agree with the issue of ”is the test dataset leaking into the training dataset” being an issue with interpreting LLM capabilities in novel contexts, but not sure I follow what you mean on the quantum computing front.<p>My understanding is that many problems have solutions that are easier to verify than to solve using classical computing. e.g. prime factorization</div><br/><div id="42494655" class="c"><input type="checkbox" id="c-42494655" checked=""/><div class="controls bullet"><span class="by">LittleTimothy</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494001">parent</a><span>|</span><a href="#42493960">next</a><span>|</span><label class="collapse" for="c-42494655">[-]</label><label class="expand" for="c-42494655">[1 more]</label></div><br/><div class="children"><div class="content">Oh it&#x27;s a totally different issue on the quantum side that leads to the same issue with difficulty verifying. There, the algorithms that Google for example is using today, aren&#x27;t like prime factorization, they&#x27;re not easy to directly verify with traditional computers, so as far as I&#x27;m aware they kind of check the result for a suitably small run, and then do the performance metrics on a large run that they <i>hope</i> gave a correct answer but aren&#x27;t able to directly verify.</div><br/></div></div></div></div><div id="42493960" class="c"><input type="checkbox" id="c-42493960" checked=""/><div class="controls bullet"><span class="by">eagerpace</span><span>|</span><a href="#42493879">parent</a><span>|</span><a href="#42494001">prev</a><span>|</span><a href="#42493905">next</a><span>|</span><label class="collapse" for="c-42493960">[-]</label><label class="expand" for="c-42493960">[1 more]</label></div><br/><div class="children"><div class="content">How much of this could be resolved if its training set were reduced? Conceivably, most of the training serves only to confuse the model when only aiming to solve a math equation.</div><br/></div></div><div id="42493905" class="c"><input type="checkbox" id="c-42493905" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#42493879">parent</a><span>|</span><a href="#42493960">prev</a><span>|</span><a href="#42494065">next</a><span>|</span><label class="collapse" for="c-42493905">[-]</label><label class="expand" for="c-42493905">[14 more]</label></div><br/><div class="children"><div class="content">If constrained by existing human knowledge to come up with an answer, won’t it fundamentally be unable to push human knowledge forward?</div><br/><div id="42493968" class="c"><input type="checkbox" id="c-42493968" checked=""/><div class="controls bullet"><span class="by">LittleTimothy</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493905">parent</a><span>|</span><a href="#42494161">next</a><span>|</span><label class="collapse" for="c-42493968">[-]</label><label class="expand" for="c-42493968">[2 more]</label></div><br/><div class="children"><div class="content">Depends on your understanding of human knowledge I guess? People talk about the frontier of human knowledge and if your view of knowledge is like that of a unique human genius pushing forward the frontier then yes - it&#x27;d be stuck. But if you think of knowledge as more complex than that you could have areas that are kind of within our frontier of knowledge (that we could reasonably know, but don&#x27;t actually know) - taking concepts that we already know in one field and applying them to some other field. Today the reason that doesn&#x27;t happen is because genius A in physics doesn&#x27;t know about the existence of genius B in mathematics (let alone understand their research), but if it&#x27;s all imbibed by &quot;The Model&quot; then it&#x27;s trivial to make that discovery.</div><br/><div id="42493986" class="c"><input type="checkbox" id="c-42493986" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493968">parent</a><span>|</span><a href="#42494161">next</a><span>|</span><label class="collapse" for="c-42493986">[-]</label><label class="expand" for="c-42493986">[1 more]</label></div><br/><div class="children"><div class="content">I was referring specifically to the parent comments statements around current AI systems.</div><br/></div></div></div></div><div id="42494161" class="c"><input type="checkbox" id="c-42494161" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493905">parent</a><span>|</span><a href="#42493968">prev</a><span>|</span><a href="#42496334">next</a><span>|</span><label class="collapse" for="c-42494161">[-]</label><label class="expand" for="c-42494161">[2 more]</label></div><br/><div class="children"><div class="content">Reasoning is essentially the creation of new knowledge from existing knowledge. The better the model can reason the less constrained it is to existing knowledge.<p>The challenge is how to figure out if a model is genuinely reasoning</div><br/><div id="42495007" class="c"><input type="checkbox" id="c-42495007" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494161">parent</a><span>|</span><a href="#42496334">next</a><span>|</span><label class="collapse" for="c-42495007">[-]</label><label class="expand" for="c-42495007">[1 more]</label></div><br/><div class="children"><div class="content">Reasoning is a very minor (but essential) part of knowledge creation.<p>Knowledge creation comes from collecting data from the real world, and cleaning it up somehow, and brainstorming creative models to explain it.<p>NN&#x2F;LLM&#x27;s version of model building is frustrating because it is quite good, but not highly &quot;explainable&quot;. Human models have higher explainability, while machine models have high predictive value on test examples due to an impenetrable mountain of algebra.</div><br/></div></div></div></div><div id="42496334" class="c"><input type="checkbox" id="c-42496334" checked=""/><div class="controls bullet"><span class="by">dinosaurdynasty</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493905">parent</a><span>|</span><a href="#42494161">prev</a><span>|</span><a href="#42493933">next</a><span>|</span><label class="collapse" for="c-42496334">[-]</label><label class="expand" for="c-42496334">[1 more]</label></div><br/><div class="children"><div class="content">There are likely lots of connections that could be made that no individual has made because no individual has <i>all of existing human knowledge</i> at their immediate disposal.</div><br/></div></div><div id="42493933" class="c"><input type="checkbox" id="c-42493933" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493905">parent</a><span>|</span><a href="#42496334">prev</a><span>|</span><a href="#42493912">next</a><span>|</span><label class="collapse" for="c-42493933">[-]</label><label class="expand" for="c-42493933">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think many expect AI to push knowledge forward? A thing that basically just regurgitates consensus historic knowledge seems badly suited to that</div><br/><div id="42493969" class="c"><input type="checkbox" id="c-42493969" checked=""/><div class="controls bullet"><span class="by">calmoo</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493933">parent</a><span>|</span><a href="#42494129">next</a><span>|</span><label class="collapse" for="c-42493969">[-]</label><label class="expand" for="c-42493969">[1 more]</label></div><br/><div class="children"><div class="content">But apparently these new frontier models can &#x27;reason&#x27; - so with that logic, they should be able to generate new knowledge?</div><br/></div></div><div id="42494129" class="c"><input type="checkbox" id="c-42494129" checked=""/><div class="controls bullet"><span class="by">tomjen3</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493933">parent</a><span>|</span><a href="#42493969">prev</a><span>|</span><a href="#42493912">next</a><span>|</span><label class="collapse" for="c-42494129">[-]</label><label class="expand" for="c-42494129">[1 more]</label></div><br/><div class="children"><div class="content">O1 was able to find the math problem in a recently published paper, so yes.</div><br/></div></div></div></div><div id="42493912" class="c"><input type="checkbox" id="c-42493912" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493905">parent</a><span>|</span><a href="#42493933">prev</a><span>|</span><a href="#42494065">next</a><span>|</span><label class="collapse" for="c-42493912">[-]</label><label class="expand" for="c-42493912">[5 more]</label></div><br/><div class="children"><div class="content">Then much of human research and development is also fundamentally impossible.</div><br/><div id="42493964" class="c"><input type="checkbox" id="c-42493964" checked=""/><div class="controls bullet"><span class="by">AnerealDew</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493912">parent</a><span>|</span><a href="#42494065">next</a><span>|</span><label class="collapse" for="c-42493964">[-]</label><label class="expand" for="c-42493964">[4 more]</label></div><br/><div class="children"><div class="content">Only if you think current &quot;AI&quot; is on the same level as human creativity and intelligence, which it clearly is not.</div><br/><div id="42494176" class="c"><input type="checkbox" id="c-42494176" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42493964">parent</a><span>|</span><a href="#42494065">next</a><span>|</span><label class="collapse" for="c-42494176">[-]</label><label class="expand" for="c-42494176">[3 more]</label></div><br/><div class="children"><div class="content">I think current &quot;AI&quot; (i.e. LLMs) is unable to push human knowledge forward, but not because it&#x27;s constrained by existing human knowledge. It&#x27;s more like peeking into a very large magic-8 ball, new answers everytime you shake it. Some useful.</div><br/><div id="42494790" class="c"><input type="checkbox" id="c-42494790" checked=""/><div class="controls bullet"><span class="by">SJC_Hacker</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494176">parent</a><span>|</span><a href="#42495407">next</a><span>|</span><label class="collapse" for="c-42494790">[-]</label><label class="expand" for="c-42494790">[1 more]</label></div><br/><div class="children"><div class="content">It may be able to push human knowledge forward to an extent.<p>In the past, there was quite a bit of low hanging fruit such that you could have polymaths able to contribute to a wide variety of fields, such as Newton.<p>But in the past 100 years or so, the problem is there is so much known, it is impossible for any single person to have deep knowledge of everything.  e.g. its rare to find a really good mathematician who also has a deep knowledge (beyond intro courses) about say, chemistry.<p>Would a sufficiently powerful AI &#x2F; ML model be able to come up with this synthesis across fields?</div><br/></div></div><div id="42495407" class="c"><input type="checkbox" id="c-42495407" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#42493879">root</a><span>|</span><a href="#42494176">parent</a><span>|</span><a href="#42494790">prev</a><span>|</span><a href="#42494065">next</a><span>|</span><label class="collapse" for="c-42495407">[-]</label><label class="expand" for="c-42495407">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a strong reason. Yes, that means ChatGPT isn&#x27;t good at wholly independently pushing knowledge forward, but a good brainstormer that is right even 10% of the time is an incredible found of knowledge.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42494065" class="c"><input type="checkbox" id="c-42494065" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#42493879">prev</a><span>|</span><a href="#42494891">next</a><span>|</span><label class="collapse" for="c-42494065">[-]</label><label class="expand" for="c-42494065">[7 more]</label></div><br/><div class="children"><div class="content">&gt; <i>As an academic mathematician who spent their entire life collaborating openly on research problems and sharing my ideas with other people, it frustrates me</i> [that] <i>I am not even to give you a coherent description of some basic facts about this dataset, for example, its size. However there is a good reason for the secrecy. Language models train on large databases of knowledge, so you moment you make a database of maths questions public, the language models will train on it.</i><p>Well, yes and no. This is only true because we are talking about closed models from closed companies like so-called &quot;OpenAI&quot;.<p>But if all models were truly open, then we could simply verify what they had been trained on, and make experiments with models that we could be sure had never seen the dataset.<p>Decades ago Microsoft (in the words of Ballmer and Gates) famously accused open source of being a &quot;cancer&quot; because of the cascading nature of the GPL.<p>But it&#x27;s the opposite. In software, and in knowledge in general, the true disease is secrecy.</div><br/><div id="42494383" class="c"><input type="checkbox" id="c-42494383" checked=""/><div class="controls bullet"><span class="by">ludwik</span><span>|</span><a href="#42494065">parent</a><span>|</span><a href="#42494891">next</a><span>|</span><label class="collapse" for="c-42494383">[-]</label><label class="expand" for="c-42494383">[6 more]</label></div><br/><div class="children"><div class="content">&gt; But if all models were truly open, then we could simply verify what they had been trained on<p>How do you verify what a particular open model was trained on if you haven’t trained it yourself? Typically, for open models, you only get the architecture and the trained weights. How can you reliably verify what the model was trained on from this?<p>Even if they provide the training set (which is not typically the case), you still have to take their word for it—that’s not really &quot;verification.&quot;</div><br/><div id="42499298" class="c"><input type="checkbox" id="c-42499298" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42494065">root</a><span>|</span><a href="#42494383">parent</a><span>|</span><a href="#42497637">next</a><span>|</span><label class="collapse" for="c-42499298">[-]</label><label class="expand" for="c-42499298">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Even if they provide the training set (which is not typically the case), you still have to take their word for it—that’s not really &quot;verification.&quot;<p>If they&#x27;ve done it right, you can re-run the training and get the same weights. And maybe you could spot-check parts of it without running the full training (e.g. if there are glitch tokens in the weights, you&#x27;d look for where they came from in the training data, and if they weren&#x27;t there at all that would be a red flag). Is it possible to release the wrong training set (or the wrong instructions) and hope you don&#x27;t get caught? Sure, but demanding that it be published and available to check raises the bar and makes it much more risky to cheat.</div><br/></div></div><div id="42497637" class="c"><input type="checkbox" id="c-42497637" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#42494065">root</a><span>|</span><a href="#42494383">parent</a><span>|</span><a href="#42499298">prev</a><span>|</span><a href="#42494442">next</a><span>|</span><label class="collapse" for="c-42497637">[-]</label><label class="expand" for="c-42497637">[1 more]</label></div><br/><div class="children"><div class="content">If they provide the training set it&#x27;s reproducible and therefore verifiable.<p>If not, it&#x27;s not really &quot;open&quot;, it&#x27;s bs-open.</div><br/></div></div><div id="42494442" class="c"><input type="checkbox" id="c-42494442" checked=""/><div class="controls bullet"><span class="by">asadotzler</span><span>|</span><a href="#42494065">root</a><span>|</span><a href="#42494383">parent</a><span>|</span><a href="#42497637">prev</a><span>|</span><a href="#42494891">next</a><span>|</span><label class="collapse" for="c-42494442">[-]</label><label class="expand" for="c-42494442">[3 more]</label></div><br/><div class="children"><div class="content">The OP said &quot;truly open&quot;  not &quot;open model&quot; or any of the other BS out there. If you are truly open you share the training corpora as well or at least a comprehensive description of what it is and where to get it.</div><br/><div id="42494735" class="c"><input type="checkbox" id="c-42494735" checked=""/><div class="controls bullet"><span class="by">ludwik</span><span>|</span><a href="#42494065">root</a><span>|</span><a href="#42494442">parent</a><span>|</span><a href="#42494891">next</a><span>|</span><label class="collapse" for="c-42494735">[-]</label><label class="expand" for="c-42494735">[2 more]</label></div><br/><div class="children"><div class="content">It seems like you skipped the second paragraph of my comment?</div><br/><div id="42498708" class="c"><input type="checkbox" id="c-42498708" checked=""/><div class="controls bullet"><span class="by">SpaceManNabs</span><span>|</span><a href="#42494065">root</a><span>|</span><a href="#42494735">parent</a><span>|</span><a href="#42494891">next</a><span>|</span><label class="collapse" for="c-42498708">[-]</label><label class="expand" for="c-42498708">[1 more]</label></div><br/><div class="children"><div class="content">Because it is mostly hogwash.<p>Lots of ai researchers have shown that you can both give credit and discredit &quot;open models&quot; when you are given a dataset and training steps.<p>Many lauded papers fell into reddit Ml or twitter ire when people couldnt reproduce the model or results.<p>If you are given the training set, the weights, the steps required, and enough compute, you can do it.<p>Having enough compute and people releasing the steps is the main impediment.<p>For my research I always release all of my code, and the order of execution steps, and of course the training set. I also give confidence intervals based on my runs so people can reproduce and see if we get similar intervals.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42494891" class="c"><input type="checkbox" id="c-42494891" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#42494065">prev</a><span>|</span><a href="#42497249">next</a><span>|</span><label class="collapse" for="c-42494891">[-]</label><label class="expand" for="c-42494891">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I am dreading the inevitable onslaught in a year or two of language model “proofs” of the Riemann hypothesis which will just contain claims which are vague or inaccurate in the middle of 10 pages of correct mathematics which the human will have to wade through to find the line which doesn’t hold up.<p>I wonder what the response of working mathematicians will be to this. If the proofs look credible it might be too tempting to try and validate them, but if there’s a deluge that could be a hug time sync. Imagine if Wiles or Perelman had produced a thousand different proofs for their respective problems.</div><br/><div id="42494942" class="c"><input type="checkbox" id="c-42494942" checked=""/><div class="controls bullet"><span class="by">bqmjjx0kac</span><span>|</span><a href="#42494891">parent</a><span>|</span><a href="#42495768">next</a><span>|</span><label class="collapse" for="c-42494942">[-]</label><label class="expand" for="c-42494942">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the coming onslaught of AI slop &quot;proofs&quot; will give a little bump to proof assistants like Coq. Of course, it would still take a human mathematician some time to verify theorem definitions.</div><br/></div></div><div id="42495768" class="c"><input type="checkbox" id="c-42495768" checked=""/><div class="controls bullet"><span class="by">Hizonner</span><span>|</span><a href="#42494891">parent</a><span>|</span><a href="#42494942">prev</a><span>|</span><a href="#42498495">next</a><span>|</span><label class="collapse" for="c-42495768">[-]</label><label class="expand" for="c-42495768">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t waste time on looking at it unless a formal proof checker can verify it.</div><br/></div></div><div id="42498495" class="c"><input type="checkbox" id="c-42498495" checked=""/><div class="controls bullet"><span class="by">kevinventullo</span><span>|</span><a href="#42494891">parent</a><span>|</span><a href="#42495768">prev</a><span>|</span><a href="#42497249">next</a><span>|</span><label class="collapse" for="c-42498495">[-]</label><label class="expand" for="c-42498495">[1 more]</label></div><br/><div class="children"><div class="content">Honestly I think it won’t be that different from today, where there is no shortage of cranks producing “proofs” of the Riemann Hypothesis and submitting them to prestigious journals.</div><br/></div></div></div></div><div id="42497249" class="c"><input type="checkbox" id="c-42497249" checked=""/><div class="controls bullet"><span class="by">charlieyu1</span><span>|</span><a href="#42494891">prev</a><span>|</span><a href="#42493783">next</a><span>|</span><label class="collapse" for="c-42497249">[-]</label><label class="expand" for="c-42497249">[1 more]</label></div><br/><div class="children"><div class="content">One thing I know is that there wouldn’t be machines entering IMO 2025. The concept of “marker” does not exist in IMO - scores are decided by negotiations between team leaders of each country and the juries. It is important to get each team leader involved for grading the work of students for their country, for accountability as well as acknowledging cultural differences. And the hundreds of people are not going to stay longer to grade AI work.</div><br/></div></div><div id="42493783" class="c"><input type="checkbox" id="c-42493783" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42497249">prev</a><span>|</span><a href="#42494059">next</a><span>|</span><label class="collapse" for="c-42493783">[-]</label><label class="expand" for="c-42493783">[10 more]</label></div><br/><div class="children"><div class="content">I am fairly optimistic about LLMs as a human math -&gt; theorem-prover translator, and as a fan of Idris I am glad that the AI community is investing in Lean. As the author shows, the answer to &quot;Can AI be useful for automated mathematical work?&quot; is clearly &quot;yes.&quot;<p>But I am confident the answer to the question in the headline is &quot;no, not for several decades.&quot; It&#x27;s not just the underwhelming benchmark results discussed in the post, or the general concern about hard undergraduate math using different skillsets than ordinary research math. IMO the deeper problem still seems to be a basic gap where LLMs can seemingly do formal math at the level of a smart graduate student but fail at quantitative&#x2F;geometric reasoning problems designed for fish. I suspect this holds for O3, based on one of the ARC problems it wasn&#x27;t able to solve: <a href="https:&#x2F;&#x2F;substackcdn.com&#x2F;image&#x2F;fetch&#x2F;f_auto,q_auto:good,fl_progressive:steep&#x2F;https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90759a16-3602-4aa8-ba8c-69f1d67c31f1_1147x638.webp" rel="nofollow">https:&#x2F;&#x2F;substackcdn.com&#x2F;image&#x2F;fetch&#x2F;f_auto,q_auto:good,fl_pr...</a> (via <a href="https:&#x2F;&#x2F;www.interconnects.ai&#x2F;p&#x2F;openais-o3-the-2024-finale-of-ai" rel="nofollow">https:&#x2F;&#x2F;www.interconnects.ai&#x2F;p&#x2F;openais-o3-the-2024-finale-of...</a>) ANNs are simply not able to form abstractions, they can only imitate them via enormous amounts of data and compute. I would say there has been <i>zero</i> progress on &quot;common sense&quot; math in computers since the invention of Lisp: we are still faking it with expert systems, even if LLM expert systems are easier to build at scale with raw data.<p>It is the same old problem
where an ANN can attain superhuman performance on level 1
of Breakout, but it has to be retrained for level 2. I am not convinced it makes sense to say AI can do math if AI doesn&#x27;t understand what &quot;four&quot; means with the same depth as a rat, even if it can solve sophisticated modular arithmetic problems. In human terms, does it make sense to say a straightedge-and-compass AI understands Euclidean geometry if it&#x27;s not capable of understanding the physical intuition behind Euclid&#x27;s axioms? It makes more sense to say it&#x27;s a brainless tool that helps with the tedium and drudgery of actually proving things in mathematics.</div><br/><div id="42494028" class="c"><input type="checkbox" id="c-42494028" checked=""/><div class="controls bullet"><span class="by">QuadmasterXLII</span><span>|</span><a href="#42493783">parent</a><span>|</span><a href="#42493814">next</a><span>|</span><label class="collapse" for="c-42494028">[-]</label><label class="expand" for="c-42494028">[3 more]</label></div><br/><div class="children"><div class="content">To give a sense if scale: It’s not that o3 failed to solve that red blue rectangle problem once: o3 spent thousands of gpu hours putting out text about that problem, creating by my math about a million pages of text, and did not find the answer anywhere in those pages. For other problems it did find the answer around the million page mark, as at the ~$3000 per problem spend setting the score was still slowly creeping up.</div><br/><div id="42494084" class="c"><input type="checkbox" id="c-42494084" checked=""/><div class="controls bullet"><span class="by">josh-sematic</span><span>|</span><a href="#42493783">root</a><span>|</span><a href="#42494028">parent</a><span>|</span><a href="#42493814">next</a><span>|</span><label class="collapse" for="c-42494084">[-]</label><label class="expand" for="c-42494084">[2 more]</label></div><br/><div class="children"><div class="content">If the trajectory of the past two years is any guide, things that can be done at great compute expense now will rapidly become possible for a fraction of the cost.</div><br/><div id="42494653" class="c"><input type="checkbox" id="c-42494653" checked=""/><div class="controls bullet"><span class="by">asadotzler</span><span>|</span><a href="#42493783">root</a><span>|</span><a href="#42494084">parent</a><span>|</span><a href="#42493814">next</a><span>|</span><label class="collapse" for="c-42494653">[-]</label><label class="expand" for="c-42494653">[1 more]</label></div><br/><div class="children"><div class="content">The trajectory is not a guide, unless you count the recent plateauing.</div><br/></div></div></div></div></div></div><div id="42493814" class="c"><input type="checkbox" id="c-42493814" checked=""/><div class="controls bullet"><span class="by">asddubs</span><span>|</span><a href="#42493783">parent</a><span>|</span><a href="#42494028">prev</a><span>|</span><a href="#42496466">next</a><span>|</span><label class="collapse" for="c-42493814">[-]</label><label class="expand" for="c-42493814">[5 more]</label></div><br/><div class="children"><div class="content">it can take my math and point out a step I missed and then show me the correct procedure but still get the wrong result because it can&#x27;t reliably multiply 2-digit numbers</div><br/><div id="42497968" class="c"><input type="checkbox" id="c-42497968" checked=""/><div class="controls bullet"><span class="by">watt</span><span>|</span><a href="#42493783">root</a><span>|</span><a href="#42493814">parent</a><span>|</span><a href="#42493856">next</a><span>|</span><label class="collapse" for="c-42497968">[-]</label><label class="expand" for="c-42497968">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s a &quot;language&quot; model (LLM), not a &quot;math&quot; model. when it is generating your answer, predicting and outputing a word after word it is _not_ multiplying your numbers internally.</div><br/><div id="42499366" class="c"><input type="checkbox" id="c-42499366" checked=""/><div class="controls bullet"><span class="by">asddubs</span><span>|</span><a href="#42493783">root</a><span>|</span><a href="#42497968">parent</a><span>|</span><a href="#42493856">next</a><span>|</span><label class="collapse" for="c-42499366">[-]</label><label class="expand" for="c-42499366">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I know. It&#x27;s just kind of interesting how it can make inferences about complicated things but not get multiplications correct that would almost definitely have been in its training set many times (two digit by two digit)</div><br/></div></div></div></div><div id="42493856" class="c"><input type="checkbox" id="c-42493856" checked=""/><div class="controls bullet"><span class="by">fifilura</span><span>|</span><a href="#42493783">root</a><span>|</span><a href="#42493814">parent</a><span>|</span><a href="#42497968">prev</a><span>|</span><a href="#42496466">next</a><span>|</span><label class="collapse" for="c-42493856">[-]</label><label class="expand" for="c-42493856">[2 more]</label></div><br/><div class="children"><div class="content">Better than an average human then.</div><br/><div id="42493918" class="c"><input type="checkbox" id="c-42493918" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#42493783">root</a><span>|</span><a href="#42493856">parent</a><span>|</span><a href="#42496466">next</a><span>|</span><label class="collapse" for="c-42493918">[-]</label><label class="expand" for="c-42493918">[1 more]</label></div><br/><div class="children"><div class="content">Different than an average human.</div><br/></div></div></div></div></div></div><div id="42496466" class="c"><input type="checkbox" id="c-42496466" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42493783">parent</a><span>|</span><a href="#42493814">prev</a><span>|</span><a href="#42494059">next</a><span>|</span><label class="collapse" for="c-42496466">[-]</label><label class="expand" for="c-42496466">[1 more]</label></div><br/><div class="children"><div class="content">Just a comment: the example o1 got wrong was actually underspecified: <a href="https:&#x2F;&#x2F;anokas.substack.com&#x2F;p&#x2F;o3-and-arc-agi-the-unsolved-tasks" rel="nofollow">https:&#x2F;&#x2F;anokas.substack.com&#x2F;p&#x2F;o3-and-arc-agi-the-unsolved-ta...</a><p>Which is actually a problem I have with ARC (and IQ tests more generally): it is computationally cheaper to go from ARC transformation rule -&gt; ARC problem than it is the other way around. But this means it’s pretty easy to generate ARC problems with non-unique solutions.</div><br/></div></div></div></div><div id="42494059" class="c"><input type="checkbox" id="c-42494059" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#42493783">prev</a><span>|</span><a href="#42493816">next</a><span>|</span><label class="collapse" for="c-42494059">[-]</label><label class="expand" for="c-42494059">[7 more]</label></div><br/><div class="children"><div class="content">So here&#x27;s what I&#x27;m perplexed about. There are statements in Presburger arithmetic that take time doubly exponential (or worse) in the size of the statement to reach via <i>any path</i> of the formal system whatsoever. These are arithmetic truths about the natural numbers. Can these statements be reached faster in ZFC? Possibly—it&#x27;s well-known that there exist shorter proofs of true statements in more powerful consistent systems.<p>But the problem then is that one can suppose there are also true short statements in ZFC which likewise require doubly exponential time to reach via any path. Presburger Arithmetic is decidable whereas ZFC is not, so these statements would require the additional axioms of ZFC for shorter proofs, but I think it&#x27;s safe to assume such statements exist.<p>Now let&#x27;s suppose an AI model can resolve the truth of these short statements quickly. That means one of three things:<p>1) The AI model can discover doubly exponential length proof paths within the framework of ZFC.<p>2) There are certain short statements in the formal language of ZFC that the AI model cannot discover the truth of.<p>3) The AI model operates outside of ZFC to find the truth of statements in the framework of some other, potentially unknown  formal system (and for arithmetical statements, the system must necessarily be sound).<p>How likely are each of these outcomes?<p>1) is not possible within any coherent, human-scale timeframe.<p>2) IMO is the most likely outcome, but then this means there are some <i>really</i> interesting things in mathematics that AI cannot discover. Perhaps the same set of things that humans find interesting. Once we have exhausted the theorems with short proofs in ZFC, there will still be an infinite number of short and interesting statements that we cannot resolve.<p>3) This would be the most bizarre outcome of all. If AI operates in a consistent way outside the framework of ZFC, then that would be equivalent to solving the halting problem for certain (infinite) sets of Turing machine configurations that ZFC cannot solve. That in itself itself isn&#x27;t too strange (e.g., it might turn out that ZFC lacks an axiom necessary to prove something as simple as the Collatz conjecture), but what would be strange is that it could find these new formal systems <i>efficiently</i>. In other words, it would have discovered an algorithmic way to procure new axioms that lead to efficient proofs of true arithmetic statements. One could also view that as an efficient algorithm for computing BB(n), which obviously we think isn&#x27;t possible. See Levin&#x27;s papers on the feasibility of extending PA in a way that leads to quickly discovering more of the halting sequence.</div><br/><div id="42499333" class="c"><input type="checkbox" id="c-42499333" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42494059">parent</a><span>|</span><a href="#42494769">next</a><span>|</span><label class="collapse" for="c-42499333">[-]</label><label class="expand" for="c-42499333">[1 more]</label></div><br/><div class="children"><div class="content">&gt; and for arithmetical statements, the system must necessarily be sound<p>Why do you say this? The AI doesn&#x27;t know or care about soundness. Probably it has mathematical intuition that makes unsound assumptions, like human mathematicians do.<p>&gt; How likely are each of these outcomes?<p>I think they&#x27;ll all be true to a certain extent, just as they are for human mathematicians. There will probably be certain classes of extremely long proofs that the AI has no trouble discovering (because they have some kind of structure, just not structure that can be expressed in ZFC), certain truths that the AI makes an intuitive leap to despite not being able to prove them in ZFC (just as human mathematicians do), and certain short statements that the AI cannot prove one way or another (like Goldbach or twin primes or what have you, again, just as human mathematicians can&#x27;t).</div><br/></div></div><div id="42494769" class="c"><input type="checkbox" id="c-42494769" checked=""/><div class="controls bullet"><span class="by">semolinapudding</span><span>|</span><a href="#42494059">parent</a><span>|</span><a href="#42499333">prev</a><span>|</span><a href="#42494597">next</a><span>|</span><label class="collapse" for="c-42494769">[-]</label><label class="expand" for="c-42494769">[1 more]</label></div><br/><div class="children"><div class="content">ZFC is way worse than Presburger arithmetic -- since it is undecidable, we know that the length of the minimal proof of a statement cannot be bounded by a computable function of the length of the statement.<p>This has little to do with the usefulness of LLMs for research-level mathematics though. I do not think that anyone is hoping to get a decision procedure out of it, but rather something that would imitate human reasoning, which is heavily based on analogies (&quot;we want to solve this problem, which shares some similarities with that other solved problem, can we apply the same proof strategy? if not, can we generalise the strategy so that it becomes applicable?&quot;).</div><br/></div></div><div id="42494597" class="c"><input type="checkbox" id="c-42494597" checked=""/><div class="controls bullet"><span class="by">wbl</span><span>|</span><a href="#42494059">parent</a><span>|</span><a href="#42494769">prev</a><span>|</span><a href="#42494174">next</a><span>|</span><label class="collapse" for="c-42494597">[-]</label><label class="expand" for="c-42494597">[2 more]</label></div><br/><div class="children"><div class="content">2 is definitely true. 3 is much more interesting and likely true but even saying it takes us into deep philosophical waters.<p>If every true theorem had a proof in a computationally bounded length the halting problem would be solvable. So the AI can&#x27;t find some of those proofs.<p>The reason I say 3 is deep is that ultimately our foundational reasons to assume ZFC+the bits we need for logic come from philosohical groundings and not everyone accepts the same ones. Ultrafinitists and large cardinal theorists are both kinds of people I&#x27;ve met.</div><br/><div id="42497244" class="c"><input type="checkbox" id="c-42497244" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#42494059">root</a><span>|</span><a href="#42494597">parent</a><span>|</span><a href="#42494174">next</a><span>|</span><label class="collapse" for="c-42497244">[-]</label><label class="expand" for="c-42497244">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that no model-dependent theorem of ZFC or its extensions (e.g., ZFC+CH, ZFC+¬CH) provides any insight into the behavior of Turing machines. If our goal is to invent an algorithm that finds better algorithms, then the philosophical angle is irrelevant. For computational purposes, we would only care about new axioms independent of ZFC if they allow us to prove additional Turing machine configurations as non-halting.</div><br/></div></div></div></div><div id="42494174" class="c"><input type="checkbox" id="c-42494174" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42494059">parent</a><span>|</span><a href="#42494597">prev</a><span>|</span><a href="#42493816">next</a><span>|</span><label class="collapse" for="c-42494174">[-]</label><label class="expand" for="c-42494174">[2 more]</label></div><br/><div class="children"><div class="content">&gt; There are statements in Presburger arithmetic that take time doubly exponential (or worse) in the size of the statement to reach via any path of the formal system whatsoever.<p>This is a correct statement about the <i>worst</i> case runtime. What is interesting for practical applications is whether such statements are among those that you are practically interested in.</div><br/><div id="42494262" class="c"><input type="checkbox" id="c-42494262" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#42494059">root</a><span>|</span><a href="#42494174">parent</a><span>|</span><a href="#42493816">next</a><span>|</span><label class="collapse" for="c-42494262">[-]</label><label class="expand" for="c-42494262">[1 more]</label></div><br/><div class="children"><div class="content">I would certainly think so. The statements mathematicians seem to be interested in tend to be at a &quot;higher level&quot; than simple but true statements like 2+3=5. And they necessarily have a short description in the formal language of ZFC, otherwise we couldn&#x27;t write them down (e.g., Fermat&#x27;s last theorem).<p>If the truth of these higher level statements instantly unlocks many other truths, then it makes sense to think of them in the same way that knowing BB(5) allows one to instantly classify any Turing machine configuration on the computation graph of all n ≤ 5 state Turing machines (on empty tape input) as halting&#x2F;non-halting.</div><br/></div></div></div></div></div></div><div id="42493816" class="c"><input type="checkbox" id="c-42493816" checked=""/><div class="controls bullet"><span class="by">ned99</span><span>|</span><a href="#42494059">prev</a><span>|</span><a href="#42494438">next</a><span>|</span><label class="collapse" for="c-42493816">[-]</label><label class="expand" for="c-42493816">[2 more]</label></div><br/><div class="children"><div class="content">I think this is a silly question, you could track AI&#x27;s doing very simple maths back in 1960 - 1970&#x27;s</div><br/><div id="42494077" class="c"><input type="checkbox" id="c-42494077" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42493816">parent</a><span>|</span><a href="#42494438">next</a><span>|</span><label class="collapse" for="c-42494077">[-]</label><label class="expand" for="c-42494077">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just the worrisome linguistic confusion between AI and LLMs.</div><br/></div></div></div></div><div id="42494438" class="c"><input type="checkbox" id="c-42494438" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#42493816">prev</a><span>|</span><a href="#42500103">next</a><span>|</span><label class="collapse" for="c-42494438">[-]</label><label class="expand" for="c-42494438">[8 more]</label></div><br/><div class="children"><div class="content">Every profession seems to have a pessimistic view of AI as soon as it starts to make progress in their domain.   Denial, Anger, Bargaining, Depression, and Acceptance.  Artists seem to be in the depression state, many programmers are still in the denial phase.  Pretty solid denial here from a mathematician.  o3 was a proof of concept, like every other domain AI enters, it&#x27;s going to keep getting better.<p>Society is CLEARLY not ready for what AI&#x27;s impact is going to be.  We&#x27;ve been through change before, but never at this scale and speed.  I think Musk&#x2F;Vivek&#x27;s DOGE thing is important, our governent has gotten quite large and bureaucratic.  But the clock has started on AI, and this is a social structural issue we&#x27;ve gotta figure out.  Putting it off means we probably become subjects to a default set of rulers if not the shoggoth itself.</div><br/><div id="42494468" class="c"><input type="checkbox" id="c-42494468" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#42494438">parent</a><span>|</span><a href="#42494604">next</a><span>|</span><label class="collapse" for="c-42494468">[-]</label><label class="expand" for="c-42494468">[2 more]</label></div><br/><div class="children"><div class="content">Or is it just white collar workers experiencing what blue collar workers have been experiencing for decades?</div><br/><div id="42494482" class="c"><input type="checkbox" id="c-42494482" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#42494438">root</a><span>|</span><a href="#42494468">parent</a><span>|</span><a href="#42494604">next</a><span>|</span><label class="collapse" for="c-42494482">[-]</label><label class="expand" for="c-42494482">[1 more]</label></div><br/><div class="children"><div class="content">So will that make society shift to the left in demand stronger of safety nets, or to the right in search of a strongman to rescue them?</div><br/></div></div></div></div><div id="42494604" class="c"><input type="checkbox" id="c-42494604" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#42494438">parent</a><span>|</span><a href="#42494468">prev</a><span>|</span><a href="#42494462">next</a><span>|</span><label class="collapse" for="c-42494604">[-]</label><label class="expand" for="c-42494604">[4 more]</label></div><br/><div class="children"><div class="content">The reason why this is so disruptive is because it will effect hundreds of fields simultaneously.<p>Previously workers in a field disrupted by automation would retrain to a different part of the economy.<p>If AI pans out to the point that there are mass layoffs in hundreds of sectors of the economy at once, then i’m not sure the process we have haphazardly set up now will work. People will have no idea where to go beyond manual labor. (But this will be difficult due to the obesity crisis - but maybe it will save lives in a weird way).</div><br/><div id="42499884" class="c"><input type="checkbox" id="c-42499884" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#42494438">root</a><span>|</span><a href="#42494604">parent</a><span>|</span><a href="#42494654">next</a><span>|</span><label class="collapse" for="c-42499884">[-]</label><label class="expand" for="c-42499884">[1 more]</label></div><br/><div class="children"><div class="content">Well it hasn’t happened yet at least (unemployment is near historic lows). How much better does AI need to get? And do we actually expect it to happen? Improving on random benchmarks is not necessarily evidence of being able to do a specific job.</div><br/></div></div><div id="42494654" class="c"><input type="checkbox" id="c-42494654" checked=""/><div class="controls bullet"><span class="by">hash872</span><span>|</span><a href="#42494438">root</a><span>|</span><a href="#42494604">parent</a><span>|</span><a href="#42499884">prev</a><span>|</span><a href="#42494462">next</a><span>|</span><label class="collapse" for="c-42494654">[-]</label><label class="expand" for="c-42494654">[2 more]</label></div><br/><div class="children"><div class="content">If there are &#x27;mass layoffs in hundreds of sectors of the economy at once&#x27;, then the economy immediately goes into Great Depression 2.0 or worse. Consumer spending is two-thirds of the US economy, when everyone loses their jobs and stops having disposable income that&#x27;s literally what a depression is</div><br/><div id="42495589" class="c"><input type="checkbox" id="c-42495589" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#42494438">root</a><span>|</span><a href="#42494654">parent</a><span>|</span><a href="#42494462">next</a><span>|</span><label class="collapse" for="c-42495589">[-]</label><label class="expand" for="c-42495589">[1 more]</label></div><br/><div class="children"><div class="content">This will create a prisoner’s dilemma for corporations then, the government will have to step in to provide incentives for insanely profitable corporations to keep the proper number of people employed or limit the rate of layoffs.</div><br/></div></div></div></div></div></div><div id="42494462" class="c"><input type="checkbox" id="c-42494462" checked=""/><div class="controls bullet"><span class="by">haolez</span><span>|</span><a href="#42494438">parent</a><span>|</span><a href="#42494604">prev</a><span>|</span><a href="#42500103">next</a><span>|</span><label class="collapse" for="c-42494462">[-]</label><label class="expand" for="c-42494462">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s a little of both. Maybe generative AI algorithms won&#x27;t overcome their initial limitations. But maybe we don&#x27;t need to overcome them to transform society in a very significant way.</div><br/></div></div></div></div><div id="42500103" class="c"><input type="checkbox" id="c-42500103" checked=""/><div class="controls bullet"><span class="by">Onavo</span><span>|</span><a href="#42494438">prev</a><span>|</span><a href="#42498188">next</a><span>|</span><label class="collapse" for="c-42500103">[-]</label><label class="expand" for="c-42500103">[1 more]</label></div><br/><div class="children"><div class="content">Considering that they have Terence Tao himself working on the problem, betting against it would be unwise.</div><br/></div></div><div id="42498188" class="c"><input type="checkbox" id="c-42498188" checked=""/><div class="controls bullet"><span class="by">witnesser2</span><span>|</span><a href="#42500103">prev</a><span>|</span><a href="#42495435">next</a><span>|</span><label class="collapse" for="c-42498188">[-]</label><label class="expand" for="c-42498188">[2 more]</label></div><br/><div class="children"><div class="content">I was not refuted sufficiently a couple of years ago.
I claimed &quot;training is open boundary&quot; etc.</div><br/><div id="42498208" class="c"><input type="checkbox" id="c-42498208" checked=""/><div class="controls bullet"><span class="by">witnesser2</span><span>|</span><a href="#42498188">parent</a><span>|</span><a href="#42495435">next</a><span>|</span><label class="collapse" for="c-42498208">[-]</label><label class="expand" for="c-42498208">[1 more]</label></div><br/><div class="children"><div class="content">Like as a few years ago, I just boringly add again &quot;you need modeling&quot; to close it.</div><br/></div></div></div></div><div id="42495435" class="c"><input type="checkbox" id="c-42495435" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42498188">prev</a><span>|</span><a href="#42493958">next</a><span>|</span><label class="collapse" for="c-42495435">[-]</label><label class="expand" for="c-42495435">[2 more]</label></div><br/><div class="children"><div class="content">My favourite moments of being a graduate student in math was showing my friends (and sometimes professors) proofs of propositions and theorems that we discussed together. To be the first to put together a coherent piece of reasoning that would convince them of the truth was immensely exciting. Those were great bonding moments amongst colleagues. The very fact that we needed each other to figure out the basics of the subject was part of what made the journey so great.<p>Now, all of that will be done by AI.<p>Reminds of the time when I finally enabled invincibility in Goldeneye 007. Rather boring.<p>I think we&#x27;ve stopped to appreciate the human struggle and experience and have placed all the value on the end product, and that&#x27;s we&#x27;re developing AI so much.<p>Yeah, there is the possibility of working with an AI but at that point, what is the point? Seems rather pointless to me in an art like mathematics.</div><br/><div id="42499450" class="c"><input type="checkbox" id="c-42499450" checked=""/><div class="controls bullet"><span class="by">sourcepluck</span><span>|</span><a href="#42495435">parent</a><span>|</span><a href="#42493958">next</a><span>|</span><label class="collapse" for="c-42499450">[-]</label><label class="expand" for="c-42499450">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Now, all of that will be done by AI.<p>No &quot;AI&quot; of any description is doing novel proofs at the moment. Not o3, or anything else.<p>LLMs are good for chatting about basic intuition with, up to and including complex subjects, if and only if there are publically available data on the topic which have been fed to the LLM during its training. They&#x27;re good at doing summaries and overviews of specific things (if you push them around and insist they don&#x27;t waffle and ignore garbage carefully and keep your critical thinking hat on, etc etc).<p>It&#x27;s like having a magnifying glass that focuses in on the small little maths question you might have, without you having to sift through ten blogs or videos or whatever.<p>That&#x27;s hardly going to replace graduate students doing proofs with professors, though, at least not with the methods being employed thus far!</div><br/></div></div></div></div><div id="42493958" class="c"><input type="checkbox" id="c-42493958" checked=""/><div class="controls bullet"><span class="by">rishicomplex</span><span>|</span><a href="#42495435">prev</a><span>|</span><a href="#42493813">next</a><span>|</span><label class="collapse" for="c-42493958">[-]</label><label class="expand" for="c-42493958">[2 more]</label></div><br/><div class="children"><div class="content">Who is the author?</div><br/><div id="42494041" class="c"><input type="checkbox" id="c-42494041" checked=""/><div class="controls bullet"><span class="by">williamstein</span><span>|</span><a href="#42493958">parent</a><span>|</span><a href="#42493813">next</a><span>|</span><label class="collapse" for="c-42494041">[-]</label><label class="expand" for="c-42494041">[1 more]</label></div><br/><div class="children"><div class="content">Kevin Buzzard</div><br/></div></div></div></div><div id="42493813" class="c"><input type="checkbox" id="c-42493813" checked=""/><div class="controls bullet"><span class="by">est</span><span>|</span><a href="#42493958">prev</a><span>|</span><a href="#42494166">next</a><span>|</span><label class="collapse" for="c-42493813">[-]</label><label class="expand" for="c-42493813">[10 more]</label></div><br/><div class="children"><div class="content">At this stage I assume everything having a sequencial pattern can and will be automated by LLM AIs.</div><br/><div id="42493936" class="c"><input type="checkbox" id="c-42493936" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#42493813">parent</a><span>|</span><a href="#42493955">next</a><span>|</span><label class="collapse" for="c-42493936">[-]</label><label class="expand" for="c-42493936">[8 more]</label></div><br/><div class="children"><div class="content">I think that’s provably incorrect for the current approach to LLMs. They all have a horizon over which they correlate tokens in the input stream.<p>So, for any LLM, if you intersperse more than that number of ‘X’ tokens between each useful token, they won’t be able to do anything resembling intelligence.<p>The current LLMs are a bit like n-gram databases that do not use letters, but larger units.</div><br/><div id="42494533" class="c"><input type="checkbox" id="c-42494533" checked=""/><div class="controls bullet"><span class="by">beng-nl</span><span>|</span><a href="#42493813">root</a><span>|</span><a href="#42493936">parent</a><span>|</span><a href="#42494476">next</a><span>|</span><label class="collapse" for="c-42494533">[-]</label><label class="expand" for="c-42494533">[5 more]</label></div><br/><div class="children"><div class="content">It’s that a bit of an unfair sabotage?<p>Naturally, humans couldn’t do it, even though they could edit the input to remove the X’s, but shouldn’t we evaluate the ability (even intelligent ability) of LLM’s on what they can generally do rather than amplify their weakness?</div><br/><div id="42497193" class="c"><input type="checkbox" id="c-42497193" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#42493813">root</a><span>|</span><a href="#42494533">parent</a><span>|</span><a href="#42494476">next</a><span>|</span><label class="collapse" for="c-42497193">[-]</label><label class="expand" for="c-42497193">[4 more]</label></div><br/><div class="children"><div class="content">Why is that unfair in reply to the claim <i>“At this stage I assume everything having a sequencial pattern can and will be automated by LLM AIs.”</i>?<p>I am not claiming LLMs aren’t or cannot be intelligent, not even that they cannot do magical things; I just rebuked a statement about the lack of limits of LLMs.<p>&gt; Naturally, humans couldn’t do it, even though they could edit the input to remove the X’s<p>So, what are you claiming: that they cannot or that they can? I think most people can and many would. Confronted with a file containing millions of X’s, many humans will wonder whether there’s something else than X’s in the file, do a ‘replace all’, discover the question hidden in that sea of X’s, and answer it.<p>There even are simple files where most humans would easily spot things without having to think of removing those X&#x27;s. Consider a file<p><pre><code>   How         X X X X X X
   many        X X X X X X
   days        X X X X X X
   are         X X X X X X
   there       X X X X X X
   in          X X X X X X
   a           X X X X X X
   week?       X X X X X X
</code></pre>
with a million X’s on the end of each line. Spotting the question in that is easy for humans, but impossible for the current bunch of LLMs</div><br/><div id="42498180" class="c"><input type="checkbox" id="c-42498180" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42493813">root</a><span>|</span><a href="#42497193">parent</a><span>|</span><a href="#42497687">next</a><span>|</span><label class="collapse" for="c-42498180">[-]</label><label class="expand" for="c-42498180">[2 more]</label></div><br/><div class="children"><div class="content">If you have a million Xs on the end of each line, when a human is looking at that file, he&#x27;s not looking at the entirety of it, but only at the part that is actually visible on-screen, so the equivalent task for an LLM would be to feed it the same subset as input. In which case they can all answer this question just fine.</div><br/></div></div></div></div></div></div><div id="42494476" class="c"><input type="checkbox" id="c-42494476" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#42493813">root</a><span>|</span><a href="#42493936">parent</a><span>|</span><a href="#42494533">prev</a><span>|</span><a href="#42500053">next</a><span>|</span><label class="collapse" for="c-42494476">[-]</label><label class="expand" for="c-42494476">[1 more]</label></div><br/><div class="children"><div class="content">The follow-up question is &quot;Does it require a paradigm shift to solve it?&quot;. And the answer could be &quot;No&quot;. Episodic memory, hierarchical learnable tokenization, online learning or whatever works well on GPUs.</div><br/></div></div></div></div><div id="42493955" class="c"><input type="checkbox" id="c-42493955" checked=""/><div class="controls bullet"><span class="by">palata</span><span>|</span><a href="#42493813">parent</a><span>|</span><a href="#42493936">prev</a><span>|</span><a href="#42494166">next</a><span>|</span><label class="collapse" for="c-42493955">[-]</label><label class="expand" for="c-42493955">[1 more]</label></div><br/><div class="children"><div class="content">At this stage I <i>hope</i> everything that needs to be reliable won&#x27;t be automated by LLM AIs.</div><br/></div></div></div></div><div id="42494166" class="c"><input type="checkbox" id="c-42494166" checked=""/><div class="controls bullet"><span class="by">4ad</span><span>|</span><a href="#42493813">prev</a><span>|</span><a href="#42496178">next</a><span>|</span><label class="collapse" for="c-42494166">[-]</label><label class="expand" for="c-42494166">[4 more]</label></div><br/><div class="children"><div class="content">&gt; FrontierMath is a secret dataset of “hundreds” of hard maths questions, curated by Epoch AI, and announced last month.<p>The database stopped being secret when it was fed to proprietary LLMs running in the cloud. If anyone is not thinking that OpenAI has trained and tuned O3 on the &quot;secret&quot; problems people fed to GPT-4o, I have a bridge to sell you.</div><br/><div id="42494284" class="c"><input type="checkbox" id="c-42494284" checked=""/><div class="controls bullet"><span class="by">fn-mote</span><span>|</span><a href="#42494166">parent</a><span>|</span><a href="#42496178">next</a><span>|</span><label class="collapse" for="c-42494284">[-]</label><label class="expand" for="c-42494284">[3 more]</label></div><br/><div class="children"><div class="content">This level of conspiracy thinking requires evidence to be useful.<p>Edit: I do see from your profile that you are a real person though, so I say this with more respect.</div><br/><div id="42495623" class="c"><input type="checkbox" id="c-42495623" checked=""/><div class="controls bullet"><span class="by">dns_snek</span><span>|</span><a href="#42494166">root</a><span>|</span><a href="#42494284">parent</a><span>|</span><a href="#42496178">next</a><span>|</span><label class="collapse" for="c-42495623">[-]</label><label class="expand" for="c-42495623">[2 more]</label></div><br/><div class="children"><div class="content">What evidence do we need that AI companies are exploiting every bit of information they can use to get ahead in the benchmarks to generate more hype? Ignoring terms&#x2F;agreements, violating copyright, and otherwise exploiting information for personal gain is the foundation of that entire industry for crying out loud.</div><br/><div id="42499429" class="c"><input type="checkbox" id="c-42499429" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42494166">root</a><span>|</span><a href="#42495623">parent</a><span>|</span><a href="#42496178">next</a><span>|</span><label class="collapse" for="c-42499429">[-]</label><label class="expand" for="c-42499429">[1 more]</label></div><br/><div class="children"><div class="content">Some people are also forgetting who is the CEO of OpenAI.<p>Sam Altman has long talked about believing in the &quot;move fast and break things&quot; way of doing business. Which is just a nicer way of saying do whatever dodgy things you can get away with.</div><br/></div></div></div></div></div></div></div></div><div id="42496178" class="c"><input type="checkbox" id="c-42496178" checked=""/><div class="controls bullet"><span class="by">0points</span><span>|</span><a href="#42494166">prev</a><span>|</span><a href="#42498272">next</a><span>|</span><label class="collapse" for="c-42496178">[-]</label><label class="expand" for="c-42496178">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How much longer this will go on for nobody knows, but there are lots of people pouring lots of money into this game so it would be a fool who bets on progress slowing down any time soon.<p>Money cannot solve the issues faced by the industry which mainly revolves around lack of training data.<p>They already used the entirety of the internet, all available video, audio and books and they are now dealing with the fact that most content online is now generated by these models, thus making it useless as training data.</div><br/></div></div><div id="42498272" class="c"><input type="checkbox" id="c-42498272" checked=""/><div class="controls bullet"><span class="by">mangomountain</span><span>|</span><a href="#42496178">prev</a><span>|</span><a href="#42493946">next</a><span>|</span><label class="collapse" for="c-42498272">[-]</label><label class="expand" for="c-42498272">[1 more]</label></div><br/><div class="children"><div class="content">In other news we’ve discovered life (our bacteria) on mars
Just joking</div><br/></div></div><div id="42493946" class="c"><input type="checkbox" id="c-42493946" checked=""/><div class="controls bullet"><span class="by">sylware</span><span>|</span><a href="#42498272">prev</a><span>|</span><a href="#42494335">next</a><span>|</span><label class="collapse" for="c-42493946">[-]</label><label class="expand" for="c-42493946">[1 more]</label></div><br/><div class="children"><div class="content">How to train an AI strapped to a formal solver.</div><br/></div></div><div id="42494335" class="c"><input type="checkbox" id="c-42494335" checked=""/><div class="controls bullet"><span class="by">retrocryptid</span><span>|</span><a href="#42493946">prev</a><span>|</span><a href="#42494319">next</a><span>|</span><label class="collapse" for="c-42494335">[-]</label><label class="expand" for="c-42494335">[1 more]</label></div><br/><div class="children"><div class="content">When did we decide that AI == LLM?  Oh don&#x27;t answer.  I know, The VC world noticed CNNs and LLMs about 10 years ago and it&#x27;s the only thing anyone&#x27;s talked about ever since.<p>Seems to me the answer to &#x27;Can AI do maths yet?&#x27; depends on what you call AI and what you call maths.  Our old departmental VAX running at a handfull of megahertz could do some very clever symbol manipulation on binomials and if you gave it a few seconds, it could even do something like theorum proving via proto-prolog.  Neither are anywhere close to the glorious GAI future we hope to sell to industry and government, but it seems worth considering how they&#x27;re different, why they worked, and whether there&#x27;s room for some hybrid approach.  Do LLMs need to know how to do math if they know how to write Prolog or  Coc statements that can do interesting things?<p>I&#x27;ve heard people say they want to build software that emulates (simulates?) how humans do arithmetic, but ask a human to add anything bigger than two digit numbers and the first thing they do is reach for a calculator.</div><br/></div></div><div id="42494319" class="c"><input type="checkbox" id="c-42494319" checked=""/><div class="controls bullet"><span class="by">sincerecook</span><span>|</span><a href="#42494335">prev</a><span>|</span><a href="#42493875">next</a><span>|</span><label class="collapse" for="c-42494319">[-]</label><label class="expand" for="c-42494319">[3 more]</label></div><br/><div class="children"><div class="content">No it can&#x27;t, and there&#x27;s no such thing as AI. How is a thing that predicts the next-most-likely word going to do novel math? It can&#x27;t even do existing math reliably because logical operations and statistical approximation are fundamentally different. It is fun watching grifters put lipstick on this thing and shop it around as a magic pig though.</div><br/><div id="42498779" class="c"><input type="checkbox" id="c-42498779" checked=""/><div class="controls bullet"><span class="by">bwfan123</span><span>|</span><a href="#42494319">parent</a><span>|</span><a href="#42493875">next</a><span>|</span><label class="collapse" for="c-42498779">[-]</label><label class="expand" for="c-42498779">[2 more]</label></div><br/><div class="children"><div class="content">openai and epochai (frontier math) are startups with a strong incentive to push such narratives. the real test will be in actual adoption in real world use cases.<p>the management class has a strong incentive to believe in this narrative, since it helps them reduce labor cost. so they are investing in it.<p>eventually, the emperor will be seen to have no clothes at least in some usecases for which it is being peddled right now.</div><br/><div id="42500442" class="c"><input type="checkbox" id="c-42500442" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#42494319">root</a><span>|</span><a href="#42498779">parent</a><span>|</span><a href="#42493875">next</a><span>|</span><label class="collapse" for="c-42500442">[-]</label><label class="expand" for="c-42500442">[1 more]</label></div><br/><div class="children"><div class="content">Epoch is a non-profit research institute, not a startup.</div><br/></div></div></div></div></div></div><div id="42493875" class="c"><input type="checkbox" id="c-42493875" checked=""/><div class="controls bullet"><span class="by">lproven</span><span>|</span><a href="#42494319">prev</a><span>|</span><a href="#42500093">next</a><span>|</span><label class="collapse" for="c-42493875">[-]</label><label class="expand" for="c-42493875">[1 more]</label></div><br/><div class="children"><div class="content">Betteridge&#x27;s Law applies.</div><br/></div></div></div></div></div></div></div></body></html>