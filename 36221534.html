<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686128463708" as="style"/><link rel="stylesheet" href="styles.css?v=1686128463708"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/Rikorose/DeepFilterNet">DeepFilterNet: Noise supression using deep filtering</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>nitinreddy88</span> | <span>19 comments</span></div><br/><div><div id="36224484" class="c"><input type="checkbox" id="c-36224484" checked=""/><div class="controls bullet"><span class="by">ZoomZoomZoom</span><span>|</span><a href="#36223049">next</a><span>|</span><label class="collapse" for="c-36224484">[-]</label><label class="expand" for="c-36224484">[1 more]</label></div><br/><div class="children"><div class="content">The demo with the vac is certainly not a success.<p>I sometimes wonder if all those filters optimise for a wrong thing. Removing noise is meaningless, unless the overall discernability improves. If you remove noise with the price of the voice becoming choppy, &quot;robotic&quot; and unnatural, you didn&#x27;t improve the situation, and in some cases you can say only made it worse.<p>What even further deteriorates legibility for most noise suppression filters is the discrepancy between the completely dry pauses and the remaining ambiance &quot;under&quot; the voice. It would be much more interesting to see some style transfer for voice ambience as an alternative to current de-verbs.<p>When dealing with voice processing I advocate for restraining from noise suppression filters for as long as possible, and I haven&#x27;t seen a publicly available noise suppression filter which could change my position yet.</div><br/></div></div><div id="36223049" class="c"><input type="checkbox" id="c-36223049" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#36224484">prev</a><span>|</span><a href="#36224452">next</a><span>|</span><label class="collapse" for="c-36223049">[-]</label><label class="expand" for="c-36223049">[1 more]</label></div><br/><div class="children"><div class="content">Frankly, what I hear is very similar to the results of classic spectral denoising, even with the characteristic FFT artifacts (for Linux, there&#x27;s Noise Repellent [1] available for advanced spectral denoising; there&#x27;s also a ton of commercial spectral processors available).<p>The demonstration could use more random background noises to separate it from FFT noise suppressors (as it&#x27;s the primary benefit of ML-based filters), and more varied speech to separate it from RNNoise [2] which tends to suppress breath and cut the sibilants in an unnatural manner. The latency is also important - is it as low as in RNNoise? What about the CPU load?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;lucianodato&#x2F;noise-repellent">https:&#x2F;&#x2F;github.com&#x2F;lucianodato&#x2F;noise-repellent</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;werman&#x2F;noise-suppression-for-voice">https:&#x2F;&#x2F;github.com&#x2F;werman&#x2F;noise-suppression-for-voice</a></div><br/></div></div><div id="36224452" class="c"><input type="checkbox" id="c-36224452" checked=""/><div class="controls bullet"><span class="by">boneitis</span><span>|</span><a href="#36223049">prev</a><span>|</span><a href="#36222321">next</a><span>|</span><label class="collapse" for="c-36224452">[-]</label><label class="expand" for="c-36224452">[1 more]</label></div><br/><div class="children"><div class="content">If you (especially on behalf of any hip, popular platforms like Discord) undertake any projects to aggressively denoise or compress audio, please (PLEASE) do us people with auditory processing difficulties a favor, and include such people in your testing.<p>I beg of you, with utmost sincerity.</div><br/></div></div><div id="36222321" class="c"><input type="checkbox" id="c-36222321" checked=""/><div class="controls bullet"><span class="by">nitinreddy88</span><span>|</span><a href="#36224452">prev</a><span>|</span><a href="#36221782">next</a><span>|</span><label class="collapse" for="c-36222321">[-]</label><label class="expand" for="c-36222321">[1 more]</label></div><br/><div class="children"><div class="content">Integrate with Pipewire: <a href="https:&#x2F;&#x2F;github.com&#x2F;Rikorose&#x2F;DeepFilterNet&#x2F;blob&#x2F;main&#x2F;ladspa&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;Rikorose&#x2F;DeepFilterNet&#x2F;blob&#x2F;main&#x2F;ladspa&#x2F;R...</a><p>Youtube demo: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;EO7n96YwnyE" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;EO7n96YwnyE</a><p>Paper Explanation: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;it90gBqkY6k" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;it90gBqkY6k</a></div><br/></div></div><div id="36221782" class="c"><input type="checkbox" id="c-36221782" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#36222321">prev</a><span>|</span><a href="#36222807">next</a><span>|</span><label class="collapse" for="c-36221782">[-]</label><label class="expand" for="c-36221782">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s so excellent how many moats are just getting obliterated.<p>I absolutely have been a real snarky hater against AI, as a horrible fuedal unobservable black box that has way too much power in the world. But open source has been doing amazing at reading the papers &amp; reproducing &amp; it&#x27;s glorious to see.<p>Amazing examples of a peership culture in action. Rising each other up is so divine. Share the knowledge &amp; means.</div><br/><div id="36221892" class="c"><input type="checkbox" id="c-36221892" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#36221782">parent</a><span>|</span><a href="#36223709">next</a><span>|</span><label class="collapse" for="c-36221892">[-]</label><label class="expand" for="c-36221892">[6 more]</label></div><br/><div class="children"><div class="content">I recently replaced an image classification pipeline that leaned heavily on classical computer vision techniques (like you&#x27;d find in OpenCV) with a neural network based approach using open models. There were about 4 years of developer effort invested in that old pipeline and I got better results with 2 months of effort invested in the new NN based system.<p>Later this year I plan on revamping an old NLP system with even more man-years of effort invested in it. I think I can beat it with neural networks too. The main reason I haven&#x27;t started already is that open language model progress is so fast that I expect significantly better building blocks in 4 months. Using these new tools feels magical, particularly when you have experienced how much effort it took to get half-as-good results with older techniques.</div><br/><div id="36222434" class="c"><input type="checkbox" id="c-36222434" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#36221782">root</a><span>|</span><a href="#36221892">parent</a><span>|</span><a href="#36222222">next</a><span>|</span><label class="collapse" for="c-36222434">[-]</label><label class="expand" for="c-36222434">[1 more]</label></div><br/><div class="children"><div class="content">To quote Lukaz Kaiser in his famous talk on 2017: &quot;stupidity is all you need&quot;</div><br/></div></div><div id="36222222" class="c"><input type="checkbox" id="c-36222222" checked=""/><div class="controls bullet"><span class="by">Fordec</span><span>|</span><a href="#36221782">root</a><span>|</span><a href="#36221892">parent</a><span>|</span><a href="#36222434">prev</a><span>|</span><a href="#36223709">next</a><span>|</span><label class="collapse" for="c-36222222">[-]</label><label class="expand" for="c-36222222">[4 more]</label></div><br/><div class="children"><div class="content">Any NN tips? I have my legacy pipelines, but haven&#x27;t put the effort to migrate yet even though I know I should.</div><br/><div id="36223034" class="c"><input type="checkbox" id="c-36223034" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#36221782">root</a><span>|</span><a href="#36222222">parent</a><span>|</span><a href="#36222996">next</a><span>|</span><label class="collapse" for="c-36223034">[-]</label><label class="expand" for="c-36223034">[1 more]</label></div><br/><div class="children"><div class="content">The ML&#x2F;DL software ecosystem is dominated by Python. Python&#x27;s dependency management can be especially tricky, so try to limit the dependencies you&#x27;re pulling in for the final deployed artifact if your final artifact is also Python based.<p>CUDA can be difficult to set up correctly on your personal development machine and if you rely on it you&#x27;re also limiting the development machines that other people can use. You&#x27;re limiting the deployment options and the CI options. This may differ if you work at a larger company that has a team specializing in these things, but I had to work out everything from initial proof-of-concept to final deployment. Some applications absolutely need the higher performance from GPU execution but it&#x27;s worth seeing if you can get away with CPU-only execution because it avoids operational complications.<p>I used the ONNX runtime (as this DeepFilterNet project does, indirectly, according to a top level comment by WiSaGaN) and I was able to get adequate inference speed running on plain CPU. It&#x27;s a small Python service that just wraps the inference logic with a command interface and a connection to Redis. It takes protocol buffer inputs from a Redis based queue, does a little bit of control logic followed by inference, and writes the results as protocol buffers to another Redis based queue.<p>The only ML libraries I have on the Python side are the ones for ONNX. This saved over a gigabyte (!) of transitive dependencies compared to the first proof-of-concept I had that relied on PyTorch for runtime inference.<p>Final advice: you actually don&#x27;t need to know much theory to start doing something useful. I hadn&#x27;t studied neural networks since graduate school 20 years ago so my theory is hopelessly outdated. I just started hacking together a little demo for myself and it was good enough that I was encouraged to take it all the way to production.</div><br/></div></div><div id="36222996" class="c"><input type="checkbox" id="c-36222996" checked=""/><div class="controls bullet"><span class="by">chintler</span><span>|</span><a href="#36221782">root</a><span>|</span><a href="#36222222">parent</a><span>|</span><a href="#36223034">prev</a><span>|</span><a href="#36222962">next</a><span>|</span><label class="collapse" for="c-36222996">[-]</label><label class="expand" for="c-36222996">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve modernized a lot of legacy pipelines and can help you if you want.</div><br/></div></div><div id="36222962" class="c"><input type="checkbox" id="c-36222962" checked=""/><div class="controls bullet"><span class="by">lettergram</span><span>|</span><a href="#36221782">root</a><span>|</span><a href="#36222222">parent</a><span>|</span><a href="#36222996">prev</a><span>|</span><a href="#36223709">next</a><span>|</span><label class="collapse" for="c-36222962">[-]</label><label class="expand" for="c-36222962">[1 more]</label></div><br/><div class="children"><div class="content">Isolate the prior logic into functional components. Find the inputs and outputs to each box. Identify which component ML can replace. Replace one after the next, sometimes merging if the ML can do it all on the gpu</div><br/></div></div></div></div></div></div><div id="36223709" class="c"><input type="checkbox" id="c-36223709" checked=""/><div class="controls bullet"><span class="by">atoav</span><span>|</span><a href="#36221782">parent</a><span>|</span><a href="#36221892">prev</a><span>|</span><a href="#36222057">next</a><span>|</span><label class="collapse" for="c-36223709">[-]</label><label class="expand" for="c-36223709">[1 more]</label></div><br/><div class="children"><div class="content">As someone who worked professionally using top of the line audio denoising for speech in cinema productions I have to say that I am underwhelmed by the results. This is very similar to what traditional algorithms would have achieved a decade ago.<p>Of course there might be potential for improvement there, maybe it is more performant or was developed way faster etc. But just listening to the audible result is not too cinvincing â yet.</div><br/></div></div><div id="36222057" class="c"><input type="checkbox" id="c-36222057" checked=""/><div class="controls bullet"><span class="by">est31</span><span>|</span><a href="#36221782">parent</a><span>|</span><a href="#36223709">prev</a><span>|</span><a href="#36222807">next</a><span>|</span><label class="collapse" for="c-36222057">[-]</label><label class="expand" for="c-36222057">[1 more]</label></div><br/><div class="children"><div class="content">Note that this isn&#x27;t just a paper reproduction but a new paper in and of itself (the github is by one of the paper&#x27;s authors). This is unbelievably amazing. I wonder how it compares to rnnoise, which is also open source and also targets real time settings.</div><br/></div></div></div></div><div id="36222807" class="c"><input type="checkbox" id="c-36222807" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#36221782">prev</a><span>|</span><a href="#36222536">next</a><span>|</span><label class="collapse" for="c-36222807">[-]</label><label class="expand" for="c-36222807">[2 more]</label></div><br/><div class="children"><div class="content">It looks like the library in Rust is using `tract-onnx` to do the inference: <a href="https:&#x2F;&#x2F;github.com&#x2F;Rikorose&#x2F;DeepFilterNet&#x2F;blob&#x2F;2a84d2a1750a5fcb608323d1b4f964d9f1c037a6&#x2F;libDF&#x2F;Cargo.toml#L112">https:&#x2F;&#x2F;github.com&#x2F;Rikorose&#x2F;DeepFilterNet&#x2F;blob&#x2F;2a84d2a1750a5...</a> I am wondering whether using Python for research, training in big data center, and Rust at edge for efficient inference would be a trend in the future. We do have a larger community of C++ right now for inference (e.g. ggml). But Rust crate as component to build applications of AI is joy to use.</div><br/><div id="36224322" class="c"><input type="checkbox" id="c-36224322" checked=""/><div class="controls bullet"><span class="by">ikhatri</span><span>|</span><a href="#36222807">parent</a><span>|</span><a href="#36222536">next</a><span>|</span><label class="collapse" for="c-36224322">[-]</label><label class="expand" for="c-36224322">[1 more]</label></div><br/><div class="children"><div class="content">You can use the onnx cpu runtime in python or c++ too. It doesnât have to be rust. And if you want GPU support you can even run models saved in the onnx format on Nvidia GPUs with the TensorRT runtime.<p>Honestly while ggml is super cool. It started as a hobby project and you probably shouldnât use it in production. ONNX has been the defacto standard for ML inference for years. What it is missing (compared to ggml) is 2-6bit inference which is helpful for large scale transformers on edge devices (and is what helped ggml gain adoption so fast).</div><br/></div></div></div></div><div id="36222536" class="c"><input type="checkbox" id="c-36222536" checked=""/><div class="controls bullet"><span class="by">narrationbox</span><span>|</span><a href="#36222807">prev</a><span>|</span><a href="#36223366">next</a><span>|</span><label class="collapse" for="c-36222536">[-]</label><label class="expand" for="c-36222536">[2 more]</label></div><br/><div class="children"><div class="content">Since it does the signal processing in the Fourier domain, does this suffer from audio artefacts e.g. hissing in the output? Torch&#x27;s inverse STFT uses Griffin-Lim which is probabilistic and if you don&#x27;t train it sufficiently, you may sometimes get noise in the output.<p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;generated&#x2F;torch.istft.html#torch-istft" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;generated&#x2F;torch.istft.html#t...</a><p>An alternative would be to use a vocoder network (or just target a neural speech codec like SoundStream).</div><br/><div id="36223266" class="c"><input type="checkbox" id="c-36223266" checked=""/><div class="controls bullet"><span class="by">thatsadude</span><span>|</span><a href="#36222536">parent</a><span>|</span><a href="#36223366">next</a><span>|</span><label class="collapse" for="c-36223266">[-]</label><label class="expand" for="c-36223266">[1 more]</label></div><br/><div class="children"><div class="content">Not all spectral methods have such artifact. The type of artifacts you mention happens when you need to do phase retrieval or try to reconstruct waveforms from melspectrogram. Deepfilternet does spectral masking on the complex spectrogram so there is no need for phase retrieval.</div><br/></div></div></div></div><div id="36223366" class="c"><input type="checkbox" id="c-36223366" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#36222536">prev</a><span>|</span><label class="collapse" for="c-36223366">[-]</label><label class="expand" for="c-36223366">[1 more]</label></div><br/><div class="children"><div class="content">How does it compare to rnnoise?</div><br/></div></div></div></div></div></div></div></body></html>