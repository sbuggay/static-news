<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687856459805" as="style"/><link rel="stylesheet" href="styles.css?v=1687856459805"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a>Â <span class="domain">(<a href="https://lilianweng.github.io">lilianweng.github.io</a>)</span></div><div class="subtext"><span>DanielKehoe</span> | <span>41 comments</span></div><br/><div><div id="36489703" class="c"><input type="checkbox" id="c-36489703" checked=""/><div class="controls bullet"><span class="by">TekMol</span><span>|</span><a href="#36489413">next</a><span>|</span><label class="collapse" for="c-36489703">[-]</label><label class="expand" for="c-36489703">[15 more]</label></div><br/><div class="children"><div class="content">Do I understand it correctly, that an LLMs are neural networks which only ever output a single &quot;token&quot;, which is a short string of a few chars? And then the whole input plus that output  is fed back into the NN to produce the next token?<p>So if you ask ChatGPT &quot;Describe Berlin&quot;, what happens is that the NN is called 6 times with these inputs:<p><pre><code>    Input: Describe Berlin.
    Outpu: Berlin
    Input: Describe Berlin. Berlin
    Outpu: is
    Input: Describe Berlin. Berlin is
    Outpu: a
    Input: Describe Berlin. Berlin is a
    Outpu: nice
    Input: Describe Berlin. Berlin is a nice
    Outpu: city
    Input: Describe Berlin. Berlin is a nice city
    Outpu: .
</code></pre>
ChatGPT&#x27;s answer:<p><pre><code>    Berlin is a nice city.
</code></pre>
Is that how LLMs work?</div><br/><div id="36490407" class="c"><input type="checkbox" id="c-36490407" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36490219">next</a><span>|</span><label class="collapse" for="c-36490407">[-]</label><label class="expand" for="c-36490407">[1 more]</label></div><br/><div class="children"><div class="content">One addition is that they don&#x27;t return just one token, but the probabilities for each token.<p>You could then greedily take the most probable single token, or select a bit more randomly (that&#x27;s where temperature comes in) in a few different ways. You can do better still by exploring a few of the possible top options to select a high probability <i>chain</i> of tokens. That&#x27;s because the most probable <i>next</i> token may not be the start of the most probable <i>sequence</i> of tokens. That&#x27;s called beam search.<p>Other than that, things like gpt can&#x27;t go back and edit what they&#x27;ve outputted, so it&#x27;s much more like listening to someone talk from the top of their head and writing down everything they say. They can say something wrong, followed by correcting themselves. The fact that the output becomes the input makes it much more obvious why &quot;lets think through step by step&quot; helps them reason, they can do simpler things then <i>see</i> the simpler steps they&#x27;ve already done to answer the more complex parts.</div><br/></div></div><div id="36490219" class="c"><input type="checkbox" id="c-36490219" checked=""/><div class="controls bullet"><span class="by">macrolime</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36490407">prev</a><span>|</span><a href="#36489789">next</a><span>|</span><label class="collapse" for="c-36490219">[-]</label><label class="expand" for="c-36490219">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s how autoregressive LLMs work. GPTs are autoregressive, but it&#x27;s not the only way an LLM can work. You can also have other types of LLM, like diffusion based, other types should also be possible.<p>At the moment, image models are mostly diffusion models while LLMs are mostly autoregressive.
With a few other examples. Google has for example created both an autoregressive image model, Parti, and a diffusion model, Imagen.<p>It&#x27;s still early for non-autoregressive LLMs.</div><br/></div></div><div id="36489789" class="c"><input type="checkbox" id="c-36489789" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36490219">prev</a><span>|</span><a href="#36490533">next</a><span>|</span><label class="collapse" for="c-36489789">[-]</label><label class="expand" for="c-36489789">[1 more]</label></div><br/><div class="children"><div class="content">Basically, yes! There are some technical quibbles (tokens don&#x27;t line up to words, there&#x27;s recent research on how to make LLMs stateful so they don&#x27;t need to pass back the entire conversation for every token but can simply initialise to a remembered state), but this is basically correct as to what LLMs are mechanically doing.<p>The really exciting stuff is how they&#x27;re doing that! Research into that question is generally called interpretability, and it&#x27;s probably what I&#x27;m most interested in at the moment.</div><br/></div></div><div id="36490533" class="c"><input type="checkbox" id="c-36490533" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36489789">prev</a><span>|</span><a href="#36490226">next</a><span>|</span><label class="collapse" for="c-36490533">[-]</label><label class="expand" for="c-36490533">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div><div id="36490226" class="c"><input type="checkbox" id="c-36490226" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36490533">prev</a><span>|</span><a href="#36489965">next</a><span>|</span><label class="collapse" for="c-36490226">[-]</label><label class="expand" for="c-36490226">[1 more]</label></div><br/><div class="children"><div class="content">Yes. &quot;What is the most probable next word given the text so far&quot; is the standard &#x27;language modeling&#x27; task from classical NLP where it was done before using Markov chains and n-grams. RNNs then transformers and huge amounts of training data made them output what we see now, versus coherent but otherwise not very impressive text 10 years ago. The large contexts of current models made it possible to generate valid code where you need to remember to close a bracket opened 200 tokens before.</div><br/></div></div><div id="36489756" class="c"><input type="checkbox" id="c-36489756" checked=""/><div class="controls bullet"><span class="by">going_ham</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36489965">prev</a><span>|</span><a href="#36489958">next</a><span>|</span><label class="collapse" for="c-36489756">[-]</label><label class="expand" for="c-36489756">[5 more]</label></div><br/><div class="children"><div class="content">This is a great example to show the underlying mechanism!</div><br/><div id="36489774" class="c"><input type="checkbox" id="c-36489774" checked=""/><div class="controls bullet"><span class="by">TekMol</span><span>|</span><a href="#36489703">root</a><span>|</span><a href="#36489756">parent</a><span>|</span><a href="#36489958">next</a><span>|</span><label class="collapse" for="c-36489774">[-]</label><label class="expand" for="c-36489774">[4 more]</label></div><br/><div class="children"><div class="content">Great, thanks for the clarification.<p>And how does the NN represent the token at the output layer? Is it a binary representation of the token number?<p>Or does it have a neuron for each token it knows and ChatGPT takes the most activated neuron as the answer?</div><br/><div id="36490104" class="c"><input type="checkbox" id="c-36490104" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#36489703">root</a><span>|</span><a href="#36489774">parent</a><span>|</span><a href="#36489813">next</a><span>|</span><label class="collapse" for="c-36490104">[-]</label><label class="expand" for="c-36490104">[1 more]</label></div><br/><div class="children"><div class="content">Tokens are integers that map to text tokens.<p>Tokens are part of words, approx 4 characters or 75% of word.<p>It gives a list of tokens with their probabilities on output.<p>It&#x27;s a short list with highest probabilities.<p>Temperature controls which tokens to pick - usually 0% = top one only (consistent results), closer to 100% means more randomness (more &quot;creativity&quot;).</div><br/></div></div><div id="36489813" class="c"><input type="checkbox" id="c-36489813" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#36489703">root</a><span>|</span><a href="#36489774">parent</a><span>|</span><a href="#36490104">prev</a><span>|</span><a href="#36489958">next</a><span>|</span><label class="collapse" for="c-36489813">[-]</label><label class="expand" for="c-36489813">[2 more]</label></div><br/><div class="children"><div class="content">Basically the latter</div><br/><div id="36490103" class="c"><input type="checkbox" id="c-36490103" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#36489703">root</a><span>|</span><a href="#36489813">parent</a><span>|</span><a href="#36489958">next</a><span>|</span><label class="collapse" for="c-36490103">[-]</label><label class="expand" for="c-36490103">[1 more]</label></div><br/><div class="children"><div class="content">The tokenization algorithms I encountered all had around 50000 tokens, which fits nicely into (and makes good use of) a 16-bit number. Is this just a coincidence or does it have advantages for the token to be a 16-bit representable number?</div><br/></div></div></div></div></div></div></div></div><div id="36489958" class="c"><input type="checkbox" id="c-36489958" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36489756">prev</a><span>|</span><a href="#36489757">next</a><span>|</span><label class="collapse" for="c-36489958">[-]</label><label class="expand" for="c-36489958">[1 more]</label></div><br/><div class="children"><div class="content">Yes, at least decoder-only transformers afaik.</div><br/></div></div><div id="36489757" class="c"><input type="checkbox" id="c-36489757" checked=""/><div class="controls bullet"><span class="by">bhy</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36489958">prev</a><span>|</span><a href="#36489988">next</a><span>|</span><label class="collapse" for="c-36489757">[-]</label><label class="expand" for="c-36489757">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div><div id="36489988" class="c"><input type="checkbox" id="c-36489988" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36489703">parent</a><span>|</span><a href="#36489757">prev</a><span>|</span><a href="#36489413">next</a><span>|</span><label class="collapse" for="c-36489988">[-]</label><label class="expand" for="c-36489988">[1 more]</label></div><br/><div class="children"><div class="content">You should get an award.</div><br/></div></div></div></div><div id="36489413" class="c"><input type="checkbox" id="c-36489413" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#36489703">prev</a><span>|</span><a href="#36489540">next</a><span>|</span><label class="collapse" for="c-36489413">[-]</label><label class="expand" for="c-36489413">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.<p>While working on smol-developer I also eventually landed on the importance of planning as mentioned in my Agents writeup <a href="https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;agents" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;agents</a> . I feel some hesitation with suggesting this because it suggests I&#x27;m not deep-learning-pilled, but I really wonder how far next-token-prediction can go with planning. When I think about planning I think about mapping out a possibility space, identifying trees of dependencies, assigning priorities, and then solving for some kind of weighted shortest path. That&#x27;s an awful lot of work to expect of a next token predictor (goodness knows its scaled far beyond what anyone thought - is there any limit to next token prediction?).<p>If there were one focus area for GPT-5, my money would be on a better architecture capable of planning.</div><br/><div id="36489818" class="c"><input type="checkbox" id="c-36489818" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#36489413">parent</a><span>|</span><a href="#36489783">next</a><span>|</span><label class="collapse" for="c-36489818">[-]</label><label class="expand" for="c-36489818">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know how far &quot;pure&quot; next-token prediction can go with planning, although I wouldn&#x27;t count them out until their performance starts noticeably plateauing. But the tree of thought architecture is a very similar concept to what you&#x27;re discussing, you should definitely give it a read: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.10601" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.10601</a><p>It&#x27;s not everything involved in traditional planning, but it may be a framework to use more traditional planning algorithms on LLM output.</div><br/></div></div><div id="36489783" class="c"><input type="checkbox" id="c-36489783" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#36489413">parent</a><span>|</span><a href="#36489818">prev</a><span>|</span><a href="#36489580">next</a><span>|</span><label class="collapse" for="c-36489783">[-]</label><label class="expand" for="c-36489783">[1 more]</label></div><br/><div class="children"><div class="content">There is a fair amount of existing work related to planning in continuous and unbounded spaces under uncertainty. It seems likely some of the existing techniques combined with modern language models could be quite effective. A couple entry points:<p>Deep Reinforcement Learning with an Unbounded Action Space - 
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.04636v3" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.04636v3</a><p>Efficient Planning in a Compact Latent Action Space - 
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.10291" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.10291</a></div><br/></div></div><div id="36489580" class="c"><input type="checkbox" id="c-36489580" checked=""/><div class="controls bullet"><span class="by">friendzis</span><span>|</span><a href="#36489413">parent</a><span>|</span><a href="#36489783">prev</a><span>|</span><a href="#36489541">next</a><span>|</span><label class="collapse" for="c-36489580">[-]</label><label class="expand" for="c-36489580">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When I think about planning I think about mapping out a possibility space, identifying trees of dependencies, assigning priorities, and then solving for some kind of weighted shortest path.<p>but that&#x27;s two things: planning and execution of a plan. The plan is the dependency graph (!), assignment of priorities and shortest path is execution. This is very important in the context of agents, autonomous or not. If you want the agent to self-correct it has to understand that there can be multiple start points or multiple end points (or both) to backtrack and pivot. And as long as it is glorified &quot;next token predictor&quot; it cannot really do that.<p>Of course some tasks indeed are ifttt style linear-ish flows where next token prediction may prove to be adequate. However, if your agent is incapable of understanding non-linear flows, can it reasonably back off from one true way hen faced with such flow?</div><br/></div></div><div id="36489541" class="c"><input type="checkbox" id="c-36489541" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#36489413">parent</a><span>|</span><a href="#36489580">prev</a><span>|</span><a href="#36489711">next</a><span>|</span><label class="collapse" for="c-36489541">[-]</label><label class="expand" for="c-36489541">[1 more]</label></div><br/><div class="children"><div class="content">I feel this is more of a limit from single scratch pad agents<p>Having a tasking agent refining prompts seems a much robust approach.<p>Bonus if it can decompose the required data properly. I&#x27;m working on a world database prompt with limited success, but implementing the retrieval with prompts is time consuming and I&#x27;ve not much time. The idea is along the line of &quot;translate the user question in SQL, you have a database of all known facts with a table for each entity&quot; and then you handle the retrieval of each table data step by step.<p>I think then you&#x27;d be able to use the postgres planner as planner, but while I&#x27;ve seen project of postgres retrieving data from random sources I don&#x27;t remember the specifics.</div><br/></div></div><div id="36489711" class="c"><input type="checkbox" id="c-36489711" checked=""/><div class="controls bullet"><span class="by">niemandhier</span><span>|</span><a href="#36489413">parent</a><span>|</span><a href="#36489541">prev</a><span>|</span><a href="#36490312">next</a><span>|</span><label class="collapse" for="c-36489711">[-]</label><label class="expand" for="c-36489711">[1 more]</label></div><br/><div class="children"><div class="content">I think that in the end predicting words is non optimal , most things we want to do are things that are related to the internal representation of concepts that exist deeper in the layers.<p>I at least do not want to predict the next token, I want to predict the next concept in a chain of reasoning, but it seems that currently we are stuck at using the same representation for autoregression we use for training.<p>Maybe we can come up with a better way to construct these chains once we understand the models better.<p>Edit: Typo</div><br/></div></div><div id="36490312" class="c"><input type="checkbox" id="c-36490312" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36489413">parent</a><span>|</span><a href="#36489711">prev</a><span>|</span><a href="#36489540">next</a><span>|</span><label class="collapse" for="c-36490312">[-]</label><label class="expand" for="c-36490312">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>(..) I really wonder how far next-token-prediction can go with planning. When I think about planning I think about mapping out a possibility space, identifying trees of dependencies, assigning priorities, and then solving for some kind of weighted shortest path.</i><p>What you really do is more like: you&#x27;ve already formed a half-baked plan before the problem even fully registers, in one cognitive step of pulling the ready-made if somewhat fuzzy graph out of the depths of unconscious - and <i>then</i> you start doing all those things you mention, while trying hard not to be biased towards or anchored to that original plan-shaped blob of words, images and emotions.<p>I think creating that first plan <i>ex nihilo</i> and putting it into words is exactly what next-token-prediction can do, because that structure of options and dependencies and priorities and  a path through them is something that&#x27;s encoded wholesale in the latent space, and needs to be projected out of it. That&#x27;s at least my understanding of what current transformer LLMs are good at - encoding every imaginable relationship between tokens and token sequences into proximity in a four- or five-digit dimensional space, and then projecting out of it on inference.<p>Going beyond that - beyond that first plan prototype, which may or may not be good, is getting increasingly explicit and thus increasingly hard. I think you can ultimately coax an LLM to do all the steps you listed and then some, by walking it through them like infinitely patient parent of a 3 year old - but not within the context window, so you need to split it into multiple invocations. This grows in cost superlinearly, as you&#x27;ll have multiple results you need to summarize to feed into next stage, and if you&#x27;re computing enough of those results then they won&#x27;t fit into context window of the summarizer, so you need to add an intermediate level - and then eventually summaries may not fit in the context window of next step, so you need another intermediate level, etc. Context window is the limiting factor here.<p>And if you&#x27;re to use LLM to emulate a mechanical&#x2F;formalized process, then it&#x27;s better to just extract structured information from the LLM, feed it to regular software optimized for purpose, and feed the results back. I.e. something similar to LLM+P approach - or similar to any time when you get LLM to &quot;use tools&quot;. If it can call out to a calculator, it can just as much call out to some PERT tool to get your list of steps sorted and critical path highlighted.<p>On a tangent:<p>&gt; <i>identifying trees of dependencies</i><p>Are you sure those are trees? :). My pet peeve with all the hip software used to manage projects and track work in our industry, is that it treats work as decomposing into flat trees. In my experience, work decomposes into directed acyclic graphs (and in certain cases, it might be useful to allow cycles to make representation more compact). There&#x27;s hardly a tool that can deal with it in work planning context, not unless you step over to &quot;old-school&quot; engineering (but those tools aren&#x27;t cheap or well-known). Which is ironic, because at least when it comes to <i>build systems</i>, software devs recognize the DAG-like nature of dependencies.</div><br/></div></div></div></div><div id="36489540" class="c"><input type="checkbox" id="c-36489540" checked=""/><div class="controls bullet"><span class="by">novaRom</span><span>|</span><a href="#36489413">prev</a><span>|</span><a href="#36489978">next</a><span>|</span><label class="collapse" for="c-36489540">[-]</label><label class="expand" for="c-36489540">[3 more]</label></div><br/><div class="children"><div class="content">How small can be a LLM transformer in order to be able to understand basic human language and search for answers on the internet? It should not contain all the facts and knowledge, but must be quick (so, it&#x27;s a small model), understand at least one language, and know how and where to look for answers.<p>Would it be sufficient to have 1B, 3B or 7B parameters to achieve this? Or is it doable with 100M or even fewer parameters? I mean vocabulary size might be quite small, max context size could also be limited to 256 or 512 tokens. Is there any paper on that maybe?</div><br/><div id="36489718" class="c"><input type="checkbox" id="c-36489718" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36489540">parent</a><span>|</span><a href="#36489609">next</a><span>|</span><label class="collapse" for="c-36489718">[-]</label><label class="expand" for="c-36489718">[1 more]</label></div><br/><div class="children"><div class="content">A team at Microsoft Research asked the same question and just published a paper about part of that at least: TinyStories: How Small Can Language Models Be and Still Speak Coherent English? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.07759" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.07759</a><p>&quot;We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.&quot;<p>They also trained a TinyStories-Instruct instruction following variant. It looks like the 28M parameter 8 layer model had a 9&#x2F;10 on grammar and consistency. You could probably combine that with something like Jsonformer or parserLLM to enforce valid formatting.</div><br/></div></div><div id="36489609" class="c"><input type="checkbox" id="c-36489609" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36489540">parent</a><span>|</span><a href="#36489718">prev</a><span>|</span><a href="#36489978">next</a><span>|</span><label class="collapse" for="c-36489609">[-]</label><label class="expand" for="c-36489609">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need a 1B+ parameter model for this workflow, but it helps, particularly for the quality of the final output.<p>&gt; know how and where to look for answers<p>The point of calling them Agents is that they don&#x27;t know <i>how</i>. All the examples, including AutoGPT that makes AI influencers go OMG AGI, are operating in a discrete action space with user-specified hints to select which action (or none at all).</div><br/></div></div></div></div><div id="36489978" class="c"><input type="checkbox" id="c-36489978" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36489540">prev</a><span>|</span><a href="#36489916">next</a><span>|</span><label class="collapse" for="c-36489978">[-]</label><label class="expand" for="c-36489978">[9 more]</label></div><br/><div class="children"><div class="content">A fairly lengthy article about Autonomous AI, and, as far as I can tell, <i>not a single word about the safety implications of such a system</i> (a short note about reliability of LLMs is all we get, and it&#x27;s not clear that the author means anything more than &quot;the thing might break&quot;).<p>I get that there are different philosophies on AI risk, but this is like reading an in-depth discussion about potential atomic bombs in 1942, with no mention of the fact that such a bomb could potentially level cities and kill millions.<p>If a field of research ever needed oversight and regulation, this is it. I&#x27;m not convinced that would solve the problem, but allowing this kind of rushing forward to continue is madness.</div><br/><div id="36490073" class="c"><input type="checkbox" id="c-36490073" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#36489978">parent</a><span>|</span><a href="#36490032">next</a><span>|</span><label class="collapse" for="c-36490073">[-]</label><label class="expand" for="c-36490073">[7 more]</label></div><br/><div class="children"><div class="content">No, this is nothing like atomic bombs. In what scenario can you kill 100,000 people with with an LLM hooked up to a Python interpreter? The AI safety grift is out of control.</div><br/><div id="36490127" class="c"><input type="checkbox" id="c-36490127" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#36489978">root</a><span>|</span><a href="#36490073">parent</a><span>|</span><a href="#36490114">next</a><span>|</span><label class="collapse" for="c-36490127">[-]</label><label class="expand" for="c-36490127">[2 more]</label></div><br/><div class="children"><div class="content">But that&#x27;s not what this article is about, it&#x27;s about attempting to create some type of AGI, highly experimental seemingly non-aligned, which is connected to...the internet. Which is why I do agree with others, even if this is a a weak attempt (I have no idea, not an AI researcher) if it was to be successful could be hyper destructive.<p>The project seems really quite loose and the author seems to be lacking in awareness with regards to the consequences of such a project. I do think others are right in saying, we really should understand more bout where people can and will try to take these ideas before we keep putting models on the internet for consumption by just anyone.</div><br/><div id="36490206" class="c"><input type="checkbox" id="c-36490206" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36489978">root</a><span>|</span><a href="#36490127">parent</a><span>|</span><a href="#36490114">next</a><span>|</span><label class="collapse" for="c-36490206">[-]</label><label class="expand" for="c-36490206">[1 more]</label></div><br/><div class="children"><div class="content">&gt; seems to not have much awareness about what the consequences of such a project might mean<p>That&#x27;s the worst part of it.<p>Is the current generation of LLMs dangerous? Probably not. Will the next generation of LLMs be dangerous? Maybe not, if we&#x27;re lucky. The one after that? Nobody knows.<p>But projects like this don&#x27;t so much rely on there being no danger, but rather, <i>the question seems to not even have been considered.</i> And that&#x27;s crazy, because the assumption that things are harmless is getting more and more shaky the more capable these models become.</div><br/></div></div></div></div><div id="36490114" class="c"><input type="checkbox" id="c-36490114" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36489978">root</a><span>|</span><a href="#36490073">parent</a><span>|</span><a href="#36490127">prev</a><span>|</span><a href="#36490096">next</a><span>|</span><label class="collapse" for="c-36490114">[-]</label><label class="expand" for="c-36490114">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>In what scenario can you kill 100,000 people with with an LLM hooked up to a Python interpreter?</i><p>When that Python interpreter is running on the computer doing model-predictive control of your country&#x27;s natural gas pipelines, or the plant that mixes your mayonnaise.<p>Just so you know, the Python interpreter is already there in those places, and so is Internet access, because suits like their dashboards with real-time graphs.<p>Remember, the S in IIoT (Industrial IoT) stands for both sanity and security :).</div><br/></div></div><div id="36490096" class="c"><input type="checkbox" id="c-36490096" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36489978">root</a><span>|</span><a href="#36490073">parent</a><span>|</span><a href="#36490114">prev</a><span>|</span><a href="#36490032">next</a><span>|</span><label class="collapse" for="c-36490096">[-]</label><label class="expand" for="c-36490096">[3 more]</label></div><br/><div class="children"><div class="content">&gt; In what scenario can you kill 100,000 people with with an LLM hooked up to a Python interpreter?<p>The article talks about LLMs being hooked up <i>to the Internet.</i> Big difference.<p>If you really can&#x27;t imagine a scenario where this might lead to people dying, you&#x27;re not trying hard enough.</div><br/><div id="36490243" class="c"><input type="checkbox" id="c-36490243" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#36489978">root</a><span>|</span><a href="#36490096">parent</a><span>|</span><a href="#36490032">next</a><span>|</span><label class="collapse" for="c-36490243">[-]</label><label class="expand" for="c-36490243">[2 more]</label></div><br/><div class="children"><div class="content">You have failed to provide a scenario, likely because no realistic scenario exists. Let&#x27;s go through an example scenario and the hurdles you have to overcome for this to work. We&#x27;ll use the &quot;mayonnaise plant&quot; example from a sibling comment.<p>1. The LLM needs to find an exploitable bug in a popular code base.<p>2. The LLM needs to write a reliable exploit for that bug.<p>3. The LLM needs to develop a worm that exploits that bug and spreads itself, opening access to the system.<p>4. The LLM needs to connect to systems and understand if they are of any significance (it found a mayonnaise plant!).<p>5. The LLM needs to understand the control protocols of their industrial control systems.<p>6. The LLM needs to understand how to make a dangerous composition from the ingredients it has on hand (let&#x27;s pretend it can dump some industrial cleaning solution that is on standby for cleaning the tanks).<p>7. The LLM needs to assume such total control over this processing plant that it can disguise the traffic and not trigger a single alarm around malfunctions.<p>What you&#x27;re vaguely hinting at is extremely high skilled labor. There are a few billion dollar businesses in those steps. I welcome you to go read up on the challenges in automated exploit generation. LLMs are nowhere close.<p>Now, you might rebut and say there are far simpler attacks, like phishing! Also an extremely hard problem. Try to send email in mass and not land in a spam filter, try to do the reconnaissance necessary to generate a believable login page. Try to leverage the sale guy&#x27;s credentials to reach any system more meaningful than the company&#x27;s Salesforce instance.<p>So once again, I ask, please walk me through a situation where an LLM gets anywhere close to killing even 1% of the number of people an atomic bomb could.</div><br/><div id="36490381" class="c"><input type="checkbox" id="c-36490381" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36489978">root</a><span>|</span><a href="#36490243">parent</a><span>|</span><a href="#36490032">next</a><span>|</span><label class="collapse" for="c-36490381">[-]</label><label class="expand" for="c-36490381">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs are nowhere close.<p><i>We</i> are nowhere close to even a rudimentary understanding of how current LLMs actually work. All we have are low-level building blocks, and lots of philosophizing about high-level output.<p>Considering that, relying on some gut feeling about what LLMs &quot;surely cannot possibly do&quot; is reckless overconfidence. Not to mention that the next generation of LLMs, with potentially entirely new emergent properties, might be just around the corner, and the time to put safeguards into place is now, not when it&#x27;s too late.<p>As for the scenario, all an LLM with Internet access would need to do is find a single remotely exploitable vulnerability in the Linux network stack. That would allow it to literally shut down the entire Internet, which would kill <i>a lot more</i> people than a single atomic bomb.</div><br/></div></div></div></div></div></div></div></div><div id="36490032" class="c"><input type="checkbox" id="c-36490032" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36489978">parent</a><span>|</span><a href="#36490073">prev</a><span>|</span><a href="#36489916">next</a><span>|</span><label class="collapse" for="c-36490032">[-]</label><label class="expand" for="c-36490032">[1 more]</label></div><br/><div class="children"><div class="content">ok but it&#x27;s an odd take for a tech site like hacker news i mean what does that say about the llm paradigm vs for example object oriented paradigm or functional programming paradigm were they just playing around before and now it&#x27;s getting real? were they deliberately doing toy things before and now the adults are in the room making real computation systems with actual consequences for the real world and not just nerds playing with toys or what?<p>Like does this mean that it will completely obsolete the works of boffins like simon peyton jones and those other esoteric languages and everyone will talk to their computers in english language and the ones that will be the most effective computer enjoyers will be the high emotional intelligence salesmen? were they going in wrong direction for so long on purpose because they didn&#x27;t want to awaken the real power of the computers for ethical reasons, or were they trying to do this the whole time but they weren&#x27;t smart enough to figure it out?</div><br/></div></div></div></div><div id="36489916" class="c"><input type="checkbox" id="c-36489916" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#36489978">prev</a><span>|</span><a href="#36490015">next</a><span>|</span><label class="collapse" for="c-36489916">[-]</label><label class="expand" for="c-36489916">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s plausible, but it was partly written by ChatGPT. From the paper: &quot;Big thank you to ChatGPT for helping me draft this section.&quot; So of course it&#x27;s plausible. That&#x27;s what ChatGPT does.<p>There are a number of systems where someone bolted together components like this. Now we need more demos and evaluations of them. I just read a comment about someone who tried a sales chatbot. It would make up nonexistent products to respond to customer requests.<p>The underlying LLM systems need some kind of confidence metric output.</div><br/></div></div><div id="36490015" class="c"><input type="checkbox" id="c-36490015" checked=""/><div class="controls bullet"><span class="by">mercurialsolo</span><span>|</span><a href="#36489916">prev</a><span>|</span><a href="#36489501">next</a><span>|</span><label class="collapse" for="c-36490015">[-]</label><label class="expand" for="c-36490015">[2 more]</label></div><br/><div class="children"><div class="content">Autonomy without alignment is a slippery road.<p>Autonomous agents need guardrails and oversight. An autonomous agent let loose with all the tools in the world will in essence lead to an outcome which is not predicted to be in our favour.<p>Which is why the Open AI app store and plugins scare me more than anything else - more likely than not they are tool and data feeders into a large scale autonomous system.</div><br/><div id="36490205" class="c"><input type="checkbox" id="c-36490205" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#36490015">parent</a><span>|</span><a href="#36489501">next</a><span>|</span><label class="collapse" for="c-36490205">[-]</label><label class="expand" for="c-36490205">[1 more]</label></div><br/><div class="children"><div class="content">&gt; An autonomous agent let loose with all the tools in the world will in essence lead to an outcome which is not predicted to be in our favour.<p>Given the current state of the art, it won&#x27;t lead to any worse outcomes than a somewhat mentally impaired human &quot;let loose on the world&quot;. And since training scales quadratically (doubling model size means double the amount of data is needed to train it optimally), and we&#x27;re already at the limits of current hardware, that&#x27;s unlikely to change much any time soon.</div><br/></div></div></div></div><div id="36489501" class="c"><input type="checkbox" id="c-36489501" checked=""/><div class="controls bullet"><span class="by">Xen9</span><span>|</span><a href="#36490015">prev</a><span>|</span><a href="#36489330">next</a><span>|</span><label class="collapse" for="c-36489501">[-]</label><label class="expand" for="c-36489501">[1 more]</label></div><br/><div class="children"><div class="content">I predict this type of cognitive engineering of LLM-multiagents to become a thing.</div><br/></div></div><div id="36489330" class="c"><input type="checkbox" id="c-36489330" checked=""/><div class="controls bullet"><span class="by">gremlinsinc</span><span>|</span><a href="#36489501">prev</a><span>|</span><a href="#36489395">next</a><span>|</span><label class="collapse" for="c-36489330">[-]</label><label class="expand" for="c-36489330">[1 more]</label></div><br/><div class="children"><div class="content">wow, this was a brilliant summary of much research into ai agents, I&#x27;ve been reading a lot about these and following this stuff, but learned a lot.</div><br/></div></div><div id="36489395" class="c"><input type="checkbox" id="c-36489395" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36489330">prev</a><span>|</span><label class="collapse" for="c-36489395">[-]</label><label class="expand" for="c-36489395">[1 more]</label></div><br/><div class="children"><div class="content"><i>However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction).</i><p>Rightâ¦sounds quite reckless?</div><br/></div></div></div></div></div></div></div></body></html>