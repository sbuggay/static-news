<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703235655988" as="style"/><link rel="stylesheet" href="styles.css?v=1703235655988"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://nlpnewsletter.substack.com/p/nlp-research-in-the-era-of-llms">NLP Research in the Era of LLMs</a> <span class="domain">(<a href="https://nlpnewsletter.substack.com">nlpnewsletter.substack.com</a>)</span></div><div class="subtext"><span>sebg</span> | <span>13 comments</span></div><br/><div><div id="38731072" class="c"><input type="checkbox" id="c-38731072" checked=""/><div class="controls bullet"><span class="by">sunpazed</span><span>|</span><a href="#38732210">next</a><span>|</span><label class="collapse" for="c-38731072">[-]</label><label class="expand" for="c-38731072">[1 more]</label></div><br/><div class="children"><div class="content">&gt; While LLM projects typically require an exorbitant amount of resources, it is important to remind ourselves that research does not need to assemble full-fledged massively expensive systems in order to have impact.<p>Check out TinyLlama; <a href="https:&#x2F;&#x2F;github.com&#x2F;jzhang38&#x2F;TinyLlama">https:&#x2F;&#x2F;github.com&#x2F;jzhang38&#x2F;TinyLlama</a><p>Four research students from the Singapore University of Technology and Design are pretraining a 1.1B Llama model on 3 trillion tokens using a handful of A100&#x27;s.<p>They&#x27;re also providing the source code, training data, and fine-tuned checkpoints for anyone to run.</div><br/></div></div><div id="38732210" class="c"><input type="checkbox" id="c-38732210" checked=""/><div class="controls bullet"><span class="by">JacobiX</span><span>|</span><a href="#38731072">prev</a><span>|</span><a href="#38731139">next</a><span>|</span><label class="collapse" for="c-38732210">[-]</label><label class="expand" for="c-38732210">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still using old-school maximum-entropy models and HMMs for NLP and integrating them into our products. The model size is less than 1MB, and the inference time is really negligible. These models are used in batch processing for billions of entries and with no access to internet. Not sure if LLMs can fit this task?</div><br/><div id="38732281" class="c"><input type="checkbox" id="c-38732281" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38732210">parent</a><span>|</span><a href="#38731139">next</a><span>|</span><label class="collapse" for="c-38732281">[-]</label><label class="expand" for="c-38732281">[1 more]</label></div><br/><div class="children"><div class="content">If inference time is negligible, you might find that a larger model with slightly better metrics is affordable. Not that an LLM is the way to go, but perhaps if you’re already in for the squeeze maybe there’s more juice to be had.</div><br/></div></div></div></div><div id="38731139" class="c"><input type="checkbox" id="c-38731139" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#38732210">prev</a><span>|</span><a href="#38730744">next</a><span>|</span><label class="collapse" for="c-38731139">[-]</label><label class="expand" for="c-38731139">[1 more]</label></div><br/><div class="children"><div class="content">I like this article<p>A tougher question for me here is the why, vs how: &#x27;Why research LLMs in academia if you can work in industry on the same problems with more resources?&#x27;. Systems researchers face something similar, where big systems are easier to research at Google etc. and get paid much more to do so.<p>The bit on Reasoning seems relevant: many industry roles for that will be taken by people ahead of the pre-graduation student, and it&#x27;s research that fewer industry labs will be doing in an extreme way than say engineering more efficient inferencing. Doing that kind of research today is a lot more exciting than when we were doing it 10 years ago, so it&#x27;s a special time.</div><br/></div></div><div id="38730744" class="c"><input type="checkbox" id="c-38730744" checked=""/><div class="controls bullet"><span class="by">owlninja</span><span>|</span><a href="#38731139">prev</a><span>|</span><label class="collapse" for="c-38730744">[-]</label><label class="expand" for="c-38730744">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still in the stone ages trying to use topic modeling on a free text field.  Can LLMs help me decipher a large dataset of human text and put it in buckets?</div><br/><div id="38732111" class="c"><input type="checkbox" id="c-38732111" checked=""/><div class="controls bullet"><span class="by">anon-feaata2311</span><span>|</span><a href="#38730744">parent</a><span>|</span><a href="#38731001">next</a><span>|</span><label class="collapse" for="c-38732111">[-]</label><label class="expand" for="c-38732111">[2 more]</label></div><br/><div class="children"><div class="content">Have a look at Using text embeddings and approximate nearest neighbour search to explore ...<p><a href="https:&#x2F;&#x2F;medium.com&#x2F;gft-engineering&#x2F;using-text-embeddings-and-approximate-nearest-neighbour-search-to-explore-shakespeares-plays-29e6bde05a16" rel="nofollow noreferrer">https:&#x2F;&#x2F;medium.com&#x2F;gft-engineering&#x2F;using-text-embeddings-and...</a><p>You can reduce sentences to vectors and then create similarity scores to build a graph over the corpus. If you choose to create clusters then use a llm to summarise them to create labels.</div><br/><div id="38732119" class="c"><input type="checkbox" id="c-38732119" checked=""/><div class="controls bullet"><span class="by">anon-feaata2311</span><span>|</span><a href="#38730744">root</a><span>|</span><a href="#38732111">parent</a><span>|</span><a href="#38731001">next</a><span>|</span><label class="collapse" for="c-38732119">[-]</label><label class="expand" for="c-38732119">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly I don&#x27;t see much progress on the core of vector DBs... Does anyone have so innovative references?</div><br/></div></div></div></div><div id="38731001" class="c"><input type="checkbox" id="c-38731001" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#38730744">parent</a><span>|</span><a href="#38732111">prev</a><span>|</span><a href="#38730860">next</a><span>|</span><label class="collapse" for="c-38731001">[-]</label><label class="expand" for="c-38731001">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely.  Give it a try. I pasted these comments into ChatGPT and asked it to score them according to slashdot’s system (which i didn’t actually define) and got this reply:<p>In Slashdot&#x27;s scoring system, comments are scored from -1 to 5, based on factors like insightfulness, informativeness, and whether they&#x27;re interesting or funny. Here are the hypothetical scores for the comments from Hacker News:<p>owlninja (42 minutes ago): This user is asking about the applicability of LLMs for text categorization in large datasets. It&#x27;s a straightforward query, indicating a need for information. Score: 3 (Interesting) - As it opens up a discussion on a relevant and technical topic.<p>causalmodels (18 minutes ago): This comment provides a direct solution with a resource (zod-gpt). Score: 4 (Informative) - It not only addresses the query but also provides a specific tool to help achieve the goal, adding value to the discussion.<p>quickthrower2 (41 minutes ago): This comment suggests an approach for creating buckets using a sample and an LLM. Score: 3 (Interesting) - It proposes a practical method, contributing constructively to the original query.<p>These scores are subjective and would depend on the perspectives of the individual moderators or the community&#x27;s voting on Slashdot.</div><br/></div></div><div id="38730860" class="c"><input type="checkbox" id="c-38730860" checked=""/><div class="controls bullet"><span class="by">causalmodels</span><span>|</span><a href="#38730744">parent</a><span>|</span><a href="#38731001">prev</a><span>|</span><a href="#38731487">next</a><span>|</span><label class="collapse" for="c-38730860">[-]</label><label class="expand" for="c-38730860">[1 more]</label></div><br/><div class="children"><div class="content">If you know your buckets you can pretty easily get most LLMs to generate structured outputs using something like zod-gpt [1].<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;dzhng&#x2F;zod-gpt">https:&#x2F;&#x2F;github.com&#x2F;dzhng&#x2F;zod-gpt</a></div><br/></div></div><div id="38731487" class="c"><input type="checkbox" id="c-38731487" checked=""/><div class="controls bullet"><span class="by">soultrees</span><span>|</span><a href="#38730744">parent</a><span>|</span><a href="#38730860">prev</a><span>|</span><a href="#38730749">next</a><span>|</span><label class="collapse" for="c-38731487">[-]</label><label class="expand" for="c-38731487">[1 more]</label></div><br/><div class="children"><div class="content">I just did this today using openAI’s function calling. I have a bunch of elements in a scene and trying to classify them between various ‘buckets’ has been the challenge. The way I set it up is that in the schema expected, you take that top level free text, wrap it in quotes and it becomes the parent object with the elementCategory as a string required inside it, with a list of all category types in the description. Then loop through and create a dynamic schema based on how you chunk your data, then add a validation step at the end to ensure that chatgpt doesn’t forget, add or change keys. I found that if you create a numberedKeywordString for your chunk, and wrap each in quotes, then stuff that numberedKeywordString into the system prompt then it’s solid and will ensure the LLM catches every key and gets the associated elementCategory value.<p>It works quite well for dynamic classification in my purposes but I’m sure there is a better way.</div><br/></div></div><div id="38730749" class="c"><input type="checkbox" id="c-38730749" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38730744">parent</a><span>|</span><a href="#38731487">prev</a><span>|</span><a href="#38731122">next</a><span>|</span><label class="collapse" for="c-38730749">[-]</label><label class="expand" for="c-38730749">[1 more]</label></div><br/><div class="children"><div class="content">It might help you create the buckets at least if you take a sample of say 1000
and give that to the LLM.</div><br/></div></div><div id="38731122" class="c"><input type="checkbox" id="c-38731122" checked=""/><div class="controls bullet"><span class="by">ta988</span><span>|</span><a href="#38730744">parent</a><span>|</span><a href="#38730749">prev</a><span>|</span><label class="collapse" for="c-38731122">[-]</label><label class="expand" for="c-38731122">[1 more]</label></div><br/><div class="children"><div class="content">absolutely here is an example with openai. First pass you explain the task and let it reply to a hundredish examples. Fix the errors, fine tune. And it should be much better. You can repeat that any time you need until you reach satisfactory results but they come quickly when I worked on large number of short texts i wanted to classify and extract info from.</div><br/></div></div></div></div></div></div></div></div></div></body></html>