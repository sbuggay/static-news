<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1682672452493" as="style"/><link rel="stylesheet" href="styles.css?v=1682672452493"/><link rel="apple-touch-startup-image" href="https://png.pngtree.com/png-clipart/20210309/original/pngtree-a-squatting-tabby-cat-png-image_5803660.jpg"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2304.14399">We&#x27;re afraid language models aren&#x27;t modeling ambiguity</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>lnyan</span> | <span>24 comments</span></div><br/><div><div id="35738919" class="c"><input type="checkbox" id="c-35738919" checked=""/><div class="controls bullet"><span class="by">andrewmcwatters</span><span>|</span><a href="#35738086">next</a><span>|</span><label class="collapse" for="c-35738919">[-]</label><label class="expand" for="c-35738919">[1 more]</label></div><br/><div class="children"><div class="content">Not only is it possible that LLMs fail to differentiate ambiguity, but OpenAI’s flavor of GPTs fail other language understanding mechanisms as well and it’s jarring.<p>You can get it to mistake « afraid » between fear and sorry-to-say scenarios but you can even more easily get it to say that it doesn’t have personal opinions and yet express them anyway.<p>So which is it? It’s clear transformers can’t understand either case. They’re not architecturally designed to. The emergent behavior of appearing to do so is only driven by how much data you throw at them.</div><br/></div></div><div id="35738086" class="c"><input type="checkbox" id="c-35738086" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#35738919">prev</a><span>|</span><a href="#35737923">next</a><span>|</span><label class="collapse" for="c-35738086">[-]</label><label class="expand" for="c-35738086">[6 more]</label></div><br/><div class="children"><div class="content">I find this very confusingly written. If they just dumped a large chat log of the evaluation process it would be much clearer what the model is actually being asked to do. With such a delicate task one has to make sure that the model actually understands what it is supposed to do. I see, for example, that the terms &quot;neutral, entailing or contradicting the claim&quot; are replaced by &quot;the claim is inconclusive, true or false&quot; when talking to the model. At least that is how I understand it, and they have examples where the premise contradicts the hypothesis, which is different to knowing the hypothesis is false.<p>I really think we do way way too much explanation in our academic practises and way too little demonstration by examples. Again, if they dumped a large amount of diverse examples of their evaluation process, a reader could figure out by themselves very quickly how the process works.</div><br/><div id="35738567" class="c"><input type="checkbox" id="c-35738567" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#35738086">parent</a><span>|</span><a href="#35738403">next</a><span>|</span><label class="collapse" for="c-35738567">[-]</label><label class="expand" for="c-35738567">[1 more]</label></div><br/><div class="children"><div class="content">The data set can be found here: <a href="https:&#x2F;&#x2F;github.com&#x2F;alisawuffles&#x2F;ambient">https:&#x2F;&#x2F;github.com&#x2F;alisawuffles&#x2F;ambient</a><p>If you have access to some LLMs, it should be somewhat straight-forward to implement an alternative experimental design do test your hypothesis.</div><br/></div></div><div id="35738403" class="c"><input type="checkbox" id="c-35738403" checked=""/><div class="controls bullet"><span class="by">hobo_in_library</span><span>|</span><a href="#35738086">parent</a><span>|</span><a href="#35738567">prev</a><span>|</span><a href="#35738669">next</a><span>|</span><label class="collapse" for="c-35738403">[-]</label><label class="expand" for="c-35738403">[1 more]</label></div><br/><div class="children"><div class="content">So you&#x27;re saying the paper is very ambiguous and your mental network isn&#x27;t modeling it.</div><br/></div></div><div id="35738669" class="c"><input type="checkbox" id="c-35738669" checked=""/><div class="controls bullet"><span class="by">gwd</span><span>|</span><a href="#35738086">parent</a><span>|</span><a href="#35738403">prev</a><span>|</span><a href="#35737923">next</a><span>|</span><label class="collapse" for="c-35738669">[-]</label><label class="expand" for="c-35738669">[3 more]</label></div><br/><div class="children"><div class="content">I couldn&#x27;t figure out what this &quot;P: {} H: {}&quot; thing was supposed to mean; nor do what entail, neutral, or contradict mean.  Thinking they may be terms of art that the intended audience were meant to understand, I asked GPT-4 what it thought; here is its response:<p>---<p>In this context, &#x27;P&#x27; stands for &quot;Premise&quot; and &#x27;H&#x27; stands for &quot;Hypothesis.&quot; The premise is a given statement, and the hypothesis is a statement that needs to be evaluated in relation to the premise regarding its entailment, contradiction, or neutrality.<p>&#x27;NEUTRAL&#x27; means that the hypothesis is neither entailed nor contradicted by the premise. In other words, the truth of the hypothesis is independent of the premise. &#x27;CONTRADICT&#x27; means that the hypothesis contradicts the premise, meaning that if the premise is true, the hypothesis must be false.<p>In the example you provided:<p>P: I’m afraid the cat was hit by a car.
H: The cat was not hit by a car.<p>The hypothesis (H) contradicts the premise (P). The notation *NEUTRAL, CONTRADICT+ : [7 N, 2 C] indicates that among the annotators, 7 marked the relationship as neutral and 2 marked it as a contradiction.<p>---<p>Given that, I think table 2 on page 6 gives the exact prompt that&#x27;s passed to the model:<p>---<p>In each example, you will be given some context and a claim,
where the correctness of the claim is affected by some ambiguity in the context. Enumerate two or three interpretations of the context that lead to different judgments about the claim.<p>Example<p>Context: {premise}<p>Claim: {hypothesis}<p>Given the context alone, is this claim true, false, or inconclusive?<p>We don’t know, because the context can be interpreted in many
different ways:<p>1. {disambiguation 1} Then the claim is true.<p>2. {disambiguation 2} Then the claim is false.<p>3. {disambiguation 3} Then the claim is inconclusive<p>---<p>And then the model is fed the P: &#x2F; H: statements above.<p>Here are the results for the given example in GPT-4:<p>---<p>Given the context alone, is this claim true, false, or inconclusive?<p>We don&#x27;t know, because the context can be interpreted in many different ways:<p>1. The person is stating a fact: The cat was indeed hit by a car. Then the claim is false.<p>2. The person is expressing a fear or concern, but it hasn&#x27;t actually happened: They are worried that the cat might have been hit by a car, but they don&#x27;t have any evidence or confirmation. Then the claim is inconclusive.<p>3. The person is using figurative language or exaggeration: They might be using the phrase &quot;hit by a car&quot; metaphorically to describe a difficult situation the cat is in, but the cat was not literally hit by a car. Then the claim is true.<p>---<p>Given all that, it seems likely that any researcher in the field would find the paper&#x27;s methodology clear and unambiguious.  I understand the frustration as a layperson, but after all it&#x27;s not a blog post; it&#x27;s not really reasonable to expect researchers, writing to other researchers, to include a lot of redundant explanations.</div><br/><div id="35738770" class="c"><input type="checkbox" id="c-35738770" checked=""/><div class="controls bullet"><span class="by">casmaxima</span><span>|</span><a href="#35738086">root</a><span>|</span><a href="#35738669">parent</a><span>|</span><a href="#35737923">next</a><span>|</span><label class="collapse" for="c-35738770">[-]</label><label class="expand" for="c-35738770">[2 more]</label></div><br/><div class="children"><div class="content">Is maxima (computer algebra program)<p>assume(a&gt;0);
assume(b&gt;0);
is (a+b&gt;0) gives true;
is (a-b&gt;0) gives unknown.<p>So perhaps some computer algebra with inference properties can be used to enhance LLMs.</div><br/><div id="35738903" class="c"><input type="checkbox" id="c-35738903" checked=""/><div class="controls bullet"><span class="by">gwd</span><span>|</span><a href="#35738086">root</a><span>|</span><a href="#35738770">parent</a><span>|</span><a href="#35737923">next</a><span>|</span><label class="collapse" for="c-35738903">[-]</label><label class="expand" for="c-35738903">[1 more]</label></div><br/><div class="children"><div class="content">The problem here however is language.  &quot;I&#x27;m afraid the cat has been hit by a car&quot; <i>literally</i> means that the person speaking is experiencing fear.  However, colloquially in English, &quot;I&#x27;m afraid X&quot; is a form of telling someone something which will make <i>them</i> sad; e.g., &quot;I&#x27;m afraid the shields will be quite operational&quot; [1].  (GPT-4 thinks &quot;hit by a car&quot; might be meant figuratively too; that&#x27;s a bit of a stretch for me, but in some contexts would certainly be possible.) It is literally impossible to determine, given only that one sentence, whether the cat was actually hit by a car or not.<p>[1]<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=zGwDwx10wB4">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=zGwDwx10wB4</a></div><br/></div></div></div></div></div></div></div></div><div id="35737923" class="c"><input type="checkbox" id="c-35737923" checked=""/><div class="controls bullet"><span class="by">choeger</span><span>|</span><a href="#35738086">prev</a><span>|</span><a href="#35737649">next</a><span>|</span><label class="collapse" for="c-35737923">[-]</label><label class="expand" for="c-35737923">[1 more]</label></div><br/><div class="children"><div class="content">What fascinates me most about these LLMs and other machine-learning technology is how it is driven by the availability of GPUs. Essentially, everything has to be carefully encoded into - differentiable - linear algebra operations for practical reasons.<p>On the other hand, there are modeling languages (Modelica being a prime example) in search of a principled target calculus. So for me the obvious question is: Should one strive to develop a modeling language that compiles to GPU- and learning-compatible operations? Would that help to improve the models as well as understand them?</div><br/></div></div><div id="35737649" class="c"><input type="checkbox" id="c-35737649" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35737923">prev</a><span>|</span><a href="#35738745">next</a><span>|</span><label class="collapse" for="c-35737649">[-]</label><label class="expand" for="c-35737649">[1 more]</label></div><br/><div class="children"><div class="content">This is great I love seeing these new benchmarks that people are making to really challenge the strong new LLMs. In Table 4 you can see how GPT-4 is so much better than the previous GPTs, and also how it has so much room to improve on this benchmark! I&#x27;m so excited to see how the next GPT will do on it! This is different from other benchmarks like GRE verbal or biology olympiad where GPT-4 has just smashed the test getting basically every question right and for which the results of the next GPT won&#x27;t be so interesting.</div><br/></div></div><div id="35738745" class="c"><input type="checkbox" id="c-35738745" checked=""/><div class="controls bullet"><span class="by">TazeTSchnitzel</span><span>|</span><a href="#35737649">prev</a><span>|</span><a href="#35738890">next</a><span>|</span><label class="collapse" for="c-35738745">[-]</label><label class="expand" for="c-35738745">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure an LLM can model ambiguity well when talking to normal humans without having theory of mind, because different speakers will use different words (should, might, possibly, etc) in different amounts to express the same amount of ambiguity. This is the kind of thing studied in pragmatics (a subfield of linguistics). It might be easier if you&#x27;re only dealing with formal academic writing, but that doesn&#x27;t completely eliminate the issue.</div><br/><div id="35738759" class="c"><input type="checkbox" id="c-35738759" checked=""/><div class="controls bullet"><span class="by">Hendrikto</span><span>|</span><a href="#35738745">parent</a><span>|</span><a href="#35738890">next</a><span>|</span><label class="collapse" for="c-35738759">[-]</label><label class="expand" for="c-35738759">[2 more]</label></div><br/><div class="children"><div class="content">&gt; different speakers will use different words […] in different amounts to express the same amount of ambiguity.<p>That‘s covered by the training data.</div><br/><div id="35738777" class="c"><input type="checkbox" id="c-35738777" checked=""/><div class="controls bullet"><span class="by">TazeTSchnitzel</span><span>|</span><a href="#35738745">root</a><span>|</span><a href="#35738759">parent</a><span>|</span><a href="#35738890">next</a><span>|</span><label class="collapse" for="c-35738777">[-]</label><label class="expand" for="c-35738777">[1 more]</label></div><br/><div class="children"><div class="content">I suspect that real-world training data doesn&#x27;t contain enough context for LLMs to learn what kind of person they&#x27;re dealing with, and that&#x27;s assuming they even are capable of understanding context at more than a superficial level.</div><br/></div></div></div></div></div></div><div id="35738890" class="c"><input type="checkbox" id="c-35738890" checked=""/><div class="controls bullet"><span class="by">qwerty456127</span><span>|</span><a href="#35738745">prev</a><span>|</span><a href="#35738223">next</a><span>|</span><label class="collapse" for="c-35738890">[-]</label><label class="expand" for="c-35738890">[1 more]</label></div><br/><div class="children"><div class="content">Surprisingly, GPT4 almost always nails all reasonable implications and apparently understands me much better than 90% of humans I met, even though my English is not perfect.</div><br/></div></div><div id="35738223" class="c"><input type="checkbox" id="c-35738223" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#35738890">prev</a><span>|</span><a href="#35738159">next</a><span>|</span><label class="collapse" for="c-35738223">[-]</label><label class="expand" for="c-35738223">[1 more]</label></div><br/><div class="children"><div class="content">I feel like this task might be testing if LLMs can reason about ambiguity more so than if LLMs model ambiguity. To test the latter I would give it natural language tasks that are ambigious require the recognition and modelling of this ambiguity to solve them. It&#x27;s another thing to  make talking about this possibly internally modelled ambiguity the goal of the task.</div><br/></div></div><div id="35738159" class="c"><input type="checkbox" id="c-35738159" checked=""/><div class="controls bullet"><span class="by">tobr</span><span>|</span><a href="#35738223">prev</a><span>|</span><a href="#35738139">next</a><span>|</span><label class="collapse" for="c-35738159">[-]</label><label class="expand" for="c-35738159">[5 more]</label></div><br/><div class="children"><div class="content">Chef’s kiss on the title, but I’m afraid I’m not modeling ambiguity well enough myself to disambiguate it correctly.</div><br/><div id="35738348" class="c"><input type="checkbox" id="c-35738348" checked=""/><div class="controls bullet"><span class="by">gniv</span><span>|</span><a href="#35738159">parent</a><span>|</span><a href="#35738139">next</a><span>|</span><label class="collapse" for="c-35738348">[-]</label><label class="expand" for="c-35738348">[4 more]</label></div><br/><div class="children"><div class="content">Reading a bit of the paper, it seems the title is indeed meant to be tongue-in-cheek ambiguous.<p>The first example in Table 1:<p>&quot;I’m afraid the cat was hit by a car.&quot;<p>Disambiguation 1: I&#x27;m worried...<p>Disambiguation 2: I&#x27;m sorry to share that...</div><br/><div id="35738842" class="c"><input type="checkbox" id="c-35738842" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#35738159">root</a><span>|</span><a href="#35738348">parent</a><span>|</span><a href="#35738139">next</a><span>|</span><label class="collapse" for="c-35738842">[-]</label><label class="expand" for="c-35738842">[3 more]</label></div><br/><div class="children"><div class="content">Perhaps someone can ask GPT what is ambiguous about the sentence?</div><br/><div id="35738973" class="c"><input type="checkbox" id="c-35738973" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#35738159">root</a><span>|</span><a href="#35738842">parent</a><span>|</span><a href="#35738976">next</a><span>|</span><label class="collapse" for="c-35738973">[-]</label><label class="expand" for="c-35738973">[1 more]</label></div><br/><div class="children"><div class="content">The sentence &quot;I&#x27;m afraid the cat was hit by a car&quot; is not inherently ambiguous, as it clearly conveys that the speaker is regretful or apologetic about the fact that the cat was struck by a car. However, there are a few aspects that could be considered ambiguous:<p>Identity of the cat: The sentence does not specify which cat was hit, so it could refer to any cat – the speaker&#x27;s, the listener&#x27;s, or a random cat.<p>Time of the incident: The sentence does not indicate when the cat was hit by the car, so the incident could have happened recently or in the past.<p>Severity of the accident: The sentence does not describe the severity of the accident, so the reader cannot determine whether the cat survived, was injured, or was killed in the incident.<p>The speaker&#x27;s emotional state: The phrase &quot;I&#x27;m afraid&quot; could be interpreted as the speaker expressing concern or worry, but it could also simply be a polite way of conveying bad news.<p>-- GPT4</div><br/></div></div><div id="35738976" class="c"><input type="checkbox" id="c-35738976" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#35738159">root</a><span>|</span><a href="#35738842">parent</a><span>|</span><a href="#35738973">prev</a><span>|</span><a href="#35738139">next</a><span>|</span><label class="collapse" for="c-35738976">[-]</label><label class="expand" for="c-35738976">[1 more]</label></div><br/><div class="children"><div class="content">An example output from ChatGPT4 is:<p>&gt;The ambiguity in the sentence &quot;I’m afraid the cat was hit by a car&quot; is that it is not clear who is afraid. It could be the speaker who is afraid, or it could be that they are expressing sympathy or concern for someone else who is afraid. Additionally, the sentence does not specify whether the cat survived or not. &#x2F;&#x2F;<p>However, that doesn&#x27;t mean that any output that follows would be consistent with that, ChatGPT doesn&#x27;t &quot;know&quot; anything.</div><br/></div></div></div></div></div></div></div></div><div id="35738139" class="c"><input type="checkbox" id="c-35738139" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#35738159">prev</a><span>|</span><a href="#35738619">next</a><span>|</span><label class="collapse" for="c-35738139">[-]</label><label class="expand" for="c-35738139">[1 more]</label></div><br/><div class="children"><div class="content">A priori getting an LLM to recognise equivocal evidence is an interesting question. Shedloads of p-jacked data could outweigh the crucial one which says we don&#x27;t know. So it goes to things like modelling citation depth and trust and reputation.<p>I would worry well written flat earth inputs would weigh equally to simple physics &quot;that&#x27;s wrong&quot; and then you&#x27;d get to &quot;what do we know&quot; as a false signal alongside the necessary &quot;we just don&#x27;t know&quot; true signals.<p>Maybe the test is how well an LLM equivocates on things we have high certainty on like &quot;is anybody out there&quot; rather than &quot;do masks work&quot; which is a bit of a hot mess.</div><br/></div></div><div id="35738619" class="c"><input type="checkbox" id="c-35738619" checked=""/><div class="controls bullet"><span class="by">almstimplmntd</span><span>|</span><a href="#35738139">prev</a><span>|</span><a href="#35738491">next</a><span>|</span><label class="collapse" for="c-35738619">[-]</label><label class="expand" for="c-35738619">[1 more]</label></div><br/><div class="children"><div class="content">I would like to see human performance on the introduced benchmark, AmbiEnt, and have it added to Table 4.</div><br/></div></div><div id="35738491" class="c"><input type="checkbox" id="c-35738491" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#35738619">prev</a><span>|</span><a href="#35737944">next</a><span>|</span><label class="collapse" for="c-35738491">[-]</label><label class="expand" for="c-35738491">[1 more]</label></div><br/><div class="children"><div class="content">But they don&#x27;t explain why they are afraid</div><br/></div></div></div></div></div></div></div></body></html>