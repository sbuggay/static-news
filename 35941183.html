<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684141261381" as="style"/><link rel="stylesheet" href="styles.css?v=1684141261381"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chipsandcheese.com/2021/07/13/arm-or-x86-isa-doesnt-matter/">ARM or x86? ISA Doesn’t Matter (2021)</a> <span class="domain">(<a href="https://chipsandcheese.com">chipsandcheese.com</a>)</span></div><div class="subtext"><span>NavinF</span> | <span>141 comments</span></div><br/><div><div id="35941518" class="c"><input type="checkbox" id="c-35941518" checked=""/><div class="controls bullet"><span class="by">tux3</span><span>|</span><a href="#35944550">next</a><span>|</span><label class="collapse" for="c-35941518">[-]</label><label class="expand" for="c-35941518">[38 more]</label></div><br/><div class="children"><div class="content">The x86 decoders consume a reasonable amount of power, but the trouble is making them wider without affecting that.<p>I have an AMD CPU. Zen CPUs come with a fairly wide backend. But the frontend is what it is (especially early Zen), and without SMT it&#x27;s essentially impossible to keep all those execution units fed.
It&#x27;s not that 8 x86 decoders wouldn&#x27;t be a benefit, it&#x27;s just that more decoders isn&#x27;t cheap in x86 cores, each extra decoder is a serious cost.<p>If you compare with the big ARM cores, having a wide frontend is not a complex research problem or an impractical cost. 8 wide ARM decode is completely practical. You even have open source superscalar RISC-V cores just publicly available on Github running on FPGAs with 8 wide decode.
Large frontends are (relatively) cheap and easy, if you&#x27;re not x86.<p>So when we notice that the narrower x86 CPU&#x27;s decode doesn&#x27;t consume that much (a &quot;drop in the ocean&quot;), that&#x27;s because it was designed narrower to keep the PPA reasonable! The reason I can&#x27;t feed my Zen backend isn&#x27;t because having a wide frontend is useless and I should just enable SMT anyways, it&#x27;s because x86 makes wide decodes much less practical than competing architectures.</div><br/><div id="35943018" class="c"><input type="checkbox" id="c-35943018" checked=""/><div class="controls bullet"><span class="by">Stratoscope</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35941616">next</a><span>|</span><label class="collapse" for="c-35943018">[-]</label><label class="expand" for="c-35943018">[4 more]</label></div><br/><div class="children"><div class="content">Acronym&#x2F;initialism definer here. There are two in your comment that I wasn&#x27;t familiar with, although I understood them once I saw them spelled out.<p>SMT is simultaneous multithreading, which may be more familiar under Intel&#x27;s name of hyper-threading.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Simultaneous_multithreading" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Simultaneous_multithreading</a><p>PPA is not the Professional Photographers of America, nor the Professional Pickleball Association, nor the Philadelphia Parking Authority, all of which came up at the top of my naïve search.<p>It&#x27;s Power-Performance-Area.<p><a href="https:&#x2F;&#x2F;en.wikichip.org&#x2F;wiki&#x2F;power-performance-area" rel="nofollow">https:&#x2F;&#x2F;en.wikichip.org&#x2F;wiki&#x2F;power-performance-area</a></div><br/><div id="35945294" class="c"><input type="checkbox" id="c-35945294" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35943018">parent</a><span>|</span><a href="#35944671">next</a><span>|</span><label class="collapse" for="c-35945294">[-]</label><label class="expand" for="c-35945294">[1 more]</label></div><br/><div class="children"><div class="content">Hyper-threading is a misnomer imho. It should have been called &quot;poor man&#x27;s threading&quot;.</div><br/></div></div><div id="35944671" class="c"><input type="checkbox" id="c-35944671" checked=""/><div class="controls bullet"><span class="by">akvadrako</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35943018">parent</a><span>|</span><a href="#35945294">prev</a><span>|</span><a href="#35941616">next</a><span>|</span><label class="collapse" for="c-35944671">[-]</label><label class="expand" for="c-35944671">[2 more]</label></div><br/><div class="children"><div class="content">SMT is not just hyper-threading, it also includes having multiple CPUs, including sockets and cores.</div><br/><div id="35944869" class="c"><input type="checkbox" id="c-35944869" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35944671">parent</a><span>|</span><a href="#35941616">next</a><span>|</span><label class="collapse" for="c-35944869">[-]</label><label class="expand" for="c-35944869">[1 more]</label></div><br/><div class="children"><div class="content">No, that&#x27;s SMP. SMT is specifically multiple instruction pointers (threads) on a single core, i.e
 Hyperthreading</div><br/></div></div></div></div></div></div><div id="35941616" class="c"><input type="checkbox" id="c-35941616" checked=""/><div class="controls bullet"><span class="by">ip26</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35943018">prev</a><span>|</span><a href="#35942138">next</a><span>|</span><label class="collapse" for="c-35941616">[-]</label><label class="expand" for="c-35941616">[17 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a trade-off between problems. Variable length instructions are not as trivial to decode wide, so you need more cleverness here. However, fixed length instructions decrease code density, which asks more of the instruction cache. Note Zen4 has a 32 KB L1 instruction cache while the M1 has a 192 KB L1 instruction cache, requiring extra cleverness here instead to handle the higher latency and area. Meanwhile, micro-op caches hide both problems.<p>The are ripple effects to consider as well. The large L1 caches of M1 (320 KB total) put capacity pressure on L2, towards larger sizes and&#x2F;or away from inclusive policy. See the 12MB shared L2. Meanwhile, the narrower decode of Zen4 puts pressure on things like branch prediction accuracy &amp; mispredict correction latency - if you predicted the wrong codepath, you can&#x27;t catch up as quickly. See the large branch predictors on Zen4.</div><br/><div id="35941796" class="c"><input type="checkbox" id="c-35941796" checked=""/><div class="controls bullet"><span class="by">cesarb</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941616">parent</a><span>|</span><a href="#35941916">next</a><span>|</span><label class="collapse" for="c-35941796">[-]</label><label class="expand" for="c-35941796">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Note Zen4 has a 32 KB L1 instruction cache while the M1 has a 192 KB L1 instruction cache, requiring extra cleverness here instead to handle the higher latency and area.<p>There&#x27;s another factor here: to have a low latency, the L1 cache has to be indexed by the bits which don&#x27;t change when translating from virtual addresses to physical addresses. That makes it harder to have a larger low-latency L1 cache when the native page size is 4KiB (AMD&#x2F;Intel) instead of 16KiB (Apple M1&#x2F;M2).<p>That is, most of the &quot;cleverness&quot; allowing for a larger L1 instruction cache is simply a larger page size.</div><br/><div id="35943074" class="c"><input type="checkbox" id="c-35943074" checked=""/><div class="controls bullet"><span class="by">brigade</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941796">parent</a><span>|</span><a href="#35942046">next</a><span>|</span><label class="collapse" for="c-35943074">[-]</label><label class="expand" for="c-35943074">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure everyone uses a VIPT L1, what evidence is there that it&#x27;s PIPT?<p>(also what do you believe happens with 4k pages on M1, since it does support those, or why did the A7 have larger caches than Zen4 a decade ago, which was well before 16k pages)</div><br/></div></div><div id="35942046" class="c"><input type="checkbox" id="c-35942046" checked=""/><div class="controls bullet"><span class="by">mjevans</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941796">parent</a><span>|</span><a href="#35943074">prev</a><span>|</span><a href="#35942902">next</a><span>|</span><label class="collapse" for="c-35942046">[-]</label><label class="expand" for="c-35942046">[1 more]</label></div><br/><div class="children"><div class="content">If the instructions are on average 3-4x less dense (take about that much more space) than the trade-off in associativity granularity and corresponding increase in cache size are logical.  The logical management circuits would be around the same size, though the number of memory cells and corresponding cost in silicon, power &#x2F; thermal, and signal propagation &#x2F; layout issues remain.</div><br/></div></div><div id="35942902" class="c"><input type="checkbox" id="c-35942902" checked=""/><div class="controls bullet"><span class="by">rep_lodsb</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941796">parent</a><span>|</span><a href="#35942046">prev</a><span>|</span><a href="#35941916">next</a><span>|</span><label class="collapse" for="c-35942902">[-]</label><label class="expand" for="c-35942902">[1 more]</label></div><br/><div class="children"><div class="content">Since its instructions have to be aligned, ARM would also have only 12 usable address bits per page.</div><br/></div></div></div></div><div id="35941916" class="c"><input type="checkbox" id="c-35941916" checked=""/><div class="controls bullet"><span class="by">codedokode</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941616">parent</a><span>|</span><a href="#35941796">prev</a><span>|</span><a href="#35941660">next</a><span>|</span><label class="collapse" for="c-35941916">[-]</label><label class="expand" for="c-35941916">[10 more]</label></div><br/><div class="children"><div class="content">x86 also uses 2-address instructions which means that you often need to use moves between registers (additional instructions), example: [1]. ARM uses 3-address instructions.<p>Also, x86 code is compact, but not as compact as in era of 8080 [2]   - here addition and multiplication require 3 bytes each, 6 bytes total. To my surprise, ARM has an add-multiply instruction and it uses just 4 bytes (instead of 8) [3].<p>And RISC-V uses 6 bytes because of shortened instruction for addition [4]<p>Of course, this simple function cannot be a replacement for proper analysis, but it seems that x86 code is not significantly denser.<p>Also to my great disappoitment none of those CPUs has checked overflow for arithmetic operation.<p>[1] <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;jsoccE5jv" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;jsoccE5jv</a><p>[2] <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;jTMs1MEzh" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;jTMs1MEzh</a><p>[3] <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;nGb8qKcxe" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;nGb8qKcxe</a><p>[4] <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;x9c115crY" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;x9c115crY</a></div><br/><div id="35942259" class="c"><input type="checkbox" id="c-35942259" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941916">parent</a><span>|</span><a href="#35942145">next</a><span>|</span><label class="collapse" for="c-35942259">[-]</label><label class="expand" for="c-35942259">[3 more]</label></div><br/><div class="children"><div class="content"><i>Of course, this simple function cannot be a replacement for proper analysis, but it seems that x86 code is not significantly denser.</i><p>Look at the demoscene for an example of dense x86 code, especially in the sub-1K categories. They routinely achieve code densities for x86 that no compilers I know of can get close to, and AFAIK I have not seen the same happen with ARM, nor MIPS or any other well-known RISC.<p>3-address instructions improve code density only in situations where <i>both</i> source operands are needed later.</div><br/><div id="35942511" class="c"><input type="checkbox" id="c-35942511" checked=""/><div class="controls bullet"><span class="by">brigade</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35942259">parent</a><span>|</span><a href="#35942145">next</a><span>|</span><label class="collapse" for="c-35942511">[-]</label><label class="expand" for="c-35942511">[2 more]</label></div><br/><div class="children"><div class="content">What percentage of code that a CPU will run over its lifetime is demoscene code? Heck, even of just simple hand-optimized assembly a CPU is likely to encounter, what percentage is <i>not</i> vector code? Because x86 vector code typically averages <i>more</i> than 4 bytes per instruction, and I have a suspicion that at least five nines of scalar instructions a CPU executes were generated by a compiler.</div><br/><div id="35943117" class="c"><input type="checkbox" id="c-35943117" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35942511">parent</a><span>|</span><a href="#35942145">next</a><span>|</span><label class="collapse" for="c-35943117">[-]</label><label class="expand" for="c-35943117">[1 more]</label></div><br/><div class="children"><div class="content">I mention that to point out the code density limits of x86 are much higher than what measurements using compiler output will show, while on the other hand I haven&#x27;t seen the same for ARM and suspect that one can&#x27;t really get much better than compiler output for it or other RISCs.<p>Having had to patch binaries on multiple occasions by inserting instructions, it is definitely not hard to do so for x86 as one can easily find &quot;slack&quot; that the compiler left behind[1], but I once had to do it for a MIPS binary, and it was definitely not easy to squeeze in the few extra instructions I needed inline; I ended up having to detour to another area with jumps instead.<p>Here&#x27;s an old paper where the authors tried to optimise for code density manually, and you can consistently see x86 beating ARM and MIPS:<p><a href="https:&#x2F;&#x2F;web.eece.maine.edu&#x2F;~vweaver&#x2F;papers&#x2F;iccd09&#x2F;iccd09_density.pdf" rel="nofollow">https:&#x2F;&#x2F;web.eece.maine.edu&#x2F;~vweaver&#x2F;papers&#x2F;iccd09&#x2F;iccd09_den...</a><p>[1] See <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15720923" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15720923</a> for an example.</div><br/></div></div></div></div></div></div><div id="35942145" class="c"><input type="checkbox" id="c-35942145" checked=""/><div class="controls bullet"><span class="by">BeeOnRope</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941916">parent</a><span>|</span><a href="#35942259">prev</a><span>|</span><a href="#35944662">next</a><span>|</span><label class="collapse" for="c-35942145">[-]</label><label class="expand" for="c-35942145">[1 more]</label></div><br/><div class="children"><div class="content">x86-64 is not even a particularly compact encoding even compared to contemporary fixed length encodings. The inherent advantage of variable length encoding is largely cancelled out by wasted encoding space for legacy cruft.<p>Aarch64 is roughly on par for encoding density.</div><br/></div></div><div id="35944662" class="c"><input type="checkbox" id="c-35944662" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941916">parent</a><span>|</span><a href="#35942145">prev</a><span>|</span><a href="#35943355">next</a><span>|</span><label class="collapse" for="c-35944662">[-]</label><label class="expand" for="c-35944662">[1 more]</label></div><br/><div class="children"><div class="content">&gt; To my surprise, ARM has an add-multiply instruction<p>Fused multiply-add is (<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Multiply%E2%80%93accumulate_operation#Fused_multiply%E2%80%93add" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Multiply%E2%80%93accumulate_...</a>) is extremely useful and part of IEEE754-2008.<p>Because of that, it would surprise me if a modern CPU with floating point wouldn’t have it.<p>&gt; And RISC-V uses 6 bytes because of shortened instruction for addition<p>That Wikipedia page claims RISC-V has fused multiply-add, so that may be compiler inefficiency. An answer to <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;57248403&#x2F;why-fused-multiply-add-operation-is-not-used" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;57248403&#x2F;why-fused-multi...</a> says gcc can generate it at <i>-O3</i>)</div><br/></div></div><div id="35943355" class="c"><input type="checkbox" id="c-35943355" checked=""/><div class="controls bullet"><span class="by">drpixie</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941916">parent</a><span>|</span><a href="#35944662">prev</a><span>|</span><a href="#35942257">next</a><span>|</span><label class="collapse" for="c-35943355">[-]</label><label class="expand" for="c-35943355">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Also to my great disappoitment none of those CPUs has checked overflow for arithmetic operation.<p>Yes - we&#x27;re stuck with the historically convenient collection of operations and types. Our CPUs are amazingly fast but very little smarter than decades ago.<p>Backwards-compatibility pretty much locks us into the same logical space. At least when people were designing new and amazing computers using actual wire (!) they felt free to implement what they thought would be useful, not just copy the existing model.</div><br/><div id="35945188" class="c"><input type="checkbox" id="c-35945188" checked=""/><div class="controls bullet"><span class="by">renox</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35943355">parent</a><span>|</span><a href="#35942257">next</a><span>|</span><label class="collapse" for="c-35945188">[-]</label><label class="expand" for="c-35945188">[1 more]</label></div><br/><div class="children"><div class="content">The only CPU I know which had integer overflow detection was MIPS. RISC V is somewhat based on MIPS but they removed this part :-(</div><br/></div></div></div></div><div id="35942257" class="c"><input type="checkbox" id="c-35942257" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941916">parent</a><span>|</span><a href="#35943355">prev</a><span>|</span><a href="#35941660">next</a><span>|</span><label class="collapse" for="c-35942257">[-]</label><label class="expand" for="c-35942257">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s incredible how large the code is when you compile some trivial piece of C with GCC for x86 or amd64, compared to one&#x27;s memory of working with C on 386 boxes.<p>What? I just wrote a simple main and a couple of small helper functions. How can we be past 20 kB of .text section?</div><br/><div id="35944594" class="c"><input type="checkbox" id="c-35944594" checked=""/><div class="controls bullet"><span class="by">throwaway2037</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35942257">parent</a><span>|</span><a href="#35941660">next</a><span>|</span><label class="collapse" for="c-35944594">[-]</label><label class="expand" for="c-35944594">[1 more]</label></div><br/><div class="children"><div class="content">Can you share a sample with Godbolt?  It might foster some good discussion here.</div><br/></div></div></div></div></div></div><div id="35941660" class="c"><input type="checkbox" id="c-35941660" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941616">parent</a><span>|</span><a href="#35941916">prev</a><span>|</span><a href="#35943277">next</a><span>|</span><label class="collapse" for="c-35941660">[-]</label><label class="expand" for="c-35941660">[1 more]</label></div><br/><div class="children"><div class="content">ARM used to find itself on the wrong side of this tradeoff in the era of 4-wide x86 decode units and 4-6 wide ARM decoders. They lost too much perf to cache size for the decoder width to make up for it.<p>It&#x27;s unclear to me if they will pull ahead on the perf&#x2F;area game with the era of 8-wide x86 decoders coming.</div><br/></div></div><div id="35943277" class="c"><input type="checkbox" id="c-35943277" checked=""/><div class="controls bullet"><span class="by">crest</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941616">parent</a><span>|</span><a href="#35941660">prev</a><span>|</span><a href="#35942138">next</a><span>|</span><label class="collapse" for="c-35943277">[-]</label><label class="expand" for="c-35943277">[1 more]</label></div><br/><div class="children"><div class="content">The M1 uses 16kiB pages to reduce TLB pressure and get more non-aliasing cache tag bits.</div><br/></div></div></div></div><div id="35942138" class="c"><input type="checkbox" id="c-35942138" checked=""/><div class="controls bullet"><span class="by">BeeOnRope</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35941616">prev</a><span>|</span><a href="#35942951">next</a><span>|</span><label class="collapse" for="c-35942138">[-]</label><label class="expand" for="c-35942138">[1 more]</label></div><br/><div class="children"><div class="content">Having a wide backend which cannot be fully utilized is common in non-x86 chips as well. The backend units are specialized while the front end less so, so to sustain the front-end bandwidth for many instruction mixed you need a wider backend so you have enough units of the <i>right type</i>.<p>Wide x86 decode definitely has a cost, but I don&#x27;t think it&#x27;s the primary limiter as you make it out to be: for several x86 generations the narrowest bottleneck has been the renamer which has crept up from 3 to 6 in a glacial manner. Admittedly rename is also complicated on x86 due to memory source&#x2F;destinations, 3 input instructions, 2 output instructions, etc.</div><br/></div></div><div id="35942951" class="c"><input type="checkbox" id="c-35942951" checked=""/><div class="controls bullet"><span class="by">adfgionionio</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35942138">prev</a><span>|</span><a href="#35941623">next</a><span>|</span><label class="collapse" for="c-35942951">[-]</label><label class="expand" for="c-35942951">[1 more]</label></div><br/><div class="children"><div class="content">The decoders are largely irrelevant. All modern x86 machines use a uop cache. The large majority of instructions hit in this cache and do not need to hit the decoders at all. As a result, the decoders can spend much of their time shut down. You already have four-ish decoders that are idle most of the time; why do you want eight-ish decoders that are idle nearly all the time?<p>No one would design an ISA like x86 these days. It definitely does use more power and more die area than strictly needed. It definitely does reduce performance in some applications. It definitely did take heroic engineering efforts to make x86 work well. But, all told, it just doesn&#x27;t matter very much.</div><br/></div></div><div id="35941623" class="c"><input type="checkbox" id="c-35941623" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35942951">prev</a><span>|</span><a href="#35943510">next</a><span>|</span><label class="collapse" for="c-35941623">[-]</label><label class="expand" for="c-35941623">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; I have an AMD CPU. Zen CPUs come with a fairly wide backend. But the frontend is what it is...<p>Zen 5 is widening the front end. My guess is with scaling coming to an end, one nice tweak with Zen 6 should be darn near the end of the performance road for a bit. Not saying the actual end, but it should be one of those sweet spots where you build a PC and it&#x27;s really good for years to come.<p>I&#x27;m still running Raven Ridge and have no need to upgrade, but I will when I can get double the cores or more at double the IPC or more, and maybe at lower power ;-)</div><br/></div></div><div id="35943510" class="c"><input type="checkbox" id="c-35943510" checked=""/><div class="controls bullet"><span class="by">13of40</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35941623">prev</a><span>|</span><a href="#35941970">next</a><span>|</span><label class="collapse" for="c-35943510">[-]</label><label class="expand" for="c-35943510">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure someone has thought of this before, but there&#x27;s x86 as the full ISA, and there&#x27;s x86 as gcc (or whatever) actually uses it.  What if you had one or two cores that actually run x86 and the rest just run a reasonable subset of it until they hit a weird instruction and hand the thread over to the more formal one?</div><br/><div id="35943639" class="c"><input type="checkbox" id="c-35943639" checked=""/><div class="controls bullet"><span class="by">StressedDev</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35943510">parent</a><span>|</span><a href="#35941970">next</a><span>|</span><label class="collapse" for="c-35943639">[-]</label><label class="expand" for="c-35943639">[2 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need special cores.  One thing a lot of people do not seem to realize is compilers do not use the full instruction set.  They use a subset and this subset is what has to be fast.  Here are the common x86 instructions I remember when I debugged disassembled source code (1).<p>- mov (an instruction to load, store data)
- Basic arithmetic instructions (add, sub, div, mul, etc.)
- Bitwise operators (or, and, xor, shift, etc.)
- Compare (cmp)
- Branch (jz, jnz, j, ret, etc.)
- There was one weird instruction I would occasionally see.  It was used to zero memory (I forget its name).  My guess is the compiler used it because it was faster than running creating a loop which zeroed memory.<p>The point is, people seem to assume ALL instructions have to be fast.  The only instructions which have to be fast are the ones the compilers are using.  The rest just have to work.<p>Making the rarely used instructions work probably takes up a negligible amount of space on a typical core.  Moving these instructions to a special core would not get you much but would make old software run slower (becuase of the context switch to the other core and&#x2F;or a limited number of cores it could run on).<p>Note that I know AMD and Intel both have a lot of guidance for compiler writers.<p>(1) I debugged disabled code because I was debugging optimized code and it was the only way to reliable determine why a function crashed or malfunctioned.</div><br/><div id="35945154" class="c"><input type="checkbox" id="c-35945154" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35943639">parent</a><span>|</span><a href="#35941970">next</a><span>|</span><label class="collapse" for="c-35945154">[-]</label><label class="expand" for="c-35945154">[1 more]</label></div><br/><div class="children"><div class="content">&gt; One thing a lot of people do not seem to realize is compilers do not use the full instruction set. They use a subset and this subset is what has to be fast.<p>That’s a self-fulfilling prophecy. Compiler writers won’t use the instructions that are (relatively) slow. If, tomorrow, Intel makes ‘add’ slow, compilers will eventually start compiling addition as negation + subtraction (and AMD will become more popular)<p>That’s what makes hardware designers unhappy sometimes. If they ship a shiny faster CPU that requires a large overhaul of compilers, it can take time for compiler writers to catch up.<p>Hardware designers shouldn’t (only) aim for getting closer to the local optimum of where compiler writers are, but also for the higher peaks of where they could be.</div><br/></div></div></div></div></div></div><div id="35941970" class="c"><input type="checkbox" id="c-35941970" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35943510">prev</a><span>|</span><a href="#35943772">next</a><span>|</span><label class="collapse" for="c-35941970">[-]</label><label class="expand" for="c-35941970">[3 more]</label></div><br/><div class="children"><div class="content">&gt;The x86 decoders consume a reasonable amount of power<p>this article states otherwise, so how is it?</div><br/><div id="35942343" class="c"><input type="checkbox" id="c-35942343" checked=""/><div class="controls bullet"><span class="by">LegionMammal978</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35941970">parent</a><span>|</span><a href="#35943772">next</a><span>|</span><label class="collapse" for="c-35942343">[-]</label><label class="expand" for="c-35942343">[2 more]</label></div><br/><div class="children"><div class="content">I think the intent here is to say that decoders&#x27; current power consumption is reasonable (i.e., within reason), but widening them could make their power consumption <i>un</i>reasonable (i.e., too high).</div><br/><div id="35943149" class="c"><input type="checkbox" id="c-35943149" checked=""/><div class="controls bullet"><span class="by">TinkersW</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35942343">parent</a><span>|</span><a href="#35943772">next</a><span>|</span><label class="collapse" for="c-35943149">[-]</label><label class="expand" for="c-35943149">[1 more]</label></div><br/><div class="children"><div class="content">Maybe but maybe not.. the E cores on Raptorlake have two 3 wide decoders, apparently it can decode 6 instructions this way(though how often or reliably that is I don&#x27;t think Intel has said).</div><br/></div></div></div></div></div></div><div id="35943772" class="c"><input type="checkbox" id="c-35943772" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35941970">prev</a><span>|</span><a href="#35942727">next</a><span>|</span><label class="collapse" for="c-35943772">[-]</label><label class="expand" for="c-35943772">[2 more]</label></div><br/><div class="children"><div class="content">Do you have references about AMD CPUs being more frequently bottlenecked by instruction decode despite uop caches (vs non x86)?</div><br/><div id="35945078" class="c"><input type="checkbox" id="c-35945078" checked=""/><div class="controls bullet"><span class="by">tux3</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35943772">parent</a><span>|</span><a href="#35942727">next</a><span>|</span><label class="collapse" for="c-35945078">[-]</label><label class="expand" for="c-35945078">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have any good benchmark to link to off the top of my head, but I&#x27;ll handwave in the general direction of Agner Fog&#x27;s guide, which is accurate in my experience (and generally a great resource): <a href="https:&#x2F;&#x2F;www.agner.org&#x2F;optimize&#x2F;microarchitecture.pdf" rel="nofollow">https:&#x2F;&#x2F;www.agner.org&#x2F;optimize&#x2F;microarchitecture.pdf</a><p>From the multiple &quot;Bottlenecks in AMD Zen&quot; sections, a common point is<p>&gt;The limiting fetch rate of up to 16 bytes per clock is a very likely bottleneck for CPU-
intensive code with large loops<p>Although admittedly, if your small hot loops fit in the uop cache that does largely mitigate the fetch&#x2F;decode problem</div><br/></div></div></div></div><div id="35942727" class="c"><input type="checkbox" id="c-35942727" checked=""/><div class="controls bullet"><span class="by">bigdict</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35943772">prev</a><span>|</span><a href="#35943199">next</a><span>|</span><label class="collapse" for="c-35942727">[-]</label><label class="expand" for="c-35942727">[3 more]</label></div><br/><div class="children"><div class="content">Is that because of ISA complexity? Especially with x86 growth by accretion while keeping backward compatibility all the way to 16-bit.<p>So you could conceivably afford a wide frontend if you restricted x86 to a subset (64 bit only, drop a bunch of weird CISC&#x27;y instructions).</div><br/><div id="35942815" class="c"><input type="checkbox" id="c-35942815" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35942727">parent</a><span>|</span><a href="#35943199">next</a><span>|</span><label class="collapse" for="c-35942815">[-]</label><label class="expand" for="c-35942815">[2 more]</label></div><br/><div class="children"><div class="content">It’s mainly (I think) the distribution of instruction lengths. x86 instructions have any length from 1 to 15 bytes.  A decoder wants to decode multiple instructions per cycle, and it generally does this in parallel, by simultaneously deciding at multiple starting points. With a fixed length ISA, to decode n instructions, you just decode them. With x86, if you simultaneously decode at offset 0, 1, …, 7, you have 8 decoders but are only likely to decode a couple of correct instructions.  The rest start in the middle of an instruction and need to be discarded.  So you either need many more parallel decoders for the same throughput or a more complex system to try to avoid throwing away so much work.</div><br/><div id="35944418" class="c"><input type="checkbox" id="c-35944418" checked=""/><div class="controls bullet"><span class="by">jabl</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35942815">parent</a><span>|</span><a href="#35943199">next</a><span>|</span><label class="collapse" for="c-35944418">[-]</label><label class="expand" for="c-35944418">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ... or a more complex system to try to avoid throwing away so much work.<p>IIRC Jim Keller said in some interview that modern x86 uses prediction and speculation (similar to branch prediction), and it works surprisingly well.</div><br/></div></div></div></div></div></div><div id="35943199" class="c"><input type="checkbox" id="c-35943199" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#35941518">parent</a><span>|</span><a href="#35942727">prev</a><span>|</span><a href="#35944550">next</a><span>|</span><label class="collapse" for="c-35943199">[-]</label><label class="expand" for="c-35943199">[2 more]</label></div><br/><div class="children"><div class="content">&gt;You even have open source superscalar RISC-V cores just publicly available on Github running on FPGAs with 8 wide decode. Large frontends are (relatively) cheap and easy, if you&#x27;re not x86.<p>Which one? I know BOOM can technically go eight wide in so far as it&#x27;s parametrizable but I suspect any BOOM backend which could support that much throughout would be a nightmare to instantiate on nearly any FPGA.</div><br/><div id="35944643" class="c"><input type="checkbox" id="c-35944643" checked=""/><div class="controls bullet"><span class="by">tux3</span><span>|</span><a href="#35941518">root</a><span>|</span><a href="#35943199">parent</a><span>|</span><a href="#35944550">next</a><span>|</span><label class="collapse" for="c-35944643">[-]</label><label class="expand" for="c-35944643">[1 more]</label></div><br/><div class="children"><div class="content">I had VROOM! in mind (<a href="https:&#x2F;&#x2F;github.com&#x2F;MoonbaseOtago&#x2F;vroom">https:&#x2F;&#x2F;github.com&#x2F;MoonbaseOtago&#x2F;vroom</a>) because I remembered it aims for 4 IPC avg with a width of 8. Though looking again it&#x27;s 8 compressed 16 bit instructions or 4 uncompressed 32 bit instruction.<p>So you could argue a real mix of instructions is not going to be all 16 bit but some 16 and some 32, so the 8 is rarely achieved in practice, and also the block diagram only shows 4 decode blocks. But it can in fact peak at 8 instructions decoded per clock, so I&#x27;ll call that 8 wide decode.<p>(You could even argue it&#x27;s especially impressive, since RISC-V technically qualifies as variable-length encoding like x86, it&#x27;s just that only the 16&#x2F;32 instructions encoding are really in use at the moment)</div><br/></div></div></div></div></div></div><div id="35944550" class="c"><input type="checkbox" id="c-35944550" checked=""/><div class="controls bullet"><span class="by">abainbridge</span><span>|</span><a href="#35941518">prev</a><span>|</span><a href="#35942914">next</a><span>|</span><label class="collapse" for="c-35944550">[-]</label><label class="expand" for="c-35944550">[3 more]</label></div><br/><div class="children"><div class="content">edit: The emphasis is wrong here. The main benefit of a relaxed memory model is that it reduces the amount of work the memory subsystem HW needs to do.<p>Another ISA difference not discussed by the article is the memory model. I expect that the memory model and the degree of reordering permitted by the architecture could have a significant impact on performance.<p>x86 imposes stricter ordering constraints on memory accesses. The choice of this memory model was made long ago, when, I guess, it was felt that this made the behaviour easier to understand for the programmer.<p>In contrast, ARM&#x27;s memory model permits more reordering of memory accesses, which can lead to potential data races and inconsistent program behavior. However, this greater flexibility can also lead to higher performance, as the processor can execute memory accesses in a more efficient manner by overlapping and reordering them.<p>I can&#x27;t find any studies that measure the impact of this, but I&#x27;d be surprised if it wasn&#x27;t a significant win for the ARM ISA in many programs.</div><br/><div id="35944692" class="c"><input type="checkbox" id="c-35944692" checked=""/><div class="controls bullet"><span class="by">tialaramex</span><span>|</span><a href="#35944550">parent</a><span>|</span><a href="#35942914">next</a><span>|</span><label class="collapse" for="c-35944692">[-]</label><label class="expand" for="c-35944692">[2 more]</label></div><br/><div class="children"><div class="content">For languages like C++ and Rust the memory model is part of the language and so is taken into consideration by the compiler backend too. Code that&#x27;s explicitly asking for Relaxed (the fastest ordering, which has no benefit on x86) is fairly rare.</div><br/><div id="35945286" class="c"><input type="checkbox" id="c-35945286" checked=""/><div class="controls bullet"><span class="by">abainbridge</span><span>|</span><a href="#35944550">root</a><span>|</span><a href="#35944692">parent</a><span>|</span><a href="#35942914">next</a><span>|</span><label class="collapse" for="c-35945286">[-]</label><label class="expand" for="c-35945286">[1 more]</label></div><br/><div class="children"><div class="content">Hmmm. Fair point. I got this argument from a HW engineer I know. You forced me to think more about what he was saying. His point was actually that the TSO model requires the HW to keep track of more stuff. More stuff uses more energy and adds latency to every memory access. He points out that ARM, RISC-V and even ia64 (Itanium) chose the relaxed model because it is better.</div><br/></div></div></div></div></div></div><div id="35942914" class="c"><input type="checkbox" id="c-35942914" checked=""/><div class="controls bullet"><span class="by">drpixie</span><span>|</span><a href="#35944550">prev</a><span>|</span><a href="#35942288">next</a><span>|</span><label class="collapse" for="c-35942914">[-]</label><label class="expand" for="c-35942914">[3 more]</label></div><br/><div class="children"><div class="content">This RISC&#x2F;CISC debate made a lot of sense when the &quot;guts&quot; of the CPU (decoder, registers, ALU, etc) occupied almost the whole chip (or box of transistors), and when memory and CPU had similar speeds.<p>The situation now is very different. The vast majority of the chip is cache. The trade-off for a much bigger decoder (or register stack, or whatever) is now just a fractionally smaller cache.<p>And on current systems the CPU and memory operate at enormously different speeds - memory is 10+ times slower than the CPU. So to keep the CPU even vaguely busy, we have resorted to enormous caches. And we use large numbers of cores &amp; threads, so that when one thread is waiting for memory, others may be able to run.<p>The old game was max performance from a limited number of gates. The current situation makes such enormous numbers of gates available (even on tiny, cheap chips) that we&#x27;re playing an altogether different game.</div><br/><div id="35944180" class="c"><input type="checkbox" id="c-35944180" checked=""/><div class="controls bullet"><span class="by">branko_d</span><span>|</span><a href="#35942914">parent</a><span>|</span><a href="#35943438">next</a><span>|</span><label class="collapse" for="c-35944180">[-]</label><label class="expand" for="c-35944180">[1 more]</label></div><br/><div class="children"><div class="content">&gt; memory is 10+ times slower than the CPU<p>The difference is much greater than that - one uncached RAM access can take hundreds of CPU cycles.</div><br/></div></div><div id="35943438" class="c"><input type="checkbox" id="c-35943438" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#35942914">parent</a><span>|</span><a href="#35944180">prev</a><span>|</span><a href="#35942288">next</a><span>|</span><label class="collapse" for="c-35943438">[-]</label><label class="expand" for="c-35943438">[1 more]</label></div><br/><div class="children"><div class="content">While area is now very cheap, power and thermal is more of an issue than ever. Huge d&#x2F;i caches are doable since they save power, huge arrays of decoders not so much. The CISC-RISC debate wasn&#x27;t just based on area; everyone back then also saw the direction that scaling was taking us.</div><br/></div></div></div></div><div id="35942288" class="c"><input type="checkbox" id="c-35942288" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#35942914">prev</a><span>|</span><a href="#35941656">next</a><span>|</span><label class="collapse" for="c-35942288">[-]</label><label class="expand" for="c-35942288">[17 more]</label></div><br/><div class="children"><div class="content">I wonder where we’d be if the idea of CPU-independent bytecode had ever really taken off - for example, the TIMI bytecode of IBM System&#x2F;38 and AS&#x2F;400, TenDRA TDF (aka OSF ANDF), WebAssembly. You could have an AOT compiler in the system firmware which the OS invokes, when a program is installed the OS uses AOT to convert it to the actual machine code, about which the OS might know nothing, and could vary incompatibly from CPU to CPU, even among different CPU models in the same family. (Maybe a JIT mode too.)<p>I guess JVM&#x2F;CIL are somewhat similar, but at a much higher level - I’m not talking about garbage collection or type safety.<p>In some ways that is true of TIMI too - it is designed to support a capability-based operating system, and hence has some rather high-level instructions, although still not as high level as JVM&#x2F;CIL - it was generally used as a compilation target for non-garbage collected languages such as RPG, COBOL, C&#x2F;C++, PL&#x2F;I, Fortran, BASIC, Pascal, etc - and hence lacks a garbage collector.</div><br/><div id="35945019" class="c"><input type="checkbox" id="c-35945019" checked=""/><div class="controls bullet"><span class="by">mananaysiempre</span><span>|</span><a href="#35942288">parent</a><span>|</span><a href="#35942562">next</a><span>|</span><label class="collapse" for="c-35945019">[-]</label><label class="expand" for="c-35945019">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I wonder where we’d be if the idea of CPU-independent bytecode had ever really taken off [...]. You could have an AOT compiler in the system firmware which the OS invokes [...].<p>You could say modern CPUs are kind of like tracing JITs. On one hand, a normal tracing JIT has much more memory to save its work than a CPU’s trace cache, but on the other, the superscalar reordering and renaming stuff is even more aggressive than a trace recorder about looking at how the code actually executes and deriving assumptions from that instead of attempting to prove them statically.<p>Why not AOT instead? In part because they can’t, of course—a tracing JIT requires about the least amount of heavyweight compiler tech out of all the possibilities, which is an advantage if you’re trying to fit the compiler into silicon. (That’s not to say a tracing JIT is easy—the cost of a simple compiler is that you need to make it hella fast for the result to be any good.)<p>But in part I suspect it’s because a standard assembly-level bytecode kind of sucks to compile ahead of time. About the most useful assumptions such a compiler can make is which things don’t interfere with others, usually memory operations, or perhaps which writes can be forwarded to reads. A tracing JIT can see some of this, a superscalar even more so; an AOT or function-at-a-time JIT, in the absence of any aliasing information or even knowing when one object ends and another begins (boo WebAssembly), can’t.<p>Ironically, memory segmentation as in the Intel 432 or 286 (or the IBM dinosaurs) feels like could help with that (or are we calling this idea “capability-based” once again?). Does anyone who isn’t just a speculating dilettante (unlike me) think that’s a reasonable thought?<p>(Wait, is a selector table just a Smalltalk-style object table with a fake moustache?)<p>Of course, even then we’d still have the problem that VLIW microcode wide enough to require no decoding and engage the entirety of a modern CPU’s physical register file and execution units would be cripplingly slow to fetch from DRAM, and the “legacy” ISAs partly serve a compression format.</div><br/></div></div><div id="35942562" class="c"><input type="checkbox" id="c-35942562" checked=""/><div class="controls bullet"><span class="by">tadfisher</span><span>|</span><a href="#35942288">parent</a><span>|</span><a href="#35945019">prev</a><span>|</span><a href="#35944438">next</a><span>|</span><label class="collapse" for="c-35942562">[-]</label><label class="expand" for="c-35942562">[4 more]</label></div><br/><div class="children"><div class="content">Transmeta Crusoe? They chose X86 machine code as their CPU-independent bytecode.</div><br/><div id="35942730" class="c"><input type="checkbox" id="c-35942730" checked=""/><div class="controls bullet"><span class="by">zdw</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35942562">parent</a><span>|</span><a href="#35942741">next</a><span>|</span><label class="collapse" for="c-35942730">[-]</label><label class="expand" for="c-35942730">[1 more]</label></div><br/><div class="children"><div class="content">In demos the Transmeta processors was shown to support multiple instruction sets - per <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transmeta#Code_Morphing_Software" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transmeta#Code_Morphing_Softwa...</a> , they demoed pico-Java , and also there were rumors of PowerPC compatibility.<p>Although you&#x27;re probably right - none of those options made it into a shipping product, only x86.</div><br/></div></div><div id="35942741" class="c"><input type="checkbox" id="c-35942741" checked=""/><div class="controls bullet"><span class="by">fathyb</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35942562">parent</a><span>|</span><a href="#35942730">prev</a><span>|</span><a href="#35944438">next</a><span>|</span><label class="collapse" for="c-35942741">[-]</label><label class="expand" for="c-35942741">[2 more]</label></div><br/><div class="children"><div class="content">I guess today&#x27;s equivalent VLIW chip would be Tachyum Prodigy, not super confident about it.. <a href="https:&#x2F;&#x2F;www.tachyum.com&#x2F;products&#x2F;#products-prodigy" rel="nofollow">https:&#x2F;&#x2F;www.tachyum.com&#x2F;products&#x2F;#products-prodigy</a></div><br/><div id="35942876" class="c"><input type="checkbox" id="c-35942876" checked=""/><div class="controls bullet"><span class="by">hurpdurpdurp</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35942741">parent</a><span>|</span><a href="#35944438">next</a><span>|</span><label class="collapse" for="c-35942876">[-]</label><label class="expand" for="c-35942876">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s denver2 cores work this way. Shipped on an android tablet about 10 years ago. Not sure what happened to them after that.</div><br/></div></div></div></div></div></div><div id="35944438" class="c"><input type="checkbox" id="c-35944438" checked=""/><div class="controls bullet"><span class="by">jabl</span><span>|</span><a href="#35942288">parent</a><span>|</span><a href="#35942562">prev</a><span>|</span><a href="#35944647">next</a><span>|</span><label class="collapse" for="c-35944438">[-]</label><label class="expand" for="c-35944438">[2 more]</label></div><br/><div class="children"><div class="content">Mill uses something similar as well. That being said, I have &lt;1% confidence in Mill ever moving past the slideware stage, so...</div><br/><div id="35944610" class="c"><input type="checkbox" id="c-35944610" checked=""/><div class="controls bullet"><span class="by">throwaway2037</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35944438">parent</a><span>|</span><a href="#35944647">next</a><span>|</span><label class="collapse" for="c-35944610">[-]</label><label class="expand" for="c-35944610">[1 more]</label></div><br/><div class="children"><div class="content">&quot;slideware&quot;: Hat tip.  I never saw that term before.  I usually see vaporware, e.g., Duke Nukem Forever.</div><br/></div></div></div></div><div id="35944647" class="c"><input type="checkbox" id="c-35944647" checked=""/><div class="controls bullet"><span class="by">leosarev</span><span>|</span><a href="#35942288">parent</a><span>|</span><a href="#35944438">prev</a><span>|</span><a href="#35942735">next</a><span>|</span><label class="collapse" for="c-35944647">[-]</label><label class="expand" for="c-35944647">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know about JVM, but CIL doesn&#x27;t have notion of type safety, it&#x27;s has been checked by compilator&#x2F;verificator and doesn&#x27;t enforced in runtime</div><br/></div></div><div id="35942735" class="c"><input type="checkbox" id="c-35942735" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#35942288">parent</a><span>|</span><a href="#35944647">prev</a><span>|</span><a href="#35943456">next</a><span>|</span><label class="collapse" for="c-35942735">[-]</label><label class="expand" for="c-35942735">[3 more]</label></div><br/><div class="children"><div class="content">Given that everything is just microcode anyway, it would be really interesting if some (ex Intel) took their design and only switched out the instruction decode to decode ARM (or whatever) instead.<p>Sure it wouldn’t be perfect since the chip is optimized based on x86-64 workloads, and they’d never publish it anyway. Plus it may only be simulated instead of spending the money on manufacturing the one-offs.<p>But boy would it be interesting to see how it performed in various dimensions, just as an exercise.</div><br/><div id="35943324" class="c"><input type="checkbox" id="c-35943324" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35942735">parent</a><span>|</span><a href="#35943456">next</a><span>|</span><label class="collapse" for="c-35943324">[-]</label><label class="expand" for="c-35943324">[2 more]</label></div><br/><div class="children"><div class="content"><i>Given that everything is just microcode anyway, it would be really interesting if some (ex Intel) took their design and only switched out the instruction decode to decode ARM (or whatever) instead.</i><p>You probably mean uops, but that thought has also crossed my mind in the past --- a multi-ISA CPU. They could add the decoders for other ISAs, along with extra GDT descriptor types for &quot;ARM mode&quot;, &quot;RISC-V mode&quot;, etc. segments like they did with V86. It&#x27;s not a new idea either, <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;NEC_V30#ISA_extensions" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;NEC_V30#ISA_extensions</a> could execute both x86 and 8080 code and of course ARM has cores with the triple-mode ARM32&#x2F;Thumb&#x2F;Aarch64 ISAs.</div><br/><div id="35943614" class="c"><input type="checkbox" id="c-35943614" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35943324">parent</a><span>|</span><a href="#35943456">next</a><span>|</span><label class="collapse" for="c-35943614">[-]</label><label class="expand" for="c-35943614">[1 more]</label></div><br/><div class="children"><div class="content">Yes I did, thanks. It’s also kind of reminiscent of the Transmeta Crusoe.<p>The problem I think multi-ISA would run into is the “master of none” issue. Intel can tune for how x86-64 works, Apple and Samsung for ARM.<p>But if one chip runs it all, it can’t tune for anything too specific.<p>It must not be worth it. I wonder if Apple would have done something like that for the M series to let it keep running Intel software. They must have tried to figure out if it was worth it right? I know they added a few instructions or an addressing mode or something to help. But they must have determined it wasn’t worth it and it could be done well enough in software.</div><br/></div></div></div></div></div></div><div id="35943456" class="c"><input type="checkbox" id="c-35943456" checked=""/><div class="controls bullet"><span class="by">drpixie</span><span>|</span><a href="#35942288">parent</a><span>|</span><a href="#35942735">prev</a><span>|</span><a href="#35942617">next</a><span>|</span><label class="collapse" for="c-35943456">[-]</label><label class="expand" for="c-35943456">[1 more]</label></div><br/><div class="children"><div class="content">(Turns to phone booth, ripping off tie) &quot;Sounds like a job for super Forth!&quot;</div><br/></div></div><div id="35942617" class="c"><input type="checkbox" id="c-35942617" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#35942288">parent</a><span>|</span><a href="#35943456">prev</a><span>|</span><a href="#35941656">next</a><span>|</span><label class="collapse" for="c-35942617">[-]</label><label class="expand" for="c-35942617">[4 more]</label></div><br/><div class="children"><div class="content">Seems irrelevant though - the internet exists. I&#x27;ll never have a problem getting the code I need, provided it exists - i.e. if all I have to do to support ARM is use the ARM compiler, then I&#x27;ll support ARM.<p>Docker with ARM-specific Linux distributions solves this, as does things like Golang with it&#x27;s &quot;just set an environment variable and don&#x27;t even worry about needing a cross-compiler&quot; toolchain.</div><br/><div id="35942726" class="c"><input type="checkbox" id="c-35942726" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35942617">parent</a><span>|</span><a href="#35943394">next</a><span>|</span><label class="collapse" for="c-35942726">[-]</label><label class="expand" for="c-35942726">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Seems irrelevant though - the internet exists. I&#x27;ll never have a problem getting the code I need, provided it exists<p>That assumes all code is open source, or else proprietary code shipped with source. That&#x27;s not the world we live in. Most businesses run at least some closed source on-premise software. Open source is great at providing solutions to problems most people have. But when you start looking at specialised software which is highly industry-specific, suddenly open source starts to look a lot more patchy, and a closed source solution is often the only realistic option.<p>For example, at many engineering firms (whatever type of engineering they may be doing), you will find heaps of closed source software being used every day. For much of it, there simply is no open source solution available – or if there is, it is missing major features, or is clunky&#x2F;buggy&#x2F;poorly-designed, and the amount of extra cost in adopting it will be a lot more than just continuing to pay for the closed source alternative.</div><br/><div id="35943083" class="c"><input type="checkbox" id="c-35943083" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35942726">parent</a><span>|</span><a href="#35943394">next</a><span>|</span><label class="collapse" for="c-35943083">[-]</label><label class="expand" for="c-35943083">[1 more]</label></div><br/><div class="children"><div class="content">I agree with this - but my point is that I think the <i>difficulty</i> of compiling to alternative architectures is more of an impediment. If it&#x27;s easy, then company&#x27;s will just do it, give or take &quot;we don&#x27;t want to support that platform&quot;.</div><br/></div></div></div></div><div id="35943394" class="c"><input type="checkbox" id="c-35943394" checked=""/><div class="controls bullet"><span class="by">drpixie</span><span>|</span><a href="#35942288">root</a><span>|</span><a href="#35942617">parent</a><span>|</span><a href="#35942726">prev</a><span>|</span><a href="#35941656">next</a><span>|</span><label class="collapse" for="c-35943394">[-]</label><label class="expand" for="c-35943394">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Seems irrelevant though - the internet exists.<p>... but it&#x27;s not <i>always</i> useful. MSI motherboards doing &quot;secure boot&quot; can&#x27;t check for key revocation until after they&#x27;ve booted :( Sometimes you just have to rely on what you&#x27;ve got.<p><a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2023&#x2F;05&#x2F;leak-of-msi-uefi-signing-keys-stokes-concerns-of-doomsday-supply-chain-attack&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2023&#x2F;05&#x2F;leak-...</a></div><br/></div></div></div></div></div></div><div id="35941656" class="c"><input type="checkbox" id="c-35941656" checked=""/><div class="controls bullet"><span class="by">TheLoafOfBread</span><span>|</span><a href="#35942288">prev</a><span>|</span><a href="#35941435">next</a><span>|</span><label class="collapse" for="c-35941656">[-]</label><label class="expand" for="c-35941656">[13 more]</label></div><br/><div class="children"><div class="content">What does matter is standardization. For example a booting process. When I have x86 image of Windows&#x2F;Linux, I can boot it on any x86 processor. When I have ARM image, well, then I can boot it on a SoC it is built for and that&#x27;s big maybe because if outside peripherals are different (i.e. different LCD driver) or lives on different pins of SoC, then I am screwed and will have at best partially working system.<p>Standardization is something what will carry x86 very far into the future despite its infectivity on low power devices.</div><br/><div id="35941873" class="c"><input type="checkbox" id="c-35941873" checked=""/><div class="controls bullet"><span class="by">nubinetwork</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35942315">next</a><span>|</span><label class="collapse" for="c-35941873">[-]</label><label class="expand" for="c-35941873">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When I have ARM image, well, then I can boot it on a SoC it is built for ... or lives on different pins of SoC, then I am screwed and will have at best partially working system.<p>That&#x27;s not even the half of it either...  what firmware does the board run?  U-boot is nice, but sometimes you aren&#x27;t lucky and you&#x27;re stuck with something proprietary.  Although if you&#x27;re extremely lucky, you&#x27;ll have a firmware that supports efi kernels.</div><br/></div></div><div id="35942315" class="c"><input type="checkbox" id="c-35942315" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35941873">prev</a><span>|</span><a href="#35941944">next</a><span>|</span><label class="collapse" for="c-35942315">[-]</label><label class="expand" for="c-35942315">[5 more]</label></div><br/><div class="children"><div class="content"><i>When I have x86 image of Windows&#x2F;Linux, I can boot it on any x86 processor.</i><p>That is largely due to IBM choosing x86, and the PC taking off in a huge way with its de-facto &quot;open&quot; design that ended up being successful and kept backwards compatibility. One could easily imagine a world in which IBM chose ARM (and it was invented earlier) for its first PC, and proprietary x86 SoCs based on Intel&#x27;s cores are everywhere instead. A world in which CISC is the new fad.<p>Note: Intel has non-PC-compatible x86 SoCs too.</div><br/><div id="35942681" class="c"><input type="checkbox" id="c-35942681" checked=""/><div class="controls bullet"><span class="by">hakfoo</span><span>|</span><a href="#35941656">root</a><span>|</span><a href="#35942315">parent</a><span>|</span><a href="#35941944">next</a><span>|</span><label class="collapse" for="c-35942681">[-]</label><label class="expand" for="c-35942681">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what those would be <i>in the modern era?</i><p>The &quot;non-PC&quot; x86 chips I can think of:<p>80186&#x2F;188 - Sort of predates the IBM hegemony, can be hammered into shape by adding replacements for onboard peripherals
80376 - a failed long-gone experiment
386CX&#x2F;EX - not entirely sure how incompatible they are
Xeon Phi&#x2F;Larrabee&#x2F;Knight&#x27;s Corner designs -- not really SoCs so much as special purpose acceleratirs.</div><br/><div id="35944030" class="c"><input type="checkbox" id="c-35944030" checked=""/><div class="controls bullet"><span class="by">Narishma</span><span>|</span><a href="#35941656">root</a><span>|</span><a href="#35942681">parent</a><span>|</span><a href="#35942793">next</a><span>|</span><label class="collapse" for="c-35944030">[-]</label><label class="expand" for="c-35944030">[1 more]</label></div><br/><div class="children"><div class="content">The PS4 (and probably PS5 as well) use x86 chips but are not PC-compatible.</div><br/></div></div><div id="35942793" class="c"><input type="checkbox" id="c-35942793" checked=""/><div class="controls bullet"><span class="by">fathyb</span><span>|</span><a href="#35941656">root</a><span>|</span><a href="#35942681">parent</a><span>|</span><a href="#35944030">prev</a><span>|</span><a href="#35941944">next</a><span>|</span><label class="collapse" for="c-35942793">[-]</label><label class="expand" for="c-35942793">[2 more]</label></div><br/><div class="children"><div class="content">I think the Moorefield and Merrifield SOC platforms are not PC compatible?</div><br/><div id="35943197" class="c"><input type="checkbox" id="c-35943197" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35941656">root</a><span>|</span><a href="#35942793">parent</a><span>|</span><a href="#35941944">next</a><span>|</span><label class="collapse" for="c-35943197">[-]</label><label class="expand" for="c-35943197">[1 more]</label></div><br/><div class="children"><div class="content">Also these: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intel_Quark" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intel_Quark</a><p>They are basically a 486 pipeline with some Pentium instructions.<p>Unfortunately it seems Intel didn&#x27;t realise that x86 without the PC legacy is worth little, so their attempts at non-PC x86 have mostly failed. On the other hand, &quot;PC-on-a-chip&quot; SoCs like <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vortex86" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vortex86</a> have enjoyed more popularity.</div><br/></div></div></div></div></div></div></div></div><div id="35941944" class="c"><input type="checkbox" id="c-35941944" checked=""/><div class="controls bullet"><span class="by">dehrmann</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35942315">prev</a><span>|</span><a href="#35944904">next</a><span>|</span><label class="collapse" for="c-35941944">[-]</label><label class="expand" for="c-35941944">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When I have x86 image of Windows&#x2F;Linux, I can boot it on any x86 processor.<p>Where this gets absurd is modern Debian supports i686 and up. You should be able to get a 27-year-old Pentium Pro to boot the same image as a Raptor Lake CPU.</div><br/></div></div><div id="35944904" class="c"><input type="checkbox" id="c-35944904" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35941944">prev</a><span>|</span><a href="#35942215">next</a><span>|</span><label class="collapse" for="c-35944904">[-]</label><label class="expand" for="c-35944904">[1 more]</label></div><br/><div class="children"><div class="content">No you can&#x27;t, specially in game consoles or embedded deployments, that although migh have x86 CPUs, the motherboard design is incompatible with a standard PC expectations.</div><br/></div></div><div id="35942215" class="c"><input type="checkbox" id="c-35942215" checked=""/><div class="controls bullet"><span class="by">Jasper_</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35944904">prev</a><span>|</span><a href="#35942276">next</a><span>|</span><label class="collapse" for="c-35942215">[-]</label><label class="expand" for="c-35942215">[1 more]</label></div><br/><div class="children"><div class="content">With the push for ARM in the datacenter, ACPI adoption is on the upswing. In theory, ACPI could be used on consumer devices as well, there&#x27;s just little incentive to do that right now.</div><br/></div></div><div id="35942276" class="c"><input type="checkbox" id="c-35942276" checked=""/><div class="controls bullet"><span class="by">eschneider</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35942215">prev</a><span>|</span><a href="#35942213">next</a><span>|</span><label class="collapse" for="c-35942276">[-]</label><label class="expand" for="c-35942276">[1 more]</label></div><br/><div class="children"><div class="content">The flip side of this is that you can almost always get exactly the SoC you need with an ARM, which makes it great for embedded applications. But yeah, lots of custom board bring up…</div><br/></div></div><div id="35942213" class="c"><input type="checkbox" id="c-35942213" checked=""/><div class="controls bullet"><span class="by">GeekyBear</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35942276">prev</a><span>|</span><a href="#35944401">next</a><span>|</span><label class="collapse" for="c-35942213">[-]</label><label class="expand" for="c-35942213">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When I have x86 image of Windows&#x2F;Linux, I can boot it on any x86 processor.<p>This has come with a stack of caveats since the advent of Secure Boot.</div><br/></div></div><div id="35944401" class="c"><input type="checkbox" id="c-35944401" checked=""/><div class="controls bullet"><span class="by">slabtickler</span><span>|</span><a href="#35941656">parent</a><span>|</span><a href="#35942213">prev</a><span>|</span><a href="#35941435">next</a><span>|</span><label class="collapse" for="c-35944401">[-]</label><label class="expand" for="c-35944401">[1 more]</label></div><br/><div class="children"><div class="content">That was mostly IBM&#x27;s doing and goes well beyond the purview of the ISA specifically. x86 doesn&#x27;t really define that you &quot;must use a BIOS&#x2F;UEFI&#x2F;etc.&quot; You can&#x27;t boot, for instance, a standard copy Windows or Linux on a Sony PS4 (which is <i>not</i> actually a PC compatible, even though it almost seems like one) without ten billion asterisks and hacks.</div><br/></div></div></div></div><div id="35941435" class="c"><input type="checkbox" id="c-35941435" checked=""/><div class="controls bullet"><span class="by">isidor3</span><span>|</span><a href="#35941656">prev</a><span>|</span><a href="#35943093">next</a><span>|</span><label class="collapse" for="c-35941435">[-]</label><label class="expand" for="c-35941435">[8 more]</label></div><br/><div class="children"><div class="content">It is interesting to me how both instruction sets have converged on splitting operations into simpler micro ops. The author briefly mentions RISC-V as having &quot;better&quot; core instructions, but it makes me wonder if having the best possible instructions would even help that much.<p>If you made a CPU that directly ran off of some convergent microcode, would you then lose because of bandwidth of getting those instructions to the chip? Or is compressing instruction streams already a pretty-well-solved problem if you&#x27;re able to do it from a clean slate, instead of being tied to what instruction representations a chip happened to be using many years ago?</div><br/><div id="35941470" class="c"><input type="checkbox" id="c-35941470" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35941435">parent</a><span>|</span><a href="#35943442">next</a><span>|</span><label class="collapse" for="c-35941470">[-]</label><label class="expand" for="c-35941470">[6 more]</label></div><br/><div class="children"><div class="content">&gt; If you made a CPU that directly ran off of some convergent microcode<p>I think that’s the original idea behind RISC</div><br/><div id="35941599" class="c"><input type="checkbox" id="c-35941599" checked=""/><div class="controls bullet"><span class="by">isidor3</span><span>|</span><a href="#35941435">root</a><span>|</span><a href="#35941470">parent</a><span>|</span><a href="#35941584">next</a><span>|</span><label class="collapse" for="c-35941599">[-]</label><label class="expand" for="c-35941599">[2 more]</label></div><br/><div class="children"><div class="content">Yes, and obviously ARM didn&#x27;t chose the instructions in its reduced set optimally, if the best implementations require those instructions to be split into smaller ones. But that doesn&#x27;t really speak to if that&#x27;s because it&#x27;s just <i>better</i> to pack instructions that way, or because these implementations of ARM and x86 just need to do it to be performant in spite of deficiencies in their instruction sets.</div><br/><div id="35942415" class="c"><input type="checkbox" id="c-35942415" checked=""/><div class="controls bullet"><span class="by">dfox</span><span>|</span><a href="#35941435">root</a><span>|</span><a href="#35941599">parent</a><span>|</span><a href="#35941584">next</a><span>|</span><label class="collapse" for="c-35942415">[-]</label><label class="expand" for="c-35942415">[1 more]</label></div><br/><div class="children"><div class="content">ARM is a weird beast across the spectrum of RISC designs. The original ISA design is inspired by Berkley RISC (which had only two stage pipeline) and then optimized to what can be reasonably cheaply done in the silicon process used, with the hardware implementation bearing striking similarity to traditional non pipelined &quot;CISC&quot; designs. This design for made cheap implementation of various instructions, like four operand ALU operations or instructions that do multiple memory accesses, which are more or less unthinkable in other RISC designs designed for pipelining and 1 IPC.</div><br/></div></div></div></div><div id="35941584" class="c"><input type="checkbox" id="c-35941584" checked=""/><div class="controls bullet"><span class="by">mafribe</span><span>|</span><a href="#35941435">root</a><span>|</span><a href="#35941470">parent</a><span>|</span><a href="#35941599">prev</a><span>|</span><a href="#35943442">next</a><span>|</span><label class="collapse" for="c-35941584">[-]</label><label class="expand" for="c-35941584">[3 more]</label></div><br/><div class="children"><div class="content">Microcode is often attributed to [1] from 1952.<p>[1] M. V. Wilkes, J. B. Stringer, <i>Micro-programming and the design of the control circuits in an electronic digital computer.</i></div><br/><div id="35941918" class="c"><input type="checkbox" id="c-35941918" checked=""/><div class="controls bullet"><span class="by">kwhitefoot</span><span>|</span><a href="#35941435">root</a><span>|</span><a href="#35941584">parent</a><span>|</span><a href="#35943442">next</a><span>|</span><label class="collapse" for="c-35941918">[-]</label><label class="expand" for="c-35941918">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s arguable that Babbage&#x27;s design for the Analytical Engine included microcode.<p>See, i.a., <a href="https:&#x2F;&#x2F;www.fourmilab.ch&#x2F;babbage&#x2F;glossary.html" rel="nofollow">https:&#x2F;&#x2F;www.fourmilab.ch&#x2F;babbage&#x2F;glossary.html</a></div><br/><div id="35944770" class="c"><input type="checkbox" id="c-35944770" checked=""/><div class="controls bullet"><span class="by">mafribe</span><span>|</span><a href="#35941435">root</a><span>|</span><a href="#35941918">parent</a><span>|</span><a href="#35943442">next</a><span>|</span><label class="collapse" for="c-35944770">[-]</label><label class="expand" for="c-35944770">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, I had no idea. Wilkes doesn&#x27;t cite Babbage, oh dear!</div><br/></div></div></div></div></div></div></div></div><div id="35943442" class="c"><input type="checkbox" id="c-35943442" checked=""/><div class="controls bullet"><span class="by">drpixie</span><span>|</span><a href="#35941435">parent</a><span>|</span><a href="#35941470">prev</a><span>|</span><a href="#35943093">next</a><span>|</span><label class="collapse" for="c-35943442">[-]</label><label class="expand" for="c-35943442">[1 more]</label></div><br/><div class="children"><div class="content">It would be interesting if a program could define some new instructions by specifying the microcode :)<p>Though that might make context switches rather expensive.<p>And if you&#x27;re going down that route, perhaps time to think about FPGAs.</div><br/></div></div></div></div><div id="35943093" class="c"><input type="checkbox" id="c-35943093" checked=""/><div class="controls bullet"><span class="by">titzer</span><span>|</span><a href="#35941435">prev</a><span>|</span><a href="#35944174">next</a><span>|</span><label class="collapse" for="c-35943093">[-]</label><label class="expand" for="c-35943093">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In fact, several workloads saw less power draw with the op cache was disabled. Decoder power draw was drowned out by power draw from other core components, especially if the op cache kept them better fed. That lines up with Jim Keller’s comment.<p>There&#x27;s a hidden problem here: disabling the opcache couldn&#x27;t possibly improve the power draw of other components. The system is simply doing less work per unit time because it issues fewer instructions in the same time.<p>This is a common benchmarking problem when the workload isn&#x27;t fixed, e.g. when a benchmark tries to run as many iterations it can in N seconds. By increasing execution speed (or in this case, decreasing it), you also increase the total amount of work done per unit time.</div><br/></div></div><div id="35944174" class="c"><input type="checkbox" id="c-35944174" checked=""/><div class="controls bullet"><span class="by">MobiusHorizons</span><span>|</span><a href="#35943093">prev</a><span>|</span><a href="#35942356">next</a><span>|</span><label class="collapse" for="c-35944174">[-]</label><label class="expand" for="c-35944174">[1 more]</label></div><br/><div class="children"><div class="content">I have wondered something for a while. x86 (32 and 64 bit) have much smaller instruction size on average than RISC ISAs. Given that modern compute performance is heavily affected by cache performance and memory latency, it seems like the smaller codesize could really help with caching performance. Is this actually a factor? Does arm need bigger caches to combat this? Or does the unpredictability of unguessable instruction offsets ruin this in some way? It&#x27;s obviously only one factor among menay, and ram is often filled with data not just code, but I&#x27;ve wondered about it for a while.</div><br/></div></div><div id="35942356" class="c"><input type="checkbox" id="c-35942356" checked=""/><div class="controls bullet"><span class="by">fanf2</span><span>|</span><a href="#35944174">prev</a><span>|</span><a href="#35942395">next</a><span>|</span><label class="collapse" for="c-35942356">[-]</label><label class="expand" for="c-35942356">[2 more]</label></div><br/><div class="children"><div class="content">x86 is not a very CISCy CISC, compared to VAX or 68020. x86 has relatively simple addressing modes and often limits instructions to one memory operand.<p>It would be interesting to see how well a modern VAX or 68k could compete with x86 and ARM.</div><br/><div id="35942651" class="c"><input type="checkbox" id="c-35942651" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#35942356">parent</a><span>|</span><a href="#35942395">next</a><span>|</span><label class="collapse" for="c-35942651">[-]</label><label class="expand" for="c-35942651">[1 more]</label></div><br/><div class="children"><div class="content">I came here to say much the same. The labels were defined (CISC&#x2F;RISC) in a time of different costs and CPUs. Roll forward, and both RISC has become more complex and CISC has adopted many features of RISC.<p>I would have loved to see more clear signs of how much L1&#x2F;L2 cache plays here, and the interconnect between cores. I suspect we&#x27;re now well down a path where writing code to fit into L1 and writing code to balance load across cores has more importance than anything else.<p>(not a VLSI or ISA person btw)</div><br/></div></div></div></div><div id="35942395" class="c"><input type="checkbox" id="c-35942395" checked=""/><div class="controls bullet"><span class="by">apatheticonion</span><span>|</span><a href="#35942356">prev</a><span>|</span><a href="#35941493">next</a><span>|</span><label class="collapse" for="c-35942395">[-]</label><label class="expand" for="c-35942395">[1 more]</label></div><br/><div class="children"><div class="content">I look forward to the day when ISA doesn&#x27;t matter.<p>Picking an ARM laptop because right now the technology is better suited to mobile applications while having an x86 desktop because that&#x27;s the best technology for that right now seems sensible.<p>Translation layers are constantly improving - maybe soon we will be able to play games and run software with the same level of performance&#x2F;reliability regardless of the CPU architecture.</div><br/></div></div><div id="35941493" class="c"><input type="checkbox" id="c-35941493" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#35942395">prev</a><span>|</span><a href="#35941419">next</a><span>|</span><label class="collapse" for="c-35941493">[-]</label><label class="expand" for="c-35941493">[22 more]</label></div><br/><div class="children"><div class="content">Decoder complexity matters. ARM with its single instruction width allows arbitrarily parallel decoders with only linear growth in transistor count. X86 with its many widths and formats requires decoders that grow exponentially in complexity with parallelism, consuming more silicon and power to achieve higher levels of instruction level parallelism. It requires a degree of brute force with many possible size branches being explored at once among other expensive tricks.<p>This is one of the major areas where the instruction sets are not equal. ARM has a distinct efficiency advantage.</div><br/><div id="35941707" class="c"><input type="checkbox" id="c-35941707" checked=""/><div class="controls bullet"><span class="by">thechao</span><span>|</span><a href="#35941493">parent</a><span>|</span><a href="#35941522">next</a><span>|</span><label class="collapse" for="c-35941707">[-]</label><label class="expand" for="c-35941707">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not exponential; it&#x27;s not even quadratic (it is superlinear), if you put any thought into the design. I worked on an x86 part with 15 decoders&#x2F;fetch unit. The area was annoying, but unimportant. (We didn&#x27;t commit 15 ops&#x2F;cycle; just pc-boundary determination.)<p>I&#x27;ve also worked on ARM&#x2F;custom GPU ISAs. The limiting rate is the total complexity of the ISA, not the encoding density.<p>In fact, from an I$ point-of-view, the tighter x86 encodings are a pretty good win — at least a few % on very long fetch sequences.</div><br/><div id="35943644" class="c"><input type="checkbox" id="c-35943644" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941707">parent</a><span>|</span><a href="#35941522">next</a><span>|</span><label class="collapse" for="c-35943644">[-]</label><label class="expand" for="c-35943644">[2 more]</label></div><br/><div class="children"><div class="content">&gt; In fact, from an I$ point-of-view, the tighter x86 encodings are a pretty good win — at least a few % on very long fetch sequences.<p>This is still what baffles me about RISC-V&#x27;s G profile not requiring the compressed ISA. The performance benefits are quite dramatic for the (comparatively) limited complexity it adds considering G already includes atomics and floating point. I think Linux is eventually going to force the issue since it&#x27;s 64-bit ABI is GC.</div><br/><div id="35945203" class="c"><input type="checkbox" id="c-35945203" checked=""/><div class="controls bullet"><span class="by">tux3</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35943644">parent</a><span>|</span><a href="#35941522">next</a><span>|</span><label class="collapse" for="c-35945203">[-]</label><label class="expand" for="c-35945203">[1 more]</label></div><br/><div class="children"><div class="content">At this point the G subset is just a historical notation shorthand, not really a target.<p>The new thing is RISC-V Architecture Profiles like RVA23, which comes with the base C extension plus extra compressed instructions specifically intended to reduce code size (Zcb), bitmanip instructions, the V vector extension now being mandatory, etc etc.<p>This is what big application cores are expected to target in the future, so if successful you could expect Linux distros to start taking advantage of those at some point</div><br/></div></div></div></div></div></div><div id="35941522" class="c"><input type="checkbox" id="c-35941522" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#35941493">parent</a><span>|</span><a href="#35941707">prev</a><span>|</span><a href="#35941665">next</a><span>|</span><label class="collapse" for="c-35941522">[-]</label><label class="expand" for="c-35941522">[17 more]</label></div><br/><div class="children"><div class="content">Does anyone actually care about this? The x86 decoders are not large on modern implementations, and putting more transistors on dice is a well-solved problem.</div><br/><div id="35941563" class="c"><input type="checkbox" id="c-35941563" checked=""/><div class="controls bullet"><span class="by">mafribe</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941522">parent</a><span>|</span><a href="#35941526">next</a><span>|</span><label class="collapse" for="c-35941563">[-]</label><label class="expand" for="c-35941563">[3 more]</label></div><br/><div class="children"><div class="content">One could argue that one of the reasons why SIMD instructions, and indeed GPUs, are  popular, is because they amortise the (transistor and power) cost of decoding over more compute units, in the case of GPUs over many more.<p>There are also other considerations, like rolling back state in OOO machines, or precise exceptions. All this becomes more complex with an  x86-style instruction set.</div><br/><div id="35942355" class="c"><input type="checkbox" id="c-35942355" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941563">parent</a><span>|</span><a href="#35944772">next</a><span>|</span><label class="collapse" for="c-35942355">[-]</label><label class="expand" for="c-35942355">[1 more]</label></div><br/><div class="children"><div class="content"><i>There are also other considerations, like rolling back state in OOO machines, or precise exceptions. All this becomes more complex with an x86-style instruction set.</i><p>It&#x27;s not really more complex, because those are backend concerns and work on the uop level, after the instructions have already been decoded into uops.</div><br/></div></div><div id="35944772" class="c"><input type="checkbox" id="c-35944772" checked=""/><div class="controls bullet"><span class="by">ben-schaaf</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941563">parent</a><span>|</span><a href="#35942355">prev</a><span>|</span><a href="#35941526">next</a><span>|</span><label class="collapse" for="c-35944772">[-]</label><label class="expand" for="c-35944772">[1 more]</label></div><br/><div class="children"><div class="content">I was under the impression SIMD is due to clock speed not scaling. Instruction parallelism is hard, so there&#x27;s a lot to gain from just making instructions wider.</div><br/></div></div></div></div><div id="35941526" class="c"><input type="checkbox" id="c-35941526" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941522">parent</a><span>|</span><a href="#35941563">prev</a><span>|</span><a href="#35941665">next</a><span>|</span><label class="collapse" for="c-35941526">[-]</label><label class="expand" for="c-35941526">[13 more]</label></div><br/><div class="children"><div class="content">It uses more power. The decoder is like another ALU that is always screaming at 100%.  It means you can easily keep up with ARM in speed but not power efficiency.</div><br/><div id="35941634" class="c"><input type="checkbox" id="c-35941634" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941526">parent</a><span>|</span><a href="#35941674">next</a><span>|</span><label class="collapse" for="c-35941634">[-]</label><label class="expand" for="c-35941634">[9 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t seem to matter <i>in practice</i>. Current generation Intel CPUs and the Apple M2 have very similar performance at the same power levels.</div><br/><div id="35941695" class="c"><input type="checkbox" id="c-35941695" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941634">parent</a><span>|</span><a href="#35941674">next</a><span>|</span><label class="collapse" for="c-35941695">[-]</label><label class="expand" for="c-35941695">[8 more]</label></div><br/><div class="children"><div class="content">How did you arrive at that conclusion? Comparing the M2 Max vs 13650HX it&#x27;s very obvious the M2 Max uses a LOT less power. It&#x27;s not even close, it&#x27;s less then HALF the power. The M2 max has a little worse performance. But it manages to beat the Intel in some benchmarks.</div><br/><div id="35941759" class="c"><input type="checkbox" id="c-35941759" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941695">parent</a><span>|</span><a href="#35941674">next</a><span>|</span><label class="collapse" for="c-35941759">[-]</label><label class="expand" for="c-35941759">[7 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t have to let the Intel chips scale up the power like that. You can lock them to whatever power level suits you. An i7-1370P configured at 20W has broadly similar performance to an M2.</div><br/><div id="35941793" class="c"><input type="checkbox" id="c-35941793" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941759">parent</a><span>|</span><a href="#35941674">next</a><span>|</span><label class="collapse" for="c-35941793">[-]</label><label class="expand" for="c-35941793">[6 more]</label></div><br/><div class="children"><div class="content">Mind linking me some power measurements at same wattage? I didn&#x27;t even know you could set a power target on Intel or Appl Mx. Well you can disable turbo boost on Intel but even then intel blows over their own marketed TDP by a lot.</div><br/><div id="35941828" class="c"><input type="checkbox" id="c-35941828" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941793">parent</a><span>|</span><a href="#35941674">next</a><span>|</span><label class="collapse" for="c-35941828">[-]</label><label class="expand" for="c-35941828">[5 more]</label></div><br/><div class="children"><div class="content">Intel introduced the &quot;running average power limit&quot; over ten years ago. <a href="https:&#x2F;&#x2F;lkml.indiana.edu&#x2F;hypermail&#x2F;linux&#x2F;kernel&#x2F;1304.0&#x2F;01322.html" rel="nofollow">https:&#x2F;&#x2F;lkml.indiana.edu&#x2F;hypermail&#x2F;linux&#x2F;kernel&#x2F;1304.0&#x2F;01322...</a></div><br/><div id="35941874" class="c"><input type="checkbox" id="c-35941874" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941828">parent</a><span>|</span><a href="#35941674">next</a><span>|</span><label class="collapse" for="c-35941874">[-]</label><label class="expand" for="c-35941874">[4 more]</label></div><br/><div class="children"><div class="content">Rapl doesn&#x27;t allow setting a limit which is always obeyed. You can set PL1 and PL2 limits. But intel CPUs will gladly go over those limits in the short term. For example when running a benchmark. That&#x27;s why I asked for specific benchmarks which include power measurements.<p>For example: <a href="https:&#x2F;&#x2F;www.notebookcheck.net&#x2F;i7-1360P-vs-M2_14731_14521.247596.0.html" rel="nofollow">https:&#x2F;&#x2F;www.notebookcheck.net&#x2F;i7-1360P-vs-M2_14731_14521.247...</a><p>this shows the M2 has a little worse performance compared to the 1360P. But the 1360P requires 2.5x the power to achieve that.</div><br/><div id="35942021" class="c"><input type="checkbox" id="c-35942021" checked=""/><div class="controls bullet"><span class="by">Panzer04</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941874">parent</a><span>|</span><a href="#35942295">next</a><span>|</span><label class="collapse" for="c-35942021">[-]</label><label class="expand" for="c-35942021">[2 more]</label></div><br/><div class="children"><div class="content">Apple’s chips are on better process nodes, which confuses the issue. That being said, you really have to test chips at the same power level to get an idea of performance per watt in a comparison.<p>You can easily double CPU power for only a few hundred MHz or 10-20% extra performance.<p>See <a href="https:&#x2F;&#x2F;www.pcworld.com&#x2F;article&#x2F;1359352&#x2F;cool-down-a-deep-dive-into-13900k-power-use-and-efficiency.html" rel="nofollow">https:&#x2F;&#x2F;www.pcworld.com&#x2F;article&#x2F;1359352&#x2F;cool-down-a-deep-div...</a>, which benchmarks chips at different power limits for an example.</div><br/><div id="35942087" class="c"><input type="checkbox" id="c-35942087" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35942021">parent</a><span>|</span><a href="#35942295">next</a><span>|</span><label class="collapse" for="c-35942087">[-]</label><label class="expand" for="c-35942087">[1 more]</label></div><br/><div class="children"><div class="content">Yes I agree with you. That doesn&#x27;t mean this is easy to achieve. With the exception of AMD chips it&#x27;s unfortunately very hard to simply &quot;benchmark with a fixed power budget&quot;.</div><br/></div></div></div></div><div id="35942295" class="c"><input type="checkbox" id="c-35942295" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941874">parent</a><span>|</span><a href="#35942021">prev</a><span>|</span><a href="#35941674">next</a><span>|</span><label class="collapse" for="c-35942295">[-]</label><label class="expand" for="c-35942295">[1 more]</label></div><br/><div class="children"><div class="content">Your model of RAPL&#x27;s abilities is too limited. &quot;PL1&#x2F;PL2&quot; is a thing that youtube reviewers have figured out, but it is a part of a larger picture. It does not make sense to discuss them without also discussing the time parameters. RAPL <i>is</i> able to hard-cap the (estimated) peak power consumption. If it lacked this feature, operating them at warehouse scale would be impossible.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35941674" class="c"><input type="checkbox" id="c-35941674" checked=""/><div class="controls bullet"><span class="by">arp242</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941526">parent</a><span>|</span><a href="#35941634">prev</a><span>|</span><a href="#35942224">next</a><span>|</span><label class="collapse" for="c-35941674">[-]</label><label class="expand" for="c-35941674">[1 more]</label></div><br/><div class="children"><div class="content">How much power does it (roughly) use? Are we talking about 1% of the overall usage? 10%? 50%?</div><br/></div></div><div id="35942086" class="c"><input type="checkbox" id="c-35942086" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#35941493">root</a><span>|</span><a href="#35941526">parent</a><span>|</span><a href="#35942224">prev</a><span>|</span><a href="#35941665">next</a><span>|</span><label class="collapse" for="c-35942086">[-]</label><label class="expand" for="c-35942086">[1 more]</label></div><br/><div class="children"><div class="content">this article states different, so how is it?</div><br/></div></div></div></div></div></div><div id="35941665" class="c"><input type="checkbox" id="c-35941665" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35941493">parent</a><span>|</span><a href="#35941522">prev</a><span>|</span><a href="#35941419">next</a><span>|</span><label class="collapse" for="c-35941665">[-]</label><label class="expand" for="c-35941665">[1 more]</label></div><br/><div class="children"><div class="content">How is it exponential? It&#x27;s only a multiplicative increase in decode positions.</div><br/></div></div></div></div><div id="35941419" class="c"><input type="checkbox" id="c-35941419" checked=""/><div class="controls bullet"><span class="by">dtx1</span><span>|</span><a href="#35941493">prev</a><span>|</span><a href="#35941885">next</a><span>|</span><label class="collapse" for="c-35941419">[-]</label><label class="expand" for="c-35941419">[15 more]</label></div><br/><div class="children"><div class="content">yeah, no. Tooling support, driver support and general Optimization matters. So does platform maturity.<p>You don&#x27;t want your phone to run x86 (and it won&#x27;t for a while) and though possible its a pain to deal with an arm server at the moment because some random library you use just won&#x27;t be compatible. And If single threaded performance matters, ARM is behind by a decade.</div><br/><div id="35941619" class="c"><input type="checkbox" id="c-35941619" checked=""/><div class="controls bullet"><span class="by">kaelinl</span><span>|</span><a href="#35941419">parent</a><span>|</span><a href="#35941498">next</a><span>|</span><label class="collapse" for="c-35941619">[-]</label><label class="expand" for="c-35941619">[3 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t the point of the article.<p>The article is commenting on CPU design: area efficiency, power efficiency, design cost, etc. They&#x27;re proposing that the reason x86 CPUs have historically beat ARM CPUs in performance, and the reason ARM CPUs have historically beat x86 CPUs in power efficiency, has nothing to do with the design of the ISA itself. You could build an ARM CPU to beat an x86 CPU in high performance computing, or vice versa. They&#x27;re saying that the format of the instructions and the particular way the operations are structured isn&#x27;t the driving factor. Instead, it&#x27;s just a historical arteract of how the ISAs were used.<p>In other words, yes, there are plenty of ecosystem reasons that these two (and potentially, more) families of chips are better for some things vs. others, but if the two companies swapped their ISAs 30 years ago we might see exactly the same ecosystem just with different instruction formats.</div><br/><div id="35942198" class="c"><input type="checkbox" id="c-35942198" checked=""/><div class="controls bullet"><span class="by">Jasper_</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35941619">parent</a><span>|</span><a href="#35944386">next</a><span>|</span><label class="collapse" for="c-35942198">[-]</label><label class="expand" for="c-35942198">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, ARM&#x27;s big advantage is that they are willing to make absolutely zero margin on chips, which allows them to play both ends of the spectrum. They can make Celeron-tier chips that don&#x27;t take any power and practically given away for free and use that as evidence of ARM&#x27;s superiority in things like the M1 and supercomputers.<p>Even as Intel is declining, it still makes way, way more money than ARM does. If Intel wanted to play in the lose-money business, it could make something to kick the pants off of ARM&#x27;s chips. It just would rather not.</div><br/></div></div><div id="35944386" class="c"><input type="checkbox" id="c-35944386" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35941619">parent</a><span>|</span><a href="#35942198">prev</a><span>|</span><a href="#35941498">next</a><span>|</span><label class="collapse" for="c-35944386">[-]</label><label class="expand" for="c-35944386">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure I buy that. x86 chips have targeted laptops for decades, yet they were absolutely walloped by Apple&#x27;s ARM chips. IIRC a big part of that is that the ARM ISA has 2&#x2F;4 byte instructions whereas x86 can be crazy sizes which made it easier for Apple to have 8 instruction decoders. At least that&#x27;s the best theory I heard for why it is so fast.</div><br/></div></div></div></div><div id="35941498" class="c"><input type="checkbox" id="c-35941498" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#35941419">parent</a><span>|</span><a href="#35941619">prev</a><span>|</span><a href="#35941576">next</a><span>|</span><label class="collapse" for="c-35941498">[-]</label><label class="expand" for="c-35941498">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know, is the only good answer. We haven&#x27;t done much trying in the past 10 years.<p>Intel&#x27;s Lakefield was doing quite well in the tablet&#x2F;MID space. It also had the disadvantages of both comparatively ancient Atom-esque cores- far worse than Intel&#x27;s new E cores-and a massive massive huge Skylake core. Oh and a replayed decade old desktop iGPU too.<p>ARM is no longer behind by all that much on single threaded. On geekbench a m2 can do 1916 points, a 7950 2300points. Slightly bigger gap on Cinebench, 1580 Vs 2050. A big part of the gap here is almost certainly the Hz being so different.<p>We just don&#x27;t know. There&#x27;s old beliefs we have held but we had so little evidence for these biases then. X86 rarely tried to be really tiny, had so much more to learn if it was to succeed. ARM rarely tried to be big, and has been learning. There&#x27;s scant evidence there are real limiting factors for either.</div><br/></div></div><div id="35941576" class="c"><input type="checkbox" id="c-35941576" checked=""/><div class="controls bullet"><span class="by">hedora</span><span>|</span><a href="#35941419">parent</a><span>|</span><a href="#35941498">prev</a><span>|</span><a href="#35941783">next</a><span>|</span><label class="collapse" for="c-35941576">[-]</label><label class="expand" for="c-35941576">[3 more]</label></div><br/><div class="children"><div class="content">These benchmarks suggest arm has been at single threaded performance parity on server since 2020:<p><a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15578&#x2F;cloud-clash-amazon-graviton2-arm-against-intel-and-amd&#x2F;5" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15578&#x2F;cloud-clash-amazon-grav...</a><p>(Apple Silicon blows them away on laptops, of course.)</div><br/><div id="35941769" class="c"><input type="checkbox" id="c-35941769" checked=""/><div class="controls bullet"><span class="by">tyingq</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35941576">parent</a><span>|</span><a href="#35941783">next</a><span>|</span><label class="collapse" for="c-35941769">[-]</label><label class="expand" for="c-35941769">[2 more]</label></div><br/><div class="children"><div class="content">I imagine some of the remaining gap might be places where inline ASM or things like SIMD, AVX, etc, exist. Where there&#x27;s been more years and a larger set of people optimizing that ASM for x86&#x2F;64 servers.</div><br/><div id="35942297" class="c"><input type="checkbox" id="c-35942297" checked=""/><div class="controls bullet"><span class="by">GeekyBear</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35941769">parent</a><span>|</span><a href="#35941783">next</a><span>|</span><label class="collapse" for="c-35942297">[-]</label><label class="expand" for="c-35942297">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the remaining gap might be places where inline ASM or things like SIMD, AVX, etc, exist<p>This was one of the takeaways back in 2017 when Cloudflare sampled Qualcomm&#x27;s very early Centriq ARM server chips.<p>&gt; At Cloudflare we use an improved version of the library, optimized for 64-bit Intel processors, and although it is written mostly in C, it does use some Intel specific intrinsics. Comparing this optimized version to the generic zlib library wouldn’t be fair. Not to worry, with little effort I adapted the library to work very well on the ARMv8 architecture, with the use of NEON and CRC32 intrinsics. In the process it is twice as fast as the generic library for some files.<p><a href="https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;arm-takes-wing&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;arm-takes-wing&#x2F;</a></div><br/></div></div></div></div></div></div><div id="35941783" class="c"><input type="checkbox" id="c-35941783" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#35941419">parent</a><span>|</span><a href="#35941576">prev</a><span>|</span><a href="#35941463">next</a><span>|</span><label class="collapse" for="c-35941783">[-]</label><label class="expand" for="c-35941783">[4 more]</label></div><br/><div class="children"><div class="content">&gt; You don&#x27;t want your phone to run x86 (and it won&#x27;t for a while)<p>It&#x27;s easily forgotten but there were Android phones which used Intel x86 processors, such as the early Asus Zenfones. They didn&#x27;t stick though.</div><br/><div id="35942181" class="c"><input type="checkbox" id="c-35942181" checked=""/><div class="controls bullet"><span class="by">rsaxvc</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35941783">parent</a><span>|</span><a href="#35942166">next</a><span>|</span><label class="collapse" for="c-35942181">[-]</label><label class="expand" for="c-35942181">[1 more]</label></div><br/><div class="children"><div class="content">You could even get an AMD chip in your XPPhone <a href="https:&#x2F;&#x2F;www.umpcportal.com&#x2F;products&#x2F;XPPhone&#x2F;XPPhone" rel="nofollow">https:&#x2F;&#x2F;www.umpcportal.com&#x2F;products&#x2F;XPPhone&#x2F;XPPhone</a> :p</div><br/></div></div><div id="35942166" class="c"><input type="checkbox" id="c-35942166" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35941783">parent</a><span>|</span><a href="#35942181">prev</a><span>|</span><a href="#35941463">next</a><span>|</span><label class="collapse" for="c-35942166">[-]</label><label class="expand" for="c-35942166">[2 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s the point, though: why didn&#x27;t they stick?  Was it a price&#x2F;performance&#x2F;power issue, which would point to ARM being somehow &quot;better&quot; on those dimensions, or was it something incidental, like production availability, or even just perhaps a lack of desire at Intel to aggressively pursue that market?</div><br/><div id="35943000" class="c"><input type="checkbox" id="c-35943000" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35942166">parent</a><span>|</span><a href="#35941463">next</a><span>|</span><label class="collapse" for="c-35943000">[-]</label><label class="expand" for="c-35943000">[1 more]</label></div><br/><div class="children"><div class="content">At least some of the attempts in question had, ahem, interesting ideas for how to combine baseband and AP on one chip.<p>(x86 has a truly atrocious thing called SMM (“system management mode”).  It is not directly exposed to anything except firmware and malware, so it could be changed in backwards-incompatible ways without breaking existing software, but for some reason neither Intel nor AMD seems willing to do so.  For desktop and server workloads, this isn’t really a big deal. For something like an iPhone, it would be a major non-selling-point.<p>Also, x86 interrupt latency is awful, and this is an ISA issue. x86 cannot currently compete in any application where this matters. FRED may improve this if the implementation is good enough.)</div><br/></div></div></div></div></div></div><div id="35941463" class="c"><input type="checkbox" id="c-35941463" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35941419">parent</a><span>|</span><a href="#35941783">prev</a><span>|</span><a href="#35941951">next</a><span>|</span><label class="collapse" for="c-35941463">[-]</label><label class="expand" for="c-35941463">[2 more]</label></div><br/><div class="children"><div class="content">Having an Oracle Cloud Free Tier ARM VPS it’s surprising how much just works, I think the only thing I couldn’t run was Chrome Remote Desktop (yes, I want to remote into my VPS sometimes, for example it’s the easiest way to leave a GUI program running in the background without leaving my PC on) and only a few other things needed extra steps. But it’s probably a lot different on desktop or if you’re running different types of programs</div><br/><div id="35941496" class="c"><input type="checkbox" id="c-35941496" checked=""/><div class="controls bullet"><span class="by">wolf550e</span><span>|</span><a href="#35941419">root</a><span>|</span><a href="#35941463">parent</a><span>|</span><a href="#35941951">next</a><span>|</span><label class="collapse" for="c-35941496">[-]</label><label class="expand" for="c-35941496">[1 more]</label></div><br/><div class="children"><div class="content">Some libraries have x86 SIMD code but no ARM SIMD code, so benchmarking real world use cases you compare SIMD vs scalar code and x86 is much faster. Server side libraries for ARM are a less mature situation than x86.</div><br/></div></div></div></div><div id="35941951" class="c"><input type="checkbox" id="c-35941951" checked=""/><div class="controls bullet"><span class="by">dehrmann</span><span>|</span><a href="#35941419">parent</a><span>|</span><a href="#35941463">prev</a><span>|</span><a href="#35941885">next</a><span>|</span><label class="collapse" for="c-35941951">[-]</label><label class="expand" for="c-35941951">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like Atom could have found a home in phones.</div><br/></div></div></div></div><div id="35941885" class="c"><input type="checkbox" id="c-35941885" checked=""/><div class="controls bullet"><span class="by">dehrmann</span><span>|</span><a href="#35941419">prev</a><span>|</span><label class="collapse" for="c-35941885">[-]</label><label class="expand" for="c-35941885">[16 more]</label></div><br/><div class="children"><div class="content">This would explain part of why Apple hasn&#x27;t been pushing M2 for the data center. Its chips are a better fit for bursty human workloads, not server workloads.</div><br/><div id="35941988" class="c"><input type="checkbox" id="c-35941988" checked=""/><div class="controls bullet"><span class="by">KerrAvon</span><span>|</span><a href="#35941885">parent</a><span>|</span><a href="#35944964">next</a><span>|</span><label class="collapse" for="c-35941988">[-]</label><label class="expand" for="c-35941988">[10 more]</label></div><br/><div class="children"><div class="content">Apple Silicon chips aren’t for sale outside Apple, and Apple hasn’t made any products relevant to data centers since they terminated the Xserve line as part of the PowerPC -&gt; Intel transition.</div><br/><div id="35942398" class="c"><input type="checkbox" id="c-35942398" checked=""/><div class="controls bullet"><span class="by">wolfendin</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35941988">parent</a><span>|</span><a href="#35942098">next</a><span>|</span><label class="collapse" for="c-35942398">[-]</label><label class="expand" for="c-35942398">[2 more]</label></div><br/><div class="children"><div class="content">The Xserve had Intel Xeon versions, not sure why the termination of that line would have anything to to do with the intel transition, since they got deprecated three years after the transition was complete.</div><br/><div id="35943040" class="c"><input type="checkbox" id="c-35943040" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942398">parent</a><span>|</span><a href="#35942098">next</a><span>|</span><label class="collapse" for="c-35943040">[-]</label><label class="expand" for="c-35943040">[1 more]</label></div><br/><div class="children"><div class="content">IIRC one problem was that MacOS was not competitive. I recall some benchmarks of applications like MySQL, comparing MacOS Server to Linux on similar or even identical hardware, with results that could only be described as abysmal on MacOS Server.</div><br/></div></div></div></div><div id="35942098" class="c"><input type="checkbox" id="c-35942098" checked=""/><div class="controls bullet"><span class="by">dehrmann</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35941988">parent</a><span>|</span><a href="#35942398">prev</a><span>|</span><a href="#35944964">next</a><span>|</span><label class="collapse" for="c-35942098">[-]</label><label class="expand" for="c-35942098">[7 more]</label></div><br/><div class="children"><div class="content">They have a CPU that&#x27;s been labeled some version of fastest or most efficient, they&#x27;re hungry for more revenue, but somehow have no interest in the data center market? There must be a reason.</div><br/><div id="35942307" class="c"><input type="checkbox" id="c-35942307" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942098">parent</a><span>|</span><a href="#35942367">next</a><span>|</span><label class="collapse" for="c-35942307">[-]</label><label class="expand" for="c-35942307">[1 more]</label></div><br/><div class="children"><div class="content">I think this link posted above is the precise reason - there are already other ARM vendors in the server market with ballpark-similar performance, with better market placement and better business relationships.  Nobody trusts Apple not to dump server again after XServe and nobody thinks they&#x27;re a super great partner to work with.<p><a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15578&#x2F;cloud-clash-amazon-graviton2-arm-against-intel-and-amd&#x2F;5" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;15578&#x2F;cloud-clash-amazon-grav...</a><p>Why would Amazon choose to pay more for an Apple branded product when they could make a higher margin just doing it themselves?  Hyperscalers are almost definitionally at the limit where scaling works and they have the volume to amortize some basic uarch work.  And it&#x27;ll be optimized for their exact PPA targets to make the lowest TCO&#x2F;etc.<p>Beyond hyperscalers, why would anyone else choose Apple over Ampere, given Apple&#x27;s general lack of commitment to the server market&#x2F;open ecosystem&#x2F;etc?  What does the cost look like on this now&#x2F;in the future?  Consider what might happen after the relationship ends, friendly or otherwise - can you keep your business operations going in terms of procurement vs committed instances etc?  What if they won&#x27;t send you any more spares or won&#x27;t sign some key structure for you (drivers&#x2F;firmware, UEFI keys, whatever, idk)?  Surely anyone involved would want to be super duper sure about this.  But Apple has gotten bored and left this market before, and they&#x27;re always the hot one in the relationship.  They can find someone else today if they need.<p>I just don&#x27;t think there&#x27;s much of a market for this that wouldn&#x27;t rather buy an Ampere or build their own clone like Graviton.</div><br/></div></div><div id="35942367" class="c"><input type="checkbox" id="c-35942367" checked=""/><div class="controls bullet"><span class="by">manv1</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942098">parent</a><span>|</span><a href="#35942307">prev</a><span>|</span><a href="#35942220">next</a><span>|</span><label class="collapse" for="c-35942367">[-]</label><label class="expand" for="c-35942367">[1 more]</label></div><br/><div class="children"><div class="content">Apple sells its products to its markets, and is mostly uninterested in the data center and embedded market. The support costs are large, and the margins are small-ish.<p>Which is too bad for the industry, but also lucky for the industry.<p>I mean, they were the first 64-bit ARM chip, period. In fact, everyone mocked it, but they also trembled in fear because 64-bit.</div><br/></div></div><div id="35942220" class="c"><input type="checkbox" id="c-35942220" checked=""/><div class="controls bullet"><span class="by">dan-robertson</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942098">parent</a><span>|</span><a href="#35942367">prev</a><span>|</span><a href="#35942736">next</a><span>|</span><label class="collapse" for="c-35942220">[-]</label><label class="expand" for="c-35942220">[2 more]</label></div><br/><div class="children"><div class="content">Data centres care about different things in CPUs. You also don’t see consumer intel CPUs (e.g. core i7, or even the high end gamer cpus &#x2F; workstation xeons) in normal data centres, even though they run x86. You <i>do</i> see arm in data centres, cf graviton on aws, but these are cpus fds signed for data centres (lots of cores, lots of memory, etc).<p>The big difference is that consumer cpus care less about virtualisation, high core counts, or having lots of memory, and more about single core performance and potentially not getting hot in your lap.</div><br/><div id="35942287" class="c"><input type="checkbox" id="c-35942287" checked=""/><div class="controls bullet"><span class="by">dehrmann</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942220">parent</a><span>|</span><a href="#35942736">next</a><span>|</span><label class="collapse" for="c-35942287">[-]</label><label class="expand" for="c-35942287">[1 more]</label></div><br/><div class="children"><div class="content">&gt; consumer cpus care less about virtualisation, high core counts, or having lots of memory, and more about single core performance and potentially not getting hot in your lap.<p>This goes with what I said in my original comment.</div><br/></div></div></div></div><div id="35942736" class="c"><input type="checkbox" id="c-35942736" checked=""/><div class="controls bullet"><span class="by">mdasen</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942098">parent</a><span>|</span><a href="#35942220">prev</a><span>|</span><a href="#35944964">next</a><span>|</span><label class="collapse" for="c-35942736">[-]</label><label class="expand" for="c-35942736">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s probably a lot of reasons.<p>The biggest thing is probably that data centers wouldn&#x27;t want an M2, M2 Pro, M2 Max, or M2 Ultra. Apple would need a specialized chip. Right now, Apple Silicon tops out at 12 cores (8 performance and 4 efficiency). If I&#x27;m a data center, I&#x27;m going to likely prefer an AMD EPYC with 64 cores (and 128 threads) over an Apple M2 with 8 performance cores. I can slice that AMD EPYC into a lot more VMs.<p>Apple would realistically need to make a speciality part for the data center. That would mean taking people off its regular products and tasking them on opening up a new product line that Apple has historically been terrible at. Now Apple has fewer people working on iPhone, Mac, etc. and those products suffer in order to try and enter a market that probably isn&#x27;t a good fit for them. Heck, data centers aren&#x27;t going to want CPUs soldered to the motherboard.<p>Not only that, it would mean tasking software people toward the project. How much of your software staff are now trying to get Linux stable on M2 - taking staff away from iOS&#x2F;macOS?<p>Apple doesn&#x27;t want to do it because it would be a big undertaking. It&#x27;s not just &quot;print some M2s and sell them to Amazon.&quot;<p>Beyond that, it&#x27;d probably be a low margin market compared to what they usually go for. With iPhone and Mac, they have huge product differentiation giving them great margins, but they wouldn&#x27;t for the datacenter. Even if they&#x27;re the fastest and most efficient, data center customers are looking for performance-per-dollar. Performance per watt matters in the data center, but not nearly as much as it matters in laptops and phones.<p>The ARM ISA would mean that they&#x27;d need to sell at a discount compared to x64 chips (even if they have better performance) because x64 is the path of least resistance for customers. So Apple would need margins lower than Intel&#x2F;AMD.<p>Plus, it would mean taking fab capacity away from iPhones and Macs. We&#x27;ve already seen how constrained that capacity can be. It took AMD 2 years to get to 5nm after Apple launched their 5nm processors. If Apple were to become a large data center player, they&#x27;d need to figure out how to prioritize that. For example, only the iPhone 14 Pro got the 4nm A16 processor last year - presumably because TSMC&#x27;s capacity was really limited. All the rumors on 3nm seem to be similarly constrained. Apple isn&#x27;t going to risk their cash-cow businesses (iPhone, Mac) for a low margin data center business so that would likely mean shipping data center CPUs that were older nodes. Heck, one of the reasons that AMD hasn&#x27;t taken over the data center is that they&#x27;ve been a bit supply constrained - and Apple would be too.<p>There are lots of reasons, but it boils down to the fact that Apple would need to build something they don&#x27;t currently make - a data center CPU, motherboard, case, open boot system so people can run other operating systems, drivers specs and docs for those other operating systems, etc. Apple would be facing a market where the ARM ISA is a negative, margins aren&#x27;t as good, and customers would be skeptical of a company whose commitment to enterprise and data centers has been terrible. Plus, Apple&#x27;s performance supremacy wouldn&#x27;t even be a total positive in the data center since they&#x27;re going to be looking at performance per dollar and there would be other companies who would accept low margins all competing in that space.<p>EDIT: I&#x27;d also note that Intel&#x27;s total revenue is $63B and AMD&#x27;s is $24B and Apple&#x27;s is $388B. Let&#x27;s say Apple is wildly successful and gets a server business as large as AMD&#x27;s. Apple maybe increases its revenue by 3% (assuming that half of AMD&#x27;s revenue comes from the data center). So when you say that Apple wants revenue, a new server business wouldn&#x27;t get them that. More likely, Apple&#x27;s data center business would be 10% the size of AMD and increase Apple&#x27;s revenue by 0.3%.</div><br/><div id="35942896" class="c"><input type="checkbox" id="c-35942896" checked=""/><div class="controls bullet"><span class="by">leeter</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942736">parent</a><span>|</span><a href="#35944964">next</a><span>|</span><label class="collapse" for="c-35942896">[-]</label><label class="expand" for="c-35942896">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Right now, Apple Silicon tops out at 12 cores (8 performance and 4 efficiency). If I&#x27;m a data center, I&#x27;m going to likely prefer an AMD EPYC with 64 cores (and 128 threads) over an Apple M2 with 8 performance cores. I can slice that AMD EPYC into a lot more VMs.<p>I think it&#x27;s worth calling out how important this is. Once you get past a certain die size and core count interconnect or &quot;fabric&quot; latency and bandwidth starts to have a much bigger impact on loads than core speed and throughput for code not optimized for that processor. Where M2 is... Apple doesn&#x27;t have to deal with that at all. AMD on the other hand has gone all in, hence chiplet designs. But yeah Apple wants nothing to do with that, hence they seem to be going for very wide but limited core count designs.</div><br/></div></div></div></div></div></div></div></div><div id="35944964" class="c"><input type="checkbox" id="c-35944964" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#35941885">parent</a><span>|</span><a href="#35941988">prev</a><span>|</span><a href="#35942952">next</a><span>|</span><label class="collapse" for="c-35944964">[-]</label><label class="expand" for="c-35944964">[1 more]</label></div><br/><div class="children"><div class="content">For what? To run GNU&#x2F;Linux instead of macOS, keep wishing for it.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MkLinux" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MkLinux</a><p>If it is to have Xserve again, no one cares about it other than companies on the Apple ecosystem, not worth the trouble, they already have Xcode Cloud for that.</div><br/></div></div><div id="35942952" class="c"><input type="checkbox" id="c-35942952" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#35941885">parent</a><span>|</span><a href="#35944964">prev</a><span>|</span><a href="#35941986">next</a><span>|</span><label class="collapse" for="c-35942952">[-]</label><label class="expand" for="c-35942952">[1 more]</label></div><br/><div class="children"><div class="content">The chips don&#x27;t have ECC and have very low maximum memory and aren&#x27;t suitable for the datacenter.</div><br/></div></div><div id="35941986" class="c"><input type="checkbox" id="c-35941986" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#35941885">parent</a><span>|</span><a href="#35942952">prev</a><span>|</span><label class="collapse" for="c-35941986">[-]</label><label class="expand" for="c-35941986">[3 more]</label></div><br/><div class="children"><div class="content">Apple doesn&#x27;t see value in going after the server market.</div><br/><div id="35942643" class="c"><input type="checkbox" id="c-35942643" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35941986">parent</a><span>|</span><label class="collapse" for="c-35942643">[-]</label><label class="expand" for="c-35942643">[2 more]</label></div><br/><div class="children"><div class="content">The consumer might replace their laptop once a year, at a fast cadence, but they don&#x27;t wake up one day and say &quot;I actually need 3 laptops&quot;.<p>Whereas datacenters are now being built constantly, and any future projection would estimate that we&#x27;re going to keep building more - and if you were unsure before, then the sudden popularity of training AIs and their massive demand for compute should&#x27;ve convinced you.<p>Apple aren&#x27;t going to leave money on the table (otherwise they&#x27;d still be shipping iPhones with chargers and including dongles with laptops): if they&#x27;re not targeting server markets, it&#x27;s because their internal modelling is telling them that what they&#x27;ve got is probably at <i>best</i> a peer-capability, rather then some type of vast excession (or would be wiped out easily by another gen of server grade chip releases).</div><br/><div id="35944954" class="c"><input type="checkbox" id="c-35944954" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#35941885">root</a><span>|</span><a href="#35942643">parent</a><span>|</span><label class="collapse" for="c-35944954">[-]</label><label class="expand" for="c-35944954">[1 more]</label></div><br/><div class="children"><div class="content">Apple already failed twice on the server market, A&#x2F;UX and Xserve, and decided it wasn&#x27;t for them.<p>They would have to offer a top option with macOS to make it relevant, as they certainly aren&#x27;t going to be offering their hardware to run GNU&#x2F;Linux on top of it.<p>It happened once, with MkLinux, and that is also something that management won&#x27;t be keen in repeating.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>