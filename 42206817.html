<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1732266086309" as="style"/><link rel="stylesheet" href="styles.css?v=1732266086309"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://dynomight.net/more-chess/">OK, I can partly explain the LLM chess weirdness now</a> <span class="domain">(<a href="https://dynomight.net">dynomight.net</a>)</span></div><div class="subtext"><span>dmazin</span> | <span>132 comments</span></div><br/><div><div id="42207156" class="c"><input type="checkbox" id="c-42207156" checked=""/><div class="controls bullet"><span class="by">tromp</span><span>|</span><a href="#42211856">next</a><span>|</span><label class="collapse" for="c-42207156">[-]</label><label class="expand" for="c-42207156">[60 more]</label></div><br/><div class="children"><div class="content">&gt; For one, gpt-3.5-turbo-instruct rarely suggests illegal moves, even in the late game. This requires “understanding” chess.<p>Here&#x27;s one way to test whether it really understands chess. Make it play the next move in 1000 random legal positions (in which no side is checkmated yet). Such positions can be generated using the ChessPositionRanking project at [1]. Does it still rarely suggest illegal moves in these totally weird positions, that will be completely unlike any it would have seen in training (and in which the legal move choice is often highly restricted) ?<p>While good for testing legality of next moves, these positions are not so useful for distinguishing their quality, since usually one side already has an overwhelming advantage.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;tromp&#x2F;ChessPositionRanking">https:&#x2F;&#x2F;github.com&#x2F;tromp&#x2F;ChessPositionRanking</a></div><br/><div id="42207882" class="c"><input type="checkbox" id="c-42207882" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42207156">parent</a><span>|</span><a href="#42208448">next</a><span>|</span><label class="collapse" for="c-42207882">[-]</label><label class="expand" for="c-42207882">[3 more]</label></div><br/><div class="children"><div class="content">Interesting tidbit I once learned from a chess livestream. Even human super-GMs have a really hard time &quot;scoring&quot; or &quot;solving&quot; extremely weird positions. That is, positions that shouldn&#x27;t come from logical opening - mid game - end game regular play.<p>It&#x27;s absolutely amazing to see a super-GM (in that case it was Hikaru) see a position, and basically &quot;play-by-play&quot; it from the beginning, to show people how they got in that position. It wasn&#x27;t his game btw. But later in that same video when asked he explained what I wrote in the first paragraph. It works with proper games, but it rarely works with weird random chess puzzles, as he put it. Or, in other words, chess puzzles that come from real games are much better than &quot;randomly generated&quot;, and make more sense even to the best of humans.</div><br/><div id="42208106" class="c"><input type="checkbox" id="c-42208106" checked=""/><div class="controls bullet"><span class="by">saghm</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207882">parent</a><span>|</span><a href="#42208140">next</a><span>|</span><label class="collapse" for="c-42208106">[-]</label><label class="expand" for="c-42208106">[1 more]</label></div><br/><div class="children"><div class="content">Super interesting (although it also makes some sense that experts would focus on &quot;likely&quot; subsets given how the number of permutations of chess games is too high for it to be feasible to learn them all)! That said, I still imagine that even most intermediate chess players would perfectly make only _legal_ moves in weird positions, even if they&#x27;re low quality.</div><br/></div></div><div id="42208140" class="c"><input type="checkbox" id="c-42208140" checked=""/><div class="controls bullet"><span class="by">MarcelOlsz</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207882">parent</a><span>|</span><a href="#42208106">prev</a><span>|</span><a href="#42208448">next</a><span>|</span><label class="collapse" for="c-42208140">[-]</label><label class="expand" for="c-42208140">[1 more]</label></div><br/><div class="children"><div class="content">Would love a link to that video!</div><br/></div></div></div></div><div id="42208448" class="c"><input type="checkbox" id="c-42208448" checked=""/><div class="controls bullet"><span class="by">snowwrestler</span><span>|</span><a href="#42207156">parent</a><span>|</span><a href="#42207882">prev</a><span>|</span><a href="#42207812">next</a><span>|</span><label class="collapse" for="c-42208448">[-]</label><label class="expand" for="c-42208448">[3 more]</label></div><br/><div class="children"><div class="content">It’s kind of crazy to assert that the systems understand chess, and then disclose further down the article that sometimes he failed to get a legal move after 10 tries and had to sub in a random move.<p>A person who understands chess well (Elo 1800, let’s say) will essentially never fail to provide a legal move on the first try.</div><br/><div id="42212194" class="c"><input type="checkbox" id="c-42212194" checked=""/><div class="controls bullet"><span class="by">navane</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208448">parent</a><span>|</span><a href="#42208574">next</a><span>|</span><label class="collapse" for="c-42212194">[-]</label><label class="expand" for="c-42212194">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure elo 1200 will only give legal moves. It&#x27;s really not hard to make legal moves in chess.</div><br/></div></div><div id="42208574" class="c"><input type="checkbox" id="c-42208574" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208448">parent</a><span>|</span><a href="#42212194">prev</a><span>|</span><a href="#42207812">next</a><span>|</span><label class="collapse" for="c-42208574">[-]</label><label class="expand" for="c-42208574">[1 more]</label></div><br/><div class="children"><div class="content">He is testing several models, some of which cannot reliably output legal moves. That&#x27;s different from saying all models including the one he thinks understands can&#x27;t generate a legal move in 10 tries.<p>3.5-turbo-instruct&#x27;s illegal move rate is about 5 or less in 8205</div><br/></div></div></div></div><div id="42207812" class="c"><input type="checkbox" id="c-42207812" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">parent</a><span>|</span><a href="#42208448">prev</a><span>|</span><a href="#42207742">next</a><span>|</span><label class="collapse" for="c-42207812">[-]</label><label class="expand" for="c-42207812">[40 more]</label></div><br/><div class="children"><div class="content">I think at this point it’s very clear LLM aren’t achieving any form of “reasoning” as commonly understood. Among other factors it can be argued that true reasoning involves symbolic logic and abstractions, and LLM are next token predictors.</div><br/><div id="42207913" class="c"><input type="checkbox" id="c-42207913" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207812">parent</a><span>|</span><a href="#42208687">next</a><span>|</span><label class="collapse" for="c-42207913">[-]</label><label class="expand" for="c-42207913">[15 more]</label></div><br/><div class="children"><div class="content">&gt; Among other factors it can be argued that true reasoning involves symbolic logic and abstractions, and LLM are next token predictors.<p>I think this is circular?<p>If an LLM is &quot;merely&quot; predicting the next tokens to put together a description of symbolic reasoning and abstractions... how is that different from really exercisng those things?<p>Can you give me an example of symbolic reasoning that I can&#x27;t handwave away as just the likely next words given the starting place?<p>I&#x27;m not saying that LLMs have those capabilities; I&#x27;m question whether there is any utility in distinguishing the &quot;actual&quot; capability from identical outputs.</div><br/><div id="42212148" class="c"><input type="checkbox" id="c-42212148" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207913">parent</a><span>|</span><a href="#42208035">next</a><span>|</span><label class="collapse" for="c-42212148">[-]</label><label class="expand" for="c-42212148">[1 more]</label></div><br/><div class="children"><div class="content">It is. As it stands, throw a loop around an LLM and act as the tape, and an LLM can obviously be made Turing complete (you can get it to execute all the steps of a minimal Turing machine, so drop temperature so its deterministic, and you have a Turing complete system). To argue that they <i>can&#x27;t</i> be made to reason is effectively to argue that there is some unknown aspect of the brain that allows us to compute functions not in the Turing computable set, which would be an astounding revelation if it could be proven. Until someone comes up with evidence for that, it is more reasonable to assume that it is a question of whether we have yet found a training mechanism that can lead to reasoning or not, not whether or not LLMs can learn to.</div><br/></div></div><div id="42208035" class="c"><input type="checkbox" id="c-42208035" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207913">parent</a><span>|</span><a href="#42212148">prev</a><span>|</span><a href="#42210106">next</a><span>|</span><label class="collapse" for="c-42208035">[-]</label><label class="expand" for="c-42208035">[9 more]</label></div><br/><div class="children"><div class="content">Mathematical reasoning is the most obvious area where it breaks down. This paper does an excellent job of proving this point with some elegant examples: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.05229" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.05229</a></div><br/><div id="42208462" class="c"><input type="checkbox" id="c-42208462" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208035">parent</a><span>|</span><a href="#42208559">next</a><span>|</span><label class="collapse" for="c-42208462">[-]</label><label class="expand" for="c-42208462">[5 more]</label></div><br/><div class="children"><div class="content">Sure, but <i>people</i> fail at mathematical reasoning. That doesn&#x27;t mean people are incapable of reasoning.<p>I&#x27;m not saying LLMs are perfect reasoners, I&#x27;m questioning the value of asserting that they cannot reason with some kind of &quot;it&#x27;s just text that looks like reasoning&quot; argument.</div><br/><div id="42208635" class="c"><input type="checkbox" id="c-42208635" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208462">parent</a><span>|</span><a href="#42210113">next</a><span>|</span><label class="collapse" for="c-42208635">[-]</label><label class="expand" for="c-42208635">[3 more]</label></div><br/><div class="children"><div class="content">People can communicate each step, and review each step as that communication is happening.<p>LLMs must be prompted for everything and don’t act on their own.<p>The value in the assertion is in preventing laymen from seeing a statistical guessing machine be correct and assuming that it always will be.<p>It’s dangerous to put so much faith in what in reality is a very good guessing machine.
You can ask it to retrace its steps, but it’s just guessing at what it’s steps were, since it didn’t actually go through real reasoning, just generated text that reads like reasoning steps.</div><br/><div id="42212237" class="c"><input type="checkbox" id="c-42212237" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208635">parent</a><span>|</span><a href="#42210079">next</a><span>|</span><label class="collapse" for="c-42212237">[-]</label><label class="expand" for="c-42212237">[1 more]</label></div><br/><div class="children"><div class="content">&gt; People can communicate each step, and review each step as that communication is happening.<p>Can, but don&#x27;t by default.<p>This is why we software developers have daily standup meetings, version control, and code review.<p>&gt; LLMs must be prompted for everything and don’t act on their own<p>And this is why we have task boards like JIRA, and quarterly goals set by management.</div><br/></div></div><div id="42210079" class="c"><input type="checkbox" id="c-42210079" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208635">parent</a><span>|</span><a href="#42212237">prev</a><span>|</span><a href="#42210113">next</a><span>|</span><label class="collapse" for="c-42210079">[-]</label><label class="expand" for="c-42210079">[1 more]</label></div><br/><div class="children"><div class="content">&gt; since it didn’t actually go through real reasoning, just generated text that reads like reasoning steps.<p>Can you elaborate on the difference? Are you bringing sentience into it? It kind of sounds like it from &quot;don&#x27;t act on their own&quot;. But reasoning and sentience are wildly different things.<p>&gt; It’s dangerous to put so much faith in what in reality is a very good guessing machine<p>Yes, exactly. That&#x27;s why I think it is good we are supplementing fallible humans with fallible LLMs; we already have the processes in place to assume that not every actor is infallible.</div><br/></div></div></div></div><div id="42210113" class="c"><input type="checkbox" id="c-42210113" checked=""/><div class="controls bullet"><span class="by">NBJack</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208462">parent</a><span>|</span><a href="#42208635">prev</a><span>|</span><a href="#42208559">next</a><span>|</span><label class="collapse" for="c-42210113">[-]</label><label class="expand" for="c-42210113">[1 more]</label></div><br/><div class="children"><div class="content">The idea is the average person would, sure. A mathematically oriented person would fair far better.<p>Throw all the math problems you want at a LLM for training; it will still fail if you step outside of the familiar.</div><br/></div></div></div></div><div id="42208559" class="c"><input type="checkbox" id="c-42208559" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208035">parent</a><span>|</span><a href="#42208462">prev</a><span>|</span><a href="#42210106">next</a><span>|</span><label class="collapse" for="c-42208559">[-]</label><label class="expand" for="c-42208559">[3 more]</label></div><br/><div class="children"><div class="content">Maybe I am not understanding the paper correctly, but it seems they tested &quot;state of the art models&quot; which is almost entirely composed of open source &lt;27B parameter models. Mostly 8B and 3B models. This is kind of like giving algebra problems to 7 year olds to &quot;test human algebra ability.&quot;<p>If you are holding up a 3B parameter model as an example of &quot;LLM&#x27;s can&#x27;t reason&quot; I&#x27;m not sure if the authors are confused or out of touch.<p>I mean, they do test 4o and O1 preview, but their performance is notablely absent from the paper&#x27;s conclusion.</div><br/><div id="42208641" class="c"><input type="checkbox" id="c-42208641" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208559">parent</a><span>|</span><a href="#42210106">next</a><span>|</span><label class="collapse" for="c-42208641">[-]</label><label class="expand" for="c-42208641">[2 more]</label></div><br/><div class="children"><div class="content">It’s difficult to reproducibly test openai models, since they can change from under you and you don’t have control over every hyperparameter.<p>It would’ve been nice to see one of the larger llama models though.</div><br/><div id="42208999" class="c"><input type="checkbox" id="c-42208999" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208641">parent</a><span>|</span><a href="#42210106">next</a><span>|</span><label class="collapse" for="c-42208999">[-]</label><label class="expand" for="c-42208999">[1 more]</label></div><br/><div class="children"><div class="content">The results are there, it&#x27;s just hidden away in the appendix. The result is that those models they don&#x27;t actually suffer drops on 4&#x2F;5 of their modified benchmarks. The one benchmark that does see actual drops that aren&#x27;t explained by margin of error is the benchmark that adds &quot;seemingly relevant but ultimately irrelevant information to problems&quot;<p>Those results are absent from the conclusion because the conclusion falls apart otherwise.</div><br/></div></div></div></div></div></div></div></div><div id="42210106" class="c"><input type="checkbox" id="c-42210106" checked=""/><div class="controls bullet"><span class="by">NBJack</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207913">parent</a><span>|</span><a href="#42208035">prev</a><span>|</span><a href="#42208602">next</a><span>|</span><label class="collapse" for="c-42210106">[-]</label><label class="expand" for="c-42210106">[3 more]</label></div><br/><div class="children"><div class="content">Inferring patterns in unfamiliar problems.<p>Take a common word problem in a 5th grade math text book. Now, change as many words as possible; instead of two trains, make it two different animals; change the location to a rarely discussed town; etc. Even better, invent words&#x2F;names to identify things.<p>Someone who has done a word problem like that will very likely recognize the logic, even if the setting is completely different.<p>Word tokenization alone should fail miserably.</div><br/><div id="42210678" class="c"><input type="checkbox" id="c-42210678" checked=""/><div class="controls bullet"><span class="by">djmips</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42210106">parent</a><span>|</span><a href="#42208602">next</a><span>|</span><label class="collapse" for="c-42210678">[-]</label><label class="expand" for="c-42210678">[2 more]</label></div><br/><div class="children"><div class="content">I have noted over my life that a lot of problems end up being a variation on solved problems from another more familiar domain but frustratingly take a long time to solve before realizing this was just like that thing you had already solved. Nevertheless, I do feel like humans do benefit from identifying meta patterns but as the chess example shows even we might be weak in unfamiliar areas.</div><br/><div id="42212067" class="c"><input type="checkbox" id="c-42212067" checked=""/><div class="controls bullet"><span class="by">Propelloni</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42210678">parent</a><span>|</span><a href="#42208602">next</a><span>|</span><label class="collapse" for="c-42212067">[-]</label><label class="expand" for="c-42212067">[1 more]</label></div><br/><div class="children"><div class="content">Learn how to solve one problem and apply the approach, logic and patterns to different problems. In German that&#x27;s called &quot;Transferleistung&quot; (roughly &quot;transfer success&quot;) and a big thing at advanced schools. Or, at least my teacher friends never stop talking about it.<p>We get better at it over time, as probably most of us can attest.</div><br/></div></div></div></div></div></div><div id="42208602" class="c"><input type="checkbox" id="c-42208602" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207913">parent</a><span>|</span><a href="#42210106">prev</a><span>|</span><a href="#42208687">next</a><span>|</span><label class="collapse" for="c-42208602">[-]</label><label class="expand" for="c-42208602">[1 more]</label></div><br/><div class="children"><div class="content">There isn’t much utility, but tbf the outputs aren’t identical.<p>One danger is the human assumption that, since something appears to have that capability in some settings, it will have that capability in all settings.<p>Thats a recipe for exploding bias, as we’ve seen with classic statistical crime detection systems.</div><br/></div></div></div></div><div id="42208687" class="c"><input type="checkbox" id="c-42208687" checked=""/><div class="controls bullet"><span class="by">xg15</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207812">parent</a><span>|</span><a href="#42207913">prev</a><span>|</span><a href="#42207941">next</a><span>|</span><label class="collapse" for="c-42208687">[-]</label><label class="expand" for="c-42208687">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t want to say that LLMs can reason, but this kind of argument always feels to shallow for me. It&#x27;s kind of like saying that bats cannot possibly fly because they have no feathers or that birds cannot have higher cognitive functions because they have no neocortex. (The latter having been an actual longstanding belief in science which has been disproven only a decade or so ago).<p>The &quot;next token prediction&quot; is just the API, it doesn&#x27;t tell you anything about the complexity of the thing that actually does the prediction. (In think there is some temptation to view LLMs as glorified Markov chains - they aren&#x27;t. They are just &quot;implementing the same API&quot; as Markov chains).<p>There is still a limit how much an LLM could reason during  prediction of a single token, as there is no recurrence between layers, so information can only be passed &quot;forward&quot;. But this limit doesn&#x27;t exist if you consider the generation of the entire text: Suddenly, you do have a recurrence, which is the prediction loop itself: The LLM can &quot;store&quot; information in a generated token and receive that information back as input in the next loop iteration.<p>I think this structure makes it quite hard to really say how much reasoning is possible.</div><br/><div id="42212175" class="c"><input type="checkbox" id="c-42212175" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208687">parent</a><span>|</span><a href="#42210018">next</a><span>|</span><label class="collapse" for="c-42212175">[-]</label><label class="expand" for="c-42212175">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But this limit doesn&#x27;t exist if you consider the generation of the entire text: Suddenly, you do have a recurrence, which is the prediction loop itself: The LLM can &quot;store&quot; information in a generated token and receive that information back as input in the next loop iteration.<p>Now consider that you can trivially show that you can get an LLM to &quot;execute&quot; on step of a Turing machine where the context is used as an IO channel, and will have shown it to be Turing complete.<p>&gt; I think this structure makes it quite hard to really say how much reasoning is possible.<p>Given the above, I think any argument that they can&#x27;t be made to reason is effectively an argument that humans can compute functions outside the Turing computable set, which we haven&#x27;t the slightest shred of evidence to suggest.</div><br/></div></div><div id="42210018" class="c"><input type="checkbox" id="c-42210018" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208687">parent</a><span>|</span><a href="#42212175">prev</a><span>|</span><a href="#42207941">next</a><span>|</span><label class="collapse" for="c-42210018">[-]</label><label class="expand" for="c-42210018">[3 more]</label></div><br/><div class="children"><div class="content">I agree with most of what you said, but “LLM can reason” is an <i>insanely huge claim</i> to make and most of the “evidence” so far is a mixture of corporate propaganda, “vibes”, and the like.<p>I’ve yet to see anything close to the level of evidence needed to support the claim.</div><br/><div id="42212228" class="c"><input type="checkbox" id="c-42212228" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42210018">parent</a><span>|</span><a href="#42212083">next</a><span>|</span><label class="collapse" for="c-42212228">[-]</label><label class="expand" for="c-42212228">[1 more]</label></div><br/><div class="children"><div class="content">To say <i>any specific</i> LLM can reason is a somewhat significant claim.<p>To say <i>LLMs as a class</i> is <i>architecturally able to be trained to reason</i> is - in the complete absence of evidence to suggest humans can compute functions outside the Turing computable - is effectively only an argument that they can implement a minimal Turing machine given the context is used as IO. Given the size of the rules needed to implement the smallest known Turing machines, it&#x27;d take a <i>really</i> tiny model for them to be unable to.<p>Now, you can then argue that it doesn&#x27;t &quot;count&quot; if it needs to be fed a huge program step by step via IO, but if it <i>can</i> do something that way, I&#x27;d need some really convincing evidence for why the static elements those steps could not progressively be embedded into a model.</div><br/></div></div><div id="42212083" class="c"><input type="checkbox" id="c-42212083" checked=""/><div class="controls bullet"><span class="by">Propelloni</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42210018">parent</a><span>|</span><a href="#42212228">prev</a><span>|</span><a href="#42207941">next</a><span>|</span><label class="collapse" for="c-42212083">[-]</label><label class="expand" for="c-42212083">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s largely dependent on what we think &quot;reason&quot; means, is it not? That&#x27;s not a pro argument from me, in my world LLMs are stochastic parrots.</div><br/></div></div></div></div></div></div><div id="42207941" class="c"><input type="checkbox" id="c-42207941" checked=""/><div class="controls bullet"><span class="by">hathawsh</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207812">parent</a><span>|</span><a href="#42208687">prev</a><span>|</span><a href="#42208809">next</a><span>|</span><label class="collapse" for="c-42207941">[-]</label><label class="expand" for="c-42207941">[7 more]</label></div><br/><div class="children"><div class="content">I think the question we&#x27;re grappling with is whether token prediction may be more tightly related to symbolic logic than we all expected. Today&#x27;s LLMs are so uncannily good at faking logic that it&#x27;s making me ponder logic itself.</div><br/><div id="42208042" class="c"><input type="checkbox" id="c-42208042" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207941">parent</a><span>|</span><a href="#42208809">next</a><span>|</span><label class="collapse" for="c-42208042">[-]</label><label class="expand" for="c-42208042">[6 more]</label></div><br/><div class="children"><div class="content">I felt the same way about a year ago, I’ve since changed my mind based on personal experience and new research.</div><br/><div id="42208129" class="c"><input type="checkbox" id="c-42208129" checked=""/><div class="controls bullet"><span class="by">hathawsh</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208042">parent</a><span>|</span><a href="#42208809">next</a><span>|</span><label class="collapse" for="c-42208129">[-]</label><label class="expand" for="c-42208129">[5 more]</label></div><br/><div class="children"><div class="content">Please elaborate.</div><br/><div id="42208655" class="c"><input type="checkbox" id="c-42208655" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208129">parent</a><span>|</span><a href="#42208809">next</a><span>|</span><label class="collapse" for="c-42208655">[-]</label><label class="expand" for="c-42208655">[4 more]</label></div><br/><div class="children"><div class="content">I work in the LLM search space and echo OC’s sentiment.<p>The more I work with LLMs the more the magic falls away and I see that they are just very good at guessing text.<p>It’s very apparent when I want to get them to do a very specific thing. 
They get inconsistent about it.</div><br/><div id="42212238" class="c"><input type="checkbox" id="c-42212238" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208655">parent</a><span>|</span><a href="#42209949">next</a><span>|</span><label class="collapse" for="c-42212238">[-]</label><label class="expand" for="c-42212238">[1 more]</label></div><br/><div class="children"><div class="content">I do contract work in the LLM space which involves me seeing a lot of human prompts, and its made the magic of <i>human</i> reasoning fall away: Humans are shocking bad at reasoning on the large.<p>One of the things I find extremely frustrating is that almost no research on LLM reasoning ability <i>benchmarks them against average humans</i>.<p>Large proportions of humans struggle to comprehend even a moderately complex sentence with any level of precision.</div><br/></div></div><div id="42209949" class="c"><input type="checkbox" id="c-42209949" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208655">parent</a><span>|</span><a href="#42212238">prev</a><span>|</span><a href="#42208809">next</a><span>|</span><label class="collapse" for="c-42209949">[-]</label><label class="expand" for="c-42209949">[2 more]</label></div><br/><div class="children"><div class="content">Pretty much the same, I work on some fairly specific document retrieval and labeling problems. After some initial excitement I’ve landed on using LLM to help train smaller, more focused, models for specific tasks.<p>Translation is a task I’ve had good results with, particularly mistral models. Which makes sense as it’s basically just “repeat this series of tokens with modifications”.<p>The closed models are practically useless from an empirical standpoint as you have no idea if the model you use Monday is the same as Tuesday. “Open” models at least negate this issue.<p>Likewise, I’ve found LLM code to be of poor quality. I think that has to do with being a very experienced and skilled programmer. What the LLM produce is at best the top answer in stack overflow-level skill.  The top answers on stack overflow are typically not optimal solutions, they are solutions up voted by novices.<p>I find LLM code is not only bad, but when I point this out the LLM then “apologizes” and gives better code. My worry is inexperienced people can’t even spot that and won’t get this best answer.<p>In fact try this - ask an LLM to generate some code then reply with “isn’t there a simpler, more maintainable, and straightforward way to do this?”</div><br/><div id="42211892" class="c"><input type="checkbox" id="c-42211892" checked=""/><div class="controls bullet"><span class="by">blharr</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42209949">parent</a><span>|</span><a href="#42208809">next</a><span>|</span><label class="collapse" for="c-42211892">[-]</label><label class="expand" for="c-42211892">[1 more]</label></div><br/><div class="children"><div class="content">There have even been times where an LLM will spit out _the exact same code_ and you have to give it the answer or a hint how to do it better</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42208809" class="c"><input type="checkbox" id="c-42208809" checked=""/><div class="controls bullet"><span class="by">Uehreka</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207812">parent</a><span>|</span><a href="#42207941">prev</a><span>|</span><a href="#42208757">next</a><span>|</span><label class="collapse" for="c-42208809">[-]</label><label class="expand" for="c-42208809">[3 more]</label></div><br/><div class="children"><div class="content">Does anyone have a hard proof that language doesn’t somehow encode reasoning in a deeper way than we commonly think?<p>I constantly hear people saying “they’re not intelligent, they’re just predicting the next token in a sequence”, and I’ll grant that I don’t think of what’s going on in my head as “predicting the next token in a sequence”, but I’ve seen enough surprising studies about the nature of free will and such that I no longer put a lot of stock in what seems “obvious” to me about how my brain works.</div><br/><div id="42209499" class="c"><input type="checkbox" id="c-42209499" checked=""/><div class="controls bullet"><span class="by">spiffytech</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208809">parent</a><span>|</span><a href="#42208757">next</a><span>|</span><label class="collapse" for="c-42209499">[-]</label><label class="expand" for="c-42209499">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I’ll grant that I don’t think of what’s going on in my head as “predicting the next token in a sequence”<p>I can&#x27;t speak to whether LLMs can think, but current evidence indicates humans can perform complex reasoning without the use of language:<p>&gt; Brain studies show that language is not essential for the cognitive processes that underlie thought.<p>&gt; For the question of how language relates to systems of thought, the most informative cases are cases of really severe impairments, so-called global aphasia, where individuals basically lose completely their ability to understand and produce language as a result of massive damage to the left hemisphere of the brain. ...<p>&gt; You can ask them to solve some math problems or to perform a social reasoning test, and all of the instructions, of course, have to be nonverbal because they can’t understand linguistic information anymore. ...<p>&gt; There are now dozens of studies that we’ve done looking at all sorts of nonlinguistic inputs and tasks, including many thinking tasks. We find time and again that the language regions are basically silent when people engage in these thinking activities.<p><a href="https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;you-dont-need-words-to-think&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;you-dont-need-wor...</a></div><br/><div id="42210379" class="c"><input type="checkbox" id="c-42210379" checked=""/><div class="controls bullet"><span class="by">SAI_Peregrinus</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42209499">parent</a><span>|</span><a href="#42208757">next</a><span>|</span><label class="collapse" for="c-42210379">[-]</label><label class="expand" for="c-42210379">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say that&#x27;s a separate problem. It&#x27;s not &quot;is the use of language necessary for reasoning?&quot; which seems to be obviously answered &quot;no&quot;, but rather &quot;is the use of language sufficient for reasoning?&quot;.</div><br/></div></div></div></div></div></div><div id="42208757" class="c"><input type="checkbox" id="c-42208757" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207812">parent</a><span>|</span><a href="#42208809">prev</a><span>|</span><a href="#42207836">next</a><span>|</span><label class="collapse" for="c-42208757">[-]</label><label class="expand" for="c-42208757">[1 more]</label></div><br/><div class="children"><div class="content">After reading the article I am more convinced it does reasoning. The base model&#x27;s reasoning capabilities are partly hidden by the chatty derived model&#x27;s logic.</div><br/></div></div><div id="42207836" class="c"><input type="checkbox" id="c-42207836" checked=""/><div class="controls bullet"><span class="by">DiogenesKynikos</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207812">parent</a><span>|</span><a href="#42208757">prev</a><span>|</span><a href="#42207742">next</a><span>|</span><label class="collapse" for="c-42207836">[-]</label><label class="expand" for="c-42207836">[8 more]</label></div><br/><div class="children"><div class="content">Effective next-token prediction requires reasoning.<p>You can also say humans are &quot;just XYZ biological system,&quot; but that doesn&#x27;t mean they don&#x27;t reason. The same goes for LLMs.</div><br/><div id="42207842" class="c"><input type="checkbox" id="c-42207842" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207836">parent</a><span>|</span><a href="#42207742">next</a><span>|</span><label class="collapse" for="c-42207842">[-]</label><label class="expand" for="c-42207842">[7 more]</label></div><br/><div class="children"><div class="content">Take a word problem for example. A child will be told the first step is to translate the problem from human language to mathematical notation (symbolic representation), then solve the math (logic).<p>A human doesn’t use next token prediction to solve word problems.</div><br/><div id="42208053" class="c"><input type="checkbox" id="c-42208053" checked=""/><div class="controls bullet"><span class="by">Majromax</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207842">parent</a><span>|</span><a href="#42208231">next</a><span>|</span><label class="collapse" for="c-42208053">[-]</label><label class="expand" for="c-42208053">[4 more]</label></div><br/><div class="children"><div class="content">But the LLM isn&#x27;t &quot;using next-token prediction&quot; to solve the problem, that&#x27;s only how it&#x27;s evaluated.<p>The &quot;real processing&quot; happens through the various transformer layers (and token-wise nonlinear networks), where it seems as if progressively richer meanings are added to each token.  That rich feature set then <i>decodes</i> to the next predicted token, but that decoding step is throwing away a lot of information contained in the latent space.<p>If language models (per Anthropic&#x27;s work) can have a direction in latent space correspond to the concept of the Golden Gate Bridge, then I think it&#x27;s reasonable (albeit far from certain) to say that LLMs are performing some kind of symbolic-ish reasoning.</div><br/><div id="42208120" class="c"><input type="checkbox" id="c-42208120" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208053">parent</a><span>|</span><a href="#42208231">next</a><span>|</span><label class="collapse" for="c-42208120">[-]</label><label class="expand" for="c-42208120">[3 more]</label></div><br/><div class="children"><div class="content">Anthropic had a vested interest in people thinking Claude is reasoning.<p>However, in coding tasks I’ve been able to find it directly regurgitating Stack overflow answers (like literally a google search turns up the code).<p>Giving coding is supposed to be Claude’s strength, and it’s clearly just parroting web data, I’m not seeing any sort of “reasoning”.<p>LLM may be <i>useful</i> but they don’t <i>think</i>. They’ve already plateaued, and given the absurd energy requirements I think they will prove to be far less impactful than people think.</div><br/><div id="42209157" class="c"><input type="checkbox" id="c-42209157" checked=""/><div class="controls bullet"><span class="by">DiogenesKynikos</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42208120">parent</a><span>|</span><a href="#42208231">next</a><span>|</span><label class="collapse" for="c-42209157">[-]</label><label class="expand" for="c-42209157">[2 more]</label></div><br/><div class="children"><div class="content">The claim that Claude is just regurgitating answers from Stackoverflow is not tenable, if you&#x27;ve spent time interacting with it.<p>You can give Claude a complex, novel problem, and it will give you a reasonable solution, which it will be able to explain to you and discuss with you.<p>You&#x27;re getting hung up on the fact that LLMs are trained on next-token prediction. I could equally dismiss human intelligence: &quot;The human brain is just a biological neural network that is adapted to maximize the chance of creating successful offspring.&quot; Sure, but the way it solves that task is clearly intelligent.</div><br/><div id="42210033" class="c"><input type="checkbox" id="c-42210033" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42209157">parent</a><span>|</span><a href="#42208231">next</a><span>|</span><label class="collapse" for="c-42210033">[-]</label><label class="expand" for="c-42210033">[1 more]</label></div><br/><div class="children"><div class="content">I’ve literally spent 100s of hours with it. I’m mystified why so many people use the “you’re holding it wrong” explanation when somebody points out real limitations.</div><br/></div></div></div></div></div></div></div></div><div id="42208231" class="c"><input type="checkbox" id="c-42208231" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207842">parent</a><span>|</span><a href="#42208053">prev</a><span>|</span><a href="#42208293">next</a><span>|</span><label class="collapse" for="c-42208231">[-]</label><label class="expand" for="c-42208231">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>A human doesn’t use next token prediction to solve word problems.</i><p>Of course they do, unless they&#x27;re particularly conscientious noobs that are able to repeatedly execute the &quot;translate to mathematical notation, then solve the math&quot; algorithm, without going insane. But those people are the exception.<p>Everyone else either gets bored half-way through reading the problem, or has already done dozens of similar problems before, or both - and jump straight to &quot;next token prediction&quot;, aka. searching the problem space &quot;by feels&quot;, and checking candidate solutions to sub-problems on the fly.<p>This kind of methodical approach you mention? We leave that to symbolic math software. The &quot;next token prediction&quot; approach is something we call &quot;experience&quot;&#x2F;&quot;expertise&quot; and a source of the thing we call &quot;insight&quot;.</div><br/></div></div><div id="42208293" class="c"><input type="checkbox" id="c-42208293" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207842">parent</a><span>|</span><a href="#42208231">prev</a><span>|</span><a href="#42207742">next</a><span>|</span><label class="collapse" for="c-42208293">[-]</label><label class="expand" for="c-42208293">[1 more]</label></div><br/><div class="children"><div class="content">is that based on a vigorous understanding of how humans think, derived from watching people (children) learn to solve word problems? How do thoughts get formed? Because I remember being given word problems with extra information, and some children trying to shove that information into a math equation despite it not being relevant. The &quot;think things though&quot; portion of ChatGPT o1-preview is hidden from us, so even though a o1-preview can solve word problems, we don&#x27;t know how it internally computes to arrive at that answer. But we do we <i>really</i> know how we do it? We can&#x27;t even explain consciousness in the first place.</div><br/></div></div></div></div></div></div></div></div><div id="42207742" class="c"><input type="checkbox" id="c-42207742" checked=""/><div class="controls bullet"><span class="by">BurningFrog</span><span>|</span><a href="#42207156">parent</a><span>|</span><a href="#42207812">prev</a><span>|</span><a href="#42212020">next</a><span>|</span><label class="collapse" for="c-42207742">[-]</label><label class="expand" for="c-42207742">[10 more]</label></div><br/><div class="children"><div class="content">Not that I understand the internals of current AI tech, but...<p>I&#x27;d expect that an AI that has seen billions of chess positions, and the moves played in them, can figure out the rules for legal moves without being told?</div><br/><div id="42207801" class="c"><input type="checkbox" id="c-42207801" checked=""/><div class="controls bullet"><span class="by">rscho</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207742">parent</a><span>|</span><a href="#42207815">next</a><span>|</span><label class="collapse" for="c-42207801">[-]</label><label class="expand" for="c-42207801">[7 more]</label></div><br/><div class="children"><div class="content">Statistical &#x27;AI&#x27; doesn&#x27;t &#x27;understand&#x27; anything, strictly speaking. It predicts a move with high probability, which could be legal or illegal.</div><br/><div id="42207852" class="c"><input type="checkbox" id="c-42207852" checked=""/><div class="controls bullet"><span class="by">Helonomoto</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207801">parent</a><span>|</span><a href="#42208372">next</a><span>|</span><label class="collapse" for="c-42207852">[-]</label><label class="expand" for="c-42207852">[3 more]</label></div><br/><div class="children"><div class="content">How do you define &#x27;understand&#x27;?<p>There is plenty of AI which learns the rules of games like Alpha Zero.<p>LLMs might not have the architecture to &#x27;learn&#x27;, but it also might. If it optimizes all possible moves one chess peace can do (which is not that much to learn) it can easily only &#x27;move&#x27; from one game set to another by this type of dictionary.</div><br/><div id="42211795" class="c"><input type="checkbox" id="c-42211795" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207852">parent</a><span>|</span><a href="#42211464">next</a><span>|</span><label class="collapse" for="c-42211795">[-]</label><label class="expand" for="c-42211795">[1 more]</label></div><br/><div class="children"><div class="content">Neither AlphaZero nor MuZero can learn the rules of chess from an empty chess board and a pile of pieces. There is no objective function so there’s nothing to train upon.<p>That would be like alien archaeologists of the future finding a chess board and some pieces in a capsule orbiting Mars after the total destruction of Earth and all recorded human thought. The archaeologists could invent their own games to play on the chess board but they’d have no way of ever knowing they were playing chess.</div><br/></div></div><div id="42211464" class="c"><input type="checkbox" id="c-42211464" checked=""/><div class="controls bullet"><span class="by">rscho</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207852">parent</a><span>|</span><a href="#42211795">prev</a><span>|</span><a href="#42208372">next</a><span>|</span><label class="collapse" for="c-42211464">[-]</label><label class="expand" for="c-42211464">[1 more]</label></div><br/><div class="children"><div class="content">Understanding a rules-based system (chess) means to be able to learn non-probabilistic rules (an abstraction over the concrete world). Humans are a mix of symbolic and probabilistic learning, allowing them to get a huge boost in performance by admitting rules. It doesn&#x27;t mean a human will never make an illegal move, but it means a much smaller probability of illegal move based on less training data. Asymptotically, performance from humans and purely probabilistic systems converge. But that also means that in appropriate situations, humans are hugely more data-efficient.</div><br/></div></div></div></div><div id="42208372" class="c"><input type="checkbox" id="c-42208372" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207801">parent</a><span>|</span><a href="#42207852">prev</a><span>|</span><a href="#42207832">next</a><span>|</span><label class="collapse" for="c-42208372">[-]</label><label class="expand" for="c-42208372">[1 more]</label></div><br/><div class="children"><div class="content">The illegal moves are interesting as it goes to &quot;understanding&quot;. In children learning to play chess, how often do they try and make illegal moves? When first learning the game I remember that I&#x27;d lose track of all the things going on at once and try to make illegal moves, but eventually the rules became second nature and I stopped trying to make illegal moves. With an ELO of 1800, I&#x27;d expect ChatGPT not to make any illegal moves.</div><br/></div></div><div id="42207832" class="c"><input type="checkbox" id="c-42207832" checked=""/><div class="controls bullet"><span class="by">griomnib</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207801">parent</a><span>|</span><a href="#42208372">prev</a><span>|</span><a href="#42207815">next</a><span>|</span><label class="collapse" for="c-42207832">[-]</label><label class="expand" for="c-42207832">[2 more]</label></div><br/><div class="children"><div class="content">Likewise with LLM you don’t know if it is truly in the “chess” branch of the statistical distribution or it is picking up something else entirely, like some arcane overlap of tokens.<p>So much of the training data (eg common crawl, pile, Reddit) is dogshit, so it generates reheated dogshit.</div><br/><div id="42207902" class="c"><input type="checkbox" id="c-42207902" checked=""/><div class="controls bullet"><span class="by">Helonomoto</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207832">parent</a><span>|</span><a href="#42207815">next</a><span>|</span><label class="collapse" for="c-42207902">[-]</label><label class="expand" for="c-42207902">[1 more]</label></div><br/><div class="children"><div class="content">You generalize this without mentioning that there are LLMs which do not just use random &#x27;dogshit&#x27;.<p>Also what does a normal human do? It looks around how to move one random piece and it uses a very small dictionary &#x2F; set of basic rules to move it. I do not remember me learning to count every piece and its options by looking up that rulebook. I learned to &#x27;see&#x27; how i can move one type of chess piece.<p>If a LLM uses only these piece moves on a mathematical level, it would do the same thing as i do.<p>And yes there is also absolutly the option for an LLM to learn some kind of meta game.</div><br/></div></div></div></div></div></div><div id="42207815" class="c"><input type="checkbox" id="c-42207815" checked=""/><div class="controls bullet"><span class="by">pvitz</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207742">parent</a><span>|</span><a href="#42207801">prev</a><span>|</span><a href="#42207835">next</a><span>|</span><label class="collapse" for="c-42207815">[-]</label><label class="expand" for="c-42207815">[1 more]</label></div><br/><div class="children"><div class="content">A system that would just output the most probable tokens based on the text it was fed and trained on the games played by players with ratings greater than 1800 would certainly fail to output the right moves to totally unlikely board positions.</div><br/></div></div><div id="42207835" class="c"><input type="checkbox" id="c-42207835" checked=""/><div class="controls bullet"><span class="by">Helonomoto</span><span>|</span><a href="#42207156">root</a><span>|</span><a href="#42207742">parent</a><span>|</span><a href="#42207815">prev</a><span>|</span><a href="#42212020">next</a><span>|</span><label class="collapse" for="c-42207835">[-]</label><label class="expand" for="c-42207835">[1 more]</label></div><br/><div class="children"><div class="content">Yes in theory it could. Depends on how it learns. Does it learn by memorization or by learning the rules. It depends on the architecture and the amount of &#x27;pressure&#x27; you put on it to be more efficient or not.</div><br/></div></div></div></div><div id="42212020" class="c"><input type="checkbox" id="c-42212020" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42207156">parent</a><span>|</span><a href="#42207742">prev</a><span>|</span><a href="#42207179">next</a><span>|</span><label class="collapse" for="c-42212020">[-]</label><label class="expand" for="c-42212020">[1 more]</label></div><br/><div class="children"><div class="content">Its training set would include a lot of randomly generated positions like that that then get played out by chess engines wouldn&#x27;t it?  Just from people messing around andbposting results. Not identical ones, but similarly oddball.</div><br/></div></div><div id="42208111" class="c"><input type="checkbox" id="c-42208111" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42207156">parent</a><span>|</span><a href="#42207179">prev</a><span>|</span><a href="#42211856">next</a><span>|</span><label class="collapse" for="c-42208111">[-]</label><label class="expand" for="c-42208111">[1 more]</label></div><br/><div class="children"><div class="content">How well does it play modified versions of chess? eg, a modified opening board like the back row is all knights, or modified movement eg rooks can move like a queen. A human should be able to reason their way through playing a modified game, but I&#x27;d expect an LLM, if it&#x27;s just parroting its training data, to suggest illegal moves, or stick to previously legal moves.</div><br/></div></div></div></div><div id="42211856" class="c"><input type="checkbox" id="c-42211856" checked=""/><div class="controls bullet"><span class="by">Jean-Papoulos</span><span>|</span><a href="#42207156">prev</a><span>|</span><a href="#42211857">next</a><span>|</span><label class="collapse" for="c-42211856">[-]</label><label class="expand" for="c-42211856">[2 more]</label></div><br/><div class="children"><div class="content">&gt;According to that figure, fine-tuning helps. And examples help. But it’s examples that make fine-tuning redundant, not the other way around.<p>This is extremely interesting. In this specific case at least, simply giving examples is equivalent to fine-tuning. This is a great discovery for me, I&#x27;ll try using examples more often.</div><br/><div id="42211958" class="c"><input type="checkbox" id="c-42211958" checked=""/><div class="controls bullet"><span class="by">jdthedisciple</span><span>|</span><a href="#42211856">parent</a><span>|</span><a href="#42211857">next</a><span>|</span><label class="collapse" for="c-42211958">[-]</label><label class="expand" for="c-42211958">[1 more]</label></div><br/><div class="children"><div class="content">To me this is very intuitively true.<p>I can&#x27;t explain why.I always had the intuition that fine-tuning was overrated.<p>One reason perhaps is that examples are &quot;right there&quot; and thus implicitly weighted much more in relation to the fine-tuned neurons.</div><br/></div></div></div></div><div id="42211857" class="c"><input type="checkbox" id="c-42211857" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#42211856">prev</a><span>|</span><a href="#42209165">next</a><span>|</span><label class="collapse" for="c-42211857">[-]</label><label class="expand" for="c-42211857">[3 more]</label></div><br/><div class="children"><div class="content">I notice there&#x27;s no prompt saying &quot;you should try to win the game&quot; yet the results are measured by how much the LLM wins.<p>Is this implicit in the &quot;you are a grandmaster chess player&quot; prompt?<p>Is there some part of the LLM training that does &quot;if this is a game, then I will always try to win&quot;?<p>Could the author improve the LLM&#x27;s odds of winning just by telling it to try and win?</div><br/><div id="42211908" class="c"><input type="checkbox" id="c-42211908" checked=""/><div class="controls bullet"><span class="by">Nashooo</span><span>|</span><a href="#42211857">parent</a><span>|</span><a href="#42209165">next</a><span>|</span><label class="collapse" for="c-42211908">[-]</label><label class="expand" for="c-42211908">[2 more]</label></div><br/><div class="children"><div class="content">IMO this is clearly implicit in the   &quot;you are a grandmaster chess player&quot; prompt. As that should make generating best possible move tokens more likely.</div><br/><div id="42212146" class="c"><input type="checkbox" id="c-42212146" checked=""/><div class="controls bullet"><span class="by">Ferret7446</span><span>|</span><a href="#42211857">root</a><span>|</span><a href="#42211908">parent</a><span>|</span><a href="#42209165">next</a><span>|</span><label class="collapse" for="c-42212146">[-]</label><label class="expand" for="c-42212146">[1 more]</label></div><br/><div class="children"><div class="content">Is it?  What if the AI is better than a grandmaster chess player and is generating the most likely next move that a grandmaster chess player might make and not the most likely move to win, which may be different?</div><br/></div></div></div></div></div></div><div id="42209165" class="c"><input type="checkbox" id="c-42209165" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#42211857">prev</a><span>|</span><a href="#42208167">next</a><span>|</span><label class="collapse" for="c-42209165">[-]</label><label class="expand" for="c-42209165">[1 more]</label></div><br/><div class="children"><div class="content">People have to quit this kind of stumbling in the dark with commercial LLMs.<p>To get to the bottom of this it would be interesting to train LLMs on nothing but chess games (can synthesize them endlessly by having Stockfish play against itself) with maybe a side helping of chess commentary and examples of chess dialogs “how many pawns are on the board?”, “where are my rooks?”, “draw the board”, competence at which would demonstrate that it has a representation of the board.<p>I don’t believe in “emergent phenomena” or that the general linguistic competence or ability to feign competence is necessary for chess playing (being smart at chess doesn’t mean you are smart at other things and vice versa). With experiments like this you might prove me wrong though.<p>This paper came out about a week ago<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2411.06655" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2411.06655</a><p>seems to get good results with a fine-tuned Llama.  I also like this one as it is about competence in chess commentary<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.20811" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.20811</a></div><br/></div></div><div id="42208167" class="c"><input type="checkbox" id="c-42208167" checked=""/><div class="controls bullet"><span class="by">xg15</span><span>|</span><a href="#42209165">prev</a><span>|</span><a href="#42207317">next</a><span>|</span><label class="collapse" for="c-42208167">[-]</label><label class="expand" for="c-42208167">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>In many ways, this feels less like engineering and more like a search for spells.</i><p>This is still my impression of LLMs in general. It&#x27;s amazing that they work, but for the next tech disruption, I&#x27;d appreciate something that doesn&#x27;t make you feel like being in a bad sci-fi movie all the time.</div><br/></div></div><div id="42207317" class="c"><input type="checkbox" id="c-42207317" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42208167">prev</a><span>|</span><a href="#42208222">next</a><span>|</span><label class="collapse" for="c-42207317">[-]</label><label class="expand" for="c-42207317">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad he improved the promoting, but he&#x27;s still leaving out two likely huge improvements.<p>1. Explain the current board position and the plan going forwards, <i>before</i> proposing a move. This lets the model actually think more, kind of like o1, but here it would guarantee a more focused processing.<p>2. Actually draw the ascii board for each step. Hopefully producing more valid moves since board + move is easier to reliably process than 20×move.</div><br/><div id="42212235" class="c"><input type="checkbox" id="c-42212235" checked=""/><div class="controls bullet"><span class="by">tedsanders</span><span>|</span><a href="#42207317">parent</a><span>|</span><a href="#42207518">next</a><span>|</span><label class="collapse" for="c-42212235">[-]</label><label class="expand" for="c-42212235">[1 more]</label></div><br/><div class="children"><div class="content">Chain of thought helps with many problems, but it actually tanks GPT’s chess performance.</div><br/></div></div><div id="42207518" class="c"><input type="checkbox" id="c-42207518" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#42207317">parent</a><span>|</span><a href="#42212235">prev</a><span>|</span><a href="#42208663">next</a><span>|</span><label class="collapse" for="c-42207518">[-]</label><label class="expand" for="c-42207518">[2 more]</label></div><br/><div class="children"><div class="content">&gt; 2. Actually draw the ascii board for each step.<p>I doubt that this is going to make much difference. 2D &quot;graphics&quot; like ASCII art are foreign to language models - the models perceive text as a stream of tokens (including newlines), so &quot;vertical&quot; relationships between lines of text aren&#x27;t obvious to them like they would be to a human viewer. Having that board diagram in the context window isn&#x27;t likely to help the model reason about the game.<p>Having the model list out the positions of each piece on the board in plain text (e.g. &quot;Black knight at c5&quot;) might be a more suitable way to reinforce the model&#x27;s positional awareness.</div><br/><div id="42209656" class="c"><input type="checkbox" id="c-42209656" checked=""/><div class="controls bullet"><span class="by">yccs27</span><span>|</span><a href="#42207317">root</a><span>|</span><a href="#42207518">parent</a><span>|</span><a href="#42208663">next</a><span>|</span><label class="collapse" for="c-42209656">[-]</label><label class="expand" for="c-42209656">[1 more]</label></div><br/><div class="children"><div class="content">With positional encoding, an ascii board diagram actually shouldn&#x27;t be that hard to read for an LLM. Columns and diagonals are just different strides through the flattened board representation.</div><br/></div></div></div></div><div id="42208663" class="c"><input type="checkbox" id="c-42208663" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42207317">parent</a><span>|</span><a href="#42207518">prev</a><span>|</span><a href="#42207619">next</a><span>|</span><label class="collapse" for="c-42208663">[-]</label><label class="expand" for="c-42208663">[1 more]</label></div><br/><div class="children"><div class="content">RE 2., I doubt it&#x27;ll help - for at least two reasons, already mentioned by &#x27;duskwuff and &#x27;daveguy.<p>RE 1., definitely worth trying, and there&#x27;s more variants of such tricks specific to models. I&#x27;m out of date on OpenAI docs, but with Anthropic models, the docs suggest <i>using XML notation</i> to label and categorize most important parts of the input. This kind of soft structure seems to improve the results coming from Claude models; I imagine they specifically trained the model to recognize it.<p>See: <a href="https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;build-with-claude&#x2F;prompt-engineering&#x2F;use-xml-tags" rel="nofollow">https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;build-with-claude&#x2F;prompt-...</a><p>In author&#x27;s case, for Anthropic models, the final prompt could look like this:<p><pre><code>  &lt;role&gt;You are a chess grandmaster.&lt;&#x2F;role&gt;
  &lt;instructions&gt;
  You will be given a partially completed game, contained in &lt;game-log&gt; tags.
  After seeing it, you should repeat the ENTIRE GAME and then give ONE new move
  Use standard algebraic notation, e.g. &quot;e4&quot; or &quot;Rdf8&quot; or &quot;R1a3&quot;.
  ALWAYS repeat the entire representation of the game so far, putting it in &lt;new-game-log&gt; tags.
  Before giving the new game log, explain your reasoning inside &lt;thinking&gt; tag block.
  &lt;&#x2F;instructions&gt;
  
  &lt;example&gt;
    &lt;request&gt;
      &lt;game-log&gt;
        *** example game ***
      &lt;&#x2F;game-log&gt;
    &lt;&#x2F;request&gt;
    &lt;reply&gt;
      &lt;thinking&gt; *** some example explanation ***&lt;&#x2F;thinking&gt;
      &lt;new-game-log&gt; *** game log + next move *** &lt;&#x2F;new-game-log&gt;
    &lt;&#x2F;reply&gt;   
   
  &lt;&#x2F;example&gt;
  
  &lt;game-log&gt;
   *** the incomplete game goes here ***
  &lt;&#x2F;game-log&gt;
</code></pre>
This kind of prompting is supposed to provide noticeable improvement for Anthropic models. Ironically, I only discovered it few weeks ago, despite having been using Claude 3.5 Sonnet extensively for months. Which goes to say, <i>RTFM is still a useful skill</i>. Maybe OpenAI models have similar affordances too, simple but somehow unnoticed? (I&#x27;ll re-check the docs myself later.)</div><br/></div></div><div id="42207619" class="c"><input type="checkbox" id="c-42207619" checked=""/><div class="controls bullet"><span class="by">daveguy</span><span>|</span><a href="#42207317">parent</a><span>|</span><a href="#42208663">prev</a><span>|</span><a href="#42207408">next</a><span>|</span><label class="collapse" for="c-42207619">[-]</label><label class="expand" for="c-42207619">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Actually draw the ascii board for each step.<p>The relative rarity of this representation in training data means it would probably degrade responses rather than improve them. I&#x27;d like to see the results of this, because I would be very surprised if it improved the responses.</div><br/></div></div><div id="42207408" class="c"><input type="checkbox" id="c-42207408" checked=""/><div class="controls bullet"><span class="by">unoti</span><span>|</span><a href="#42207317">parent</a><span>|</span><a href="#42207619">prev</a><span>|</span><a href="#42208200">next</a><span>|</span><label class="collapse" for="c-42207408">[-]</label><label class="expand" for="c-42207408">[3 more]</label></div><br/><div class="children"><div class="content">I came here to basically say the same thing.  The improvements the OP saw by asking it to repeat all the moves so far gives the LLM more time and space to think.  I have this hypothesis giving it more time and space to think in other ways could improve performance even more, something like showing the current board position and asking it to perform an analysis of the position, list key challenges and strengths, asking it for a list of strategies possible from here, then asking it to select a strategy amongst the listed strategies, then asking it for its move.  In general, asking it to really think rather than blurt out a move.  The examples would be key here.<p>These ideas were proven to work very well in the ReAct paper (and by extension, the CoT Chain of Thought paper).  Could also extend this by asking it to do this N times and stop when we get the same answer a majority of times (this is an idea stolen from the CoT-SC paper, chain of through self-consistency).</div><br/><div id="42207503" class="c"><input type="checkbox" id="c-42207503" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#42207317">root</a><span>|</span><a href="#42207408">parent</a><span>|</span><a href="#42208200">next</a><span>|</span><label class="collapse" for="c-42207503">[-]</label><label class="expand" for="c-42207503">[2 more]</label></div><br/><div class="children"><div class="content">It would be awesome if the author released a framework to play with this. I&#x27;d like to test things out, but I don&#x27;t want to spend time redoing all his work from scratch.</div><br/><div id="42208387" class="c"><input type="checkbox" id="c-42208387" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42207317">root</a><span>|</span><a href="#42207503">parent</a><span>|</span><a href="#42208200">next</a><span>|</span><label class="collapse" for="c-42208387">[-]</label><label class="expand" for="c-42208387">[1 more]</label></div><br/><div class="children"><div class="content">Just have ChatGPT write the framework</div><br/></div></div></div></div></div></div><div id="42208200" class="c"><input type="checkbox" id="c-42208200" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#42207317">parent</a><span>|</span><a href="#42207408">prev</a><span>|</span><a href="#42208222">next</a><span>|</span><label class="collapse" for="c-42208200">[-]</label><label class="expand" for="c-42208200">[1 more]</label></div><br/><div class="children"><div class="content">The fact that he hasn&#x27;t tried this leads me to think that deep down he doesn&#x27;t want the models to succeed and really just wants to make more charts.</div><br/></div></div></div></div><div id="42208222" class="c"><input type="checkbox" id="c-42208222" checked=""/><div class="controls bullet"><span class="by">jey</span><span>|</span><a href="#42207317">prev</a><span>|</span><a href="#42211830">next</a><span>|</span><label class="collapse" for="c-42208222">[-]</label><label class="expand" for="c-42208222">[1 more]</label></div><br/><div class="children"><div class="content">Could be interesting to create a tokenizer that’s optimized for representing chess moves and then training a LLM (from scratch?) on stockfish games. (Using a custom tokenizer should improve the quality for a given size of the LLM model. So it doesn’t have to waste a lot of layers on encode and decode, and the “natural” latent representation is more straightforward)</div><br/></div></div><div id="42211830" class="c"><input type="checkbox" id="c-42211830" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#42208222">prev</a><span>|</span><a href="#42207682">next</a><span>|</span><label class="collapse" for="c-42211830">[-]</label><label class="expand" for="c-42211830">[1 more]</label></div><br/><div class="children"><div class="content">It might be worth trying the experiment where the prompt is formatted such that each chess turn corresponds to one chat message.</div><br/></div></div><div id="42207682" class="c"><input type="checkbox" id="c-42207682" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#42211830">prev</a><span>|</span><a href="#42207569">next</a><span>|</span><label class="collapse" for="c-42207682">[-]</label><label class="expand" for="c-42207682">[11 more]</label></div><br/><div class="children"><div class="content"><i>&gt; I was astonished that half the internet is convinced that OpenAI is cheating.</i><p>If you have a problem and all of your potential solutions are unlikely, then it&#x27;s fine to assume the least unlikely solution while acknowledging that it&#x27;s statistically probable that you&#x27;re also wrong. IOW if you have ten potential solutions to a problem and you estimate that the most likely solution has an 11% chance of being true, it&#x27;s fine to assume that solution despite the fact that, by your own estimate, you have an 89% chance of being wrong.<p>The &quot;OpenAI is secretly calling out to a chess engine&quot; hypothesis always seemed unlikely to me (you&#x27;d think it would play much better, if so), but it seemed the easiest solution (Occam&#x27;s razor) and I wouldn&#x27;t have been <i>surprised</i> to learn it was true (it&#x27;s not like OpenAI has a reputation of being trustworthy).</div><br/><div id="42207999" class="c"><input type="checkbox" id="c-42207999" checked=""/><div class="controls bullet"><span class="by">slibhb</span><span>|</span><a href="#42207682">parent</a><span>|</span><a href="#42207772">next</a><span>|</span><label class="collapse" for="c-42207999">[-]</label><label class="expand" for="c-42207999">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it has anything to do with your logic here. Actually, people just like talking shit about OpenAI on HN. It gets you upvotes.</div><br/><div id="42208942" class="c"><input type="checkbox" id="c-42208942" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#42207682">root</a><span>|</span><a href="#42207999">parent</a><span>|</span><a href="#42207772">next</a><span>|</span><label class="collapse" for="c-42208942">[-]</label><label class="expand" for="c-42208942">[1 more]</label></div><br/><div class="children"><div class="content">LLM cynicism exceeds LLM hype at this point.</div><br/></div></div></div></div><div id="42207772" class="c"><input type="checkbox" id="c-42207772" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42207682">parent</a><span>|</span><a href="#42207999">prev</a><span>|</span><a href="#42208498">next</a><span>|</span><label class="collapse" for="c-42207772">[-]</label><label class="expand" for="c-42207772">[1 more]</label></div><br/><div class="children"><div class="content">&gt;but it seemed the easiest solution (Occam&#x27;s razor)<p>In my opinion, it only seems like the easiest solution on the surface taking basically nothing into account. By the time you start looking at everything in context, it just seems bizarre.</div><br/></div></div><div id="42208498" class="c"><input type="checkbox" id="c-42208498" checked=""/><div class="controls bullet"><span class="by">influx</span><span>|</span><a href="#42207682">parent</a><span>|</span><a href="#42207772">prev</a><span>|</span><a href="#42207755">next</a><span>|</span><label class="collapse" for="c-42208498">[-]</label><label class="expand" for="c-42208498">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t call delegating specialized problems to specialized engines cheating. While it should be documented, in a full AI system, I want the best answer regardless of the technology used.</div><br/></div></div><div id="42207755" class="c"><input type="checkbox" id="c-42207755" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42207682">parent</a><span>|</span><a href="#42208498">prev</a><span>|</span><a href="#42207569">next</a><span>|</span><label class="collapse" for="c-42207755">[-]</label><label class="expand" for="c-42207755">[6 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not really how Occam&#x27;s razor works. The entire company colluding and lying to the public isn&#x27;t &quot;easy&quot;. Easy is more along the lines of &quot;for some reason it is good at chess but we&#x27;re not sure why&quot;.</div><br/><div id="42207966" class="c"><input type="checkbox" id="c-42207966" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42207682">root</a><span>|</span><a href="#42207755">parent</a><span>|</span><a href="#42208354">next</a><span>|</span><label class="collapse" for="c-42207966">[-]</label><label class="expand" for="c-42207966">[3 more]</label></div><br/><div class="children"><div class="content">One of the reasons I thought that was unlikely was personal pride. OpenAI researchers are proud of the work that they do. Cheating by calling out to a chess engine is something they would be ashamed of.</div><br/><div id="42208090" class="c"><input type="checkbox" id="c-42208090" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#42207682">root</a><span>|</span><a href="#42207966">parent</a><span>|</span><a href="#42208354">next</a><span>|</span><label class="collapse" for="c-42208090">[-]</label><label class="expand" for="c-42208090">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; OpenAI researchers are proud of the work that they do.</i><p>Well, the failed revolution from last year combined with the non-profit bait-and-switch pretty much conclusively proved that OpenAI researchers are in it for the money first and foremost, and pride has a dollar value.</div><br/><div id="42208131" class="c"><input type="checkbox" id="c-42208131" checked=""/><div class="controls bullet"><span class="by">fkyoureadthedoc</span><span>|</span><a href="#42207682">root</a><span>|</span><a href="#42208090">parent</a><span>|</span><a href="#42208354">next</a><span>|</span><label class="collapse" for="c-42208131">[-]</label><label class="expand" for="c-42208131">[1 more]</label></div><br/><div class="children"><div class="content">How much say do individual researchers even have in this move?<p>And how does that prove anything about their motivations &quot;first and foremost&quot;? They could be in it because they like the work itself, and secondary concerns like open or not don&#x27;t matter to them. There&#x27;s basically infinite interpretations of their motivations.</div><br/></div></div></div></div></div></div><div id="42208354" class="c"><input type="checkbox" id="c-42208354" checked=""/><div class="controls bullet"><span class="by">dogleash</span><span>|</span><a href="#42207682">root</a><span>|</span><a href="#42207755">parent</a><span>|</span><a href="#42207966">prev</a><span>|</span><a href="#42207569">next</a><span>|</span><label class="collapse" for="c-42208354">[-]</label><label class="expand" for="c-42208354">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The entire company colluding and lying to the public isn&#x27;t &quot;easy&quot;.<p>Why not?  Stop calling it &quot;the entire company colluding and lying&quot; and start calling it a &quot;messaging strategy among the people not prevented from speaking by NDA.&quot;  That will pass a casual Occam&#x27;s test that &quot;lying&quot; failed.  But they both mean the same exact thing.</div><br/><div id="42208726" class="c"><input type="checkbox" id="c-42208726" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42207682">root</a><span>|</span><a href="#42208354">parent</a><span>|</span><a href="#42207569">next</a><span>|</span><label class="collapse" for="c-42208726">[-]</label><label class="expand" for="c-42208726">[1 more]</label></div><br/><div class="children"><div class="content">It won&#x27;t, for the same reason - whenever you&#x27;re proposing a conspiracy theory, you have to explain what stops every person involved from leaking the conspiracy, whether on purpose or by accident. This gets superlinearly harder with number of people involved, and extra hard when there are incentives rewarding leaks (and leaking OpenAI secrets has some strong potential rewards).<p>Occam&#x27;s test applies to the full proposal, <i>including</i> the explanation of things outlined above.</div><br/></div></div></div></div></div></div></div></div><div id="42207569" class="c"><input type="checkbox" id="c-42207569" checked=""/><div class="controls bullet"><span class="by">ChrisArchitect</span><span>|</span><a href="#42207682">prev</a><span>|</span><a href="#42211401">next</a><span>|</span><label class="collapse" for="c-42207569">[-]</label><label class="expand" for="c-42207569">[1 more]</label></div><br/><div class="children"><div class="content">Related from last week:<p><i>Something weird is happening with LLMs and Chess</i><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42138276">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42138276</a></div><br/></div></div><div id="42211401" class="c"><input type="checkbox" id="c-42211401" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#42207569">prev</a><span>|</span><a href="#42211909">next</a><span>|</span><label class="collapse" for="c-42211401">[-]</label><label class="expand" for="c-42211401">[1 more]</label></div><br/><div class="children"><div class="content">I get that it would make evals even more expensive, but I would also try chain-of-thought! Have it explain its goals and reasoning for the next move before making it. It might be an awful idea for something like chess, but it seems to help elsewhere.</div><br/></div></div><div id="42211909" class="c"><input type="checkbox" id="c-42211909" checked=""/><div class="controls bullet"><span class="by">byyoung3</span><span>|</span><a href="#42211401">prev</a><span>|</span><a href="#42207734">next</a><span>|</span><label class="collapse" for="c-42211909">[-]</label><label class="expand" for="c-42211909">[1 more]</label></div><br/><div class="children"><div class="content">sometimes new training techniques will lead to regressions in certain tasks. My guess is this is exactly what has happened.</div><br/></div></div><div id="42207734" class="c"><input type="checkbox" id="c-42207734" checked=""/><div class="controls bullet"><span class="by">tmalsburg2</span><span>|</span><a href="#42211909">prev</a><span>|</span><a href="#42209750">next</a><span>|</span><label class="collapse" for="c-42207734">[-]</label><label class="expand" for="c-42207734">[3 more]</label></div><br/><div class="children"><div class="content">Why not use temperature 0 for sampling? If the top-ranked move is not legal, it can’t play chess.</div><br/><div id="42207932" class="c"><input type="checkbox" id="c-42207932" checked=""/><div class="controls bullet"><span class="by">thornewolf</span><span>|</span><a href="#42207734">parent</a><span>|</span><a href="#42209750">next</a><span>|</span><label class="collapse" for="c-42207932">[-]</label><label class="expand" for="c-42207932">[2 more]</label></div><br/><div class="children"><div class="content">sometimes skilled chess players make illegal moves</div><br/><div id="42211759" class="c"><input type="checkbox" id="c-42211759" checked=""/><div class="controls bullet"><span class="by">atiedebee</span><span>|</span><a href="#42207734">root</a><span>|</span><a href="#42207932">parent</a><span>|</span><a href="#42209750">next</a><span>|</span><label class="collapse" for="c-42211759">[-]</label><label class="expand" for="c-42211759">[1 more]</label></div><br/><div class="children"><div class="content">Extremely rare. The only time this happened that I&#x27;m aware of was quite recent but the players only had a second or 2 remaining on the clock, so time pressure is definitely the reason there</div><br/></div></div></div></div></div></div><div id="42209750" class="c"><input type="checkbox" id="c-42209750" checked=""/><div class="controls bullet"><span class="by">joshka</span><span>|</span><a href="#42207734">prev</a><span>|</span><a href="#42207031">next</a><span>|</span><label class="collapse" for="c-42209750">[-]</label><label class="expand" for="c-42209750">[1 more]</label></div><br/><div class="children"><div class="content">Why are you telling it not to explain? Allowing the LLM space to &quot;think&quot; may be helpful, and would be definitely worth explorying?<p>Why are you manually guessing ways to improve this? Why not let the LLMs do this for themselves and find iteratively better prompts?</div><br/></div></div><div id="42207031" class="c"><input type="checkbox" id="c-42207031" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#42209750">prev</a><span>|</span><a href="#42207878">next</a><span>|</span><label class="collapse" for="c-42207031">[-]</label><label class="expand" for="c-42207031">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Theory 1: Large enough base models are good at chess, but this doesn’t persist through instruction tuning to chat models.<p>I lean mostly towards this and also the chess notations - not sure if it might get chopped during tokenization unless it&#x27;s very precisely processed.<p>It&#x27;s like designing an LLM just for predicting protein sequence because the sequencing matters. The base data might have it but i don&#x27;t think that&#x27;s the intention for it to continue.</div><br/><div id="42207162" class="c"><input type="checkbox" id="c-42207162" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#42207031">parent</a><span>|</span><a href="#42207878">next</a><span>|</span><label class="collapse" for="c-42207162">[-]</label><label class="expand" for="c-42207162">[1 more]</label></div><br/><div class="children"><div class="content">This makes me wonder what scenarios would be unlocked if OpenAI gave access to gpt4-instruct.<p>I wonder if they avoid that due to the potential for negative press from the outputs of a more &quot;raw&quot; model.</div><br/></div></div></div></div><div id="42207878" class="c"><input type="checkbox" id="c-42207878" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#42207031">prev</a><span>|</span><a href="#42208010">next</a><span>|</span><label class="collapse" for="c-42207878">[-]</label><label class="expand" for="c-42207878">[2 more]</label></div><br/><div class="children"><div class="content">LLMs are fundamentally text-completion. The Chat-based tuning that goes on top of it is impressive but they are fundamentally text-completion, that&#x27;s where most of the training energy goes. I keep this in mind with a lot of my prompting and get good results.<p>Regurgitating and Examples are both ways to lean into that and try to recover whatever has been lost by Chat-based tuning.</div><br/><div id="42208096" class="c"><input type="checkbox" id="c-42208096" checked=""/><div class="controls bullet"><span class="by">zi_</span><span>|</span><a href="#42207878">parent</a><span>|</span><a href="#42208010">next</a><span>|</span><label class="collapse" for="c-42208096">[-]</label><label class="expand" for="c-42208096">[1 more]</label></div><br/><div class="children"><div class="content">what else do you think about when prompting, which you&#x27;ve found to be useful?</div><br/></div></div></div></div><div id="42208010" class="c"><input type="checkbox" id="c-42208010" checked=""/><div class="controls bullet"><span class="by">blixt</span><span>|</span><a href="#42207878">prev</a><span>|</span><a href="#42208427">next</a><span>|</span><label class="collapse" for="c-42208010">[-]</label><label class="expand" for="c-42208010">[1 more]</label></div><br/><div class="children"><div class="content">Really interesting findings around fine-tuning. Goes to show it doesn&#x27;t really affect the deeper &quot;functionality&quot; of the LLM (if you think of the LLM running a set of small functions on very high-dimensional numbers to produce a token).<p>Using regurgitation to get around the assistant&#x2F;user token separation is another fun tool for the toolbox, relevant for whenever you want a model that doesn&#x27;t support continuation actually perform continuation (at the cost of a lot of latency).<p>I wonder if any type of reflection or chains of thought would help it play better. I wouldn&#x27;t be surprised if getting the LLM to write an analysis of the game in English is more likely to move it out of distribution than to make it pick better chess moves.</div><br/></div></div><div id="42208427" class="c"><input type="checkbox" id="c-42208427" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#42208010">prev</a><span>|</span><a href="#42209810">next</a><span>|</span><label class="collapse" for="c-42208427">[-]</label><label class="expand" for="c-42208427">[1 more]</label></div><br/><div class="children"><div class="content">You can easily construct a game board from a sequence of moves by maintaining the game state somewhere. But you can also know where a piece is bases on only its last move. I&#x27;m curious what happens if you don&#x27;t feed it a position, but feed it a sequence of moves including illegal ones but end up at a given valid position. The author mention that LLMs will play differently when the same position is arrived at via different sequences. I&#x27;m suggesting to really play with that by putting illegal moves in the sequence.<p>I doubt it&#x27;s doing much more than a static analysis of the a board position, or even moving based mostly on just a few recent moves by key pieces.</div><br/></div></div><div id="42209810" class="c"><input type="checkbox" id="c-42209810" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#42208427">prev</a><span>|</span><a href="#42207542">next</a><span>|</span><label class="collapse" for="c-42209810">[-]</label><label class="expand" for="c-42209810">[1 more]</label></div><br/><div class="children"><div class="content">Very good follow-up to the original article. Thank you!</div><br/></div></div><div id="42207542" class="c"><input type="checkbox" id="c-42207542" checked=""/><div class="controls bullet"><span class="by">gallerdude</span><span>|</span><a href="#42209810">prev</a><span>|</span><a href="#42208052">next</a><span>|</span><label class="collapse" for="c-42207542">[-]</label><label class="expand" for="c-42207542">[3 more]</label></div><br/><div class="children"><div class="content">Very interesting - have you tried using `o1` yet? I made a program which makes LLM&#x27;s complete WORDLE puzzles, and the difference between `4o` and `o1` is absolutely astonishing.</div><br/><div id="42208004" class="c"><input type="checkbox" id="c-42208004" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42207542">parent</a><span>|</span><a href="#42207623">next</a><span>|</span><label class="collapse" for="c-42208004">[-]</label><label class="expand" for="c-42208004">[1 more]</label></div><br/><div class="children"><div class="content">OK, that was fun. I just tried o1-preview on today&#x27;s Wordle and it got it on the third guess: <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;673f9169-3654-8006-8c0b-07c53a2c5881" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;673f9169-3654-8006-8c0b-07c53a2c58...</a></div><br/></div></div><div id="42207623" class="c"><input type="checkbox" id="c-42207623" checked=""/><div class="controls bullet"><span class="by">gallerdude</span><span>|</span><a href="#42207542">parent</a><span>|</span><a href="#42208004">prev</a><span>|</span><a href="#42208052">next</a><span>|</span><label class="collapse" for="c-42207623">[-]</label><label class="expand" for="c-42207623">[1 more]</label></div><br/><div class="children"><div class="content">4o-mini: 16%
4o: 50%
o1-mini: 97%
o1: 100%<p>* disclaimer - only n=7 on o1. Others are like 100-300 each</div><br/></div></div></div></div><div id="42208052" class="c"><input type="checkbox" id="c-42208052" checked=""/><div class="controls bullet"><span class="by">MisterTea</span><span>|</span><a href="#42207542">prev</a><span>|</span><a href="#42210059">next</a><span>|</span><label class="collapse" for="c-42208052">[-]</label><label class="expand" for="c-42208052">[1 more]</label></div><br/><div class="children"><div class="content">This happened to a friend who was trying to sim basketball games. It kept forgetting who had the ball or outright made illegal or confusing moves. After a few days of wrestling with the AI he gave up. GPT is amazing at following a linear conversation but had no cognitive ability to keep track of a dynamic scenario.</div><br/></div></div><div id="42207441" class="c"><input type="checkbox" id="c-42207441" checked=""/><div class="controls bullet"><span class="by">seizethecheese</span><span>|</span><a href="#42210059">prev</a><span>|</span><a href="#42208973">next</a><span>|</span><label class="collapse" for="c-42207441">[-]</label><label class="expand" for="c-42207441">[3 more]</label></div><br/><div class="children"><div class="content">All the hand wringing about openAI cheating suggests a question: why so much mistrust?<p>My guess would be that the persona of the openAI team on platforms like Twitter is very cliquey. This, I think, naturally leads to mistrust. A clique feels more likely to cheat than some other sort of group.</div><br/><div id="42208018" class="c"><input type="checkbox" id="c-42208018" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42207441">parent</a><span>|</span><a href="#42208886">next</a><span>|</span><label class="collapse" for="c-42208018">[-]</label><label class="expand" for="c-42208018">[1 more]</label></div><br/><div class="children"><div class="content">I wrote about this last year. The levels of trust people have in companies working in AI is notably low: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Dec&#x2F;14&#x2F;ai-trust-crisis&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Dec&#x2F;14&#x2F;ai-trust-crisis&#x2F;</a></div><br/></div></div><div id="42208886" class="c"><input type="checkbox" id="c-42208886" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#42207441">parent</a><span>|</span><a href="#42208018">prev</a><span>|</span><a href="#42208973">next</a><span>|</span><label class="collapse" for="c-42208886">[-]</label><label class="expand" for="c-42208886">[1 more]</label></div><br/><div class="children"><div class="content">My take on this is that people tend to be afraid of what they can&#x27;t understand or explain. To do away with that feeling, they just say &#x27;it can&#x27;t reason&#x27;. While nobody on earth can put a finger on what reasoning is, other than that it is a human trait.</div><br/></div></div></div></div><div id="42208973" class="c"><input type="checkbox" id="c-42208973" checked=""/><div class="controls bullet"><span class="by">drivingmenuts</span><span>|</span><a href="#42207441">prev</a><span>|</span><a href="#42207845">next</a><span>|</span><label class="collapse" for="c-42208973">[-]</label><label class="expand" for="c-42208973">[1 more]</label></div><br/><div class="children"><div class="content">Why would a chess-playing AI be tuned to do anything except play chess? Just seems like a waste. A bunch of small, specialized AI&#x27;s seems like a better idea than spending time trying to build a new one.<p>Maybe less morally challenging, as well. You wouldn&#x27;t be trying to install &quot;sentience&quot;.</div><br/></div></div><div id="42207845" class="c"><input type="checkbox" id="c-42207845" checked=""/><div class="controls bullet"><span class="by">atemerev</span><span>|</span><a href="#42208973">prev</a><span>|</span><a href="#42207921">next</a><span>|</span><label class="collapse" for="c-42207845">[-]</label><label class="expand" for="c-42207845">[5 more]</label></div><br/><div class="children"><div class="content">Ah, half of the commentariat still think that “LLMs can’t reason”. Even if they have enough state space for reasoning, and clearly demonstrate that.</div><br/><div id="42210398" class="c"><input type="checkbox" id="c-42210398" checked=""/><div class="controls bullet"><span class="by">sourcepluck</span><span>|</span><a href="#42207845">parent</a><span>|</span><a href="#42208013">next</a><span>|</span><label class="collapse" for="c-42210398">[-]</label><label class="expand" for="c-42210398">[2 more]</label></div><br/><div class="children"><div class="content">Most people, as far as I&#x27;m aware, don&#x27;t have an issue with the idea that LLMs are producing behaviour which gives the appearance of reasoning as far as we understand it today. Which essentially means, it makes sentences that are gramatical, responsive and contextual based on what you said (quite often). It&#x27;s at least pretty cool that we&#x27;ve got machines to do that, most people seem to think.<p>The issue is that there might be more to <i>reason</i> than <i>appearing to reason</i>. We just don&#x27;t know. I&#x27;m not sure how it&#x27;s apparently so unknown or unappreciated by people in the computer world, but there are major unresolved questions in science and philosophy around things like thinking, reasoning, language, consciousness, and the mind. No amount of techno-optimism can change this fact.<p>The issue is we have not gotten further than more or less educated guesses as to what those words mean. LLMs bring that interesting fact to light, even providing humanity with a wonderful nudge to keep grappling with these unsolved questions, and perhaps make some progress.<p>To be clear, they certainly are sometimes passably good when it comes to summarising selectively and responsively the terabytes and terabytes of data they&#x27;ve been trained on, don&#x27;t get me wrong, and I am enjoying that new thing in the world. And if you want to define <i>reason</i> like that, feel free.</div><br/><div id="42211993" class="c"><input type="checkbox" id="c-42211993" checked=""/><div class="controls bullet"><span class="by">atemerev</span><span>|</span><a href="#42207845">root</a><span>|</span><a href="#42210398">parent</a><span>|</span><a href="#42208013">next</a><span>|</span><label class="collapse" for="c-42211993">[-]</label><label class="expand" for="c-42211993">[1 more]</label></div><br/><div class="children"><div class="content">LLMs can _play chess_. With the game positions previously unseen. How’s that not actual logical reasoning?</div><br/></div></div></div></div><div id="42208013" class="c"><input type="checkbox" id="c-42208013" checked=""/><div class="controls bullet"><span class="by">lottin</span><span>|</span><a href="#42207845">parent</a><span>|</span><a href="#42210398">prev</a><span>|</span><a href="#42207935">next</a><span>|</span><label class="collapse" for="c-42208013">[-]</label><label class="expand" for="c-42208013">[1 more]</label></div><br/><div class="children"><div class="content">&quot;The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.&quot; - Edsger Dijkstra</div><br/></div></div><div id="42207935" class="c"><input type="checkbox" id="c-42207935" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42207845">parent</a><span>|</span><a href="#42208013">prev</a><span>|</span><a href="#42207921">next</a><span>|</span><label class="collapse" for="c-42207935">[-]</label><label class="expand" for="c-42207935">[1 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s not real reasoning because it is just outputting likely next tokens that are identical to what we&#x27;d expect with reasoning. &#x2F;s</div><br/></div></div></div></div><div id="42208298" class="c"><input type="checkbox" id="c-42208298" checked=""/><div class="controls bullet"><span class="by">sourcepluck</span><span>|</span><a href="#42207921">prev</a><span>|</span><a href="#42207105">next</a><span>|</span><label class="collapse" for="c-42208298">[-]</label><label class="expand" for="c-42208298">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t like being directly critical, people learning in public can be good and instructive. But I regret the time I&#x27;ve put into both this article and the last one and perhaps someone else can be saved the same time.<p>This is someone with limited knowledge of chess, statistics and LLMs doing a series of public articles as they learn a little tiny bit about chess, statistics and LLMs. And it garners upvotes and attention off the coat-tails of AI excitement. Which is fair enough, it&#x27;s the (semi-)public internet, but it sort of masquerades as being half-serious &quot;research&quot;, and it kind of held things together for the first article, but this one really is thrown together to keep the buzz going of the last one.<p>The TL;DR :: one of the AIs being just-above-terrible, compared to all the others being completely terrible, a fact already of dubious interest, is down to - we don&#x27;t know. Maybe a difference in training sets. Tons of speculation. A few graphs.</div><br/></div></div><div id="42207105" class="c"><input type="checkbox" id="c-42207105" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#42208298">prev</a><span>|</span><label class="collapse" for="c-42207105">[-]</label><label class="expand" for="c-42207105">[11 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why this whole line of posts is worthy of the front page. They seem like one&#x27;s personal experiments in a limited capacity, unworthy of sharing. It is obvious the observed outputs are because instruction tuning is incompatible with the prompt used by the user. Secondly, the user even failed to provide a chess board diagram (represented as text) to the model. The user also failed to tune any models. Overall, in the absence of an ascii diagram, it&#x27;s all a waste of time.</div><br/><div id="42207280" class="c"><input type="checkbox" id="c-42207280" checked=""/><div class="controls bullet"><span class="by">synarchefriend</span><span>|</span><a href="#42207105">parent</a><span>|</span><a href="#42207249">next</a><span>|</span><label class="collapse" for="c-42207280">[-]</label><label class="expand" for="c-42207280">[9 more]</label></div><br/><div class="children"><div class="content">The model was trained on games in PGN notation. It would be shocking if it found ASCII art easier to understand than what it was actually trained on.</div><br/><div id="42207397" class="c"><input type="checkbox" id="c-42207397" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#42207105">root</a><span>|</span><a href="#42207280">parent</a><span>|</span><a href="#42207249">next</a><span>|</span><label class="collapse" for="c-42207397">[-]</label><label class="expand" for="c-42207397">[8 more]</label></div><br/><div class="children"><div class="content">Well, clearly you&#x27;re not interested in experimentation, only in assumptions.</div><br/><div id="42207648" class="c"><input type="checkbox" id="c-42207648" checked=""/><div class="controls bullet"><span class="by">daveguy</span><span>|</span><a href="#42207105">root</a><span>|</span><a href="#42207397">parent</a><span>|</span><a href="#42207658">next</a><span>|</span><label class="collapse" for="c-42207648">[-]</label><label class="expand" for="c-42207648">[1 more]</label></div><br/><div class="children"><div class="content">How does stating the outcome you expect imply you are not interested in experimentation? Hypothesis formation is the very first step in experimentation.</div><br/></div></div><div id="42207658" class="c"><input type="checkbox" id="c-42207658" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42207105">root</a><span>|</span><a href="#42207397">parent</a><span>|</span><a href="#42207648">prev</a><span>|</span><a href="#42208073">next</a><span>|</span><label class="collapse" for="c-42207658">[-]</label><label class="expand" for="c-42207658">[1 more]</label></div><br/><div class="children"><div class="content">Most people who understand LLMs and how they are trained would be shocked. In practice, that&#x27;s an objectively true statement.</div><br/></div></div><div id="42208073" class="c"><input type="checkbox" id="c-42208073" checked=""/><div class="controls bullet"><span class="by">BeetleB</span><span>|</span><a href="#42207105">root</a><span>|</span><a href="#42207397">parent</a><span>|</span><a href="#42207658">prev</a><span>|</span><a href="#42207591">next</a><span>|</span><label class="collapse" for="c-42208073">[-]</label><label class="expand" for="c-42208073">[4 more]</label></div><br/><div class="children"><div class="content">Please, please show us your experiments.</div><br/><div id="42208221" class="c"><input type="checkbox" id="c-42208221" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#42207105">root</a><span>|</span><a href="#42208073">parent</a><span>|</span><a href="#42207591">next</a><span>|</span><label class="collapse" for="c-42208221">[-]</label><label class="expand" for="c-42208221">[3 more]</label></div><br/><div class="children"><div class="content">I am not the one writing and posting useless articles, even harmful articles, also distorting the understanding of LLMs. Ask the ones who do to perform better experiments.</div><br/><div id="42208553" class="c"><input type="checkbox" id="c-42208553" checked=""/><div class="controls bullet"><span class="by">multjoy</span><span>|</span><a href="#42207105">root</a><span>|</span><a href="#42208221">parent</a><span>|</span><a href="#42209054">next</a><span>|</span><label class="collapse" for="c-42208553">[-]</label><label class="expand" for="c-42208553">[1 more]</label></div><br/><div class="children"><div class="content">You know that the LLM isn&#x27;t actually your friend, don&#x27;t you?</div><br/></div></div><div id="42209054" class="c"><input type="checkbox" id="c-42209054" checked=""/><div class="controls bullet"><span class="by">BeetleB</span><span>|</span><a href="#42207105">root</a><span>|</span><a href="#42208221">parent</a><span>|</span><a href="#42208553">prev</a><span>|</span><a href="#42207591">next</a><span>|</span><label class="collapse" for="c-42209054">[-]</label><label class="expand" for="c-42209054">[1 more]</label></div><br/><div class="children"><div class="content">So to quote yourself:<p>&gt; Well, clearly you&#x27;re not interested in experimentation</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>