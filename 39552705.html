<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709283672982" as="style"/><link rel="stylesheet" href="styles.css?v=1709283672982"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://gonzoml.substack.com/p/big-post-about-big-context">Big Post About Big Context</a> <span class="domain">(<a href="https://gonzoml.substack.com">gonzoml.substack.com</a>)</span></div><div class="subtext"><span>che_shr_cat</span> | <span>13 comments</span></div><br/><div><div id="39558247" class="c"><input type="checkbox" id="c-39558247" checked=""/><div class="controls bullet"><span class="by">bloaf</span><span>|</span><a href="#39557779">next</a><span>|</span><label class="collapse" for="c-39558247">[-]</label><label class="expand" for="c-39558247">[2 more]</label></div><br/><div class="children"><div class="content">I think this kind of haystack recall is going to be extremely valuable to large enterprises with massive volumes of standards&#x2F;practices&#x2F;technical documents.<p>The status quo is that you have employees ingest and internalize this material over years of employment, then have those experienced employees manage large projects by telling the project if they&#x27;re in compliance with the organization&#x27;s standards.<p>A reliable recall system like this isn&#x27;t just a drop in replacement for <i>new</i> employees, its a drop in replacement for a lot of what makes the <i>expensive</i> employees important: the backwards-and-forwards knowledge of the standards&#x2F;practices.</div><br/><div id="39559489" class="c"><input type="checkbox" id="c-39559489" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#39558247">parent</a><span>|</span><a href="#39557779">next</a><span>|</span><label class="collapse" for="c-39559489">[-]</label><label class="expand" for="c-39559489">[1 more]</label></div><br/><div class="children"><div class="content">Although our standard template constructors store all of mankind&#x27;s knowledge and obviate the need for dangerous thinking, only a tech-priest of great experience may adequately consecrate them.</div><br/></div></div></div></div><div id="39557779" class="c"><input type="checkbox" id="c-39557779" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#39558247">prev</a><span>|</span><label class="collapse" for="c-39557779">[-]</label><label class="expand" for="c-39557779">[10 more]</label></div><br/><div class="children"><div class="content">One point the article argues is that with 1M+ token context windows, we shouldn&#x27;t use RAG to include chunks from a larger collection -- just include everything! This is a very important open question to me, as I have been building a fairly nuanced RAG pipeline for AI coding.<p>With GPT-4 Turbo, it is definitely NOT a good idea to just throw lots of extraneous code into the 128k content window. That distracts and confuses GPT, makes it code worse and less able to follow complex system prompts. You get much better results if you curate a smaller portion of the code that is relevant to the task at hand.<p>I am really interested to find out if Gemini has these same tendencies. If so, quality RAG is going to remain valuable for curating the context. If not, then Gemini would have a huge advantage over GPT-4. It would be really valuable to be able to naively harness such a large context window.</div><br/><div id="39558400" class="c"><input type="checkbox" id="c-39558400" checked=""/><div class="controls bullet"><span class="by">hansonw</span><span>|</span><a href="#39557779">parent</a><span>|</span><a href="#39558567">next</a><span>|</span><label class="collapse" for="c-39558400">[-]</label><label class="expand" for="c-39558400">[1 more]</label></div><br/><div class="children"><div class="content">If you think about it, RAG is a relatively primitive “first pass attention layer” that is binary and semi-heuristic based. I think it’s fairly safe to say that in the long term RAG will be integrated into the model architecture somehow, just a matter of when :)</div><br/></div></div><div id="39558567" class="c"><input type="checkbox" id="c-39558567" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#39557779">parent</a><span>|</span><a href="#39558400">prev</a><span>|</span><a href="#39558205">next</a><span>|</span><label class="collapse" for="c-39558567">[-]</label><label class="expand" for="c-39558567">[4 more]</label></div><br/><div class="children"><div class="content">There is a lot of emerging evidence that ability to reason, and ability to compare within-context dramatically declines with large context.<p>It is unable to find edits: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;mengdi_en&#x2F;status&#x2F;1763338245696274469" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;mengdi_en&#x2F;status&#x2F;1763338245696274469</a><p>It hallucinates changes badly: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;thomasahle&#x2F;status&#x2F;1763408041960231010" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;thomasahle&#x2F;status&#x2F;1763408041960231010</a><p>To be fair, finding diffs is an odd way to use LLMs, but does show the highly variable abilities of these models over bigger contexts.<p>In other models, reasoning performance declines with more tokens (no data yet here for Gemini 1.5 Pro): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2402.14848v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2402.14848v1</a><p>Note in that last chart that significant declines in reasoning are apparent over just a few thousand tokens.</div><br/><div id="39559793" class="c"><input type="checkbox" id="c-39559793" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#39557779">root</a><span>|</span><a href="#39558567">parent</a><span>|</span><a href="#39558658">next</a><span>|</span><label class="collapse" for="c-39559793">[-]</label><label class="expand" for="c-39559793">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not surprised, those documents probably contain a lot of information which causes the model to get confused&#x2F;lost&#x2F;etc.<p>It also seems to line up with random paragraph location doing the worst in that paper.</div><br/></div></div><div id="39558658" class="c"><input type="checkbox" id="c-39558658" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#39557779">root</a><span>|</span><a href="#39558567">parent</a><span>|</span><a href="#39559793">prev</a><span>|</span><a href="#39558205">next</a><span>|</span><label class="collapse" for="c-39558658">[-]</label><label class="expand" for="c-39558658">[2 more]</label></div><br/><div class="children"><div class="content">I took the text of Macbeth and changed the dialect of one line:<p>Original: &quot;The rest, that are within the note of expectation, alreadie are i&#x27;th&#x27; Court&quot;<p>New: &quot;It looks like everyone is here already&quot;<p>Then I gave the entire text to Gemini, ~400000 tokens, and asked it to analyze the play&#x27;s diction and style, and to identify any lines that didn&#x27;t match the prevailing dialect.<p>It found that one line I&#x27;d changed.</div><br/><div id="39559300" class="c"><input type="checkbox" id="c-39559300" checked=""/><div class="controls bullet"><span class="by">interstice</span><span>|</span><a href="#39557779">root</a><span>|</span><a href="#39558658">parent</a><span>|</span><a href="#39558205">next</a><span>|</span><label class="collapse" for="c-39559300">[-]</label><label class="expand" for="c-39559300">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t it quite likely it already had the whole text from its training data? Which might make it a bit of a different case.</div><br/></div></div></div></div></div></div><div id="39558205" class="c"><input type="checkbox" id="c-39558205" checked=""/><div class="controls bullet"><span class="by">meandmycode</span><span>|</span><a href="#39557779">parent</a><span>|</span><a href="#39558567">prev</a><span>|</span><a href="#39558334">next</a><span>|</span><label class="collapse" for="c-39558205">[-]</label><label class="expand" for="c-39558205">[2 more]</label></div><br/><div class="children"><div class="content">Also, while the cost of tokens is lower, let&#x27;s argue it&#x27;s cheap enough not to care. Reading 1m tokens surely isn&#x27;t realistic for latency?</div><br/><div id="39558336" class="c"><input type="checkbox" id="c-39558336" checked=""/><div class="controls bullet"><span class="by">hansonw</span><span>|</span><a href="#39557779">root</a><span>|</span><a href="#39558205">parent</a><span>|</span><a href="#39558334">next</a><span>|</span><label class="collapse" for="c-39558336">[-]</label><label class="expand" for="c-39558336">[1 more]</label></div><br/><div class="children"><div class="content">If sub-quadratic architectures (eg Mamba) become a thing, it will become feasible to precompute most of the work for a fixed prefix (i.e. system prompt) and the latency can be pretty minimal. Even with current transformers, if you have a fixed system prompt, you can save the KV cache and it helps a lot (though the inference time of each incremental token is still linear).</div><br/></div></div></div></div><div id="39558334" class="c"><input type="checkbox" id="c-39558334" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39557779">parent</a><span>|</span><a href="#39558205">prev</a><span>|</span><label class="collapse" for="c-39558334">[-]</label><label class="expand" for="c-39558334">[2 more]</label></div><br/><div class="children"><div class="content">Cosign, I feel like the longer context window is mostly a dud, especially above 16K tokens.<p>I don&#x27;t miss sweating the details on what exactly to fit into 4096, but if I just use GPT-4, I&#x27;m left with the distinct impression the current context extension stuff is hacky, and something needs to be done in training to actually make use of it.<p>Google miraculously getting 1M context with perfect recall is more fuel for my &quot;don&#x27;t tell me, show me&quot; fire when it comes to Google. They&#x27;re batting roughly 0 for 2 years now</div><br/><div id="39559827" class="c"><input type="checkbox" id="c-39559827" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#39557779">root</a><span>|</span><a href="#39558334">parent</a><span>|</span><label class="collapse" for="c-39559827">[-]</label><label class="expand" for="c-39559827">[1 more]</label></div><br/><div class="children"><div class="content">The main thing I&#x27;d like large context for is the ability to shove a ton of API&#x2F;Python&#x2F;etc documentation into the window for a given problem so it knows better what to do.<p>In terms of the actual output it can be annoying if the convo goes on too long and it hits the window, but for technical things this hardly matters as you can just restart it - more of an RP issue as i understand.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>