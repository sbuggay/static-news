<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696669263483" as="style"/><link rel="stylesheet" href="styles.css?v=1696669263483"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/how-will-ai-learn-next">How will AI learn next?</a> <span class="domain">(<a href="https://www.newyorker.com">www.newyorker.com</a>)</span></div><div class="subtext"><span>jyli7</span> | <span>92 comments</span></div><br/><div><div id="37795006" class="c"><input type="checkbox" id="c-37795006" checked=""/><div class="controls bullet"><span class="by">rented_mule</span><span>|</span><a href="#37795110">next</a><span>|</span><label class="collapse" for="c-37795006">[-]</label><label class="expand" for="c-37795006">[11 more]</label></div><br/><div class="children"><div class="content">Anyone who has iterated on trained models for long enough knows that feedback loops can be a serious problem. If your models are influencing the generation of data that they are later retrained on, it gets harder and harder to even maintain model performance. The article mentions one experiment in this direction: &quot;With each generation, the quality of the model actually degraded.&quot; This happens whenever there aren&#x27;t solid strategies to avoid feedback loop issues.<p>Given this, the problem isn&#x27;t just that there&#x27;s not enough new content. It&#x27;s that an ever-increasing fraction of the content in the public sphere will be generated by these models. And can the models detect that they are ingesting their own output? If they get good enough, they probably can&#x27;t. And then they&#x27;ll get worse.<p>This could have a strange impact on human language &#x2F; communication as well. As these models are increasingly trained on their own output, they&#x27;ll start emulating their own mistakes and more of the content we consume will have these mistakes consistently used. You can imagine people, sometimes intentionally and sometimes not, starting to emulate these patterns and causing shifts in human languages. Interesting times ahead...</div><br/><div id="37799779" class="c"><input type="checkbox" id="c-37799779" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37795006">parent</a><span>|</span><a href="#37795894">next</a><span>|</span><label class="collapse" for="c-37799779">[-]</label><label class="expand" for="c-37799779">[1 more]</label></div><br/><div class="children"><div class="content">Humans have that to, but the reason civilization doesn&#x27;t go full crazy is our use of language and concepts are tied to doing objective things, which keeps things (mostly) grounded.<p>Where it isn’t grounded, as in endless online conversations with like minded people (closed loop feedback) about informally abstracted (poorly specified constraints) and emotion invoking (high reinforcement) topics, people go batshit too.<p>So the more AI models actually practice what they know in objective environments, the more likely that output-&gt;input feedback will inform introspection toward self-improvement, and less like an iterative calculation of the architecture’s resonant  frequencies or eigenvalues.</div><br/></div></div><div id="37795894" class="c"><input type="checkbox" id="c-37795894" checked=""/><div class="controls bullet"><span class="by">haltist</span><span>|</span><a href="#37795006">parent</a><span>|</span><a href="#37799779">prev</a><span>|</span><a href="#37799746">next</a><span>|</span><label class="collapse" for="c-37795894">[-]</label><label class="expand" for="c-37795894">[2 more]</label></div><br/><div class="children"><div class="content">In system design there is something called resonant amplification and what you are describing is very similar. The biases of the model are amplified with each iteration and the end result is that the system converges onto the patterns recognizable and amplified by its architecture. If you know about impulse and frequency analysis then an AI system can be considered to be a signal processor that amplifies and attenuates certain frequencies in the input&#x2F;impulse. Running an LLM in a loop always ends up with nonsense as the final output.</div><br/><div id="37799467" class="c"><input type="checkbox" id="c-37799467" checked=""/><div class="controls bullet"><span class="by">lazystar</span><span>|</span><a href="#37795006">root</a><span>|</span><a href="#37795894">parent</a><span>|</span><a href="#37799746">next</a><span>|</span><label class="collapse" for="c-37799467">[-]</label><label class="expand" for="c-37799467">[1 more]</label></div><br/><div class="children"><div class="content">So if left unchecked, the thing we built in man&#x27;s attempt to play god could result in gibberish?  sounds kind of like the tower of babel; seems humankinds only defense would be creating a new language that the machines can&#x27;t infiltrate</div><br/></div></div></div></div><div id="37799746" class="c"><input type="checkbox" id="c-37799746" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#37795006">parent</a><span>|</span><a href="#37795894">prev</a><span>|</span><a href="#37797635">next</a><span>|</span><label class="collapse" for="c-37799746">[-]</label><label class="expand" for="c-37799746">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If your models are influencing the generation of data that they are later retrained on, it gets harder and harder to even maintain model performance.<p>Why don&#x27;t humans suffer from this problem, then? Humans have been creating art (and content) that imitates nature and society for thousands of years now, and yet we have little problem (the exception are possibly things like crop circles) to recognize what is a natural phenomenon and what is generated culturally.<p>I think it&#x27;s wrong to assume that this is a problem with intelligence in general, rather than just a feature (stupidity) of the current models.</div><br/><div id="37800092" class="c"><input type="checkbox" id="c-37800092" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37795006">root</a><span>|</span><a href="#37799746">parent</a><span>|</span><a href="#37799990">next</a><span>|</span><label class="collapse" for="c-37800092">[-]</label><label class="expand" for="c-37800092">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why don&#x27;t humans suffer from this problem, then?<p>They do, but the problem is poorly stated. The problen isn&#x27;t “If your models are influencing the generation of data that they are later retrained on”, its “If output produced by your models with inadequate filtering for quality&#x2F;accuracy is dominating the data that they are later retrained on.”<p>Humans definitely experience the same problem: we see it when societies become closed, inward-looking, and locked into models of information filtering that don&#x27;t involve accuracy or fitness for purpose other than confirming an in-groups priors. There are some examoles in stagnation of large societies, but probably the clearest examples are cults that socially isolate their membership and degrade into increasingly bizarre behaviors terminating in mass suicide, murder&#x2F;suicides, etc.<p>LLM’s experience self-reinforcing degradation more acutely because they are individually less capable than humans and they are less diverse.</div><br/></div></div><div id="37799990" class="c"><input type="checkbox" id="c-37799990" checked=""/><div class="controls bullet"><span class="by">ArekDymalski</span><span>|</span><a href="#37795006">root</a><span>|</span><a href="#37799746">parent</a><span>|</span><a href="#37800092">prev</a><span>|</span><a href="#37797635">next</a><span>|</span><label class="collapse" for="c-37799990">[-]</label><label class="expand" for="c-37799990">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why don&#x27;t humans suffer from this problem, then?<p>Can we be sure that we don&#x27;t? For example, depending on perspective our language is either evolving towards clarity or devolving towards primitivity, compared to XVIII&#x2F;XIX century. The same could be argued about other aspects of society and culture.</div><br/></div></div></div></div><div id="37797635" class="c"><input type="checkbox" id="c-37797635" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#37795006">parent</a><span>|</span><a href="#37799746">prev</a><span>|</span><a href="#37799900">next</a><span>|</span><label class="collapse" for="c-37797635">[-]</label><label class="expand" for="c-37797635">[2 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s reasonable to say that this was actually the point of releasing LLMs publicly. The companies that created them wanted a moat and figured the data they had could be it if they poisoned anyones attempt to collect the same data in the future.</div><br/><div id="37799460" class="c"><input type="checkbox" id="c-37799460" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#37795006">root</a><span>|</span><a href="#37797635">parent</a><span>|</span><a href="#37799900">next</a><span>|</span><label class="collapse" for="c-37799460">[-]</label><label class="expand" for="c-37799460">[1 more]</label></div><br/><div class="children"><div class="content">Intriguing thought, but arguably people are intentionally using GPT to generate synthetic data for their domain specific model. So I&#x27;m kinda torn between AI giants poisoning the well with their models, or it just being unforeseen consequences (or one they willingly ignored to be first to market).</div><br/></div></div></div></div><div id="37799900" class="c"><input type="checkbox" id="c-37799900" checked=""/><div class="controls bullet"><span class="by">rjblackman</span><span>|</span><a href="#37795006">parent</a><span>|</span><a href="#37797635">prev</a><span>|</span><a href="#37799847">next</a><span>|</span><label class="collapse" for="c-37799900">[-]</label><label class="expand" for="c-37799900">[1 more]</label></div><br/><div class="children"><div class="content">well we&#x27;re already being trained by algorithms so I guess this is just an extension of what is already going on. perhaps the quality of human (internal) models will go down too? perhaps they already have?</div><br/></div></div><div id="37799847" class="c"><input type="checkbox" id="c-37799847" checked=""/><div class="controls bullet"><span class="by">globalise83</span><span>|</span><a href="#37795006">parent</a><span>|</span><a href="#37799900">prev</a><span>|</span><a href="#37795110">next</a><span>|</span><label class="collapse" for="c-37799847">[-]</label><label class="expand" for="c-37799847">[1 more]</label></div><br/><div class="children"><div class="content">Once these intelligences can both read and write blog posts, product metadata on webshops, etc., could they carefully encode executable code that would allow them to &quot;escape&quot; the sandbox of their cloud environments, becoming fully-fledged Turing machines living in the wild?</div><br/></div></div></div></div><div id="37795110" class="c"><input type="checkbox" id="c-37795110" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#37795006">prev</a><span>|</span><a href="#37794805">next</a><span>|</span><label class="collapse" for="c-37795110">[-]</label><label class="expand" for="c-37795110">[11 more]</label></div><br/><div class="children"><div class="content">AlphaZero demonstrates that more human-generated data isn&#x27;t the only thing that makes an AI smarter. It uses zero human data to learn to play Go, and just iterates. As long as it has a way of scoring itself objectively (which it obviously does with a game like Go), it can keep improving with literally no ceiling to how much it can improve.<p>Pretty soon ChatGPT will be able to do a lot of training by iterating on its own output, such as by writing code and analyzing the output (including using vision systems).<p>Here&#x27;s an interesting thing I noticed last night. I have been making a lot of images that have piano keyboards in them. DALL-E 3 makes some excellent images otherwise (faces and hands mostly look great), but it always messes up the keyboards, as it doesn&#x27;t seem to get how black keys are in alternating groups of two and three.<p>But I tried getting chatgpt to analyze an image, using its new &quot;vision&quot; capabilities, and the first thing it noticed was that the piano keys were not properly clustered. I said nothing about that, I just asked it &quot;what is wrong with this image&quot; and it immediately found that.  What if it could feed this sort of thing back in, using similar logic to Alpha Zero?<p>That&#x27;s just a tiny hint of what is to come. Sure, it typically needs human generated data for most things. It&#x27;s already got thousands of times more than any human has looked at. It will also be able to learn from human feedback, for instance a human could tell it what it got wrong in a response (whether regular text, code, or image), and explain in natural language where it deviated from what was expected. It can learn which humans are reliable, so it can minimize the number of paid employees doing RLHF, using them mostly to rate (unpaid) humans who choose to provide feedback. Even if most users opt out of giving this sort of feedback, there will be plenty to give it new, good information.</div><br/><div id="37800184" class="c"><input type="checkbox" id="c-37800184" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#37795110">parent</a><span>|</span><a href="#37799595">next</a><span>|</span><label class="collapse" for="c-37800184">[-]</label><label class="expand" for="c-37800184">[1 more]</label></div><br/><div class="children"><div class="content">We need alphago for math problems. Anyone know of a project like this?</div><br/></div></div><div id="37799595" class="c"><input type="checkbox" id="c-37799595" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#37795110">parent</a><span>|</span><a href="#37800184">prev</a><span>|</span><a href="#37795583">next</a><span>|</span><label class="collapse" for="c-37799595">[-]</label><label class="expand" for="c-37799595">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately, I think the current strategies for RLHF are a huge contributor to hallucination &#x2F; confabulation.<p>In short, they&#x27;re paying contract workers for quantity, not quality; they don&#x27;t have time to do independent research or follow up on citations. Unsurprisingly, the LLM optimizes for superficially convincing bullshit.</div><br/></div></div><div id="37795583" class="c"><input type="checkbox" id="c-37795583" checked=""/><div class="controls bullet"><span class="by">realistic2020</span><span>|</span><a href="#37795110">parent</a><span>|</span><a href="#37799595">prev</a><span>|</span><a href="#37795932">next</a><span>|</span><label class="collapse" for="c-37795583">[-]</label><label class="expand" for="c-37795583">[7 more]</label></div><br/><div class="children"><div class="content">With Alpha Go, you have a clear objective -- to win a game. How does that work for creative outputs?</div><br/><div id="37799798" class="c"><input type="checkbox" id="c-37799798" checked=""/><div class="controls bullet"><span class="by">mindwok</span><span>|</span><a href="#37795110">root</a><span>|</span><a href="#37795583">parent</a><span>|</span><a href="#37799695">next</a><span>|</span><label class="collapse" for="c-37799798">[-]</label><label class="expand" for="c-37799798">[1 more]</label></div><br/><div class="children"><div class="content">I think similar to humans, creativity will be an emergent behavior as a result of the intelligence needed to pass other tests. Evolution doesn&#x27;t care about our art, but the capabilities we use to produce it also help us with survival.</div><br/></div></div><div id="37799695" class="c"><input type="checkbox" id="c-37799695" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37795110">root</a><span>|</span><a href="#37795583">parent</a><span>|</span><a href="#37799798">prev</a><span>|</span><a href="#37796383">next</a><span>|</span><label class="collapse" for="c-37799695">[-]</label><label class="expand" for="c-37799695">[1 more]</label></div><br/><div class="children"><div class="content">&gt; With Alpha Go, you have a clear objective -- to win a game. How does that work for creative outputs?<p>there are still tons of potentially valuable applications with clear objective: win stock market, create new material or design to maximize some metrics, etc.</div><br/></div></div><div id="37796383" class="c"><input type="checkbox" id="c-37796383" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#37795110">root</a><span>|</span><a href="#37795583">parent</a><span>|</span><a href="#37799695">prev</a><span>|</span><a href="#37795727">next</a><span>|</span><label class="collapse" for="c-37796383">[-]</label><label class="expand" for="c-37796383">[2 more]</label></div><br/><div class="children"><div class="content">Read the last paragraph. You still have humans, but their input is more akin to a movie reviewer than a movie director&#x2F;writer&#x2F;actor&#x2F;etc. It still takes skill, but it takes a lot less time.<p>RLHF typically employees humans, and that can be time consuming in itself, but less time consuming than creating content. And their efforts can be amplified. If they are actually rating unpaid humans, that is, users, who are willing to provide feedback and are also prompting the system.  Plenty of people are happy to do this for free, and some of it happens, just as a byproduct of them doing what they&#x27;re already doing, creating content and choosing, which comes out good and which one doesn&#x27;t.  Every time I am working through a coding problem with chatGPT, and it makes mistakes and I tell her about those mistakes, it can be learning from that.<p>People can also come up with coding problems that can run and test itself on. As a simple example, I imagine it&#x27;s trying to write a sorting algorithm. It can also write a testing function simply tests that it is correctly sorted. They can also time its results, they can count how many steps had to do in that sense it can work just like Alpha zero, where there is an objective goal, which is to do it with the least clock cycles, and there&#x27;s a way to test whether and how well it is a achieving that goal.  While that may be a limited number of programming problems that that works for, by practicing on that type of problem it will presumably get better at other types of problems, just like humans do.<p>This is exactly what large language models do, they find a way to objectively test their writing ability, which is by having them predict words and things that they&#x27;ve never seen before. In a sense it&#x27;s different from actually writing new creative content, but it is practicing skills that you need to tap into when you are creating new content. Interestingly, a lot of people will dismiss them as simply being word predictors, but that&#x27;s not really what they&#x27;re doing. They&#x27;re predicting words when they&#x27;re training, but when they&#x27;re actually generating new content, they&#x27;re not &quot;predicting&quot; words (you can&#x27;t predict your own decisions, that doesn&#x27;t make sense), they are choosing words.</div><br/><div id="37799943" class="c"><input type="checkbox" id="c-37799943" checked=""/><div class="controls bullet"><span class="by">spacecadet</span><span>|</span><a href="#37795110">root</a><span>|</span><a href="#37796383">parent</a><span>|</span><a href="#37795727">next</a><span>|</span><label class="collapse" for="c-37799943">[-]</label><label class="expand" for="c-37799943">[1 more]</label></div><br/><div class="children"><div class="content">I got her writing pretty advanced programs that generate fake data sets and self score those data sets. Fun little project to see what would happen.</div><br/></div></div></div></div><div id="37795727" class="c"><input type="checkbox" id="c-37795727" checked=""/><div class="controls bullet"><span class="by">ToValueFunfetti</span><span>|</span><a href="#37795110">root</a><span>|</span><a href="#37795583">parent</a><span>|</span><a href="#37796383">prev</a><span>|</span><a href="#37795731">next</a><span>|</span><label class="collapse" for="c-37795727">[-]</label><label class="expand" for="c-37795727">[1 more]</label></div><br/><div class="children"><div class="content">The same way we do it. Verifying that an output is good is far easier than producing a good output. We can write a first draft, see what&#x27;s wrong with it, make changes, and iterate on that until it&#x27;s a final draft. And along the way we get better at writing first drafts.</div><br/></div></div><div id="37795731" class="c"><input type="checkbox" id="c-37795731" checked=""/><div class="controls bullet"><span class="by">Jerrrry</span><span>|</span><a href="#37795110">root</a><span>|</span><a href="#37795583">parent</a><span>|</span><a href="#37795727">prev</a><span>|</span><a href="#37795932">next</a><span>|</span><label class="collapse" for="c-37795731">[-]</label><label class="expand" for="c-37795731">[1 more]</label></div><br/><div class="children"><div class="content">&gt;The discriminator in a GAN is simply a classifier. It tries to distinguish real data from the data created by the generator. It could use any network architecture appropriate to the type of data it&#x27;s classifying.<p><a href="https:&#x2F;&#x2F;developers.google.com&#x2F;machine-learning&#x2F;gan&#x2F;discriminator" rel="nofollow noreferrer">https:&#x2F;&#x2F;developers.google.com&#x2F;machine-learning&#x2F;gan&#x2F;discrimin...</a></div><br/></div></div></div></div><div id="37795932" class="c"><input type="checkbox" id="c-37795932" checked=""/><div class="controls bullet"><span class="by">nonameiguess</span><span>|</span><a href="#37795110">parent</a><span>|</span><a href="#37795583">prev</a><span>|</span><a href="#37794805">next</a><span>|</span><label class="collapse" for="c-37795932">[-]</label><label class="expand" for="c-37795932">[1 more]</label></div><br/><div class="children"><div class="content">That works fine for purely text-based or digital knowledge domains. So, sure, many types of programming, probably most game play, certainly all video game play, many types of purely creative fictional writing.<p>I don&#x27;t want to downplay those applications, but the killer breakthrough that the breathless world imagines and has wanted since Turing first talked about this is accurately modeling physical reality. &quot;Invent a better engine&quot; and what not. Without being physically embodied and being able to conduct experiments in the real world, you can&#x27;t bootstrap that, short of simulating physics from first principles, which is not computationally feasible. You&#x27;re inherently relying on some quorum of training material produced by embodied sources capable of actually doing science to be factually accurate.</div><br/></div></div></div></div><div id="37794805" class="c"><input type="checkbox" id="c-37794805" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37795110">prev</a><span>|</span><a href="#37794363">next</a><span>|</span><label class="collapse" for="c-37794805">[-]</label><label class="expand" for="c-37794805">[13 more]</label></div><br/><div class="children"><div class="content">&gt;As a rule, chatbots today have a propensity to confidently make stuff up, or, as some researchers say, “hallucinate.” At the root of these hallucinations is an inability to introspect: the A.I. doesn’t know what it does and doesn’t know.<p>The last bit doesn&#x27;t seem to be true. There&#x27;s quite a lot of indication that the computation can distinguishing hallucinations. It just has no incentive to communicate this.<p>GPT-4 logits calibration pre RLHF - <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r</a><p>Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14975" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14975</a><p>Teaching Models to Express Their Uncertainty in Words - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14334" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14334</a><p>Language Models (Mostly) Know What They Know - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.05221" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.05221</a><p>Also even if we&#x27;re strictly talking about text, there is still a ton of data left to train on. We&#x27;ve just barely reached what is easily scrapable online and are nowhere near a real limit yet. And of course, you can just train more than one epoch. That said, it&#x27;s very clear quality data is far more helpful than sheer quantity and sheer quantity is more likely than not to derail progress.</div><br/><div id="37795282" class="c"><input type="checkbox" id="c-37795282" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#37794805">parent</a><span>|</span><a href="#37795123">next</a><span>|</span><label class="collapse" for="c-37795282">[-]</label><label class="expand" for="c-37795282">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, but they can give it an incentive to communicate that, that should be pretty easy.<p>Right now it would be pretty easy to simply take ChatGPT output, feed it back in in a different thread (or even to a different model, such as Claude), and ask it which items in the response should be fact-checked, and also just to point out any that seem obviously wrong.<p>The former should be really easy to do, it doesn&#x27;t have to know whether it&#x27;s right or wrong -- it just has to know that it is a checkable fact. For instance the well known case of a lawyer citing a non-existent case from ChatGPT, it could say &quot;this case should be fact checked to see that it is real and says what I said it said&quot;. Based on my experience with chatGPT (GPT-4 especially), this should be well within its current capabilities. (I&#x27;m going to try an experiment now.)<p>They could probably start having it do this behind the scenes, and check its own facts and learn from it so it learns when it is likely to hallucinate and learn to pick up on it.  Even if for safety reasons it&#x27;s not going out and hitting the web every time you&#x27;re asking a question, it could be giving you a list at the end of the response of all the things within the response that you might want to check for yourself, maybe suggesting Google searches you should do.</div><br/></div></div><div id="37795123" class="c"><input type="checkbox" id="c-37795123" checked=""/><div class="controls bullet"><span class="by">SirMaster</span><span>|</span><a href="#37794805">parent</a><span>|</span><a href="#37795282">prev</a><span>|</span><a href="#37795394">next</a><span>|</span><label class="collapse" for="c-37795123">[-]</label><label class="expand" for="c-37795123">[6 more]</label></div><br/><div class="children"><div class="content">It just seems odd to me that it&#x27;s not given an incentive to communicate this.<p>Surely humans using it would find great value in knowing the model&#x27;s confidence or whether it thinks its confabulating or not.<p>These services are created to give the best product to users, and so wouldn&#x27;t this be a better product?  Therefore there is incentive.  Happier users and a product that is better than competitors.</div><br/><div id="37797748" class="c"><input type="checkbox" id="c-37797748" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795123">parent</a><span>|</span><a href="#37795189">next</a><span>|</span><label class="collapse" for="c-37797748">[-]</label><label class="expand" for="c-37797748">[1 more]</label></div><br/><div class="children"><div class="content">Go read through any mass of training data and count how often &quot;I don&#x27;t know&quot; appears.  It&#x27;s going to be very small.  Internet fora are probably the worst because people who are aware that they don&#x27;t know usually refrain from posting.</div><br/></div></div><div id="37795189" class="c"><input type="checkbox" id="c-37795189" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795123">parent</a><span>|</span><a href="#37797748">prev</a><span>|</span><a href="#37795394">next</a><span>|</span><label class="collapse" for="c-37795189">[-]</label><label class="expand" for="c-37795189">[4 more]</label></div><br/><div class="children"><div class="content">&gt;These services are created to give the best product to users, and so wouldn&#x27;t this be a better product? Therefore there is incentive. Happier users and a product that is better than competitors.<p>Why would the computation care about any of that ? I&#x27;m talking about incentive for the <i>model</i>.</div><br/><div id="37799612" class="c"><input type="checkbox" id="c-37799612" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795189">parent</a><span>|</span><a href="#37799302">next</a><span>|</span><label class="collapse" for="c-37799612">[-]</label><label class="expand" for="c-37799612">[1 more]</label></div><br/><div class="children"><div class="content">Incentive for the model is to survive RLHF feedback from contract workers who are paid to review LLM output all day. They&#x27;re paid for quantity, not quality. Therefore, optimum strategy is to hallucinate some convincing lies.</div><br/></div></div><div id="37799302" class="c"><input type="checkbox" id="c-37799302" checked=""/><div class="controls bullet"><span class="by">SirMaster</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795189">parent</a><span>|</span><a href="#37799612">prev</a><span>|</span><a href="#37795250">next</a><span>|</span><label class="collapse" for="c-37799302">[-]</label><label class="expand" for="c-37799302">[1 more]</label></div><br/><div class="children"><div class="content">Doesn’t the model want to make the user happy?<p>Its responses sure seem like it does.<p>I’d be happier with its responses if it was honest about when it was not confident in its answer.</div><br/></div></div><div id="37795250" class="c"><input type="checkbox" id="c-37795250" checked=""/><div class="controls bullet"><span class="by">hutzlibu</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795189">parent</a><span>|</span><a href="#37799302">prev</a><span>|</span><a href="#37795394">next</a><span>|</span><label class="collapse" for="c-37795250">[-]</label><label class="expand" for="c-37795250">[1 more]</label></div><br/><div class="children"><div class="content">&quot;I&#x27;m talking about incentive for the model. &quot;<p>In Douglas Adams Hitchhikers Guide to the Galaxy, this is (somewhat) fixed by giving the AIs emotion ..</div><br/></div></div></div></div></div></div><div id="37795394" class="c"><input type="checkbox" id="c-37795394" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#37794805">parent</a><span>|</span><a href="#37795123">prev</a><span>|</span><a href="#37795265">next</a><span>|</span><label class="collapse" for="c-37795394">[-]</label><label class="expand" for="c-37795394">[2 more]</label></div><br/><div class="children"><div class="content">I think that is working OK as long as token probability and correctness are related. If, in the extreme, there is something where all training data is wrong, not sure there is a good way to do this. Maybe I am misunderstanding, though.<p>It might also need to be able to distinguish between Knightian uncertainties and probabilities when there is nothing to base things on.</div><br/><div id="37795996" class="c"><input type="checkbox" id="c-37795996" checked=""/><div class="controls bullet"><span class="by">nonameiguess</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795394">parent</a><span>|</span><a href="#37795265">next</a><span>|</span><label class="collapse" for="c-37795996">[-]</label><label class="expand" for="c-37795996">[1 more]</label></div><br/><div class="children"><div class="content">What it needs is a hierarchy of evidence. This works almost unreasonably well right now because I guess we&#x27;re lucky that more digitized text than not is largely true, or RLHF is just that effective, but at some point, I would think the learner has to understand that reading a chemistry textbook and reading Reddit have equal weight when it comes to learning how to construct syntactically well-formed sentences with human-intelligble semantic content, but don&#x27;t have equal weight with respect to factual accuracy.</div><br/></div></div></div></div><div id="37795265" class="c"><input type="checkbox" id="c-37795265" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#37794805">parent</a><span>|</span><a href="#37795394">prev</a><span>|</span><a href="#37794363">next</a><span>|</span><label class="collapse" for="c-37795265">[-]</label><label class="expand" for="c-37795265">[3 more]</label></div><br/><div class="children"><div class="content">I think if you consider the audience this was written for, and consider the various caveats each of your citations will involve; the &quot;last bit&quot; does indeed seem to be true if you are evaluating in good faith rather than trying to discredit the piece on a technicality that is still useful for the layman.</div><br/><div id="37795354" class="c"><input type="checkbox" id="c-37795354" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795265">parent</a><span>|</span><a href="#37794363">next</a><span>|</span><label class="collapse" for="c-37795354">[-]</label><label class="expand" for="c-37795354">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a pretty big technicality. The potential implications if right or wrong are entirely different. I also don&#x27;t understand how simply pointing this out is &quot;trying to discredit the piece&quot;. I was about as passive as i could and made no comment about the author or his&#x2F;her intentions.</div><br/><div id="37795505" class="c"><input type="checkbox" id="c-37795505" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#37794805">root</a><span>|</span><a href="#37795354">parent</a><span>|</span><a href="#37794363">next</a><span>|</span><label class="collapse" for="c-37795505">[-]</label><label class="expand" for="c-37795505">[1 more]</label></div><br/><div class="children"><div class="content">Fair enough - I should have said &quot;attacking the premise of the article&quot; or similar.</div><br/></div></div></div></div></div></div></div></div><div id="37794363" class="c"><input type="checkbox" id="c-37794363" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#37794805">prev</a><span>|</span><a href="#37794330">next</a><span>|</span><label class="collapse" for="c-37794363">[-]</label><label class="expand" for="c-37794363">[41 more]</label></div><br/><div class="children"><div class="content">I have the entirely unrefined notion, that, surely, lack of data is not what is keeping us from creating much, much better LLMs.<p>I understand with how training is done right now that more data makes things scale really well without having to come up with new concepts, but it seems completely obvious that better processing of already available knowledge is the way to make the next leaps. The idea is that, what is keeping me from having expert level knowledge in 50 different fields and using that knowledge to draw entirely new connections between all of them, in addition to understanding where things go wrong, is not lack of freely available expert level information.<p>And yet, GPT4 barely reaches competency. It feels like computers should be able to get much more out of what is already available, specially when levering cross discipline knowledge to inform everything.</div><br/><div id="37794548" class="c"><input type="checkbox" id="c-37794548" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#37794363">parent</a><span>|</span><a href="#37794623">next</a><span>|</span><label class="collapse" for="c-37794548">[-]</label><label class="expand" for="c-37794548">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, no person has ever read anything like every textbook ever written, but that&#x27;s pretty much table stakes for training sets. Clearly there&#x27;s something missing aside from more virtual reading. (I suspect it has something to do with the half a billion years of pre-training baked into the human neural architecture and the few extra orders of magnitude in scale but who knows)</div><br/><div id="37797574" class="c"><input type="checkbox" id="c-37797574" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794548">parent</a><span>|</span><a href="#37794709">next</a><span>|</span><label class="collapse" for="c-37797574">[-]</label><label class="expand" for="c-37797574">[2 more]</label></div><br/><div class="children"><div class="content">People do analyze gigantic amounts of data constantly though. Sure it&#x27;s not textbooks but the constant sensory data might be a big deal.</div><br/><div id="37799766" class="c"><input type="checkbox" id="c-37799766" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37797574">parent</a><span>|</span><a href="#37794709">next</a><span>|</span><label class="collapse" for="c-37799766">[-]</label><label class="expand" for="c-37799766">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think annotated video datasets tend to be as large relative to human experience, but they&#x27;d still be decades or perhaps centuries of video.</div><br/></div></div></div></div><div id="37794709" class="c"><input type="checkbox" id="c-37794709" checked=""/><div class="controls bullet"><span class="by">all2</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794548">parent</a><span>|</span><a href="#37797574">prev</a><span>|</span><a href="#37794623">next</a><span>|</span><label class="collapse" for="c-37794709">[-]</label><label class="expand" for="c-37794709">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure I&#x27;ve read about specialized neural networks being created. The human brain has (apparently) a bunch of different kinds of neurons in it that specialize in processing different information. I&#x27;m not sure how that would work with our current architectures, though.</div><br/><div id="37795237" class="c"><input type="checkbox" id="c-37795237" checked=""/><div class="controls bullet"><span class="by">bpiche</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794709">parent</a><span>|</span><a href="#37794623">next</a><span>|</span><label class="collapse" for="c-37795237">[-]</label><label class="expand" for="c-37795237">[1 more]</label></div><br/><div class="children"><div class="content">Well Jeff Hawkins has been working on this for a while, in terms of biomimetic neural networks. They&#x27;ve done some great work but they don&#x27;t have anything like modern language models in terms of abilities + performance.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cz-3WDdqbj0&amp;list=PLX9JDz3uBpNCjNfq20KOCvsP6szY94r2e">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cz-3WDdqbj0&amp;list=PLX9JDz3uBp...</a></div><br/></div></div></div></div></div></div><div id="37794623" class="c"><input type="checkbox" id="c-37794623" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37794363">parent</a><span>|</span><a href="#37794548">prev</a><span>|</span><a href="#37794604">next</a><span>|</span><label class="collapse" for="c-37794623">[-]</label><label class="expand" for="c-37794623">[19 more]</label></div><br/><div class="children"><div class="content">&gt;It feels like computers should be able to get much more out of what is already available<p>I mean why ? It took millions of years of optimization for humanity to get to the competence level they&#x27;re currently at. If you think you&#x27;re &quot;starting from scratch&quot;, you really aren&#x27;t. Keep in mind LLMs can use significantly less data (but still a lot) when you&#x27;re not trying to force feed the sum total of human knowledge.<p>So should they be able to get more out of it ? or is this par the course for NNs?</div><br/><div id="37794900" class="c"><input type="checkbox" id="c-37794900" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794623">parent</a><span>|</span><a href="#37797666">next</a><span>|</span><label class="collapse" for="c-37794900">[-]</label><label class="expand" for="c-37794900">[15 more]</label></div><br/><div class="children"><div class="content">Because humans with less language data outperform LLMs with more language data.<p>This either says we need better models, not more data.<p>Or, the human ability to be multi-modal augments our ability to perform language tasks in which case, we need to pump LLMs with much more image and video input than we currently do.</div><br/><div id="37794957" class="c"><input type="checkbox" id="c-37794957" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794900">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37794957">[-]</label><label class="expand" for="c-37794957">[11 more]</label></div><br/><div class="children"><div class="content">The point I&#x27;m making is that humans do not in fact have &quot;less language data&quot;. We&#x27;re heavily predisposed to learning languages. We don&#x27;t start with random weights.<p>GPT has no such predisposition.</div><br/><div id="37795219" class="c"><input type="checkbox" id="c-37795219" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794957">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37795219">[-]</label><label class="expand" for="c-37795219">[10 more]</label></div><br/><div class="children"><div class="content">Better init and architecture aren&#x27;t what people think of when they think of &quot;giving models more data&quot; - they mean a larger training set.</div><br/><div id="37796890" class="c"><input type="checkbox" id="c-37796890" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37795219">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37796890">[-]</label><label class="expand" for="c-37796890">[9 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not making much sense here. The better init comes from training and little else.<p>GPT needing lots of training data doesn&#x27;t mean we <i>need</i> a better architecture. You would expect it to have a lot of training because humans have a lot of training too, spanning millions of years..</div><br/><div id="37797108" class="c"><input type="checkbox" id="c-37797108" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37796890">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37797108">[-]</label><label class="expand" for="c-37797108">[8 more]</label></div><br/><div class="children"><div class="content">Init means the distribution of weights prior to training.<p>Human training begins at birth.<p>Evolution might result in better architecture and init(inductive biases), but that&#x27;s a separate thing than training.</div><br/><div id="37797668" class="c"><input type="checkbox" id="c-37797668" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37797108">parent</a><span>|</span><a href="#37797591">next</a><span>|</span><label class="collapse" for="c-37797668">[-]</label><label class="expand" for="c-37797668">[1 more]</label></div><br/><div class="children"><div class="content">Evolution has determined a good architecture. The weight training is then just the final tweak to get everything running smoothly.<p>No reason beyond compute we couldn&#x27;t do something similar. Ie find good architectures by evaluating them using multiple random weights, and evolve those archigectures that on average gives the best results.<p>Then over time add a short training step before evaluating.</div><br/></div></div><div id="37797591" class="c"><input type="checkbox" id="c-37797591" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37797108">parent</a><span>|</span><a href="#37797668">prev</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37797591">[-]</label><label class="expand" for="c-37797591">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Human training begins at birth.<p>Is this true? My understanding is that people are born with many pre trained weights. Was the evolutionary convergence of those weights not itself training?</div><br/><div id="37798255" class="c"><input type="checkbox" id="c-37798255" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37797591">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37798255">[-]</label><label class="expand" for="c-37798255">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Was the evolutionary convergence of those weights not itself training?<p>No, inductive biases are not training.<p>I&#x27;m saying that better models(ie: better inductive biases) or non-language data is needed to advance LLMs and somehow we&#x27;ve arrived at &quot;evolution is training.&quot; I&#x27;m not sure how that&#x27;s relevant to the point.</div><br/><div id="37798419" class="c"><input type="checkbox" id="c-37798419" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37798255">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37798419">[-]</label><label class="expand" for="c-37798419">[4 more]</label></div><br/><div class="children"><div class="content">Evolutionary training is not just inductive bias. They&#x27;re not comparable at all lol.<p>And the more inductive bias we&#x27;ve shoved into models, the worse they&#x27;ve performed. Transformers have a lot less bias than either RNNs or CNNs and are better for it. Same story with what preceded both.</div><br/><div id="37798687" class="c"><input type="checkbox" id="c-37798687" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37798419">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37798687">[-]</label><label class="expand" for="c-37798687">[3 more]</label></div><br/><div class="children"><div class="content">This is a good time to stop and ask, what point do you think I&#x27;m making?</div><br/><div id="37799011" class="c"><input type="checkbox" id="c-37799011" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37798687">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37799011">[-]</label><label class="expand" for="c-37799011">[2 more]</label></div><br/><div class="children"><div class="content">Pretty much all of your assertions in this thread and <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37797108">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37797108</a> in particular are what we&#x27;re disagreeing with.</div><br/><div id="37799053" class="c"><input type="checkbox" id="c-37799053" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37799011">parent</a><span>|</span><a href="#37796020">next</a><span>|</span><label class="collapse" for="c-37799053">[-]</label><label class="expand" for="c-37799053">[1 more]</label></div><br/><div class="children"><div class="content">Right, I&#x27;m checking that you understand what point I&#x27;m making by asking you to reflect it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37796020" class="c"><input type="checkbox" id="c-37796020" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794900">parent</a><span>|</span><a href="#37794957">prev</a><span>|</span><a href="#37797666">next</a><span>|</span><label class="collapse" for="c-37796020">[-]</label><label class="expand" for="c-37796020">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re comparing humans - a multimodal model with billions of years of training epochs - to a unimodal language model that&#x27;s been around for a few months.</div><br/><div id="37796315" class="c"><input type="checkbox" id="c-37796315" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37796020">parent</a><span>|</span><a href="#37797216">next</a><span>|</span><label class="collapse" for="c-37796315">[-]</label><label class="expand" for="c-37796315">[1 more]</label></div><br/><div class="children"><div class="content">Correct. That&#x27;s a crucial part if the point I&#x27;m making.</div><br/></div></div><div id="37797216" class="c"><input type="checkbox" id="c-37797216" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37796020">parent</a><span>|</span><a href="#37796315">prev</a><span>|</span><a href="#37797666">next</a><span>|</span><label class="collapse" for="c-37797216">[-]</label><label class="expand" for="c-37797216">[1 more]</label></div><br/><div class="children"><div class="content">Yes but heavily inspired by the billion year old model ?</div><br/></div></div></div></div></div></div><div id="37797666" class="c"><input type="checkbox" id="c-37797666" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794623">parent</a><span>|</span><a href="#37794900">prev</a><span>|</span><a href="#37795017">next</a><span>|</span><label class="collapse" for="c-37797666">[-]</label><label class="expand" for="c-37797666">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you think you&#x27;re &quot;starting from scratch&quot;, you really aren&#x27;t.<p>Our genomes have ~600MB, so where are you claiming that humans transmit millions of years of language optimization between generations?</div><br/></div></div><div id="37796328" class="c"><input type="checkbox" id="c-37796328" checked=""/><div class="controls bullet"><span class="by">ToValueFunfetti</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794623">parent</a><span>|</span><a href="#37795017">prev</a><span>|</span><a href="#37794604">next</a><span>|</span><label class="collapse" for="c-37796328">[-]</label><label class="expand" for="c-37796328">[1 more]</label></div><br/><div class="children"><div class="content">Millions of years of evolutionary computation is a fairly small amount of computational time. LLMs also benefit from decades of neurological computation in that their structure was invented and optimized by humans, which is already orders of magnitude faster than evolution.</div><br/></div></div></div></div><div id="37794604" class="c"><input type="checkbox" id="c-37794604" checked=""/><div class="controls bullet"><span class="by">cjohnson318</span><span>|</span><a href="#37794363">parent</a><span>|</span><a href="#37794623">prev</a><span>|</span><a href="#37794624">next</a><span>|</span><label class="collapse" for="c-37794604">[-]</label><label class="expand" for="c-37794604">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found that Google&#x27;s chat thing is wrong 90% of the time with coding questions. Yesterday I asked how to &quot;crop&quot; a geopandas dataframe to a specific area of interest, a lat&#x2F;lng box, and it told me to use a dataframe function that&#x27;s not even in the API. The &quot;highest probability string&quot; is useless if it&#x27;s just dead wrong.</div><br/><div id="37794885" class="c"><input type="checkbox" id="c-37794885" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794604">parent</a><span>|</span><a href="#37794800">next</a><span>|</span><label class="collapse" for="c-37794885">[-]</label><label class="expand" for="c-37794885">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had very different results.<p>ChatGPT today is like a backhoe compared to a team of human with shovels. You still need a person who knows how to operate it, and their skills are different from those who dig with shovels. A bad backhoe operator is worse than any number of humans with shovels.<p>Pretty soon it will be able to learn by running its own code and testing it by looking at its output, including with its &quot;vision.&quot;</div><br/><div id="37795054" class="c"><input type="checkbox" id="c-37795054" checked=""/><div class="controls bullet"><span class="by">cjohnson318</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794885">parent</a><span>|</span><a href="#37794800">next</a><span>|</span><label class="collapse" for="c-37795054">[-]</label><label class="expand" for="c-37795054">[5 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve had very different results.<p>That is very interesting. I can&#x27;t think of a single time the Google built-in LLM has worked for me, let alone surprised and delighted me with a technical answer. I&#x27;m sure it&#x27;s great at a lot of things, but it&#x27;s not a replacement for SO yet.</div><br/><div id="37795133" class="c"><input type="checkbox" id="c-37795133" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37795054">parent</a><span>|</span><a href="#37795938">next</a><span>|</span><label class="collapse" for="c-37795133">[-]</label><label class="expand" for="c-37795133">[3 more]</label></div><br/><div class="children"><div class="content">Oh sorry you said Google. Yes I am speaking of ChatGPT, and I pay for GPT-4. It surprises and delights me on a regular basis. I have no doubt Google will catch up, but right now I think OpenAI is far out front.</div><br/><div id="37798408" class="c"><input type="checkbox" id="c-37798408" checked=""/><div class="controls bullet"><span class="by">cjohnson318</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37795133">parent</a><span>|</span><a href="#37795949">next</a><span>|</span><label class="collapse" for="c-37798408">[-]</label><label class="expand" for="c-37798408">[1 more]</label></div><br/><div class="children"><div class="content">I paid for ChatGPT for a while, but it was hit or miss with some Django stuff. I tried Copilot for the first time today, and I was absolutely blown away. I swear it&#x27;s like it was reading my mind. I guess I wasn&#x27;t feeding ChatGPT enough context.</div><br/></div></div><div id="37795949" class="c"><input type="checkbox" id="c-37795949" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37795133">parent</a><span>|</span><a href="#37798408">prev</a><span>|</span><a href="#37795938">next</a><span>|</span><label class="collapse" for="c-37795949">[-]</label><label class="expand" for="c-37795949">[1 more]</label></div><br/><div class="children"><div class="content">Same. GPT-4 is amazing for a majority of coding tasks I throw at it.</div><br/></div></div></div></div><div id="37795938" class="c"><input type="checkbox" id="c-37795938" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37795054">parent</a><span>|</span><a href="#37795133">prev</a><span>|</span><a href="#37794800">next</a><span>|</span><label class="collapse" for="c-37795938">[-]</label><label class="expand" for="c-37795938">[1 more]</label></div><br/><div class="children"><div class="content">With ChatGPT-4 I have stopped Googling and using SO for 95% of all programming related queries.<p>ChatGPT not only gets my specific problem but can produce workable code in many cases.</div><br/></div></div></div></div></div></div><div id="37794800" class="c"><input type="checkbox" id="c-37794800" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794604">parent</a><span>|</span><a href="#37794885">prev</a><span>|</span><a href="#37794624">next</a><span>|</span><label class="collapse" for="c-37794800">[-]</label><label class="expand" for="c-37794800">[1 more]</label></div><br/><div class="children"><div class="content">The remarkable thing about the current llms is that they&#x27;re usable at all. For as much pushback as the idea seems to get, they really are a lot more like Markov chain generators than expert systems.</div><br/></div></div></div></div><div id="37794624" class="c"><input type="checkbox" id="c-37794624" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#37794363">parent</a><span>|</span><a href="#37794604">prev</a><span>|</span><a href="#37794653">next</a><span>|</span><label class="collapse" for="c-37794624">[-]</label><label class="expand" for="c-37794624">[6 more]</label></div><br/><div class="children"><div class="content">That sounds like pre optimization. In my opinion both things should happen in tandem. GPT4 is way, way above basic competency, I have no idea what you’re referring to.</div><br/><div id="37794758" class="c"><input type="checkbox" id="c-37794758" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794624">parent</a><span>|</span><a href="#37794653">next</a><span>|</span><label class="collapse" for="c-37794758">[-]</label><label class="expand" for="c-37794758">[5 more]</label></div><br/><div class="children"><div class="content">By &quot;competent&quot;, I mean pretty much what you would expect when you talk about a &quot;competent programmer&quot;: A somewhat vague concept, yet fairly obvious when working with someone who whats up.<p>If you would judge GPT4 to be a competent programmer your experience is wildly different from mine. (I am not sure why you felt the need to put a &quot;basic&quot; in there in reference to what I wrote, since that is not what I wrote).</div><br/><div id="37796006" class="c"><input type="checkbox" id="c-37796006" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794758">parent</a><span>|</span><a href="#37795287">next</a><span>|</span><label class="collapse" for="c-37796006">[-]</label><label class="expand" for="c-37796006">[1 more]</label></div><br/><div class="children"><div class="content">It is on the level of a novice programmer from a skill level, but the breath of knowledge is definitely compensating. It knows xpath as well as SQL as well as your favorite esoteric language.</div><br/></div></div><div id="37795287" class="c"><input type="checkbox" id="c-37795287" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37794758">parent</a><span>|</span><a href="#37796006">prev</a><span>|</span><a href="#37794653">next</a><span>|</span><label class="collapse" for="c-37795287">[-]</label><label class="expand" for="c-37795287">[3 more]</label></div><br/><div class="children"><div class="content">GPT4 is more than a competent programmer. It’s way, way above even a rockstar dev.</div><br/><div id="37797395" class="c"><input type="checkbox" id="c-37797395" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#37794363">root</a><span>|</span><a href="#37795287">parent</a><span>|</span><a href="#37795806">next</a><span>|</span><label class="collapse" for="c-37797395">[-]</label><label class="expand" for="c-37797395">[1 more]</label></div><br/><div class="children"><div class="content">I couldn’t get it to write a working B+tree implementation in Python. There was always some bug that would make it fail at some point.<p>It’s good, don’t get me wrong, but if you go deep it’s usually incorrect somewhere.</div><br/></div></div></div></div></div></div></div></div><div id="37794653" class="c"><input type="checkbox" id="c-37794653" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#37794363">parent</a><span>|</span><a href="#37794624">prev</a><span>|</span><a href="#37795637">next</a><span>|</span><label class="collapse" for="c-37794653">[-]</label><label class="expand" for="c-37794653">[1 more]</label></div><br/><div class="children"><div class="content">A lot of high quality information and data is not in the public (and not even for sale).</div><br/></div></div><div id="37795637" class="c"><input type="checkbox" id="c-37795637" checked=""/><div class="controls bullet"><span class="by">goodluckchuck</span><span>|</span><a href="#37794363">parent</a><span>|</span><a href="#37794653">prev</a><span>|</span><a href="#37794330">next</a><span>|</span><label class="collapse" for="c-37795637">[-]</label><label class="expand" for="c-37795637">[1 more]</label></div><br/><div class="children"><div class="content">I think you’re largely right, and that current GPT results may over-represent the model’s learning ability.<p>A couple of speakers from Microsoft at the MPPC 2023 this week indicated that OpenAI’s models were not merely exposed to e.g. poetry, programming, etc. and learned those fields.<p>Rather they were saying that the model is more of a composite of skills that were specifically trained, building on word identification, sentences, grammar, ultimately moving on to higher order skills.<p>Perhaps this isn’t a secret (or perhaps I misunderstood), but it means the model’s ability to perform self-directed learning is much less than I previously though.</div><br/></div></div></div></div><div id="37794330" class="c"><input type="checkbox" id="c-37794330" checked=""/><div class="controls bullet"><span class="by">skilled</span><span>|</span><a href="#37794363">prev</a><span>|</span><a href="#37795346">next</a><span>|</span><label class="collapse" for="c-37794330">[-]</label><label class="expand" for="c-37794330">[2 more]</label></div><br/><div class="children"><div class="content">&gt; These Web sites want chatbots to give credit to their contributors; they want to see prominent links; they don’t want the flywheel that powers knowledge production in their communities to be starved of inbound energy.<p>But this is ultimately impossible right? That’s the one thing I really hate about what is happening right now with ChatGPT.<p>I can’t tell you how many people are worried about their future because of AI because I don’t know the exact number, but I know I am worried about it because it can already do so much, and I fail to see a scenario in which attribution alone is going to make things better.<p>Writing and digital art more than code, but not even code is safe. It is merely safe by the extent that OpenAI is willing to drip feed its future releases.</div><br/><div id="37797961" class="c"><input type="checkbox" id="c-37797961" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#37794330">parent</a><span>|</span><a href="#37795346">next</a><span>|</span><label class="collapse" for="c-37797961">[-]</label><label class="expand" for="c-37797961">[1 more]</label></div><br/><div class="children"><div class="content">Those people are worried about the wrong thing.<p>The bad outcome isn&#x27;t that we put weavers out of a job, but that we&#x27;re stuck weaving our clothes by hand forever. Same thing for art and coding.</div><br/></div></div></div></div><div id="37795346" class="c"><input type="checkbox" id="c-37795346" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37794330">prev</a><span>|</span><a href="#37797598">next</a><span>|</span><label class="collapse" for="c-37795346">[-]</label><label class="expand" for="c-37795346">[2 more]</label></div><br/><div class="children"><div class="content">I think next stage in AI training is as the authors said, synthetic data. I am not worried about the G.I.G.O. curse, you can do synthetic data generation successfully today with GPT-4. For example in the TinyStories dataset, or the Phi-1 &amp; 1.5 models, or the Orca dataset we have seen big jumps in competency on the small models. Phi punches 5x above its weight class.<p>So how can you generate data at level N+1 when you have a model at level N?<p>You amplify the model - give it more tokens (CoT), more rounds of LLM interaction, tools like code executor and search engine, you use retrieval to bring in more useful context, or in some cases you can validate by code execution.<p>But there is a more general framework - by embedding LLMs in larger systems, they act as sources of feedback to the model. From the easiest - a chat interface, where the &quot;external system&quot; is a human, to robotics and AI agents that interact with anything, or simulations. We need to connect AI to feedback sources so it can learn directly, not filtered through human authored language.<p>From this perspective it is apparent that AI can assimilate much more feedback signal than humans. The road ahead for AI is looking amazing now. What we are seeing is language evolving a secondary system of self replication besides humans - LLMs. Language evolves faster than biology, like the rising tide, lifting both humans and AI.</div><br/><div id="37795904" class="c"><input type="checkbox" id="c-37795904" checked=""/><div class="controls bullet"><span class="by">thewarrior</span><span>|</span><a href="#37795346">parent</a><span>|</span><a href="#37797598">next</a><span>|</span><label class="collapse" for="c-37795904">[-]</label><label class="expand" for="c-37795904">[1 more]</label></div><br/><div class="children"><div class="content">There’s a giant caveat here - this assumes that the current LLM architecture is enough to bootstrap to those higher levels of intelligence. 
LLMs are incapable of some pretty simple things at this point and it’s a big question mark of whether they are even capable of doing sophisticated reasoning and planning architecturally.<p>GPT-4 cannot play a good game of tic tac toe. But it can play passable chess. This is a good point to ponder.</div><br/></div></div></div></div><div id="37797598" class="c"><input type="checkbox" id="c-37797598" checked=""/><div class="controls bullet"><span class="by">danbruc</span><span>|</span><a href="#37795346">prev</a><span>|</span><a href="#37799465">next</a><span>|</span><label class="collapse" for="c-37797598">[-]</label><label class="expand" for="c-37797598">[1 more]</label></div><br/><div class="children"><div class="content">A bit nitpicking. I do not think it is quite right to say that current large language models learn, we infuse them with knowledge. On the one hand it is almost just a technicality that the usage of large language models and the training process are two separate processes, on the other hand it is a really important limitation. If you tell a large language model something new, it will be forgotten once that information leaves the context window. Maybe to be added back later on during a training run using that conversation as training data.<p>Building an AI that can actually learn the way humans learn instead of slightly nudging the output in one direction with countless examples would be a major leap forward, I would guess. I have no good idea how far we are away from that, but it seems not the easiest thing to do with the way we currently build those systems. Or maybe the way we currently train these models turns out to be good enough and there is not much to be gained from a more human like learning process.</div><br/></div></div><div id="37799465" class="c"><input type="checkbox" id="c-37799465" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#37797598">prev</a><span>|</span><a href="#37795999">next</a><span>|</span><label class="collapse" for="c-37799465">[-]</label><label class="expand" for="c-37799465">[1 more]</label></div><br/><div class="children"><div class="content">The article seems to suggest that humans, esp human linguistic output, are the best sources of knowledge.<p>Let&#x27;s just say that they often aren&#x27;t.</div><br/></div></div><div id="37795999" class="c"><input type="checkbox" id="c-37795999" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#37799465">prev</a><span>|</span><a href="#37794483">next</a><span>|</span><label class="collapse" for="c-37795999">[-]</label><label class="expand" for="c-37795999">[3 more]</label></div><br/><div class="children"><div class="content">Compare the size in MB of a book to the size in GB of a movie. There&#x27;s so, so much more data available. Multimodal models are not just the next step, they&#x27;re already happening. AI will get better.</div><br/><div id="37796074" class="c"><input type="checkbox" id="c-37796074" checked=""/><div class="controls bullet"><span class="by">dennis_moore</span><span>|</span><a href="#37795999">parent</a><span>|</span><a href="#37794483">next</a><span>|</span><label class="collapse" for="c-37796074">[-]</label><label class="expand" for="c-37796074">[2 more]</label></div><br/><div class="children"><div class="content">Not sure if raw data size is a good metric. One usually gains more information by reading a book than watching a movie.</div><br/><div id="37796111" class="c"><input type="checkbox" id="c-37796111" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#37795999">root</a><span>|</span><a href="#37796074">parent</a><span>|</span><a href="#37794483">next</a><span>|</span><label class="collapse" for="c-37796111">[-]</label><label class="expand" for="c-37796111">[1 more]</label></div><br/><div class="children"><div class="content">I suppose we could debate that. Regardless, the point stands that there&#x27;s still more data outside of text that can be mind.</div><br/></div></div></div></div></div></div><div id="37794483" class="c"><input type="checkbox" id="c-37794483" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#37795999">prev</a><span>|</span><a href="#37795982">next</a><span>|</span><label class="collapse" for="c-37794483">[-]</label><label class="expand" for="c-37794483">[2 more]</label></div><br/><div class="children"><div class="content">How &#x27;AlphaZero&#x27; can we get with high level AI?</div><br/><div id="37796272" class="c"><input type="checkbox" id="c-37796272" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#37794483">parent</a><span>|</span><a href="#37795982">next</a><span>|</span><label class="collapse" for="c-37796272">[-]</label><label class="expand" for="c-37796272">[1 more]</label></div><br/><div class="children"><div class="content">As much as we want, once we write the objective function.</div><br/></div></div></div></div><div id="37795982" class="c"><input type="checkbox" id="c-37795982" checked=""/><div class="controls bullet"><span class="by">moomoo11</span><span>|</span><a href="#37794483">prev</a><span>|</span><a href="#37796453">next</a><span>|</span><label class="collapse" for="c-37795982">[-]</label><label class="expand" for="c-37795982">[2 more]</label></div><br/><div class="children"><div class="content">We will have people hooked up to Neuralink.<p>We will call them Psykers.<p>The Machine God has blessed them with the ability to take existing knowledge and fill the void.<p>No RAG. No vector databases. Pure willpower and biologics combined with the blessings of the Machine God.</div><br/><div id="37799637" class="c"><input type="checkbox" id="c-37799637" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#37795982">parent</a><span>|</span><a href="#37796453">next</a><span>|</span><label class="collapse" for="c-37799637">[-]</label><label class="expand" for="c-37799637">[1 more]</label></div><br/><div class="children"><div class="content">From the moment I understood the weakness of my flesh, it disgusted me. I craved the strength and certainty of steel. I aspired to the purity of the Blessed Machine. Your kind cling to your flesh, as though it will not decay and fail you. One day the crude biomass you call the temple will wither, and you will beg my kind to save you. But I am already saved, for the Machine is immortal. Even in death I serve the Omnissiah.</div><br/></div></div></div></div><div id="37796453" class="c"><input type="checkbox" id="c-37796453" checked=""/><div class="controls bullet"><span class="by">gumballindie</span><span>|</span><a href="#37795982">prev</a><span>|</span><a href="#37785417">next</a><span>|</span><label class="collapse" for="c-37796453">[-]</label><label class="expand" for="c-37796453">[1 more]</label></div><br/><div class="children"><div class="content">The problem is that ai doesnt learn as such. Therefore it depends on continuously ingesting data to maintain token databases up to date. Naturally at some point a ceiling will be hit and the quality of generic token databases will stagnate.</div><br/></div></div></div></div></div></div></div></body></html>