<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688115646420" as="style"/><link rel="stylesheet" href="styles.css?v=1688115646420"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://lmsys.org/blog/2023-06-29-longchat/">How long can open-source LLMs truly promise on context length?</a> <span class="domain">(<a href="https://lmsys.org">lmsys.org</a>)</span></div><div class="subtext"><span>dacheng2</span> | <span>43 comments</span></div><br/><div><div id="36529337" class="c"><input type="checkbox" id="c-36529337" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#36528478">next</a><span>|</span><label class="collapse" for="c-36529337">[-]</label><label class="expand" for="c-36529337">[14 more]</label></div><br/><div class="children"><div class="content">I think this is more of an issue of smaller models with larger context aren&#x27;t a magic bullet and I&#x27;m not sure anyone really paying attention thought they would be.<p>Falcon 40BN, after extensive use, is the only model that has crossed the &quot;I&#x27;d build a product on this&quot; threshold in open source. I expect my tune won&#x27;t change there until we see larger Open Source models trained on high quality data. These are the models that will benefit from the extended context lengths.<p>But longer context without a model capable of utilizing the context to actually do things, is yeah, unsurprisingly, not useful.<p>There&#x27;s also not really an incentive to currently make that happen in Open Source. If you can train a big model, with a unique high quality dataset you&#x27;ve assembled, using some novel techniques on some performant serving infrastructure, you basically have a blank check right now.<p>Edit: On my last point, I&#x27;m going to say I was being very jaded. I think the community is doing a lot of awesome things and I know of a few efforts personally to train these larger models for open source consumption. I don&#x27;t mean to be so overly negative here.</div><br/><div id="36529510" class="c"><input type="checkbox" id="c-36529510" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36529337">parent</a><span>|</span><a href="#36529556">next</a><span>|</span><label class="collapse" for="c-36529510">[-]</label><label class="expand" for="c-36529510">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty amazing to me that there are any open source models that have crossed that threshold - especially given that prior to the Llama release in March there were hardly any open source models that felt like they could catch up with GPT-3 at all.<p>If you&#x27;re looking for an incentive to make this happen in open source, how about the team behind MPT-30B just getting acquired for $1.3 billion.<p>There are a LOT of well-funded groups trying to make an impact in open source models right now.</div><br/><div id="36529557" class="c"><input type="checkbox" id="c-36529557" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36529510">parent</a><span>|</span><a href="#36529556">next</a><span>|</span><label class="collapse" for="c-36529557">[-]</label><label class="expand" for="c-36529557">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m highly skeptical that people are just going to leave the secret sauce out in the open for anyone to directly compete with them, feels a bit too idealistic to me.<p>If you look at Mosaic, their value, their secret sauce, is the infrastructure. The model, to say it cynically, was marketing. Now it&#x27;s true it can be marketing and help push open source forward at the same time.<p>More of a separate point, but I also think models are going to be fairly quickly commoditized on a couple years timescale, so the infrastructure to train a model or serve a model, or the thing built on the model is the value, but right now models are still the value because they&#x27;re so new and we haven&#x27;t finished (in fact we&#x27;re really only starting) the race to the bottom.</div><br/><div id="36529613" class="c"><input type="checkbox" id="c-36529613" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36529557">parent</a><span>|</span><a href="#36530744">next</a><span>|</span><label class="collapse" for="c-36529613">[-]</label><label class="expand" for="c-36529613">[2 more]</label></div><br/><div class="children"><div class="content">Sure, OpenAI and Anthropic clearly have some secret sauce that they&#x27;re holding back... but the rate at which the open source research community has been catching up is quite frankly astonishing.<p>I would be very surprised if the closed source research teams can maintain their advantage over the next 12-24 months - especially given the number of enormous GPU clusters that are becoming available now.</div><br/><div id="36529657" class="c"><input type="checkbox" id="c-36529657" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36529613">parent</a><span>|</span><a href="#36530744">next</a><span>|</span><label class="collapse" for="c-36529657">[-]</label><label class="expand" for="c-36529657">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t want to diminish the efforts of the community, I agree the work is enormous and the strides are pretty crazy. I honestly hope I&#x27;m wrong. I was talking with a friend and discussing that this is one of the coolest collaborative periods I&#x27;ve seen in tech so far. Hopefully it plays out your way and not mine :)</div><br/></div></div></div></div><div id="36530744" class="c"><input type="checkbox" id="c-36530744" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36529557">parent</a><span>|</span><a href="#36529613">prev</a><span>|</span><a href="#36530580">next</a><span>|</span><label class="collapse" for="c-36530744">[-]</label><label class="expand" for="c-36530744">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been testing the MPT models and they’re better than all the other open source stuff I’ve tried. I’m using my task-based, one-shot prompts that are about 1-2K tokens. Seems pretty on par with GPT-3.5. Marketing or whatever, first time I’ve been at all impressed.</div><br/></div></div><div id="36530580" class="c"><input type="checkbox" id="c-36530580" checked=""/><div class="controls bullet"><span class="by">cced</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36529557">parent</a><span>|</span><a href="#36530744">prev</a><span>|</span><a href="#36529556">next</a><span>|</span><label class="collapse" for="c-36530580">[-]</label><label class="expand" for="c-36530580">[1 more]</label></div><br/><div class="children"><div class="content">What is secret about infrastructure?</div><br/></div></div></div></div></div></div><div id="36529556" class="c"><input type="checkbox" id="c-36529556" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36529337">parent</a><span>|</span><a href="#36529510">prev</a><span>|</span><a href="#36530863">next</a><span>|</span><label class="collapse" for="c-36529556">[-]</label><label class="expand" for="c-36529556">[6 more]</label></div><br/><div class="children"><div class="content">Falcon was trained by a research think tank, and this is going to happen more and more as AI compute gets cheaper and the frameworks get better. We aren&#x27;t going to be stuck with overpriced A100s running early PyTorch eager mode code for long.<p>High quality &quot;open&quot; data is an tricky problem, but the community has already come up with some impressive datasets for finetunes, like the roleplaying collection used for Chronos and Pygmalion. I think this is going to get better too, epecially if cheap models can &quot;distill&quot; lower quality human datasets.</div><br/><div id="36529616" class="c"><input type="checkbox" id="c-36529616" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36529556">parent</a><span>|</span><a href="#36530519">next</a><span>|</span><label class="collapse" for="c-36529616">[-]</label><label class="expand" for="c-36529616">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We aren&#x27;t going to be stuck with overpriced A100s running early PyTorch eager mode code for long.<p>I agree and am eagerly awaiting this on the hardware side and trying to participate in this where I can on the software side.</div><br/></div></div><div id="36530519" class="c"><input type="checkbox" id="c-36530519" checked=""/><div class="controls bullet"><span class="by">KRAKRISMOTT</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36529556">parent</a><span>|</span><a href="#36529616">prev</a><span>|</span><a href="#36530863">next</a><span>|</span><label class="collapse" for="c-36530519">[-]</label><label class="expand" for="c-36530519">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Falcon was trained by a research think tank</i><p>A research think tank subsidized by UAE’s oil wealth. Somebody get Masayoshi on the line, the Saudis must be hungry for deals for their vision fund.</div><br/><div id="36532184" class="c"><input type="checkbox" id="c-36532184" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36530519">parent</a><span>|</span><a href="#36531458">next</a><span>|</span><label class="collapse" for="c-36532184">[-]</label><label class="expand" for="c-36532184">[1 more]</label></div><br/><div class="children"><div class="content">Schmidhuber, the father of deep learning, is already on it: <a href="https:&#x2F;&#x2F;cemse.kaust.edu.sa&#x2F;people&#x2F;person&#x2F;jurgen-schmidhuber" rel="nofollow noreferrer">https:&#x2F;&#x2F;cemse.kaust.edu.sa&#x2F;people&#x2F;person&#x2F;jurgen-schmidhuber</a> .</div><br/></div></div><div id="36531458" class="c"><input type="checkbox" id="c-36531458" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36530519">parent</a><span>|</span><a href="#36532184">prev</a><span>|</span><a href="#36530863">next</a><span>|</span><label class="collapse" for="c-36531458">[-]</label><label class="expand" for="c-36531458">[2 more]</label></div><br/><div class="children"><div class="content">Unnecessary ad hominem. Every research tank is subsidized by <i>someone</i>.</div><br/><div id="36531891" class="c"><input type="checkbox" id="c-36531891" checked=""/><div class="controls bullet"><span class="by">KRAKRISMOTT</span><span>|</span><a href="#36529337">root</a><span>|</span><a href="#36531458">parent</a><span>|</span><a href="#36530863">next</a><span>|</span><label class="collapse" for="c-36531891">[-]</label><label class="expand" for="c-36531891">[1 more]</label></div><br/><div class="children"><div class="content">Very true, MIT should disclose their military affiliations too when publishing. Otherwise, they are just dishonest</div><br/></div></div></div></div></div></div></div></div><div id="36530863" class="c"><input type="checkbox" id="c-36530863" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#36529337">parent</a><span>|</span><a href="#36529556">prev</a><span>|</span><a href="#36528478">next</a><span>|</span><label class="collapse" for="c-36530863">[-]</label><label class="expand" for="c-36530863">[1 more]</label></div><br/><div class="children"><div class="content">The phi-1 paper suggests small models trained on higher quality data hold promise as well. If confirmed that would be great news for the potential accesibility of training open source models.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.11644" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.11644</a></div><br/></div></div></div></div><div id="36528478" class="c"><input type="checkbox" id="c-36528478" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36529337">prev</a><span>|</span><a href="#36529640">next</a><span>|</span><label class="collapse" for="c-36528478">[-]</label><label class="expand" for="c-36528478">[5 more]</label></div><br/><div class="children"><div class="content">When it rains, it pours.<p>We have the SuperHOT LORA and the associated paper, the Salesforce model, MPT, and this. Any other long context news I am missing?</div><br/><div id="36531063" class="c"><input type="checkbox" id="c-36531063" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36528478">parent</a><span>|</span><a href="#36529187">next</a><span>|</span><label class="collapse" for="c-36531063">[-]</label><label class="expand" for="c-36531063">[1 more]</label></div><br/><div class="children"><div class="content">SuperHOT: <a href="https:&#x2F;&#x2F;kaiokendev.github.io&#x2F;til" rel="nofollow noreferrer">https:&#x2F;&#x2F;kaiokendev.github.io&#x2F;til</a><p>LoRA: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35288015">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35288015</a><p>The Salesforce Model (XGen): <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36514936">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36514936</a><p>MPT (Mosaic): <a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-30b" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-30b</a></div><br/></div></div><div id="36529187" class="c"><input type="checkbox" id="c-36529187" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#36528478">parent</a><span>|</span><a href="#36531063">prev</a><span>|</span><a href="#36529640">next</a><span>|</span><label class="collapse" for="c-36529187">[-]</label><label class="expand" for="c-36529187">[3 more]</label></div><br/><div class="children"><div class="content">Yup, someone supposedly improved superhot scaling already.<p>Here you go, <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;14lz7j5&#x2F;ntkaware_scaled_rope_allows_llama_models_to_have&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;14lz7j5&#x2F;ntkawar...</a></div><br/><div id="36531669" class="c"><input type="checkbox" id="c-36531669" checked=""/><div class="controls bullet"><span class="by">ijk</span><span>|</span><a href="#36528478">root</a><span>|</span><a href="#36529187">parent</a><span>|</span><a href="#36529288">next</a><span>|</span><label class="collapse" for="c-36531669">[-]</label><label class="expand" for="c-36531669">[1 more]</label></div><br/><div class="children"><div class="content">Turns out that <i>dynamic</i> NTK-Aware scaling further improves on that, with perplexity performance equal to or better than non-scaled at all context lengths: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;14mrgpr&#x2F;dynamically_scaled_rope_further_increases&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;14mrgpr&#x2F;dynamic...</a></div><br/></div></div><div id="36529288" class="c"><input type="checkbox" id="c-36529288" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36528478">root</a><span>|</span><a href="#36529187">parent</a><span>|</span><a href="#36531669">prev</a><span>|</span><a href="#36529640">next</a><span>|</span><label class="collapse" for="c-36529288">[-]</label><label class="expand" for="c-36529288">[1 more]</label></div><br/><div class="children"><div class="content">Ah yeah I confused rope with SuperHOT.</div><br/></div></div></div></div></div></div><div id="36529640" class="c"><input type="checkbox" id="c-36529640" checked=""/><div class="controls bullet"><span class="by">gtirloni</span><span>|</span><a href="#36528478">prev</a><span>|</span><a href="#36528936">next</a><span>|</span><label class="collapse" for="c-36529640">[-]</label><label class="expand" for="c-36529640">[1 more]</label></div><br/><div class="children"><div class="content">Why was the title editorialized?</div><br/></div></div><div id="36528936" class="c"><input type="checkbox" id="c-36528936" checked=""/><div class="controls bullet"><span class="by">jrm4</span><span>|</span><a href="#36529640">prev</a><span>|</span><a href="#36530516">next</a><span>|</span><label class="collapse" for="c-36528936">[-]</label><label class="expand" for="c-36528936">[2 more]</label></div><br/><div class="children"><div class="content">On one hand, I confess that I&#x27;m not familiar with exactly how the tech works, but this absolutely sounds like cope.  I&#x27;ve literally never ever ever ever seen open source get beat by &quot;proprietary&quot; in this regard. We shall see.</div><br/><div id="36529060" class="c"><input type="checkbox" id="c-36529060" checked=""/><div class="controls bullet"><span class="by">anamexis</span><span>|</span><a href="#36528936">parent</a><span>|</span><a href="#36530516">next</a><span>|</span><label class="collapse" for="c-36529060">[-]</label><label class="expand" for="c-36529060">[1 more]</label></div><br/><div class="children"><div class="content">In what regard?</div><br/></div></div></div></div><div id="36530516" class="c"><input type="checkbox" id="c-36530516" checked=""/><div class="controls bullet"><span class="by">shubhamgrg04</span><span>|</span><a href="#36528936">prev</a><span>|</span><a href="#36529379">next</a><span>|</span><label class="collapse" for="c-36530516">[-]</label><label class="expand" for="c-36530516">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t condensing rotary embeddings just a clever hack to circumvent the original training limitations? Is it really a sustainable solution for extending context length in language models?</div><br/><div id="36531030" class="c"><input type="checkbox" id="c-36531030" checked=""/><div class="controls bullet"><span class="by">kaiokendev</span><span>|</span><a href="#36530516">parent</a><span>|</span><a href="#36529379">next</a><span>|</span><label class="collapse" for="c-36531030">[-]</label><label class="expand" for="c-36531030">[1 more]</label></div><br/><div class="children"><div class="content">I am not sure how to parse this. Can you clarify?</div><br/></div></div></div></div><div id="36529379" class="c"><input type="checkbox" id="c-36529379" checked=""/><div class="controls bullet"><span class="by">ReactForAll</span><span>|</span><a href="#36530516">prev</a><span>|</span><a href="#36528976">next</a><span>|</span><label class="collapse" for="c-36529379">[-]</label><label class="expand" for="c-36529379">[1 more]</label></div><br/><div class="children"><div class="content">LOL it was released like a few days ago, lets give it sometime maybe, but yes it&#x27;s great to raise that these things should be tested better.</div><br/></div></div><div id="36528976" class="c"><input type="checkbox" id="c-36528976" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#36529379">prev</a><span>|</span><a href="#36529144">next</a><span>|</span><label class="collapse" for="c-36528976">[-]</label><label class="expand" for="c-36528976">[1 more]</label></div><br/><div class="children"><div class="content">Why don&#x27;t they evaluate GPT-4, like they evaluated GPT-3.5? And why not try longer context windows?</div><br/></div></div><div id="36529144" class="c"><input type="checkbox" id="c-36529144" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#36528976">prev</a><span>|</span><label class="collapse" for="c-36529144">[-]</label><label class="expand" for="c-36529144">[16 more]</label></div><br/><div class="children"><div class="content">Is the tl;dr: without sufficient context length, the conversation gets confusing, because the AI can&#x27;t keep track of the full context of the conversation?<p>It&#x27;s like being in conversation with an Alzheimer&#x27;s patient, who keeps forgetting the last sentences?<p>So, open source models don&#x27;t do well, while closed source models do?<p>And, this post documents a way to better measure that important metric?</div><br/><div id="36529320" class="c"><input type="checkbox" id="c-36529320" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36529144">parent</a><span>|</span><label class="collapse" for="c-36529320">[-]</label><label class="expand" for="c-36529320">[15 more]</label></div><br/><div class="children"><div class="content">LLM&#x27;s have <i>no</i> short term memory. Their internal state never changes.<p>When you have a conversation with one, you are actually feeding it the entire conversation every time you reply, usually with specific formatting the model is trained to &quot;recognize&quot; as a ongoing conversation, like its continuing a long story. There are some other formatting details, like the initial &quot;instruction&quot; always being preserved at the top as the old conversation gets truncated, and there is some caching done so it doesn&#x27;t have to encode the entire conversation every time.<p>The context size is how much conversation you can feed them at once.<p>The popular open models were essentially limited to ~2K tokens until recently.</div><br/><div id="36531771" class="c"><input type="checkbox" id="c-36531771" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36529320">parent</a><span>|</span><a href="#36530017">next</a><span>|</span><label class="collapse" for="c-36531771">[-]</label><label class="expand" for="c-36531771">[1 more]</label></div><br/><div class="children"><div class="content">Why truncate the conversation? Why not have the LLM generate a summary, and use that instead of a truncated conversation?<p>That seems closer to what we do, we recall the important bits without every single word said.</div><br/></div></div><div id="36530017" class="c"><input type="checkbox" id="c-36530017" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36529320">parent</a><span>|</span><a href="#36531771">prev</a><span>|</span><a href="#36529770">next</a><span>|</span><label class="collapse" for="c-36530017">[-]</label><label class="expand" for="c-36530017">[7 more]</label></div><br/><div class="children"><div class="content">&gt; LLM&#x27;s have no short term memory. Their internal state never changes.<p>Many have interpreted recent advances in LLM as a sign that strong AI is just around the corner. Yet, their static nature is an obvious blocker to ever achieving that – humans, even animals, learn continuously, unlike LLMs where training and runtime are two completely separate processes.<p>Which makes me wonder – how difficult would it be to have something like an LLM which continuously learnt just like humans do? Could it be done by modifying existing LLM architectures such as Transformers, or would it require a completely different architecture?</div><br/><div id="36532019" class="c"><input type="checkbox" id="c-36532019" checked=""/><div class="controls bullet"><span class="by">sirk390</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36530017">parent</a><span>|</span><a href="#36530196">next</a><span>|</span><label class="collapse" for="c-36532019">[-]</label><label class="expand" for="c-36532019">[2 more]</label></div><br/><div class="children"><div class="content">In humans, sleep is also required fo learning, so it&#x27;s not fully continuous. An AI that occasionnaly retrains using the new knowledge would still be very interesting.</div><br/><div id="36532093" class="c"><input type="checkbox" id="c-36532093" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36532019">parent</a><span>|</span><a href="#36530196">next</a><span>|</span><label class="collapse" for="c-36532093">[-]</label><label class="expand" for="c-36532093">[1 more]</label></div><br/><div class="children"><div class="content">From what I understand, there are two different processes – a continuous process which runs while we are awake, building new connections between neurons (acquiring new &quot;weights&quot;); and a pruning process which runs while we are sleep, sorting through those new connections and deciding which to keep and which to discard. That&#x27;s very different from an AI that occasionally retrains.</div><br/></div></div></div></div><div id="36530196" class="c"><input type="checkbox" id="c-36530196" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36530017">parent</a><span>|</span><a href="#36532019">prev</a><span>|</span><a href="#36529770">next</a><span>|</span><label class="collapse" for="c-36530196">[-]</label><label class="expand" for="c-36530196">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s actually really easy. Recurrent neural networks have been shown to be totally capable of handling language modeling. For example, rwkv has pretty good recall on par with human short term memory (in my opinion). The issue is training. If you train rwkv like an Rnn it&#x27;s really expensive and time consuming. If you train it like a gpt.. then it&#x27;s tractable<p>The math works the same either way. Keep in mind... Llms learn orders of magnitude faster than humans due to parallel nature of transformers.</div><br/><div id="36530333" class="c"><input type="checkbox" id="c-36530333" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36530196">parent</a><span>|</span><a href="#36529770">next</a><span>|</span><label class="collapse" for="c-36530333">[-]</label><label class="expand" for="c-36530333">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s it though – in recurrent neural networks, there is still a distinction between internal state (dynamic) and weights (static).<p>Whereas, in biological brains, the weights are updated continuously.</div><br/><div id="36532058" class="c"><input type="checkbox" id="c-36532058" checked=""/><div class="controls bullet"><span class="by">sirk390</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36530333">parent</a><span>|</span><a href="#36529770">next</a><span>|</span><label class="collapse" for="c-36532058">[-]</label><label class="expand" for="c-36532058">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Whereas, in biological brains, the weights are updated continuously.
My personal impression is that many &quot;weights&quot; are updated during sleep time. For example when training juggling, I will make no progress at all for hours of training. But later, after a night of sleep, I will have a large and instant progress.</div><br/><div id="36532180" class="c"><input type="checkbox" id="c-36532180" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36532058">parent</a><span>|</span><a href="#36529770">next</a><span>|</span><label class="collapse" for="c-36532180">[-]</label><label class="expand" for="c-36532180">[1 more]</label></div><br/><div class="children"><div class="content">If I learn something new in the morning, very often I still remember it in the afternoon, even though I haven&#x27;t been to sleep yet.<p>Neuroscientists&#x2F;psychologists&#x2F;etc believe [0] humans have four tiers of memory: sensory memory (stores what you are experiencing <i>right now</i>, lasts for less than a second); working memory (lasts up to 30 seconds); intermediate-term memory (lasts 2-3 hours); long-term memory (anything from 30 minutes ago until the end of your life).<p>We don&#x27;t need to sleep to form new long-term memories – if at dinner time you can still remember what you ate for breakfast (I usually can if I think about it), that&#x27;s your long-term memory at work. What we need sleep for, is pruning our long-term memory – each night the brain basically runs a compression process, deciding which long-term memories to keep and which to throw away (forget), and how much detail to keep for each memory.<p>Regarding your juggling example – most neuroscientists believe that the brain stores different types of memories differently. How to perform a task is a procedural memory, and new or improved motor skills such as juggling are a particular form of procedural memory. How the brain processes them is likely quite different from how it processes episodic memories (events of your life) or semantic memories (facts, general knowledge, etc). Sleep may play a somewhat different role for each different memory type, so what&#x27;s true for learning juggling may not be true for learning facts.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intermediate-term_memory" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intermediate-term_memory</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="36529770" class="c"><input type="checkbox" id="c-36529770" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36529320">parent</a><span>|</span><a href="#36530017">prev</a><span>|</span><a href="#36529885">next</a><span>|</span><label class="collapse" for="c-36529770">[-]</label><label class="expand" for="c-36529770">[1 more]</label></div><br/><div class="children"><div class="content">Yes the entire conversation is fed every time. But also the web interfaces for example on ChatGPT or Bing do this automatically as part of their implementation detail, so the one above you is exactly right.</div><br/></div></div><div id="36529885" class="c"><input type="checkbox" id="c-36529885" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36529320">parent</a><span>|</span><a href="#36529770">prev</a><span>|</span><label class="collapse" for="c-36529885">[-]</label><label class="expand" for="c-36529885">[5 more]</label></div><br/><div class="children"><div class="content">We already know how to do &quot;short term memory&quot;. We can mathematically condense&#x2F;average embedding representations of text and use these as input (i.e. combine a page of text into the representation space of one or several tokens, textual inversion style). Once embeddings get good enough, this kind of memory will be ubiquitous. Not sure why we are collectively having amnesia about sentence transformers and related ideas<p>It&#x27;s worth noting that &quot;soft-prompts&quot; were a thing that the coomer crowd was already using with openassistant. Not sure why the mainstream NLP crowd is being so damn slow in implementing stuff like this<p>NLP is shocking undertooled, and I wrote a whole gist complaining about this that got to the front page of HN awhile ago: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865ca4bb328eb58faf" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...</a></div><br/><div id="36531765" class="c"><input type="checkbox" id="c-36531765" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36529885">parent</a><span>|</span><a href="#36530051">next</a><span>|</span><label class="collapse" for="c-36531765">[-]</label><label class="expand" for="c-36531765">[1 more]</label></div><br/><div class="children"><div class="content">I occasionally remark or ask, how is it that LLM APIs are always either text -&gt; text, or at best (for smaller models) text -&gt; embeddings, with the idea being of you stuffing the latter in some database for future lookups - where the most interesting thing to expose should be <i>embeddings -&gt; text</i>.<p>As in, I want to be able to feed the LLM arbitrary embedding vectors - whether they come from averaging embeddings of different prompts, some general optimization pattern (textual inversion style, indeed), or just iteratively shifting the input vector in some direction, walking down the latent space and generating output every step, just to see what happens.<p>I ask that, and I get blank stares, or non-answers like &quot;oh, it&#x27;s always been available, you can e.g. put text in, get embeddings out&quot;. Yeah, but I want to do the reverse. It&#x27;s like I was making so big a category error, asking such nonsensical question, that no one can even parse it. But am I?</div><br/></div></div><div id="36530051" class="c"><input type="checkbox" id="c-36530051" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36529885">parent</a><span>|</span><a href="#36531765">prev</a><span>|</span><label class="collapse" for="c-36530051">[-]</label><label class="expand" for="c-36530051">[3 more]</label></div><br/><div class="children"><div class="content">Soft prompts and textual inversion, as I understand them, required considerable hardware resources and were effectively made obsolete by low precision LORA training.<p>But the text token merging people do for negative prompts is fast iirc.<p>Not that you are wrong. LLM prompting does seem quite primitive compared to zoo of encoding&#x2F;prompting methods we have for SD... And some of it is directly applicable to LLMs. I never even thought about that before.</div><br/><div id="36531697" class="c"><input type="checkbox" id="c-36531697" checked=""/><div class="controls bullet"><span class="by">ijk</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36530051">parent</a><span>|</span><a href="#36531471">next</a><span>|</span><label class="collapse" for="c-36531697">[-]</label><label class="expand" for="c-36531697">[1 more]</label></div><br/><div class="children"><div class="content">Soft prompts for language models are still viable, but the most accessible version of open source soft prompt training was a TPU-based implementation that depended on features of Google Colab that were no longer available.<p>You can modify it to run locally on a GPU, though it is a bit slow to train. It provides different advantages when compared to a LoRA.</div><br/></div></div><div id="36531471" class="c"><input type="checkbox" id="c-36531471" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36529144">root</a><span>|</span><a href="#36530051">parent</a><span>|</span><a href="#36531697">prev</a><span>|</span><label class="collapse" for="c-36531471">[-]</label><label class="expand" for="c-36531471">[1 more]</label></div><br/><div class="children"><div class="content">All of it is directly applicable to LLMs.<p>Textual Inversion&#x2F;Soft Prompts are not made &quot;obsolete&quot; at all by Loras. Textual Inversion is far faster to train, extremely effective on newer models like SDXL which have better clip encoding, and has neat properties of not messing with the general weights of the rest of the model, which is sometimes desirable.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>