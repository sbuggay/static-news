<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688634067077" as="style"/><link rel="stylesheet" href="styles.css?v=1688634067077"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/InternLM/InternLM">InternLM – new open source 7B LLM</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>freediver</span> | <span>18 comments</span></div><br/><div><div id="36612791" class="c"><input type="checkbox" id="c-36612791" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#36612627">next</a><span>|</span><label class="collapse" for="c-36612791">[-]</label><label class="expand" for="c-36612791">[1 more]</label></div><br/><div class="children"><div class="content">Note that this is apparently a 7B version of a 104B model trained with the intention of competing with OpenAI offerings on the Chinese market [1]. There is a number of those projects: Baichuan, ChatGLM2, InternLM and some more iirc, and they all have small-scale opensource versions.<p>For what it&#x27;s worth, I&#x27;ve tried out ChatGLM2-6B and Baichuan converted to LLaMA (the architecture is literally identical in that case). They&#x27;re okay, though underwhelming given their reported benchmarks; probably the main point of creating them is gaining experience for engineers, and feedback from the wider community that has less incentive to downplay their shortcomings.<p>Surprisingly, they do not appear censored in any particularly &quot;Chinese&quot; political direction, but they share sensibilities of ChatGPT and Claude.<p>1. <a href="https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;InternLM-techreport">https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;InternLM-techreport</a></div><br/></div></div><div id="36612627" class="c"><input type="checkbox" id="c-36612627" checked=""/><div class="controls bullet"><span class="by">tyfon</span><span>|</span><a href="#36612791">prev</a><span>|</span><a href="#36612904">next</a><span>|</span><label class="collapse" for="c-36612627">[-]</label><label class="expand" for="c-36612627">[6 more]</label></div><br/><div class="children"><div class="content">&gt; trust_remote_code=True<p>This is a hard no from me, anyone know why this is so common in models from China? I&#x27;m not getting into conspiracies or anything here, but I&#x27;ve seen it in quite a few others from there.<p>I wouldn&#x27;t run a model with this requirement from anyone else for that matter.</div><br/><div id="36612959" class="c"><input type="checkbox" id="c-36612959" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#36612627">parent</a><span>|</span><a href="#36612703">next</a><span>|</span><label class="collapse" for="c-36612959">[-]</label><label class="expand" for="c-36612959">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because the model architecture hasn&#x27;t been added to huggingface&#x2F;transformers yet, because it literally was just published today.<p><pre><code>    &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModel
    &gt;&gt;&gt; model = AutoModel.from_pretrained(&quot;internlm&#x2F;internlm-chat-7b&quot;, trust_remote_code=True, device=&#x27;cuda&#x27;)
</code></pre>
Here, the &quot;trust_remote_code=True&quot; means &quot;download the model code from huggingface repo &#x27;internlm&#x2F;internlm-chat-7b&#x27;&quot;, along with the weight, and run it. If it&#x27;s False, the library would use builtin model architectures hardcoded in huggingface&#x2F;transformers and only download the weight.<p>The scary flag is here because, of course, newcomers may not realize that model == code and if you load arbitrary model you are likely executing arbitrary code.<p>Wonder why, for example, you don&#x27;t remember seeing LLaMA had this on release day? Because they don&#x27;t use huggingface transformers library and don&#x27;t use huggingface to distribute their model. You just clone and run their code from GitHub, and... how is this not &quot;trust_remote_code&quot;?</div><br/></div></div><div id="36612703" class="c"><input type="checkbox" id="c-36612703" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#36612627">parent</a><span>|</span><a href="#36612959">prev</a><span>|</span><a href="#36612735">next</a><span>|</span><label class="collapse" for="c-36612703">[-]</label><label class="expand" for="c-36612703">[1 more]</label></div><br/><div class="children"><div class="content">I believe it&#x27;s because the model architecture isn&#x27;t added to Huggingface transformer library, so it needs to eval some python code (i.e. load a pickle) to create the PyTorch model. Have not noticed it to be specific to models from China, almost all lesser known models have to do this.</div><br/></div></div><div id="36612735" class="c"><input type="checkbox" id="c-36612735" checked=""/><div class="controls bullet"><span class="by">jabbany</span><span>|</span><a href="#36612627">parent</a><span>|</span><a href="#36612703">prev</a><span>|</span><a href="#36612689">next</a><span>|</span><label class="collapse" for="c-36612735">[-]</label><label class="expand" for="c-36612735">[2 more]</label></div><br/><div class="children"><div class="content">Seems pretty common though, for defining custom architecture configs whatnot?<p>AFAIK the &quot;remote code&quot; is still openly hosted on huggingface so you can audit it if you like. Seems no more dangerous than things like `pip install some_random_library`?</div><br/><div id="36613019" class="c"><input type="checkbox" id="c-36613019" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#36612627">root</a><span>|</span><a href="#36612735">parent</a><span>|</span><a href="#36612689">next</a><span>|</span><label class="collapse" for="c-36613019">[-]</label><label class="expand" for="c-36613019">[1 more]</label></div><br/><div class="children"><div class="content">I like this pip metaphor. If we had required `--trust-remote-code` for every `npm install` we could have avoided left-pad and most of the software supply chain drama in the past years.</div><br/></div></div></div></div><div id="36612689" class="c"><input type="checkbox" id="c-36612689" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36612627">parent</a><span>|</span><a href="#36612735">prev</a><span>|</span><a href="#36612904">next</a><span>|</span><label class="collapse" for="c-36612689">[-]</label><label class="expand" for="c-36612689">[1 more]</label></div><br/><div class="children"><div class="content">Mind pasting the link to that line? Am on mobile and can’t find it myself easily.</div><br/></div></div></div></div><div id="36612904" class="c"><input type="checkbox" id="c-36612904" checked=""/><div class="controls bullet"><span class="by">exizt88</span><span>|</span><a href="#36612627">prev</a><span>|</span><a href="#36612692">next</a><span>|</span><label class="collapse" for="c-36612904">[-]</label><label class="expand" for="c-36612904">[4 more]</label></div><br/><div class="children"><div class="content">&gt; The code in this repository is open-source under the Apache-2.0 license. The InternLM weights are fully open for academic research and also allow commercial use with written permission from the official team. For inquiries about commercial licenses and collaborations, please contact internlm@pjlab.org.cn.<p>This makes me much less excited about this model.</div><br/><div id="36613038" class="c"><input type="checkbox" id="c-36613038" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#36612904">parent</a><span>|</span><a href="#36613060">next</a><span>|</span><label class="collapse" for="c-36613038">[-]</label><label class="expand" for="c-36613038">[1 more]</label></div><br/><div class="children"><div class="content">Is that even valid? This seems to be the only place where they’ve made this exemption, it’s not written in the license. Even the weights on hugging face are licensed under apache 2.0.<p>Doesn’t apache 2.0 allow for fairly unrestricted commercial use? Isn’t that the whole point of using that license?</div><br/></div></div><div id="36613060" class="c"><input type="checkbox" id="c-36613060" checked=""/><div class="controls bullet"><span class="by">ohgodplsno</span><span>|</span><a href="#36612904">parent</a><span>|</span><a href="#36613038">prev</a><span>|</span><a href="#36612692">next</a><span>|</span><label class="collapse" for="c-36613060">[-]</label><label class="expand" for="c-36613060">[2 more]</label></div><br/><div class="children"><div class="content">Less excited that you can&#x27;t freely take work from academics to resell it in a shitty SaaS company ?</div><br/><div id="36613076" class="c"><input type="checkbox" id="c-36613076" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#36612904">root</a><span>|</span><a href="#36613060">parent</a><span>|</span><a href="#36612692">next</a><span>|</span><label class="collapse" for="c-36613076">[-]</label><label class="expand" for="c-36613076">[1 more]</label></div><br/><div class="children"><div class="content">In Europe we call that a success story.</div><br/></div></div></div></div></div></div><div id="36612692" class="c"><input type="checkbox" id="c-36612692" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#36612904">prev</a><span>|</span><a href="#36612897">next</a><span>|</span><label class="collapse" for="c-36612692">[-]</label><label class="expand" for="c-36612692">[1 more]</label></div><br/><div class="children"><div class="content">A related question -- when fine tuning a model like this to a specific corpus, how does the fine tuning effect the actual chat capability, since the chat model weights seem to come as a separate model? Does one fine tune the LLM+Chat model directly? If so, does that not require some kind of prompt based training as opposed to just lookahead prediction? Does one have to fine tune the LLM and then repeat whatever they do to get the LLM+Chat model?</div><br/></div></div><div id="36612897" class="c"><input type="checkbox" id="c-36612897" checked=""/><div class="controls bullet"><span class="by">freediver</span><span>|</span><a href="#36612692">prev</a><span>|</span><a href="#36612677">next</a><span>|</span><label class="collapse" for="c-36612897">[-]</label><label class="expand" for="c-36612897">[1 more]</label></div><br/><div class="children"><div class="content">Tried it with a few PyLLMs benchmarks [1] and it did not do very well.<p>Q. Use m to substitute p, a to substitute e, n to substitute a, g to substitute c, o to substitute h, how to spell peach under this rule?<p>A.<p>m: p<p>a: e<p>n: a<p>g: c<p>o: h<p>how: peach<p>Q. write three sentences each ending with the word apple<p>A.<p>1. The apple tree was covered in apples.<p>2. She sliced the apple with a knife.<p>3. The apple juice was too sweet.<p>Wonder if the synthetic benchmarks are very biased and not that useful.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;kagisearch&#x2F;pyllms&#x2F;blob&#x2F;6c11cd51b099d33e61d209712f0fc660838c3e5f&#x2F;llms&#x2F;llms.py#L157">https:&#x2F;&#x2F;github.com&#x2F;kagisearch&#x2F;pyllms&#x2F;blob&#x2F;6c11cd51b099d33e61...</a></div><br/></div></div><div id="36612677" class="c"><input type="checkbox" id="c-36612677" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36612897">prev</a><span>|</span><a href="#36612602">next</a><span>|</span><label class="collapse" for="c-36612677">[-]</label><label class="expand" for="c-36612677">[1 more]</label></div><br/><div class="children"><div class="content">I welcome new models! The more, the merrier.<p>That said, this model has been tailored but they are comparing it to non-finetuned LLaMA-7B in their benchmark? That seems a bit fainthearted.</div><br/></div></div><div id="36612602" class="c"><input type="checkbox" id="c-36612602" checked=""/><div class="controls bullet"><span class="by">gnrlst</span><span>|</span><a href="#36612677">prev</a><span>|</span><a href="#36612565">next</a><span>|</span><label class="collapse" for="c-36612602">[-]</label><label class="expand" for="c-36612602">[2 more]</label></div><br/><div class="children"><div class="content">Is this also censored&#x2F;nerfed? I&#x27;d love to play with a &quot;raw&quot; unnerfed model to fully grasp what an LLM can do (and see how biased it is). Does anyone have any recommendations for unnerfed models to try out?</div><br/><div id="36612648" class="c"><input type="checkbox" id="c-36612648" checked=""/><div class="controls bullet"><span class="by">lioeters</span><span>|</span><a href="#36612602">parent</a><span>|</span><a href="#36612565">next</a><span>|</span><label class="collapse" for="c-36612648">[-]</label><label class="expand" for="c-36612648">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?search=uncensored&amp;sort=trending" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;models?search=uncensored&amp;sort=trendin...</a></div><br/></div></div></div></div><div id="36612565" class="c"><input type="checkbox" id="c-36612565" checked=""/><div class="controls bullet"><span class="by">kasia_wieczorek</span><span>|</span><a href="#36612602">prev</a><span>|</span><label class="collapse" for="c-36612565">[-]</label><label class="expand" for="c-36612565">[1 more]</label></div><br/><div class="children"><div class="content">Great name for a simple LLM hehe</div><br/></div></div></div></div></div></div></div></body></html>