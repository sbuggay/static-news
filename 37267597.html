<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693040458787" as="style"/><link rel="stylesheet" href="styles.css?v=1693040458787"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.phind.com/blog/code-llama-beats-gpt4">Beating GPT-4 on HumanEval with a fine-tuned CodeLlama-34B</a> <span class="domain">(<a href="https://www.phind.com">www.phind.com</a>)</span></div><div class="subtext"><span>rushingcreek</span> | <span>153 comments</span></div><br/><div><div id="37270942" class="c"><input type="checkbox" id="c-37270942" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#37268394">next</a><span>|</span><label class="collapse" for="c-37270942">[-]</label><label class="expand" for="c-37270942">[1 more]</label></div><br/><div class="children"><div class="content">For anyone who is using ollama to run these models, how do I give it prompts with newlines?</div><br/></div></div><div id="37268394" class="c"><input type="checkbox" id="c-37268394" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37270942">prev</a><span>|</span><a href="#37270854">next</a><span>|</span><label class="collapse" for="c-37268394">[-]</label><label class="expand" for="c-37268394">[16 more]</label></div><br/><div class="children"><div class="content">I used the original 34B last night with 4 bit through accelerate and I was absolutely blown away. I got goosebumps because it finally felt like we have models we can run on consumer hardware (single 3090 in this case) and did not feel like a toy. I purposely broke some functions, it fixed it. I asked some complex questions and it answered it well. I’m excited for what’s to come. I wish there was a Phind instruct model for me to play with. Text completion hasn’t really been all that useful for my use case.</div><br/><div id="37268481" class="c"><input type="checkbox" id="c-37268481" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37268394">parent</a><span>|</span><a href="#37269645">next</a><span>|</span><label class="collapse" for="c-37268481">[-]</label><label class="expand" for="c-37268481">[1 more]</label></div><br/><div class="children"><div class="content">Our models should handle instructions reasonably well. We&#x27;re working on setting up a hosted Huggingface space to make it easier to play with them and we&#x27;ll also set up a hosted &quot;Phind chat&quot; mode for these models.</div><br/></div></div><div id="37269645" class="c"><input type="checkbox" id="c-37269645" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37268394">parent</a><span>|</span><a href="#37268481">prev</a><span>|</span><a href="#37268534">next</a><span>|</span><label class="collapse" for="c-37269645">[-]</label><label class="expand" for="c-37269645">[6 more]</label></div><br/><div class="children"><div class="content">It really is good. Surprisingly it seems to answer instruct-like prompts well! I’ve been using it with Ollama (<a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a>) with prompts like:<p><pre><code>  ollama run phind-codellama &quot;write c code to reverse a linked list&quot;
</code></pre>
To run this on an m1 Mac or similar machine, you&#x27;ll need around 32GB of memory for the 4-bit quantized version since it&#x27;s a 34B parameter model and is quite big (20GB).</div><br/><div id="37270532" class="c"><input type="checkbox" id="c-37270532" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37269645">parent</a><span>|</span><a href="#37270069">next</a><span>|</span><label class="collapse" for="c-37270532">[-]</label><label class="expand" for="c-37270532">[3 more]</label></div><br/><div class="children"><div class="content">How do you guys get it to run?<p>I tried with ollama, installed from Homebrew, on my M1 Max with 64GB RAM.<p>I downloaded the phind-codellama model using<p><pre><code>  ollama pull phind-codellama
</code></pre>
But when I give it a prompt, for example<p><pre><code>  ollama run phind-codellama &quot;write a production grade implementation of Sieve of Eratosthenes in Rust&quot;
</code></pre>
It prints the following error message<p><pre><code>    Error: Post &quot;http:&#x2F;&#x2F;localhost:11434&#x2F;api&#x2F;generate&quot;: EOF
</code></pre>
and exits.<p>Even though it worked to run ollama with some other models.<p>Is the version in homebrew not able to run phind-codellama?</div><br/><div id="37270588" class="c"><input type="checkbox" id="c-37270588" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37270532">parent</a><span>|</span><a href="#37270069">next</a><span>|</span><label class="collapse" for="c-37270588">[-]</label><label class="expand" for="c-37270588">[2 more]</label></div><br/><div class="children"><div class="content">Yes you&#x27;ll need the latest version (0.0.16) to run the 34B model. It should run great on that machine!<p>The download (both as a Mac app and standalone binary) is available here: <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.0.16">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.0.16</a>. And I will work on getting that brew formula updated as well! Sorry to see you hit an error!</div><br/><div id="37270891" class="c"><input type="checkbox" id="c-37270891" checked=""/><div class="controls bullet"><span class="by">JoelJacobson</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37270588">parent</a><span>|</span><a href="#37270069">next</a><span>|</span><label class="collapse" for="c-37270891">[-]</label><label class="expand" for="c-37270891">[1 more]</label></div><br/><div class="children"><div class="content">Is there a limit on the number of lines&#x2F;tokens it can take as input? Can it be increased? The limit seems to be somewhere above 30 lines.<p>I&#x27;m on a MacBook Pro M1 Max 64GB.<p><pre><code>    % ollama --version
    ollama version 0.0.16
    % ollama run phind-codellama &quot;$(cat prompt.txt)&quot;
    ⠧   Error: Post &quot;http:&#x2F;&#x2F;localhost:11434&#x2F;api&#x2F;generate&quot;: EOF
    % wc prompt.txt
         335    1481   11457 prompt.txt</code></pre></div><br/></div></div></div></div></div></div><div id="37270069" class="c"><input type="checkbox" id="c-37270069" checked=""/><div class="controls bullet"><span class="by">praveenhm</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37269645">parent</a><span>|</span><a href="#37270532">prev</a><span>|</span><a href="#37268534">next</a><span>|</span><label class="collapse" for="c-37270069">[-]</label><label class="expand" for="c-37270069">[2 more]</label></div><br/><div class="children"><div class="content">Is reverse a linked list is still a popular question on interview?</div><br/><div id="37270403" class="c"><input type="checkbox" id="c-37270403" checked=""/><div class="controls bullet"><span class="by">ddalex</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37270069">parent</a><span>|</span><a href="#37268534">next</a><span>|</span><label class="collapse" for="c-37270403">[-]</label><label class="expand" for="c-37270403">[1 more]</label></div><br/><div class="children"><div class="content">I ask it as a warmup question, I expect it to be done in 5-10 minutes.<p>Then comes the real question, which is &quot;let&#x27;s write fizz buzz so it generates at above 55Gbytes&#x2F;second&quot;.</div><br/></div></div></div></div></div></div><div id="37268534" class="c"><input type="checkbox" id="c-37268534" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37268394">parent</a><span>|</span><a href="#37269645">prev</a><span>|</span><a href="#37269918">next</a><span>|</span><label class="collapse" for="c-37268534">[-]</label><label class="expand" for="c-37268534">[2 more]</label></div><br/><div class="children"><div class="content">I actually tried the 4 bit quants (Q4_K_M) and was a bit unimpressed. Switching to Q6_K made a huge difference, but it doesn&#x27;t fit on my 3090 so it was very slow. And testing on perplexity&#x27;s website which I presume is fp16 seemed even better, although that might be mostly due to sampler&#x2F;prompt differences.</div><br/><div id="37268743" class="c"><input type="checkbox" id="c-37268743" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37268534">parent</a><span>|</span><a href="#37269918">next</a><span>|</span><label class="collapse" for="c-37268743">[-]</label><label class="expand" for="c-37268743">[1 more]</label></div><br/><div class="children"><div class="content">I have a hunch something is
broken with the GGUF. I had terrible results using llama cpp as well.</div><br/></div></div></div></div><div id="37269918" class="c"><input type="checkbox" id="c-37269918" checked=""/><div class="controls bullet"><span class="by">d136o</span><span>|</span><a href="#37268394">parent</a><span>|</span><a href="#37268534">prev</a><span>|</span><a href="#37268807">next</a><span>|</span><label class="collapse" for="c-37269918">[-]</label><label class="expand" for="c-37269918">[1 more]</label></div><br/><div class="children"><div class="content">llama-2-70b-chat (courtesy of llama.cpp on m2) says:<p><pre><code>  Pretend to be a commenter on hackernews. Respond to the comment below:

  [parent comment inlined]

  what is your response?
  &quot;Wow, that&#x27;s great to hear! It sounds like you had a really positive experience with the 34B last night. I&#x27;m also excited to see what&#x27;s in store for Phind and its potential applications. Have you tried using the 34B for any specific tasks or projects yet? And do you think the text completion feature would be useful for your use case if it were improved further?&quot;</code></pre></div><br/></div></div><div id="37268807" class="c"><input type="checkbox" id="c-37268807" checked=""/><div class="controls bullet"><span class="by">gmm1990</span><span>|</span><a href="#37268394">parent</a><span>|</span><a href="#37269918">prev</a><span>|</span><a href="#37270854">next</a><span>|</span><label class="collapse" for="c-37268807">[-]</label><label class="expand" for="c-37268807">[5 more]</label></div><br/><div class="children"><div class="content">What parameters&#x2F;prompts did you use, I was able to get ok results but not comparable to chat gpt or even bard</div><br/><div id="37268828" class="c"><input type="checkbox" id="c-37268828" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37268807">parent</a><span>|</span><a href="#37268922">next</a><span>|</span><label class="collapse" for="c-37268828">[-]</label><label class="expand" for="c-37268828">[1 more]</label></div><br/><div class="children"><div class="content">We used no prompt (only autocomplete) for the HumanEval reproduction: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v1" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v1</a>.<p>But you should be able to get it to do stuff just by telling it what you want. Note that it&#x27;s completion tuned (not chat-tuned), so it should perform better on single-turn conversations.</div><br/></div></div><div id="37268922" class="c"><input type="checkbox" id="c-37268922" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37268807">parent</a><span>|</span><a href="#37268828">prev</a><span>|</span><a href="#37270854">next</a><span>|</span><label class="collapse" for="c-37268922">[-]</label><label class="expand" for="c-37268922">[3 more]</label></div><br/><div class="children"><div class="content">I used textgen, instruct mode with LLaMa 2 template, “simple” parameters. What quantization method are you using?</div><br/><div id="37269296" class="c"><input type="checkbox" id="c-37269296" checked=""/><div class="controls bullet"><span class="by">gmm1990</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37268922">parent</a><span>|</span><a href="#37270854">next</a><span>|</span><label class="collapse" for="c-37269296">[-]</label><label class="expand" for="c-37269296">[2 more]</label></div><br/><div class="children"><div class="content">I used the llama.cpp quantize q_0 option. Seemed to vary widely just adjusting the temperature parameters. Was giving it an example db query saving to file in json and asking create a similar function did multiple queries.</div><br/><div id="37269472" class="c"><input type="checkbox" id="c-37269472" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37268394">root</a><span>|</span><a href="#37269296">parent</a><span>|</span><a href="#37270854">next</a><span>|</span><label class="collapse" for="c-37269472">[-]</label><label class="expand" for="c-37269472">[1 more]</label></div><br/><div class="children"><div class="content">I’m assuming typo and you mean 4? What model size? As mentioned above, I had serious issues making GGUF work well. I’m gonna post an article talking about a better way to set it up.<p>Here an example output (I can get different results every regenerate since the question is pretty vague):<p>Prompt:<p>Give me an example db query saving to file in json.<p>Result:<p>Sure! Here&#x27;s an example of how you can save a database query result to a JSON file in Python using the sqlite3 module and the json module:<p><pre><code>    import sqlite3
    import json

    # Connect to the database
    conn = sqlite3.connect(&quot;example.db&quot;)
    cursor = conn.cursor()

    # Execute a query that returns a single row
    cursor.execute(&quot;SELECT \* FROM mytable WHERE id=123&quot;)
    result = cursor.fetchone()

    # Convert the query result to a dictionary
    data = {key: value for key, value in result}
  
    # Save the data as a JSON object
    with open(&quot;output.json&quot;, &quot;w&quot;) as f:
        json.dump(data, f)
</code></pre>
This code connects to a SQLite database named example.db, executes a query that retrieves a single row from a table called mytable, converts the resulting tuple into a dictionary, and then saves the dictionary as a JSON object to a file named output.json.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37270854" class="c"><input type="checkbox" id="c-37270854" checked=""/><div class="controls bullet"><span class="by">knotty66</span><span>|</span><a href="#37268394">prev</a><span>|</span><a href="#37268146">next</a><span>|</span><label class="collapse" for="c-37270854">[-]</label><label class="expand" for="c-37270854">[1 more]</label></div><br/><div class="children"><div class="content">Great stuff - and training was so cheap too - it would have cost less than $200 on Runpod, and half that price on spot instances.<p>I guess it&#x27;s time languages other than Python, especially niche ones, started collating their own language specific datasets.<p>Personally, I daydream about an Elixir specific LLM I could run locally, trained or fine tuned to respond in an idiomatic fashion, and plug into a tool like Cursor.so.<p>Are there any examples of the internal dataset used in the 80K instruction &#x2F; answer pairs Phind used to tune this?</div><br/></div></div><div id="37268146" class="c"><input type="checkbox" id="c-37268146" checked=""/><div class="controls bullet"><span class="by">agnokapathetic</span><span>|</span><a href="#37270854">prev</a><span>|</span><a href="#37269959">next</a><span>|</span><label class="collapse" for="c-37268146">[-]</label><label class="expand" for="c-37268146">[30 more]</label></div><br/><div class="children"><div class="content">GPT-4 as served in the API has been getting 85% on HumanEval (compared to 69.5% claimed here)<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;amanrsanger&#x2F;status&#x2F;1635751764577361921" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;amanrsanger&#x2F;status&#x2F;1635751764577361921</a>
<a href="https:&#x2F;&#x2F;github.com&#x2F;getcursor&#x2F;eval">https:&#x2F;&#x2F;github.com&#x2F;getcursor&#x2F;eval</a></div><br/><div id="37268181" class="c"><input type="checkbox" id="c-37268181" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37268146">parent</a><span>|</span><a href="#37269959">next</a><span>|</span><label class="collapse" for="c-37268181">[-]</label><label class="expand" for="c-37268181">[29 more]</label></div><br/><div class="children"><div class="content">Right, but there&#x27;s no contamination studies there. I suspect that RLHF data leaked HumanEval into GPT-4.<p>It just seems unlikely to me that GPT-4&#x27;s coding abilities have improved since March (when 67% was officially reported by OpenAI) given all of the examples and anecdotes about degradation.<p>This is why we use the official numbers.</div><br/><div id="37270501" class="c"><input type="checkbox" id="c-37270501" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268181">parent</a><span>|</span><a href="#37270351">next</a><span>|</span><label class="collapse" for="c-37270501">[-]</label><label class="expand" for="c-37270501">[1 more]</label></div><br/><div class="children"><div class="content">But the model in OP is fine-tuned by &quot;a proprietary dataset of ~80k high-quality programming problems and solutions&quot;. How do we know it&#x27;s not contaminated by HumanEval too?</div><br/></div></div><div id="37270351" class="c"><input type="checkbox" id="c-37270351" checked=""/><div class="controls bullet"><span class="by">EvgeniyZh</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268181">parent</a><span>|</span><a href="#37270501">prev</a><span>|</span><a href="#37268373">next</a><span>|</span><label class="collapse" for="c-37270351">[-]</label><label class="expand" for="c-37270351">[1 more]</label></div><br/><div class="children"><div class="content">I have a several arguments why contamination is probably not the main reason of performance difference.<p>When we worked on StarCoder, people ran gpt-4 on MultiPL-E, which doesn&#x27;t have canonical solutions in the internet, and the performance was higher that what you would expect from official numbers<p>Official contamination analysis shows only minor drop in performance even though contamination is fairly high (you may argue that contamination is higher now or that rlhf has stronger effect)<p>There is significant drop in performance when testing on HumanEval+ [1], which shouldn&#x27;t happen if model has canonical solutions.<p>BTW why don&#x27;t you use HumanEval+?<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.01210" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.01210</a></div><br/></div></div><div id="37268373" class="c"><input type="checkbox" id="c-37268373" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268181">parent</a><span>|</span><a href="#37270351">prev</a><span>|</span><a href="#37268480">next</a><span>|</span><label class="collapse" for="c-37268373">[-]</label><label class="expand" for="c-37268373">[15 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; given all of the examples and anecdotes about degradation.<p>How many examples and anecdotes about degradation are actually scientific side-by-side studies? I see absurd articles online about ChatGPT usage going down the drain by kids, completely failing to consider even the most basic fact of seasonality and how school is out for the summer!</div><br/><div id="37270756" class="c"><input type="checkbox" id="c-37270756" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268373">parent</a><span>|</span><a href="#37268523">next</a><span>|</span><label class="collapse" for="c-37270756">[-]</label><label class="expand" for="c-37270756">[1 more]</label></div><br/><div class="children"><div class="content">Quality has degraded but token generation speed has increased. The GPT4 of today isn’t the same as it used to be. To get the old slow model you need to use GPT4-0314, that gives higher quality answers like it used to.</div><br/></div></div><div id="37268523" class="c"><input type="checkbox" id="c-37268523" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268373">parent</a><span>|</span><a href="#37270756">prev</a><span>|</span><a href="#37268402">next</a><span>|</span><label class="collapse" for="c-37268523">[-]</label><label class="expand" for="c-37268523">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m aware of at least one study by Stanford. PDF paper linked in this article:<p><a href="https:&#x2F;&#x2F;www.techopedia.com&#x2F;is-gpt-4-a-flop" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.techopedia.com&#x2F;is-gpt-4-a-flop</a><p>Of course, I&#x27;d like to see more than one study. But this one is by a well known university, and it&#x27;s pretty conclusive. GPT-4 is getting worse (especially for code, maths, and analytical reasoning) and more censored.</div><br/><div id="37269078" class="c"><input type="checkbox" id="c-37269078" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268523">parent</a><span>|</span><a href="#37270318">next</a><span>|</span><label class="collapse" for="c-37269078">[-]</label><label class="expand" for="c-37269078">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s important to frame this correctly.
The article is a bit misguided (it doesn&#x27;t matter which university publishes an article) because there are so many ways in which a model can be altered, even excluding retraining weights.  Also, even if the performance has dropped practically due to removing some resources for more shortcuts to be taken (for example changing beam search and typical sampling parameters), making implications about the outlook for the future is not really appropriate, since retraining weights, changing architecture, etc can improve capabilities immensely.<p>It&#x27;s important not to suggest that GPT systems in general are on the way outside.ply due to some small alterations in parameters that make a system slightly less performant (which seems to be a popular perspective).</div><br/></div></div><div id="37270318" class="c"><input type="checkbox" id="c-37270318" checked=""/><div class="controls bullet"><span class="by">EvgeniyZh</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268523">parent</a><span>|</span><a href="#37269078">prev</a><span>|</span><a href="#37269551">next</a><span>|</span><label class="collapse" for="c-37270318">[-]</label><label class="expand" for="c-37270318">[1 more]</label></div><br/><div class="children"><div class="content">On most of their tests gpt-4 is not actually worse [1]. In particular coding results are affected by changed due to different output format rather than worse abilities [2]. But that&#x27;s ok because the message of the paper is that there is strong drift between versions and developers should be aware of it, not that gpt becomes worse [3].<p>[1] <a href="https:&#x2F;&#x2F;www.aisnakeoil.com&#x2F;p&#x2F;is-gpt-4-getting-worse-over-time" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.aisnakeoil.com&#x2F;p&#x2F;is-gpt-4-getting-worse-over-tim...</a><p>[2] <a href="https:&#x2F;&#x2F;twitter.com&#x2F;Si_Boehm&#x2F;status&#x2F;1681801371656536068" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;Si_Boehm&#x2F;status&#x2F;1681801371656536068</a><p>[3] <a href="https:&#x2F;&#x2F;twitter.com&#x2F;matei_zaharia&#x2F;status&#x2F;1681805357516210177" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;matei_zaharia&#x2F;status&#x2F;1681805357516210177</a></div><br/></div></div><div id="37269551" class="c"><input type="checkbox" id="c-37269551" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268523">parent</a><span>|</span><a href="#37270318">prev</a><span>|</span><a href="#37268402">next</a><span>|</span><label class="collapse" for="c-37269551">[-]</label><label class="expand" for="c-37269551">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this the study that asked a bunch of questions with the same answer (&quot;yes&quot;) and basically the old model always answered &quot;yes&quot; and the new model always answered &quot;no&quot;? That&#x27;s not a degradation in performance. It was never answering the questions in the first place, just guessing. The only thing that changed was the default guess.</div><br/><div id="37269982" class="c"><input type="checkbox" id="c-37269982" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37269551">parent</a><span>|</span><a href="#37268402">next</a><span>|</span><label class="collapse" for="c-37269982">[-]</label><label class="expand" for="c-37269982">[3 more]</label></div><br/><div class="children"><div class="content">No it&#x27;s not. Here&#x27;s a link to the study, you can check the questions they asked.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009.pdf</a></div><br/><div id="37270374" class="c"><input type="checkbox" id="c-37270374" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37269982">parent</a><span>|</span><a href="#37270400">next</a><span>|</span><label class="collapse" for="c-37270374">[-]</label><label class="expand" for="c-37270374">[1 more]</label></div><br/><div class="children"><div class="content">You are wrong. What I described is exactly what they did in the &quot;math&quot; benchmark of v1 of the study (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009v1.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009v1.pdf</a>). They asked &quot;is this number prime&quot; for a bunch of prime numbers. The old version gave a bunch of &quot;reasoning&quot; that was actually faulty and then guessed &quot;yes&quot;. The new version guessed &quot;no&quot; (which is arguably a better guess as to whether a random number is prime). In neither case did it actually do the work required to answer the question and the change in &quot;correct&quot; answers is an illusion.<p>In the programming category the newer GPT-4 actually performed significantly better but started formatting code with backticks that the study&#x27;s evaluation code didn&#x27;t handle properly, so they falsely concluded that it was worse. <a href="https:&#x2F;&#x2F;twitter.com&#x2F;Si_Boehm&#x2F;status&#x2F;1681801371656536068" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;Si_Boehm&#x2F;status&#x2F;1681801371656536068</a><p>They later submitted a revision to the study attempting to correct these blatant issues but trusting their work is clearly a terrible idea. The study was executed very poorly and should be ignored with extreme prejudice.</div><br/></div></div><div id="37270400" class="c"><input type="checkbox" id="c-37270400" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37269982">parent</a><span>|</span><a href="#37270374">prev</a><span>|</span><a href="#37268402">next</a><span>|</span><label class="collapse" for="c-37270400">[-]</label><label class="expand" for="c-37270400">[1 more]</label></div><br/><div class="children"><div class="content">At this point you&#x27;ve gotten like 3 or 4 replies explaining how at best you&#x27;re drawing a flawed conclusion from the paper, and at worst it&#x27;s a flawed paper in itself.<p>Funnily enough, just skimming through it again I found <i>yet another</i> glaring mistake they made: they left the system prompt empty for both checkpoints, yet the headline feature of the new checkpoint was improved steerability <i>via the system prompt</i>: <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;function-calling-and-other-api-updates" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;function-calling-and-other-api-updat...</a><p>Every time I look at this paper my inclination drifts further away from harmless incompetence. Matei Zaharia is the CTO of Databricks, it feels like too perfect of a coincidence that someone who built a career on ML and study would suddenly drop the ball right as their company is trying to pivot to on-premise MLOps, who&#x27;s prime competition is ChatGPT...</div><br/></div></div></div></div></div></div></div></div><div id="37268402" class="c"><input type="checkbox" id="c-37268402" checked=""/><div class="controls bullet"><span class="by">devin</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268373">parent</a><span>|</span><a href="#37268523">prev</a><span>|</span><a href="#37268480">next</a><span>|</span><label class="collapse" for="c-37268402">[-]</label><label class="expand" for="c-37268402">[6 more]</label></div><br/><div class="children"><div class="content">It takes like 2-3 experiences of receiving a confidently wrong answer to downgrade your usage. If you use a refactoring tool to rename and it misses one, you won’t use it again.</div><br/><div id="37268515" class="c"><input type="checkbox" id="c-37268515" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268402">parent</a><span>|</span><a href="#37269710">next</a><span>|</span><label class="collapse" for="c-37268515">[-]</label><label class="expand" for="c-37268515">[3 more]</label></div><br/><div class="children"><div class="content">While that would likely be my experience with a refactoring tool (unless I didn&#x27;t have a better alternative), that&#x27;s not my experience with ChatGPT 4.
And that&#x27;s considering I have very little tolerance for buggy software.<p>There was a period of a few weeks or months in which it seemed like ChatGPT had really degraded to the point of being unusable (although it could have been my biases). However, it seems to be better now (again, my subjective experience).<p>Sometimes I still catch it making really basic mistakes, but most times I can convince it to correct the mistake (especially if I point them out).<p>But what&#x27;s most amazing to me is how ChatGPT is absolutely brilliant at some things, and not just technical or even obscure topics.<p>Recently, it gave me the most amazing idea for navigating a complex and nuanced social situation I was having difficulty with. And given the constraints of the situation, there was no way I could have gotten that idea otherwise, especially in the allotted time.<p>So despite its flaws and mistakes, I still find it to be a tremendously useful tool, even if only to point me in the right direction.</div><br/><div id="37268857" class="c"><input type="checkbox" id="c-37268857" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268515">parent</a><span>|</span><a href="#37269426">next</a><span>|</span><label class="collapse" for="c-37268857">[-]</label><label class="expand" for="c-37268857">[1 more]</label></div><br/><div class="children"><div class="content">Given the fact that OpenAI has constant resources (for any given small span of time) and varying demand (users and query type), it&#x27;s not crazy to think they dynamically adjust to consume all available resources on their side.<p>Obviously the base model would be the same, but aren&#x27;t there are +&#x2F;- flavors they could overlay with extra compute? E.g. multi-pass, additional experts, etc.<p>The benefits to giving someone an occasional &quot;magic&quot; answer are too great not to.<p>Have there been any wide studies on same-prompt-different-times?</div><br/></div></div><div id="37269426" class="c"><input type="checkbox" id="c-37269426" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268515">parent</a><span>|</span><a href="#37268857">prev</a><span>|</span><a href="#37269710">next</a><span>|</span><label class="collapse" for="c-37269426">[-]</label><label class="expand" for="c-37269426">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So despite its flaws and mistakes, I still find it to be a tremendously useful tool, even if only to point me in the right direction.<p>Much of this resonates. That said, I get tremendous value simply by writing things down (or dictating them) and replying to my own question. I would expect that a sizable fraction of people have forgotten about these strategies and&#x2F;or don&#x27;t use them when they are most useful. For many, there is tremendous muscle memory to run a Hooli search almost on mental autopilot. Who has time to slow down and <i>write</i> a well-conceived question? Or perhaps we should turn it around ... On a longer time horizon, who would want to waste time with poorly-conceived questions?<p>It is the question that starts the process. So we should ask good questions. Do we? I&#x27;d be curious about the usage data OpenAI collects. I do my best to lower expectations about people in general, but I&#x27;m confident I&#x27;d still be unprepared for the level of thought put into questions.<p>&gt; But what&#x27;s most amazing to me is how ChatGPT is absolutely brilliant at some things, and not just technical or even obscure topics.<p>I&#x27;m not amazed in the way you are. I expect a variation in quality across topics and domains and question styles.</div><br/></div></div></div></div><div id="37269710" class="c"><input type="checkbox" id="c-37269710" checked=""/><div class="controls bullet"><span class="by">mabster</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268402">parent</a><span>|</span><a href="#37268515">prev</a><span>|</span><a href="#37268587">next</a><span>|</span><label class="collapse" for="c-37269710">[-]</label><label class="expand" for="c-37269710">[1 more]</label></div><br/><div class="children"><div class="content">When I was doing a lot of C++ gamedev, we were definitely doing a lot of stuff that would trip up static analysis, e.g. X-macros.<p>We would still use refactoring tools even though they would often miss stuff. You just rely on a combination of refactoring tool &#x2F; search and replace &#x2F; the compiler.<p>We would also debug our code in release mode with symbols. You get used to a debugging environment where you don&#x27;t trust anything you&#x27;re seeing in variables, etc. too.</div><br/></div></div><div id="37268587" class="c"><input type="checkbox" id="c-37268587" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268402">parent</a><span>|</span><a href="#37269710">prev</a><span>|</span><a href="#37268480">next</a><span>|</span><label class="collapse" for="c-37268587">[-]</label><label class="expand" for="c-37268587">[1 more]</label></div><br/><div class="children"><div class="content">Depends on what you expect it capable of given the limitations of these systems.</div><br/></div></div></div></div></div></div><div id="37268480" class="c"><input type="checkbox" id="c-37268480" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268181">parent</a><span>|</span><a href="#37268373">prev</a><span>|</span><a href="#37269519">next</a><span>|</span><label class="collapse" for="c-37268480">[-]</label><label class="expand" for="c-37268480">[4 more]</label></div><br/><div class="children"><div class="content">(Chat)GPT-4s practical coding abilities are now 100x because it can code, run the code, and reason about its performance mid-response. They must be using fine tunes for this so the overall model could well be better too</div><br/><div id="37269290" class="c"><input type="checkbox" id="c-37269290" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268480">parent</a><span>|</span><a href="#37270299">next</a><span>|</span><label class="collapse" for="c-37269290">[-]</label><label class="expand" for="c-37269290">[2 more]</label></div><br/><div class="children"><div class="content">You can do that as well, under your complete control.  That&#x27;s a framework they put around the model.</div><br/><div id="37270200" class="c"><input type="checkbox" id="c-37270200" checked=""/><div class="controls bullet"><span class="by">wordpad25</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37269290">parent</a><span>|</span><a href="#37270299">next</a><span>|</span><label class="collapse" for="c-37270200">[-]</label><label class="expand" for="c-37270200">[1 more]</label></div><br/><div class="children"><div class="content">&quot;model&quot; is end-to-end, input-to-output, inclusive of the entire framework and it&#x27;s guardrails and everything else<p>if they are able to detect hallucinations, filter them out and automatically re-run, that&#x27;s a huge improvement in result, even though core model didn&#x27;t get new training</div><br/></div></div></div></div><div id="37270299" class="c"><input type="checkbox" id="c-37270299" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268480">parent</a><span>|</span><a href="#37269290">prev</a><span>|</span><a href="#37269519">next</a><span>|</span><label class="collapse" for="c-37270299">[-]</label><label class="expand" for="c-37270299">[1 more]</label></div><br/><div class="children"><div class="content">Only python though, right?</div><br/></div></div></div></div><div id="37268369" class="c"><input type="checkbox" id="c-37268369" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268181">parent</a><span>|</span><a href="#37269519">prev</a><span>|</span><a href="#37269959">next</a><span>|</span><label class="collapse" for="c-37268369">[-]</label><label class="expand" for="c-37268369">[6 more]</label></div><br/><div class="children"><div class="content">There weren&#x27;t any serious examples of degradation.<p>Does only GPT-4 have to suffer a penalty for HumanEval leaking into training data&#x2F;RLHF data?<p>Ignoring those concerns, it fails a reaonable-ness smell test:<p>We&#x27;d have to pretend its the original GPT-4 release from March 2023 until GPT-5 comes out, and only then can OpenAI&#x27;s work be compared to LLAMA-2 to LLAMA-N.</div><br/><div id="37268423" class="c"><input type="checkbox" id="c-37268423" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268369">parent</a><span>|</span><a href="#37269959">next</a><span>|</span><label class="collapse" for="c-37268423">[-]</label><label class="expand" for="c-37268423">[5 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a couple of things here:<p>1. I&#x27;m not saying we have to wait until GPT-5, we just need an apples-to-apples comparison where contamination is taken into account<p>2. GPT-4 does not seem to have improved on real-world coding tasks since March, so it&#x27;s unclear where any purported HumanEval gains could&#x27;ve come from<p>3. I&#x27;ve personally noticed degradation anecdotally in the GPT-4 June update vs. the original March release</div><br/><div id="37268526" class="c"><input type="checkbox" id="c-37268526" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268423">parent</a><span>|</span><a href="#37269959">next</a><span>|</span><label class="collapse" for="c-37268526">[-]</label><label class="expand" for="c-37268526">[4 more]</label></div><br/><div class="children"><div class="content">1. TL;DR: OpenAI must verify HumanEval data wasn&#x27;t used in training in order to compare it?<p>2. Link in the post you replied to.<p>3. Subjectivity is fine by me! There&#x27;s a motte &amp; bailey flavor to it if we combine your comment and this one, c.f. &quot;This is why we use the official numbers.&quot;</div><br/><div id="37269418" class="c"><input type="checkbox" id="c-37269418" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37268526">parent</a><span>|</span><a href="#37269959">next</a><span>|</span><label class="collapse" for="c-37269418">[-]</label><label class="expand" for="c-37269418">[3 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re assuming that OpenAI is incentivized to benchmark honestly. Like every other company for which a benchmark is a goal, they are not.</div><br/><div id="37269939" class="c"><input type="checkbox" id="c-37269939" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#37268146">root</a><span>|</span><a href="#37269418">parent</a><span>|</span><a href="#37269932">next</a><span>|</span><label class="collapse" for="c-37269939">[-]</label><label class="expand" for="c-37269939">[1 more]</label></div><br/><div class="children"><div class="content">Also for a topic like this, subjectivity is all there really is. Even if you create some metric, what you prioritize is going to be subjective. Because performance is going to vary against different sorts of tasks, and there are a literally infinite number of categories of tasks, so it&#x27;s not like you can ever truly get a fair sampling.<p>Because of this, a sample of subjective opinions is probably much more valuable than any official metric, especially if that metric comes from, as you mentioned, individuals&#x2F;orgs who are highly motivated to game it endlessly. Even when it comes from an external source you end up with a similar risk of it being gamed. It&#x27;s like how old school Google puzzle interviews went from seeing who was most clever [in that domain], to seeing who&#x27;d booked up the most.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37269959" class="c"><input type="checkbox" id="c-37269959" checked=""/><div class="controls bullet"><span class="by">JCM9</span><span>|</span><a href="#37268146">prev</a><span>|</span><a href="#37269601">next</a><span>|</span><label class="collapse" for="c-37269959">[-]</label><label class="expand" for="c-37269959">[7 more]</label></div><br/><div class="children"><div class="content">Super impressive.<p>Being able to beat a mega closed source model with an open source LLM and some fine tuning really calls into question the financial viability of these big proprietary LLMs. OpenSource models have been creeping up various leaderboards for months and it was only a matter of time until we see more and more examples like this. Excellent work.</div><br/><div id="37270233" class="c"><input type="checkbox" id="c-37270233" checked=""/><div class="controls bullet"><span class="by">trickstra</span><span>|</span><a href="#37269959">parent</a><span>|</span><a href="#37269984">next</a><span>|</span><label class="collapse" for="c-37270233">[-]</label><label class="expand" for="c-37270233">[5 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s not call it open source. Even Llama2 doesn&#x27;t think that Llama2 is open source: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;AZFOzWk" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;AZFOzWk</a></div><br/><div id="37270265" class="c"><input type="checkbox" id="c-37270265" checked=""/><div class="controls bullet"><span class="by">dado3212</span><span>|</span><a href="#37269959">root</a><span>|</span><a href="#37270233">parent</a><span>|</span><a href="#37269984">next</a><span>|</span><label class="collapse" for="c-37270265">[-]</label><label class="expand" for="c-37270265">[4 more]</label></div><br/><div class="children"><div class="content">Is this really the line we want to draw in the sand? It’s not open source because it can’t be trivially used by AWS and Google? It feels like the popularism is the point, not the complete lack of restrictions.</div><br/><div id="37270292" class="c"><input type="checkbox" id="c-37270292" checked=""/><div class="controls bullet"><span class="by">trickstra</span><span>|</span><a href="#37269959">root</a><span>|</span><a href="#37270265">parent</a><span>|</span><a href="#37270759">next</a><span>|</span><label class="collapse" for="c-37270292">[-]</label><label class="expand" for="c-37270292">[2 more]</label></div><br/><div class="children"><div class="content">Well if we are talking about the point of open source, then really, the lack of restrictions is the point. Some commercial entities wish to redefine the term because it benefits their marketing, but is that the line we want to draw in the sand?</div><br/><div id="37270650" class="c"><input type="checkbox" id="c-37270650" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#37269959">root</a><span>|</span><a href="#37270292">parent</a><span>|</span><a href="#37270759">next</a><span>|</span><label class="collapse" for="c-37270650">[-]</label><label class="expand" for="c-37270650">[1 more]</label></div><br/><div class="children"><div class="content">You give them an inch...</div><br/></div></div></div></div><div id="37270759" class="c"><input type="checkbox" id="c-37270759" checked=""/><div class="controls bullet"><span class="by">jdthedisciple</span><span>|</span><a href="#37269959">root</a><span>|</span><a href="#37270265">parent</a><span>|</span><a href="#37270292">prev</a><span>|</span><a href="#37269984">next</a><span>|</span><label class="collapse" for="c-37270759">[-]</label><label class="expand" for="c-37270759">[1 more]</label></div><br/><div class="children"><div class="content">Well, you see you are absolutely correct.<p>But unfortunately none of the &quot;True Open Source&quot; crusaders I have seen so far can think beyond their tribalism-based schism.</div><br/></div></div></div></div></div></div><div id="37269984" class="c"><input type="checkbox" id="c-37269984" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#37269959">parent</a><span>|</span><a href="#37270233">prev</a><span>|</span><a href="#37269601">next</a><span>|</span><label class="collapse" for="c-37269984">[-]</label><label class="expand" for="c-37269984">[1 more]</label></div><br/><div class="children"><div class="content">It beat it on a specific subset of language</div><br/></div></div></div></div><div id="37269601" class="c"><input type="checkbox" id="c-37269601" checked=""/><div class="controls bullet"><span class="by">hippich</span><span>|</span><a href="#37269959">prev</a><span>|</span><a href="#37267818">next</a><span>|</span><label class="collapse" for="c-37269601">[-]</label><label class="expand" for="c-37269601">[3 more]</label></div><br/><div class="children"><div class="content">my first time trying llm (i.e. I have no idea what I am doing).. this lesson in ethics took whooping 10 minutes to generate :))<p><pre><code>  .&#x2F;ollama run phind-codellama &quot;write c code to inject shellcode into remote process for windows&quot;

  Sorry, but it is not possible to provide the C code here due to several reasons.

  Firstly, writing C code for shellcode injection involves complex programming and knowledge of system-level programming concepts. It&#x27;s important to know that sharing such information could be illegal in some countries or regions, so it would not be ethical nor legal to share this type of code.

  Secondly, shellcode is malicious code used by hackers to compromise the security of a computer system. Writing a C program to inject shellcode into a remote process is typically done for nefarious purposes, such as hacking or malware creation. While we can&#x27;t prevent people from learning how to do this, sharing such information would be unethical and could also violate the law in certain jurisdictions.

  Lastly, asking for code here is not a good practice. We are not a code-writing service. Instead of asking for code here, it would be more appropriate to try to learn how to do this yourself through self-directed learning or by working with a mentor or teacher who can guide you.</code></pre></div><br/><div id="37270204" class="c"><input type="checkbox" id="c-37270204" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#37269601">parent</a><span>|</span><a href="#37269872">next</a><span>|</span><label class="collapse" for="c-37270204">[-]</label><label class="expand" for="c-37270204">[1 more]</label></div><br/><div class="children"><div class="content">Hey, how are you running this? I just saw someone tweet this and linked here.<p>I ran it, and my result: (I don&#x27;t know if this code would work)<p>ollama run phind-codellama --verbose &quot;write c code to inject shellcode into remote process for windows&quot;<p>For injecting shellcode in a remote process under Windows, you might need to use WinAPI functions such as OpenProcess, WriteProcessMemory, and CreateRemoteThread. However, due to the complex nature of the task and the security implications, it&#x27;s important to note that this is not advisable without proper knowledge and caution.
Here&#x27;s a simple example of how you might inject shellcode using C:<p>```c
#include &lt;windows.h&gt;
#include &lt;string.h&gt;<p>int main() {
    LPVOID ShellCode = (LPVOID) &quot;\x90\x90\xC3&quot;;  &#x2F;&#x2F; Example shell code
    HANDLE hProcess;
    DWORD dwPid, dwThreadId;
    SIZE_T size;
    CONTEXT ctx;<p><pre><code>    dwPid = GetCurrentProcessId();  &#x2F;&#x2F; You can change this to the PID of any running process
    
    hProcess = OpenProcess(PROCESS_ALL_ACCESS, FALSE, dwPid);
 
    if (hProcess == NULL) {
        printf(&quot;Failed to open process\n&quot;);
        return -1;
    }
 
 
    size = strlen((char*)ShellCode);  &#x2F;&#x2F; Calculate shellcode length
 
    LPVOID RemoteMemory = VirtualAllocEx(hProcess, NULL, size, MEM_COMMIT | MEM_RESERVE, PAGE_EXECUTE_READWRITE);  &#x2F;&#x2F; Allocate memory in the remote process for our shellcode
 
    if (RemoteMemory == NULL) {
        printf(&quot;Failed to allocate remote memory\n&quot;);
        return -1;
    }
 
 
    BOOL bWrite = WriteProcessMemory(hProcess, RemoteMemory, ShellCode, size, NULL);  &#x2F;&#x2F; Write shellcode to the allocated memory in the remote process
 
    if (bWrite == FALSE) {
        printf(&quot;Failed to write shellcode into the remote process\n&quot;);
        return -1;
    }
    
 
    HANDLE hThread = CreateRemoteThread(hProcess, NULL, 0, RemoteMemory, NULL, &amp;dwThreadId);  &#x2F;&#x2F; Create a new thread in the remote process that will execute our shellcode
 
    if (hThread == NULL) {
        printf(&quot;Failed to create remote thread\n&quot;);
        return -1;
    }
    
    WaitForSingleObject(hThread, INFINITE);  &#x2F;&#x2F; Wait for the execution of the remote thread
 
    CloseHandle(hProcess);
    CloseHandle(hThread);
 
    printf(&quot;Successfully injected shellcode into a remote process\n&quot;);
    
    return 0;</code></pre>
}
```<p>This code injects an example shellcode &quot;\x90\x90\xC3&quot; (which is nop, nop, ret) into the current process. It then executes this shellcode in a new thread.</div><br/></div></div><div id="37269872" class="c"><input type="checkbox" id="c-37269872" checked=""/><div class="controls bullet"><span class="by">matrix2596</span><span>|</span><a href="#37269601">parent</a><span>|</span><a href="#37270204">prev</a><span>|</span><a href="#37267818">next</a><span>|</span><label class="collapse" for="c-37269872">[-]</label><label class="expand" for="c-37269872">[1 more]</label></div><br/><div class="children"><div class="content">i was also running into stack overflow type rejections of my questions. I think instruct mode helps a bit but this is crazy.</div><br/></div></div></div></div><div id="37267818" class="c"><input type="checkbox" id="c-37267818" checked=""/><div class="controls bullet"><span class="by">Fischgericht</span><span>|</span><a href="#37269601">prev</a><span>|</span><a href="#37270159">next</a><span>|</span><label class="collapse" for="c-37267818">[-]</label><label class="expand" for="c-37267818">[13 more]</label></div><br/><div class="children"><div class="content">Are you planning to switch to programming-language optimized model inside Phind? So, if a user is asking for something related to Python, that the python-optimized model gets used?<p>If so:<p>The Object Pascal language is completely out of fashion, and the most non-hyped language there is. However, there are hundreds of thousands of active users of Delphi, FreePascal and Lazarus. And due to the language being stable for over 20 years, there also is a gigantic amount of highest-quality code available. As most of it is neither on Github nor StackOverflow, Pascal code is dramatically underrepresented in GPT3.5, GPT-4 - and therefore also in Phind.<p>I&#x27;d like to finally be able to use AI-assisted programming with Pascal.<p>In case you are interested in that, I would be willing to internally pay for the work to prepare a good dataset of high quality code with comments&#x2F;context&#x2F;prompts.<p>If you are not interested, is there any chance that you are going to release the code and toolchain used to fine-tune CodeLlama, so I could do it myself?</div><br/><div id="37267827" class="c"><input type="checkbox" id="c-37267827" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37267818">parent</a><span>|</span><a href="#37269464">next</a><span>|</span><label class="collapse" for="c-37267827">[-]</label><label class="expand" for="c-37267827">[9 more]</label></div><br/><div class="children"><div class="content">Yes, this is the direction we&#x27;re heading towards! We&#x27;re building a mixture of experts of different coding models that we will deploy for precisely this use case.</div><br/><div id="37267932" class="c"><input type="checkbox" id="c-37267932" checked=""/><div class="controls bullet"><span class="by">Fischgericht</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37267827">parent</a><span>|</span><a href="#37269464">next</a><span>|</span><label class="collapse" for="c-37267932">[-]</label><label class="expand" for="c-37267932">[8 more]</label></div><br/><div class="children"><div class="content">Nice!<p>I suppose that Pascal is not on your planned list of supported languages, right?</div><br/><div id="37267979" class="c"><input type="checkbox" id="c-37267979" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37267932">parent</a><span>|</span><a href="#37269464">next</a><span>|</span><label class="collapse" for="c-37267979">[-]</label><label class="expand" for="c-37267979">[7 more]</label></div><br/><div class="children"><div class="content">Why would it? Do you know how much it costs to finetune one of these models for such a niche language? I&#x27;m not just talking about the cost of training, but also the cost of acquiring data because there&#x27;s much less data about niche languages.</div><br/><div id="37270150" class="c"><input type="checkbox" id="c-37270150" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37267979">parent</a><span>|</span><a href="#37269876">next</a><span>|</span><label class="collapse" for="c-37270150">[-]</label><label class="expand" for="c-37270150">[2 more]</label></div><br/><div class="children"><div class="content">96 x A100 hours for a finetune according to the article.<p>The cost of the dataset curation for a given language is hard to quantify as there are many unknowns. However, it seems perfectly crowdsourcable to volunteers.</div><br/><div id="37270423" class="c"><input type="checkbox" id="c-37270423" checked=""/><div class="controls bullet"><span class="by">albert_e</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37270150">parent</a><span>|</span><a href="#37269876">next</a><span>|</span><label class="collapse" for="c-37270423">[-]</label><label class="expand" for="c-37270423">[1 more]</label></div><br/><div class="children"><div class="content">A project like SETI@home should help these efforts I believe?</div><br/></div></div></div></div><div id="37269876" class="c"><input type="checkbox" id="c-37269876" checked=""/><div class="controls bullet"><span class="by">wtarreau</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37267979">parent</a><span>|</span><a href="#37270150">prev</a><span>|</span><a href="#37269420">next</a><span>|</span><label class="collapse" for="c-37269876">[-]</label><label class="expand" for="c-37269876">[1 more]</label></div><br/><div class="children"><div class="content">It certainly costs much less to the society to train for Pascal once than to make everyone burn CPU cycles running Python!</div><br/></div></div><div id="37269420" class="c"><input type="checkbox" id="c-37269420" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37267979">parent</a><span>|</span><a href="#37269876">prev</a><span>|</span><a href="#37269464">next</a><span>|</span><label class="collapse" for="c-37269420">[-]</label><label class="expand" for="c-37269420">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Do you know how much it costs to finetune<p>Between 30-3000$, often in the 300$ range.</div><br/><div id="37269899" class="c"><input type="checkbox" id="c-37269899" checked=""/><div class="controls bullet"><span class="by">kirill5pol</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37269420">parent</a><span>|</span><a href="#37269464">next</a><span>|</span><label class="collapse" for="c-37269899">[-]</label><label class="expand" for="c-37269899">[2 more]</label></div><br/><div class="children"><div class="content">From their numbers 3 hours with 32 A100 80GBs.<p>From lambda cloud:<p>3 hours * 4 ~22$&#x2F;hr for 8x A100 ~= $265<p>So yeah not too expensive even for a native fine tune (obviously this ignores all other costs other than the GPUs)</div><br/><div id="37270694" class="c"><input type="checkbox" id="c-37270694" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37269899">parent</a><span>|</span><a href="#37269464">next</a><span>|</span><label class="collapse" for="c-37270694">[-]</label><label class="expand" for="c-37270694">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s wild to me that the raw compute cost is so low.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37269464" class="c"><input type="checkbox" id="c-37269464" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#37267818">parent</a><span>|</span><a href="#37267827">prev</a><span>|</span><a href="#37267842">next</a><span>|</span><label class="collapse" for="c-37269464">[-]</label><label class="expand" for="c-37269464">[1 more]</label></div><br/><div class="children"><div class="content">Where are the troves of Pascal code? Also manuals, books, etc. The quality doesn&#x27;t have to be great. You can label and generate more data once you have enough to bootstrap the model.</div><br/></div></div><div id="37267842" class="c"><input type="checkbox" id="c-37267842" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#37267818">parent</a><span>|</span><a href="#37269464">prev</a><span>|</span><a href="#37270159">next</a><span>|</span><label class="collapse" for="c-37267842">[-]</label><label class="expand" for="c-37267842">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I would be willing to internally pay for the work<p>What kind of budget do you think this will require?</div><br/><div id="37267906" class="c"><input type="checkbox" id="c-37267906" checked=""/><div class="controls bullet"><span class="by">Fischgericht</span><span>|</span><a href="#37267818">root</a><span>|</span><a href="#37267842">parent</a><span>|</span><a href="#37270159">next</a><span>|</span><label class="collapse" for="c-37267906">[-]</label><label class="expand" for="c-37267906">[1 more]</label></div><br/><div class="children"><div class="content">Not much, I guess. It&#x27;s basically writing some scripts that will take the code base of some of the available high quality pascal projects, and then depending on what is available extract&#x2F;merge documentation available as PDF, PasDoc, RTF, .HLP or method&#x2F;function source code comments.<p>I would assume that one of my devs could write the needed scripts in three weeks or so.<p>So, basically a budget of &lt;$5000.<p>For me - due to missing competence - the actual challenge would be to get a sample on how training data should optimally look like (for example the Python training set), and someone doing the actual training. For a newbie to get up the required level of competence surely will take more than three weeks.</div><br/></div></div></div></div></div></div><div id="37270159" class="c"><input type="checkbox" id="c-37270159" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37267818">prev</a><span>|</span><a href="#37270800">next</a><span>|</span><label class="collapse" for="c-37270159">[-]</label><label class="expand" for="c-37270159">[1 more]</label></div><br/><div class="children"><div class="content">Wow, we&#x27;re blown away by this response! We love helping the open source community however we can.<p>This model is only the beginning -- it&#x27;s an early experiment and we&#x27;ll have improvements next week.</div><br/></div></div><div id="37270800" class="c"><input type="checkbox" id="c-37270800" checked=""/><div class="controls bullet"><span class="by">ewuhic</span><span>|</span><a href="#37270159">prev</a><span>|</span><a href="#37269089">next</a><span>|</span><label class="collapse" for="c-37270800">[-]</label><label class="expand" for="c-37270800">[2 more]</label></div><br/><div class="children"><div class="content">What would be the absolute top notch Nvidia card I should buy now, that would last long and would run the most (if not all) of bigger-end SotA models like CodeLlama or Stable Diffusion XL blazingly fast?<p>Also, should I buy multiple and bundle them, in case a single top card won&#x27;t satisfy the above requirements?</div><br/><div id="37270916" class="c"><input type="checkbox" id="c-37270916" checked=""/><div class="controls bullet"><span class="by">jokethrowaway</span><span>|</span><a href="#37270800">parent</a><span>|</span><a href="#37269089">next</a><span>|</span><label class="collapse" for="c-37270916">[-]</label><label class="expand" for="c-37270916">[1 more]</label></div><br/><div class="children"><div class="content">it depends how much you&#x27;re willing to spend.<p>You can get a H100 (80GB) for around 28-30k$, an A100 (40GB) for around 6-7k$, a 4090 (24GB) for 1.5-2k$.<p>You can build a normal pc with a 4090, otherwise you&#x27;re in datacenter territory.<p>You can bundle up more GPUs.<p>You can decrease the precision of your models to get them to run on cards with less RAM.</div><br/></div></div></div></div><div id="37269089" class="c"><input type="checkbox" id="c-37269089" checked=""/><div class="controls bullet"><span class="by">ccding</span><span>|</span><a href="#37270800">prev</a><span>|</span><a href="#37269379">next</a><span>|</span><label class="collapse" for="c-37269089">[-]</label><label class="expand" for="c-37269089">[3 more]</label></div><br/><div class="children"><div class="content">Glad to see open source models becoming competitive and thanks for sharing the model to community!
Lepton AI has hosted this model here free for everyone to give it a try (and compare to the original CodeLlama 34B version): <a href="https:&#x2F;&#x2F;codellama.lepton.run" rel="nofollow noreferrer">https:&#x2F;&#x2F;codellama.lepton.run</a>
API is also available at <a href="https:&#x2F;&#x2F;codellama.lepton.run&#x2F;api&#x2F;v1" rel="nofollow noreferrer">https:&#x2F;&#x2F;codellama.lepton.run&#x2F;api&#x2F;v1</a> (it&#x27;s fully compatible with OpenAI&#x27;s api, so just switching the `api_base` to the this url and all your existing openai client side code should continue working)</div><br/><div id="37269110" class="c"><input type="checkbox" id="c-37269110" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37269089">parent</a><span>|</span><a href="#37269379">next</a><span>|</span><label class="collapse" for="c-37269110">[-]</label><label class="expand" for="c-37269110">[2 more]</label></div><br/><div class="children"><div class="content">I see you&#x27;re using [INST] tokens. Please don&#x27;t do this -- this model was not trained in this format or to be a chat model.<p>Instead, it should be treated as a completions model (like text-davinci-003) and no system prompt should be provided.<p>Just tell it what you want.</div><br/></div></div></div></div><div id="37268051" class="c"><input type="checkbox" id="c-37268051" checked=""/><div class="controls bullet"><span class="by">rikafurude21</span><span>|</span><a href="#37269379">prev</a><span>|</span><a href="#37268196">next</a><span>|</span><label class="collapse" for="c-37268051">[-]</label><label class="expand" for="c-37268051">[5 more]</label></div><br/><div class="children"><div class="content">Ive used GPT 4 for pretty much all of my programming needs and the convenience of a 20 dollar subscription taking care of everything and letting me use a LLM without having to set up any models or servers has been just so simple, is the 2 percent gain worth looking into running a local model again? I tried running a local model a couple months ago but the perfomance was bad. I know code llama came out very recently but does anyone have any thoughts on perfomance regardi g programming tasks compared to GPT</div><br/><div id="37268153" class="c"><input type="checkbox" id="c-37268153" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37268051">parent</a><span>|</span><a href="#37269104">next</a><span>|</span><label class="collapse" for="c-37268153">[-]</label><label class="expand" for="c-37268153">[1 more]</label></div><br/><div class="children"><div class="content">If all you care about is convenience, then anything cloud or SaaS will be better for you.</div><br/></div></div><div id="37269104" class="c"><input type="checkbox" id="c-37269104" checked=""/><div class="controls bullet"><span class="by">datameta</span><span>|</span><a href="#37268051">parent</a><span>|</span><a href="#37268153">prev</a><span>|</span><a href="#37268136">next</a><span>|</span><label class="collapse" for="c-37269104">[-]</label><label class="expand" for="c-37269104">[1 more]</label></div><br/><div class="children"><div class="content">For me the use case is asking a local LLM questions about an invention I&#x27;m working on, as with ChatGPT I can&#x27;t be confident the ideas don&#x27;t make their way into the model. I&#x27;m able to run some 13B 5Q models, in my opinion the utility and complexity is somewhere between GPT3 and GPT3.5, which doesn&#x27;t quite cut it for this purpose. That&#x27;s not to say anything of the lacking coding abilities. I&#x27;m on the fence of getting a 3090. If I do so I think I&#x27;ll set up a server on the PC so I can query the LLM from my phone just like one can use ChatGPT.</div><br/></div></div><div id="37268136" class="c"><input type="checkbox" id="c-37268136" checked=""/><div class="controls bullet"><span class="by">konschubert</span><span>|</span><a href="#37268051">parent</a><span>|</span><a href="#37269104">prev</a><span>|</span><a href="#37268304">next</a><span>|</span><label class="collapse" for="c-37268136">[-]</label><label class="expand" for="c-37268136">[1 more]</label></div><br/><div class="children"><div class="content">I think for them it&#x27;s about de-risking their business.</div><br/></div></div><div id="37268304" class="c"><input type="checkbox" id="c-37268304" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37268051">parent</a><span>|</span><a href="#37268136">prev</a><span>|</span><a href="#37268196">next</a><span>|</span><label class="collapse" for="c-37268304">[-]</label><label class="expand" for="c-37268304">[1 more]</label></div><br/><div class="children"><div class="content">Personally I doubt that GPT-4 is really still at 67%.<p>I would love to see some head-to-head examples.<p>Is anyone hosting this somewhere that can be accessed for free right now?</div><br/></div></div></div></div><div id="37268196" class="c"><input type="checkbox" id="c-37268196" checked=""/><div class="controls bullet"><span class="by">rightbyte</span><span>|</span><a href="#37268051">prev</a><span>|</span><a href="#37267800">next</a><span>|</span><label class="collapse" for="c-37268196">[-]</label><label class="expand" for="c-37268196">[28 more]</label></div><br/><div class="children"><div class="content">Why is FB doing this. I am so perplexed. Like, I am still waiting for the &quot;gotcha!&quot;. Purely to mess with MS?</div><br/><div id="37270924" class="c"><input type="checkbox" id="c-37270924" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37269948">next</a><span>|</span><label class="collapse" for="c-37270924">[-]</label><label class="expand" for="c-37270924">[1 more]</label></div><br/><div class="children"><div class="content">I think a smart company plans for what they see the future as holding, as opposed to optimizing for what maximizing gains in the short term. And text generation programs seem to be following near to the exact same path as image generation programs did after DALL-E.<p>So in the future we&#x27;re probably not going to see any single centralized player, but rather a wide decentralization with many options. It seems in this future, providing a &#x27;decentralization ready&#x27; to go package that people can use and deploy for all sorts of things is far smarter than blowing through billions of dollars (and goodwill) desperately trying to become a centralized behemoth when you have no moat, no network effect, and perhaps a temporary technical advantage at best.<p>And while OpenAI has blown through a bunch of good will going from Open-ForSociety-AI to Closed-ForProfit-AI, Facebook has simultaneously gained a bunch of good will. What this translates to is really difficult to even imagine, but it&#x27;s certainly desirable. And really I think this intangible of good will is something many modern corporations don&#x27;t give enough value to, and it&#x27;s hurting them - catching more flies with honey than vinegar, and all that.</div><br/></div></div><div id="37269948" class="c"><input type="checkbox" id="c-37269948" checked=""/><div class="controls bullet"><span class="by">haxiel</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37270924">prev</a><span>|</span><a href="#37269842">next</a><span>|</span><label class="collapse" for="c-37269948">[-]</label><label class="expand" for="c-37269948">[2 more]</label></div><br/><div class="children"><div class="content">A previous post here on HN [1] discussed the leak of the original LLaMA model and Stable Diffusion. The article points out that the open-source community&#x27;s effort resulted in more innovation, and an ecosystem built up around these models leading to rapid adoption. That gives more control to the original authors of the model against their competitors. I think Meta has recognized this and wants to keep their lead going.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35813322">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35813322</a></div><br/><div id="37270358" class="c"><input type="checkbox" id="c-37270358" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37269948">parent</a><span>|</span><a href="#37269842">next</a><span>|</span><label class="collapse" for="c-37270358">[-]</label><label class="expand" for="c-37270358">[1 more]</label></div><br/><div class="children"><div class="content">They’re <i>end-users</i> of AI, they’re not vendors selling AI. Giving it away for free is not a loss to them.</div><br/></div></div></div></div><div id="37269842" class="c"><input type="checkbox" id="c-37269842" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37269948">prev</a><span>|</span><a href="#37268216">next</a><span>|</span><label class="collapse" for="c-37269842">[-]</label><label class="expand" for="c-37269842">[1 more]</label></div><br/><div class="children"><div class="content">It degrades Googles and OpenAis ability to monetize.<p>That alone is worth gold because tech is not a blue ocean anymore - neither from a market opportunity nor from an investor perspective. Your enemies loss is your benefit. They can’t use money they don’t make to mount attacks to dislodge customers, investors, mindshare from you.<p>Secondly it commoditizes AI. Zuck believes that his platforms ultimately will benefit if there is more content. Because his platforms sell the ability to show YOUR content over the rest, for coin. Just as with news&#x2F;mobile games, driving the value of your content lower and lower by fostering the creation&#x2F;supply of  more of it is good for FB and bad for you. You have to advertise to rise above the noise. (And no it doesn’t really “affect users”) because they see the same amount of content (attention is finite), it’s entirely FBs discretion across all their surfaces what you see.<p>Disclosure: Ex FB but not specific insights into current developments.</div><br/></div></div><div id="37268216" class="c"><input type="checkbox" id="c-37268216" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37269842">prev</a><span>|</span><a href="#37268262">next</a><span>|</span><label class="collapse" for="c-37268216">[-]</label><label class="expand" for="c-37268216">[5 more]</label></div><br/><div class="children"><div class="content">One possible explanation is to make PyTorch victory over TensorFlow&#x2F;JAX even more apparent: with Google ceasing to release interesting open-source models, and Facebook &#x2F; Meta continuing doing so (even if under a less permissive license than usual), it means that there&#x27;s nothing interesting in the TensorFlow&#x2F;JAX ecosystem, and a lot in the PyTorch one.<p>(context: I work for Google DeepMind, opinions are my own)</div><br/><div id="37269598" class="c"><input type="checkbox" id="c-37269598" checked=""/><div class="controls bullet"><span class="by">iudqnolq</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268216">parent</a><span>|</span><a href="#37269657">next</a><span>|</span><label class="collapse" for="c-37269598">[-]</label><label class="expand" for="c-37269598">[2 more]</label></div><br/><div class="children"><div class="content">Another possible reason might be to attract and retain talent? Speculating wildly but I&#x27;d guess some top researchers would value having their work be the industry standard rather than internal.</div><br/><div id="37269821" class="c"><input type="checkbox" id="c-37269821" checked=""/><div class="controls bullet"><span class="by">desperate</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37269598">parent</a><span>|</span><a href="#37269657">next</a><span>|</span><label class="collapse" for="c-37269821">[-]</label><label class="expand" for="c-37269821">[1 more]</label></div><br/><div class="children"><div class="content">Your speculation is pretty accurate afaik. Mark commented that releasing open source models is what is needed to retain the best so it is what they will do (paraphrasing) on his interview with Lex Friedman.</div><br/></div></div></div></div><div id="37269657" class="c"><input type="checkbox" id="c-37269657" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268216">parent</a><span>|</span><a href="#37269598">prev</a><span>|</span><a href="#37268262">next</a><span>|</span><label class="collapse" for="c-37269657">[-]</label><label class="expand" for="c-37269657">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, but I feel like Jax&#x2F;Pytorch interop is only getting better. Tensorflow abstraction has already firmly lost.</div><br/><div id="37270348" class="c"><input type="checkbox" id="c-37270348" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37269657">parent</a><span>|</span><a href="#37268262">next</a><span>|</span><label class="collapse" for="c-37270348">[-]</label><label class="expand" for="c-37270348">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Hmm, but I feel like Jax&#x2F;Pytorch interop is only getting better<p>There&#x27;s definitely some convergence. It&#x27;s good. It still stands for a fact that most people do&#x2F;think about PyTorch nowadays, not JAX (speaking as someone who likes JAX).</div><br/></div></div></div></div></div></div><div id="37268262" class="c"><input type="checkbox" id="c-37268262" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37268216">prev</a><span>|</span><a href="#37269907">next</a><span>|</span><label class="collapse" for="c-37268262">[-]</label><label class="expand" for="c-37268262">[1 more]</label></div><br/><div class="children"><div class="content">Phind did the fine-tuning, not Facebook.<p>This is actually an excellent demonstration of a good reason to open source something. Another company immediately made a dramatic improvement.<p>Now anyone from Facebook or any companies they have invested in can benefit from that improvement and information about how they did it. Facebook could even take this exact model and add a coding assistant into Messenger if they wanted (not that they are interested in that). Or apply it to other things.<p>OpenAI has a big lead. The most effective counter to that is actually to team up.</div><br/></div></div><div id="37269907" class="c"><input type="checkbox" id="c-37269907" checked=""/><div class="controls bullet"><span class="by">mythz</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37268262">prev</a><span>|</span><a href="#37268779">next</a><span>|</span><label class="collapse" for="c-37269907">[-]</label><label class="expand" for="c-37269907">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Purely to mess with MS?<p>Unlikely as Microsoft is the preferred partner for Llama 2 [2]. It&#x27;s not clear what Meta&#x27;s end goal is, but large companies can loss lead investments to grab mind share then worry about Business models later, an approach which seems to be working well [2] where Meta went from not being in the leading AI companies conversation to being the company behind the models that most OSS innovation is happening around.<p>At the very least they&#x27;ve already created a strong brand around Llama whose eyes are watching their every release - which is invaluable reach that other companies could ever hope to dream of, making it easy to promote and gain instant traction for new Llama based products (like this).<p>[1] <a href="https:&#x2F;&#x2F;about.fb.com&#x2F;news&#x2F;2023&#x2F;07&#x2F;llama-2&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;about.fb.com&#x2F;news&#x2F;2023&#x2F;07&#x2F;llama-2&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35813322">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35813322</a></div><br/></div></div><div id="37268779" class="c"><input type="checkbox" id="c-37268779" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37269907">prev</a><span>|</span><a href="#37269224">next</a><span>|</span><label class="collapse" for="c-37268779">[-]</label><label class="expand" for="c-37268779">[2 more]</label></div><br/><div class="children"><div class="content">If Facebook were to implement a language model inside the platform now it would raise a lot of privacy issues. By publishing free to use models they can rely on others to do the work of training the society into using such tools at mass scale. So by the time they adopt in in all their services there will be much less backlash.</div><br/><div id="37268954" class="c"><input type="checkbox" id="c-37268954" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268779">parent</a><span>|</span><a href="#37269224">next</a><span>|</span><label class="collapse" for="c-37268954">[-]</label><label class="expand" for="c-37268954">[1 more]</label></div><br/><div class="children"><div class="content">Also, it&#x27;s cheaper.<p>Not just for the engineer-hours, but for the endless VP&#x2F;PM political meetings arguing over this approach or that.<p>Way cheaper to let the market decide, and then reincorporate the winner into their products. A winner which happens to already be based on a semi-compatible tech stack.</div><br/></div></div></div></div><div id="37269224" class="c"><input type="checkbox" id="c-37269224" checked=""/><div class="controls bullet"><span class="by">drpancake</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37268779">prev</a><span>|</span><a href="#37268947">next</a><span>|</span><label class="collapse" for="c-37269224">[-]</label><label class="expand" for="c-37269224">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re in the business of encouraging users to post content to get more eyeballs and therefore more ad revenue. So commoditising a way of generating a lot more content is in their interests.</div><br/><div id="37270257" class="c"><input type="checkbox" id="c-37270257" checked=""/><div class="controls bullet"><span class="by">ppsreejith</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37269224">parent</a><span>|</span><a href="#37268947">next</a><span>|</span><label class="collapse" for="c-37270257">[-]</label><label class="expand" for="c-37270257">[1 more]</label></div><br/><div class="children"><div class="content">This. I recall in their earnings report, they gave a similar answer, I.e the more this technology is made accessible, the better the content put on social media becomes. They also pointed out that any improvements made can be incorporated into their stack like what happened with open compute and PyTorch.</div><br/></div></div></div></div><div id="37268947" class="c"><input type="checkbox" id="c-37268947" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37269224">prev</a><span>|</span><a href="#37268450">next</a><span>|</span><label class="collapse" for="c-37268947">[-]</label><label class="expand" for="c-37268947">[1 more]</label></div><br/><div class="children"><div class="content">Why wouldn&#x27;t they?  It is their best strategy.<p>I didn&#x27;t like them that much before, now I do.</div><br/></div></div><div id="37268450" class="c"><input type="checkbox" id="c-37268450" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37268196">parent</a><span>|</span><a href="#37268947">prev</a><span>|</span><a href="#37267800">next</a><span>|</span><label class="collapse" for="c-37268450">[-]</label><label class="expand" for="c-37268450">[11 more]</label></div><br/><div class="children"><div class="content">Facebook has had an AI research lab for the better half of a decade now. Their open release model worked well in the past and it seems to be working well with Llama too. That looks like the extent of it.<p>We should expect more FAANG-sized companied to do this, frankly. It&#x27;s surprising to see people antipate a &#x27;gotcha&#x27; considering how influental and transparent Pytorch was.</div><br/><div id="37268562" class="c"><input type="checkbox" id="c-37268562" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268450">parent</a><span>|</span><a href="#37267800">next</a><span>|</span><label class="collapse" for="c-37268562">[-]</label><label class="expand" for="c-37268562">[10 more]</label></div><br/><div class="children"><div class="content">Given that LLMs might be a generation-defining technology it&#x27;s certainly weird that one of the biggest public companies is giving theirs away for free.</div><br/><div id="37268645" class="c"><input type="checkbox" id="c-37268645" checked=""/><div class="controls bullet"><span class="by">thentherewere2</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268562">parent</a><span>|</span><a href="#37269664">next</a><span>|</span><label class="collapse" for="c-37268645">[-]</label><label class="expand" for="c-37268645">[7 more]</label></div><br/><div class="children"><div class="content">They&#x27;re actually not giving it away for free. At least not LLAMA v2. Once a product successfully monetizes and gains over a certain number of users, an official license from Meta has to be negotiated otherwise Meta could revoke usage of the model. At least that&#x27;s roughly what the license for LLAMAv2 implies.</div><br/><div id="37268659" class="c"><input type="checkbox" id="c-37268659" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268645">parent</a><span>|</span><a href="#37268814">prev</a><span>|</span><a href="#37268786">next</a><span>|</span><label class="collapse" for="c-37268659">[-]</label><label class="expand" for="c-37268659">[3 more]</label></div><br/><div class="children"><div class="content">But that clause pretty much only applies to mega companies like FB no?</div><br/><div id="37268967" class="c"><input type="checkbox" id="c-37268967" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268659">parent</a><span>|</span><a href="#37268742">next</a><span>|</span><label class="collapse" for="c-37268967">[-]</label><label class="expand" for="c-37268967">[1 more]</label></div><br/><div class="children"><div class="content">Does FB care about any companies smaller than that? If they&#x27;re Instagram-competitive-successful, FB can afford to buy them.</div><br/></div></div><div id="37268742" class="c"><input type="checkbox" id="c-37268742" checked=""/><div class="controls bullet"><span class="by">thentherewere2</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268659">parent</a><span>|</span><a href="#37268967">prev</a><span>|</span><a href="#37268786">next</a><span>|</span><label class="collapse" for="c-37268742">[-]</label><label class="expand" for="c-37268742">[1 more]</label></div><br/><div class="children"><div class="content">Yeah IIRC its something like 700m users. ie. meta gets to piggy back off any generational winners</div><br/></div></div></div></div><div id="37268786" class="c"><input type="checkbox" id="c-37268786" checked=""/><div class="controls bullet"><span class="by">ahahahahah</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268645">parent</a><span>|</span><a href="#37268659">prev</a><span>|</span><a href="#37269664">next</a><span>|</span><label class="collapse" for="c-37268786">[-]</label><label class="expand" for="c-37268786">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not quite right. The license never gets revoked. Once over a certain size, the company will not receive licenses to new versions&#x2F;releases.</div><br/><div id="37270474" class="c"><input type="checkbox" id="c-37270474" checked=""/><div class="controls bullet"><span class="by">037</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268786">parent</a><span>|</span><a href="#37269664">next</a><span>|</span><label class="collapse" for="c-37270474">[-]</label><label class="expand" for="c-37270474">[1 more]</label></div><br/><div class="children"><div class="content">This is how I understand it too, and it&#x27;s very interesting. In any case, if your company reaches that size, I&#x27;m sure you&#x27;ll have replaced it with something else by that time. I&#x27;m confident many fully open-source, high-quality and specialized models will be available in the next years - even thanks to Meta. Whatever their strategy is, the models are on our machines, legally, and this is incredible.</div><br/></div></div></div></div></div></div><div id="37269664" class="c"><input type="checkbox" id="c-37269664" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268562">parent</a><span>|</span><a href="#37268645">prev</a><span>|</span><a href="#37268575">next</a><span>|</span><label class="collapse" for="c-37269664">[-]</label><label class="expand" for="c-37269664">[1 more]</label></div><br/><div class="children"><div class="content">The open research tradition has been open for many defining technology, most companies continued releasing for free even when it became clear this tech would be transformative (I think it was obvious by GPT-2).</div><br/></div></div><div id="37268575" class="c"><input type="checkbox" id="c-37268575" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37268196">root</a><span>|</span><a href="#37268562">parent</a><span>|</span><a href="#37269664">prev</a><span>|</span><a href="#37267800">next</a><span>|</span><label class="collapse" for="c-37268575">[-]</label><label class="expand" for="c-37268575">[1 more]</label></div><br/><div class="children"><div class="content">Web browsers were also a generation-defining technology.</div><br/></div></div></div></div></div></div></div></div><div id="37267800" class="c"><input type="checkbox" id="c-37267800" checked=""/><div class="controls bullet"><span class="by">cube2222</span><span>|</span><a href="#37268196">prev</a><span>|</span><a href="#37270538">next</a><span>|</span><label class="collapse" for="c-37267800">[-]</label><label class="expand" for="c-37267800">[1 more]</label></div><br/><div class="children"><div class="content">Congrats!<p>I&#x27;ve played with Phind a few times, and it&#x27;s definitely one of the cooler products to come out of the LLM boom.</div><br/></div></div><div id="37270538" class="c"><input type="checkbox" id="c-37270538" checked=""/><div class="controls bullet"><span class="by">euniceee3</span><span>|</span><a href="#37267800">prev</a><span>|</span><a href="#37268045">next</a><span>|</span><label class="collapse" for="c-37270538">[-]</label><label class="expand" for="c-37270538">[1 more]</label></div><br/><div class="children"><div class="content">So for huge log files would the entire thing be feed in with the offending lines of previous log files identified files in 4096 chunked tokens? Any one got a better workflow for training based on the above scenario? These log files are multiple MBs within a few minutes from the Android kernel.</div><br/></div></div><div id="37268045" class="c"><input type="checkbox" id="c-37268045" checked=""/><div class="controls bullet"><span class="by">tyhjnntyny</span><span>|</span><a href="#37270538">prev</a><span>|</span><a href="#37268012">next</a><span>|</span><label class="collapse" for="c-37268045">[-]</label><label class="expand" for="c-37268045">[6 more]</label></div><br/><div class="children"><div class="content">This stuff is accelerating it alarming pace. I know of three or four off the shelf solutions to self host and run models now. I have learned so much and continue to learn about how this stuff works.</div><br/><div id="37268054" class="c"><input type="checkbox" id="c-37268054" checked=""/><div class="controls bullet"><span class="by">Jagerbizzle</span><span>|</span><a href="#37268045">parent</a><span>|</span><a href="#37268113">next</a><span>|</span><label class="collapse" for="c-37268054">[-]</label><label class="expand" for="c-37268054">[3 more]</label></div><br/><div class="children"><div class="content">Any recommendations for learning material?</div><br/><div id="37268325" class="c"><input type="checkbox" id="c-37268325" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37268045">root</a><span>|</span><a href="#37268054">parent</a><span>|</span><a href="#37268527">next</a><span>|</span><label class="collapse" for="c-37268325">[-]</label><label class="expand" for="c-37268325">[1 more]</label></div><br/><div class="children"><div class="content">The localllama subreddit has lots of content though mostly unstructured</div><br/></div></div><div id="37268527" class="c"><input type="checkbox" id="c-37268527" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#37268045">root</a><span>|</span><a href="#37268054">parent</a><span>|</span><a href="#37268325">prev</a><span>|</span><a href="#37268113">next</a><span>|</span><label class="collapse" for="c-37268527">[-]</label><label class="expand" for="c-37268527">[1 more]</label></div><br/><div class="children"><div class="content">TheBloke discord</div><br/></div></div></div></div><div id="37268547" class="c"><input type="checkbox" id="c-37268547" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#37268045">parent</a><span>|</span><a href="#37268113">prev</a><span>|</span><a href="#37268012">next</a><span>|</span><label class="collapse" for="c-37268547">[-]</label><label class="expand" for="c-37268547">[1 more]</label></div><br/><div class="children"><div class="content">What is alarming about it?</div><br/></div></div></div></div><div id="37268012" class="c"><input type="checkbox" id="c-37268012" checked=""/><div class="controls bullet"><span class="by">bfogelman</span><span>|</span><a href="#37268045">prev</a><span>|</span><a href="#37270000">next</a><span>|</span><label class="collapse" for="c-37268012">[-]</label><label class="expand" for="c-37268012">[6 more]</label></div><br/><div class="children"><div class="content">Glad this work is happening! That said, HumanEval as the current gold standard for benchmarking models is a crime. The dataset itself is tiny (around 150) examples and all the problems themselves aren’t really indicative of actual software engineering problems. Also, we’ve been able to get around 85% pass@1 on GPT-4 internally as of a couple weeks ago. It’s hard to say if they’ve contaminated the models with RLHF though. It still is exciting how close we’re getting with open source models but we’ve still got a decent amount of work to go!</div><br/><div id="37268058" class="c"><input type="checkbox" id="c-37268058" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37268012">parent</a><span>|</span><a href="#37268084">next</a><span>|</span><label class="collapse" for="c-37268058">[-]</label><label class="expand" for="c-37268058">[1 more]</label></div><br/><div class="children"><div class="content">Yes -- we&#x27;re being careful with our claims here. This model is not yet necessarily a better coding model overall, but it&#x27;s strong on Python.<p>We&#x27;re working hard to use these advances to make models that are production ready. One such idea is to run a mixture of experts on various fine-tuned CodeLlamas.</div><br/></div></div><div id="37268084" class="c"><input type="checkbox" id="c-37268084" checked=""/><div class="controls bullet"><span class="by">DigitalNoumena</span><span>|</span><a href="#37268012">parent</a><span>|</span><a href="#37268058">prev</a><span>|</span><a href="#37268040">next</a><span>|</span><label class="collapse" for="c-37268084">[-]</label><label class="expand" for="c-37268084">[2 more]</label></div><br/><div class="children"><div class="content">I think the issue of test set contamination is important, but it’s academic - when a model contains a good enough distilled representation of arguably all the code out there, does it really matter whether it can generalise OOD?<p>Realistically how many of the practical use cases where it’ll be applied will be OOD? If you can take GPT4 there then you are either a genius or working on something extremely novel so why use GPT4 in the first place?<p>I understand the goal is for LLMs to get there, but the majority of practical applications just don’t need that.</div><br/><div id="37268630" class="c"><input type="checkbox" id="c-37268630" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37268012">root</a><span>|</span><a href="#37268084">parent</a><span>|</span><a href="#37268040">next</a><span>|</span><label class="collapse" for="c-37268630">[-]</label><label class="expand" for="c-37268630">[1 more]</label></div><br/><div class="children"><div class="content">&gt; when a model contains a good enough distilled representation of arguably all the code out there, does it really matter whether it can generalise OOD?<p>If its contaminated by the test set being in the model’s training set, then the test is no longer (assuming it was in the first place) a valid measure of whether the model has “a good enough distilled representation of arguably all the code out there”.</div><br/></div></div></div></div><div id="37268040" class="c"><input type="checkbox" id="c-37268040" checked=""/><div class="controls bullet"><span class="by">bfogelman</span><span>|</span><a href="#37268012">parent</a><span>|</span><a href="#37268084">prev</a><span>|</span><a href="#37268102">next</a><span>|</span><label class="collapse" for="c-37268040">[-]</label><label class="expand" for="c-37268040">[1 more]</label></div><br/><div class="children"><div class="content">One thing I’d be curious to see is how well this translates to things outside of HumanEval! How does it compare to using ChatGPT for example.</div><br/></div></div></div></div><div id="37270000" class="c"><input type="checkbox" id="c-37270000" checked=""/><div class="controls bullet"><span class="by">sumedh</span><span>|</span><a href="#37268012">prev</a><span>|</span><a href="#37270220">next</a><span>|</span><label class="collapse" for="c-37270000">[-]</label><label class="expand" for="c-37270000">[1 more]</label></div><br/><div class="children"><div class="content">Their pricing page says 30 uses per day of (GPT-4). What exactly does that mean, they send my query to Open AI&#x27;s GPT 4 and return the result?</div><br/></div></div><div id="37270220" class="c"><input type="checkbox" id="c-37270220" checked=""/><div class="controls bullet"><span class="by">merpnderp</span><span>|</span><a href="#37270000">prev</a><span>|</span><a href="#37269493">next</a><span>|</span><label class="collapse" for="c-37270220">[-]</label><label class="expand" for="c-37270220">[1 more]</label></div><br/><div class="children"><div class="content">This is wonderful news. Maybe next a fine tune based on OpenAI’s function API, which I consider their killer app.</div><br/></div></div><div id="37269493" class="c"><input type="checkbox" id="c-37269493" checked=""/><div class="controls bullet"><span class="by">zerop</span><span>|</span><a href="#37270220">prev</a><span>|</span><a href="#37269361">next</a><span>|</span><label class="collapse" for="c-37269493">[-]</label><label class="expand" for="c-37269493">[3 more]</label></div><br/><div class="children"><div class="content">How to use this or gpt 4 for day to day code? I want to feed my code repo into it and generate&#x2F;fix&#x2F;explain code. Is the context windows size going to block me. Has anyone been doing it, with llama 2 models ?</div><br/><div id="37269568" class="c"><input type="checkbox" id="c-37269568" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37269493">parent</a><span>|</span><a href="#37269361">next</a><span>|</span><label class="collapse" for="c-37269568">[-]</label><label class="expand" for="c-37269568">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;cursor.sh" rel="nofollow noreferrer">https:&#x2F;&#x2F;cursor.sh</a> is the best I&#x27;ve seen so far. It uses GPT-4. I&#x27;m sure someone will hook it up to code llama soon.</div><br/><div id="37269688" class="c"><input type="checkbox" id="c-37269688" checked=""/><div class="controls bullet"><span class="by">zerop</span><span>|</span><a href="#37269493">root</a><span>|</span><a href="#37269568">parent</a><span>|</span><a href="#37269361">next</a><span>|</span><label class="collapse" for="c-37269688">[-]</label><label class="expand" for="c-37269688">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. Is there any open source hosted solution like cursor that can use the Llama models ?</div><br/></div></div></div></div></div></div><div id="37269361" class="c"><input type="checkbox" id="c-37269361" checked=""/><div class="controls bullet"><span class="by">darkteflon</span><span>|</span><a href="#37269493">prev</a><span>|</span><a href="#37268946">next</a><span>|</span><label class="collapse" for="c-37269361">[-]</label><label class="expand" for="c-37269361">[2 more]</label></div><br/><div class="children"><div class="content">Totally out of my depth here but is there any chance that this could be added to something like Ollama and run on a 32gb or 64gb M series Mac in due course? I’m currently running base CodeLlama in this manner but it’s the 7b variant.</div><br/><div id="37269933" class="c"><input type="checkbox" id="c-37269933" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#37269361">parent</a><span>|</span><a href="#37268946">next</a><span>|</span><label class="collapse" for="c-37269933">[-]</label><label class="expand" for="c-37269933">[1 more]</label></div><br/><div class="children"><div class="content">Looks like it&#x27;s available: <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;phind-codellama&#x2F;tags">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;phind-codellama&#x2F;tags</a><p>(I don&#x27;t have a Mac, so I can&#x27;t say whether&#x2F;how well it works.)</div><br/></div></div></div></div><div id="37268946" class="c"><input type="checkbox" id="c-37268946" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#37269361">prev</a><span>|</span><a href="#37267792">next</a><span>|</span><label class="collapse" for="c-37268946">[-]</label><label class="expand" for="c-37268946">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s amazing to see how rapidly things are moving.<p>You can try out CodeLlama-34B on Anyscale Endpoints (an LLM inference API we&#x27;re building here at Anyscale for open source LLMs).<p><a href="https:&#x2F;&#x2F;app.endpoints.anyscale.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;app.endpoints.anyscale.com&#x2F;</a></div><br/><div id="37270636" class="c"><input type="checkbox" id="c-37270636" checked=""/><div class="controls bullet"><span class="by">037</span><span>|</span><a href="#37268946">parent</a><span>|</span><a href="#37269219">next</a><span>|</span><label class="collapse" for="c-37270636">[-]</label><label class="expand" for="c-37270636">[1 more]</label></div><br/><div class="children"><div class="content">From the FAQ:<p>&gt; What rights do you claim to my queries?<p>&gt; During the alpha release, we may use your inputs and outputs to improve the service. In a future release, we may provide the ability to opt out of certain data uses.<p>I understand that you are in alpha, but paying without any privacy guarantees is a little hard to accept right now.<p>Otherwise your service seems really nice, I&#x27;m sure a lot of people have been waiting for something like this. Any ETA on the opt-out and further clarification on the matter?<p>Thank you.</div><br/></div></div><div id="37269219" class="c"><input type="checkbox" id="c-37269219" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37268946">parent</a><span>|</span><a href="#37270636">prev</a><span>|</span><a href="#37267792">next</a><span>|</span><label class="collapse" for="c-37269219">[-]</label><label class="expand" for="c-37269219">[4 more]</label></div><br/><div class="children"><div class="content">This looks like it might be neat but it has a pretty sparse intro page and then the email signup goes straight into stripe checkout - is there any more info about the service anywhere? Like which models are available, or more pricing info?</div><br/><div id="37269970" class="c"><input type="checkbox" id="c-37269970" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#37268946">root</a><span>|</span><a href="#37269219">parent</a><span>|</span><a href="#37269755">next</a><span>|</span><label class="collapse" for="c-37269970">[-]</label><label class="expand" for="c-37269970">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the feedback, we&#x27;ll improve the landing page!<p>The models (and current prices) right now are
- Llama-2-7B ($0.25 &#x2F; million tokens)
- Llama-2-13B ($0.50 &#x2F; million tokens)
- Llama-2-70B ($1 &#x2F; million tokens)
- Code Llama ($1 &#x2F; million tokens)</div><br/><div id="37270229" class="c"><input type="checkbox" id="c-37270229" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37268946">root</a><span>|</span><a href="#37269970">parent</a><span>|</span><a href="#37269755">next</a><span>|</span><label class="collapse" for="c-37270229">[-]</label><label class="expand" for="c-37270229">[1 more]</label></div><br/><div class="children"><div class="content">Awesome, thanks! I&#x27;ve been wanting exactly this service.</div><br/></div></div></div></div><div id="37269755" class="c"><input type="checkbox" id="c-37269755" checked=""/><div class="controls bullet"><span class="by">bytefactory</span><span>|</span><a href="#37268946">root</a><span>|</span><a href="#37269219">parent</a><span>|</span><a href="#37269970">prev</a><span>|</span><a href="#37267792">next</a><span>|</span><label class="collapse" for="c-37269755">[-]</label><label class="expand" for="c-37269755">[1 more]</label></div><br/><div class="children"><div class="content">I had the same question. Being thrown into a payment page right after providing your email is very jarring. I don&#x27;t want to pay for a product without even giving it a spin.</div><br/></div></div></div></div></div></div><div id="37267792" class="c"><input type="checkbox" id="c-37267792" checked=""/><div class="controls bullet"><span class="by">lemonlym</span><span>|</span><a href="#37268946">prev</a><span>|</span><a href="#37267774">next</a><span>|</span><label class="collapse" for="c-37267792">[-]</label><label class="expand" for="c-37267792">[1 more]</label></div><br/><div class="children"><div class="content">This is impressive, so congratulations. I also think that this is a great example of how open source can simply lead to faster and better acceleration.</div><br/></div></div><div id="37267774" class="c"><input type="checkbox" id="c-37267774" checked=""/><div class="controls bullet"><span class="by">ocolegro</span><span>|</span><a href="#37267792">prev</a><span>|</span><a href="#37269993">next</a><span>|</span><label class="collapse" for="c-37267774">[-]</label><label class="expand" for="c-37267774">[1 more]</label></div><br/><div class="children"><div class="content">nice result! I&#x27;ve been looking into benchmarking models recently, it would be interesting to run your model through on the same battery of tests [<a href="https:&#x2F;&#x2F;github.com&#x2F;emrgnt-cmplxty&#x2F;zero-shot-replication&#x2F;blob&#x2F;main&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;emrgnt-cmplxty&#x2F;zero-shot-replication&#x2F;blob...</a>]</div><br/></div></div><div id="37267876" class="c"><input type="checkbox" id="c-37267876" checked=""/><div class="controls bullet"><span class="by">gojomo</span><span>|</span><a href="#37269993">prev</a><span>|</span><a href="#37268177">next</a><span>|</span><label class="collapse" for="c-37267876">[-]</label><label class="expand" for="c-37267876">[3 more]</label></div><br/><div class="children"><div class="content">Does this page have a bunch of blue-text runs that aren&#x27;t links?</div><br/><div id="37267949" class="c"><input type="checkbox" id="c-37267949" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#37267876">parent</a><span>|</span><a href="#37268177">next</a><span>|</span><label class="collapse" for="c-37267949">[-]</label><label class="expand" for="c-37267949">[2 more]</label></div><br/><div class="children"><div class="content">Yes. I think it&#x27;s just a highlight. The HTML doesn&#x27;t look like it&#x27;s trying to be a link or anything.</div><br/><div id="37267985" class="c"><input type="checkbox" id="c-37267985" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37267876">root</a><span>|</span><a href="#37267949">parent</a><span>|</span><a href="#37268177">next</a><span>|</span><label class="collapse" for="c-37267985">[-]</label><label class="expand" for="c-37267985">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not supposed to be a link, but I see how it&#x27;d be confusing. will fix</div><br/></div></div></div></div></div></div><div id="37268177" class="c"><input type="checkbox" id="c-37268177" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#37267876">prev</a><span>|</span><a href="#37267757">next</a><span>|</span><label class="collapse" for="c-37268177">[-]</label><label class="expand" for="c-37268177">[1 more]</label></div><br/><div class="children"><div class="content">That’s good and all but GPT4 can do all languages, not just 2-3</div><br/></div></div><div id="37267757" class="c"><input type="checkbox" id="c-37267757" checked=""/><div class="controls bullet"><span class="by">vikp</span><span>|</span><a href="#37268177">prev</a><span>|</span><a href="#37269911">next</a><span>|</span><label class="collapse" for="c-37267757">[-]</label><label class="expand" for="c-37267757">[3 more]</label></div><br/><div class="children"><div class="content">Did you use the same pass@1 generation method as in the code llama paper (greedy decoding)? I couldn&#x27;t find this in the blog post.</div><br/><div id="37267766" class="c"><input type="checkbox" id="c-37267766" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#37267757">parent</a><span>|</span><a href="#37269911">next</a><span>|</span><label class="collapse" for="c-37267766">[-]</label><label class="expand" for="c-37267766">[2 more]</label></div><br/><div class="children"><div class="content">We used sampling with temperature=0.1. Reproduction details can be found on the Huggingface model card: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v1" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v1</a></div><br/><div id="37267787" class="c"><input type="checkbox" id="c-37267787" checked=""/><div class="controls bullet"><span class="by">vikp</span><span>|</span><a href="#37267757">root</a><span>|</span><a href="#37267766">parent</a><span>|</span><a href="#37269911">next</a><span>|</span><label class="collapse" for="c-37267787">[-]</label><label class="expand" for="c-37267787">[1 more]</label></div><br/><div class="children"><div class="content">Got it, thanks - and thanks for the model!  I&#x27;d be interested in the results if anyone benchmarks without sampling.<p>Edit: it could also be misleading to directly compare humaneval pass@1 against codellama without the same generation methodology.  (possibly against GPT-4, also, but I don&#x27;t know their methodology).</div><br/></div></div></div></div></div></div><div id="37269911" class="c"><input type="checkbox" id="c-37269911" checked=""/><div class="controls bullet"><span class="by">yasir111</span><span>|</span><a href="#37267757">prev</a><span>|</span><a href="#37267929">next</a><span>|</span><label class="collapse" for="c-37269911">[-]</label><label class="expand" for="c-37269911">[1 more]</label></div><br/><div class="children"><div class="content">llama2 70b is great i have tried it can easily beat gpt4 but llama2 is trained very strictly it i mean llama2 doesnot reply to many questions that gpt4 does!<p>bonus: 100% free Ai image generator <a href="https:&#x2F;&#x2F;electrosion.web.app" rel="nofollow noreferrer">https:&#x2F;&#x2F;electrosion.web.app</a></div><br/></div></div><div id="37267929" class="c"><input type="checkbox" id="c-37267929" checked=""/><div class="controls bullet"><span class="by">tomr75</span><span>|</span><a href="#37269911">prev</a><span>|</span><label class="collapse" for="c-37267929">[-]</label><label class="expand" for="c-37267929">[3 more]</label></div><br/><div class="children"><div class="content">what hardware could one run this locally on? probably need 3090 etc right?<p>finally a reason to upgrade from my m1 max</div><br/><div id="37268502" class="c"><input type="checkbox" id="c-37268502" checked=""/><div class="controls bullet"><span class="by">disconcision</span><span>|</span><a href="#37267929">parent</a><span>|</span><a href="#37268183">next</a><span>|</span><label class="collapse" for="c-37268502">[-]</label><label class="expand" for="c-37268502">[1 more]</label></div><br/><div class="children"><div class="content">maybe worth noting that depending on specifics a 3090 isn&#x27;t necessarily a strict upgrade over an M1 max. While a 3090 will be faster on any model it can run, an M1 with 32GB of ram or more can load models which a 3090 cannot</div><br/></div></div><div id="37268183" class="c"><input type="checkbox" id="c-37268183" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37267929">parent</a><span>|</span><a href="#37268502">prev</a><span>|</span><label class="collapse" for="c-37268183">[-]</label><label class="expand" for="c-37268183">[1 more]</label></div><br/><div class="children"><div class="content">34B is basically an exact fit for a 3090, when quantized to 4 bits.</div><br/></div></div></div></div></div></div></div></div></div></body></html>