<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1717923671388" as="style"/><link rel="stylesheet" href="styles.css?v=1717923671388"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2406.02528">Scalable MatMul-Free Language Modeling</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>lykahb</span> | <span>6 comments</span></div><br/><div><div id="40622937" class="c"><input type="checkbox" id="c-40622937" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#40622081">next</a><span>|</span><label class="collapse" for="c-40622937">[-]</label><label class="expand" for="c-40622937">[1 more]</label></div><br/><div class="children"><div class="content">One thing I didn’t figure out from just the paper: how does one train these parameters that are not even approximately real numbers?  Specifically, most of the parameters are ternary (i.e. -1, 0, or 1).  The approximate gradient discussed in the paper will (I think) give some <i>real</i> gradient on each parameter, and that can be further processed by the learning rate schedule, but the result is still a real number g_i for each parameter a_i.  Normally one would update a_i to a_i + g_i, but with these ternary parameters, a_i + g_i isn’t ternary!<p>So what’s the extra trick to make the model stay quantized?  Does one evaluate the gradients on a whole bunch of training inputs, add them up, apply some randomness, and then re-quantize the model?  Or is it something else?</div><br/></div></div><div id="40622081" class="c"><input type="checkbox" id="c-40622081" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#40622937">prev</a><span>|</span><a href="#40622157">next</a><span>|</span><label class="collapse" for="c-40622081">[-]</label><label class="expand" for="c-40622081">[1 more]</label></div><br/><div class="children"><div class="content">Wow - This seems at first read to be really impressive work. They got scaling laws up to a reasonable size, 2.7B, and also run a few downstream tasks. Would be interesting to see how a comparable model trained by someone else does, to check their scores against those.<p>They get real (61%!?) memory savings during training, and inference too.<p>On top of all that, they then go build an FPGA core which is programmed with a custom assembler. And their code is posted and works seamlessly with huggingface transformers?! Absolutely going to test this out.</div><br/></div></div><div id="40622157" class="c"><input type="checkbox" id="c-40622157" checked=""/><div class="controls bullet"><span class="by">gabesullice</span><span>|</span><a href="#40622081">prev</a><span>|</span><a href="#40621930">next</a><span>|</span><label class="collapse" for="c-40622157">[-]</label><label class="expand" for="c-40622157">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of ghotz&#x27;s interview: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;wE1ZoMGIZHM" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;wE1ZoMGIZHM</a></div><br/><div id="40622763" class="c"><input type="checkbox" id="c-40622763" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#40622157">parent</a><span>|</span><a href="#40621930">next</a><span>|</span><label class="collapse" for="c-40622763">[-]</label><label class="expand" for="c-40622763">[1 more]</label></div><br/><div class="children"><div class="content">geohot is shilling his product, which aims to capitalize on making accelerator manufacturers compete with each other on FLOP &#x2F; tensor core throughput.<p>the OP outlines what could be an entirely different compute paradigm for LLMs, hence the FPGA study.  they just happen to also get impressive performance on GPUs making the most of the available interface.</div><br/></div></div></div></div><div id="40621930" class="c"><input type="checkbox" id="c-40621930" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#40622157">prev</a><span>|</span><label class="collapse" for="c-40621930">[-]</label><label class="expand" for="c-40621930">[1 more]</label></div><br/><div class="children"><div class="content">There was another matmul-free language model paper released a year ago FYI:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.17190" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.17190</a></div><br/></div></div></div></div></div></div></div></body></html>