<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687078847126" as="style"/><link rel="stylesheet" href="styles.css?v=1687078847126"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://pkolaczk.github.io/server-slower-than-a-laptop/">How a Single Line of Code Made a 24-Core Server Slower Than a Laptop (2021)</a> <span class="domain">(<a href="https://pkolaczk.github.io">pkolaczk.github.io</a>)</span></div><div class="subtext"><span>xk3</span> | <span>75 comments</span></div><br/><div><div id="36378475" class="c"><input type="checkbox" id="c-36378475" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#36377809">next</a><span>|</span><label class="collapse" for="c-36378475">[-]</label><label class="expand" for="c-36378475">[1 more]</label></div><br/><div class="children"><div class="content">Hmm so the embarrassingly parallel task wasn&#x27;t embarrassingly parallel because of an implementation detail, basically?</div><br/></div></div><div id="36377809" class="c"><input type="checkbox" id="c-36377809" checked=""/><div class="controls bullet"><span class="by">samsquire</span><span>|</span><a href="#36378475">prev</a><span>|</span><a href="#36376824">next</a><span>|</span><label class="collapse" for="c-36377809">[-]</label><label class="expand" for="c-36377809">[2 more]</label></div><br/><div class="children"><div class="content">This is really good. Thank you for this blog post.<p>Asynchronous code, coroutines, async&#x2F;await, parallelising problems is my deep interest and I blog about it everyday.<p>I think the easiest way to parallelise is to shard your data per thread and treat your multithreaded (or multimachine) architecture as a tree - not a graph - where dataflow doesn&#x27;t need to pass between tree branches. This is similar to the Rust&#x27;s &quot;no interior mutabiliy&quot; and Rust data structures pattern.<p>My machine can lock and unlock 61570760 times a second. But it can count to 2 billion in 1 second. So locks are expensive.<p>I recently worked at parallelising the A* graph search algorithm that I&#x27;m using for code generation&#x2F;program synthesis.<p>For 16 processes it takes 35 seconds to synthesise a program but with 3 processes it takes 21 seconds. I think my approach to parallelising A* needs a redesign.<p>We hit Amdahl&#x27;s law when it comes to parallelising. I need to split up my problem into spaces that don&#x27;t require synchronization&#x2F;serialisation.<p>EDIT: I&#x27;ve mentioned this whitepaper before (&quot;Scalability! But at what COST?&quot;) but this whitepaper would be useful reading of anybody working on multithreaded or distributed systems. In summary: single threaded programs can easily be faster and more performant (wall clock time) than multithreaded&#x2F;multimachine distributed machines, but they don&#x27;t scale.<p><a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;conference&#x2F;hotos15&#x2F;hotos15-paper-mcsherry.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;conference&#x2F;hotos15&#x2F;hotos...</a></div><br/><div id="36378011" class="c"><input type="checkbox" id="c-36378011" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#36377809">parent</a><span>|</span><a href="#36376824">next</a><span>|</span><label class="collapse" for="c-36378011">[-]</label><label class="expand" for="c-36378011">[1 more]</label></div><br/><div class="children"><div class="content">The basic algorithm of A* is very sequential, isn&#x27;t it? It works by taking the best scoring unexpanded node and expand that. Most of the time, there&#x27;s only one such node. When expansion has finished, you need to re-sort the queue&#x2F;heap of unexpanded nodes. All those steps are sequential. So I guess the only gain is when the node expansion can be done in parallel; expanding the top-N nodes probably is counterproductive for many problems. How much you gain then depends on the time expansion takes. The advantage of parallelism then depends on how much time one step &quot;down&quot; takes.</div><br/></div></div></div></div><div id="36376824" class="c"><input type="checkbox" id="c-36376824" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#36377809">prev</a><span>|</span><a href="#36378044">next</a><span>|</span><label class="collapse" for="c-36376824">[-]</label><label class="expand" for="c-36376824">[45 more]</label></div><br/><div class="children"><div class="content">Atomics don’t scale. In this age that needs to be widespread elementary knowledge. They are particularly bad on armv8 without atomic extensions because that platform has no equivalent to “lock; xadd” and an atomic increment could theoretically become an infinite loop.</div><br/><div id="36377091" class="c"><input type="checkbox" id="c-36377091" checked=""/><div class="controls bullet"><span class="by">trws</span><span>|</span><a href="#36376824">parent</a><span>|</span><a href="#36377832">next</a><span>|</span><label class="collapse" for="c-36377091">[-]</label><label class="expand" for="c-36377091">[3 more]</label></div><br/><div class="children"><div class="content">Let’s be clear here, while you are 100% right that armv8 atomics kinda suck by default, neither compare and swap nor load linked store conditional scale <i>but</i> some atomics can scale if implemented and used appropriately. As parent points out, an atomic increment can scale, we proved they could scale to the performance of a load in the 80s for goodness sake. The fact that arm, ppc, and some others tend to implement these in <i>the absolute worst way possible</i> for performance doesn’t mean atomics can’t scale.</div><br/><div id="36377408" class="c"><input type="checkbox" id="c-36377408" checked=""/><div class="controls bullet"><span class="by">ghusbands</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377091">parent</a><span>|</span><a href="#36377832">next</a><span>|</span><label class="collapse" for="c-36377408">[-]</label><label class="expand" for="c-36377408">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not being clear - are you claiming that they do scale well on ARM&#x2F;PPC, despite the &quot;absolute worst&quot; implementation or that they don&#x27;t?</div><br/><div id="36377935" class="c"><input type="checkbox" id="c-36377935" checked=""/><div class="controls bullet"><span class="by">bonzini</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377408">parent</a><span>|</span><a href="#36377832">next</a><span>|</span><label class="collapse" for="c-36377935">[-]</label><label class="expand" for="c-36377935">[1 more]</label></div><br/><div class="children"><div class="content">He means that locks anyway have the same problems as atomics on platforms with load locked&#x2F;store conditional. Therefore yes, that&#x27;s a problem of the platform, but even on arm&#x2F;ppc atomics scale better than locks.<p>Which is true, but eliminating sharing works even better if possible as proved by the article.</div><br/></div></div></div></div></div></div><div id="36377832" class="c"><input type="checkbox" id="c-36377832" checked=""/><div class="controls bullet"><span class="by">eldenring</span><span>|</span><a href="#36376824">parent</a><span>|</span><a href="#36377091">prev</a><span>|</span><a href="#36377815">next</a><span>|</span><label class="collapse" for="c-36377832">[-]</label><label class="expand" for="c-36377832">[3 more]</label></div><br/><div class="children"><div class="content">Atomics scale very well if you are reading often and writing rarely.</div><br/><div id="36378256" class="c"><input type="checkbox" id="c-36378256" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377832">parent</a><span>|</span><a href="#36377984">next</a><span>|</span><label class="collapse" for="c-36378256">[-]</label><label class="expand" for="c-36378256">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. Atomics are a red herring. The single writer principle should be the fundamental guideline.</div><br/></div></div><div id="36377984" class="c"><input type="checkbox" id="c-36377984" checked=""/><div class="controls bullet"><span class="by">anaisbetts</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377832">parent</a><span>|</span><a href="#36378256">prev</a><span>|</span><a href="#36377815">next</a><span>|</span><label class="collapse" for="c-36377984">[-]</label><label class="expand" for="c-36377984">[1 more]</label></div><br/><div class="children"><div class="content">Yep, uncontended atomics are quite fast. When they&#x27;re contended is when things start to slow down like OP has seen</div><br/></div></div></div></div><div id="36377815" class="c"><input type="checkbox" id="c-36377815" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36376824">parent</a><span>|</span><a href="#36377832">prev</a><span>|</span><a href="#36376862">next</a><span>|</span><label class="collapse" for="c-36377815">[-]</label><label class="expand" for="c-36377815">[1 more]</label></div><br/><div class="children"><div class="content">It’s not just theoretical. In bad situations (many cores, heavy contention…) you can get cores to starve each other as they try to each poke the monitor and fail continuously. I know of at least one platform which moved to LSE immediately partly because it fixed stuff like this. LL&#x2F;SC is nice from some perspectives but it fails if you scale it up and also can be difficult to reason about (cough, cough, Linux getting their cmpxchg implementation wrong for years…)</div><br/></div></div><div id="36376862" class="c"><input type="checkbox" id="c-36376862" checked=""/><div class="controls bullet"><span class="by">iaaan</span><span>|</span><a href="#36376824">parent</a><span>|</span><a href="#36377815">prev</a><span>|</span><a href="#36378303">next</a><span>|</span><label class="collapse" for="c-36376862">[-]</label><label class="expand" for="c-36376862">[32 more]</label></div><br/><div class="children"><div class="content">Ignorant question: what&#x27;s the alternative? A normal mutex? I just sort of assumed atomics were abstractions around some type and a mutex.</div><br/><div id="36376913" class="c"><input type="checkbox" id="c-36376913" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376862">parent</a><span>|</span><a href="#36376880">next</a><span>|</span><label class="collapse" for="c-36376913">[-]</label><label class="expand" for="c-36376913">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the other way around. Mutex is an abstraction over atomics.</div><br/></div></div><div id="36376880" class="c"><input type="checkbox" id="c-36376880" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376862">parent</a><span>|</span><a href="#36376913">prev</a><span>|</span><a href="#36376873">next</a><span>|</span><label class="collapse" for="c-36376880">[-]</label><label class="expand" for="c-36376880">[25 more]</label></div><br/><div class="children"><div class="content">Partition your data structure to be per-CPU and get rid of any sharing.</div><br/><div id="36377506" class="c"><input type="checkbox" id="c-36377506" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376880">parent</a><span>|</span><a href="#36376946">next</a><span>|</span><label class="collapse" for="c-36377506">[-]</label><label class="expand" for="c-36377506">[13 more]</label></div><br/><div class="children"><div class="content">&gt; Just don&#x27;t use any shared state, bro<p>Thank you, Sherlock.<p>But for the rest of us: when you need shared state, lockfree atomic spinlocks are roughly 1000 times more performant than mutexes. (Not a scientific estimate, numbers taken from real-word experience.)</div><br/><div id="36377931" class="c"><input type="checkbox" id="c-36377931" checked=""/><div class="controls bullet"><span class="by">cyberax</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377506">parent</a><span>|</span><a href="#36377783">next</a><span>|</span><label class="collapse" for="c-36377931">[-]</label><label class="expand" for="c-36377931">[4 more]</label></div><br/><div class="children"><div class="content">No, they are not.<p>The slow part of locking is invalidation of cache lines, and this has to happen with spinlocks anyway. Modern mutex implementations also first try to acquire the lock optimistically, so in the uncontended case they are as fast as userspace spinlocks (modulo inlining).<p>And if you have a contended lock, then userspace spinlocks are a PITA. You need to take care of fairness, ideally deal with the scheduler (yield to a thread that is not spinning on the same spinlock), and so on.<p>You can do all of that properly, but even then, you&#x27;re looking at maaaaybe 10-20% performance increase in real-world applications.<p>Pure spinlocks can win only in contrived cases, like only having exactly two threads contending for the lock, with short locked sections.</div><br/><div id="36378087" class="c"><input type="checkbox" id="c-36378087" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377931">parent</a><span>|</span><a href="#36377783">next</a><span>|</span><label class="collapse" for="c-36378087">[-]</label><label class="expand" for="c-36378087">[3 more]</label></div><br/><div class="children"><div class="content">Pure spin locks will always win for uncontended locks.</div><br/><div id="36378169" class="c"><input type="checkbox" id="c-36378169" checked=""/><div class="controls bullet"><span class="by">NobodyNada</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36378087">parent</a><span>|</span><a href="#36377783">next</a><span>|</span><label class="collapse" for="c-36378169">[-]</label><label class="expand" for="c-36378169">[2 more]</label></div><br/><div class="children"><div class="content">Without any contention at all, a spinlock and a mutex are identical: a single compare-and-swap.</div><br/><div id="36378263" class="c"><input type="checkbox" id="c-36378263" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36378169">parent</a><span>|</span><a href="#36377783">next</a><span>|</span><label class="collapse" for="c-36378263">[-]</label><label class="expand" for="c-36378263">[1 more]</label></div><br/><div class="children"><div class="content">Technically you can use a release-store on a spinlock unlock path, but you need a CAS for a mutex.</div><br/></div></div></div></div></div></div></div></div><div id="36377783" class="c"><input type="checkbox" id="c-36377783" checked=""/><div class="controls bullet"><span class="by">petters</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377506">parent</a><span>|</span><a href="#36377931">prev</a><span>|</span><a href="#36378290">next</a><span>|</span><label class="collapse" for="c-36377783">[-]</label><label class="expand" for="c-36377783">[4 more]</label></div><br/><div class="children"><div class="content">&gt; spinlocks are roughly 1000 times more performant than mutexes<p>That is absolutely not universal. See e.g. but there are of course many places discussing this:
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21970050">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21970050</a></div><br/><div id="36377796" class="c"><input type="checkbox" id="c-36377796" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377783">parent</a><span>|</span><a href="#36378079">next</a><span>|</span><label class="collapse" for="c-36377796">[-]</label><label class="expand" for="c-36377796">[1 more]</label></div><br/><div class="children"><div class="content">Locking is hard in general. Don’t do an unbounded spin in userspace is typically good advice though. The typical mutex construction these days will spin for a little while in an attempt to take advantage of mostly uncontended locks and then yield to the kernel.</div><br/></div></div><div id="36378079" class="c"><input type="checkbox" id="c-36378079" checked=""/><div class="controls bullet"><span class="by">happymellon</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377783">parent</a><span>|</span><a href="#36377796">prev</a><span>|</span><a href="#36378290">next</a><span>|</span><label class="collapse" for="c-36378079">[-]</label><label class="expand" for="c-36378079">[2 more]</label></div><br/><div class="children"><div class="content">This immediately popped into my head too when I saw the comment.<p>Spinlocks are terrible, and written by people who are trying to do quick hacks because they work in terrible environments and are taught to do bad things.<p>There is a reason that most GPU drivers are just lists of hacks to get games working correctly.</div><br/><div id="36378456" class="c"><input type="checkbox" id="c-36378456" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36378079">parent</a><span>|</span><a href="#36378290">next</a><span>|</span><label class="collapse" for="c-36378456">[-]</label><label class="expand" for="c-36378456">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, spinlocks sound cool but actually they are terrible. Hybrid locks are generally quite good.
The only way to get better is to pin cores and let nothing else run on them. Only then will spinlocks have a chance to increase performance by a tiny bit.</div><br/></div></div></div></div></div></div><div id="36378290" class="c"><input type="checkbox" id="c-36378290" checked=""/><div class="controls bullet"><span class="by">gnulinux</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377506">parent</a><span>|</span><a href="#36377783">prev</a><span>|</span><a href="#36377900">next</a><span>|</span><label class="collapse" for="c-36378290">[-]</label><label class="expand" for="c-36378290">[2 more]</label></div><br/><div class="children"><div class="content">This is not true, you should never use spinlocks in userspace unless you know exactly what you&#x27;re doing. Using spinlocks well requires scheduling, which you have no control over in userspace, this is why it makes more sense for kernel to use spinlocks. Without scheduling, with spinlocks, random threads can starve for no reason.</div><br/><div id="36378365" class="c"><input type="checkbox" id="c-36378365" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36378290">parent</a><span>|</span><a href="#36377900">next</a><span>|</span><label class="collapse" for="c-36378365">[-]</label><label class="expand" for="c-36378365">[1 more]</label></div><br/><div class="children"><div class="content">You are right, theoretically.<p>Unfortunately, the userspace part of pthreads is not tuned for performance and does a lot of ridiculous things if you care about parallelism.<p>(Mostly I was complaining about the poor quality of userspace system libraries.)</div><br/></div></div></div></div><div id="36377900" class="c"><input type="checkbox" id="c-36377900" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377506">parent</a><span>|</span><a href="#36378290">prev</a><span>|</span><a href="#36376946">next</a><span>|</span><label class="collapse" for="c-36377900">[-]</label><label class="expand" for="c-36377900">[2 more]</label></div><br/><div class="children"><div class="content">mutexes are not that slow.<p>Uncontested (single thread):<p><pre><code>  incrementing using atomics took 0.002011 s (0.2513 ns &#x2F; increment)
  incrementing using mutex took   0.005515 s (0.6894 ns &#x2F; increment)
</code></pre>
Contested (8 threads trying to increment a single protected integer):<p><pre><code>  incrementing using atomics took 0.1069 s (13.36 ns &#x2F; increment)
  incrementing using mutex took   1.970 s (246.3 ns &#x2F; increment)
</code></pre>
So mutexes are roughly the same speed in the uncontested case, and about 20x slower in this heavily contested case.
This is on Windows.</div><br/><div id="36377941" class="c"><input type="checkbox" id="c-36377941" checked=""/><div class="controls bullet"><span class="by">cyberax</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377900">parent</a><span>|</span><a href="#36376946">next</a><span>|</span><label class="collapse" for="c-36377941">[-]</label><label class="expand" for="c-36377941">[1 more]</label></div><br/><div class="children"><div class="content">I guess you have more than 8 physical CPUs?</div><br/></div></div></div></div></div></div><div id="36376946" class="c"><input type="checkbox" id="c-36376946" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376880">parent</a><span>|</span><a href="#36377506">prev</a><span>|</span><a href="#36377617">next</a><span>|</span><label class="collapse" for="c-36376946">[-]</label><label class="expand" for="c-36376946">[10 more]</label></div><br/><div class="children"><div class="content">how is this done in rust or c?</div><br/><div id="36377022" class="c"><input type="checkbox" id="c-36377022" checked=""/><div class="controls bullet"><span class="by">bottled_poe</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376946">parent</a><span>|</span><a href="#36377282">next</a><span>|</span><label class="collapse" for="c-36377022">[-]</label><label class="expand" for="c-36377022">[5 more]</label></div><br/><div class="children"><div class="content">This is not a language problem. It’s an algorithm design problem. There’s no silver bullet. The basic principle is to divide the problem space into independent blocks of work. How to achieve that depends on the problem.</div><br/><div id="36377561" class="c"><input type="checkbox" id="c-36377561" checked=""/><div class="controls bullet"><span class="by">cyber_kinetist</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377022">parent</a><span>|</span><a href="#36377267">next</a><span>|</span><label class="collapse" for="c-36377561">[-]</label><label class="expand" for="c-36377561">[1 more]</label></div><br/><div class="children"><div class="content">For a good example, there&#x27;s a tricky parallelization problem in physical simulation, which you have update edge&#x2F;triangle&#x2F;bending wing forces in a mesh structure without any race conditions. (This becomes especially thorny if you want to parallelize your algorithm to the GPU.) A surprising solution for this is graph coloring, where you &quot;color&quot; each element without having two elements that interfere with each other the same color. Then you can safely parellelize the updates of all elements inside each color group, since the same color guarantees absolutely no interference.</div><br/></div></div><div id="36377267" class="c"><input type="checkbox" id="c-36377267" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377022">parent</a><span>|</span><a href="#36377561">prev</a><span>|</span><a href="#36377308">next</a><span>|</span><label class="collapse" for="c-36377267">[-]</label><label class="expand" for="c-36377267">[1 more]</label></div><br/><div class="children"><div class="content">Algorithm design problems, when general enough, warrant being treated as language problems!<p>Every feature of programming languages started in this fashion.</div><br/></div></div><div id="36377308" class="c"><input type="checkbox" id="c-36377308" checked=""/><div class="controls bullet"><span class="by">inopinatus</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377022">parent</a><span>|</span><a href="#36377267">prev</a><span>|</span><a href="#36377282">next</a><span>|</span><label class="collapse" for="c-36377308">[-]</label><label class="expand" for="c-36377308">[2 more]</label></div><br/><div class="children"><div class="content">There are programming languages e.g. occam designed with the intention of exposing parallelism algebraically, but I wouldn&#x27;t call them a silver bullet either.</div><br/><div id="36377766" class="c"><input type="checkbox" id="c-36377766" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377308">parent</a><span>|</span><a href="#36377282">next</a><span>|</span><label class="collapse" for="c-36377766">[-]</label><label class="expand" for="c-36377766">[1 more]</label></div><br/><div class="children"><div class="content">They are great <i>in theory</i>, but sucks in practice.<p>We don&#x27;t have good optimising compiler for that</div><br/></div></div></div></div></div></div><div id="36377282" class="c"><input type="checkbox" id="c-36377282" checked=""/><div class="controls bullet"><span class="by">inopinatus</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376946">parent</a><span>|</span><a href="#36377022">prev</a><span>|</span><a href="#36377310">next</a><span>|</span><label class="collapse" for="c-36377282">[-]</label><label class="expand" for="c-36377282">[1 more]</label></div><br/><div class="children"><div class="content">For something small like a counter you can use thread_local since C11, but for substantial parallelized computation, designing the division of work to avoid shared writes typically entails a scheduling function that parcels up the work, sets up memory allocation in advance to avoid conflict, and then hands an entirely unshared execution context for each thread to the thread start function, most likely as a pointer to a app-specific struct (since that is what pthread_create allows for), and then subsequently applies a combination operation in the thread reaper loop to collate results (extra brownie points accrue when a reduce function is written to vectorize).<p>The memory allocator plays a significant role, since allocation strategy needs to be per-thread-&#x2F;per-CPU-cache-aware. Choosing and then tuning a different malloc (e.g. tcmalloc, jemalloc) to the one in your platform&#x27;s default library is a non-trivial matter but may have enormous impact both on overall performance and memory demand.<p>In addition, when you design computation this way it is relatively easy to hadoopify it later, since it&#x27;s basically map-reduce writ small.</div><br/></div></div><div id="36376956" class="c"><input type="checkbox" id="c-36376956" checked=""/><div class="controls bullet"><span class="by">flatline</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376946">parent</a><span>|</span><a href="#36377310">prev</a><span>|</span><a href="#36377552">next</a><span>|</span><label class="collapse" for="c-36376956">[-]</label><label class="expand" for="c-36376956">[1 more]</label></div><br/><div class="children"><div class="content">MPI partitioning by rank? I’m curious what other solutions there may be.</div><br/></div></div><div id="36377552" class="c"><input type="checkbox" id="c-36377552" checked=""/><div class="controls bullet"><span class="by">pitaj</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376946">parent</a><span>|</span><a href="#36376956">prev</a><span>|</span><a href="#36377617">next</a><span>|</span><label class="collapse" for="c-36377552">[-]</label><label class="expand" for="c-36377552">[1 more]</label></div><br/><div class="children"><div class="content">In rust, you can usually use the rayon library which handles partitioning and scheduling for you.</div><br/></div></div></div></div><div id="36377617" class="c"><input type="checkbox" id="c-36377617" checked=""/><div class="controls bullet"><span class="by">stevefan1999</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376880">parent</a><span>|</span><a href="#36376946">prev</a><span>|</span><a href="#36376873">next</a><span>|</span><label class="collapse" for="c-36377617">[-]</label><label class="expand" for="c-36377617">[1 more]</label></div><br/><div class="children"><div class="content">aka share-nothing architecture</div><br/></div></div></div></div><div id="36376873" class="c"><input type="checkbox" id="c-36376873" checked=""/><div class="controls bullet"><span class="by">bbatha</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376862">parent</a><span>|</span><a href="#36376880">prev</a><span>|</span><a href="#36376897">next</a><span>|</span><label class="collapse" for="c-36376873">[-]</label><label class="expand" for="c-36376873">[1 more]</label></div><br/><div class="children"><div class="content">Other way around mutexes abstract around atomics but in typical implementations will yield to the kernel scheduler fairly. That system is called futexes on Linux. The kernel will also use atomics on its end.</div><br/></div></div><div id="36376897" class="c"><input type="checkbox" id="c-36376897" checked=""/><div class="controls bullet"><span class="by">throwdbaaway</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376862">parent</a><span>|</span><a href="#36376873">prev</a><span>|</span><a href="#36378303">next</a><span>|</span><label class="collapse" for="c-36376897">[-]</label><label class="expand" for="c-36376897">[4 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t share atomics among threads. For example, envoy proxy mostly doesn&#x27;t share atomics among threads, and can scale nicely on arm64 without requiring the atomic extensions.</div><br/><div id="36377054" class="c"><input type="checkbox" id="c-36377054" checked=""/><div class="controls bullet"><span class="by">cdogl</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376897">parent</a><span>|</span><a href="#36378303">next</a><span>|</span><label class="collapse" for="c-36377054">[-]</label><label class="expand" for="c-36377054">[3 more]</label></div><br/><div class="children"><div class="content">Honest question: why would atomics be necessary or useful if data isn’t shared between threads?</div><br/><div id="36377224" class="c"><input type="checkbox" id="c-36377224" checked=""/><div class="controls bullet"><span class="by">NL807</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377054">parent</a><span>|</span><a href="#36377240">next</a><span>|</span><label class="collapse" for="c-36377224">[-]</label><label class="expand" for="c-36377224">[1 more]</label></div><br/><div class="children"><div class="content">Because at some point data has to be exchanged across threads. For example a task queue might have tasks that can independently executed in a thread pool, but the queue index has to be atomically modified when some other thread emplaced a new task. Or if you want to transfer ownership of a heap allocated object between threads, you need to atomically transfer the pointer, or modify the reference count of that pointer. Things like that.</div><br/></div></div><div id="36377240" class="c"><input type="checkbox" id="c-36377240" checked=""/><div class="controls bullet"><span class="by">tormeh</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377054">parent</a><span>|</span><a href="#36377224">prev</a><span>|</span><a href="#36378303">next</a><span>|</span><label class="collapse" for="c-36377240">[-]</label><label class="expand" for="c-36377240">[1 more]</label></div><br/><div class="children"><div class="content">You can and should use atomics, just not in any kind of hot loop. Using atomics is fine but expensive.</div><br/></div></div></div></div></div></div></div></div><div id="36378303" class="c"><input type="checkbox" id="c-36378303" checked=""/><div class="controls bullet"><span class="by">mjan22640</span><span>|</span><a href="#36376824">parent</a><span>|</span><a href="#36376862">prev</a><span>|</span><a href="#36377774">next</a><span>|</span><label class="collapse" for="c-36378303">[-]</label><label class="expand" for="c-36378303">[1 more]</label></div><br/><div class="children"><div class="content">the x86 lock prefix does the same loop, this is the best performant and scaling way to do it</div><br/></div></div><div id="36377774" class="c"><input type="checkbox" id="c-36377774" checked=""/><div class="controls bullet"><span class="by">throwawaylinux</span><span>|</span><a href="#36376824">parent</a><span>|</span><a href="#36378303">prev</a><span>|</span><a href="#36376895">next</a><span>|</span><label class="collapse" for="c-36377774">[-]</label><label class="expand" for="c-36377774">[1 more]</label></div><br/><div class="children"><div class="content">`lock ; xadd` isn&#x27;t really fundamentally different than `ll ; add ; sc; b again`<p>The latter is a bit clunky but the core more or less implements them in the same way. Acquire a line exclusive, load value, increment it, write it back. And you can hold the line exclusive such that the conditional store failure cause is mostly a formality, and can&#x27;t actually become an infinite loop.<p>No general purpose atomics are done by shipping the operation to the cache or to memory controllers, it just doesn&#x27;t work[*]. So even if they look slightly different in the core, they all end up looking exactly the same at the caches and coherency protocols, and <i>that</i> is where atomics are slow. Well any sharing of cache lines updates really.<p>[*] EDIT: That is to say it doesn&#x27;t work for performance, for many reasons. Some CPUs do have &quot;remote atomics&quot; something like that which does exactly this, but they are not intended to be broadly used.</div><br/></div></div><div id="36376895" class="c"><input type="checkbox" id="c-36376895" checked=""/><div class="controls bullet"><span class="by">TedDoesntTalk</span><span>|</span><a href="#36376824">parent</a><span>|</span><a href="#36377774">prev</a><span>|</span><a href="#36378044">next</a><span>|</span><label class="collapse" for="c-36376895">[-]</label><label class="expand" for="c-36376895">[3 more]</label></div><br/><div class="children"><div class="content">What about variables locked with mutexes or semaphores?</div><br/><div id="36377035" class="c"><input type="checkbox" id="c-36377035" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36376895">parent</a><span>|</span><a href="#36378044">next</a><span>|</span><label class="collapse" for="c-36377035">[-]</label><label class="expand" for="c-36377035">[2 more]</label></div><br/><div class="children"><div class="content">locks and mutexes will perform worse than things like atomic increment.</div><br/><div id="36377111" class="c"><input type="checkbox" id="c-36377111" checked=""/><div class="controls bullet"><span class="by">bluGill</span><span>|</span><a href="#36376824">root</a><span>|</span><a href="#36377035">parent</a><span>|</span><a href="#36378044">next</a><span>|</span><label class="collapse" for="c-36377111">[-]</label><label class="expand" for="c-36377111">[1 more]</label></div><br/><div class="children"><div class="content">Maybe. It depends on the algorithm. If you share a lot of data than a mutex, update it all then unlock is fastest. If you share little data atomics can be faster. This is case be case on both the algorithm and hardware, so nobody can say which is better.<p>Of course not sharing at all is of course best, but often you have no choice in that.</div><br/></div></div></div></div></div></div></div></div><div id="36378044" class="c"><input type="checkbox" id="c-36378044" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#36376824">prev</a><span>|</span><a href="#36378075">next</a><span>|</span><label class="collapse" for="c-36378044">[-]</label><label class="expand" for="c-36378044">[2 more]</label></div><br/><div class="children"><div class="content">Ouch. I had no idea that contended Arc could be that expensive.<p>I found a contention bug inside of Wine a few weeks ago. Something that is supposed to be &quot;lockless&quot; really had three nested spinlocks. With many threads contending for a lock, performance would drop to about 1% of normal.[1]<p>[1] <a href="https:&#x2F;&#x2F;bugs.winehq.org&#x2F;show_bug.cgi?id=54979" rel="nofollow noreferrer">https:&#x2F;&#x2F;bugs.winehq.org&#x2F;show_bug.cgi?id=54979</a></div><br/><div id="36378343" class="c"><input type="checkbox" id="c-36378343" checked=""/><div class="controls bullet"><span class="by">lionkor</span><span>|</span><a href="#36378044">parent</a><span>|</span><a href="#36378075">next</a><span>|</span><label class="collapse" for="c-36378343">[-]</label><label class="expand" for="c-36378343">[1 more]</label></div><br/><div class="children"><div class="content">lockless is often the wrong term, goal, idea and solution. mutexes&#x2F;futexes do very well, almost zero cost when not contended.</div><br/></div></div></div></div><div id="36378075" class="c"><input type="checkbox" id="c-36378075" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#36378044">prev</a><span>|</span><a href="#36376994">next</a><span>|</span><label class="collapse" for="c-36378075">[-]</label><label class="expand" for="c-36378075">[1 more]</label></div><br/><div class="children"><div class="content">I feel like using hybrid-rc [1] (biased reference counting [2]) instead of Arc should be more popular. You rarely need to send data between threads so when you do you pay the atomic cost but otherwise you’re doing normal super fast arithmetic.<p>[1] <a href="https:&#x2F;&#x2F;docs.rs&#x2F;hybrid-rc&#x2F;latest&#x2F;hybrid_rc&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.rs&#x2F;hybrid-rc&#x2F;latest&#x2F;hybrid_rc&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3243176.3243195" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3243176.3243195</a></div><br/></div></div><div id="36376994" class="c"><input type="checkbox" id="c-36376994" checked=""/><div class="controls bullet"><span class="by">asicsp</span><span>|</span><a href="#36378075">prev</a><span>|</span><a href="#36376948">next</a><span>|</span><label class="collapse" for="c-36376994">[-]</label><label class="expand" for="c-36376994">[1 more]</label></div><br/><div class="children"><div class="content">Previous discussion: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29747921">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29747921</a> <i>(617 points | Dec 31, 2021 | 195 comments)</i></div><br/></div></div><div id="36376948" class="c"><input type="checkbox" id="c-36376948" checked=""/><div class="controls bullet"><span class="by">lll-o-lll</span><span>|</span><a href="#36376994">prev</a><span>|</span><a href="#36378309">next</a><span>|</span><label class="collapse" for="c-36376948">[-]</label><label class="expand" for="c-36376948">[4 more]</label></div><br/><div class="children"><div class="content">Same problem occurs with c++ std::shared_ptr. I guess all reference counting has this inherent scaling issue due to contention ruining cache lines. Makes me wonder how&#x2F;if you get linear parallelism in Swift.</div><br/><div id="36378413" class="c"><input type="checkbox" id="c-36378413" checked=""/><div class="controls bullet"><span class="by">hayley-patton</span><span>|</span><a href="#36376948">parent</a><span>|</span><a href="#36377911">next</a><span>|</span><label class="collapse" for="c-36378413">[-]</label><label class="expand" for="c-36378413">[1 more]</label></div><br/><div class="children"><div class="content">Coalescing reference counting [0] avoids almost all synchronisation. n.b. The abstract says they do &quot;not require any synchronized operation in its write barrier&quot; but they rely on a micro-architectural hack; in practice I&#x27;d expect one atomic test-and-set per modified object per collection.<p>[0] <a href="https:&#x2F;&#x2F;sites.cs.ucsb.edu&#x2F;~ckrintz&#x2F;racelab&#x2F;gc&#x2F;papers&#x2F;levanoni-on-the-fly-rc.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;sites.cs.ucsb.edu&#x2F;~ckrintz&#x2F;racelab&#x2F;gc&#x2F;papers&#x2F;levanon...</a></div><br/></div></div><div id="36377911" class="c"><input type="checkbox" id="c-36377911" checked=""/><div class="controls bullet"><span class="by">malkia</span><span>|</span><a href="#36376948">parent</a><span>|</span><a href="#36378413">prev</a><span>|</span><a href="#36378429">next</a><span>|</span><label class="collapse" for="c-36377911">[-]</label><label class="expand" for="c-36377911">[1 more]</label></div><br/><div class="children"><div class="content">Not sure what is currently in swift, but this paper described biased reference counting approach - e.g. in way two counters - one non-atomic to be used only by specific thread (supposed owner?), and another (atomic) by all other threads - so the sum of these two shows the real reference count (somewhat). Paper here - <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3243176.3243195" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3243176.3243195</a><p>(Before reading the paper I was expecting that the additional bytes were put for the split counter, plus thread id - but it actually packs them using lower bits for reference counting).<p>I wonder what abseil&#x2F;folly&#x2F;tbb do - need to check (we are heavy std::shared_ptr users, but I don&#x27;t think 14 bits as described in the paper above would be enough for our use case)</div><br/></div></div><div id="36378429" class="c"><input type="checkbox" id="c-36378429" checked=""/><div class="controls bullet"><span class="by">FpUser</span><span>|</span><a href="#36376948">parent</a><span>|</span><a href="#36377911">prev</a><span>|</span><a href="#36378309">next</a><span>|</span><label class="collapse" for="c-36378429">[-]</label><label class="expand" for="c-36378429">[1 more]</label></div><br/><div class="children"><div class="content">There are no miracles here because it is not a language &quot;feature&quot;. It is a property of algorithms. When you divide your large task into parts and schedule execution of those on multiple threads make absolutely sure that there is no locking (atomics are locking) happening inside each individual task.</div><br/></div></div></div></div><div id="36378309" class="c"><input type="checkbox" id="c-36378309" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#36376948">prev</a><span>|</span><a href="#36377363">next</a><span>|</span><label class="collapse" for="c-36378309">[-]</label><label class="expand" for="c-36378309">[1 more]</label></div><br/><div class="children"><div class="content">Reminder: refcounting is garbage collection. There are parallelism friendly GCs too. I wonder if the same interface in Rust could accommodate them.</div><br/></div></div><div id="36377363" class="c"><input type="checkbox" id="c-36377363" checked=""/><div class="controls bullet"><span class="by">malkia</span><span>|</span><a href="#36378309">prev</a><span>|</span><a href="#36378338">next</a><span>|</span><label class="collapse" for="c-36377363">[-]</label><label class="expand" for="c-36377363">[2 more]</label></div><br/><div class="children"><div class="content">in C++ std::shared_ptr (similar story) has similar effect. One of our applications (3D editor) went way slower when an artist were given a server-class machine (NUMA) and we had to ensure that all threads would run on a single CPU socket (yes they were still accessing the &quot;other&quot; memory, but it was better somehow).</div><br/><div id="36378040" class="c"><input type="checkbox" id="c-36378040" checked=""/><div class="controls bullet"><span class="by">BazookaMusic</span><span>|</span><a href="#36377363">parent</a><span>|</span><a href="#36378338">next</a><span>|</span><label class="collapse" for="c-36378040">[-]</label><label class="expand" for="c-36378040">[1 more]</label></div><br/><div class="children"><div class="content">I might be wrong on this explanation, but the reason why it was faster might have been the following:<p>During execution you had two kinds of memory locations, some in CPU caches and some in RAM. By running all the threads on one socket, everything accessed from the cache was just a fast cache access. Everything accessed from the memory was a slower memory load. Frequently loaded&#x2F;stored locations will tend to go to the cache.<p>In the NUMA setup, you would have a larger cache (more than one socket) which would mean that more locations were likely to be in the cache. However, if a core on a socket tries to access a location which is on another socket&#x27;s cache, it will use the interconnect between them to access it.<p>If you have an unfortunate memory layout, this can make it so that you end up having a large percentage of the accesses using the interconnect (slower than cache access) and values get swapped between the caches constantly, which forces subsequent accesses to also use the interconnect.<p>Another way to avoid this except using just one socket is for the designer of a program to consider NUMA nodes as separate processing units and design around that. Both should be processing separate data and they should only share small amounts of data for synchronization&#x2F;communication. Then the caches will be much less affected.</div><br/></div></div></div></div><div id="36378338" class="c"><input type="checkbox" id="c-36378338" checked=""/><div class="controls bullet"><span class="by">lionkor</span><span>|</span><a href="#36377363">prev</a><span>|</span><a href="#36378051">next</a><span>|</span><label class="collapse" for="c-36378338">[-]</label><label class="expand" for="c-36378338">[1 more]</label></div><br/><div class="children"><div class="content">Arc and shared_ptr should be used sparingly - especially in languages with more or less real ownership semantics, sharing ownership of state seems like a hack job.</div><br/></div></div><div id="36378051" class="c"><input type="checkbox" id="c-36378051" checked=""/><div class="controls bullet"><span class="by">OscarTheGrinch</span><span>|</span><a href="#36378338">prev</a><span>|</span><a href="#36377814">next</a><span>|</span><label class="collapse" for="c-36378051">[-]</label><label class="expand" for="c-36378051">[3 more]</label></div><br/><div class="children"><div class="content">Hey, noob question guy here. Can anyone explain why the last graph shows a slight performance drop going from 48 to 96 threads?</div><br/><div id="36378073" class="c"><input type="checkbox" id="c-36378073" checked=""/><div class="controls bullet"><span class="by">frankreyes</span><span>|</span><a href="#36378051">parent</a><span>|</span><a href="#36378062">next</a><span>|</span><label class="collapse" for="c-36378073">[-]</label><label class="expand" for="c-36378073">[1 more]</label></div><br/><div class="children"><div class="content">It can be many things, my guess is likely memory bandwidth. There&#x27;s so much MB&#x2F;s the RAM can handle. Also, above 48 cores, those are hyper threading and for CPU bound tasks, hyper threading is known to be slower.<p>Example: <a href="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;7804711" rel="nofollow noreferrer">https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;7804711</a><p>Edit: looks like there&#x27;s only 12 cores per CPU so that&#x27;s 24 physical cores. 48 HT cores. So the drop must be cache trashing?</div><br/></div></div><div id="36378062" class="c"><input type="checkbox" id="c-36378062" checked=""/><div class="controls bullet"><span class="by">kovacs_x</span><span>|</span><a href="#36378051">parent</a><span>|</span><a href="#36378073">prev</a><span>|</span><a href="#36377814">next</a><span>|</span><label class="collapse" for="c-36378062">[-]</label><label class="expand" for="c-36378062">[1 more]</label></div><br/><div class="children"><div class="content">96 processes on 24 cores, not 96 cores.<p>apparently 2 process per core is more efficient than 4.</div><br/></div></div></div></div><div id="36377814" class="c"><input type="checkbox" id="c-36377814" checked=""/><div class="controls bullet"><span class="by">alexott</span><span>|</span><a href="#36378051">prev</a><span>|</span><a href="#36377819">next</a><span>|</span><label class="collapse" for="c-36377814">[-]</label><label class="expand" for="c-36377814">[1 more]</label></div><br/><div class="children"><div class="content">Not directly related, but <a href="https:&#x2F;&#x2F;github.com&#x2F;nosqlbench&#x2F;nosqlbench">https:&#x2F;&#x2F;github.com&#x2F;nosqlbench&#x2F;nosqlbench</a> is very flexible benchmark tool for Cassandra and other distributed systems</div><br/></div></div><div id="36377819" class="c"><input type="checkbox" id="c-36377819" checked=""/><div class="controls bullet"><span class="by">eldenring</span><span>|</span><a href="#36377814">prev</a><span>|</span><a href="#36377864">next</a><span>|</span><label class="collapse" for="c-36377819">[-]</label><label class="expand" for="c-36377819">[1 more]</label></div><br/><div class="children"><div class="content">Good article, but if i had to guess the subsequent L3 cache access after an increment is likely far overshadowed by the overhead of coherence messages, or the cores communicating between each other.</div><br/></div></div><div id="36377864" class="c"><input type="checkbox" id="c-36377864" checked=""/><div class="controls bullet"><span class="by">VectorLock</span><span>|</span><a href="#36377819">prev</a><span>|</span><a href="#36378279">next</a><span>|</span><label class="collapse" for="c-36377864">[-]</label><label class="expand" for="c-36377864">[3 more]</label></div><br/><div class="children"><div class="content">This is what I&#x27;d describe as the deepest of the deep magic.</div><br/><div id="36378276" class="c"><input type="checkbox" id="c-36378276" checked=""/><div class="controls bullet"><span class="by">hahhahanananana</span><span>|</span><a href="#36377864">parent</a><span>|</span><a href="#36378097">next</a><span>|</span><label class="collapse" for="c-36378276">[-]</label><label class="expand" for="c-36378276">[1 more]</label></div><br/><div class="children"><div class="content">Would love to see someone recommend some beginner-friendly books to learn more about the theory behind this blogpost. Seems like a CS degree is the only straightforward way.</div><br/></div></div><div id="36378097" class="c"><input type="checkbox" id="c-36378097" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36377864">parent</a><span>|</span><a href="#36378276">prev</a><span>|</span><a href="#36378279">next</a><span>|</span><label class="collapse" for="c-36378097">[-]</label><label class="expand" for="c-36378097">[1 more]</label></div><br/><div class="children"><div class="content">Oh, the rabbit hole goes far deeper than this.</div><br/></div></div></div></div><div id="36378279" class="c"><input type="checkbox" id="c-36378279" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#36377864">prev</a><span>|</span><a href="#36377598">next</a><span>|</span><label class="collapse" for="c-36378279">[-]</label><label class="expand" for="c-36378279">[1 more]</label></div><br/><div class="children"><div class="content">Also 24-core servers from cloud services usually run your app slower than a laptop anyway.</div><br/></div></div><div id="36377598" class="c"><input type="checkbox" id="c-36377598" checked=""/><div class="controls bullet"><span class="by">Sniffnoy</span><span>|</span><a href="#36378279">prev</a><span>|</span><a href="#36378402">next</a><span>|</span><label class="collapse" for="c-36377598">[-]</label><label class="expand" for="c-36377598">[2 more]</label></div><br/><div class="children"><div class="content">(2021)</div><br/><div id="36377788" class="c"><input type="checkbox" id="c-36377788" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36377598">parent</a><span>|</span><a href="#36378402">next</a><span>|</span><label class="collapse" for="c-36377788">[-]</label><label class="expand" for="c-36377788">[1 more]</label></div><br/><div class="children"><div class="content">Updated</div><br/></div></div></div></div></div></div></div></div></div></body></html>