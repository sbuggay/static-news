<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719997253630" as="style"/><link rel="stylesheet" href="styles.css?v=1719997253630"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://medicalxpress.com/news/2024-06-reveals-ai-medical-images-biased.html">Study reveals why AI models that analyze medical images can be biased</a> <span class="domain">(<a href="https://medicalxpress.com">medicalxpress.com</a>)</span></div><div class="subtext"><span>wglb</span> | <span>30 comments</span></div><br/><div><div id="40863680" class="c"><input type="checkbox" id="c-40863680" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#40862678">next</a><span>|</span><label class="collapse" for="c-40863680">[-]</label><label class="expand" for="c-40863680">[2 more]</label></div><br/><div class="children"><div class="content">I’m suspicious that there’s another factor in play: images being correctly labeled in the training set. Even from high-end hospitals, my personal experience leads me to believe that radiologists make both major types of error on a regular basis: calling out abnormalities in an image that are entirely irrelevant and missing actual relevant problems in an image that are subsequently easily seen by a doctor who is aware of what seems to be actually wrong.  To top it off, sure many patients end up being diagnosed with whatever the radiologist saw, and no one ever confirms that the diagnosis was really correct. (And how could they?  A lot of conditions are hard to diagnose!)<p>So the training data is probably hugely biased, and the models will learn to predict the training labels as opposed to any magically correct “ground truth”. And internally detecting demographics and producing an output biased by demographics may well result in a better match to the training data than a perfect, unbiased output would be.</div><br/><div id="40864044" class="c"><input type="checkbox" id="c-40864044" checked=""/><div class="controls bullet"><span class="by">resource_waste</span><span>|</span><a href="#40863680">parent</a><span>|</span><a href="#40862678">next</a><span>|</span><label class="collapse" for="c-40864044">[-]</label><label class="expand" for="c-40864044">[1 more]</label></div><br/><div class="children"><div class="content">Yeah this is actually it.<p>Its much easier for physicians to blame technology than their profession.</div><br/></div></div></div></div><div id="40862678" class="c"><input type="checkbox" id="c-40862678" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#40863680">prev</a><span>|</span><a href="#40862944">next</a><span>|</span><label class="collapse" for="c-40862678">[-]</label><label class="expand" for="c-40862678">[2 more]</label></div><br/><div class="children"><div class="content">I have not been able to fully digest this paper yet, and medical data is not my speciality. It is interesting but not surprising that models appear able to determine the demographics of the patient that even radiologists are unable to. It is also not surprising that models use this to &quot;cheat&quot; (find demographic shortcuts in disease classification).<p>My understanding is that doctors may unconsciously do this as well, ignoring a possible diagnosis because they don&#x27;t expect a patient of a certain demographic to have a particular issue.<p>I would expect radiologists who practice in very different demographic environments would not do as well when evaluating images another environment.<p>At the end of the day radiology is more an art than a science, so the training data may well be faulty. Krupinski (2010) wrote in an interesting paper [1]:<p>&quot;Medical images need to be interpreted because they are not self-explanatory... In radiology alone, estimates suggest that, in some areas, there may be up to a 30% miss rate and an equally high false positive rate ... interpretation errors can be caused by a host of psychophysical processes ... radiologists are less accurate after a day of reading diagnostic images and that their ability to focus on the display screen is reduced because of myopia. &quot;<p>I would hope datasets included a substantial amount of images that were originally mis-classified as a human.<p>[1] <a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3881280&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3881280&#x2F;</a></div><br/><div id="40862983" class="c"><input type="checkbox" id="c-40862983" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#40862678">parent</a><span>|</span><a href="#40862944">next</a><span>|</span><label class="collapse" for="c-40862983">[-]</label><label class="expand" for="c-40862983">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not too hard to train yourself to identify sex or approximate age. (I claim this because of time spent reviewing model output for work that I have done to build models to estimate age.) The reason radiologists don&#x27;t do this is that there is no clinical reason to do it, so it&#x27;s not a skill that they develop. (Deep neural networks can also beat radiologists at bird classification, another task that is not relevant for their job.)</div><br/></div></div></div></div><div id="40862944" class="c"><input type="checkbox" id="c-40862944" checked=""/><div class="controls bullet"><span class="by">ogarten</span><span>|</span><a href="#40862678">prev</a><span>|</span><a href="#40862956">next</a><span>|</span><label class="collapse" for="c-40862944">[-]</label><label class="expand" for="c-40862944">[5 more]</label></div><br/><div class="children"><div class="content">How does this surprise anyone?<p>Medical data for AI training is almost always sources in some more or less shady country because they lack any privacy regulations. It&#x27;s then annotated by a hoard of cheap workers who may or may not have advanced medical training.<p>Even &quot;normal medicine&quot; is extremely biased towards male people fitting inside the norm which is why a lot of things are not detected early enough in women or in people who do not match that norm.<p>Next thing: Doctors often think that their annotations are the absolute gold standard but they don&#x27;t necessarily know everything that is in an X-Ray or an MRI.<p>A few years ago we tried to build synthetic data for this exact purpose by <i>simulating</i> medical images for 3D body models with different diseases and nobody we talked to cared about it, because &quot;we have good data&quot;.</div><br/><div id="40863342" class="c"><input type="checkbox" id="c-40863342" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#40862944">parent</a><span>|</span><a href="#40863093">next</a><span>|</span><label class="collapse" for="c-40863342">[-]</label><label class="expand" for="c-40863342">[1 more]</label></div><br/><div class="children"><div class="content">I know GPT4o can diagnose medical images. Is their model likely to be using the same kind of datasets as these models for medical systems?</div><br/></div></div><div id="40863093" class="c"><input type="checkbox" id="c-40863093" checked=""/><div class="controls bullet"><span class="by">aprilthird2021</span><span>|</span><a href="#40862944">parent</a><span>|</span><a href="#40863342">prev</a><span>|</span><a href="#40864062">next</a><span>|</span><label class="collapse" for="c-40863093">[-]</label><label class="expand" for="c-40863093">[2 more]</label></div><br/><div class="children"><div class="content">Yep, you nailed it. You really don&#x27;t have to think hard about why AI which only learns from what we feed it and can access has gaps and biases more pronounced than the real world. AI lives in the internet world, it&#x27;s trained on horrible cesspools of anonymous text like 4chan and reddit. No wonder it will be biased. If you only try to feed it sanitary data you wouldn&#x27;t have enough to get the results we get now.</div><br/><div id="40864069" class="c"><input type="checkbox" id="c-40864069" checked=""/><div class="controls bullet"><span class="by">DrScientist</span><span>|</span><a href="#40862944">root</a><span>|</span><a href="#40863093">parent</a><span>|</span><a href="#40864062">next</a><span>|</span><label class="collapse" for="c-40864069">[-]</label><label class="expand" for="c-40864069">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You really don&#x27;t have to think hard about why AI which only learns from what we feed it<p>Sadly I&#x27;d say that people are no different.<p>&gt;  it&#x27;s trained on horrible cesspools of ....<p>So it&#x27;s really not the future of AI we should be worrying about...</div><br/></div></div></div></div><div id="40864062" class="c"><input type="checkbox" id="c-40864062" checked=""/><div class="controls bullet"><span class="by">resource_waste</span><span>|</span><a href="#40862944">parent</a><span>|</span><a href="#40863093">prev</a><span>|</span><a href="#40862956">next</a><span>|</span><label class="collapse" for="c-40864062">[-]</label><label class="expand" for="c-40864062">[1 more]</label></div><br/><div class="children"><div class="content">I have been quite anti-HIPPA since realizing how &#x27;privacy&#x27; was the excuse to stunt science.<p>My conspiracy: With massive medical data, ML&#x2F;AI would have been &#x27;discovered&#x27;&#x2F;built sooner. Limiting the data makes it so only a few people can be specialists under the supervision of medical cartels.</div><br/></div></div></div></div><div id="40862956" class="c"><input type="checkbox" id="c-40862956" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#40862944">prev</a><span>|</span><a href="#40862668">next</a><span>|</span><label class="collapse" for="c-40862956">[-]</label><label class="expand" for="c-40862956">[4 more]</label></div><br/><div class="children"><div class="content">The authors refer to a literature describing &quot;shortcuts&quot; as &quot;<i>correlations that are present in the data but have no real clinical basis, for instance deep models using the hospital as a shortcut for disease prediction</i>&quot;. It feels like a parallel language is developing. Most of us would call such a phenomenon &quot;overfitting&quot; or describe some specific issue with generalization. That example is not a shortcut in any normal sense of the word unless you are providing the hospital via some extra path.<p>They call demographics like age and sex &quot;shortcuts&quot; but I find this to be a frustrating term since it seems to obscure what&#x27;s happening under the hood. (They cite many papers using the same word, so I&#x27;m not blaming them for this usage.) Men are typically larger; old bones do not look like young bones. There is plenty of biology involved in what they refer to as demographic shortcuts.<p>I think you could take the same results and say &quot;Models are able to distinguish men from women. For our purposes, it&#x27;s important that they cannot do this. Therefore, we did XYZ on these weakly labeled public databases.&quot; But perhaps that sounds less exciting.</div><br/><div id="40863007" class="c"><input type="checkbox" id="c-40863007" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#40862956">parent</a><span>|</span><a href="#40862668">next</a><span>|</span><label class="collapse" for="c-40863007">[-]</label><label class="expand" for="c-40863007">[3 more]</label></div><br/><div class="children"><div class="content">This is not overfitting precisely because of the bias&#x2F;variance tradeoff.<p>A model overfits if it is unnecessarily COMPLEX for the training data.<p>If there is bias in the training (and validation and test) data that allows a SIMPLE model to fit the data because of a spurious correlation, that is not overfitting.</div><br/><div id="40863033" class="c"><input type="checkbox" id="c-40863033" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#40862956">root</a><span>|</span><a href="#40863007">parent</a><span>|</span><a href="#40862668">next</a><span>|</span><label class="collapse" for="c-40863033">[-]</label><label class="expand" for="c-40863033">[2 more]</label></div><br/><div class="children"><div class="content">There is no reason to believe that an X-ray model&#x27;s correct estimation of age is due to &quot;spurious correlation&quot;. Rather, it seems to be &quot;undesirable correlation&quot;.</div><br/><div id="40864039" class="c"><input type="checkbox" id="c-40864039" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40862956">root</a><span>|</span><a href="#40863033">parent</a><span>|</span><a href="#40862668">next</a><span>|</span><label class="collapse" for="c-40864039">[-]</label><label class="expand" for="c-40864039">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Rather, it seems to be &quot;undesirable correlation&quot;.</i><p>More specifically, <i>politically undesirable</i> correlation - as in, &quot;it&#x27;s there, but its existence upsets some people&quot;. It&#x27;s pretty obvious and self-evident that there are meaningful biological differences related to age, sex, and other demographics. Whether or not they&#x27;re clinically relevant for a specific diagnosis under question is one thing, but they <i>are</i> clinically relevant for great many diagnoses; trying to &quot;de-bias&quot; reality here will only lead to unnecessary suffering and loss of life.</div><br/></div></div></div></div></div></div></div></div><div id="40862668" class="c"><input type="checkbox" id="c-40862668" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#40862956">prev</a><span>|</span><a href="#40862723">next</a><span>|</span><label class="collapse" for="c-40862668">[-]</label><label class="expand" for="c-40862668">[4 more]</label></div><br/><div class="children"><div class="content">```
The researchers also found that they could retrain the models in a way that improves their fairness. However, their approach to &quot;debiasing&quot; worked best when the models were tested on the same types of patients on whom they were trained, such as patients from the same hospital. When these models were applied to patients from different hospitals, the fairness gaps reappeared.<p>&quot;I think the main takeaways are, first, you should thoroughly evaluate any external models on your own data because any fairness guarantees that model developers provide on their training data may not transfer to your population. Second, whenever sufficient data is available, you should train models on your own data,&quot; says Haoran Zhang, an MIT graduate student and one of the lead authors of the new paper.
```<p>This is just overfitting. Why are they training whole models on only one hospital worth of data when they appear to have access to five? They should be training on all of the data in the world they can get their hands on then maybe fine tuning on their specific hospital (maybe they have higher quality outcomes data that verifies the readings) if there are still accuracy issues. The last five years have taught us that gobbling up everything (even if it&#x27;s not the best quality) is the way.</div><br/><div id="40862794" class="c"><input type="checkbox" id="c-40862794" checked=""/><div class="controls bullet"><span class="by">hdhshdhshdjd</span><span>|</span><a href="#40862668">parent</a><span>|</span><a href="#40862723">next</a><span>|</span><label class="collapse" for="c-40862794">[-]</label><label class="expand" for="c-40862794">[3 more]</label></div><br/><div class="children"><div class="content">It could be an instrumentation issue, different hospitals use different machines.</div><br/><div id="40862811" class="c"><input type="checkbox" id="c-40862811" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#40862668">root</a><span>|</span><a href="#40862794">parent</a><span>|</span><a href="#40862723">next</a><span>|</span><label class="collapse" for="c-40862811">[-]</label><label class="expand" for="c-40862811">[2 more]</label></div><br/><div class="children"><div class="content">Likely this and radiologist practices and storage methods and patient demographics and a bunch of other things. However, if they&#x27;re training these models with the intention of using them in other hospitals I&#x27;d still define that as overfitting.</div><br/></div></div></div></div></div></div><div id="40862723" class="c"><input type="checkbox" id="c-40862723" checked=""/><div class="controls bullet"><span class="by">wiradikusuma</span><span>|</span><a href="#40862668">prev</a><span>|</span><a href="#40863943">next</a><span>|</span><label class="collapse" for="c-40862723">[-]</label><label class="expand" for="c-40862723">[10 more]</label></div><br/><div class="children"><div class="content">Instead of trying to make the model &quot;fair&quot;, can we do &quot;model = models.getByRace(x)&quot; so we have optimized model for each, instead of being &quot;jack of all trades&quot;?</div><br/><div id="40862756" class="c"><input type="checkbox" id="c-40862756" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#40862723">parent</a><span>|</span><a href="#40863972">next</a><span>|</span><label class="collapse" for="c-40862756">[-]</label><label class="expand" for="c-40862756">[3 more]</label></div><br/><div class="children"><div class="content">This paper covers many demographics, not just ethnicity. So you are wanting to really do &quot;model = models.get(a, b, x, y, z)&quot;. It may be possible, but with medical data in such short supply, you would be lucky to get a large enough dataset.<p>You would also need to assume there no overlap in the distribution in the training data for each strata of training data. I would imagine that if you taught someone to detect cancer in x-rays, and only used images of people aged 80+, you might still have some success (&gt;0.5) at detecting cancer in people aged 20-30.</div><br/><div id="40862778" class="c"><input type="checkbox" id="c-40862778" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40862723">root</a><span>|</span><a href="#40862756">parent</a><span>|</span><a href="#40863972">next</a><span>|</span><label class="collapse" for="c-40862778">[-]</label><label class="expand" for="c-40862778">[2 more]</label></div><br/><div class="children"><div class="content">This seems like a place where generative augmentation could help - generate an image of a person with demographic values x, y, z and an early stage lung cancer. Then you can generate images for more corners of the days, more evenly, and train the classifier on less biased data.</div><br/><div id="40862816" class="c"><input type="checkbox" id="c-40862816" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#40862723">root</a><span>|</span><a href="#40862778">parent</a><span>|</span><a href="#40863972">next</a><span>|</span><label class="collapse" for="c-40862816">[-]</label><label class="expand" for="c-40862816">[1 more]</label></div><br/><div class="children"><div class="content">If your model already understand the distribution of the data (fundamentally this is what statistical models are trying to discover) well enough for generative models then they probably can do classification well enough.<p>The greater issue is that, for privacy and other reasons, there are many demographics that are not represented at all, or under represented, in medical image datasets.</div><br/></div></div></div></div></div></div><div id="40863972" class="c"><input type="checkbox" id="c-40863972" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#40862723">parent</a><span>|</span><a href="#40862756">prev</a><span>|</span><a href="#40862876">next</a><span>|</span><label class="collapse" for="c-40863972">[-]</label><label class="expand" for="c-40863972">[1 more]</label></div><br/><div class="children"><div class="content">It seems like the problem is the entire premise.<p>Suppose that older people are more likely to get cancer, so older people with cancer are more represented in the training data. Then we discover that it has a 20% false negative rate for older people but a 35% false negative rate for younger people, i.e. it has a better handle on what cancer looks like in older people. This is fairly intrinsic, the only <i>real</i> way to fix it is to provide it with more samples of images of younger people with cancer, which may not be available.<p>But you can also &quot;fix&quot; it by raising the false negative rate for older people to 35%. Doing this is idiotic and should not be done, but it allows people to claim that the model is now &quot;fair&quot;, and so they will have the incentive to do things like that as long as we continue to demand &quot;fairness&quot; above other considerations.<p>Moreover, you&#x27;re likely to see similar effects with all kinds of other groups. Maybe farm workers are more likely to get skin cancer because they spend more time in the sun, and you could see the same kind of disparity in its ability to detect cancer in farm workers because the effects of doing farm work also have other long-term physical correlates that show up in the image. Probably nobody is studying this because farm workers are not a protected class, but it&#x27;s just a thing that happens whenever you divide the population into distinct groups. That doesn&#x27;t mean there is inherently anything to be done about it. It&#x27;s a facet of what data is available.</div><br/></div></div><div id="40862876" class="c"><input type="checkbox" id="c-40862876" checked=""/><div class="controls bullet"><span class="by">acheong08</span><span>|</span><a href="#40862723">parent</a><span>|</span><a href="#40863972">prev</a><span>|</span><a href="#40862887">next</a><span>|</span><label class="collapse" for="c-40862876">[-]</label><label class="expand" for="c-40862876">[1 more]</label></div><br/><div class="children"><div class="content">Race doesn’t tell you much about a patient’s possible afflictions. While there may be some predispositions, there is also a lot more nuance than the color of their skin. Environmental factors play a much larger role (e.g “Chinese” Malaysians are much more likely to have joint issues due to high humidity and overuse of ACs)</div><br/></div></div><div id="40862887" class="c"><input type="checkbox" id="c-40862887" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#40862723">parent</a><span>|</span><a href="#40862876">prev</a><span>|</span><a href="#40862909">next</a><span>|</span><label class="collapse" for="c-40862887">[-]</label><label class="expand" for="c-40862887">[1 more]</label></div><br/><div class="children"><div class="content">You can just do one big model with static enrichment (effectively throw &quot;race&quot; into the model as a feature) and achieve the same thing. But the quality will always be worse for some sub-populations due to different amounts of available data. There&#x27;s no easy way to fix that.</div><br/></div></div><div id="40862909" class="c"><input type="checkbox" id="c-40862909" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#40862723">parent</a><span>|</span><a href="#40862887">prev</a><span>|</span><a href="#40862759">next</a><span>|</span><label class="collapse" for="c-40862909">[-]</label><label class="expand" for="c-40862909">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not how this works. If the model does something like &quot;this image is of race X, race X has more lung cancers, therefore this image is likely lung cancer&quot;, then it&#x27;s not helpful to anyone. You need the model to evaluate based on the image itself, not based on correlations that it can infer from the image - fairness is just a happy byproduct of that.</div><br/><div id="40863010" class="c"><input type="checkbox" id="c-40863010" checked=""/><div class="controls bullet"><span class="by">chaorace</span><span>|</span><a href="#40862723">root</a><span>|</span><a href="#40862909">parent</a><span>|</span><a href="#40862759">next</a><span>|</span><label class="collapse" for="c-40863010">[-]</label><label class="expand" for="c-40863010">[1 more]</label></div><br/><div class="children"><div class="content">I think what the parent comment meant was that you could force the model to divert its attention elsewhere if you removed race as a variable by making the training data uniform in terms of race. I think it&#x27;s a smart thought, though I doubt it&#x27;d work due to the fuzziness of &quot;race&quot; as a construct. Even if you grouped people using some combination of their self-classified and&#x2F;or observed racial identity, the model would probably start identifying (<i>and thus start cheating using</i>) even subtler &quot;sub-racial&quot; biomarkers.<p>If you ask me, it&#x27;s probably more effective to compensate for the model&#x27;s learned racial bias using weights derived from the model outputs via statistical analysis.</div><br/></div></div></div></div><div id="40862759" class="c"><input type="checkbox" id="c-40862759" checked=""/><div class="controls bullet"><span class="by">worthless-trash</span><span>|</span><a href="#40862723">parent</a><span>|</span><a href="#40862909">prev</a><span>|</span><a href="#40863943">next</a><span>|</span><label class="collapse" for="c-40862759">[-]</label><label class="expand" for="c-40862759">[1 more]</label></div><br/><div class="children"><div class="content">Is race really the biggest differentiator in model bias ? I would have imagined that environmental factors would have been the largest  impact in most models bias.<p>I guess that these factors are harder to prove though.</div><br/></div></div></div></div><div id="40863943" class="c"><input type="checkbox" id="c-40863943" checked=""/><div class="controls bullet"><span class="by">adammarples</span><span>|</span><a href="#40862723">prev</a><span>|</span><a href="#40863838">next</a><span>|</span><label class="collapse" for="c-40863943">[-]</label><label class="expand" for="c-40863943">[1 more]</label></div><br/><div class="children"><div class="content">Is it just me or did the article at no point explain why medical models produce biased results? I seems to take this as a given, and an uninteresting one at that, and focuses on trying to correct it, without explaining why it happens in the first place. Yes, those models could use race as a shortcut to, presumably, not diagnose cancer in black people for example, but why is the race shortcut boosting model training accuracy? I am still none the wiser.</div><br/></div></div></div></div></div></div></div></body></html>