<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726304449836" as="style"/><link rel="stylesheet" href="styles.css?v=1726304449836"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arcprize.org/blog/openai-o1-results-arc-prize">OpenAI o1 Results on ARC-AGI-Pub</a> <span class="domain">(<a href="https://arcprize.org">arcprize.org</a>)</span></div><div class="subtext"><span>z7</span> | <span>79 comments</span></div><br/><div><div id="41536806" class="c"><input type="checkbox" id="c-41536806" checked=""/><div class="controls bullet"><span class="by">meowface</span><span>|</span><a href="#41537209">next</a><span>|</span><label class="collapse" for="c-41536806">[-]</label><label class="expand" for="c-41536806">[6 more]</label></div><br/><div class="children"><div class="content">Takeaway:<p>&gt;o1-preview is about on par with Anthropic&#x27;s Claude 3.5 Sonnet in terms of accuracy but takes about 10X longer to achieve similar results to Sonnet.<p>Scores:<p>&gt;GPT-4o: 9%<p>&gt;o1-preview: 21%<p>&gt;Claude 3.5 Sonnet: 21%<p>&gt;MindsAI: 46% (current highest score)</div><br/><div id="41536830" class="c"><input type="checkbox" id="c-41536830" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#41536806">parent</a><span>|</span><a href="#41538288">next</a><span>|</span><label class="collapse" for="c-41536830">[-]</label><label class="expand" for="c-41536830">[1 more]</label></div><br/><div class="children"><div class="content">There were rumors that 3.5 Sonnet heavily used synthetic data for training, in the same way that OpenAI plans to use o1 to train Orion. Maybe this confirm it?</div><br/></div></div><div id="41538288" class="c"><input type="checkbox" id="c-41538288" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#41536806">parent</a><span>|</span><a href="#41536830">prev</a><span>|</span><a href="#41536873">next</a><span>|</span><label class="collapse" for="c-41538288">[-]</label><label class="expand" for="c-41538288">[1 more]</label></div><br/><div class="children"><div class="content">who knows what MindsAI is?</div><br/></div></div><div id="41536873" class="c"><input type="checkbox" id="c-41536873" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41536806">parent</a><span>|</span><a href="#41538288">prev</a><span>|</span><a href="#41537209">next</a><span>|</span><label class="collapse" for="c-41536873">[-]</label><label class="expand" for="c-41536873">[3 more]</label></div><br/><div class="children"><div class="content">The takeaway is also that o1-preview is a major improvement compare to GPT-4o.<p>Anthropic is just ahead.</div><br/><div id="41538111" class="c"><input type="checkbox" id="c-41538111" checked=""/><div class="controls bullet"><span class="by">disgruntledphd2</span><span>|</span><a href="#41536806">root</a><span>|</span><a href="#41536873">parent</a><span>|</span><a href="#41537040">next</a><span>|</span><label class="collapse" for="c-41538111">[-]</label><label class="expand" for="c-41538111">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a little embarrassing for OpenAI though?</div><br/></div></div><div id="41537040" class="c"><input type="checkbox" id="c-41537040" checked=""/><div class="controls bullet"><span class="by">meowface</span><span>|</span><a href="#41536806">root</a><span>|</span><a href="#41536873">parent</a><span>|</span><a href="#41538111">prev</a><span>|</span><a href="#41537209">next</a><span>|</span><label class="collapse" for="c-41537040">[-]</label><label class="expand" for="c-41537040">[1 more]</label></div><br/><div class="children"><div class="content">True. I&#x27;ve updated my post to include some of the scores.</div><br/></div></div></div></div></div></div><div id="41537209" class="c"><input type="checkbox" id="c-41537209" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#41536806">prev</a><span>|</span><a href="#41537162">next</a><span>|</span><label class="collapse" for="c-41537209">[-]</label><label class="expand" for="c-41537209">[19 more]</label></div><br/><div class="children"><div class="content">In my opinion this blog post is a little bit misleading about the difference between o1 and earlier models. When I first heard about ARC-AGI (a few months ago, I think) I took a few of the ARC tasks and spent a few hours testing all the most powerful models. I was kind of surprised by how completely the models fell on their faces, even with heavy-handed feedback and various prompting techniques. None of the models came close to solving even the easiest puzzles. So today I tried again with o1-preview, and the model solved (probably the easiest) puzzle without any kind of fancy prompting:<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e4b209-8d98-8011-a0c7-b354a68fabca" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e4b209-8d98-8011-a0c7-b354a68fab...</a><p>Anyways, I&#x27;m not trying to make any grand claims about AGI in general, or about ARC-AGI as a benchmark, but I do think that o1 is a leap towards LLM-based solutions to ARC.</div><br/><div id="41537375" class="c"><input type="checkbox" id="c-41537375" checked=""/><div class="controls bullet"><span class="by">kobe_bryant</span><span>|</span><a href="#41537209">parent</a><span>|</span><a href="#41537669">next</a><span>|</span><label class="collapse" for="c-41537375">[-]</label><label class="expand" for="c-41537375">[6 more]</label></div><br/><div class="children"><div class="content">So it gives you the wrong answer and then you keep telling it how to fix it until it does? What does fancy prompting look like then, just feeding it the solution piece by piece?</div><br/><div id="41537563" class="c"><input type="checkbox" id="c-41537563" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537375">parent</a><span>|</span><a href="#41537669">next</a><span>|</span><label class="collapse" for="c-41537563">[-]</label><label class="expand" for="c-41537563">[5 more]</label></div><br/><div class="children"><div class="content">Basically yes, but there&#x27;s a very wide range of how explicit the feedback could be. Here&#x27;s an example where I tell gpt-4 exactly what the rule is and it still fails:<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e514d3-ca0c-8011-8d1e-43234391a031" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e514d3-ca0c-8011-8d1e-43234391a0...</a><p>and an example using gpt-4o:<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e515da-a848-8011-987f-71dab56446f0" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e515da-a848-8011-987f-71dab56446...</a><p>I&#x27;d share similar examples using claude-3.5-sonnet but I can&#x27;t figure out how to do it from the claud.ai ui.<p>To be clear, my point is not at all that o1 is so incredibly smart. IMO the ARC-AGI puzzles show very clearly how dumb even the most advanced models are. My point is just that o1 does seem to be noticeably better at solving these problems than previous models.</div><br/><div id="41537658" class="c"><input type="checkbox" id="c-41537658" checked=""/><div class="controls bullet"><span class="by">seaal</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537563">parent</a><span>|</span><a href="#41537669">next</a><span>|</span><label class="collapse" for="c-41537658">[-]</label><label class="expand" for="c-41537658">[4 more]</label></div><br/><div class="children"><div class="content">All examples are 404&#x27;d for me.</div><br/><div id="41537751" class="c"><input type="checkbox" id="c-41537751" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537658">parent</a><span>|</span><a href="#41538036">next</a><span>|</span><label class="collapse" for="c-41537751">[-]</label><label class="expand" for="c-41537751">[2 more]</label></div><br/><div class="children"><div class="content">Hmm. My first thought was that I shared non-public links, but I double-checked I can access them from another machine.</div><br/><div id="41537836" class="c"><input type="checkbox" id="c-41537836" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537751">parent</a><span>|</span><a href="#41538036">next</a><span>|</span><label class="collapse" for="c-41537836">[-]</label><label class="expand" for="c-41537836">[1 more]</label></div><br/><div class="children"><div class="content">FYI They load fine for me.</div><br/></div></div></div></div><div id="41538036" class="c"><input type="checkbox" id="c-41538036" checked=""/><div class="controls bullet"><span class="by">Zr01</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537658">parent</a><span>|</span><a href="#41537751">prev</a><span>|</span><a href="#41537669">next</a><span>|</span><label class="collapse" for="c-41538036">[-]</label><label class="expand" for="c-41538036">[1 more]</label></div><br/><div class="children"><div class="content">The pages fail to load on old web browsers.</div><br/></div></div></div></div></div></div></div></div><div id="41537669" class="c"><input type="checkbox" id="c-41537669" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#41537209">parent</a><span>|</span><a href="#41537375">prev</a><span>|</span><a href="#41537409">next</a><span>|</span><label class="collapse" for="c-41537669">[-]</label><label class="expand" for="c-41537669">[4 more]</label></div><br/><div class="children"><div class="content">Both Chat GPT 4o and Claude 3.5 can trivially solve this puzzle if you direct them to do program synthesis to solve it.   (that is write a program that solves it - e.g. <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;wDTWYcSx" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;wDTWYcSx</a>).<p>Without program synthesis (the way you are doing it), the LLM inevitably fails to change the correct position (bad counting and what not)</div><br/><div id="41537735" class="c"><input type="checkbox" id="c-41537735" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537669">parent</a><span>|</span><a href="#41537409">next</a><span>|</span><label class="collapse" for="c-41537735">[-]</label><label class="expand" for="c-41537735">[3 more]</label></div><br/><div class="children"><div class="content">and what prompt you gave them to generate program? Did you tell explicitly that they need to fill cornered cells? If yes, it is not what benchmark is about. Benchmark is to ask LLM to figure out what is the pattern.<p>I entered task to Claude and asked to write py code, and it failed to recognize pattern:<p>To solve this puzzle, we need to implement a program that follows the pattern observed in the given examples. It appears that the rule is to replace &#x27;O&#x27; with &#x27;X&#x27; when it&#x27;s adjacent (horizontally, vertically, or diagonally) to exactly two &#x27;@&#x27; symbols. Let&#x27;s write a Python program to solve this:</div><br/><div id="41537807" class="c"><input type="checkbox" id="c-41537807" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537735">parent</a><span>|</span><a href="#41537409">next</a><span>|</span><label class="collapse" for="c-41537807">[-]</label><label class="expand" for="c-41537807">[2 more]</label></div><br/><div class="children"><div class="content">arc reasoning challenge.
I&#x27;m going to give you 2 example input&#x2F;output pairs and then a third bare input. Please produce the correct third output.<p>It used its COT to understand cornering -- then I got it to write a program.<p>But as I try again, it&#x27;s not reliable.</div><br/></div></div></div></div></div></div><div id="41537409" class="c"><input type="checkbox" id="c-41537409" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#41537209">parent</a><span>|</span><a href="#41537669">prev</a><span>|</span><a href="#41537303">next</a><span>|</span><label class="collapse" for="c-41537409">[-]</label><label class="expand" for="c-41537409">[7 more]</label></div><br/><div class="children"><div class="content">Author here. Which aspects are misleading? How can it be improved?</div><br/><div id="41537739" class="c"><input type="checkbox" id="c-41537739" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537409">parent</a><span>|</span><a href="#41537468">next</a><span>|</span><label class="collapse" for="c-41537739">[-]</label><label class="expand" for="c-41537739">[1 more]</label></div><br/><div class="children"><div class="content">I think the post is great, clear and fair and all that. And I definitely agree with the general point that o1 shows some amount of improvement on generality but with a massive tradeoff on cost.<p>I&#x27;m going to think through what I find &quot;misleading&quot; as I write this...<p>Ok so I guess it&#x27;s that I wouldn&#x27;t be surprised at all if we learn that models can improve a ton w.r.t. human-in-the-loop prompt engineering (e.g. ChatGPT) without a commensurate improvement in programmatic prompt engineering.<p>It&#x27;s very difficult to get a Python-driven claude-3.5-sonnet agent to solve ARC tasks and it&#x27;s also very difficult to get claude-3.5-sonnet to solve ARC tasks via the claude.ai UI. The blog post shows that it&#x27;s also very difficult to get a Python-driven o1-preview agent to solve ARC tasks. From a cursory exploration of o1-preview&#x27;s capabilities in the ChatGPT UI my intuition is that it&#x27;s <i>significantly smarter</i> than claude-3.5-sonnet based on how much better it responds to my human-in-the-loop feedback.<p>So I guess my point is that many people will probably come away from the blog post thinking &quot;there&#x27;s nothing to see here&quot;, o1-preview is more of the same thing, but it seems to me that it&#x27;s very clearly <i>qualitatively different</i> than previous models.<p>Aside: This isn&#x27;t a problem with the blog post at all IMO, we don&#x27;t need to litter every benchmark post with a million caveats&#x2F;exceptions&#x2F;disclaimers&#x2F;etc.</div><br/></div></div><div id="41537468" class="c"><input type="checkbox" id="c-41537468" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537409">parent</a><span>|</span><a href="#41537739">prev</a><span>|</span><a href="#41537303">next</a><span>|</span><label class="collapse" for="c-41537468">[-]</label><label class="expand" for="c-41537468">[5 more]</label></div><br/><div class="children"><div class="content">I think the parent post is complaining that insufficient acknowledgement is given to how good o1 is, because in their contrived testing, it seems better than previous models.<p>I don’t think that’s true though, it’s hard to be <i>more</i> fair and explicit than:<p>&gt; OpenAI o1-preview and o1-mini both outperform GPT-4o on the ARC-AGI public evaluation dataset. o1-preview is about on par with Anthropic&#x27;s Claude 3.5 Sonnet in terms of accuracy but takes about 10X longer to achieve similar results to Sonnet.<p>Ie. it’s just not that great, and it’s enormously slow.<p>That probably wasn’t what people wanted to hear, even if it is literally what the results show.<p>You cant run away from the numbers:<p>&gt; It took 70 hours on the 400 public tasks compared to only 30 minutes for GPT-4o and Claude 3.5 Sonnet.<p>(Side note: readers may be getting confused about what “test-time scaling” is, and why that’s important. TLDR: more compute is getting better results at inference time. That’s a big deal, because previously, pouring more compute at inference time didn’t seem to make much real difference; but overall I don’t see how anything you’ve said is either inaccurate or misleading)</div><br/><div id="41537580" class="c"><input type="checkbox" id="c-41537580" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537468">parent</a><span>|</span><a href="#41537576">next</a><span>|</span><label class="collapse" for="c-41537580">[-]</label><label class="expand" for="c-41537580">[1 more]</label></div><br/><div class="children"><div class="content">I personally am slightly surprised at o1&#x27;s modest performance on ARC-AGI given the large leaps in performance on other objectively hard benchmarks like IOI and AIME.<p>Curiosity is the first step towards new ideas.<p>ARC Prize&#x27;s whole goal is to inspire curiosity like this and to encourage more AI researchers to explore and  openly share new approaches towards AGI.</div><br/></div></div><div id="41537576" class="c"><input type="checkbox" id="c-41537576" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537468">parent</a><span>|</span><a href="#41537580">prev</a><span>|</span><a href="#41537593">next</a><span>|</span><label class="collapse" for="c-41537576">[-]</label><label class="expand" for="c-41537576">[2 more]</label></div><br/><div class="children"><div class="content">What does minutes and hours even mean? Software comparison using absolute time duration is meaningless without a description of the system it was executed on; e.g. SHA256 hashes per second on a Win10 OS and i7-14100 processor. For a product as complex as multiuser TB-sized LLMs, compute time is dependent on everything from the VM software stack to the physical networking and memory caching architecture.<p>CPU&#x2F;GPU cycles, FLOPs, IOPs, or even <i>joules</i> would be superior measurements.</div><br/><div id="41537705" class="c"><input type="checkbox" id="c-41537705" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537576">parent</a><span>|</span><a href="#41537593">next</a><span>|</span><label class="collapse" for="c-41537705">[-]</label><label class="expand" for="c-41537705">[1 more]</label></div><br/><div class="children"><div class="content">These are API calls to a remote server. We don&#x27;t have the option of scaling up or even measuring the compute they use to run them, so for better or worse the server cluster has to be measured as part of their model service offering.</div><br/></div></div></div></div><div id="41537593" class="c"><input type="checkbox" id="c-41537593" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#41537209">root</a><span>|</span><a href="#41537468">parent</a><span>|</span><a href="#41537576">prev</a><span>|</span><a href="#41537303">next</a><span>|</span><label class="collapse" for="c-41537593">[-]</label><label class="expand" for="c-41537593">[1 more]</label></div><br/><div class="children"><div class="content">I agree with basically everything you said but I think you&#x27;ve misunderstood my point. I&#x27;ll reply to the other comment with more.</div><br/></div></div></div></div></div></div><div id="41537303" class="c"><input type="checkbox" id="c-41537303" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41537209">parent</a><span>|</span><a href="#41537409">prev</a><span>|</span><a href="#41537162">next</a><span>|</span><label class="collapse" for="c-41537303">[-]</label><label class="expand" for="c-41537303">[1 more]</label></div><br/><div class="children"><div class="content">Interesting part if you check CoT output, the way it solved: it said the pattern is to make number of filled cells even in each row with neat layout, which is interesting side effect, but not what task was about.<p>It is also referring on some &quot;assistant&quot;, looks like they have some mysterious component in addition to LLM, or another LLM.</div><br/></div></div></div></div><div id="41537162" class="c"><input type="checkbox" id="c-41537162" checked=""/><div class="controls bullet"><span class="by">Stevvo</span><span>|</span><a href="#41537209">prev</a><span>|</span><a href="#41537008">next</a><span>|</span><label class="collapse" for="c-41537162">[-]</label><label class="expand" for="c-41537162">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Greenblatt&quot; shown with 42% in the bar chart is GPT-4o with a strategy: <a href="https:&#x2F;&#x2F;substack.com&#x2F;@ryangreenblatt&#x2F;p-145731248" rel="nofollow">https:&#x2F;&#x2F;substack.com&#x2F;@ryangreenblatt&#x2F;p-145731248</a><p>So, how well might o1 do with Greenblatt&#x27;s strategy?</div><br/><div id="41537419" class="c"><input type="checkbox" id="c-41537419" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#41537162">parent</a><span>|</span><a href="#41537008">next</a><span>|</span><label class="collapse" for="c-41537419">[-]</label><label class="expand" for="c-41537419">[1 more]</label></div><br/><div class="children"><div class="content">I bet pretty well! Someone should try this. It&#x27;s likely expensive but sampling could give you confidence to keep going. Ryan&#x27;s approach costs about $10k to run the full 400 public eval set at current 4o prices -- which is the arbitrary limit we set for the public leaderboard.</div><br/></div></div></div></div><div id="41537008" class="c"><input type="checkbox" id="c-41537008" checked=""/><div class="controls bullet"><span class="by">w4</span><span>|</span><a href="#41537162">prev</a><span>|</span><a href="#41535847">next</a><span>|</span><label class="collapse" for="c-41537008">[-]</label><label class="expand" for="c-41537008">[18 more]</label></div><br/><div class="children"><div class="content">&gt; <i>o1&#x27;s performance increase did come with a time cost. It took 70 hours on the 400 public tasks compared to only 30 minutes for GPT-4o and Claude 3.5 Sonnet.</i><p>Sheesh. We&#x27;re going to need more compute.</div><br/><div id="41537070" class="c"><input type="checkbox" id="c-41537070" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#41537008">parent</a><span>|</span><a href="#41537670">next</a><span>|</span><label class="collapse" for="c-41537070">[-]</label><label class="expand" for="c-41537070">[15 more]</label></div><br/><div class="children"><div class="content">Intelligence is something that gets monotone easier as compute increases and trivial at the large compute limit (for instance can brute force simulate a human at large enough compute). So increasing compute is the most sure way to ensure success at reaching above human level intelligence (agi)</div><br/><div id="41537215" class="c"><input type="checkbox" id="c-41537215" checked=""/><div class="controls bullet"><span class="by">etrautmann</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537070">parent</a><span>|</span><a href="#41537129">next</a><span>|</span><label class="collapse" for="c-41537215">[-]</label><label class="expand" for="c-41537215">[3 more]</label></div><br/><div class="children"><div class="content">This is…highly speculative and fairly ridiculous to anyone who’s attempted to do so</div><br/><div id="41538138" class="c"><input type="checkbox" id="c-41538138" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537215">parent</a><span>|</span><a href="#41537129">next</a><span>|</span><label class="collapse" for="c-41538138">[-]</label><label class="expand" for="c-41538138">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m giving a proof of a theoretical fact not saying it&#x27;s feasible</div><br/><div id="41538169" class="c"><input type="checkbox" id="c-41538169" checked=""/><div class="controls bullet"><span class="by">komali2</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41538138">parent</a><span>|</span><a href="#41537129">next</a><span>|</span><label class="collapse" for="c-41538169">[-]</label><label class="expand" for="c-41538169">[1 more]</label></div><br/><div class="children"><div class="content">proof + fact, and theoretical, are very different words, I&#x27;m really confused by your meaning here</div><br/></div></div></div></div></div></div><div id="41537129" class="c"><input type="checkbox" id="c-41537129" checked=""/><div class="controls bullet"><span class="by">trehalose</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537070">parent</a><span>|</span><a href="#41537215">prev</a><span>|</span><a href="#41538186">next</a><span>|</span><label class="collapse" for="c-41537129">[-]</label><label class="expand" for="c-41537129">[9 more]</label></div><br/><div class="children"><div class="content">How does one &quot;brute force simulate a human&quot;? If compute is the limiting factor, then isn&#x27;t it <i>currently</i> possible to brute force simulate a human, just extremely slowly?</div><br/><div id="41537625" class="c"><input type="checkbox" id="c-41537625" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537129">parent</a><span>|</span><a href="#41537384">next</a><span>|</span><label class="collapse" for="c-41537625">[-]</label><label class="expand" for="c-41537625">[3 more]</label></div><br/><div class="children"><div class="content">I guess technically, one can try to simulate every single atoms and their interactions with each others to get this result.<p>However, considering how many atoms there are in a cubic foot of meat, this isn&#x27;t very possible even with current compute. Even trying to solve a PDE with, I don&#x27;t know, 1e7 factors, is already a hard to crack issue although technically, it is computable.<p>Now take that to the number of atoms in a meatbag and you quickly see why it is pointless to put any effort into this &quot;extremely slowly&quot; way.</div><br/><div id="41538121" class="c"><input type="checkbox" id="c-41538121" checked=""/><div class="controls bullet"><span class="by">black_knight</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537625">parent</a><span>|</span><a href="#41537728">next</a><span>|</span><label class="collapse" for="c-41538121">[-]</label><label class="expand" for="c-41538121">[1 more]</label></div><br/><div class="children"><div class="content">We have no way of knowing the initial conditions for this (position etc of each fundamental particle in any brain), even if we assume that we have a good enough grasp on fundamental physics to know the rules.</div><br/></div></div><div id="41537728" class="c"><input type="checkbox" id="c-41537728" checked=""/><div class="controls bullet"><span class="by">trehalose</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537625">parent</a><span>|</span><a href="#41538121">prev</a><span>|</span><a href="#41537384">next</a><span>|</span><label class="collapse" for="c-41537728">[-]</label><label class="expand" for="c-41537728">[1 more]</label></div><br/><div class="children"><div class="content">But <i>if</i> we had enough compute, it&#x27;d be trivial, right? I mean, I didn&#x27;t think so, but the guy I replied to seems to know so. No, in all seriousness, I realize that &quot;extremely slowly&quot; is an understatement.<p>In davidzheng&#x27;s defense, I assume he likely meant a higher-level simulation of a human, one designed to act indistinguishably from an atom-level simulation.<p>I just think calling that &quot;trivial with enough compute&quot; is mistaking merely having the materials for having mastered them.</div><br/></div></div></div></div><div id="41537384" class="c"><input type="checkbox" id="c-41537384" checked=""/><div class="controls bullet"><span class="by">soared</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537129">parent</a><span>|</span><a href="#41537625">prev</a><span>|</span><a href="#41537656">next</a><span>|</span><label class="collapse" for="c-41537384">[-]</label><label class="expand" for="c-41537384">[3 more]</label></div><br/><div class="children"><div class="content">Something something monkey at a typewriter writing Shakespeare</div><br/><div id="41538142" class="c"><input type="checkbox" id="c-41538142" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537384">parent</a><span>|</span><a href="#41537484">next</a><span>|</span><label class="collapse" for="c-41538142">[-]</label><label class="expand" for="c-41538142">[1 more]</label></div><br/><div class="children"><div class="content">This is a more water tight  proof of the same fact (so we don&#x27;t have to argue about physics)</div><br/></div></div><div id="41537484" class="c"><input type="checkbox" id="c-41537484" checked=""/><div class="controls bullet"><span class="by">rrrix1</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537384">parent</a><span>|</span><a href="#41538142">prev</a><span>|</span><a href="#41537656">next</a><span>|</span><label class="collapse" for="c-41537484">[-]</label><label class="expand" for="c-41537484">[1 more]</label></div><br/><div class="children"><div class="content">Get out of my head!</div><br/></div></div></div></div><div id="41537656" class="c"><input type="checkbox" id="c-41537656" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537129">parent</a><span>|</span><a href="#41537384">prev</a><span>|</span><a href="#41538186">next</a><span>|</span><label class="collapse" for="c-41537656">[-]</label><label class="expand" for="c-41537656">[2 more]</label></div><br/><div class="children"><div class="content">Human brain has 1000 trillion synapses between 68 billion neurons. What are you going to simulate them on?<p>And it&#x27;s not like you can copy brain&#x27;s connectivity exactly. Such technologies don&#x27;t exist.</div><br/><div id="41537804" class="c"><input type="checkbox" id="c-41537804" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537656">parent</a><span>|</span><a href="#41538186">next</a><span>|</span><label class="collapse" for="c-41537804">[-]</label><label class="expand" for="c-41537804">[1 more]</label></div><br/><div class="children"><div class="content">I have a computer like that, embedded in my head even! It&#x27;s good for real-time simulation, but has trouble simulating the same human from even a couple weeks before.<p>In all seriousness, it&#x27;s simultaneously wondrous and terrifying to imagine the hypothetical tooling needed for such a simulation.</div><br/></div></div></div></div></div></div><div id="41538186" class="c"><input type="checkbox" id="c-41538186" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537070">parent</a><span>|</span><a href="#41537129">prev</a><span>|</span><a href="#41537194">next</a><span>|</span><label class="collapse" for="c-41538186">[-]</label><label class="expand" for="c-41538186">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Intelligence is something that gets monotone easier as compute increases and trivial at the large compute limit (for instance can brute force simulate a human at large enough compute)<p>It gets monotone easier but the increase can be so slow that even using all the energy in the observable universe wouldn&#x27;t make a meaningful difference, e.g. for problems in the exponential complexity class.</div><br/></div></div></div></div><div id="41537670" class="c"><input type="checkbox" id="c-41537670" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#41537008">parent</a><span>|</span><a href="#41537070">prev</a><span>|</span><a href="#41535847">next</a><span>|</span><label class="collapse" for="c-41537670">[-]</label><label class="expand" for="c-41537670">[2 more]</label></div><br/><div class="children"><div class="content">Polar icecaps shuddering at the thought</div><br/><div id="41537809" class="c"><input type="checkbox" id="c-41537809" checked=""/><div class="controls bullet"><span class="by">asimpleusecase</span><span>|</span><a href="#41537008">root</a><span>|</span><a href="#41537670">parent</a><span>|</span><a href="#41535847">next</a><span>|</span><label class="collapse" for="c-41537809">[-]</label><label class="expand" for="c-41537809">[1 more]</label></div><br/><div class="children"><div class="content">That is the next major challenge. Ok you can solve a logic puzzle with a gilzillon watts now go power that same level of compute with a cheese burger, or if you are vegan a nice salad.</div><br/></div></div></div></div></div></div><div id="41535847" class="c"><input type="checkbox" id="c-41535847" checked=""/><div class="controls bullet"><span class="by">alphabetting</span><span>|</span><a href="#41537008">prev</a><span>|</span><a href="#41537166">next</a><span>|</span><label class="collapse" for="c-41535847">[-]</label><label class="expand" for="c-41535847">[11 more]</label></div><br/><div class="children"><div class="content">This is best AGI benchmark out there in my opinion. Surprising results that underscore how good Sonnet is.</div><br/><div id="41536815" class="c"><input type="checkbox" id="c-41536815" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#41535847">parent</a><span>|</span><a href="#41536050">next</a><span>|</span><label class="collapse" for="c-41536815">[-]</label><label class="expand" for="c-41536815">[4 more]</label></div><br/><div class="children"><div class="content">If ARC-AGI were a good benchmark for &quot;AGI&quot;, then MindsAI should effectively be blowing away current frontier models by order of magnitude. I don&#x27;t know what MindsAI is, but the post implies they&#x27;re basically fine-tuning or using a very specific strategy for ARC-AGI that isn&#x27;t really generalizable to other tasks.<p>I think it&#x27;s a nice benchmark of a certain type of spatial&#x2F;visual intelligence, but if you have a model or technique specifically fine-tuned for ARC-AGI then it&#x27;s no longer A&quot;G&quot;I</div><br/><div id="41537005" class="c"><input type="checkbox" id="c-41537005" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#41535847">root</a><span>|</span><a href="#41536815">parent</a><span>|</span><a href="#41538333">prev</a><span>|</span><a href="#41536975">next</a><span>|</span><label class="collapse" for="c-41537005">[-]</label><label class="expand" for="c-41537005">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps a benchmark could be a good approximate upper bound for something without being a good approximate lower bound for that thing?</div><br/></div></div><div id="41536975" class="c"><input type="checkbox" id="c-41536975" checked=""/><div class="controls bullet"><span class="by">nightski</span><span>|</span><a href="#41535847">root</a><span>|</span><a href="#41536815">parent</a><span>|</span><a href="#41537005">prev</a><span>|</span><a href="#41536050">next</a><span>|</span><label class="collapse" for="c-41536975">[-]</label><label class="expand" for="c-41536975">[1 more]</label></div><br/><div class="children"><div class="content">I mean, there are a lot of tasks that frontier models excel at which many humans wouldn&#x27;t be able to complete.</div><br/></div></div></div></div><div id="41536050" class="c"><input type="checkbox" id="c-41536050" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#41535847">parent</a><span>|</span><a href="#41536815">prev</a><span>|</span><a href="#41537166">next</a><span>|</span><label class="collapse" for="c-41536050">[-]</label><label class="expand" for="c-41536050">[6 more]</label></div><br/><div class="children"><div class="content">Disagree. My opinion is that solving ARC-AGI won&#x27;t get us any closer to AGI and it&#x27;s mostly a distraction.</div><br/><div id="41536812" class="c"><input type="checkbox" id="c-41536812" checked=""/><div class="controls bullet"><span class="by">meowface</span><span>|</span><a href="#41535847">root</a><span>|</span><a href="#41536050">parent</a><span>|</span><a href="#41536083">next</a><span>|</span><label class="collapse" for="c-41536812">[-]</label><label class="expand" for="c-41536812">[1 more]</label></div><br/><div class="children"><div class="content">I mostly agree, but I think it&#x27;s fair to say that ARC-AGI is a necessary but definitely not sufficient milestone when it comes to the evaluation of a purported AGI.</div><br/></div></div><div id="41536083" class="c"><input type="checkbox" id="c-41536083" checked=""/><div class="controls bullet"><span class="by">alphabetting</span><span>|</span><a href="#41535847">root</a><span>|</span><a href="#41536050">parent</a><span>|</span><a href="#41536812">prev</a><span>|</span><a href="#41537673">next</a><span>|</span><label class="collapse" for="c-41536083">[-]</label><label class="expand" for="c-41536083">[2 more]</label></div><br/><div class="children"><div class="content">How so? I think if a team is fine-tuning specifically to beat ARC that could be true but when you look at Sonnet and o1 getting 20%, I think a standalone frontier model beating it would mean we are close or already at AGI.</div><br/><div id="41538127" class="c"><input type="checkbox" id="c-41538127" checked=""/><div class="controls bullet"><span class="by">authorfly</span><span>|</span><a href="#41535847">root</a><span>|</span><a href="#41536083">parent</a><span>|</span><a href="#41537673">next</a><span>|</span><label class="collapse" for="c-41538127">[-]</label><label class="expand" for="c-41538127">[1 more]</label></div><br/><div class="children"><div class="content">The creation and iteration of ARC has been designed in part to avoid this.<p>Francis talks in his &quot;mid-career&quot; work (2015-2019) about priors for general intelligence and avoiding allowing them. While he admits ARC allows for some priors, it was at the time his best reasonable human effort in 2019 to put together and extremely prior-less training set, as he explained on podcasts around that time (e.g. Lex Fridman). The point of this is that humans, with our priors, are able to reliably get the majority of the puzzles correct, and with time, we can even correct mistakes or recognise mistakes in submissions without feedback (I am expanding on his point a little here based on conference conversations so don&#x27;t take this as his position or at least his position today).<p>100 different humans will even get very different items correct&#x2F;incorrect.<p>The problem with AI getting 21% correct is that, if it always gets the same 21% correct, it means for 79% of prior-less problems, it has no hope as an intelligent system.<p>Humans on the other hand, a group of 10000 could obviously get 99% or 100% correct despite none of them having priors for all of them in all liklihood given humans don&#x27;t tend to get them all right (and well - because Francis created 100% of them!).<p>The goal of ARC as I understood it in 2019, is not to create a single model that gets a majority correct, to show AGI, it has to be an intelligent system, which can handle prior or priorless situations, as good as a group of humans, on diverse and unseen test sets, ideally without any finetuning or training specifically on this task, at all.<p>From 2019 (I read his paper when it came out believe it or not!), he held a secret set that he alone has that I believe is still unpublished, and at the time the low number of items (hundreds) was designed to prevent effective finetuning(then &#x27;training&#x27;) but nowadays few shot training shows that it is clearly possible to do on-the-spot training, which is why in talks Francis gave, I remember him positing that any advanced in short term learning via examples should be ignored e.g. each example should be zero shot, which I believe is how most benchmarks are currently done. The puzzles are all &quot;different in different ways&quot; besides the common element of dynamic grids and providing multiple grids as input.<p>It&#x27;s also key to know Francis was quite avant-garde in 2019: his work was ofcourse respected, but he became more prominent recently. He took a very bullish&#x2F;optimistic position on AI advances at the time (no doubt based on keras and seeing transformers trained using it), but he has been proven right.</div><br/></div></div></div></div><div id="41537673" class="c"><input type="checkbox" id="c-41537673" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#41535847">root</a><span>|</span><a href="#41536050">parent</a><span>|</span><a href="#41536083">prev</a><span>|</span><a href="#41536818">next</a><span>|</span><label class="collapse" for="c-41537673">[-]</label><label class="expand" for="c-41537673">[1 more]</label></div><br/><div class="children"><div class="content">I think solving ARC-AGI will be necessary but not sufficient. My bet is that the converse will not be true - a model that will be considered &quot;AGI&quot; but does poorly on ARC-AGI. So in that sense, I think this is an important benchmark.</div><br/></div></div><div id="41536818" class="c"><input type="checkbox" id="c-41536818" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#41535847">root</a><span>|</span><a href="#41536050">parent</a><span>|</span><a href="#41537673">prev</a><span>|</span><a href="#41537166">next</a><span>|</span><label class="collapse" for="c-41536818">[-]</label><label class="expand" for="c-41536818">[1 more]</label></div><br/><div class="children"><div class="content">Is that mainly because AGI is one of those &quot;I&#x27;ll know it when I see it&quot; things?</div><br/></div></div></div></div></div></div><div id="41537166" class="c"><input type="checkbox" id="c-41537166" checked=""/><div class="controls bullet"><span class="by">mrcwinn</span><span>|</span><a href="#41535847">prev</a><span>|</span><a href="#41537092">next</a><span>|</span><label class="collapse" for="c-41537166">[-]</label><label class="expand" for="c-41537166">[3 more]</label></div><br/><div class="children"><div class="content">How is Anthropic accomplishing this despite (seemingly) arriving later?What advantage do they have?</div><br/><div id="41537506" class="c"><input type="checkbox" id="c-41537506" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#41537166">parent</a><span>|</span><a href="#41537254">next</a><span>|</span><label class="collapse" for="c-41537506">[-]</label><label class="expand" for="c-41537506">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic currently does much less hype stuff comparing to openai. It&#x27;s remarkable that openai was like this until the GPT-4 release, and completly changed since Sam Altman started touring countries.</div><br/></div></div><div id="41537254" class="c"><input type="checkbox" id="c-41537254" checked=""/><div class="controls bullet"><span class="by">changoplatanero</span><span>|</span><a href="#41537166">parent</a><span>|</span><a href="#41537506">prev</a><span>|</span><a href="#41537092">next</a><span>|</span><label class="collapse" for="c-41537254">[-]</label><label class="expand" for="c-41537254">[1 more]</label></div><br/><div class="children"><div class="content">One theory I heard is that Dario was always interested in RL whereas Ilya was interested in other stuff until more recently. So Anthropic could have had an earlier start on some of this latest RL stuff.</div><br/></div></div></div></div><div id="41537092" class="c"><input type="checkbox" id="c-41537092" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#41537166">prev</a><span>|</span><a href="#41536773">next</a><span>|</span><label class="collapse" for="c-41537092">[-]</label><label class="expand" for="c-41537092">[1 more]</label></div><br/><div class="children"><div class="content">The test you really want is the apples-to-apples comparison between GPT-4o faced with the same CoT and other context annealing that presumably, uh, Q* sorry Strawberry now feeds it (on your dime). This would of course require seeing the tokens you are paying for instead of being threatened with bans for asking about them.<p>Compared to the difficulty in assembling the data and compute and other resources needed to train something like GPT-4-1106 (which are staggering), training an auxiliary model with a relatively straightforward, differentiable, well-behaved loss on a task like &quot;which CoT framing is better according to human click proxy&quot; is just not at that same scale.</div><br/></div></div><div id="41536773" class="c"><input type="checkbox" id="c-41536773" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#41537092">prev</a><span>|</span><a href="#41538309">next</a><span>|</span><label class="collapse" for="c-41536773">[-]</label><label class="expand" for="c-41536773">[4 more]</label></div><br/><div class="children"><div class="content">As expected, I&#x27;ve always believed that with the right data allowing the LLM to be trained to imitate reasoning, it&#x27;s possible to improve its performance. However, this is still pattern matching, and I suspect that this approach may not be very effective for creating true generalization. As a result, once o1 becomes generally available, we will likely notice the persistent hallucinations and faulty reasoning, especially when the problem is sufficiently new or complex, beyond the &quot;reasoning programs&quot; or &quot;reasoning patterns&quot; the model learned during the reinforcement learning phase.
<a href="https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;openai-o1-release-agi-reasoning" rel="nofollow">https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;openai-o1-release-agi-reasoning</a></div><br/><div id="41536918" class="c"><input type="checkbox" id="c-41536918" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#41536773">parent</a><span>|</span><a href="#41536799">next</a><span>|</span><label class="collapse" for="c-41536918">[-]</label><label class="expand" for="c-41536918">[1 more]</label></div><br/><div class="children"><div class="content">My feeling is that this is one reason they decided to hide the reasoning tokens.</div><br/></div></div><div id="41536799" class="c"><input type="checkbox" id="c-41536799" checked=""/><div class="controls bullet"><span class="by">wslh</span><span>|</span><a href="#41536773">parent</a><span>|</span><a href="#41536918">prev</a><span>|</span><a href="#41536960">next</a><span>|</span><label class="collapse" for="c-41536799">[-]</label><label class="expand" for="c-41536799">[1 more]</label></div><br/><div class="children"><div class="content">So basically it&#x27;s a kind of overfitting with pattern matching features? This doesn&#x27;t undermine the power of LLMs but it is great to study their limitations.</div><br/></div></div><div id="41536960" class="c"><input type="checkbox" id="c-41536960" checked=""/><div class="controls bullet"><span class="by">poopiokaka</span><span>|</span><a href="#41536773">parent</a><span>|</span><a href="#41536799">prev</a><span>|</span><a href="#41538309">next</a><span>|</span><label class="collapse" for="c-41536960">[-]</label><label class="expand" for="c-41536960">[1 more]</label></div><br/><div class="children"><div class="content">“As expected I’m right”</div><br/></div></div></div></div><div id="41538309" class="c"><input type="checkbox" id="c-41538309" checked=""/><div class="controls bullet"><span class="by">perching_aix</span><span>|</span><a href="#41536773">prev</a><span>|</span><a href="#41537048">next</a><span>|</span><label class="collapse" for="c-41538309">[-]</label><label class="expand" for="c-41538309">[2 more]</label></div><br/><div class="children"><div class="content">Is it possible for me, a human, to undertake these benchmarks?</div><br/><div id="41538315" class="c"><input type="checkbox" id="c-41538315" checked=""/><div class="controls bullet"><span class="by">terhechte</span><span>|</span><a href="#41538309">parent</a><span>|</span><a href="#41537048">next</a><span>|</span><label class="collapse" for="c-41538315">[-]</label><label class="expand" for="c-41538315">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s examples on the homepage, and there&#x27;s a link to the Kaggle notebook in the article.<p><a href="https:&#x2F;&#x2F;arcprize.org" rel="nofollow">https:&#x2F;&#x2F;arcprize.org</a></div><br/></div></div></div></div><div id="41537048" class="c"><input type="checkbox" id="c-41537048" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#41538309">prev</a><span>|</span><a href="#41537273">next</a><span>|</span><label class="collapse" for="c-41537048">[-]</label><label class="expand" for="c-41537048">[2 more]</label></div><br/><div class="children"><div class="content">TL;DR (direct quote):<p><i>“In summary, o1 represents a paradigm shift from &quot;memorize the answers&quot; to &quot;memorize the reasoning&quot; but is not a departure from the broader paradigm of fitting a curve to a distribution in order to boost performance by making everything in-distribution.”</i><p><i>“We still need new ideas for AGI.”</i></div><br/><div id="41537295" class="c"><input type="checkbox" id="c-41537295" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#41537048">parent</a><span>|</span><a href="#41537273">next</a><span>|</span><label class="collapse" for="c-41537295">[-]</label><label class="expand" for="c-41537295">[1 more]</label></div><br/><div class="children"><div class="content">This sounds very fair, but I think fundamentally humans memorize reasoning a lot more than you’d expect. A spark of inspiration is not memorized reasoning, but not many people can claim to enjoy that capability.</div><br/></div></div></div></div><div id="41537273" class="c"><input type="checkbox" id="c-41537273" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#41537048">prev</a><span>|</span><a href="#41536725">next</a><span>|</span><label class="collapse" for="c-41537273">[-]</label><label class="expand" for="c-41537273">[4 more]</label></div><br/><div class="children"><div class="content">Why is this considered such a great AGI test? It seems possible to extensively train a model on the algorithms used to solve these cases, and some cases feel beyond what a human could straightforwardly figure out.</div><br/><div id="41537458" class="c"><input type="checkbox" id="c-41537458" checked=""/><div class="controls bullet"><span class="by">isotypic</span><span>|</span><a href="#41537273">parent</a><span>|</span><a href="#41537451">next</a><span>|</span><label class="collapse" for="c-41537458">[-]</label><label class="expand" for="c-41537458">[1 more]</label></div><br/><div class="children"><div class="content">Do you have some examples of ones you found beyond what a human could straightforwardly figure out? I tried a bunch and they all seemed reasonable, so I would be interested in seeing - I didn&#x27;t try all 400, for obvious reasons, so I don&#x27;t doubt there are difficult ones.<p>I think regardless one of the reasons people are interested in it is that is a fairly simple logic puzzle - given some examples, extrapolate a pattern, execute the pattern - that humans achieve high accuracy on (a study linked on the website has ~84% accuracy for humans, some more recent study seems to put it closer to 75%). Yet ML approaches have yet to reach that level, in contrast to other problems ML has been applied to.<p>Given there is a large prize pool for the challenge, I would imagine actually training a model in the way you describe would already have been tried and is more difficult that it seems.</div><br/></div></div><div id="41537451" class="c"><input type="checkbox" id="c-41537451" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41537273">parent</a><span>|</span><a href="#41537458">prev</a><span>|</span><a href="#41537278">next</a><span>|</span><label class="collapse" for="c-41537451">[-]</label><label class="expand" for="c-41537451">[1 more]</label></div><br/><div class="children"><div class="content">There is a hidden test set with new puzzle types not seen in the open part. It&#x27;s designed so that humans do well and AI models have a hard time.</div><br/></div></div><div id="41537278" class="c"><input type="checkbox" id="c-41537278" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41537273">parent</a><span>|</span><a href="#41537451">prev</a><span>|</span><a href="#41536725">next</a><span>|</span><label class="collapse" for="c-41537278">[-]</label><label class="expand" for="c-41537278">[1 more]</label></div><br/><div class="children"><div class="content">I think huge advantage is that they keep eval tests private, so corps can&#x27;t finetune them to model and claim breakthrough, which possibly happened with many other benchmarks.</div><br/></div></div></div></div><div id="41536725" class="c"><input type="checkbox" id="c-41536725" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41537273">prev</a><span>|</span><a href="#41537231">next</a><span>|</span><label class="collapse" for="c-41536725">[-]</label><label class="expand" for="c-41536725">[4 more]</label></div><br/><div class="children"><div class="content">It really shows how far ahead Anthropic is&#x2F;was when they released Claude 3.5 Sonnet.<p>That being said, the ARC-agi test is mostly a visual test that would be much easier to beat when these models will truly be multimodal (not just appending a separate vision encoder after training) in my opinion.<p>I wonder what the graph will look like in a year from now, the models have improved a lot in the last one.</div><br/><div id="41536940" class="c"><input type="checkbox" id="c-41536940" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#41536725">parent</a><span>|</span><a href="#41537231">next</a><span>|</span><label class="collapse" for="c-41536940">[-]</label><label class="expand" for="c-41536940">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I wonder what the graph will look like in a year from now, the models have improved a lot in the last one.<p>Potentially not great.<p>If you look at the AIME accuracy graph on the OpenAI page [1] you will notice that the x-axis is logarithmic. Which is a problem because (a) compute in general has never scaled that well and (b) semiconductor fabrication will inevitably get harder as we approach smaller sizes.<p>So it looks like unless there is some ground-breaking research in the pipeline the current transformer architecture will likely start to stall out.<p>[1] <a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;</a></div><br/><div id="41537541" class="c"><input type="checkbox" id="c-41537541" checked=""/><div class="controls bullet"><span class="by">accountnum</span><span>|</span><a href="#41536725">root</a><span>|</span><a href="#41536940">parent</a><span>|</span><a href="#41536967">next</a><span>|</span><label class="collapse" for="c-41537541">[-]</label><label class="expand" for="c-41537541">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a problem, because the point at which we are in the logarithmic curve is the only thing that matters. No one in their right mind ever expected anything linear, because that would imply that creating a perfect oracle is possible.<p>More compute hasn&#x27;t been the driving factor of the last developments, the driving factor has been distillation and synthetic data. Since we&#x27;ve seen massive success with that, I really struggle to understand why people continue to doomsay the transformer. I hear these same arguments year after year and people never learn.</div><br/></div></div><div id="41536967" class="c"><input type="checkbox" id="c-41536967" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41536725">root</a><span>|</span><a href="#41536940">parent</a><span>|</span><a href="#41537541">prev</a><span>|</span><a href="#41537231">next</a><span>|</span><label class="collapse" for="c-41536967">[-]</label><label class="expand" for="c-41536967">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m very optimistic about it because native multimodal LLMs have hardly been explored.<p>Also in general, I have yet to see these models plateau, Claude 3.5 Sonnet is a day and night different compared to previous models.</div><br/></div></div></div></div></div></div><div id="41537231" class="c"><input type="checkbox" id="c-41537231" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#41536725">prev</a><span>|</span><a href="#41538218">next</a><span>|</span><label class="collapse" for="c-41537231">[-]</label><label class="expand" for="c-41537231">[1 more]</label></div><br/><div class="children"><div class="content">This tests vision, not intelligence. A reasoning test dependent on noisy information is borderline useless.</div><br/></div></div><div id="41538218" class="c"><input type="checkbox" id="c-41538218" checked=""/><div class="controls bullet"><span class="by">bulbosaur123</span><span>|</span><a href="#41537231">prev</a><span>|</span><label class="collapse" for="c-41538218">[-]</label><label class="expand" for="c-41538218">[1 more]</label></div><br/><div class="children"><div class="content">Ok, I have a practical question. How do I use this o1 thing to view codebase for my game app and then simply add new features based on my prompts? Is it possible rn? How?</div><br/></div></div></div></div></div></div></div></body></html>