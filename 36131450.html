<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685523667010" as="style"/><link rel="stylesheet" href="styles.css?v=1685523667010"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/FranxYao/chain-of-thought-hub">Chain-of-Thought Hub: Measuring LLMs&#x27; Reasoning Performance</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>Garcia98</span> | <span>22 comments</span></div><br/><div><div id="36133877" class="c"><input type="checkbox" id="c-36133877" checked=""/><div class="controls bullet"><span class="by">vladf</span><span>|</span><a href="#36133548">next</a><span>|</span><label class="collapse" for="c-36133877">[-]</label><label class="expand" for="c-36133877">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Also be careful that GPT-4&#x2F; 3.5&#x27;s performance on GSM8K is not true few-shot -- in GPT-4 report they said that they mixed a portion of GSM8K training set to train the model<p>It&#x27;d be really valuable to have &quot;fuzzed&quot; versions of these benchmarks, where you replace quantities in the questions with randomly-sampled values, so that this wasn&#x27;t a concern. Of course, then the score would itself be a random variable, but you could just return an interval.</div><br/><div id="36135079" class="c"><input type="checkbox" id="c-36135079" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#36133877">parent</a><span>|</span><a href="#36133548">next</a><span>|</span><label class="collapse" for="c-36135079">[-]</label><label class="expand" for="c-36135079">[1 more]</label></div><br/><div class="children"><div class="content">seeing identical problems with different values still doesn&#x27;t count as zero shot. it is better though, for sure</div><br/></div></div></div></div><div id="36133548" class="c"><input type="checkbox" id="c-36133548" checked=""/><div class="controls bullet"><span class="by">deadmutex</span><span>|</span><a href="#36133877">prev</a><span>|</span><a href="#36135761">next</a><span>|</span><label class="collapse" for="c-36133548">[-]</label><label class="expand" for="c-36133548">[1 more]</label></div><br/><div class="children"><div class="content">For those unfamiliar with the benchmarks, it would be good to know if a higher or lower score was better. E.g. are they measuring accuracy or error rate, etc.<p>You can infer it by reading the text, and checking the table carefully, but it would be nice if the answer is easier to find.</div><br/></div></div><div id="36135761" class="c"><input type="checkbox" id="c-36135761" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#36133548">prev</a><span>|</span><a href="#36132810">next</a><span>|</span><label class="collapse" for="c-36135761">[-]</label><label class="expand" for="c-36135761">[1 more]</label></div><br/><div class="children"><div class="content">I have been amused by how bad GTP-4 and Bard are at playing tic-tac-toe. Also utterly clueless at othello.</div><br/></div></div><div id="36132810" class="c"><input type="checkbox" id="c-36132810" checked=""/><div class="controls bullet"><span class="by">freediver</span><span>|</span><a href="#36135761">prev</a><span>|</span><a href="#36134571">next</a><span>|</span><label class="collapse" for="c-36132810">[-]</label><label class="expand" for="c-36132810">[1 more]</label></div><br/><div class="children"><div class="content">Less scientific, but arguably more practical benchmarks here:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;kagisearch&#x2F;pyllms#model-benchmarks">https:&#x2F;&#x2F;github.com&#x2F;kagisearch&#x2F;pyllms#model-benchmarks</a></div><br/></div></div><div id="36134571" class="c"><input type="checkbox" id="c-36134571" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#36132810">prev</a><span>|</span><a href="#36132952">next</a><span>|</span><label class="collapse" for="c-36134571">[-]</label><label class="expand" for="c-36134571">[6 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-4 Early which is supposedly to be more powerful than GPT-4 Launch (OpenAI paid a lot of alignment tax to make GPT-4 safer)<p>What does “safer” mean?<p>Does it mean censored?</div><br/><div id="36135431" class="c"><input type="checkbox" id="c-36135431" checked=""/><div class="controls bullet"><span class="by">sgk284</span><span>|</span><a href="#36134571">parent</a><span>|</span><a href="#36134687">next</a><span>|</span><label class="collapse" for="c-36135431">[-]</label><label class="expand" for="c-36135431">[1 more]</label></div><br/><div class="children"><div class="content">Safer means constraining the kinds of answers the model will provide (e.g. it won&#x27;t try to talk you into committing self-harm, it won&#x27;t teach you how to make a break laws, etc...). It will generally avoid sensitive topics. Is &quot;censorship&quot; the right word though? It depends – is it considered self-censorship if I refuse to tell you how hack into a computer? Is refusing to engage in a conversation censorship or constraint?</div><br/></div></div><div id="36134687" class="c"><input type="checkbox" id="c-36134687" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#36134571">parent</a><span>|</span><a href="#36135431">prev</a><span>|</span><a href="#36134872">next</a><span>|</span><label class="collapse" for="c-36134687">[-]</label><label class="expand" for="c-36134687">[1 more]</label></div><br/><div class="children"><div class="content">See IRS Publication 946 for details on the alignment tax.</div><br/></div></div><div id="36134872" class="c"><input type="checkbox" id="c-36134872" checked=""/><div class="controls bullet"><span class="by">droopyEyelids</span><span>|</span><a href="#36134571">parent</a><span>|</span><a href="#36134687">prev</a><span>|</span><a href="#36134700">next</a><span>|</span><label class="collapse" for="c-36134872">[-]</label><label class="expand" for="c-36134872">[2 more]</label></div><br/><div class="children"><div class="content">Censorship is the crayon word for _reducing risk_: reputational, legal, compliance, hr risk, and many more.<p>A good prompt for this would be “What are the most common types of risk a company manages?”</div><br/><div id="36134923" class="c"><input type="checkbox" id="c-36134923" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#36134571">root</a><span>|</span><a href="#36134872">parent</a><span>|</span><a href="#36134700">next</a><span>|</span><label class="collapse" for="c-36134923">[-]</label><label class="expand" for="c-36134923">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean by “crayon word”?<p>Wouldn’t “safe” be the crayon word for censorship?<p>In any case, you’re right, it seems like they are addressing their own risks&#x2F;safety, not their users’</div><br/></div></div></div></div></div></div><div id="36132952" class="c"><input type="checkbox" id="c-36132952" checked=""/><div class="controls bullet"><span class="by">jxf</span><span>|</span><a href="#36134571">prev</a><span>|</span><label class="collapse" for="c-36132952">[-]</label><label class="expand" for="c-36132952">[10 more]</label></div><br/><div class="children"><div class="content">Q: What does &quot;alignment tax&quot; mean in this sentence?<p>&gt; OpenAI paid a lot of alignment tax to make GPT-4 safer.</div><br/><div id="36135443" class="c"><input type="checkbox" id="c-36135443" checked=""/><div class="controls bullet"><span class="by">sgk284</span><span>|</span><a href="#36132952">parent</a><span>|</span><a href="#36133507">next</a><span>|</span><label class="collapse" for="c-36135443">[-]</label><label class="expand" for="c-36135443">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI touches a little on this on page 12 of the GPT-4 technical report (<a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;papers&#x2F;gpt-4.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;papers&#x2F;gpt-4.pdf</a>). Prior to aligning to safer outputs, the model&#x27;s confidence in an answer is highly correlated with that actual accuracy of the answer. After alignment though, the model&#x27;s confidence in its answers is basically arbitrary and has no bearing on whether or not the answer is actually correct.</div><br/></div></div><div id="36133507" class="c"><input type="checkbox" id="c-36133507" checked=""/><div class="controls bullet"><span class="by">vellum</span><span>|</span><a href="#36132952">parent</a><span>|</span><a href="#36135443">prev</a><span>|</span><a href="#36133164">next</a><span>|</span><label class="collapse" for="c-36133507">[-]</label><label class="expand" for="c-36133507">[1 more]</label></div><br/><div class="children"><div class="content">From OpenAI&#x27;s RLHF paper[1]: &quot;By default, when we train a PPO model on our API distribution, it suffers from an “alignment tax”, as its performance on several public NLP datasets decreases.&quot; On the HELM[2] site, you can see accuracy benchmarks for InstructGPT &lt;OpenAI model&gt; vs baseline models. The InstructGPT models perform worse on a lot of benchmarks.<p>1 - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2203.02155.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2203.02155.pdf</a><p>2 - <a href="https:&#x2F;&#x2F;crfm.stanford.edu&#x2F;helm&#x2F;v0.1.0&#x2F;?group=question_answer" rel="nofollow">https:&#x2F;&#x2F;crfm.stanford.edu&#x2F;helm&#x2F;v0.1.0&#x2F;?group=question_answer</a>...</div><br/></div></div><div id="36133164" class="c"><input type="checkbox" id="c-36133164" checked=""/><div class="controls bullet"><span class="by">RayVR</span><span>|</span><a href="#36132952">parent</a><span>|</span><a href="#36133507">prev</a><span>|</span><a href="#36132971">next</a><span>|</span><label class="collapse" for="c-36133164">[-]</label><label class="expand" for="c-36133164">[2 more]</label></div><br/><div class="children"><div class="content">restricting the distribution of potential output imposes a cost. &quot;Alignment&quot; here likely refers to aligning the model to the desired safety parameters.<p>I&#x27;m not in the llm research business but I would expect that the best and worst&#x2F;most dangerous outputs come from the tails of distributions. I imagine the tuning for safety often results in fewer really good and really bad answers by trimming these tails.<p>Edit:
I asked chatGPT4: <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;a2c7d380-c6eb-4745-b91d-c3996a06c13f" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;a2c7d380-c6eb-4745-b91d-c3996a...</a></div><br/><div id="36134495" class="c"><input type="checkbox" id="c-36134495" checked=""/><div class="controls bullet"><span class="by">babyshake</span><span>|</span><a href="#36132952">root</a><span>|</span><a href="#36133164">parent</a><span>|</span><a href="#36132971">next</a><span>|</span><label class="collapse" for="c-36134495">[-]</label><label class="expand" for="c-36134495">[1 more]</label></div><br/><div class="children"><div class="content">I have found in practice it can be annoying for ChatGPT to start lecturing me in response to a prompt that is not particularly controversial or edgy. I think this is a problem with the one-size-fits-all models. To give a kind of rough analogy, imagine that every time you watched a film or show - which would most likely be an older film or show - with cigarette smoking, your smart TV showed a pop up dialog warning you about the dangers of smoking. If you&#x27;re an educated adult who already knows about these dangers, you might just find that annoying and condescending, and not &quot;aligning&quot; with your preferences.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>