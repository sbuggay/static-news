<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697101265565" as="style"/><link rel="stylesheet" href="styles.css?v=1697101265565"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/traceloop/openllmetry">Show HN: OpenLLMetry – OpenTelemetry-based observability for LLMs</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>nirga</span> | <span>35 comments</span></div><br/><div><div id="37850507" class="c"><input type="checkbox" id="c-37850507" checked=""/><div class="controls bullet"><span class="by">Areibman</span><span>|</span><a href="#37850583">next</a><span>|</span><label class="collapse" for="c-37850507">[-]</label><label class="expand" for="c-37850507">[3 more]</label></div><br/><div class="children"><div class="content">LLM observability strikes me as an extremely, extremely crowded space. And YC has funded an enormous number of them.<p>What do you think is the key differentiator between you and everyone else? Is vendor lock-in really that huge of an issue?<p>[0] <a href="https:&#x2F;&#x2F;hegel-ai.com">https:&#x2F;&#x2F;hegel-ai.com</a>, <a href="https:&#x2F;&#x2F;www.vellum.ai&#x2F;">https:&#x2F;&#x2F;www.vellum.ai&#x2F;</a>, <a href="https:&#x2F;&#x2F;www.parea.ai">https:&#x2F;&#x2F;www.parea.ai</a>, <a href="http:&#x2F;&#x2F;baserun.ai">http:&#x2F;&#x2F;baserun.ai</a>, <a href="https:&#x2F;&#x2F;www.trychatter.ai">https:&#x2F;&#x2F;www.trychatter.ai</a>, <a href="https:&#x2F;&#x2F;talc.ai">https:&#x2F;&#x2F;talc.ai</a>, <a href="https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;bettertest">https:&#x2F;&#x2F;github.com&#x2F;BerriAI&#x2F;bettertest</a>, <a href="https:&#x2F;&#x2F;langfuse.com">https:&#x2F;&#x2F;langfuse.com</a></div><br/><div id="37850576" class="c"><input type="checkbox" id="c-37850576" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37850507">parent</a><span>|</span><a href="#37852321">next</a><span>|</span><label class="collapse" for="c-37850576">[-]</label><label class="expand" for="c-37850576">[1 more]</label></div><br/><div class="children"><div class="content">Note that these products aren&#x27;t the same, even though they all fall under the category of observability - similarly to how you&#x27;d use Datadog AND Sentry, although both can be called &quot;observability platforms&quot;.<p>I do think vendor locking is a key differentiator, which some of the reasons why OpenTelemetry succeeded in the first place. I know that my previous company switched to OpenTelemetry for exactly this reason. You get the flexibility of using any platform you&#x27;d want (since we&#x27;re compatible with OpenTelemetry), so it&#x27;s not vendor-locking you to a specific platform with specific capabilities. Why use any of the ones you mention - maybe Datadog is enough if your use case is simple?<p>But there are more advantages - you get much more than just observability to the LLM itself - you can see calls to vector DBs, network calls, DB queries, etc. - this can be extremely useful IMO for RAG and autonomous agents for example</div><br/></div></div><div id="37852321" class="c"><input type="checkbox" id="c-37852321" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#37850507">parent</a><span>|</span><a href="#37850576">prev</a><span>|</span><a href="#37850583">next</a><span>|</span><label class="collapse" for="c-37852321">[-]</label><label class="expand" for="c-37852321">[1 more]</label></div><br/><div class="children"><div class="content">Just to add to Nir&#x27;s answer here:<p>Let&#x27;s say your application takes several steps to build up a prompt dynamically, such as a RAG pipeline. You&#x27;ll end up with a different prompt for potentially each user, depending on the application.<p>The result is you&#x27;ve likely increased the accuracy of the LLM, but at the expense of understanding the whole system&#x27;s behavior by introducing more steps upstream of the LLM call. Those steps could be super simple, or they could be (like in our case) dozens of steps that could all potentially fail or have a bug or whatever.<p>And so how do you wrangle all of this in context? You need something like OpenLLMetry that treats a request to an LLM as one of several components that make up a request and&#x2F;or user experience. Otherwise you&#x27;re just throwing stuff at the wall, guessing at what could improve stuff (or guessing at what could make an eval score better).</div><br/></div></div></div></div><div id="37850583" class="c"><input type="checkbox" id="c-37850583" checked=""/><div class="controls bullet"><span class="by">caniszczyk</span><span>|</span><a href="#37850507">prev</a><span>|</span><a href="#37847904">next</a><span>|</span><label class="collapse" for="c-37850583">[-]</label><label class="expand" for="c-37850583">[2 more]</label></div><br/><div class="children"><div class="content">Any thoughts of contributing this upstream directly or to CNCF?<p>We would be interested in hosting and supporting this type of work.<p>You can reach out to me via cra@linuxfoundation.org if you want to chat</div><br/><div id="37850602" class="c"><input type="checkbox" id="c-37850602" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37850583">parent</a><span>|</span><a href="#37847904">next</a><span>|</span><label class="collapse" for="c-37850602">[-]</label><label class="expand" for="c-37850602">[1 more]</label></div><br/><div class="children"><div class="content">Sure, would love to! I&#x27;ll ping you.</div><br/></div></div></div></div><div id="37847904" class="c"><input type="checkbox" id="c-37847904" checked=""/><div class="controls bullet"><span class="by">ramenmeal</span><span>|</span><a href="#37850583">prev</a><span>|</span><a href="#37853844">next</a><span>|</span><label class="collapse" for="c-37847904">[-]</label><label class="expand" for="c-37847904">[2 more]</label></div><br/><div class="children"><div class="content">What is the difference from using OpenLLMetry versus using OTel directly? Is the issue that there aren&#x27;t conventions for the needed attributes?</div><br/><div id="37847960" class="c"><input type="checkbox" id="c-37847960" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37847904">parent</a><span>|</span><a href="#37853844">next</a><span>|</span><label class="collapse" for="c-37847960">[-]</label><label class="expand" for="c-37847960">[1 more]</label></div><br/><div class="children"><div class="content">2 differences:<p>1. You don&#x27;t have instrumentations for libraries like OpenAI, LangChain, etc. so you need to manually open spans<p>2. As you said, there are no semantic conventions for logging things like prompts and chains.<p>What we did is just defined the new set of semantic conventions, and built the instrumentations. But we&#x27;re using vanilla OpenTelemetry so it&#x27;s fully compatible with standard OpenTelemetry.</div><br/></div></div></div></div><div id="37853844" class="c"><input type="checkbox" id="c-37853844" checked=""/><div class="controls bullet"><span class="by">hossko</span><span>|</span><a href="#37847904">prev</a><span>|</span><a href="#37853833">next</a><span>|</span><label class="collapse" for="c-37853844">[-]</label><label class="expand" for="c-37853844">[4 more]</label></div><br/><div class="children"><div class="content">Hello,<p>Is it possible to use Traceloop&#x27;s LLM instrumentations with already existing opentelemetry implementation ?</div><br/><div id="37854025" class="c"><input type="checkbox" id="c-37854025" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37853844">parent</a><span>|</span><a href="#37853833">next</a><span>|</span><label class="collapse" for="c-37854025">[-]</label><label class="expand" for="c-37854025">[3 more]</label></div><br/><div class="children"><div class="content">Yes, ofc. The LLM instrumentations are just like all other instrumentations.</div><br/><div id="37854306" class="c"><input type="checkbox" id="c-37854306" checked=""/><div class="controls bullet"><span class="by">hossko</span><span>|</span><a href="#37853844">root</a><span>|</span><a href="#37854025">parent</a><span>|</span><a href="#37853833">next</a><span>|</span><label class="collapse" for="c-37854306">[-]</label><label class="expand" for="c-37854306">[2 more]</label></div><br/><div class="children"><div class="content">Thank you,<p>Does it work on Azure OpenAI calls for langchain ? seems it did not work for me or im missing somethin</div><br/><div id="37854365" class="c"><input type="checkbox" id="c-37854365" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37853844">root</a><span>|</span><a href="#37854306">parent</a><span>|</span><a href="#37853833">next</a><span>|</span><label class="collapse" for="c-37854365">[-]</label><label class="expand" for="c-37854365">[1 more]</label></div><br/><div class="children"><div class="content">It should work, but LangChain has many quirks so it can depend on which syntax you&#x27;re using. Ping us on slack and we&#x27;ll assist -<p><a href="https:&#x2F;&#x2F;join.slack.com&#x2F;t&#x2F;traceloopcommunity&#x2F;shared_invite&#x2F;zt-1plpfpm6r-zOHKI028VkpcWdobX65C~g" rel="nofollow noreferrer">https:&#x2F;&#x2F;join.slack.com&#x2F;t&#x2F;traceloopcommunity&#x2F;shared_invite&#x2F;zt...</a></div><br/></div></div></div></div></div></div></div></div><div id="37853833" class="c"><input type="checkbox" id="c-37853833" checked=""/><div class="controls bullet"><span class="by">hossam-shehab</span><span>|</span><a href="#37853844">prev</a><span>|</span><a href="#37850047">next</a><span>|</span><label class="collapse" for="c-37853833">[-]</label><label class="expand" for="c-37853833">[1 more]</label></div><br/><div class="children"><div class="content">Hey,<p>Is it possible to used Traceloop LLM instrumentations only with already existing opentelemetry implementation</div><br/></div></div><div id="37850047" class="c"><input type="checkbox" id="c-37850047" checked=""/><div class="controls bullet"><span class="by">jeffchao</span><span>|</span><a href="#37853833">prev</a><span>|</span><a href="#37849325">next</a><span>|</span><label class="collapse" for="c-37850047">[-]</label><label class="expand" for="c-37850047">[2 more]</label></div><br/><div class="children"><div class="content">Cool! It looks like you effectively do auto instrumentation. Have you found there to be interesting nuances between LLM providers? Tracing is great and trace aggrgegates (with context!) cross-vendor would be even more awesome.</div><br/><div id="37850274" class="c"><input type="checkbox" id="c-37850274" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37850047">parent</a><span>|</span><a href="#37849325">next</a><span>|</span><label class="collapse" for="c-37850274">[-]</label><label class="expand" for="c-37850274">[1 more]</label></div><br/><div class="children"><div class="content">Wow, where do I start? The APIs are not that similar, but we&#x27;re trying to use the same set of semantic conventions for everyone so for example you&#x27;ll always get the model version, or the temperature in the same attribute. Which kinda means it&#x27;s identical cross-vendor, at least on the o11y side.<p>Here are all the semantic conventions we&#x27;ve defined so far -
<a href="https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry&#x2F;tree&#x2F;main&#x2F;packages&#x2F;opentelemetry-semantic-conventions-ai&#x2F;opentelemetry&#x2F;semconv&#x2F;ai">https:&#x2F;&#x2F;github.com&#x2F;traceloop&#x2F;openllmetry&#x2F;tree&#x2F;main&#x2F;packages&#x2F;...</a></div><br/></div></div></div></div><div id="37849325" class="c"><input type="checkbox" id="c-37849325" checked=""/><div class="controls bullet"><span class="by">serverlessmom</span><span>|</span><a href="#37850047">prev</a><span>|</span><a href="#37853420">next</a><span>|</span><label class="collapse" for="c-37849325">[-]</label><label class="expand" for="c-37849325">[2 more]</label></div><br/><div class="children"><div class="content">Pretty neat! I assume it&#x27;s just measuring traces right now? Any plans to add some top level metrics like build times, prompt length, etc?</div><br/><div id="37849501" class="c"><input type="checkbox" id="c-37849501" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37849325">parent</a><span>|</span><a href="#37853420">next</a><span>|</span><label class="collapse" for="c-37849501">[-]</label><label class="expand" for="c-37849501">[1 more]</label></div><br/><div class="children"><div class="content">Yes, only traces for now. We do want to send out metrics for prompt length, token usage, etc. like you mentioned. Hopefully will be available soon (and we welcome contributions :) )</div><br/></div></div></div></div><div id="37853420" class="c"><input type="checkbox" id="c-37853420" checked=""/><div class="controls bullet"><span class="by">VadimPR</span><span>|</span><a href="#37849325">prev</a><span>|</span><a href="#37851962">next</a><span>|</span><label class="collapse" for="c-37853420">[-]</label><label class="expand" for="c-37853420">[2 more]</label></div><br/><div class="children"><div class="content">Will vLLM be supported as well?</div><br/><div id="37854180" class="c"><input type="checkbox" id="c-37854180" checked=""/><div class="controls bullet"><span class="by">GalKlm</span><span>|</span><a href="#37853420">parent</a><span>|</span><a href="#37851962">next</a><span>|</span><label class="collapse" for="c-37854180">[-]</label><label class="expand" for="c-37854180">[1 more]</label></div><br/><div class="children"><div class="content">Hey it&#x27;s Gal from Traceloop,<p>That&#x27;s a good tbh. I wonder whether we should implement instrumentations for LLMs &quot;hosting solutions&quot; or for specific LLMs (E.g. LLaMa&#x2F;Falcon) and ignore the hosting solution (not sure if that&#x27;s even possible though as it sort of dictates the inference api).<p>wdyt?</div><br/></div></div></div></div><div id="37851962" class="c"><input type="checkbox" id="c-37851962" checked=""/><div class="controls bullet"><span class="by">archibaldJ</span><span>|</span><a href="#37853420">prev</a><span>|</span><a href="#37847496">next</a><span>|</span><label class="collapse" for="c-37851962">[-]</label><label class="expand" for="c-37851962">[4 more]</label></div><br/><div class="children"><div class="content">any smooth way to get this work with javascript? would love to use this in a project but my inferences are all in js</div><br/><div id="37852140" class="c"><input type="checkbox" id="c-37852140" checked=""/><div class="controls bullet"><span class="by">tomerf2</span><span>|</span><a href="#37851962">parent</a><span>|</span><a href="#37847496">next</a><span>|</span><label class="collapse" for="c-37852140">[-]</label><label class="expand" for="c-37852140">[3 more]</label></div><br/><div class="children"><div class="content">Definitely! (Tomer from Traceloop here)<p>We&#x27;ve already started developing the typescript SDK. Would love to see exactly what your use case is, so we can prioritize specific instrumentation and collaborate on it. We&#x27;ll ping you.</div><br/><div id="37853768" class="c"><input type="checkbox" id="c-37853768" checked=""/><div class="controls bullet"><span class="by">archibaldJ</span><span>|</span><a href="#37851962">root</a><span>|</span><a href="#37852140">parent</a><span>|</span><a href="#37847496">next</a><span>|</span><label class="collapse" for="c-37853768">[-]</label><label class="expand" for="c-37853768">[2 more]</label></div><br/><div class="children"><div class="content">Nice!<p>Does traceloop support OpenTelemetry Protocol File Exporter?<p>I&#x27;m the maintainer of Insomnium (<a href="https:&#x2F;&#x2F;github.com&#x2F;ArchGPT&#x2F;insomnium">https:&#x2F;&#x2F;github.com&#x2F;ArchGPT&#x2F;insomnium</a>) and I&#x27;m building a LanceDB-based prompt orchestration framework for automated software development that I&#x27;m integrating into Insomnium these few weeks. (The orchestration framework will also be open-source soon) Traceloop cloud looks good but I think for simple cases my users will prefer to have a 100% local solution.<p>would be nice to have a simple API to export to local; thanks!</div><br/><div id="37854043" class="c"><input type="checkbox" id="c-37854043" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37851962">root</a><span>|</span><a href="#37853768">parent</a><span>|</span><a href="#37847496">next</a><span>|</span><label class="collapse" for="c-37854043">[-]</label><label class="expand" for="c-37854043">[1 more]</label></div><br/><div class="children"><div class="content">Yes, since we&#x27;re using vanilla OpenTelemetry, you can set your exporter to whatever you want, including OpenTelemetry Protocol File Exporter. But I&#x27;d still use some sort of a dashboard, like Jaeger or one of the open source observability platforms like SigNoz or HyperDX that you can run locally.</div><br/></div></div></div></div></div></div></div></div><div id="37847496" class="c"><input type="checkbox" id="c-37847496" checked=""/><div class="controls bullet"><span class="by">brianhorakh</span><span>|</span><a href="#37851962">prev</a><span>|</span><a href="#37851992">next</a><span>|</span><label class="collapse" for="c-37847496">[-]</label><label class="expand" for="c-37847496">[2 more]</label></div><br/><div class="children"><div class="content">Any plans for pgvector? Graphana tempo</div><br/><div id="37847614" class="c"><input type="checkbox" id="c-37847614" checked=""/><div class="controls bullet"><span class="by">GalKlm</span><span>|</span><a href="#37847496">parent</a><span>|</span><a href="#37851992">next</a><span>|</span><label class="collapse" for="c-37847614">[-]</label><label class="expand" for="c-37847614">[1 more]</label></div><br/><div class="children"><div class="content">Hey Gal from Traceloop here,<p>We definitely have pgvector on our roadmap (which tbh I think we better publish in the repo). For Graphana tempo, it&#x27;s just a matter of making sure that it works as a destination - we&#x27;ll do it today&#x2F;tomorrow.</div><br/></div></div></div></div><div id="37851992" class="c"><input type="checkbox" id="c-37851992" checked=""/><div class="controls bullet"><span class="by">nadavwiz</span><span>|</span><a href="#37847496">prev</a><span>|</span><a href="#37850412">next</a><span>|</span><label class="collapse" for="c-37851992">[-]</label><label class="expand" for="c-37851992">[1 more]</label></div><br/><div class="children"><div class="content">Love it!</div><br/></div></div><div id="37850412" class="c"><input type="checkbox" id="c-37850412" checked=""/><div class="controls bullet"><span class="by">peter_d_sherman</span><span>|</span><a href="#37851992">prev</a><span>|</span><a href="#37847830">next</a><span>|</span><label class="collapse" for="c-37850412">[-]</label><label class="expand" for="c-37850412">[2 more]</label></div><br/><div class="children"><div class="content">Great idea!<p>Observability (AKA, debug&#x2F;proxy&#x2F;statistics&#x2F;logging&#x2F;visualization layer) -- for LLM&#x27;s (AKA Chat AI&#x27;s)...<p>Hmmm, you know, I would love something for ChatGPT (and other AI chatbots) -- where you could open a second tab or window -- and see (and potentially interact with) debug info and statistics from prompts given to that AI in its main input window, in realtime...<p>Sort of like what Unix&#x27;s STDERR is for programs running on Unix -- but an &quot;AI STDERR&quot; AKA debug channel, for AI&#x27;s...<p>I&#x27;m guessing (but not knowing) that in the future, there will be standards defined for debug interfaces to AI&#x27;s, standards defined for the data formats and protocols traversing those interfaces, and standards defined for such things as error, warning, hint, and informational messages...<p>Oh sure, a given AI company could pick a series of their own interfaces, data protocols and how to interpret that data.<p>But if so, that &quot;AI debug interface&quot; -- wouldn&#x27;t be universal.<p>Of course, on the flip side, if a universal &quot;AI debug interface&quot; were ever established, perhaps such a thing would eventually suffer from the complexities, over-engineering and bloatedness that plague many &quot;designed-by-committee&quot; standards in today&#x27;s world.<p>So, it will be interesting to see what the future holds...<p>To take an Elon Musk quote and twist it around (basically abuse it! &lt;g&gt;):<p>&quot;Proper engineering of future designed-by-committee standards with respect to AI interfaces and protocols is NOT guaranteed -- but excitement is!&quot;<p>:-) &lt;g&gt; :-)<p>Anyway, with respect to the main subject&#x2F;article&#x2F;authors, it&#x27;s a very interesting and future-thinking idea what you&#x27;re doing, you&#x27;re breaking new ground, and I wish you all of the future success with your company, business, product and product ideas!</div><br/><div id="37850530" class="c"><input type="checkbox" id="c-37850530" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37850412">parent</a><span>|</span><a href="#37847830">next</a><span>|</span><label class="collapse" for="c-37850530">[-]</label><label class="expand" for="c-37850530">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Related to what you&#x27;re saying, I was actually expecting some reactions from devs who&#x27;d ask &quot;why is it a separate repo and not part of opentelemetry from day 1?&quot;.<p>And for that my answer would be that I think having a separate repo would allow this to evolve in a more natural way, and faster (whereas OpenTelemetry, given it&#x27;s massive adoption already, evolves much slower, with committees etc.).<p>Then, at some point when this is stabilized and useful - we can merge.<p>Kind of like Tesla&#x27;s NACS vs. CCS</div><br/></div></div></div></div><div id="37847830" class="c"><input type="checkbox" id="c-37847830" checked=""/><div class="controls bullet"><span class="by">robertlagrant</span><span>|</span><a href="#37850412">prev</a><span>|</span><a href="#37847484">next</a><span>|</span><label class="collapse" for="c-37847830">[-]</label><label class="expand" for="c-37847830">[3 more]</label></div><br/><div class="children"><div class="content">Would&#x27;ve preferred LLMetry, My Dear Watson.</div><br/><div id="37847848" class="c"><input type="checkbox" id="c-37847848" checked=""/><div class="controls bullet"><span class="by">nirga</span><span>|</span><a href="#37847830">parent</a><span>|</span><a href="#37848587">next</a><span>|</span><label class="collapse" for="c-37847848">[-]</label><label class="expand" for="c-37847848">[1 more]</label></div><br/><div class="children"><div class="content">but it&#x27;s open! :)</div><br/></div></div></div></div><div id="37847484" class="c"><input type="checkbox" id="c-37847484" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#37847830">prev</a><span>|</span><a href="#37849098">next</a><span>|</span><label class="collapse" for="c-37847484">[-]</label><label class="expand" for="c-37847484">[2 more]</label></div><br/><div class="children"><div class="content">Worst pun ever, starred.</div><br/></div></div><div id="37849098" class="c"><input type="checkbox" id="c-37849098" checked=""/><div class="controls bullet"><span class="by">hartator</span><span>|</span><a href="#37847484">prev</a><span>|</span><label class="collapse" for="c-37849098">[-]</label><label class="expand" for="c-37849098">[2 more]</label></div><br/><div class="children"><div class="content">&gt; observability<p>I really don&#x27;t like that word for some reason. It&#x27;s abstracting away something simple. Logs? Graphs? Debug data? Telemetry data? There is way better words for &quot;this&quot;.</div><br/></div></div></div></div></div></div></div></body></html>