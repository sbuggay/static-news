<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1727773257269" as="style"/><link rel="stylesheet" href="styles.css?v=1727773257269"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://pytorch.org/blog/pytorch-native-architecture-optimization/">PyTorch Native Architecture Optimization: Torchao</a> <span class="domain">(<a href="https://pytorch.org">pytorch.org</a>)</span></div><div class="subtext"><span>jonbaer</span> | <span>31 comments</span></div><br/><div><div id="41703309" class="c"><input type="checkbox" id="c-41703309" checked=""/><div class="controls bullet"><span class="by">Atheb</span><span>|</span><a href="#41705947">next</a><span>|</span><label class="collapse" for="c-41703309">[-]</label><label class="expand" for="c-41703309">[13 more]</label></div><br/><div class="children"><div class="content">You got to give it to the pytorch team, they&#x27;re really great at bringing complex optimization schemes (mixed-precision, torch.compile, etc) down to a simple to use API. I&#x27;m glad I moved from TF&#x2F;Kerasto Pytorch around 2018-2019 and never looked back. I&#x27;m eager to try this as well.</div><br/><div id="41703843" class="c"><input type="checkbox" id="c-41703843" checked=""/><div class="controls bullet"><span class="by">ansk</span><span>|</span><a href="#41703309">parent</a><span>|</span><a href="#41705947">next</a><span>|</span><label class="collapse" for="c-41703843">[-]</label><label class="expand" for="c-41703843">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen and ignored a lot of &quot;pytorch good, tensorflow bad&quot; takes in my time, but this is so egregiously wrong I can&#x27;t help but chime in.  Facilitating graph-level optimizations has been one of the most central tenets of tensorflow&#x27;s design philosophy since its inception.  The XLA compiler was designed in close collaboration with the tensorflow team and was available in the tensorflow API as far back as 2017.  It&#x27;s not an exaggeration to say that pytorch is 5+ years behind on this front.  Before anyone invokes the words &quot;pythonic&quot; or &quot;ergonomic&quot;, I&#x27;d like to note that the tensorflow 2 API for compilation is nearly identical to torch.compile.</div><br/><div id="41704267" class="c"><input type="checkbox" id="c-41704267" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41703843">parent</a><span>|</span><a href="#41704546">next</a><span>|</span><label class="collapse" for="c-41704267">[-]</label><label class="expand" for="c-41704267">[8 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not about the API. its about the documentation + ecosystem.<p>TF&#x27;s doesn&#x27;t seem very good.   I just tried to figure out how to learn a linear mapping with TF and went through this:<p>1. googled &quot;linear layer in tensorflow&quot; and got to the page about linear.<p>2. spent 5 minutes trying to understand why monotonicity would be a central tenet of the documentation<p>3. realizing that&#x27;s not the right &quot;linear&quot; I couldn&#x27;t think of what the appropriate name would be<p>4. I know MLPs have them, google &quot;tensorflow mlp example&quot;<p>5. click the apr &#x27;24 page: <a href="https:&#x2F;&#x2F;www.tensorflow.org&#x2F;guide&#x2F;core&#x2F;mlp_core" rel="nofollow">https:&#x2F;&#x2F;www.tensorflow.org&#x2F;guide&#x2F;core&#x2F;mlp_core</a><p>6. read through 10[!] code blocks that are basically just boiler-plate setup of data and visualizations. entirely unrelated to MLPs<p>7. realize they call it &quot;dense&quot; in tensorflow world<p>8. see that &quot;dense&quot; needs to be implemented manually<p>9. think that&#x27;s strange, google &quot;tensorflow dense layer&quot;<p>10. find a keras API (<a href="https:&#x2F;&#x2F;www.tensorflow.org&#x2F;api_docs&#x2F;python&#x2F;tf&#x2F;keras&#x2F;layers&#x2F;Dense" rel="nofollow">https:&#x2F;&#x2F;www.tensorflow.org&#x2F;api_docs&#x2F;python&#x2F;tf&#x2F;keras&#x2F;layers&#x2F;D...</a>)</div><br/><div id="41706055" class="c"><input type="checkbox" id="c-41706055" checked=""/><div class="controls bullet"><span class="by">shmel</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704267">parent</a><span>|</span><a href="#41704313">next</a><span>|</span><label class="collapse" for="c-41706055">[-]</label><label class="expand" for="c-41706055">[1 more]</label></div><br/><div class="children"><div class="content">Oh god, you just gave me a flashback =) The last time I properly used TF was in early 2019, I am so happy that I don&#x27;t have to deal with this anymore.</div><br/></div></div><div id="41704313" class="c"><input type="checkbox" id="c-41704313" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704267">parent</a><span>|</span><a href="#41706055">prev</a><span>|</span><a href="#41704880">next</a><span>|</span><label class="collapse" for="c-41704313">[-]</label><label class="expand" for="c-41704313">[1 more]</label></div><br/><div class="children"><div class="content">11. notice that there&#x27;s a unicode rendering error (&quot;&amp;#x27;&quot; for apostrophe) on kernel_initializer and bias_initializer default arguments in the documentation, and wonder why on earth for such a high-level API one would want to expose  lora_rank as a first class construct. Also, 3 out of the 5 links in the &quot;Used in the guide&quot; links point to TF1 to TF2 migration articles - TF2 was released 5 years ago.</div><br/></div></div><div id="41704880" class="c"><input type="checkbox" id="c-41704880" checked=""/><div class="controls bullet"><span class="by">__rito__</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704267">parent</a><span>|</span><a href="#41704313">prev</a><span>|</span><a href="#41704347">next</a><span>|</span><label class="collapse" for="c-41704880">[-]</label><label class="expand" for="c-41704880">[1 more]</label></div><br/><div class="children"><div class="content">Re 6: TF&#x2F;Keras team motivates random people to write long tutorials and be featured in the official site and their tutorial be included in the official guides. I have seen a lot of subpar devs&#x2F;AI people write subpar tutorials and brag on twitter how their tutorials are included in the official Keras site.<p>I have seen some good ones, too, of course.</div><br/></div></div><div id="41704347" class="c"><input type="checkbox" id="c-41704347" checked=""/><div class="controls bullet"><span class="by">n_u</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704267">parent</a><span>|</span><a href="#41704880">prev</a><span>|</span><a href="#41705699">next</a><span>|</span><label class="collapse" for="c-41704347">[-]</label><label class="expand" for="c-41704347">[2 more]</label></div><br/><div class="children"><div class="content">To add onto this I feel like one of the hard things about TF is that there is like at least 3 ways to do everything because they have supported multiple APIs and migrated to eager. So if you find an example or an open source project it might not be for the flavor of tensorflow that your codebase is in.</div><br/><div id="41704866" class="c"><input type="checkbox" id="c-41704866" checked=""/><div class="controls bullet"><span class="by">__rito__</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704347">parent</a><span>|</span><a href="#41705699">next</a><span>|</span><label class="collapse" for="c-41704866">[-]</label><label class="expand" for="c-41704866">[1 more]</label></div><br/><div class="children"><div class="content">Moreover, the way you find might not be the best or the most efficient way.</div><br/></div></div></div></div><div id="41705699" class="c"><input type="checkbox" id="c-41705699" checked=""/><div class="controls bullet"><span class="by">mft_</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704267">parent</a><span>|</span><a href="#41704347">prev</a><span>|</span><a href="#41705761">next</a><span>|</span><label class="collapse" for="c-41705699">[-]</label><label class="expand" for="c-41705699">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, this example holds true for roughly half of the Python ecosystem; and you can square the level of frustration if it&#x27;s also anything coming from Google.<p>(This pattern is relatively easy to understand: smart people creating something get their gratification from the creation process, not writing tedious documentation; and this is systemically embedded for people at Google, who are probably <i>directly</i> incentivised in a similar way.)</div><br/></div></div><div id="41705761" class="c"><input type="checkbox" id="c-41705761" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704267">parent</a><span>|</span><a href="#41705699">prev</a><span>|</span><a href="#41704546">next</a><span>|</span><label class="collapse" for="c-41705761">[-]</label><label class="expand" for="c-41705761">[1 more]</label></div><br/><div class="children"><div class="content">I feel like that with every single Google api doc. if there&#x27;s a variable called x, the documentation will be &quot;variable to store x&quot;. and you need to create&#x2F;supply 5 different resources before you can create an x, but these will each require 5 further things to be figured out before you can create one of them.</div><br/></div></div></div></div><div id="41704546" class="c"><input type="checkbox" id="c-41704546" checked=""/><div class="controls bullet"><span class="by">catgary</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41703843">parent</a><span>|</span><a href="#41704267">prev</a><span>|</span><a href="#41704728">next</a><span>|</span><label class="collapse" for="c-41704546">[-]</label><label class="expand" for="c-41704546">[2 more]</label></div><br/><div class="children"><div class="content">I think tensorflow-datasets and tensorflow-serving are great, but for model development I think most people use JAX and then export it to a tensorflow SavedModel with Orbax.</div><br/><div id="41705322" class="c"><input type="checkbox" id="c-41705322" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41704546">parent</a><span>|</span><a href="#41704728">next</a><span>|</span><label class="collapse" for="c-41705322">[-]</label><label class="expand" for="c-41705322">[1 more]</label></div><br/><div class="children"><div class="content">But IIUC Jax also leverages XLA and for the purpose of this discussion the frontend matters only inasmuch people feel productive in using it. Whether that&#x27;s TF or Jax.</div><br/></div></div></div></div><div id="41704728" class="c"><input type="checkbox" id="c-41704728" checked=""/><div class="controls bullet"><span class="by">whymauri</span><span>|</span><a href="#41703309">root</a><span>|</span><a href="#41703843">parent</a><span>|</span><a href="#41704546">prev</a><span>|</span><a href="#41705947">next</a><span>|</span><label class="collapse" for="c-41704728">[-]</label><label class="expand" for="c-41704728">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m so sorry but Tensorflow is simply one of the worst parts of my job.</div><br/></div></div></div></div></div></div><div id="41705947" class="c"><input type="checkbox" id="c-41705947" checked=""/><div class="controls bullet"><span class="by">Evidlo</span><span>|</span><a href="#41703309">prev</a><span>|</span><a href="#41703732">next</a><span>|</span><label class="collapse" for="c-41705947">[-]</label><label class="expand" for="c-41705947">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We’re happy to officially launch torchao, a PyTorch native library that makes models faster and smaller by leveraging low bit dtypes<p>Will this let me use uint8 arrays as indexing arrays?  A problem I have is that pytorch forces me to use uint64 for fancy indexing.</div><br/></div></div><div id="41703732" class="c"><input type="checkbox" id="c-41703732" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41705947">prev</a><span>|</span><a href="#41703143">next</a><span>|</span><label class="collapse" for="c-41703732">[-]</label><label class="expand" for="c-41703732">[14 more]</label></div><br/><div class="children"><div class="content">Hi! I&#x27;m Mark from the PyTorch team at Meta and work on torchao. If you have any questions about the library or really anything at all about performance, don&#x27;t hesitate to ask!</div><br/><div id="41705491" class="c"><input type="checkbox" id="c-41705491" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#41703732">parent</a><span>|</span><a href="#41704986">next</a><span>|</span><label class="collapse" for="c-41705491">[-]</label><label class="expand" for="c-41705491">[2 more]</label></div><br/><div class="children"><div class="content">Great stuff!<p>A minor nitpick on the copy (and even then, it might just be me): I find &quot;97% speedup&quot; and &quot;50% speedup&quot; really hard to parse — a &quot;30x speedup&quot; or &quot;97% reduction of time taken&quot; immediately tell me what is being achieved!<p>Great results once I get my head around them, though!</div><br/><div id="41705905" class="c"><input type="checkbox" id="c-41705905" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41705491">parent</a><span>|</span><a href="#41704986">next</a><span>|</span><label class="collapse" for="c-41705905">[-]</label><label class="expand" for="c-41705905">[1 more]</label></div><br/><div class="children"><div class="content">Fwiw I&#x27;m pretty sure 97% speedup is 197% of the speed of the baseline, so roughly double.</div><br/></div></div></div></div><div id="41704986" class="c"><input type="checkbox" id="c-41704986" checked=""/><div class="controls bullet"><span class="by">dark__paladin</span><span>|</span><a href="#41703732">parent</a><span>|</span><a href="#41705491">prev</a><span>|</span><a href="#41704007">next</a><span>|</span><label class="collapse" for="c-41704986">[-]</label><label class="expand" for="c-41704986">[2 more]</label></div><br/><div class="children"><div class="content">First off, well done, this looks exciting. I haven&#x27;t had a chance to interact with the library yet — should torchao be seen as a dev-friendly quantization interface? I.e., if my team was working on new quantization techniques, does the API provide easy tooling for implementing and benchmarking new quantization algorithms? Or is this closer to a &quot;toolbox of finished (generally) finished products&quot;?</div><br/><div id="41705252" class="c"><input type="checkbox" id="c-41705252" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41704986">parent</a><span>|</span><a href="#41704007">next</a><span>|</span><label class="collapse" for="c-41705252">[-]</label><label class="expand" for="c-41705252">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s both! For this blog we decided to discuss our best end user facing numbers to keep things simple. We briefly hint at our contributor guide here <a href="https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;ao&#x2F;issues&#x2F;391">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;ao&#x2F;issues&#x2F;391</a> which does a tour of the APIs we provide developers implementing new algorithms<p>But we have had quantization algorithm developers such as HQQ or Autoround merge their code in to get composability and serialization for free. We view quantization algorithms as the top layer and going down you have quantized tensors, quant primitives like dequant&#x2F;quant and finally basic dtypes like uint1-7 and float3-8. Personally why I spent so much time on AO was I was hoping we could make it easier for people to express their quantization algorithms in easy to read PyTorch code and if they must use custom kernels we also have some tutorials for how to integrate custom cuda and triton ops.<p>Most of those discussions have been happening on #torchao on discord.gg&#x2F;gpumode so if you need to chat back and forth feel free to reach out to the team there otherwise Github also works.</div><br/></div></div></div></div><div id="41704007" class="c"><input type="checkbox" id="c-41704007" checked=""/><div class="controls bullet"><span class="by">DhawalModi</span><span>|</span><a href="#41703732">parent</a><span>|</span><a href="#41704986">prev</a><span>|</span><a href="#41704295">next</a><span>|</span><label class="collapse" for="c-41704007">[-]</label><label class="expand" for="c-41704007">[3 more]</label></div><br/><div class="children"><div class="content">Hi Mark, the library looks cool, excited to try it out. Coincidentally I am starting work on a project that is investigating a lot of Post training quantization methods. I read the blog and I am curious to understand what kind of overheads are involved in quantizing a layer?</div><br/><div id="41704100" class="c"><input type="checkbox" id="c-41704100" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41704007">parent</a><span>|</span><a href="#41704295">next</a><span>|</span><label class="collapse" for="c-41704100">[-]</label><label class="expand" for="c-41704100">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a bunch of overhead associated with PTQ - but TL;DR is that much of that overhead goes away when you&#x27;re using `torch.compile()` and `torchao.autoquant()`<p>Essentially the latency overhead comes from quantizing and dequantizing weights and activations. For large layers this overhead is small because by quantizing your weights for example you reduce memory bandwidth pressure but for small layers the overhead of potentially looking up a table, reading scaling factors, quantization&#x2F;dequantization and finally handling zero points might not be worth it.<p>However, even if such overhead exists you can still quantize your model and get it to be smaller it might not be faster is the problem. We solve the speed problem in 2 ways - `torch.compile()` will fuse operations like a dequant and matmul into a single kernel and `torchao.autoquant()` will do kernel level profiling to see whether a layer is actually made faster when quantizing and if not it skips quantizing that layer.</div><br/><div id="41704217" class="c"><input type="checkbox" id="c-41704217" checked=""/><div class="controls bullet"><span class="by">DhawalModi</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41704100">parent</a><span>|</span><a href="#41704295">next</a><span>|</span><label class="collapse" for="c-41704217">[-]</label><label class="expand" for="c-41704217">[1 more]</label></div><br/><div class="children"><div class="content">I see, thank you for the explanation!</div><br/></div></div></div></div></div></div><div id="41704295" class="c"><input type="checkbox" id="c-41704295" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41703732">parent</a><span>|</span><a href="#41704007">prev</a><span>|</span><a href="#41704059">next</a><span>|</span><label class="collapse" for="c-41704295">[-]</label><label class="expand" for="c-41704295">[2 more]</label></div><br/><div class="children"><div class="content">Why don&#x27;t they merge this into Pytorch? Why so many packages?</div><br/><div id="41704323" class="c"><input type="checkbox" id="c-41704323" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41704295">parent</a><span>|</span><a href="#41704059">next</a><span>|</span><label class="collapse" for="c-41704323">[-]</label><label class="expand" for="c-41704323">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s different tradeoffs, spinning up a separate repo is what we call &quot;out of core&quot; vs having everything in PyTorch &quot;in core&quot;<p>Basically PyTorch is a large library where CI takes a long time to run which means merging code is hard and adding new dependencies is challenging and there are stringent constraints on BC breaking changes<p>Instead what torchao did and many other repos like torchtune, torchchat, torchtitan did was move out of core and it helps keep the core PyTorch library leaner with a smaller binary size and it really lets the team &quot;out of core&quot; focus on optimizing for their needs<p>Unfortunately the argument for what gets better changes over time, for example torch.compile initially a new repo called torchdynamo was built out of core to move fast but eventually merged back because everyone wanted to use it. Now torch.compile dev velocity is still quite fast and so now we have to tell people to use nightlies instead of official stable releases to which some people have asked me why don&#x27;t you move torch.compile out of core<p>My 2c is the ecosystem will be much stronger and teams can move faster if they develop out of core so that&#x27;s the tradeoff we picked for torchao. We managed to for example merge a few custom CPP kernels like fp6 or Marlin that would have challenging to motivate in core since those are still quite experimental and need to stand the test of time.</div><br/></div></div></div></div><div id="41704059" class="c"><input type="checkbox" id="c-41704059" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#41703732">parent</a><span>|</span><a href="#41704295">prev</a><span>|</span><a href="#41703143">next</a><span>|</span><label class="collapse" for="c-41704059">[-]</label><label class="expand" for="c-41704059">[4 more]</label></div><br/><div class="children"><div class="content">Thanks for the hard work, any idea what the roadmap is for MPS support?</div><br/><div id="41704077" class="c"><input type="checkbox" id="c-41704077" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41704059">parent</a><span>|</span><a href="#41703143">next</a><span>|</span><label class="collapse" for="c-41704077">[-]</label><label class="expand" for="c-41704077">[3 more]</label></div><br/><div class="children"><div class="content">Most of our performance relies on leveraging torch.compile which generates Triton kernels which run fast on CPU and GPU but not MPS since Triton does not support generating Metal kernels. So you lose the nice story of writing low bit code in pure PyTorch but also get it running fast.<p>In these cases the only path forward we have is writing custom Metal kernels and plugging those in. That work is still ongoing and we&#x27;ll hopefully have more to share soon.</div><br/><div id="41704167" class="c"><input type="checkbox" id="c-41704167" checked=""/><div class="controls bullet"><span class="by">underanalyzer</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41704077">parent</a><span>|</span><a href="#41703143">next</a><span>|</span><label class="collapse" for="c-41704167">[-]</label><label class="expand" for="c-41704167">[2 more]</label></div><br/><div class="children"><div class="content">This might not be the right place for this question but, as someone who has made a couple very modest mps backend contributions, I&#x27;m curious why not add metal support to triton (or a fork if openai won&#x27;t allow it) rather than maintain a whole separate backend?</div><br/><div id="41704189" class="c"><input type="checkbox" id="c-41704189" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41703732">root</a><span>|</span><a href="#41704167">parent</a><span>|</span><a href="#41703143">next</a><span>|</span><label class="collapse" for="c-41704189">[-]</label><label class="expand" for="c-41704189">[1 more]</label></div><br/><div class="children"><div class="content">Mostly comes down to what&#x27;s fastest to develop, it&#x27;s faster to write a few custom kernels than it is to develop a new compiler backend<p>Granted after more upfront effort compilers are just such a significant UX boost that indeed you are making me question why I don&#x27;t spend more time working on this myself lol</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41703143" class="c"><input type="checkbox" id="c-41703143" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#41703732">prev</a><span>|</span><a href="#41704256">next</a><span>|</span><label class="collapse" for="c-41703143">[-]</label><label class="expand" for="c-41703143">[1 more]</label></div><br/><div class="children"><div class="content">This is a cool project! Understanding lower bits is still on my to do list, perhaps I&#x27;ll spin this up for a go</div><br/></div></div><div id="41704256" class="c"><input type="checkbox" id="c-41704256" checked=""/><div class="controls bullet"><span class="by">CalChris</span><span>|</span><a href="#41703143">prev</a><span>|</span><label class="collapse" for="c-41704256">[-]</label><label class="expand" for="c-41704256">[1 more]</label></div><br/><div class="children"><div class="content">Is this what <i>Mojo</i> is supposed to be?</div><br/></div></div></div></div></div></div></div></body></html>