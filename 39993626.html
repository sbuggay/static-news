<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712826053866" as="style"/><link rel="stylesheet" href="styles.css?v=1712826053866"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/google-deepmind/recurrentgemma">Implementation of Google&#x27;s Griffin Architecture – RNN LLM</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>milliondreams</span> | <span>34 comments</span></div><br/><div><div id="39994628" class="c"><input type="checkbox" id="c-39994628" checked=""/><div class="controls bullet"><span class="by">VHRanger</span><span>|</span><a href="#39998813">next</a><span>|</span><label class="collapse" for="c-39994628">[-]</label><label class="expand" for="c-39994628">[21 more]</label></div><br/><div class="children"><div class="content">Like RWKV and Mamba, this is mixing some RNN properties to avoid the issues transformers have.<p>However I&#x27;m curious about their scaling claims. They have a plot that shows how the model scales in training with the FLOPs you throw at it.<p>But the issue we should rather be concerned with is the wall time of training for a set amount of hardware.<p>Back in 2018, we could train medium sized RNNs, the issue was with wall time of training and training stability.</div><br/><div id="39994871" class="c"><input type="checkbox" id="c-39994871" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39994628">parent</a><span>|</span><a href="#39994916">next</a><span>|</span><label class="collapse" for="c-39994871">[-]</label><label class="expand" for="c-39994871">[4 more]</label></div><br/><div class="children"><div class="content">transformers were also just better at the LM task than 2018 RNNs for equal amount of flop training</div><br/><div id="39995007" class="c"><input type="checkbox" id="c-39995007" checked=""/><div class="controls bullet"><span class="by">VHRanger</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39994871">parent</a><span>|</span><a href="#39994916">next</a><span>|</span><label class="collapse" for="c-39995007">[-]</label><label class="expand" for="c-39995007">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, that&#x27;s just the training stability part to my knowledge</div><br/><div id="39995136" class="c"><input type="checkbox" id="c-39995136" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39995007">parent</a><span>|</span><a href="#39994916">next</a><span>|</span><label class="collapse" for="c-39995136">[-]</label><label class="expand" for="c-39995136">[2 more]</label></div><br/><div class="children"><div class="content">they&#x27;re also just less capable models. like just adding attention on top of an RNN made them a lot better</div><br/><div id="39997630" class="c"><input type="checkbox" id="c-39997630" checked=""/><div class="controls bullet"><span class="by">SpaceManNabs</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39995136">parent</a><span>|</span><a href="#39994916">next</a><span>|</span><label class="collapse" for="c-39997630">[-]</label><label class="expand" for="c-39997630">[1 more]</label></div><br/><div class="children"><div class="content">Calculating self-attention is still quadratic though. So you get the negatives of transformers there too.</div><br/></div></div></div></div></div></div></div></div><div id="39994916" class="c"><input type="checkbox" id="c-39994916" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#39994628">parent</a><span>|</span><a href="#39994871">prev</a><span>|</span><a href="#39995050">next</a><span>|</span><label class="collapse" for="c-39994916">[-]</label><label class="expand" for="c-39994916">[15 more]</label></div><br/><div class="children"><div class="content">Do you know the downside with RWKV? Based on how they present it, it seems like the best thing since sliced bread, but I would have assumed that it would have been widely adopted if that were the case.</div><br/><div id="39997465" class="c"><input type="checkbox" id="c-39997465" checked=""/><div class="controls bullet"><span class="by">kouteiheika</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39994916">parent</a><span>|</span><a href="#39999012">next</a><span>|</span><label class="collapse" for="c-39997465">[-]</label><label class="expand" for="c-39997465">[4 more]</label></div><br/><div class="children"><div class="content">The downside is that it&#x27;s bad (like, <i>really</i> bad) on a certain subset of tasks. I once trained RWKVv4 model on a machine translation task and no matter how much I scaled it up it just didn&#x27;t work <i>at all</i>, while an equivalent transformer did the job without a problem.<p>Intuitively this does make sense, because a transformer can at any time &quot;look back&quot; at the source sentence and at what it has previously generated (due to its attention mechanism) for every token it outputs, while an RNN like RWKV has to compress this into its internal state which is both lossy and limited in size.<p>I haven&#x27;t looked at the new versions of RWKV (apparently we&#x27;re at v6 now), but hopefully it performs better now. In the end I think that a hybrid architecture probably makes the most sense - have some sort of an attention mechanism for the near context, and an RNN-like state for far context, and that would give you the best of both worlds.</div><br/><div id="39997751" class="c"><input type="checkbox" id="c-39997751" checked=""/><div class="controls bullet"><span class="by">inciampati</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39997465">parent</a><span>|</span><a href="#39999012">next</a><span>|</span><label class="collapse" for="c-39997751">[-]</label><label class="expand" for="c-39997751">[3 more]</label></div><br/><div class="children"><div class="content">What about multiple passes over the data? Make it recurse.</div><br/><div id="39997773" class="c"><input type="checkbox" id="c-39997773" checked=""/><div class="controls bullet"><span class="by">kouteiheika</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39997751">parent</a><span>|</span><a href="#39999012">next</a><span>|</span><label class="collapse" for="c-39997773">[-]</label><label class="expand" for="c-39997773">[2 more]</label></div><br/><div class="children"><div class="content">I also tried that - try to get it to iteratively &quot;refine&quot; its translation. I don&#x27;t remember all of the details at this point, but in general it didn&#x27;t help much. (Although maybe I just did it suboptimally and there might have been a better way to do it.)<p>I&#x27;m guessing scaling the model up <i>massively</i> would probably make it work in one shot (so that whatever it was translating would fit into its state), but I didn&#x27;t really have the compute to try that.</div><br/><div id="39998296" class="c"><input type="checkbox" id="c-39998296" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39997773">parent</a><span>|</span><a href="#39999012">next</a><span>|</span><label class="collapse" for="c-39998296">[-]</label><label class="expand" for="c-39998296">[1 more]</label></div><br/><div class="children"><div class="content">Lstm?</div><br/></div></div></div></div></div></div></div></div><div id="39999012" class="c"><input type="checkbox" id="c-39999012" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39994916">parent</a><span>|</span><a href="#39997465">prev</a><span>|</span><a href="#39995039">next</a><span>|</span><label class="collapse" for="c-39999012">[-]</label><label class="expand" for="c-39999012">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if this is the type of answer you&#x27;re looking for, but RWKV is not really recurrent the same way RNNs are recurrent. This quasi-recurrentness allows it and its comrades to use algorithms like parallel SCAN to achieve log N complexity when parallelised. But you pay for that in terms of state-tracking.<p>There&#x27;s a cool talk here if you care to know the details:<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4-VXe1yPDjk" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4-VXe1yPDjk</a></div><br/></div></div><div id="39995039" class="c"><input type="checkbox" id="c-39995039" checked=""/><div class="controls bullet"><span class="by">VHRanger</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39994916">parent</a><span>|</span><a href="#39999012">prev</a><span>|</span><a href="#39995621">next</a><span>|</span><label class="collapse" for="c-39995039">[-]</label><label class="expand" for="c-39995039">[4 more]</label></div><br/><div class="children"><div class="content">It seems only OK as a model? Looking at the LLM chat leaderboard it&#x27;s 71st and the 14B version is worse than a lot of 7B models:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;lmsys&#x2F;chatbot-arena-leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;lmsys&#x2F;chatbot-arena-leaderboar...</a><p>Also, llama.cpp makes inference accessible for a lot of people, and it&#x27;s not available for RWKV.<p>Not to knock on the model, I&#x27;m sure it&#x27;s good. I also like that it&#x27;s a succesful example of citizen science.<p>It&#x27;s just not popular enough to have the inference infrastructure transformers have, not established enough to attract enough money to get 60B+ models trained, and so on.</div><br/><div id="39995196" class="c"><input type="checkbox" id="c-39995196" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39995039">parent</a><span>|</span><a href="#39998773">next</a><span>|</span><label class="collapse" for="c-39995196">[-]</label><label class="expand" for="c-39995196">[1 more]</label></div><br/><div class="children"><div class="content">This leaderboard is not the best for comparing model architectures, the dataset and finetuning have too much influence. I think perplexity on a particular dataset would be a better way to compare</div><br/></div></div><div id="39998773" class="c"><input type="checkbox" id="c-39998773" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39995039">parent</a><span>|</span><a href="#39995196">prev</a><span>|</span><a href="#39995130">next</a><span>|</span><label class="collapse" for="c-39998773">[-]</label><label class="expand" for="c-39998773">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Also, llama.cpp makes inference accessible for a lot of people, and it&#x27;s not available for RWKV.<p>It absolutely is: <a href="https:&#x2F;&#x2F;github.com&#x2F;RWKV&#x2F;rwkv.cpp">https:&#x2F;&#x2F;github.com&#x2F;RWKV&#x2F;rwkv.cpp</a> .</div><br/></div></div><div id="39995130" class="c"><input type="checkbox" id="c-39995130" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39995039">parent</a><span>|</span><a href="#39998773">prev</a><span>|</span><a href="#39995621">next</a><span>|</span><label class="collapse" for="c-39995130">[-]</label><label class="expand" for="c-39995130">[1 more]</label></div><br/><div class="children"><div class="content">i believe it is undertrained, at minimum</div><br/></div></div></div></div><div id="39995621" class="c"><input type="checkbox" id="c-39995621" checked=""/><div class="controls bullet"><span class="by">jimmyl02</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39994916">parent</a><span>|</span><a href="#39995039">prev</a><span>|</span><a href="#39995050">next</a><span>|</span><label class="collapse" for="c-39995621">[-]</label><label class="expand" for="c-39995621">[5 more]</label></div><br/><div class="children"><div class="content">From what I know about RWKV, it&#x27;s mostly a one man effort and doesn&#x27;t have the same data pipeline &#x2F; resources as most major labs. It&#x27;s a bit unfortunate but I&#x27;m curious about the performance given the same training corpus as OpenAI&#x27;s GPTs. Maybe some labs have tried internally but haven&#x27;t released results? On the other hand it makes sense to invest more money into transformer training runs as they have been proven to work.<p>They really burst onto the scene and brought back RNNs in the world of transformers. The claim that RWKV isn&#x27;t paralleizable during training also seems to be refuted in their readme. I&#x27;d guess it&#x27;s generalizable performance as there is a difference between doing well on benchmarks and being usable. Personally I&#x27;ve tried running the weights a long time ago when it was first released and the results weren&#x27;t usable but I&#x27;m sure there has been considerable progress since then.</div><br/><div id="39997515" class="c"><input type="checkbox" id="c-39997515" checked=""/><div class="controls bullet"><span class="by">kouteiheika</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39995621">parent</a><span>|</span><a href="#39998035">next</a><span>|</span><label class="collapse" for="c-39997515">[-]</label><label class="expand" for="c-39997515">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The claim that RWKV isn&#x27;t paralleizable during training also seems to be refuted in their readme.<p>RNNs are trivially parallizable (I&#x27;ve done it myself), as long as you&#x27;re training them on multiple documents in parallel and have enough memory for the state for each document. You just train them 1 token at a time across N documents, instead of the transformer-like N tokens at a time across 1 document.</div><br/><div id="39997667" class="c"><input type="checkbox" id="c-39997667" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39997515">parent</a><span>|</span><a href="#39998035">next</a><span>|</span><label class="collapse" for="c-39997667">[-]</label><label class="expand" for="c-39997667">[2 more]</label></div><br/><div class="children"><div class="content">RWKV is parallel at the level of the sequence like a transformer. Its formulation allows for each timestep t to be calculated in parallel except for a single serial scan at the end for aggregation which they use a custom cuda kernel to do.</div><br/><div id="39997753" class="c"><input type="checkbox" id="c-39997753" checked=""/><div class="controls bullet"><span class="by">kouteiheika</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39997667">parent</a><span>|</span><a href="#39998035">next</a><span>|</span><label class="collapse" for="c-39997753">[-]</label><label class="expand" for="c-39997753">[1 more]</label></div><br/><div class="children"><div class="content">I know. I trained RWKV myself using both methods, like a transformer and like an RNN.<p>Ultimately it probably doesn&#x27;t matter that you can train it like a transformer because you can just train it in parallel on multiple documents simultaneously one token at a time, and, at least from my experience, this worked just as well, if not better.<p>Plus, doing it this way is more general because you don&#x27;t need any custom kernels to do it, and it also helps the model to learn to deal with an &quot;infinite&quot; context better (while if you train it like a transformer its performance will regress once you evaluate it outside of the context window on which you&#x27;ve trained it, at least from what I&#x27;ve seen in my training runs).</div><br/></div></div></div></div></div></div><div id="39998035" class="c"><input type="checkbox" id="c-39998035" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#39994628">root</a><span>|</span><a href="#39995621">parent</a><span>|</span><a href="#39997515">prev</a><span>|</span><a href="#39995050">next</a><span>|</span><label class="collapse" for="c-39998035">[-]</label><label class="expand" for="c-39998035">[1 more]</label></div><br/><div class="children"><div class="content">I played around with RWKV some time ago (maybe early 2023?) with similarly disappointing results, but my suspicion was that this was a dataset&#x2F;training issue, not an architectural one. Leaderboard performance has improved a lot since then, and anecdotally, I&#x27;ve seen&#x2F;heard some quite decent RWKV TTS experiments, so I&#x27;m bullish.<p>Also, the team has incorporated&#x2F;raised money from investors (recursal.ai), so it&#x27;s no longer a one man effort.</div><br/></div></div></div></div></div></div><div id="39995050" class="c"><input type="checkbox" id="c-39995050" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39994628">parent</a><span>|</span><a href="#39994916">prev</a><span>|</span><a href="#39998813">next</a><span>|</span><label class="collapse" for="c-39995050">[-]</label><label class="expand" for="c-39995050">[1 more]</label></div><br/><div class="children"><div class="content">The paper shows that the speed is comparable to transformer models, faster with smaller with &quot;long&quot; sequence length like 8k.</div><br/></div></div></div></div><div id="39998813" class="c"><input type="checkbox" id="c-39998813" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#39994628">prev</a><span>|</span><a href="#39994640">next</a><span>|</span><label class="collapse" for="c-39998813">[-]</label><label class="expand" for="c-39998813">[2 more]</label></div><br/><div class="children"><div class="content">For anyone interested in a C++ implementation, our github.com&#x2F;google&#x2F;gemma.cpp now supports this model.</div><br/><div id="39999864" class="c"><input type="checkbox" id="c-39999864" checked=""/><div class="controls bullet"><span class="by">JyrkiAlakuijala</span><span>|</span><a href="#39998813">parent</a><span>|</span><a href="#39994640">next</a><span>|</span><label class="collapse" for="c-39999864">[-]</label><label class="expand" for="c-39999864">[1 more]</label></div><br/><div class="children"><div class="content">Fun fact -- gemma.cpp uses highway, an amazing high performance computation library originally developed in the JPEG XL effort.</div><br/></div></div></div></div><div id="39994640" class="c"><input type="checkbox" id="c-39994640" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39998813">prev</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39994640">[-]</label><label class="expand" for="c-39994640">[8 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t get one detail: they selected 6B transformer as baseline and compared it to 7B Griffin<p>Why wouldn&#x27;t select equal size models?..</div><br/><div id="39994681" class="c"><input type="checkbox" id="c-39994681" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#39994640">parent</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39994681">[-]</label><label class="expand" for="c-39994681">[7 more]</label></div><br/><div class="children"><div class="content">They probably had them for some reason and it was cheaper not to retrain one of them again</div><br/><div id="39994714" class="c"><input type="checkbox" id="c-39994714" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39994640">root</a><span>|</span><a href="#39994681">parent</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39994714">[-]</label><label class="expand" for="c-39994714">[6 more]</label></div><br/><div class="children"><div class="content">Its just performance comparison is misleading then, they report marginal improvements which is expected just because of models size differences..</div><br/><div id="39995082" class="c"><input type="checkbox" id="c-39995082" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39994640">root</a><span>|</span><a href="#39994714">parent</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39995082">[-]</label><label class="expand" for="c-39995082">[5 more]</label></div><br/><div class="children"><div class="content">It also performs better on any other size.</div><br/><div id="39995123" class="c"><input type="checkbox" id="c-39995123" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39994640">root</a><span>|</span><a href="#39995082">parent</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39995123">[-]</label><label class="expand" for="c-39995123">[4 more]</label></div><br/><div class="children"><div class="content">They have baseline transformer of max size 6B in tables. Other models are trained on very different data and probably differently.</div><br/><div id="39995262" class="c"><input type="checkbox" id="c-39995262" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39994640">root</a><span>|</span><a href="#39995123">parent</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39995262">[-]</label><label class="expand" for="c-39995262">[3 more]</label></div><br/><div class="children"><div class="content">All the MQA transformers, Hawk and Griffin are trained on the same MassiveText dataset so no.</div><br/><div id="39995353" class="c"><input type="checkbox" id="c-39995353" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39994640">root</a><span>|</span><a href="#39995262">parent</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39995353">[-]</label><label class="expand" for="c-39995353">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but MQA is limited to 6B size, while &quot;other&quot; larger non-RNN models in table(Llama-2) are not trained on the same dataset, and Hawk and Griffin are 7B. Sorry, I don&#x27;t understand your point.</div><br/><div id="39995445" class="c"><input type="checkbox" id="c-39995445" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39994640">root</a><span>|</span><a href="#39995353">parent</a><span>|</span><a href="#39995604">next</a><span>|</span><label class="collapse" for="c-39995445">[-]</label><label class="expand" for="c-39995445">[1 more]</label></div><br/><div class="children"><div class="content">The point is that it also beats the baseline on every other size (1B and 3B). So it wouldn&#x27;t be surprising to see it beat a 7B transformer model like the 6B model. Note 2 on page 5 probably explains why the sizes are different.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39995604" class="c"><input type="checkbox" id="c-39995604" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39994640">prev</a><span>|</span><label class="collapse" for="c-39995604">[-]</label><label class="expand" for="c-39995604">[2 more]</label></div><br/><div class="children"><div class="content">im not smart enough to know the significance of this...is Griffin like MAMBA?</div><br/><div id="39996525" class="c"><input type="checkbox" id="c-39996525" checked=""/><div class="controls bullet"><span class="by">VHRanger</span><span>|</span><a href="#39995604">parent</a><span>|</span><label class="collapse" for="c-39996525">[-]</label><label class="expand" for="c-39996525">[1 more]</label></div><br/><div class="children"><div class="content">Yes, like RWKV and Mamba this is a new generation of models that are more like big RNNs than pure transformers we have now</div><br/></div></div></div></div></div></div></div></div></div></body></html>