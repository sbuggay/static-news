<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691744463831" as="style"/><link rel="stylesheet" href="styles.css?v=1691744463831"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/microsoft/Llama-2-Onnx">Llama 2 on ONNX runs locally</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>tmoneyy</span> | <span>69 comments</span></div><br/><div><div id="37083455" class="c"><input type="checkbox" id="c-37083455" checked=""/><div class="controls bullet"><span class="by">c_o_n_v_e_x</span><span>|</span><a href="#37082245">next</a><span>|</span><label class="collapse" for="c-37083455">[-]</label><label class="expand" for="c-37083455">[12 more]</label></div><br/><div class="children"><div class="content">For anyone running locally, could you please describe your hardware setup?  CPU only? CPU+GPU(s)?  How much memory? What type of CPU? Particularly interested in larger models (say &gt;30b params).<p>For transparency, I work for an x86 motherboard manufacturer and the LLM-on-local-hw space is very interesting.  If you&#x27;re having trouble finding the right HW, would love to hear those pain points.</div><br/><div id="37083603" class="c"><input type="checkbox" id="c-37083603" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37083455">parent</a><span>|</span><a href="#37085974">next</a><span>|</span><label class="collapse" for="c-37083603">[-]</label><label class="expand" for="c-37083603">[1 more]</label></div><br/><div class="children"><div class="content">The most popular performant desktop llm runtimes are pure GPU (exLLAMA) or GPU + CPU (llama.cpp). People stuff the biggest models that will fit into the collective RAM + VRAM pool, up to ~48GB for llama 70B. Sometimes users will split models across two 24GB CUDA GPUs.<p>Inference for me is bottlenecked by my GPU and CPU RAM bandwidth. TBH the biggest frustration is that OEMs like y&#x27;all can&#x27;t double up VRAM like you could in the old days, or sell platforms with a beefy IGP, and that quad channel+ CPUs are too expensive.<p>Vulkan is a popular target runtimes seem to be heading for. IGPs with access to lots of memory capacity + bandwidth will be very desirable, and I hear Intel&#x2F;AMD are cooking up quad channel IGPs.<p>On the server side, everyone is running Nvidia boxes, I guess. But I had a dream about an affordable llama.cpp host: The cheapest Sapphire Rapids HBM SKUs, with no DIMM slots, on a tiny, dirt cheap motherboard you can pack into a rack like sardines. Llama.cpp is bottlenecked by bandwidth, and ~64GB is perfect.</div><br/></div></div><div id="37085974" class="c"><input type="checkbox" id="c-37085974" checked=""/><div class="controls bullet"><span class="by">mk_stjames</span><span>|</span><a href="#37083455">parent</a><span>|</span><a href="#37083603">prev</a><span>|</span><a href="#37084169">next</a><span>|</span><label class="collapse" for="c-37085974">[-]</label><label class="expand" for="c-37085974">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re at a motherboard manufacturer I have some definite points for you to hear.<p>One is.. there are essentially zero motherboards that space out x16 pci-e slots so that you can appropriately use more than 2 triple-slot GPUs.   3090s and 4090s are all triple slot cards, but often motherboard are putting x16 slots spaced 2-apart, with x8 or less slots in between.  There may be a few that allow you to fit 2x cards, but none that would support 3x I don&#x27;t think, and definitely none that do 4x.  Obviously that would result in a non-standard length motherboard (much taller).  But in the ML world it would be appreciated because it would be possible to build quad card systems without watercooling cards or using A5000&#x2F;A6000 or other dual slot, expensive datacenter cards.<p>And then, even for dual-slot cards like the A5000&#x2F;A6000 etc again there are very few motherboards that you can get the x16 slots spaced appropriately.  The Supermicro H12SSL-i is about the only one that gets 4 x16 slots double-slot spaced appropriately and in a way that you could run 4 blower or WC&#x27;d cards and not overlap something else.  And then, even when you do, you have the problem of the pin headers on the bottom of the motherboard interfere with the last card.  That location of the pin headers is archaic and annoying and just needs to die.<p>Remember those mining-rig specialty motherboards, with all the wide-spaced pci-e slots for like 8x GPU&#x27;s at once?  we need that, but with x16-bandwidth slots.  Those mining cards were typically only x1 bandwidth slots (even if they were x16 length) because for mining, bandwidth between cards and CPU isn&#x27;t a problem, but for ML it is.<p>Sure, these won&#x27;t fit the typical ATX case standards.  But if you build it, they will come.</div><br/></div></div><div id="37084169" class="c"><input type="checkbox" id="c-37084169" checked=""/><div class="controls bullet"><span class="by">ImprobableTruth</span><span>|</span><a href="#37083455">parent</a><span>|</span><a href="#37085974">prev</a><span>|</span><a href="#37085591">next</a><span>|</span><label class="collapse" for="c-37084169">[-]</label><label class="expand" for="c-37084169">[7 more]</label></div><br/><div class="children"><div class="content">I use two 3090s to run the 70b model at a good speed. Takes 32 gigs of vram, more depending on context. I tried CPU+GPU (5900X + 3090) but with extended context it&#x27;s slow enough that I wouldn&#x27;t recommend it (~1 token&#x2F;s). CPU only gets &quot;let it run over night&quot; slow. Works ok-ish for with a small context though (even if it&#x27;s still &quot;non-interactive&quot; slow).</div><br/><div id="37084352" class="c"><input type="checkbox" id="c-37084352" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#37083455">root</a><span>|</span><a href="#37084169">parent</a><span>|</span><a href="#37085591">next</a><span>|</span><label class="collapse" for="c-37084352">[-]</label><label class="expand" for="c-37084352">[6 more]</label></div><br/><div class="children"><div class="content">What’s the difference in output quality between that and the 33b parameter model? That would fit entirely in vram, right?</div><br/><div id="37084563" class="c"><input type="checkbox" id="c-37084563" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37083455">root</a><span>|</span><a href="#37084352">parent</a><span>|</span><a href="#37085591">next</a><span>|</span><label class="collapse" for="c-37084563">[-]</label><label class="expand" for="c-37084563">[5 more]</label></div><br/><div class="children"><div class="content">The 33B model is llama V1. Facebook reportedly held back 34B llama v2 because it failed some safety metrics.<p>So... Generally the quality is worse, but the available set of finetunes is totally different. Some llama v1 33b finetunes are not available in 70B, and extremely good at their niche.<p>Also 70B should get more than 1 token&#x2F;sec on a single 3090 offloaded to CPU. I dunno what framework op is using.</div><br/><div id="37085358" class="c"><input type="checkbox" id="c-37085358" checked=""/><div class="controls bullet"><span class="by">a20eac1d</span><span>|</span><a href="#37083455">root</a><span>|</span><a href="#37084563">parent</a><span>|</span><a href="#37084873">next</a><span>|</span><label class="collapse" for="c-37085358">[-]</label><label class="expand" for="c-37085358">[1 more]</label></div><br/><div class="children"><div class="content">Any chance you could point me in the right direction on how to set something like this up?<p>Right now, I&#x27;m using pure CPU Llama but only the 17B version, based on I believe llama.cpp. How do I mix both CPU and GPU together for more performance?</div><br/></div></div><div id="37084873" class="c"><input type="checkbox" id="c-37084873" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#37083455">root</a><span>|</span><a href="#37084563">parent</a><span>|</span><a href="#37085358">prev</a><span>|</span><a href="#37085591">next</a><span>|</span><label class="collapse" for="c-37084873">[-]</label><label class="expand" for="c-37084873">[3 more]</label></div><br/><div class="children"><div class="content">what example niche?</div><br/><div id="37085149" class="c"><input type="checkbox" id="c-37085149" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37083455">root</a><span>|</span><a href="#37084873">parent</a><span>|</span><a href="#37085591">next</a><span>|</span><label class="collapse" for="c-37085149">[-]</label><label class="expand" for="c-37085149">[2 more]</label></div><br/><div class="children"><div class="content">- roleplaying (chronos merged with airoboros)<p>- theraputic&#x2F;friend style chat (Samantha)<p>- translation (various single language finetines)<p>- medical advice (can&#x27;t remember this one)<p>This is non exhaustive. And Llama V2&#x27;s extended native context does really help some niches (like storytelling) that a few 33B models are still pretty good at.</div><br/><div id="37085321" class="c"><input type="checkbox" id="c-37085321" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#37083455">root</a><span>|</span><a href="#37085149">parent</a><span>|</span><a href="#37085591">next</a><span>|</span><label class="collapse" for="c-37085321">[-]</label><label class="expand" for="c-37085321">[1 more]</label></div><br/><div class="children"><div class="content">&gt; medical advice (can&#x27;t remember this one)<p>You&#x27;re probably thinking of Clinical Camel: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;augtoma&#x2F;qCammel-70-x" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;augtoma&#x2F;qCammel-70-x</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37085591" class="c"><input type="checkbox" id="c-37085591" checked=""/><div class="controls bullet"><span class="by">sandGorgon</span><span>|</span><a href="#37083455">parent</a><span>|</span><a href="#37084169">prev</a><span>|</span><a href="#37084090">next</a><span>|</span><label class="collapse" for="c-37085591">[-]</label><label class="expand" for="c-37085591">[1 more]</label></div><br/><div class="children"><div class="content">In EdgeChains, we run models using DeepJavaLibrary (DJL). Preferably only CPU - more focused on edge+embedding usecases</div><br/></div></div></div></div><div id="37082245" class="c"><input type="checkbox" id="c-37082245" checked=""/><div class="controls bullet"><span class="by">rrherr</span><span>|</span><a href="#37083455">prev</a><span>|</span><a href="#37083552">next</a><span>|</span><label class="collapse" for="c-37082245">[-]</label><label class="expand" for="c-37082245">[21 more]</label></div><br/><div class="children"><div class="content">How does this compare to using <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a> with <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?search=thebloke&#x2F;llama-2-ggml" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;models?search=thebloke&#x2F;llama-2-ggml</a> ?</div><br/><div id="37082380" class="c"><input type="checkbox" id="c-37082380" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082245">parent</a><span>|</span><a href="#37082370">next</a><span>|</span><label class="collapse" for="c-37082380">[-]</label><label class="expand" for="c-37082380">[16 more]</label></div><br/><div class="children"><div class="content">Very unfavorably. Mostly because the ONNX models are FP32&#x2F;FP16 (so ~3-4x the RAM use), but also because llama.cpp is well optimized with many features (like prompt caching, grammar, device splitting, context extending, cfg...)<p>MLC&#x27;s Apache TVM implementation is also excellent. The autotuning in particular is like black magic.</div><br/><div id="37082968" class="c"><input type="checkbox" id="c-37082968" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082380">parent</a><span>|</span><a href="#37082637">next</a><span>|</span><label class="collapse" for="c-37082968">[-]</label><label class="expand" for="c-37082968">[1 more]</label></div><br/><div class="children"><div class="content">Speaking of MLC - recently discovered they have a iphone app that can do lama7b locally on high end iphones at decent pace. Bit hard to find in the store given the ocean of API front end apps - called MLCchat.</div><br/></div></div><div id="37082637" class="c"><input type="checkbox" id="c-37082637" checked=""/><div class="controls bullet"><span class="by">skeletoncrew</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082380">parent</a><span>|</span><a href="#37082968">prev</a><span>|</span><a href="#37082370">next</a><span>|</span><label class="collapse" for="c-37082637">[-]</label><label class="expand" for="c-37082637">[14 more]</label></div><br/><div class="children"><div class="content">I tried quite a few of these and the ONNX one seems the most elegantly put together of all. I’m impressed.<p>Speed can be improved. Quick and dirty&#x2F;hype solutions, not sure.<p>I really hope ONNX gets traction it deserves.</div><br/><div id="37083126" class="c"><input type="checkbox" id="c-37083126" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082637">parent</a><span>|</span><a href="#37082675">next</a><span>|</span><label class="collapse" for="c-37083126">[-]</label><label class="expand" for="c-37083126">[3 more]</label></div><br/><div class="children"><div class="content">Unless I&#x27;m working in an academic project I couldn&#x27;t care less about elegancy.<p>Speed (and ergo costs) trumps elegancy in industrial settings imho, specially considering how expensive to run LLMs are. Such an OSS dependency can be improved or at least &quot;wrapped&quot; to isolate it from the rest of your codebase.</div><br/><div id="37083461" class="c"><input type="checkbox" id="c-37083461" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37083126">parent</a><span>|</span><a href="#37082675">next</a><span>|</span><label class="collapse" for="c-37083461">[-]</label><label class="expand" for="c-37083461">[2 more]</label></div><br/><div class="children"><div class="content">As do features.<p>Even if llama.cpp was ~30% slower and the same size as ONNX, something like a custom grammar implementation or an extended context would be a huge deal.</div><br/><div id="37085416" class="c"><input type="checkbox" id="c-37085416" checked=""/><div class="controls bullet"><span class="by">kanwisher</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37083461">parent</a><span>|</span><a href="#37082675">next</a><span>|</span><label class="collapse" for="c-37085416">[-]</label><label class="expand" for="c-37085416">[1 more]</label></div><br/><div class="children"><div class="content">ONNX doesn’t give you any benefits over something like llama.cpp and it’s significantly slower for zero advantage</div><br/></div></div></div></div></div></div><div id="37082675" class="c"><input type="checkbox" id="c-37082675" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082637">parent</a><span>|</span><a href="#37083126">prev</a><span>|</span><a href="#37082670">next</a><span>|</span><label class="collapse" for="c-37082675">[-]</label><label class="expand" for="c-37082675">[9 more]</label></div><br/><div class="children"><div class="content">&gt; Quick and dirty&#x2F;hype solutions, not sure.<p>Curious what you mean by this</div><br/><div id="37082824" class="c"><input type="checkbox" id="c-37082824" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082675">parent</a><span>|</span><a href="#37082670">next</a><span>|</span><label class="collapse" for="c-37082824">[-]</label><label class="expand" for="c-37082824">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s tough to hear and communicate, but TL;DR: it&#x27;s very good for HN headlines to make a $X.cpp but it&#x27;s not the right tool for products.<p>There&#x27;s only going to be more of these models and ONNX starts from the right place, cross-platform and from base principles rather than coupling tightly to one model structure.<p>Most importantly, it is freakin&#x27; awesome, the comments thus far, 30 in, don&#x27;t reflect what its like to use or it&#x27;s technical realities.*<p>* the main threads of discussion are &quot;not even wrong&quot;: float16 is big compared to float4 (its trivial to quantize to your liking) and looking for an alternative that supports CoreML (ONNX is the magic that makes your model take advantage of CoreML &#x2F; WebGPU &#x2F; WebGL &#x2F; whatever Android&#x27;s marketing name for its API is etc. etc. etc.)</div><br/><div id="37083030" class="c"><input type="checkbox" id="c-37083030" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082824">parent</a><span>|</span><a href="#37083009">next</a><span>|</span><label class="collapse" for="c-37083030">[-]</label><label class="expand" for="c-37083030">[5 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s very good for HN headlines to make a $X.cpp but it&#x27;s not the right tool for products.<p>Funny how it&#x27;s the opposite.<p>ONNX in this case, outside of the HN headline and saying &quot;we did it&quot; is almost useless.<p>LLMs are so heavy that you can&#x27;t afford running a suboptimized version. This FP16 ONNX takes 4x as much memory and is probably 5-10x slower than something hand optimized such as llama.cpp or exllama with 4 bit quants.</div><br/><div id="37083241" class="c"><input type="checkbox" id="c-37083241" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37083030">parent</a><span>|</span><a href="#37083009">next</a><span>|</span><label class="collapse" for="c-37083241">[-]</label><label class="expand" for="c-37083241">[4 more]</label></div><br/><div class="children"><div class="content">Right, and as my comment alludes to, quantize it to FP4, they got a nice CLI tool that took me about 15 seconds to do that to a 150 MB model. Here youd do that with each shard</div><br/><div id="37083414" class="c"><input type="checkbox" id="c-37083414" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37083241">parent</a><span>|</span><a href="#37083277">next</a><span>|</span><label class="collapse" for="c-37083414">[-]</label><label class="expand" for="c-37083414">[2 more]</label></div><br/><div class="children"><div class="content">Straight 4 bit quantization with LLMs sacrifices some quality. The currently popular frameworks quantize the models in blocks to get better results, which also makes runtime performance less straightforward.<p>I think ONNX would need to natively support this &quot;packed&quot; format or otherwise quantize fp16 models from disk on the fly... Which is a problem, as the FP16 models are huge.</div><br/><div id="37083978" class="c"><input type="checkbox" id="c-37083978" checked=""/><div class="controls bullet"><span class="by">ImprobableTruth</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37083414">parent</a><span>|</span><a href="#37083277">next</a><span>|</span><label class="collapse" for="c-37083978">[-]</label><label class="expand" for="c-37083978">[1 more]</label></div><br/><div class="children"><div class="content">&quot;some&quot; quality - naive quantization (that isn&#x27;t just the usual full-&gt;half) absolutely mutilates the model, to the point that picking a smaller model is much better.<p>gpt-q is also more than just group wise quantization and takes a decent while. Quantizing models without a major performance hit is not possible on the fly.</div><br/></div></div></div></div></div></div></div></div><div id="37083009" class="c"><input type="checkbox" id="c-37083009" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082824">parent</a><span>|</span><a href="#37083030">prev</a><span>|</span><a href="#37083100">next</a><span>|</span><label class="collapse" for="c-37083009">[-]</label><label class="expand" for="c-37083009">[1 more]</label></div><br/><div class="children"><div class="content">When hardware is so expensive and so difficult to obtain, performance starts to trump the cost of “single model implementation” pretty quickly.<p>It can be cheaper to deploy Llama.cpp and then foobar.cpp (6 months from now) than it is to have inference that is 2x slower.<p>Interestingly in the LLM space all these model servers seem to be converging to using the same API as OpenAI, making it easy to swap containers to get a different model+inference server with 0 code change.</div><br/></div></div><div id="37083100" class="c"><input type="checkbox" id="c-37083100" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082824">parent</a><span>|</span><a href="#37083009">prev</a><span>|</span><a href="#37082670">next</a><span>|</span><label class="collapse" for="c-37083100">[-]</label><label class="expand" for="c-37083100">[1 more]</label></div><br/><div class="children"><div class="content">TVM is all of this. It even has compilation support for exotic devices like FPGAs, phone ASICs and such. MLIR frameworks are heading this direction too.<p>&gt; its trivial to quantize to your liking<p>...Except its not implemented in the demo. Also, quantization is far from simple.<p>All this sounds good, but I have seen cool sounding ONNX demos for years, and (outside of some fairly quick one off TensorRT demos) I havent really seen the pavement hit the road.</div><br/></div></div></div></div></div></div><div id="37082670" class="c"><input type="checkbox" id="c-37082670" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082637">parent</a><span>|</span><a href="#37082675">prev</a><span>|</span><a href="#37082370">next</a><span>|</span><label class="collapse" for="c-37082670">[-]</label><label class="expand" for="c-37082670">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ONNX one seems the most elegantly put together of all.<p>What do you mean by this? The demo UI? Code quality?</div><br/></div></div></div></div></div></div><div id="37082370" class="c"><input type="checkbox" id="c-37082370" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#37082245">parent</a><span>|</span><a href="#37082380">prev</a><span>|</span><a href="#37082334">next</a><span>|</span><label class="collapse" for="c-37082370">[-]</label><label class="expand" for="c-37082370">[3 more]</label></div><br/><div class="children"><div class="content">Ggml &#x2F; llama.cpp has a lot of hardware optimizations built in now, CPU, GPU and specific instruction sets like for apple silicon (I&#x27;m not familiar with the names). I would want to know how many of those are also present in onnx and available to this model.<p>There are currently also more quantization options available as mentioned. Though those incur a performance loss (they make the model faster but worse) so it depends on what you&#x27;re optimizing for.</div><br/><div id="37082429" class="c"><input type="checkbox" id="c-37082429" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082245">root</a><span>|</span><a href="#37082370">parent</a><span>|</span><a href="#37082334">next</a><span>|</span><label class="collapse" for="c-37082429">[-]</label><label class="expand" for="c-37082429">[2 more]</label></div><br/><div class="children"><div class="content">ONNX is a format. There are different runtimes for different devices... But I can&#x27;t speak for any of them.<p>&gt; specific instruction sets like for apple silicon<p>You are thinking of the Accelerate framework support, which is basically Apple&#x27;s ARM CPU SIMD library.<p>But Llama.cpp also has a Metal GPU backend, which is the defacto backend for Apple devices now.</div><br/></div></div></div></div><div id="37082334" class="c"><input type="checkbox" id="c-37082334" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37082245">parent</a><span>|</span><a href="#37082370">prev</a><span>|</span><a href="#37083552">next</a><span>|</span><label class="collapse" for="c-37082334">[-]</label><label class="expand" for="c-37082334">[1 more]</label></div><br/><div class="children"><div class="content">These are still FP16&#x2F;32 models, almost certainly a few times slower and larger than the latest N bit quantized GGMLs.</div><br/></div></div></div></div><div id="37083552" class="c"><input type="checkbox" id="c-37083552" checked=""/><div class="controls bullet"><span class="by">abrookewood</span><span>|</span><a href="#37082245">prev</a><span>|</span><a href="#37086320">next</a><span>|</span><label class="collapse" for="c-37083552">[-]</label><label class="expand" for="c-37083552">[1 more]</label></div><br/><div class="children"><div class="content">For anyone unsure what ONNX actually is: &quot;ONNX is an open format built to represent machine learning models ... [which] defines a common set of operators ... a common file format ... [and should make]  it easier to access hardware optimizations&quot;.<p>[0] <a href="https:&#x2F;&#x2F;onnx.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;onnx.ai&#x2F;</a></div><br/></div></div><div id="37086320" class="c"><input type="checkbox" id="c-37086320" checked=""/><div class="controls bullet"><span class="by">hint23</span><span>|</span><a href="#37083552">prev</a><span>|</span><a href="#37082393">next</a><span>|</span><label class="collapse" for="c-37086320">[-]</label><label class="expand" for="c-37086320">[1 more]</label></div><br/><div class="children"><div class="content">For best performance on x86 CPU and Nvidia GPU, ts_server is interesting ( <a href="https:&#x2F;&#x2F;bellard.org&#x2F;ts_server" rel="nofollow noreferrer">https:&#x2F;&#x2F;bellard.org&#x2F;ts_server</a> ).</div><br/></div></div><div id="37082393" class="c"><input type="checkbox" id="c-37082393" checked=""/><div class="controls bullet"><span class="by">hashtag-til</span><span>|</span><a href="#37086320">prev</a><span>|</span><a href="#37083583">next</a><span>|</span><label class="collapse" for="c-37082393">[-]</label><label class="expand" for="c-37082393">[7 more]</label></div><br/><div class="children"><div class="content">This is very cool! I really hope the ONNX project gets much more adoption in the next months and years and help reduce the fragmentation in the ML ecosystem.</div><br/><div id="37082539" class="c"><input type="checkbox" id="c-37082539" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082393">parent</a><span>|</span><a href="#37082540">next</a><span>|</span><label class="collapse" for="c-37082539">[-]</label><label class="expand" for="c-37082539">[5 more]</label></div><br/><div class="children"><div class="content">Eh... I have seen ONNX demos for years, and they tend to stay barebones and slow, kinda like this.<p>NCNN, MLIR and TVM based ports have been far more impressive.</div><br/><div id="37083476" class="c"><input type="checkbox" id="c-37083476" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#37082393">root</a><span>|</span><a href="#37082539">parent</a><span>|</span><a href="#37083102">next</a><span>|</span><label class="collapse" for="c-37083476">[-]</label><label class="expand" for="c-37083476">[2 more]</label></div><br/><div class="children"><div class="content">Lololol show me an &quot;MLIR&quot; port. Do you mean tensorflow port or jax port or torch port (that uses torch-mlir)? Or do you really mean llama implemented in linalg&#x2F;tosa&#x2F;tendor?</div><br/><div id="37083814" class="c"><input type="checkbox" id="c-37083814" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082393">root</a><span>|</span><a href="#37083476">parent</a><span>|</span><a href="#37083102">next</a><span>|</span><label class="collapse" for="c-37083814">[-]</label><label class="expand" for="c-37083814">[1 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t talking about Llama specifically. I was thinking of the SHARK Stable Diffusion port (which uses MLIR&#x2F;IREE), as it considerably outpaced the ONNX runtime.<p>But apparently the performance of llama on torch-mlir is progressing: 
<a href="https:&#x2F;&#x2F;github.com&#x2F;nod-ai&#x2F;SHARK&#x2F;issues&#x2F;1707">https:&#x2F;&#x2F;github.com&#x2F;nod-ai&#x2F;SHARK&#x2F;issues&#x2F;1707</a></div><br/></div></div></div></div><div id="37083102" class="c"><input type="checkbox" id="c-37083102" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#37082393">root</a><span>|</span><a href="#37082539">parent</a><span>|</span><a href="#37083476">prev</a><span>|</span><a href="#37082540">next</a><span>|</span><label class="collapse" for="c-37083102">[-]</label><label class="expand" for="c-37083102">[2 more]</label></div><br/><div class="children"><div class="content">Have you used TVM much? Is it only good for inference?</div><br/><div id="37083347" class="c"><input type="checkbox" id="c-37083347" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082393">root</a><span>|</span><a href="#37083102">parent</a><span>|</span><a href="#37082540">next</a><span>|</span><label class="collapse" for="c-37083347">[-]</label><label class="expand" for="c-37083347">[1 more]</label></div><br/><div class="children"><div class="content">Inference only, as far as I know.<p>TBH the mlc projects were my first exposure, but they are all impressive, as are the bits of source I poked through. And the autotuning makes a <i>huge</i> difference in my quick testing.</div><br/></div></div></div></div></div></div><div id="37082540" class="c"><input type="checkbox" id="c-37082540" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#37082393">parent</a><span>|</span><a href="#37082539">prev</a><span>|</span><a href="#37083583">next</a><span>|</span><label class="collapse" for="c-37082540">[-]</label><label class="expand" for="c-37082540">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure there&#x27;s much chance of that happening. ONNX seems to be the broadest in coverage, but for basically any model ONNX supports, there&#x27;s a faster alternative.<p>For the latest generative&#x2F;transformer stuff (whisper, llama, etc) it&#x27;s often specialized C(++) stuff, but torch 2.0 compilation keeps geting better, BetterTransformers, TensorRT, etc.</div><br/></div></div></div></div><div id="37083583" class="c"><input type="checkbox" id="c-37083583" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#37082393">prev</a><span>|</span><a href="#37085931">next</a><span>|</span><label class="collapse" for="c-37083583">[-]</label><label class="expand" for="c-37083583">[4 more]</label></div><br/><div class="children"><div class="content">How does Llama 2 compare to GPT-4? I see a lot of discussion about it but not much comparison. I don&#x27;t have the hardware to run the 13b or 30b model locally so I&#x27;d be running it in the cloud anyway. In that case, should I stick with GPT-4?</div><br/><div id="37084013" class="c"><input type="checkbox" id="c-37084013" checked=""/><div class="controls bullet"><span class="by">ImprobableTruth</span><span>|</span><a href="#37083583">parent</a><span>|</span><a href="#37085931">next</a><span>|</span><label class="collapse" for="c-37084013">[-]</label><label class="expand" for="c-37084013">[3 more]</label></div><br/><div class="children"><div class="content">GPT-4 trounces everything else available. I&#x27;d say that 70b is about 3.5 level, unless you&#x27;re doing something where finetuning greatly benefits you.<p>13b is alright for toy applications, but the difference to gpt 3.5 (let alone gpt4) is huge.</div><br/><div id="37084061" class="c"><input type="checkbox" id="c-37084061" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#37083583">root</a><span>|</span><a href="#37084013">parent</a><span>|</span><a href="#37086164">next</a><span>|</span><label class="collapse" for="c-37084061">[-]</label><label class="expand" for="c-37084061">[1 more]</label></div><br/><div class="children"><div class="content">Vicuna 1.5 13b seems comparable to GPT 3.5 to me, and the fact that you can run it locally on last gen commodity hardware is incredible.<p>I bought a dodgy used mining 3090 last year and now my computer is writing poetry in the terminal in real time.</div><br/></div></div><div id="37086164" class="c"><input type="checkbox" id="c-37086164" checked=""/><div class="controls bullet"><span class="by">spacebanana7</span><span>|</span><a href="#37083583">root</a><span>|</span><a href="#37084013">parent</a><span>|</span><a href="#37084061">prev</a><span>|</span><a href="#37085931">next</a><span>|</span><label class="collapse" for="c-37086164">[-]</label><label class="expand" for="c-37086164">[1 more]</label></div><br/><div class="children"><div class="content">Llama 2 has uncensored versions. For many applications that alone makes it superior.</div><br/></div></div></div></div></div></div><div id="37085931" class="c"><input type="checkbox" id="c-37085931" checked=""/><div class="controls bullet"><span class="by">FL33TW00D</span><span>|</span><a href="#37083583">prev</a><span>|</span><a href="#37082447">next</a><span>|</span><label class="collapse" for="c-37085931">[-]</label><label class="expand" for="c-37085931">[2 more]</label></div><br/><div class="children"><div class="content">I think unfortunately ONNX is doomed.
The spec is incredibly bloated by now, and the fact that all graphs are entirely static just isn&#x27;t feasible for modern ML.</div><br/><div id="37085992" class="c"><input type="checkbox" id="c-37085992" checked=""/><div class="controls bullet"><span class="by">fouronnes3</span><span>|</span><a href="#37085931">parent</a><span>|</span><a href="#37082447">next</a><span>|</span><label class="collapse" for="c-37085992">[-]</label><label class="expand" for="c-37085992">[1 more]</label></div><br/><div class="children"><div class="content">Is there an alternative then?</div><br/></div></div></div></div><div id="37082447" class="c"><input type="checkbox" id="c-37082447" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#37085931">prev</a><span>|</span><a href="#37082269">next</a><span>|</span><label class="collapse" for="c-37082447">[-]</label><label class="expand" for="c-37082447">[12 more]</label></div><br/><div class="children"><div class="content">Does anyone know the feasibility of converting the ONNX model to CoreML for accelerated inference on Apple devices?</div><br/><div id="37082471" class="c"><input type="checkbox" id="c-37082471" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#37082447">parent</a><span>|</span><a href="#37082655">next</a><span>|</span><label class="collapse" for="c-37082471">[-]</label><label class="expand" for="c-37082471">[2 more]</label></div><br/><div class="children"><div class="content">They used to have this: <a href="https:&#x2F;&#x2F;github.com&#x2F;onnx&#x2F;onnx-coreml">https:&#x2F;&#x2F;github.com&#x2F;onnx&#x2F;onnx-coreml</a></div><br/><div id="37082798" class="c"><input type="checkbox" id="c-37082798" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37082447">root</a><span>|</span><a href="#37082471">parent</a><span>|</span><a href="#37082655">next</a><span>|</span><label class="collapse" for="c-37082798">[-]</label><label class="expand" for="c-37082798">[1 more]</label></div><br/><div class="children"><div class="content">They still do. HN is way behind on ONNX and I&#x27;d go so far as to say it&#x27;s the &quot;Plastics.&quot;[1] of 2023.<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PSxihhBzCjk">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PSxihhBzCjk</a></div><br/></div></div></div></div><div id="37082655" class="c"><input type="checkbox" id="c-37082655" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37082447">parent</a><span>|</span><a href="#37082471">prev</a><span>|</span><a href="#37082681">next</a><span>|</span><label class="collapse" for="c-37082655">[-]</label><label class="expand" for="c-37082655">[6 more]</label></div><br/><div class="children"><div class="content">If you’re working with LLMs, just use this - <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a><p>It has Metal support.</div><br/><div id="37082890" class="c"><input type="checkbox" id="c-37082890" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37082447">root</a><span>|</span><a href="#37082655">parent</a><span>|</span><a href="#37082681">next</a><span>|</span><label class="collapse" for="c-37082890">[-]</label><label class="expand" for="c-37082890">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s sort of a non-sequitor, so does ONNX. Conversely, $X.cpp is great for local hobbyist stuff but not at all for deployment to iOS.</div><br/><div id="37083023" class="c"><input type="checkbox" id="c-37083023" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37082447">root</a><span>|</span><a href="#37082890">parent</a><span>|</span><a href="#37082681">next</a><span>|</span><label class="collapse" for="c-37083023">[-]</label><label class="expand" for="c-37083023">[4 more]</label></div><br/><div class="children"><div class="content">Nobody is deploying 3+GB models to iOS beyond some enthusiast “because you can” apps. Amazing tech but not feasible for any mainstream use yet.<p>Eg: <a href="https:&#x2F;&#x2F;apps.apple.com&#x2F;app&#x2F;id6444050820" rel="nofollow noreferrer">https:&#x2F;&#x2F;apps.apple.com&#x2F;app&#x2F;id6444050820</a></div><br/><div id="37083959" class="c"><input type="checkbox" id="c-37083959" checked=""/><div class="controls bullet"><span class="by">hustwindmaple1</span><span>|</span><a href="#37082447">root</a><span>|</span><a href="#37083023">parent</a><span>|</span><a href="#37083374">next</a><span>|</span><label class="collapse" for="c-37083959">[-]</label><label class="expand" for="c-37083959">[1 more]</label></div><br/><div class="children"><div class="content">If you have played any large mobile games, then you would not be surprised to see apps downloading massive files during first open.</div><br/></div></div><div id="37083374" class="c"><input type="checkbox" id="c-37083374" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082447">root</a><span>|</span><a href="#37083023">parent</a><span>|</span><a href="#37083959">prev</a><span>|</span><a href="#37083269">next</a><span>|</span><label class="collapse" for="c-37083374">[-]</label><label class="expand" for="c-37083374">[1 more]</label></div><br/><div class="children"><div class="content">A small download + an in-app weights download (and a space requirement warning) is probably sane, right?</div><br/></div></div><div id="37083269" class="c"><input type="checkbox" id="c-37083269" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#37082447">root</a><span>|</span><a href="#37083023">parent</a><span>|</span><a href="#37083374">prev</a><span>|</span><a href="#37082681">next</a><span>|</span><label class="collapse" for="c-37083269">[-]</label><label class="expand" for="c-37083269">[1 more]</label></div><br/><div class="children"><div class="content">The size makes it tough for App Store deployment, but I could imagine using a local LLM on-device for an enterprise app.</div><br/></div></div></div></div></div></div></div></div><div id="37082681" class="c"><input type="checkbox" id="c-37082681" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37082447">parent</a><span>|</span><a href="#37082655">prev</a><span>|</span><a href="#37082802">next</a><span>|</span><label class="collapse" for="c-37082681">[-]</label><label class="expand" for="c-37082681">[1 more]</label></div><br/><div class="children"><div class="content">MLC&#x27;s Apache TVM implementation can also compile to Metal.<p>Not sure if they made an autotuning profile for it yet.</div><br/></div></div><div id="37082802" class="c"><input type="checkbox" id="c-37082802" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37082447">parent</a><span>|</span><a href="#37082681">prev</a><span>|</span><a href="#37082269">next</a><span>|</span><label class="collapse" for="c-37082802">[-]</label><label class="expand" for="c-37082802">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Not even wrong&quot; question, ONNX is a runtime that can use&#x2F;uses CoreML.</div><br/><div id="37083238" class="c"><input type="checkbox" id="c-37083238" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#37082447">root</a><span>|</span><a href="#37082802">parent</a><span>|</span><a href="#37082269">next</a><span>|</span><label class="collapse" for="c-37083238">[-]</label><label class="expand" for="c-37083238">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t realize that! I wonder how performant a small Llama would be on iOS.</div><br/></div></div></div></div></div></div><div id="37082269" class="c"><input type="checkbox" id="c-37082269" checked=""/><div class="controls bullet"><span class="by">glitchc</span><span>|</span><a href="#37082447">prev</a><span>|</span><a href="#37083004">next</a><span>|</span><label class="collapse" for="c-37082269">[-]</label><label class="expand" for="c-37082269">[4 more]</label></div><br/><div class="children"><div class="content">How was this allowed? I was under the impression that companies the size of Microsoft needed to contact Meta to negotiate a license.<p>Excerpt from the license:<p><i>Additional Commercial Terms. If, on the Llama 2 version release date, the 
monthly active users of the products or services made available by or for Licensee, 
or Licensee&#x27;s affiliates, is greater than 700 million monthly active users in the 
preceding calendar month, you must request a license from Meta, which Meta may 
grant to you in its sole discretion, and you are not authorized to exercise any of the 
rights under this Agreement unless or until Meta otherwise expressly grants you 
such rights.</i></div><br/><div id="37082291" class="c"><input type="checkbox" id="c-37082291" checked=""/><div class="controls bullet"><span class="by">thadk</span><span>|</span><a href="#37082269">parent</a><span>|</span><a href="#37082283">next</a><span>|</span><label class="collapse" for="c-37082291">[-]</label><label class="expand" for="c-37082291">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Meta and Microsoft have been longtime partners on AI, starting with a collaboration to integrate ONNX Runtime with PyTorch to create a great developer experience for PyTorch on Azure, and Meta’s choice of Azure as a strategic cloud provider. (sic)<p><a href="https:&#x2F;&#x2F;blogs.microsoft.com&#x2F;blog&#x2F;2023&#x2F;07&#x2F;18&#x2F;microsoft-and-meta-expand-their-ai-partnership-with-llama-2-on-azure-and-windows&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blogs.microsoft.com&#x2F;blog&#x2F;2023&#x2F;07&#x2F;18&#x2F;microsoft-and-me...</a></div><br/></div></div><div id="37082283" class="c"><input type="checkbox" id="c-37082283" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37082269">parent</a><span>|</span><a href="#37082291">prev</a><span>|</span><a href="#37082362">next</a><span>|</span><label class="collapse" for="c-37082283">[-]</label><label class="expand" for="c-37082283">[1 more]</label></div><br/><div class="children"><div class="content">&gt; To get access permissions to the Llama 2 model, please fill out the Llama 2 access request form. If allowable, you will receive GitHub access in the next 48 hours, but usually much sooner.<p>I guess they send the form to Meta?<p>Anyway, I hope this is not what Open Source will be like from now on.</div><br/></div></div><div id="37082362" class="c"><input type="checkbox" id="c-37082362" checked=""/><div class="controls bullet"><span class="by">stu2b50</span><span>|</span><a href="#37082269">parent</a><span>|</span><a href="#37082283">prev</a><span>|</span><a href="#37083004">next</a><span>|</span><label class="collapse" for="c-37082362">[-]</label><label class="expand" for="c-37082362">[1 more]</label></div><br/><div class="children"><div class="content">So they negotiated a license? Meta partnered with Azure for the Llama 2 launch, there’s no reason to think that they’re antagonistic towards each other.</div><br/></div></div></div></div><div id="37083004" class="c"><input type="checkbox" id="c-37083004" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37082269">prev</a><span>|</span><a href="#37085373">next</a><span>|</span><label class="collapse" for="c-37083004">[-]</label><label class="expand" for="c-37083004">[1 more]</label></div><br/><div class="children"><div class="content">Sounds similar to llama&#x27;s chat edition. Found that to be pretty solid in itself so hopefully this is even better.</div><br/></div></div></div></div></div></div></div></body></html>