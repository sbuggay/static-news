<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735030877304" as="style"/><link rel="stylesheet" href="styles.css?v=1735030877304"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference">Making AMD GPUs competitive for LLM inference (2023)</a> <span class="domain">(<a href="https://blog.mlc.ai">blog.mlc.ai</a>)</span></div><div class="subtext"><span>plasticchris</span> | <span>104 comments</span></div><br/><div><div id="42499817" class="c"><input type="checkbox" id="c-42499817" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#42498936">next</a><span>|</span><label class="collapse" for="c-42499817">[-]</label><label class="expand" for="c-42499817">[3 more]</label></div><br/><div class="children"><div class="content">The problem is that performance achievements on AMD consumer-grade GPUs (RX7900XTX) are not representative&#x2F;transferrable to the Datacenter grade GPUs (MI300X). Consumer GPUs are based on RDNA architecture, while datacenter GPUs are based on the CDNA architecture, and only sometime in ~2026 AMD is expected to release unifying UDNA architecture [1]. At CentML we are currently working on integrating AMD CDNA and HIP support into our Hidet deep learning compiler [2], which will also power inference workloads for all Nvidia GPUs, AMD GPUs, Google TPU and AWS Inf2 chips on our platform [3]<p>[1] <a href="https:&#x2F;&#x2F;www.jonpeddie.com&#x2F;news&#x2F;amd-to-integrate-cdna-and-rdna-architectures-to-compete-in-ai&#x2F;#:~:text=In%202019%2C%20AMD%20transitioned%20away,Well%2C%20there&#x27;s%20plenty%20of%20time" rel="nofollow">https:&#x2F;&#x2F;www.jonpeddie.com&#x2F;news&#x2F;amd-to-integrate-cdna-and-rdn...</a>.
[2] <a href="https:&#x2F;&#x2F;centml.ai&#x2F;hidet&#x2F;" rel="nofollow">https:&#x2F;&#x2F;centml.ai&#x2F;hidet&#x2F;</a>
[3] <a href="https:&#x2F;&#x2F;centml.ai&#x2F;platform&#x2F;" rel="nofollow">https:&#x2F;&#x2F;centml.ai&#x2F;platform&#x2F;</a></div><br/><div id="42500328" class="c"><input type="checkbox" id="c-42500328" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42499817">parent</a><span>|</span><a href="#42498936">next</a><span>|</span><label class="collapse" for="c-42500328">[-]</label><label class="expand" for="c-42500328">[2 more]</label></div><br/><div class="children"><div class="content">The problem is that the specs of AMD consumer-grade GPUs do not translate to computer performance when you try and chain more than one together.<p>I have 7 NVidia 4090s under my desk happily chugging along on week long training runs. I once managed to get a Radeon VII to run for six hours without shitting itself.</div><br/><div id="42500515" class="c"><input type="checkbox" id="c-42500515" checked=""/><div class="controls bullet"><span class="by">tspng</span><span>|</span><a href="#42499817">root</a><span>|</span><a href="#42500328">parent</a><span>|</span><a href="#42498936">next</a><span>|</span><label class="collapse" for="c-42500515">[-]</label><label class="expand" for="c-42500515">[1 more]</label></div><br/><div class="children"><div class="content">Wow, are these 7 RTX 4090s in a single setup? Care to share more how you build it (case, cooling, power, ..)?</div><br/></div></div></div></div></div></div><div id="42498936" class="c"><input type="checkbox" id="c-42498936" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#42499817">prev</a><span>|</span><a href="#42498963">next</a><span>|</span><label class="collapse" for="c-42498936">[-]</label><label class="expand" for="c-42498936">[30 more]</label></div><br/><div class="children"><div class="content">I have come across quite few startups who are trying a similar idea: break the nvidia monopoly by utilizing AMD GPUs (for inference at least): Felafax, Lamini, tensorwave (partially), SlashML. Even saw optimistic claims like CUDA moat is only 18 months deep from some of them [1]. Let&#x27;s see.<p>[1] <a href="https:&#x2F;&#x2F;www.linkedin.com&#x2F;feed&#x2F;update&#x2F;urn:li:activity:7275885292513906689&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.linkedin.com&#x2F;feed&#x2F;update&#x2F;urn:li:activity:7275885...</a></div><br/><div id="42499742" class="c"><input type="checkbox" id="c-42499742" checked=""/><div class="controls bullet"><span class="by">pinsiang</span><span>|</span><a href="#42498936">parent</a><span>|</span><a href="#42498964">next</a><span>|</span><label class="collapse" for="c-42499742">[-]</label><label class="expand" for="c-42499742">[1 more]</label></div><br/><div class="children"><div class="content">AMD GPUs are becoming a serious contender for LLM inference. vLLM is already showing impressive performance on AMD [1], even with consumer-grade Radeon cards (even support GGUF) [2]. This could be a game-changer for folks who want to run LLMs without shelling out for expensive NVIDIA hardware.<p>[1] <a href="https:&#x2F;&#x2F;blog.vllm.ai&#x2F;2024&#x2F;10&#x2F;23&#x2F;vllm-serving-amd.html" rel="nofollow">https:&#x2F;&#x2F;blog.vllm.ai&#x2F;2024&#x2F;10&#x2F;23&#x2F;vllm-serving-amd.html</a>
[2] <a href="https:&#x2F;&#x2F;embeddedllm.com&#x2F;blog&#x2F;vllm-now-supports-running-gguf-on-amd-radeon-gpu" rel="nofollow">https:&#x2F;&#x2F;embeddedllm.com&#x2F;blog&#x2F;vllm-now-supports-running-gguf-...</a></div><br/></div></div><div id="42498964" class="c"><input type="checkbox" id="c-42498964" checked=""/><div class="controls bullet"><span class="by">ryukoposting</span><span>|</span><a href="#42498936">parent</a><span>|</span><a href="#42499742">prev</a><span>|</span><a href="#42499843">next</a><span>|</span><label class="collapse" for="c-42498964">[-]</label><label class="expand" for="c-42498964">[24 more]</label></div><br/><div class="children"><div class="content">Peculiar business model, at a glance. It seems like they&#x27;re doing work that AMD ought to be doing, and is probably doing behind the scenes. Who is the customer for a third-party GPU driver shim?</div><br/><div id="42499043" class="c"><input type="checkbox" id="c-42499043" checked=""/><div class="controls bullet"><span class="by">dpkirchner</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42498964">parent</a><span>|</span><a href="#42498996">next</a><span>|</span><label class="collapse" for="c-42499043">[-]</label><label class="expand" for="c-42499043">[19 more]</label></div><br/><div class="children"><div class="content">Could be trying to make themselves a target for a big acquihire.</div><br/><div id="42499453" class="c"><input type="checkbox" id="c-42499453" checked=""/><div class="controls bullet"><span class="by">to11mtm</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499043">parent</a><span>|</span><a href="#42499495">next</a><span>|</span><label class="collapse" for="c-42499453">[-]</label><label class="expand" for="c-42499453">[15 more]</label></div><br/><div class="children"><div class="content">Cynical take: Try to get acquired by Intel for Arc.</div><br/><div id="42499908" class="c"><input type="checkbox" id="c-42499908" checked=""/><div class="controls bullet"><span class="by">dangero</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499453">parent</a><span>|</span><a href="#42499974">next</a><span>|</span><label class="collapse" for="c-42499908">[-]</label><label class="expand" for="c-42499908">[2 more]</label></div><br/><div class="children"><div class="content">More cynical take: Trying to get acquired by nvidia</div><br/><div id="42500190" class="c"><input type="checkbox" id="c-42500190" checked=""/><div class="controls bullet"><span class="by">dizhn</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499908">parent</a><span>|</span><a href="#42499974">next</a><span>|</span><label class="collapse" for="c-42500190">[-]</label><label class="expand" for="c-42500190">[1 more]</label></div><br/><div class="children"><div class="content">Person below says they (the whole team) already joined Nvidia.</div><br/></div></div></div></div><div id="42499974" class="c"><input type="checkbox" id="c-42499974" checked=""/><div class="controls bullet"><span class="by">dogma1138</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499453">parent</a><span>|</span><a href="#42499908">prev</a><span>|</span><a href="#42499639">next</a><span>|</span><label class="collapse" for="c-42499974">[-]</label><label class="expand" for="c-42499974">[4 more]</label></div><br/><div class="children"><div class="content">Intel is in a vastly better shape than AMD, they have the software pretty much nailed down.</div><br/><div id="42500245" class="c"><input type="checkbox" id="c-42500245" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499974">parent</a><span>|</span><a href="#42500229">next</a><span>|</span><label class="collapse" for="c-42500245">[-]</label><label class="expand" for="c-42500245">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve recently been poking around with Intel oneAPI and IPEX-LLM. While there are things that I find refreshing (like their ability to actually respond to bug reports in a timely manner, or at all) on a whole, support&#x2F;maturity actually doesn&#x27;t match the current state of ROCm.<p>PyTorch requires it&#x27;s own support kit separate from the oneAPI Toolkit (and runs slightly different versions of everything), the vLLM xpu support doesn&#x27;t work - both source <i>and</i> the docker failed to build&#x2F;run for me. The IPEX-LLM whisper support is completely borked, etc, etc.</div><br/></div></div><div id="42500229" class="c"><input type="checkbox" id="c-42500229" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499974">parent</a><span>|</span><a href="#42500245">prev</a><span>|</span><a href="#42500200">next</a><span>|</span><label class="collapse" for="c-42500229">[-]</label><label class="expand" for="c-42500229">[1 more]</label></div><br/><div class="children"><div class="content">Someone never used intel killer wifi software.</div><br/></div></div><div id="42500200" class="c"><input type="checkbox" id="c-42500200" checked=""/><div class="controls bullet"><span class="by">indolering</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499974">parent</a><span>|</span><a href="#42500229">prev</a><span>|</span><a href="#42499639">next</a><span>|</span><label class="collapse" for="c-42500200">[-]</label><label class="expand" for="c-42500200">[1 more]</label></div><br/><div class="children"><div class="content">Tell that to the board.</div><br/></div></div></div></div><div id="42499639" class="c"><input type="checkbox" id="c-42499639" checked=""/><div class="controls bullet"><span class="by">shiroiushi</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499453">parent</a><span>|</span><a href="#42499974">prev</a><span>|</span><a href="#42499495">next</a><span>|</span><label class="collapse" for="c-42499639">[-]</label><label class="expand" for="c-42499639">[8 more]</label></div><br/><div class="children"><div class="content">More cynical take: this would be a bad strategy, because Intel hasn&#x27;t shown much competence in its leadership for a long time, especially in regards to GPUs.</div><br/><div id="42499806" class="c"><input type="checkbox" id="c-42499806" checked=""/><div class="controls bullet"><span class="by">rockskon</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499639">parent</a><span>|</span><a href="#42499495">next</a><span>|</span><label class="collapse" for="c-42499806">[-]</label><label class="expand" for="c-42499806">[7 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve actually been making positive moves with GPUs lately along with a success story for the B580.</div><br/><div id="42500498" class="c"><input type="checkbox" id="c-42500498" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499806">parent</a><span>|</span><a href="#42499861">next</a><span>|</span><label class="collapse" for="c-42500498">[-]</label><label class="expand" for="c-42500498">[1 more]</label></div><br/><div class="children"><div class="content">B580 being a &quot;success&quot; is purely a business decision as a loss leader to get their name into the market. A larger die on a newer node than either Nvidia or AMD means their per-unit costs <i>are</i> higher, and are selling it at a lower price.<p>That&#x27;s not a long-term success strategy. Maybe good for getting your name in the conversation, but not sustainable.</div><br/></div></div><div id="42499861" class="c"><input type="checkbox" id="c-42499861" checked=""/><div class="controls bullet"><span class="by">schmidtleonard</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499806">parent</a><span>|</span><a href="#42500498">prev</a><span>|</span><a href="#42499495">next</a><span>|</span><label class="collapse" for="c-42499861">[-]</label><label class="expand" for="c-42499861">[5 more]</label></div><br/><div class="children"><div class="content">Yeah but MLID says they are losing money on every one and have been winding down the internal development resources. That doesn&#x27;t bode well for the future.<p>I want to believe he&#x27;s wrong, but on the parts of his show where I am in a position to verify, he generally checks out. Whatever the opposite of Gell-Mann Amnesia is, he&#x27;s got it going for him.</div><br/><div id="42500007" class="c"><input type="checkbox" id="c-42500007" checked=""/><div class="controls bullet"><span class="by">oofabz</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499861">parent</a><span>|</span><a href="#42499952">next</a><span>|</span><label class="collapse" for="c-42500007">[-]</label><label class="expand" for="c-42500007">[1 more]</label></div><br/><div class="children"><div class="content">The die size of the B580 is 272 mm2, which is a lot of silicon for $249. The performance of the GPU is good for its price but bad for its die size. Manufacturing cost is closely tied to die size.<p>272 mm2 puts the B580 in the same league as the Radeon 7700XT, a $449 card, and the GeForce 4070 Super, which is $599. The idea that Intel is selling these cards at a loss sounds reasonable to me.</div><br/></div></div><div id="42499952" class="c"><input type="checkbox" id="c-42499952" checked=""/><div class="controls bullet"><span class="by">sodality2</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499861">parent</a><span>|</span><a href="#42500007">prev</a><span>|</span><a href="#42499937">next</a><span>|</span><label class="collapse" for="c-42499952">[-]</label><label class="expand" for="c-42499952">[1 more]</label></div><br/><div class="children"><div class="content">MLID on Intel is starting to become the same as UserBenchmark on AMD (except for the generally reputable sources)... he&#x27;s beginning to sound like he simply wants Intel to fail, to my insider-info-lacking ears. For competition&#x27;s sake I <i>really</i> hope that MLID has it wrong (at least the opining about the imminent failure of Intel&#x27;s GPU division), and that the B series will encourage Intel to push farther to spark more competition in the GPU space.</div><br/></div></div><div id="42499937" class="c"><input type="checkbox" id="c-42499937" checked=""/><div class="controls bullet"><span class="by">derektank</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499861">parent</a><span>|</span><a href="#42499952">prev</a><span>|</span><a href="#42499495">next</a><span>|</span><label class="collapse" for="c-42499937">[-]</label><label class="expand" for="c-42499937">[2 more]</label></div><br/><div class="children"><div class="content">Wait, are they losing money on every one in the sense that they haven&#x27;t broken even on research and development yet? Or in the sense that they cost more to manufacture than they&#x27;re sold at? Because one is much worse than the other.</div><br/><div id="42500364" class="c"><input type="checkbox" id="c-42500364" checked=""/><div class="controls bullet"><span class="by">rockskon</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499937">parent</a><span>|</span><a href="#42499495">next</a><span>|</span><label class="collapse" for="c-42500364">[-]</label><label class="expand" for="c-42500364">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re trying to unseat Radeon as the budget card.  That means making a more enticing offer than AMD for a temporary period of time.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42499495" class="c"><input type="checkbox" id="c-42499495" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499043">parent</a><span>|</span><a href="#42499453">prev</a><span>|</span><a href="#42498996">next</a><span>|</span><label class="collapse" for="c-42499495">[-]</label><label class="expand" for="c-42499495">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Could be trying to make themselves a target for a big acquihire.<p>Is this something anyone sets out to do?</div><br/><div id="42499525" class="c"><input type="checkbox" id="c-42499525" checked=""/><div class="controls bullet"><span class="by">ryukoposting</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499495">parent</a><span>|</span><a href="#42499506">next</a><span>|</span><label class="collapse" for="c-42499525">[-]</label><label class="expand" for="c-42499525">[1 more]</label></div><br/><div class="children"><div class="content">It definitely is, yes.</div><br/></div></div><div id="42499506" class="c"><input type="checkbox" id="c-42499506" checked=""/><div class="controls bullet"><span class="by">seeknotfind</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499495">parent</a><span>|</span><a href="#42499525">prev</a><span>|</span><a href="#42498996">next</a><span>|</span><label class="collapse" for="c-42499506">[-]</label><label class="expand" for="c-42499506">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div></div></div></div></div><div id="42498996" class="c"><input type="checkbox" id="c-42498996" checked=""/><div class="controls bullet"><span class="by">tesch1</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42498964">parent</a><span>|</span><a href="#42499043">prev</a><span>|</span><a href="#42499297">next</a><span>|</span><label class="collapse" for="c-42498996">[-]</label><label class="expand" for="c-42498996">[1 more]</label></div><br/><div class="children"><div class="content">AMD. Just one more dot to connect ;)</div><br/></div></div><div id="42499297" class="c"><input type="checkbox" id="c-42499297" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42498964">parent</a><span>|</span><a href="#42498996">prev</a><span>|</span><a href="#42499334">next</a><span>|</span><label class="collapse" for="c-42499297">[-]</label><label class="expand" for="c-42499297">[2 more]</label></div><br/><div class="children"><div class="content">It would be interesting to find out AMD is funding these other companies to ensure the shim happens while they focus on not doing it.</div><br/><div id="42499482" class="c"><input type="checkbox" id="c-42499482" checked=""/><div class="controls bullet"><span class="by">bushbaba</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42499297">parent</a><span>|</span><a href="#42499334">next</a><span>|</span><label class="collapse" for="c-42499482">[-]</label><label class="expand" for="c-42499482">[1 more]</label></div><br/><div class="children"><div class="content">AMD is kind of doing that funding by pricing its GPUs low and&#x2F;or giving them away at cost to these startups</div><br/></div></div></div></div><div id="42499334" class="c"><input type="checkbox" id="c-42499334" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42498964">parent</a><span>|</span><a href="#42499297">prev</a><span>|</span><a href="#42499843">next</a><span>|</span><label class="collapse" for="c-42499334">[-]</label><label class="expand" for="c-42499334">[1 more]</label></div><br/><div class="children"><div class="content">Is this effort benefiting everyone? I.e. where is it going &#x2F; is it open source?</div><br/></div></div></div></div><div id="42499843" class="c"><input type="checkbox" id="c-42499843" checked=""/><div class="controls bullet"><span class="by">llama-mini</span><span>|</span><a href="#42498936">parent</a><span>|</span><a href="#42498964">prev</a><span>|</span><a href="#42498998">next</a><span>|</span><label class="collapse" for="c-42499843">[-]</label><label class="expand" for="c-42499843">[1 more]</label></div><br/><div class="children"><div class="content">From Lamini, we have a private AMD GPU cluster, ready to serve any one who want to try MI300x or MI250 with inference and tuning.<p>We just onboarded a customer to move from openai API to on-prem solution, currently evaluating MI300x for inference.<p>Email me at my profile email.</div><br/></div></div><div id="42498998" class="c"><input type="checkbox" id="c-42498998" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42498936">parent</a><span>|</span><a href="#42499843">prev</a><span>|</span><a href="#42498963">next</a><span>|</span><label class="collapse" for="c-42498998">[-]</label><label class="expand" for="c-42498998">[3 more]</label></div><br/><div class="children"><div class="content">Tinygrad was another one, but they ended up getting frustrated with AMD and semi-pivoted to Nvidia.</div><br/><div id="42499521" class="c"><input type="checkbox" id="c-42499521" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42498998">parent</a><span>|</span><a href="#42499975">next</a><span>|</span><label class="collapse" for="c-42499521">[-]</label><label class="expand" for="c-42499521">[1 more]</label></div><br/><div class="children"><div class="content">This is discussed in the lex Friedman episode. AMD’s own demo would kernel panic when run in a loop [1].<p>[1] <a href="https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=dNrTrx42DGQ&amp;t=3218" rel="nofollow">https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=dNrTrx42DGQ&amp;t=3218</a></div><br/></div></div><div id="42499975" class="c"><input type="checkbox" id="c-42499975" checked=""/><div class="controls bullet"><span class="by">noch</span><span>|</span><a href="#42498936">root</a><span>|</span><a href="#42498998">parent</a><span>|</span><a href="#42499521">prev</a><span>|</span><a href="#42498963">next</a><span>|</span><label class="collapse" for="c-42499975">[-]</label><label class="expand" for="c-42499975">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Tinygrad was another one, but they ended up getting frustrated with AMD and semi-pivoted to Nvidia.<p>From their announcement on 20241219[^0]:<p>&quot;<i>We are the only company to get AMD on MLPerf</i>, and <i>we have a completely custom driver that&#x27;s 50x simpler than the stock one</i>. A bit shocked by how little AMD cared, but we&#x27;ll take the trillions instead of them.&quot;<p>From 20241211[^1]:<p>&quot;We gave up and soon <i>tinygrad will depend on 0 AMD code</i> except what&#x27;s required by code signing.<p>We did this for the 7900XTX (tinybox red). If AMD was thinking strategically, they&#x27;d be begging us to take some free MI300s to add support for it.&quot;<p>---<p>[^0]: <a href="https:&#x2F;&#x2F;x.com&#x2F;__tinygrad__&#x2F;status&#x2F;1869620002015572023" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;__tinygrad__&#x2F;status&#x2F;1869620002015572023</a><p>[^1]: <a href="https:&#x2F;&#x2F;x.com&#x2F;__tinygrad__&#x2F;status&#x2F;1866889544299319606" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;__tinygrad__&#x2F;status&#x2F;1866889544299319606</a></div><br/></div></div></div></div></div></div><div id="42498963" class="c"><input type="checkbox" id="c-42498963" checked=""/><div class="controls bullet"><span class="by">jroesch</span><span>|</span><a href="#42498936">prev</a><span>|</span><a href="#42498849">next</a><span>|</span><label class="collapse" for="c-42498963">[-]</label><label class="expand" for="c-42498963">[27 more]</label></div><br/><div class="children"><div class="content">Note: this is old work, and much of the team working on TVM, and MLC were from OctoAI and we have all recently joined NVIDIA.</div><br/><div id="42499082" class="c"><input type="checkbox" id="c-42499082" checked=""/><div class="controls bullet"><span class="by">sebmellen</span><span>|</span><a href="#42498963">parent</a><span>|</span><a href="#42498849">next</a><span>|</span><label class="collapse" for="c-42499082">[-]</label><label class="expand" for="c-42499082">[26 more]</label></div><br/><div class="children"><div class="content">Is there no hope for AMD anymore? After George Hotz&#x2F;Tinygrad gave up on AMD I feel there’s no realistic chance of using their chips to break the CUDA dominance.</div><br/><div id="42499414" class="c"><input type="checkbox" id="c-42499414" checked=""/><div class="controls bullet"><span class="by">comex</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499082">parent</a><span>|</span><a href="#42499131">next</a><span>|</span><label class="collapse" for="c-42499414">[-]</label><label class="expand" for="c-42499414">[3 more]</label></div><br/><div class="children"><div class="content">Maybe from Modular (the company Chris Lattner is working for).  In this recent announcement they said they had achieved competitive ML performance… on NVIDIA GPUs, but with their own custom stack completely replacing CUDA.  And they’re targeting AMD next.<p><a href="https:&#x2F;&#x2F;www.modular.com&#x2F;blog&#x2F;introducing-max-24-6-a-gpu-native-generative-ai-platform" rel="nofollow">https:&#x2F;&#x2F;www.modular.com&#x2F;blog&#x2F;introducing-max-24-6-a-gpu-nati...</a></div><br/><div id="42499452" class="c"><input type="checkbox" id="c-42499452" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499414">parent</a><span>|</span><a href="#42499131">next</a><span>|</span><label class="collapse" for="c-42499452">[-]</label><label class="expand" for="c-42499452">[2 more]</label></div><br/><div class="children"><div class="content">Ah yes, the programming language (Mojo) that requires an account before I can use it...</div><br/><div id="42500090" class="c"><input type="checkbox" id="c-42500090" checked=""/><div class="controls bullet"><span class="by">melodyogonna</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499452">parent</a><span>|</span><a href="#42499131">next</a><span>|</span><label class="collapse" for="c-42500090">[-]</label><label class="expand" for="c-42500090">[1 more]</label></div><br/><div class="children"><div class="content">Mojo no longer requires an account to install.<p>But that is irrelevant to the conversation because this is not about Mojo but something they call MAX. [1]<p>1. <a href="https:&#x2F;&#x2F;www.modular.com&#x2F;max" rel="nofollow">https:&#x2F;&#x2F;www.modular.com&#x2F;max</a></div><br/></div></div></div></div></div></div><div id="42499131" class="c"><input type="checkbox" id="c-42499131" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499082">parent</a><span>|</span><a href="#42499414">prev</a><span>|</span><a href="#42499159">next</a><span>|</span><label class="collapse" for="c-42499131">[-]</label><label class="expand" for="c-42499131">[6 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;x.com&#x2F;dylan522p&#x2F;status&#x2F;1871287937268383867" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;dylan522p&#x2F;status&#x2F;1871287937268383867</a></div><br/><div id="42499273" class="c"><input type="checkbox" id="c-42499273" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499131">parent</a><span>|</span><a href="#42499159">next</a><span>|</span><label class="collapse" for="c-42499273">[-]</label><label class="expand" for="c-42499273">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s almost word for word what geohotz said last year?</div><br/><div id="42499337" class="c"><input type="checkbox" id="c-42499337" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499273">parent</a><span>|</span><a href="#42499355">next</a><span>|</span><label class="collapse" for="c-42499337">[-]</label><label class="expand" for="c-42499337">[1 more]</label></div><br/><div class="children"><div class="content">What part?<p>I assume the part where she said there&#x27;s &quot;gaps in the software stack&quot;, because that&#x27;s the only part that&#x27;s attributed to her.<p>But I must be wrong because that  hasn&#x27;t been in dispute or in the news in a decade, it&#x27;s not a geohot discovery from last year.<p>Hell I remember a subargument of a subargument re: this being an issue a decade ago in macOS dev (TL;Dr whether to invest in opencl)</div><br/></div></div><div id="42499355" class="c"><input type="checkbox" id="c-42499355" checked=""/><div class="controls bullet"><span class="by">bn-l</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499273">parent</a><span>|</span><a href="#42499337">prev</a><span>|</span><a href="#42499159">next</a><span>|</span><label class="collapse" for="c-42499355">[-]</label><label class="expand" for="c-42499355">[3 more]</label></div><br/><div class="children"><div class="content">I went through the thread. There’s an argument to be made in firing Su for being so spaced out as to miss an op for their own CUDA for free.</div><br/><div id="42499470" class="c"><input type="checkbox" id="c-42499470" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499355">parent</a><span>|</span><a href="#42499159">next</a><span>|</span><label class="collapse" for="c-42499470">[-]</label><label class="expand" for="c-42499470">[2 more]</label></div><br/><div class="children"><div class="content">Not remotely, how did you get to that idea?</div><br/><div id="42499953" class="c"><input type="checkbox" id="c-42499953" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499470">parent</a><span>|</span><a href="#42499159">next</a><span>|</span><label class="collapse" for="c-42499953">[-]</label><label class="expand" for="c-42499953">[1 more]</label></div><br/><div class="children"><div class="content">Kids this days (shakes fist)<p>tl;dr there&#x27;s a non-unsubstantial # of people who learn a lot from geohot. I&#x27;d say about 3% of people here will be confused if you thought of him as less than a top technical expert across many comp sci fields.<p>And he did the geohot thing recently, way tl;dr: acted like there was a scandal being covered up by AMD around drivers that was causing them to &quot;lose&quot; to nVidia.<p>He then framed AMD not engaging with him on this topic as further covering-up and choosing to lose.<p>So if you&#x27;re of a certain set of experiences, you see an anodyne quote from the CEO that would have been utterly unsurprising dating back to when ATI was still a company, and you&#x27;d read it as the CEO breezily admitting in public that geohot was right about how there was malfeasance, followed by a cover up, implying extreme dereliction of duty, because she either helped or didn&#x27;t realize till now.<p>I&#x27;d argue this is partially due to stonk-ification of discussions, there was a vague, yet often communicated, sense there was something illegal happening. Idea was it was financial dereliction of duty to shareholders.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42499159" class="c"><input type="checkbox" id="c-42499159" checked=""/><div class="controls bullet"><span class="by">dismalaf</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499082">parent</a><span>|</span><a href="#42499131">prev</a><span>|</span><a href="#42499638">next</a><span>|</span><label class="collapse" for="c-42499159">[-]</label><label class="expand" for="c-42499159">[1 more]</label></div><br/><div class="children"><div class="content">IMO the hope shouldn&#x27;t be that AMD specifically wins, rather it&#x27;s best for consumers that hardware becomes commoditized and prices come down.<p>And that&#x27;s what&#x27;s happening, slowly anyway.  Google, Apple and Amazon all have their own AI chips, Intel has Gaudi, AMD had their thing, and the software is at least working on more than just Nvidia.  Which is a win.  Even if it&#x27;s not perfect.  I&#x27;m personally hoping that everyone piles in on a standard like SYCL.</div><br/></div></div><div id="42499638" class="c"><input type="checkbox" id="c-42499638" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499082">parent</a><span>|</span><a href="#42499159">prev</a><span>|</span><a href="#42499112">next</a><span>|</span><label class="collapse" for="c-42499638">[-]</label><label class="expand" for="c-42499638">[2 more]</label></div><br/><div class="children"><div class="content">The world is bigger than AMD and Nvidia. Plenty of interesting new AI-tuned non-GPU accelerators coming online.</div><br/><div id="42500424" class="c"><input type="checkbox" id="c-42500424" checked=""/><div class="controls bullet"><span class="by">grigio</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499638">parent</a><span>|</span><a href="#42499112">next</a><span>|</span><label class="collapse" for="c-42500424">[-]</label><label class="expand" for="c-42500424">[1 more]</label></div><br/><div class="children"><div class="content">I hope, name some NPU who can run a 70B model..</div><br/></div></div></div></div><div id="42499112" class="c"><input type="checkbox" id="c-42499112" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499082">parent</a><span>|</span><a href="#42499638">prev</a><span>|</span><a href="#42498849">next</a><span>|</span><label class="collapse" for="c-42499112">[-]</label><label class="expand" for="c-42499112">[13 more]</label></div><br/><div class="children"><div class="content">Not really.<p>AMD is constitutionally incapable of shipping anything but mid range hardware that requires no innovation.<p>The only reason why they are doing so well in CPUs right now is that Intel has basically destroyed itself without any outside help.</div><br/><div id="42499548" class="c"><input type="checkbox" id="c-42499548" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499112">parent</a><span>|</span><a href="#42499168">next</a><span>|</span><label class="collapse" for="c-42499548">[-]</label><label class="expand" for="c-42499548">[5 more]</label></div><br/><div class="children"><div class="content">Everything is comparative. AMD isn&#x27;t perfect. As an Ex Shareholder I have argued they did well partly because of Intel&#x27;s downfall. In terms of execution it is far from perfect.<p>But Nvidia is a different beast. It is a bit like Apple in the late 00s where you take business, forecast, marketing, operation, software, hardware, sales etc You take any part of it and they are all industry leading. And having industry leading capability is only part of the game, having it all work together is completely another thing. And unlike Apple where they lost direction once Steve Jobs passed away and weren&#x27;t sure about how to deploy capital. Jensen is still here, and they have more resources now making Nvidia even more competitive.<p>It is often most people underestimate the magnitude of the task required, ( I like to tell the story again about an Intel GPU engineer in 2016 arguing they could take dGPU market shares by 2020, and we are now 2025 ), over estimate the capability of an organisation, under estimate the rival&#x27;s speed of innovation and execution. These three thing combined is why most people are often off the estimate by an order of magnitude.</div><br/><div id="42499624" class="c"><input type="checkbox" id="c-42499624" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499548">parent</a><span>|</span><a href="#42499168">next</a><span>|</span><label class="collapse" for="c-42499624">[-]</label><label class="expand" for="c-42499624">[4 more]</label></div><br/><div class="children"><div class="content">Yeah, no.<p>We are in the middle of a monopoly squeeze by NVidia on the most innovative part of the economy right now. I expect the DOJ to hit them harder than they did MS in the 90s given the bullshit they are pulling and the drag on the economy they are causing.<p>By comparison if AMD could write a driver that didn&#x27;t shit itself when it had to multiply more than two matrices in a row they&#x27;d be selling cards faster than they can make them. You don&#x27;t need to sell the best shovels in a gold rush to make mountains of money, but you can&#x27;t sell teaspoons as premium shovels and expect people to come back.</div><br/><div id="42499902" class="c"><input type="checkbox" id="c-42499902" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499624">parent</a><span>|</span><a href="#42499779">next</a><span>|</span><label class="collapse" for="c-42499902">[-]</label><label class="expand" for="c-42499902">[1 more]</label></div><br/><div class="children"><div class="content">&gt;We are in the middle of a monopoly squeeze by NVidia on the most innovative part of the economy right now.<p>I am not sure which part of Nvidia is monopoly. That is like suggesting TSMC has a monopoly.</div><br/></div></div><div id="42499779" class="c"><input type="checkbox" id="c-42499779" checked=""/><div class="controls bullet"><span class="by">kadoban</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499624">parent</a><span>|</span><a href="#42499902">prev</a><span>|</span><a href="#42499675">next</a><span>|</span><label class="collapse" for="c-42499779">[-]</label><label class="expand" for="c-42499779">[1 more]</label></div><br/><div class="children"><div class="content">What effect did the DOJ have on MS in the 90s? Didn&#x27;t all of that get rolled back before they had to pay a dime, and all it amounted to was that browser choice screen that was around for a while? Hardly a crippling blow. If anything that showed the weakness of regulators in fights against big tech, just outlast them and you&#x27;re fine.</div><br/></div></div><div id="42499675" class="c"><input type="checkbox" id="c-42499675" checked=""/><div class="controls bullet"><span class="by">shiroiushi</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499624">parent</a><span>|</span><a href="#42499779">prev</a><span>|</span><a href="#42499168">next</a><span>|</span><label class="collapse" for="c-42499675">[-]</label><label class="expand" for="c-42499675">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I expect the DOJ to hit them harder than they did MS in the 90s given the bullshit they are pulling and the drag on the economy they are causing.<p>It sounds like you&#x27;re expecting extreme competence from the DOJ.  Given their history with regulating big tech companies, and even worse, the incoming administration, I think this is a very unrealistic expectation.</div><br/></div></div></div></div></div></div><div id="42499168" class="c"><input type="checkbox" id="c-42499168" checked=""/><div class="controls bullet"><span class="by">perching_aix</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499112">parent</a><span>|</span><a href="#42499548">prev</a><span>|</span><a href="#42499985">next</a><span>|</span><label class="collapse" for="c-42499168">[-]</label><label class="expand" for="c-42499168">[6 more]</label></div><br/><div class="children"><div class="content">And I&#x27;m supposed to believe that HN is this amazing platform for technology and science discussions, totally unlike its peers...</div><br/><div id="42499260" class="c"><input type="checkbox" id="c-42499260" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499168">parent</a><span>|</span><a href="#42499331">next</a><span>|</span><label class="collapse" for="c-42499260">[-]</label><label class="expand" for="c-42499260">[1 more]</label></div><br/><div class="children"><div class="content">The above take is worded a bit cynical but is their general approach to GPUs lately across the board e.g. <a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;326415&#x2F;amd-confirms-retreat-from-the-enthusiast-gpu-segment-to-focus-on-gaining-market-share" rel="nofollow">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;326415&#x2F;amd-confirms-retreat-from...</a><p>Also I&#x27;d take HN as being being an amazing platform for the overall consistency and quality of moderation. Anything beyond that depends more on who you&#x27;re talking to than where at.</div><br/></div></div><div id="42499331" class="c"><input type="checkbox" id="c-42499331" checked=""/><div class="controls bullet"><span class="by">petesergeant</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499168">parent</a><span>|</span><a href="#42499260">prev</a><span>|</span><a href="#42499603">next</a><span>|</span><label class="collapse" for="c-42499331">[-]</label><label class="expand" for="c-42499331">[1 more]</label></div><br/><div class="children"><div class="content">Maybe be the change you want to see and tell us what the real story is?</div><br/></div></div><div id="42499603" class="c"><input type="checkbox" id="c-42499603" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499168">parent</a><span>|</span><a href="#42499331">prev</a><span>|</span><a href="#42499985">next</a><span>|</span><label class="collapse" for="c-42499603">[-]</label><label class="expand" for="c-42499603">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really care what you believe.<p>Everyone whose dug deep into what AMD is doing has left in disgust if they are lucky and bankruptcy if they are not.<p>If I can save someone else from wasting $100,000 on hardware and six months of their life then my post has done more good than the AMD marketing department ever will.</div><br/><div id="42499999" class="c"><input type="checkbox" id="c-42499999" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499603">parent</a><span>|</span><a href="#42499985">next</a><span>|</span><label class="collapse" for="c-42499999">[-]</label><label class="expand" for="c-42499999">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If I can save someone else from wasting $100,000 on hardware and six months of their life then my post has done more good than the AMD marketing department ever will.<p>This seems like unuseful advice if you&#x27;ve already given up on them.<p>You tried it and at some point in the past it wasn&#x27;t ready. But by not being ready they&#x27;re losing money, so they have a direct incentive to fix it. Which would take a certain amount of time, but once you&#x27;ve given up you no longer know if they&#x27;ve done it yet or not, at which point your advice would be stale.<p>Meanwhile the people who attempt it apparently seem to get acquired by Nvidia, for some strange reason. Which implies it should be a worthwhile thing to do. If they&#x27;ve fixed it by now which you wouldn&#x27;t know if you&#x27;ve stopped looking, or they fix it in the near future, you have a competitive advantage because you have access to lower cost GPUs than your rivals. If not, but you&#x27;ve demonstrated a serious attempt to fix it for everyone yourself, Nvidia comes to you with a sack full of money to make sure you don&#x27;t finish, and then you get a sack full of money. That&#x27;s win&#x2F;win, so rather than nobody doing it, it seems like everybody should be doing it.</div><br/><div id="42500311" class="c"><input type="checkbox" id="c-42500311" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499999">parent</a><span>|</span><a href="#42499985">next</a><span>|</span><label class="collapse" for="c-42500311">[-]</label><label class="expand" for="c-42500311">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tried it three times.<p>I&#x27;ve seen people try it every six months for two decades now.<p>At some point you just have to accept that AMD is not a serious company, but is a second rate copycat and there is no way to change that without firing everyone from middle management up.<p>I&#x27;m deeply worried about stagnation in the CPU space now that they are top dog and Intel is dead in the water.<p>Here&#x27;s hoping China and Risk V save us.<p>&gt;Meanwhile the people who attempt it apparently seem to get acquired by Nvidia<p>Everyone I&#x27;ve seen base jumping has gotten a sponsorship from redbull, ergo. everyone should basejump.<p>Ignore the red smears around the parking lot.</div><br/></div></div></div></div></div></div></div></div><div id="42499985" class="c"><input type="checkbox" id="c-42499985" checked=""/><div class="controls bullet"><span class="by">lofaszvanitt</span><span>|</span><a href="#42498963">root</a><span>|</span><a href="#42499112">parent</a><span>|</span><a href="#42499168">prev</a><span>|</span><a href="#42498849">next</a><span>|</span><label class="collapse" for="c-42499985">[-]</label><label class="expand" for="c-42499985">[1 more]</label></div><br/><div class="children"><div class="content">It had to destroy itself. These companies do not act on their own...</div><br/></div></div></div></div></div></div></div></div><div id="42498849" class="c"><input type="checkbox" id="c-42498849" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42498963">prev</a><span>|</span><a href="#42499317">next</a><span>|</span><label class="collapse" for="c-42498849">[-]</label><label class="expand" for="c-42498849">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Aug 9, 2023<p>Ignoring the very old (in ML time) date of the article...<p>What&#x27;s the catch? People are still struggling with this a year later so I have to assume it doesn&#x27;t work as well as claimed.<p>I&#x27;m guessing this is buggy in practice and only works for the HF models they chose to test with?</div><br/><div id="42498940" class="c"><input type="checkbox" id="c-42498940" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#42498849">parent</a><span>|</span><a href="#42498921">next</a><span>|</span><label class="collapse" for="c-42498940">[-]</label><label class="expand" for="c-42498940">[6 more]</label></div><br/><div class="children"><div class="content">It’s not terribly hard to port ML inference to alternative GPU APIs. I did it for D3D11 and the performance is pretty good too: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml</a><p>The only catch is, for some reason developers of ML libraries like PyTorch aren’t interested in open GPU APIs like D3D or Vulkan. Instead, they focus on proprietary ones i.e. CUDA and to lesser extent ROCm. I don’t know why that is.<p>D3D-based videogames are heavily using GPU compute for more than a decade now. Since Valve shipped SteamDeck, the same now applies to Vulkan on Linux. By now, both technologies are stable, reliable and performant.</div><br/><div id="42499019" class="c"><input type="checkbox" id="c-42499019" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42498849">root</a><span>|</span><a href="#42498940">parent</a><span>|</span><a href="#42498921">next</a><span>|</span><label class="collapse" for="c-42499019">[-]</label><label class="expand" for="c-42499019">[5 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t part of it because the first-party libraries like cuDNN are only available through CUDA? Nvidia has poured a ton of effort into tuning those libraries so it&#x27;s hard to justify not using them.</div><br/><div id="42499097" class="c"><input type="checkbox" id="c-42499097" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#42498849">root</a><span>|</span><a href="#42499019">parent</a><span>|</span><a href="#42498921">next</a><span>|</span><label class="collapse" for="c-42499097">[-]</label><label class="expand" for="c-42499097">[4 more]</label></div><br/><div class="children"><div class="content">Unlike training, ML inference is almost always bound by memory bandwidth as opposed to computations. For this reason, tensor cores, cuDNN, and other advanced shenanigans make very little sense for the use case.<p>OTOH, general-purpose compute instead of fixed-function blocks used by cuDNN enables custom compression algorithms for these weights which does help, by saving memory bandwidth. For example, I did custom 5 bits&#x2F;weight quantization which works on all GPUs, no hardware support necessary, just simple HLSL codes: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml?tab=readme-ov-file#bcml1-codec">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml?tab=readme-ov-file#bcml1-co...</a></div><br/><div id="42499408" class="c"><input type="checkbox" id="c-42499408" checked=""/><div class="controls bullet"><span class="by">boroboro4</span><span>|</span><a href="#42498849">root</a><span>|</span><a href="#42499097">parent</a><span>|</span><a href="#42498921">next</a><span>|</span><label class="collapse" for="c-42499408">[-]</label><label class="expand" for="c-42499408">[3 more]</label></div><br/><div class="children"><div class="content">Only local (read batch size 1) ML inference is memory bound, production loads are pretty much compute bound. Prefill phase is very compute bound, and with continuous batching generation phase is getting mixed with prefill, which makes whole process altogether to be compute bound too. So no, tensor cores and all other shenanigans absolutely critical for performant inference infrastructure.</div><br/><div id="42499609" class="c"><input type="checkbox" id="c-42499609" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#42498849">root</a><span>|</span><a href="#42499408">parent</a><span>|</span><a href="#42498921">next</a><span>|</span><label class="collapse" for="c-42499609">[-]</label><label class="expand" for="c-42499609">[2 more]</label></div><br/><div class="children"><div class="content">PyTorch is a project by Linux foundation. The about page with the mission of the foundation contains phrases like “empowering generations of open source innovators”, “democratize code”, and “removing barriers to adoption”.<p>I would argue running local inference with batch size=1 is more useful for empowering innovators compared to running production loads on shared servers owned by companies. Local inference increases count of potential innovators by orders of magnitude.<p>BTW, in the long run it may also benefit these companies because in theory, an easy migration path from CUDA puts a downward pressure on nVidia’s prices.</div><br/><div id="42499731" class="c"><input type="checkbox" id="c-42499731" checked=""/><div class="controls bullet"><span class="by">idonotknowwhy</span><span>|</span><a href="#42498849">root</a><span>|</span><a href="#42499609">parent</a><span>|</span><a href="#42498921">next</a><span>|</span><label class="collapse" for="c-42499731">[-]</label><label class="expand" for="c-42499731">[1 more]</label></div><br/><div class="children"><div class="content">Most people running local inference do so thorough quants with llamacpp (which runs on everything) or awq&#x2F;exl2&#x2F;mlx with vllm&#x2F;tabbyAPI&#x2F;lmstudio which are much faster to than using pytorch directly</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42499317" class="c"><input type="checkbox" id="c-42499317" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#42498849">prev</a><span>|</span><a href="#42498985">next</a><span>|</span><label class="collapse" for="c-42499317">[-]</label><label class="expand" for="c-42499317">[3 more]</label></div><br/><div class="children"><div class="content">A used 3090 is $600-900, performs better than 7900, and is much more versatile because CUDA</div><br/><div id="42499439" class="c"><input type="checkbox" id="c-42499439" checked=""/><div class="controls bullet"><span class="by">Uehreka</span><span>|</span><a href="#42499317">parent</a><span>|</span><a href="#42498985">next</a><span>|</span><label class="collapse" for="c-42499439">[-]</label><label class="expand" for="c-42499439">[2 more]</label></div><br/><div class="children"><div class="content">Reality check for anyone considering this: I just got a used 3090 for $900 last month. It works great.<p>I would not recommend buying one for $600, it probably either won’t arrive or will be broken. Someone will reply saying they got one for $600 and it works, that doesn’t mean it will happen if you do it.<p>I’d say the market is realistically $900-1100, maybe $800 if you know the person or can watch the card running first.<p>All that said, this advice will expire in a month or two when the 5090 comes out.</div><br/><div id="42499716" class="c"><input type="checkbox" id="c-42499716" checked=""/><div class="controls bullet"><span class="by">idonotknowwhy</span><span>|</span><a href="#42499317">root</a><span>|</span><a href="#42499439">parent</a><span>|</span><a href="#42498985">next</a><span>|</span><label class="collapse" for="c-42499716">[-]</label><label class="expand" for="c-42499716">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve bought 5 used and they&#x27;re all perfect. But that&#x27;s what buyer protection on ebay is for. Had to send back an Epyc mobo with bent pins and ebay handled it fine.</div><br/></div></div></div></div></div></div><div id="42498985" class="c"><input type="checkbox" id="c-42498985" checked=""/><div class="controls bullet"><span class="by">lasermike026</span><span>|</span><a href="#42499317">prev</a><span>|</span><a href="#42499666">next</a><span>|</span><label class="collapse" for="c-42498985">[-]</label><label class="expand" for="c-42498985">[1 more]</label></div><br/><div class="children"><div class="content">I believe these efforts are very important.  If we want this stuff to be practical we are going to have to work on efficiency.  Price efficiency is good.  Power and compute efficiency would be better.<p>I have been playing with llama.cpp to run interference on conventional cpus.  No conclusions but it&#x27;s interesting.  I need to look at llamafile next.</div><br/></div></div><div id="42499666" class="c"><input type="checkbox" id="c-42499666" checked=""/><div class="controls bullet"><span class="by">mattfrommars</span><span>|</span><a href="#42498985">prev</a><span>|</span><a href="#42499128">next</a><span>|</span><label class="collapse" for="c-42499666">[-]</label><label class="expand" for="c-42499666">[8 more]</label></div><br/><div class="children"><div class="content">Great, I have yet to understand why does not the ML community really push or move away from CUDA? To me, it feel like a dinosaur move to build on top of CUDA which is screaming proprietary nothing about it is open source or cross platform.<p>The reason why I say its dinosaur is, imagine, we as a dev community continued to build on top of Flash or Microsoft Silverlight...<p>LLM and ML has been out for quiet a while, with AI&#x2F;LLM advancement, the transition must have been much quicker to move cross platform. But this hasn&#x27;t yet and not sure when it will happen.<p>Building a translation layer on top CUDA is not the answer either to this problem.</div><br/><div id="42499710" class="c"><input type="checkbox" id="c-42499710" checked=""/><div class="controls bullet"><span class="by">idonotknowwhy</span><span>|</span><a href="#42499666">parent</a><span>|</span><a href="#42499686">next</a><span>|</span><label class="collapse" for="c-42499710">[-]</label><label class="expand" for="c-42499710">[2 more]</label></div><br/><div class="children"><div class="content">For me personally, hacking together projects as a hobbiest, 2 reasons :<p>1. It just works. When i tried to build things on Intel Arcs, i spent way more hours bikeshedding ipex and driver issues than developing<p>2. LLMs seem to have more cuda code in their training data. I can leverage claude and 4o to help me build things with cuda, but trying to get them to help me do the same things on ipex just doesn&#x27;t work.<p>I&#x27;d very much love a translation layer for Cuda, like a dxvk or wine equivalent.<p>Would save a lot of money since Arc gpus are in the bargain bin and nvidia cloud servers are double the price of AMD.<p>As it stands now, my dual Intel Arc rig is now just a llama.cpp inference server for the family to use.</div><br/><div id="42500507" class="c"><input type="checkbox" id="c-42500507" checked=""/><div class="controls bullet"><span class="by">FloatArtifact</span><span>|</span><a href="#42499666">root</a><span>|</span><a href="#42499710">parent</a><span>|</span><a href="#42499686">next</a><span>|</span><label class="collapse" for="c-42500507">[-]</label><label class="expand" for="c-42500507">[1 more]</label></div><br/><div class="children"><div class="content">What kind of model learn and what&#x27;s its token output on intel gpu&#x27;s?</div><br/></div></div></div></div><div id="42499686" class="c"><input type="checkbox" id="c-42499686" checked=""/><div class="controls bullet"><span class="by">dwood_dev</span><span>|</span><a href="#42499666">parent</a><span>|</span><a href="#42499710">prev</a><span>|</span><a href="#42499128">next</a><span>|</span><label class="collapse" for="c-42499686">[-]</label><label class="expand" for="c-42499686">[5 more]</label></div><br/><div class="children"><div class="content">Except I never hear complaints about CUDA from a quality perspective. The complaints are always about lock in to the best GPUs on the market. The desire to shift away is to make cheaper hardware with inferior software quality more usable. Flash was an abomination, CUDA is not.</div><br/><div id="42500022" class="c"><input type="checkbox" id="c-42500022" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#42499666">root</a><span>|</span><a href="#42499686">parent</a><span>|</span><a href="#42499695">next</a><span>|</span><label class="collapse" for="c-42500022">[-]</label><label class="expand" for="c-42500022">[2 more]</label></div><br/><div class="children"><div class="content">Flash was popular because it was an attractive platform for the developer. Back then there was no HTML5 and browsers didn&#x27;t otherwise support a lot of the things Flash did. Flash <i>Player</i> was an abomination, it was crashy and full of security vulnerabilities, but that was a problem for the user rather than the developer and it was the developer choosing what to use to make the site.<p>This is pretty much exactly what happens with CUDA. Developers like it but then the users have to use expensive hardware with proprietary drivers&#x2F;firmware, which is the relevant abomination. But users have <i>some</i> ability to influence developers, so as soon as we get the GPU equivalent of HTML5, what happens?</div><br/><div id="42500548" class="c"><input type="checkbox" id="c-42500548" checked=""/><div class="controls bullet"><span class="by">wqaatwt</span><span>|</span><a href="#42499666">root</a><span>|</span><a href="#42500022">parent</a><span>|</span><a href="#42499695">next</a><span>|</span><label class="collapse" for="c-42500548">[-]</label><label class="expand" for="c-42500548">[1 more]</label></div><br/><div class="children"><div class="content">&gt; users have to use expensive hardware with proprietary drivers&#x2F;firmware<p>What do you mean by that? People trying to run their own models are not “the users” they are a tiny insignificant niche segment.</div><br/></div></div></div></div><div id="42499695" class="c"><input type="checkbox" id="c-42499695" checked=""/><div class="controls bullet"><span class="by">xedrac</span><span>|</span><a href="#42499666">root</a><span>|</span><a href="#42499686">parent</a><span>|</span><a href="#42500022">prev</a><span>|</span><a href="#42499128">next</a><span>|</span><label class="collapse" for="c-42499695">[-]</label><label class="expand" for="c-42499695">[2 more]</label></div><br/><div class="children"><div class="content">Maybe the situation has gotten better in recent years,  but my experience with Nvidia toolchains was a complete nightmare back in 2018.</div><br/><div id="42499820" class="c"><input type="checkbox" id="c-42499820" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#42499666">root</a><span>|</span><a href="#42499695">parent</a><span>|</span><a href="#42499128">next</a><span>|</span><label class="collapse" for="c-42499820">[-]</label><label class="expand" for="c-42499820">[1 more]</label></div><br/><div class="children"><div class="content">The cuda situation is definitely better. The nvidia struggles are now with the higher-level software they’re pushing (triton, tensor-llm, riva, etc), tools that are the most performant option when they work, but a garbage developer experience when you step outside the golden path</div><br/></div></div></div></div></div></div></div></div><div id="42499128" class="c"><input type="checkbox" id="c-42499128" checked=""/><div class="controls bullet"><span class="by">zamalek</span><span>|</span><a href="#42499666">prev</a><span>|</span><a href="#42500369">next</a><span>|</span><label class="collapse" for="c-42499128">[-]</label><label class="expand" for="c-42499128">[1 more]</label></div><br/><div class="children"><div class="content">I have been playing around with Phi-4 Q6 on my 7950x and 7900XT (with HSA_OVERRIDE_GFX_VERSION). It&#x27;s bloody fast, even with CPU alone - in practical terms it beats hosted models due to the roundtrip time. Obviously perf is more important if you&#x27;re hosting this stuff, but we&#x27;ve definitely reached AMD usability at home.</div><br/></div></div><div id="42500369" class="c"><input type="checkbox" id="c-42500369" checked=""/><div class="controls bullet"><span class="by">mrcsharp</span><span>|</span><a href="#42499128">prev</a><span>|</span><a href="#42499134">next</a><span>|</span><label class="collapse" for="c-42500369">[-]</label><label class="expand" for="c-42500369">[1 more]</label></div><br/><div class="children"><div class="content">I will only consider AMD GPUs for LLM when I can easily make my AMD GPU available within WSL and Docker on Windows.<p>For now, it is as if AMD does not exist in this field for me.</div><br/></div></div><div id="42499134" class="c"><input type="checkbox" id="c-42499134" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42500369">prev</a><span>|</span><a href="#42498837">next</a><span>|</span><label class="collapse" for="c-42499134">[-]</label><label class="expand" for="c-42499134">[1 more]</label></div><br/><div class="children"><div class="content">Previously:<p><i>Making AMD GPUs competitive for LLM inference</i> <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37066522">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37066522</a> (August 9, 2023 — 354 points, 132 comments)</div><br/></div></div><div id="42498837" class="c"><input type="checkbox" id="c-42498837" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42499134">prev</a><span>|</span><a href="#42500217">next</a><span>|</span><label class="collapse" for="c-42498837">[-]</label><label class="expand" for="c-42498837">[14 more]</label></div><br/><div class="children"><div class="content">Intriguing. I thought AMD GPUs didn&#x27;t have tensor cores (or matrix multiplication units) like NVidia. I believe they are only dot product &#x2F; fused multiply and accumulate instructions.<p>Are these LLMs just absurdly memory bound so it doesn&#x27;t matter?</div><br/><div id="42499428" class="c"><input type="checkbox" id="c-42499428" checked=""/><div class="controls bullet"><span class="by">boroboro4</span><span>|</span><a href="#42498837">parent</a><span>|</span><a href="#42498860">next</a><span>|</span><label class="collapse" for="c-42499428">[-]</label><label class="expand" for="c-42499428">[2 more]</label></div><br/><div class="children"><div class="content">They absolutely do have similar cores to tensor cores, it&#x27;s called matrix cores. And they have particular instructions to utilize them (MFMA).
Note I&#x27;m talking about DC compute chips, like MI300.<p>LLMs aren&#x27;t memory bound in production loads, they are pretty much compute bound too, at least in prefill phase, but in practice in general too.</div><br/><div id="42499445" class="c"><input type="checkbox" id="c-42499445" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42499428">parent</a><span>|</span><a href="#42498860">next</a><span>|</span><label class="collapse" for="c-42499445">[-]</label><label class="expand" for="c-42499445">[1 more]</label></div><br/><div class="children"><div class="content">Ya people in these comments don&#x27;t know what they&#x27;re talking about (no one ever does in these threads). AMDGPU has had MMA and WMMA for a while now<p><a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;rocWMMA&#x2F;en&#x2F;latest&#x2F;what-is-rocwmma.html" rel="nofollow">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;projects&#x2F;rocWMMA&#x2F;en&#x2F;latest&#x2F;what-is...</a></div><br/></div></div></div></div><div id="42498860" class="c"><input type="checkbox" id="c-42498860" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42498837">parent</a><span>|</span><a href="#42499428">prev</a><span>|</span><a href="#42498862">next</a><span>|</span><label class="collapse" for="c-42498860">[-]</label><label class="expand" for="c-42498860">[10 more]</label></div><br/><div class="children"><div class="content">They don’t, but GPUs were designed for doing matrix multiplications even without the special hardware instructions for doing matrix multiplication tiles. Also, the forward pass for transformers is memory bound, and that is what does token generation.</div><br/><div id="42498919" class="c"><input type="checkbox" id="c-42498919" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42498860">parent</a><span>|</span><a href="#42498862">next</a><span>|</span><label class="collapse" for="c-42498919">[-]</label><label class="expand" for="c-42498919">[9 more]</label></div><br/><div class="children"><div class="content">Well sure, but in other GPU tasks, like Raytracing, the difference between these GPUs is far more pronounced.<p>And AMD has passable Raytracing units (NVidias are better but the difference is bigger than these LLM results).<p>If RAM is the main bottleneck then CPUs should be on the table.</div><br/><div id="42499002" class="c"><input type="checkbox" id="c-42499002" checked=""/><div class="controls bullet"><span class="by">IX-103</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42498919">parent</a><span>|</span><a href="#42498969">next</a><span>|</span><label class="collapse" for="c-42499002">[-]</label><label class="expand" for="c-42499002">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If RAM is the main bottleneck then CPUs should be on the table<p>That&#x27;s certainly not the case. The graphics memory model is very different from the CPU memory model. Graphics memory is explicitly designed for multiple simultaneous reads (spread across several different buses) at the cost of generality (only portions of memory may be available on each bus) and speed (the extra complexity means reads are slower). This makes then fast at doing simple operations on a large amount of data.<p>CPU memory only has one bus, so only a single read can happen at a time (a cache line read), but can happen relatively quickly. So CPUs are better for workloads with high memory locality and frequent reuse of memory locations (as is common in procedural programs).</div><br/><div id="42500058" class="c"><input type="checkbox" id="c-42500058" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42499002">parent</a><span>|</span><a href="#42498969">next</a><span>|</span><label class="collapse" for="c-42500058">[-]</label><label class="expand" for="c-42500058">[1 more]</label></div><br/><div class="children"><div class="content">&gt; CPU memory only has one bus<p>If people are paying $15,000 or more per GPU, then I can choose $15,000 CPUs like EPYC that have 12-channels or dual-socket 24-channel RAM.<p>Even desktop CPUs are dual-channel at a minimum, and arguably DDR5 is closer to 2 or 4 buses per channel.<p>Now yes, GPU RAM can be faster, but guess what?<p><a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;pc-components&#x2F;cpus&#x2F;amd-crafts-custom-epyc-cpu-for-microsoft-azure-with-hbm3-memory-cpu-with-88-zen-4-cores-and-450gb-of-hbm3-may-be-repurposed-mi300c-four-chips-hit-7-tb-s" rel="nofollow">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;pc-components&#x2F;cpus&#x2F;amd-crafts-c...</a><p>GPUs are about extremely parallel performance, above and beyond what traditional single-threaded (or limited-SIMD) CPUs can do.<p>But if you&#x27;re waiting on RAM anyway?? Then the compute-method doesn&#x27;t matter. Its all about RAM.</div><br/></div></div></div></div><div id="42498969" class="c"><input type="checkbox" id="c-42498969" checked=""/><div class="controls bullet"><span class="by">webmaven</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42498919">parent</a><span>|</span><a href="#42499002">prev</a><span>|</span><a href="#42499066">next</a><span>|</span><label class="collapse" for="c-42498969">[-]</label><label class="expand" for="c-42498969">[1 more]</label></div><br/><div class="children"><div class="content">RAM is (often) the bottleneck for highly parallel GPUs, but not for CPUs.<p>Though the distinction between the two categories is blurring.</div><br/></div></div><div id="42499066" class="c"><input type="checkbox" id="c-42499066" checked=""/><div class="controls bullet"><span class="by">schmidtleonard</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42498919">parent</a><span>|</span><a href="#42498969">prev</a><span>|</span><a href="#42498862">next</a><span>|</span><label class="collapse" for="c-42499066">[-]</label><label class="expand" for="c-42499066">[5 more]</label></div><br/><div class="children"><div class="content">CPUs have pitiful RAM bandwidth compared to GPUs. The speeds aren&#x27;t so different but GPU RAM busses are wiiiiiiiide.</div><br/><div id="42499225" class="c"><input type="checkbox" id="c-42499225" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42499066">parent</a><span>|</span><a href="#42498862">next</a><span>|</span><label class="collapse" for="c-42499225">[-]</label><label class="expand" for="c-42499225">[4 more]</label></div><br/><div class="children"><div class="content">Compute Express Link (CXL) should mostly solve limited RAM with CPU:<p>1) Compute Express Link (CXL):<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Compute_Express_Link" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Compute_Express_Link</a><p>PCIe vs. CXL for Memory and Storage:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38125885">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38125885</a></div><br/><div id="42499371" class="c"><input type="checkbox" id="c-42499371" checked=""/><div class="controls bullet"><span class="by">schmidtleonard</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42499225">parent</a><span>|</span><a href="#42498862">next</a><span>|</span><label class="collapse" for="c-42499371">[-]</label><label class="expand" for="c-42499371">[3 more]</label></div><br/><div class="children"><div class="content">Gigabytes per second? What is this, bandwidth for ants?<p>My years old pleb tier non-HBM GPU has more than 4 times the bandwidth you would get from a PCIe Gen 7 x16 link, which doesn&#x27;t even officially exist yet.</div><br/><div id="42499660" class="c"><input type="checkbox" id="c-42499660" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42499371">parent</a><span>|</span><a href="#42499938">next</a><span>|</span><label class="collapse" for="c-42499660">[-]</label><label class="expand" for="c-42499660">[1 more]</label></div><br/><div class="children"><div class="content">Yes CXL will soon benefit from PCIe Gen 7 x16 with expected 64GB&#x2F;s in 2025 and the non-HBM bandwidth I&#x2F;O alternative is increasing rapidly by the day. For most inferences of near real-time LLM it will be feasible. For majority of SME companies and other DIY users (humans or ants) with their localized LLM should not be any issues [1],[2]. In addition new techniques for more efficient LLM are being discover to reduce the memory consumption [3].<p>[1] Forget ChatGPT: why researchers now run small AIs on their laptops:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41609393">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41609393</a><p>[2] Welcome to LLMflation – LLM inference cost is going down fast:<p><a href="https:&#x2F;&#x2F;a16z.com&#x2F;llmflation-llm-inference-cost&#x2F;" rel="nofollow">https:&#x2F;&#x2F;a16z.com&#x2F;llmflation-llm-inference-cost&#x2F;</a><p>[3] New LLM optimization technique slashes memory costs up to 75%:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42411409">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42411409</a></div><br/></div></div><div id="42499938" class="c"><input type="checkbox" id="c-42499938" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#42498837">root</a><span>|</span><a href="#42499371">parent</a><span>|</span><a href="#42499660">prev</a><span>|</span><a href="#42498862">next</a><span>|</span><label class="collapse" for="c-42499938">[-]</label><label class="expand" for="c-42499938">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 4 times the bandwidth you would get from a PCIe Gen 7 x16 link<p>So you have a full terabyte per second of bandwidth?  What GPU is that?<p>(The 64GB&#x2F;s number is an x4 link.  If you meant you have over four times that, then it sounds like CXL would be pretty competitive.)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42498862" class="c"><input type="checkbox" id="c-42498862" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42498837">parent</a><span>|</span><a href="#42498860">prev</a><span>|</span><a href="#42500217">next</a><span>|</span><label class="collapse" for="c-42498862">[-]</label><label class="expand" for="c-42498862">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Are these LLMs just absurdly memory bound so it doesn&#x27;t matter?<p>During inference? Definitely. Training is another story.</div><br/></div></div></div></div><div id="42500217" class="c"><input type="checkbox" id="c-42500217" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#42498837">prev</a><span>|</span><a href="#42498843">next</a><span>|</span><label class="collapse" for="c-42500217">[-]</label><label class="expand" for="c-42500217">[1 more]</label></div><br/><div class="children"><div class="content">Just an FYI, this is writeup from August 2023 and a lot has changed (for the better!) for RDNA3 AI&#x2F;ML support.<p>That being said, I did some very recent inference testing on an W7900 (using the same testing methodology used by Embedded LLM&#x27;s recent post to compare to vLLM&#x27;s recently added Radeon GGUF support [1]) and MLC continues to perform quite well. On Llama 3.1 8B, MLC&#x27;s q4f16_1 (4.21MB weights) performed +35% faster than llama.cpp w&#x2F; Q4_K_M w&#x2F; their ROCm&#x2F;HIP backend (4.30MB weights, 2% size difference).<p>That makes MLC still the generally fastest standalone inference engine for RDNA3 by a country mile. However, you have much less flexibility with quants and by and large have to compile your own for every model, so llama.cpp is probably still more flexible for general use. Also llama.cpp&#x27;s (recently added to llama-server) speculative decoding can also give some pretty sizable performance gains. Using a 70B Q4_K_M + 1B Q8_0 draft model improves output token throughput by 59% on the same ShareGPT testing. I&#x27;ve also been running tests with Qwen2.5-Coder and using a 0.5-3B draft model for speculative decoding gives even bigger gains on average (depends highly on acceptance rate).<p>Note, I think for local use, vLLM GGUF is still not suitable at all. When testing w&#x2F; a 70B Q4_K_M model (only 40GB), loading, engine warmup, and graph compilation took on avg 40 minutes. llama.cpp takes 7-8s to load the same model.<p>At this point for RDNA3, basically everything I need works&#x2F;runs for my use cases (primarily LLM development and local inferencing), but almost always slower than an RTX 3090&#x2F;A6000 Ampere (a new 24GB 7900 XTX is $850 atm, used or refurbished 24 GB RTX 3090s are in in the same ballpark, about $800 atm; a new 48GB W7900 goes for $3600 while an 48GB A6000 (Ampere) goes for $4600). The efficiency gains can be sizable. Eg, on my standard llama-bench test w&#x2F; llama2-7b-q4_0, the RTX 3090 gets a tg128 of 168 t&#x2F;s while the 7900 XTX only gets 118 t&#x2F;s even though both have similar memory bandwidth (936.2 GB&#x2F;s vs 960 GB&#x2F;s). It&#x27;s also worth noting that since the beginning of the year, the llama.cpp CUDA implementation has gotten almost 25% faster, while the ROCm version&#x27;s performance has stayed static.<p>There is an actively (solo dev) maintained fork of llama.cpp that sticks close to HEAD but basically applies a rocWMMA patch that can improve performance if you use the llama.cpp FA (still performs worse than w&#x2F; FA disabled) and in certain long-context inference generations (on llama-bench and w&#x2F; this ShareGPT serving test you won&#x27;t see much difference) here: <a href="https:&#x2F;&#x2F;github.com&#x2F;hjc4869&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;hjc4869&#x2F;llama.cpp</a> - The fact that no one from AMD has shown any interest in helping improve llama.cpp performance (despite often citing llama.cpp-based apps in marketing&#x2F;blog posts, etc is disappointing ... but sadly on brand for AMD GPUs).<p>Anyway, for those interested in more information and testing for AI&#x2F;ML setup for RDNA3 (and AMD ROCm in general), I keep a doc with lots of details here: <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;howto&#x2F;AMD-GPUs" rel="nofollow">https:&#x2F;&#x2F;llm-tracker.info&#x2F;howto&#x2F;AMD-GPUs</a><p>[1] <a href="https:&#x2F;&#x2F;embeddedllm.com&#x2F;blog&#x2F;vllm-now-supports-running-gguf-on-amd-radeon-gpu" rel="nofollow">https:&#x2F;&#x2F;embeddedllm.com&#x2F;blog&#x2F;vllm-now-supports-running-gguf-...</a></div><br/></div></div><div id="42498843" class="c"><input type="checkbox" id="c-42498843" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#42500217">prev</a><span>|</span><a href="#42500122">next</a><span>|</span><label class="collapse" for="c-42498843">[-]</label><label class="expand" for="c-42498843">[1 more]</label></div><br/><div class="children"><div class="content">[2023]<p>Btw, this is from MLC-LLM which makes WebLLM and other good stuff.</div><br/></div></div><div id="42500122" class="c"><input type="checkbox" id="c-42500122" checked=""/><div class="controls bullet"><span class="by">aussieguy1234</span><span>|</span><a href="#42498843">prev</a><span>|</span><a href="#42499425">next</a><span>|</span><label class="collapse" for="c-42500122">[-]</label><label class="expand" for="c-42500122">[1 more]</label></div><br/><div class="children"><div class="content">I got a &quot;gaming&quot; PC for LLM inference with an RTX 3060. I could have gotten more VRAM for my buck with AMD, but didn&#x27;t because at the time alot of inference needed CUDA.<p>As soon AMD is as good as Nvidia for inference, I&#x27;ll switch over.<p>But I&#x27;ve read on here that their hardware engineers aren&#x27;t even given enough hardware to test with...</div><br/></div></div><div id="42499416" class="c"><input type="checkbox" id="c-42499416" checked=""/><div class="controls bullet"><span class="by">leonewton253</span><span>|</span><a href="#42499425">prev</a><span>|</span><label class="collapse" for="c-42499416">[-]</label><label class="expand" for="c-42499416">[2 more]</label></div><br/><div class="children"><div class="content">This benchmark doest look right. Is it using the tensor cores in the Nvidia gpu? AMD does not have AI cores so should run noticeably slower.</div><br/><div id="42499532" class="c"><input type="checkbox" id="c-42499532" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#42499416">parent</a><span>|</span><label class="collapse" for="c-42499532">[-]</label><label class="expand" for="c-42499532">[1 more]</label></div><br/><div class="children"><div class="content">AMD has WMMA.</div><br/></div></div></div></div></div></div></div></div></div></body></html>