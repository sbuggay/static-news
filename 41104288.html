<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1722502852513" as="style"/><link rel="stylesheet" href="styles.css?v=1722502852513"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/">Amazon&#x27;s exabyte-scale migration from Apache Spark to Ray on EC2</a> <span class="domain">(<a href="https://aws.amazon.com">aws.amazon.com</a>)</span></div><div class="subtext"><span>nojito</span> | <span>67 comments</span></div><br/><div><div id="41123421" class="c"><input type="checkbox" id="c-41123421" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#41125982">next</a><span>|</span><label class="collapse" for="c-41123421">[-]</label><label class="expand" for="c-41123421">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;m one of the creators of Ray. A few thoughts :)<p>1. This is truly impressive work from AWS. Patrick Ames began speaking about this a couple years ago, though at this point the blog post is probably the best reference. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=h7svj_oAY14" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=h7svj_oAY14</a><p>2. This is not a &quot;typical&quot; Ray use case. I&#x27;m not aware of any other exabyte scale data processing workloads. Our bread and butter is ML workloads: training, inference, and unstructured data processing.<p>3. We have a data processing library called Ray Data for ingesting and processing data, often done in conjunction with training and inference. However, I believe in this particular use case, the heavy lifting is largely done with Ray&#x27;s core APIs (tasks &amp; actors), which are lower level and more flexible, which makes sense for highly custom use cases. Most Ray users use the Ray libraries (train, data, serve), but power users often use the Ray core APIs.<p>4. Since people often ask about data processing with Ray and Spark, Spark use cases tend to be more geared toward structured data and CPU processing. If you are joining a bunch of tables together or running SQL queries, Spark is going to be way better. If you&#x27;re working with unstructured data (images, text, video, audio, etc), need mixed CPU &amp; GPU compute, are doing deep learning and running inference, etc, then Ray is going to be much better.</div><br/><div id="41124055" class="c"><input type="checkbox" id="c-41124055" checked=""/><div class="controls bullet"><span class="by">justsocrateasin</span><span>|</span><a href="#41123421">parent</a><span>|</span><a href="#41124504">next</a><span>|</span><label class="collapse" for="c-41124055">[-]</label><label class="expand" for="c-41124055">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m just learning about this tool now and had a brief question if you have the time:<p>The paper mentions support for zero-copy intranode object sharing which links to serialization in the Ray docs - <a href="https:&#x2F;&#x2F;docs.ray.io&#x2F;en&#x2F;latest&#x2F;ray-core&#x2F;objects&#x2F;serialization.html#serialization" rel="nofollow">https:&#x2F;&#x2F;docs.ray.io&#x2F;en&#x2F;latest&#x2F;ray-core&#x2F;objects&#x2F;serialization...</a><p>I&#x27;m really curious how this is performant - I recently tried building a pipeline that leveraged substantial multiprocessing in Python, and found that my process was bottlenecked by the serialization&#x2F;deserialization that occurs during Python multiprocessing. Would love any reading or explanation you can provide as to how this doesn&#x27;t also bottleneck a process in Ray, since it seems that data transferred between workers and nodes will need to serialized and deserialized.<p>Thanks in advance! Really cool tool, hopefully I&#x27;ll be able to use it sooner rather than later.</div><br/><div id="41124208" class="c"><input type="checkbox" id="c-41124208" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#41123421">root</a><span>|</span><a href="#41124055">parent</a><span>|</span><a href="#41124504">next</a><span>|</span><label class="collapse" for="c-41124208">[-]</label><label class="expand" for="c-41124208">[3 more]</label></div><br/><div class="children"><div class="content">Your right that the serialization &#x2F; deserialization overhead can quickly exceed the compute time. To avoid this you have to get a lot of small things right. And given our focus on ML workloads, this is particularly important when sharing large numerical arrays between processes (especially processes running on the same node).<p>One of the key things is to make sure the serialized object is stored in a data format where the serialized object does not need to be &quot;transformed&quot; in order to access it. For example, a numpy array can be created in O(1) time from a serialized blob by initializing a Python object with the right shape and dtype and a pointer to the right offset in the serialized blob. We also use projects like Apache Arrow that put a lot of care into this.<p>Example in more detail:<p>Imagine the object you are passing from process A to process B is a 1GB numpy array of floats. In the serialization step, process A produces a serialized blob of bytes that is basically just the 1GB numpy array plus a little bit of metadata. Process A writes that serialized blob into shared memory. This step of &quot;writing into shared memory&quot; still involves O(N) work, where N is the size of the array (though you can have multiple threads do the memcpy in parallel and be limited just by memory bandwidth).<p>In the deserialization step, process B accesses the same shared memory blob (process A and B are on the same machine). It reads a tiny bit of metadata to figure out the type of the serialized object and shape and so on. Then it constructs a numpy array with the correct shape and type and with a <i>pointer</i> to the actual data in shared memory at the right offset. Therefore it doesn&#x27;t need to touch all of the bytes of data, it just does O(1) work instead of O(N).<p>That&#x27;s the basic idea. You can imagine generalizing this beyond numpy arrays, but it&#x27;s most effective for objects that include large numerical data (e.g., objects that include numpy arrays).<p>There are a bunch of little details to get right, e.g., serializing directly into shared memory instead of creating a serialized copy in process A and then copying it into shared memory. Doing the write into shared memory in parallel with a bunch of threads. Getting the deserialization right. You also have to make sure that the starting addresses of the numpy arrays are 64-byte aligned (if memory serves) so that you don&#x27;t accidentally trigger a copy later on.<p>EDIT: I edited the above to add more detail.</div><br/><div id="41124421" class="c"><input type="checkbox" id="c-41124421" checked=""/><div class="controls bullet"><span class="by">Xophmeister</span><span>|</span><a href="#41123421">root</a><span>|</span><a href="#41124208">parent</a><span>|</span><a href="#41124504">next</a><span>|</span><label class="collapse" for="c-41124421">[-]</label><label class="expand" for="c-41124421">[2 more]</label></div><br/><div class="children"><div class="content">This is probably a naive question, but how do two processes share address space? mmap?</div><br/><div id="41124465" class="c"><input type="checkbox" id="c-41124465" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#41123421">root</a><span>|</span><a href="#41124421">parent</a><span>|</span><a href="#41124504">next</a><span>|</span><label class="collapse" for="c-41124465">[-]</label><label class="expand" for="c-41124465">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, mmap, I think this is the relevant line [1].<p>Fun fact, very early on, we used to create one mmapped file per serialized object, but that very quickly broke down.<p>Then we switched to mmapping one large file at the start and storing all of the serialized objects in that file. But then as objects get allocated and deallocated, you need to manage the memory inside of that mmapped file, and we just repurposed a malloc implementation to handle that.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;ray&#x2F;blob&#x2F;21202f6ddc3ceaf74fbc1a450fe71be0bb3bfd5f&#x2F;src&#x2F;ray&#x2F;object_manager&#x2F;plasma&#x2F;shared_memory.cc#L29">https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;ray&#x2F;blob&#x2F;21202f6ddc3ceaf74fbc...</a></div><br/></div></div></div></div></div></div></div></div><div id="41124504" class="c"><input type="checkbox" id="c-41124504" checked=""/><div class="controls bullet"><span class="by">zacmps</span><span>|</span><a href="#41123421">parent</a><span>|</span><a href="#41124055">prev</a><span>|</span><a href="#41124707">next</a><span>|</span><label class="collapse" for="c-41124504">[-]</label><label class="expand" for="c-41124504">[2 more]</label></div><br/><div class="children"><div class="content">Super cool to see you here.<p>I&#x27;ve also looked at ray for running data pipelines before (at much much smaller scales) for the reasons you suggest (unstructured data, mixed CPU&#x2F;GPU compute).<p>One thing I&#x27;ve wanted is an incremental computation framework (i.e., salsa [1]) built on ray so that I can write jobs that transparently reuse intermediate results from an object store if their dependents haven&#x27;t changed.<p>Do you know if anyone has thought of building something like this?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;salsa-rs&#x2F;salsa">https:&#x2F;&#x2F;github.com&#x2F;salsa-rs&#x2F;salsa</a></div><br/><div id="41125120" class="c"><input type="checkbox" id="c-41125120" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#41123421">root</a><span>|</span><a href="#41124504">parent</a><span>|</span><a href="#41124707">next</a><span>|</span><label class="collapse" for="c-41125120">[-]</label><label class="expand" for="c-41125120">[1 more]</label></div><br/><div class="children"><div class="content">Other folks have built data processing libraries on top of Ray: Modin and Daft come to mind.<p>But I&#x27;m not aware of anything exactly like what you&#x27;re referring to!</div><br/></div></div></div></div><div id="41124707" class="c"><input type="checkbox" id="c-41124707" checked=""/><div class="controls bullet"><span class="by">theLiminator</span><span>|</span><a href="#41123421">parent</a><span>|</span><a href="#41124504">prev</a><span>|</span><a href="#41125264">next</a><span>|</span><label class="collapse" for="c-41124707">[-]</label><label class="expand" for="c-41124707">[2 more]</label></div><br/><div class="children"><div class="content">Curious if you know how well Ray works with multithreaded python libraries? For example, when using jax with ray, I have to ensure the import ordering imports ray first, as forking a threaded process leads to deadlocks in Python. Do you know how to ensure that ray handles forking the python interpreter correctly?</div><br/><div id="41124965" class="c"><input type="checkbox" id="c-41124965" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#41123421">root</a><span>|</span><a href="#41124707">parent</a><span>|</span><a href="#41125264">next</a><span>|</span><label class="collapse" for="c-41124965">[-]</label><label class="expand" for="c-41124965">[1 more]</label></div><br/><div class="children"><div class="content">Multi-threaded libraries (e.g., numpy and PyTorch on CPUs come to mind) are well supported. In scenarios where many processes are each running heavily multi-threaded computations, it can help to pin specific processes to specific cores (e.g., using tools like psutil) to avoid contention.<p>The scenario where a Ray task forks is probably not very well supported. You can certainly start a subprocess from within a Ray task, but I think forking could easily cause issues.<p>You can definitely use Ray + Jax, but you probably need to avoid forking a process within a Ray worker.</div><br/></div></div></div></div><div id="41125264" class="c"><input type="checkbox" id="c-41125264" checked=""/><div class="controls bullet"><span class="by">nubinetwork</span><span>|</span><a href="#41123421">parent</a><span>|</span><a href="#41124707">prev</a><span>|</span><a href="#41125982">next</a><span>|</span><label class="collapse" for="c-41125264">[-]</label><label class="expand" for="c-41125264">[1 more]</label></div><br/><div class="children"><div class="content">&gt; this is not a typical ray use case<p>Must be good enough if you&#x27;re willing to dogfood it though?</div><br/></div></div></div></div><div id="41125982" class="c"><input type="checkbox" id="c-41125982" checked=""/><div class="controls bullet"><span class="by">jaychia</span><span>|</span><a href="#41123421">prev</a><span>|</span><a href="#41123806">next</a><span>|</span><label class="collapse" for="c-41125982">[-]</label><label class="expand" for="c-41125982">[1 more]</label></div><br/><div class="children"><div class="content">I work on Daft and we’ve been collaborating with the team at Amazon to make this happen for about a year now!<p>We love Ray, and are excited about the awesome ecosystem of useful + scalable tools that run on it for model training and serving. We hope that Daft can complement the rest of the Ray ecosystem to enable large scale ETL&#x2F;analytics to also run on your existing Ray clusters. If you have an existing Ray cluster setup, you absolutely should have access to best-in-class ETL&#x2F;analytics without having to run a separate Spark cluster.<p>Also, on the nerdier side of things - the primitives that Ray provides gives us a real opportunity to build a solid non-JVM based, vectorized distributed query engine. We’re already seeing extremely good performance improvements here vs Spark, and are really excited about some of the upcoming work to get even better performance and memory stability.<p>This collaboration with Amazon really battle-tested our framework :) happy to answer any questions if folks have them.</div><br/></div></div><div id="41123806" class="c"><input type="checkbox" id="c-41123806" checked=""/><div class="controls bullet"><span class="by">parhamn</span><span>|</span><a href="#41125982">prev</a><span>|</span><a href="#41126109">next</a><span>|</span><label class="collapse" for="c-41123806">[-]</label><label class="expand" for="c-41123806">[5 more]</label></div><br/><div class="children"><div class="content">Im curious, how do data scientists use these massive datasets, especially the old stuff. Is it more of a compliance and need&#x2F;should-save type thing or is the data actually useful? Im baffled by these numbers having never used a large BI tool, and am genuinely curious how the data is actually used operationally.<p>As a layman, I imagine lots of it loses relevancy very quickly, e.g Amazon sales data from 5 years ago is marginally useful to determining future trends and analyzing new consumer behavior regimes?</div><br/><div id="41123848" class="c"><input type="checkbox" id="c-41123848" checked=""/><div class="controls bullet"><span class="by">gregw2</span><span>|</span><a href="#41123806">parent</a><span>|</span><a href="#41124320">next</a><span>|</span><label class="collapse" for="c-41123848">[-]</label><label class="expand" for="c-41123848">[2 more]</label></div><br/><div class="children"><div class="content">If you have seasonal demand patterns, you generally need three years history to do good predictive analytics.<p>I do tend to agree data from five years ago is rarely relevant BUT our business is still using for some BI purposes data from the fiscal year before COVID as a comparison baseline for certain analytics&#x2F;business processes which have been slow to reach pre-COVID levels of performance. So that means we are now using data 6 years old, comparing this year to that pre-COVID year for certain analytics!</div><br/><div id="41126438" class="c"><input type="checkbox" id="c-41126438" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#41123806">root</a><span>|</span><a href="#41123848">parent</a><span>|</span><a href="#41124320">next</a><span>|</span><label class="collapse" for="c-41126438">[-]</label><label class="expand" for="c-41126438">[1 more]</label></div><br/><div class="children"><div class="content">Yeah 100%. I worked in wind energy for a while and the DS team would be pulling as much data as they could get to establish a baseline for normality due to seasonal trends in the wind. This also varied enormously around the world - for e.g. the UK is fairly windy all year, but India typically gets 2&#x2F;3 of it&#x27;s generated wind energy in the monsoon season which is about 3 months.</div><br/></div></div></div></div><div id="41124320" class="c"><input type="checkbox" id="c-41124320" checked=""/><div class="controls bullet"><span class="by">smfjaw</span><span>|</span><a href="#41123806">parent</a><span>|</span><a href="#41123848">prev</a><span>|</span><a href="#41126344">next</a><span>|</span><label class="collapse" for="c-41124320">[-]</label><label class="expand" for="c-41124320">[1 more]</label></div><br/><div class="children"><div class="content">I work in finance and it&#x27;s great having big historical datasets, even if the figures are far lower in previous years it&#x27;s good to see system &#x27;shocks&#x27; and these can be used at a different magnitude&#x2F;scaled for future forecasting</div><br/></div></div><div id="41126344" class="c"><input type="checkbox" id="c-41126344" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#41123806">parent</a><span>|</span><a href="#41124320">prev</a><span>|</span><a href="#41126109">next</a><span>|</span><label class="collapse" for="c-41126344">[-]</label><label class="expand" for="c-41126344">[1 more]</label></div><br/><div class="children"><div class="content">Customer retention, infrequent purchases, and time series forecasts all benefit from having at least several years of data.</div><br/></div></div></div></div><div id="41126109" class="c"><input type="checkbox" id="c-41126109" checked=""/><div class="controls bullet"><span class="by">hiyer</span><span>|</span><a href="#41123806">prev</a><span>|</span><a href="#41123029">next</a><span>|</span><label class="collapse" for="c-41126109">[-]</label><label class="expand" for="c-41126109">[1 more]</label></div><br/><div class="children"><div class="content">We chose Ray over Spark in my previous company mostly because we were a Python shop and Ray is Python-native (though it&#x27;s implemented in C++ I believe). It worked very well for us even for real-time queries - though we were obviously nowhere near the scale that AWS is at.</div><br/></div></div><div id="41123029" class="c"><input type="checkbox" id="c-41123029" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#41126109">prev</a><span>|</span><a href="#41123079">next</a><span>|</span><label class="collapse" for="c-41123029">[-]</label><label class="expand" for="c-41123029">[1 more]</label></div><br/><div class="children"><div class="content">Video from the author deep diving this. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=h7svj_oAY14" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=h7svj_oAY14</a></div><br/></div></div><div id="41123079" class="c"><input type="checkbox" id="c-41123079" checked=""/><div class="controls bullet"><span class="by">zitterbewegung</span><span>|</span><a href="#41123029">prev</a><span>|</span><a href="#41105935">next</a><span>|</span><label class="collapse" for="c-41123079">[-]</label><label class="expand" for="c-41123079">[3 more]</label></div><br/><div class="children"><div class="content">I was in a workshop that taught me Ray. It was interesting to know that the people who started Spark were also involved in making Ray.<p>This is not badmouthing either project just an observation and if you architected one task you would be good at attacking the same problem better .</div><br/><div id="41123400" class="c"><input type="checkbox" id="c-41123400" checked=""/><div class="controls bullet"><span class="by">fs111</span><span>|</span><a href="#41123079">parent</a><span>|</span><a href="#41105935">next</a><span>|</span><label class="collapse" for="c-41123400">[-]</label><label class="expand" for="c-41123400">[2 more]</label></div><br/><div class="children"><div class="content">Well spark was really a showcase Project for mesos when it was created. Now everyone knows a lot more</div><br/><div id="41126660" class="c"><input type="checkbox" id="c-41126660" checked=""/><div class="controls bullet"><span class="by">bigcat12345678</span><span>|</span><a href="#41123079">root</a><span>|</span><a href="#41123400">parent</a><span>|</span><a href="#41105935">next</a><span>|</span><label class="collapse" for="c-41126660">[-]</label><label class="expand" for="c-41126660">[1 more]</label></div><br/><div class="children"><div class="content">Spark was never a showcase for mesos<p>Mesos was a derivative idea from some sporadic idea from 2 level scheduling from inside Google based on mapreduce<p>Mesos was wrong from day one, they thought they have the right idea, but was really caught up by a Small group of Google engineers who happened to be from academia. These engineers were very good at having theoretically nice idea<p>In the time mesos was invented, Google had its own mesos, which is a similar project by learning wrong lesson from Borg. That thing is Omega.<p>Eventually everyone decided that Borg was right all along, thus the kubernetes</div><br/></div></div></div></div></div></div><div id="41105935" class="c"><input type="checkbox" id="c-41105935" checked=""/><div class="controls bullet"><span class="by">quadrature</span><span>|</span><a href="#41123079">prev</a><span>|</span><a href="#41105332">next</a><span>|</span><label class="collapse" for="c-41105935">[-]</label><label class="expand" for="c-41105935">[4 more]</label></div><br/><div class="children"><div class="content">Anyone know enough about ray to comment on what the exact performance unlock was ?. They mention that it gave them enough control over the distribution of work so that they could avoid unnecessary reads&#x2F;write. That seems like a good win but I would assume that doing compaction in python would be quite slow.</div><br/><div id="41106142" class="c"><input type="checkbox" id="c-41106142" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41105935">parent</a><span>|</span><a href="#41105332">next</a><span>|</span><label class="collapse" for="c-41106142">[-]</label><label class="expand" for="c-41106142">[3 more]</label></div><br/><div class="children"><div class="content">Some of the initial differentiators are described at the bottom of our design doc at <a href="https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;deltacat&#x2F;blob&#x2F;main&#x2F;deltacat&#x2F;compute&#x2F;compactor&#x2F;TheFlashCompactorDesign.pdf">https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;deltacat&#x2F;blob&#x2F;main&#x2F;deltacat&#x2F;c...</a>.  But yes, controlling file I&#x2F;O was also an important part of this since it allowed us to (1) run more targeted downloads&#x2F;reads of only the Parquet row groups and columns participating in compaction and (2) track dirty&#x2F;clean files to skip unnecessary re-writes of &quot;clean&quot; files that weren&#x27;t altered by compaction. Also, just better leveraging catalog metadata (e.g., primary key indexes if available) to filter out more files in the initial scan, and to copy clean files into the compacted variant by reference (when supported by the underlying catalog format).<p>The trick with doing compaction in Python was to ensure that the most performance-sensitive code was delegated to more optimal C++ (e.g, Ray and Arrow) and Rust (e.g., Daft) code paths. If we did all of our per-record processing ops in pure Python, compaction would indeed be much slower.</div><br/><div id="41123726" class="c"><input type="checkbox" id="c-41123726" checked=""/><div class="controls bullet"><span class="by">ZeroCool2u</span><span>|</span><a href="#41105935">root</a><span>|</span><a href="#41106142">parent</a><span>|</span><a href="#41109738">next</a><span>|</span><label class="collapse" for="c-41123726">[-]</label><label class="expand" for="c-41123726">[1 more]</label></div><br/><div class="children"><div class="content">This is one of the first times I&#x27;ve heard of people using Daft in the wild. Would you be able to elaborate on where Daft came in handy?<p>Edit: Nvm, I kept reading! Thanks for the interesting post!</div><br/></div></div><div id="41109738" class="c"><input type="checkbox" id="c-41109738" checked=""/><div class="controls bullet"><span class="by">quadrature</span><span>|</span><a href="#41105935">root</a><span>|</span><a href="#41106142">parent</a><span>|</span><a href="#41123726">prev</a><span>|</span><a href="#41105332">next</a><span>|</span><label class="collapse" for="c-41109738">[-]</label><label class="expand" for="c-41109738">[1 more]</label></div><br/><div class="children"><div class="content">Thanks a lot for the explanation. Sounds a lot like how Pyspark allows for declarative definitions of computation that is actually executed in Java.</div><br/></div></div></div></div></div></div><div id="41105332" class="c"><input type="checkbox" id="c-41105332" checked=""/><div class="controls bullet"><span class="by">mannyv</span><span>|</span><a href="#41105935">prev</a><span>|</span><a href="#41126345">next</a><span>|</span><label class="collapse" for="c-41105332">[-]</label><label class="expand" for="c-41105332">[15 more]</label></div><br/><div class="children"><div class="content">Crazy that the project took almost 4 years end-to-end, and it&#x27;s still ongoing.<p>I had no idea anything at AWS had that long of an attention span.<p>It&#x27;s funny and telling that in the end, it&#x27;s all backed by CSVs in s3. Long live CSV!</div><br/><div id="41105534" class="c"><input type="checkbox" id="c-41105534" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41105332">parent</a><span>|</span><a href="#41105503">next</a><span>|</span><label class="collapse" for="c-41105534">[-]</label><label class="expand" for="c-41105534">[3 more]</label></div><br/><div class="children"><div class="content">Hi mannyv - one of the devs that worked on the migration here. It has been a pretty long project - approached with caution due to the criticality of keeping our BI datasets healthy - but the preliminary results produced year-over-year kept looking promising enough to keep after it. =)<p>Also, we mostly have Parquet data cataloged in S3 today, but delimited text is indeed ubiquitous and surprisingly sticky, so we continue to maintain some very large datasets natively in this format. However, while the table&#x27;s data producer may prefer to write delimited text, they are almost always converted to Parquet during the compaction process to produce a read-optimized table variant downstream.</div><br/><div id="41123862" class="c"><input type="checkbox" id="c-41123862" checked=""/><div class="controls bullet"><span class="by">gregw2</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41105534">parent</a><span>|</span><a href="#41105503">next</a><span>|</span><label class="collapse" for="c-41123862">[-]</label><label class="expand" for="c-41123862">[2 more]</label></div><br/><div class="children"><div class="content">Are you all shifting over to storing as iceberg-enriched parquet yet and letting it (within, say Athena) manage compaction or thinking about it, or is it not worth it since this new Ray+Parquet thing is working for you?</div><br/><div id="41124022" class="c"><input type="checkbox" id="c-41124022" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41123862">parent</a><span>|</span><a href="#41105503">next</a><span>|</span><label class="collapse" for="c-41124022">[-]</label><label class="expand" for="c-41124022">[1 more]</label></div><br/><div class="children"><div class="content">As alluded to in the blog post, Ray+Parquet+Iceberg is the next frontier we&#x27;d like to make our compactor and similar procedures available on in open source so that the community can start bringing similar benefits for their Iceberg workloads. Stay tuned. =)</div><br/></div></div></div></div></div></div><div id="41105503" class="c"><input type="checkbox" id="c-41105503" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#41105332">parent</a><span>|</span><a href="#41105534">prev</a><span>|</span><a href="#41122387">next</a><span>|</span><label class="collapse" for="c-41105503">[-]</label><label class="expand" for="c-41105503">[1 more]</label></div><br/><div class="children"><div class="content">They reference parquet files, not sure if it&#x27;s only CSV or CSV even figures in that heavily other than the first iteration before migrating to spark</div><br/></div></div><div id="41122387" class="c"><input type="checkbox" id="c-41122387" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#41105332">parent</a><span>|</span><a href="#41105503">prev</a><span>|</span><a href="#41124152">next</a><span>|</span><label class="collapse" for="c-41122387">[-]</label><label class="expand" for="c-41122387">[6 more]</label></div><br/><div class="children"><div class="content">Most people are moving away from CSV for big datasets, except in exceptional cases involving linear reads (append only ETL). CSV has one big upside which is human readability. But so many downsides: poor random access, no typing, no compression, complex parser needing to handle exceptions.</div><br/><div id="41122866" class="c"><input type="checkbox" id="c-41122866" checked=""/><div class="controls bullet"><span class="by">100pctremote</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41122387">parent</a><span>|</span><a href="#41124152">next</a><span>|</span><label class="collapse" for="c-41122866">[-]</label><label class="expand" for="c-41122866">[5 more]</label></div><br/><div class="children"><div class="content">Most people don&#x27;t directly query or otherwise operate on raw CSV, though. Large source datasets in CSV format still reign in many enterprises, but these are typically read into a dataframe, manipulated and stored as Parquet and the like, then operated upon by DuckDB, Polars, etc., or modeled (E.g. DBT) and pushed to an OLAP target.</div><br/><div id="41123363" class="c"><input type="checkbox" id="c-41123363" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41122866">parent</a><span>|</span><a href="#41124152">next</a><span>|</span><label class="collapse" for="c-41123363">[-]</label><label class="expand" for="c-41123363">[4 more]</label></div><br/><div class="children"><div class="content">There are folks who still directly query CSV formats in a data lake using a query engine like Athena or Spark or Redshift Spectrum — which ends up being much slower and consuming more resources than is necessary due to full table scans.<p>CSV is only good for append only.<p>But so is Parquet and if you can write Parquet from the get go, you save on storage as well has have a directly queryable column store from the start.<p>CSV still exists because of legacy data generating processes and dearth of Parquet familiarity among many software engineers. CSV is simple to generate and easy to troubleshoot without specialized tools (compared to Parquet which requires tools like Visidata). But you pay for it elsewhere.</div><br/><div id="41125141" class="c"><input type="checkbox" id="c-41125141" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41123363">parent</a><span>|</span><a href="#41123731">next</a><span>|</span><label class="collapse" for="c-41125141">[-]</label><label class="expand" for="c-41125141">[2 more]</label></div><br/><div class="children"><div class="content">how about using Sqlite database files as an interchange format?</div><br/><div id="41125716" class="c"><input type="checkbox" id="c-41125716" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41125141">parent</a><span>|</span><a href="#41123731">next</a><span>|</span><label class="collapse" for="c-41125716">[-]</label><label class="expand" for="c-41125716">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t thought about sqlite as a data interchange format, but I was looking at deploying sqlite as a data lake format some time ago, and found it wanting.<p>1. Dynamically typed (with type affinity) [1]. This causes problems with there are multiple data generating processes. The new sqlite has a STRICT table type that enforces types but only for the few basic types that it has.<p>2. Doesn&#x27;t have a date&#x2F;time type [1]. This is problematic because you can store dates as TEXT, REAL or INTEGER (it&#x27;s up to the developer) and if you have sqlite files from &gt; 1 source, date fields could be any of those types, and you have to convert between them.<p>3. Isn&#x27;t columnar, so complex analytics at scale is not performant.<p>I guess one can use sqlite as a data interchange format, but it&#x27;s not ideal.<p>One area sqlite does excel in is as a application file format [2] and that&#x27;s where it is mostly used [3].<p>[1] <a href="https:&#x2F;&#x2F;www.sqlite.org&#x2F;datatype3.html" rel="nofollow">https:&#x2F;&#x2F;www.sqlite.org&#x2F;datatype3.html</a><p>[2] <a href="https:&#x2F;&#x2F;www.sqlite.org&#x2F;appfileformat.html" rel="nofollow">https:&#x2F;&#x2F;www.sqlite.org&#x2F;appfileformat.html</a><p>[3] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SQLite#Notable_uses" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SQLite#Notable_uses</a></div><br/></div></div></div></div><div id="41123731" class="c"><input type="checkbox" id="c-41123731" checked=""/><div class="controls bullet"><span class="by">cmollis</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41123363">parent</a><span>|</span><a href="#41125141">prev</a><span>|</span><a href="#41124152">next</a><span>|</span><label class="collapse" for="c-41123731">[-]</label><label class="expand" for="c-41123731">[1 more]</label></div><br/><div class="children"><div class="content">exactly.. parquet is good for append only..  stream mods to parquet in new partitions.. compact, repeat.</div><br/></div></div></div></div></div></div></div></div><div id="41124152" class="c"><input type="checkbox" id="c-41124152" checked=""/><div class="controls bullet"><span class="by">whoevercares</span><span>|</span><a href="#41105332">parent</a><span>|</span><a href="#41122387">prev</a><span>|</span><a href="#41125624">next</a><span>|</span><label class="collapse" for="c-41124152">[-]</label><label class="expand" for="c-41124152">[2 more]</label></div><br/><div class="children"><div class="content">Is it really AWS? I don’t recall any service called BDT</div><br/><div id="41125580" class="c"><input type="checkbox" id="c-41125580" checked=""/><div class="controls bullet"><span class="by">p0rkbelly</span><span>|</span><a href="#41105332">root</a><span>|</span><a href="#41124152">parent</a><span>|</span><a href="#41125624">next</a><span>|</span><label class="collapse" for="c-41125580">[-]</label><label class="expand" for="c-41125580">[1 more]</label></div><br/><div class="children"><div class="content">The second paragraph discusses that BDT is an internal team at Amazon Retail. They used AWS and Ray to do this.</div><br/></div></div></div></div><div id="41125624" class="c"><input type="checkbox" id="c-41125624" checked=""/><div class="controls bullet"><span class="by">qwerp</span><span>|</span><a href="#41105332">parent</a><span>|</span><a href="#41124152">prev</a><span>|</span><a href="#41122345">next</a><span>|</span><label class="collapse" for="c-41125624">[-]</label><label class="expand" for="c-41125624">[1 more]</label></div><br/><div class="children"><div class="content">our plans are measured in centuries</div><br/></div></div></div></div><div id="41126345" class="c"><input type="checkbox" id="c-41126345" checked=""/><div class="controls bullet"><span class="by">jameskraus</span><span>|</span><a href="#41105332">prev</a><span>|</span><a href="#41123311">next</a><span>|</span><label class="collapse" for="c-41126345">[-]</label><label class="expand" for="c-41126345">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if there are any good primers to these technologies. Maybe a DDIA-like book or some lectures?</div><br/></div></div><div id="41123311" class="c"><input type="checkbox" id="c-41123311" checked=""/><div class="controls bullet"><span class="by">igmor</span><span>|</span><a href="#41126345">prev</a><span>|</span><a href="#41123983">next</a><span>|</span><label class="collapse" for="c-41123311">[-]</label><label class="expand" for="c-41123311">[3 more]</label></div><br/><div class="children"><div class="content">Can you share any data on how big of a cluster is running Ray jobs?</div><br/><div id="41123634" class="c"><input type="checkbox" id="c-41123634" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41123311">parent</a><span>|</span><a href="#41123983">next</a><span>|</span><label class="collapse" for="c-41123634">[-]</label><label class="expand" for="c-41123634">[2 more]</label></div><br/><div class="children"><div class="content">From the blog post, the largest individual Ray cluster that was observed running a production compaction job in Q1 had 26,846 vCPUs and ~210TiB of RAM. This is roughly equivalent to a Ray cluster composed of 839 r5.8xlarge EC2 nodes (w&#x2F; 32 vCPUs and 256GiB RAM per node).</div><br/><div id="41124811" class="c"><input type="checkbox" id="c-41124811" checked=""/><div class="controls bullet"><span class="by">layoric</span><span>|</span><a href="#41123311">root</a><span>|</span><a href="#41123634">parent</a><span>|</span><a href="#41123983">next</a><span>|</span><label class="collapse" for="c-41124811">[-]</label><label class="expand" for="c-41124811">[1 more]</label></div><br/><div class="children"><div class="content">For those interested, this would be at a cost of:<p>- ~$1691&#x2F;hour on demand<p>- ~$1065&#x2F;hour reserved<p>- ~$521&#x2F;hour spot<p>Not including any related data transfer costs.</div><br/></div></div></div></div></div></div><div id="41123983" class="c"><input type="checkbox" id="c-41123983" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41123311">prev</a><span>|</span><a href="#41105201">next</a><span>|</span><label class="collapse" for="c-41123983">[-]</label><label class="expand" for="c-41123983">[2 more]</label></div><br/><div class="children"><div class="content">Can you help us understand how others can use and derive value from Ray DeltaCAT? What would be the specific use cases?</div><br/><div id="41124392" class="c"><input type="checkbox" id="c-41124392" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41123983">parent</a><span>|</span><a href="#41105201">next</a><span>|</span><label class="collapse" for="c-41124392">[-]</label><label class="expand" for="c-41124392">[1 more]</label></div><br/><div class="children"><div class="content">Some of DeltaCAT&#x27;s goals and use cases have been discussed in this 2022 talk: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;M3pZDp1zock?t=4676" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;M3pZDp1zock?t=4676</a><p>Today, our immediate next goal for DeltaCAT is to ensure that the compactor, and similar procedures for Ray, can run on Apache Iceberg. So, if you&#x27;re an Iceberg user relying on procedures like Spark&#x27;s &quot;rewrite_data_files&quot; and&#x2F;or &quot;rewrite_positional_delete_files&quot; to compact your datasets today, then DeltaCAT will let you easily run similar compaction procedures on Ray to realize similar efficiency&#x2F;scale improvements (even if it winds up delegating some of the work to other projects like PyIceberg, Daft, etc. along the way).<p>Going forward, we&#x27;d like DeltaCAT to also provide better general-purpose abstractions (e.g., reading&#x2F;writing&#x2F;altering large datasets) to simplify writing Ray apps in Python that work across (1) different catalog formats like Iceberg, Hudi, and Delta and (2) different distributed data processing frameworks like Ray Data, Daft, Modin, etc.<p>From the perspective of an internal DeltaCAT developer, another goal is to just reduce the maintainability burden and dev hours required to write something like a compactor that works across multiple catalogs (i.e., by ensuring that all interfaces used by such a procedure can be readily implemented for multiple catalog formats like Iceberg, Hudi, Delta, etc.).</div><br/></div></div></div></div><div id="41105201" class="c"><input type="checkbox" id="c-41105201" checked=""/><div class="controls bullet"><span class="by">Narhem</span><span>|</span><a href="#41123983">prev</a><span>|</span><a href="#41122621">next</a><span>|</span><label class="collapse" for="c-41105201">[-]</label><label class="expand" for="c-41105201">[2 more]</label></div><br/><div class="children"><div class="content">Absolutely insane work. So much data you’d think they would come up with a custom solution instead of using the “newest available toolkit” but I understand how much of a mess dealing with that much data is.</div><br/><div id="41105596" class="c"><input type="checkbox" id="c-41105596" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41105201">parent</a><span>|</span><a href="#41122621">next</a><span>|</span><label class="collapse" for="c-41105596">[-]</label><label class="expand" for="c-41105596">[1 more]</label></div><br/><div class="children"><div class="content">Hi Narhem - one of the devs that worked on the migration here. The data volume, and subsequent compute power required to process it, is actually one of the things that led us to Ray (or Ray Core specifically) since it had the distributed computing primitives (tasks and actors) that we needed to build out our envisioned solution with very few compromises. One thing we DIDN&#x27;T want to do was just throw another one-liner SQL statement running on a new data processing framework at the problem, since that leads us back to the problems we had with Spark - not enough low-level control for such an important problem.<p>In short, after evaluating our options, Ray seemed to strike the best balance between the one efficiency extreme of, say, building out custom &quot;compaction-optimized&quot; hardware&#x2F;clusters, and the other maintainability extreme of just letting the latest managed cloud service run a 1-liner SQL statement for us without ever looking under the hood.<p>Regardless, I expect both our existing solution and the distributed compute frameworks leveraged to deliver it to continue to evolve over time.</div><br/></div></div></div></div><div id="41122621" class="c"><input type="checkbox" id="c-41122621" checked=""/><div class="controls bullet"><span class="by">jiripospisil</span><span>|</span><a href="#41105201">prev</a><span>|</span><a href="#41122996">next</a><span>|</span><label class="collapse" for="c-41122621">[-]</label><label class="expand" for="c-41122621">[5 more]</label></div><br/><div class="children"><div class="content">&gt; From the typical Amazon EC2 customer’s perspective, this translates to saving over $120MM&#x2F;year on Amazon EC2 on-demand R5 instance charges.<p>Does the sales team know about this? &#x2F;jk</div><br/><div id="41123034" class="c"><input type="checkbox" id="c-41123034" checked=""/><div class="controls bullet"><span class="by">andrewxdiamond</span><span>|</span><a href="#41122621">parent</a><span>|</span><a href="#41122651">next</a><span>|</span><label class="collapse" for="c-41123034">[-]</label><label class="expand" for="c-41123034">[2 more]</label></div><br/><div class="children"><div class="content">Amazon sales and business teams are constantly focused on reducing costs for customers and they celebrate this internally too.<p>I have seen dozens of big ticket “we saved this customer $xxxK&#x2F;year” posts on slack and other internal venues, the customer obsession is real.</div><br/><div id="41123575" class="c"><input type="checkbox" id="c-41123575" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41122621">root</a><span>|</span><a href="#41123034">parent</a><span>|</span><a href="#41122651">next</a><span>|</span><label class="collapse" for="c-41123575">[-]</label><label class="expand" for="c-41123575">[1 more]</label></div><br/><div class="children"><div class="content">Even the reduced cost is still ten times what it costs on a traditional provider, though, right?</div><br/></div></div></div></div><div id="41122651" class="c"><input type="checkbox" id="c-41122651" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#41122621">parent</a><span>|</span><a href="#41123034">prev</a><span>|</span><a href="#41122835">next</a><span>|</span><label class="collapse" for="c-41122651">[-]</label><label class="expand" for="c-41122651">[1 more]</label></div><br/><div class="children"><div class="content">This assumes they aren&#x27;t reinvesting the money back to other tangible improvements in the services, though?</div><br/></div></div></div></div><div id="41122996" class="c"><input type="checkbox" id="c-41122996" checked=""/><div class="controls bullet"><span class="by">100pctremote</span><span>|</span><a href="#41122621">prev</a><span>|</span><a href="#41109306">next</a><span>|</span><label class="collapse" for="c-41122996">[-]</label><label class="expand" for="c-41122996">[1 more]</label></div><br/><div class="children"><div class="content">Rather nuts. New challenge: build datacenters quickly enough to support the new platform.</div><br/></div></div><div id="41109306" class="c"><input type="checkbox" id="c-41109306" checked=""/><div class="controls bullet"><span class="by">whoiscroberts</span><span>|</span><a href="#41122996">prev</a><span>|</span><a href="#41116058">next</a><span>|</span><label class="collapse" for="c-41109306">[-]</label><label class="expand" for="c-41109306">[2 more]</label></div><br/><div class="children"><div class="content">Ray user here, what language actors are they using? Ray support Python Java and cpp actors…</div><br/><div id="41110592" class="c"><input type="checkbox" id="c-41110592" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41109306">parent</a><span>|</span><a href="#41116058">next</a><span>|</span><label class="collapse" for="c-41110592">[-]</label><label class="expand" for="c-41110592">[1 more]</label></div><br/><div class="children"><div class="content">We wrote the compactor in Python but, as noted in my previous response to quadrature, most of the performance sensitive code is written in C++ and Rust (but still invoked from Python).</div><br/></div></div></div></div><div id="41116058" class="c"><input type="checkbox" id="c-41116058" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#41109306">prev</a><span>|</span><a href="#41126612">next</a><span>|</span><label class="collapse" for="c-41116058">[-]</label><label class="expand" for="c-41116058">[5 more]</label></div><br/><div class="children"><div class="content">Are we talking about big data ETL here? I did not know Ray was suited for it.</div><br/><div id="41120504" class="c"><input type="checkbox" id="c-41120504" checked=""/><div class="controls bullet"><span class="by">thedood</span><span>|</span><a href="#41116058">parent</a><span>|</span><a href="#41126612">next</a><span>|</span><label class="collapse" for="c-41120504">[-]</label><label class="expand" for="c-41120504">[4 more]</label></div><br/><div class="children"><div class="content">This is a specialized ETL use-case - similar to taking a single SQL query and creating a dedicated distributed application tailored to run only that query. The lower-level primitives in Ray Core (tasks and actors) are general purpose enough to make building this type of application possible, but you&#x27;ll be hard pressed to quickly (i.e., with less than 1 day of effort) make any arbitrary SQL query or dataframe operation run with better efficiency or scale on Ray than on dedicated data processing frameworks like Spark. IMO, the main value add of frameworks like Spark lies more in unlocking &quot;good enough&quot; efficiency and scale for almost any ETL job relatively quickly and easily, even if it may not run your ETL job optimally.</div><br/><div id="41124259" class="c"><input type="checkbox" id="c-41124259" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41116058">root</a><span>|</span><a href="#41120504">parent</a><span>|</span><a href="#41126612">next</a><span>|</span><label class="collapse" for="c-41124259">[-]</label><label class="expand" for="c-41124259">[3 more]</label></div><br/><div class="children"><div class="content">Speaking as a distributed computing nerd, Ray is definitely one of the more interesting and exciting frameworks I&#x27;ve seen in a while.  It&#x27;s one of those systems where reading the manual, I can see that I&#x27;m not going to have to learn anything new, because the mental model resembles so many distributed systems I&#x27;ve worked with before (I dunno about anybody else, but tensorflow is an example of a distributed system that forced me to forget basically everything I knew before I could be even remotely productive in it).<p>Unclear if it&#x27;s in the best interests of anyscale to promote Ray as a general purpose cluster productivity tool, even if it&#x27;s good at that more general use case.</div><br/><div id="41124520" class="c"><input type="checkbox" id="c-41124520" checked=""/><div class="controls bullet"><span class="by">robertnishihara</span><span>|</span><a href="#41116058">root</a><span>|</span><a href="#41124259">parent</a><span>|</span><a href="#41126612">next</a><span>|</span><label class="collapse" for="c-41124520">[-]</label><label class="expand" for="c-41124520">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad you find it exciting!<p>Our intention from the start was for Ray to be general purpose. And the core Ray APIs are quite general (basically just scheduling a Python function somewhere in a cluster or instantiating a Python class as a process somewhere in the cluster).<p>We had AI use cases in mind from the start, since we were grad students in AI. But the generality has really been important since AI workloads encompass a huge variety of computational patterns (allreduce style communication patterns on GPUs for training, embarrassingly parallel data processing workloads on spot instances, and so on).</div><br/><div id="41124656" class="c"><input type="checkbox" id="c-41124656" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41116058">root</a><span>|</span><a href="#41124520">parent</a><span>|</span><a href="#41126612">next</a><span>|</span><label class="collapse" for="c-41124656">[-]</label><label class="expand" for="c-41124656">[1 more]</label></div><br/><div class="children"><div class="content">Oh, I know all that, I used to work at Google and give lots of money to the various groups associated with Ion Stoica&#x27;s groups at Berkeley to help stimulate more open source alternatives to Borg&#x2F;MapReduce&#x2F;Flume&#x2F;TensorFlow.  Keep up the good work.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41126612" class="c"><input type="checkbox" id="c-41126612" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#41116058">prev</a><span>|</span><a href="#41124223">next</a><span>|</span><label class="collapse" for="c-41126612">[-]</label><label class="expand" for="c-41126612">[1 more]</label></div><br/><div class="children"><div class="content">Are they that bored over there at amazon.</div><br/></div></div><div id="41124223" class="c"><input type="checkbox" id="c-41124223" checked=""/><div class="controls bullet"><span class="by">whoevercares</span><span>|</span><a href="#41126612">prev</a><span>|</span><a href="#41124014">next</a><span>|</span><label class="collapse" for="c-41124223">[-]</label><label class="expand" for="c-41124223">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if similar performance can be achieved with Spark accelerator like <a href="https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;datafusion-comet">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;datafusion-comet</a>. Of course it didn’t exist 4 years ago, but would it cheaper to build?</div><br/><div id="41125912" class="c"><input type="checkbox" id="c-41125912" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#41124223">parent</a><span>|</span><a href="#41124014">next</a><span>|</span><label class="collapse" for="c-41125912">[-]</label><label class="expand" for="c-41125912">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like a lot of the value here was in a more appropriate level of abstraction, not in the speed of the compute itself.<p>Ray allowed them to optimize elements that spark didn’t, and that was what improved performance, not that spark itself was slow.</div><br/></div></div></div></div><div id="41124014" class="c"><input type="checkbox" id="c-41124014" checked=""/><div class="controls bullet"><span class="by">jgalt212</span><span>|</span><a href="#41124223">prev</a><span>|</span><label class="collapse" for="c-41124014">[-]</label><label class="expand" for="c-41124014">[2 more]</label></div><br/><div class="children"><div class="content">Slightly flip, but it&#x27;s interesting that no one believes in or brags about cost savings via statistical sampling techniques these days.</div><br/><div id="41124231" class="c"><input type="checkbox" id="c-41124231" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41124014">parent</a><span>|</span><label class="collapse" for="c-41124231">[-]</label><label class="expand" for="c-41124231">[1 more]</label></div><br/><div class="children"><div class="content">well, I can save money by eating only lentils, but I prefer a richer diet.  As do BI folks in a highly profitable company.</div><br/></div></div></div></div></div></div></div></div></div></body></html>