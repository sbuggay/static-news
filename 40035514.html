<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713171651646" as="style"/><link rel="stylesheet" href="styles.css?v=1713171651646"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.3blue1brown.com/lessons/attention">Visualizing Attention, a Transformer&#x27;s Heart [video]</a>Â <span class="domain">(<a href="https://www.3blue1brown.com">www.3blue1brown.com</a>)</span></div><div class="subtext"><span>rohitpaulk</span> | <span>69 comments</span></div><br/><div><div id="40037174" class="c"><input type="checkbox" id="c-40037174" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#40037545">next</a><span>|</span><label class="collapse" for="c-40037174">[-]</label><label class="expand" for="c-40037174">[21 more]</label></div><br/><div class="children"><div class="content">I have found the youtube videos by CodeEmporium to be simpler to follow <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Nw_PJdmydZY" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Nw_PJdmydZY</a><p>Transformer is hard to describe with analogies, and TBF there is no good explanation why it works, so it may be better to just present the mechanism, &quot;leaving the interpretation to the viewer&quot;. Also, it&#x27;s simpler to describe dot products as vectors projecting on one another</div><br/><div id="40037770" class="c"><input type="checkbox" id="c-40037770" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40037174">parent</a><span>|</span><a href="#40037765">next</a><span>|</span><label class="collapse" for="c-40037770">[-]</label><label class="expand" for="c-40037770">[17 more]</label></div><br/><div class="children"><div class="content">The explanation is just that NNs are a stat fitting alg learning a conditional probability distribution, P(next_word|previous_words). Their weights are a model of this distribution. LLMs are a hardware innovation: they make it possible for GPUs to compute this at scale across TBs of data.<p>Why does, &#x27;mat&#x27; follow from &#x27;the cat sat on the ...&#x27; because &#x27;mat&#x27; is the most frequent word in the dataset; and the NN is a model of those frequencies.<p>Why is &#x27;London in UK&#x27; &quot;known&quot; but &#x27;London in France&#x27; isnt? Just because &#x27;UK&#x27; much more frequently occurs in the dataset.<p>The algorithm isnt doing anything other than aligning computation to hardware; the computation isnt doing anything interesting. The value comes from the conditional probability structure in the data. -- that comes from people arranging words usefully, because they&#x27;re communicating information with one another</div><br/><div id="40038237" class="c"><input type="checkbox" id="c-40038237" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037770">parent</a><span>|</span><a href="#40038204">next</a><span>|</span><label class="collapse" for="c-40038237">[-]</label><label class="expand" for="c-40038237">[1 more]</label></div><br/><div class="children"><div class="content">This is wrong, or at least a simplification to the point of removing any value.<p>&gt;  NNs are a stat fitting alg learning a conditional probability distribution, P(next_word|previous_words).<p>They are trained to maximise this, yes.<p>&gt;  Their weights are a model of this distribution.<p>That doesn&#x27;t really follow, but let&#x27;s leave that.<p>&gt; Why does, &#x27;mat&#x27; follow from &#x27;the cat sat on the ...&#x27; because &#x27;mat&#x27; is the most frequent word in the dataset; and the NN is a model of those frequencies.<p>Here&#x27;s the rub. If how you describe them is all they&#x27;re doing then a sequence of never-before-seen words would have <i>no</i> valid response. All words would be equally likely. It would mean that a single brand new word would result in absolute gibberish following it as there&#x27;s nothing to go on.<p>Let&#x27;s try:<p>Input: I have one kjsdhlisrnj and I add another kjsdhlisrnj, tell me how many kjsdhlisrnj I now have.<p>Result: You now have two kjsdhlisrnj.<p>I would wager a solid amount that kjsdhlisrnj never appears in the input data. If it does pick another one, it doesn&#x27;t matter.<p>So we are learning something <i>more general</i> than the frequencies of sequences of tokens.<p>I always end up pointing to this but OthelloGPT is very interesting <a href="https:&#x2F;&#x2F;thegradient.pub&#x2F;othello&#x2F;" rel="nofollow">https:&#x2F;&#x2F;thegradient.pub&#x2F;othello&#x2F;</a><p>While it&#x27;s <i>trained</i> on sequences of moves, what it <i>does</i> is more than just &quot;sequence a,b,c is followed by d most often&quot;</div><br/></div></div><div id="40038204" class="c"><input type="checkbox" id="c-40038204" checked=""/><div class="controls bullet"><span class="by">michaelt</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037770">parent</a><span>|</span><a href="#40038237">prev</a><span>|</span><a href="#40037806">next</a><span>|</span><label class="collapse" for="c-40038204">[-]</label><label class="expand" for="c-40038204">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not really an explanation that tells people all that much, though.<p>I can explain that car engines &#x27;just&#x27; convert gasoline into forward motion. But if a the person hearing the explanation is hoping to learn what a cam belt or a gearbox is, or why cars are more reliable now than they were in the 1970s, or what premium gas is for, or whether helicopter engines work on the same principle - they&#x27;re going to need a more detailed explanation.</div><br/><div id="40038235" class="c"><input type="checkbox" id="c-40038235" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40038204">parent</a><span>|</span><a href="#40037806">next</a><span>|</span><label class="collapse" for="c-40038235">[-]</label><label class="expand" for="c-40038235">[1 more]</label></div><br/><div class="children"><div class="content">It explains the LLM&#x2F;NN. If you want to explain why it emits words in a certain order you need to explain how reality generated the dataset, ie., you need to explain how people communicate (and so on).<p>There is no mystery why an NN trained on the night sky would generate nightsky-like photos; the mystery is why those photos have those patterns... solving that is called astrophysics.<p>Why do people, in reasoning through physics problems, write symbols in a certain order? Well, explain physics, reasoning, mathematical notation, and so on. The ordering of the symbols gives rise to a certain utility of immitating that order -- but it isnt explained by that order. That&#x27;s circular: &quot;LLMs generate text in the order they do, because that&#x27;s the order of the text they were given&quot;</div><br/></div></div></div></div><div id="40037806" class="c"><input type="checkbox" id="c-40037806" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037770">parent</a><span>|</span><a href="#40038204">prev</a><span>|</span><a href="#40037890">next</a><span>|</span><label class="collapse" for="c-40037806">[-]</label><label class="expand" for="c-40037806">[3 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re downplaying the importance of the attention&#x2F;transformer architecture here. If it was &quot;just&quot; a matter of throwing compute at probabilities, then we wouldn&#x27;t need any special architecture at all.<p>P(next_word|previous_words) is ridiculously hard to estimate in a way that is actually useful. Remember how bad text generation used to be before GPT? There is innovation in discovering an architecture that makes it possible to learn P(next_word|previous_words), in addition to the computing techniques and hardware improvements required to make it work.</div><br/><div id="40037999" class="c"><input type="checkbox" id="c-40037999" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037806">parent</a><span>|</span><a href="#40037890">next</a><span>|</span><label class="collapse" for="c-40037999">[-]</label><label class="expand" for="c-40037999">[2 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s really hard -- the innovation is aligning the really basic dot-product similarity mechanism to hardware. You can use basically any NN structure to do the same task, the issue is that they&#x27;re untrainable because they arent parallizable.<p>There is no innovation here in the sense of a brand new algorithm for modelling conditional probabilities -- the innovation is in adapting the algorithm for GPU training on text&#x2F;etc.</div><br/><div id="40038167" class="c"><input type="checkbox" id="c-40038167" checked=""/><div class="controls bullet"><span class="by">bruce343434</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037999">parent</a><span>|</span><a href="#40037890">next</a><span>|</span><label class="collapse" for="c-40038167">[-]</label><label class="expand" for="c-40038167">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why you seem to have such a bone to pick with transformers but imo it&#x27;s still interesting to learn about it, and reading your dismissively toned drivel of &quot;just&quot; and &quot;simply&quot; makes me tired. You&#x27;re barking up the wrong tree man, what are you on about.</div><br/></div></div></div></div></div></div><div id="40037890" class="c"><input type="checkbox" id="c-40037890" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037770">parent</a><span>|</span><a href="#40037806">prev</a><span>|</span><a href="#40037852">next</a><span>|</span><label class="collapse" for="c-40037890">[-]</label><label class="expand" for="c-40037890">[5 more]</label></div><br/><div class="children"><div class="content">You are more speaking about n-gram models here. NNs do far more than that.<p>Or if you just want to say that NNs are used as a statistical model here: Well, yea, but that doesn&#x27;t really tell you anything. Everything can be a statistical model.<p>E.g., you could also say &quot;this is exactly the way the human brain works&quot;, but it doesn&#x27;t really tell you anything how it really works.</div><br/><div id="40038238" class="c"><input type="checkbox" id="c-40038238" checked=""/><div class="controls bullet"><span class="by">cornholio</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037890">parent</a><span>|</span><a href="#40037989">next</a><span>|</span><label class="collapse" for="c-40038238">[-]</label><label class="expand" for="c-40038238">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;this is exactly the way the human brain works&quot;<p>I&#x27;m always puzzled by such assertions. A cursory look at the technical aspects of an iterated attention - perceptron transformation clearly shows it&#x27;s just a convoluted and powerful way to query the training data, a &quot;fancy&quot; Markov chain. The only rationality it can exhibit is that which is already embedded in the dataset. If trained on nonsensical data it would generate nonsense and if trained with a partially non-sensical dataset it will generate an average between truth and nonsense that maximizes some abstract algorithmic goal.<p>There is no knowledge generation going on, no rational examination of the dataset through the lens of an internal model of reality that allows the rejection of invalid premises. The intellectual food already chewed and digested in the form of the training weights, with the model just mechanically extracting the nutrients, as opposed to venturing in the outside world to hunt.<p>So if it works &quot;just like the human brain&quot;, it does so in a very remote sense, just like a basic neural net works &quot;just like the human brain&quot;, i.e individual neurons.</div><br/></div></div><div id="40037989" class="c"><input type="checkbox" id="c-40037989" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037890">parent</a><span>|</span><a href="#40038238">prev</a><span>|</span><a href="#40037852">next</a><span>|</span><label class="collapse" for="c-40037989">[-]</label><label class="expand" for="c-40037989">[3 more]</label></div><br/><div class="children"><div class="content">My description is true of any statistical learning algorithm.<p>The thing that people are looking to for answers, the NN itself, does not have them. That&#x27;s like looking to Newton&#x27;s compass to understand his general law of gravitation.<p>The reason that LLMs trained on the internet and every ebook has the structure of human communication is because the dataset has that structure. Why does the data have that structure? this requires science, there is no explanation &quot;in the compass&quot;.<p>NNs are statistical models trained on data -- drawing analogies to animals is a mystification that causes people&#x27;s ability to think clearly he to jump out the window. No one compares stock price models to the human brain; no banking regulator says, &quot;well your volatility estimates were off because your machines had the wrong thoughts&quot;. This is pseudoscience.<p>Animals are not statistical learning algorithms, so the reason that&#x27;s uninformative is because it&#x27;s false. Animals are in direct causal contact with the world and uncover its structure through interventional action and counterfactual reasoning. The structure of animal bodies, and the general learning strategies are well-known, and having nothing to do with LLMs&#x2F;NNs.<p>The reason that I know &quot;The cup is in my hand&quot; is not because P(&quot;The cup is in my hand&quot;|HistoricalTexts) &gt; P(not &quot;The cup is in my hand&quot;|HistoricalTexts)</div><br/><div id="40038178" class="c"><input type="checkbox" id="c-40038178" checked=""/><div class="controls bullet"><span class="by">Demlolomot</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037989">parent</a><span>|</span><a href="#40037852">next</a><span>|</span><label class="collapse" for="c-40038178">[-]</label><label class="expand" for="c-40038178">[2 more]</label></div><br/><div class="children"><div class="content">If learning in real life over 5-20 years shows the same result as a LLM being trained by billions of tokens, than yes it can be compared.<p>And there are a lot of people out there who do not a lot of reasoning.<p>After all optical illusions exist, our brain generalizes.<p>The same thing happens with words like the riddle about the doctor operating on a child were we discover that the doctor is actually a female.<p>And while llms only use text, we can already see how multimodal models become better, architecture gets better and hardware too.</div><br/><div id="40038205" class="c"><input type="checkbox" id="c-40038205" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40038178">parent</a><span>|</span><a href="#40037852">next</a><span>|</span><label class="collapse" for="c-40038205">[-]</label><label class="expand" for="c-40038205">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what your motivation in comparison is; mine is science, ie., explanation.<p>I&#x27;m not interested that your best friend emits the same words in the same order as an LLM; i&#x27;m more interested that he does so because he enjoys you company whereas the LLM does not.<p>Engineer&#x27;s overstep their mission when they assume that because you can substitute one thing for another, and sell a product in doing so, that this is informative. It isnt. I&#x27;m not interested in whether you can replace the sky for a skybox and have no one notice -- who cares? What might fool an ape is <i>everything</i>, and what that matters for science is <i>nothing</i>.</div><br/></div></div></div></div></div></div></div></div><div id="40037852" class="c"><input type="checkbox" id="c-40037852" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037770">parent</a><span>|</span><a href="#40037890">prev</a><span>|</span><a href="#40037887">next</a><span>|</span><label class="collapse" for="c-40037852">[-]</label><label class="expand" for="c-40037852">[2 more]</label></div><br/><div class="children"><div class="content">People specifically would like to know what the attention calculations add to this learning of the distribution</div><br/><div id="40037935" class="c"><input type="checkbox" id="c-40037935" checked=""/><div class="controls bullet"><span class="by">ffwd</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037852">parent</a><span>|</span><a href="#40037887">next</a><span>|</span><label class="collapse" for="c-40037935">[-]</label><label class="expand" for="c-40037935">[1 more]</label></div><br/><div class="children"><div class="content">Just speculating but I think attention enables differentiation of semantic concepts for a word or sentence within a particular context. Like for any total set of training data you have a lesser number of semantic concepts (like let&#x27;s say you have 10000 words, then it might contain 2000 semantic concepts, and those concepts are defined by the sentence structure and surrounding words, which is why they have a particular meaning), and then attention allows to differentiate those different contexts at different levels (words&#x2F;etc).
Also the fact you can do this attention at runtime&#x2F;inference means you can generate the context from the prompt, which enables the flexibility of variable prompt&#x2F;variable output but you lose the precision of giving an exact prompt and getting an exact answer</div><br/></div></div></div></div><div id="40037887" class="c"><input type="checkbox" id="c-40037887" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037770">parent</a><span>|</span><a href="#40037852">prev</a><span>|</span><a href="#40037855">next</a><span>|</span><label class="collapse" for="c-40037887">[-]</label><label class="expand" for="c-40037887">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why does, &#x27;mat&#x27; follow from &#x27;the cat sat on the ...&#x27; because &#x27;mat&#x27; is the most frequent word in the dataset; and the NN is a model of those frequencies.<p>What about cases that are not present in the dataset?<p>The model must be doing <i>something</i> besides storing raw probabilities to avoid overfitting and enable generalization (imagine that you could have a very performant model - when it works - but it sometimes would spew &quot;Invalid input, this was not in the dataset so I don&#x27;t have a conditional probability and I will bail out&quot;)</div><br/></div></div><div id="40037855" class="c"><input type="checkbox" id="c-40037855" checked=""/><div class="controls bullet"><span class="by">forrestthewoods</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037770">parent</a><span>|</span><a href="#40037887">prev</a><span>|</span><a href="#40037765">next</a><span>|</span><label class="collapse" for="c-40037855">[-]</label><label class="expand" for="c-40037855">[2 more]</label></div><br/><div class="children"><div class="content">I find this take super weak sauce and shallow.<p>This recent $10,000 challenge is super super interesting imho. <a href="https:&#x2F;&#x2F;twitter.com&#x2F;VictorTaelin&#x2F;status&#x2F;1778100581837480178" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;VictorTaelin&#x2F;status&#x2F;1778100581837480178</a><p>State of the art models are doing more than âjustâ predicting the probability of the next symbol.</div><br/><div id="40038017" class="c"><input type="checkbox" id="c-40038017" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037855">parent</a><span>|</span><a href="#40037765">next</a><span>|</span><label class="collapse" for="c-40038017">[-]</label><label class="expand" for="c-40038017">[1 more]</label></div><br/><div class="children"><div class="content">You underestimate the properties of the sequential-conditional structure of human communication.<p>Consider how a clever 6yo could fake being a physicist with access to a library of physics textbooks and a shredder. All the work is done for them. You&#x27;d need to be a physicist to spot them faking it.<p>Of course, LLMs are in a much better position than having shredded physics textbooks -- they have shreddings of all books. So you actually have to try to expose this process, rather than just gullibly prompt using confirmation bias. It&#x27;s trivial to show they work this way, both formally and practically.<p>The issue is, practically, gullible people aren&#x27;t trying.</div><br/></div></div></div></div></div></div><div id="40037765" class="c"><input type="checkbox" id="c-40037765" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#40037174">parent</a><span>|</span><a href="#40037770">prev</a><span>|</span><a href="#40037545">next</a><span>|</span><label class="collapse" for="c-40037765">[-]</label><label class="expand" for="c-40037765">[3 more]</label></div><br/><div class="children"><div class="content">&gt; TBF there is no good explanation why it works<p>My mental justification for attention has always been that the output of the transformer is a sequence of new token vectors such that each individual output token vector incorporates contextual information from the surrounding input token vectors. I know it&#x27;s incomplete, but it&#x27;s better than nothing at all.</div><br/><div id="40038174" class="c"><input type="checkbox" id="c-40038174" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037765">parent</a><span>|</span><a href="#40038054">next</a><span>|</span><label class="collapse" for="c-40038174">[-]</label><label class="expand" for="c-40038174">[1 more]</label></div><br/><div class="children"><div class="content">&gt; TBF there is no good explanation why it works<p>I thought the general consesus was: &quot;transformers allow neural networks to have dynamic weights&quot;.<p>As opposed to the previous architectures, were every edge connecting two neurons always has the same weight.</div><br/></div></div><div id="40038054" class="c"><input type="checkbox" id="c-40038054" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#40037174">root</a><span>|</span><a href="#40037765">parent</a><span>|</span><a href="#40038174">prev</a><span>|</span><a href="#40037545">next</a><span>|</span><label class="collapse" for="c-40038054">[-]</label><label class="expand" for="c-40038054">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re effectively steering the predictions based on adjacent vectors (and precursors from the prompt). That mental model works fine.</div><br/></div></div></div></div></div></div><div id="40037545" class="c"><input type="checkbox" id="c-40037545" checked=""/><div class="controls bullet"><span class="by">rayval</span><span>|</span><a href="#40037174">prev</a><span>|</span><a href="#40037081">next</a><span>|</span><label class="collapse" for="c-40037545">[-]</label><label class="expand" for="c-40037545">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a compelling visualization of the functioning  of an LLM when processing a simple request: <a href="https:&#x2F;&#x2F;bbycroft.net&#x2F;llm" rel="nofollow">https:&#x2F;&#x2F;bbycroft.net&#x2F;llm</a><p>This complements the detailed description provided by 3blue1brown</div><br/></div></div><div id="40037081" class="c"><input type="checkbox" id="c-40037081" checked=""/><div class="controls bullet"><span class="by">tylerneylon</span><span>|</span><a href="#40037545">prev</a><span>|</span><a href="#40035945">next</a><span>|</span><label class="collapse" for="c-40037081">[-]</label><label class="expand" for="c-40037081">[3 more]</label></div><br/><div class="children"><div class="content">Awesome video. This helps to show how the Q*K matrix multiplication is a bottleneck, because if you have sequence (context window) length S, then you need to store an SxS size matrix (the result of all queries times all keys) in memory.<p>One great way to improve on this bottleneck is a new-ish idea called Ring Attention. This is a good article explaining it:<p><a href="https:&#x2F;&#x2F;learnandburn.ai&#x2F;p&#x2F;how-to-build-a-10m-token-context" rel="nofollow">https:&#x2F;&#x2F;learnandburn.ai&#x2F;p&#x2F;how-to-build-a-10m-token-context</a><p>(I edited that article.)</div><br/><div id="40037419" class="c"><input type="checkbox" id="c-40037419" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#40037081">parent</a><span>|</span><a href="#40037275">next</a><span>|</span><label class="collapse" for="c-40037419">[-]</label><label class="expand" for="c-40037419">[1 more]</label></div><br/><div class="children"><div class="content">Oh with Flash Attention, you never have to construct the (S, S) matrix ever (also in article) Since its softmax(Q @ K^T &#x2F; sqrt(d)) @ V, you can form the final output in tiles.<p>In Unsloth, memory usage scales linearly (not quadratically) due to Flash Attention (+ you get 2x faster finetuning, 80% less VRAM use + 2x faster inference). Still O(N^2) FLOPs though.<p>On that note, on long contexts, Unsloth&#x27;s latest release fits 4x longer contexts than HF+FA2 with +1.9% overhead. So 228K context on H100.</div><br/></div></div><div id="40037275" class="c"><input type="checkbox" id="c-40037275" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#40037081">parent</a><span>|</span><a href="#40037419">prev</a><span>|</span><a href="#40035945">next</a><span>|</span><label class="collapse" for="c-40037275">[-]</label><label class="expand" for="c-40037275">[1 more]</label></div><br/><div class="children"><div class="content">He lists Ring Attention and half a dozen other techniques, but they&#x27;re not within the scope of this video: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;eMlx5fFNoYc?t=784" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;eMlx5fFNoYc?t=784</a></div><br/></div></div></div></div><div id="40035945" class="c"><input type="checkbox" id="c-40035945" checked=""/><div class="controls bullet"><span class="by">promiseofbeans</span><span>|</span><a href="#40037081">prev</a><span>|</span><a href="#40037203">next</a><span>|</span><label class="collapse" for="c-40035945">[-]</label><label class="expand" for="c-40035945">[1 more]</label></div><br/><div class="children"><div class="content">His previous post &#x27;But what is a GPT?&#x27; is also really good: <a href="https:&#x2F;&#x2F;www.3blue1brown.com&#x2F;lessons&#x2F;gpt" rel="nofollow">https:&#x2F;&#x2F;www.3blue1brown.com&#x2F;lessons&#x2F;gpt</a></div><br/></div></div><div id="40037203" class="c"><input type="checkbox" id="c-40037203" checked=""/><div class="controls bullet"><span class="by">abotsis</span><span>|</span><a href="#40035945">prev</a><span>|</span><a href="#40036100">next</a><span>|</span><label class="collapse" for="c-40037203">[-]</label><label class="expand" for="c-40037203">[2 more]</label></div><br/><div class="children"><div class="content">I think what made this so digestible for me were the animations. The timing, how they expand&#x2F;contract and unfold while heâs speaking.. is all very well done.</div><br/><div id="40037294" class="c"><input type="checkbox" id="c-40037294" checked=""/><div class="controls bullet"><span class="by">_delirium</span><span>|</span><a href="#40037203">parent</a><span>|</span><a href="#40036100">next</a><span>|</span><label class="collapse" for="c-40037294">[-]</label><label class="expand" for="c-40037294">[1 more]</label></div><br/><div class="children"><div class="content">That is definitely one of the things he does better than most. He actually wrote his own custom animation library for math animations: <a href="https:&#x2F;&#x2F;github.com&#x2F;3b1b&#x2F;manim">https:&#x2F;&#x2F;github.com&#x2F;3b1b&#x2F;manim</a></div><br/></div></div></div></div><div id="40036100" class="c"><input type="checkbox" id="c-40036100" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#40037203">prev</a><span>|</span><a href="#40036825">next</a><span>|</span><label class="collapse" for="c-40036100">[-]</label><label class="expand" for="c-40036100">[14 more]</label></div><br/><div class="children"><div class="content">I finally understand this! Why did every other video make it so confusing!</div><br/><div id="40036212" class="c"><input type="checkbox" id="c-40036212" checked=""/><div class="controls bullet"><span class="by">chrishare</span><span>|</span><a href="#40036100">parent</a><span>|</span><a href="#40037930">next</a><span>|</span><label class="collapse" for="c-40036212">[-]</label><label class="expand" for="c-40036212">[2 more]</label></div><br/><div class="children"><div class="content">It is confusing, 3b1b is just that good.</div><br/><div id="40037441" class="c"><input type="checkbox" id="c-40037441" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40036100">root</a><span>|</span><a href="#40036212">parent</a><span>|</span><a href="#40037930">next</a><span>|</span><label class="collapse" for="c-40037441">[-]</label><label class="expand" for="c-40037441">[1 more]</label></div><br/><div class="children"><div class="content">At the same time it feels extremely simple<p>attention(Q,K,V) = softmax (Q K^T â dK ) @ V<p>is just half a row; the multi-head, masking and positional stuff just toppings<p>we have many basic algorithms in CS that are more involved, it&#x27;s amazing we get language understanding from such simple math</div><br/></div></div></div></div><div id="40037930" class="c"><input type="checkbox" id="c-40037930" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#40036100">parent</a><span>|</span><a href="#40036212">prev</a><span>|</span><a href="#40036630">next</a><span>|</span><label class="collapse" for="c-40037930">[-]</label><label class="expand" for="c-40037930">[2 more]</label></div><br/><div class="children"><div class="content">Not sure if you mean it as rhetorical question but I think it&#x27;s an interesting question. I think there are at least three factors why most people are confused about Transformers:<p>1. The standard terminology is &quot;meh&quot; at most. The word &quot;attention&quot; itself is just barely intuitive, &quot;self-attention&quot; is worse, and don&#x27;t get me started about &quot;key&quot; and &quot;value&quot;.<p>2. The key papers (Attention is All You Need, the BERT paper, etc.) are badly written. This is probably an unpopular opinion. But note that I&#x27;m not diminishing their merits. It&#x27;s perfectly compatible to write a hugely impactful, transformative paper describing an amazing breakthrough, but just don&#x27;t explain it very well. And that&#x27;s exactly what happened, IMO.<p>3. The way in which these architectures were discovered was largely by throwing things at the wall and seeing what sticked. There is no reflection process that ended on a prediction that such an architecture would work well, which was then empirically verified. It&#x27;s empirical all the way through. This means that we don&#x27;t have a full understanding of why it works so well, all explanations are post hoc rationalizations (in fact, lately there is some work implying that other architectures may work equally well if tweaked enough). It&#x27;s hard to explain something that you don&#x27;t even fully understand.<p>Everyone who is trying to explain transformers has to overcome these three disadvantages... so most explanations are confusing.</div><br/><div id="40038033" class="c"><input type="checkbox" id="c-40038033" checked=""/><div class="controls bullet"><span class="by">cmplxconjugate</span><span>|</span><a href="#40036100">root</a><span>|</span><a href="#40037930">parent</a><span>|</span><a href="#40036630">next</a><span>|</span><label class="collapse" for="c-40038033">[-]</label><label class="expand" for="c-40038033">[1 more]</label></div><br/><div class="children"><div class="content">&gt;This is probably an unpopular opinion.<p>I wouldn&#x27;t say so. Historically it&#x27;s quite common. Maxwell&#x27;s EM papers used such convoluted notation it it quite difficult to read. It wasn&#x27;t until they were reformulated in vector calculus that they became infinitely more digestible.<p>I think though your third point is the most important; right now people are focused on results.</div><br/></div></div></div></div><div id="40036630" class="c"><input type="checkbox" id="c-40036630" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#40036100">parent</a><span>|</span><a href="#40037930">prev</a><span>|</span><a href="#40037477">next</a><span>|</span><label class="collapse" for="c-40036630">[-]</label><label class="expand" for="c-40036630">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m someone who would love to get better at making educational videos&#x2F;content. 3b1b is obviously the gold standard here.<p>I&#x27;m curious what things other videos did worse compared to 3b1b?</div><br/><div id="40036713" class="c"><input type="checkbox" id="c-40036713" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#40036100">root</a><span>|</span><a href="#40036630">parent</a><span>|</span><a href="#40037477">next</a><span>|</span><label class="collapse" for="c-40036713">[-]</label><label class="expand" for="c-40036713">[1 more]</label></div><br/><div class="children"><div class="content">I think he had a good, intuitive understanding that he wanted to communicate and he made it come through.<p>I like how he was able to avoid going into the weeds and stay focused on leading you to understanding. I remember another video where I got really hung up on positional encoding and I felt like I could t continue until I understood that. Or other videos that overfocus on matrix operations or softmax, etc.</div><br/></div></div></div></div><div id="40037072" class="c"><input type="checkbox" id="c-40037072" checked=""/><div class="controls bullet"><span class="by">ur-whale</span><span>|</span><a href="#40036100">parent</a><span>|</span><a href="#40037477">prev</a><span>|</span><a href="#40036216">next</a><span>|</span><label class="collapse" for="c-40037072">[-]</label><label class="expand" for="c-40037072">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Why did every other video make it so confusing!<p>In my experience, with very few notable exceptions (e.g. Feynmann), researchers are the worst when it comes to clearly explaining to others what they&#x27;re doing.<p>I&#x27;m at the point where I&#x27;m starting believe that pedagogy and research generally are mutually exclusive skills.</div><br/><div id="40038184" class="c"><input type="checkbox" id="c-40038184" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#40036100">root</a><span>|</span><a href="#40037072">parent</a><span>|</span><a href="#40036216">next</a><span>|</span><label class="collapse" for="c-40038184">[-]</label><label class="expand" for="c-40038184">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s extraordinarily difficult to imagine how it feels not to understand something. Great educators can bridge that gap. I don&#x27;t think it&#x27;s correlated with research ability in any way. It&#x27;s just a very rare skill set, to be able to empathize with people who don&#x27;t understand what you do.</div><br/></div></div></div></div><div id="40036216" class="c"><input type="checkbox" id="c-40036216" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#40036100">parent</a><span>|</span><a href="#40037072">prev</a><span>|</span><a href="#40036825">next</a><span>|</span><label class="collapse" for="c-40036216">[-]</label><label class="expand" for="c-40036216">[4 more]</label></div><br/><div class="children"><div class="content">Because:<p>1. good communication requires an intelligence that most people sadly lack<p>2. because the type of people who are smart enough to invent transformers have zero incentive to make them easily understandable.<p>most documents are written by authors subconsciously desperate to mentally flex on their peers.</div><br/><div id="40036892" class="c"><input type="checkbox" id="c-40036892" checked=""/><div class="controls bullet"><span class="by">penguin_booze</span><span>|</span><a href="#40036100">root</a><span>|</span><a href="#40036216">parent</a><span>|</span><a href="#40038010">next</a><span>|</span><label class="collapse" for="c-40036892">[-]</label><label class="expand" for="c-40036892">[2 more]</label></div><br/><div class="children"><div class="content">Pedagogy requires empathy, to know what it&#x27;s like to not know something. They&#x27;ll often draw on experiences the listener is already familiar with, and then bridge the gap. This skill is orthogonal to the mastery of the subject itself, which I think is the reason most descriptions sound confusing, inadequate, and&#x2F;or incomprehensible.<p>Often, the disseminating medium is a one-sided, like a video or a blog post, which doesn&#x27;t help, either. A conversational interaction would help the expert sense why someone outside the domain find the subject confusing (&quot;ah, I see what you mean&quot;...), discuss common pitfalls (&quot;you might think it&#x27;s like this... but no, it&#x27;s more like this...&quot;) etc.</div><br/></div></div><div id="40038010" class="c"><input type="checkbox" id="c-40038010" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#40036100">root</a><span>|</span><a href="#40036216">parent</a><span>|</span><a href="#40036892">prev</a><span>|</span><a href="#40036825">next</a><span>|</span><label class="collapse" for="c-40038010">[-]</label><label class="expand" for="c-40038010">[1 more]</label></div><br/><div class="children"><div class="content">2. It&#x27;s not malice. The longer you have understood something the harder it is to explain it, since you already forgot what it was like to not understand it.</div><br/></div></div></div></div></div></div><div id="40036825" class="c"><input type="checkbox" id="c-40036825" checked=""/><div class="controls bullet"><span class="by">namelosw</span><span>|</span><a href="#40036100">prev</a><span>|</span><a href="#40037558">next</a><span>|</span><label class="collapse" for="c-40036825">[-]</label><label class="expand" for="c-40036825">[1 more]</label></div><br/><div class="children"><div class="content">You might also want to check out other 3b1b videos on neural networks since there are sort of progressions between each video <a href="https:&#x2F;&#x2F;www.3blue1brown.com&#x2F;topics&#x2F;neural-networks" rel="nofollow">https:&#x2F;&#x2F;www.3blue1brown.com&#x2F;topics&#x2F;neural-networks</a></div><br/></div></div><div id="40037558" class="c"><input type="checkbox" id="c-40037558" checked=""/><div class="controls bullet"><span class="by">justanotherjoe</span><span>|</span><a href="#40036825">prev</a><span>|</span><a href="#40036223">next</a><span>|</span><label class="collapse" for="c-40037558">[-]</label><label class="expand" for="c-40037558">[3 more]</label></div><br/><div class="children"><div class="content">It seems he brushes over the positional encoding, which for me was the most puzzling part of transformers.  The way I understood it, positional encoding is much like dates.  Just like dates, there are repeating minutes, hours, days, months...etc.  Each of these values has shorter &#x27;wavelength&#x27; than the next.  The values are then used to identify the position of each tokens.   Like, &#x27;oh, im seeing january 5th tokens.  I&#x27;m january 4th.  This means this is after me&#x27;.  
Of course the real pos.encoding is much smoother and doesn&#x27;t have abrupt end like dates&#x2F;times, but i think this was the original motivation for positional encodings.</div><br/><div id="40037790" class="c"><input type="checkbox" id="c-40037790" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#40037558">parent</a><span>|</span><a href="#40036223">next</a><span>|</span><label class="collapse" for="c-40037790">[-]</label><label class="expand" for="c-40037790">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s one way to think about it.<p>It&#x27;s clever way to encode &quot;position in sequence&quot; as some kind of smooth signal that can be added to each input vector. You might appreciate this detailed explanation: <a href="https:&#x2F;&#x2F;towardsdatascience.com&#x2F;master-positional-encoding-part-i-63c05d90a0c3" rel="nofollow">https:&#x2F;&#x2F;towardsdatascience.com&#x2F;master-positional-encoding-pa...</a><p>Incidentally, you can encode dates (e.g. day of week) in a model as sin(day of week) and cos(day of week) to ensure that &quot;day 7&quot; is mathematically adjacent to &quot;day 1&quot;.</div><br/></div></div></div></div><div id="40036223" class="c"><input type="checkbox" id="c-40036223" checked=""/><div class="controls bullet"><span class="by">YossarianFrPrez</span><span>|</span><a href="#40037558">prev</a><span>|</span><a href="#40037114">next</a><span>|</span><label class="collapse" for="c-40036223">[-]</label><label class="expand" for="c-40036223">[2 more]</label></div><br/><div class="children"><div class="content">This video (with a slightly different title on YouTube) helped me realize that the attention mechanism isn&#x27;t exactly a specific function so much as it is a meta-function. If I understand it correctly, Attention + learned weights effectively enables a Transformer to learn a semi-arbitrary function, one which involves a matching mechanism (i.e., the scaled dot-product.)</div><br/><div id="40036339" class="c"><input type="checkbox" id="c-40036339" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#40036223">parent</a><span>|</span><a href="#40037114">next</a><span>|</span><label class="collapse" for="c-40036339">[-]</label><label class="expand" for="c-40036339">[1 more]</label></div><br/><div class="children"><div class="content">Indeed. The power of attention is that it searches the space of functions and surfaces the best function given the constraints. This is why I think linear attention will never come close to the ability of standard attention, the quadratic term is a necessary feature of searching over all pairs of inputs and outputs.</div><br/></div></div></div></div><div id="40037114" class="c"><input type="checkbox" id="c-40037114" checked=""/><div class="controls bullet"><span class="by">mehulashah</span><span>|</span><a href="#40036223">prev</a><span>|</span><a href="#40036244">next</a><span>|</span><label class="collapse" for="c-40037114">[-]</label><label class="expand" for="c-40037114">[1 more]</label></div><br/><div class="children"><div class="content">This is one of the best explanations that Iâve seen on the topic. I wish there was more work, however, not on how Transfomers work, but why they work. We are still figuring it out, but I feel that the exploration is not at all systematic.</div><br/></div></div><div id="40036244" class="c"><input type="checkbox" id="c-40036244" checked=""/><div class="controls bullet"><span class="by">mastazi</span><span>|</span><a href="#40037114">prev</a><span>|</span><a href="#40036858">next</a><span>|</span><label class="collapse" for="c-40036244">[-]</label><label class="expand" for="c-40036244">[2 more]</label></div><br/><div class="children"><div class="content">That example with the &quot;was&quot; token at the end of a murder novel is genius (at 3:58 - 4:28 in the video) really easy for a non technical person to understand.</div><br/><div id="40036446" class="c"><input type="checkbox" id="c-40036446" checked=""/><div class="controls bullet"><span class="by">hamburga</span><span>|</span><a href="#40036244">parent</a><span>|</span><a href="#40036858">next</a><span>|</span><label class="collapse" for="c-40036446">[-]</label><label class="expand" for="c-40036446">[1 more]</label></div><br/><div class="children"><div class="content">I think Ilya gets credit for that example â Iâve heard him use it in his interview with Jensen Huang.</div><br/></div></div></div></div><div id="40036858" class="c"><input type="checkbox" id="c-40036858" checked=""/><div class="controls bullet"><span class="by">rollinDyno</span><span>|</span><a href="#40036244">prev</a><span>|</span><a href="#40036302">next</a><span>|</span><label class="collapse" for="c-40036858">[-]</label><label class="expand" for="c-40036858">[5 more]</label></div><br/><div class="children"><div class="content">Hold on, every predicted token is only a function of the previous token? I must have something wrong. This would mean that within the embedding of &quot;was&quot;, which is of length 12,228 in this example. Is it really possible that this space is so rich as to have a single point in it encapsulate a whole novel?</div><br/><div id="40036939" class="c"><input type="checkbox" id="c-40036939" checked=""/><div class="controls bullet"><span class="by">vanjajaja1</span><span>|</span><a href="#40036858">parent</a><span>|</span><a href="#40036926">next</a><span>|</span><label class="collapse" for="c-40036939">[-]</label><label class="expand" for="c-40036939">[2 more]</label></div><br/><div class="children"><div class="content">at that point what it has is not a representation of the input, its a representation of what the next output could be. ie. its a lossy process and you can&#x27;t extract what came in the past, only the details relevant to next word prediction<p>(is my understanding)</div><br/><div id="40037457" class="c"><input type="checkbox" id="c-40037457" checked=""/><div class="controls bullet"><span class="by">rollinDyno</span><span>|</span><a href="#40036858">root</a><span>|</span><a href="#40036939">parent</a><span>|</span><a href="#40036926">next</a><span>|</span><label class="collapse" for="c-40037457">[-]</label><label class="expand" for="c-40037457">[1 more]</label></div><br/><div class="children"><div class="content">If the point was the presentation of only the next token, and predicted tokens were a function of only the preceding token, then the vector of the new token wouldnât have the information to produce new tokens that kept telling the novel.</div><br/></div></div></div></div><div id="40036926" class="c"><input type="checkbox" id="c-40036926" checked=""/><div class="controls bullet"><span class="by">faramarz</span><span>|</span><a href="#40036858">parent</a><span>|</span><a href="#40036939">prev</a><span>|</span><a href="#40036302">next</a><span>|</span><label class="collapse" for="c-40036926">[-]</label><label class="expand" for="c-40036926">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not about a single point encapsulating a novel, but how sequences of such embeddings can represent complex ideas when processed by the model&#x27;s layers.<p>each prediction is based on a weighted context of all previous tokens, not just the immediately preceding one.</div><br/><div id="40037434" class="c"><input type="checkbox" id="c-40037434" checked=""/><div class="controls bullet"><span class="by">rollinDyno</span><span>|</span><a href="#40036858">root</a><span>|</span><a href="#40036926">parent</a><span>|</span><a href="#40036302">next</a><span>|</span><label class="collapse" for="c-40037434">[-]</label><label class="expand" for="c-40037434">[1 more]</label></div><br/><div class="children"><div class="content">That weighted context is the 12228 dimensional vector, no?<p>I suppose that when you each element in the vector weighs 16 bits then the space is immense and capable to have a novel in a point.</div><br/></div></div></div></div></div></div><div id="40036302" class="c"><input type="checkbox" id="c-40036302" checked=""/><div class="controls bullet"><span class="by">spacecadet</span><span>|</span><a href="#40036858">prev</a><span>|</span><a href="#40036639">next</a><span>|</span><label class="collapse" for="c-40036302">[-]</label><label class="expand" for="c-40036302">[6 more]</label></div><br/><div class="children"><div class="content">Fun video. Much of my &quot;art&quot; lately has been dissecting models, injecting or altering attention, and creating animated visualizations of their inner workings. Some really fun shit.</div><br/><div id="40036532" class="c"><input type="checkbox" id="c-40036532" checked=""/><div class="controls bullet"><span class="by">j_bum</span><span>|</span><a href="#40036302">parent</a><span>|</span><a href="#40036517">next</a><span>|</span><label class="collapse" for="c-40036532">[-]</label><label class="expand" for="c-40036532">[4 more]</label></div><br/><div class="children"><div class="content">Link? Sounds fun and reminds me of this tweet [0]<p>[0] <a href="https:&#x2F;&#x2F;x.com&#x2F;jaschasd&#x2F;status&#x2F;1756930242965606582" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;jaschasd&#x2F;status&#x2F;1756930242965606582</a></div><br/><div id="40036549" class="c"><input type="checkbox" id="c-40036549" checked=""/><div class="controls bullet"><span class="by">spacecadet</span><span>|</span><a href="#40036302">root</a><span>|</span><a href="#40036532">parent</a><span>|</span><a href="#40036517">next</a><span>|</span><label class="collapse" for="c-40036549">[-]</label><label class="expand" for="c-40036549">[3 more]</label></div><br/><div class="children"><div class="content">Nah someone down voted it. And yes, it looks like that + 20 others that are animated.</div><br/><div id="40036741" class="c"><input type="checkbox" id="c-40036741" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#40036302">root</a><span>|</span><a href="#40036549">parent</a><span>|</span><a href="#40036517">next</a><span>|</span><label class="collapse" for="c-40036741">[-]</label><label class="expand" for="c-40036741">[2 more]</label></div><br/><div class="children"><div class="content">Downvotes == empty boats.  If &quot;Empty Boat parable&quot; doesn&#x27;t ring a bell, Google it...</div><br/><div id="40036882" class="c"><input type="checkbox" id="c-40036882" checked=""/><div class="controls bullet"><span class="by">globalnode</span><span>|</span><a href="#40036302">root</a><span>|</span><a href="#40036741">parent</a><span>|</span><a href="#40036517">next</a><span>|</span><label class="collapse" for="c-40036882">[-]</label><label class="expand" for="c-40036882">[1 more]</label></div><br/><div class="children"><div class="content">unless an algorithm decides to block or devalue the content, but yeah i looked it up, very interesting parable, thanks for sharing.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40036639" class="c"><input type="checkbox" id="c-40036639" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#40036302">prev</a><span>|</span><a href="#40036052">next</a><span>|</span><label class="collapse" for="c-40036639">[-]</label><label class="expand" for="c-40036639">[2 more]</label></div><br/><div class="children"><div class="content">I like the way he uses a low-rank decomposition of the Value matrix instead of Value+Output matrices. Much more intuitive!</div><br/><div id="40037536" class="c"><input type="checkbox" id="c-40037536" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40036639">parent</a><span>|</span><a href="#40036052">next</a><span>|</span><label class="collapse" for="c-40037536">[-]</label><label class="expand" for="c-40037536">[1 more]</label></div><br/><div class="children"><div class="content">It is the first time I hear about the Value matrix being low rank, so for me this was the confusing part. Codebases I have seen also have value + output matrixes so it is clearer that Q,K,V are similar sizes and there&#x27;s a separate projection matrix that adapts to the dimensions of the next network layer.
UPDATE: He mentions this in the last sections of the video.</div><br/></div></div></div></div><div id="40036052" class="c"><input type="checkbox" id="c-40036052" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40036639">prev</a><span>|</span><a href="#40035979">next</a><span>|</span><label class="collapse" for="c-40036052">[-]</label><label class="expand" for="c-40036052">[2 more]</label></div><br/><div class="children"><div class="content">It always blows my mind that Grant Sanderson can explain complex topics in such a clear, understandable way.<p>I&#x27;ve seen several tutorials, visualisations, and blogs explaining Transformers, but I didn&#x27;t fully understand them until this video.</div><br/><div id="40036229" class="c"><input type="checkbox" id="c-40036229" checked=""/><div class="controls bullet"><span class="by">chrishare</span><span>|</span><a href="#40036052">parent</a><span>|</span><a href="#40035979">next</a><span>|</span><label class="collapse" for="c-40036229">[-]</label><label class="expand" for="c-40036229">[1 more]</label></div><br/><div class="children"><div class="content">His content and impact is phenomenal</div><br/></div></div></div></div><div id="40035979" class="c"><input type="checkbox" id="c-40035979" checked=""/><div class="controls bullet"><span class="by">nostrebored</span><span>|</span><a href="#40036052">prev</a><span>|</span><label class="collapse" for="c-40035979">[-]</label><label class="expand" for="c-40035979">[2 more]</label></div><br/><div class="children"><div class="content">Working in a closely related space and this instantly became part of my team&#x27;s onboarding docs.<p>Worth noting that a lot of the visualization code is available in Github.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;3b1b&#x2F;videos&#x2F;tree&#x2F;master&#x2F;_2024&#x2F;transformers">https:&#x2F;&#x2F;github.com&#x2F;3b1b&#x2F;videos&#x2F;tree&#x2F;master&#x2F;_2024&#x2F;transformer...</a></div><br/><div id="40036011" class="c"><input type="checkbox" id="c-40036011" checked=""/><div class="controls bullet"><span class="by">sthatipamala</span><span>|</span><a href="#40035979">parent</a><span>|</span><label class="collapse" for="c-40036011">[-]</label><label class="expand" for="c-40036011">[1 more]</label></div><br/><div class="children"><div class="content">Sounds interesting; what else is part of those onboarding docs?</div><br/></div></div></div></div></div></div></div></div></div></body></html>