<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700211657985" as="style"/><link rel="stylesheet" href="styles.css?v=1700211657985"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://venturebeat.com/ai/what-meta-learned-from-galactica-the-doomed-model-launched-two-weeks-before-chatgpt/">What Meta learned from Galactica, the doomed model</a> <span class="domain">(<a href="https://venturebeat.com">venturebeat.com</a>)</span></div><div class="subtext"><span>swyx</span> | <span>22 comments</span></div><br/><div><div id="38300651" class="c"><input type="checkbox" id="c-38300651" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38300502">next</a><span>|</span><label class="collapse" for="c-38300651">[-]</label><label class="expand" for="c-38300651">[4 more]</label></div><br/><div class="children"><div class="content">I think the reason is because expectations were different. ChatGPT was released to the general public for general use. Galactica was really only noticed by the egg head press and egg head professionals (like myself) who rightly identified the failure of these statistical language models to have a grounding in facts. People whose job it is to notice details are going to push your model to the limit.<p>But it’s not like ChatGPT was better. If I recall, RLHF actually made hallucinations slightly worse. Even today OpenAI wouldn’t claim to be able to accurately summarize papers. It’s just that there was an endless supply of “write X in the style of Y” that was like catnip for journalists and kept them busy for months.</div><br/><div id="38300955" class="c"><input type="checkbox" id="c-38300955" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38300651">parent</a><span>|</span><a href="#38300787">next</a><span>|</span><label class="collapse" for="c-38300955">[-]</label><label class="expand" for="c-38300955">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>the failure of these statistical language models to have a grounding in facts</i><p>I feel the &quot;grounding in facts&quot; was a big mistake, and to a large extent still is. The worst case of it was symbolic linguistic and knowledge graphs. Why? Because symbols don&#x27;t exist, facts and concepts and <i>words</i> are fuzzy boundaries, their meaning defined mostly or entirely through associations with other words. <i>We</i> don&#x27;t learn symbolically, we don&#x27;t have strong &quot;grounding in facts&quot; - not at the language layer.<p>I&#x27;d rather say that language and understanding, the way we do it, is statistical in nature, and &quot;grounding in facts&quot; is done through feedback.</div><br/></div></div><div id="38300787" class="c"><input type="checkbox" id="c-38300787" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#38300651">parent</a><span>|</span><a href="#38300955">prev</a><span>|</span><a href="#38300952">next</a><span>|</span><label class="collapse" for="c-38300787">[-]</label><label class="expand" for="c-38300787">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT relies very much on confirmation bias to do its magic. You ask something trivial you already know (and could have probably Googled in 15 seconds), you get back something glib and smooth in reply and you are wowed by how smart GPT seems to be.<p>It is significantly less impressive when you ask something you don&#x27;t already know and can&#x27;t Google easily.</div><br/></div></div><div id="38300952" class="c"><input type="checkbox" id="c-38300952" checked=""/><div class="controls bullet"><span class="by">runnedrun</span><span>|</span><a href="#38300651">parent</a><span>|</span><a href="#38300787">prev</a><span>|</span><a href="#38300502">next</a><span>|</span><label class="collapse" for="c-38300952">[-]</label><label class="expand" for="c-38300952">[1 more]</label></div><br/><div class="children"><div class="content">Yea ChatGPT wasn&#x27;t better at non hallucinating, but RLHF made the model much more useful for the average consumer, so consumers used it, instead of just other ML people. And, like you said, consumers are a bit less critical than experts from the field.</div><br/></div></div></div></div><div id="38300502" class="c"><input type="checkbox" id="c-38300502" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#38300651">prev</a><span>|</span><a href="#38300363">next</a><span>|</span><label class="collapse" for="c-38300502">[-]</label><label class="expand" for="c-38300502">[2 more]</label></div><br/><div class="children"><div class="content">Maybe I’m not understanding the article correctly, but what was the lesson? That Facebook should have told people it would lie, similar to how OpenAI did?<p>I’m actually interested in what the internal lessons for Facebook would’ve been to see their product so replaced by a similar product only two weeks later, but unless I’m misunderstanding everything, the article doesn’t really seem to touch in it. At least not beyond the fact that people still want the model, and, that it’s part of llama and Facebooks new push for more open models.</div><br/><div id="38300881" class="c"><input type="checkbox" id="c-38300881" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#38300502">parent</a><span>|</span><a href="#38300363">next</a><span>|</span><label class="collapse" for="c-38300881">[-]</label><label class="expand" for="c-38300881">[1 more]</label></div><br/><div class="children"><div class="content">They do state the lesson. It just isn&#x27;t interesting:<p>&gt; “The gap between the expectation, and where the research was, was too big.”<p>&gt; Overall, Pineau said, “If I was to do it today, we would just manage the release.”</div><br/></div></div></div></div><div id="38300363" class="c"><input type="checkbox" id="c-38300363" checked=""/><div class="controls bullet"><span class="by">MenhirMike</span><span>|</span><a href="#38300502">prev</a><span>|</span><a href="#38300833">next</a><span>|</span><label class="collapse" for="c-38300363">[-]</label><label class="expand" for="c-38300363">[3 more]</label></div><br/><div class="children"><div class="content">Rookie mistake, it should have been named Galactus. Though of course, even though Galactus has omniscience, it doesn&#x27;t have futuresight, so perhaps it would still have been doomed.</div><br/><div id="38300450" class="c"><input type="checkbox" id="c-38300450" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#38300363">parent</a><span>|</span><a href="#38300833">next</a><span>|</span><label class="collapse" for="c-38300450">[-]</label><label class="expand" for="c-38300450">[2 more]</label></div><br/><div class="children"><div class="content">The all knowing user service provider aggregator</div><br/><div id="38300771" class="c"><input type="checkbox" id="c-38300771" checked=""/><div class="controls bullet"><span class="by">pastrami_panda</span><span>|</span><a href="#38300363">root</a><span>|</span><a href="#38300450">parent</a><span>|</span><a href="#38300833">next</a><span>|</span><label class="collapse" for="c-38300771">[-]</label><label class="expand" for="c-38300771">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but it still doesn&#x27;t support ISO timestamps: <a href="https:&#x2F;&#x2F;github.com&#x2F;acchiao&#x2F;galactus&#x2F;issues&#x2F;34">https:&#x2F;&#x2F;github.com&#x2F;acchiao&#x2F;galactus&#x2F;issues&#x2F;34</a></div><br/></div></div></div></div></div></div><div id="38300833" class="c"><input type="checkbox" id="c-38300833" checked=""/><div class="controls bullet"><span class="by">Topfi</span><span>|</span><a href="#38300363">prev</a><span>|</span><a href="#38300565">next</a><span>|</span><label class="collapse" for="c-38300833">[-]</label><label class="expand" for="c-38300833">[2 more]</label></div><br/><div class="children"><div class="content">I feel that the article doesn&#x27;t really cover what aspects made Galactica more open to criticism and led to its reception, outside of pointing to a lack of awareness for hallucinations in the wider public, which were equally present during the initial launch of ChatGPT (as most users never encountered OpenAIs disclosures on that topic, heck, we had lawyers relying on these models months after hallucinations as a phenomenon had been more widely covered). Maybe I missed it, but honestly, the article doesn&#x27;t contain much information about what Meta learned specifically from Galactica (over general reception to LLMs across the industry) other than that misadventure being the claimed reason for Llama 1&#x27;s release strategy, which has already been superseded by being less restrictive on Llama 2 access.<p>For me, the main reasons why Galactica was more heavily criticized than ChatGPT were twofold. For one, the more scientific aspirations that this model had from the outset, proclaimed boldly in the associated paper [0] made any factual errors or problematic output far less justifiable, compared to a model that, through its branding and design, was more conversational in nature.<p>That, I feel, leads directly into the second important reason. The initial launch of ChatGPT, very cleverly, targeted users of all backgrounds, making it more likely that the initial interaction the majority of users had with the model was more conversational and humorous than the type of systematic picking apart the more scientifically inclined crowd tends to partake in. This led to initial coverage of ChatGPT being filled with amazement by laypeople, drowning out a lot of criticism and more measured reactions in a sea of hype, whereas Galactica was consistently bombarded with less favorable reactions.<p>Overall, I still feel that the approach they took in building Galactica is one that should be explored further, and I am somewhat saddened that more focused LLMs have become less favored by researchers. I remain hopeful that as we explore the limitations of universal, conversational models, there will be a resurgence of more specialized LLMs similar to Galactica.<p>[0] <a href="https:&#x2F;&#x2F;galactica.org&#x2F;static&#x2F;paper.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;galactica.org&#x2F;static&#x2F;paper.pdf</a></div><br/><div id="38301130" class="c"><input type="checkbox" id="c-38301130" checked=""/><div class="controls bullet"><span class="by">epups</span><span>|</span><a href="#38300833">parent</a><span>|</span><a href="#38300565">next</a><span>|</span><label class="collapse" for="c-38301130">[-]</label><label class="expand" for="c-38301130">[1 more]</label></div><br/><div class="children"><div class="content">I think you are spot on, a LLM focused on science and medicine has a much higher bar to pass when it comes to accuracy.<p>I tried Galactica when it came out, and I have to say that subjectively at least the results looked much inferior to what the benchmarks suggest. In the paper they claim to be substantially better than GPT-3 on their larger models, while in my personal experience even some generous queries produced garbage output. I cannot remember whether the version available at the site was the largest model, however.</div><br/></div></div></div></div><div id="38300565" class="c"><input type="checkbox" id="c-38300565" checked=""/><div class="controls bullet"><span class="by">riffraff</span><span>|</span><a href="#38300833">prev</a><span>|</span><a href="#38300756">next</a><span>|</span><label class="collapse" for="c-38300565">[-]</label><label class="expand" for="c-38300565">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I misremember, but isn&#x27;t the fault about expectations vs reality due to how this was introduced?<p>I had the feeling Meta released this with great fanfare saying &quot;do science with it&quot; so of course there was backlash when it was shown to hallucinate.<p>I mean the website still sounds like it
<a href="https:&#x2F;&#x2F;galactica.org&#x2F;mission&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;galactica.org&#x2F;mission&#x2F;</a></div><br/></div></div><div id="38300756" class="c"><input type="checkbox" id="c-38300756" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#38300565">prev</a><span>|</span><a href="#38300621">next</a><span>|</span><label class="collapse" for="c-38300756">[-]</label><label class="expand" for="c-38300756">[3 more]</label></div><br/><div class="children"><div class="content">i think the only real mistake they made was taking twitter drama seriously.</div><br/><div id="38301121" class="c"><input type="checkbox" id="c-38301121" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#38300756">parent</a><span>|</span><a href="#38300838">next</a><span>|</span><label class="collapse" for="c-38301121">[-]</label><label class="expand" for="c-38301121">[1 more]</label></div><br/><div class="children"><div class="content">So say we all.</div><br/></div></div><div id="38300838" class="c"><input type="checkbox" id="c-38300838" checked=""/><div class="controls bullet"><span class="by">corethree</span><span>|</span><a href="#38300756">parent</a><span>|</span><a href="#38301121">prev</a><span>|</span><a href="#38300621">next</a><span>|</span><label class="collapse" for="c-38300838">[-]</label><label class="expand" for="c-38300838">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. There was the exact same push back on chatgpt but they just didn&#x27;t care.</div><br/></div></div></div></div><div id="38300621" class="c"><input type="checkbox" id="c-38300621" checked=""/><div class="controls bullet"><span class="by">dr_kiszonka</span><span>|</span><a href="#38300756">prev</a><span>|</span><a href="#38300878">next</a><span>|</span><label class="collapse" for="c-38300621">[-]</label><label class="expand" for="c-38300621">[2 more]</label></div><br/><div class="children"><div class="content">This model had exactly the feature I have been looking for the last three days! What I need is something to semi-reliably &quot;Translate Python code
to Math.&quot; Sadly, it doesn&#x27;t seem available anymore and I can&#x27;t find anything I could use. (There are only tools for translation in the opposite direction, i.e., math to code.)</div><br/><div id="38300708" class="c"><input type="checkbox" id="c-38300708" checked=""/><div class="controls bullet"><span class="by">_kb</span><span>|</span><a href="#38300621">parent</a><span>|</span><a href="#38300878">next</a><span>|</span><label class="collapse" for="c-38300708">[-]</label><label class="expand" for="c-38300708">[1 more]</label></div><br/><div class="children"><div class="content">Which is interesting (at least my simpleton perspective). The mathematical form is designed to communicate the essence of an idea rather than exact process, so seems much more suited to LLM sourced expression.</div><br/></div></div></div></div><div id="38300878" class="c"><input type="checkbox" id="c-38300878" checked=""/><div class="controls bullet"><span class="by">pmontra</span><span>|</span><a href="#38300621">prev</a><span>|</span><a href="#38300445">next</a><span>|</span><label class="collapse" for="c-38300878">[-]</label><label class="expand" for="c-38300878">[1 more]</label></div><br/><div class="children"><div class="content">The problem was that they told us that it was an AI for science and when you confabulate science you definitely do a very bad impression.</div><br/></div></div><div id="38300445" class="c"><input type="checkbox" id="c-38300445" checked=""/><div class="controls bullet"><span class="by">harryquach</span><span>|</span><a href="#38300878">prev</a><span>|</span><a href="#38300726">next</a><span>|</span><label class="collapse" for="c-38300445">[-]</label><label class="expand" for="c-38300445">[1 more]</label></div><br/><div class="children"><div class="content">Gives me a chuckle every time <a href="https:&#x2F;&#x2F;youtu.be&#x2F;y8OnoxKotPQ?si=xZsDfNwAivhdOBGD" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;y8OnoxKotPQ?si=xZsDfNwAivhdOBGD</a></div><br/></div></div><div id="38300726" class="c"><input type="checkbox" id="c-38300726" checked=""/><div class="controls bullet"><span class="by">happytiger</span><span>|</span><a href="#38300445">prev</a><span>|</span><a href="#38300663">next</a><span>|</span><label class="collapse" for="c-38300726">[-]</label><label class="expand" for="c-38300726">[1 more]</label></div><br/><div class="children"><div class="content">It’s incredibly difficult to separate the cognitive dissonance of the folks that built Meta’s platform from the reality of what they were facing, especially in light of success that open AI has had.<p>Was it a failure? Was it a failure internally to recognize the potential market opportunity? Was it a lack of product management? Was it a lack of vision? Really is no way to know. And after the fact, people always have all kinds of justifications for lessons and learning, but don’t necessarily have an explanation for why they missed the mark truly.<p>This article is very difficult to read and doesn’t really have a clear focus or point, at least not in a way that’s compelling to discuss. So it’s hard to understand what the author is trying to get across.<p>Meta a history of launching products that are well timed but miss a connection with customers, a price point, or don’t jive with regulators, and I think it has a lot to do with the focus on pleasing internal stakeholders.<p>There is a very large number of people that are in a very large &quot;management organization,&quot; and a lot of the focus of their product seems to be more and more focused on themselves.<p>Google has the same issue.<p>It’s difficult for them to launch things that are compelling for users <i>outside the company</i> that aren’t major product, launches because so much of what gets support inside the organization becomes what’s <i>important to the organization</i>  And that’s rarely an unproven market outside the company that requires experimentation without adequate ass covering style justifications and nebulous revenue possibilities.<p>I guarantee you there was some people at Meta that understood the AI opportunity and were ready to launch something that was inline with the interest of customers like open AI was.<p>So what caused the miss?<p>There’s an academic platform that has some level of hallucination, but it wasn’t the fact that it had hallucination that caused it not to get the interest of customers.<p>It was the fact that they focused on a very narrow, technical problem that was perhaps very interesting to the engineers who were building it, and perhaps got support inside the organization, but didn’t really really capture the hearts and minds of the customers that would help it grow and thrive.<p>Analyzing why that’s happening requires overcoming the cognitive dissonance of justification that always comes when you’ve missed a market opportunity, and I really do wonder what the real story it is for folks that are inside trying to get out with products that are relevant right now.</div><br/></div></div><div id="38300663" class="c"><input type="checkbox" id="c-38300663" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38300726">prev</a><span>|</span><label class="collapse" for="c-38300663">[-]</label><label class="expand" for="c-38300663">[1 more]</label></div><br/><div class="children"><div class="content">All knowing user info provider service aggregator?</div><br/></div></div></div></div></div></div></div></body></html>