<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721638876449" as="style"/><link rel="stylesheet" href="styles.css?v=1721638876449"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://neuml.github.io/txtai/">txtai: Open-source vector search and RAG for minimalists</a> <span class="domain">(<a href="https://neuml.github.io">neuml.github.io</a>)</span></div><div class="subtext"><span>dmezzetti</span> | <span>28 comments</span></div><br/><div><div id="41028183" class="c"><input type="checkbox" id="c-41028183" checked=""/><div class="controls bullet"><span class="by">ipsi</span><span>|</span><a href="#41027957">next</a><span>|</span><label class="collapse" for="c-41028183">[-]</label><label class="expand" for="c-41028183">[11 more]</label></div><br/><div class="children"><div class="content">So here&#x27;s something I&#x27;ve been wanting to do for a while, but have kinda been struggling to figure out _how_ to do it. txtai looks like it has all the tools necessary to do the job, I&#x27;m just not sure which tool(s), and how I&#x27;d use them.<p>Basically, I&#x27;d like to be able to take PDFs of, say, D&amp;D books, extract that data (this step is, at least, something I can already do), and load it into an LLM to be able to ask questions like:<p>* What does the feat &quot;Sentinel&quot; do?<p>* Who is Elminster?<p>* Which God(s) do Elves worship in Faerûn?<p>* Where I can I find the spell &quot;Crusader&#x27;s Mantle&quot;?<p>And so on. Given this data is all under copyright, I&#x27;d probably have to stick to using a local LLM to avoid problems. And, while I wouldn&#x27;t expect it to have good answers to all (or possibly any!) of those questions, I&#x27;d nevertheless love to be able to give it a try.<p>I&#x27;m just not sure where to start - I think I&#x27;d want to fine-tune an existing model since this is all natural language content, but I get a bit lost after that. Do I need to pre-process the content to add extra information that I can&#x27;t fetch relatively automatically. e.g., page numbers are simple to add in, but would I need to mark out things like chapter&#x2F;section headings, or in-character vs out-of-character text? Do I need to add all the content in as a series of questions and answers, like &quot;What information is on page 52 of the Player&#x27;s Handbook? =&gt; &lt;text of page&gt;&quot;?</div><br/><div id="41030590" class="c"><input type="checkbox" id="c-41030590" checked=""/><div class="controls bullet"><span class="by">muzani</span><span>|</span><a href="#41028183">parent</a><span>|</span><a href="#41028939">next</a><span>|</span><label class="collapse" for="c-41030590">[-]</label><label class="expand" for="c-41030590">[1 more]</label></div><br/><div class="children"><div class="content">Use RAG.<p>Fine tune will bias something to return specific answers. It&#x27;s great for tone and classification. It&#x27;s terrible for information. If you get info out of it, it&#x27;s because it&#x27;s a consistent hallucination.<p>Embeddings will turn the whole thing into a bunch of numbers. So something like Sentinel will probably match with similar feats. Embeddings are perfect for searching. You can convert images and sound to these numbers too.<p>But these numbers can&#x27;t be stored in any regular DB. Most of the time it&#x27;s somewhere in memory, then thrown out. I haven&#x27;t looked deep into txtai but it looks like what it does. This is okay, but it&#x27;s a little slow and wasteful as you&#x27;re running the embeddings each time. So that&#x27;s what vector DBs are for. But unless you&#x27;re running this at scale where every cent adds up, you don&#x27;t really need one.<p>As for preprocessing, many embedding models are already good enough. I&#x27;d say try it first, try different models, then tweak as needed. Generally proprietary models do better than open source, but there&#x27;s likely an open source one designed for game books, which would do best on an unprocessed D&amp;D book.<p>However it&#x27;s likely to be poor at matching pages afaik, unless you attach that info.</div><br/></div></div><div id="41028939" class="c"><input type="checkbox" id="c-41028939" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#41028183">parent</a><span>|</span><a href="#41030590">prev</a><span>|</span><a href="#41028513">next</a><span>|</span><label class="collapse" for="c-41028939">[-]</label><label class="expand" for="c-41028939">[1 more]</label></div><br/><div class="children"><div class="content">First I would calculate the number of tokens you actually need.  If its less than 32k there are plenty of ways to pull this off without RAG.  If more (millions), you should understand RAG is an approximation technique and results may not be as high quality.  If wayyyy more (billions), you might actually want to finetune</div><br/></div></div><div id="41028513" class="c"><input type="checkbox" id="c-41028513" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41028183">parent</a><span>|</span><a href="#41028939">prev</a><span>|</span><a href="#41029034">next</a><span>|</span><label class="collapse" for="c-41028513">[-]</label><label class="expand" for="c-41028513">[1 more]</label></div><br/><div class="children"><div class="content">Fine-tuning is almost certainly the wrong way to go about this. It&#x27;s not a good way of adding small amounts of new knowledge to a model because the existing knowledge tends to overwhelm anything you attempt to add in the fine-tuning steps.<p>Look into different RAG and tool usage mechanisms instead. You might even be able to get good results from dumping large amounts of information into a long context model like Gemini Flash.</div><br/></div></div><div id="41029034" class="c"><input type="checkbox" id="c-41029034" checked=""/><div class="controls bullet"><span class="by">fancy_pantser</span><span>|</span><a href="#41028183">parent</a><span>|</span><a href="#41028513">prev</a><span>|</span><a href="#41028278">next</a><span>|</span><label class="collapse" for="c-41029034">[-]</label><label class="expand" for="c-41029034">[1 more]</label></div><br/><div class="children"><div class="content">No fine-tuning is necessary. You can use something reasonably good at RAG that&#x27;s small enough to run locally like the Command-R model run by Ollama and a small embedding model like Nomic. There are dozens of simple interfaces that will let you import files to create a RAG knowledgebase to interact with as you describe, AnythingLLM is a popular one. Just point it at your locally-running LLM or tell them to download one using the interface. Behind the scenes they store everything in LanceDB or similar and perform the searching for you when you submit a prompt in the simple chat interface.</div><br/></div></div><div id="41028278" class="c"><input type="checkbox" id="c-41028278" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#41028183">parent</a><span>|</span><a href="#41029034">prev</a><span>|</span><a href="#41030671">next</a><span>|</span><label class="collapse" for="c-41028278">[-]</label><label class="expand" for="c-41028278">[3 more]</label></div><br/><div class="children"><div class="content">Based on what you&#x27;re looking to do, it sounds like Retrieval Augmented Generation (RAG) should help. This article has an example on how to do that with txtai: <a href="https:&#x2F;&#x2F;neuml.hashnode.dev&#x2F;build-rag-pipelines-with-txtai" rel="nofollow">https:&#x2F;&#x2F;neuml.hashnode.dev&#x2F;build-rag-pipelines-with-txtai</a><p>RAG sounds sophisticated but it&#x27;s actually quite simple. For each question, a database (vector database, keyword, relational etc) is first searched. The top n results are then inserted into a prompt and that is what is run with the LLM.<p>Before fine-tuning, I&#x27;d try that out first. I&#x27;m planning to have another example notebook out soon building on this.</div><br/><div id="41028515" class="c"><input type="checkbox" id="c-41028515" checked=""/><div class="controls bullet"><span class="by">ipsi</span><span>|</span><a href="#41028183">root</a><span>|</span><a href="#41028278">parent</a><span>|</span><a href="#41030671">next</a><span>|</span><label class="collapse" for="c-41028515">[-]</label><label class="expand" for="c-41028515">[2 more]</label></div><br/><div class="children"><div class="content">Ah, that&#x27;s very helpful, thanks! I&#x27;ll have a dig into this at some point relatively soon.<p>An example of how I might provide references with page numbers or chapter names would be great (even if this means a more complex text-extraction pipeline). As would examples showing anything I can do to indicate differences that are obvious to <i>me</i> but that an LLM would be unlikely to pick up, such as the previously mentioned in-character vs out-of-character distinction. This is mostly relevant for asking questions about the setting, where in-character information might be suspect (&quot;unreliable narrator&quot;), while out-of-character information is generally fully accurate.<p>Tangentially, is this something that I could reasonably experiment with without a GPU? While I do have a 4090, it&#x27;s in my Windows gaming machine, which isn&#x27;t really set up for AI&#x2F;LLM&#x2F;etc development.</div><br/><div id="41028748" class="c"><input type="checkbox" id="c-41028748" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#41028183">root</a><span>|</span><a href="#41028515">parent</a><span>|</span><a href="#41030671">next</a><span>|</span><label class="collapse" for="c-41028748">[-]</label><label class="expand" for="c-41028748">[1 more]</label></div><br/><div class="children"><div class="content">Will do, I&#x27;ll have the new notebooks published within the next couple weeks.<p>In terms of a no GPU setup, yes it&#x27;s possible but it will be slow. As long as you&#x27;re OK with slow response times, then it will eventually come back with answers.</div><br/></div></div></div></div></div></div><div id="41030671" class="c"><input type="checkbox" id="c-41030671" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41028183">parent</a><span>|</span><a href="#41028278">prev</a><span>|</span><a href="#41030563">next</a><span>|</span><label class="collapse" for="c-41030671">[-]</label><label class="expand" for="c-41030671">[1 more]</label></div><br/><div class="children"><div class="content">All the people saying &quot;don&#x27;t use fine-tuning&quot; don&#x27;t realize that most of traditional fine-tuning&#x27;s issues are due to modifying <i>all</i> of the weights in your model, which causes catastrophic forgetting<p>There&#x27;s tons of parameter efficient fine-tuning methods, i.e. lora, &quot;soft prompts&quot;, ReFt, etc which are actually good to use alongside RAG and will likely supercharge your solution compared to &quot;simply using RAG&quot;. The fewer parameters you modify, the more knowledge is &quot;preserved&quot;.<p>Also, look into the Graph-RAG&#x2F;Semantic Graph stuff in txtai. As usual, David (author of txtai) was implementing code for things that the market only just now cares about years ago.</div><br/></div></div><div id="41030563" class="c"><input type="checkbox" id="c-41030563" checked=""/><div class="controls bullet"><span class="by">spacecadet</span><span>|</span><a href="#41028183">parent</a><span>|</span><a href="#41030671">prev</a><span>|</span><a href="#41028609">next</a><span>|</span><label class="collapse" for="c-41030563">[-]</label><label class="expand" for="c-41030563">[1 more]</label></div><br/><div class="children"><div class="content">Very easy to do with Milvus and LangChain. I built a private slack bot that takes PDFs, chunks it into Milvus using PyMuPDF, the uses LangChain for recall, its surprising good for what your describe and took maybe 2 hours to build and run locally.</div><br/></div></div></div></div><div id="41027957" class="c"><input type="checkbox" id="c-41027957" checked=""/><div class="controls bullet"><span class="by">fastneutron</span><span>|</span><a href="#41028183">prev</a><span>|</span><a href="#41031130">next</a><span>|</span><label class="collapse" for="c-41027957">[-]</label><label class="expand" for="c-41027957">[2 more]</label></div><br/><div class="children"><div class="content">I’ve been building a RAG mini app with txtai these past few weeks and it’s been pretty smooth. I’m between this and llamaindex as the backend for a larger app I want to build for a small-to-midsize customer.<p>With the (potentially) obvious bias towards your own framework, are there situations in which you would <i>not</i> recommend it for a particular application?</div><br/><div id="41028198" class="c"><input type="checkbox" id="c-41028198" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#41027957">parent</a><span>|</span><a href="#41031130">next</a><span>|</span><label class="collapse" for="c-41028198">[-]</label><label class="expand" for="c-41028198">[1 more]</label></div><br/><div class="children"><div class="content">Glad to hear txtai is on your list.<p>I recently wrote an article (<a href="https:&#x2F;&#x2F;medium.com&#x2F;neuml&#x2F;vector-search-rag-landscape-a-review-with-txtai-a7f37ad0e187" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;neuml&#x2F;vector-search-rag-landscape-a-revie...</a>) comparing txtai with other popular frameworks. I was expecting to find some really interesting and innovative things in the others. But from my perspective I was underwhelmed.<p>I&#x27;m a big fan of simplicity and none of them are following that strategy. Agentic workflows seem like a big fancy term but I don&#x27;t see the value currently. Things are hard enough as it is.<p>If your team is already using another framework, I&#x27;m sure anything can work. Some of the other projects are VC-backed with larger teams. In some cases, that may be important.</div><br/></div></div></div></div><div id="41031130" class="c"><input type="checkbox" id="c-41031130" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#41027957">prev</a><span>|</span><a href="#41027382">next</a><span>|</span><label class="collapse" for="c-41031130">[-]</label><label class="expand" for="c-41031130">[1 more]</label></div><br/><div class="children"><div class="content">I did some prototyping with txtai for the RAG used in aider’s interactive help feature [0]. This lets users ask aider questions about using aider, customizing settings, troubleshooting, using LLMs, etc.<p>I really liked the simplicity of txtai. But it seems to require Java as a dependency! Aider is an end user cli tool, and ultimately I couldn’t take on the support burden of asking my users to install Java.<p>[0] <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;troubleshooting&#x2F;support.html" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;troubleshooting&#x2F;support.html</a></div><br/></div></div><div id="41027382" class="c"><input type="checkbox" id="c-41027382" checked=""/><div class="controls bullet"><span class="by">pjot</span><span>|</span><a href="#41031130">prev</a><span>|</span><a href="#41031428">next</a><span>|</span><label class="collapse" for="c-41027382">[-]</label><label class="expand" for="c-41027382">[1 more]</label></div><br/><div class="children"><div class="content">I’ve done something similar, but using duckDB as the backend&#x2F;vector store. You can use embeddings from wherever. My demo uses OpenAI.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;patricktrainer&#x2F;duckdb-embedding-search">https:&#x2F;&#x2F;github.com&#x2F;patricktrainer&#x2F;duckdb-embedding-search</a></div><br/></div></div><div id="41031428" class="c"><input type="checkbox" id="c-41031428" checked=""/><div class="controls bullet"><span class="by">staticautomatic</span><span>|</span><a href="#41027382">prev</a><span>|</span><a href="#41027566">next</a><span>|</span><label class="collapse" for="c-41031428">[-]</label><label class="expand" for="c-41031428">[1 more]</label></div><br/><div class="children"><div class="content">Looks pretty cool! Is this intended to be a simple alternative to, say, cobbling together something with LangChain and Chroma?</div><br/></div></div><div id="41027566" class="c"><input type="checkbox" id="c-41027566" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#41031428">prev</a><span>|</span><a href="#41027097">next</a><span>|</span><label class="collapse" for="c-41027566">[-]</label><label class="expand" for="c-41027566">[1 more]</label></div><br/><div class="children"><div class="content">Hello, author of txtai here. txtai was created back in 2020 starting with semantic search of medical literature. It has since grown into a framework for vector search, retrieval augmented generation (RAG) and large language model (LLM) orchestration&#x2F;workflows.<p>The goal of txtai is to be simple, performant, innovative and easy-to-use. It had vector search before many current projects existed. Semantic Graphs were added in 2022 before the Generative AI wave of 2023&#x2F;2024. GraphRAG is a hot topic but txtai had examples of using graphs to build search contexts back in 2022&#x2F;2023.<p>There is a commitment to quality and performance, especially with local models. For example, it&#x27;s vector embeddings component streams vectors to disk during indexing and uses mmaped arrays to enable indexing large datasets locally on a single node. txtai&#x27;s BM25 component is built from the scratch to work efficiently in Python leading to 6x better memory utilization and faster search performance than the BM25 Python library most commonly used.<p>I often see others complain about AI&#x2F;LLM&#x2F;RAG frameworks, so I wanted to share this project as many don&#x27;t know it exists.<p>Link to source (Apache 2.0): <a href="https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai">https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai</a></div><br/></div></div><div id="41027097" class="c"><input type="checkbox" id="c-41027097" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#41027566">prev</a><span>|</span><a href="#41027333">next</a><span>|</span><label class="collapse" for="c-41027097">[-]</label><label class="expand" for="c-41027097">[1 more]</label></div><br/><div class="children"><div class="content">This looks interesting. I&#x27;ve been wanting to build some tools to help feed text documents into Stable Diffusion and this looks like it could be helpful. Are there any other libs people are aware of that they&#x27;d recommend in this space?</div><br/></div></div><div id="41027333" class="c"><input type="checkbox" id="c-41027333" checked=""/><div class="controls bullet"><span class="by">janice1999</span><span>|</span><a href="#41027097">prev</a><span>|</span><a href="#41025981">next</a><span>|</span><label class="collapse" for="c-41027333">[-]</label><label class="expand" for="c-41027333">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s frustrating when developers of ML projects don&#x27;t state even the most basic requirements. Do I need an Nvidia 4090 or a cluster of H100s to run this?</div><br/><div id="41030682" class="c"><input type="checkbox" id="c-41030682" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41027333">parent</a><span>|</span><a href="#41028758">next</a><span>|</span><label class="collapse" for="c-41030682">[-]</label><label class="expand" for="c-41030682">[1 more]</label></div><br/><div class="children"><div class="content">The embedding models at the heart of txtai can be small enough to run on intel CPUs from ten years ago. It&#x27;s extremely frustrating when HN commentators don&#x27;t do even the most basic research into the product that they are critiquing.</div><br/></div></div><div id="41028758" class="c"><input type="checkbox" id="c-41028758" checked=""/><div class="controls bullet"><span class="by">malux85</span><span>|</span><a href="#41027333">parent</a><span>|</span><a href="#41030682">prev</a><span>|</span><a href="#41027480">next</a><span>|</span><label class="collapse" for="c-41028758">[-]</label><label class="expand" for="c-41028758">[5 more]</label></div><br/><div class="children"><div class="content">It’s frustrating when people ask for hardware requirements without stating what they are trying to do, do you have 100,000,000 books to index or do you have 5 articles? What are the context lengths you need? What about latency?<p>How can someone tell you what hardware you need when you give literally no information about what you’re trying to do?</div><br/><div id="41029717" class="c"><input type="checkbox" id="c-41029717" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#41027333">root</a><span>|</span><a href="#41028758">parent</a><span>|</span><a href="#41027480">next</a><span>|</span><label class="collapse" for="c-41029717">[-]</label><label class="expand" for="c-41029717">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a difference between &quot;how many CPU-hours will my task need&quot; and &quot;how much memory does this program use to even start up&quot;.</div><br/><div id="41030181" class="c"><input type="checkbox" id="c-41030181" checked=""/><div class="controls bullet"><span class="by">malux85</span><span>|</span><a href="#41027333">root</a><span>|</span><a href="#41029717">parent</a><span>|</span><a href="#41030167">next</a><span>|</span><label class="collapse" for="c-41030181">[-]</label><label class="expand" for="c-41030181">[2 more]</label></div><br/><div class="children"><div class="content">Having some idea of the task will guide the choice of model, which will be an enormous factor in memory use (I.e. whether it will startup or not)<p>Do you need a 70b param model or a 7b model? Theres thousands and thousands of dollars hardware difference there<p>With no idea of the task, one can’t even ball park it</div><br/><div id="41030929" class="c"><input type="checkbox" id="c-41030929" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#41027333">root</a><span>|</span><a href="#41030181">parent</a><span>|</span><a href="#41030167">next</a><span>|</span><label class="collapse" for="c-41030929">[-]</label><label class="expand" for="c-41030929">[1 more]</label></div><br/><div class="children"><div class="content">This particular tool has a page listing recommended models: <a href="https:&#x2F;&#x2F;neuml.github.io&#x2F;txtai&#x2F;models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;neuml.github.io&#x2F;txtai&#x2F;models&#x2F;</a></div><br/></div></div></div></div></div></div></div></div><div id="41027480" class="c"><input type="checkbox" id="c-41027480" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#41027333">parent</a><span>|</span><a href="#41028758">prev</a><span>|</span><a href="#41025981">next</a><span>|</span><label class="collapse" for="c-41027480">[-]</label><label class="expand" for="c-41027480">[1 more]</label></div><br/><div class="children"><div class="content">A RTX 3090 is more than enough for 7B LLMs. With 4-bit quantization, you can run inference with an even larger LLM using a 24GB GPU.<p>If you&#x27;re using remote API services, you might be able to just use a CPU.</div><br/></div></div></div></div><div id="41025981" class="c"><input type="checkbox" id="c-41025981" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#41027333">prev</a><span>|</span><label class="collapse" for="c-41025981">[-]</label><label class="expand" for="c-41025981">[1 more]</label></div><br/><div class="children"><div class="content">Link to source (Apache 2.0): <a href="https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai">https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai</a></div><br/></div></div></div></div></div></div></div></body></html>