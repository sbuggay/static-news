<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702890066737" as="style"/><link rel="stylesheet" href="styles.css?v=1702890066737"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://discourse.llvm.org/t/rfc-add-xegpu-dialect-for-intel-gpus/75723">Intel proposes XeGPU dialect for LLVM MLIR</a> <span class="domain">(<a href="https://discourse.llvm.org">discourse.llvm.org</a>)</span></div><div class="subtext"><span>artagnon</span> | <span>13 comments</span></div><br/><div><div id="38678531" class="c"><input type="checkbox" id="c-38678531" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38679545">next</a><span>|</span><label class="collapse" for="c-38678531">[-]</label><label class="expand" for="c-38678531">[1 more]</label></div><br/><div class="children"><div class="content">Inference is going to be interesting in 2025.<p>By that time we will have a good number of MI300 hosts. AMD Strix Halo (and the Intel equivalent?) will be out for high memory jobs locally. Intel Falcon Shores and who knows will finally be coming out, and from the looks of it the software ecosystem will be at least a little more hardware agnostic.</div><br/></div></div><div id="38679545" class="c"><input type="checkbox" id="c-38679545" checked=""/><div class="controls bullet"><span class="by">CalChris</span><span>|</span><a href="#38678531">prev</a><span>|</span><a href="#38679270">next</a><span>|</span><label class="collapse" for="c-38679545">[-]</label><label class="expand" for="c-38679545">[1 more]</label></div><br/><div class="children"><div class="content">“XeGPU dialect provides an abstraction that closely models Xe instructions.”<p>How is that an abstraction? It sounds more like a representation.</div><br/></div></div><div id="38679270" class="c"><input type="checkbox" id="c-38679270" checked=""/><div class="controls bullet"><span class="by">viksit</span><span>|</span><a href="#38679545">prev</a><span>|</span><a href="#38677578">next</a><span>|</span><label class="collapse" for="c-38679270">[-]</label><label class="expand" for="c-38679270">[3 more]</label></div><br/><div class="children"><div class="content">could someone eli5 about what this means for engineers working on systems from an app perspective &#x2F; higher level perspective?<p>(have worked extensively with tf &#x2F; pytorch)</div><br/><div id="38679497" class="c"><input type="checkbox" id="c-38679497" checked=""/><div class="controls bullet"><span class="by">zer0zzz</span><span>|</span><a href="#38679270">parent</a><span>|</span><a href="#38679382">next</a><span>|</span><label class="collapse" for="c-38679497">[-]</label><label class="expand" for="c-38679497">[1 more]</label></div><br/><div class="children"><div class="content">When the core compiler infrastructure generalizes more of the GPU support bits, it means you can pay less for hardware because the software stack is less coupled with one vendor’s hardware.</div><br/></div></div><div id="38679382" class="c"><input type="checkbox" id="c-38679382" checked=""/><div class="controls bullet"><span class="by">gavinray</span><span>|</span><a href="#38679270">parent</a><span>|</span><a href="#38679497">prev</a><span>|</span><a href="#38677578">next</a><span>|</span><label class="collapse" for="c-38679382">[-]</label><label class="expand" for="c-38679382">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely nothing, this is a &quot;sausage being made&quot; sort of thing.</div><br/></div></div></div></div><div id="38677578" class="c"><input type="checkbox" id="c-38677578" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#38679270">prev</a><span>|</span><a href="#38677558">next</a><span>|</span><label class="collapse" for="c-38677578">[-]</label><label class="expand" for="c-38677578">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;discourse.llvm.org&#x2F;t&#x2F;rfc-add-xegpu-dialect-for-intel-gpus&#x2F;75723" rel="nofollow noreferrer">https:&#x2F;&#x2F;discourse.llvm.org&#x2F;t&#x2F;rfc-add-xegpu-dialect-for-intel...</a> :<p>&gt; <i>XeGPU dialect models a subset of Xe GPU’s unique features focusing on GEMM performance. The operations include 2d load, dpas, atomic, scattered load, 1d load, named barrier, mfence, and compile-hint. These operations provide a minimum set to support high-performance MLIR GEMM implementation for a wide range of GEMM shapes. XeGPU dialect complements Arith, Math, Vector, and Memref dialects. This allows XeGPU based MLIR GEMM implementation fused with other operations lowered through existing MLIR dialects.</i></div><br/></div></div><div id="38675910" class="c"><input type="checkbox" id="c-38675910" checked=""/><div class="controls bullet"><span class="by">KingLancelot</span><span>|</span><a href="#38677558">prev</a><span>|</span><a href="#38675671">next</a><span>|</span><label class="collapse" for="c-38675910">[-]</label><label class="expand" for="c-38675910">[2 more]</label></div><br/><div class="children"><div class="content">Not the way to do this.<p>Accelerators already have a common middle layer.<p><a href="https:&#x2F;&#x2F;discourse.llvm.org&#x2F;t&#x2F;rfc-introducing-llvm-project-offload&#x2F;74302&#x2F;23" rel="nofollow noreferrer">https:&#x2F;&#x2F;discourse.llvm.org&#x2F;t&#x2F;rfc-introducing-llvm-project-of...</a></div><br/><div id="38678884" class="c"><input type="checkbox" id="c-38678884" checked=""/><div class="controls bullet"><span class="by">superb_dev</span><span>|</span><a href="#38675910">parent</a><span>|</span><a href="#38675671">next</a><span>|</span><label class="collapse" for="c-38678884">[-]</label><label class="expand" for="c-38678884">[1 more]</label></div><br/><div class="children"><div class="content">From the thread it looks like AMD and Nvidia have something similar, maybe Intel is wants feature parity</div><br/></div></div></div></div><div id="38675671" class="c"><input type="checkbox" id="c-38675671" checked=""/><div class="controls bullet"><span class="by">gardenfelder</span><span>|</span><a href="#38675910">prev</a><span>|</span><label class="collapse" for="c-38675671">[-]</label><label class="expand" for="c-38675671">[2 more]</label></div><br/><div class="children"><div class="content">Direct link: <a href="https:&#x2F;&#x2F;hai.stanford.edu&#x2F;news&#x2F;how-well-do-large-language-models-support-clinician-information-needs" rel="nofollow noreferrer">https:&#x2F;&#x2F;hai.stanford.edu&#x2F;news&#x2F;how-well-do-large-language-mod...</a></div><br/><div id="38677030" class="c"><input type="checkbox" id="c-38677030" checked=""/><div class="controls bullet"><span class="by">mplewis9z</span><span>|</span><a href="#38675671">parent</a><span>|</span><label class="collapse" for="c-38677030">[-]</label><label class="expand" for="c-38677030">[1 more]</label></div><br/><div class="children"><div class="content">You replied to the wrong topic.</div><br/></div></div></div></div></div></div></div></div></div></body></html>