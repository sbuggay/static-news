<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716886857253" as="style"/><link rel="stylesheet" href="styles.css?v=1716886857253"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.15071">Grokked Transformers Are Implicit Reasoners</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>jasondavies</span> | <span>19 comments</span></div><br/><div><div id="40497258" class="c"><input type="checkbox" id="c-40497258" checked=""/><div class="controls bullet"><span class="by">campers</span><span>|</span><a href="#40496765">next</a><span>|</span><label class="collapse" for="c-40497258">[-]</label><label class="expand" for="c-40497258">[1 more]</label></div><br/><div class="children"><div class="content">This is the interesting result where their GPT-2 sized transformer blows away GPT4 and Gemini 1.5 in connecting together facts<p><pre><code>  The difficulty of such a task is two-fold. First, the search space is large. For example, on average, each query entity connects with more than 50 facts, and each bridge entity in the ground truth proof connects with more than 900 facts. Second, there are no surface form clues to exploit and bias the search towards the ground truth proof, unlike most conventional QA benchmarks where the proof steps are transparent from the query.

  To test LLMs based on non-parametric memory, we translate the facts into natural language by simple templates (Appendix F). Facts&#x2F;queries for each attribute are grouped&#x2F;tested separately. We test both the vanilla setup where all facts (28.2K on average) are loaded into the LLM context, and the retrieval-augmented setup (5.4K facts retrieved on average) where the two-hop neighborhoods of the two query entities are retrieved, which includes enough facts to deduce the answer. We also try both standard prompting where the model answers directly, and chain-of-thought (CoT) prompting where the model is prompted to verbalize the reasoning. We test GPT-4-Turbo and Gemini-Pro-1.5, where for GPT-4-Turbo we only test the retrieval-augmented setup due to context length limit.

  Table 1:Results on the complex reasoning task. Direct&#x2F;CoT: predict the answer directly&#x2F;verbalize the reasoning steps. “+R”: retrieval augmentation.

                      GPT-4-Turbo      Gemini-Pro-1.5        Grokked Transformer
               Direct+R CoT+R  Direct CoT  Direct+R. CoT+R
  Accuracy (%)  33.3     31.3   28.7  11.3  37.3     12.0       99.3</code></pre></div><br/></div></div><div id="40496765" class="c"><input type="checkbox" id="c-40496765" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#40497258">prev</a><span>|</span><a href="#40496508">next</a><span>|</span><label class="collapse" for="c-40496765">[-]</label><label class="expand" for="c-40496765">[1 more]</label></div><br/><div class="children"><div class="content">Since I first learned about grokking, I&#x27;ve had a strong suspicion that getting a handle on it and figuring out how to aid it should be the central question in AI. We are currently stuck in a local minimum, where memorizing circuits perform well-enough to handle a whole lot of economically viable use cases. But the profit function has guided us into a valley dominated by a data and compute hungry architecture that isn&#x27;t ideal for learning generalizing circuits (partially because the memorizing circuits are so effective! We relatively quickly get to a flat loss landscape, after which we blindly jump around for countless epochs in a kind of Brownian motion until we get into an area where regularizers can drive generalization). Research like this paper is incredibly important.<p>I thought this was the most interesting bit from the paper:<p>&gt; Training data distribution, instead of training data size, qualitatively influences generalization behavior.</div><br/></div></div><div id="40496508" class="c"><input type="checkbox" id="c-40496508" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#40496765">prev</a><span>|</span><a href="#40497480">next</a><span>|</span><label class="collapse" for="c-40496508">[-]</label><label class="expand" for="c-40496508">[4 more]</label></div><br/><div class="children"><div class="content">I just learned about grokking; reminds me of double descent, and I looked up a 2022 paper called &quot;Unifying grokking and double descent&quot;. I&#x27;m still unclear on what the difference is. My basic understanding of double descent was that the regularization loss made the model focus on regularization after fitting the train data.</div><br/><div id="40497324" class="c"><input type="checkbox" id="c-40497324" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#40496508">parent</a><span>|</span><a href="#40497480">next</a><span>|</span><label class="collapse" for="c-40497324">[-]</label><label class="expand" for="c-40497324">[3 more]</label></div><br/><div class="children"><div class="content">Grokking is a sudden huge jump in test accuracy with increasing training steps, well after training accuracy has fully converged. Double descent is test performance increasing, decreasing, and then finally rising again as model parameters are increased.</div><br/><div id="40497658" class="c"><input type="checkbox" id="c-40497658" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#40496508">root</a><span>|</span><a href="#40497324">parent</a><span>|</span><a href="#40497480">next</a><span>|</span><label class="collapse" for="c-40497658">[-]</label><label class="expand" for="c-40497658">[2 more]</label></div><br/><div class="children"><div class="content">What they share is a subversion of the naive framework that ML works simply by performing gradient descent over a loss landscape. Double descent subverts it by showing that learning isn&#x27;t monotonic in parameter count; grokking subverts it by learning after training convergence.<p>I&#x27;d put the lottery ticket hypothesis in the same bucket of &quot;things that may happen that don&#x27;t make sense at all for a simple optimization procedure.&quot;</div><br/><div id="40498201" class="c"><input type="checkbox" id="c-40498201" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#40496508">root</a><span>|</span><a href="#40497658">parent</a><span>|</span><a href="#40497480">next</a><span>|</span><label class="collapse" for="c-40498201">[-]</label><label class="expand" for="c-40498201">[1 more]</label></div><br/><div class="children"><div class="content">My takeaway from the paper is that you can guide training by adding&#x2F;switching to a more difficult loss function after you got the basics right. Looks like they never got to overfitting grokking, so maybe there’s more to discover further down the training alley.</div><br/></div></div></div></div></div></div></div></div><div id="40497480" class="c"><input type="checkbox" id="c-40497480" checked=""/><div class="controls bullet"><span class="by">sturza</span><span>|</span><a href="#40496508">prev</a><span>|</span><a href="#40495634">next</a><span>|</span><label class="collapse" for="c-40497480">[-]</label><label class="expand" for="c-40497480">[2 more]</label></div><br/><div class="children"><div class="content">Grokking is all you need?</div><br/><div id="40498703" class="c"><input type="checkbox" id="c-40498703" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40497480">parent</a><span>|</span><a href="#40495634">next</a><span>|</span><label class="collapse" for="c-40498703">[-]</label><label class="expand" for="c-40498703">[1 more]</label></div><br/><div class="children"><div class="content">you still need attention!</div><br/></div></div></div></div><div id="40495634" class="c"><input type="checkbox" id="c-40495634" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#40497480">prev</a><span>|</span><a href="#40496793">next</a><span>|</span><label class="collapse" for="c-40495634">[-]</label><label class="expand" for="c-40495634">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of that old quote about “the difference between average and state of the art is forgetting to turn it off over summer break” or similar.<p>I wonder if this is why smaller LLMs seem to punch above their weight, are they further along in the process of distilling the data down into understanding?</div><br/></div></div><div id="40496793" class="c"><input type="checkbox" id="c-40496793" checked=""/><div class="controls bullet"><span class="by">syntaxfree</span><span>|</span><a href="#40495634">prev</a><span>|</span><label class="collapse" for="c-40496793">[-]</label><label class="expand" for="c-40496793">[9 more]</label></div><br/><div class="children"><div class="content">&gt; delve</div><br/><div id="40496807" class="c"><input type="checkbox" id="c-40496807" checked=""/><div class="controls bullet"><span class="by">bzalasky</span><span>|</span><a href="#40496793">parent</a><span>|</span><a href="#40497471">next</a><span>|</span><label class="collapse" for="c-40496807">[-]</label><label class="expand" for="c-40496807">[6 more]</label></div><br/><div class="children"><div class="content">Had the exact same thought after reading the abstract… FWIW, delve only appears in the abstract. Having not read the rest of the paper yet, I might give the authors the benefit of the doubt that they used an LLM to summarize their findings for the abstract, but didn&#x27;t abuse an LLM in writing the entire paper.</div><br/><div id="40497123" class="c"><input type="checkbox" id="c-40497123" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#40496793">root</a><span>|</span><a href="#40496807">parent</a><span>|</span><a href="#40497471">next</a><span>|</span><label class="collapse" for="c-40497123">[-]</label><label class="expand" for="c-40497123">[5 more]</label></div><br/><div class="children"><div class="content">Putting aside the possibility that they just happened to use the word “delve,” IMO we still have to figure out the convention for this sort of thing. I don’t particularly value the time scientists spend writing the prose around their ideas, the ideas themselves are the valuable part.<p>One possibility, for example, could be journals allow AI written submissions but also require and distribute the prompts. Then we could just read the prompts and be spared stuff like the passive voice dance.<p>They probably abused a compiler to generate their program instead of writing it in assembly.</div><br/><div id="40497465" class="c"><input type="checkbox" id="c-40497465" checked=""/><div class="controls bullet"><span class="by">Sysreq2</span><span>|</span><a href="#40496793">root</a><span>|</span><a href="#40497123">parent</a><span>|</span><a href="#40497210">next</a><span>|</span><label class="collapse" for="c-40497465">[-]</label><label class="expand" for="c-40497465">[3 more]</label></div><br/><div class="children"><div class="content">Soon AI will turn a chickenscrath of notes into a wonderful email. And then turn it back automatically for the end reader.<p>We put to much emphasis on the look rather than the substance. People are afraid to send out an email with 2 words: Meeting Friday and instead pad it out with pleasantry and detail, context and importance, but none of that really matters.</div><br/><div id="40497908" class="c"><input type="checkbox" id="c-40497908" checked=""/><div class="controls bullet"><span class="by">ericjmorey</span><span>|</span><a href="#40496793">root</a><span>|</span><a href="#40497465">parent</a><span>|</span><a href="#40497210">next</a><span>|</span><label class="collapse" for="c-40497908">[-]</label><label class="expand" for="c-40497908">[2 more]</label></div><br/><div class="children"><div class="content">&#x27;Meeting Friday&quot; is not enough information to have me attend the meeting. So I&#x27;m not sure what this analogy was supposed to illustrate.</div><br/><div id="40498010" class="c"><input type="checkbox" id="c-40498010" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#40496793">root</a><span>|</span><a href="#40497908">parent</a><span>|</span><a href="#40497210">next</a><span>|</span><label class="collapse" for="c-40498010">[-]</label><label class="expand" for="c-40498010">[1 more]</label></div><br/><div class="children"><div class="content">Depends on who it is from I guess.</div><br/></div></div></div></div></div></div><div id="40497210" class="c"><input type="checkbox" id="c-40497210" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#40496793">root</a><span>|</span><a href="#40497123">parent</a><span>|</span><a href="#40497465">prev</a><span>|</span><a href="#40497471">next</a><span>|</span><label class="collapse" for="c-40497210">[-]</label><label class="expand" for="c-40497210">[1 more]</label></div><br/><div class="children"><div class="content">A compiler yields deterministic results though.</div><br/></div></div></div></div></div></div><div id="40497471" class="c"><input type="checkbox" id="c-40497471" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#40496793">parent</a><span>|</span><a href="#40496807">prev</a><span>|</span><a href="#40497679">next</a><span>|</span><label class="collapse" for="c-40497471">[-]</label><label class="expand" for="c-40497471">[1 more]</label></div><br/><div class="children"><div class="content">?</div><br/></div></div><div id="40497679" class="c"><input type="checkbox" id="c-40497679" checked=""/><div class="controls bullet"><span class="by">qazxcvbnm</span><span>|</span><a href="#40496793">parent</a><span>|</span><a href="#40497471">prev</a><span>|</span><label class="collapse" for="c-40497679">[-]</label><label class="expand" for="c-40497679">[1 more]</label></div><br/><div class="children"><div class="content">With a quick skim, the paper delivers on its promise. It&#x27;s not a particularly long or difficult paper to follow.<p>&gt; Causal tracing. The transformer could be viewed as a causal graph that propagates information from the input to the output through a grid of intermediate states, which allows for a variety of causal
analyses on its internal computation<p>&gt; [...] There are in total three steps:<p>&gt; 1. The normal run records the model’s hidden state activations on a regular input [...]<p>&gt; 2. In the perturbed run, a slightly perturbed input is fed to the model which changes the prediction, where again the hidden state activations are recorded. [...] Specifically, for the hidden state of interest, we replace the input token at the same position as the state to be a random alternative of the same type (e.g., r1 → r′1) that leads to a different target prediction (e.g., t → t′).<p>&gt; 3. Intervention. During the normal run, we intervene the state of interest by replacing its activation with its activation in the perturbed run. We then run the remaining computations and measure if the target state (top-1 token through logit lens) is altered. The ratio of such alterations (between 0 and 1) quantitatively characterizes the causal strength between the state of interest and the target.<p>&gt; The generalizing circuit. [...] The discovered generalizing circuit (i.e., the causal computational pathways after grokking) is illustrated in Figure 4(a). Specifically, we locate a highly interpretable causal graph consisting of states in layer 0, 5, and 8, [...]. Layer 5 splits the circuit into lower and upper
layers, where 1) the lower layers retrieve the first-hop fact (h, r1, b) from the input h, r1, store the bridge entity b in S[5, r1], and “delay” the processing of r2 to S[5, r2]; 2) the upper layers retrieve the second-hop fact (b, r2, t) from S[5, r1] and S[5, r2], and store the tail t to the output state S[8, r2].<p>&gt; What happens during grokking? To understand the underlying mechanism behind grokking, we track the strengths of causal connections and results from logit lens across different model checkpoints during grokking (the “start” of grokking is the point when training performance saturates). We observe two notable amplifications (within the identified graph) that happen during grokking. The
first is the causal connection between S[5, r1] and the final prediction t, which is very weak before grokking and grows significantly during grokking. The second is the r2 component of S[5, r2] via logit lens, for which we plot its mean reciprocal rank (MRR).
Additionally, we find that the state S[5, r1] has a large component of the bridge entity b throughout grokking. These observations strongly suggest that the model is gradually forming the second hop in the upper layers (5-8) during grokking. This also indicates that, before grokking, the model is very likely mostly memorizing the examples in train_inferred by directly
associating (h, r1, r2) with t, without going through the first hop<p>&gt; Why does grokking happen? These observations suggest a natural explanation of why grokking happens through the lens of circuit efficiency. Specifically, as illustrated above, there exist both a memorizing circuit Cmem and a generalizing circuit Cgen that can fit the training data [...]</div><br/></div></div></div></div></div></div></div></div></div></body></html>