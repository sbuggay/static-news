<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1738314065005" as="style"/><link rel="stylesheet" href="styles.css?v=1738314065005"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2501.16396">TopoNets: High performing vision and language models with brain-like topography</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>mayukhdeb</span> | <span>39 comments</span></div><br/><div><div id="42885723" class="c"><input type="checkbox" id="c-42885723" checked=""/><div class="controls bullet"><span class="by">gzer0</span><span>|</span><a href="#42884509">next</a><span>|</span><label class="collapse" for="c-42885723">[-]</label><label class="expand" for="c-42885723">[1 more]</label></div><br/><div class="children"><div class="content">I spent time working with Andrej and the rest of the FSD team back in 2020&#x2F;2021, and we had plenty of conversations on how human visual processing maps onto our neural network architectures.
Our approach—transformer-based attention blocks, multi-scale feature extraction, and temporal fusion—mirrors elements of the biological visual cortex (retina → LGN → V1 → V2 → V4 → IT) which break down raw inputs and integrate them over time.<p>It’s amazing how closely this synthetic perceptual pipeline parallels the way our own brains interpret the world.<p>The key insight we discovered was that explicitly enforcing brain-like topographic organization (as some academic work attempts) isn&#x27;t necessary - what matters is having the right functional components that parallel biological visual processing. Our experience showed that the key elements of biological visual processing - like hierarchical feature extraction and temporal integration - emerge <i>naturally</i> when you build architectures that have to solve real visual tasks.<p>The brain&#x27;s organization serves its function, not the other way around. This was validated by the real-world performance of our synthetic visual cortex in the Tesla FSD stack.<p>Link to the 2021 Tesla AI day talk: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;j0z4FweCy4M?t=3010s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;j0z4FweCy4M?t=3010s</a></div><br/></div></div><div id="42884509" class="c"><input type="checkbox" id="c-42884509" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42885723">prev</a><span>|</span><a href="#42884874">next</a><span>|</span><label class="collapse" for="c-42884509">[-]</label><label class="expand" for="c-42884509">[22 more]</label></div><br/><div class="children"><div class="content">The main reason topography emerges in physical brains is because spatially distant connections are physically difficult and expensive in biological systems. Artificial neural nets have no such trade-off. So what&#x27;s the motivation here? I can understand this might be a very good regularizer, so it could help with generalization error on small-data tasks. But hard to see why this should be on the critical path to AGI. As compute and data grows, you want less inductive bias. For example, CNN will beat ViT on small data tasks, but that flips with enough scale because ViT imposes less inductive bias. Or at least any inductive bias should be chosen because it models the structure of the data well, such as with causal transformers and language.</div><br/><div id="42885030" class="c"><input type="checkbox" id="c-42885030" checked=""/><div class="controls bullet"><span class="by">AYBABTME</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42884545">next</a><span>|</span><label class="collapse" for="c-42885030">[-]</label><label class="expand" for="c-42885030">[2 more]</label></div><br/><div class="children"><div class="content">Locality of data and computation is very important in neural nets. It&#x27;s the number one reason why training and inference are as slow as they are. It&#x27;s why GPUs need super expensive HBM memory, why NVLink is a thing, why Infiniband is a thing.<p>If the problem of training and inference on neural networks can be optimized so that a topology can be used to keep closely related data together, we will see huge advancements in training and inference speed, and probably in model size as a result.<p>And speed isn&#x27;t just speed. Speed makes impossible (not enough time in our lifetime) things possible.<p>A huge factor in Deepseek being able to train on H800 (half HBM bandwith as H100) is that they used GPU cores to compress&#x2F;decompress the data moved around between the GPU memory and the compute units. This reduces latency in accessing data and made up for the slower memory bandwith (which translates in higher latency when fetching data). Anything that reduces the latency of memory accesses is a huge accelerator for neural nets. The number one way to achieve this is to keep related data next to each other, so that it fits in the closest caches possible.</div><br/><div id="42885130" class="c"><input type="checkbox" id="c-42885130" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42885030">parent</a><span>|</span><a href="#42884545">next</a><span>|</span><label class="collapse" for="c-42885130">[-]</label><label class="expand" for="c-42885130">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s true, but isn&#x27;t OP also correct? Ie. it&#x27;s about speed, which implies locality, which implies approaches like MoE which does exactly that and it&#x27;s unlike physical brain topology?<p>Having said that it would be fun to see things like rearrangement data moves based on temerature of silicon parts after training cycle.</div><br/></div></div></div></div><div id="42884545" class="c"><input type="checkbox" id="c-42884545" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42885030">prev</a><span>|</span><a href="#42884664">next</a><span>|</span><label class="collapse" for="c-42884545">[-]</label><label class="expand" for="c-42884545">[5 more]</label></div><br/><div class="children"><div class="content">Unless GPUs work markedly differently somehow or there’s been some fundamental shift in computer architecture I’m not aware of, spatial locality is still a factor in computers.<p>Aside from HW acceleration today, designs like Cebras would benefit heavily by reducing the amount of random access from accessing the weights (and thus freeing up cross-chip memory bandwidth for other things).</div><br/><div id="42884570" class="c"><input type="checkbox" id="c-42884570" checked=""/><div class="controls bullet"><span class="by">whynotminot</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884545">parent</a><span>|</span><a href="#42884665">next</a><span>|</span><label class="collapse" for="c-42884570">[-]</label><label class="expand" for="c-42884570">[3 more]</label></div><br/><div class="children"><div class="content">This makes me remember game developers back when games could still be played directly from the physical disc. They would often duplicate data to different parts of the disc, knowing that certain data would often be streamed from disc together, so that seek times were minimized.<p>But those game devs knew where everything was spatially on the disc, and how the data would generally be used during gameplay. It was consistent.<p>Do engineers have a lot of insight into how models get loaded spatially onto a given GPU at run time? Is this constant? Is it variable on a per GPU basis? I would think it would have to be.<p>Hard to optimize for this.</div><br/><div id="42884685" class="c"><input type="checkbox" id="c-42884685" checked=""/><div class="controls bullet"><span class="by">jaek</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884570">parent</a><span>|</span><a href="#42884665">next</a><span>|</span><label class="collapse" for="c-42884685">[-]</label><label class="expand" for="c-42884685">[2 more]</label></div><br/><div class="children"><div class="content">This brings to mind The Story of Mel  from programming folklore.<p><a href="http:&#x2F;&#x2F;beza1e1.tuxen.de&#x2F;lore&#x2F;story_of_mel.html" rel="nofollow">http:&#x2F;&#x2F;beza1e1.tuxen.de&#x2F;lore&#x2F;story_of_mel.html</a></div><br/><div id="42884946" class="c"><input type="checkbox" id="c-42884946" checked=""/><div class="controls bullet"><span class="by">abrookewood</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884685">parent</a><span>|</span><a href="#42884665">next</a><span>|</span><label class="collapse" for="c-42884946">[-]</label><label class="expand" for="c-42884946">[1 more]</label></div><br/><div class="children"><div class="content">Such a good read - some people really are on another level in their chosen field.</div><br/></div></div></div></div></div></div><div id="42884665" class="c"><input type="checkbox" id="c-42884665" checked=""/><div class="controls bullet"><span class="by">harles</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884545">parent</a><span>|</span><a href="#42884570">prev</a><span>|</span><a href="#42884664">next</a><span>|</span><label class="collapse" for="c-42884665">[-]</label><label class="expand" for="c-42884665">[1 more]</label></div><br/><div class="children"><div class="content">That could explain compute efficiency, but has nothing to do with the parameter efficiency pointed at in the paper.</div><br/></div></div></div></div><div id="42884664" class="c"><input type="checkbox" id="c-42884664" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42884545">prev</a><span>|</span><a href="#42884757">next</a><span>|</span><label class="collapse" for="c-42884664">[-]</label><label class="expand" for="c-42884664">[1 more]</label></div><br/><div class="children"><div class="content">Maybe this would be relevant for datacenters with significant distance between machines, or multidatacenter systems.</div><br/></div></div><div id="42884757" class="c"><input type="checkbox" id="c-42884757" checked=""/><div class="controls bullet"><span class="by">jv22222</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42884664">prev</a><span>|</span><a href="#42884529">next</a><span>|</span><label class="collapse" for="c-42884757">[-]</label><label class="expand" for="c-42884757">[1 more]</label></div><br/><div class="children"><div class="content">I had this idea the other day. Not sure if it relates  but maybe?<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;justinvincent&#x2F;status&#x2F;1884357300703400274" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;justinvincent&#x2F;status&#x2F;1884357300703400274</a></div><br/></div></div><div id="42884529" class="c"><input type="checkbox" id="c-42884529" checked=""/><div class="controls bullet"><span class="by">xpl</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42884757">prev</a><span>|</span><a href="#42884607">next</a><span>|</span><label class="collapse" for="c-42884529">[-]</label><label class="expand" for="c-42884529">[7 more]</label></div><br/><div class="children"><div class="content"><i>&gt; So what&#x27;s the motivation here?</i><p>Better interpretability, I suppose. Could give insights into how cognition works.</div><br/><div id="42884678" class="c"><input type="checkbox" id="c-42884678" checked=""/><div class="controls bullet"><span class="by">mayukhdeb</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884529">parent</a><span>|</span><a href="#42884595">next</a><span>|</span><label class="collapse" for="c-42884678">[-]</label><label class="expand" for="c-42884678">[1 more]</label></div><br/><div class="children"><div class="content">The motivation was to induce structure in the weights of neural nets and see if the functional organization that emerges aligns with that of the brain or not. Turns out, it does -- both for vision and language.<p>The gains in parameter efficiency was a surprise even to us when we first tried it out.</div><br/></div></div><div id="42884595" class="c"><input type="checkbox" id="c-42884595" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884529">parent</a><span>|</span><a href="#42884678">prev</a><span>|</span><a href="#42884607">next</a><span>|</span><label class="collapse" for="c-42884595">[-]</label><label class="expand" for="c-42884595">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true, and interpretability is helpful for AI safety.</div><br/><div id="42884686" class="c"><input type="checkbox" id="c-42884686" checked=""/><div class="controls bullet"><span class="by">mayukhdeb</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884595">parent</a><span>|</span><a href="#42884607">next</a><span>|</span><label class="collapse" for="c-42884686">[-]</label><label class="expand" for="c-42884686">[4 more]</label></div><br/><div class="children"><div class="content">Indeed. What&#x27;s cool is that we were able to localize literal &quot;regions&quot; in the GPTs which encoded toxic concepts related to racism, politics, etc. A similar video can be found here: <a href="https:&#x2F;&#x2F;toponets.github.io" rel="nofollow">https:&#x2F;&#x2F;toponets.github.io</a><p>More work is being done on this as we speak.</div><br/><div id="42884897" class="c"><input type="checkbox" id="c-42884897" checked=""/><div class="controls bullet"><span class="by">fakeparmesean</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884686">parent</a><span>|</span><a href="#42884724">next</a><span>|</span><label class="collapse" for="c-42884897">[-]</label><label class="expand" for="c-42884897">[1 more]</label></div><br/><div class="children"><div class="content">My understanding coming from mechanistic interpretability is that models are typically (or always) in superposition, meaning that most or all neurons are forced to encode semantically unrelated concepts because there are more concepts than neurons in a typical LM. We train SAEs (where we apply L1 reg and a sparsity penalty to “encourage” the encoder output latents to yield sparse representations of the originating raw activations), to hopefully disentangle these features, or make them more monosemantic.This allows us to use the SAE as a sort of microscope to see what’s going on in the LM, and apply techniques like activation patching to localize features of interest, which sounds similar to what you’ve described. I’m curious what this work means for mech interp. Is this a novel alternative to mitigating polysemanticity? Or perhaps neurons are still encoding multiple features, but the features tend to have greater semantical overlap? Fascinating stuff!</div><br/></div></div><div id="42884724" class="c"><input type="checkbox" id="c-42884724" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884686">parent</a><span>|</span><a href="#42884897">prev</a><span>|</span><a href="#42884607">next</a><span>|</span><label class="collapse" for="c-42884724">[-]</label><label class="expand" for="c-42884724">[2 more]</label></div><br/><div class="children"><div class="content">Was it toxicity though as understood by the model, or just a cluster of concepts that you&#x27;ve chosen to label as toxic?<p>I.e., is this something that could (and therefore, will) be turned towards identifying toxic concepts as understood by the chinese or us government, or to identify (say) pro-union concepts so they can be down-weighted in a released model, etc?</div><br/><div id="42884760" class="c"><input type="checkbox" id="c-42884760" checked=""/><div class="controls bullet"><span class="by">mayukhdeb</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884724">parent</a><span>|</span><a href="#42884607">next</a><span>|</span><label class="collapse" for="c-42884760">[-]</label><label class="expand" for="c-42884760">[1 more]</label></div><br/><div class="children"><div class="content">We localized &quot;toxic&quot; neurons by contrasting the activations of each neuron for toxic v&#x2F;s normal texts. It&#x27;s a method inspired by old-school neuroscience.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42884607" class="c"><input type="checkbox" id="c-42884607" checked=""/><div class="controls bullet"><span class="by">mercer</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42884529">prev</a><span>|</span><a href="#42884674">next</a><span>|</span><label class="collapse" for="c-42884607">[-]</label><label class="expand" for="c-42884607">[1 more]</label></div><br/><div class="children"><div class="content">I imagine it could be easier to make sense of the &#x27;biological&#x27; patterns that way? like, having bottlenecks or spatially-related challenges might have to be simulated too, to make sense of the ingested &#x27;biological&#x27; information.</div><br/></div></div><div id="42884674" class="c"><input type="checkbox" id="c-42884674" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42884607">prev</a><span>|</span><a href="#42884575">next</a><span>|</span><label class="collapse" for="c-42884674">[-]</label><label class="expand" for="c-42884674">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps they are more easily compressible? Once a bunch of nearby weights have similar roles one may not need all of them.</div><br/><div id="42884744" class="c"><input type="checkbox" id="c-42884744" checked=""/><div class="controls bullet"><span class="by">mayukhdeb</span><span>|</span><a href="#42884509">root</a><span>|</span><a href="#42884674">parent</a><span>|</span><a href="#42884575">next</a><span>|</span><label class="collapse" for="c-42884744">[-]</label><label class="expand" for="c-42884744">[1 more]</label></div><br/><div class="children"><div class="content">Yep. That is exactly the idea here. Our compression method is super duper naive. We literally keep every n-th weight column and discard the rest. Turns out that even after getting rid of 80% of the weight columns in this way, we were able to retain the same performance in a 125M GPT.</div><br/></div></div></div></div><div id="42884575" class="c"><input type="checkbox" id="c-42884575" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42884509">parent</a><span>|</span><a href="#42884674">prev</a><span>|</span><a href="#42884956">next</a><span>|</span><label class="collapse" for="c-42884575">[-]</label><label class="expand" for="c-42884575">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The main reason topography emerges in physical brains is because spatially distant connections are physically difficult and expensive in biological systems.<p>The brain itself seems to have bottlenecks that aren&#x27;t distance related, like hemispheres and the corpus callosum that are preserved over all placental mammals and other mammalian groups have something similar and still hemispheres.  Maybe it&#x27;s just an artifact of bilateral symmetry that is stuck in there from path dependence, or forcing a redundancy  to make damage more recoverable, but maybe it has a big regularizing or alternatively specializing effect (regularization like dropout tends to force more distributed representations which seems kind of opposite to this work and other work like &quot;Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability,&quot; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.08746" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.08746</a> ).</div><br/></div></div></div></div><div id="42884874" class="c"><input type="checkbox" id="c-42884874" checked=""/><div class="controls bullet"><span class="by">FrereKhan</span><span>|</span><a href="#42884509">prev</a><span>|</span><a href="#42885551">next</a><span>|</span><label class="collapse" for="c-42884874">[-]</label><label class="expand" for="c-42884874">[1 more]</label></div><br/><div class="children"><div class="content">This paper imports an arbitrarily-chosen aspect of cortical architecture — topological maps of function — and ignores every other aspect of biological neural tissue. The resulting models show lower performance for the same number of parameters — not surprising, since they are more constrained compared with baseline. They may be slightly more robust against pruning — not surprising, since they are more regularised.<p>The figures show individual seeds, presumably, with no statistical analysis in the performance or pruning comparisons, so the null hypothesis is there is no difference between toponets and baseline. I would never let this paper be submitted by my team.<p>We haven&#x27;t learned anything about the brain, or about ANNs.</div><br/></div></div><div id="42885551" class="c"><input type="checkbox" id="c-42885551" checked=""/><div class="controls bullet"><span class="by">igleria</span><span>|</span><a href="#42884874">prev</a><span>|</span><a href="#42884504">next</a><span>|</span><label class="collapse" for="c-42885551">[-]</label><label class="expand" for="c-42885551">[2 more]</label></div><br/><div class="children"><div class="content">This is excellent. Since reading <a href="https:&#x2F;&#x2F;books.google.de&#x2F;books&#x2F;about&#x2F;Models_of_the_Mind.html?id=4NqPEAAAQBAJ&amp;redir_esc=y" rel="nofollow">https:&#x2F;&#x2F;books.google.de&#x2F;books&#x2F;about&#x2F;Models_of_the_Mind.html?...</a> I&#x27;ve been expecting someone to start looking back into biology to try to move forward. I guess the poster is one of the authors. Kudos!</div><br/></div></div><div id="42884504" class="c"><input type="checkbox" id="c-42884504" checked=""/><div class="controls bullet"><span class="by">slama</span><span>|</span><a href="#42885551">prev</a><span>|</span><a href="#42884608">next</a><span>|</span><label class="collapse" for="c-42884504">[-]</label><label class="expand" for="c-42884504">[2 more]</label></div><br/><div class="children"><div class="content">The title here doesn&#x27;t seem to match. The paper is called &quot;TopoNets: High Performing Vision and Language Models with Brain-Like Topography&quot;<p>Even with their new method, models with topography seem to perform worse than models without.</div><br/><div id="42884830" class="c"><input type="checkbox" id="c-42884830" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#42884504">parent</a><span>|</span><a href="#42884608">next</a><span>|</span><label class="collapse" for="c-42884830">[-]</label><label class="expand" for="c-42884830">[1 more]</label></div><br/><div class="children"><div class="content">Submitted title was &quot;Inducing brain-like structure in GPT&#x27;s weights makes them parameter efficient&quot;. We&#x27;ve reverted it now in keeping with the site guidelines (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html</a>).<p>Since the submitter appears to be one of the authors, maybe they can explain the connection between the two titles? (Or maybe they already have! I haven&#x27;t read the entire thread)</div><br/></div></div></div></div><div id="42884608" class="c"><input type="checkbox" id="c-42884608" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42884504">prev</a><span>|</span><a href="#42884476">next</a><span>|</span><label class="collapse" for="c-42884608">[-]</label><label class="expand" for="c-42884608">[2 more]</label></div><br/><div class="children"><div class="content">I hate to dog on research papers. They’re work to write. That said, I think this paper is not likely to be of interest to AI researchers — instead it may be of interest to Neuroscience folks or other brain research types.<p>The lede — adding topography worsens networks at similar weights — is not only buried, it’s obscured with statements claiming that topo networks show less upheaval when scaled down, e.g. they are more efficient than similar weight networks.<p>It’s hard for me to see how both these things can be true — the graphs show the more topography is added, the worse the networks perform at the trained model sizes.<p>To have the second statement “They compress better and are therefore more efficient” also be true, I think you’d need to show a pretty remarkable claim, which is that while a model trained <i>at the same scale</i> as a llama architecture is worse, when you scale them both <i>down</i>, this model becomes not only better than the scaled down llama, but also better than a natively trained model at the new smaller scale.<p>There is no proof of this in the paper, and good reason to be skeptical of this idea based on the data presented.<p>That said, like a lot of ideas in AI, this .. works! You can train a model successfully imposing these outside structures on it, and that model doesn’t even suck very much. Which is a cool statement about complexity theory and the resilience of these architectures, in my opinion. But I don’t think it says much else about either the brain or underlying AI ‘truths’.</div><br/></div></div><div id="42884476" class="c"><input type="checkbox" id="c-42884476" checked=""/><div class="controls bullet"><span class="by">LZ_Khan</span><span>|</span><a href="#42884608">prev</a><span>|</span><a href="#42884806">next</a><span>|</span><label class="collapse" for="c-42884476">[-]</label><label class="expand" for="c-42884476">[3 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t there be a comparison in performance on common benchmarks to other models?<p>Like a 7B toponet model vs a 7B Llama model?<p>As a layperson I don&#x27;t understand why topology is a thing to optimize for.</div><br/><div id="42884530" class="c"><input type="checkbox" id="c-42884530" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#42884476">parent</a><span>|</span><a href="#42884806">next</a><span>|</span><label class="collapse" for="c-42884530">[-]</label><label class="expand" for="c-42884530">[2 more]</label></div><br/><div class="children"><div class="content">The only potential benefit shown in the paper is the topologically local models seem to be more resilient after pruning.<p>So you may be able to prune a 7B model down to 6B while maintaining most of the capability.</div><br/><div id="42884663" class="c"><input type="checkbox" id="c-42884663" checked=""/><div class="controls bullet"><span class="by">mayukhdeb</span><span>|</span><a href="#42884476">root</a><span>|</span><a href="#42884530">parent</a><span>|</span><a href="#42884806">next</a><span>|</span><label class="collapse" for="c-42884663">[-]</label><label class="expand" for="c-42884663">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The only potential benefit<p>Other benefits:<p>1. Significantly lower dimensionality of internal representations
2. More interpretable (see: <a href="https:&#x2F;&#x2F;toponets.github.io" rel="nofollow">https:&#x2F;&#x2F;toponets.github.io</a>)<p>&gt; 7B model down to 6B<p>We remove ~80% of the parameters in topographic layers and retain the same performance in the model. The drop in parameter count is not significant because we did not experiment with applying TopoLoss in all of the layers of the model (did not align with the goal of the paper)<p>We are currently performing those strong sparsity experiments internally, and the results look very promising!</div><br/></div></div></div></div></div></div><div id="42884806" class="c"><input type="checkbox" id="c-42884806" checked=""/><div class="controls bullet"><span class="by">michalsustr</span><span>|</span><a href="#42884476">prev</a><span>|</span><a href="#42885081">next</a><span>|</span><label class="collapse" for="c-42884806">[-]</label><label class="expand" for="c-42884806">[1 more]</label></div><br/><div class="children"><div class="content">The blurring in the sheets and the topo loss reminded me of <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.05446" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.05446</a></div><br/></div></div><div id="42885081" class="c"><input type="checkbox" id="c-42885081" checked=""/><div class="controls bullet"><span class="by">devmor</span><span>|</span><a href="#42884806">prev</a><span>|</span><a href="#42884566">next</a><span>|</span><label class="collapse" for="c-42885081">[-]</label><label class="expand" for="c-42885081">[1 more]</label></div><br/><div class="children"><div class="content">Is this &quot;brain-like&quot; in any functional way, or &quot;brain-like&quot; in the same way that a tall rectangle is &quot;door-like&quot; even if it doesn&#x27;t share any functions with a door?<p>I know quite a bit about machine learning, but very little to nothing about neuroscience and human cognition, so I am curious how an expert (that didn&#x27;t work on the paper) would describe it.<p>(Forgive me for the pre-emptive negativity but I am so utterly exhausted by dishonest comparisons to sapient thought in the field of artificial intelligence that it has nearly drained me of the incredible amount of enthusiasm I used to carry for it.)</div><br/></div></div><div id="42884566" class="c"><input type="checkbox" id="c-42884566" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#42885081">prev</a><span>|</span><a href="#42884498">next</a><span>|</span><label class="collapse" for="c-42884566">[-]</label><label class="expand" for="c-42884566">[2 more]</label></div><br/><div class="children"><div class="content">They bury the part where inducing brain like structure hurts performance!<p>This is a method to just hurt your network in exchange for nothing useful at all aside from some sketchy story that this is &quot;brain like&quot;.</div><br/><div id="42884640" class="c"><input type="checkbox" id="c-42884640" checked=""/><div class="controls bullet"><span class="by">mayukhdeb</span><span>|</span><a href="#42884566">parent</a><span>|</span><a href="#42884498">next</a><span>|</span><label class="collapse" for="c-42884640">[-]</label><label class="expand" for="c-42884640">[1 more]</label></div><br/><div class="children"><div class="content">Our goal was never to optimize for performance. There&#x27;s a long standing hypothesis that topographic structure in the human brain leads to metabolic efficiency. Thanks to topography in ANNs, we were able to test out this hypothesis in a computational setting.<p>&gt; sketchy story this is &quot;brain like&quot;.<p>we reproduce the hallmarks of functional organization seen in the visual and language cortex of the brain. I encourage you to read the paper before making such comments</div><br/></div></div></div></div></div></div></div></div></div></body></html>