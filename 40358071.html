<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715763670889" as="style"/><link rel="stylesheet" href="styles.css?v=1715763670889"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://deepmind.google/technologies/gemini/flash/">Gemini Flash</a>Â <span class="domain">(<a href="https://deepmind.google">deepmind.google</a>)</span></div><div class="subtext"><span>meetpateltech</span> | <span>119 comments</span></div><br/><div><div id="40360397" class="c"><input type="checkbox" id="c-40360397" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40358661">next</a><span>|</span><label class="collapse" for="c-40360397">[-]</label><label class="expand" for="c-40360397">[1 more]</label></div><br/><div class="children"><div class="content">I upgraded my llm-gemini plugin to provide CLI access to Gemini Flash:<p><pre><code>    pipx install llm # or brew install llm
    llm install llm-gemini --upgrade
    llm keys set gemini
    # paste API key here
    llm -m gemini-1.5-flash-latest &#x27;a short poem about otters&#x27;
</code></pre>
<a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-gemini&#x2F;releases&#x2F;tag&#x2F;0.1a4">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-gemini&#x2F;releases&#x2F;tag&#x2F;0.1a4</a></div><br/></div></div><div id="40358661" class="c"><input type="checkbox" id="c-40358661" checked=""/><div class="controls bullet"><span class="by">xianshou</span><span>|</span><a href="#40360397">prev</a><span>|</span><a href="#40358412">next</a><span>|</span><label class="collapse" for="c-40358661">[-]</label><label class="expand" for="c-40358661">[26 more]</label></div><br/><div class="children"><div class="content">Looking at MMLU and other benchmarks, this essentially means sub-second first-token latency with Llama 3 70B quality (but not GPT-4 &#x2F; Opus), native multimodality, and 1M context.<p>Not bad compared to rolling your own, but among frontier models the main competitive differentiator was native multimodality. With the release of GPT-4o I&#x27;m not clear on why an organization not bound to GCP would pick Gemini. 128k context (4o) is fine unless you&#x27;re processing whole books&#x2F;movies at once. Is anyone doing this at scale in a way that can&#x27;t be filtered down from 1M to 100k?</div><br/><div id="40358746" class="c"><input type="checkbox" id="c-40358746" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40359782">next</a><span>|</span><label class="collapse" for="c-40358746">[-]</label><label class="expand" for="c-40358746">[16 more]</label></div><br/><div class="children"><div class="content">With 1M tokens you can dump 2000 pages of documents into the context windows before starting a chat.<p>Gemini&#x27;s strength isn&#x27;t in being able to answer logic puzzles, it&#x27;s strength is in its context length. Studying for an exam? Just put the entire textbook in the chat. Need to use a dead language for an old test system with no information on the internet? Drop the 1300 page reference manual in and ask away.</div><br/><div id="40358907" class="c"><input type="checkbox" id="c-40358907" checked=""/><div class="controls bullet"><span class="by">ianbicking</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40358746">parent</a><span>|</span><a href="#40360605">next</a><span>|</span><label class="collapse" for="c-40358907">[-]</label><label class="expand" for="c-40358907">[12 more]</label></div><br/><div class="children"><div class="content">How much do those input tokens cost?<p>According to <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;pricing" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;pricing</a> it&#x27;s $0.70&#x2F;million input tokens (for a long context). That will be per-exchange, so every little back and forth will cost around that much (if you&#x27;re using a substantial portion of the context window).<p>And while I haven&#x27;t tested Gemini, most LLMs get increasingly wonky as the context goes up, more likely to fixate, more likely to forget instructions.<p>That big context window could definitely be great for certain tasks (especially information extraction), but it doesn&#x27;t feel like a generally useful feature.</div><br/><div id="40359349" class="c"><input type="checkbox" id="c-40359349" checked=""/><div class="controls bullet"><span class="by">mcbuilder</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40358907">parent</a><span>|</span><a href="#40359339">next</a><span>|</span><label class="collapse" for="c-40359349">[-]</label><label class="expand" for="c-40359349">[5 more]</label></div><br/><div class="children"><div class="content">That per exchange context cost is what really puts me off using cloud LLM for anything serious. I know batching and everything is needed in the data center, and important for keeping around KVQ cache, you basically need to fully take over machine to get an interactive session to get the context costs to scale with sequence length. So it&#x27;s useful, but more in the case of a local LLaMA type situation if you want a conversation.</div><br/><div id="40360210" class="c"><input type="checkbox" id="c-40360210" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40359349">parent</a><span>|</span><a href="#40359339">next</a><span>|</span><label class="collapse" for="c-40360210">[-]</label><label class="expand" for="c-40360210">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if we could implement the equivalent of a JIT compilation, whereby context sequences which get repeatedly reused would be used for an online fine-tuning.</div><br/><div id="40362561" class="c"><input type="checkbox" id="c-40362561" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40360210">parent</a><span>|</span><a href="#40362360">next</a><span>|</span><label class="collapse" for="c-40362561">[-]</label><label class="expand" for="c-40362561">[1 more]</label></div><br/><div class="children"><div class="content">No, but you can just cache the state after processing the prompt. <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;main#prompt-caching">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;...</a></div><br/></div></div></div></div></div></div><div id="40359339" class="c"><input type="checkbox" id="c-40359339" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40358907">parent</a><span>|</span><a href="#40359349">prev</a><span>|</span><a href="#40360046">next</a><span>|</span><label class="collapse" for="c-40359339">[-]</label><label class="expand" for="c-40359339">[5 more]</label></div><br/><div class="children"><div class="content">Is there a way to amortize that cost over several queries, i.e. &quot;pre-bake&quot; a document into a context persisted in some form to allow cheaper follow-up queries about it?</div><br/><div id="40360096" class="c"><input type="checkbox" id="c-40360096" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40359339">parent</a><span>|</span><a href="#40359449">next</a><span>|</span><label class="collapse" for="c-40360096">[-]</label><label class="expand" for="c-40360096">[2 more]</label></div><br/><div class="children"><div class="content">They announced that today, calling it &quot;context caching&quot; - but it looks like it&#x27;s only going to be available for Gemini Pro 1.5, not for Gemini Flash.<p>It reduces prompt costs by half for those shared prefix tokens, but you have to pay $4.50&#x2F;million tokens&#x2F;hour to keep that cache warm - so probably not a useful optimization for most lower traffic applications.<p><a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;caching" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;caching</a></div><br/><div id="40360211" class="c"><input type="checkbox" id="c-40360211" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40360096">parent</a><span>|</span><a href="#40359449">next</a><span>|</span><label class="collapse" for="c-40360211">[-]</label><label class="expand" for="c-40360211">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It reduces prompt costs by half for those shared prefix tokens, but you have to pay $4.50&#x2F;million tokens&#x2F;hour to keep that cache warm - so probably not a useful optimization for most lower traffic applications<p>That&#x27;s on a model with $3.5&#x2F;1M input token cost, so half price on cached prefix tokens for $4.5&#x2F;1M&#x2F;hour breaks even at a little over 2.5 requests&#x2F;hour using the cached prefix.</div><br/></div></div></div></div><div id="40359449" class="c"><input type="checkbox" id="c-40359449" checked=""/><div class="controls bullet"><span class="by">inlined</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40359339">parent</a><span>|</span><a href="#40360096">prev</a><span>|</span><a href="#40361029">next</a><span>|</span><label class="collapse" for="c-40359449">[-]</label><label class="expand" for="c-40359449">[1 more]</label></div><br/><div class="children"><div class="content">Though I&#x27;m not familiar with the specifics, they announced &quot;context caching&quot;</div><br/></div></div><div id="40361029" class="c"><input type="checkbox" id="c-40361029" checked=""/><div class="controls bullet"><span class="by">gcanyon</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40359339">parent</a><span>|</span><a href="#40359449">prev</a><span>|</span><a href="#40360046">next</a><span>|</span><label class="collapse" for="c-40361029">[-]</label><label class="expand" for="c-40361029">[1 more]</label></div><br/><div class="children"><div class="content">Depending on the output window limit, the first query could be something like: &quot;Summarize this down to its essential details&quot; -- then use that to feed future queries.<p>Tediously, it would be possible to do this chapter by chapter in order to exceed the output limit building something for future inputs.<p>Of course, the summary might not fulfill the same functionality as the original source document. YMMV</div><br/></div></div></div></div><div id="40360046" class="c"><input type="checkbox" id="c-40360046" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40358907">parent</a><span>|</span><a href="#40359339">prev</a><span>|</span><a href="#40360605">next</a><span>|</span><label class="collapse" for="c-40360046">[-]</label><label class="expand" for="c-40360046">[1 more]</label></div><br/><div class="children"><div class="content">Can anyone speculate on how G arrived at this price, and perhaps how it contrasts with how OAI arrived at its updated pricing? (realizing it can&#x27;t be held up directly to GPT x at the moment)</div><br/></div></div></div></div><div id="40360605" class="c"><input type="checkbox" id="c-40360605" checked=""/><div class="controls bullet"><span class="by">tk90</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40358746">parent</a><span>|</span><a href="#40358907">prev</a><span>|</span><a href="#40358854">next</a><span>|</span><label class="collapse" for="c-40360605">[-]</label><label class="expand" for="c-40360605">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t there retrieval degradation with such a large context size? I would still think that a RAG system on 128K is still better than No Rag + 1M context window, no? (assuming text only)</div><br/></div></div><div id="40358854" class="c"><input type="checkbox" id="c-40358854" checked=""/><div class="controls bullet"><span class="by">tulip4attoo</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40358746">parent</a><span>|</span><a href="#40360605">prev</a><span>|</span><a href="#40359782">next</a><span>|</span><label class="collapse" for="c-40358854">[-]</label><label class="expand" for="c-40358854">[2 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t really use it, right? There&#x27;s no way to debug if you&#x27;re doing it like this. Also, the accuracy isn&#x27;t high, and it can&#x27;t answer complicated questions, making it quite useless for the cost.</div><br/><div id="40359166" class="c"><input type="checkbox" id="c-40359166" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40358854">parent</a><span>|</span><a href="#40359782">next</a><span>|</span><label class="collapse" for="c-40359166">[-]</label><label class="expand" for="c-40359166">[1 more]</label></div><br/><div class="children"><div class="content">Please make your substantive points without crossing into personal attack. Your comment would be fine without the first sentence.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html</a></div><br/></div></div></div></div></div></div><div id="40359782" class="c"><input type="checkbox" id="c-40359782" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40358746">prev</a><span>|</span><a href="#40358897">next</a><span>|</span><label class="collapse" for="c-40359782">[-]</label><label class="expand" for="c-40359782">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no way it&#x27;s Llama 3 70b quality.<p>I&#x27;ve been trying to work Gemini 1.5 Pro into our workstream for all kinds of stuff and it is so bad. Unbelievable amount of hallucinations, especially when you introduce video or audio.<p>I&#x27;m not sure I can think of a single use case where a high hallucination tiny multimodal model is practical in most businesses. Without reliability it&#x27;s just a toy.</div><br/><div id="40360163" class="c"><input type="checkbox" id="c-40360163" checked=""/><div class="controls bullet"><span class="by">dibujaron</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40359782">parent</a><span>|</span><a href="#40358897">next</a><span>|</span><label class="collapse" for="c-40360163">[-]</label><label class="expand" for="c-40360163">[1 more]</label></div><br/><div class="children"><div class="content">Seconding this. Gemini 1.5 is comically bad at basic tasks that GPT4 breezes through, not to mention GPT4o.</div><br/></div></div></div></div><div id="40358897" class="c"><input type="checkbox" id="c-40358897" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40359782">prev</a><span>|</span><a href="#40359507">next</a><span>|</span><label class="collapse" for="c-40358897">[-]</label><label class="expand" for="c-40358897">[1 more]</label></div><br/><div class="children"><div class="content">&gt; With the release of GPT-4o I&#x27;m not clear on why an organization not bound to GCP would pick Gemini.<p>Price for anything, particularly multimodal tasks that with OpenAI GPT-4o is the cheapest model, that doesn&#x27;t need GPT-4 quality. GPT-3.5-Turbo â which itself is 1&#x2F;10 the cost of GPT-4o, is $0.5&#x2F;1M tokens on input, $1.50&#x2F;1M on output, with a 16K context window. Gemini 1.5 Flash, for prompts up to 128K, is $0.35&#x2F;1M tokens on input, and $0.53&#x2F;1M tokens on output.<p>For tasks that require multimodality but not GPT-4 smarts (which I think includes a lot of document-processing tasks, for which GPT-4 with Vision and now GPT-4 are magical but pricy), Gemini Flash looks like close to a 95% price cut.</div><br/></div></div><div id="40359507" class="c"><input type="checkbox" id="c-40359507" checked=""/><div class="controls bullet"><span class="by">thefourthchime</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40358897">prev</a><span>|</span><a href="#40360256">next</a><span>|</span><label class="collapse" for="c-40359507">[-]</label><label class="expand" for="c-40359507">[1 more]</label></div><br/><div class="children"><div class="content">I tried to use the 1M tokens with Gemini a couple of months ago. It either crashed or responded ___very__ slowly and then crashed.<p>I tried a half dozen times and gave up, I hope this one is faster and more stable.</div><br/></div></div><div id="40360256" class="c"><input type="checkbox" id="c-40360256" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40359507">prev</a><span>|</span><a href="#40359368">next</a><span>|</span><label class="collapse" for="c-40360256">[-]</label><label class="expand" for="c-40360256">[1 more]</label></div><br/><div class="children"><div class="content">I guess it depends on what you want to do.<p>E.g. I want to send an entire code base in a context. It might not fit into 128k.<p>Filtering down is a complex task by itself. It&#x27;s much easier to call a single API.<p>Regarding quality of responses, I&#x27;ve seen both disappointing and brilliant responses from Gemini. Do maybe worth trying. But it will probably take several iterations until it can be relied upon.</div><br/></div></div><div id="40359368" class="c"><input type="checkbox" id="c-40359368" checked=""/><div class="controls bullet"><span class="by">mupuff1234</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40360256">prev</a><span>|</span><a href="#40360761">next</a><span>|</span><label class="collapse" for="c-40359368">[-]</label><label class="expand" for="c-40359368">[2 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s a bit like asking why would someone need a 1gb Gmail when 50mb yahoo account is clearly enough.<p>It means you can dump context without thinking about it twice and without needing to hack some solutions to deal with context overflow etc.<p>And given that most use cases most likely deal with text and not multimodal the advantage seems pretty clear imo.</div><br/><div id="40359718" class="c"><input type="checkbox" id="c-40359718" checked=""/><div class="controls bullet"><span class="by">tedsanders</span><span>|</span><a href="#40358661">root</a><span>|</span><a href="#40359368">parent</a><span>|</span><a href="#40360761">next</a><span>|</span><label class="collapse" for="c-40359718">[-]</label><label class="expand" for="c-40359718">[1 more]</label></div><br/><div class="children"><div class="content">Long context is a little bit different than extra email storage. Having 1 gb of storage instead of 50 mb has essentially no downside to the user experience.<p>But submitting 1M input tokens instead of 100k input tokens:<p>- Causes your costs to go up ~10x<p>- Causes your latency to go up ~10x (or between 1x and 10x)<p>- Can result in worse answers (especially if the model gets distracted by irrelevant info)<p>So longer context is great, yes, but it&#x27;s not a no-brainer like more email storage. It brings costs. And whether those costs are worth it depends on what you&#x27;re doing.</div><br/></div></div></div></div><div id="40360761" class="c"><input type="checkbox" id="c-40360761" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40359368">prev</a><span>|</span><a href="#40360174">next</a><span>|</span><label class="collapse" for="c-40360761">[-]</label><label class="expand" for="c-40360761">[1 more]</label></div><br/><div class="children"><div class="content">GPT-3.5 has 0.5s average first-token latency and Claude3 Haiku 0.4s.</div><br/></div></div><div id="40360174" class="c"><input type="checkbox" id="c-40360174" checked=""/><div class="controls bullet"><span class="by">chimney</span><span>|</span><a href="#40358661">parent</a><span>|</span><a href="#40360761">prev</a><span>|</span><a href="#40358412">next</a><span>|</span><label class="collapse" for="c-40360174">[-]</label><label class="expand" for="c-40360174">[1 more]</label></div><br/><div class="children"><div class="content">Price.</div><br/></div></div></div></div><div id="40358412" class="c"><input type="checkbox" id="c-40358412" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#40358661">prev</a><span>|</span><a href="#40359531">next</a><span>|</span><label class="collapse" for="c-40358412">[-]</label><label class="expand" for="c-40358412">[21 more]</label></div><br/><div class="children"><div class="content">1M token context by default is the big feature here IMO, but we need better benchmarks to measure what that really means.<p>My intuition is that as contexts get longer we start hitting the limits of how much comprehension can be embedded in a single point of vector space, and will need better architectures for selecting the relevant portions of the context.</div><br/><div id="40364521" class="c"><input type="checkbox" id="c-40364521" checked=""/><div class="controls bullet"><span class="by">shoelessone</span><span>|</span><a href="#40358412">parent</a><span>|</span><a href="#40358996">next</a><span>|</span><label class="collapse" for="c-40364521">[-]</label><label class="expand" for="c-40364521">[1 more]</label></div><br/><div class="children"><div class="content">&gt; My intuition is that as contexts get longer we start hitting the limits of how much comprehension can be embedded in a single point of vector space, and will need better architectures for selecting the relevant portions of the context.<p>Is it possible to explain what this means in a way that somebody only roughly familiar with vectors and vector databases? Or recommend an article or further reading on the topic?</div><br/></div></div><div id="40358996" class="c"><input type="checkbox" id="c-40358996" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40358412">parent</a><span>|</span><a href="#40364521">prev</a><span>|</span><a href="#40362216">next</a><span>|</span><label class="collapse" for="c-40358996">[-]</label><label class="expand" for="c-40358996">[16 more]</label></div><br/><div class="children"><div class="content">&gt; 1M token context by default is the big feature here IMO, but we need better benchmarks to measure what that really means.<p>Multimodality in a model That&#x27;s between 4-7% the cost per token of OpenAIâs cheapest multimodal model is an important feature when you are talking about production use and not just economically unsustainable demos.</div><br/><div id="40359766" class="c"><input type="checkbox" id="c-40359766" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#40358412">root</a><span>|</span><a href="#40358996">parent</a><span>|</span><a href="#40359100">next</a><span>|</span><label class="collapse" for="c-40359766">[-]</label><label class="expand" for="c-40359766">[1 more]</label></div><br/><div class="children"><div class="content">The problem is that even 1.5 Pro seems completely useless for long context multimodal stuff.<p>I have tried it for so many use cases in video &#x2F; audio and it hallucinates an unbelievable amount. More than any other model I&#x27;ve ever used.<p>So if 1.5 Pro can&#x27;t even handle simple tasks without hallucination, I imagine this tiny model is even more useless.</div><br/></div></div><div id="40359100" class="c"><input type="checkbox" id="c-40359100" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40358412">root</a><span>|</span><a href="#40358996">parent</a><span>|</span><a href="#40359766">prev</a><span>|</span><a href="#40362216">next</a><span>|</span><label class="collapse" for="c-40359100">[-]</label><label class="expand" for="c-40359100">[14 more]</label></div><br/><div class="children"><div class="content">In preview, can&#x27;t be used in production, they already rug-pulled people building on Gemini w&#x2F;r&#x2F;t cost and RPM, and they&#x27;re pointedly not putting any RPM or cost on the page. (seriously, try finding info on cost, RPM, or release right now, you&#x27;re linked in circles.)<p>Agree on OpenAI multimodal but it&#x27;s sort of a stilted example itself, it&#x27;s because OpenAI has a hole in its lineup - ex. Claude Haiku is multimodal, faster, and significantly cheaper than GPT 3.5.</div><br/><div id="40359154" class="c"><input type="checkbox" id="c-40359154" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40358412">root</a><span>|</span><a href="#40359100">parent</a><span>|</span><a href="#40359168">next</a><span>|</span><label class="collapse" for="c-40359154">[-]</label><label class="expand" for="c-40359154">[2 more]</label></div><br/><div class="children"><div class="content">&gt; they&#x27;re pointedly not putting any RPM or cost on the page<p>360 RPM base limit, pricing is posted.<p>&gt; seriously, try finding info on cost, RPM, or release right now,<p>I wasn&#x27;t making up numbers, its on their Gemini API pricing page: <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;pricing" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;pricing</a></div><br/><div id="40359244" class="c"><input type="checkbox" id="c-40359244" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40358412">root</a><span>|</span><a href="#40359154">parent</a><span>|</span><a href="#40359168">next</a><span>|</span><label class="collapse" for="c-40359244">[-]</label><label class="expand" for="c-40359244">[1 more]</label></div><br/><div class="children"><div class="content">Nice, thanks (btw, I didn&#x27;t think you were making it up, it was in the keynote!)</div><br/></div></div></div></div><div id="40359168" class="c"><input type="checkbox" id="c-40359168" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#40358412">root</a><span>|</span><a href="#40359100">parent</a><span>|</span><a href="#40359154">prev</a><span>|</span><a href="#40362216">next</a><span>|</span><label class="collapse" for="c-40359168">[-]</label><label class="expand" for="c-40359168">[11 more]</label></div><br/><div class="children"><div class="content">+1 on Haiku being oft overlooked.</div><br/><div id="40359259" class="c"><input type="checkbox" id="c-40359259" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40358412">root</a><span>|</span><a href="#40359168">parent</a><span>|</span><a href="#40362216">next</a><span>|</span><label class="collapse" for="c-40359259">[-]</label><label class="expand" for="c-40359259">[10 more]</label></div><br/><div class="children"><div class="content">Shows the power of the brand and the limit of names consumers will recall long term<p>&quot;Who are the biggest soda or potato chip makers?&quot;</div><br/></div></div></div></div></div></div></div></div><div id="40362216" class="c"><input type="checkbox" id="c-40362216" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#40358412">parent</a><span>|</span><a href="#40358996">prev</a><span>|</span><a href="#40359040">next</a><span>|</span><label class="collapse" for="c-40362216">[-]</label><label class="expand" for="c-40362216">[2 more]</label></div><br/><div class="children"><div class="content">Limitations of single point in vector space of what dimension?<p>Iâm not sure itâs public knowledge, but itâs an architecture choice. They choose how big to make the embedding dimension.<p>My point is just that thereâs no limitation in principle, itâs just a matter of how they design it and resource constraints.</div><br/><div id="40362682" class="c"><input type="checkbox" id="c-40362682" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#40358412">root</a><span>|</span><a href="#40362216">parent</a><span>|</span><a href="#40359040">next</a><span>|</span><label class="collapse" for="c-40362682">[-]</label><label class="expand" for="c-40362682">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for responding to that point - it&#x27;s the one most on my mind.<p>So OpenAI&#x27;s large embedding model has 3072 dimensions, though in practice far fewer are probably used. Clearly you can&#x27;t compress 1M tokens down to 3072. Yet those 3072 numbers are all you&#x27;ve got for capturing the full meaning of the previous token when predicting the next one; including all 1M tokens of modifying context.<p>So perhaps human language is simply never complex enough to need more than 3072 numbers to represent a given train of thought, but that doesn&#x27;t seem clear to me.<p>Edit: Since Gemini is relevant here, it looks like their text embedding model is 768 dimensions.</div><br/></div></div></div></div><div id="40359040" class="c"><input type="checkbox" id="c-40359040" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40358412">parent</a><span>|</span><a href="#40362216">prev</a><span>|</span><a href="#40359531">next</a><span>|</span><label class="collapse" for="c-40359040">[-]</label><label class="expand" for="c-40359040">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it&#x27;s not very good in practice, you can get a halfway decent demo out of it (&quot;look I gave it 6.5 harry potters and it made an SVG map connecting characters with annotations!!&quot;...some of the characters...spare annotations...cost $20). Just good enough to fool you a couple times when you try to make it work 10 times.</div><br/></div></div></div></div><div id="40359531" class="c"><input type="checkbox" id="c-40359531" checked=""/><div class="controls bullet"><span class="by">nightski</span><span>|</span><a href="#40358412">prev</a><span>|</span><a href="#40360056">next</a><span>|</span><label class="collapse" for="c-40359531">[-]</label><label class="expand" for="c-40359531">[1 more]</label></div><br/><div class="children"><div class="content">A lightweight model that you can only use in the cloud?  That is amusing. These tech megacorps are really intent on owning your usage of AI.  But we must not let that be the future.</div><br/></div></div><div id="40360056" class="c"><input type="checkbox" id="c-40360056" checked=""/><div class="controls bullet"><span class="by">kherud</span><span>|</span><a href="#40359531">prev</a><span>|</span><a href="#40362191">next</a><span>|</span><label class="collapse" for="c-40360056">[-]</label><label class="expand" for="c-40360056">[9 more]</label></div><br/><div class="children"><div class="content">Now that context length seems abundant for most tasks, I&#x27;m wondering why sub-word tokens are still used. I&#x27;m really curious how character-based LLMs would compare. With 2 M context, the compute bottleneck fades away. I&#x27;m not sure though what role the vocabulary size has. Maybe a large size is critical, since the embedding already contains a big chunk of the knowledge. On the other hand, using a character-based vocabulary would solve multiple problems, I think, like glitch tokens and possibly things like arithmetic and rhyming capabilities. Implementing sub-word tokenizers correctly and training them seems also quite complex. On a character level this should be trivial.</div><br/><div id="40360183" class="c"><input type="checkbox" id="c-40360183" checked=""/><div class="controls bullet"><span class="by">AaronFriel</span><span>|</span><a href="#40360056">parent</a><span>|</span><a href="#40360396">next</a><span>|</span><label class="collapse" for="c-40360183">[-]</label><label class="expand" for="c-40360183">[3 more]</label></div><br/><div class="children"><div class="content">The attention mechanism is vastly more efficient to train when it can attend to larger, more meaningful tokens. For inference servers, a significant amount of memory goes into the KV cache, and as you note, to build up the embedding through attention would then require correlating far more tokens, each of which is &quot;less meaningful&quot;.<p>I think we may get to this point eventually, in the limit we will want multimodal LLMs that understand images and sounds down to the pixel and frequency, and it seems like for text, too, we will eventually want that as well.</div><br/><div id="40360912" class="c"><input type="checkbox" id="c-40360912" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#40360056">root</a><span>|</span><a href="#40360183">parent</a><span>|</span><a href="#40360601">next</a><span>|</span><label class="collapse" for="c-40360912">[-]</label><label class="expand" for="c-40360912">[1 more]</label></div><br/><div class="children"><div class="content">Maybe you could just use a good-old 1D-CNN for the bottom 3-4 layers. Then the model has been able to combine characters into roughly token length chunks anyway.<p>Just make sure to have some big MLPs at the start too, to enrich the &quot;tokens&quot; with the information currently stored in the embedding tables.</div><br/></div></div><div id="40360601" class="c"><input type="checkbox" id="c-40360601" checked=""/><div class="controls bullet"><span class="by">yk</span><span>|</span><a href="#40360056">root</a><span>|</span><a href="#40360183">parent</a><span>|</span><a href="#40360912">prev</a><span>|</span><a href="#40360396">next</a><span>|</span><label class="collapse" for="c-40360601">[-]</label><label class="expand" for="c-40360601">[1 more]</label></div><br/><div class="children"><div class="content">&gt; a significant amount of memory goes into the KV cache<p>Is there a good paper (or talk) how inference looks at scale? (Kinda like ELI-using-single-gpus)</div><br/></div></div></div></div><div id="40360396" class="c"><input type="checkbox" id="c-40360396" checked=""/><div class="controls bullet"><span class="by">darby_eight</span><span>|</span><a href="#40360056">parent</a><span>|</span><a href="#40360183">prev</a><span>|</span><a href="#40360236">next</a><span>|</span><label class="collapse" for="c-40360396">[-]</label><label class="expand" for="c-40360396">[3 more]</label></div><br/><div class="children"><div class="content">&gt; On a character level this should be trivial.<p>Characters are not the semantic components of wordsâthese are syllables. Generally speaking, anyway. I&#x27;ve got to imagine this approach would yield higher quality results than the roman alphabet. I&#x27;m curious if this could be tested by just looking at how LLMs handle English vs Chinese.</div><br/><div id="40360614" class="c"><input type="checkbox" id="c-40360614" checked=""/><div class="controls bullet"><span class="by">inbetween</span><span>|</span><a href="#40360056">root</a><span>|</span><a href="#40360396">parent</a><span>|</span><a href="#40360236">next</a><span>|</span><label class="collapse" for="c-40360614">[-]</label><label class="expand" for="c-40360614">[2 more]</label></div><br/><div class="children"><div class="content">The minimal semantic parts of words are morphemes. Syllables are phonological units (roughly: the minimal unit for rhythmic purposes such as stress, etc)</div><br/><div id="40360770" class="c"><input type="checkbox" id="c-40360770" checked=""/><div class="controls bullet"><span class="by">darby_eight</span><span>|</span><a href="#40360056">root</a><span>|</span><a href="#40360614">parent</a><span>|</span><a href="#40360236">next</a><span>|</span><label class="collapse" for="c-40360770">[-]</label><label class="expand" for="c-40360770">[1 more]</label></div><br/><div class="children"><div class="content">Only in languages that have morphemes! This is hardly a universal attribute of language so much as an attribute of those that use an alphabet to encode sounds. It makes more sense to just bypass the encoding and directly consider the speech.<p>Besides, considering morphemes as semantic often results in a completely different meaning than we actually intend. We aren&#x27;t trying to train a chatbot to speak in prefixes and suffixes, we&#x27;re trying to train a chatbot to speak in natural language, even if it is encoded to latin script before output.</div><br/></div></div></div></div></div></div><div id="40360236" class="c"><input type="checkbox" id="c-40360236" checked=""/><div class="controls bullet"><span class="by">joaogui1</span><span>|</span><a href="#40360056">parent</a><span>|</span><a href="#40360396">prev</a><span>|</span><a href="#40360242">next</a><span>|</span><label class="collapse" for="c-40360236">[-]</label><label class="expand" for="c-40360236">[1 more]</label></div><br/><div class="children"><div class="content">I would say 2 big problems are:<p>1. latency, which would get worse if you have to sequentially generate more output<p>2. These models very roughly turn tokens -&gt; &quot;average meaning&quot; on the embedding layer, followed by attention layers that combine the meanings, and feed forward layers that match the current meaning combination to some kind of learned archetype&#x2F;prototype almost. When you move from word parts to characters all of that becomes more confusing (what&#x27;s the average meaning of a?) and so I don&#x27;t think there are good enough techniques to learn character-based models yet</div><br/></div></div><div id="40360242" class="c"><input type="checkbox" id="c-40360242" checked=""/><div class="controls bullet"><span class="by">novaRom</span><span>|</span><a href="#40360056">parent</a><span>|</span><a href="#40360236">prev</a><span>|</span><a href="#40362191">next</a><span>|</span><label class="collapse" for="c-40360242">[-]</label><label class="expand" for="c-40360242">[1 more]</label></div><br/><div class="children"><div class="content">In AI music generation we have much better results with large vocabulary sizes of 10^6 order, my uneducated guess is that&#x27;s because transformers are not universal pattern recognizers, they can catch patterns on a certain granularity level only.</div><br/></div></div></div></div><div id="40362191" class="c"><input type="checkbox" id="c-40362191" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#40360056">prev</a><span>|</span><a href="#40362673">next</a><span>|</span><label class="collapse" for="c-40362191">[-]</label><label class="expand" for="c-40362191">[2 more]</label></div><br/><div class="children"><div class="content">15.3 On NYT Connections benchmark:<p>GPT-4 turbo (gpt-4-0125-preview) 31.0<p>GPT-4o 30.7<p>GPT-4 turbo (gpt-4-turbo-2024-04-09) 29.7<p>GPT-4 turbo (gpt-4-1106-preview) 28.8<p>Claude 3 Opus 27.3<p>GPT-4 (0613) 26.1<p>Llama 3 Instruct 70B 24.0<p>Gemini Pro 1.5 19.9<p>Mistral Large 17.7<p>-----&gt; Gemini 1.5 Flash 15.3<p>Mistral Medium 15.0<p>Gemini Pro 1.0 14.2<p>Llama 3 Instruct 8B 12.3<p>Mixtral-8x22B Instruct 12.2</div><br/><div id="40363010" class="c"><input type="checkbox" id="c-40363010" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#40362191">parent</a><span>|</span><a href="#40362673">next</a><span>|</span><label class="collapse" for="c-40363010">[-]</label><label class="expand" for="c-40363010">[1 more]</label></div><br/><div class="children"><div class="content">So many high-performing, yet poorly-named OpenAI models in that list.</div><br/></div></div></div></div><div id="40362673" class="c"><input type="checkbox" id="c-40362673" checked=""/><div class="controls bullet"><span class="by">mrcwinn</span><span>|</span><a href="#40362191">prev</a><span>|</span><a href="#40363167">next</a><span>|</span><label class="collapse" for="c-40362673">[-]</label><label class="expand" for="c-40362673">[5 more]</label></div><br/><div class="children"><div class="content">I will say Google certainly has the better branding team. I like Gemini, Gems, and so on. âChatGPTâ is quite a clunky mess. OpenAI just feels like a faceless entity.<p>All things that could change but seems late in the game at this point. They certainly had the money to be more creative as they came to market.</div><br/><div id="40364475" class="c"><input type="checkbox" id="c-40364475" checked=""/><div class="controls bullet"><span class="by">precompute</span><span>|</span><a href="#40362673">parent</a><span>|</span><a href="#40363005">next</a><span>|</span><label class="collapse" for="c-40364475">[-]</label><label class="expand" for="c-40364475">[1 more]</label></div><br/><div class="children"><div class="content">&quot;ChatGPT&quot; is like &quot;Google&quot;.  &quot;Gemini&quot; is never replacing that.</div><br/></div></div><div id="40363005" class="c"><input type="checkbox" id="c-40363005" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#40362673">parent</a><span>|</span><a href="#40364475">prev</a><span>|</span><a href="#40363167">next</a><span>|</span><label class="collapse" for="c-40363005">[-]</label><label class="expand" for="c-40363005">[3 more]</label></div><br/><div class="children"><div class="content">OpenAI desperately needs a marketing consult.<p>&quot;GPT4o&quot;? Seriously?<p>Even &quot;GPT4 Omni&quot; is easier in conversation, and that&#x27;s what the &quot;o&quot; stands for!<p>They severely underestimate the number of casual users they have.</div><br/><div id="40363487" class="c"><input type="checkbox" id="c-40363487" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#40362673">root</a><span>|</span><a href="#40363005">parent</a><span>|</span><a href="#40363502">next</a><span>|</span><label class="collapse" for="c-40363487">[-]</label><label class="expand" for="c-40363487">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI doesnât need marketing because everybody knows whoâs the best. Same reason that if I asked you whatâs the best violin you would say Stradivari, even though youâve never seen an ad for one.<p>OpenAI could call their model the â[poo emoji] 5000â for all the difference it would make.</div><br/></div></div><div id="40363502" class="c"><input type="checkbox" id="c-40363502" checked=""/><div class="controls bullet"><span class="by">cpeterso</span><span>|</span><a href="#40362673">root</a><span>|</span><a href="#40363005">parent</a><span>|</span><a href="#40363487">prev</a><span>|</span><a href="#40363167">next</a><span>|</span><label class="collapse" for="c-40363502">[-]</label><label class="expand" for="c-40363502">[1 more]</label></div><br/><div class="children"><div class="content">Goggle âGeminiâ is a much better product name than any name OpenAI has, but the Gemini product family could use some structure:<p><pre><code>  Gemini Advanced (âwith Ultra 1.0â)
  Gemini Ultra
  Gemini Pro
  Gemini Flash
  Gemini Nano-1
  Gemini Nano-2</code></pre></div><br/></div></div></div></div></div></div><div id="40363167" class="c"><input type="checkbox" id="c-40363167" checked=""/><div class="controls bullet"><span class="by">michaelteter</span><span>|</span><a href="#40362673">prev</a><span>|</span><a href="#40360150">next</a><span>|</span><label class="collapse" for="c-40363167">[-]</label><label class="expand" for="c-40363167">[2 more]</label></div><br/><div class="children"><div class="content">If Gemini Flash is just faster Gemini, then I would say that bad answers aren&#x27;t better when delivered more quickly.<p>I ran Gemini Pro side by side with ChatGPT 4 for a few months on practical coding, systems architecture, and occasional general questions.  ChatGPT was more useful at least 80% of the time.  Gemini was either wrong or laboriously meandering in reaching a useful answer that it wasn&#x27;t worth using, in my experience.<p>Faster isn&#x27;t what I needed... Maybe it&#x27;s also &quot;smarter&quot; (more useful) too now?</div><br/></div></div><div id="40360150" class="c"><input type="checkbox" id="c-40360150" checked=""/><div class="controls bullet"><span class="by">alephxyz</span><span>|</span><a href="#40363167">prev</a><span>|</span><a href="#40360577">next</a><span>|</span><label class="collapse" for="c-40360150">[-]</label><label class="expand" for="c-40360150">[1 more]</label></div><br/><div class="children"><div class="content">Not very informative. They&#x27;re selling it as the fast&#x2F;cheap option but they don&#x27;t benchmark inference speed or compare it with non-gemini models.<p>According to <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;pricing" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;pricing</a> it&#x27;s priced a bit lower than gpt3.5-turbo but no idea how it compares to it.</div><br/></div></div><div id="40360577" class="c"><input type="checkbox" id="c-40360577" checked=""/><div class="controls bullet"><span class="by">quantisan</span><span>|</span><a href="#40360150">prev</a><span>|</span><a href="#40359198">next</a><span>|</span><label class="collapse" for="c-40360577">[-]</label><label class="expand" for="c-40360577">[2 more]</label></div><br/><div class="children"><div class="content">Price (input)
$0.35 &#x2F; 1 million tokens (for prompts up to 128K tokens)
$0.70 &#x2F; 1 million tokens (for prompts longer than 128K)<p>Price (output)
$0.53 &#x2F; 1 million tokens (for prompts up to 128K tokens)
$1.05 &#x2F; 1 million tokens (for prompts longer than 128K)<p>---<p>Compared to GPT-3.5 Turbo<p>Input  US$0.50 &#x2F; 1M tokens
Output US$1.50 &#x2F; 1M tokens</div><br/></div></div><div id="40359198" class="c"><input type="checkbox" id="c-40359198" checked=""/><div class="controls bullet"><span class="by">numbers</span><span>|</span><a href="#40360577">prev</a><span>|</span><a href="#40361209">next</a><span>|</span><label class="collapse" for="c-40359198">[-]</label><label class="expand" for="c-40359198">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s ironic that when you ask these AI chatbots what their own context size is, they don&#x27;t know. ChatGPT doesn&#x27;t even know about 4o existing in 4o.</div><br/><div id="40359584" class="c"><input type="checkbox" id="c-40359584" checked=""/><div class="controls bullet"><span class="by">advisedwang</span><span>|</span><a href="#40359198">parent</a><span>|</span><a href="#40360436">next</a><span>|</span><label class="collapse" for="c-40359584">[-]</label><label class="expand" for="c-40359584">[1 more]</label></div><br/><div class="children"><div class="content">Ask a human how many neurons they have. Hell, over history humans haven&#x27;t even consistently understood that the brain is where cognition happens.</div><br/></div></div><div id="40360436" class="c"><input type="checkbox" id="c-40360436" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40359198">parent</a><span>|</span><a href="#40359584">prev</a><span>|</span><a href="#40359245">next</a><span>|</span><label class="collapse" for="c-40360436">[-]</label><label class="expand" for="c-40360436">[1 more]</label></div><br/><div class="children"><div class="content">The models didn&#x27;t exist when their training data was collected.<p>But... that&#x27;s not really an excuse any more. Model vendors should understand now that the most natural thing in the world is for people to ask models directly about their own abilities and architecture.<p>I think models should have a final layer of fine-tuning or even system prompting to help them answer these kinds of questions in a useful way.</div><br/></div></div><div id="40359245" class="c"><input type="checkbox" id="c-40359245" checked=""/><div class="controls bullet"><span class="by">SoftTalker</span><span>|</span><a href="#40359198">parent</a><span>|</span><a href="#40360436">prev</a><span>|</span><a href="#40361209">next</a><span>|</span><label class="collapse" for="c-40359245">[-]</label><label class="expand" for="c-40359245">[6 more]</label></div><br/><div class="children"><div class="content">Does a monkey know that it is a monkey?</div><br/><div id="40359297" class="c"><input type="checkbox" id="c-40359297" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40359198">root</a><span>|</span><a href="#40359245">parent</a><span>|</span><a href="#40361209">next</a><span>|</span><label class="collapse" for="c-40359297">[-]</label><label class="expand" for="c-40359297">[5 more]</label></div><br/><div class="children"><div class="content">I think &quot;yes&quot; is the most likely answer here<p>animals have a lot more intelligence than they typically get attributed<p>Tool use, names, language, social structure and behavior, even drug use has been shown across many species</div><br/><div id="40359731" class="c"><input type="checkbox" id="c-40359731" checked=""/><div class="controls bullet"><span class="by">chaorace</span><span>|</span><a href="#40359198">root</a><span>|</span><a href="#40359297">parent</a><span>|</span><a href="#40361209">next</a><span>|</span><label class="collapse" for="c-40359731">[-]</label><label class="expand" for="c-40359731">[4 more]</label></div><br/><div class="children"><div class="content">Okay, but the monkey doesn&#x27;t <i>know</i> that it knows that it&#x27;s a monkey.</div><br/><div id="40361095" class="c"><input type="checkbox" id="c-40361095" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#40359198">root</a><span>|</span><a href="#40359731">parent</a><span>|</span><a href="#40362312">next</a><span>|</span><label class="collapse" for="c-40361095">[-]</label><label class="expand" for="c-40361095">[2 more]</label></div><br/><div class="children"><div class="content">are you sure?<p>Many animals recognize themselves and their species as separate concepts</div><br/><div id="40363939" class="c"><input type="checkbox" id="c-40363939" checked=""/><div class="controls bullet"><span class="by">keefle</span><span>|</span><a href="#40359198">root</a><span>|</span><a href="#40361095">parent</a><span>|</span><a href="#40362312">next</a><span>|</span><label class="collapse" for="c-40363939">[-]</label><label class="expand" for="c-40363939">[1 more]</label></div><br/><div class="children"><div class="content">He meant something more meta I believe. Knowing you are a monkey is one thing, and knowing that you know you are a monkey is a another thing. It&#x27;s about being cognisant of the fact that there is something called knowledge and you have it</div><br/></div></div></div></div><div id="40362312" class="c"><input type="checkbox" id="c-40362312" checked=""/><div class="controls bullet"><span class="by">fourthark</span><span>|</span><a href="#40359198">root</a><span>|</span><a href="#40359731">parent</a><span>|</span><a href="#40361095">prev</a><span>|</span><a href="#40361209">next</a><span>|</span><label class="collapse" for="c-40362312">[-]</label><label class="expand" for="c-40362312">[1 more]</label></div><br/><div class="children"><div class="content">How do you know?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40361561" class="c"><input type="checkbox" id="c-40361561" checked=""/><div class="controls bullet"><span class="by">chefkd</span><span>|</span><a href="#40361209">prev</a><span>|</span><a href="#40360081">next</a><span>|</span><label class="collapse" for="c-40361561">[-]</label><label class="expand" for="c-40361561">[1 more]</label></div><br/><div class="children"><div class="content">Any links to companies working on a fully local only AI?</div><br/></div></div><div id="40360081" class="c"><input type="checkbox" id="c-40360081" checked=""/><div class="controls bullet"><span class="by">nojvek</span><span>|</span><a href="#40361561">prev</a><span>|</span><a href="#40362600">next</a><span>|</span><label class="collapse" for="c-40360081">[-]</label><label class="expand" for="c-40360081">[3 more]</label></div><br/><div class="children"><div class="content">Will wait for Meta to release Flash equivalent weights.<p>Multi-Modal modals running offline on mobile devices with millisecond latencies per token seems the future.<p>Where is Apple in all of this. Why is Siri still so shit?</div><br/><div id="40360506" class="c"><input type="checkbox" id="c-40360506" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40360081">parent</a><span>|</span><a href="#40362600">next</a><span>|</span><label class="collapse" for="c-40360506">[-]</label><label class="expand" for="c-40360506">[2 more]</label></div><br/><div class="children"><div class="content">Apple made a deal with OpenAI for GPT4o, the stakes are indeed high, can&#x27;t be caught with pants down. iPhone needs to remain the premium brand.</div><br/></div></div></div></div><div id="40362600" class="c"><input type="checkbox" id="c-40362600" checked=""/><div class="controls bullet"><span class="by">stan_kirdey</span><span>|</span><a href="#40360081">prev</a><span>|</span><a href="#40361153">next</a><span>|</span><label class="collapse" for="c-40362600">[-]</label><label class="expand" for="c-40362600">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been diligently trying to use Gemini 1.5 Pro, and it is not even on the level of Llama3-70B. I really hope Gemini improves, even if it gets reduced context length.</div><br/><div id="40363025" class="c"><input type="checkbox" id="c-40363025" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#40362600">parent</a><span>|</span><a href="#40361153">next</a><span>|</span><label class="collapse" for="c-40363025">[-]</label><label class="expand" for="c-40363025">[1 more]</label></div><br/><div class="children"><div class="content">FAIR really swung for the fences with Llama3. It&#x27;s a very impressive model, but the 8K context size is quite limiting for most use-cases.</div><br/></div></div></div></div><div id="40361153" class="c"><input type="checkbox" id="c-40361153" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40362600">prev</a><span>|</span><a href="#40362646">next</a><span>|</span><label class="collapse" for="c-40361153">[-]</label><label class="expand" for="c-40361153">[1 more]</label></div><br/><div class="children"><div class="content">The website talks about a specific benchmark:<p>&gt; Python code generation. Held out dataset HumanEval-like, not leaked on the web<p>What I find interesting here is that for this particular benchmark _not_ publishing the benchmark is advertised as a feature (instead of as a sign of &#x27;trust me, bro, we have a great benchmark&#x27;), and I can understand why.  Still these are strange times we live in.</div><br/></div></div><div id="40362646" class="c"><input type="checkbox" id="c-40362646" checked=""/><div class="controls bullet"><span class="by">webprofusion</span><span>|</span><a href="#40361153">prev</a><span>|</span><a href="#40361070">next</a><span>|</span><label class="collapse" for="c-40362646">[-]</label><label class="expand" for="c-40362646">[2 more]</label></div><br/><div class="children"><div class="content">Uh guys, yeah.. Adobe are on the phone saying something about trademark infringement, apparently Flash is something else? I don&#x27;t know, I&#x27;ve never heard of it..</div><br/><div id="40363903" class="c"><input type="checkbox" id="c-40363903" checked=""/><div class="controls bullet"><span class="by">exodust</span><span>|</span><a href="#40362646">parent</a><span>|</span><a href="#40361070">next</a><span>|</span><label class="collapse" for="c-40363903">[-]</label><label class="expand" for="c-40363903">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly, until your comment I hadn&#x27;t made any connection with old Flash, even though I spent hundreds of hours making Flash games.<p>This suggests names don&#x27;t stick around for long and can be re-used. Perhaps Google could bring back &quot;Buzz&quot; and &quot;Wave&quot; since enough time has passed!</div><br/></div></div></div></div><div id="40361070" class="c"><input type="checkbox" id="c-40361070" checked=""/><div class="controls bullet"><span class="by">objektif</span><span>|</span><a href="#40362646">prev</a><span>|</span><a href="#40358643">next</a><span>|</span><label class="collapse" for="c-40361070">[-]</label><label class="expand" for="c-40361070">[1 more]</label></div><br/><div class="children"><div class="content">Does Goog have anything like openai assistant via API? If they had I would definitely give it a try.</div><br/></div></div><div id="40358643" class="c"><input type="checkbox" id="c-40358643" checked=""/><div class="controls bullet"><span class="by">cynicalsecurity</span><span>|</span><a href="#40361070">prev</a><span>|</span><a href="#40360676">next</a><span>|</span><label class="collapse" for="c-40358643">[-]</label><label class="expand" for="c-40358643">[4 more]</label></div><br/><div class="children"><div class="content">Feed 1 mln tokens<p>@<p>Get blocked by some silly overly sensitive &quot;safety&quot; trigger</div><br/><div id="40359046" class="c"><input type="checkbox" id="c-40359046" checked=""/><div class="controls bullet"><span class="by">gpm</span><span>|</span><a href="#40358643">parent</a><span>|</span><a href="#40360676">next</a><span>|</span><label class="collapse" for="c-40359046">[-]</label><label class="expand" for="c-40359046">[3 more]</label></div><br/><div class="children"><div class="content">Last I checked you could disable the safety triggers as an API user with gemini (which doesn&#x27;t alleviate your obligation to follow the TOS as to the uses of the model).</div><br/><div id="40359134" class="c"><input type="checkbox" id="c-40359134" checked=""/><div class="controls bullet"><span class="by">VS1999</span><span>|</span><a href="#40358643">root</a><span>|</span><a href="#40359046">parent</a><span>|</span><a href="#40360676">next</a><span>|</span><label class="collapse" for="c-40359134">[-]</label><label class="expand" for="c-40359134">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not working with a company that can just write in the ToS &quot;we can do anything we want. lol. lmao&quot; and expect me to follow it religiously. Corporations need less control over speech, not more.</div><br/><div id="40364044" class="c"><input type="checkbox" id="c-40364044" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#40358643">root</a><span>|</span><a href="#40359134">parent</a><span>|</span><a href="#40360676">next</a><span>|</span><label class="collapse" for="c-40364044">[-]</label><label class="expand" for="c-40364044">[1 more]</label></div><br/><div class="children"><div class="content">I mean, you are using a service they&#x27;re providing - many would say they they&#x27;re exercising their rights by gatekeeping how it&#x27;s used. There are pretty good models out there you could use however you want for your own purpose, whatever it is. I occasionally fine-tune Mixtral on HN posts+comments and chat with comments. An emergent Dang actually once told me off for flame-baiting a free speech comment.</div><br/></div></div></div></div></div></div></div></div><div id="40360676" class="c"><input type="checkbox" id="c-40360676" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40358643">prev</a><span>|</span><a href="#40360718">next</a><span>|</span><label class="collapse" for="c-40360676">[-]</label><label class="expand" for="c-40360676">[14 more]</label></div><br/><div class="children"><div class="content">We&#x27;re witnessing a race to the bottom on pricing as it&#x27;s happening. Competition based solely or mainly on pricing is a defining characteristic of a commodity market, i.e., a market in which competing products are interchangeable, and buyers are happy to switch to the cheapest option for a given level of quality.<p>There&#x27;s an old saying that if you&#x27;re selling a commodity, &quot;you can only be as smart as your dumbest competitor.&quot;<p>If we want to be more polite, we could say instead: &quot;you can only price your service as high as your lowest-cost competitor.&quot;<p>It seems that a lot of capital that has been &quot;invested&quot; to train AI models is, ahem, unlikely ever to be recovered.</div><br/><div id="40360758" class="c"><input type="checkbox" id="c-40360758" checked=""/><div class="controls bullet"><span class="by">daghamm</span><span>|</span><a href="#40360676">parent</a><span>|</span><a href="#40360752">next</a><span>|</span><label class="collapse" for="c-40360758">[-]</label><label class="expand" for="c-40360758">[1 more]</label></div><br/><div class="children"><div class="content">Is this race to the bottom or just Googles new TPUs being extremly efficient?</div><br/></div></div><div id="40360752" class="c"><input type="checkbox" id="c-40360752" checked=""/><div class="controls bullet"><span class="by">Delmololo</span><span>|</span><a href="#40360676">parent</a><span>|</span><a href="#40360758">prev</a><span>|</span><a href="#40360756">next</a><span>|</span><label class="collapse" for="c-40360752">[-]</label><label class="expand" for="c-40360752">[2 more]</label></div><br/><div class="children"><div class="content">But the race to the bottom has an opposition right?<p>So people expect to see a return of investment which will create the bottom of pricing (at least as soon as the old money ran out)<p>I&#x27;m also curious if AI is a good example because ai will become fundamental. This means if you don&#x27;t invest you might be gone therefore it&#x27;s more like a fee in case the investment would not pan out.</div><br/><div id="40361048" class="c"><input type="checkbox" id="c-40361048" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#40360676">root</a><span>|</span><a href="#40360752">parent</a><span>|</span><a href="#40360756">next</a><span>|</span><label class="collapse" for="c-40361048">[-]</label><label class="expand" for="c-40361048">[1 more]</label></div><br/><div class="children"><div class="content">Supply and demand determines price, not the hopes and dreams of investors.</div><br/></div></div></div></div><div id="40360756" class="c"><input type="checkbox" id="c-40360756" checked=""/><div class="controls bullet"><span class="by">r0m4n0</span><span>|</span><a href="#40360676">parent</a><span>|</span><a href="#40360752">prev</a><span>|</span><a href="#40360871">next</a><span>|</span><label class="collapse" for="c-40360756">[-]</label><label class="expand" for="c-40360756">[1 more]</label></div><br/><div class="children"><div class="content">Google is building on top of and integrated with their cloud offerings. Having first party solutions like this gives big cloud customers an easy way to integrate. For Google itâs just another tool in the chest that gets sold to these big enterprises. Many go all in on all the same cloud products. Also the models are only the building blocks. Other cloud products at Google will be built with this and sold as a service<p>Not so sure about Open AI thoughâ¦</div><br/></div></div><div id="40360871" class="c"><input type="checkbox" id="c-40360871" checked=""/><div class="controls bullet"><span class="by">rmbyrro</span><span>|</span><a href="#40360676">parent</a><span>|</span><a href="#40360756">prev</a><span>|</span><a href="#40360717">next</a><span>|</span><label class="collapse" for="c-40360871">[-]</label><label class="expand" for="c-40360871">[3 more]</label></div><br/><div class="children"><div class="content">Google figured it can&#x27;t beat OpenAI technically, but they sure know they can beat them financially and infrastructurally.</div><br/><div id="40361364" class="c"><input type="checkbox" id="c-40361364" checked=""/><div class="controls bullet"><span class="by">tfsh</span><span>|</span><a href="#40360676">root</a><span>|</span><a href="#40360871">parent</a><span>|</span><a href="#40361023">next</a><span>|</span><label class="collapse" for="c-40361364">[-]</label><label class="expand" for="c-40361364">[1 more]</label></div><br/><div class="children"><div class="content">In technical terms they&#x27;re on par. But you&#x27;re correct about Google being able to bet on their decades of infrastructure</div><br/></div></div><div id="40361023" class="c"><input type="checkbox" id="c-40361023" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#40360676">root</a><span>|</span><a href="#40360871">parent</a><span>|</span><a href="#40361364">prev</a><span>|</span><a href="#40360717">next</a><span>|</span><label class="collapse" for="c-40361023">[-]</label><label class="expand" for="c-40361023">[1 more]</label></div><br/><div class="children"><div class="content">Is infrastructure and scale not an expression of technical ability? It should have been obvious that Meta and Google would bury a tiny company with less than 1000 employees given the amount of capital they can leverage for compute, talent, and data. Google literally invented GPT.</div><br/></div></div></div></div><div id="40360717" class="c"><input type="checkbox" id="c-40360717" checked=""/><div class="controls bullet"><span class="by">Aloisius</span><span>|</span><a href="#40360676">parent</a><span>|</span><a href="#40360871">prev</a><span>|</span><a href="#40361013">next</a><span>|</span><label class="collapse" for="c-40360717">[-]</label><label class="expand" for="c-40360717">[4 more]</label></div><br/><div class="children"><div class="content">Price competition isn&#x27;t limited to commodities.</div><br/><div id="40360743" class="c"><input type="checkbox" id="c-40360743" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40360676">root</a><span>|</span><a href="#40360717">parent</a><span>|</span><a href="#40361013">next</a><span>|</span><label class="collapse" for="c-40360743">[-]</label><label class="expand" for="c-40360743">[3 more]</label></div><br/><div class="children"><div class="content">I never said it was.</div><br/><div id="40360888" class="c"><input type="checkbox" id="c-40360888" checked=""/><div class="controls bullet"><span class="by">Aloisius</span><span>|</span><a href="#40360676">root</a><span>|</span><a href="#40360743">parent</a><span>|</span><a href="#40360803">next</a><span>|</span><label class="collapse" for="c-40360888">[-]</label><label class="expand" for="c-40360888">[1 more]</label></div><br/><div class="children"><div class="content">Then why imply that it is a commodity because they (partly) compete on price?<p>Fungibility is the defining characteristic of commodities. While these products can be used to accomplish the same task, we&#x27;re not near real fungibility yet.</div><br/></div></div><div id="40360803" class="c"><input type="checkbox" id="c-40360803" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#40360676">root</a><span>|</span><a href="#40360743">parent</a><span>|</span><a href="#40360888">prev</a><span>|</span><a href="#40361013">next</a><span>|</span><label class="collapse" for="c-40360803">[-]</label><label class="expand" for="c-40360803">[1 more]</label></div><br/><div class="children"><div class="content">You never said it wasn&#x27;t, either :-P</div><br/></div></div></div></div></div></div><div id="40361013" class="c"><input type="checkbox" id="c-40361013" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#40360676">parent</a><span>|</span><a href="#40360717">prev</a><span>|</span><a href="#40360870">next</a><span>|</span><label class="collapse" for="c-40361013">[-]</label><label class="expand" for="c-40361013">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re saying the quiet part out loud here.</div><br/></div></div></div></div><div id="40359022" class="c"><input type="checkbox" id="c-40359022" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40360718">prev</a><span>|</span><label class="collapse" for="c-40359022">[-]</label><label class="expand" for="c-40359022">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s absolutely unconscionable that Gemini Ultra got memory-holed. I can&#x27;t trust anything that Google says about benchmarks.<p>It seemingly existed only so in December 2023, Gemini ~= GPT-4. (April 2023 version) (on paper) (&quot;32-shot CoT&quot; vs. 5-shot GPT-4)</div><br/><div id="40359337" class="c"><input type="checkbox" id="c-40359337" checked=""/><div class="controls bullet"><span class="by">CSMastermind</span><span>|</span><a href="#40359022">parent</a><span>|</span><a href="#40359326">next</a><span>|</span><label class="collapse" for="c-40359337">[-]</label><label class="expand" for="c-40359337">[2 more]</label></div><br/><div class="children"><div class="content">Anyone who uses both products regularly will tell you that Gemini Advanced is far behind GPT-4 and Claude 3 Opus.<p>Pretending that they have a model internally that&#x27;s on par but they&#x27;re not releasing it is a very &quot;my girlfriend goes to another school&quot; move and makes no sense if they&#x27;re a business that&#x27;s actually trying to compete.</div><br/></div></div><div id="40359326" class="c"><input type="checkbox" id="c-40359326" checked=""/><div class="controls bullet"><span class="by">summerlight</span><span>|</span><a href="#40359022">parent</a><span>|</span><a href="#40359337">prev</a><span>|</span><label class="collapse" for="c-40359326">[-]</label><label class="expand" for="c-40359326">[6 more]</label></div><br/><div class="children"><div class="content">Gemini Ultra is 1.0 with 8k window. This is 1.5 with 1m window. Your feeling is based on incorrect assumption.</div><br/><div id="40359889" class="c"><input type="checkbox" id="c-40359889" checked=""/><div class="controls bullet"><span class="by">anoncareer0212</span><span>|</span><a href="#40359022">root</a><span>|</span><a href="#40359326">parent</a><span>|</span><label class="collapse" for="c-40359889">[-]</label><label class="expand" for="c-40359889">[5 more]</label></div><br/><div class="children"><div class="content">And?<p>You&#x27;re replying to a comment that points out Gemini Ultra was never released, wasn&#x27;t mentioned today, and it&#x27;s the only model Google&#x27;s benchmarking at GPT-4 level. They didn&#x27;t say anything about feelings or context window.</div><br/><div id="40359954" class="c"><input type="checkbox" id="c-40359954" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#40359022">root</a><span>|</span><a href="#40359889">parent</a><span>|</span><a href="#40359981">next</a><span>|</span><label class="collapse" for="c-40359954">[-]</label><label class="expand" for="c-40359954">[2 more]</label></div><br/><div class="children"><div class="content">Gemini Ultra has been available for people to try via Gemini Advanced (formerly Bard) for a few months</div><br/><div id="40360798" class="c"><input type="checkbox" id="c-40360798" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#40359022">root</a><span>|</span><a href="#40359954">parent</a><span>|</span><a href="#40359981">next</a><span>|</span><label class="collapse" for="c-40360798">[-]</label><label class="expand" for="c-40360798">[1 more]</label></div><br/><div class="children"><div class="content">It says it may fall back to a worse model under load and there is no way to tell which you are getting.  I think chatgpt has at times done something similar though.</div><br/></div></div></div></div><div id="40359981" class="c"><input type="checkbox" id="c-40359981" checked=""/><div class="controls bullet"><span class="by">summerlight</span><span>|</span><a href="#40359022">root</a><span>|</span><a href="#40359889">parent</a><span>|</span><a href="#40359954">prev</a><span>|</span><label class="collapse" for="c-40359981">[-]</label><label class="expand" for="c-40359981">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You&#x27;re replying to a comment that points out Gemini Ultra was never released<p>What are you even talking about? How do you know it&#x27;s memory-holed if you haven&#x27;t used it? The API is not GA, but the model can be used through the chatbot subscription. GP is talking about their lack of trust on Google&#x27;s claim of 1M context token, not GPT-4 level reasoning. If you&#x27;re expect GPT-4 level performance with cost-efficient models, that&#x27;s another problem.</div><br/><div id="40362637" class="c"><input type="checkbox" id="c-40362637" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40359022">root</a><span>|</span><a href="#40359981">parent</a><span>|</span><label class="collapse" for="c-40362637">[-]</label><label class="expand" for="c-40362637">[1 more]</label></div><br/><div class="children"><div class="content">Idk why you&#x27;re so aggro, they&#x27;re right, I meant the GPT-4 level reasoning</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>