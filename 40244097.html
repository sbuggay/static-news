<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714986073264" as="style"/><link rel="stylesheet" href="styles.css?v=1714986073264"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://jeroenjanssens.com/dsatcl/">Data Science at the Command Line, 2nd Edition (2021)</a> <span class="domain">(<a href="https://jeroenjanssens.com">jeroenjanssens.com</a>)</span></div><div class="subtext"><span>aragonite</span> | <span>15 comments</span></div><br/><div><div id="40272623" class="c"><input type="checkbox" id="c-40272623" checked=""/><div class="controls bullet"><span class="by">vcdimension</span><span>|</span><a href="#40272279">next</a><span>|</span><label class="collapse" for="c-40272623">[-]</label><label class="expand" for="c-40272623">[1 more]</label></div><br/><div class="children"><div class="content">Remembering all those unix tools and their uses can be tricky.
I wrote a couple of shell scripts that allow you to build command pipelines one step at a time, choosing a tool from a menu at each step, and with the ability to preview the results while tweaking the command line flags at each step. At any point you can go back to the previous step and continue: <a href="https:&#x2F;&#x2F;github.com&#x2F;vapniks&#x2F;fzf-tool-launcher">https:&#x2F;&#x2F;github.com&#x2F;vapniks&#x2F;fzf-tool-launcher</a>
<a href="https:&#x2F;&#x2F;github.com&#x2F;vapniks&#x2F;fzfrepl">https:&#x2F;&#x2F;github.com&#x2F;vapniks&#x2F;fzfrepl</a></div><br/></div></div><div id="40272279" class="c"><input type="checkbox" id="c-40272279" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#40272623">prev</a><span>|</span><a href="#40269986">next</a><span>|</span><label class="collapse" for="c-40272279">[-]</label><label class="expand" for="c-40272279">[1 more]</label></div><br/><div class="children"><div class="content">Worth highlighting that <i>make</i> defines a DAG, and that you can run <i>make</i> tasks in parallel which will automatically bottleneck (as desired) on common dependencies, and fan out otherwise.<p>The sort of massive C++ build that make can handle are typically much more complicated than your average ETL pipeline. So, there is plenty of room to grow into <i>make</i>.</div><br/></div></div><div id="40269986" class="c"><input type="checkbox" id="c-40269986" checked=""/><div class="controls bullet"><span class="by">staplung</span><span>|</span><a href="#40272279">prev</a><span>|</span><a href="#40269421">next</a><span>|</span><label class="collapse" for="c-40269986">[-]</label><label class="expand" for="c-40269986">[3 more]</label></div><br/><div class="children"><div class="content">Having not read the book I don&#x27;t know if it delves into the speedups that can come from the fact that pipelines have processes running in parallel. A nice article about how much faster it can be to process data on the command line vs something like Hadoop: <a href="https:&#x2F;&#x2F;adamdrake.com&#x2F;command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html" rel="nofollow">https:&#x2F;&#x2F;adamdrake.com&#x2F;command-line-tools-can-be-235x-faster-...</a><p>Clearly, doesn&#x27;t work in all cases but often enough, it&#x27;s not only simpler to set up but runs much faster besides.</div><br/><div id="40271494" class="c"><input type="checkbox" id="c-40271494" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#40269986">parent</a><span>|</span><a href="#40269421">next</a><span>|</span><label class="collapse" for="c-40271494">[-]</label><label class="expand" for="c-40271494">[2 more]</label></div><br/><div class="children"><div class="content">Always worth giving a shoutout to the &quot;Scalability! But at what COST?&quot; paper (pdf <a href="https:&#x2F;&#x2F;www.frankmcsherry.org&#x2F;assets&#x2F;COST.pdf" rel="nofollow">https:&#x2F;&#x2F;www.frankmcsherry.org&#x2F;assets&#x2F;COST.pdf</a>)<p><pre><code>  We offer a new metric for big data platforms, COST, or the Configuration that Outperforms a Single Thread. The COST of a given platform for a given problem is the hardware configuration required before the platform outperforms a competent single-threaded implementation. COST weighs a system’s scalability against the overheads introduced by the system, and indicates the actual performance gains of the system, without rewarding systems that bring substantial but parallelizable overheads.</code></pre></div><br/><div id="40272251" class="c"><input type="checkbox" id="c-40272251" checked=""/><div class="controls bullet"><span class="by">Joel_Mckay</span><span>|</span><a href="#40269986">root</a><span>|</span><a href="#40271494">parent</a><span>|</span><a href="#40269421">next</a><span>|</span><label class="collapse" for="c-40272251">[-]</label><label class="expand" for="c-40272251">[1 more]</label></div><br/><div class="children"><div class="content">After numerous trials with CUDA enabled machines, I found the &quot;cost function&quot; models are better at determining efficiency.<p>For example:<p>*  A single GPU machine is often more efficient than multiple cards due to  bus i&#x2F;o bandwidth constraints, lack of software support, and obscure driver failure modes<p>* A single CPU machine is often more efficient than multi-chip solutions due to memory access latency and caching issues<p>* A sequential drive data access machine is often more efficient than arrays due to pipelined memory cache layout and baked access latency<p>* A single CPU core bound process is often more efficient due to avoiding threading and or mailbox overheads<p>Thus, if a problem is truly separable, than it is sometimes wiser to bind n-many slow jobs to n-many cores rather than parallelize 1 job to try to get it done more quickly.<p>I found this rather surprising in processing large video media data sets.<p>Some may disagree, but they are mostly 15% to 30% more wrong for truly separable tasks. YMMV =3</div><br/></div></div></div></div></div></div><div id="40269421" class="c"><input type="checkbox" id="c-40269421" checked=""/><div class="controls bullet"><span class="by">faizshah</span><span>|</span><a href="#40269986">prev</a><span>|</span><a href="#40244112">next</a><span>|</span><label class="collapse" for="c-40269421">[-]</label><label class="expand" for="c-40269421">[7 more]</label></div><br/><div class="children"><div class="content">This is a great book, there’s a few tools I would add.<p>Datasette, clickhouse local (cli) and duckdb.<p>I think ripgrep is a big omission, ripgrep | xargs jq and find -exec jq is one of my most common data science workflows cause you can get stuff done in a few minutes. An example of where I use this is to debug Infrastructure as Code that is generated for many regions and AZs quickly.<p>Another one I like in this space is Bioninformatics Data Skills, for some reason Bioinformaticians use CLI workflows a lot and this book covers a lot of good info for those who are just starting out like tmux, make, git, ssh, background processes.<p>Two other techniques I like are git-scraping (tracking the changes of data over time or just saving snapshots of your data to git so you can diff it): <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2020&#x2F;Oct&#x2F;9&#x2F;git-scraping&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2020&#x2F;Oct&#x2F;9&#x2F;git-scraping&#x2F;</a> I most recently have used this technique to diff changes to build artifacts over time.<p>This technique is not really CLI related per se, but I really like the http range query technique of hosting data: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27016630">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27016630</a> There’s simple ways to use this idea (like cooking up a quick h2o web server conf) to host data quickly.<p>I also like the makefile data pipeline idea, I believe the technique is described in the book but I first heard of it from this HN comment: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18896204">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18896204</a> The basic idea is that you can use make to orchestrate the steps of your command line data science workflow and let make figure out when your intermediate data needs to be generated again. A good example is this map reduce with make here: <a href="https:&#x2F;&#x2F;www.benevolent.com&#x2F;news-and-media&#x2F;blog-and-videos&#x2F;how-use-makefiles-run-simple-map-reduce-data-pipeline&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.benevolent.com&#x2F;news-and-media&#x2F;blog-and-videos&#x2F;ho...</a></div><br/><div id="40269630" class="c"><input type="checkbox" id="c-40269630" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#40269421">parent</a><span>|</span><a href="#40244112">next</a><span>|</span><label class="collapse" for="c-40269630">[-]</label><label class="expand" for="c-40269630">[6 more]</label></div><br/><div class="children"><div class="content">&quot;ripgrep + find -exec jq&quot;  what does this imply?</div><br/><div id="40269686" class="c"><input type="checkbox" id="c-40269686" checked=""/><div class="controls bullet"><span class="by">faizshah</span><span>|</span><a href="#40269421">root</a><span>|</span><a href="#40269630">parent</a><span>|</span><a href="#40244112">next</a><span>|</span><label class="collapse" for="c-40269686">[-]</label><label class="expand" for="c-40269686">[5 more]</label></div><br/><div class="children"><div class="content">I guess I should have written it more explicitly but my two most common techniques are:<p>- ripgrep PATTERN | xargs jq<p>- find PATTERN -exec jq<p>In both cases you have a large amount of data in your file system that is in JSON and you want to extract a subset of it for further processing. Ripgrep is an extremely fast way to do a content based search for a pattern and find is a fast way to do a file path based search. Then jq lets you extract the data.<p>For other types of data I use the same technique of first using ripgrep to find the candidate files and then piping to a text processor like awk&#x2F;perl&#x2F;ruby to actually process the data.<p>If you need FTS rather than regex search then SQLite FTS5 is my go to.<p>So a full example is something like:<p>`find . -name “*.pod.json” -print0 | xargs -0 -P 12 -I {} sh -c ‘jq -r “select(.spec.containers != null) | .spec.containers | to_entries[]” sh {} \; | jq -s ‘sort_by(.image)’`<p>Something like that so it’s sort of like a map-reduce you first narrow the subset of inputs by first finding by file pattern, then you pull out the relevant data from each in a parallel xargs, then you reduce it with a jq -s. This technique is used because jq is very slow on large files and your later processing scripts might be slow so the first step of a good pipeline is to first throw away all the data you don’t need first.</div><br/><div id="40270879" class="c"><input type="checkbox" id="c-40270879" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#40269421">root</a><span>|</span><a href="#40269686">parent</a><span>|</span><a href="#40269799">next</a><span>|</span><label class="collapse" for="c-40270879">[-]</label><label class="expand" for="c-40270879">[1 more]</label></div><br/><div class="children"><div class="content">If you write a book I will buy it.</div><br/></div></div><div id="40269799" class="c"><input type="checkbox" id="c-40269799" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#40269421">root</a><span>|</span><a href="#40269686">parent</a><span>|</span><a href="#40270879">prev</a><span>|</span><a href="#40244112">next</a><span>|</span><label class="collapse" for="c-40269799">[-]</label><label class="expand" for="c-40269799">[3 more]</label></div><br/><div class="children"><div class="content">Thanks. I use &#x27;rg&#x27; a lot but it finds part of the json text, not sure how &#x27;jq&#x27; can be used with that since you&#x27;re not finding a full json object in general</div><br/><div id="40269889" class="c"><input type="checkbox" id="c-40269889" checked=""/><div class="controls bullet"><span class="by">faizshah</span><span>|</span><a href="#40269421">root</a><span>|</span><a href="#40269799">parent</a><span>|</span><a href="#40244112">next</a><span>|</span><label class="collapse" for="c-40269889">[-]</label><label class="expand" for="c-40269889">[2 more]</label></div><br/><div class="children"><div class="content">Oh I see, so what I do is I use rg -l aka —-files-with-matches. To produce the relevant files and I use the filepath as the key for the next part of the processing pipeline.<p>So the pattern is to reduce the candidate set of files with rg or find then extract the relevant parts using xargs jq then pipe to jq -s to produce the dataset.</div><br/><div id="40270613" class="c"><input type="checkbox" id="c-40270613" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#40269421">root</a><span>|</span><a href="#40269889">parent</a><span>|</span><a href="#40244112">next</a><span>|</span><label class="collapse" for="c-40270613">[-]</label><label class="expand" for="c-40270613">[1 more]</label></div><br/><div class="children"><div class="content">That makes sense, thanks!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40244112" class="c"><input type="checkbox" id="c-40244112" checked=""/><div class="controls bullet"><span class="by">aragonite</span><span>|</span><a href="#40269421">prev</a><span>|</span><label class="collapse" for="c-40244112">[-]</label><label class="expand" for="c-40244112">[2 more]</label></div><br/><div class="children"><div class="content">Previous HN discussion:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16245873">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16245873</a></div><br/><div id="40266999" class="c"><input type="checkbox" id="c-40266999" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40244112">parent</a><span>|</span><label class="collapse" for="c-40266999">[-]</label><label class="expand" for="c-40266999">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Macroexpanded:<p><i>Data Science at the Command Line, 2nd edition (free; 2021)</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30115066">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30115066</a> - Jan 2022 (1 comment)<p><i>2nd Edition of Data Science at the Command Line Released (Free)</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29589381">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29589381</a> - Dec 2021 (1 comment)<p><i>Data Science at the Command Line</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16245873">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16245873</a> - Jan 2018 (35 comments)<p><i>Command-line tools for data science</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=6412190">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=6412190</a> - Sept 2013 (78 comments)</div><br/></div></div></div></div></div></div></div></div></div></body></html>