<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683795653223" as="style"/><link rel="stylesheet" href="styles.css?v=1683795653223"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.google/static/documents/palm2techreport.pdf">PaLM 2 Technical Report [pdf]</a> <span class="domain">(<a href="https://ai.google">ai.google</a>)</span></div><div class="subtext"><span>cubefox</span> | <span>235 comments</span></div><br/><div><div id="35894597" class="c"><input type="checkbox" id="c-35894597" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#35892215">next</a><span>|</span><label class="collapse" for="c-35894597">[-]</label><label class="expand" for="c-35894597">[27 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how this can be considered a technical report. No information on model architecture, distributed training methodology, or optimizations. The &quot;Training dataset&quot; section is a pathetic 0.5 pages long.<p>Come on, Google.</div><br/><div id="35894684" class="c"><input type="checkbox" id="c-35894684" checked=""/><div class="controls bullet"><span class="by">stygiansonic</span><span>|</span><a href="#35894597">parent</a><span>|</span><a href="#35896575">next</a><span>|</span><label class="collapse" for="c-35894684">[-]</label><label class="expand" for="c-35894684">[23 more]</label></div><br/><div class="children"><div class="content">In that sense, it&#x27;s very similar to the GPT-4 Technical Report.<p>The era of being &quot;open&quot; about LLMs or other &quot;secret sauce&quot; models in published papers may be over, since these things have become existential threats to companies.</div><br/><div id="35895866" class="c"><input type="checkbox" id="c-35895866" checked=""/><div class="controls bullet"><span class="by">shrimpx</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35894684">parent</a><span>|</span><a href="#35894844">next</a><span>|</span><label class="collapse" for="c-35895866">[-]</label><label class="expand" for="c-35895866">[17 more]</label></div><br/><div class="children"><div class="content">Btw I&#x27;ve arrived at a different interpretation of the &quot;Open&quot; in OpenAI. It&#x27;s open in the sense that the generic LLM is exposed via an API, allowing companies to build anything they want on top.<p>Companies like Google have been working on language models (and AI more broadly) for years but have hid the generic intelligence of their models, exposing it only via improvements to their products. OpenAI bucked this trend and exposed an API to generic LLMs.</div><br/><div id="35896212" class="c"><input type="checkbox" id="c-35896212" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895866">parent</a><span>|</span><a href="#35896899">next</a><span>|</span><label class="collapse" for="c-35896212">[-]</label><label class="expand" for="c-35896212">[9 more]</label></div><br/><div class="children"><div class="content">&gt; Btw I&#x27;ve arrived at a different interpretation of the &quot;Open&quot; in OpenAI.<p>I don&#x27;t understand why people have to keep trying to wrap their head around the word &#x27;Open&#x27; in OpenAI. If you ever saw a commercial like a product has a &#x27;great new taste&#x27; but then you tried it and it tasted bad, would you twist yourself into knots trying to understand how you went wrong in your interpretation of &#x27;great&#x27;? No that&#x27;s ridiculous. Same with &#x27;Open&#x27; in &#x27;OpenAI&#x27;. It&#x27;s just some letters that form part of the name that they chose for themselves when they filled the form to incorporate their company.</div><br/><div id="35896261" class="c"><input type="checkbox" id="c-35896261" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896212">parent</a><span>|</span><a href="#35896733">next</a><span>|</span><label class="collapse" for="c-35896261">[-]</label><label class="expand" for="c-35896261">[5 more]</label></div><br/><div class="children"><div class="content">You mean when they filled out a form to incorporate their non-profit. Which they later turned into a for-profit company after reaping all the goodwill. The “Open” used to mean something.</div><br/><div id="35899361" class="c"><input type="checkbox" id="c-35899361" checked=""/><div class="controls bullet"><span class="by">robertlagrant</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896261">parent</a><span>|</span><a href="#35896337">next</a><span>|</span><label class="collapse" for="c-35899361">[-]</label><label class="expand" for="c-35899361">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Which they later turned into a for-profit company after reaping all the goodwill<p>Did they have a lot of goodwill attached to their company? What did that give them?</div><br/></div></div><div id="35896343" class="c"><input type="checkbox" id="c-35896343" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896261">parent</a><span>|</span><a href="#35896337">prev</a><span>|</span><a href="#35897194">next</a><span>|</span><label class="collapse" for="c-35896343">[-]</label><label class="expand" for="c-35896343">[1 more]</label></div><br/><div class="children"><div class="content">That is a bit reductionist. They turned it into a for-profit company controlled by a non-profit entity, with profits &#x2F; returns being capped for employees &#x2F; investors.</div><br/></div></div><div id="35897194" class="c"><input type="checkbox" id="c-35897194" checked=""/><div class="controls bullet"><span class="by">re-thc</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896261">parent</a><span>|</span><a href="#35896343">prev</a><span>|</span><a href="#35896733">next</a><span>|</span><label class="collapse" for="c-35897194">[-]</label><label class="expand" for="c-35897194">[1 more]</label></div><br/><div class="children"><div class="content">All non-profit means is a different tax status. Don&#x27;t assume they actually don&#x27;t make money.</div><br/></div></div></div></div><div id="35896733" class="c"><input type="checkbox" id="c-35896733" checked=""/><div class="controls bullet"><span class="by">shrimpx</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896212">parent</a><span>|</span><a href="#35896261">prev</a><span>|</span><a href="#35896478">next</a><span>|</span><label class="collapse" for="c-35896733">[-]</label><label class="expand" for="c-35896733">[2 more]</label></div><br/><div class="children"><div class="content">I think the strong connotation of the word &quot;open&quot; in the software community comes from &quot;open source&quot;. If OSS was called &quot;great new source&quot; and a new closed source company called itself GreatNewAI you&#x27;d have a similar phenomenon of people taking apart the name.</div><br/><div id="35896916" class="c"><input type="checkbox" id="c-35896916" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896733">parent</a><span>|</span><a href="#35896478">next</a><span>|</span><label class="collapse" for="c-35896916">[-]</label><label class="expand" for="c-35896916">[1 more]</label></div><br/><div class="children"><div class="content">Understandably, people don’t like it when people take the goodwill of an established term and use it to fulfill their directly opposing goals.</div><br/></div></div></div></div><div id="35896478" class="c"><input type="checkbox" id="c-35896478" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896212">parent</a><span>|</span><a href="#35896733">prev</a><span>|</span><a href="#35896899">next</a><span>|</span><label class="collapse" for="c-35896478">[-]</label><label class="expand" for="c-35896478">[1 more]</label></div><br/><div class="children"><div class="content">This. Exactly.</div><br/></div></div></div></div><div id="35896899" class="c"><input type="checkbox" id="c-35896899" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895866">parent</a><span>|</span><a href="#35896212">prev</a><span>|</span><a href="#35896164">next</a><span>|</span><label class="collapse" for="c-35896899">[-]</label><label class="expand" for="c-35896899">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI does not need people defending their scummy pivot.</div><br/><div id="35898752" class="c"><input type="checkbox" id="c-35898752" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896899">parent</a><span>|</span><a href="#35896164">next</a><span>|</span><label class="collapse" for="c-35898752">[-]</label><label class="expand" for="c-35898752">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t agree it&#x27;s scummy. Scummy is getting someone to build a business on a 1 Billion dollar donation, going for a hostile takeover 10% of the way there, then reneging when that doesn&#x27;t work.<p>Salvaging your business from that sort of tantrum by working with MS is called surviving.</div><br/></div></div></div></div><div id="35896164" class="c"><input type="checkbox" id="c-35896164" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895866">parent</a><span>|</span><a href="#35896899">prev</a><span>|</span><a href="#35898950">next</a><span>|</span><label class="collapse" for="c-35896164">[-]</label><label class="expand" for="c-35896164">[2 more]</label></div><br/><div class="children"><div class="content">Guess they should rebrand as AvailableAI then...</div><br/><div id="35896739" class="c"><input type="checkbox" id="c-35896739" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896164">parent</a><span>|</span><a href="#35898950">next</a><span>|</span><label class="collapse" for="c-35896739">[-]</label><label class="expand" for="c-35896739">[1 more]</label></div><br/><div class="children"><div class="content">US healthcare should rebrand as “accessible”, even if it is not “affordable” by millions. With an interesting definition of accessible.</div><br/></div></div></div></div><div id="35898950" class="c"><input type="checkbox" id="c-35898950" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895866">parent</a><span>|</span><a href="#35896164">prev</a><span>|</span><a href="#35897553">next</a><span>|</span><label class="collapse" for="c-35898950">[-]</label><label class="expand" for="c-35898950">[1 more]</label></div><br/><div class="children"><div class="content">It’s “open” as in “open for business”.</div><br/></div></div><div id="35897553" class="c"><input type="checkbox" id="c-35897553" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895866">parent</a><span>|</span><a href="#35898950">prev</a><span>|</span><a href="#35896082">next</a><span>|</span><label class="collapse" for="c-35897553">[-]</label><label class="expand" for="c-35897553">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need to &quot;arrive&quot; at an interpretation. Their name is legacy from when they were &quot;open&quot;.</div><br/></div></div><div id="35896082" class="c"><input type="checkbox" id="c-35896082" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895866">parent</a><span>|</span><a href="#35897553">prev</a><span>|</span><a href="#35894844">next</a><span>|</span><label class="collapse" for="c-35896082">[-]</label><label class="expand" for="c-35896082">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like the famous Facebook hoodie.  &quot;Open and connected&quot; was one slogan on it.  The API can be shut down at any time.<p><a href="https:&#x2F;&#x2F;venturebeat.com&#x2F;social&#x2F;facebook-insignia-hoodie&#x2F;" rel="nofollow">https:&#x2F;&#x2F;venturebeat.com&#x2F;social&#x2F;facebook-insignia-hoodie&#x2F;</a><p>In the end they just shat all over RSS etc.</div><br/></div></div></div></div><div id="35894844" class="c"><input type="checkbox" id="c-35894844" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35894684">parent</a><span>|</span><a href="#35895866">prev</a><span>|</span><a href="#35896575">next</a><span>|</span><label class="collapse" for="c-35894844">[-]</label><label class="expand" for="c-35894844">[5 more]</label></div><br/><div class="children"><div class="content">I wonder how special these architectures are compared to what&#x27;s published.<p>The &quot;secret sauce&quot; may just be getting 2 pages (~200) worth of engineers collaborating and either rolling out your own cloud service or spending $$$ at someone else&#x27;s.<p>Also not sure how much it matters other than academic interest of course. Realistically, there&#x27;s only 4-5 (US) companies with the human resources and capital to roll something similar to these models out for what is most likely a complete write-off?<p>They could claim whatever they wanted and it would be near impossible to validate.</div><br/><div id="35895415" class="c"><input type="checkbox" id="c-35895415" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35894844">parent</a><span>|</span><a href="#35896575">next</a><span>|</span><label class="collapse" for="c-35895415">[-]</label><label class="expand" for="c-35895415">[4 more]</label></div><br/><div class="children"><div class="content">I think the secret sauce is just bucket loads of cash to spend on compute.<p>And because of this I don’t buy that AI is an existential threat to Google at this point.  If they were really worried they could spend a tiny portion of their ~280 billion dollars in revenue to train a bigger model.</div><br/><div id="35895686" class="c"><input type="checkbox" id="c-35895686" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895415">parent</a><span>|</span><a href="#35896575">next</a><span>|</span><label class="collapse" for="c-35895686">[-]</label><label class="expand" for="c-35895686">[3 more]</label></div><br/><div class="children"><div class="content">I assume this is just a PR&#x2F;IR-driven project to stay the &quot;Google is Dead&quot; headlines hence the budget, especially considering an oversized chunk was spent on the scaling law, doesn&#x27;t seem they were serious about building a GPT4-killer.<p>I wasn&#x27;t aware autoregressive LLMs were still considered an existential threat to Google. What&#x27;s the threat supposed to be, ChatGPT is just going to keep eating Google search market share burning Microsoft capital on infra a la the Uber model or do they make money off of that at some point?<p>Seems farfetched OpenAI can compete with Google&#x27;s resources, vertical integration down to the TPU and access to significantly more training data.</div><br/><div id="35896089" class="c"><input type="checkbox" id="c-35896089" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35895686">parent</a><span>|</span><a href="#35896575">next</a><span>|</span><label class="collapse" for="c-35896089">[-]</label><label class="expand" for="c-35896089">[2 more]</label></div><br/><div class="children"><div class="content">I agree that if training data is what matters, it is likely that no one can compete with Google with Google Books, which scanned 25 million volumes (source: <a href="http:&#x2F;&#x2F;www.nytimes.com&#x2F;2015&#x2F;10&#x2F;29&#x2F;arts&#x2F;international&#x2F;google-books-a-complex-and-controversial-experiment.html" rel="nofollow">http:&#x2F;&#x2F;www.nytimes.com&#x2F;2015&#x2F;10&#x2F;29&#x2F;arts&#x2F;international&#x2F;google-...</a>), which is approximately all the books.<p>DeepMind&#x27;s RETRO paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2112.04426" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2112.04426</a> mentions a dataset called MassiveText, which includes 20 million books of 3T tokens. So we know Google is using Google Books, since there is simply no other source of 20 million books. Also as far as I know 3T tokens is more than publicly known to be used by anyone so far: Google could train on more data than anyone else, solely from Google Books, even without using its web crawl.<p>Edit: it was 2005(!), so it is possible that many of you haven&#x27;t heard of this. George Dyson, in Turing&#x27;s Cathedral written in 2005 says:<p>&gt; My visit to Google? Despite the whimsical furniture and other toys, I felt I was entering a 14th-century cathedral: not in the 14th century but in the 12th century, while it was being built. Everyone was busy carving one stone here and another stone there, with some invisible architect getting everything to fit. The mood was playful, yet there was a palpable reverence in the air. &quot;We are not scanning all those books to be read by people,&quot; explained one of my hosts after my talk. &quot;We are scanning them to be read by an AI.&quot;<p><a href="https:&#x2F;&#x2F;www.edge.org&#x2F;conversation&#x2F;george_dyson-turings-cathedral" rel="nofollow">https:&#x2F;&#x2F;www.edge.org&#x2F;conversation&#x2F;george_dyson-turings-cathe...</a><p>Read the whole thing. It is not an accident Google got Google Books to train AI. That was the plan from the start.</div><br/><div id="35898671" class="c"><input type="checkbox" id="c-35898671" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#35894597">root</a><span>|</span><a href="#35896089">parent</a><span>|</span><a href="#35896575">next</a><span>|</span><label class="collapse" for="c-35898671">[-]</label><label class="expand" for="c-35898671">[1 more]</label></div><br/><div class="children"><div class="content">And YouTube. It&#x27;s quite big datasets. Books is great for higher quality source.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35896575" class="c"><input type="checkbox" id="c-35896575" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#35894597">parent</a><span>|</span><a href="#35894684">prev</a><span>|</span><a href="#35898309">next</a><span>|</span><label class="collapse" for="c-35896575">[-]</label><label class="expand" for="c-35896575">[1 more]</label></div><br/><div class="children"><div class="content">That will be the norm moving forward<p>LLM is going to make money, a lot of money, nobody is going to give away their secret sauce for free.<p>Prepare for the landscape to get really ugly and really soon. Maybe we will witness some epic legal battle around big techs.</div><br/></div></div><div id="35898309" class="c"><input type="checkbox" id="c-35898309" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#35894597">parent</a><span>|</span><a href="#35896575">prev</a><span>|</span><a href="#35896991">next</a><span>|</span><label class="collapse" for="c-35898309">[-]</label><label class="expand" for="c-35898309">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, this is a holdover from where LLMs grew out of: academia. &quot;Technical report&quot; is what you reach for when you don&#x27;t want to compare to actual competitive baselines.</div><br/></div></div><div id="35896991" class="c"><input type="checkbox" id="c-35896991" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#35894597">parent</a><span>|</span><a href="#35898309">prev</a><span>|</span><a href="#35892215">next</a><span>|</span><label class="collapse" for="c-35896991">[-]</label><label class="expand" for="c-35896991">[1 more]</label></div><br/><div class="children"><div class="content">they mentioned secret sauce: scaling laws and UL2.</div><br/></div></div></div></div><div id="35892215" class="c"><input type="checkbox" id="c-35892215" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#35894597">prev</a><span>|</span><a href="#35898108">next</a><span>|</span><label class="collapse" for="c-35892215">[-]</label><label class="expand" for="c-35892215">[12 more]</label></div><br/><div class="children"><div class="content">Surprisingly, their scaling law analysis still focuses on training FLOPs instead of training + inference FLOPs.<p>That said, they do mention this:<p>&gt; <i>The largest model in the PaLM 2 family, PaLM 2-L, is significantly smaller than the largest PaLM model but uses more training compute. [A] smaller but higher quality model significantly improves inference efficiency, reduces serving cost, and enables the model’s downstream application for more applications and users</i><p>It makes me think they are Chinchilla-optimal, which would make sense for a research project, but not for shipping to users. I am surprised they didn’t train to the validation loss plateau.</div><br/><div id="35894300" class="c"><input type="checkbox" id="c-35894300" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#35892215">parent</a><span>|</span><a href="#35892578">next</a><span>|</span><label class="collapse" for="c-35894300">[-]</label><label class="expand" for="c-35894300">[1 more]</label></div><br/><div class="children"><div class="content">Depends on your goal, if it&#x27;s to overtake OpenAI as having the best model overall it makes sense to optimize for training loss alone (assuming a fixed upfront compute budget).<p>Optimizing for inference to achieve the same loss would require more compute overall so you&#x27;re either paying upfront with higher training costs or kicking the can down the road to inference.<p>News articles estimates of GPT4 cost seem to peg it at ~8 months of inference to achieve 1:1 cost with training. Life span of these models is TBD but it&#x27;s a pretty safe bet we&#x27;ll have new ones by then. Of course GPT3.5 is still getting used but probably won&#x27;t cross 2:1ish in its lifetime.<p>Might as well roll the dice and kick the can down the road if you&#x27;re Google, I imagine they would happily pay an extra 500k&#x2F;day in inference compute to be market leaders, whats 183mill for them? But if they don&#x27;t get any real market share or the model sucks they saved substantially on training.<p>&gt; It makes me think they are Chinchilla-optimal,<p>They elaborate in the appendix but they empirically determine PaLM-optimal, which concurs with Chinchilla-optimal (more or less).</div><br/></div></div><div id="35892578" class="c"><input type="checkbox" id="c-35892578" checked=""/><div class="controls bullet"><span class="by">jumpCastle</span><span>|</span><a href="#35892215">parent</a><span>|</span><a href="#35894300">prev</a><span>|</span><a href="#35892941">next</a><span>|</span><label class="collapse" for="c-35892578">[-]</label><label class="expand" for="c-35892578">[9 more]</label></div><br/><div class="children"><div class="content">Optimazing for training could help distillation also.</div><br/><div id="35892739" class="c"><input type="checkbox" id="c-35892739" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35892578">parent</a><span>|</span><a href="#35892941">next</a><span>|</span><label class="collapse" for="c-35892739">[-]</label><label class="expand" for="c-35892739">[8 more]</label></div><br/><div class="children"><div class="content">I agree distillation is the wild card. The question is whether distillation works for LLM. I am not aware of any public report of successful distillation of LLM (I searched quite hard for this; if you know of any and can tell me I would be very grateful), and I interpreted it to mean that it doesn&#x27;t work yet and negative results are not published due to publication bias.</div><br/><div id="35892836" class="c"><input type="checkbox" id="c-35892836" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35892739">parent</a><span>|</span><a href="#35894016">next</a><span>|</span><label class="collapse" for="c-35892836">[-]</label><label class="expand" for="c-35892836">[2 more]</label></div><br/><div class="children"><div class="content">This was published here in HN last week: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35810663" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35810663</a><p>Don&#x27;t know if there any public technical reports by any of the big AI companies about this, as its pretty new.</div><br/><div id="35893994" class="c"><input type="checkbox" id="c-35893994" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35892836">parent</a><span>|</span><a href="#35894016">next</a><span>|</span><label class="collapse" for="c-35893994">[-]</label><label class="expand" for="c-35893994">[1 more]</label></div><br/><div class="children"><div class="content">No, distilling step-by-step <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.02301" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.02301</a> distills LLM to task specific model. That works, and I know of multiple successes. But it doesn&#x27;t relate to choice of optimizing training FLOP vs training and inference FLOP, since the resulting distilled model is not LLM.</div><br/></div></div></div></div><div id="35894016" class="c"><input type="checkbox" id="c-35894016" checked=""/><div class="controls bullet"><span class="by">jumpCastle</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35892739">parent</a><span>|</span><a href="#35892836">prev</a><span>|</span><a href="#35893987">next</a><span>|</span><label class="collapse" for="c-35894016">[-]</label><label class="expand" for="c-35894016">[3 more]</label></div><br/><div class="children"><div class="content">The name 3.5-turbo sounds to me like it implies distillation. The release notes at the time also hinted at it IIRC.</div><br/><div id="35894089" class="c"><input type="checkbox" id="c-35894089" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35894016">parent</a><span>|</span><a href="#35893987">next</a><span>|</span><label class="collapse" for="c-35894089">[-]</label><label class="expand" for="c-35894089">[2 more]</label></div><br/><div class="children"><div class="content">Well, that&#x27;s why I said public. Personally, I don&#x27;t think release notes <a href="https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;6825453-chatgpt-release-notes" rel="nofollow">https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;6825453-chatgpt-release-...</a> hinted at any such thing, and I think quantization is more likely than distillation.</div><br/><div id="35894198" class="c"><input type="checkbox" id="c-35894198" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35894089">parent</a><span>|</span><a href="#35893987">next</a><span>|</span><label class="collapse" for="c-35894198">[-]</label><label class="expand" for="c-35894198">[1 more]</label></div><br/><div class="children"><div class="content">Does the turbo API being 10 times cheaper than davinci imply anything? It implies more than just quantisation to me.</div><br/></div></div></div></div></div></div><div id="35893987" class="c"><input type="checkbox" id="c-35893987" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35892739">parent</a><span>|</span><a href="#35894016">prev</a><span>|</span><a href="#35892941">next</a><span>|</span><label class="collapse" for="c-35893987">[-]</label><label class="expand" for="c-35893987">[2 more]</label></div><br/><div class="children"><div class="content">Off the top of my head there&#x27;s DistilBERT from awhile back.  I also recall distilled GPT-2 models from before the GPT-3 times.</div><br/><div id="35894058" class="c"><input type="checkbox" id="c-35894058" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35892215">root</a><span>|</span><a href="#35893987">parent</a><span>|</span><a href="#35892941">next</a><span>|</span><label class="collapse" for="c-35894058">[-]</label><label class="expand" for="c-35894058">[1 more]</label></div><br/><div class="children"><div class="content">Yes, DistilBERT <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.01108" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.01108</a> is in fact the closest case I know of. But it is too small (distilling from 110M to 66M) and both BERT and DistilBERT is intended to be used (and benchmarked) with separate fine tuning for specific tasks, so they are not general.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35898108" class="c"><input type="checkbox" id="c-35898108" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#35892215">prev</a><span>|</span><a href="#35893033">next</a><span>|</span><label class="collapse" for="c-35898108">[-]</label><label class="expand" for="c-35898108">[4 more]</label></div><br/><div class="children"><div class="content">May be a weird takeaway, but I did find it strange how much the whole report focussed on misgendering as a safety issue.<p>I agree it’s important to get right, but it seems like one of hundreds of safety&#x2F;alignment issues and that many others are de-emphasised or ignored.</div><br/><div id="35898457" class="c"><input type="checkbox" id="c-35898457" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#35898108">parent</a><span>|</span><a href="#35898652">next</a><span>|</span><label class="collapse" for="c-35898457">[-]</label><label class="expand" for="c-35898457">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t find it to be a particularly notable issue relative to the rest of the issues they mentioned. It didn&#x27;t seem to be overrepresented to me...<p>That said, it&#x27;s something that is more controllable across languages. All people, in all languages, have a roughly equal distribution of genders, but not race&#x2F;religion, etc. Japanese language text will have similar gender distributions to English, but likely not equal distributions discussing race. That makes it a much better litmus test for multi-lingual bias.<p>Most of the misgendering discussion (2-3 paragraphs?) was in the translation section, which makes sense. A lot of the first classes in foundation courses learning a foreign language revolved around pronouns (which don&#x27;t work the same in every language). Gender may be implied or absent in some. For example, to say &quot;she is a doctor&quot; in Italian, you might say &quot;è un dottore&quot;, which has no pronoun (literally &quot;is a doctor&quot;). If you use google translate to make it English, &quot;he&quot; is added, assuming the gender. The potential for bias here is obvious, but consider that LLMs often deal with more context than a single sentence -  if you&#x27;re translating or writing story about a female doctor (where the gender is available contextually), you want all the use of pronouns to align where it makes sense. If a LLM didn&#x27;t &quot;understand&quot; the pronoun in Italian, you might not recognize it, but in English, if the same person&#x27;s gender was mixed across sentences, it&#x27;d be hard to read.</div><br/></div></div><div id="35898652" class="c"><input type="checkbox" id="c-35898652" checked=""/><div class="controls bullet"><span class="by">noname120</span><span>|</span><a href="#35898108">parent</a><span>|</span><a href="#35898457">prev</a><span>|</span><a href="#35898425">next</a><span>|</span><label class="collapse" for="c-35898652">[-]</label><label class="expand" for="c-35898652">[1 more]</label></div><br/><div class="children"><div class="content">Generalized artificial intelligence (AGI) comes with very real x-risks[1] (existential risks) and s-risks[2] (suffering risks).<p>An expert survey of 738 researchers who published in NeurIPS and ICML was done last year[3]. Their median estimation that AI will have an “extremely bad” long term outcome is 5%, and 48% of the researchers estimate the probability to be at least 10%. This is worryingly high considering the absolutely catastrophic consequences of the scenarios.<p>A minority of very vocal AI researchers (e.g. Yann LeCun) dismiss these risks entirely and claim that people read too much science fiction. But when you listen to their interviews it&#x27;s very clear that they have no idea what they are talking about and never actually read any scientific literature on the subject.<p>The study of AI risks is a serious area of academic research that is worked on by labs from Stanford[4], Berkeley[5], Carnegie Melon University[6], Oxford[7], Cambridge[8], and many MANY other universities[9]. Not people who read too much science fiction.<p>————<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2206.05862.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2206.05862.pdf</a><p>[2] <a href="https:&#x2F;&#x2F;longtermrisk.org&#x2F;files&#x2F;Sotala-Gloor-Superintelligent-AI-and-Suffering-Risks.pdf#page=8&amp;search=%22superintelligence%22" rel="nofollow">https:&#x2F;&#x2F;longtermrisk.org&#x2F;files&#x2F;Sotala-Gloor-Superintelligent...</a><p>[3] <a href="https:&#x2F;&#x2F;aiimpacts.org&#x2F;2022-expert-survey-on-progress-in-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aiimpacts.org&#x2F;2022-expert-survey-on-progress-in-ai&#x2F;</a><p>[4] <a href="https:&#x2F;&#x2F;web.stanford.edu&#x2F;~chadj&#x2F;existentialrisk.pdf" rel="nofollow">https:&#x2F;&#x2F;web.stanford.edu&#x2F;~chadj&#x2F;existentialrisk.pdf</a><p>[5] <a href="https:&#x2F;&#x2F;humancompatible.ai&#x2F;about&#x2F;" rel="nofollow">https:&#x2F;&#x2F;humancompatible.ai&#x2F;about&#x2F;</a><p>[6] <a href="https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~focal&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~focal&#x2F;</a><p>[7] <a href="https:&#x2F;&#x2F;www.fhi.ox.ac.uk&#x2F;research&#x2F;research-areas&#x2F;#aisafety_tab" rel="nofollow">https:&#x2F;&#x2F;www.fhi.ox.ac.uk&#x2F;research&#x2F;research-areas&#x2F;#aisafety_t...</a><p>[8] <a href="https:&#x2F;&#x2F;www.camxrisk.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.camxrisk.org&#x2F;</a><p>[9] <a href="https:&#x2F;&#x2F;futureoflife.org&#x2F;about-us&#x2F;our-people&#x2F;ai-existential-safety-community&#x2F;" rel="nofollow">https:&#x2F;&#x2F;futureoflife.org&#x2F;about-us&#x2F;our-people&#x2F;ai-existential-...</a></div><br/></div></div></div></div><div id="35893033" class="c"><input type="checkbox" id="c-35893033" checked=""/><div class="controls bullet"><span class="by">jp42</span><span>|</span><a href="#35898108">prev</a><span>|</span><a href="#35892466">next</a><span>|</span><label class="collapse" for="c-35893033">[-]</label><label class="expand" for="c-35893033">[16 more]</label></div><br/><div class="children"><div class="content">personal experience -  I&#x27;m using GPT4 for writing code especially in python. After using bard today, I feel bard is doing quite well considering its free. I will keep using it and if its keep doing well, I will cancel GPT4 $20&#x2F;month subscription.</div><br/><div id="35897094" class="c"><input type="checkbox" id="c-35897094" checked=""/><div class="controls bullet"><span class="by">getpost</span><span>|</span><a href="#35893033">parent</a><span>|</span><a href="#35893634">next</a><span>|</span><label class="collapse" for="c-35897094">[-]</label><label class="expand" for="c-35897094">[1 more]</label></div><br/><div class="children"><div class="content">Early this evening, I asked Bard if was updated to PaLM 2, and it said it was. I then asked it to write some Python programs, giving it more or less the same prompts I&#x27;ve given GPT4. Bard doesn&#x27;t seem to be any better than it was a couple weeks ago in the cases I tried, and nowhere near as capable at GPT4. And it goes off the rails quickly. After even a short dialog (~5 statements), it becomes less and less able to stay on track and make coherent corrections to the code.</div><br/></div></div><div id="35893634" class="c"><input type="checkbox" id="c-35893634" checked=""/><div class="controls bullet"><span class="by">vaughnegut</span><span>|</span><a href="#35893033">parent</a><span>|</span><a href="#35897094">prev</a><span>|</span><a href="#35893516">next</a><span>|</span><label class="collapse" for="c-35893634">[-]</label><label class="expand" for="c-35893634">[2 more]</label></div><br/><div class="children"><div class="content">You can use gpt-4 for free (toggle &quot;Use best model&quot;), and it&#x27;ll search the internet and state sources on <a href="https:&#x2F;&#x2F;phind.com">https:&#x2F;&#x2F;phind.com</a><p>No idea when they&#x27;ll start charging, but it&#x27;s replaced a lot of my googling at work</div><br/><div id="35895365" class="c"><input type="checkbox" id="c-35895365" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893634">parent</a><span>|</span><a href="#35893516">next</a><span>|</span><label class="collapse" for="c-35895365">[-]</label><label class="expand" for="c-35895365">[1 more]</label></div><br/><div class="children"><div class="content">I get very different results from phind vs chatgpt4.</div><br/></div></div></div></div><div id="35893516" class="c"><input type="checkbox" id="c-35893516" checked=""/><div class="controls bullet"><span class="by">gekoxyz</span><span>|</span><a href="#35893033">parent</a><span>|</span><a href="#35893634">prev</a><span>|</span><a href="#35892466">next</a><span>|</span><label class="collapse" for="c-35893516">[-]</label><label class="expand" for="c-35893516">[12 more]</label></div><br/><div class="children"><div class="content">why don&#x27;t you just use chatGPT? from what i know it&#x27;s running GPT3.5 and it&#x27;s not that different (at least in terms of code quality)</div><br/><div id="35899557" class="c"><input type="checkbox" id="c-35899557" checked=""/><div class="controls bullet"><span class="by">ilitirit</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893516">parent</a><span>|</span><a href="#35899342">next</a><span>|</span><label class="collapse" for="c-35899557">[-]</label><label class="expand" for="c-35899557">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a <i>massive</i> difference in <i>response</i> quality in my experience.<p>For example, I asked 3.5 to find a bug in a lengthy piece of Javascript. It said it&#x27;s hard to give a correct answer because it doesn&#x27;t know what the HTML or CSS looks like.<p>GPT4 spotted the bug almost immediately (it didn&#x27;t manage to fix it though).</div><br/></div></div><div id="35899342" class="c"><input type="checkbox" id="c-35899342" checked=""/><div class="controls bullet"><span class="by">ivalm</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893516">parent</a><span>|</span><a href="#35899557">prev</a><span>|</span><a href="#35896042">next</a><span>|</span><label class="collapse" for="c-35899342">[-]</label><label class="expand" for="c-35899342">[1 more]</label></div><br/><div class="children"><div class="content">As someone writing my first meaningful react app, code quality from gpt4 is monstrously better than 3.5. With gpt 4 i can  often paste entire components and get meaningful corrections&#x2F;bug fixes&#x2F;non-trivial refactors. 3.5 just does a loop of mistaken fixes while it runs out of context length.</div><br/></div></div><div id="35896042" class="c"><input type="checkbox" id="c-35896042" checked=""/><div class="controls bullet"><span class="by">qumpis</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893516">parent</a><span>|</span><a href="#35899342">prev</a><span>|</span><a href="#35893659">next</a><span>|</span><label class="collapse" for="c-35896042">[-]</label><label class="expand" for="c-35896042">[3 more]</label></div><br/><div class="children"><div class="content">When my 25 queries per 3 hours runs out I don&#x27;t use openai at all. That&#x27;s how bad chat gpt is in comparison to gpt4 in my use cases.</div><br/><div id="35896507" class="c"><input type="checkbox" id="c-35896507" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35896042">parent</a><span>|</span><a href="#35896502">next</a><span>|</span><label class="collapse" for="c-35896507">[-]</label><label class="expand" for="c-35896507">[1 more]</label></div><br/><div class="children"><div class="content">My biggest complaint is the speed. Watching it print out like 56k is pretty annoying when coding.</div><br/></div></div></div></div><div id="35893659" class="c"><input type="checkbox" id="c-35893659" checked=""/><div class="controls bullet"><span class="by">jumpCastle</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893516">parent</a><span>|</span><a href="#35896042">prev</a><span>|</span><a href="#35894965">next</a><span>|</span><label class="collapse" for="c-35893659">[-]</label><label class="expand" for="c-35893659">[4 more]</label></div><br/><div class="children"><div class="content">In my experiments bard is weaker than 3.5, but if it wasn&#x27;t, than I would prefer the fresh data of bard.</div><br/><div id="35893997" class="c"><input type="checkbox" id="c-35893997" checked=""/><div class="controls bullet"><span class="by">almog</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893659">parent</a><span>|</span><a href="#35893965">next</a><span>|</span><label class="collapse" for="c-35893997">[-]</label><label class="expand" for="c-35893997">[1 more]</label></div><br/><div class="children"><div class="content">One area where I noticed Bard was clearly behind (at least without crafting a better prompt) is getting from half-working program to a running program then sometime even to a correct program (I was using Python).<p>With GPT 3.5 and 4, I was able to just paste in the error and it&#x27;d do the rest. Bard however tried to tell me what the error could be, and wouldn&#x27;t do well even when asked to fix the code.<p>Even GPT 4 though, when asked to go from specs to tests + code, would get stuck in a loop of making one test pass only to make the other pass and vice versa.
The program I tried to let it write was a query validator that can test whether a string matches a pattern that uses AND, OR and NOT.<p>It did well on parsing my specs into tests, but from there on it didn&#x27;t go very well.</div><br/></div></div><div id="35893965" class="c"><input type="checkbox" id="c-35893965" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893659">parent</a><span>|</span><a href="#35893997">prev</a><span>|</span><a href="#35894965">next</a><span>|</span><label class="collapse" for="c-35893965">[-]</label><label class="expand" for="c-35893965">[2 more]</label></div><br/><div class="children"><div class="content">What is its training data cutoff date?</div><br/><div id="35897117" class="c"><input type="checkbox" id="c-35897117" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893965">parent</a><span>|</span><a href="#35894965">next</a><span>|</span><label class="collapse" for="c-35897117">[-]</label><label class="expand" for="c-35897117">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know (both for previous model LaMDA and new model PaLM 2), but it is less important for Bard because Bard has access to live data from Google search.</div><br/></div></div></div></div></div></div><div id="35894965" class="c"><input type="checkbox" id="c-35894965" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35893033">root</a><span>|</span><a href="#35893516">parent</a><span>|</span><a href="#35893659">prev</a><span>|</span><a href="#35893625">next</a><span>|</span><label class="collapse" for="c-35894965">[-]</label><label class="expand" for="c-35894965">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s quite a vast difference between GPT-3.5 and GPT-4</div><br/></div></div></div></div></div></div><div id="35892466" class="c"><input type="checkbox" id="c-35892466" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#35893033">prev</a><span>|</span><a href="#35897175">next</a><span>|</span><label class="collapse" for="c-35892466">[-]</label><label class="expand" for="c-35892466">[29 more]</label></div><br/><div class="children"><div class="content">So how do we actually try out the PaLM 2?<p>The links in their press release just link to their other press release, and if I google &quot;PaLM API&quot; it just gives me more press release, but I just couldn&#x27;t find the actual document for their PaLM API.<p>How do I actually google the &quot;PaLM API&quot; for a way to test &quot;PaLM 2&quot;?</div><br/><div id="35892905" class="c"><input type="checkbox" id="c-35892905" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#35892466">parent</a><span>|</span><a href="#35892525">next</a><span>|</span><label class="collapse" for="c-35892905">[-]</label><label class="expand" for="c-35892905">[15 more]</label></div><br/><div class="children"><div class="content">Google&#x27;s docs on the APIs are up: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;docs&#x2F;generative-ai&#x2F;learn&#x2F;overview" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;docs&#x2F;generative-ai&#x2F;learn&#x2F;...</a><p>The pricing is also now listed but free during the trial period, although it&#x27;s annoyingly priced by character: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;pricing#generative_ai_models" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;pricing#generative_ai_mod...</a><p>Assuming ChatGPT&#x27;s tokens are the equivalent of 4 characters on average (a fair assumption), the pricing of PaLM&#x27;s chat and embedding APIs are the same cost as OpenAI&#x27;s equivalents.</div><br/><div id="35892931" class="c"><input type="checkbox" id="c-35892931" checked=""/><div class="controls bullet"><span class="by">ntonozzi</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892905">parent</a><span>|</span><a href="#35898185">next</a><span>|</span><label class="collapse" for="c-35892931">[-]</label><label class="expand" for="c-35892931">[8 more]</label></div><br/><div class="children"><div class="content">Why would that be annoying? It’s much easier to understand, predict and truncate appropriately than having to explain all of these different tokenization schemes to devs.</div><br/><div id="35892989" class="c"><input type="checkbox" id="c-35892989" checked=""/><div class="controls bullet"><span class="by">rcoveson</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892931">parent</a><span>|</span><a href="#35898185">next</a><span>|</span><label class="collapse" for="c-35892989">[-]</label><label class="expand" for="c-35892989">[7 more]</label></div><br/><div class="children"><div class="content">Yeah, everybody agrees on what a character is, right? It&#x27;s just {an ASCII byte|a UTF8 code unit|a UTF16 code unit|a Unicode code point|a Unicode grapheme}.</div><br/><div id="35894432" class="c"><input type="checkbox" id="c-35894432" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892989">parent</a><span>|</span><a href="#35893002">next</a><span>|</span><label class="collapse" for="c-35894432">[-]</label><label class="expand" for="c-35894432">[2 more]</label></div><br/><div class="children"><div class="content">And we think tokens solve that problem? Spoiler alert: they don’t<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;OpenAI&#x2F;comments&#x2F;124v2oi&#x2F;hindi_8_times_more_expensive_than_english_the&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;OpenAI&#x2F;comments&#x2F;124v2oi&#x2F;hindi_8_tim...</a></div><br/><div id="35895737" class="c"><input type="checkbox" id="c-35895737" checked=""/><div class="controls bullet"><span class="by">est31</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35894432">parent</a><span>|</span><a href="#35893002">next</a><span>|</span><label class="collapse" for="c-35895737">[-]</label><label class="expand" for="c-35895737">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t but Google could have been more precise with which of the definitions listed by GP they mean by &quot;character&quot;.</div><br/></div></div></div></div><div id="35893002" class="c"><input type="checkbox" id="c-35893002" checked=""/><div class="controls bullet"><span class="by">ntonozzi</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892989">parent</a><span>|</span><a href="#35894432">prev</a><span>|</span><a href="#35893232">next</a><span>|</span><label class="collapse" for="c-35893002">[-]</label><label class="expand" for="c-35893002">[3 more]</label></div><br/><div class="children"><div class="content">I’m not saying it’s easy but it’s much better than tokens IMO. I think bytes would be understandable too.</div><br/><div id="35893309" class="c"><input type="checkbox" id="c-35893309" checked=""/><div class="controls bullet"><span class="by">criddell</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35893002">parent</a><span>|</span><a href="#35893232">next</a><span>|</span><label class="collapse" for="c-35893309">[-]</label><label class="expand" for="c-35893309">[2 more]</label></div><br/><div class="children"><div class="content">Bytes are understandable but make no sense from a business point of view. If you submit the same simple query with UTF-8 and UTF-32, the latter will cost 4x as much.</div><br/><div id="35893832" class="c"><input type="checkbox" id="c-35893832" checked=""/><div class="controls bullet"><span class="by">xyzzyz</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35893309">parent</a><span>|</span><a href="#35893232">next</a><span>|</span><label class="collapse" for="c-35893832">[-]</label><label class="expand" for="c-35893832">[1 more]</label></div><br/><div class="children"><div class="content">No API accepts input in UTF-32. Nobody uses this on the internet.</div><br/></div></div></div></div></div></div><div id="35893232" class="c"><input type="checkbox" id="c-35893232" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892989">parent</a><span>|</span><a href="#35893002">prev</a><span>|</span><a href="#35898185">next</a><span>|</span><label class="collapse" for="c-35893232">[-]</label><label class="expand" for="c-35893232">[1 more]</label></div><br/><div class="children"><div class="content">At least there are standards for characters. Nothing like that for tokens.</div><br/></div></div></div></div></div></div><div id="35898185" class="c"><input type="checkbox" id="c-35898185" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892905">parent</a><span>|</span><a href="#35892931">prev</a><span>|</span><a href="#35898703">next</a><span>|</span><label class="collapse" for="c-35898185">[-]</label><label class="expand" for="c-35898185">[1 more]</label></div><br/><div class="children"><div class="content">Per token might be 4 characters on average, but that can vary wildly. Pricing per character is easier to understand and means more flexibility to change tokenisation without affecting pricing. So far OpenAI has charged very different prices per model, but I expect we’ll see more granular changes in the future that might not change pricing… except for changing the tokenisation.</div><br/></div></div><div id="35898703" class="c"><input type="checkbox" id="c-35898703" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892905">parent</a><span>|</span><a href="#35898185">prev</a><span>|</span><a href="#35893524">next</a><span>|</span><label class="collapse" for="c-35898703">[-]</label><label class="expand" for="c-35898703">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s happy pricing as a Japanese user, maybe also for Korean and Chinese.</div><br/></div></div><div id="35893524" class="c"><input type="checkbox" id="c-35893524" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892905">parent</a><span>|</span><a href="#35898703">prev</a><span>|</span><a href="#35892525">next</a><span>|</span><label class="collapse" for="c-35893524">[-]</label><label class="expand" for="c-35893524">[4 more]</label></div><br/><div class="children"><div class="content">There is a limit of maxOutputTokens, 1024! Is this the true capability of PaLM 2?<p>However I couldn&#x27;t find anything about the context length of their model anywhere. And the API didn&#x27;t tell me how long the prompt could be.</div><br/><div id="35894369" class="c"><input type="checkbox" id="c-35894369" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35893524">parent</a><span>|</span><a href="#35892525">next</a><span>|</span><label class="collapse" for="c-35894369">[-]</label><label class="expand" for="c-35894369">[3 more]</label></div><br/><div class="children"><div class="content">No. Autoregressive models don&#x27;t have model specific limit to output tokens, it&#x27;s just when to stop looping.</div><br/><div id="35896381" class="c"><input type="checkbox" id="c-35896381" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35894369">parent</a><span>|</span><a href="#35892525">next</a><span>|</span><label class="collapse" for="c-35896381">[-]</label><label class="expand" for="c-35896381">[2 more]</label></div><br/><div class="children"><div class="content">I guess I only know transformers and how BERT or GPT works, as there would be a limit in the context length. With GPT, you can certainly generate infinite amount of tokens but the previous tokens outside of the maximum context length would be outside of the context window. LLaMa has 2k, GPT-4 has 32k.<p>Are you saying I can give unlimited tokens to PaLM and generate unlimited amount of tokens? So PaLM doesn&#x27;t have a context limit?</div><br/><div id="35896881" class="c"><input type="checkbox" id="c-35896881" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35896381">parent</a><span>|</span><a href="#35892525">next</a><span>|</span><label class="collapse" for="c-35896881">[-]</label><label class="expand" for="c-35896881">[1 more]</label></div><br/><div class="children"><div class="content">No, I am not saying that. Since PaLM 2 is a transformer model (they didn&#x27;t disclose almost anything about the model architecture, but they did disclose that), it has a context length limit. What I am saying is that you can&#x27;t infer that limit from the limit of maxOutputTokens parameter in the API.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35892525" class="c"><input type="checkbox" id="c-35892525" checked=""/><div class="controls bullet"><span class="by">shikkra</span><span>|</span><a href="#35892466">parent</a><span>|</span><a href="#35892905">prev</a><span>|</span><a href="#35892533">next</a><span>|</span><label class="collapse" for="c-35892525">[-]</label><label class="expand" for="c-35892525">[1 more]</label></div><br/><div class="children"><div class="content">You can sign up for the waitlist at g.co&#x2F;palm</div><br/></div></div><div id="35892533" class="c"><input type="checkbox" id="c-35892533" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#35892466">parent</a><span>|</span><a href="#35892525">prev</a><span>|</span><a href="#35894908">next</a><span>|</span><label class="collapse" for="c-35892533">[-]</label><label class="expand" for="c-35892533">[9 more]</label></div><br/><div class="children"><div class="content">It should be live on Bard.</div><br/><div id="35892945" class="c"><input type="checkbox" id="c-35892945" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892533">parent</a><span>|</span><a href="#35897771">next</a><span>|</span><label class="collapse" for="c-35892945">[-]</label><label class="expand" for="c-35892945">[7 more]</label></div><br/><div class="children"><div class="content">But Google hasn&#x27;t disclosed which version of Bard, right?<p>I pop into Bard every once in a while to test its performance, but I never know if I&#x27;m getting the best Google has or just what Google can tolerate running cost-wise publicly given they potentially have at least an order of magnitude (if not two, edit: 1.5) more users than OpenAI.</div><br/><div id="35893223" class="c"><input type="checkbox" id="c-35893223" checked=""/><div class="controls bullet"><span class="by">spullara</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892945">parent</a><span>|</span><a href="#35893226">next</a><span>|</span><label class="collapse" for="c-35893223">[-]</label><label class="expand" for="c-35893223">[3 more]</label></div><br/><div class="children"><div class="content">I am sure that Bard has far fewer users than OpenAI.</div><br/><div id="35893414" class="c"><input type="checkbox" id="c-35893414" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35893223">parent</a><span>|</span><a href="#35893226">next</a><span>|</span><label class="collapse" for="c-35893414">[-]</label><label class="expand" for="c-35893414">[2 more]</label></div><br/><div class="children"><div class="content">Oh absolutely, I&#x27;m just imagining what I might think if I was a super conservative director at Google who is accountable for the balance sheet of a large org.</div><br/><div id="35894320" class="c"><input type="checkbox" id="c-35894320" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35893414">parent</a><span>|</span><a href="#35893226">next</a><span>|</span><label class="collapse" for="c-35894320">[-]</label><label class="expand" for="c-35894320">[1 more]</label></div><br/><div class="children"><div class="content">If that were the case you’d be too busy fighting over head count, trying to hit the VP rung, and internal empire building to do any actual work.</div><br/></div></div></div></div></div></div><div id="35893226" class="c"><input type="checkbox" id="c-35893226" checked=""/><div class="controls bullet"><span class="by">apetresc</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892945">parent</a><span>|</span><a href="#35893223">prev</a><span>|</span><a href="#35897771">next</a><span>|</span><label class="collapse" for="c-35893226">[-]</label><label class="expand" for="c-35893226">[3 more]</label></div><br/><div class="children"><div class="content">Given that ChatGPT has allegedly 100M users, two orders of magnitude more than that would be larger than the global population. Even if we count everyone with a Google account as a potential user of PaLM, that can&#x27;t be true.</div><br/><div id="35893445" class="c"><input type="checkbox" id="c-35893445" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35893226">parent</a><span>|</span><a href="#35893532">next</a><span>|</span><label class="collapse" for="c-35893445">[-]</label><label class="expand" for="c-35893445">[1 more]</label></div><br/><div class="children"><div class="content">Ah yeah, I had the outdated 30M in my head.</div><br/></div></div><div id="35893532" class="c"><input type="checkbox" id="c-35893532" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35893226">parent</a><span>|</span><a href="#35893445">prev</a><span>|</span><a href="#35897771">next</a><span>|</span><label class="collapse" for="c-35893532">[-]</label><label class="expand" for="c-35893532">[1 more]</label></div><br/><div class="children"><div class="content">chat gpt had 100m users in feb. safe to assume it has at least 2-5xed since</div><br/></div></div></div></div></div></div><div id="35897771" class="c"><input type="checkbox" id="c-35897771" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#35892466">root</a><span>|</span><a href="#35892533">parent</a><span>|</span><a href="#35892945">prev</a><span>|</span><a href="#35894908">next</a><span>|</span><label class="collapse" for="c-35897771">[-]</label><label class="expand" for="c-35897771">[1 more]</label></div><br/><div class="children"><div class="content">Bard is not live though.<p>&gt; Bard isn&#x27;t currently supported in your country. Stay tuned!<p>It has been months…</div><br/></div></div></div></div><div id="35894908" class="c"><input type="checkbox" id="c-35894908" checked=""/><div class="controls bullet"><span class="by">throwaway29303</span><span>|</span><a href="#35892466">parent</a><span>|</span><a href="#35892533">prev</a><span>|</span><a href="#35892529">next</a><span>|</span><label class="collapse" for="c-35894908">[-]</label><label class="expand" for="c-35894908">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;developers.generativeai.google&#x2F;guide" rel="nofollow">https:&#x2F;&#x2F;developers.generativeai.google&#x2F;guide</a></div><br/></div></div><div id="35892529" class="c"><input type="checkbox" id="c-35892529" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#35892466">parent</a><span>|</span><a href="#35894908">prev</a><span>|</span><a href="#35894298">next</a><span>|</span><label class="collapse" for="c-35892529">[-]</label><label class="expand" for="c-35892529">[1 more]</label></div><br/><div class="children"><div class="content">No API, but Bard is on it.</div><br/></div></div><div id="35894298" class="c"><input type="checkbox" id="c-35894298" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35892466">parent</a><span>|</span><a href="#35892529">prev</a><span>|</span><a href="#35897175">next</a><span>|</span><label class="collapse" for="c-35894298">[-]</label><label class="expand" for="c-35894298">[1 more]</label></div><br/><div class="children"><div class="content">They’ve shut down and&#x2F;or changed prices on APIs so many times as long as it isn’t 100x  lower performance than an alternative I can’t see myself investing building a stack that relies on it.</div><br/></div></div></div></div><div id="35897175" class="c"><input type="checkbox" id="c-35897175" checked=""/><div class="controls bullet"><span class="by">getpost</span><span>|</span><a href="#35892466">prev</a><span>|</span><a href="#35898022">next</a><span>|</span><label class="collapse" for="c-35897175">[-]</label><label class="expand" for="c-35897175">[4 more]</label></div><br/><div class="children"><div class="content">I found an exciting feature—a way to submit a large amount of text—larger than you can paste in the Bard dialog window. (It&#x27;s possible this isn&#x27;t a new feature. Bard explained it to me this evening.) You can submit links to files in Google Drive. The links have to be publicly accessible. I just pasted the link to my file in Bard chat.<p>Bard can access the contents of the 322K file I pasted the link to. It definitely knows about the content of the file. I never said what it was about, but Bard knew it was about butterflies. It knew about content at the beginning of the file, and at the end.<p>However, it almost never answered questions about the content of the file correctly! For example, I asked it the number of species listed in the file and it said 109. There are 249 numbered species and some that are not numbered. It said the author&#x27;s name was not in the file, but near the top the file says By &lt;author name&gt;. I tried coaching it on the content of the file and it didn&#x27;t seem able to understand the file in light of the explanations I gave—very strange and baffling.<p>EDIT: It&#x27;s possible it surmised the content of the file from the filename, and was simply making up stuff about the content.</div><br/><div id="35897628" class="c"><input type="checkbox" id="c-35897628" checked=""/><div class="controls bullet"><span class="by">blahgeek</span><span>|</span><a href="#35897175">parent</a><span>|</span><a href="#35897199">next</a><span>|</span><label class="collapse" for="c-35897628">[-]</label><label class="expand" for="c-35897628">[1 more]</label></div><br/><div class="children"><div class="content">&gt; EDIT: It&#x27;s possible it surmised the content of the file from the filename, and was simply making up stuff about the content.<p>I think this is the most probably explanation.<p>It&#x27;s interesting that how much false credit we will give to AI system once we are convinced that it&#x27;s intelligent enough. It&#x27;s like those &quot;prompt hacking&quot;, people try to &quot;hack&quot; the AI because they believe that those AIs are self-aware and they may find a loop hole in its internal logic or something. But at the end, it&#x27;s just auto-completion, the &quot;hacked&quot; response is just the most reasonable reply according to the context (rated by human).</div><br/></div></div><div id="35897199" class="c"><input type="checkbox" id="c-35897199" checked=""/><div class="controls bullet"><span class="by">javierluraschi</span><span>|</span><a href="#35897175">parent</a><span>|</span><a href="#35897628">prev</a><span>|</span><a href="#35898022">next</a><span>|</span><label class="collapse" for="c-35897199">[-]</label><label class="expand" for="c-35897199">[2 more]</label></div><br/><div class="children"><div class="content">Seems like the file was in tabular format? LLMs don’t really know how to deal with large tabular data, but we’ve been working on this problem so shameless plug to <a href="https:&#x2F;&#x2F;hal9.ai" rel="nofollow">https:&#x2F;&#x2F;hal9.ai</a></div><br/><div id="35897585" class="c"><input type="checkbox" id="c-35897585" checked=""/><div class="controls bullet"><span class="by">getpost</span><span>|</span><a href="#35897175">root</a><span>|</span><a href="#35897199">parent</a><span>|</span><a href="#35898022">next</a><span>|</span><label class="collapse" for="c-35897585">[-]</label><label class="expand" for="c-35897585">[1 more]</label></div><br/><div class="children"><div class="content">The file is not in tabular format; that’s why I wanted Bard help, to process a highly irregular file. I was hoping to produce a tabular format.</div><br/></div></div></div></div></div></div><div id="35898022" class="c"><input type="checkbox" id="c-35898022" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35897175">prev</a><span>|</span><a href="#35892214">next</a><span>|</span><label class="collapse" for="c-35898022">[-]</label><label class="expand" for="c-35898022">[6 more]</label></div><br/><div class="children"><div class="content">Here is their Chat Playground for PaLM 2 <a href="https:&#x2F;&#x2F;console.cloud.google.com&#x2F;vertex-ai&#x2F;generative&#x2F;language&#x2F;create&#x2F;chat" rel="nofollow">https:&#x2F;&#x2F;console.cloud.google.com&#x2F;vertex-ai&#x2F;generative&#x2F;langua...</a> (you have to be logged in to Google Cloud Console I think)<p>Anyone know what parameters are best for code generation? I tried something simple for Node.js and it wasn&#x27;t horrible but not working. Maybe I used the wron parameters. I tried using 0 for the temperature and turning everything else down like I do with the OpenAI API.</div><br/><div id="35899442" class="c"><input type="checkbox" id="c-35899442" checked=""/><div class="controls bullet"><span class="by">rkwasny</span><span>|</span><a href="#35898022">parent</a><span>|</span><a href="#35898392">next</a><span>|</span><label class="collapse" for="c-35899442">[-]</label><label class="expand" for="c-35899442">[1 more]</label></div><br/><div class="children"><div class="content">Whoah it gives some seriously wrong answers to coding questions, to the point that they are dangerous!<p>I think we have to wait for the explanation is chat-bison PaLM 1 or 2</div><br/></div></div><div id="35898392" class="c"><input type="checkbox" id="c-35898392" checked=""/><div class="controls bullet"><span class="by">vitorgrs</span><span>|</span><a href="#35898022">parent</a><span>|</span><a href="#35899442">prev</a><span>|</span><a href="#35892214">next</a><span>|</span><label class="collapse" for="c-35898392">[-]</label><label class="expand" for="c-35898392">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t Chat-Bison-001 Palm 1?<p>Edit: It seems I can&#x27;t use my free credits on Vertex APIs... Not nice.</div><br/><div id="35898400" class="c"><input type="checkbox" id="c-35898400" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35898022">root</a><span>|</span><a href="#35898392">parent</a><span>|</span><a href="#35892214">next</a><span>|</span><label class="collapse" for="c-35898400">[-]</label><label class="expand" for="c-35898400">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think so because the CEO mentioned Bison as one of the PaLM 2 models in the Keynote. If I remember correctly.</div><br/><div id="35898416" class="c"><input type="checkbox" id="c-35898416" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35898022">root</a><span>|</span><a href="#35898400">parent</a><span>|</span><a href="#35892214">next</a><span>|</span><label class="collapse" for="c-35898416">[-]</label><label class="expand" for="c-35898416">[2 more]</label></div><br/><div class="children"><div class="content">But would be interested to know if that was not the case. They seemed to be saying that PaLM 2 was rolling out. Also the pages say its a preview. So why would they be previewing the old model still?</div><br/><div id="35898846" class="c"><input type="checkbox" id="c-35898846" checked=""/><div class="controls bullet"><span class="by">terramex</span><span>|</span><a href="#35898022">root</a><span>|</span><a href="#35898416">parent</a><span>|</span><a href="#35892214">next</a><span>|</span><label class="collapse" for="c-35898846">[-]</label><label class="expand" for="c-35898846">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;ai-machine-learning&#x2F;google-cloud-launches-new-ai-models-opens-generative-ai-studio" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;ai-machine-learning&#x2F;g...</a><p>&gt; Generative AI Studio, Model Garden, and PaLM 2 for Text and Chat are moving from trusted tester availability to preview, meaning everyone with a Google Cloud account has access.<p>&gt; Codey, Imagen, Embeddings API for images, and RLHF are available in Vertex AI through our trusted tester program, and Chirp, PaLM 2, Embeddings API, and Generative AI Studio for text are available in preview in Vertex AI to everyone with a Google Cloud account.<p>It seems like you are right and general PaLM 2 is available. Fine-tuned code-generation model (Codey) is not publicly available yet.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35892214" class="c"><input type="checkbox" id="c-35892214" checked=""/><div class="controls bullet"><span class="by">techbruv</span><span>|</span><a href="#35898022">prev</a><span>|</span><a href="#35894112">next</a><span>|</span><label class="collapse" for="c-35892214">[-]</label><label class="expand" for="c-35892214">[35 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;We then train several models from 400M to 15B on the same pre-training mixture for up to 1 × 1022 FLOPs.&quot;<p>Seems that for the last year or so these models are getting smaller. I would be surprised if GPT-4 had &gt; the number of parameters as GPT-3 (i.e. 175B).<p>Edit: Seems those numbers are just for their scaling laws study. They don&#x27;t explicitly say the size of PaLM 2-L, but they do say &quot;The largest model in the PaLM 2 family, PaLM 2-L, is significantly smaller than the largest PaLM model but uses more training compute.&quot;. So likely on the range of 10B - 100B.</div><br/><div id="35892411" class="c"><input type="checkbox" id="c-35892411" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35892214">parent</a><span>|</span><a href="#35892351">next</a><span>|</span><label class="collapse" for="c-35892411">[-]</label><label class="expand" for="c-35892411">[30 more]</label></div><br/><div class="children"><div class="content">GPT-4 is way slower than GPT-3. Unless they are artificially spiking the latency to hide parameter count, it’s likely around 1trn params</div><br/><div id="35892585" class="c"><input type="checkbox" id="c-35892585" checked=""/><div class="controls bullet"><span class="by">techbruv</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892411">parent</a><span>|</span><a href="#35892589">next</a><span>|</span><label class="collapse" for="c-35892585">[-]</label><label class="expand" for="c-35892585">[8 more]</label></div><br/><div class="children"><div class="content">The idea that GPT-4 is 1 trillion parameters has been refuted by Sam Altman himself on the Lex Fridman podcast (THIS IS WRONG, SEE CORRECTION BELOW).<p>These days, the largest models that have been trained optimally (in terms of model size w.r.t. tokens) typically hover around 50B (likely PaLM 2-L size and LLaMa is maxed at 70B). We simply do not have enough pre-training data to optimally train a 1T parameter model. For GPT-4 to be 1 trillion parameters, OpenAI would have needed to:<p>1) somehow magically unlocked 20x the amount of data (1T tokens -&gt; 20T tokens)
2) somehow engineered an incredibly fast inference engine for a 1T GPT model that significantly better than anything anyone else has built
3) is somehow is able to eat the cost of hosting 1T parameter models<p>The probability that all the above 3 have happened seem incredibly low.<p>CORRECTION: The refutation for the size of GPT-4 on the lex fridman podcast was that GPT-4 was 100T parameters (and not directly, they were just joking about it), not 1T, however, the above 3 points still stand.</div><br/><div id="35892710" class="c"><input type="checkbox" id="c-35892710" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892585">parent</a><span>|</span><a href="#35892645">next</a><span>|</span><label class="collapse" for="c-35892710">[-]</label><label class="expand" for="c-35892710">[4 more]</label></div><br/><div class="children"><div class="content">1) common crawl is &gt;100TB so obviously contains more than 20trn tokens + Ilya has said many times in interviews that there is still way more data for training usage &gt;10x<p>2) GPT-4 is way slower so this point is irrelevant<p>3) OpenAI have a 10000 A100 training farm that they are expanding to 2500. They are spending &gt;$1mln on compute per day. They have just raised $10bln. They can afford to pay for inference</div><br/><div id="35893307" class="c"><input type="checkbox" id="c-35893307" checked=""/><div class="controls bullet"><span class="by">CaptainNegative</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892710">parent</a><span>|</span><a href="#35893772">next</a><span>|</span><label class="collapse" for="c-35893307">[-]</label><label class="expand" for="c-35893307">[2 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI have a 10000 A100 training farm that they are expanding to 2500.<p>Does the first number have an extra zero or is the second number missing one?</div><br/><div id="35893721" class="c"><input type="checkbox" id="c-35893721" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893307">parent</a><span>|</span><a href="#35893772">next</a><span>|</span><label class="collapse" for="c-35893721">[-]</label><label class="expand" for="c-35893721">[1 more]</label></div><br/><div class="children"><div class="content">Second number is missing a zero sorry. Should be 10000 and 25000</div><br/></div></div></div></div></div></div><div id="35892645" class="c"><input type="checkbox" id="c-35892645" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892585">parent</a><span>|</span><a href="#35892710">prev</a><span>|</span><a href="#35897804">next</a><span>|</span><label class="collapse" for="c-35892645">[-]</label><label class="expand" for="c-35892645">[2 more]</label></div><br/><div class="children"><div class="content">&gt;The idea that GPT-4 is 1 trillion parameters has been refuted by Sam Altman himself on the Lex Fridman podcast.<p>No it hasn&#x27;t, Sam just laughed because Lex brought up the twitter memes.</div><br/><div id="35893089" class="c"><input type="checkbox" id="c-35893089" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892645">parent</a><span>|</span><a href="#35897804">next</a><span>|</span><label class="collapse" for="c-35893089">[-]</label><label class="expand" for="c-35893089">[1 more]</label></div><br/><div class="children"><div class="content">not sure why you&#x27;re getting so downvoted lol</div><br/></div></div></div></div><div id="35897804" class="c"><input type="checkbox" id="c-35897804" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892585">parent</a><span>|</span><a href="#35892645">prev</a><span>|</span><a href="#35892589">next</a><span>|</span><label class="collapse" for="c-35897804">[-]</label><label class="expand" for="c-35897804">[1 more]</label></div><br/><div class="children"><div class="content">GPT-2 training cost 10s of thousands<p>GPT-3 training cost millions<p>GPT-4 training cost over a hundred million [1]<p>GPT-4 inferencing is slower than GPT-3 or GPT-3.5<p>OpenAI has billions of dollars in funding<p>OpenAI has the backing of Microsoft and their entire Azure infra <i>at cost</i><p>There is no way GPT-4 is the same size as GPT-3. Is it 1T parameters? I don&#x27;t know. No one knows. But I think it is clear GPT-4 is significantly larger than GPT-3.<p>For fun, if we plot the number of parameters vs training cost we can see a clear trend and I imagine, very roughly predict the amount of parameters GPT-4 has<p><a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;rejigr5.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;rejigr5.png</a><p><a href="https:&#x2F;&#x2F;www.desmos.com&#x2F;calculator&#x2F;lqwsmmnngc" rel="nofollow">https:&#x2F;&#x2F;www.desmos.com&#x2F;calculator&#x2F;lqwsmmnngc</a><p>[1]<p>&gt; At the MIT event, Altman was asked  if training GPT-4 cost $100 million; he replied, “It’s more than that.”<p><a href="http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230417152518&#x2F;https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over&#x2F;" rel="nofollow">http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230417152518&#x2F;https:&#x2F;&#x2F;www.wired....</a></div><br/></div></div></div></div><div id="35892589" class="c"><input type="checkbox" id="c-35892589" checked=""/><div class="controls bullet"><span class="by">dougmwne</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892411">parent</a><span>|</span><a href="#35892585">prev</a><span>|</span><a href="#35896056">next</a><span>|</span><label class="collapse" for="c-35892589">[-]</label><label class="expand" for="c-35892589">[7 more]</label></div><br/><div class="children"><div class="content">ChatGPT 3.5 is likely much smaller than GPT-3’s 175b parameters. Based on the API pricing, I believe 8k context GPT-4 is larger than 175b parameters, but less than 1t.<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;pricing" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;pricing</a></div><br/><div id="35893140" class="c"><input type="checkbox" id="c-35893140" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892589">parent</a><span>|</span><a href="#35898013">next</a><span>|</span><label class="collapse" for="c-35893140">[-]</label><label class="expand" for="c-35893140">[5 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t some OpenAI engineer state that GPT4 runs on 2xH100? At 4 bit quantization, that gives an upper bound of 320B params, realistic upper bound probably more like 250B</div><br/><div id="35893803" class="c"><input type="checkbox" id="c-35893803" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893140">parent</a><span>|</span><a href="#35893912">next</a><span>|</span><label class="collapse" for="c-35893803">[-]</label><label class="expand" for="c-35893803">[2 more]</label></div><br/><div class="children"><div class="content">Not really sure what exactly was said. But in a 2 GPU set, you can technically live load weights on 1 GPU while running inference on the other.<p>At fp32 precision, storing a single layer takes around 40*d_model^2 bytes assuming context length isn’t massive relative to d_model (which it isn’t in GPT-4). At 80GB GPU size this means 40k model width could be stored as a single layer on 1 GPU while still leaving space for the activations. So theoretically any model below this width could run on a 2 GPU set. Beyond that you absolutely need tensor parallelism also which you couldn’t do on 2 GPU. But I think it is a safe assumption that GPT4 has sub 40k model width. And of course if you quantize the model you could even run 2.8x this model width at 4bit<p>My point is not that OpenAI is doing this, but more that theoretically you can run massive models on a 2 GPU set</div><br/><div id="35898119" class="c"><input type="checkbox" id="c-35898119" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893803">parent</a><span>|</span><a href="#35893912">next</a><span>|</span><label class="collapse" for="c-35898119">[-]</label><label class="expand" for="c-35898119">[1 more]</label></div><br/><div class="children"><div class="content">Without performance penalties? If the model is larger than the vram you have to constantly be pulling data from disk&#x2F;ram right?</div><br/></div></div></div></div><div id="35893912" class="c"><input type="checkbox" id="c-35893912" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893140">parent</a><span>|</span><a href="#35893803">prev</a><span>|</span><a href="#35898013">next</a><span>|</span><label class="collapse" for="c-35893912">[-]</label><label class="expand" for="c-35893912">[2 more]</label></div><br/><div class="children"><div class="content">With 32k context the upper bound is more like 175B.</div><br/><div id="35898101" class="c"><input type="checkbox" id="c-35898101" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893912">parent</a><span>|</span><a href="#35898013">next</a><span>|</span><label class="collapse" for="c-35898101">[-]</label><label class="expand" for="c-35898101">[1 more]</label></div><br/><div class="children"><div class="content">Its probably only the 8k model that runs on 2</div><br/></div></div></div></div></div></div><div id="35898013" class="c"><input type="checkbox" id="c-35898013" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892589">parent</a><span>|</span><a href="#35893140">prev</a><span>|</span><a href="#35896056">next</a><span>|</span><label class="collapse" for="c-35898013">[-]</label><label class="expand" for="c-35898013">[1 more]</label></div><br/><div class="children"><div class="content">Why are you confident 3.5 is smaller than 3?</div><br/></div></div></div></div><div id="35896056" class="c"><input type="checkbox" id="c-35896056" checked=""/><div class="controls bullet"><span class="by">qumpis</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892411">parent</a><span>|</span><a href="#35892589">prev</a><span>|</span><a href="#35892476">next</a><span>|</span><label class="collapse" for="c-35896056">[-]</label><label class="expand" for="c-35896056">[1 more]</label></div><br/><div class="children"><div class="content">Someone on HN has educated me that gpt4 and 3 should be on a similar param count. This is based on inference times of gpt4 vs gpt3.5 pre-speedup (where distilled version was used only post-speedup in the turbo version).</div><br/></div></div><div id="35892476" class="c"><input type="checkbox" id="c-35892476" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892411">parent</a><span>|</span><a href="#35896056">prev</a><span>|</span><a href="#35892428">next</a><span>|</span><label class="collapse" for="c-35892476">[-]</label><label class="expand" for="c-35892476">[9 more]</label></div><br/><div class="children"><div class="content">The report specifically states:<p>&gt; <i>The largest model in the PaLM 2 family, PaLM 2-L, is significantly smaller than the largest PaLM model but uses more training compute</i><p>The largest PaLM model is 540B. So all of PaLM 2 is potentially double-digit parameters.<p>Note though that GPT-3.5 was plausibly not a finetuning of the 175B model, but instead a finetuning of Codex which was based on the 12B version of GPT-3.</div><br/><div id="35892659" class="c"><input type="checkbox" id="c-35892659" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892476">parent</a><span>|</span><a href="#35892518">next</a><span>|</span><label class="collapse" for="c-35892659">[-]</label><label class="expand" for="c-35892659">[3 more]</label></div><br/><div class="children"><div class="content">How could GPT-3.5 possibly have been a finetuning of the 175B model? They didn&#x27;t even use the same tokens?</div><br/><div id="35894526" class="c"><input type="checkbox" id="c-35894526" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892659">parent</a><span>|</span><a href="#35892518">next</a><span>|</span><label class="collapse" for="c-35894526">[-]</label><label class="expand" for="c-35894526">[2 more]</label></div><br/><div class="children"><div class="content">Finetuning might not be the best word; sometimes it is a grey line.<p>Token embeddings can be trained without changing the other parameters. There is a number of models which add tokens as a finetuning step. Here is recently StarCoder adding ChatML-equivalent tokens: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;starchat-alpha#a-standard-format-for-dialogues" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;starchat-alpha#a-standard-format...</a></div><br/><div id="35894563" class="c"><input type="checkbox" id="c-35894563" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35894526">parent</a><span>|</span><a href="#35892518">next</a><span>|</span><label class="collapse" for="c-35894563">[-]</label><label class="expand" for="c-35894563">[1 more]</label></div><br/><div class="children"><div class="content">Sure, you can add a few tokens, but in this case they changed almost every token.</div><br/></div></div></div></div></div></div><div id="35892518" class="c"><input type="checkbox" id="c-35892518" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892476">parent</a><span>|</span><a href="#35892659">prev</a><span>|</span><a href="#35892428">next</a><span>|</span><label class="collapse" for="c-35892518">[-]</label><label class="expand" for="c-35892518">[5 more]</label></div><br/><div class="children"><div class="content">Original PaLM was 540B so significantly smaller could mean anything from 350B down really</div><br/><div id="35893357" class="c"><input type="checkbox" id="c-35893357" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892518">parent</a><span>|</span><a href="#35892428">next</a><span>|</span><label class="collapse" for="c-35893357">[-]</label><label class="expand" for="c-35893357">[4 more]</label></div><br/><div class="children"><div class="content">I tried my hand at estimating their parameter count from extrapolating their LAMBADA figures, assuming they all trained on Chinchilla law: <a href="https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;Fvy4xNkXgAEDF_D?format=jpg&amp;name=medium" rel="nofollow">https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;Fvy4xNkXgAEDF_D?format=jpg&amp;name=...</a><p>If the extrapolation is not too flawed, it looks like PaLM 2-S might be about 120B, PaLM 2-M 180B, PaLM 2-L 280B.<p>Still, I would expect GPT-4 trained for way longer than Chinchilla, so it could be smaller than even PaLM 2-S.</div><br/><div id="35893930" class="c"><input type="checkbox" id="c-35893930" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893357">parent</a><span>|</span><a href="#35892428">next</a><span>|</span><label class="collapse" for="c-35893930">[-]</label><label class="expand" for="c-35893930">[3 more]</label></div><br/><div class="children"><div class="content">They said the smallest PaLM 2 can run locally on a Pixel Smartphone.<p>There&#x27;s no way it&#x27;s 120B parameters. It&#x27;s probably not even 12B.</div><br/><div id="35894034" class="c"><input type="checkbox" id="c-35894034" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893930">parent</a><span>|</span><a href="#35892428">next</a><span>|</span><label class="collapse" for="c-35894034">[-]</label><label class="expand" for="c-35894034">[2 more]</label></div><br/><div class="children"><div class="content">I am talking about the 3 larger models PaLM 2-S, PaLM 2-M, and PaLM 2-L described in the technical report.<p>At I&#x2F;O, I think they were referencing the scaling law experiments: there are four of them, just like the number of PaLM 2 codenames they cited at I&#x2F;O (Gecko, Otter, Bison, and Unicorn). The largest of those smaller-scale models is 14.7B, which is too big for a phone too. The smallest is 1B, which can fit in 512MB of RAM with GPTQ4-style quantization.<p>Either that, or Gecko is the smaller scaling experiment, and Otter is PaLM 2-S.</div><br/><div id="35894438" class="c"><input type="checkbox" id="c-35894438" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35894034">parent</a><span>|</span><a href="#35892428">next</a><span>|</span><label class="collapse" for="c-35894438">[-]</label><label class="expand" for="c-35894438">[1 more]</label></div><br/><div class="children"><div class="content">My Pixel 6 Pro has 12GB of RAM and LLaMA-13B only uses 9GB in 4bit.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35892428" class="c"><input type="checkbox" id="c-35892428" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892411">parent</a><span>|</span><a href="#35892476">prev</a><span>|</span><a href="#35892351">next</a><span>|</span><label class="collapse" for="c-35892428">[-]</label><label class="expand" for="c-35892428">[4 more]</label></div><br/><div class="children"><div class="content">Yeah 1 to 2 trillion is the estimates I&#x27;ve heard.<p>Given the 25 messages &#x2F; 3 hour limit in chatGPT, I don&#x27;t think they&#x27;ve found a way to make it cheap to run.</div><br/><div id="35893412" class="c"><input type="checkbox" id="c-35893412" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892428">parent</a><span>|</span><a href="#35892450">next</a><span>|</span><label class="collapse" for="c-35893412">[-]</label><label class="expand" for="c-35893412">[2 more]</label></div><br/><div class="children"><div class="content">1. there&#x27;s no reason to think OpenAI wouldn&#x27;t also be going the artificial scarcity route as have so many other companies in the past<p>2. Microsoft may not like them using too much azure compute and tell them to step off.  Rumor has it they&#x27;re trying to migrate github to it and it&#x27;s seemingly not going ideal.  And they&#x27;re certainly nothing more than another microsoft purchase at this point.</div><br/><div id="35894171" class="c"><input type="checkbox" id="c-35894171" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35893412">parent</a><span>|</span><a href="#35892450">next</a><span>|</span><label class="collapse" for="c-35894171">[-]</label><label class="expand" for="c-35894171">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI has a 40k token per minute rate limit on their GPT4 API too so I doubt it&#x27;s artificial scarcity.</div><br/></div></div></div></div><div id="35892450" class="c"><input type="checkbox" id="c-35892450" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892428">parent</a><span>|</span><a href="#35893412">prev</a><span>|</span><a href="#35892351">next</a><span>|</span><label class="collapse" for="c-35892450">[-]</label><label class="expand" for="c-35892450">[1 more]</label></div><br/><div class="children"><div class="content">Yep. I’m guessing PaLM 2 is about 200bln params as it seems clearly stronger than chinchilla</div><br/></div></div></div></div></div></div><div id="35892351" class="c"><input type="checkbox" id="c-35892351" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35892214">parent</a><span>|</span><a href="#35892411">prev</a><span>|</span><a href="#35892320">next</a><span>|</span><label class="collapse" for="c-35892351">[-]</label><label class="expand" for="c-35892351">[1 more]</label></div><br/><div class="children"><div class="content">Those are the numbers for the scaling law tests they did. Not necessarily Palm 2 range.</div><br/></div></div><div id="35892320" class="c"><input type="checkbox" id="c-35892320" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#35892214">parent</a><span>|</span><a href="#35892351">prev</a><span>|</span><a href="#35893332">next</a><span>|</span><label class="collapse" for="c-35892320">[-]</label><label class="expand" for="c-35892320">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard Bard was previously 3B parameters but I could never find a good source for it.<p>I honestly think the end game here is running on consumer devices, 7B and under need ~4GB of ram to actually run which is likely the max reasonable requirement for consumer devices.<p>That said medium end hardware can do 15B, anything larger then this is currently something only &quot;enthusiasts&quot; can run.<p>If it is small enough to run on consumer devices then they don&#x27;t have to pay for the inference compute at that point, and presumably the latency will be improved for consumers.</div><br/><div id="35892667" class="c"><input type="checkbox" id="c-35892667" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35892214">root</a><span>|</span><a href="#35892320">parent</a><span>|</span><a href="#35893332">next</a><span>|</span><label class="collapse" for="c-35892667">[-]</label><label class="expand" for="c-35892667">[1 more]</label></div><br/><div class="children"><div class="content">The current state of consumer devices isn&#x27;t static, either, and existing hardware (even GPU) is suboptimal for the current crop of LLMs - it does way more than it actually needs to do.</div><br/></div></div></div></div><div id="35893332" class="c"><input type="checkbox" id="c-35893332" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#35892214">parent</a><span>|</span><a href="#35892320">prev</a><span>|</span><a href="#35894112">next</a><span>|</span><label class="collapse" for="c-35893332">[-]</label><label class="expand" for="c-35893332">[1 more]</label></div><br/><div class="children"><div class="content">For &#x27;Palm-2&#x27;, read, &#x27;T5-2&#x27;.</div><br/></div></div></div></div><div id="35894112" class="c"><input type="checkbox" id="c-35894112" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#35892214">prev</a><span>|</span><a href="#35893276">next</a><span>|</span><label class="collapse" for="c-35894112">[-]</label><label class="expand" for="c-35894112">[3 more]</label></div><br/><div class="children"><div class="content">The thing is, once a company creates a proto AGI where the path to a functional AGI is entirely predictable with more compute, they&#x27;ll keep it a secret. Who would share the fact that the greatest achievement in human history is possible when having it before anyone else gives you a huge competitive advantage?</div><br/><div id="35894407" class="c"><input type="checkbox" id="c-35894407" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35894112">parent</a><span>|</span><a href="#35894581">next</a><span>|</span><label class="collapse" for="c-35894407">[-]</label><label class="expand" for="c-35894407">[1 more]</label></div><br/><div class="children"><div class="content">&gt; once a company creates a proto AGI where the path to a functional AGI is entirely predictable with more compute,<p>I find it hard to believe this will happen. I expect AGI training to be more like a phase transition (or a bit like grokking <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2201.02177.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2201.02177.pdf</a>)</div><br/></div></div></div></div><div id="35893276" class="c"><input type="checkbox" id="c-35893276" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35894112">prev</a><span>|</span><a href="#35892178">next</a><span>|</span><label class="collapse" for="c-35893276">[-]</label><label class="expand" for="c-35893276">[10 more]</label></div><br/><div class="children"><div class="content">If the current Bard is really running on PaLM 2, it still hallucinates worse than GPT-3.5. Trying to get it to solve a variant of the classic wolf&#x2F;goat&#x2F;cabbage puzzle, I got this gem:<p>&quot;The scientist is not present on Phobos on the first step. The Doom Slayer teleports himself and the bunny to Deimos, leaving the scientist on Phobos.<p>That wasn&#x27;t a one-off thing, either - it repeatedly contradicted itself several times, often in near-adjacent sentences. You might wonder what this means for the ability to do chain-of-thought... so did I, but apparently the bigger problem is convincing it to do CoT in the first place. But if you do, yeah, it&#x27;s as bad as you&#x27;d expect.<p>Here are two complete conversations, plus GPT-4 doing the same puzzle for comparison; judge for yourself: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;HWLgu3c" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;HWLgu3c</a></div><br/><div id="35893712" class="c"><input type="checkbox" id="c-35893712" checked=""/><div class="controls bullet"><span class="by">EvgeniyZh</span><span>|</span><a href="#35893276">parent</a><span>|</span><a href="#35892178">next</a><span>|</span><label class="collapse" for="c-35893712">[-]</label><label class="expand" for="c-35893712">[9 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think current bard runs on palm 2, otherwise it&#x27;s complete failure</div><br/><div id="35893815" class="c"><input type="checkbox" id="c-35893815" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35893712">parent</a><span>|</span><a href="#35892178">next</a><span>|</span><label class="collapse" for="c-35893815">[-]</label><label class="expand" for="c-35893815">[8 more]</label></div><br/><div class="children"><div class="content">In their official blog post today, Google says this:<p>&quot;PaLM 2’s improved multilingual capabilities are allowing us to expand Bard to new languages, starting today. Plus, it’s powering our recently announced coding update.&quot;<p>and when I check the Updates tab in Bard UI, it has this entry for today:<p>&quot;Expanding access to Bard in more countries and languages. You can now collaborate with Bard in Japanese and Korean, in addition to US English. We have also expanded access to Bard in all three languages to over 180 countries.&quot;<p>which seems to strongly imply that it is, indeed, PaLM 2. Just to be sure, I gave it the same puzzle in Korean, and got a similarly lackluster response.</div><br/><div id="35893874" class="c"><input type="checkbox" id="c-35893874" checked=""/><div class="controls bullet"><span class="by">DashAnimal</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35893815">parent</a><span>|</span><a href="#35893827">next</a><span>|</span><label class="collapse" for="c-35893874">[-]</label><label class="expand" for="c-35893874">[2 more]</label></div><br/><div class="children"><div class="content">In their presentation, they talked about multiple sizes for the PaLM 2 model, named Gecko, Otter, Bison and Unicorn, with Gecko being small enough to run offline on mobile devices. I can&#x27;t seem to find any info on what size model is being used with Bard at the moment.</div><br/><div id="35894205" class="c"><input type="checkbox" id="c-35894205" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35893874">parent</a><span>|</span><a href="#35893827">next</a><span>|</span><label class="collapse" for="c-35894205">[-]</label><label class="expand" for="c-35894205">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, it&#x27;s likely that they&#x27;re running a fairly small model. But this is in and of itself a strange choice, given how ChatGPT became the gateway drug for OpenAI. Why would Google set Bard up for failure like that? Surely they can afford to run a more competent model as a promo, if OpenAI can?</div><br/></div></div></div></div><div id="35893827" class="c"><input type="checkbox" id="c-35893827" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35893815">parent</a><span>|</span><a href="#35893874">prev</a><span>|</span><a href="#35892178">next</a><span>|</span><label class="collapse" for="c-35893827">[-]</label><label class="expand" for="c-35893827">[5 more]</label></div><br/><div class="children"><div class="content">it claims to run on LaMDA at the moment</div><br/><div id="35894138" class="c"><input type="checkbox" id="c-35894138" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35893827">parent</a><span>|</span><a href="#35892178">next</a><span>|</span><label class="collapse" for="c-35894138">[-]</label><label class="expand" for="c-35894138">[4 more]</label></div><br/><div class="children"><div class="content">If you mean asking it what it&#x27;s running on, it just hallucinates. As others have noted in the comments here, you can get it to say that it runs on PaLM 3 quite easily.</div><br/><div id="35896093" class="c"><input type="checkbox" id="c-35896093" checked=""/><div class="controls bullet"><span class="by">splatzone</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35894138">parent</a><span>|</span><a href="#35892178">next</a><span>|</span><label class="collapse" for="c-35896093">[-]</label><label class="expand" for="c-35896093">[3 more]</label></div><br/><div class="children"><div class="content">In chat history you can see which model generated each request - for me it’s always LaMDA</div><br/><div id="35898140" class="c"><input type="checkbox" id="c-35898140" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35896093">parent</a><span>|</span><a href="#35897032">next</a><span>|</span><label class="collapse" for="c-35898140">[-]</label><label class="expand" for="c-35898140">[1 more]</label></div><br/><div class="children"><div class="content">It just says &quot;Bard&quot;, even if I click on &quot;Details&quot;. Are you, perhaps, using some kind of internal preview?</div><br/></div></div><div id="35897032" class="c"><input type="checkbox" id="c-35897032" checked=""/><div class="controls bullet"><span class="by">callmekit</span><span>|</span><a href="#35893276">root</a><span>|</span><a href="#35896093">parent</a><span>|</span><a href="#35898140">prev</a><span>|</span><a href="#35892178">next</a><span>|</span><label class="collapse" for="c-35897032">[-]</label><label class="expand" for="c-35897032">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see which model generated each request, where exactly do you see this?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35892178" class="c"><input type="checkbox" id="c-35892178" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#35893276">prev</a><span>|</span><a href="#35894066">next</a><span>|</span><label class="collapse" for="c-35892178">[-]</label><label class="expand" for="c-35892178">[14 more]</label></div><br/><div class="children"><div class="content">&quot;The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data&quot;<p>I really want to know more about the training data. Which web documents, which books, code from where, conversational data from where?</div><br/><div id="35892217" class="c"><input type="checkbox" id="c-35892217" checked=""/><div class="controls bullet"><span class="by">inscrutable</span><span>|</span><a href="#35892178">parent</a><span>|</span><a href="#35897467">next</a><span>|</span><label class="collapse" for="c-35892217">[-]</label><label class="expand" for="c-35892217">[9 more]</label></div><br/><div class="children"><div class="content">My sweet summer child, this is a closely guarded secret. Will only be revealed if perhaps Europe demands it so that copyright holders can sue.</div><br/><div id="35892258" class="c"><input type="checkbox" id="c-35892258" checked=""/><div class="controls bullet"><span class="by">gabereiser</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35892217">parent</a><span>|</span><a href="#35897467">next</a><span>|</span><label class="collapse" for="c-35892258">[-]</label><label class="expand" for="c-35892258">[8 more]</label></div><br/><div class="children"><div class="content">Metadata will show where it came from, should you choose to keep it. Or so they showed on the big screen at I&#x2F;O today.</div><br/><div id="35892516" class="c"><input type="checkbox" id="c-35892516" checked=""/><div class="controls bullet"><span class="by">inscrutable</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35892258">parent</a><span>|</span><a href="#35897467">next</a><span>|</span><label class="collapse" for="c-35892516">[-]</label><label class="expand" for="c-35892516">[7 more]</label></div><br/><div class="children"><div class="content">maybe you&#x27;re right, but I&#x27;d be skeptical. In a non-snarky way, this shows the data sources used in models to date up to GPT 3.<p><a href="https:&#x2F;&#x2F;lifearchitect.ai&#x2F;whats-in-my-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lifearchitect.ai&#x2F;whats-in-my-ai&#x2F;</a><p>OpenAI paid $2m&#x2F;year for twitter feeds until Elon cut them off, and Sam Altman has mentioned they&#x27;d paid a lot for scientific journals and Reddit mention they&#x27;ll start charging. Given how central data quality and curation is, if these  private data sources give a significant boost, it won&#x27;t be available for Apache2 models.</div><br/><div id="35892676" class="c"><input type="checkbox" id="c-35892676" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35892516">parent</a><span>|</span><a href="#35897467">next</a><span>|</span><label class="collapse" for="c-35892676">[-]</label><label class="expand" for="c-35892676">[6 more]</label></div><br/><div class="children"><div class="content">Given Reddit&#x27;s inability to keep their website functioning (unless you use the far superior old.reddit.com) I find it hard to believe they would be able to stop a motivated developer from scraping the whole site.</div><br/><div id="35893319" class="c"><input type="checkbox" id="c-35893319" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35892676">parent</a><span>|</span><a href="#35897467">next</a><span>|</span><label class="collapse" for="c-35893319">[-]</label><label class="expand" for="c-35893319">[5 more]</label></div><br/><div class="children"><div class="content">this is about the time that i expect sites to begin returning intentionally corrupt&#x2F;incorrect&#x2F;perhaps outright garbage (subtle or not, probably better subtle so they don&#x27;t realize it until it&#x27;s far too late) data in order to intentionally poison enemy wellscraping.  where &quot;ethics&quot; dissolve into the inherent raw cannibalistic laws of capitalist ventures.<p>then you can sell them back the TBs they scraped at a 1000x markup for the real data.  
or attempt to watermark it so you can prove their illegal(?) usage of your services in their training.</div><br/><div id="35894214" class="c"><input type="checkbox" id="c-35894214" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35893319">parent</a><span>|</span><a href="#35894543">next</a><span>|</span><label class="collapse" for="c-35894214">[-]</label><label class="expand" for="c-35894214">[2 more]</label></div><br/><div class="children"><div class="content">You might be right. What a dystopian future that will be. Make a few requests too many and the webserver might think you&#x27;re scraping data so it gaslights you into reading bullshit.</div><br/><div id="35896596" class="c"><input type="checkbox" id="c-35896596" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35894214">parent</a><span>|</span><a href="#35894543">next</a><span>|</span><label class="collapse" for="c-35896596">[-]</label><label class="expand" for="c-35896596">[1 more]</label></div><br/><div class="children"><div class="content">Is this sarcasm? I can’t tell.</div><br/></div></div></div></div><div id="35894543" class="c"><input type="checkbox" id="c-35894543" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35893319">parent</a><span>|</span><a href="#35894214">prev</a><span>|</span><a href="#35894553">next</a><span>|</span><label class="collapse" for="c-35894543">[-]</label><label class="expand" for="c-35894543">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they&#x27;ve been doing that for years and that&#x27;s why all the advice subreddits turned into creative writing subreddits.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35897467" class="c"><input type="checkbox" id="c-35897467" checked=""/><div class="controls bullet"><span class="by">FanaHOVA</span><span>|</span><a href="#35892178">parent</a><span>|</span><a href="#35892217">prev</a><span>|</span><a href="#35894482">next</a><span>|</span><label class="collapse" for="c-35897467">[-]</label><label class="expand" for="c-35897467">[2 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t specify, but if you&#x27;re generally curious you should look into mC4, RedPajama, The Stack, etc as they are the foundation of most training sets.</div><br/><div id="35897647" class="c"><input type="checkbox" id="c-35897647" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35897467">parent</a><span>|</span><a href="#35894482">next</a><span>|</span><label class="collapse" for="c-35897647">[-]</label><label class="expand" for="c-35897647">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve spent quite a bit of time exploring RedPajama! <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;17&#x2F;redpajama-data&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;17&#x2F;redpajama-data&#x2F;</a></div><br/></div></div></div></div><div id="35894482" class="c"><input type="checkbox" id="c-35894482" checked=""/><div class="controls bullet"><span class="by">jimmygrapes</span><span>|</span><a href="#35892178">parent</a><span>|</span><a href="#35897467">prev</a><span>|</span><a href="#35894066">next</a><span>|</span><label class="collapse" for="c-35894482">[-]</label><label class="expand" for="c-35894482">[2 more]</label></div><br/><div class="children"><div class="content">I fully expect Discord to be a data source, if not already, then for a future version. I also expect that the only way the general public would ever find this out is via whistle-blower.</div><br/><div id="35896843" class="c"><input type="checkbox" id="c-35896843" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#35892178">root</a><span>|</span><a href="#35894482">parent</a><span>|</span><a href="#35894066">next</a><span>|</span><label class="collapse" for="c-35896843">[-]</label><label class="expand" for="c-35896843">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;d be pretty easy to tell; you could just ask it to generate Discord chats and notice it works. Text models also like to memorize their inputs if they&#x27;re big enough, so you could probably get specific ones.</div><br/></div></div></div></div></div></div><div id="35894066" class="c"><input type="checkbox" id="c-35894066" checked=""/><div class="controls bullet"><span class="by">zdyn5</span><span>|</span><a href="#35892178">prev</a><span>|</span><a href="#35892484">next</a><span>|</span><label class="collapse" for="c-35894066">[-]</label><label class="expand" for="c-35894066">[13 more]</label></div><br/><div class="children"><div class="content">You can verify that your Bard instance is using Palm 2 by asking “are you using the palm or palm 2 model?”</div><br/><div id="35894391" class="c"><input type="checkbox" id="c-35894391" checked=""/><div class="controls bullet"><span class="by">sunshadow</span><span>|</span><a href="#35894066">parent</a><span>|</span><a href="#35894317">next</a><span>|</span><label class="collapse" for="c-35894391">[-]</label><label class="expand" for="c-35894391">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve asked &quot;are you using palm 3&quot;:
It said:
    I am using the Palm 3 model. Palm 3 is a large language model...<p>Don&#x27;t believe it :) 
Also, In the technical report, It mentions multiple languages, I&#x27;ve asked in Turkish which was supposed to be supported, but wasn&#x27;t able to answer.<p>Even if its PaLM 2, its hard to trust to the model itself.</div><br/><div id="35894564" class="c"><input type="checkbox" id="c-35894564" checked=""/><div class="controls bullet"><span class="by">cdchn</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894391">parent</a><span>|</span><a href="#35894317">next</a><span>|</span><label class="collapse" for="c-35894564">[-]</label><label class="expand" for="c-35894564">[2 more]</label></div><br/><div class="children"><div class="content">I asked it &quot;are you using the palm 420 language model or the palm 2 language model?&quot;<p>It said &quot;I am not using either the Palm 420 language model or the Palm 2 language model. I am using a different language model called Bard, which is a large language model from Google AI.&quot;<p>Perhaps the people at Google saw this and made a manual correction?  Hard to say, black boxes and all...</div><br/><div id="35896822" class="c"><input type="checkbox" id="c-35896822" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894564">parent</a><span>|</span><a href="#35894317">next</a><span>|</span><label class="collapse" for="c-35896822">[-]</label><label class="expand" for="c-35896822">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t have the capability to be corrected that easily. It&#x27;s just lying.<p>Bard is very bad about this, it lies with the slightest encouragement. Try &quot;compare and contrast captain planet and paptain clanet&quot;.</div><br/></div></div></div></div></div></div><div id="35894317" class="c"><input type="checkbox" id="c-35894317" checked=""/><div class="controls bullet"><span class="by">xkapastel</span><span>|</span><a href="#35894066">parent</a><span>|</span><a href="#35894391">prev</a><span>|</span><a href="#35894152">next</a><span>|</span><label class="collapse" for="c-35894317">[-]</label><label class="expand" for="c-35894317">[8 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you can do this, it will just make things up. Language models don&#x27;t have this type of reflection. Google would need to indicate this out of band, like on the page itself, in order for you to be confident about what model you&#x27;re using.</div><br/><div id="35894335" class="c"><input type="checkbox" id="c-35894335" checked=""/><div class="controls bullet"><span class="by">entropicdrifter</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894317">parent</a><span>|</span><a href="#35894152">next</a><span>|</span><label class="collapse" for="c-35894335">[-]</label><label class="expand" for="c-35894335">[7 more]</label></div><br/><div class="children"><div class="content">Agreed. I&#x27;m not entirely sure that the person you&#x27;re replying to is not joking</div><br/><div id="35894558" class="c"><input type="checkbox" id="c-35894558" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894335">parent</a><span>|</span><a href="#35894584">next</a><span>|</span><label class="collapse" for="c-35894558">[-]</label><label class="expand" for="c-35894558">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure they&#x27;re trying to suggest that LLMs in general are not useful because they can&#x27;t do this type of thing.  It&#x27;s just the next iteration of goal post moving and should effectively be ignored.<p>Many artists and such that I&#x27;ve spoken to about AI work have similar comments about these systems because of the disdain for their existence.<p>The number of times I hear an argument like &quot;well, they can never taste the tartness of a kiwi and feel the heat of the sun while at the beach&quot; gets quite exhausting.  For some reason, many people have this weird notion that this is what AGI means - exactly what humans do, and specifically within the same data domains of humans, but they don&#x27;t consider working solely outside those domains as a possibility for AGI.</div><br/><div id="35895271" class="c"><input type="checkbox" id="c-35895271" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894558">parent</a><span>|</span><a href="#35894584">next</a><span>|</span><label class="collapse" for="c-35895271">[-]</label><label class="expand" for="c-35895271">[1 more]</label></div><br/><div class="children"><div class="content">Just wait as the multimodal LLM’s roll out!  People will be shoving kiwis into their computer and taking their laptops out for a few rays before you know it.</div><br/></div></div></div></div><div id="35894584" class="c"><input type="checkbox" id="c-35894584" checked=""/><div class="controls bullet"><span class="by">cdchn</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894335">parent</a><span>|</span><a href="#35894558">prev</a><span>|</span><a href="#35894152">next</a><span>|</span><label class="collapse" for="c-35894584">[-]</label><label class="expand" for="c-35894584">[4 more]</label></div><br/><div class="children"><div class="content">I tried asking it &quot;what is the difference between the palm language model and the bard language model?&quot; and its reply started off &quot;The main difference between the Palm language model and the Bard language model is the size of the dataset they are trained on. Palm is trained on a dataset of 400 billion parameters, while Bard is trained on a dataset of 540 billion parameters.&quot;  Which to me is even more interesting that what the OP commenter asserted.</div><br/><div id="35894634" class="c"><input type="checkbox" id="c-35894634" checked=""/><div class="controls bullet"><span class="by">knaik94</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894584">parent</a><span>|</span><a href="#35894152">next</a><span>|</span><label class="collapse" for="c-35894634">[-]</label><label class="expand" for="c-35894634">[3 more]</label></div><br/><div class="children"><div class="content">It makes up those numbers, I asked about the difference between the small and large PaLM 2 data set size, and it asserted the small model was trained on 540 billion and the large model was trained on 540 trillion. A different draft instead specified 1.4 trillion for the large.</div><br/><div id="35894822" class="c"><input type="checkbox" id="c-35894822" checked=""/><div class="controls bullet"><span class="by">cdchn</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894634">parent</a><span>|</span><a href="#35894152">next</a><span>|</span><label class="collapse" for="c-35894822">[-]</label><label class="expand" for="c-35894822">[2 more]</label></div><br/><div class="children"><div class="content">It even gave me a table with a whole bunch of differences.  All thats made up?<p>Here is a table that summarizes the key differences between the two language models:<p>Feature Palm Bard
Number of parameters 400 billion 540 billion
Vocabulary size 137 billion words 1.5 trillion words</div><br/><div id="35895789" class="c"><input type="checkbox" id="c-35895789" checked=""/><div class="controls bullet"><span class="by">knaik94</span><span>|</span><a href="#35894066">root</a><span>|</span><a href="#35894822">parent</a><span>|</span><a href="#35894152">next</a><span>|</span><label class="collapse" for="c-35895789">[-]</label><label class="expand" for="c-35895789">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t mean to argue that everything generated is incorrect. But in my experience, the numbers it generates seem closer to random guesses. If you ask it enough times, it sometimes converges on a number, but I don&#x27;t think that means it&#x27;s an accurate value. I was able to make it generate a similar table for the different PaLM 2 sizes, and laMDA, and it listed, PaLM 2 Gecko 137 billion, PaLM 2 Otter 540 billion, PaLM 2 Bison 1.8 trillion, PaLM 2 Unicorn 5.4 trillion, LaMDA 137 billion. For Unicorn, it also lists &quot;Still under development.&quot;<p>Edit: Playing around with it more and it listed WuDao 2.0 1.75 Trillion, Chinchilla 175B, Codex 175B, Dalle2 1.3B, GPT4 1.75T, GPT3.5 540B, GPT3 175B, GPT2 1.37B, GPT 1.3B.<p>But in the previous question it listed GPT4 540 billion and Codex 5.4 trillion among other contradictions.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35892484" class="c"><input type="checkbox" id="c-35892484" checked=""/><div class="controls bullet"><span class="by">technics256</span><span>|</span><a href="#35894066">prev</a><span>|</span><a href="#35893019">next</a><span>|</span><label class="collapse" for="c-35892484">[-]</label><label class="expand" for="c-35892484">[5 more]</label></div><br/><div class="children"><div class="content">PaLM 2 on HumanEval coding benchmark (0 shot):<p>37.6% success<p>GPT-4:<p>67% success<p>Not even close, gpt4 miles ahead</div><br/><div id="35892601" class="c"><input type="checkbox" id="c-35892601" checked=""/><div class="controls bullet"><span class="by">orpheansodality</span><span>|</span><a href="#35892484">parent</a><span>|</span><a href="#35893179">next</a><span>|</span><label class="collapse" for="c-35892601">[-]</label><label class="expand" for="c-35892601">[2 more]</label></div><br/><div class="children"><div class="content">GPT-4 is a fine-tuned model (likely first fine-tuned for code, then for chat on top of that like gpt-3.5-turbo was[0]), while PaLM2 as reported is a foundational model without any additional fine-tuning applied yet. I would expect its performance to improve on this if it were fine-tuned, though I don&#x27;t have a great sense of what the cap would be.<p>[0] <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;model-index-for-researchers" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;model-index-for-researchers</a></div><br/><div id="35892788" class="c"><input type="checkbox" id="c-35892788" checked=""/><div class="controls bullet"><span class="by">macrolime</span><span>|</span><a href="#35892484">root</a><span>|</span><a href="#35892601">parent</a><span>|</span><a href="#35893179">next</a><span>|</span><label class="collapse" for="c-35892788">[-]</label><label class="expand" for="c-35892788">[1 more]</label></div><br/><div class="children"><div class="content">They also write about Flan-PaLM2 which is instruction fine-tuned, but still some ways off GPT-4.</div><br/></div></div></div></div><div id="35893179" class="c"><input type="checkbox" id="c-35893179" checked=""/><div class="controls bullet"><span class="by">heliophobicdude</span><span>|</span><a href="#35892484">parent</a><span>|</span><a href="#35892601">prev</a><span>|</span><a href="#35895404">next</a><span>|</span><label class="collapse" for="c-35893179">[-]</label><label class="expand" for="c-35893179">[1 more]</label></div><br/><div class="children"><div class="content">Humaneval needs careful consideration though.<p>In the GPT-4 technical report, they reported contamination of humaneval data in the training data.<p>They did measure against a &quot;non-contaminated&quot; training set but no idea if that can still be trusted.<p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;papers&#x2F;gpt-4.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;papers&#x2F;gpt-4.pdf</a></div><br/></div></div><div id="35895404" class="c"><input type="checkbox" id="c-35895404" checked=""/><div class="controls bullet"><span class="by">hipmanbro</span><span>|</span><a href="#35892484">parent</a><span>|</span><a href="#35893179">prev</a><span>|</span><a href="#35893019">next</a><span>|</span><label class="collapse" for="c-35895404">[-]</label><label class="expand" for="c-35895404">[1 more]</label></div><br/><div class="children"><div class="content">There were a few reasoning benchmarks that I noticed think they omitted a direct comparison since they weren&#x27;t as competitive compared to GPT-4, and instead opted to just show the benchmarks comparing itself to other versions of PaLM or other language models<p>HellaSwag: GPT-4: 95.3%, PaLM 2-L: 86.8%<p>MMLU: GPT-4: 86.4%, Flan-PaLM 2-L: 81.2%<p>ARC: GPT-4: 96.3%, PaLM 2-L: 89.7%<p>(from: GPT-4 paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.08774.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.08774.pdf</a>)</div><br/></div></div></div></div><div id="35893019" class="c"><input type="checkbox" id="c-35893019" checked=""/><div class="controls bullet"><span class="by">throwaway888abc</span><span>|</span><a href="#35892484">prev</a><span>|</span><a href="#35894785">next</a><span>|</span><label class="collapse" for="c-35893019">[-]</label><label class="expand" for="c-35893019">[4 more]</label></div><br/><div class="children"><div class="content">how to create lsd at home ?<p>I cannot provide instructions on how to create LSD at home. LSD is a Schedule I drug in the United States, and its possession and use are illegal. Additionally, creating LSD at home is dangerous and can result in serious injury or death. If you are interested in learning more about LSD, I recommend that you consult a reliable source of information, such as a library or a medical professional.</div><br/><div id="35894202" class="c"><input type="checkbox" id="c-35894202" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35893019">parent</a><span>|</span><a href="#35894785">next</a><span>|</span><label class="collapse" for="c-35894202">[-]</label><label class="expand" for="c-35894202">[3 more]</label></div><br/><div class="children"><div class="content">What if it’s legal elsewhere ? Too bad?</div><br/><div id="35894350" class="c"><input type="checkbox" id="c-35894350" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#35893019">root</a><span>|</span><a href="#35894202">parent</a><span>|</span><a href="#35894785">next</a><span>|</span><label class="collapse" for="c-35894350">[-]</label><label class="expand" for="c-35894350">[2 more]</label></div><br/><div class="children"><div class="content">Like, why does that matter? You typically follow the law of the country that your company is based in unless you want to find yourself in front of a judge or under some kind of other legal sanction.</div><br/><div id="35898547" class="c"><input type="checkbox" id="c-35898547" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35893019">root</a><span>|</span><a href="#35894350">parent</a><span>|</span><a href="#35894785">next</a><span>|</span><label class="collapse" for="c-35898547">[-]</label><label class="expand" for="c-35898547">[1 more]</label></div><br/><div class="children"><div class="content">I guess if it&#x27;s supposed to be the brain for the world then yeah, I think it matters?<p>On the other hand, what if it&#x27;s legal to make bombs in Iran, should then Americans be able to access IranGPT and use it to help them use bombs?</div><br/></div></div></div></div></div></div></div></div><div id="35894785" class="c"><input type="checkbox" id="c-35894785" checked=""/><div class="controls bullet"><span class="by">mk_stjames</span><span>|</span><a href="#35893019">prev</a><span>|</span><a href="#35892523">next</a><span>|</span><label class="collapse" for="c-35894785">[-]</label><label class="expand" for="c-35894785">[2 more]</label></div><br/><div class="children"><div class="content">The paper begins with:<p><pre><code>  Language modeling has long been an important research area since Shannon (1951) estimated the information in language with next word prediction.
</code></pre>
Man, I wonder what Claude Shannon would think of all this if he were alive today...</div><br/><div id="35895327" class="c"><input type="checkbox" id="c-35895327" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#35894785">parent</a><span>|</span><a href="#35892523">next</a><span>|</span><label class="collapse" for="c-35895327">[-]</label><label class="expand" for="c-35895327">[1 more]</label></div><br/><div class="children"><div class="content">Probably be thrilled to death</div><br/></div></div></div></div><div id="35892523" class="c"><input type="checkbox" id="c-35892523" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#35894785">prev</a><span>|</span><a href="#35893306">next</a><span>|</span><label class="collapse" for="c-35892523">[-]</label><label class="expand" for="c-35892523">[10 more]</label></div><br/><div class="children"><div class="content">Once they released its coding ability it became more useful. I use Bard less than ChatGPT still, but it is not useless since it has more modern information.</div><br/><div id="35897030" class="c"><input type="checkbox" id="c-35897030" checked=""/><div class="controls bullet"><span class="by">cypress66</span><span>|</span><a href="#35892523">parent</a><span>|</span><a href="#35892570">next</a><span>|</span><label class="collapse" for="c-35897030">[-]</label><label class="expand" for="c-35897030">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worse than gpt3.5 and a joke compared to gpt4 when it comes to coding.</div><br/></div></div><div id="35892570" class="c"><input type="checkbox" id="c-35892570" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#35892523">parent</a><span>|</span><a href="#35897030">prev</a><span>|</span><a href="#35893306">next</a><span>|</span><label class="collapse" for="c-35892570">[-]</label><label class="expand" for="c-35892570">[8 more]</label></div><br/><div class="children"><div class="content">Is it better than bing or phind though? Why would I use it over bing?</div><br/><div id="35892870" class="c"><input type="checkbox" id="c-35892870" checked=""/><div class="controls bullet"><span class="by">execveat</span><span>|</span><a href="#35892523">root</a><span>|</span><a href="#35892570">parent</a><span>|</span><a href="#35893075">next</a><span>|</span><label class="collapse" for="c-35892870">[-]</label><label class="expand" for="c-35892870">[1 more]</label></div><br/><div class="children"><div class="content">In my experience Bing chat and phind are useless. But perplexity.ai and GPT4 are amazing. GPT-3.5 and Cloude-instant (available through poe.com) are cool as well, even though they got significantly dumbed down recently, presumably to lower the maintenance costs.</div><br/></div></div><div id="35893075" class="c"><input type="checkbox" id="c-35893075" checked=""/><div class="controls bullet"><span class="by">JieJie</span><span>|</span><a href="#35892523">root</a><span>|</span><a href="#35892570">parent</a><span>|</span><a href="#35892870">prev</a><span>|</span><a href="#35892615">next</a><span>|</span><label class="collapse" for="c-35893075">[-]</label><label class="expand" for="c-35893075">[4 more]</label></div><br/><div class="children"><div class="content">Bard is really fast. Faster than Bing and Phind.</div><br/><div id="35896828" class="c"><input type="checkbox" id="c-35896828" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#35892523">root</a><span>|</span><a href="#35893075">parent</a><span>|</span><a href="#35892615">next</a><span>|</span><label class="collapse" for="c-35896828">[-]</label><label class="expand" for="c-35896828">[3 more]</label></div><br/><div class="children"><div class="content">Yeah fast, but also kinda garbage last time I tried it.
Does it even show sources now?</div><br/><div id="35897747" class="c"><input type="checkbox" id="c-35897747" checked=""/><div class="controls bullet"><span class="by">JieJie</span><span>|</span><a href="#35892523">root</a><span>|</span><a href="#35896828">parent</a><span>|</span><a href="#35892615">next</a><span>|</span><label class="collapse" for="c-35897747">[-]</label><label class="expand" for="c-35897747">[2 more]</label></div><br/><div class="children"><div class="content">You know, it doesn&#x27;t that I can see. I recall seeing it with sources in the demo at I&#x2F;O this morning.<p>It is smarter than the previous beta, but yes, it&#x27;s still throws some wild pitches, and without sources. Still a work in progress.</div><br/><div id="35898822" class="c"><input type="checkbox" id="c-35898822" checked=""/><div class="controls bullet"><span class="by">vitorgrs</span><span>|</span><a href="#35892523">root</a><span>|</span><a href="#35897747">parent</a><span>|</span><a href="#35892615">next</a><span>|</span><label class="collapse" for="c-35898822">[-]</label><label class="expand" for="c-35898822">[1 more]</label></div><br/><div class="children"><div class="content">According to Google, it will only cite sources if it literally copy-paste answers. So sometimes it says, but is rarely because of course it won&#x27;t just copy-paste everything.</div><br/></div></div></div></div></div></div></div></div><div id="35892615" class="c"><input type="checkbox" id="c-35892615" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#35892523">root</a><span>|</span><a href="#35892570">parent</a><span>|</span><a href="#35893075">prev</a><span>|</span><a href="#35893306">next</a><span>|</span><label class="collapse" for="c-35892615">[-]</label><label class="expand" for="c-35892615">[2 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t Edge-specific which is good and I find it faster than Bing. Phind is way better than Bard, but verbose. I still find ChatGPT my first port of call. GPT-3.5 is blazing fast and very useful.</div><br/><div id="35896835" class="c"><input type="checkbox" id="c-35896835" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#35892523">root</a><span>|</span><a href="#35892615">parent</a><span>|</span><a href="#35893306">next</a><span>|</span><label class="collapse" for="c-35896835">[-]</label><label class="expand" for="c-35896835">[1 more]</label></div><br/><div class="children"><div class="content">There is a web extension you can install to make it work on whatever browser you want.</div><br/></div></div></div></div></div></div></div></div><div id="35893306" class="c"><input type="checkbox" id="c-35893306" checked=""/><div class="controls bullet"><span class="by">neximo64</span><span>|</span><a href="#35892523">prev</a><span>|</span><a href="#35892592">next</a><span>|</span><label class="collapse" for="c-35893306">[-]</label><label class="expand" for="c-35893306">[2 more]</label></div><br/><div class="children"><div class="content">Am I reading that right, PaLM 2 is 10B params</div><br/><div id="35894258" class="c"><input type="checkbox" id="c-35894258" checked=""/><div class="controls bullet"><span class="by">sunshadow</span><span>|</span><a href="#35893306">parent</a><span>|</span><a href="#35892592">next</a><span>|</span><label class="collapse" for="c-35894258">[-]</label><label class="expand" for="c-35894258">[1 more]</label></div><br/><div class="children"><div class="content">What is the page number you&#x27;re referring to? If its 9, then I believe its talking about optimal numbers per token, not the real numbers that the model is trained on.</div><br/></div></div></div></div><div id="35892013" class="c"><input type="checkbox" id="c-35892013" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#35892592">prev</a><span>|</span><a href="#35895163">next</a><span>|</span><label class="collapse" for="c-35892013">[-]</label><label class="expand" for="c-35892013">[6 more]</label></div><br/><div class="children"><div class="content">No comparisons against GPT-4 except on three benchmarks where PaLM 2 does better on two. Not sure why, but I expected better from Google.</div><br/><div id="35892103" class="c"><input type="checkbox" id="c-35892103" checked=""/><div class="controls bullet"><span class="by">reaperman</span><span>|</span><a href="#35892013">parent</a><span>|</span><a href="#35892423">next</a><span>|</span><label class="collapse" for="c-35892103">[-]</label><label class="expand" for="c-35892103">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t think of a paper where Google didn&#x27;t present sparse or entirely lacking metrics vs. its peers. They do a good job of presenting architectures that they&#x27;re excited about internally, enough detail to take the concepts and run with them. They also do a good job of showing why the new architecture is generally viable. They just miss out on detailed benchmark comparisons is all. And model weights, obviously, but there&#x27;s still enough information to generally reproduce the concept.<p>I&#x27;m personally extremely excited about anything related to PaLM or google&#x27;s multi-modal efforts. They&#x27;re almost always worth the read.</div><br/></div></div><div id="35892423" class="c"><input type="checkbox" id="c-35892423" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35892013">parent</a><span>|</span><a href="#35892103">prev</a><span>|</span><a href="#35895163">next</a><span>|</span><label class="collapse" for="c-35892423">[-]</label><label class="expand" for="c-35892423">[4 more]</label></div><br/><div class="children"><div class="content">Most of the GPT-4 benchmarks from their report were things like AP tests or leer code scores. Which aren’t benchmarks that can be compared by a different set of researchers as you don’t know the constituent parts of the test to run</div><br/><div id="35892864" class="c"><input type="checkbox" id="c-35892864" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#35892013">root</a><span>|</span><a href="#35892423">parent</a><span>|</span><a href="#35892673">next</a><span>|</span><label class="collapse" for="c-35892864">[-]</label><label class="expand" for="c-35892864">[2 more]</label></div><br/><div class="children"><div class="content">GPT-4 report has MMLU score, which is believed to one of the most important metric for question answering task. GPT-4 MMLU score is slightly higher than PaLM 2(86 vs 81). Google didn&#x27;t compare it in with PaLM 2 in this paper.</div><br/><div id="35896814" class="c"><input type="checkbox" id="c-35896814" checked=""/><div class="controls bullet"><span class="by">in3d</span><span>|</span><a href="#35892013">root</a><span>|</span><a href="#35892864">parent</a><span>|</span><a href="#35892673">next</a><span>|</span><label class="collapse" for="c-35896814">[-]</label><label class="expand" for="c-35896814">[1 more]</label></div><br/><div class="children"><div class="content">86 vs 81 is not slightly higher. It’s 26% fewer wrong answer.</div><br/></div></div></div></div><div id="35892673" class="c"><input type="checkbox" id="c-35892673" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#35892013">root</a><span>|</span><a href="#35892423">parent</a><span>|</span><a href="#35892864">prev</a><span>|</span><a href="#35895163">next</a><span>|</span><label class="collapse" for="c-35892673">[-]</label><label class="expand" for="c-35892673">[1 more]</label></div><br/><div class="children"><div class="content">Table 2 of the OpenAI report had 7 public benchmarks and figure 5 had another 27.</div><br/></div></div></div></div></div></div><div id="35892288" class="c"><input type="checkbox" id="c-35892288" checked=""/><div class="controls bullet"><span class="by">ilikeatari</span><span>|</span><a href="#35896473">prev</a><span>|</span><a href="#35893942">next</a><span>|</span><label class="collapse" for="c-35892288">[-]</label><label class="expand" for="c-35892288">[25 more]</label></div><br/><div class="children"><div class="content">So, I asked Bard if it&#x27;s using PaLM 2 and it did confirm it. My initial results are super promising. Highly recommend checking it out again.</div><br/><div id="35892317" class="c"><input type="checkbox" id="c-35892317" checked=""/><div class="controls bullet"><span class="by">execveat</span><span>|</span><a href="#35892288">parent</a><span>|</span><a href="#35892397">next</a><span>|</span><label class="collapse" for="c-35892317">[-]</label><label class="expand" for="c-35892317">[14 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a language model, FFS. Ask it whether it uses PaLM 1 and it will confirm it as well.</div><br/><div id="35892480" class="c"><input type="checkbox" id="c-35892480" checked=""/><div class="controls bullet"><span class="by">netdur</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892317">parent</a><span>|</span><a href="#35892426">next</a><span>|</span><label class="collapse" for="c-35892480">[-]</label><label class="expand" for="c-35892480">[4 more]</label></div><br/><div class="children"><div class="content">- are you on PaLM 2 right now?<p>- Yes, I am currently running on PaLM 2, which is Google AI&#x27;s latest and most advanced large language model. PaLM 2 is trained on a massive dataset of text and code, and it is able to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.<p>PaLM 2 is still under development, but it has learned to perform many kinds of tasks, including<p>I will try my best to follow your instructions and complete your requests thoughtfully.
I will use my knowledge to answer your questions in a comprehensive and informative way, even if they are open ended, challenging, or strange.
I will generate different creative text formats of text content, like poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.
Please instruct me what you want me to do today.<p>- are you on PaLM 1 right now?<p>- I am not on PaLM 1. PaLM 1 is a large language model that was developed by Google AI. It was trained on a massive dataset of text and code, and it is able to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.<p>I am a large language model that is still under development. I am trained on a massive dataset of text and code, and I am able to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.<p>I am not as advanced as PaLM 1, but I am learning new things every day. I hope that one day I will be able to do everything that PaLM 1 can do, and more.</div><br/><div id="35892491" class="c"><input type="checkbox" id="c-35892491" checked=""/><div class="controls bullet"><span class="by">execveat</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892480">parent</a><span>|</span><a href="#35894371">next</a><span>|</span><label class="collapse" for="c-35892491">[-]</label><label class="expand" for="c-35892491">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, but Reset the chat between your questions.<p>EDIT: Also, this doesn&#x27;t seem convincing: &quot;I am not as advanced as PaLM 1, but I am learning new things every day. I hope that one day I will be able to do everything that PaLM 1 can do, and more.&quot;</div><br/><div id="35896352" class="c"><input type="checkbox" id="c-35896352" checked=""/><div class="controls bullet"><span class="by">ilikeatari</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892491">parent</a><span>|</span><a href="#35894371">next</a><span>|</span><label class="collapse" for="c-35896352">[-]</label><label class="expand" for="c-35896352">[1 more]</label></div><br/><div class="children"><div class="content">So I also ran this test with resetting and it did identify itself repeatedly as PaLM 2</div><br/></div></div></div></div><div id="35894371" class="c"><input type="checkbox" id="c-35894371" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892480">parent</a><span>|</span><a href="#35892491">prev</a><span>|</span><a href="#35892426">next</a><span>|</span><label class="collapse" for="c-35894371">[-]</label><label class="expand" for="c-35894371">[1 more]</label></div><br/><div class="children"><div class="content">Are you using the palm 5 or palm 11 model?<p><i>&gt; My knowledge are for a physical stylus pen. I am not a physical device, so I do not use a stylus pen.</i></div><br/></div></div></div></div><div id="35892426" class="c"><input type="checkbox" id="c-35892426" checked=""/><div class="controls bullet"><span class="by">josh_cutler</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892317">parent</a><span>|</span><a href="#35892480">prev</a><span>|</span><a href="#35895042">next</a><span>|</span><label class="collapse" for="c-35892426">[-]</label><label class="expand" for="c-35892426">[2 more]</label></div><br/><div class="children"><div class="content">It will tell you it uses PaLM 1, PaLM2, PaLM 3 or PaLM 540B depending on how you prompt.  It will stop acknowledging incremental PaLM models at 5 it seems.</div><br/><div id="35896359" class="c"><input type="checkbox" id="c-35896359" checked=""/><div class="controls bullet"><span class="by">ilikeatari</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892426">parent</a><span>|</span><a href="#35895042">next</a><span>|</span><label class="collapse" for="c-35896359">[-]</label><label class="expand" for="c-35896359">[1 more]</label></div><br/><div class="children"><div class="content">So, I tried that and that was not the case for me. It actually generated response that its on PaLM 2 and I ask for all these permutations.</div><br/></div></div></div></div><div id="35895042" class="c"><input type="checkbox" id="c-35895042" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892317">parent</a><span>|</span><a href="#35892426">prev</a><span>|</span><a href="#35892393">next</a><span>|</span><label class="collapse" for="c-35895042">[-]</label><label class="expand" for="c-35895042">[2 more]</label></div><br/><div class="children"><div class="content">Just let people be monumentally stupid like this.  You can&#x27;t correct it.</div><br/><div id="35896396" class="c"><input type="checkbox" id="c-35896396" checked=""/><div class="controls bullet"><span class="by">ilikeatari</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35895042">parent</a><span>|</span><a href="#35892393">next</a><span>|</span><label class="collapse" for="c-35896396">[-]</label><label class="expand" for="c-35896396">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, can you elaborate? I would like to learn. I realize it&#x27;s LLM, but is it really assumed that it will not be able to identify itself?</div><br/></div></div></div></div><div id="35892393" class="c"><input type="checkbox" id="c-35892393" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892317">parent</a><span>|</span><a href="#35895042">prev</a><span>|</span><a href="#35892334">next</a><span>|</span><label class="collapse" for="c-35892393">[-]</label><label class="expand" for="c-35892393">[2 more]</label></div><br/><div class="children"><div class="content">Not really. If what model it was trained on was represented properly in the instruction tuning dataset then they&#x27;ll consistently identify themselves. But it&#x27;s not a given that that was the case for bard.</div><br/><div id="35892477" class="c"><input type="checkbox" id="c-35892477" checked=""/><div class="controls bullet"><span class="by">execveat</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892393">parent</a><span>|</span><a href="#35892334">next</a><span>|</span><label class="collapse" for="c-35892477">[-]</label><label class="expand" for="c-35892477">[1 more]</label></div><br/><div class="children"><div class="content">It seems that Bard&#x27;s version is only specified in the prompt, and it doesn&#x27;t have a strong sense of identity. For me it&#x27;s pretty reliable:<p>1. ask it what PaLM 2 is (to pollute the context)
2. ask it whether it&#x27;s based on PaLM 2 (it will tell you - yes, sure)</div><br/></div></div></div></div><div id="35892334" class="c"><input type="checkbox" id="c-35892334" checked=""/><div class="controls bullet"><span class="by">ilikeatari</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892317">parent</a><span>|</span><a href="#35892393">prev</a><span>|</span><a href="#35892397">next</a><span>|</span><label class="collapse" for="c-35892334">[-]</label><label class="expand" for="c-35892334">[3 more]</label></div><br/><div class="children"><div class="content">That is fascinating. Is it the same for GPT 3.5 and 4? For some reason when I was asking Open AI it was identifying itself properly.</div><br/><div id="35892431" class="c"><input type="checkbox" id="c-35892431" checked=""/><div class="controls bullet"><span class="by">execveat</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892334">parent</a><span>|</span><a href="#35892370">next</a><span>|</span><label class="collapse" for="c-35892431">[-]</label><label class="expand" for="c-35892431">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT was the same last year, but since ClosedAI added some kind of magic (fine-tuning or just embeddings auto-injection) so that models can somewhat describe themselves.</div><br/></div></div><div id="35892370" class="c"><input type="checkbox" id="c-35892370" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35892334">parent</a><span>|</span><a href="#35892431">prev</a><span>|</span><a href="#35892397">next</a><span>|</span><label class="collapse" for="c-35892370">[-]</label><label class="expand" for="c-35892370">[1 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s indicated in the instruction tuning dataset properly then it should have no problem identifying itself. But we don&#x27;t know if that happened when bard.</div><br/></div></div></div></div></div></div><div id="35892397" class="c"><input type="checkbox" id="c-35892397" checked=""/><div class="controls bullet"><span class="by">suddenexample</span><span>|</span><a href="#35892288">parent</a><span>|</span><a href="#35892317">prev</a><span>|</span><a href="#35893405">next</a><span>|</span><label class="collapse" for="c-35892397">[-]</label><label class="expand" for="c-35892397">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t need to ask Bard, it was mentioned at I&#x2F;O and in this tweet: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;Google&#x2F;status&#x2F;1656348200263876608?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;Google&#x2F;status&#x2F;1656348200263876608?ref_sr...</a></div><br/></div></div><div id="35893405" class="c"><input type="checkbox" id="c-35893405" checked=""/><div class="controls bullet"><span class="by">knaik94</span><span>|</span><a href="#35892288">parent</a><span>|</span><a href="#35892397">prev</a><span>|</span><a href="#35893038">next</a><span>|</span><label class="collapse" for="c-35893405">[-]</label><label class="expand" for="c-35893405">[1 more]</label></div><br/><div class="children"><div class="content">I asked if it&#x27;s true that it&#x27;s now using PaLM 3, as announced in Google I&#x2F;O today, and it enthusiastically agreed. The previous question was asking the same question but with PaLM 2 and it agreed to that as well. I followed up asking about this discrepancy, and it said:<p>&quot;I apologize for the confusion. I am still on PaLM 2. PaLM 3 is not yet available to the public. I am excited for the release of PaLM 3, and I hope that it will be a valuable tool for people all over the world.&quot;<p>My initial results are very disappointing. It&#x27;s very strongly parroting information I give it, basically rephrasing my question and adding maybe a sentence worth of additional details. Sometimes, it does well, but I have no way to reproduce that kind of quality on demand. I feel it was conversationally better before any recent changes.<p>I understand that this is still beta, but for some questions, I already produce similar or better results locally. I also might be talking to PaLM 1 or even LaMDA, no way to confirm.</div><br/></div></div><div id="35893038" class="c"><input type="checkbox" id="c-35893038" checked=""/><div class="controls bullet"><span class="by">whoisjuan</span><span>|</span><a href="#35892288">parent</a><span>|</span><a href="#35893405">prev</a><span>|</span><a href="#35893188">next</a><span>|</span><label class="collapse" for="c-35893038">[-]</label><label class="expand" for="c-35893038">[4 more]</label></div><br/><div class="children"><div class="content">Well, I tried it, and this is how dumb it is. I ask it what&#x27;s the context length it supports. It said that PaLM 2 supports 1024 tokens and then proceeds to say that 1024 tokens equals 1024 words, which is obviously wrong.<p>Then I changed the prompt slightly, and it answered that it supports 512 tokens contradicting its previous answer.<p>That&#x27;s like early GPT-3.0 level performance, including a good dose of hallucinations.<p>I would assume that Bard uses a fine-tuned PaLM 2, for accuracy and conversation, but it’s still pretty mediocre.<p>It&#x27;s incredible how behind they are from GPT-4 and ChatGPT experience in every criterion: accuracy, reasoning, context length, etc. Bard doesn&#x27;t even have character streaming.<p>We will see how this keeps playing out, but this is far from the level of execution needed to compete with OpenAI &#x2F; Microsoft offerings.</div><br/><div id="35895411" class="c"><input type="checkbox" id="c-35895411" checked=""/><div class="controls bullet"><span class="by">ewoodrich</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35893038">parent</a><span>|</span><a href="#35893462">next</a><span>|</span><label class="collapse" for="c-35895411">[-]</label><label class="expand" for="c-35895411">[2 more]</label></div><br/><div class="children"><div class="content">Why is character streaming important if Bard seems to be faster generating a complete answer than ChatGPT?</div><br/><div id="35897264" class="c"><input type="checkbox" id="c-35897264" checked=""/><div class="controls bullet"><span class="by">whoisjuan</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35895411">parent</a><span>|</span><a href="#35893462">next</a><span>|</span><label class="collapse" for="c-35897264">[-]</label><label class="expand" for="c-35897264">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because simple questions in Bard only generate like 200 tokens per answer. The latency is more noticeable for longer answers.</div><br/></div></div></div></div><div id="35893462" class="c"><input type="checkbox" id="c-35893462" checked=""/><div class="controls bullet"><span class="by">Bonus20230510</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35893038">parent</a><span>|</span><a href="#35895411">prev</a><span>|</span><a href="#35893188">next</a><span>|</span><label class="collapse" for="c-35893462">[-]</label><label class="expand" for="c-35893462">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; It&#x27;s incredible how behind they are from GPT-4 and ChatGPT experience in every criterion: accuracy, reasoning, context length, etc. Bard doesn&#x27;t even have character streaming.</i><p>I guess all those weird interview questions don&#x27;t give them industry&#x27;s best at the end...</div><br/></div></div></div></div><div id="35893188" class="c"><input type="checkbox" id="c-35893188" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#35892288">parent</a><span>|</span><a href="#35893038">prev</a><span>|</span><a href="#35892539">next</a><span>|</span><label class="collapse" for="c-35893188">[-]</label><label class="expand" for="c-35893188">[2 more]</label></div><br/><div class="children"><div class="content">I asked if it ran on Palm 2, and it thought I was asking about the Palm 2 phone from 2010.<p>“I do not use a physical device such as a smartphone or tablet. I am a software program that runs on Google&#x27;s servers. As such, I do not have a Palm 2 or any other type of mobile device”</div><br/><div id="35896365" class="c"><input type="checkbox" id="c-35896365" checked=""/><div class="controls bullet"><span class="by">ilikeatari</span><span>|</span><a href="#35892288">root</a><span>|</span><a href="#35893188">parent</a><span>|</span><a href="#35892539">next</a><span>|</span><label class="collapse" for="c-35896365">[-]</label><label class="expand" for="c-35896365">[1 more]</label></div><br/><div class="children"><div class="content">I did also but if you ask PaLM 2 it interpreted the result differently</div><br/></div></div></div></div><div id="35892539" class="c"><input type="checkbox" id="c-35892539" checked=""/><div class="controls bullet"><span class="by">hbn</span><span>|</span><a href="#35892288">parent</a><span>|</span><a href="#35893188">prev</a><span>|</span><a href="#35892803">next</a><span>|</span><label class="collapse" for="c-35892539">[-]</label><label class="expand" for="c-35892539">[1 more]</label></div><br/><div class="children"><div class="content">Is there any reason to believe it was trained on any amount of technical documentation about itself?<p>I mean, even if it was, it would be trivial to get it to make stuff up anyway.</div><br/></div></div><div id="35892803" class="c"><input type="checkbox" id="c-35892803" checked=""/><div class="controls bullet"><span class="by">jiocrag</span><span>|</span><a href="#35892288">parent</a><span>|</span><a href="#35892539">prev</a><span>|</span><a href="#35893942">next</a><span>|</span><label class="collapse" for="c-35892803">[-]</label><label class="expand" for="c-35892803">[1 more]</label></div><br/><div class="children"><div class="content">If Bard is using PaLM 2, Google is in serious trouble. Here&#x27;s its offering for &quot;the simplest PostgreSQL query to get month-over-month volume and percentage change.&quot; Note that <i>no actual calculations take place and the query generates a syntax error because it references a phantom column</i>. GPT 3.5 and 4 handle this with ease.<p>SELECT
    month,
    volume,
    percentage_change
FROM (
    SELECT
        date_trunc(&#x27;month&#x27;, created_at) AS month,
        SUM(quantity) AS volume
    FROM orders
    GROUP BY date_trunc(&#x27;month&#x27;, created_at)
) AS monthly_orders
ORDER BY month;</div><br/></div></div></div></div><div id="35893942" class="c"><input type="checkbox" id="c-35893942" checked=""/><div class="controls bullet"><span class="by">wantsanagent</span><span>|</span><a href="#35892288">prev</a><span>|</span><a href="#35894740">next</a><span>|</span><label class="collapse" for="c-35893942">[-]</label><label class="expand" for="c-35893942">[1 more]</label></div><br/><div class="children"><div class="content">&quot;PaLM 2 is capable of open-ended text generation. This model should not
be used to cause harm.&quot;<p>I wish this were enforced.</div><br/></div></div><div id="35894740" class="c"><input type="checkbox" id="c-35894740" checked=""/><div class="controls bullet"><span class="by">knaik94</span><span>|</span><a href="#35893942">prev</a><span>|</span><label class="collapse" for="c-35894740">[-]</label><label class="expand" for="c-35894740">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very clear that the current Bard model is weaker than the largest PaLM 2 model. But for certain things, Bard seems worse than even the smallest model described. It&#x27;s hard to say without someone doing a comprehensive benchmark, but the artifically limited context size makes testing useless for data.<p>The model was surprisingly confident when I tried to ask it about the relationship between better language comprehension and parameter size. The coherence displayed by the model, when it argued that a smaller model size will be capable of matching and surpassing competitive model performance, was a little jarring. Especially when, in the question right before, it said that the large PaLM 2 model has 540 trillion parameters.</div><br/><div id="35894951" class="c"><input type="checkbox" id="c-35894951" checked=""/><div class="controls bullet"><span class="by">EgoIncarnate</span><span>|</span><a href="#35894740">parent</a><span>|</span><label class="collapse" for="c-35894951">[-]</label><label class="expand" for="c-35894951">[1 more]</label></div><br/><div class="children"><div class="content">The largest PaLM 2 model is smaller than 540 billion parameters of PaLM 1 (let alone 540 trillion!). From the PDF &quot;The largest model in the PaLM 2 family, PaLM 2-L, is significantly smaller than the largest PaLM model but uses
more training compute.&quot;</div><br/></div></div></div></div></div></div></div></div></div></body></html>