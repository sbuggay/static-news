<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715677264332" as="style"/><link rel="stylesheet" href="styles.css?v=1715677264332"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.intel.com/content/www/us/en/newsroom/news/intel-powered-aurora-supercomputer-breaks-exascale-barrier.html">Intel announces the Aurora supercomputer has broken the exascale barrier</a> <span class="domain">(<a href="https://www.intel.com">www.intel.com</a>)</span></div><div class="subtext"><span>mepian</span> | <span>76 comments</span></div><br/><div><div id="40349550" class="c"><input type="checkbox" id="c-40349550" checked=""/><div class="controls bullet"><span class="by">CaliforniaKarl</span><span>|</span><a href="#40349834">next</a><span>|</span><label class="collapse" for="c-40349550">[-]</label><label class="expand" for="c-40349550">[7 more]</label></div><br/><div class="children"><div class="content">More context: This is related to today&#x27;s release of the Spring Top 500 list (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40346788">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40346788</a>).  Aurora rated 1,012.00 PetaFLOPS&#x2F;second Rmax, and is in 2nd place, behind Frontier.<p>In the November 2023 list, Aurora was also in second place, with an Rmax of 585.34 PetaFLOPS&#x2F;second.<p>See <a href="https:&#x2F;&#x2F;www.top500.org&#x2F;system&#x2F;180183&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.top500.org&#x2F;system&#x2F;180183&#x2F;</a> for the specs on Aurora, and <a href="https:&#x2F;&#x2F;www.top500.org&#x2F;system&#x2F;180047&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.top500.org&#x2F;system&#x2F;180047&#x2F;</a> for the specs on Frontier.<p>See <a href="https:&#x2F;&#x2F;www.top500.org&#x2F;project&#x2F;top500_description&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.top500.org&#x2F;project&#x2F;top500_description&#x2F;</a> and <a href="https:&#x2F;&#x2F;www.top500.org&#x2F;project&#x2F;linpack&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.top500.org&#x2F;project&#x2F;linpack&#x2F;</a> for a description of Rmax and the LINPACK benchmark, by which supercomputers are generally ranked.  The Top 500 list only includes supercomputers that are able to run the LINPACK benchmark, and where the owner is willing to publish the results.<p>The jump in Aurora&#x27;s Rmax scope is explained by Aurora&#x27;s difficult birth.  <a href="https:&#x2F;&#x2F;morethanmoore.substack.com&#x2F;p&#x2F;5-years-late-only-2" rel="nofollow">https:&#x2F;&#x2F;morethanmoore.substack.com&#x2F;p&#x2F;5-years-late-only-2</a> (published when the November 2023 list came out) has a good explanation of what&#x27;s been going on.</div><br/><div id="40352754" class="c"><input type="checkbox" id="c-40352754" checked=""/><div class="controls bullet"><span class="by">imrehg</span><span>|</span><a href="#40349550">parent</a><span>|</span><a href="#40352786">next</a><span>|</span><label class="collapse" for="c-40352754">[-]</label><label class="expand" for="c-40352754">[1 more]</label></div><br/><div class="children"><div class="content">Looking at the two specs, interesting to see how Frontier (the first, running AMD CPUs) has much better power efficiency than Aurora (the second, running Intel), 18.89 kW&#x2F;PFLOPS vs 38.24 kW&#x2F;PFLOPS respectively... Good advertisement for AMD? :)</div><br/></div></div><div id="40351333" class="c"><input type="checkbox" id="c-40351333" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#40349550">parent</a><span>|</span><a href="#40352786">prev</a><span>|</span><a href="#40349834">next</a><span>|</span><label class="collapse" for="c-40351333">[-]</label><label class="expand" for="c-40351333">[4 more]</label></div><br/><div class="children"><div class="content">What does FLOPS&#x2F;second mean? Isn’t FLOPS already per second? Are they accelerating?</div><br/><div id="40353004" class="c"><input type="checkbox" id="c-40353004" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#40349550">root</a><span>|</span><a href="#40351333">parent</a><span>|</span><a href="#40351361">next</a><span>|</span><label class="collapse" for="c-40353004">[-]</label><label class="expand" for="c-40353004">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d actually be interested in an estimate of the world&#x27;s overall flop&#x2F;s^2. Could someone please run a back of the envelope calculation for me, e.g. looking at least year&#x27;s data?</div><br/></div></div><div id="40351361" class="c"><input type="checkbox" id="c-40351361" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40349550">root</a><span>|</span><a href="#40351333">parent</a><span>|</span><a href="#40353004">prev</a><span>|</span><a href="#40352288">next</a><span>|</span><label class="collapse" for="c-40351361">[-]</label><label class="expand" for="c-40351361">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the top500 pages cited use Flop&#x2F;s (apparently using Flop for “Floating point operations” – not sure which “o” and “p” are used), I’ve could swear I’ve seen FLOPS and seen it expanded specifically as “<i>FL</i>oating point <i>O</i>perations <i>P</i>er <i>S</i>econd” when I first encountered it, FLOPS&#x2F;s seems to be using “FLOPS” like the “Flop” above (probably as “<i>FL</i>oating point <i>OP</i>eration<i>S</i>”, in which case the “&#x2F;s” makes sense.)</div><br/></div></div><div id="40352288" class="c"><input type="checkbox" id="c-40352288" checked=""/><div class="controls bullet"><span class="by">kortilla</span><span>|</span><a href="#40349550">root</a><span>|</span><a href="#40351333">parent</a><span>|</span><a href="#40351361">prev</a><span>|</span><a href="#40349834">next</a><span>|</span><label class="collapse" for="c-40352288">[-]</label><label class="expand" for="c-40352288">[1 more]</label></div><br/><div class="children"><div class="content">Some people treat FLOPS as “FLoating point OPerationS”.</div><br/></div></div></div></div></div></div><div id="40349834" class="c"><input type="checkbox" id="c-40349834" checked=""/><div class="controls bullet"><span class="by">Harmohit</span><span>|</span><a href="#40349550">prev</a><span>|</span><a href="#40349623">next</a><span>|</span><label class="collapse" for="c-40349834">[-]</label><label class="expand" for="c-40349834">[18 more]</label></div><br/><div class="children"><div class="content">Serious question: my understanding of HPC is that there are many workloads running on a given supercomputer at any time. There is no singular workload that takes up the entire or most of the resources of a supercomputer.<p>Is my understanding correct? If yes, then why is it important to build supercomputers with more and more compute? Wouldn&#x27;t it be better to build smaller systems that focus more on power&#x2F;cost&#x2F;space efficiency?</div><br/><div id="40350049" class="c"><input type="checkbox" id="c-40350049" checked=""/><div class="controls bullet"><span class="by">bitfilped</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40350606">next</a><span>|</span><label class="collapse" for="c-40350049">[-]</label><label class="expand" for="c-40350049">[3 more]</label></div><br/><div class="children"><div class="content">Most of the time yes, HPC systems are shared among many users. Sometimes though the whole system (or near it) will be used in a single run. These are sometimes referred to as &quot;hero runs&quot; and while they&#x27;re more common for benchmarking and burn-ins there are some tightly-coupled workloads that perform well in that style of execution. It really depends on a number of factors like the workloads being run, the number of users, and what the primary business purpose of the HPC resource is. Sites that have to run both types of jobs will typically allow any user to schedule jobs most of the time but then pre-reserve blocks of time for hero runs to take place where other user jobs are held until the primary scheduled run is over.</div><br/><div id="40350474" class="c"><input type="checkbox" id="c-40350474" checked=""/><div class="controls bullet"><span class="by">Harmohit</span><span>|</span><a href="#40349834">root</a><span>|</span><a href="#40350049">parent</a><span>|</span><a href="#40350606">next</a><span>|</span><label class="collapse" for="c-40350474">[-]</label><label class="expand" for="c-40350474">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the reply! Can you give some examples of these &quot;hero runs&quot;?</div><br/><div id="40350491" class="c"><input type="checkbox" id="c-40350491" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40349834">root</a><span>|</span><a href="#40350474">parent</a><span>|</span><a href="#40350606">next</a><span>|</span><label class="collapse" for="c-40350491">[-]</label><label class="expand" for="c-40350491">[1 more]</label></div><br/><div class="children"><div class="content">At our university, at least when I studied there some 15 years ago, the whole cluster was occupied doing weather predictions each and every night.<p>No point in staying up waiting for a job, it&#x27;d get rescheduled in the early morning at best.<p>It wasn&#x27;t the largest cluster around, IIRC 768 quad-core nodes, but I&#x27;m sure the meteorological department would find a way to utilize any extra capacity, so still requiring the whole thing all night.</div><br/></div></div></div></div></div></div><div id="40350606" class="c"><input type="checkbox" id="c-40350606" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40350049">prev</a><span>|</span><a href="#40350490">next</a><span>|</span><label class="collapse" for="c-40350606">[-]</label><label class="expand" for="c-40350606">[7 more]</label></div><br/><div class="children"><div class="content">There&#x27;s many variables that go into supercomputers, of which &quot;company&#x2F;country propaganda&quot; is just one of them.<p>Supercomputer admins would love to have a single code that used the whole machine, both the compute elements and the network elements, at close to 100%.  In fact they spend a significant fraction on network elements to unblock the compute elements, but few codes are really so light on networking that the program scales to the full core count of the machine.  So, instead they usually have several codes which can scale up to a significant fraction of the machine and then backfill with smaller jobs to keep the utilization up (because the acquisition cost and the running cost are so high).<p>Supercomputers have limited utility- beyond country bragging rights, only a few problems really justify spending this kind of resource.  I intentionally switched my own research in molecular dynamics away from supercomputers (where I&#x27;d run one job on 64-128 processors for a 96X speedup) to closet cllusters, where I&#x27;d run 128 indpendent jobs for a 128X speedup, but then have to do a bunch of post-processing to make the results comparable to the long, large runs on the supercomputer (<a href="https:&#x2F;&#x2F;research.google&#x2F;pubs&#x2F;cloud-based-simulations-on-google-exacycle-reveal-ligand-modulation-of-gpcr-activation-pathways&#x2F;" rel="nofollow">https:&#x2F;&#x2F;research.google&#x2F;pubs&#x2F;cloud-based-simulations-on-goog...</a>).  I actually was really relieved when my work no longer depended on expensive resources with little support, as my scientific productivity went up and my costs went way down.<p>I feel that supercomputers are good at one thing: if you need to make your country&#x27;s flagship submarine about 10% faster&#x2F;quieter than the competition.</div><br/><div id="40350663" class="c"><input type="checkbox" id="c-40350663" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349834">root</a><span>|</span><a href="#40350606">parent</a><span>|</span><a href="#40350744">next</a><span>|</span><label class="collapse" for="c-40350663">[-]</label><label class="expand" for="c-40350663">[4 more]</label></div><br/><div class="children"><div class="content">Ever used GPT-3, DALL-E, or other LLMs?<p>The GPUs used to train them only existed because the DoE explicitly worked with Nvidia on a decade-long roadmap for delivery in it&#x27;s various supercomputers, and would often work in tandem with private sector players to coordinate purchases and R&amp;D (for example, protein folding and just about every Big Pharma company).<p>Hell, the only reason AMD EPYC exists is for the same reason.</div><br/><div id="40350816" class="c"><input type="checkbox" id="c-40350816" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40349834">root</a><span>|</span><a href="#40350663">parent</a><span>|</span><a href="#40350744">next</a><span>|</span><label class="collapse" for="c-40350816">[-]</label><label class="expand" for="c-40350816">[3 more]</label></div><br/><div class="children"><div class="content">Yes.  In my computational history I have: used the largest non-classified DOE supercomputers, built my own modest closet clusters, developed an embarassingly parallel computing system using Google&#x27;s idle prod cycles, and helped debug training of LLMs when I worked on the TPU team at Google.  I work for big pharma now (and my phd is in biophysics) and I&#x27;m also comfortable with other HPC domains.<p>I know the DOE&#x2F;Nvidia history quite well as the Chief Scientist of NVIDIA visited LBL around 2005(6? 7?) and talked about their new hardware they were just starting to build and sell, with the goal of getting them into supercomputers.<p>We asked if they had double precision performance yet (because that was a must for many supercomputer jobs), but at the time, nvidia DP was still lagging SP (I guess it still does?) and we also quibbled about their non-compliance with some esoteric details in IEEE 754. The best part of the whole talk was when he walked us through the idea of visualizing our operations by drawing the matrices as textures, because you can easily see the NaNs- they render as nvidia Green!<p>I left DOE (Berkeley Lab) shortly after to work in industry because it was clear that ML wasn&#x27;t going to be innovated in the government labs.</div><br/><div id="40351596" class="c"><input type="checkbox" id="c-40351596" checked=""/><div class="controls bullet"><span class="by">whoknw</span><span>|</span><a href="#40349834">root</a><span>|</span><a href="#40350816">parent</a><span>|</span><a href="#40350744">next</a><span>|</span><label class="collapse" for="c-40351596">[-]</label><label class="expand" for="c-40351596">[2 more]</label></div><br/><div class="children"><div class="content">Thank you for that comment, as someone who has had exposure to the DOE as well (and LBL at that) I can only echo your sentiment. I&#x27;d even go further and state that is hard for me to believe that any innovation can happen in the calcified structures of governmental labs. Maybe the classified ones are different.<p>The &quot;DOE made NVIDIA&quot; myth is a story I haven&#x27;t seen pushed outside the DOE complex. It is true that the supercomputers the DOE pushes could be considered industry subsidies, by providing industry companies with a steady customer with a very high tolerance for unfinished products. That applies to NVIDIA, AMD, Intel, HPE&#x2F;Cray and IBM more or less equally.<p>I also want to stress what often gets overlooked: supercomputers are hell to operate and use. Aurora runs on Slingshot, a Cray interconnect. Those things look good on paper. Examples: Cray Aries (and &quot;network quiesces&quot;) or Cray DataWarp. Who knows how Slingshot actually works in practice, for a hero run it only needs to hold things together for a few hours. As long as you get a high TOP500 ranking, a supercomputer is a success.<p>There is no market for those things anymore and they are beholden to the same economics as everything else, hence codes that can&#x27;t afford an army of PostDocs to work around the bugs and design decisions that are only necessary due to scale of those systems are better suited to plain old mid-range clusters. And I haven&#x27;t even mentioned the eccentric userland of supercomputers.<p>There are many reasons the DOE affords to run those behemoths. Some more trivial and petty than most people would like to believe. Like the author of the parent post, I have come to believe that the best bang for the buck on scientific output can be found elsewhere.</div><br/><div id="40352526" class="c"><input type="checkbox" id="c-40352526" checked=""/><div class="controls bullet"><span class="by">close04</span><span>|</span><a href="#40349834">root</a><span>|</span><a href="#40351596">parent</a><span>|</span><a href="#40350744">next</a><span>|</span><label class="collapse" for="c-40352526">[-]</label><label class="expand" for="c-40352526">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I have come to believe that the best bang for the buck on scientific output can be found elsewhere.<p>The best bang for the buck is never at the very top. The top is just for the biggest bang.</div><br/></div></div></div></div></div></div></div></div><div id="40350744" class="c"><input type="checkbox" id="c-40350744" checked=""/><div class="controls bullet"><span class="by">CaliforniaKarl</span><span>|</span><a href="#40349834">root</a><span>|</span><a href="#40350606">parent</a><span>|</span><a href="#40350663">prev</a><span>|</span><a href="#40350490">next</a><span>|</span><label class="collapse" for="c-40350744">[-]</label><label class="expand" for="c-40350744">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I feel that supercomputers are good at one thing: if you need to make your country&#x27;s flagship submarine about 10% faster&#x2F;quieter than the competition.<p>Or doing Numerical Weather Prediction. :-)<p>But seriously, as a cluster sysadmin, the “128 jobs, followed by post-processing” is great for me, because it lets those separate jobs be scheduled as soon as resources are available.<p>&gt; expensive resources with little support<p>Unfortunately, there isn’t as much funding available in places for user training and consultation.  Good writing &amp; education is a skill, and folks aren’t always interested in a job that has is term-limited, or whose future is otherwise unclear.</div><br/></div></div></div></div><div id="40350490" class="c"><input type="checkbox" id="c-40350490" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40350606">prev</a><span>|</span><a href="#40349959">next</a><span>|</span><label class="collapse" for="c-40350490">[-]</label><label class="expand" for="c-40350490">[1 more]</label></div><br/><div class="children"><div class="content">I have a project on Frontier. Generally these systems (including Frontier) use slurm[0] for scheduling and workload management.<p>The OLCF Frontier user guide[1] has some information on scheduling and Frontier specific quirks (very minor).<p>Current status of jobs on Frontier:<p>[kkielhofner@login11.frontier ~]$ squeue -h -t running -r | wc -l<p>137<p>[kkielhofner@login11.frontier ~]$ squeue -h -t pending -r | wc -l<p>1016<p>The running jobs are relatively low because there are some massive jobs using a significant number of nodes ATM.<p>[0] - <a href="https:&#x2F;&#x2F;slurm.schedmd.com&#x2F;documentation.html" rel="nofollow">https:&#x2F;&#x2F;slurm.schedmd.com&#x2F;documentation.html</a><p>[1] - <a href="https:&#x2F;&#x2F;docs.olcf.ornl.gov&#x2F;systems&#x2F;frontier_user_guide.html" rel="nofollow">https:&#x2F;&#x2F;docs.olcf.ornl.gov&#x2F;systems&#x2F;frontier_user_guide.html</a><p>EDIT: I give up on HN code formatting</div><br/></div></div><div id="40349959" class="c"><input type="checkbox" id="c-40349959" checked=""/><div class="controls bullet"><span class="by">sseagull</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40350490">prev</a><span>|</span><a href="#40350199">next</a><span>|</span><label class="collapse" for="c-40349959">[-]</label><label class="expand" for="c-40349959">[1 more]</label></div><br/><div class="children"><div class="content">You are generally correct, however there are workloads that do use larger portions of a supercomputer that wouldn&#x27;t be feasible on smaller systems.<p>Also, I guess I&#x27;m not sure what you mean by &quot;smaller systems that focus more on power&#x2F;cost&#x2F;space&quot;. A proper queueing system generally efficiently allocates the resources of a large supercomputer to smaller tasks, while also making larger tasks possible in the first place. And I imagine there&#x27;s somewhat an efficiency of scale in a large installation like this.<p>There are, of course, many many smaller supercomputers, such as at most medium to large universities. But even those often have 10-50k cores or so.<p>(In general, efficiency is a consideration when building&#x2F;running, but not of using. Scientists want the most computational power they can get, power usage be damned :) )<p>edit: A related topic is capacity vs. capability: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Supercomputer#Capability_versus_capacity" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Supercomputer#Capability_versu...</a></div><br/></div></div><div id="40350199" class="c"><input type="checkbox" id="c-40350199" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40349959">prev</a><span>|</span><a href="#40350206">next</a><span>|</span><label class="collapse" for="c-40350199">[-]</label><label class="expand" for="c-40350199">[1 more]</label></div><br/><div class="children"><div class="content">&gt; my understanding correct<p>Yes<p>&gt; why is it important to build supercomputers with more and more compute<p>A mix of<p>- research in distributed systems (there are plenty of open questions in Concurrency, Parallelization, Computer Architecture, etc)<p>- a way to maintain an ecosystem of large vendors (Intel, AMD, Nvidia and plenty of smaller vendors all get a piece of the pie to subsidize R&amp;D)<p>- some problems are EXTREMELY computationally and financially expensive, so they require large On-Prem compute capabilities (eg. Protein folding, machine learning when I was in undergrad [DGX-100s were subsidized by Aurora], etc)<p>- some problems are extremely sensitive for national security reasons and it&#x27;s best to keep all personnel in a single region (eg. Nuclear simulations, turbine simulations, some niche ML work, etc)<p>In reality you need to do both, and planners know this fact, and have known this fact for decades</div><br/></div></div><div id="40350206" class="c"><input type="checkbox" id="c-40350206" checked=""/><div class="controls bullet"><span class="by">trueismywork</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40350199">prev</a><span>|</span><a href="#40350037">next</a><span>|</span><label class="collapse" for="c-40350206">[-]</label><label class="expand" for="c-40350206">[1 more]</label></div><br/><div class="children"><div class="content">Bigger systems when utilized at 100% are more efficient than multiple smaller systems when utilized at 100%, in terms of engineering work, software, etc.<p>But also, bigger systems have more opportunities to achieve higher utilization than smaller systems due to the dynamics of bin packing problem.</div><br/></div></div><div id="40350037" class="c"><input type="checkbox" id="c-40350037" checked=""/><div class="controls bullet"><span class="by">tkuraku</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40350206">prev</a><span>|</span><a href="#40350214">next</a><span>|</span><label class="collapse" for="c-40350037">[-]</label><label class="expand" for="c-40350037">[2 more]</label></div><br/><div class="children"><div class="content">While in general there can be many smaller workloads running in parallel. However, periodically the whole supercomputer can be reserved for a &quot;hero&quot; run.</div><br/></div></div><div id="40350214" class="c"><input type="checkbox" id="c-40350214" checked=""/><div class="controls bullet"><span class="by">prpl</span><span>|</span><a href="#40349834">parent</a><span>|</span><a href="#40350037">prev</a><span>|</span><a href="#40349623">next</a><span>|</span><label class="collapse" for="c-40350214">[-]</label><label class="expand" for="c-40350214">[1 more]</label></div><br/><div class="children"><div class="content">there’s also Bell Prize submissions, which is the only time some machines get completely reserved</div><br/></div></div></div></div><div id="40349623" class="c"><input type="checkbox" id="c-40349623" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40349834">prev</a><span>|</span><a href="#40349574">next</a><span>|</span><label class="collapse" for="c-40349623">[-]</label><label class="expand" for="c-40349623">[25 more]</label></div><br/><div class="children"><div class="content">Off topic but &quot;Break the ___ barrier&quot; has got to be my least favorite expression that PR people love. It&#x27;s a &quot;tell&quot; that an article was written as meaningless pop science fluff instead of anything serious. The sound barrier is a real physical phenomenon. This is not. There&#x27;s no barrier! Nothing was broken!</div><br/><div id="40350116" class="c"><input type="checkbox" id="c-40350116" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">parent</a><span>|</span><a href="#40349574">next</a><span>|</span><label class="collapse" for="c-40350116">[-]</label><label class="expand" for="c-40350116">[24 more]</label></div><br/><div class="children"><div class="content">The Exascale barrier is an actual barrier in HPC&#x2F;Distributed Systems.<p>It took 15-20 years to reach this point [0]<p>A lot of innovations in the GPU and distributed ML space were subsidized by this research.<p>Concurrent and Parallel Computing are VERY hard problems.<p>[0] - <a href="http:&#x2F;&#x2F;helper.ipam.ucla.edu&#x2F;publications&#x2F;nmetut&#x2F;nmetut_19423.pdf" rel="nofollow">http:&#x2F;&#x2F;helper.ipam.ucla.edu&#x2F;publications&#x2F;nmetut&#x2F;nmetut_19423...</a></div><br/><div id="40350193" class="c"><input type="checkbox" id="c-40350193" checked=""/><div class="controls bullet"><span class="by">dghlsakjg</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350116">parent</a><span>|</span><a href="#40349574">next</a><span>|</span><label class="collapse" for="c-40350193">[-]</label><label class="expand" for="c-40350193">[23 more]</label></div><br/><div class="children"><div class="content">The comment isn&#x27;t saying that the benchmark isn&#x27;t useful. They are saying that there is no &#x27;barrier&#x27; to be broken.<p>The sound barrier was relevant because there was a significant physical effects to overcome specifically when going trans-sonic. It wasn&#x27;t a question of just adding more powerful engines to existing aircraft. They didn&#x27;t choose the sound barrier because its a nice number, it was a big deal because all sorts of things behaved outside of their understanding of aerodynamics at that point. People died in the pursuit of understanding the sound barrier.<p>The &#x27;exascale barrier&#x27;, afaict, is just another number chosen specifically because it is a round(ish) number. It didn&#x27;t turn computer scientists into smoking holes in the desert when it went wrong. This is an incremental improvement in an incredible field, but not a world changing watershed moment.</div><br/><div id="40350247" class="c"><input type="checkbox" id="c-40350247" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350193">parent</a><span>|</span><a href="#40350232">next</a><span>|</span><label class="collapse" for="c-40350247">[-]</label><label class="expand" for="c-40350247">[18 more]</label></div><br/><div class="children"><div class="content">When Exascale was defined as a barrier in the mid-late 2000s, a lot of computing technologies and techniques that are taken for granted today did not exist outside of the lab.<p>For example, FPGAs were considered as much more viable for sparse matrix computation instead of GPUs, BLAS implementations were not as robust yet, parallel programming APIs like CUDA and Vulkan were in their infancy, etc.<p>Just because you didn&#x27;t do well in your systems classes or you think spinning up an EC2 instance on AWS is &quot;easy&quot; doesn&#x27;t mean it&#x27;s an easy problem.<p>That&#x27;s like saying Einstein or Planck are dummies because AP Physics E&amp;M students can handle basic relatively and quantum theory.</div><br/><div id="40350651" class="c"><input type="checkbox" id="c-40350651" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350247">parent</a><span>|</span><a href="#40350733">next</a><span>|</span><label class="collapse" for="c-40350651">[-]</label><label class="expand" for="c-40350651">[6 more]</label></div><br/><div class="children"><div class="content">Exascale was an arbitrary target- it didn&#x27;t unlock any magical capabilities.
In fact the supercomputer folks are now saying the real barrier that will unlock their science is &quot;zettaflops&quot; (just saw a post from a luminary about it).<p>(also please don&#x27;t be rude or condescending,  it detracts from your argument)</div><br/><div id="40350709" class="c"><input type="checkbox" id="c-40350709" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350651">parent</a><span>|</span><a href="#40350733">next</a><span>|</span><label class="collapse" for="c-40350709">[-]</label><label class="expand" for="c-40350709">[5 more]</label></div><br/><div class="children"><div class="content">Exascale is the primary target decided among everyone in the HPC community in the mid-late 2000s because FLOPS (floating point operations per second) is the unit used to benchmark, as there are various different variables like compiler, architecture, etc are very difficult to account for.<p>It&#x27;s functionally the same as arguing that GB or TB are arbitrary units to represent storage.</div><br/><div id="40350784" class="c"><input type="checkbox" id="c-40350784" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350709">parent</a><span>|</span><a href="#40350733">next</a><span>|</span><label class="collapse" for="c-40350784">[-]</label><label class="expand" for="c-40350784">[4 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s correct: GB and TB are arbitrary units. We use them because historically, successive multiples of 1000 have been used to represent significant changes in size, mass, and velocity.<p>I used to work with&#x2F;for NERSC and was around when they announced exascale as a target, and now they want to target zettascale.  There is no magic threshold where science simulations suddenly work better as you scale up.  It&#x27;s mainly about setting goals that are 10-15 years away to stimulate spending and research.</div><br/><div id="40350838" class="c"><input type="checkbox" id="c-40350838" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350784">parent</a><span>|</span><a href="#40350733">next</a><span>|</span><label class="collapse" for="c-40350838">[-]</label><label class="expand" for="c-40350838">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I used to work with&#x2F;for NERSC and was around when they announced exascale as a target<p>We most likely crossed paths. How close were you to faculty in AMPLab?<p>&gt; It&#x27;s mainly about setting goals that are 10-15 years away to stimulate spending and research.<p>And that&#x27;s not a barrier to you?<p>That feels very condescending about applied research or the amount of effort put into the entire Distributed Systems field.</div><br/><div id="40352351" class="c"><input type="checkbox" id="c-40352351" checked=""/><div class="controls bullet"><span class="by">kortilla</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350838">parent</a><span>|</span><a href="#40350917">next</a><span>|</span><label class="collapse" for="c-40352351">[-]</label><label class="expand" for="c-40352351">[1 more]</label></div><br/><div class="children"><div class="content">It’s not a barrier because there is nothing qualitatively different at 999 vs 1000. It’s just a goal.<p>This is not condescending to the field at all. Crossing an arbitrary goal that is very difficult to get to is still impressive. Just stop using the “breaking the barrier” phrase.</div><br/></div></div><div id="40350917" class="c"><input type="checkbox" id="c-40350917" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350838">parent</a><span>|</span><a href="#40352351">prev</a><span>|</span><a href="#40350733">next</a><span>|</span><label class="collapse" for="c-40350917">[-]</label><label class="expand" for="c-40350917">[1 more]</label></div><br/><div class="children"><div class="content">I am being condescending about the effort put into the distributed systems field- very specifically, about classical supercomputers.<p>How close was I to faculty in AMPLab?  Pretty close; I attended a retreat one year, helped steer funding their way, tried to hire Matei into Google Research, and have chatted with Patterson extensively around the time he wrote this (<a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2011&#x2F;12&#x2F;06&#x2F;science&#x2F;david-patterson-enlist-computer-scientists-in-cancer-fight.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2011&#x2F;12&#x2F;06&#x2F;science&#x2F;david-patterson-e...</a>) then later when he worked on TPUs at Google.<p>(I&#x27;m not a stellar researcher or anything, really more of a functionary, but the one thing I do have is an absolutely realistic understanding of how academic and industrial HPC works)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40350733" class="c"><input type="checkbox" id="c-40350733" checked=""/><div class="controls bullet"><span class="by">marshray</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350247">parent</a><span>|</span><a href="#40350651">prev</a><span>|</span><a href="#40350316">next</a><span>|</span><label class="collapse" for="c-40350733">[-]</label><label class="expand" for="c-40350733">[6 more]</label></div><br/><div class="children"><div class="content">What specifically about the system changes as you cross from &quot;almost exascale&quot; to &quot;definitely exascale&quot; to justify calling it a &#x27;barrier&#x27;?</div><br/><div id="40350770" class="c"><input type="checkbox" id="c-40350770" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350733">parent</a><span>|</span><a href="#40350316">next</a><span>|</span><label class="collapse" for="c-40350770">[-]</label><label class="expand" for="c-40350770">[5 more]</label></div><br/><div class="children"><div class="content">The same reason we choose to define a Gigabyte as 10^9 or use the Richter scale to measure earthquakes.<p>We need a benchmark to delineate between large magnitudes.<p>Furthermore, there are very real engineering problems that had to be solved to even reach this point.<p>A lot of noobs take GPUs, Scipy, BLAS, CUDA, etc for granted when in reality all of these were subsidized by HPC research.<p>The DoE&#x27;s Exascale project was one of the largest buyers for compute for much of the 21st century and helped subsidize Nvidia, Intel, AMD, and other vendors when they were at their worst financially.</div><br/><div id="40350825" class="c"><input type="checkbox" id="c-40350825" checked=""/><div class="controls bullet"><span class="by">marshray</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350770">parent</a><span>|</span><a href="#40350316">next</a><span>|</span><label class="collapse" for="c-40350825">[-]</label><label class="expand" for="c-40350825">[4 more]</label></div><br/><div class="children"><div class="content">Did you even read the question?</div><br/><div id="40350850" class="c"><input type="checkbox" id="c-40350850" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350825">parent</a><span>|</span><a href="#40350849">next</a><span>|</span><label class="collapse" for="c-40350850">[-]</label><label class="expand" for="c-40350850">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re wasting your time by engaging here.</div><br/><div id="40350867" class="c"><input type="checkbox" id="c-40350867" checked=""/><div class="controls bullet"><span class="by">marshray</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350850">parent</a><span>|</span><a href="#40350849">next</a><span>|</span><label class="collapse" for="c-40350867">[-]</label><label class="expand" for="c-40350867">[1 more]</label></div><br/><div class="children"><div class="content">Indeed.<p>I rarely do these days.</div><br/></div></div></div></div><div id="40350849" class="c"><input type="checkbox" id="c-40350849" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350825">parent</a><span>|</span><a href="#40350850">prev</a><span>|</span><a href="#40350316">next</a><span>|</span><label class="collapse" for="c-40350849">[-]</label><label class="expand" for="c-40350849">[1 more]</label></div><br/><div class="children"><div class="content">And I answered - building the logistics and ecosystem</div><br/></div></div></div></div></div></div></div></div><div id="40350316" class="c"><input type="checkbox" id="c-40350316" checked=""/><div class="controls bullet"><span class="by">dghlsakjg</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350247">parent</a><span>|</span><a href="#40350733">prev</a><span>|</span><a href="#40350232">next</a><span>|</span><label class="collapse" for="c-40350316">[-]</label><label class="expand" for="c-40350316">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Just because you didn&#x27;t do well in your systems classes or you think spinning up an EC2 instance on AWS is &quot;easy&quot; doesn&#x27;t mean it&#x27;s an easy problem.<p>Please leave personal attacks out of this, it is not in the spirit of HN, or in helping to see people&#x27;s perspectives.<p>I&#x27;m not saying its not a hard problem, not at all. I respect the hell out of the work that has been done here.<p>I&#x27;m saying that the sound barrier is called a barrier for a very good reason. Aerodynamics on one side of the sound barrier are different than aerodynamics on the other. It is a different game entirely. That is why it is considered a barrier. A plane that has superb subsonic aerodynamics will not perform well on the other side of that barrier<p>Exascale computers on the other hand, while truly amazing, are not operating differently by hitting 10^18 FLOPS. If you&#x27;re computer does 10^18 -20 FLOPS it is not operating in a fundamentally different set of rules than one running above the exaflop benchmark.<p>I never said that the achievement wasn&#x27;t laudable. I argued that there is no barrier there.<p>If I&#x27;m wrong I would appreciate you explaining why doing things at 10^18 FLOPS is fundamentally different than computing just below that benchmark.</div><br/><div id="40350336" class="c"><input type="checkbox" id="c-40350336" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350316">parent</a><span>|</span><a href="#40350232">next</a><span>|</span><label class="collapse" for="c-40350336">[-]</label><label class="expand" for="c-40350336">[4 more]</label></div><br/><div class="children"><div class="content">Because the unit used to measure IO is FLOPS [0], and most societies have settled on base-10 as their numerical system of choice for thousands of years. Furthermore, it is not possible to predict the exact number of cycles you&#x27;ll need, since that depends on the architecture, compiler and many other factors. This is why FLOPS are used as the unit of choice.<p>Each jump in flops by a magnitude of 10^3 is a significant problem in concurrency, IO, parallelism, storage, and existing compute infrastructure.<p>Managing racks is difficult, managing concurrent workloads is difficult, managing I&#x2F;O and storage is difficult, designing compute infra like FPGAs&#x2F;GPUs&#x2F;CPUs for this is difficult, etc.<p>[0] - <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;FLOPS" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;FLOPS</a></div><br/><div id="40350368" class="c"><input type="checkbox" id="c-40350368" checked=""/><div class="controls bullet"><span class="by">dghlsakjg</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350336">parent</a><span>|</span><a href="#40350232">next</a><span>|</span><label class="collapse" for="c-40350368">[-]</label><label class="expand" for="c-40350368">[3 more]</label></div><br/><div class="children"><div class="content">So its just an arbitrary base-10 number, and not a constraint imposed by physics or some other outside constraint like the sound barrier?<p>That&#x27;s kind of my point, an Exaflop is a benchmark and not a &#x27;barrier&#x27;. The sound barrier wasn&#x27;t a 10^3 change in speed, the difference between a subsonic plane and a supersonic plane is measured in percentages when it comes to speed, e.g. a plane that is happy at .8 Mach for a top speed is going to be designed under a completely different set of rules than one that tops out at 1.2 Mach.<p>Again, I&#x27;m not saying that the accomplishments are insignificant, I&#x27;m just arguing press-release semantics here.</div><br/><div id="40350397" class="c"><input type="checkbox" id="c-40350397" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350368">parent</a><span>|</span><a href="#40350232">next</a><span>|</span><label class="collapse" for="c-40350397">[-]</label><label class="expand" for="c-40350397">[2 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re not being facetious I recommend listening to the 2022 ACM Gordon Bell Award Winner lecture.<p>What you&#x27;re doing is the equivalent of asking why do we use Gigabyte or Terabyte as a metric.<p>Just reaching 10^12 floating point operations per second was not something that was done until 2018.</div><br/><div id="40352369" class="c"><input type="checkbox" id="c-40352369" checked=""/><div class="controls bullet"><span class="by">kortilla</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350397">parent</a><span>|</span><a href="#40350232">next</a><span>|</span><label class="collapse" for="c-40352369">[-]</label><label class="expand" for="c-40352369">[1 more]</label></div><br/><div class="children"><div class="content">No, it’s not equivalent at all. Nobody is asking about the unit of measure.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40350232" class="c"><input type="checkbox" id="c-40350232" checked=""/><div class="controls bullet"><span class="by">trueismywork</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350193">parent</a><span>|</span><a href="#40350247">prev</a><span>|</span><a href="#40351378">next</a><span>|</span><label class="collapse" for="c-40350232">[-]</label><label class="expand" for="c-40350232">[3 more]</label></div><br/><div class="children"><div class="content">It is definitely not incremental. If you watch some talk on Gordon Bell prize for Frontier, you see that the dynamics have changed completely.<p>Data, software stack, I&#x2F;O have suddenly become bottlenecks in multiple places. So yes, it is a watershed moment..</div><br/><div id="40350335" class="c"><input type="checkbox" id="c-40350335" checked=""/><div class="controls bullet"><span class="by">dghlsakjg</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350232">parent</a><span>|</span><a href="#40350730">next</a><span>|</span><label class="collapse" for="c-40350335">[-]</label><label class="expand" for="c-40350335">[1 more]</label></div><br/><div class="children"><div class="content">Can you explain how 10^18 FLOPS is fundamentally different than (10^18 - 20) FLOPS? Do the conventional rules of computing completely change at that exact number?</div><br/></div></div><div id="40350730" class="c"><input type="checkbox" id="c-40350730" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350232">parent</a><span>|</span><a href="#40350335">prev</a><span>|</span><a href="#40351378">next</a><span>|</span><label class="collapse" for="c-40350730">[-]</label><label class="expand" for="c-40350730">[1 more]</label></div><br/><div class="children"><div class="content">But that same point was made at every 3 levels of magnitude improvement in supercomputing history.</div><br/></div></div></div></div><div id="40351378" class="c"><input type="checkbox" id="c-40351378" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#40349623">root</a><span>|</span><a href="#40350193">parent</a><span>|</span><a href="#40350232">prev</a><span>|</span><a href="#40349574">next</a><span>|</span><label class="collapse" for="c-40351378">[-]</label><label class="expand" for="c-40351378">[1 more]</label></div><br/><div class="children"><div class="content">“It didn&#x27;t turn computer scientists into smoking holes in the desert when it went wrong. “<p>Shame, really, HPC could use a little bit of high stakes adventure to make it sexier. (Funny how risking death makes things more attractive ?)<p>There’s something about working with equipment where the line between top performance and a smoking hole is a matter of degree.<p>Also opens up a lot more Netflix production opportunities and I bet code safety would get a bump as well.</div><br/></div></div></div></div></div></div></div></div><div id="40349574" class="c"><input type="checkbox" id="c-40349574" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#40349623">prev</a><span>|</span><a href="#40350406">next</a><span>|</span><label class="collapse" for="c-40349574">[-]</label><label class="expand" for="c-40349574">[3 more]</label></div><br/><div class="children"><div class="content">On target to be somewhat slower than Frontier at double the power consumption.</div><br/><div id="40349814" class="c"><input type="checkbox" id="c-40349814" checked=""/><div class="controls bullet"><span class="by">dralley</span><span>|</span><a href="#40349574">parent</a><span>|</span><a href="#40349783">next</a><span>|</span><label class="collapse" for="c-40349814">[-]</label><label class="expand" for="c-40349814">[1 more]</label></div><br/><div class="children"><div class="content">And very, very late</div><br/></div></div><div id="40349783" class="c"><input type="checkbox" id="c-40349783" checked=""/><div class="controls bullet"><span class="by">edward28</span><span>|</span><a href="#40349574">parent</a><span>|</span><a href="#40349814">prev</a><span>|</span><a href="#40350406">next</a><span>|</span><label class="collapse" for="c-40349783">[-]</label><label class="expand" for="c-40349783">[1 more]</label></div><br/><div class="children"><div class="content">Just use more efficiency cores that aren&#x27;t efficient.</div><br/></div></div></div></div><div id="40350406" class="c"><input type="checkbox" id="c-40350406" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#40349574">prev</a><span>|</span><a href="#40349869">next</a><span>|</span><label class="collapse" for="c-40350406">[-]</label><label class="expand" for="c-40350406">[5 more]</label></div><br/><div class="children"><div class="content">&quot;and is the fastest AI system in the world dedicated to AI for open science&quot;<p>Cool.  Please ask it to sue for peace in several parts of the world, in an open way.  Whilst it is at it, get it to work out how to be realistically carbon neutral.<p>I&#x27;m all in favour of willy waving when you have something to wave but in the end this beast will not favour humanity as a whole.  It will suck up useful resources and spit out 
some sort of profit somewhere for someone else to enjoy.</div><br/><div id="40350436" class="c"><input type="checkbox" id="c-40350436" checked=""/><div class="controls bullet"><span class="by">cmdrk</span><span>|</span><a href="#40350406">parent</a><span>|</span><a href="#40350649">next</a><span>|</span><label class="collapse" for="c-40350436">[-]</label><label class="expand" for="c-40350436">[2 more]</label></div><br/><div class="children"><div class="content">They’re simply latching onto the AI buzzwords for the good press. Leadership class HPCs have been designed around GPUs for over a decade now, it just so happens they can use those GPUs to run the AI models in addition to the QCD or Black hole simulations etc that they’ve been doing for ages.</div><br/><div id="40350514" class="c"><input type="checkbox" id="c-40350514" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40350406">root</a><span>|</span><a href="#40350436">parent</a><span>|</span><a href="#40350649">next</a><span>|</span><label class="collapse" for="c-40350514">[-]</label><label class="expand" for="c-40350514">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s entire DGX and Maxwell product line was subsidized by Aurora&#x27;s precursor, and Nvidia worked very closely with Argonne to solve a number of problems in GPU concurrency.<p>A lot of the foundational models used today were trained on Aurora and its predecessors, as well as tangential research such as containerizarion (eg. In the early 2010s, a joint research project between ANL&#x27;s Computing team, one of the world&#x27;s largest Pharma companies, and Nvidia became one of the largest customers of Docker and sponsored a lot of it&#x27;s development)</div><br/></div></div></div></div><div id="40350649" class="c"><input type="checkbox" id="c-40350649" checked=""/><div class="controls bullet"><span class="by">melling</span><span>|</span><a href="#40350406">parent</a><span>|</span><a href="#40350436">prev</a><span>|</span><a href="#40349869">next</a><span>|</span><label class="collapse" for="c-40350649">[-]</label><label class="expand" for="c-40350649">[2 more]</label></div><br/><div class="children"><div class="content">We are in an age of incessant whining</div><br/></div></div></div></div><div id="40349869" class="c"><input type="checkbox" id="c-40349869" checked=""/><div class="controls bullet"><span class="by">mperham</span><span>|</span><a href="#40350406">prev</a><span>|</span><a href="#40350451">next</a><span>|</span><label class="collapse" for="c-40349869">[-]</label><label class="expand" for="c-40349869">[1 more]</label></div><br/><div class="children"><div class="content">Every single paragraph contains the word &quot;AI&quot;.</div><br/></div></div><div id="40350451" class="c"><input type="checkbox" id="c-40350451" checked=""/><div class="controls bullet"><span class="by">gary_0</span><span>|</span><a href="#40349869">prev</a><span>|</span><a href="#40350227">next</a><span>|</span><label class="collapse" for="c-40350451">[-]</label><label class="expand" for="c-40350451">[1 more]</label></div><br/><div class="children"><div class="content">According to Wikipedia[0] it uses 38.7MW of power, beating Fugaku (29.9MW) to be #1 in the TOP500 for power consumption.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Aurora_(supercomputer)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Aurora_(supercomputer)</a></div><br/></div></div><div id="40350227" class="c"><input type="checkbox" id="c-40350227" checked=""/><div class="controls bullet"><span class="by">porphyra</span><span>|</span><a href="#40350451">prev</a><span>|</span><a href="#40349502">next</a><span>|</span><label class="collapse" for="c-40350227">[-]</label><label class="expand" for="c-40350227">[10 more]</label></div><br/><div class="children"><div class="content">I feel like a lot of the big GPU clusters that companies like Meta are using would score highly on the benchmark, even though Linpack seems to be based on fp64 which the H200 is kinda sucky at. However, there&#x27;s only one GH200-based machine (Alps, with 2688 GH200 nodes) in the top 10 even though I&#x27;m fairly sure it&#x27;s quite a bit smaller than whatever Meta is training Llama 3 on. Is there any reason why we don&#x27;t see those show up in TOP500?</div><br/><div id="40350758" class="c"><input type="checkbox" id="c-40350758" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40350227">parent</a><span>|</span><a href="#40350586">next</a><span>|</span><label class="collapse" for="c-40350758">[-]</label><label class="expand" for="c-40350758">[1 more]</label></div><br/><div class="children"><div class="content">The machines that the large ML companies are using are typically not going to run TOP500 competitively.  They tend to skip the MPI stack, and weren&#x27;t optimized for LINPACK performance.  When I worked on TPUs there was no interest in attempting to benchmark them using supercomputer workloads (probably a good thing).</div><br/></div></div><div id="40350586" class="c"><input type="checkbox" id="c-40350586" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40350227">parent</a><span>|</span><a href="#40350758">prev</a><span>|</span><a href="#40350238">next</a><span>|</span><label class="collapse" for="c-40350586">[-]</label><label class="expand" for="c-40350586">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I feel like a lot of the big GPU clusters that companies like Meta are using would score highly<p>Some might, but most of the work done on GPU compute was subsidized by DoE Exascale projects like Aurora, like my anecdote about the DGX product line above.</div><br/></div></div><div id="40350238" class="c"><input type="checkbox" id="c-40350238" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#40350227">parent</a><span>|</span><a href="#40350586">prev</a><span>|</span><a href="#40350634">next</a><span>|</span><label class="collapse" for="c-40350238">[-]</label><label class="expand" for="c-40350238">[6 more]</label></div><br/><div class="children"><div class="content">They are busy running training&#x2F;inference on them and haven&#x27;t benchmarked them?</div><br/><div id="40350246" class="c"><input type="checkbox" id="c-40350246" checked=""/><div class="controls bullet"><span class="by">porphyra</span><span>|</span><a href="#40350227">root</a><span>|</span><a href="#40350238">parent</a><span>|</span><a href="#40350634">next</a><span>|</span><label class="collapse" for="c-40350246">[-]</label><label class="expand" for="c-40350246">[5 more]</label></div><br/><div class="children"><div class="content">So these benchmark lists like TOP500 are just fundamentally flawed because the serious players are too busy doing actual work than to participate?</div><br/><div id="40351484" class="c"><input type="checkbox" id="c-40351484" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#40350227">root</a><span>|</span><a href="#40350246">parent</a><span>|</span><a href="#40350404">next</a><span>|</span><label class="collapse" for="c-40351484">[-]</label><label class="expand" for="c-40351484">[1 more]</label></div><br/><div class="children"><div class="content">TOP500 and other similar HPC benchmark lists are only for FP64 computations.<p>While both NVIDIA and AMD design their top GPU model for both FP64 and AI&#x2F;ML workloads, to save on the design cost, you can do AI training using GPUs that have only high AI performance (like RTX 4090 or its workstation counterpart, RTX 6000), without implementing FP64 operations at all (the FP64 performance of RTX 4090 is negligible, being worse than of any decent cheap CPU, it is provided only for compatibility in testing).</div><br/></div></div><div id="40350404" class="c"><input type="checkbox" id="c-40350404" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#40350227">root</a><span>|</span><a href="#40350246">parent</a><span>|</span><a href="#40351484">prev</a><span>|</span><a href="#40350393">next</a><span>|</span><label class="collapse" for="c-40350404">[-]</label><label class="expand" for="c-40350404">[1 more]</label></div><br/><div class="children"><div class="content">It is a good advertisement for interconnect vendors.</div><br/></div></div><div id="40350393" class="c"><input type="checkbox" id="c-40350393" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#40350227">root</a><span>|</span><a href="#40350246">parent</a><span>|</span><a href="#40350404">prev</a><span>|</span><a href="#40350634">next</a><span>|</span><label class="collapse" for="c-40350393">[-]</label><label class="expand" for="c-40350393">[2 more]</label></div><br/><div class="children"><div class="content">I mean there is no rule if you buy a supercomputer that you have to benchmark it and submit the results. This said, in the days before AI the number and type of players that had this amount of compute were also commonly the types to submit their scores to said benchmark lists.</div><br/><div id="40350791" class="c"><input type="checkbox" id="c-40350791" checked=""/><div class="controls bullet"><span class="by">CaliforniaKarl</span><span>|</span><a href="#40350227">root</a><span>|</span><a href="#40350393">parent</a><span>|</span><a href="#40350634">next</a><span>|</span><label class="collapse" for="c-40350791">[-]</label><label class="expand" for="c-40350791">[1 more]</label></div><br/><div class="children"><div class="content">Most players that had this amount of compute also tend to pay less than the equivalent corporate wage, and so besides being a good way to stress-test your cluster (prior to general availability), it gives you the positive feeling of “I helped make this, I help run this.”</div><br/></div></div></div></div></div></div></div></div><div id="40350634" class="c"><input type="checkbox" id="c-40350634" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40350227">parent</a><span>|</span><a href="#40350238">prev</a><span>|</span><a href="#40349502">next</a><span>|</span><label class="collapse" for="c-40350634">[-]</label><label class="expand" for="c-40350634">[1 more]</label></div><br/><div class="children"><div class="content">A single 8xH100 node hits 15.6 fp16 petaFlops</div><br/></div></div></div></div><div id="40349502" class="c"><input type="checkbox" id="c-40349502" checked=""/><div class="controls bullet"><span class="by">Gerlo</span><span>|</span><a href="#40350227">prev</a><span>|</span><label class="collapse" for="c-40349502">[-]</label><label class="expand" for="c-40349502">[5 more]</label></div><br/><div class="children"><div class="content">Aurora was supposed to go into production years ago and be the first exascale supercomputer. Intel is dog shit, so this kept getting delayed. Now they&#x27;re going for the two exaflop mark. It&#x27;s pathetic that Aurora is only now benchmarking over an exaflop, and it&#x27;s even more pathetic than this is apparently newsworthy.</div><br/><div id="40349590" class="c"><input type="checkbox" id="c-40349590" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#40349502">parent</a><span>|</span><a href="#40350176">next</a><span>|</span><label class="collapse" for="c-40349590">[-]</label><label class="expand" for="c-40349590">[2 more]</label></div><br/><div class="children"><div class="content">The link is Intel&#x27;s own website. Has excellent quotes on it like:<p>&gt; Why It Matters: Designed as an AI-centric system from its inception<p>Announcement was in 2015. I&#x27;m curious whether Argonne are pleased with it.</div><br/><div id="40349648" class="c"><input type="checkbox" id="c-40349648" checked=""/><div class="controls bullet"><span class="by">Gerlo</span><span>|</span><a href="#40349502">root</a><span>|</span><a href="#40349590">parent</a><span>|</span><a href="#40350176">next</a><span>|</span><label class="collapse" for="c-40349648">[-]</label><label class="expand" for="c-40349648">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Announcement was in 2015. I&#x27;m curious whether Argonne are pleased with it.<p>All I&#x27;ll say is that you can probably guess how they feel about it given the context.</div><br/></div></div></div></div><div id="40350176" class="c"><input type="checkbox" id="c-40350176" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#40349502">parent</a><span>|</span><a href="#40349590">prev</a><span>|</span><label class="collapse" for="c-40350176">[-]</label><label class="expand" for="c-40350176">[2 more]</label></div><br/><div class="children"><div class="content">Intel was chosen because the DoE wanted to foster an entire ecosystem of GPU vendors, because having a single vendor is a MASSIVE bottleneck.<p>Intel, AMD, and Nvidia were all vendors on Exascale projects [0]<p>&gt; Intel is dog shit<p>Intel has issues with execution, but their engineers are still top notch. They are the last American company to actually do semiconductor fabrication, and only fell behind TSMC and Samsung in fabrication only 6-7 years ago because they didn&#x27;t choose to invest in EUV lithography instead of other methods.<p>[0] - <a href="http:&#x2F;&#x2F;helper.ipam.ucla.edu&#x2F;publications&#x2F;nmetut&#x2F;nmetut_19423.pdf" rel="nofollow">http:&#x2F;&#x2F;helper.ipam.ucla.edu&#x2F;publications&#x2F;nmetut&#x2F;nmetut_19423...</a></div><br/><div id="40350807" class="c"><input type="checkbox" id="c-40350807" checked=""/><div class="controls bullet"><span class="by">CaliforniaKarl</span><span>|</span><a href="#40349502">root</a><span>|</span><a href="#40350176">parent</a><span>|</span><label class="collapse" for="c-40350807">[-]</label><label class="expand" for="c-40350807">[1 more]</label></div><br/><div class="children"><div class="content">Case in point, Intel’s discrete consumer GPUs: They’re making major gains with each driver release, and I hope to see them seriously competing with nvidia in the gaming market.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>