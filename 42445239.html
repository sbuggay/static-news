<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734512460715" as="style"/><link rel="stylesheet" href="styles.css?v=1734512460715"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/hao-ai-lab/FastVideo">FastVideo: a lightweight framework for accelerating large video diffusion models</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>zhisbug</span> | <span>24 comments</span></div><br/><div><div id="42446758" class="c"><input type="checkbox" id="c-42446758" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#42445240">next</a><span>|</span><label class="collapse" for="c-42446758">[-]</label><label class="expand" for="c-42446758">[8 more]</label></div><br/><div class="children"><div class="content">For anyone that wants to test the original (non-distilled) HunyuanVideo (which is an amazing model) we have 580p version taking under a minute and 720p version taking around 2.5-3 minutes in our playground: <a href="https:&#x2F;&#x2F;fal.ai&#x2F;models&#x2F;fal-ai&#x2F;hunyuan-video" rel="nofollow">https:&#x2F;&#x2F;fal.ai&#x2F;models&#x2F;fal-ai&#x2F;hunyuan-video</a> (it requires github login &amp; and is pay-per-use but new accounts get some free credits).</div><br/><div id="42447771" class="c"><input type="checkbox" id="c-42447771" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42446758">parent</a><span>|</span><a href="#42445240">next</a><span>|</span><label class="collapse" for="c-42447771">[-]</label><label class="expand" for="c-42447771">[7 more]</label></div><br/><div class="children"><div class="content">Open source video models are going to beat closed source. Ecosystem and tools matter.<p>Midjourney has name recognition, but nobody talks about Dall-E anymore. The same will happen to Sora. Flux and Stable Diffusion won images, and Hunyuan and similar will win video.<p>Hunyuan, LTX-1, Mochi-1, and all the other open models from non-leading foundation model companies will eventually leapfrog Sora and Veo. Because you can program against them and run them locally or in your own cloud. You can fine tune them to do whatever you want. You can build audio reactive models, controllable models, interactive art walls, you name it.<p>Sora and Veo just aren&#x27;t interesting. They&#x27;re at one end of the quality spectrum, and open models will quickly close that gap and then some.</div><br/><div id="42447948" class="c"><input type="checkbox" id="c-42447948" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#42446758">root</a><span>|</span><a href="#42447771">parent</a><span>|</span><a href="#42447816">next</a><span>|</span><label class="collapse" for="c-42447948">[-]</label><label class="expand" for="c-42447948">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Open source video models are going to beat closed source. Ecosystem and tools matter.
Midjourney has name recognition, but nobody talks about Dall-E anymore. The same will happen to Sora. Flux and Stable Diffusion won images, and Hunyuan and similar will win video.<p>Neither Flux (except the distilled Flux Schnell model) nor Stable Diffusion has open licensed weights, Stable Diffusion and Flux Dev are weights-available with limited, non-open licenses, Flux Pro is hosted-only.</div><br/><div id="42448046" class="c"><input type="checkbox" id="c-42448046" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42446758">root</a><span>|</span><a href="#42447948">parent</a><span>|</span><a href="#42447816">next</a><span>|</span><label class="collapse" for="c-42448046">[-]</label><label class="expand" for="c-42448046">[3 more]</label></div><br/><div class="children"><div class="content">Just because the OSI doesn&#x27;t like Open RAIL doesn&#x27;t make it not open source unless you&#x27;re strictly talking about the OSD. The OSI can&#x27;t even figure where the boundaries of open models lie - data, training code, weights, etc.<p>The RAIL licenses do have usage restrictions (eg. against harming minors, use in defamation, etc.), but they&#x27;re completely unenforced.<p>Flux Schnell is Apache. LTX-1 is Apache.</div><br/><div id="42448380" class="c"><input type="checkbox" id="c-42448380" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#42446758">root</a><span>|</span><a href="#42448046">parent</a><span>|</span><a href="#42448841">next</a><span>|</span><label class="collapse" for="c-42448380">[-]</label><label class="expand" for="c-42448380">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Just because the OSI doesn’t like Open RAIL doesn’t make it not open source unless you’re strictly talking about the OSD.<p>If you aren’t talking about the OSD, you end up reducing “open source” to a semantically-null buzzword. But, in any case, I intentionally didn’t mention “open source”. The weights are under a use-restrictive license, not an open license, even leaving out the debates over what “source” is. And tha’s just SD1.x, SD2.x, and  SDXL, which have the CreativeML OpenRAIL-M license (SD1.x) or CreativeML OpenRAIL++M licenses (SD2.x&#x2F;SDXL). SD3.x has a far more restrictive license, as does Flux Dev.<p>&gt; Flux Schnell is Apache.<p>Huh. It’s almost like I should have explicitly except Flux Schnell from the other Stable Diffusion and Flux models when I said they didn’t have open licenses.<p>Oh, I did.<p>&gt; LTX-1 is Apache.<p>Yes, it is. LTX-1 is “neither Flux (except the distilled Flux Schnell model) nor Stable Diffusion”. AuraFlow (an image model) is also Apache, and while its behind Flux – Dev or Schnell – or SDXL in current mindshare, it got picked – largely for licensing reasons – as the basis for the next version of Pony Diffusion, a popular (largely, though not exclusively, for  NSFW capabilities) community model series whose previous versions were based on SD1.5 and SDXL, which gives it a good chance of becoming a major player.</div><br/></div></div></div></div></div></div><div id="42447816" class="c"><input type="checkbox" id="c-42447816" checked=""/><div class="controls bullet"><span class="by">creato</span><span>|</span><a href="#42446758">root</a><span>|</span><a href="#42447771">parent</a><span>|</span><a href="#42447948">prev</a><span>|</span><a href="#42445240">next</a><span>|</span><label class="collapse" for="c-42447816">[-]</label><label class="expand" for="c-42447816">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what your take on GIMP vs. Photoshop would be?</div><br/><div id="42447867" class="c"><input type="checkbox" id="c-42447867" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42446758">root</a><span>|</span><a href="#42447816">parent</a><span>|</span><a href="#42445240">next</a><span>|</span><label class="collapse" for="c-42447867">[-]</label><label class="expand" for="c-42447867">[1 more]</label></div><br/><div class="children"><div class="content">Nobody is itching to put GIMP into their product, but everyone can think of ways to build upon Llama and Flux and provide new value.</div><br/></div></div></div></div></div></div></div></div><div id="42445240" class="c"><input type="checkbox" id="c-42445240" checked=""/><div class="controls bullet"><span class="by">zhisbug</span><span>|</span><a href="#42446758">prev</a><span>|</span><a href="#42446583">next</a><span>|</span><label class="collapse" for="c-42445240">[-]</label><label class="expand" for="c-42445240">[1 more]</label></div><br/><div class="children"><div class="content">Hugginface model and data link: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;FastVideo" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;FastVideo</a></div><br/></div></div><div id="42446583" class="c"><input type="checkbox" id="c-42446583" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#42445240">prev</a><span>|</span><a href="#42446991">next</a><span>|</span><label class="collapse" for="c-42446583">[-]</label><label class="expand" for="c-42446583">[6 more]</label></div><br/><div class="children"><div class="content">We need videocards with lots of memory. Give me a 4080 with 192gb. I would be happy. We really need AMD to come up with new cards to wake up NVidia and start some fierce competition</div><br/><div id="42446665" class="c"><input type="checkbox" id="c-42446665" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42446583">parent</a><span>|</span><a href="#42446991">next</a><span>|</span><label class="collapse" for="c-42446665">[-]</label><label class="expand" for="c-42446665">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not really feasible to scale GDDR-based designs that big. The 5090 is expected to have 32GB which probably means the workstation variant will have 64GB, but that&#x27;s the limit of the conventional GPU memory architecture for now. HBM is fast and high capacity but prohibitively expensive, and LPDDR is cheap and high capacity but relatively slow, so there&#x27;s no free lunch to be had.</div><br/><div id="42446747" class="c"><input type="checkbox" id="c-42446747" checked=""/><div class="controls bullet"><span class="by">andybak</span><span>|</span><a href="#42446583">root</a><span>|</span><a href="#42446665">parent</a><span>|</span><a href="#42446991">next</a><span>|</span><label class="collapse" for="c-42446747">[-]</label><label class="expand" for="c-42446747">[4 more]</label></div><br/><div class="children"><div class="content">What would it take to have a unified memory architecture to rival Apple&#x27;s ? Is it theoretically possible with PC motherboards and GPUs that sit in card slots of some form?</div><br/><div id="42447878" class="c"><input type="checkbox" id="c-42447878" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#42446583">root</a><span>|</span><a href="#42446747">parent</a><span>|</span><a href="#42446972">next</a><span>|</span><label class="collapse" for="c-42447878">[-]</label><label class="expand" for="c-42447878">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is it theoretically possible with PC motherboards and GPUs that sit in card slots of some form?<p>As far as I&#x27;m aware, not with the speed that unified memory gets. The trace and path lengths alone put hard caps on what can be done in terms of signalling speed. But I&#x27;m not an expert, I&#x27;m recounting what I was told when I asked the same question! Perhaps the state of the art has improved in this space?</div><br/></div></div><div id="42446908" class="c"><input type="checkbox" id="c-42446908" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#42446583">root</a><span>|</span><a href="#42446747">parent</a><span>|</span><a href="#42446972">prev</a><span>|</span><a href="#42446991">next</a><span>|</span><label class="collapse" for="c-42446908">[-]</label><label class="expand" for="c-42446908">[1 more]</label></div><br/><div class="children"><div class="content">NVIDIA Jetson?</div><br/></div></div></div></div></div></div></div></div><div id="42446991" class="c"><input type="checkbox" id="c-42446991" checked=""/><div class="controls bullet"><span class="by">pauloday</span><span>|</span><a href="#42446583">prev</a><span>|</span><a href="#42446468">next</a><span>|</span><label class="collapse" for="c-42446991">[-]</label><label class="expand" for="c-42446991">[3 more]</label></div><br/><div class="children"><div class="content">Someone wrote the following comment then deleted it. I spent 30 minutes on my response and wanted to post it anyway. Apologies if the original comment was deleted by a mod, I hope this is OK to post.<p>---QUOTE---<p>My &quot;test&quot; for video generation turning movie making on its head is when a model can add the missing Tom Bombadil chapters to Peter Jackson&#x27;s LOTR movies.<p>Probably 20 - 30 minutes of HD, aesthetically synced, scripted etc with minimal editing after a detailed prompt and source material.<p>Qualifier - the AI just has to follow the book script, third party tools ok to use for lip syncing and audio :)<p>I said 5 years away last year.<p>Feels like it might be more like 1 - 2 years.<p>What do you think?<p>---END QUOTE---<p>My response:<p>I think we&#x27;re getting into diminishing returns territory with this AI stuff. These video&#x2F;image generators are impressive but they don&#x27;t &quot;understand&quot; physical reality and probably never will without a breakthrough. You can see this in the demo videos, the best looking ones are glorified still images and the worst are whenever something physical happens, like the lemon being picked up or the guy eating cereal. These examples may get better, but I really doubt they&#x27;ll ever look like real unaltered camera footage without adding an understanding of how our physical reality works into the model somehow.<p>For the script generation, Fellowship of the Ring is not a movie script and requires serious interpretation and planning to be converted into one. Especially if you want it to fit into Jackson&#x27;s films at all. If nothing else the dialog and frequency of songs&#x2F;poetry are very different. The current text generators aren&#x27;t really capable of that kind of planning yet, but I wouldn&#x27;t be surprised if there&#x27;s a screenwritten treatment of that chapter floating around on the internet somewhere, or at least bits of one. It has certainly ingested The Fellowship of the Rings, and plenty of screenplays plus the books they were based on. So maybe chatgpt can make a convincing script. I asked the free version and got some dialog that seems fine, but absolutely no scene direction at all. I&#x27;m willing to believe that was either an issue with my prompting or something that can be fixed in 5 years. So at least the script may be possible.<p>As for converting it into an actual piece of film, I don&#x27;t think that&#x27;s currently possible without a breakthrough on planning. There&#x27;s a reason these video demos aren&#x27;t usually very long, it&#x27;s because they aren&#x27;t good at scene changes. People&#x27;s faces change, rooms change shape, etc. Maybe that can be fixed through engineering, but film editing is hard. It&#x27;s not easy to plan and chain together shots in a way that gives a proper sense of physical reality while conveying everything a scene needs to.<p>Take a look at Dan Olsen&#x27;s video analyzing the editing of Suicide Squad[1]. That movie was edited by a trailerhouse and it shows. A big issue is that the scenes and shots don&#x27;t flow together very well - it&#x27;s edited like a bunch of separate shots and scenes rather than a coherent whole. As a result it&#x27;s generally considered one of the worst films big budget ever made. And from my (admittedly limited) understanding&#x2F;playing around with these generators, they aren&#x27;t even remotely close to being able to do the type of planning needed to pull that kind of editing off, much less something on the level of Jackson&#x27;s adaptation. Again I could be wrong but it really seems like another &quot;Attention is All You Need&quot; level breakthrough to get there.<p>So I&#x27;d say no, I don&#x27;t think we&#x27;ll get what you describe, at least not at any level of quality, in 1-2 years. 5 years sounds more realistic but I really believe we&#x27;d need another huge breakthrough to get there, and those are hard to come by. Assuming one will happen in any given time period seems foolish. But a lot of smart people are working on that, so maybe we&#x27;ll get it. But I don&#x27;t think we&#x27;ll even get there in 10 years with just engineering improvements on the current stuff. Scientific progress isn&#x27;t linear.<p>Yours and a lot of other predictions about AI stuff really remind me of how all the futurists in the 50&#x27;s thought we&#x27;d be able to freeze and unfreeze humans in a few short years. They thought that because it&#x27;s actually really easy to do that with hamsters, but it turns out scaling the process up isn&#x27;t so easy (Tom Scott has a good video tangentially related to this[2]). I think a lot of people are standing near the top of the steep part of a sigmoid curve and saying &quot;Wow look how far we&#x27;ve come in just 3 years! The next 3 years are going to be insane!&quot; When in reality we just have a long plateau of minor improvements in front of us. But who knows, maybe that next breakthrough is right around the corner.<p>[1]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mDclQowcE9I" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mDclQowcE9I</a>
[2]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=2tdiKTSdE9Y" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=2tdiKTSdE9Y</a></div><br/><div id="42447303" class="c"><input type="checkbox" id="c-42447303" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#42446991">parent</a><span>|</span><a href="#42447418">next</a><span>|</span><label class="collapse" for="c-42447303">[-]</label><label class="expand" for="c-42447303">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  getting into diminishing returns<p>The same applies to image generation. Asking AI to create something based on a rough idea is straightforward and can yield amazing results at times. However, fine-tuning the details of an image is incredibly challenging without manual intervention—especially for aspects that are intuitive to humans but lack sufficient representation in the AI&#x27;s training data.<p>Honestly, I&#x27;d say even text generation, whether it&#x27;s coding or copywriting—arguably what generative AI excels at—often hits this same limitation.</div><br/></div></div><div id="42447418" class="c"><input type="checkbox" id="c-42447418" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42446991">parent</a><span>|</span><a href="#42447303">prev</a><span>|</span><a href="#42446468">next</a><span>|</span><label class="collapse" for="c-42447418">[-]</label><label class="expand" for="c-42447418">[1 more]</label></div><br/><div class="children"><div class="content">Have you seen Veo 2 just launched by Google?
Its quality and physics understanding appear far ahead of the competition.<p><a href="https:&#x2F;&#x2F;deepmind.google&#x2F;technologies&#x2F;veo&#x2F;veo-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;technologies&#x2F;veo&#x2F;veo-2&#x2F;</a><p>Also, planning might be around the corner with test-time compute applied to video generation.</div><br/></div></div></div></div><div id="42446461" class="c"><input type="checkbox" id="c-42446461" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#42446468">prev</a><span>|</span><a href="#42446965">next</a><span>|</span><label class="collapse" for="c-42446461">[-]</label><label class="expand" for="c-42446461">[3 more]</label></div><br/><div class="children"><div class="content">Does the distillation done here have a large impact on quality compared to the original &quot;slow&quot; models?</div><br/><div id="42447338" class="c"><input type="checkbox" id="c-42447338" checked=""/><div class="controls bullet"><span class="by">fc417fc802</span><span>|</span><a href="#42446461">parent</a><span>|</span><a href="#42446965">next</a><span>|</span><label class="collapse" for="c-42447338">[-]</label><label class="expand" for="c-42447338">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;FastVideo&#x2F;FastHunyuan#evaluation" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;FastVideo&#x2F;FastHunyuan#evaluation</a><p>Seems like the images are sharper and noisier. The originals seem to have more blur.</div><br/><div id="42448337" class="c"><input type="checkbox" id="c-42448337" checked=""/><div class="controls bullet"><span class="by">woodson</span><span>|</span><a href="#42446461">root</a><span>|</span><a href="#42447338">parent</a><span>|</span><a href="#42446965">next</a><span>|</span><label class="collapse" for="c-42448337">[-]</label><label class="expand" for="c-42448337">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, one would use more than 6 steps with the original Hunyuan model, so perhaps that’s why they’re so blurry. But that’s even slower.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>