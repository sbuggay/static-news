<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1722934862187" as="style"/><link rel="stylesheet" href="styles.css?v=1722934862187"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ym2132.github.io/Progressive_GAN">The Path to StyleGan2 – Implementing the Progressive Growing GAN</a> <span class="domain">(<a href="https://ym2132.github.io">ym2132.github.io</a>)</span></div><div class="subtext"><span>Two_hands</span> | <span>9 comments</span></div><br/><div><div id="41166258" class="c"><input type="checkbox" id="c-41166258" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><label class="collapse" for="c-41166258">[-]</label><label class="expand" for="c-41166258">[8 more]</label></div><br/><div class="children"><div class="content">I know GANs aren&#x27;t all the rage now, but if you&#x27;re interested in ML, they should not be overlooked.<p>We still use GANs a lot. They&#x27;re way faster than diffusion models. Good luck getting a diffusion model to perform upscaling and denoising on a real time video call. I&#x27;m sure we&#x27;ll get there, but right now you can do this with a GAN on cheap consumer hardware. You don&#x27;t need a 4080, DLSS was released with the 20 series cards. They are just naturally computationally cheaper, but yeah, they do have trade-offs (though arguable since ML goes through hype phases and everyone jumps ship from one thing to another and few revisit. But when revisits happen, they tend to be competitive. See ResNets Strike Back for even CNNs vs ViTs. But there&#x27;s more nuance here).<p>There is a reason your upscaling model is a GAN. Sure, diffusion can do this too. But why is everyone using ESRGAN? There&#x27;s a reason for this.<p>Also, I think it is important to remember that GAN is really about a technique, not about generating images. You have a model generating things, and another model telling you something is a good output or not. LLM people... does this sound familiar?<p>To the author: I think it is worth pointing to Tero Karras&#x27;s Nivida page. This group defined the status quo of GANs. You&#x27;ll find that the vast majority of GAN research built off of their research. As quite a large portion of are literal forks. Though a fair amount of this is due to the great optimization they did, with custom cuda kernels (this is not the limiting compute factor in diffusion). <a href="https:&#x2F;&#x2F;research.nvidia.com&#x2F;person&#x2F;tero-karras" rel="nofollow">https:&#x2F;&#x2F;research.nvidia.com&#x2F;person&#x2F;tero-karras</a></div><br/><div id="41167193" class="c"><input type="checkbox" id="c-41167193" checked=""/><div class="controls bullet"><span class="by">Two_hands</span><span>|</span><a href="#41166258">parent</a><span>|</span><label class="collapse" for="c-41167193">[-]</label><label class="expand" for="c-41167193">[7 more]</label></div><br/><div class="children"><div class="content">I didn’t know that GANs were still in use, that’s pretty cool.<p>As a technique I think it’s quite stunning, from an ML perspective. Hence why I’ve decided to write these blog posts. The GAN just has something about which makes it riveting to work with.<p>I’ve realised that Tero Karras made major contributions, I can across the PGGAN from the StyleGAN2. What did you mean by your last sentence, what is the limiting compute factor for GANs?</div><br/><div id="41168346" class="c"><input type="checkbox" id="c-41168346" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41166258">root</a><span>|</span><a href="#41167193">parent</a><span>|</span><a href="#41167488">next</a><span>|</span><label class="collapse" for="c-41168346">[-]</label><label class="expand" for="c-41168346">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I didn’t know that GANs were still in use<p>Their still in use literally by every latent diffusion image generator, they typically target the latents of a GAN-trained decoder.<p>Same for audio, most audio models generate some representation that is converted to audio by a GAN-trained codec.</div><br/><div id="41168431" class="c"><input type="checkbox" id="c-41168431" checked=""/><div class="controls bullet"><span class="by">Two_hands</span><span>|</span><a href="#41166258">root</a><span>|</span><a href="#41168346">parent</a><span>|</span><a href="#41167488">next</a><span>|</span><label class="collapse" for="c-41168431">[-]</label><label class="expand" for="c-41168431">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t looked into latent diffusion yet. But what are you saying the output is converted to images&#x2F;audio using GANs?</div><br/><div id="41168881" class="c"><input type="checkbox" id="c-41168881" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41166258">root</a><span>|</span><a href="#41168431">parent</a><span>|</span><a href="#41167488">next</a><span>|</span><label class="collapse" for="c-41168881">[-]</label><label class="expand" for="c-41168881">[1 more]</label></div><br/><div class="children"><div class="content">FYI, when most people say &quot;diffusion&quot; they are referring to &quot;latent diffusion&quot; (which is identical to &quot;stable diffusion&quot;). As for GAN&#x27;s role, it&#x27;s more like what I reference in the other comment. I wouldn&#x27;t call them &quot;part&quot; of every (latent) diffusion model, but I would say they&#x27;re a common part of the pipeline to the production of quality images (so I&#x27;ll not deny &quot;part&quot;).<p>As for audio, the above comment is true. This is typically at (as referenced) the end stage of the model. You&#x27;ll also find Normalizing Flows commonly used in the middle of the model and used so you can have interpretable control over your latent space. NFs are a commonly overlooked architecture, but if you get to learning about Neural ODEs (NODEs), SDEs, Schrodinger Bridges, etc, then you&#x27;ll find these are in the same family of models. If you like math you&#x27;ll likely fall in love with these types of models.</div><br/></div></div></div></div></div></div><div id="41167488" class="c"><input type="checkbox" id="c-41167488" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41166258">root</a><span>|</span><a href="#41167193">parent</a><span>|</span><a href="#41168346">prev</a><span>|</span><label class="collapse" for="c-41167488">[-]</label><label class="expand" for="c-41167488">[3 more]</label></div><br/><div class="children"><div class="content">Oh yeah, they&#x27;re still alive but you won&#x27;t see them getting published as often due to both most people switching to diffusion and the self fulfilling prophesy of considering things dead. But yeah, if you look at any diffusion platform like Automatic1111 you&#x27;ll find that GANs are a popular choice of upscaler[0,1]. So you use them together to try to benefit from each of their advantages.<p>Also, if you look at the top scores for FFHQ at the 256[2] or 1024[3] resolutions, you see GANs winning, and by a good amount. The best diffusion model is #4, and LDM (Stable Diffusion) is #25. Most diffusion research has avoided this dataset due to scale, but this is changing. Probably worth noting that StyleSAN is about a method, not an architecture. Also the #2 on [2] looks to be a smaller lab and they complain about limited compute and spend time arguing about why they think if they scale they&#x27;d perform better. They do have some compelling evidence given their FFHQ success is beating much bigger models. But they don&#x27;t seem to have as much success on LSUN. They also are less successful on 1024, but they again claim limited compute so hard to say. They don&#x27;t appear to be published in a conference, so I guess they are in fact stuck.<p><pre><code>  &gt; What did you mean by your last sentence, what is the limiting compute factor for GANs?
</code></pre>
Sorry, I meant the limiting compute factor for diffusion. Why GANs are faster. I felt it was worth mentioning since I mentioned that Karras wrote custom cuda kernels for StyleGAN, and this does have a significant impact on speed. In the appendix of StyleGAN2 at the end of B under &quot;Performance optimizations&quot; they mention that their kernels result in an improvement of 30% in training time and 20% improvements on memory footprint.<p>But the limiting factor between diffusion models and (typical) GANs is that GANs are typically formulated with just a decoder. On the other hand, Diffusion has a full encoder decoder network. This is even true for Latent Diffusion models (i.e. Stable Diffusion), which specifically was designed to tackle the compute challenges of a standard diffusion model. The backbone is a UNet (almost a VAE + residual connections), which is an autoencoder and decoder (there are ViT based backbones, but these are still in the same parameter ballpark). So it is just a challenging architecture to reduce in size. There are clear benefits for doing so, but when it comes to practical applications you have to consider a wide variety of factors. I mean think about the computational costs of generating a 256x256 image with SD, that&#x27;s a few gigs on your GPU. You need procumer hardware to get 1024 and I can tell you that on a 4080S that images are not instantaneously generated lol. So you&#x27;re not going to use that in a compute constrained environment like gaming. But on the other hand, I can generate 60 imgs&#x2F;s on a 2080Ti with StyleGAN2 (haven&#x27;t checked on my 4080S). There are things like ArtSpew, that start getting closer but the image quality is crap (this is being improved FWIW). But also PGAN is crazy fast...<p>For more specifics I&#x27;m not sure how to accurately explain without getting into the math and a conversation about density estimation. But I don&#x27;t think that is well suited for a HN conversation. This should be enough to point you in the right direction for that though.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui&#x2F;wiki&#x2F;Features#stable-diffusion-upscale">https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui&#x2F;wiki...</a><p>[1] <a href="https:&#x2F;&#x2F;medium.com&#x2F;rendernet&#x2F;using-hires-fix-to-upscale-your-stable-diffusion-images-8d8e2826593e" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;rendernet&#x2F;using-hires-fix-to-upscale-your...</a><p>[2] <a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;image-generation-on-ffhq-256-x-256" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;image-generation-on-ffhq-256...</a><p>[3] <a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;image-generation-on-ffhq-1024-x-1024" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;image-generation-on-ffhq-102...</a></div><br/><div id="41168481" class="c"><input type="checkbox" id="c-41168481" checked=""/><div class="controls bullet"><span class="by">Two_hands</span><span>|</span><a href="#41166258">root</a><span>|</span><a href="#41167488">parent</a><span>|</span><label class="collapse" for="c-41168481">[-]</label><label class="expand" for="c-41168481">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the self fulfilling prophesy of considering things dead<p>This is quite sad, GANs are an amazing piece of tech and it doesn&#x27;t seem like they are finished yet. The rule in ML is that it&#x27;s never over for a method, so maybe someone somewhere will get GANs fashionable again. There&#x27;s many things like this in ML though...<p>On the FFHQ point, are you saying currently GANs are better at benchmarks like FFHQ where the target is realistic looking images? Or better at representing the training data?<p>&gt; Karras wrote custom cuda kernels for StyleGAN<p>I didnt know they wrote custom kernels, perhaps for my StyleGAN post I can try triton and write a custom kernel for the operations. However, I&#x27;ve never looked into this.<p>What does it mean to have a backbone? Does it just mean the underlying architecture used in the method? Also, on the decoder only vs encoder-decoder point, taken that way it&#x27;s very difficult (almost impossible) to have diffusion models have a better efficiency than GANs?<p>Thanks for the detailed comment, you&#x27;ve given me a lot to think about.</div><br/><div id="41168830" class="c"><input type="checkbox" id="c-41168830" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41166258">root</a><span>|</span><a href="#41168481">parent</a><span>|</span><label class="collapse" for="c-41168830">[-]</label><label class="expand" for="c-41168830">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; The rule in ML 
</code></pre>
There&#x27;s definitely attempts to revive things (in the general sense, not just GANs), but most successes appear to come from large labs dedicating equal computational resources to the older models and often by changing names. This can make things more confusing and make things appear to be changing faster, but once you can see this, you&#x27;ll have an easier time keeping up (so being new, watch out for this). I&#x27;ll give some examples that are easier to read[0-2] (i.e. don&#x27;t need expert knowledge to understand the nuances).<p>As an insider (ML researcher), my complaint isn&#x27;t so much about that we have a large proportion of people chasing one specific avenue, it is that we gate keep newer methods. I think you can see a similar effect on HN when new models are proposed. They are trashed due to lack of beating existing models (this is true even beyond ML!). There will always be reasons to critique works and I don&#x27;t want discourage criticism, but I do want to discourage dismissal. It hinders progress, because progress is made in small steps, not leaps and bounds. I think this can get confusing for someone entering the field (I&#x27;m sorry if I&#x27;ve misjudged, I&#x27;m inferring from the comments).<p><pre><code>  &gt; On the FFHQ point
</code></pre>
This is an excellent question that unfortunately I don&#x27;t know the answer to. I think you&#x27;ll find this work helpful[3], it has the largest human study. But despite its name, StyleNAT performed best in specifically FFHQ. What I would say is that there are good arguments to make the diffusion models are better at representing a diversity of images (making them well suited for things like art generation) but theoretically GANs are approximating your full density distribution. There&#x27;s some talks by Goodfellow discussing this but I don&#x27;t recall which ones.<p><pre><code>  &gt; I didnt know they wrote custom kernels
</code></pre>
As you&#x27;ve probably found, the StyleGAN code is not the easiest to read lol. Since you&#x27;re using pytorch, you can find them here[4]. I encourage you to look at these, especially if you&#x27;ve never seen CUDA code before. Because the biggest takeaway will be that you&#x27;ll see how easy it is to add a custom kernel, and given the earlier comment you&#x27;ll see the utility ^__^<p>I&#x27;m not going to discourage you from trying triton, but I&#x27;ll note that pytorch&#x27;s compile goes a long way. Definitely _start_ there (see TensorRT).<p><pre><code>  &gt; What does it mean to have a backbone? Does it just mean the underlying architecture used in the method?
</code></pre>
Exactly! So in the case of a diffusion model it is the UNet (the neural network part, and specifically this part estimates the parameters for the probability distribution. If this doesn&#x27;t make sense now, it will later. If you are struggling to understand diffusion models after spending some time reading the papers, come back to this comment). You&#x27;ll also find the term &quot;backbone&quot; used in application based models such as in Semantic Segmentation, Object Detection, Pose Estimation, and much more. In those cases, these are typically pretrained, so recognize this as a hyper-parameter.<p><pre><code>  &gt; Also, on the decoder only vs encoder-decoder point
</code></pre>
I&#x27;m going to say something frustrating. In short: yes. If we get a but more nuanced: no. If we get really nuanced: yes. I know this isn&#x27;t a great answer, but it can be really difficult to understand. On the surface, yes because you need to encode the variable and your model needs to transform a dimension starting at R^N and ending in R^N. While a (I have to stress, colloquial[5]) GAN transforms a R^M space to R^N where M &lt;&lt; N. With more nuance you can argue it is the backbone. But to be detailed, you&#x27;ll find that there are fundamental factors placing computational bounds on the theoretical performance of these architectures. To get there you&#x27;ll need to carefully study Goodfellow&#x27;s original paper (some follow-ups expanding on the analysis) as well as the &quot;original&quot; diffusion paper by Sohl-Dickstein[6] (quotes because this is debatable, but the claim has reasonable merit), and you should become familiar with Aapo Hyvärinen[7]. The last is by far the hardest part and the confusion is normal. I know quite a few well renowned and intelligent people who struggled (personally I went through the &quot;this is hard&quot;, &quot;this is easy&quot;, &quot;ah fuck, I actually don&#x27;t understand anything&quot; cycle for a bit. But that&#x27;s just a signal that you&#x27;re progressing :).<p><pre><code>  &gt; Thanks for the detailed comment, you&#x27;ve given me a lot to think about.
</code></pre>
Great! I hope this can help provide direction (I read your other post). The best and worth thing about machine learning is that there&#x27;s so much depth. It can both be intimidating and easy to miss. But if you&#x27;re passionate about learning (as it appears) you&#x27;ll find the knowledge gained is highly rewarding, if unfortunately hard to gain.<p>And I apologize for being so verbose. It is a bad habit.<p>[0] Diffusion Models Beat GANs on Image Synthesis (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2105.05233" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2105.05233</a>) The two authors are rockstars. Given your blog post, I think you&#x27;ll enjoy a lot of their work. Which includes diffusion.<p>[1] ResNet strikes back: An improved training procedure in timm (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2110.00476" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2110.00476</a>) Again, all three are great people to follow. You won&#x27;t see many papers by Wightman, but you&#x27;ll see his work with (now) Hugging Face. Notably he&#x27;s one of the most important players in ViTs.<p>[2] A ConvNet for the 2020s (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2201.03545" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2201.03545</a>) and ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.00808" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.00808</a>)<p>[3] Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.04675" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.04675</a>)<p>[4] <a href="https:&#x2F;&#x2F;github.com&#x2F;NVlabs&#x2F;stylegan2-ada-pytorch&#x2F;tree&#x2F;main&#x2F;torch_utils&#x2F;ops">https:&#x2F;&#x2F;github.com&#x2F;NVlabs&#x2F;stylegan2-ada-pytorch&#x2F;tree&#x2F;main&#x2F;to...</a><p>[5] I&#x27;m sorry, I still have difficulties explaining this, especially simply. There&#x27;s a lot of points here. But one is easy to understand and what I mentioned before: GAN is a training method, not an architecture. A bit more nuance will be found by reading this far underappreciated work: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.03263" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.03263</a>. The last point I want to mention is to never forget that &quot;generative&quot; is a general term and these models are good for generating __data__. Images are data, but to think this is the only type of data a GAN (or any model. I literally mean any[3]) can generate is naive. All of this gets harder to explain and I don&#x27;t have the skill to do so in a simple manner and am afraid it will just come off as a rant.<p>[6] Deep Unsupervised Learning using Nonequilibrium Thermodynamics (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1503.03585" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1503.03585</a>)<p>[7] &quot;Score matching&quot; and &quot;Noise-contrastive estimation&quot; will be the most beneficial <a href="https:&#x2F;&#x2F;www.cs.helsinki.fi&#x2F;u&#x2F;ahyvarin&#x2F;papers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cs.helsinki.fi&#x2F;u&#x2F;ahyvarin&#x2F;papers&#x2F;</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>