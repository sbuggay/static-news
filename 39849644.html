<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711789269389" as="style"/><link rel="stylesheet" href="styles.css?v=1711789269389"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://mobiusml.github.io/1bit_blog/">Towards 1-bit Machine Learning Models</a> <span class="domain">(<a href="https://mobiusml.github.io">mobiusml.github.io</a>)</span></div><div class="subtext"><span>homarp</span> | <span>120 comments</span></div><br/><div><div id="39870848" class="c"><input type="checkbox" id="c-39870848" checked=""/><div class="controls bullet"><span class="by">vladf</span><span>|</span><a href="#39867775">next</a><span>|</span><label class="collapse" for="c-39870848">[-]</label><label class="expand" for="c-39870848">[10 more]</label></div><br/><div class="children"><div class="content">Really strong binary results. So strong it was fishy. I hope someone can explain my confusion below.<p>&gt; We compared the performance of the Llama2-7B model in three configurations: FP16 (full precision), HQQ (without fine-tuning), and HQQ+ (with adapter layers) using a group-size of 8.<p>Interesting, what is &quot;group-size of 8&quot;?<p>From their HQQ post (<a href="https:&#x2F;&#x2F;mobiusml.github.io&#x2F;hqq_blog&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mobiusml.github.io&#x2F;hqq_blog&#x2F;</a>), it&#x27;s the block size at which they add scales (presumably 16-bit) and shifts (in that post, it&#x27;s 8-bit).<p>So for every 8 binary weights we have a 16-bit scale and 8-bit shift?<p>&gt; Fine-tuning with Low-Rank Adapters<p>They say they inline the shift into the LoRA but how can you do this, block-wise, without increasing your LoRA rank by num-blocks (they claim to only use 1 additional rank)?<p>Then, the reported 7B sizes, in GB:<p>&gt; 13.5 (fp16) 1.76 (HQQ 1-bit) 1.85 (HQQ+ 1-bit) 2.72 (quip# 2-bit)<p>those numbers would make sense if it was _actually_ 1 bit. But if you include the overhead of 16-bit scales (and why is the shift inlineable into lora? still unexplained) it&#x27;d be more like 3-bit.<p>From their HF page:<p>&gt; This version offloads the meta-data to the CPU, so only the binary weights and the low-rank adapters are stored in the GPU memory.<p>Interesting, so we have to go back to CPU to rescale? Is this how they counted GB? This should have been clearly caveated in the table. I also am amazed they got latency lower than quip if they pingpong to CPU.</div><br/><div id="39871517" class="c"><input type="checkbox" id="c-39871517" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39870848">parent</a><span>|</span><a href="#39867775">next</a><span>|</span><label class="collapse" for="c-39871517">[-]</label><label class="expand" for="c-39871517">[9 more]</label></div><br/><div class="children"><div class="content">When one does quantization, it&#x27;s done in blocks. Bitsandbytes uses a blocksize of 64 I think. W * scale + zero_point is needed for each group size of 8. So you need 2 numbers in fp16 for each 64 group of numbers. For BnB, you get 4.5bit approx since 64*4bit + 16bit + 16bit = 288&#x2F;64 = 4.5. So 4bit is actually 4.5bit.<p>For HQQ 1bit, a group size of 8 needs 2 fp16 numbers (you mentioned 8bit for shift). So you need 8 * 1bit + 16bit + 8bit for each group ie 32bits for each group size of 8. Or 4bits per param.<p>I&#x27;m assuming the scale and zero_point are both moved to 8bit maybe so 8*1bit + 8bit + 8bit = 24bit &#x2F; 8 = 3bits per param?<p>&quot;This version offloads the meta-data to the CPU, so only the binary weights and the low-rank adapters are stored in the GPU memory.&quot;, so the 8+8 scale &#x2F; zero_point moves to the CPU. So GPU memory 1bit, but CPU meta data is the rest - very smart!</div><br/><div id="39871801" class="c"><input type="checkbox" id="c-39871801" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39871517">parent</a><span>|</span><a href="#39871609">next</a><span>|</span><label class="collapse" for="c-39871801">[-]</label><label class="expand" for="c-39871801">[6 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;This version offloads the meta-data to the CPU, so only the binary weights and the low-rank adapters are stored in the GPU memory.&quot;, so the 8+8 scale &#x2F; zero_point moves to the CPU. So GPU memory 1bit, but CPU meta data is the rest - very smart!<p>Doesn&#x27;t it need all the weight metadata for a layer to use that layer?<p>* If yes, then can&#x27;t <i>any</i> algorithm offload x% of its data as a balancing act between speed and RAM?<p>* If no, then what&#x27;s it for and when does it get used?</div><br/><div id="39871940" class="c"><input type="checkbox" id="c-39871940" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39871801">parent</a><span>|</span><a href="#39871609">next</a><span>|</span><label class="collapse" for="c-39871940">[-]</label><label class="expand" for="c-39871940">[5 more]</label></div><br/><div class="children"><div class="content">Oh yes you need all the metadata, but because it&#x27;s 2 numbers the scale and zero_point, I think the movement of singular digits are super fast to the GPU registers - cannot confirm though.<p>It&#x27;s like in cuBLAS you do alpha<i>A</i>B + beta*C, and alpha and beta are both scalars which can be on the CPU, and moved to the GPU in nanaseconds.<p>I&#x27;m unsure though since I haven&#x27;t tested it</div><br/><div id="39872183" class="c"><input type="checkbox" id="c-39872183" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39871940">parent</a><span>|</span><a href="#39871609">next</a><span>|</span><label class="collapse" for="c-39872183">[-]</label><label class="expand" for="c-39872183">[4 more]</label></div><br/><div class="children"><div class="content">It still has to go through the entire memory system.  It&#x27;s hard for me to imagine that transferring a number from the CPU to the GPU is faster than transferring a byte, and if you have 2 CPU-resident numbers per GPU-resident byte that&#x27;s a lot of transferring.</div><br/><div id="39872465" class="c"><input type="checkbox" id="c-39872465" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39872183">parent</a><span>|</span><a href="#39871609">next</a><span>|</span><label class="collapse" for="c-39872465">[-]</label><label class="expand" for="c-39872465">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree - fair point there definitely is a latency transfer overhead. I would suspect one had prefetch it by calling `.to(&quot;cuda&quot;, non_blocking = True)` say 2 layers ahead, so you can in theory hide the movement.<p>I think somewhere the blog did mention HQQ for 1 bit is slower for now, maybe due to the transfer overhead, although I couldn&#x27;t exactly remember where</div><br/><div id="39872576" class="c"><input type="checkbox" id="c-39872576" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39872465">parent</a><span>|</span><a href="#39871609">next</a><span>|</span><label class="collapse" for="c-39872576">[-]</label><label class="expand" for="c-39872576">[2 more]</label></div><br/><div class="children"><div class="content">My point is more that if it&#x27;s that many bytes flowing around on demand, you&#x27;re basically swapping layers in and out of the GPU as you use them (or x% of each layer).<p>Which is fine, and it&#x27;s a valid feature, but you don&#x27;t need to split those bytes into &quot;data&quot; and &quot;metadata&quot; to make that happen.<p>Is there actually something they gain from this particular method of splitting?</div><br/><div id="39872904" class="c"><input type="checkbox" id="c-39872904" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39872576">parent</a><span>|</span><a href="#39871609">next</a><span>|</span><label class="collapse" for="c-39872904">[-]</label><label class="expand" for="c-39872904">[1 more]</label></div><br/><div class="children"><div class="content">I guess it&#x27;s mainly to reduce VRAM usage. Assuming we don&#x27;t do this, then a 7b model with 1bit group size 8 will use 3GB or something of VRAM, whilst a 4bit group size of 64 will use 4GB approx.<p>Assume we have a 100b model - with 4bit, VRAM is around 57GB or so. With 1bit, 43GB VRAM, but by moving the scalars and zero point to RAM, VRAM use is like 15GB or something, at the cost of like 28GB RAM usage.<p>I guess maybe a valid approach is to dynamically select which ones to move to RAM or VRAM, given your VRAM budget. Say you have a 40GB GPU, clearly move more of the scalars to GPU. But if you have a weak GPU, then you need to use more RAM.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39871609" class="c"><input type="checkbox" id="c-39871609" checked=""/><div class="controls bullet"><span class="by">vladf</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39871517">parent</a><span>|</span><a href="#39871801">prev</a><span>|</span><a href="#39867775">next</a><span>|</span><label class="collapse" for="c-39871609">[-]</label><label class="expand" for="c-39871609">[2 more]</label></div><br/><div class="children"><div class="content">Err, you are just restating what I’m saying, without addressing the concerns.<p>1 - is it fair to use ram in two places and report only one of them without any asterisk? (If you think this is fair-oh boy wait till you hear about my 0GB hbm use inference algorithm)<p>2 - i know how subchannel quantization works. Are they hitting those reported latency numbers with per layer cpu pingpong to rescale?</div><br/><div id="39871969" class="c"><input type="checkbox" id="c-39871969" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39870848">root</a><span>|</span><a href="#39871609">parent</a><span>|</span><a href="#39867775">next</a><span>|</span><label class="collapse" for="c-39871969">[-]</label><label class="expand" for="c-39871969">[1 more]</label></div><br/><div class="children"><div class="content">Oh sorry - did not expect to restate what you said whoops - all train of thought!<p>You can see from <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;mobiuslabsgmbh&#x2F;Llama-2-7b-chat-hf_1bitgs8_hqq&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;mobiuslabsgmbh&#x2F;Llama-2-7b-chat-hf_1bi...</a> that the model disk space is 3GB + 100MB of LoRA weights. I also uploaded a 4bit one to <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;llama-2-7b-bnb-4bit&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;unsloth&#x2F;llama-2-7b-bnb-4bit&#x2F;tree&#x2F;main</a> which uses 3.87GB.<p>So because of the offloading trick, the GPU VRAM is less, but in actuality, still 3GB is needed.<p>Unsure on latency sadly</div><br/></div></div></div></div></div></div></div></div><div id="39867775" class="c"><input type="checkbox" id="c-39867775" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39870848">prev</a><span>|</span><a href="#39867259">next</a><span>|</span><label class="collapse" for="c-39867775">[-]</label><label class="expand" for="c-39867775">[28 more]</label></div><br/><div class="children"><div class="content">I believe the future is 1 bit models - for both training and inference.<p>When people make custom silicon for 1 bit models, they&#x27;ll find that it is <i>sooooo</i> much more power and silicon-space efficient to do 1 bit math than 16 bit floating point - like 100x or more.<p>That extra model size will vastly overshadow any worse performance of the models.</div><br/><div id="39870283" class="c"><input type="checkbox" id="c-39870283" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39869403">next</a><span>|</span><label class="collapse" for="c-39870283">[-]</label><label class="expand" for="c-39870283">[5 more]</label></div><br/><div class="children"><div class="content">I believe the future is 4*4 bit look up tables with output latches, with a bit to&#x2F;from each Cartesian neighbor. Clock them like the colors of a chessboard, in 2 phases, and you don&#x27;t have to worry about timing dependencies.<p>All of your code gets converted to a directed acyclic graph(DAG), executing at Ghz rates if you want.<p>Imagine a machine that can output a million parallel GPT-4 streams at 1000 tokens per second each.<p>If the architecture changes it&#x27;s just a different DAG. Unlike with FPGAs and their custom blocks that have to be optimally used, you can compile a DAG almost instantly.</div><br/><div id="39871833" class="c"><input type="checkbox" id="c-39871833" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39870283">parent</a><span>|</span><a href="#39871768">next</a><span>|</span><label class="collapse" for="c-39871833">[-]</label><label class="expand" for="c-39871833">[1 more]</label></div><br/><div class="children"><div class="content">1. If you write FPGA code as a grid of lookup tables then I would expect it to be easy to compile instantly.<p>2. In what way is this &quot;acyclic&quot;?<p>3. Won&#x27;t putting your code <i>into</i> this form be the hard part?  Even if you start with a DAG, 99.99% of them won&#x27;t fit this form without intense restructuring.  So you just moved the hard step over by one.</div><br/></div></div><div id="39871768" class="c"><input type="checkbox" id="c-39871768" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39870283">parent</a><span>|</span><a href="#39871833">prev</a><span>|</span><a href="#39869403">next</a><span>|</span><label class="collapse" for="c-39871768">[-]</label><label class="expand" for="c-39871768">[3 more]</label></div><br/><div class="children"><div class="content">Is this something from a research finding or is it your idea?</div><br/><div id="39872430" class="c"><input type="checkbox" id="c-39872430" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39871768">parent</a><span>|</span><a href="#39869403">next</a><span>|</span><label class="collapse" for="c-39872430">[-]</label><label class="expand" for="c-39872430">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s my idea... I&#x27;ve been writing about it for over a decade. I hope to get a small version of it made via tiny tapeout later this year once I&#x27;m out of the precariat.<p>The idea is that you can decompose any non-cyclic code, for example, a matrix multiply, FFT, etc.. into a directed graph of bitwise operations, then map those operations out into the grid of LUTs. Pipe inputs in one side of the grid, and get the outputs out the other side, and all of the bitwise operations happen in lock step parallelism. (Unlike an FPGA which is asyncronous)<p>If you can decompose one stage of an LLM down into a bitwise graph of computation, you can easily map it into the grid.  If there&#x27;s a bad cell in the grid, you can map around it.<p>Because all of the high speed logic lines are short (to the neighbors) you don&#x27;t have the long lines &#x2F; high capacitance issues driving signals all the way across an FPGA or ASIC, thus you can really crank up the clock rate (and&#x2F;or it&#x27;s very power efficient).<p>It should be trivial, design wise, to scale from a 16x16 grid up through chips that could crank out Petaflops.<p>Here&#x27;s a simulator for the thing, written in Pascal.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;mikewarot&#x2F;Bitgrid">https:&#x2F;&#x2F;github.com&#x2F;mikewarot&#x2F;Bitgrid</a></div><br/><div id="39872736" class="c"><input type="checkbox" id="c-39872736" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39872430">parent</a><span>|</span><a href="#39869403">next</a><span>|</span><label class="collapse" for="c-39872736">[-]</label><label class="expand" for="c-39872736">[1 more]</label></div><br/><div class="children"><div class="content">You can simulate this on a GPU now. This is like running a CA on a GPU with slightly different rules.<p>I think you would also be delighted to know that many of the non-GPU ai accelerators are in a mesh topology.<p>Cerebras, Tenstorrent, Esperanto<p><a href="https:&#x2F;&#x2F;www.esperanto.ai&#x2F;wp-content&#x2F;uploads&#x2F;2021&#x2F;08&#x2F;HC2021.Esperanto.Ditzel.Final_.pdf" rel="nofollow">https:&#x2F;&#x2F;www.esperanto.ai&#x2F;wp-content&#x2F;uploads&#x2F;2021&#x2F;08&#x2F;HC2021.E...</a></div><br/></div></div></div></div></div></div></div></div><div id="39869403" class="c"><input type="checkbox" id="c-39869403" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39870283">prev</a><span>|</span><a href="#39872720">next</a><span>|</span><label class="collapse" for="c-39869403">[-]</label><label class="expand" for="c-39869403">[2 more]</label></div><br/><div class="children"><div class="content">Probably more than 100x for inference. Not only are you drastically reducing the number of bits and replacing float math with integer math, you can do matrix multiplication with only addition (as pointed out in the BitNet b1.58 paper). Additions require a lot less hardware to implement than multiplication. Adding one-bit or two-bit numbers requires barely any hardware at all. A traditional two-bit adder without carry bit is three xor gates and an and gate.</div><br/><div id="39872931" class="c"><input type="checkbox" id="c-39872931" checked=""/><div class="controls bullet"><span class="by">fasa99</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869403">parent</a><span>|</span><a href="#39872720">next</a><span>|</span><label class="collapse" for="c-39872931">[-]</label><label class="expand" for="c-39872931">[1 more]</label></div><br/><div class="children"><div class="content">to me the most exciting thing is that if is training that is speed up on the order of 100x-1000x, a large cluster may be well suited to gradient-descend hyperparameter tuning parameters by LLM training again and again at scale  -- this is the first foot in a door towards an AI that iteratively may improve itself</div><br/></div></div></div></div><div id="39872720" class="c"><input type="checkbox" id="c-39872720" checked=""/><div class="controls bullet"><span class="by">bgnn</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39869403">prev</a><span>|</span><a href="#39869686">next</a><span>|</span><label class="collapse" for="c-39872720">[-]</label><label class="expand" for="c-39872720">[1 more]</label></div><br/><div class="children"><div class="content">This! Maybe just integer, but not floating point. That&#x27;s a ridiculous way to do computation when you don&#x27;t really need the precision.</div><br/></div></div><div id="39869686" class="c"><input type="checkbox" id="c-39869686" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39872720">prev</a><span>|</span><a href="#39869698">next</a><span>|</span><label class="collapse" for="c-39869686">[-]</label><label class="expand" for="c-39869686">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I believe the future is 1 bit models - for both training and inference.<p>1 bit&#x27;s nothin&#x27;. The future is training directly on electronic&#x2F;photonic circuits.</div><br/></div></div><div id="39869698" class="c"><input type="checkbox" id="c-39869698" checked=""/><div class="controls bullet"><span class="by">bionhoward</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39869686">prev</a><span>|</span><a href="#39869052">next</a><span>|</span><label class="collapse" for="c-39869698">[-]</label><label class="expand" for="c-39869698">[4 more]</label></div><br/><div class="children"><div class="content">Isn’t 1bit too low for optimal radix economy (Euler’s number) though?<p>I want to see somebody try “imbalanced quaternary” -,0,+,2</div><br/><div id="39871845" class="c"><input type="checkbox" id="c-39871845" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869698">parent</a><span>|</span><a href="#39869937">next</a><span>|</span><label class="collapse" for="c-39871845">[-]</label><label class="expand" for="c-39871845">[1 more]</label></div><br/><div class="children"><div class="content">I bet the optimal &quot;large&quot; value is bigger than 2.</div><br/></div></div><div id="39869937" class="c"><input type="checkbox" id="c-39869937" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869698">parent</a><span>|</span><a href="#39871845">prev</a><span>|</span><a href="#39870763">next</a><span>|</span><label class="collapse" for="c-39869937">[-]</label><label class="expand" for="c-39869937">[1 more]</label></div><br/><div class="children"><div class="content">Haven&#x27;t heard this argument before. But from the Wikipedia article it seems base 3 has the best asymptomatic radix economy, but isn&#x27;t much better than base 2 and base 2 is seemingly easier to program and optimize.<p>Since this is a new argument I&#x27;ve not heard, would be curious if you had links or some more explanation.</div><br/></div></div><div id="39870763" class="c"><input type="checkbox" id="c-39870763" checked=""/><div class="controls bullet"><span class="by">johnmorrison</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869698">parent</a><span>|</span><a href="#39869937">prev</a><span>|</span><a href="#39869052">next</a><span>|</span><label class="collapse" for="c-39870763">[-]</label><label class="expand" for="c-39870763">[1 more]</label></div><br/><div class="children"><div class="content">people are indeed working on -1,0,1,2 Q2 models, I read something about it the other day but don&#x27;t recall the title.<p>they mentioned decomposition of Q2 into ternary+binary i.e. [[1,2],[-1,0]] -&gt; [[1,1],[0,0]] + [[0,1],[-1,0]]</div><br/></div></div></div></div><div id="39869143" class="c"><input type="checkbox" id="c-39869143" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39869052">prev</a><span>|</span><a href="#39870188">next</a><span>|</span><label class="collapse" for="c-39869143">[-]</label><label class="expand" for="c-39869143">[9 more]</label></div><br/><div class="children"><div class="content">For training how do you get any kind of meaningful derivative with it?</div><br/><div id="39872740" class="c"><input type="checkbox" id="c-39872740" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869143">parent</a><span>|</span><a href="#39869445">next</a><span>|</span><label class="collapse" for="c-39872740">[-]</label><label class="expand" for="c-39872740">[1 more]</label></div><br/><div class="children"><div class="content">Maybe something probabilistic?</div><br/></div></div><div id="39869445" class="c"><input type="checkbox" id="c-39869445" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869143">parent</a><span>|</span><a href="#39872740">prev</a><span>|</span><a href="#39869560">next</a><span>|</span><label class="collapse" for="c-39869445">[-]</label><label class="expand" for="c-39869445">[5 more]</label></div><br/><div class="children"><div class="content">Maybe evolutionary algorithms instead? Hasn&#x27;t proven super useful historically, but maybe at the scale of enormous LLMs it will be?</div><br/><div id="39870726" class="c"><input type="checkbox" id="c-39870726" checked=""/><div class="controls bullet"><span class="by">VHRanger</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869445">parent</a><span>|</span><a href="#39869743">next</a><span>|</span><label class="collapse" for="c-39870726">[-]</label><label class="expand" for="c-39870726">[2 more]</label></div><br/><div class="children"><div class="content">Nope, they&#x27;re orders of magnitude more inefficient because they don&#x27;t leverage gradient descent.<p>Rule of thumb in optimization: real numbers are easy, integers are hard</div><br/><div id="39871776" class="c"><input type="checkbox" id="c-39871776" checked=""/><div class="controls bullet"><span class="by">markisus</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39870726">parent</a><span>|</span><a href="#39869743">next</a><span>|</span><label class="collapse" for="c-39871776">[-]</label><label class="expand" for="c-39871776">[1 more]</label></div><br/><div class="children"><div class="content">This may be the status quo because of the so called &quot;hardware lottery&quot; which has historically been optimized for floating point. I&#x27;m speculating, but if hardware designers were instead only concerned about raw xnor density and throughput, we might end up with chips powerful enough that giant 1-bit nets could be trained purely through evolution.</div><br/></div></div></div></div><div id="39869743" class="c"><input type="checkbox" id="c-39869743" checked=""/><div class="controls bullet"><span class="by">bionhoward</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869445">parent</a><span>|</span><a href="#39870726">prev</a><span>|</span><a href="#39869560">next</a><span>|</span><label class="collapse" for="c-39869743">[-]</label><label class="expand" for="c-39869743">[2 more]</label></div><br/><div class="children"><div class="content">Evolutionary algorithms made you, didn’t they?</div><br/><div id="39870359" class="c"><input type="checkbox" id="c-39870359" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869743">parent</a><span>|</span><a href="#39869560">next</a><span>|</span><label class="collapse" for="c-39870359">[-]</label><label class="expand" for="c-39870359">[1 more]</label></div><br/><div class="children"><div class="content">That does not prove that they can beat gradient decent.</div><br/></div></div></div></div></div></div><div id="39869560" class="c"><input type="checkbox" id="c-39869560" checked=""/><div class="controls bullet"><span class="by">chalst</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869143">parent</a><span>|</span><a href="#39869445">prev</a><span>|</span><a href="#39870188">next</a><span>|</span><label class="collapse" for="c-39869560">[-]</label><label class="expand" for="c-39869560">[2 more]</label></div><br/><div class="children"><div class="content">The OP explicitly excludes training.</div><br/><div id="39870551" class="c"><input type="checkbox" id="c-39870551" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39869560">parent</a><span>|</span><a href="#39870188">next</a><span>|</span><label class="collapse" for="c-39870551">[-]</label><label class="expand" for="c-39870551">[1 more]</label></div><br/><div class="children"><div class="content">The one I replied to said 1-bit for both training and inference.</div><br/></div></div></div></div></div></div><div id="39870188" class="c"><input type="checkbox" id="c-39870188" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39869143">prev</a><span>|</span><a href="#39868435">next</a><span>|</span><label class="collapse" for="c-39870188">[-]</label><label class="expand" for="c-39870188">[2 more]</label></div><br/><div class="children"><div class="content">Doesn’t training need higher precision to avoid getting stuck at local minima, at least with back propagation style learning?<p>Maybe something alternate like evolutionary algorithms could work in a domain like this, but so far those have proven to be less efficient.</div><br/><div id="39870207" class="c"><input type="checkbox" id="c-39870207" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39870188">parent</a><span>|</span><a href="#39868435">next</a><span>|</span><label class="collapse" for="c-39870207">[-]</label><label class="expand" for="c-39870207">[1 more]</label></div><br/><div class="children"><div class="content">A recent paper used a single ternary &quot;trit&quot; ~1.5 bits per parameter for training. <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39535800">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39535800</a> They said it would be difficult to apply this technique to pre-trained models and had to be trained in 1-trit from scratch.</div><br/></div></div></div></div><div id="39868435" class="c"><input type="checkbox" id="c-39868435" checked=""/><div class="controls bullet"><span class="by">JHonaker</span><span>|</span><a href="#39867775">parent</a><span>|</span><a href="#39870188">prev</a><span>|</span><a href="#39867259">next</a><span>|</span><label class="collapse" for="c-39868435">[-]</label><label class="expand" for="c-39868435">[2 more]</label></div><br/><div class="children"><div class="content">&gt; That extra model size will vastly overshadow any worse performance of the models.<p>...What?</div><br/><div id="39868486" class="c"><input type="checkbox" id="c-39868486" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39867775">root</a><span>|</span><a href="#39868435">parent</a><span>|</span><a href="#39867259">next</a><span>|</span><label class="collapse" for="c-39868486">[-]</label><label class="expand" for="c-39868486">[1 more]</label></div><br/><div class="children"><div class="content">I think OP was referring to parameter size. You can make up for quantization by having more parameters.</div><br/></div></div></div></div></div></div><div id="39867259" class="c"><input type="checkbox" id="c-39867259" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#39867775">prev</a><span>|</span><a href="#39866387">next</a><span>|</span><label class="collapse" for="c-39867259">[-]</label><label class="expand" for="c-39867259">[3 more]</label></div><br/><div class="children"><div class="content">It seems the trick here is they first quantize it to 1- or 2-bit, and then they fine-tune the quantization bias parameters (the parameters that dequantize from 1-2 to 16 bit) via LoRA. Then they have specialized kernels to do matrix multiplication at the bit level.<p>Also, the 2-bit model seems much better than the 1-bit model - they use [-1, 0, 1, 2] - I wonder if &#x27;2&#x27; is needed in light of the 1.58b paper (which claims -1 is def. needed).</div><br/><div id="39871510" class="c"><input type="checkbox" id="c-39871510" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#39867259">parent</a><span>|</span><a href="#39869040">next</a><span>|</span><label class="collapse" for="c-39871510">[-]</label><label class="expand" for="c-39871510">[1 more]</label></div><br/><div class="children"><div class="content">Which is itself a little counterintuitive as the arxiv papers they cite say models need to be pretrained from the ground up using 1- or 2-bit (or 1.58bit).  It definitely adds some interesting data points for the open source community who are experimenting in every possible direction.</div><br/></div></div><div id="39869040" class="c"><input type="checkbox" id="c-39869040" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#39867259">parent</a><span>|</span><a href="#39871510">prev</a><span>|</span><a href="#39866387">next</a><span>|</span><label class="collapse" for="c-39869040">[-]</label><label class="expand" for="c-39869040">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, and it kinda makes sense. You quantize, which invariably means you lose some precision, but then you can finetune post-quantization to recover at least some of it. Neat idea.</div><br/></div></div></div></div><div id="39866387" class="c"><input type="checkbox" id="c-39866387" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39867259">prev</a><span>|</span><a href="#39869371">next</a><span>|</span><label class="collapse" for="c-39866387">[-]</label><label class="expand" for="c-39866387">[8 more]</label></div><br/><div class="children"><div class="content">1-bit weights have been a thing since at least 2016:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.06160" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.06160</a></div><br/><div id="39866594" class="c"><input type="checkbox" id="c-39866594" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39866387">parent</a><span>|</span><a href="#39870330">next</a><span>|</span><label class="collapse" for="c-39866594">[-]</label><label class="expand" for="c-39866594">[4 more]</label></div><br/><div class="children"><div class="content">XNOR-Net was the &quot;first&quot; in that generation, as I recall. 
It&#x27;s not a new idea at all though.<p>Check out the dates on these papers -<p><a href="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;286901" rel="nofollow">https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;286901</a> &lt; 1994<p><a href="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;344783" rel="nofollow">https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;344783</a> &lt; 1993<p><a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;BF00337115" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;BF00337115</a> &lt; 1986</div><br/><div id="39867514" class="c"><input type="checkbox" id="c-39867514" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#39866387">root</a><span>|</span><a href="#39866594">parent</a><span>|</span><a href="#39867542">next</a><span>|</span><label class="collapse" for="c-39867514">[-]</label><label class="expand" for="c-39867514">[2 more]</label></div><br/><div class="children"><div class="content">First version of BNN was submitted to ArXiv a month before XNOR-Net: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1602.02830" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1602.02830</a></div><br/><div id="39868357" class="c"><input type="checkbox" id="c-39868357" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39866387">root</a><span>|</span><a href="#39867514">parent</a><span>|</span><a href="#39867542">next</a><span>|</span><label class="collapse" for="c-39868357">[-]</label><label class="expand" for="c-39868357">[1 more]</label></div><br/><div class="children"><div class="content">Oh nice, good catch!</div><br/></div></div></div></div><div id="39867542" class="c"><input type="checkbox" id="c-39867542" checked=""/><div class="controls bullet"><span class="by">thechao</span><span>|</span><a href="#39866387">root</a><span>|</span><a href="#39866594">parent</a><span>|</span><a href="#39867514">prev</a><span>|</span><a href="#39870330">next</a><span>|</span><label class="collapse" for="c-39867542">[-]</label><label class="expand" for="c-39867542">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re just speed-running the NN pendulum, at this point.</div><br/></div></div></div></div><div id="39870330" class="c"><input type="checkbox" id="c-39870330" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#39866387">parent</a><span>|</span><a href="#39866594">prev</a><span>|</span><a href="#39866766">next</a><span>|</span><label class="collapse" for="c-39870330">[-]</label><label class="expand" for="c-39870330">[2 more]</label></div><br/><div class="children"><div class="content">They have been a thing for decades<p><a href="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;363432" rel="nofollow">https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;363432</a></div><br/><div id="39872807" class="c"><input type="checkbox" id="c-39872807" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39866387">root</a><span>|</span><a href="#39870330">parent</a><span>|</span><a href="#39866766">next</a><span>|</span><label class="collapse" for="c-39872807">[-]</label><label class="expand" for="c-39872807">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that paper&#x27;s methods could be applied to LLMs</div><br/></div></div></div></div><div id="39866766" class="c"><input type="checkbox" id="c-39866766" checked=""/><div class="controls bullet"><span class="by">rapatel0</span><span>|</span><a href="#39866387">parent</a><span>|</span><a href="#39870330">prev</a><span>|</span><a href="#39869371">next</a><span>|</span><label class="collapse" for="c-39866766">[-]</label><label class="expand" for="c-39866766">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget Michael<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.00363" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.00363</a> &lt; 2015</div><br/></div></div></div></div><div id="39869371" class="c"><input type="checkbox" id="c-39869371" checked=""/><div class="controls bullet"><span class="by">jstmm</span><span>|</span><a href="#39866387">prev</a><span>|</span><a href="#39866842">next</a><span>|</span><label class="collapse" for="c-39869371">[-]</label><label class="expand" for="c-39869371">[6 more]</label></div><br/><div class="children"><div class="content">In both the &#x27;1-bit Model&#x27; and &#x27;2-bit Model&#x27; tables, the forward time (sec) for Llama2-7B with FP16 (full precision) is 0.1 s, whereas it&#x27;s ~0.231, ~0.257, ~0.353 s respectively for HQQ (1-bit) &#x2F; HQQ+ (1-bit) &#x2F; Quip# (2-bit) meaning the FP16 model has ~3x lower inference time.<p>On the contrary, in the BitNet b1.56 paper [0] the authors report their 7b model has 2.9x reduced inference latency.<p>It&#x27;s not clear to me what&#x27;s happening here. Can someone explain why the 1&#x2F;2bit HQQ&#x2F;HQQ+ models are so much slower than the BitNet b1.56 models?<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2402.17764.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2402.17764.pdf</a></div><br/><div id="39869687" class="c"><input type="checkbox" id="c-39869687" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39869371">parent</a><span>|</span><a href="#39869429">next</a><span>|</span><label class="collapse" for="c-39869687">[-]</label><label class="expand" for="c-39869687">[3 more]</label></div><br/><div class="children"><div class="content">GPU&#x27;s aren&#x27;t really designed for 1 bit math...    They don&#x27;t perform much faster than floating point math.<p>Whereas a custom ASIC or updated design of GPU could give <i>massive</i> speedups with 1 bit math.</div><br/><div id="39869877" class="c"><input type="checkbox" id="c-39869877" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#39869371">root</a><span>|</span><a href="#39869687">parent</a><span>|</span><a href="#39869794">next</a><span>|</span><label class="collapse" for="c-39869877">[-]</label><label class="expand" for="c-39869877">[1 more]</label></div><br/><div class="children"><div class="content">Yes, exactly. Neither GPUs nor CPUs are setup for 1 bit math. Pulling 1 or 2 bits out of a word isn&#x27;t all that straightforward on CPU or GPU - lots of shifting and masking. I wonder how long it&#x27;s going to be before we see custom hardware for bitnets? I suspect we&#x27;ll see it on FPGAs first.</div><br/></div></div><div id="39869794" class="c"><input type="checkbox" id="c-39869794" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39869371">root</a><span>|</span><a href="#39869687">parent</a><span>|</span><a href="#39869877">prev</a><span>|</span><a href="#39869429">next</a><span>|</span><label class="collapse" for="c-39869794">[-]</label><label class="expand" for="c-39869794">[1 more]</label></div><br/><div class="children"><div class="content">For 1 bit math, at least it should be possible to populate every other bit of an integer type, right? Surely one could do <i>better</i> with a dedicated type for this, but at least we could pack 16 single-bit weights into a 32 bit int for addition, right?</div><br/></div></div></div></div><div id="39869429" class="c"><input type="checkbox" id="c-39869429" checked=""/><div class="controls bullet"><span class="by">thatguysaguy</span><span>|</span><a href="#39869371">parent</a><span>|</span><a href="#39869687">prev</a><span>|</span><a href="#39870001">next</a><span>|</span><label class="collapse" for="c-39869429">[-]</label><label class="expand" for="c-39869429">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like these guys didn&#x27;t use custom kernels, but BitNet did.</div><br/></div></div><div id="39870001" class="c"><input type="checkbox" id="c-39870001" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39869371">parent</a><span>|</span><a href="#39869429">prev</a><span>|</span><a href="#39866842">next</a><span>|</span><label class="collapse" for="c-39870001">[-]</label><label class="expand" for="c-39870001">[1 more]</label></div><br/><div class="children"><div class="content">Real world GPU performance is hugely influenced by hand optimization of the CUDA kernels.</div><br/></div></div></div></div><div id="39866842" class="c"><input type="checkbox" id="c-39866842" checked=""/><div class="controls bullet"><span class="by">grungegun</span><span>|</span><a href="#39869371">prev</a><span>|</span><a href="#39868993">next</a><span>|</span><label class="collapse" for="c-39866842">[-]</label><label class="expand" for="c-39866842">[8 more]</label></div><br/><div class="children"><div class="content">Does anyone know if this works on vanilla deep networks? These quantization articles always seem to target LLM&#x27;s which leads me to wonder if there&#x27;s something special about the LLM architecture vs a vanilla deep architecture.</div><br/><div id="39868947" class="c"><input type="checkbox" id="c-39868947" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#39866842">parent</a><span>|</span><a href="#39867235">next</a><span>|</span><label class="collapse" for="c-39868947">[-]</label><label class="expand" for="c-39868947">[4 more]</label></div><br/><div class="children"><div class="content">Transformer LLMs are just a bunch of MLPs (linear layers) where you sometimes multiply&#x2F;softmax the output in a funny way (attention). In other words, they&#x27;re arguably more &quot;vanilla deep net&quot; than most architectures (e.g., conv nets).<p>(There are also positional&#x2F;token embeddings and normalization but those are a tiny minority of the parameters)</div><br/><div id="39870220" class="c"><input type="checkbox" id="c-39870220" checked=""/><div class="controls bullet"><span class="by">grungegun</span><span>|</span><a href="#39866842">root</a><span>|</span><a href="#39868947">parent</a><span>|</span><a href="#39869761">next</a><span>|</span><label class="collapse" for="c-39870220">[-]</label><label class="expand" for="c-39870220">[1 more]</label></div><br/><div class="children"><div class="content">So there&#x27;s no performance gain for quantization enabled by the transformer architecture? It seems very strange that quantization works so well since in most of my experiments, the internal model weights of mlps look random.</div><br/></div></div><div id="39869761" class="c"><input type="checkbox" id="c-39869761" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#39866842">root</a><span>|</span><a href="#39868947">parent</a><span>|</span><a href="#39870220">prev</a><span>|</span><a href="#39867235">next</a><span>|</span><label class="collapse" for="c-39869761">[-]</label><label class="expand" for="c-39869761">[2 more]</label></div><br/><div class="children"><div class="content">Ok, but what does a perceptron look like in 1-bit? Would it be just some simple logic gate, like an OR-gate?</div><br/><div id="39870217" class="c"><input type="checkbox" id="c-39870217" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#39866842">root</a><span>|</span><a href="#39869761">parent</a><span>|</span><a href="#39867235">next</a><span>|</span><label class="collapse" for="c-39870217">[-]</label><label class="expand" for="c-39870217">[1 more]</label></div><br/><div class="children"><div class="content">Not my area of expertise but I&#x27;d assume it becomes a decision tree or something.<p>Edit: lol <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39868508">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39868508</a></div><br/></div></div></div></div></div></div><div id="39867235" class="c"><input type="checkbox" id="c-39867235" checked=""/><div class="controls bullet"><span class="by">alephxyz</span><span>|</span><a href="#39866842">parent</a><span>|</span><a href="#39868947">prev</a><span>|</span><a href="#39867188">next</a><span>|</span><label class="collapse" for="c-39867235">[-]</label><label class="expand" for="c-39867235">[1 more]</label></div><br/><div class="children"><div class="content">LLMs have been trending towards obscenely large number of parameters (314B for grok), which makes quantization crucial if you want to run them without a Meta-sized budget.</div><br/></div></div><div id="39867188" class="c"><input type="checkbox" id="c-39867188" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#39866842">parent</a><span>|</span><a href="#39867235">prev</a><span>|</span><a href="#39866966">next</a><span>|</span><label class="collapse" for="c-39867188">[-]</label><label class="expand" for="c-39867188">[1 more]</label></div><br/><div class="children"><div class="content">Certainly does, people have been doing this in computer vision for years.</div><br/></div></div></div></div><div id="39868993" class="c"><input type="checkbox" id="c-39868993" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39866842">prev</a><span>|</span><a href="#39866963">next</a><span>|</span><label class="collapse" for="c-39868993">[-]</label><label class="expand" for="c-39868993">[1 more]</label></div><br/><div class="children"><div class="content">The most exciting part about ternary or binary weights is the inevitable hardware revolution for AI dedicated chips that&#x27;s going to result from it.</div><br/></div></div><div id="39866963" class="c"><input type="checkbox" id="c-39866963" checked=""/><div class="controls bullet"><span class="by">ianbicking</span><span>|</span><a href="#39868993">prev</a><span>|</span><a href="#39866175">next</a><span>|</span><label class="collapse" for="c-39866963">[-]</label><label class="expand" for="c-39866963">[2 more]</label></div><br/><div class="children"><div class="content">Reduction to decision tree!<p>But I&#x27;m unclear how it actually run, and the article talks about the conversion and training but doesn&#x27;t describe how it runs... I suppose because it&#x27;s obvious to someone who has followed quantization.<p>Thinking out loud... if you have a model of just 1 and 0, my first thought is that the outputs are 1&#x27;s and 0&#x27;s but I think that&#x27;s wrong. Instead it&#x27;s a bunch of floats, and you multiply them by 1 or 0 (in a sense you are sampling the output of the previous layer?), add them up, and put the result through some activation function. And two-bit quantization sounds kind of similar, just with a _little_ scale, going from -1 to 2 instead of 0 to 1.<p>It seems kind of interesting that you now have a bunch of weights that are exactly 0, meaning you can assert something about what parameters and weights affect what parts of the output. Though in some sense the way they compress the weights down to one bit is also a heuristic you could use to interpret the original model... this just makes it clearer that in totality you are making a defensible simplification, because the end result is still a working model.<p>It also seems like you could make a lot of mathematical assertions about a one bit model that would be harder to make otherwise. Like maybe you could start thinking of a model as an equation, a _particular_ (though giant) equation, and look at its properties and consider symbolic transformations to that equation.</div><br/><div id="39868508" class="c"><input type="checkbox" id="c-39868508" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#39866963">parent</a><span>|</span><a href="#39866175">next</a><span>|</span><label class="collapse" for="c-39868508">[-]</label><label class="expand" for="c-39868508">[1 more]</label></div><br/><div class="children"><div class="content">I like the decision tree analogy</div><br/></div></div></div></div><div id="39866175" class="c"><input type="checkbox" id="c-39866175" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#39866963">prev</a><span>|</span><a href="#39871418">next</a><span>|</span><label class="collapse" for="c-39866175">[-]</label><label class="expand" for="c-39866175">[14 more]</label></div><br/><div class="children"><div class="content">I like how it goes from 13.5 GB to 1.76 GB and gets comparable results. Definitely there is some underlying process of why this works so well that we are missing. Are the bits selecting the different subspaces? Can we improve this process by using better normalization layers &#x2F; gating units?</div><br/><div id="39866354" class="c"><input type="checkbox" id="c-39866354" checked=""/><div class="controls bullet"><span class="by">nobodyandproud</span><span>|</span><a href="#39866175">parent</a><span>|</span><a href="#39866474">next</a><span>|</span><label class="collapse" for="c-39866354">[-]</label><label class="expand" for="c-39866354">[12 more]</label></div><br/><div class="children"><div class="content">I wonder what information theory can tell us here.</div><br/><div id="39866688" class="c"><input type="checkbox" id="c-39866688" checked=""/><div class="controls bullet"><span class="by">rapatel0</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39866354">parent</a><span>|</span><a href="#39866674">next</a><span>|</span><label class="collapse" for="c-39866688">[-]</label><label class="expand" for="c-39866688">[10 more]</label></div><br/><div class="children"><div class="content">The entropy of English is ~1 bit per letter (Measured in a funny way by Shannon - <a href="https:&#x2F;&#x2F;cs.stanford.edu&#x2F;people&#x2F;eroberts&#x2F;courses&#x2F;soco&#x2F;projects&#x2F;1999-00&#x2F;information-theory&#x2F;entropy_of_english_9.html" rel="nofollow">https:&#x2F;&#x2F;cs.stanford.edu&#x2F;people&#x2F;eroberts&#x2F;courses&#x2F;soco&#x2F;project...</a>)<p>In general, it&#x27;s a bit funny in how we build ML models. You take a bucket of matrices, fill them with random numbers, and then start to introduce &quot;biases&quot; through back propagation. A model can converge down loss, but most of them are still filled with random noise.<p>Binary weights are somewhat obvious in retrospect. Weights indicate the strength of an association between two neurons. Intuitively, most of the value is probably just that an association between two neurons exists, and whether it&#x27;s positive or negatively associated.</div><br/><div id="39868371" class="c"><input type="checkbox" id="c-39868371" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39866688">parent</a><span>|</span><a href="#39870170">next</a><span>|</span><label class="collapse" for="c-39868371">[-]</label><label class="expand" for="c-39868371">[1 more]</label></div><br/><div class="children"><div class="content">Wow, fascinating read. 1999<p><i>Would a monkey who knew the n-gram frequencies of the letters in English where n is large be able to produce credible English text? Furthermore, does this monkey &quot;know&quot; English? If the N-gram monkey is behind one door and a human is behind the other, could a third-party observer tell which was the monkey? This question rings of the Turing test for artificial intelligence, to which there is no easy answer.</i><p>No easy answer indeed!</div><br/></div></div><div id="39870170" class="c"><input type="checkbox" id="c-39870170" checked=""/><div class="controls bullet"><span class="by">nobodyandproud</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39866688">parent</a><span>|</span><a href="#39868371">prev</a><span>|</span><a href="#39867426">next</a><span>|</span><label class="collapse" for="c-39870170">[-]</label><label class="expand" for="c-39870170">[1 more]</label></div><br/><div class="children"><div class="content">Down to one bit but that’s taking the 2.62 bits and then applying the redundancy factor.<p>What’s cool is that the differentiable activation function is important—-to avoid the linear, perceptron limitation—but the weight scaling can be so simple, at least for LLMs.<p>It makes me wonder whether the extra layers are effectively compensating; in other words, can the number of layers or hidden neurons be trimmed down if we then add more bits to each weight and still see equivalent effectiveness?</div><br/></div></div><div id="39867426" class="c"><input type="checkbox" id="c-39867426" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39866688">parent</a><span>|</span><a href="#39870170">prev</a><span>|</span><a href="#39866674">next</a><span>|</span><label class="collapse" for="c-39867426">[-]</label><label class="expand" for="c-39867426">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised that 1-bit works. Trinary (-1, 0, 1) makes sense, but if you only have 0 and 1, the asymmetry ought to be a problem. Or is multiplying an XOR, so 1x1 = 0?</div><br/><div id="39870020" class="c"><input type="checkbox" id="c-39870020" checked=""/><div class="controls bullet"><span class="by">gpm</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39867426">parent</a><span>|</span><a href="#39869676">next</a><span>|</span><label class="collapse" for="c-39870020">[-]</label><label class="expand" for="c-39870020">[1 more]</label></div><br/><div class="children"><div class="content">It seems like they have learned floating point parameters x and y so that dequantized(bit 0) = x and dequantized(bit 1) = y. Thus there is no built in asymmetry. Or more precisely they learned a zero point and a scale but it&#x27;s equivalent to this simpler model in the 1 bit case.<p>It still seems like there would be a problem because either [x, y] looks like [0ish, 1ish] and you can&#x27;t have negative weights, or [x, y] looks like [-1ish, 1ish] and you can&#x27;t have &quot;don&#x27;t care&quot; weights. But if you have some redundancy in your neurons I guess this is acceptable because you can just cancel out the positive contribution from a neuron you don&#x27;t care about with a negative contribution from a very similar neuron that you also don&#x27;t care about.</div><br/></div></div><div id="39869676" class="c"><input type="checkbox" id="c-39869676" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39867426">parent</a><span>|</span><a href="#39870020">prev</a><span>|</span><a href="#39868228">next</a><span>|</span><label class="collapse" for="c-39869676">[-]</label><label class="expand" for="c-39869676">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more like, addition is an XOR. (In fact, AND as mult and XOR as add are GF(2)). Throw in NOT (or, really, just a constant 1) and you can compute any circuit.<p>Biologically, inhibitory neurons are every bit as important as excitatory ones, so if you squint just right, XOR looks like a neuron&#x27;s activation being inhibited by another presynaptic neuron.</div><br/></div></div><div id="39868228" class="c"><input type="checkbox" id="c-39868228" checked=""/><div class="controls bullet"><span class="by">edflsafoiewq</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39867426">parent</a><span>|</span><a href="#39869676">prev</a><span>|</span><a href="#39869799">next</a><span>|</span><label class="collapse" for="c-39868228">[-]</label><label class="expand" for="c-39868228">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a scale and bias afterwards, so its not necessarily asymmetric.</div><br/></div></div><div id="39869799" class="c"><input type="checkbox" id="c-39869799" checked=""/><div class="controls bullet"><span class="by">rapatel0</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39867426">parent</a><span>|</span><a href="#39868228">prev</a><span>|</span><a href="#39866674">next</a><span>|</span><label class="collapse" for="c-39869799">[-]</label><label class="expand" for="c-39869799">[3 more]</label></div><br/><div class="children"><div class="content">Yeah basically. In binary, multiplication is an XNOR operation.<p>00 = 1<p>01 = 0<p>10 = 0<p>11 = 1</div><br/><div id="39870046" class="c"><input type="checkbox" id="c-39870046" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39869799">parent</a><span>|</span><a href="#39869979">next</a><span>|</span><label class="collapse" for="c-39870046">[-]</label><label class="expand" for="c-39870046">[1 more]</label></div><br/><div class="children"><div class="content">XNOR does not distribute over AND or any other binary operators (try 0 XNOR (0 AND 1)), nor does it have a multiplicative identity, so it&#x27;s not really multiplication in a way that&#x27;s useful.</div><br/></div></div><div id="39869979" class="c"><input type="checkbox" id="c-39869979" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39869799">parent</a><span>|</span><a href="#39870046">prev</a><span>|</span><a href="#39866674">next</a><span>|</span><label class="collapse" for="c-39869979">[-]</label><label class="expand" for="c-39869979">[1 more]</label></div><br/><div class="children"><div class="content">0*0 = 1? I&#x27;ve always hear it as being the output of AND</div><br/></div></div></div></div></div></div></div></div><div id="39866674" class="c"><input type="checkbox" id="c-39866674" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#39866175">root</a><span>|</span><a href="#39866354">parent</a><span>|</span><a href="#39866688">prev</a><span>|</span><a href="#39866474">next</a><span>|</span><label class="collapse" for="c-39866674">[-]</label><label class="expand" for="c-39866674">[1 more]</label></div><br/><div class="children"><div class="content">Neural networks work, but why they work is not well known. Researchers continue to find new “free lunches” all the time.</div><br/></div></div></div></div><div id="39866474" class="c"><input type="checkbox" id="c-39866474" checked=""/><div class="controls bullet"><span class="by">convolvatron</span><span>|</span><a href="#39866175">parent</a><span>|</span><a href="#39866354">prev</a><span>|</span><a href="#39871418">next</a><span>|</span><label class="collapse" for="c-39866474">[-]</label><label class="expand" for="c-39866474">[1 more]</label></div><br/><div class="children"><div class="content">idk that we need to think that hard. we have shown can build arithmetic functions from networks of logic gates given sufficient connectivity. so in some sense it just drops the learning network below the level of words and asked it to make its own math.<p>since this generalizability thing seems to be real, this kind of directly folds in quantization-aware-training, doesn&#x27;t it?</div><br/></div></div></div></div><div id="39871418" class="c"><input type="checkbox" id="c-39871418" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39866175">prev</a><span>|</span><a href="#39871975">next</a><span>|</span><label class="collapse" for="c-39871418">[-]</label><label class="expand" for="c-39871418">[1 more]</label></div><br/><div class="children"><div class="content">Highly respect HQQ team&#x27;s work - same accuracy as GPTQ &#x2F; AWQ and with no activation aware tuning calibration part - ie no more 3 hour calibration runs! A big fan of the 4bit ones especially, and the 3,2, and now 1bit ones!<p>Also super cool idea of 1bit needing some calibration like AWQ - no calibration data shows very bad results, but with some LoRA adapters and finetuning, a great recovery of performance is possible.<p>Planning to add support for this inside Unsloth to make all low bit finetuning 2x faster and save tonnes of VRAM!</div><br/></div></div><div id="39871975" class="c"><input type="checkbox" id="c-39871975" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#39871418">prev</a><span>|</span><a href="#39865855">next</a><span>|</span><label class="collapse" for="c-39871975">[-]</label><label class="expand" for="c-39871975">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if there is a correspondence to Sony&#x27;s 1-bit DSD audio stream? Could hardware developed for this purpose be used for some part of the processing?</div><br/></div></div><div id="39865855" class="c"><input type="checkbox" id="c-39865855" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#39871975">prev</a><span>|</span><a href="#39866756">next</a><span>|</span><label class="collapse" for="c-39865855">[-]</label><label class="expand" for="c-39865855">[6 more]</label></div><br/><div class="children"><div class="content">I look forward to the inevitable probabilistic sub-bit machine learning models :)</div><br/><div id="39867046" class="c"><input type="checkbox" id="c-39867046" checked=""/><div class="controls bullet"><span class="by">paul_mk1</span><span>|</span><a href="#39865855">parent</a><span>|</span><a href="#39866294">next</a><span>|</span><label class="collapse" for="c-39867046">[-]</label><label class="expand" for="c-39867046">[1 more]</label></div><br/><div class="children"><div class="content">Sub 1-bit has been done at least as far back as 2016 for VGG style networks (my work).<p>I was able to get 0.68 &quot;effective&quot; bits.<p>The idea is that in each forward pass you add noise to <i>each</i> weight independently drawn from normal distribution, and when you calculate snr, it&#x27;s sub 1 bit. Points to the idea that a stochastic memory element can be used.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.01981" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1606.01981</a></div><br/></div></div><div id="39866294" class="c"><input type="checkbox" id="c-39866294" checked=""/><div class="controls bullet"><span class="by">Cheer2171</span><span>|</span><a href="#39865855">parent</a><span>|</span><a href="#39867046">prev</a><span>|</span><a href="#39866025">next</a><span>|</span><label class="collapse" for="c-39866294">[-]</label><label class="expand" for="c-39866294">[2 more]</label></div><br/><div class="children"><div class="content">My 0-bit model can predict if you have cancer with 99.5% accuracy. It doesn&#x27;t even need input data! Don&#x27;t ask about the false negative rate though.</div><br/><div id="39867122" class="c"><input type="checkbox" id="c-39867122" checked=""/><div class="controls bullet"><span class="by">asah</span><span>|</span><a href="#39865855">root</a><span>|</span><a href="#39866294">parent</a><span>|</span><a href="#39866025">next</a><span>|</span><label class="collapse" for="c-39867122">[-]</label><label class="expand" for="c-39867122">[1 more]</label></div><br/><div class="children"><div class="content">My 0-bit, no-input model can predict if you have cancer with 99.5% accuracy and 0.5% false negative rate. Don&#x27;t ask about whether the cancer cell(s) are benign.</div><br/></div></div></div></div><div id="39866025" class="c"><input type="checkbox" id="c-39866025" checked=""/><div class="controls bullet"><span class="by">Cieric</span><span>|</span><a href="#39865855">parent</a><span>|</span><a href="#39866294">prev</a><span>|</span><a href="#39865916">next</a><span>|</span><label class="collapse" for="c-39866025">[-]</label><label class="expand" for="c-39866025">[1 more]</label></div><br/><div class="children"><div class="content">I assume this is ment to be a joke, but isn&#x27;t this actually being worked on? (minus the probabilistic portion) <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16795" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16795</a></div><br/></div></div></div></div><div id="39866756" class="c"><input type="checkbox" id="c-39866756" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#39865855">prev</a><span>|</span><a href="#39866020">next</a><span>|</span><label class="collapse" for="c-39866756">[-]</label><label class="expand" for="c-39866756">[5 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Adjacency_matrix" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Adjacency_matrix</a><p>What would a graph look like when edges are represented with less than 1-bit?</div><br/><div id="39867471" class="c"><input type="checkbox" id="c-39867471" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39866756">parent</a><span>|</span><a href="#39870288">next</a><span>|</span><label class="collapse" for="c-39867471">[-]</label><label class="expand" for="c-39867471">[2 more]</label></div><br/><div class="children"><div class="content">It would have predictible structure.  There would be groupwise, regular sparsity.  You could exploit these patterns to infer properties like, &quot;there is no connection from this vertex to this entire span of verticies&quot;.<p>Just think about representing a compressed version of a matrix.  You want to have a short encoding for matrices are ones that are more ordered in some real, exploitable way.  With some determnistic computation of the compressed representation, you could produce the full adjacency matrix.</div><br/><div id="39868364" class="c"><input type="checkbox" id="c-39868364" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#39866756">root</a><span>|</span><a href="#39867471">parent</a><span>|</span><a href="#39870288">next</a><span>|</span><label class="collapse" for="c-39868364">[-]</label><label class="expand" for="c-39868364">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v206&#x2F;meller23a&#x2F;meller23a.pdf" rel="nofollow">https:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v206&#x2F;meller23a&#x2F;meller23a.pdf</a><p>It seems to have a lot of similarities to the technique in OPs article.</div><br/></div></div></div></div><div id="39870288" class="c"><input type="checkbox" id="c-39870288" checked=""/><div class="controls bullet"><span class="by">selecsosi</span><span>|</span><a href="#39866756">parent</a><span>|</span><a href="#39867471">prev</a><span>|</span><a href="#39866020">next</a><span>|</span><label class="collapse" for="c-39870288">[-]</label><label class="expand" for="c-39870288">[2 more]</label></div><br/><div class="children"><div class="content">I think the problem is that you are also trying to represent the biases of directionality in the connection e.g.: [-1,0,1]. In the adjacency matrix, if you store the sign (direction) of the edge, then I think that would map fairly isomorphically.</div><br/><div id="39872706" class="c"><input type="checkbox" id="c-39872706" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#39866756">root</a><span>|</span><a href="#39870288">parent</a><span>|</span><a href="#39866020">next</a><span>|</span><label class="collapse" for="c-39872706">[-]</label><label class="expand" for="c-39872706">[1 more]</label></div><br/><div class="children"><div class="content">Basically factorized into a giant state machine. Would be logical.</div><br/></div></div></div></div></div></div><div id="39866020" class="c"><input type="checkbox" id="c-39866020" checked=""/><div class="controls bullet"><span class="by">iamgopal</span><span>|</span><a href="#39866756">prev</a><span>|</span><a href="#39867470">next</a><span>|</span><label class="collapse" for="c-39866020">[-]</label><label class="expand" for="c-39866020">[4 more]</label></div><br/><div class="children"><div class="content">So new AI chip will be coming optimised for large scale non-byte bit only AI world ?</div><br/><div id="39866390" class="c"><input type="checkbox" id="c-39866390" checked=""/><div class="controls bullet"><span class="by">thesz</span><span>|</span><a href="#39866020">parent</a><span>|</span><a href="#39867470">next</a><span>|</span><label class="collapse" for="c-39866390">[-]</label><label class="expand" for="c-39866390">[3 more]</label></div><br/><div class="children"><div class="content">Every CPU and GPU now has popcount [1] instruction. You can thank US three letter agencies for that [2].<p><pre><code>  [1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hamming_weight#Processor_support
  [2] https:&#x2F;&#x2F;vaibhavsagar.com&#x2F;blog&#x2F;2019&#x2F;09&#x2F;08&#x2F;popcount&#x2F;
</code></pre>
Multiplication then is just bitwise AND and a popcount.</div><br/><div id="39866681" class="c"><input type="checkbox" id="c-39866681" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#39866020">root</a><span>|</span><a href="#39866390">parent</a><span>|</span><a href="#39867470">next</a><span>|</span><label class="collapse" for="c-39866681">[-]</label><label class="expand" for="c-39866681">[2 more]</label></div><br/><div class="children"><div class="content">Makes you wonder what the agencies know that we don’t…</div><br/><div id="39866735" class="c"><input type="checkbox" id="c-39866735" checked=""/><div class="controls bullet"><span class="by">rapatel0</span><span>|</span><a href="#39866020">root</a><span>|</span><a href="#39866681">parent</a><span>|</span><a href="#39867470">next</a><span>|</span><label class="collapse" for="c-39866735">[-]</label><label class="expand" for="c-39866735">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just cryptography and encoding. Popcount is useful for understanding entropy.</div><br/></div></div></div></div></div></div></div></div><div id="39867470" class="c"><input type="checkbox" id="c-39867470" checked=""/><div class="controls bullet"><span class="by">r1chardnl</span><span>|</span><a href="#39866020">prev</a><span>|</span><a href="#39867998">next</a><span>|</span><label class="collapse" for="c-39867470">[-]</label><label class="expand" for="c-39867470">[2 more]</label></div><br/><div class="children"><div class="content">Is this similar to binary search or could it be a binary search where you&#x27;re just excluding all possibilities that you know can&#x27;t be the probable result that you&#x27;re looking for?</div><br/><div id="39868329" class="c"><input type="checkbox" id="c-39868329" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#39867470">parent</a><span>|</span><a href="#39867998">next</a><span>|</span><label class="collapse" for="c-39868329">[-]</label><label class="expand" for="c-39868329">[1 more]</label></div><br/><div class="children"><div class="content">Feed forward networks are effectively DAGs<p>With the smaller model size, quantized models have less accuracy and less stable training, but large models take advantage of increased parallelism.<p>Binary search is more linear, circuits are a better lens than turing machines.<p>While it still needs to be verified, if you look into what a uniform consent depth threshold circuit is, that will help.<p>Although I guess binary weights may be in AC0 and not TC0, but that may not hold with billions of parameters.</div><br/></div></div></div></div><div id="39867998" class="c"><input type="checkbox" id="c-39867998" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39867470">prev</a><span>|</span><a href="#39866517">next</a><span>|</span><label class="collapse" for="c-39867998">[-]</label><label class="expand" for="c-39867998">[4 more]</label></div><br/><div class="children"><div class="content">Stupid question: how can you do deep learning on 1 bit? DL works by calculating a gradient and following it to minimize a loss function, but if all your parameters are 1 bit, there&#x27;s no such thing as a gradient as you don&#x27;t have differentiation at all. So how does it work?</div><br/><div id="39868198" class="c"><input type="checkbox" id="c-39868198" checked=""/><div class="controls bullet"><span class="by">Centigonal</span><span>|</span><a href="#39867998">parent</a><span>|</span><a href="#39868176">next</a><span>|</span><label class="collapse" for="c-39868198">[-]</label><label class="expand" for="c-39868198">[1 more]</label></div><br/><div class="children"><div class="content">Not a stupid question at all.<p>Early quantization approaches just quantized a high-bit-precision pre-trained model - no need to calculate gradients on the quantized weights. BitNet[1] changed the game by training a low-precision model from scratch. It achieves this by keeping high precision in the gradients, optimizer state, and in &quot;latent weights&quot; which are then quantized on the fly. I don&#x27;t really understand the finer details of how this works, so check out the paper if you&#x27;re interested.<p>This article&#x27;s approach is interesting. They start by quantizing a pre-trained high-precision model, and then they fine-tune the quantized model using LoRA (which dramatically improves the performance of the quantized model). They don&#x27;t talk about the bit depth of the values in the LoRA matrices, so it may be that those are higher bit-depth.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.11453.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.11453.pdf</a></div><br/></div></div><div id="39868176" class="c"><input type="checkbox" id="c-39868176" checked=""/><div class="controls bullet"><span class="by">edflsafoiewq</span><span>|</span><a href="#39867998">parent</a><span>|</span><a href="#39868198">prev</a><span>|</span><a href="#39866517">next</a><span>|</span><label class="collapse" for="c-39868176">[-]</label><label class="expand" for="c-39868176">[2 more]</label></div><br/><div class="children"><div class="content">Not sure what you mean, why wouldn&#x27;t there be a gradient? There is a problem that small changes to weights may have no effect (eg ((0 + 0.3) + 0.3) + 0.4 = 0 if you have to round the result to 0 or 1 after every step), but to fix this I believe everyone maintains high precision weights for the backwards pass, from which the quantized weights are derived.</div><br/><div id="39872833" class="c"><input type="checkbox" id="c-39872833" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#39867998">root</a><span>|</span><a href="#39868176">parent</a><span>|</span><a href="#39866517">next</a><span>|</span><label class="collapse" for="c-39872833">[-]</label><label class="expand" for="c-39872833">[1 more]</label></div><br/><div class="children"><div class="content">You could treat fractional addition as probabilistic operation.</div><br/></div></div></div></div></div></div><div id="39866517" class="c"><input type="checkbox" id="c-39866517" checked=""/><div class="controls bullet"><span class="by">redskyluan</span><span>|</span><a href="#39867998">prev</a><span>|</span><a href="#39866135">next</a><span>|</span><label class="collapse" for="c-39866517">[-]</label><label class="expand" for="c-39866517">[3 more]</label></div><br/><div class="children"><div class="content">what will be the next step of 1 bit?
could it be 0.5 bit?
Any kind of quantization is not gonna to last long.</div><br/><div id="39866800" class="c"><input type="checkbox" id="c-39866800" checked=""/><div class="controls bullet"><span class="by">pixelpoet</span><span>|</span><a href="#39866517">parent</a><span>|</span><a href="#39866563">next</a><span>|</span><label class="collapse" for="c-39866800">[-]</label><label class="expand" for="c-39866800">[1 more]</label></div><br/><div class="children"><div class="content">The way things are going we&#x27;ll have -1 bit before long :)</div><br/></div></div><div id="39866563" class="c"><input type="checkbox" id="c-39866563" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#39866517">parent</a><span>|</span><a href="#39866800">prev</a><span>|</span><a href="#39866135">next</a><span>|</span><label class="collapse" for="c-39866563">[-]</label><label class="expand" for="c-39866563">[1 more]</label></div><br/><div class="children"><div class="content">Not using traditional numbers at all, I imagine.  It&#x27;s possible that trying to build neural nets out of vectors that are themselves built with quantized scalars is a suboptimal approach.</div><br/></div></div></div></div><div id="39866135" class="c"><input type="checkbox" id="c-39866135" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#39866517">prev</a><span>|</span><label class="collapse" for="c-39866135">[-]</label><label class="expand" for="c-39866135">[13 more]</label></div><br/><div class="children"><div class="content">As an AI layman, I am for some reason excited by the low-bit learning models.<p>Apple, now leaving behind the albatros that was the Apple car, could be the one to roll their own silicon and put an LLM in our back pockets.<p>Goodbye Siri, hello Seriously.</div><br/><div id="39866198" class="c"><input type="checkbox" id="c-39866198" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#39866135">parent</a><span>|</span><a href="#39866712">next</a><span>|</span><label class="collapse" for="c-39866198">[-]</label><label class="expand" for="c-39866198">[4 more]</label></div><br/><div class="children"><div class="content">Or we could end up getting another Google integration, with iOS being too rigid and locked-down to feasibly support third-party private AI alternatives. Fate has a cruel sense of humor, sometimes.</div><br/><div id="39866379" class="c"><input type="checkbox" id="c-39866379" checked=""/><div class="controls bullet"><span class="by">odyssey7</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39866198">parent</a><span>|</span><a href="#39866712">next</a><span>|</span><label class="collapse" for="c-39866379">[-]</label><label class="expand" for="c-39866379">[3 more]</label></div><br/><div class="children"><div class="content">Are you referring to the Swift version of TensorFlow?<p><a href="https:&#x2F;&#x2F;www.infoworld.com&#x2F;article&#x2F;3608151&#x2F;swift-for-tensorflow-project-shuts-down.html" rel="nofollow">https:&#x2F;&#x2F;www.infoworld.com&#x2F;article&#x2F;3608151&#x2F;swift-for-tensorfl...</a></div><br/><div id="39868092" class="c"><input type="checkbox" id="c-39868092" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39866379">parent</a><span>|</span><a href="#39866712">next</a><span>|</span><label class="collapse" for="c-39868092">[-]</label><label class="expand" for="c-39868092">[2 more]</label></div><br/><div class="children"><div class="content">I believe they&#x27;re referring to these rumors: <a href="https:&#x2F;&#x2F;www.macrumors.com&#x2F;2024&#x2F;03&#x2F;21&#x2F;apple-iphone-ai-talks-google-openai-ongoing&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.macrumors.com&#x2F;2024&#x2F;03&#x2F;21&#x2F;apple-iphone-ai-talks-g...</a></div><br/><div id="39868972" class="c"><input type="checkbox" id="c-39868972" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39868092">parent</a><span>|</span><a href="#39866712">next</a><span>|</span><label class="collapse" for="c-39868972">[-]</label><label class="expand" for="c-39868972">[1 more]</label></div><br/><div class="children"><div class="content">Could be a way for Apple to buy time as well until they do have something competitive.</div><br/></div></div></div></div></div></div></div></div><div id="39866712" class="c"><input type="checkbox" id="c-39866712" checked=""/><div class="controls bullet"><span class="by">quentinp</span><span>|</span><a href="#39866135">parent</a><span>|</span><a href="#39866198">prev</a><span>|</span><a href="#39866711">next</a><span>|</span><label class="collapse" for="c-39866712">[-]</label><label class="expand" for="c-39866712">[3 more]</label></div><br/><div class="children"><div class="content">Actually they plan to put an LLM in your back pocket using flash memory, not silicon: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.11514" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.11514</a></div><br/><div id="39866942" class="c"><input type="checkbox" id="c-39866942" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39866712">parent</a><span>|</span><a href="#39866711">next</a><span>|</span><label class="collapse" for="c-39866942">[-]</label><label class="expand" for="c-39866942">[2 more]</label></div><br/><div class="children"><div class="content">The flash doesn&#x27;t do the computations though, that&#x27;s just a method of getting it to the processor</div><br/><div id="39868568" class="c"><input type="checkbox" id="c-39868568" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39866942">parent</a><span>|</span><a href="#39866711">next</a><span>|</span><label class="collapse" for="c-39868568">[-]</label><label class="expand" for="c-39868568">[1 more]</label></div><br/><div class="children"><div class="content">It would be better to have eeprom or some such directly attached as memory. No loading.</div><br/></div></div></div></div></div></div><div id="39866711" class="c"><input type="checkbox" id="c-39866711" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#39866135">parent</a><span>|</span><a href="#39866712">prev</a><span>|</span><a href="#39866381">next</a><span>|</span><label class="collapse" for="c-39866711">[-]</label><label class="expand" for="c-39866711">[4 more]</label></div><br/><div class="children"><div class="content">&gt; could be the one to roll their own silicon and put an LLM in our back pockets.<p>Wouldn&#x27;t they have to do a lot of catching up with everyone else already working on it?</div><br/><div id="39866974" class="c"><input type="checkbox" id="c-39866974" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39866711">parent</a><span>|</span><a href="#39866381">next</a><span>|</span><label class="collapse" for="c-39866974">[-]</label><label class="expand" for="c-39866974">[3 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve been designing their own chips a while now, including with an NPU.<p>Also because of their unified memory design, they actually have insane bandwidth which is incredibly useful for LLMs. IMO they may have a head-start in that respect for on-device inference of large models (e.g. 1B+ params).</div><br/><div id="39869881" class="c"><input type="checkbox" id="c-39869881" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39866974">parent</a><span>|</span><a href="#39866381">next</a><span>|</span><label class="collapse" for="c-39869881">[-]</label><label class="expand" for="c-39869881">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think people are running 1B+ models on the Neural Engine these days. The high-performance models I&#x27;ve seen all rely on Metal Performance Shaders, which scales with how powerful your GPU is. It&#x27;s not terribly slow on iPhone, but I think some people get the wrong idea and correlate an ambient processor like the Neural Engine with LLMs.<p>The bigger bottleneck seems like memory, to me. iPhones have traditionally skimped on RAM moreso than even cheap and midrange Android counterparts. I can imagine running an LLM in the background on my S10 - it&#x27;s a bit harder to envision iOS swapping everything smoothly on a similarly-aged iPhone.</div><br/><div id="39871707" class="c"><input type="checkbox" id="c-39871707" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#39866135">root</a><span>|</span><a href="#39869881">parent</a><span>|</span><a href="#39866381">next</a><span>|</span><label class="collapse" for="c-39871707">[-]</label><label class="expand" for="c-39871707">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but we&#x27;re discussing 1.8-bit models that, again I&#x27;m a layman, I assume are over an order of magnitude smaller in their memory overhead.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>