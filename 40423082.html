<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716282065083" as="style"/><link rel="stylesheet" href="styles.css?v=1716282065083"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.09818">Chameleon: Meta&#x27;s New Multi-Modal LLM</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>gabrielbirnbaum</span> | <span>21 comments</span></div><br/><div><div id="40424268" class="c"><input type="checkbox" id="c-40424268" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#40424326">next</a><span>|</span><label class="collapse" for="c-40424268">[-]</label><label class="expand" for="c-40424268">[1 more]</label></div><br/><div class="children"><div class="content">Relevant thread on &#x2F;r&#x2F;locallama ([1]). A relevant quote from the comments:<p>&gt; There&#x27;s a Twitter thread from one of the authors ([2]). This part seems pretty important: &quot;The models in this paper were done training 5 months ago. We&#x27;ve progressed significantly since then.&quot;<p>1. <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1ctsala&#x2F;newly_published_work_from_fair_chameleon&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1ctsala&#x2F;newly_p...</a><p>2. <a href="https:&#x2F;&#x2F;x.com&#x2F;ArmenAgha&#x2F;status&#x2F;1791275549815648473" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;ArmenAgha&#x2F;status&#x2F;1791275549815648473</a></div><br/></div></div><div id="40424326" class="c"><input type="checkbox" id="c-40424326" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#40424268">prev</a><span>|</span><a href="#40425691">next</a><span>|</span><label class="collapse" for="c-40424326">[-]</label><label class="expand" for="c-40424326">[2 more]</label></div><br/><div class="children"><div class="content">There’s some pretty nice fundamental research in here, and I appreciate the publication very much. What stood out to me is their discussion of the difficulties of using softmax against different tokenization spaces; super interesting analysis (they say different modalities compete by upping their own strength relative to other modalities, leading to divergence), and the ultimate fix, (I can’t remember it right now, and leave this as a tease to the interested paper reader).<p>They also noted the problem was most pronounced once they got up to 34b sized. It’s a good reminder training large scale models leads to new interesting problems. I imagine a lot of techniques and know how are not published; all those little bits of experience add up to a lot of competitive advantage in many venues, so once again, thanks to Zuck and co for publishing.</div><br/><div id="40424396" class="c"><input type="checkbox" id="c-40424396" checked=""/><div class="controls bullet"><span class="by">md_rumpf</span><span>|</span><a href="#40424326">parent</a><span>|</span><a href="#40425691">next</a><span>|</span><label class="collapse" for="c-40424396">[-]</label><label class="expand" for="c-40424396">[1 more]</label></div><br/><div class="children"><div class="content">the modality competition was one of my favorite insights, too!</div><br/></div></div></div></div><div id="40425691" class="c"><input type="checkbox" id="c-40425691" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40424326">prev</a><span>|</span><a href="#40424593">next</a><span>|</span><label class="collapse" for="c-40425691">[-]</label><label class="expand" for="c-40425691">[4 more]</label></div><br/><div class="children"><div class="content">Am I reading this correctly:<p>Training time was 4282407hrs. At, conservatively, 200w gpus, that&#x27;s (4282407*200)&#x2F;1_000_000_000 GWh ~= 1 GWh. At 10c&#x2F;kWh that&#x27;s $100,000 ?<p>So if you have a single eqv GPU at home, it&#x27;s 500yrs of training time and $100k in energy costs. Or, in practice, 3000 gpus for 2mo.<p>The AI industry has to hope the world doesnt change fast enough for these models to be useless.<p>EDIT: price is $100k</div><br/><div id="40425721" class="c"><input type="checkbox" id="c-40425721" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#40425691">parent</a><span>|</span><a href="#40425724">next</a><span>|</span><label class="collapse" for="c-40425721">[-]</label><label class="expand" for="c-40425721">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the figures. I suppose with expenses like that, they will be motivated to research methods of updating models which have already been trained.</div><br/></div></div><div id="40425724" class="c"><input type="checkbox" id="c-40425724" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#40425691">parent</a><span>|</span><a href="#40425721">prev</a><span>|</span><a href="#40424593">next</a><span>|</span><label class="collapse" for="c-40425724">[-]</label><label class="expand" for="c-40425724">[2 more]</label></div><br/><div class="children"><div class="content">1 GWh is 1 million kWh, multiplied by $0.1 that should give $100k in energy costs?</div><br/><div id="40425801" class="c"><input type="checkbox" id="c-40425801" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#40425691">root</a><span>|</span><a href="#40425724">parent</a><span>|</span><a href="#40424593">next</a><span>|</span><label class="collapse" for="c-40425801">[-]</label><label class="expand" for="c-40425801">[1 more]</label></div><br/><div class="children"><div class="content">Yes, thanks. I had assumed I had been off by a factor somewhere. Yet, 100k seems small -- the total cost of production is in the 10mil+ range.</div><br/></div></div></div></div></div></div><div id="40424593" class="c"><input type="checkbox" id="c-40424593" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#40425691">prev</a><span>|</span><a href="#40424166">next</a><span>|</span><label class="collapse" for="c-40424593">[-]</label><label class="expand" for="c-40424593">[1 more]</label></div><br/><div class="children"><div class="content">Compared to Mirasol3B[1] this is not supporting audio as a modality. What Google has done with Mirasol3B made the demo of &quot;Astro&quot; in Google I&#x2F;O possible. They do a little of cheating by converting audio to images(spectrogram) and video to 25 photo frames with some sort of attention system to things that change during those frames. So the tokenizer is basically the same for audio and video and images.<p>I believe Meta is going to this direction with multimodality as well. The new GPT voice mode is probably using the same architecture.<p>What&#x27;s mind boggling is that models perform better at the same parameter size with new modality added to them!<p>It seems obvious that 3D is the next modality.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2311.05698" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2311.05698</a></div><br/></div></div><div id="40424166" class="c"><input type="checkbox" id="c-40424166" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#40424593">prev</a><span>|</span><a href="#40424116">next</a><span>|</span><label class="collapse" for="c-40424166">[-]</label><label class="expand" for="c-40424166">[1 more]</label></div><br/><div class="children"><div class="content">It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.</div><br/></div></div><div id="40424116" class="c"><input type="checkbox" id="c-40424116" checked=""/><div class="controls bullet"><span class="by">elijahbenizzy</span><span>|</span><a href="#40424166">prev</a><span>|</span><a href="#40424773">next</a><span>|</span><label class="collapse" for="c-40424116">[-]</label><label class="expand" for="c-40424116">[6 more]</label></div><br/><div class="children"><div class="content">What&#x27;s cool (and tough to keep up with) with this wave of tech is just how quickly it moves.<p>On the plus side there&#x27;s a lot of interesting things and it is generally easy to follow&#x2F;figure out what they did.<p>On the minus side it&#x27;s a little exhausting, and there&#x27;s so much money in it feels like the vast majority of it is grifting. To add to that, the people who are trying to catalog it (the AI guy on LI) are the griftiest of them all.<p>I&#x27;ve found the best way to keep up is find one topic you want to learn about, deep-dive, and read all the related papers, then explore breadth-first from there until you find another topic...</div><br/><div id="40424742" class="c"><input type="checkbox" id="c-40424742" checked=""/><div class="controls bullet"><span class="by">polskibus</span><span>|</span><a href="#40424116">parent</a><span>|</span><a href="#40424169">next</a><span>|</span><label class="collapse" for="c-40424742">[-]</label><label class="expand" for="c-40424742">[1 more]</label></div><br/><div class="children"><div class="content">How do you keep your knowledge after deep dive? Do you try to use it somehow? I found that reading a lot usually does not contribute to long term proficiency in a given topic, unless followed by non trivial amount of practice.</div><br/></div></div><div id="40424169" class="c"><input type="checkbox" id="c-40424169" checked=""/><div class="controls bullet"><span class="by">361994752</span><span>|</span><a href="#40424116">parent</a><span>|</span><a href="#40424742">prev</a><span>|</span><a href="#40424159">next</a><span>|</span><label class="collapse" for="c-40424169">[-]</label><label class="expand" for="c-40424169">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s still in the early phase where the bubble is building up. This is necessary if we want a prosperous market. Hopefully, after the bubble bursts, some good companies will remain (which is very likely).</div><br/></div></div><div id="40424159" class="c"><input type="checkbox" id="c-40424159" checked=""/><div class="controls bullet"><span class="by">randmeerkat</span><span>|</span><a href="#40424116">parent</a><span>|</span><a href="#40424169">prev</a><span>|</span><a href="#40424773">next</a><span>|</span><label class="collapse" for="c-40424159">[-]</label><label class="expand" for="c-40424159">[3 more]</label></div><br/><div class="children"><div class="content">&gt; On the minus side it&#x27;s a little exhausting, and there&#x27;s so much money in it feels like the vast majority of it is grifting.<p>It is all grifting. The moment someone creates something that can improve upon itself there will be an intelligence explosion and it won’t need press releases or debates about its intelligence. The current path of research will not lead to that, if there was something to be discovered here it would have been discovered already. It’s just new to the general consumer and there’s a wow factor associated with it like crypto and NFTs before. The truth is tech has lost its momentum and is desperate to find a new trick.</div><br/><div id="40424225" class="c"><input type="checkbox" id="c-40424225" checked=""/><div class="controls bullet"><span class="by">elijahbenizzy</span><span>|</span><a href="#40424116">root</a><span>|</span><a href="#40424159">parent</a><span>|</span><a href="#40424773">next</a><span>|</span><label class="collapse" for="c-40424225">[-]</label><label class="expand" for="c-40424225">[2 more]</label></div><br/><div class="children"><div class="content">I think the opposite. There is value in intelligent software, but IMO we’re a long way from AGI. So lots of grifting but some gold along the way. And it’s intellectually interesting&#x2F;nuanced (cool math, interesting infra), unlike crypto which was more of a massive energy burning waste than anyone likes to admit.</div><br/><div id="40424305" class="c"><input type="checkbox" id="c-40424305" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#40424116">root</a><span>|</span><a href="#40424225">parent</a><span>|</span><a href="#40424773">next</a><span>|</span><label class="collapse" for="c-40424305">[-]</label><label class="expand" for="c-40424305">[1 more]</label></div><br/><div class="children"><div class="content">If our standard is cool math, crypto is also full of cool math. (Have you read Vitalik&#x27;s explanation of Quadratic Arithmetic Programs?) Our standard can&#x27;t be that low.<p><a href="https:&#x2F;&#x2F;medium.com&#x2F;@VitalikButerin&#x2F;quadratic-arithmetic-programs-from-zero-to-hero-f6d558cea649" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@VitalikButerin&#x2F;quadratic-arithmetic-prog...</a></div><br/></div></div></div></div></div></div></div></div><div id="40424773" class="c"><input type="checkbox" id="c-40424773" checked=""/><div class="controls bullet"><span class="by">kriro</span><span>|</span><a href="#40424116">prev</a><span>|</span><a href="#40424401">next</a><span>|</span><label class="collapse" for="c-40424773">[-]</label><label class="expand" for="c-40424773">[1 more]</label></div><br/><div class="children"><div class="content">Only browsed but this is really interesting and I&#x27;m glad it was published.<p>I understand why a unified model is an interesting thing to work on but doesn&#x27;t the discovery of &quot;modal-competition&quot; suggest that at least short term it might be even better to train specialized models for each modality and some sort of modality-supervisor (glue code model)?</div><br/></div></div><div id="40424401" class="c"><input type="checkbox" id="c-40424401" checked=""/><div class="controls bullet"><span class="by">md_rumpf</span><span>|</span><a href="#40424773">prev</a><span>|</span><a href="#40424576">next</a><span>|</span><label class="collapse" for="c-40424401">[-]</label><label class="expand" for="c-40424401">[2 more]</label></div><br/><div class="children"><div class="content">every 3rd sentsnce is &quot;the model was not trained on data from meta&#x27;s products&quot;</div><br/><div id="40424582" class="c"><input type="checkbox" id="c-40424582" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#40424401">parent</a><span>|</span><a href="#40424576">next</a><span>|</span><label class="collapse" for="c-40424582">[-]</label><label class="expand" for="c-40424582">[1 more]</label></div><br/><div class="children"><div class="content">That makes sense. You probably don&#x27;t want to train your LLM against your uncle&#x27;s dubious claims about the government, flat earthers and the likes content :)</div><br/></div></div></div></div><div id="40424576" class="c"><input type="checkbox" id="c-40424576" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#40424401">prev</a><span>|</span><a href="#40424964">next</a><span>|</span><label class="collapse" for="c-40424576">[-]</label><label class="expand" for="c-40424576">[1 more]</label></div><br/><div class="children"><div class="content">Does Meta plan to open source these models?</div><br/></div></div><div id="40424964" class="c"><input type="checkbox" id="c-40424964" checked=""/><div class="controls bullet"><span class="by">dankle</span><span>|</span><a href="#40424576">prev</a><span>|</span><label class="collapse" for="c-40424964">[-]</label><label class="expand" for="c-40424964">[1 more]</label></div><br/><div class="children"><div class="content">Are they downloadable?</div><br/></div></div></div></div></div></div></div></body></html>