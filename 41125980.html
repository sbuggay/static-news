<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1722502852513" as="style"/><link rel="stylesheet" href="styles.css?v=1722502852513"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/pytorch/torchchat">PyTorch – Torchchat: Chat with LLMs Everywhere</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>constantinum</span> | <span>5 comments</span></div><br/><div><div id="41126598" class="c"><input type="checkbox" id="c-41126598" checked=""/><div class="controls bullet"><span class="by">gleenn</span><span>|</span><a href="#41126639">next</a><span>|</span><label class="collapse" for="c-41126598">[-]</label><label class="expand" for="c-41126598">[1 more]</label></div><br/><div class="children"><div class="content">This looks awesome, the instructions are basically a one-liner to get a Python program to start up a chat program, and it&#x27;s optimized for a lot of hardware you can run locally like if you have an Nvidia GPU or Apple M processor. Super cool work bringing this functionality to local apps and to just play with a lot of popular models. Great work</div><br/></div></div><div id="41126639" class="c"><input type="checkbox" id="c-41126639" checked=""/><div class="controls bullet"><span class="by">daghamm</span><span>|</span><a href="#41126598">prev</a><span>|</span><a href="#41126748">next</a><span>|</span><label class="collapse" for="c-41126639">[-]</label><label class="expand" for="c-41126639">[2 more]</label></div><br/><div class="children"><div class="content">Does pytorch have better acceleration on x64 CPUs nowadays?<p>Last time I played with LLMs on CPU with pytorch you had to replace some stuff with libraries from Intel otherwise your performance would be really bad.</div><br/><div id="41126929" class="c"><input type="checkbox" id="c-41126929" checked=""/><div class="controls bullet"><span class="by">gleenn</span><span>|</span><a href="#41126639">parent</a><span>|</span><a href="#41126748">next</a><span>|</span><label class="collapse" for="c-41126929">[-]</label><label class="expand" for="c-41126929">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t find it again in this doc but pretty sure it supports MKL which at least is Intel&#x27;s faster math library. Better than a stick in the eye. Also certainly faster than plain CPUs but almost certainly way slower than something with more massively parallel matrix processing.</div><br/></div></div></div></div></div></div></div></div></div></body></html>