<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712048452241" as="style"/><link rel="stylesheet" href="styles.css?v=1712048452241"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.youtube.com/watch?v=wjZofJX0v4M">But what is a GPT?  Visual intro to Transformers [video]</a>Â <span class="domain">(<a href="https://www.youtube.com">www.youtube.com</a>)</span></div><div class="subtext"><span>huhhuh</span> | <span>47 comments</span></div><br/><div><div id="39899346" class="c"><input type="checkbox" id="c-39899346" checked=""/><div class="controls bullet"><span class="by">user_7832</span><span>|</span><a href="#39898356">next</a><span>|</span><label class="collapse" for="c-39899346">[-]</label><label class="expand" for="c-39899346">[30 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve just started this video, but already have a question if anyone&#x27;s familiar with GPT workings - I thought that these models chose the next word based on what&#x27;s most likely. But if they choose based on &quot;one of the likely&quot; words, could (in general) that not lead to a situation where the list of predictions for the next word are much less likely? Running possibilities of &quot;two words together&quot;, then, would be more beneficial if computationally possible (and so on for 3, 4 and n words). Does this exist?<p>(I realize that choosing the most likely word wouldn&#x27;t necessarily solve the issue, but choosing the most likely phrase possibly might.)<p>Edit, post seeing the video and comments: it&#x27;s beam search, along with temperature to control these things.</div><br/><div id="39903086" class="c"><input type="checkbox" id="c-39903086" checked=""/><div class="controls bullet"><span class="by">authorfly</span><span>|</span><a href="#39899346">parent</a><span>|</span><a href="#39899434">next</a><span>|</span><label class="collapse" for="c-39903086">[-]</label><label class="expand" for="c-39903086">[1 more]</label></div><br/><div class="children"><div class="content">In practice, beam search doesn&#x27;t seem to work well for generative models.<p>Temperature and top_k (two very similar parameters) were both introduced to account for the fact that human text is unpredictable stochastically for each sentence someone might say as such - as shown in this 2021 similar graph&#x2F;reproduction of an older graph from the 2018&#x2F;2019 HF documentation: <a href="https:&#x2F;&#x2F;lilianweng.github.io&#x2F;posts&#x2F;2021-01-02-controllable-text-generation&#x2F;beam_search_less_surprising.png" rel="nofollow">https:&#x2F;&#x2F;lilianweng.github.io&#x2F;posts&#x2F;2021-01-02-controllable-t...</a><p>It could be that beam search with much longer length does turn out to be better or some merging of the techniques works well, but I don&#x27;t think so. The query-key-value part of transformers is focused on a single total in many ways - in relation to the overall context. The architecture is not meant for longer forms as such - there is no default &quot;two token&quot; system. And with 50k-100k tokens in most GPT models, you would be looking at 50k*50k = A great deal more parameters and then issues with sparsity of data.<p>Just everything about GPT models (e.g. learned positional encodings&#x2F;embeddings depending on the model iteration) is so focused on bringing the richness of a single token or single token index that the architecture is not designed for beam search like this one could say. Without considering the training complications.</div><br/></div></div><div id="39899434" class="c"><input type="checkbox" id="c-39899434" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#39899346">parent</a><span>|</span><a href="#39903086">prev</a><span>|</span><a href="#39902020">next</a><span>|</span><label class="collapse" for="c-39899434">[-]</label><label class="expand" for="c-39899434">[19 more]</label></div><br/><div class="children"><div class="content">The temperature setting is used to select how rare of a next token is possible. If set to 0 the. The top of the likely list is chosen, if set greater than 0 then some lower probability tokens may be chosen.</div><br/><div id="39900449" class="c"><input type="checkbox" id="c-39900449" checked=""/><div class="controls bullet"><span class="by">DrawTR</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899434">parent</a><span>|</span><a href="#39900296">next</a><span>|</span><label class="collapse" for="c-39900449">[-]</label><label class="expand" for="c-39900449">[13 more]</label></div><br/><div class="children"><div class="content">Can this be potentially dangerous -- e.g. if a user types &quot;The answer to the expression 2 + 2 is&quot;, isn&#x27;t there a chance it chooses an output beyond the most likely one?</div><br/><div id="39901017" class="c"><input type="checkbox" id="c-39901017" checked=""/><div class="controls bullet"><span class="by">weitendorf</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900449">parent</a><span>|</span><a href="#39900722">next</a><span>|</span><label class="collapse" for="c-39901017">[-]</label><label class="expand" for="c-39901017">[5 more]</label></div><br/><div class="children"><div class="content">Yes, although it&#x27;s also possible that the most likely token is incorrect and perhaps the next 4 most likely tokens would lead to a correct answer.<p>For example if you ask a model what is 0^0, the highest probability output may be &quot;1&quot;, which is incorrect. The next most probable outputs may be words like &quot;although&quot;, &quot;because&quot;, &quot;Due to&quot;, &quot;unfortunately&quot;, etc. as the model prepares to explain to the user that the value of the expression is undefined; because there are many more ways to express and explain the undefined answer than there are to express a naively incorrect answer, the correct answer is split across more tokens so that even if eg the softmax value of &quot;1&quot; is 0.1 and across &quot;although&quot;+&quot;because&quot;+&quot;due to&quot;+&quot;unfortunately&quot;&gt;0.3, at temperature of 0, &quot;1&quot; gets chosen. At slightly higher temperatures, sampling across all outputs would increase the probability of a correct answer.<p>So it&#x27;s true that increasing the temperature increases the probability that the model outputs tokens other than the single-most-likely token, but that might be what you want. Temperature purely controls the distribution of tokens, not &quot;answers&quot;.</div><br/><div id="39901595" class="c"><input type="checkbox" id="c-39901595" checked=""/><div class="controls bullet"><span class="by">Newlaptop</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39901017">parent</a><span>|</span><a href="#39900722">next</a><span>|</span><label class="collapse" for="c-39901595">[-]</label><label class="expand" for="c-39901595">[4 more]</label></div><br/><div class="children"><div class="content">Not sure if you were making a joke, but 0^0 is often defined as 1.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zero_to_the_power_of_zero" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zero_to_the_power_of_zero</a></div><br/><div id="39903602" class="c"><input type="checkbox" id="c-39903602" checked=""/><div class="controls bullet"><span class="by">zild3d</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39901595">parent</a><span>|</span><a href="#39902187">next</a><span>|</span><label class="collapse" for="c-39903602">[-]</label><label class="expand" for="c-39903602">[1 more]</label></div><br/><div class="children"><div class="content">perhaps a hallucination</div><br/></div></div><div id="39902187" class="c"><input type="checkbox" id="c-39902187" checked=""/><div class="controls bullet"><span class="by">weitendorf</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39901595">parent</a><span>|</span><a href="#39903602">prev</a><span>|</span><a href="#39900722">next</a><span>|</span><label class="collapse" for="c-39902187">[-]</label><label class="expand" for="c-39902187">[2 more]</label></div><br/><div class="children"><div class="content">I honestly had forgot that, if I ever knew it. But I think the point stands that in many contexts you&#x27;d rather have the nuances of this kind of thing explained to you - able to represented by many different sequences of tokens, each individually being low probability - instead of simply taking the single-highest probability token &quot;1&quot;.</div><br/><div id="39903608" class="c"><input type="checkbox" id="c-39903608" checked=""/><div class="controls bullet"><span class="by">zild3d</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39902187">parent</a><span>|</span><a href="#39900722">next</a><span>|</span><label class="collapse" for="c-39903608">[-]</label><label class="expand" for="c-39903608">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d rather it recognize it should enter a calculator mode to evaluate the expression, and then can give context with the normal GPT behavior</div><br/></div></div></div></div></div></div></div></div><div id="39900722" class="c"><input type="checkbox" id="c-39900722" checked=""/><div class="controls bullet"><span class="by">Habgdnv</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900449">parent</a><span>|</span><a href="#39901017">prev</a><span>|</span><a href="#39901297">next</a><span>|</span><label class="collapse" for="c-39900722">[-]</label><label class="expand" for="c-39900722">[2 more]</label></div><br/><div class="children"><div class="content">Unless you screw something, a different next token does not mean wrong answer. Examples:<p>(80% of the time) The answer to the expression 2 + 2 is <i>4</i><p>(15% of the time) The answer to the expression 2 + 2 is <i>Four</i><p>(5% of the time) The answer to the expression 2 + 2 is <i>certainly</i><p>(95% of the time) The answer to the expression 2 + 2 is certainly <i>Four</i><p>This is how you can asp ChatGPT the same question few times and it can give you different words each time, and still be correct.</div><br/><div id="39900959" class="c"><input type="checkbox" id="c-39900959" checked=""/><div class="controls bullet"><span class="by">weitendorf</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900722">parent</a><span>|</span><a href="#39901297">next</a><span>|</span><label class="collapse" for="c-39900959">[-]</label><label class="expand" for="c-39900959">[1 more]</label></div><br/><div class="children"><div class="content">That assumes that the model is assigning vanishingly small weights to truly incorrect answers, which doesn&#x27;t necessarily hold up in practice. So I think &quot;Unless you screw something&quot; is doing a lot of work there<p>I think a more correct explanation would be that increasing temperature doesn&#x27;t necessarily increase the probability of a truly incorrect answer proportionately to the temperature increase (because the same correct answer could be represented by many different sequences of tokens), but if the model assigns a non-zero value to any incorrect output after applying softmax (which it most likely does), increasing the temperature does increase the probability of that incorrect output being returned.</div><br/></div></div></div></div><div id="39901297" class="c"><input type="checkbox" id="c-39901297" checked=""/><div class="controls bullet"><span class="by">x-complexity</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900449">parent</a><span>|</span><a href="#39900722">prev</a><span>|</span><a href="#39902644">next</a><span>|</span><label class="collapse" for="c-39901297">[-]</label><label class="expand" for="c-39901297">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Can this be potentially dangerous -- e.g. if a user types &quot;The answer to the expression 2 + 2 is&quot;, isn&#x27;t there a chance it chooses an output beyond the most likely one?<p>This is where the semi-ambiguity of the human languages helps a lot with.<p>There are multiple ways to answer with &quot;4&quot; that are acceptable, meaning that it just needs to be close enough to the desired outcome to work. This means that there isn&#x27;t a single point that needs to be precisely aimed at, but a broader plot of space that&#x27;s relatively easier to hit.<p>The hefty tolerances, redundancies, &amp; general lossiness of the human language act as a metaphorical gravity well to drag LLMs to the most probable answer.</div><br/></div></div><div id="39902644" class="c"><input type="checkbox" id="c-39902644" checked=""/><div class="controls bullet"><span class="by">pelillian</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900449">parent</a><span>|</span><a href="#39901297">prev</a><span>|</span><a href="#39900521">next</a><span>|</span><label class="collapse" for="c-39902644">[-]</label><label class="expand" for="c-39902644">[1 more]</label></div><br/><div class="children"><div class="content">Thatâs why we use top p and top k! They limit the probability space to a certain % or number of tokens ordered by likelihood</div><br/></div></div><div id="39900521" class="c"><input type="checkbox" id="c-39900521" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900449">parent</a><span>|</span><a href="#39902644">prev</a><span>|</span><a href="#39903061">next</a><span>|</span><label class="collapse" for="c-39900521">[-]</label><label class="expand" for="c-39900521">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but the chance is quite small if the gap between &quot;4&quot; and any other token is quite large.</div><br/></div></div><div id="39903061" class="c"><input type="checkbox" id="c-39903061" checked=""/><div class="controls bullet"><span class="by">MrYellowP</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900449">parent</a><span>|</span><a href="#39900521">prev</a><span>|</span><a href="#39900296">next</a><span>|</span><label class="collapse" for="c-39903061">[-]</label><label class="expand" for="c-39903061">[2 more]</label></div><br/><div class="children"><div class="content">&gt; potentially dangerous<p>&gt; 2 + 2<p>You <i>really</i> couldn&#x27;t come up with an actual example of something that would be <i>dangerous?</i> I&#x27;d appreciate that, because I&#x27;m not seeing reason to believe that an &quot;output beyond the most likely one&quot; output would end up ever being <i>dangerous</i>, as in, harming someone or putting someone&#x27;s life at risk.<p>Thanks.</div><br/><div id="39903175" class="c"><input type="checkbox" id="c-39903175" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39903061">parent</a><span>|</span><a href="#39900296">next</a><span>|</span><label class="collapse" for="c-39903175">[-]</label><label class="expand" for="c-39903175">[1 more]</label></div><br/><div class="children"><div class="content">That depends on how many people are putting blind faith in terrible AI. If it&#x27;s your doctor or your parole board, AI making a mistake could be horrible for you.</div><br/></div></div></div></div></div></div><div id="39900296" class="c"><input type="checkbox" id="c-39900296" checked=""/><div class="controls bullet"><span class="by">davekeck</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899434">parent</a><span>|</span><a href="#39900449">prev</a><span>|</span><a href="#39900075">next</a><span>|</span><label class="collapse" for="c-39900296">[-]</label><label class="expand" for="c-39900296">[3 more]</label></div><br/><div class="children"><div class="content">&gt; then some lower probability tokens may be chosen<p>Can you explain how it chooses one of the lower-probability tokens? Is it just random?</div><br/><div id="39900428" class="c"><input type="checkbox" id="c-39900428" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900296">parent</a><span>|</span><a href="#39900075">next</a><span>|</span><label class="collapse" for="c-39900428">[-]</label><label class="expand" for="c-39900428">[2 more]</label></div><br/><div class="children"><div class="content">Reducing temperature reduces the impact of differences between raw output values giving a higher probability to pick other tokens.</div><br/><div id="39901373" class="c"><input type="checkbox" id="c-39901373" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39900428">parent</a><span>|</span><a href="#39900075">next</a><span>|</span><label class="collapse" for="c-39901373">[-]</label><label class="expand" for="c-39901373">[1 more]</label></div><br/><div class="children"><div class="content">Oops backwards. Increasing temperature...</div><br/></div></div></div></div></div></div><div id="39900075" class="c"><input type="checkbox" id="c-39900075" checked=""/><div class="controls bullet"><span class="by">not_a_dane</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899434">parent</a><span>|</span><a href="#39900296">prev</a><span>|</span><a href="#39899645">next</a><span>|</span><label class="collapse" for="c-39900075">[-]</label><label class="expand" for="c-39900075">[1 more]</label></div><br/><div class="children"><div class="content">It is the part of softmax layer, but not all the time.</div><br/></div></div><div id="39899645" class="c"><input type="checkbox" id="c-39899645" checked=""/><div class="controls bullet"><span class="by">user_7832</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899434">parent</a><span>|</span><a href="#39900075">prev</a><span>|</span><a href="#39902020">next</a><span>|</span><label class="collapse" for="c-39899645">[-]</label><label class="expand" for="c-39899645">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, learnt something new today!</div><br/></div></div></div></div><div id="39902020" class="c"><input type="checkbox" id="c-39902020" checked=""/><div class="controls bullet"><span class="by">ahzhou</span><span>|</span><a href="#39899346">parent</a><span>|</span><a href="#39899434">prev</a><span>|</span><a href="#39899422">next</a><span>|</span><label class="collapse" for="c-39902020">[-]</label><label class="expand" for="c-39902020">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this is a fundamental weakness with LLMs. Unfortunately this is likely unsolvable because the search space is exponential. Techniques like beam search help, but can only introduce a constant scaling factor.<p>That said, LLM reach their current performance despite this limitation.</div><br/></div></div><div id="39899422" class="c"><input type="checkbox" id="c-39899422" checked=""/><div class="controls bullet"><span class="by">mvsin</span><span>|</span><a href="#39899346">parent</a><span>|</span><a href="#39902020">prev</a><span>|</span><a href="#39900890">next</a><span>|</span><label class="collapse" for="c-39899422">[-]</label><label class="expand" for="c-39899422">[5 more]</label></div><br/><div class="children"><div class="content">Something like this does exist, production systems rarely use greedy search but have more holistic search algorithms.<p>An example is Beam Search:<a href="https:&#x2F;&#x2F;www.width.ai&#x2F;post&#x2F;what-is-beam-search" rel="nofollow">https:&#x2F;&#x2F;www.width.ai&#x2F;post&#x2F;what-is-beam-search</a><p>Essentially we keep a window of probabilities of predicted tokens to improve the final quality of output.</div><br/><div id="39899643" class="c"><input type="checkbox" id="c-39899643" checked=""/><div class="controls bullet"><span class="by">user_7832</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899422">parent</a><span>|</span><a href="#39901677">next</a><span>|</span><label class="collapse" for="c-39899643">[-]</label><label class="expand" for="c-39899643">[3 more]</label></div><br/><div class="children"><div class="content">Thanks, that&#x27;s exactly what I was looking for! Any idea if it&#x27;s possible to use beam search on local models like mistral? It sounds like the choice of beam search vs say top-p or top-k should be in the software and not embedded, right?</div><br/><div id="39900079" class="c"><input type="checkbox" id="c-39900079" checked=""/><div class="controls bullet"><span class="by">activatedgeek</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899643">parent</a><span>|</span><a href="#39899848">next</a><span>|</span><label class="collapse" for="c-39900079">[-]</label><label class="expand" for="c-39900079">[1 more]</label></div><br/><div class="children"><div class="content">If you use HuggingFace models, then a few simpler decoding algorithms are already implemented for `generate` method of all supported models.<p>Here is a blog post that describes it: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;how-to-generate" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;how-to-generate</a>.<p>I will warn you though that beam search is typically what you do NOT want. Beam search approximately optimizes for the &quot;highest likely sequence at the token level.&quot; This is rarely what you need in practice with open-ended generations (e.g. a question-answering chat bot). In practice, you need &quot;highest likely semantic sequence,&quot; which is much harder problem.<p>Of course, various approximations for semantic alignment are currently in the literature, but still a wide open problem.</div><br/></div></div><div id="39899848" class="c"><input type="checkbox" id="c-39899848" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899643">parent</a><span>|</span><a href="#39900079">prev</a><span>|</span><a href="#39901677">next</a><span>|</span><label class="collapse" for="c-39899848">[-]</label><label class="expand" for="c-39899848">[1 more]</label></div><br/><div class="children"><div class="content">This is actually a great question for which I found an interesting attempt: <a href="https:&#x2F;&#x2F;andys.page&#x2F;posts&#x2F;llm_sampling_strategies&#x2F;" rel="nofollow">https:&#x2F;&#x2F;andys.page&#x2F;posts&#x2F;llm_sampling_strategies&#x2F;</a><p>(No affiliation)</div><br/></div></div></div></div><div id="39901677" class="c"><input type="checkbox" id="c-39901677" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#39899346">root</a><span>|</span><a href="#39899422">parent</a><span>|</span><a href="#39899643">prev</a><span>|</span><a href="#39900890">next</a><span>|</span><label class="collapse" for="c-39901677">[-]</label><label class="expand" for="c-39901677">[1 more]</label></div><br/><div class="children"><div class="content">&gt; production systems rarely use greedy search<p>I have no idea why you say this. Most of our pipelines will run greedy, for reproducibility.<p>Maybe we turn the temp up <i>if</i> we are returning conversational text back to a user.</div><br/></div></div></div></div><div id="39900890" class="c"><input type="checkbox" id="c-39900890" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#39899346">parent</a><span>|</span><a href="#39899422">prev</a><span>|</span><a href="#39900272">next</a><span>|</span><label class="collapse" for="c-39900890">[-]</label><label class="expand" for="c-39900890">[1 more]</label></div><br/><div class="children"><div class="content">Thereâs some fancier stuff too like techniques that take into account where recent tokens were drawn from in the distribution and update either the top_p or the temperature so that sequences of tokens have a minimum unlikeliness. Beam search is less common with really large models because the computation is really expensive.</div><br/></div></div><div id="39900272" class="c"><input type="checkbox" id="c-39900272" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#39899346">parent</a><span>|</span><a href="#39900890">prev</a><span>|</span><a href="#39899475">next</a><span>|</span><label class="collapse" for="c-39900272">[-]</label><label class="expand" for="c-39900272">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a whole bunch of different normalization and sampling techniques that you can perform that can alter the quality or expressiveness of the model, e.g. <a href="https:&#x2F;&#x2F;docs.sillytavern.app&#x2F;usage&#x2F;common-settings&#x2F;#sampler-parameters" rel="nofollow">https:&#x2F;&#x2F;docs.sillytavern.app&#x2F;usage&#x2F;common-settings&#x2F;#sampler-...</a></div><br/></div></div></div></div><div id="39898356" class="c"><input type="checkbox" id="c-39898356" checked=""/><div class="controls bullet"><span class="by">lucidrains</span><span>|</span><a href="#39899346">prev</a><span>|</span><a href="#39899833">next</a><span>|</span><label class="collapse" for="c-39898356">[-]</label><label class="expand" for="c-39898356">[6 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t think of anyone better to teach attention mechanism to the masses. This is a dream come true</div><br/><div id="39898758" class="c"><input type="checkbox" id="c-39898758" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#39898356">parent</a><span>|</span><a href="#39899833">next</a><span>|</span><label class="collapse" for="c-39898758">[-]</label><label class="expand" for="c-39898758">[5 more]</label></div><br/><div class="children"><div class="content">Incredible. This 3B1B series was started 6 years ago and keeps going today with chapter 5.<p>If you haven&#x27;t seen the first few chapters, I cannot recommend enough.</div><br/><div id="39899662" class="c"><input type="checkbox" id="c-39899662" checked=""/><div class="controls bullet"><span class="by">user_7832</span><span>|</span><a href="#39898356">root</a><span>|</span><a href="#39898758">parent</a><span>|</span><a href="#39899833">next</a><span>|</span><label class="collapse" for="c-39899662">[-]</label><label class="expand" for="c-39899662">[4 more]</label></div><br/><div class="children"><div class="content">Would you be able to compare them to Andrew Ng&#x27;s course?</div><br/><div id="39900888" class="c"><input type="checkbox" id="c-39900888" checked=""/><div class="controls bullet"><span class="by">abraxas</span><span>|</span><a href="#39898356">root</a><span>|</span><a href="#39899662">parent</a><span>|</span><a href="#39899729">next</a><span>|</span><label class="collapse" for="c-39900888">[-]</label><label class="expand" for="c-39900888">[1 more]</label></div><br/><div class="children"><div class="content">I personally preferred Andrej Karpathy&#x27;s CS231n taught by him and his private videos about neural nets in general and transformers in particular. He has a youtube vid where he builds one from scratch in Python!<p>3BlueOneBrown videos are a great complement to Karpathy&#x27;s lectures to aid in visualising what is going on.</div><br/></div></div><div id="39899729" class="c"><input type="checkbox" id="c-39899729" checked=""/><div class="controls bullet"><span class="by">sk11001</span><span>|</span><a href="#39898356">root</a><span>|</span><a href="#39899662">parent</a><span>|</span><a href="#39900888">prev</a><span>|</span><a href="#39900037">next</a><span>|</span><label class="collapse" for="c-39899729">[-]</label><label class="expand" for="c-39899729">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not really comparable - if you&#x27;re wondering if you should do one or the other, you should do both.</div><br/></div></div><div id="39900037" class="c"><input type="checkbox" id="c-39900037" checked=""/><div class="controls bullet"><span class="by">ctrw</span><span>|</span><a href="#39898356">root</a><span>|</span><a href="#39899662">parent</a><span>|</span><a href="#39899729">prev</a><span>|</span><a href="#39899833">next</a><span>|</span><label class="collapse" for="c-39900037">[-]</label><label class="expand" for="c-39900037">[1 more]</label></div><br/><div class="children"><div class="content">The way you compare a technical drawing of a steam engine to The Fighting Temeraire oil painting.</div><br/></div></div></div></div></div></div></div></div><div id="39899833" class="c"><input type="checkbox" id="c-39899833" checked=""/><div class="controls bullet"><span class="by">Vespasian</span><span>|</span><a href="#39898356">prev</a><span>|</span><a href="#39898873">next</a><span>|</span><label class="collapse" for="c-39899833">[-]</label><label class="expand" for="c-39899833">[4 more]</label></div><br/><div class="children"><div class="content">If you liked that, Andrej karpathy has a few interesting videos on his channels explaining Neural Networks and their inner workings which are aimed at people who know how to program.</div><br/><div id="39900130" class="c"><input type="checkbox" id="c-39900130" checked=""/><div class="controls bullet"><span class="by">jtonz</span><span>|</span><a href="#39899833">parent</a><span>|</span><a href="#39898873">next</a><span>|</span><label class="collapse" for="c-39900130">[-]</label><label class="expand" for="c-39900130">[3 more]</label></div><br/><div class="children"><div class="content">As a reasonably experienced programmer that has watched Andrej&#x27;s videos the one thing I would recommend is that they not be used as a starting point to learn neural networks but as a reinforcement or enhancement method once you know the fundamentals.<p>I was ignorant enough to try and jump straight in to his videos and despite him recommending I watch his preceeding videos I incorrectly assumed I could figure it out as I went. There is verbiage in there that you simply must know to get the most out of it. After giving up, going away and filling in the gaps though some other learnings, I went back and his videos become (understandably) massively more valueable for me.<p>I would strongly recommend anyone else wanting to learn neural networks that they learn from my mistake.</div><br/><div id="39900390" class="c"><input type="checkbox" id="c-39900390" checked=""/><div class="controls bullet"><span class="by">kovrik</span><span>|</span><a href="#39899833">root</a><span>|</span><a href="#39900130">parent</a><span>|</span><a href="#39898873">next</a><span>|</span><label class="collapse" for="c-39900390">[-]</label><label class="expand" for="c-39900390">[2 more]</label></div><br/><div class="children"><div class="content">Could you please share what other learning materials you used?</div><br/><div id="39902838" class="c"><input type="checkbox" id="c-39902838" checked=""/><div class="controls bullet"><span class="by">6mian</span><span>|</span><a href="#39899833">root</a><span>|</span><a href="#39900390">parent</a><span>|</span><a href="#39898873">next</a><span>|</span><label class="collapse" for="c-39902838">[-]</label><label class="expand" for="c-39902838">[1 more]</label></div><br/><div class="children"><div class="content">For me 3brown1blue series: <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=aircAruvnKk" rel="nofollow">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=aircAruvnKk</a> was an excellent introduction that made Andrej&#x27;s videos understandable. Then I did 3 first chapters of fastai book, but found it too high level, while I was interested in how things works under the hood.<p>Going through Andrej&#x27;s makemore tutorials required quite a lot of time but it&#x27;s definitely worth it. I used free tier of Google Colab until the last one.<p>Pausing the video a lot after he explains what he plans to do and trying to do it by myself was a very rewarding way to learn, with a lot of &quot;aha&quot; moments.</div><br/></div></div></div></div></div></div></div></div><div id="39898873" class="c"><input type="checkbox" id="c-39898873" checked=""/><div class="controls bullet"><span class="by">yinser</span><span>|</span><a href="#39899833">prev</a><span>|</span><a href="#39901873">next</a><span>|</span><label class="collapse" for="c-39898873">[-]</label><label class="expand" for="c-39898873">[1 more]</label></div><br/><div class="children"><div class="content">What an unbelievable salve for all the April Fool&#x27;s content. Pipe this directly into my veins.</div><br/></div></div><div id="39901873" class="c"><input type="checkbox" id="c-39901873" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#39898873">prev</a><span>|</span><a href="#39900315">next</a><span>|</span><label class="collapse" for="c-39901873">[-]</label><label class="expand" for="c-39901873">[1 more]</label></div><br/><div class="children"><div class="content">Also relevant would be this interactive visualization: <a href="https:&#x2F;&#x2F;bbycroft.net&#x2F;llm" rel="nofollow">https:&#x2F;&#x2F;bbycroft.net&#x2F;llm</a><p>Prior discussion: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38505211">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38505211</a></div><br/></div></div><div id="39900315" class="c"><input type="checkbox" id="c-39900315" checked=""/><div class="controls bullet"><span class="by">throwawayk7h</span><span>|</span><a href="#39901873">prev</a><span>|</span><a href="#39900281">next</a><span>|</span><label class="collapse" for="c-39900315">[-]</label><label class="expand" for="c-39900315">[2 more]</label></div><br/><div class="children"><div class="content">The next token is taken by sampling the logits in the final column after unembedding. But isn&#x27;t that just the last token again? Or is the matrix resized to N+1 at some step?</div><br/><div id="39900631" class="c"><input type="checkbox" id="c-39900631" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#39900315">parent</a><span>|</span><a href="#39900281">next</a><span>|</span><label class="collapse" for="c-39900631">[-]</label><label class="expand" for="c-39900631">[1 more]</label></div><br/><div class="children"><div class="content">There is an end-of-sequence token appended to the input sequence, and this is what is transformed into the predicted next token.</div><br/></div></div></div></div><div id="39900281" class="c"><input type="checkbox" id="c-39900281" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#39900315">prev</a><span>|</span><a href="#39900051">next</a><span>|</span><label class="collapse" for="c-39900281">[-]</label><label class="expand" for="c-39900281">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t wait for the next videos. I think I&#x27;ll finally be able to internalize and understand how these things work.</div><br/></div></div><div id="39900051" class="c"><input type="checkbox" id="c-39900051" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#39900281">prev</a><span>|</span><label class="collapse" for="c-39900051">[-]</label><label class="expand" for="c-39900051">[1 more]</label></div><br/><div class="children"><div class="content">3B1B is one of the best stem educators in YouTube.</div><br/></div></div></div></div></div></div></div></body></html>