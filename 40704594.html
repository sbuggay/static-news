<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718701258927" as="style"/><link rel="stylesheet" href="styles.css?v=1718701258927"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.tomshardware.com/tech-industry/optical-pcie-70-connection-hits-a-blazing-128-gts">Optical PCIe 7.0 connection hits 128 GT/s</a> <span class="domain">(<a href="https://www.tomshardware.com">www.tomshardware.com</a>)</span></div><div class="subtext"><span>WithinReason</span> | <span>61 comments</span></div><br/><div><div id="40712649" class="c"><input type="checkbox" id="c-40712649" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#40712220">next</a><span>|</span><label class="collapse" for="c-40712649">[-]</label><label class="expand" for="c-40712649">[18 more]</label></div><br/><div class="children"><div class="content">So I guess what this makes me wonder is: Why are we using electrical signals to connect the data lanes between components and computers these days, rather than moving everything to optical for data movement (obviously power would stay electrical, but that&#x27;s already on separate lines)? I assume there&#x27;s an element of cost, and once the photons get where they&#x27;re going they have to be turned back into electrical signals to actually be used until such time as we get around to getting pure light based computers working (someday but not yet...), but that must not overwhelm the advantages or we wouldn&#x27;t be looking at this being developed.</div><br/><div id="40713112" class="c"><input type="checkbox" id="c-40713112" checked=""/><div class="controls bullet"><span class="by">AceJohnny2</span><span>|</span><a href="#40712649">parent</a><span>|</span><a href="#40712948">next</a><span>|</span><label class="collapse" for="c-40713112">[-]</label><label class="expand" for="c-40713112">[12 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I assume there&#x27;s an element of cost, and once the photons get where they&#x27;re going they have to be turned back into electrical signals to actually be used until such time as we get around to getting pure light based computers working (someday but not yet...)</i><p>You got it. We can&#x27;t make optical transceivers as good as electrical ones. Not as small or power-efficient.<p>They require significantly different fabrication processes, and we don&#x27;t know how to fab them into the same chip as electrical ones. I mean: you can either have photonics, or performant digital (or analog) electronics.<p>We&#x27;ve gotten really, <i>really</i> good at making small electronics, per the latest tech coming out of Intel &amp; TSMC. We are... not that good at making photonics.</div><br/><div id="40713801" class="c"><input type="checkbox" id="c-40713801" checked=""/><div class="controls bullet"><span class="by">hughesjj</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40713112">parent</a><span>|</span><a href="#40712948">next</a><span>|</span><label class="collapse" for="c-40713801">[-]</label><label class="expand" for="c-40713801">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Not as small or power-efficient.<p>I wonder what the latency for switching medium is these days too (for the super small transceivers).  To my understanding optical is better for attenuation than electric (less noise, and thus easier to shove more frequencies and higher frequencies on the same pipe), and can be faster (both medium dependent, neither yet approaching the upper bound of c).<p>I&#x27;m imaging the latency incurred by the transceiver is eventually offset from the gains in the signal path (for signal paths relevant to circuit boards and ICs)</div><br/><div id="40713907" class="c"><input type="checkbox" id="c-40713907" checked=""/><div class="controls bullet"><span class="by">cycomanic</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40713801">parent</a><span>|</span><a href="#40712948">next</a><span>|</span><label class="collapse" for="c-40713907">[-]</label><label class="expand" for="c-40713907">[10 more]</label></div><br/><div class="children"><div class="content">Depending how you do the actual modulation, optical modulation does not add any significant latency (there can be no processing involved and the extra transmission length you&#x27;d need for the modulation (i.e. Electroopotic conversion, rf amplifiers...) is negligible.<p>The big issue is really 1. Photonic waveguides are much larger than electronic ones (due to the wavelength) 2. You loose dynamic range and in EO conversion (shot noise is significant at optical frequencies) 3. Co integration of optics and photonics components is nontrivial due to the different materials and processes. 4. Power efficiency of EO conversion is also not that great.<p>Where photonics shines is for transmission of high frequencies (i.e. a lot of data) over long distances and being immune to EM interference. So there is certainly a tradeoff for at what transmission distances to go optical and as data rates keep going up the tradeoff length has become shorter and shorter. Intel, Nvidia, AMD et al. All do research into optical interconnect.</div><br/><div id="40715187" class="c"><input type="checkbox" id="c-40715187" checked=""/><div class="controls bullet"><span class="by">lloeki</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40713907">parent</a><span>|</span><a href="#40714147">next</a><span>|</span><label class="collapse" for="c-40715187">[-]</label><label class="expand" for="c-40715187">[3 more]</label></div><br/><div class="children"><div class="content">I seem to recall information travels slower in fiber (optical) vs wires (electric), resp. ~2c&#x2F;3 vs ~c, or am I remembering it wrong? Or is it a significantly different optical medium?<p>If so, does that matter at all here? Dunno if that holds up for such kind of devices and&#x2F;or at these scales (much shorter distance, but also much higher speed).</div><br/><div id="40715301" class="c"><input type="checkbox" id="c-40715301" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40715187">parent</a><span>|</span><a href="#40714147">next</a><span>|</span><label class="collapse" for="c-40715301">[-]</label><label class="expand" for="c-40715301">[2 more]</label></div><br/><div class="children"><div class="content">IIRC the lower speed in fiber-optic cables has to do with the the refractive index of the glass, and maybe some bouncing introduced by curvature.<p>I&#x27;m not sure what kind of refractive indices are possible in much smaller photonic circuits, particularly if it&#x27;s not practical to develop and run everything in a permanent vacuum.</div><br/></div></div></div></div><div id="40714147" class="c"><input type="checkbox" id="c-40714147" checked=""/><div class="controls bullet"><span class="by">aborsy</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40713907">parent</a><span>|</span><a href="#40715187">prev</a><span>|</span><a href="#40712948">next</a><span>|</span><label class="collapse" for="c-40714147">[-]</label><label class="expand" for="c-40714147">[6 more]</label></div><br/><div class="children"><div class="content">Photonic wavelengths are shorter than electronic wavelengths. Why are photonics waveguides bigger then?</div><br/><div id="40714222" class="c"><input type="checkbox" id="c-40714222" checked=""/><div class="controls bullet"><span class="by">murkt</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40714147">parent</a><span>|</span><a href="#40714304">next</a><span>|</span><label class="collapse" for="c-40714222">[-]</label><label class="expand" for="c-40714222">[3 more]</label></div><br/><div class="children"><div class="content">What’s an electronic wavelength in this context? What’s its size? Photonic ones I assume are in near-IR, on the order of a micrometer.</div><br/><div id="40715255" class="c"><input type="checkbox" id="c-40715255" checked=""/><div class="controls bullet"><span class="by">aborsy</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40714222">parent</a><span>|</span><a href="#40714304">next</a><span>|</span><label class="collapse" for="c-40715255">[-]</label><label class="expand" for="c-40715255">[2 more]</label></div><br/><div class="children"><div class="content">Optical fiber communication operates at 1.55 micro meters or 193THz. Electronics operates in the electromagnetic spectrum or at GHz. There might be no fixed size or wavelength in either case, but the shortest wavelength for radio signals that can be transmitted with acceptable loss with today’s technology is mili meters.<p>Which brings the question: why operating wavelengths are smaller but “waveguides” are bigger in optical fiber communication. In fact, fiber itself is a waveguide and its diameter is tens of micro meters.</div><br/><div id="40715452" class="c"><input type="checkbox" id="c-40715452" checked=""/><div class="controls bullet"><span class="by">silizium</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40715255">parent</a><span>|</span><a href="#40714304">next</a><span>|</span><label class="collapse" for="c-40715452">[-]</label><label class="expand" for="c-40715452">[1 more]</label></div><br/><div class="children"><div class="content">The full optical wave is contained in the dielectric conductor. This conductor needs it&#x27;s minimum cross section such that the wave can propagate. If it is too small then the wave can not propagate. Also there is a maximum cross section if you want single mode operation.<p>You get to this result if you take the electromagntic wave equation - a partial differential equation - and solve that for your transmission line configuration.<p>The proper analogy in the realm of electrical waveguides is the hollow waveguide. The hollow waveguide supports TE- and TM-modes but not TEM modes just like a dielectric conductor. The size is also a function of the dielectric constant ε.<p>What we mostly use are TEM waveguides like microstrips or coaxial cables. The difference between electrical waveguides that supports TEM modes and waveguides that supports TE&#x2F;TM modes is that the former has two independent potential planes and the latter only one. Also TEM waveguides do not have a lower cutoff frequency. A TEM wave with any frequency can propagate on any microstrip configuration.<p>This is not true for TE&#x2F;TM waves.<p>What&#x27;s important to understand is that for microstrips&#x2F;coaxial cables the power isn&#x27;t transferred in the metal but in the space (dielectric) around the metal. So what happens if you have a second conductor in that space? You get crosstalk! So TEM transmission lines do not contain the wave like hollow waveguides or optical fibers.<p>Now the question, how big is the microstrip? Is it just the width of the signal conductor? No, it is not.</div><br/></div></div></div></div></div></div><div id="40714304" class="c"><input type="checkbox" id="c-40714304" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40714147">parent</a><span>|</span><a href="#40714222">prev</a><span>|</span><a href="#40714743">next</a><span>|</span><label class="collapse" for="c-40714304">[-]</label><label class="expand" for="c-40714304">[1 more]</label></div><br/><div class="children"><div class="content">electrical connections don&#x27;t require electrons fly all the way across dielectric materials</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40712948" class="c"><input type="checkbox" id="c-40712948" checked=""/><div class="controls bullet"><span class="by">asadhaider</span><span>|</span><a href="#40712649">parent</a><span>|</span><a href="#40713112">prev</a><span>|</span><a href="#40712677">next</a><span>|</span><label class="collapse" for="c-40712948">[-]</label><label class="expand" for="c-40712948">[1 more]</label></div><br/><div class="children"><div class="content">So we&#x27;re moving towards crystals from Stargate? Neat</div><br/></div></div><div id="40712677" class="c"><input type="checkbox" id="c-40712677" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#40712649">parent</a><span>|</span><a href="#40712948">prev</a><span>|</span><a href="#40713271">next</a><span>|</span><label class="collapse" for="c-40712677">[-]</label><label class="expand" for="c-40712677">[3 more]</label></div><br/><div class="children"><div class="content">Since thunderbolt is related to PCIe, there&#x27;s this that goes into copper vs optical there: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thunderbolt_(interface)#Copper_vs._optical" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thunderbolt_(interface)#Copper...</a></div><br/><div id="40712989" class="c"><input type="checkbox" id="c-40712989" checked=""/><div class="controls bullet"><span class="by">watersb</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40712677">parent</a><span>|</span><a href="#40713271">next</a><span>|</span><label class="collapse" for="c-40712989">[-]</label><label class="expand" for="c-40712989">[2 more]</label></div><br/><div class="children"><div class="content">Intel&#x27;s product name foe Thunderbolt was initially was &quot;Light Peak&quot;.</div><br/><div id="40714401" class="c"><input type="checkbox" id="c-40714401" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#40712649">root</a><span>|</span><a href="#40712989">parent</a><span>|</span><a href="#40713271">next</a><span>|</span><label class="collapse" for="c-40714401">[-]</label><label class="expand" for="c-40714401">[1 more]</label></div><br/><div class="children"><div class="content">I believe it was originally supposed to be optical.</div><br/></div></div></div></div></div></div><div id="40713271" class="c"><input type="checkbox" id="c-40713271" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#40712649">parent</a><span>|</span><a href="#40712677">prev</a><span>|</span><a href="#40712220">next</a><span>|</span><label class="collapse" for="c-40713271">[-]</label><label class="expand" for="c-40713271">[1 more]</label></div><br/><div class="children"><div class="content">Changing from light to electricity (and vice versa) is relatively slow, expensive, and cumbersome.<p>Additionally, we don’t have a decent way of transferring significant power over fiber optics.<p>So since everything has to have copper power fed to it anyway, unless there is some compelling reason (like distance) to make optical&#x2F;fibers disadvantages worth it, copper only is usually simpler and better.<p>At least for now.</div><br/></div></div></div></div><div id="40712220" class="c"><input type="checkbox" id="c-40712220" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#40712649">prev</a><span>|</span><a href="#40714143">next</a><span>|</span><label class="collapse" for="c-40712220">[-]</label><label class="expand" for="c-40712220">[17 more]</label></div><br/><div class="children"><div class="content">GT&#x2F;s = gigatranfers per second.</div><br/><div id="40712523" class="c"><input type="checkbox" id="c-40712523" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#40712220">parent</a><span>|</span><a href="#40714143">next</a><span>|</span><label class="collapse" for="c-40712523">[-]</label><label class="expand" for="c-40712523">[16 more]</label></div><br/><div class="children"><div class="content">What&#x27;s a transfer? That like a packet or a single bit?</div><br/><div id="40712584" class="c"><input type="checkbox" id="c-40712584" checked=""/><div class="controls bullet"><span class="by">Arnavion</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712523">parent</a><span>|</span><a href="#40714329">next</a><span>|</span><label class="collapse" for="c-40712584">[-]</label><label class="expand" for="c-40712584">[11 more]</label></div><br/><div class="children"><div class="content">One bit, but it&#x27;s a bit of the underlying signal layer which has a 1-2% redundancy over the actual data. PCIe 2.0 and earlier encode 8b data in 10b signal. 3.0 to 5.0 encode 128b data in 130b signal. 6.0 and 7.0 do a more complicated thing: <a href="https:&#x2F;&#x2F;pcisig.com&#x2F;blog&#x2F;pcie%C2%AE-60-specification-webinar-qa-deeper-dive-flit-mode-pam4-and-forward-error-correction-fec" rel="nofollow">https:&#x2F;&#x2F;pcisig.com&#x2F;blog&#x2F;pcie%C2%AE-60-specification-webinar-...</a><p>Also the speed is per lane, eg an x8 slot &#x2F; port &#x2F; device is called that because it has 8 lanes, which all transfer in parallel.</div><br/><div id="40715086" class="c"><input type="checkbox" id="c-40715086" checked=""/><div class="controls bullet"><span class="by">lloeki</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712584">parent</a><span>|</span><a href="#40713642">next</a><span>|</span><label class="collapse" for="c-40715086">[-]</label><label class="expand" for="c-40715086">[1 more]</label></div><br/><div class="children"><div class="content">So, it&#x27;s still bits all the way down? Whether we&#x27;re talking about app, IP, or LL it&#x27;s always bits&#x2F;s, each level bringing a cost due to encapsulation. And then at PHY there&#x27;s baud.<p>Is non-ISO unit &quot;T&quot; &#x2F; &quot;transfer&quot; a marketing term or really specialised jargon? &quot;transfer&quot; just doesn&#x27;t click in my mind, at best &quot;a transfer&quot; (countable) is about moving a sizeable aggregate chunk that has some semantic meaning, not a single fundamental quantum of information.<p>Unrelated: &quot;gigatesla per second&quot; is such a mind-boggling unit.</div><br/></div></div><div id="40713642" class="c"><input type="checkbox" id="c-40713642" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712584">parent</a><span>|</span><a href="#40715086">prev</a><span>|</span><a href="#40714334">next</a><span>|</span><label class="collapse" for="c-40713642">[-]</label><label class="expand" for="c-40713642">[3 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s a bit of the underlying signal layer which has a 1-2% redundancy over the actual data. PCIe 2.0 and earlier encode 8b data in 10b signal. 3.0 to 5.0 encode 128b data in 130b signal. 6.0 and 7.0 do a more complicated thing<p>Though the exact details of the overhead don&#x27;t matter very much.  They add 6% extra bits, good enough.<p>The part I want to call out as complicated&#x2F;confusing is that a PCIe 7.0 lane puts out a voltage 64 billion times per second, but because each voltage is based on two bits that counts as 128 billion &quot;transfers&quot;.</div><br/><div id="40714256" class="c"><input type="checkbox" id="c-40714256" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40713642">parent</a><span>|</span><a href="#40714334">next</a><span>|</span><label class="collapse" for="c-40714256">[-]</label><label class="expand" for="c-40714256">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, the overhead isn&#x27;t a big deal <i>now</i> that the overhead is single digit.  Back when it was 20% in PCIe 2 it was a much bigger discrepancy.</div><br/><div id="40714333" class="c"><input type="checkbox" id="c-40714333" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40714256">parent</a><span>|</span><a href="#40714334">next</a><span>|</span><label class="collapse" for="c-40714333">[-]</label><label class="expand" for="c-40714333">[1 more]</label></div><br/><div class="children"><div class="content">Back then they were adding the overhead on top of the baseline speed, not subtracting it.  With 1 and 2 you got the full 1&#x2F;4 and 1&#x2F;2 gbps of data per lane, but then 3 was only .985 instead of 1.  So I&#x27;d argue that 6% for PCIe 6 and 7 is the most meaningful the overhead has ever been.</div><br/></div></div></div></div></div></div><div id="40712626" class="c"><input type="checkbox" id="c-40712626" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712584">parent</a><span>|</span><a href="#40714334">prev</a><span>|</span><a href="#40714329">next</a><span>|</span><label class="collapse" for="c-40712626">[-]</label><label class="expand" for="c-40712626">[5 more]</label></div><br/><div class="children"><div class="content">Edit: Nope, I misread. As reply notes, 16<i>G</i>B&#x2F;s&#x2F;lane.<p>So... That&#x27;s <i>about</i> 16 terabytes per second per lane. AKA more bandwidth than I can imagine any use for, though I&#x27;m sure we will find ways to take advantage...<p>(Seriously, that&#x27;s enough to move 16 largish laptop drives <i>every second</i>, on a single lane.)</div><br/><div id="40713833" class="c"><input type="checkbox" id="c-40713833" checked=""/><div class="controls bullet"><span class="by">hughesjj</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712626">parent</a><span>|</span><a href="#40712668">next</a><span>|</span><label class="collapse" for="c-40713833">[-]</label><label class="expand" for="c-40713833">[1 more]</label></div><br/><div class="children"><div class="content">Assuming it was 16tb&#x2F;s.... Imagine a JIT data lake loading stuff into main memory like brrr...<p>Actually at that point, a pcie7 nvme would be faster than ddr6<p><a href="https:&#x2F;&#x2F;www.pcworld.com&#x2F;article&#x2F;2237799&#x2F;ddr6-ram-what-you-should-already-know-about-the-upcoming-ram-standard.html" rel="nofollow">https:&#x2F;&#x2F;www.pcworld.com&#x2F;article&#x2F;2237799&#x2F;ddr6-ram-what-you-sh...</a><p>That said, per-pin, 16GB&#x2F;s seems to be the same ballpark as contemporary (to pcie7) main or graphics memory..... Like, actually more if I&#x27;m reading this right?<p><a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;21287&#x2F;jedec-publishes-gddr7-specifications-pam3-ecc-higher-density" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;21287&#x2F;jedec-publishes-gddr7-s...</a></div><br/></div></div><div id="40712668" class="c"><input type="checkbox" id="c-40712668" checked=""/><div class="controls bullet"><span class="by">Arnavion</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712626">parent</a><span>|</span><a href="#40713833">prev</a><span>|</span><a href="#40712893">next</a><span>|</span><label class="collapse" for="c-40712668">[-]</label><label class="expand" for="c-40712668">[2 more]</label></div><br/><div class="children"><div class="content">&gt;So... That&#x27;s about 16 terabytes per second per lane.<p>If you assume 1T&#x2F;s = 1b&#x2F;s, 128GT&#x2F;s is 128Gb&#x2F;s = 16GB&#x2F;s, not 16TB&#x2F;s</div><br/><div id="40712786" class="c"><input type="checkbox" id="c-40712786" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712668">parent</a><span>|</span><a href="#40712893">next</a><span>|</span><label class="collapse" for="c-40712786">[-]</label><label class="expand" for="c-40712786">[1 more]</label></div><br/><div class="children"><div class="content">Oops, yep, I misread that. Thanks for the correction</div><br/></div></div></div></div></div></div></div></div><div id="40712629" class="c"><input type="checkbox" id="c-40712629" checked=""/><div class="controls bullet"><span class="by">jaydeegee</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712523">parent</a><span>|</span><a href="#40714329">prev</a><span>|</span><a href="#40712638">next</a><span>|</span><label class="collapse" for="c-40712629">[-]</label><label class="expand" for="c-40712629">[2 more]</label></div><br/><div class="children"><div class="content">A transfer is 1 action to the size of the width of the channel.</div><br/><div id="40712784" class="c"><input type="checkbox" id="c-40712784" checked=""/><div class="controls bullet"><span class="by">pkaye</span><span>|</span><a href="#40712220">root</a><span>|</span><a href="#40712629">parent</a><span>|</span><a href="#40712638">next</a><span>|</span><label class="collapse" for="c-40712784">[-]</label><label class="expand" for="c-40712784">[1 more]</label></div><br/><div class="children"><div class="content">Then you have to define what an action is.</div><br/></div></div></div></div></div></div></div></div><div id="40714143" class="c"><input type="checkbox" id="c-40714143" checked=""/><div class="controls bullet"><span class="by">Taniwha</span><span>|</span><a href="#40712220">prev</a><span>|</span><a href="#40712753">next</a><span>|</span><label class="collapse" for="c-40714143">[-]</label><label class="expand" for="c-40714143">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth being clear about what Cadence are announcing here (Cadence sells VLSI tooling and libraries) - they have 2 things:<p>- cells for a chip to send&#x2F;receive at 128Gb&#x2F;s - this solution requires 8 of them running in parallel (like 8 PCIe lanes)
- a module that takes 8 lanes in&#x2F;out and drives&#x2F;receives a single fiber</div><br/></div></div><div id="40712753" class="c"><input type="checkbox" id="c-40712753" checked=""/><div class="controls bullet"><span class="by">mistyvales</span><span>|</span><a href="#40714143">prev</a><span>|</span><a href="#40712640">next</a><span>|</span><label class="collapse" for="c-40712753">[-]</label><label class="expand" for="c-40712753">[13 more]</label></div><br/><div class="children"><div class="content">Here I am still on PCI-E 3.0...</div><br/><div id="40712764" class="c"><input type="checkbox" id="c-40712764" checked=""/><div class="controls bullet"><span class="by">Arnavion</span><span>|</span><a href="#40712753">parent</a><span>|</span><a href="#40713462">next</a><span>|</span><label class="collapse" for="c-40712764">[-]</label><label class="expand" for="c-40712764">[5 more]</label></div><br/><div class="children"><div class="content">Most hardware (NVMe drives, GPUs, etc) doesn&#x27;t run at more than 4.0 speeds anyway. The primary advantage of 5.0 and higher is that it&#x27;ll allow that hardware to use fewer CPU lanes, eg what requires 4.0 x4 could use 6.0 x1.</div><br/><div id="40714274" class="c"><input type="checkbox" id="c-40714274" checked=""/><div class="controls bullet"><span class="by">steelbrain</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40712764">parent</a><span>|</span><a href="#40713002">next</a><span>|</span><label class="collapse" for="c-40714274">[-]</label><label class="expand" for="c-40714274">[2 more]</label></div><br/><div class="children"><div class="content">&gt; eg what requires 4.0 x4 could use 6.0 x1<p>FWIW, this is only true for newer hardware. ie if you plugged in a pcie gen3x16 device into a pcie gen4x8 slot, although the bandwidth provided is in the same ballpark, the device will only run at pcie gen3x8.<p>So we&#x27;ll need until the devices upgrade themselves to gen4 in this scenario to make use of higher bandwidth.</div><br/><div id="40714486" class="c"><input type="checkbox" id="c-40714486" checked=""/><div class="controls bullet"><span class="by">Arnavion</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40714274">parent</a><span>|</span><a href="#40713002">next</a><span>|</span><label class="collapse" for="c-40714486">[-]</label><label class="expand" for="c-40714486">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not saying the device itself would negotiate a higher PCIe version. I&#x27;m saying that the 4.0 x4 M.2 NVMe slot on your mobo would map to only one 6.0 CPU lane.</div><br/></div></div></div></div><div id="40713002" class="c"><input type="checkbox" id="c-40713002" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40712764">parent</a><span>|</span><a href="#40714274">prev</a><span>|</span><a href="#40713462">next</a><span>|</span><label class="collapse" for="c-40713002">[-]</label><label class="expand" for="c-40713002">[2 more]</label></div><br/><div class="children"><div class="content">It kinda worked that way in the past too. IIRC, newer AGP graphics cards would be keyed for 4x or 8x slots but would barely use more bandwidth than the original 2x slots provided.</div><br/><div id="40714183" class="c"><input type="checkbox" id="c-40714183" checked=""/><div class="controls bullet"><span class="by">lightedman</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40713002">parent</a><span>|</span><a href="#40713462">next</a><span>|</span><label class="collapse" for="c-40714183">[-]</label><label class="expand" for="c-40714183">[1 more]</label></div><br/><div class="children"><div class="content">We didn&#x27;t really bottleneck AGP8X until nVidia dropped the 200 series GeForce GPU. Then PCI-E was pretty much a requirement.</div><br/></div></div></div></div></div></div><div id="40713462" class="c"><input type="checkbox" id="c-40713462" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#40712753">parent</a><span>|</span><a href="#40712764">prev</a><span>|</span><a href="#40712640">next</a><span>|</span><label class="collapse" for="c-40713462">[-]</label><label class="expand" for="c-40713462">[7 more]</label></div><br/><div class="children"><div class="content">It felt like we were on 3 for a long time, and then all of a sudden got 4 through 6 (and soon 7) in quick succession.  I&#x27;d be curious to know what motivated that - maybe GPGPU taking off?</div><br/><div id="40714326" class="c"><input type="checkbox" id="c-40714326" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40713462">parent</a><span>|</span><a href="#40713976">next</a><span>|</span><label class="collapse" for="c-40714326">[-]</label><label class="expand" for="c-40714326">[1 more]</label></div><br/><div class="children"><div class="content">the big use cases are inter-computer communication and nvme ssds. pcie 4x16 gets you 400 gbps Ethernet. 6x16 will be 1.6 tbps. for SSDs, it&#x27;s the difference between needing 4 and 1 lanes to saturate your bandwidth. a modern 2u server at this point can have an ungodly amount of incredibly fast storage, and can expose all that data to everyone else without a bandwidth bottleneck.</div><br/></div></div><div id="40713976" class="c"><input type="checkbox" id="c-40713976" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40713462">parent</a><span>|</span><a href="#40714326">prev</a><span>|</span><a href="#40713534">next</a><span>|</span><label class="collapse" for="c-40713976">[-]</label><label class="expand" for="c-40713976">[1 more]</label></div><br/><div class="children"><div class="content">Definitely data centre usage of some sort.</div><br/></div></div><div id="40713534" class="c"><input type="checkbox" id="c-40713534" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40713462">parent</a><span>|</span><a href="#40713976">prev</a><span>|</span><a href="#40712640">next</a><span>|</span><label class="collapse" for="c-40713534">[-]</label><label class="expand" for="c-40713534">[4 more]</label></div><br/><div class="children"><div class="content">AI&#x2F;GPU communication is definitely driving it forward now. It is a speed race for how quickly you can move data around.</div><br/><div id="40714280" class="c"><input type="checkbox" id="c-40714280" checked=""/><div class="controls bullet"><span class="by">starspangled</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40713534">parent</a><span>|</span><a href="#40712640">next</a><span>|</span><label class="collapse" for="c-40714280">[-]</label><label class="expand" for="c-40714280">[3 more]</label></div><br/><div class="children"><div class="content">Really? I hadn&#x27;t heard of GPU or GPGPU pushing bandwidth recently. Networking certainly does. 400GbE cards exceed PCIe 4.0 x16 bandwidth, 800 is here, and 1.6 apparently in the works. Disk too though, just because a single disk (or even network phy) may not max out a PCI slot does not mean you want to dedicate more lanes than necessary to them because you likely want a bunch of them.</div><br/><div id="40714359" class="c"><input type="checkbox" id="c-40714359" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40714280">parent</a><span>|</span><a href="#40714425">next</a><span>|</span><label class="collapse" for="c-40714359">[-]</label><label class="expand" for="c-40714359">[1 more]</label></div><br/><div class="children"><div class="content">We are at PCIe5 in the Dell XE9680. We add in 8x400G cards and they talk directly to the Network&#x2F; 8xGPUs (via rocev2).<p>800G ethernet is here at the switches (Dell Z9864F-ON is beautiful... 128 ports of 400G), but not yet at the server&#x2F;NIC level, that comes with PCIe6. We are also limited to 16 chassis&#x2F;128 GPUs in a single cluster right now.<p>NVMe is getting faster all the time, but is pretty standard now. We put 122TB into each server, so that enables local caching of data, if needed.<p>All of this is designed for the highest speed available today that we can get on the various bus where data is transferred.</div><br/></div></div><div id="40714425" class="c"><input type="checkbox" id="c-40714425" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40712753">root</a><span>|</span><a href="#40714280">parent</a><span>|</span><a href="#40714359">prev</a><span>|</span><a href="#40712640">next</a><span>|</span><label class="collapse" for="c-40714425">[-]</label><label class="expand" for="c-40714425">[1 more]</label></div><br/><div class="children"><div class="content">Nvlink 4.0 used to connect H100 GPUs today is almost as fast as PCIe-7.0 (12.5GBs vs 16GBs). By the time PCIe-7.0 is available I’m sure NVlink will be much faster. So, yeah, GPUs are currently the most bandwidth hungry devices on the market.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40712640" class="c"><input type="checkbox" id="c-40712640" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#40712753">prev</a><span>|</span><a href="#40713383">next</a><span>|</span><label class="collapse" for="c-40712640">[-]</label><label class="expand" for="c-40712640">[1 more]</label></div><br/><div class="children"><div class="content">Marvell was recentlty showing off a a PCIe 6.0 Alaska-P retimer that&#x27;s good for pcb, copper cable, &amp; optical interconnect. Doesn&#x27;t include optical transceivers but does show the growing interest in optical PCIe. <a href="https:&#x2F;&#x2F;www.servethehome.com&#x2F;marvell-extending-pcie-gen6-reach-from-3-5-inches-to-meters&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.servethehome.com&#x2F;marvell-extending-pcie-gen6-rea...</a><p>Right now optical seems exotic &amp; expensive but we seem near a severe tipping point. Copper keeps facing increasingly channeling signal integrity challenges, requiring expensive &amp; energy consuming retimers. Meanwhile we think we can keep scaling optical down, integrating silicon photonics, getting increasingly lower pJ&#x2F;b energy costs. Without the range &amp; signal integrity issues. Not super duper deep but this 2 year old Cadence blog post goes into it, and it seems indeed to be where things are heading. <a href="https:&#x2F;&#x2F;community.cadence.com&#x2F;cadence_blogs_8&#x2F;b&#x2F;breakfast-bytes&#x2F;posts&#x2F;photonics-keynote" rel="nofollow">https:&#x2F;&#x2F;community.cadence.com&#x2F;cadence_blogs_8&#x2F;b&#x2F;breakfast-by...</a></div><br/></div></div><div id="40713383" class="c"><input type="checkbox" id="c-40713383" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#40712640">prev</a><span>|</span><a href="#40713837">next</a><span>|</span><label class="collapse" for="c-40713383">[-]</label><label class="expand" for="c-40713383">[3 more]</label></div><br/><div class="children"><div class="content">GigaTeras?</div><br/><div id="40713439" class="c"><input type="checkbox" id="c-40713439" checked=""/><div class="controls bullet"><span class="by">SebJansen</span><span>|</span><a href="#40713383">parent</a><span>|</span><a href="#40713837">next</a><span>|</span><label class="collapse" for="c-40713439">[-]</label><label class="expand" for="c-40713439">[2 more]</label></div><br/><div class="children"><div class="content">GigaTransfers.<p>128 GT is 8192 Gbps</div><br/><div id="40713673" class="c"><input type="checkbox" id="c-40713673" checked=""/><div class="controls bullet"><span class="by">lights0123</span><span>|</span><a href="#40713383">root</a><span>|</span><a href="#40713439">parent</a><span>|</span><a href="#40713837">next</a><span>|</span><label class="collapse" for="c-40713673">[-]</label><label class="expand" for="c-40713673">[1 more]</label></div><br/><div class="children"><div class="content">For DDR RAM, which uses 64 parallel lines. PCI-E transmits one bit per transfer, which has its practical bandwidth reduced after error correction.</div><br/></div></div></div></div></div></div><div id="40713837" class="c"><input type="checkbox" id="c-40713837" checked=""/><div class="controls bullet"><span class="by">banish-m4</span><span>|</span><a href="#40713383">prev</a><span>|</span><a href="#40714290">next</a><span>|</span><label class="collapse" for="c-40713837">[-]</label><label class="expand" for="c-40713837">[2 more]</label></div><br/><div class="children"><div class="content">Damn. Just put together 2 7900X3D + H13SAE-MF + RTX 4070 Ti SUPER + 100GbE-SR4 NICs with PCIe 5.0 and ATX 3.0. Mostly for bare-metal NIC load testing. Already obsolete.<p>In other news, I&#x27;m getting a Xeon with 4 whole GiB of DDR2 ECC RAM shipped from China that has 3 ISA slots. ;D</div><br/><div id="40715177" class="c"><input type="checkbox" id="c-40715177" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#40713837">parent</a><span>|</span><a href="#40714290">next</a><span>|</span><label class="collapse" for="c-40715177">[-]</label><label class="expand" for="c-40715177">[1 more]</label></div><br/><div class="children"><div class="content">What kind of 100GbE throughput you’re getting?</div><br/></div></div></div></div><div id="40714290" class="c"><input type="checkbox" id="c-40714290" checked=""/><div class="controls bullet"><span class="by">Copenjin</span><span>|</span><a href="#40713837">prev</a><span>|</span><a href="#40712595">next</a><span>|</span><label class="collapse" for="c-40714290">[-]</label><label class="expand" for="c-40714290">[2 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t they use proper units? I refuse to even read this.</div><br/><div id="40714845" class="c"><input type="checkbox" id="c-40714845" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#40714290">parent</a><span>|</span><a href="#40712595">next</a><span>|</span><label class="collapse" for="c-40714845">[-]</label><label class="expand" for="c-40714845">[1 more]</label></div><br/><div class="children"><div class="content">These are the standard and correct units.</div><br/></div></div></div></div></div></div></div></div></div></body></html>