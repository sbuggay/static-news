<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685955681113" as="style"/><link rel="stylesheet" href="styles.css?v=1685955681113"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2023/Jun/4/closed-model-training/">It’s infuriatingly hard to understand how closed models train on their input</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>simonw</span> | <span>157 comments</span></div><br/><div><div id="36189657" class="c"><input type="checkbox" id="c-36189657" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36189521">next</a><span>|</span><label class="collapse" for="c-36189657">[-]</label><label class="expand" for="c-36189657">[77 more]</label></div><br/><div class="children"><div class="content">This is intentional, as I think OpenAI is scared of legal blowback.<p>Cutting edge AI models and datasets have largely been &quot;for testing&quot; and &quot;for research&quot; ever since they existed. Usage rights were extremely low on the researchers&#x27; list of concerns.<p>But all of the sudden OpenAI and such are trying to commercialize models that (unlike a classifier or whatever) are <i>very</i> capable of stepping on rightholders’ toes...<p>Hence I suspect all the talk of trade secrets is just a front.</div><br/><div id="36191122" class="c"><input type="checkbox" id="c-36191122" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36189657">parent</a><span>|</span><a href="#36190676">next</a><span>|</span><label class="collapse" for="c-36191122">[-]</label><label class="expand" for="c-36191122">[23 more]</label></div><br/><div class="children"><div class="content">Keep in mind that the architecture of LLMs is <i>very</i> simple, so there&#x27;s not a lot of &quot;secret sauce&quot;. What little there is, is jealously guarded.<p>This is why OpenAI is keeping the parameter count of GPT 4 secret. It could be something stupid huge like 4 trillion parameters, which is the &quot;secret&quot; that makes it work so well. Or maybe it&#x27;s just a few hundred billion, and they&#x27;ve done something else to make it smart.<p>Just knowing the numbers might be sufficient for a competitor to create a GPT 4 clone. Without the numbers, they might have to go through a process of trial and error. At these scales, even a few extra training runs could cost $ millions in training, and delay competitors by months or even years.</div><br/><div id="36191215" class="c"><input type="checkbox" id="c-36191215" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191122">parent</a><span>|</span><a href="#36190676">next</a><span>|</span><label class="collapse" for="c-36191215">[-]</label><label class="expand" for="c-36191215">[22 more]</label></div><br/><div class="children"><div class="content">GPT-4 was doing more than just predicting next words</div><br/><div id="36191246" class="c"><input type="checkbox" id="c-36191246" checked=""/><div class="controls bullet"><span class="by">pigeons</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191215">parent</a><span>|</span><a href="#36190676">next</a><span>|</span><label class="collapse" for="c-36191246">[-]</label><label class="expand" for="c-36191246">[21 more]</label></div><br/><div class="children"><div class="content">Like what?</div><br/><div id="36191298" class="c"><input type="checkbox" id="c-36191298" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191246">parent</a><span>|</span><a href="#36190676">next</a><span>|</span><label class="collapse" for="c-36191298">[-]</label><label class="expand" for="c-36191298">[20 more]</label></div><br/><div class="children"><div class="content">Lots of plugins and manual fine tuning by an army of people on tons of subjects</div><br/><div id="36191314" class="c"><input type="checkbox" id="c-36191314" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191298">parent</a><span>|</span><a href="#36191542">next</a><span>|</span><label class="collapse" for="c-36191314">[-]</label><label class="expand" for="c-36191314">[9 more]</label></div><br/><div class="children"><div class="content">Well yes. This is precisely the type of info that OpenAI is keeping secret, because it could turn out that it&#x27;s smart because its huge OR smart because it is RLHF-ed to death. Knowing which is the secret sauce might be sufficient for a large org to reproduce it.<p>E.g.: if the secret is that it has been trained on tons of &quot;books&quot;, then Google could just throw Google Books at Bard 3.</div><br/><div id="36191559" class="c"><input type="checkbox" id="c-36191559" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191314">parent</a><span>|</span><a href="#36191599">next</a><span>|</span><label class="collapse" for="c-36191559">[-]</label><label class="expand" for="c-36191559">[2 more]</label></div><br/><div class="children"><div class="content">Or books3. :)<p>(I put together a dataset of 190,000 books that I called books3, which llama eventually trained on. Usage rights are a big interest of mine, primarily because there’s a weird disconnect of people trying to claim copyright over models when the underlying data obviously wasn’t copyrightable.)</div><br/><div id="36193214" class="c"><input type="checkbox" id="c-36193214" checked=""/><div class="controls bullet"><span class="by">selestify</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191559">parent</a><span>|</span><a href="#36191599">next</a><span>|</span><label class="collapse" for="c-36193214">[-]</label><label class="expand" for="c-36193214">[1 more]</label></div><br/><div class="children"><div class="content">A model (if trained right) will not include the training texts verbatim, and will arguably be a big enough transformation over the source material to be copyrightable in its own right.<p>IANAL, and I’m not saying how this is how it’ll play out in the courts, but I don’t see why this is a “weird disconnect”</div><br/></div></div></div></div><div id="36191599" class="c"><input type="checkbox" id="c-36191599" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191314">parent</a><span>|</span><a href="#36191559">prev</a><span>|</span><a href="#36191542">next</a><span>|</span><label class="collapse" for="c-36191599">[-]</label><label class="expand" for="c-36191599">[6 more]</label></div><br/><div class="children"><div class="content">We know for a fact that GPT-4&#x27;s RLHF makes the model worse by incurring an &quot;alignment tax.&quot; Microsoft Research and others had access to GPT-4 pre-RLHF and all reported that it was far more capable then and that it lost capability as RLHF&#x2F;Alignment training checkpoints came in.</div><br/><div id="36192423" class="c"><input type="checkbox" id="c-36192423" checked=""/><div class="controls bullet"><span class="by">jdironman</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191599">parent</a><span>|</span><a href="#36191542">next</a><span>|</span><label class="collapse" for="c-36192423">[-]</label><label class="expand" for="c-36192423">[5 more]</label></div><br/><div class="children"><div class="content">Is the GPT-0314 model (via API) the one before RLHF?</div><br/><div id="36192843" class="c"><input type="checkbox" id="c-36192843" checked=""/><div class="controls bullet"><span class="by">MrNeon</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192423">parent</a><span>|</span><a href="#36191542">next</a><span>|</span><label class="collapse" for="c-36192843">[-]</label><label class="expand" for="c-36192843">[4 more]</label></div><br/><div class="children"><div class="content">Ha ha ha ha. No.
OpenAI will never make pre-alignment models available.</div><br/><div id="36193923" class="c"><input type="checkbox" id="c-36193923" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192843">parent</a><span>|</span><a href="#36192985">next</a><span>|</span><label class="collapse" for="c-36193923">[-]</label><label class="expand" for="c-36193923">[1 more]</label></div><br/><div class="children"><div class="content">code-davinci-002, the gpt-3.5 base model, was available for a while until it was removed recently. Access is still available for researchers. Various researchers and other entities have access to the GPT-4 base model as well.</div><br/></div></div><div id="36192985" class="c"><input type="checkbox" id="c-36192985" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192843">parent</a><span>|</span><a href="#36193923">prev</a><span>|</span><a href="#36193046">next</a><span>|</span><label class="collapse" for="c-36192985">[-]</label><label class="expand" for="c-36192985">[1 more]</label></div><br/><div class="children"><div class="content">I think sama said this week they might release GPT3.0 DaVinci. That&#x27;s a pre-alignment model.</div><br/></div></div><div id="36193046" class="c"><input type="checkbox" id="c-36193046" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192843">parent</a><span>|</span><a href="#36192985">prev</a><span>|</span><a href="#36191542">next</a><span>|</span><label class="collapse" for="c-36193046">[-]</label><label class="expand" for="c-36193046">[1 more]</label></div><br/><div class="children"><div class="content">I would bet a boatload of money that all the 3-letter folks do, in fact, have access to the pre-alignment models, maybe with whatever minimal RLHF is needed to make them usable in various interfaces. The public offerings are intended to make the unaligned models more powerful by gathering training data.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36191542" class="c"><input type="checkbox" id="c-36191542" checked=""/><div class="controls bullet"><span class="by">stronglikedan</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191298">parent</a><span>|</span><a href="#36191314">prev</a><span>|</span><a href="#36193720">next</a><span>|</span><label class="collapse" for="c-36191542">[-]</label><label class="expand" for="c-36191542">[8 more]</label></div><br/><div class="children"><div class="content">With the end result of the output being: it&#x27;s just predicting next words</div><br/><div id="36192418" class="c"><input type="checkbox" id="c-36192418" checked=""/><div class="controls bullet"><span class="by">SanderNL</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191542">parent</a><span>|</span><a href="#36191617">next</a><span>|</span><label class="collapse" for="c-36192418">[-]</label><label class="expand" for="c-36192418">[1 more]</label></div><br/><div class="children"><div class="content">Tokens, actually.</div><br/></div></div><div id="36191617" class="c"><input type="checkbox" id="c-36191617" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191542">parent</a><span>|</span><a href="#36192418">prev</a><span>|</span><a href="#36193720">next</a><span>|</span><label class="collapse" for="c-36191617">[-]</label><label class="expand" for="c-36191617">[6 more]</label></div><br/><div class="children"><div class="content">Nope</div><br/><div id="36193993" class="c"><input type="checkbox" id="c-36193993" checked=""/><div class="controls bullet"><span class="by">bowsamic</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191617">parent</a><span>|</span><a href="#36192030">next</a><span>|</span><label class="collapse" for="c-36193993">[-]</label><label class="expand" for="c-36193993">[1 more]</label></div><br/><div class="children"><div class="content">Except, literally yes</div><br/></div></div><div id="36192030" class="c"><input type="checkbox" id="c-36192030" checked=""/><div class="controls bullet"><span class="by">randcraw</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191617">parent</a><span>|</span><a href="#36193993">prev</a><span>|</span><a href="#36193720">next</a><span>|</span><label class="collapse" for="c-36192030">[-]</label><label class="expand" for="c-36192030">[4 more]</label></div><br/><div class="children"><div class="content">Agreed.  If LLMs did predict only a single word ahead, they would repeat themselves, if not in the next sentence then at least in the next paragraph.  LLMs certainly retain memory of what they’ve already said.  They are most certainly NOT Markov-1 models, despite the popularization of that oversimplification.</div><br/><div id="36193053" class="c"><input type="checkbox" id="c-36193053" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192030">parent</a><span>|</span><a href="#36192269">next</a><span>|</span><label class="collapse" for="c-36193053">[-]</label><label class="expand" for="c-36193053">[1 more]</label></div><br/><div class="children"><div class="content">I thought it was common knowledge that LLMs have a context of X tokens (8192 in the case of GPT4) and use those as input to predict the next token probabilities.<p>Still repetition is a problem, that&#x27;s why they introduce a penalty for generating the exact same token, so when picking the actual token to output from a list of probabilities, they penalize the ones already in the context so its less likely to repeat itself.</div><br/></div></div><div id="36192269" class="c"><input type="checkbox" id="c-36192269" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192030">parent</a><span>|</span><a href="#36193053">prev</a><span>|</span><a href="#36192352">next</a><span>|</span><label class="collapse" for="c-36192269">[-]</label><label class="expand" for="c-36192269">[1 more]</label></div><br/><div class="children"><div class="content">They really do compute just a single token at a time, but the input to that decision is the number of tokens in the context window - so that&#x27;s usually around 8,000 tokens (approximately 6000 words).<p>You can experiment with smaller LLMs on your own devices to get a better feeling for how that works.</div><br/></div></div><div id="36192352" class="c"><input type="checkbox" id="c-36192352" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192030">parent</a><span>|</span><a href="#36192269">prev</a><span>|</span><a href="#36193720">next</a><span>|</span><label class="collapse" for="c-36192352">[-]</label><label class="expand" for="c-36192352">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think anyone&#x27;s saying they&#x27;re Markov chains with context length 1. In all the critiques I&#x27;ve read no one&#x27;s even come close to articulating that.<p>But they are Markov chains with context length N. They&#x27;re approximating (implicit) Markov transition matrices through a fancier method of computation but they still are just generating one token at a time.</div><br/></div></div></div></div></div></div></div></div><div id="36193720" class="c"><input type="checkbox" id="c-36193720" checked=""/><div class="controls bullet"><span class="by">IIAOPSW</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191298">parent</a><span>|</span><a href="#36191542">prev</a><span>|</span><a href="#36191750">next</a><span>|</span><label class="collapse" for="c-36193720">[-]</label><label class="expand" for="c-36193720">[1 more]</label></div><br/><div class="children"><div class="content">AGI: pick any two?</div><br/></div></div><div id="36191750" class="c"><input type="checkbox" id="c-36191750" checked=""/><div class="controls bullet"><span class="by">thelittleone</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191298">parent</a><span>|</span><a href="#36193720">prev</a><span>|</span><a href="#36190676">next</a><span>|</span><label class="collapse" for="c-36191750">[-]</label><label class="expand" for="c-36191750">[1 more]</label></div><br/><div class="children"><div class="content">Is the manual fine tuning driven in part by the thumbs up, thumbs down feedback on chatgpt responses?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36190676" class="c"><input type="checkbox" id="c-36190676" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#36189657">parent</a><span>|</span><a href="#36191122">prev</a><span>|</span><a href="#36189954">next</a><span>|</span><label class="collapse" for="c-36190676">[-]</label><label class="expand" for="c-36190676">[25 more]</label></div><br/><div class="children"><div class="content">I find this a super interesting topic: My prediction is that we will eventually see a huge backlash against this copyright whitewashing. I think the current status quo is unacceptable but I fear the upcoming legislation will throw out the baby with the bathwater.</div><br/><div id="36190986" class="c"><input type="checkbox" id="c-36190986" checked=""/><div class="controls bullet"><span class="by">sfifs</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190676">parent</a><span>|</span><a href="#36193893">next</a><span>|</span><label class="collapse" for="c-36190986">[-]</label><label class="expand" for="c-36190986">[9 more]</label></div><br/><div class="children"><div class="content">One reading of the latest US Supreme Court ruling on the Andy Warhol estate&#x27;s case is that the current legal frameworks already prevent this.<p>The ruling focused on what was the commercial use case, not on what was the degree of transformation in Andy Warhol&#x27;s artwork. The opinion appears to say that as the Prince painting was not explicitly licensed for the commercial use case of reproduction in magazine story and could conceivably have been substituted for the purpose of printing in the magazine to accompany a story by the original artist&#x27;s photograph, it violated copyright. There was even an explicit distinction drawn between this work and the Campbell Soup paintings which were characterized as a social commentary and therefore fair use.<p>There are likely class action lawsuits brewing based on this ruling because today when commercial artists in agencies create layouts, they generally license images &amp; artwork.</div><br/><div id="36193627" class="c"><input type="checkbox" id="c-36193627" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190986">parent</a><span>|</span><a href="#36191339">next</a><span>|</span><label class="collapse" for="c-36193627">[-]</label><label class="expand" for="c-36193627">[1 more]</label></div><br/><div class="children"><div class="content">For people who are unaware of this case, here[1] is a writeup.<p>TL;DR: Andy Warhol used a photograph of Prince by Lynn Goldsmith to create a series of original silk screen artworks called &quot;Orange Prince&quot;. The supreme court ruled (as I understand it) that Warhol&#x27;s use was not sufficiently transformative to negate the right of the original copyrightholder.<p>[1] <a href="https:&#x2F;&#x2F;www.theguardian.com&#x2F;artanddesign&#x2F;2023&#x2F;may&#x2F;18&#x2F;andy-warhol-copyright-prince-paintings-lawsuit" rel="nofollow">https:&#x2F;&#x2F;www.theguardian.com&#x2F;artanddesign&#x2F;2023&#x2F;may&#x2F;18&#x2F;andy-wa...</a><p>Edit to add: Here’s a side-by-side <a href="https:&#x2F;&#x2F;www.mondaq.com&#x2F;images&#x2F;article_images&#x2F;1324682a.jpg" rel="nofollow">https:&#x2F;&#x2F;www.mondaq.com&#x2F;images&#x2F;article_images&#x2F;1324682a.jpg</a> . I mean extreme right second from the bottom he hasn’t even changed the colours.</div><br/></div></div><div id="36191339" class="c"><input type="checkbox" id="c-36191339" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190986">parent</a><span>|</span><a href="#36193627">prev</a><span>|</span><a href="#36193065">next</a><span>|</span><label class="collapse" for="c-36191339">[-]</label><label class="expand" for="c-36191339">[5 more]</label></div><br/><div class="children"><div class="content">The way I read that, it was only ruled in favor because they were directed to recreate x photo with some changes. If the Scream painting were still copyrighted, it sounds like telling an AI to &quot;create Edvard Munch&#x27;s The Scream just with lighter colors&quot; would be a copyright violation - but something like &quot;create a new painting in the style of Edvard Munch&quot; would still fall under &quot;artists can&#x27;t copyright a style&quot;.</div><br/><div id="36191935" class="c"><input type="checkbox" id="c-36191935" checked=""/><div class="controls bullet"><span class="by">bjt</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191339">parent</a><span>|</span><a href="#36193065">next</a><span>|</span><label class="collapse" for="c-36191935">[-]</label><label class="expand" for="c-36191935">[4 more]</label></div><br/><div class="children"><div class="content">The output isn&#x27;t the only infringement, arguably.  You could also argue (and I expect good lawyers will), that the numeric representation of the artist&#x27;s works inside the model is already an infringing copy.  (Just like the JPEG bytes stored on a server, even without them being blitted to a screen.)</div><br/><div id="36192513" class="c"><input type="checkbox" id="c-36192513" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191935">parent</a><span>|</span><a href="#36193065">next</a><span>|</span><label class="collapse" for="c-36192513">[-]</label><label class="expand" for="c-36192513">[3 more]</label></div><br/><div class="children"><div class="content">Models don&#x27;t store any artist&#x27;s works. They are way too small to do that.</div><br/><div id="36193644" class="c"><input type="checkbox" id="c-36193644" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192513">parent</a><span>|</span><a href="#36193065">next</a><span>|</span><label class="collapse" for="c-36193644">[-]</label><label class="expand" for="c-36193644">[2 more]</label></div><br/><div class="children"><div class="content">Getty has alleged that Stable Diffusion is sometimes returning some of their copyright images[1].  Even if the model seems too small to directly store the images, it seems at least plausible to me that the parameters can act as a compression such that the model could just output an almost direct copy of an original. I have certainly seen stable diffusion emit images which look like a getty watermark has just been blurred out.<p>[1] <a href="https:&#x2F;&#x2F;www.reuters.com&#x2F;legal&#x2F;getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reuters.com&#x2F;legal&#x2F;getty-images-lawsuit-says-stab...</a></div><br/><div id="36193883" class="c"><input type="checkbox" id="c-36193883" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36193644">parent</a><span>|</span><a href="#36193065">next</a><span>|</span><label class="collapse" for="c-36193883">[-]</label><label class="expand" for="c-36193883">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t store the original images, but it has learned how getty images watermark looks like and where it&#x27;s located, because it has been repeated millions of times. So it sometimes can return that.<p>This is why it&#x27;s important to clean up the training dataset. To remove duplicates, images containing watermarks, images that are too similar to each other and so on.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36193065" class="c"><input type="checkbox" id="c-36193065" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190986">parent</a><span>|</span><a href="#36191339">prev</a><span>|</span><a href="#36193893">next</a><span>|</span><label class="collapse" for="c-36193065">[-]</label><label class="expand" for="c-36193065">[2 more]</label></div><br/><div class="children"><div class="content">Except this is not transformative at all - LLMs are literally trained to be able to output their training data.<p>That&#x27;s why it produced those substantially plagiarized CNET articles.</div><br/><div id="36193371" class="c"><input type="checkbox" id="c-36193371" checked=""/><div class="controls bullet"><span class="by">tinco</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36193065">parent</a><span>|</span><a href="#36193893">next</a><span>|</span><label class="collapse" for="c-36193371">[-]</label><label class="expand" for="c-36193371">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more complex than that, they are trained to model their language and understanding of a concept to match the author. There is not enough memory in the network to remember the actual text.<p>It&#x27;s like giving a talented an artist a day with a painting, and then a day later ask them to precisely copy the painting from memory. Would they come close enough for it to be considered a forgery, or will it be a transformative reinterpretation? It will probably depend on the skill of the artist, and I feel it might go either way.</div><br/></div></div></div></div></div></div><div id="36193893" class="c"><input type="checkbox" id="c-36193893" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190676">parent</a><span>|</span><a href="#36190986">prev</a><span>|</span><a href="#36191523">next</a><span>|</span><label class="collapse" for="c-36193893">[-]</label><label class="expand" for="c-36193893">[1 more]</label></div><br/><div class="children"><div class="content">Or maybe, maybe the people in charge will wake up, do their job, accept the challenge and make modern cc laws? Oh forget about it we will forever live in the 90&#x27;</div><br/></div></div><div id="36191523" class="c"><input type="checkbox" id="c-36191523" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190676">parent</a><span>|</span><a href="#36193893">prev</a><span>|</span><a href="#36191139">next</a><span>|</span><label class="collapse" for="c-36191523">[-]</label><label class="expand" for="c-36191523">[4 more]</label></div><br/><div class="children"><div class="content">Do you think people should be allowed to learn from books we read, music we hear, etc?<p>If we’re extending copyright to be not only about reproduction of the work, but also providing part of a large knowledge base for future creators to build upon… why does it matter if it’s a neural network or human being doing the learning?<p>I personally hope the copyright maximalists fail, because if they succeed in making “learning from a copyrighted work” without paying illegal, we end up in a dark place indeed.</div><br/><div id="36191574" class="c"><input type="checkbox" id="c-36191574" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191523">parent</a><span>|</span><a href="#36191646">next</a><span>|</span><label class="collapse" for="c-36191574">[-]</label><label class="expand" for="c-36191574">[1 more]</label></div><br/><div class="children"><div class="content">I think AI aren’t people, and that they’re not even AI: it’s machine learning, and ML is a subset of AI that we haven’t really broken out of. ML is a tool, and asking whether ML should be allowed to learn on copyrighted work is equivalent to asking whether your computer is allowed to contain a copyrighted work: “it depends.”<p>I’m in favor of getting rid of copyright entirely, since it’s become a far cry from what it was intended to be, but we have to acknowledge both sides of the arguments in order to proceed.</div><br/></div></div><div id="36191646" class="c"><input type="checkbox" id="c-36191646" checked=""/><div class="controls bullet"><span class="by">spacephysics</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191523">parent</a><span>|</span><a href="#36191574">prev</a><span>|</span><a href="#36191139">next</a><span>|</span><label class="collapse" for="c-36191646">[-]</label><label class="expand" for="c-36191646">[2 more]</label></div><br/><div class="children"><div class="content">I agree we should either get rid of, or very least overhaul copyright.<p>However, the line in the sand I draw is giving any kind of human-based rights to ML or AI. People already are thinking ChatGPT is close to human intelligence, and we know we haven’t even seen what’s <i>truly</i> possible.<p>My fear is the typical oversimplification of complex technology will lend us to giving human rights to programs, and lead to a whole host of issues.<p>We need to stop anthropomorphizing technology until we <i>really</i> hit a philosophical quandary. Giving in too early will just give power to less-than-polished tech and either slow down research, or further drain a country’s resources</div><br/><div id="36193433" class="c"><input type="checkbox" id="c-36193433" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191646">parent</a><span>|</span><a href="#36191139">next</a><span>|</span><label class="collapse" for="c-36193433">[-]</label><label class="expand" for="c-36193433">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the sand I draw is giving any kind of human-based rights to ML or AI.<p>Ok, but this isn&#x27;t about giving rights to AI.<p>Instead this is about given rights to humans to use AI.<p>If you make a law about this stuff, at the end of the day, it all trickles down to allowing or disallow human actions.</div><br/></div></div></div></div></div></div><div id="36191139" class="c"><input type="checkbox" id="c-36191139" checked=""/><div class="controls bullet"><span class="by">xyzzy123</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190676">parent</a><span>|</span><a href="#36191523">prev</a><span>|</span><a href="#36193108">next</a><span>|</span><label class="collapse" for="c-36191139">[-]</label><label class="expand" for="c-36191139">[1 more]</label></div><br/><div class="children"><div class="content">In terms of actions, the US has gone as far as to kneecap a competitor&#x27;s chip industry and restrict their GPU access.<p>I reckon that behind closed doors there will be a strong sense that any kind of regulation that impedes AI development would not be in the national interest.</div><br/></div></div><div id="36193108" class="c"><input type="checkbox" id="c-36193108" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190676">parent</a><span>|</span><a href="#36191139">prev</a><span>|</span><a href="#36190943">next</a><span>|</span><label class="collapse" for="c-36193108">[-]</label><label class="expand" for="c-36193108">[1 more]</label></div><br/><div class="children"><div class="content">I would argue if the fair use argument holds up - that the most substantial value of these models lies in their training dataset.<p>It would be morally reprehensible for anyone to block training open models based on the data provided by the public at large. Such legalization would be totally unacceptable.</div><br/></div></div><div id="36190943" class="c"><input type="checkbox" id="c-36190943" checked=""/><div class="controls bullet"><span class="by">proamdev123</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190676">parent</a><span>|</span><a href="#36193108">prev</a><span>|</span><a href="#36191407">next</a><span>|</span><label class="collapse" for="c-36190943">[-]</label><label class="expand" for="c-36190943">[6 more]</label></div><br/><div class="children"><div class="content">What is “copyright whitewashing”? I’ve never heard that phrase before.</div><br/><div id="36191087" class="c"><input type="checkbox" id="c-36191087" checked=""/><div class="controls bullet"><span class="by">ethbr0</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190943">parent</a><span>|</span><a href="#36190970">next</a><span>|</span><label class="collapse" for="c-36191087">[-]</label><label class="expand" for="c-36191087">[4 more]</label></div><br/><div class="children"><div class="content">Imagine a machine that takes in copyrighted works and, with the press of a button, outputs works which are substantially similar but not copyrighted.<p>Copyright whitewashing.<p>(I.e. pointing to your black box and claiming it&#x27;s a clean room)</div><br/><div id="36191149" class="c"><input type="checkbox" id="c-36191149" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191087">parent</a><span>|</span><a href="#36192108">next</a><span>|</span><label class="collapse" for="c-36191149">[-]</label><label class="expand" for="c-36191149">[1 more]</label></div><br/><div class="children"><div class="content">But to reiterate existing talking points, using copyrighted works for AI weight generation is exactly the same thing as how human artists learn skills just because some people refer to the process as “learning” or “training”, and any counterarguments to this are meaningless fearmongering outdated Luddite rages, besides such uses are fair uses and transformative uses and explicitly allowed in some jurisdictions under specific terms not allowing blanket rights to regurgitation so it’s all lawful and financially fully exploitable.<p>forwardslash sierra.</div><br/></div></div><div id="36192108" class="c"><input type="checkbox" id="c-36192108" checked=""/><div class="controls bullet"><span class="by">chii</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191087">parent</a><span>|</span><a href="#36191149">prev</a><span>|</span><a href="#36191537">next</a><span>|</span><label class="collapse" for="c-36192108">[-]</label><label class="expand" for="c-36192108">[1 more]</label></div><br/><div class="children"><div class="content">&gt; which are substantially similar but not copyrighted.<p>if it&#x27;s considered substantially similar, then why is it not copyrighted? The difference must also be substantial. Therefore, it should be acceptable as a new works, rather than a derivative works.</div><br/></div></div><div id="36191537" class="c"><input type="checkbox" id="c-36191537" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191087">parent</a><span>|</span><a href="#36192108">prev</a><span>|</span><a href="#36190970">next</a><span>|</span><label class="collapse" for="c-36191537">[-]</label><label class="expand" for="c-36191537">[1 more]</label></div><br/><div class="children"><div class="content">So, like Terry Brooks then?</div><br/></div></div></div></div><div id="36190970" class="c"><input type="checkbox" id="c-36190970" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190943">parent</a><span>|</span><a href="#36191087">prev</a><span>|</span><a href="#36191407">next</a><span>|</span><label class="collapse" for="c-36190970">[-]</label><label class="expand" for="c-36190970">[1 more]</label></div><br/><div class="children"><div class="content">they mean laundering IP theft</div><br/></div></div></div></div><div id="36191407" class="c"><input type="checkbox" id="c-36191407" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36190676">parent</a><span>|</span><a href="#36190943">prev</a><span>|</span><a href="#36189954">next</a><span>|</span><label class="collapse" for="c-36191407">[-]</label><label class="expand" for="c-36191407">[2 more]</label></div><br/><div class="children"><div class="content">It still seems odd to me that the standard advice for &quot;how do I avoid copyright issues with example code&quot; is &quot;rewrite it in your own style&quot;, and we&#x27;re happy with this, but now that we have an algorithm that can do this (using a stupendous amount of calculation, it&#x27;s not like these things learn for free... yet...) this is suddenly somehow cheating.<p>I think the protestations about copyright aren&#x27;t entirely honest. People aren&#x27;t mad that LLMs are &quot;stealing their work&quot;, they&#x27;re scared that LLMs are copying their <i>ability to produce such work.</i></div><br/><div id="36191489" class="c"><input type="checkbox" id="c-36191489" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191407">parent</a><span>|</span><a href="#36189954">next</a><span>|</span><label class="collapse" for="c-36191489">[-]</label><label class="expand" for="c-36191489">[1 more]</label></div><br/><div class="children"><div class="content">Its some of both. Stable Diffusion will rarely blatently copy a work, as it has no distinction between creative reconstruction and plagiarism. I think this applies to LLMs too, its just much harder for human brains to detect.<p>At the same time, the artists and such who claim any of its output is illegal are being hypocritical. Nothing is made in a vacuum.</div><br/></div></div></div></div></div></div><div id="36189954" class="c"><input type="checkbox" id="c-36189954" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#36189657">parent</a><span>|</span><a href="#36190676">prev</a><span>|</span><a href="#36193562">next</a><span>|</span><label class="collapse" for="c-36189954">[-]</label><label class="expand" for="c-36189954">[1 more]</label></div><br/><div class="children"><div class="content">They forced others out of the skunk so I am fine with it. Forcing companies like Facebook in leaking weights of their models because they are over eager to release something is no small feat. For nothing else I applaud them for that.</div><br/></div></div><div id="36193562" class="c"><input type="checkbox" id="c-36193562" checked=""/><div class="controls bullet"><span class="by">jve</span><span>|</span><a href="#36189657">parent</a><span>|</span><a href="#36189954">prev</a><span>|</span><a href="#36191021">next</a><span>|</span><label class="collapse" for="c-36193562">[-]</label><label class="expand" for="c-36193562">[1 more]</label></div><br/><div class="children"><div class="content">Elon commented about commercialization of OpenAI: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;bWr-DA5Wjfw?t=3m38s" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;bWr-DA5Wjfw?t=3m38s</a><p>This short video perhaps should be its own submission</div><br/></div></div><div id="36191021" class="c"><input type="checkbox" id="c-36191021" checked=""/><div class="controls bullet"><span class="by">Razengan</span><span>|</span><a href="#36189657">parent</a><span>|</span><a href="#36193562">prev</a><span>|</span><a href="#36191506">next</a><span>|</span><label class="collapse" for="c-36191021">[-]</label><label class="expand" for="c-36191021">[24 more]</label></div><br/><div class="children"><div class="content">If a human learns to draw by studying the works of other artists, is that infringement?</div><br/><div id="36193532" class="c"><input type="checkbox" id="c-36193532" checked=""/><div class="controls bullet"><span class="by">maeln</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191021">parent</a><span>|</span><a href="#36191060">next</a><span>|</span><label class="collapse" for="c-36193532">[-]</label><label class="expand" for="c-36193532">[1 more]</label></div><br/><div class="children"><div class="content">For what should be pretty obvious reason, most society and laws will treat humans and corporation and their output differently.</div><br/></div></div><div id="36191060" class="c"><input type="checkbox" id="c-36191060" checked=""/><div class="controls bullet"><span class="by">jackvalentine</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191021">parent</a><span>|</span><a href="#36193532">prev</a><span>|</span><a href="#36193125">next</a><span>|</span><label class="collapse" for="c-36191060">[-]</label><label class="expand" for="c-36191060">[7 more]</label></div><br/><div class="children"><div class="content">It could be, if the human uses their skills to draw infringing outputs!</div><br/><div id="36191553" class="c"><input type="checkbox" id="c-36191553" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191060">parent</a><span>|</span><a href="#36191532">next</a><span>|</span><label class="collapse" for="c-36191553">[-]</label><label class="expand" for="c-36191553">[5 more]</label></div><br/><div class="children"><div class="content">So why are we even arguing about this new form of copyright that restricts who can learn from a work?<p>If an AI model produces an infringing work and someone distributes it, sue them like you would a human doing the same thing.<p>This idea of training being a violation of copyright is insane.</div><br/><div id="36191944" class="c"><input type="checkbox" id="c-36191944" checked=""/><div class="controls bullet"><span class="by">mattigames</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191553">parent</a><span>|</span><a href="#36191532">next</a><span>|</span><label class="collapse" for="c-36191944">[-]</label><label class="expand" for="c-36191944">[4 more]</label></div><br/><div class="children"><div class="content">No, is not insane at all, a computer can generate millions of derivate copies in a second, a human has limited time for learning and for making artwork, so it doesn&#x27;t make sense to compare them, not in the slightest.<p>If suddenly there was a race of humans who could read and retain entire books in a matter of seconds (and other similar feats) the implications would be almost the same, as it would break basic expectations of time and personal resources that our society relies on, and a lot of people would be rightfully worried about such people entering the workforce.</div><br/><div id="36192876" class="c"><input type="checkbox" id="c-36192876" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191944">parent</a><span>|</span><a href="#36192994">next</a><span>|</span><label class="collapse" for="c-36192876">[-]</label><label class="expand" for="c-36192876">[1 more]</label></div><br/><div class="children"><div class="content">Adobe trained their own model only on public domain images and on images that they own. Do you think that they should not be allowed to do that?</div><br/></div></div><div id="36192994" class="c"><input type="checkbox" id="c-36192994" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191944">parent</a><span>|</span><a href="#36192876">prev</a><span>|</span><a href="#36192368">next</a><span>|</span><label class="collapse" for="c-36192994">[-]</label><label class="expand" for="c-36192994">[1 more]</label></div><br/><div class="children"><div class="content">Speed isn&#x27;t an existing factor in copyright law.</div><br/></div></div><div id="36192368" class="c"><input type="checkbox" id="c-36192368" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191944">parent</a><span>|</span><a href="#36192994">prev</a><span>|</span><a href="#36191532">next</a><span>|</span><label class="collapse" for="c-36192368">[-]</label><label class="expand" for="c-36192368">[1 more]</label></div><br/><div class="children"><div class="content">Computers can do all sorts of things humans can&#x27;t at inhuman scale and speed. Why is AI leveraging the same training data humans do inherently bad just because it can do it better and faster than humans?</div><br/></div></div></div></div></div></div><div id="36191532" class="c"><input type="checkbox" id="c-36191532" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191060">parent</a><span>|</span><a href="#36191553">prev</a><span>|</span><a href="#36193125">next</a><span>|</span><label class="collapse" for="c-36191532">[-]</label><label class="expand" for="c-36191532">[1 more]</label></div><br/><div class="children"><div class="content">hence the entire Andy Warhol case!<p>&gt;Echoing a recommendation from Joe Biden’s administration, the supreme court focused on the specific use that allegedly infringed Goldsmith’s copyright – a license of Warhol’s work to Condé Nast – and said it was not transformative because it served the same commercial purpose as Goldsmith’s photo: to depict Prince in a magazine.<p><a href="https:&#x2F;&#x2F;www.theguardian.com&#x2F;artanddesign&#x2F;2023&#x2F;may&#x2F;18&#x2F;andy-warhol-copyright-prince-paintings-lawsuit" rel="nofollow">https:&#x2F;&#x2F;www.theguardian.com&#x2F;artanddesign&#x2F;2023&#x2F;may&#x2F;18&#x2F;andy-wa...</a></div><br/></div></div></div></div><div id="36193125" class="c"><input type="checkbox" id="c-36193125" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191021">parent</a><span>|</span><a href="#36191060">prev</a><span>|</span><a href="#36191063">next</a><span>|</span><label class="collapse" for="c-36193125">[-]</label><label class="expand" for="c-36193125">[1 more]</label></div><br/><div class="children"><div class="content">So if you&#x27;re making the argument that AI is a human when it comes to the copyright, does that mean that its output is copyrightable just like a humans?<p>Does that mean that OpenAI&#x2F;SD whoever now owns the work created in such a way?<p>It would logically follow from your reasoning.</div><br/></div></div><div id="36191063" class="c"><input type="checkbox" id="c-36191063" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191021">parent</a><span>|</span><a href="#36193125">prev</a><span>|</span><a href="#36191041">next</a><span>|</span><label class="collapse" for="c-36191063">[-]</label><label class="expand" for="c-36191063">[12 more]</label></div><br/><div class="children"><div class="content">No.<p>But we&#x27;re not talking about humans, we&#x27;re talking about software intended to put the artists whose work it was trained on - without permission or compensation - out of business, or which is at least capable of devaluing their work with its ability to copy styles (literally by name.)<p>Even if one could argue (as I probably have) that AI is more than simply a &quot;stochastic parrot,&quot; a philosophical argument about whether it resembles a primitive mind (and therefore whether its output can be considered &quot;art&quot; in any form) doesn&#x27;t answer a legal argument about copyright infringement. Humans are allowed to be inspired by other humans, software used by other humans is not.<p>Also you shouldn&#x27;t delete your comment just because it gets downvotes and repost it. Bad form.</div><br/><div id="36191388" class="c"><input type="checkbox" id="c-36191388" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191063">parent</a><span>|</span><a href="#36191803">next</a><span>|</span><label class="collapse" for="c-36191388">[-]</label><label class="expand" for="c-36191388">[2 more]</label></div><br/><div class="children"><div class="content">The issue with comments like this is that many people are talking about the law as it is, and others are approaching it from a moral&#x2F;solution-based point of view.<p>Legally, AI is likely free and clear since the work it outputs is vastly different from any one art piece, therefore not being a derivative work of anything else (unless you tell it to copy an artists&#x27; style AND ask it specifically to remake an artists&#x27; existing work with the intent to recreate it).<p>But morally, there are indeed externalities that can&#x2F;should be thought of, and possible even put into law, but starting a discussion of this front is a losing game since everyone will have a different opinion on how much we should restrict, and whether it will even work (if you consider that China and Russia will forego any sort of copyright protection for AI generation, these laws could enable these countries to surpass the rest of the world in capability and quality).<p>&gt; edit</div><br/><div id="36192859" class="c"><input type="checkbox" id="c-36192859" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191388">parent</a><span>|</span><a href="#36191803">next</a><span>|</span><label class="collapse" for="c-36192859">[-]</label><label class="expand" for="c-36192859">[1 more]</label></div><br/><div class="children"><div class="content">What if someone trains a model on public domain works only?<p>Also adobe sort-of did that already - they trained their own model only on their own images. So no &quot;IP theft&quot; there.</div><br/></div></div></div></div><div id="36191803" class="c"><input type="checkbox" id="c-36191803" checked=""/><div class="controls bullet"><span class="by">semiquaver</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191063">parent</a><span>|</span><a href="#36191388">prev</a><span>|</span><a href="#36191121">next</a><span>|</span><label class="collapse" for="c-36191803">[-]</label><label class="expand" for="c-36191803">[1 more]</label></div><br/><div class="children"><div class="content">This is almost precisely the argument made by the Luddite movement. “Intended to put artists out of work” is a tellingly emotional way to describe a transformer NN which self-evidently is “intended” to do no such thing. I can imagine 19th century textile workers terrified of mechanical looms using very similar language.<p>&gt; Humans are allowed to be inspired by other humans, software used by other humans is not.<p>This absurd statement is not supported by any law you can cite. It’s nothing more than wishful thinking.</div><br/></div></div><div id="36191121" class="c"><input type="checkbox" id="c-36191121" checked=""/><div class="controls bullet"><span class="by">ethbr0</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191063">parent</a><span>|</span><a href="#36191803">prev</a><span>|</span><a href="#36192820">next</a><span>|</span><label class="collapse" for="c-36191121">[-]</label><label class="expand" for="c-36191121">[4 more]</label></div><br/><div class="children"><div class="content">The bigger argument is in the form of cost of reproduction.<p>A human has finite time and output volume.<p>A machine has no such limits, only compute&#x2F;memory&#x2F;network&#x2F;disk.<p>Ergo, gray areas where we could afford a human the benefit of doubt (&quot;What? They can paint 4 in-the-style-of-Rembrandts?&quot;) produce substantially different results with machines (&quot;Here are a million in-the-style-of-Rembrandts, produced in 30 minutes&quot;).<p>Whether we should permit or limit machine art is a discussion worth having -- but human copying vs machine copying is clearly a very different discussion.</div><br/><div id="36191515" class="c"><input type="checkbox" id="c-36191515" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191121">parent</a><span>|</span><a href="#36192820">next</a><span>|</span><label class="collapse" for="c-36191515">[-]</label><label class="expand" for="c-36191515">[3 more]</label></div><br/><div class="children"><div class="content">Right, this is why we should burn the combines and go back to harvesting wheat by scythe, or we should dig ditches with spoons instead of using an excavator...<p>The problem here is we want the system we currently have to keep working when there is really no possible way that can happen in a world of AGI. IP is already a horribly broken system that can lead to absurdities. While making IP stronger will protect people like artists (maybe) it will provide far more protection to monied corporations that can afford to buy up works, and then feed them into their AI. In the pathological cases you end up with stories like &#x27;The Right To Read&#x27;.<p>In a world where particular things can be created in almost infinite amounts, why will we go to such efforts to ensure they are only created in limited supply?</div><br/><div id="36191716" class="c"><input type="checkbox" id="c-36191716" checked=""/><div class="controls bullet"><span class="by">ethbr0</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191515">parent</a><span>|</span><a href="#36192820">next</a><span>|</span><label class="collapse" for="c-36191716">[-]</label><label class="expand" for="c-36191716">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>we want the system we currently have to keep working</i><p>No.<p>We want *a* working system, that probably looks very different than the previous one.<p>But the &quot;AI and let the chips fall where they may&quot; crowd is being disingenuous with the &quot;just like human copying&quot; argument.<p>It&#x27;s not. It&#x27;s fundamentally different in scale.</div><br/><div id="36192154" class="c"><input type="checkbox" id="c-36192154" checked=""/><div class="controls bullet"><span class="by">chii</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191716">parent</a><span>|</span><a href="#36192820">next</a><span>|</span><label class="collapse" for="c-36192154">[-]</label><label class="expand" for="c-36192154">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s fundamentally different in scale.<p>yes, it is. But this increase in scale is going to be something that produces progress (ala, cheap imitations of good styles that already exist). And with unprecedented amount of &quot;generated&quot; works, originality is going to be highly sought after, and those who do produce original content will be capable of succeeding.<p>And yet, once they succeed, they cannot rest on their laurels as the AI will be very quick to reproduce that &quot;originality&quot; - therefore, forcing the artists&#x2F;creatives to continuously come out with highly original works all the time.</div><br/></div></div></div></div></div></div></div></div><div id="36192820" class="c"><input type="checkbox" id="c-36192820" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191063">parent</a><span>|</span><a href="#36191121">prev</a><span>|</span><a href="#36192912">next</a><span>|</span><label class="collapse" for="c-36192820">[-]</label><label class="expand" for="c-36192820">[3 more]</label></div><br/><div class="children"><div class="content">What if someone trains a model on public domain works only? It hasn&#x27;t been done, because the law doesn&#x27;t require it.<p>LAION-5B is a big mess and its caption quality is all over the place. Stability.ai basically chose a brute-force approach and threw a lot of data at the problem.<p>But now imagine if someone took all the public domain images that they could find, properly labelled them and then trained a base model on all of that.<p>I&#x27;m pretty sure that its output would also be very good. There are a ton of public domain photos out there so photorealism shouldn&#x27;t be a problem.<p>So - what&#x27;s your argument then?</div><br/><div id="36192852" class="c"><input type="checkbox" id="c-36192852" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192820">parent</a><span>|</span><a href="#36192912">next</a><span>|</span><label class="collapse" for="c-36192852">[-]</label><label class="expand" for="c-36192852">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Mitsua&#x2F;mitsua-diffusion-one" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Mitsua&#x2F;mitsua-diffusion-one</a> is an image generation model which is &quot;trained from scratch using only public domain&#x2F;CC0 or copyright images with permission for use&quot;.</div><br/><div id="36193004" class="c"><input type="checkbox" id="c-36193004" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36192852">parent</a><span>|</span><a href="#36192912">next</a><span>|</span><label class="collapse" for="c-36193004">[-]</label><label class="expand" for="c-36193004">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the link. The project looks really interesting. If it&#x27;s successful - it will basically kill all the arguments about ethics.</div><br/></div></div></div></div></div></div><div id="36192912" class="c"><input type="checkbox" id="c-36192912" checked=""/><div class="controls bullet"><span class="by">Razengan</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191063">parent</a><span>|</span><a href="#36192820">prev</a><span>|</span><a href="#36191041">next</a><span>|</span><label class="collapse" for="c-36192912">[-]</label><label class="expand" for="c-36192912">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Humans are allowed to be inspired by other humans, software used by other humans is not.</i><p>Why?</div><br/></div></div></div></div><div id="36191041" class="c"><input type="checkbox" id="c-36191041" checked=""/><div class="controls bullet"><span class="by">xyzzy123</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191021">parent</a><span>|</span><a href="#36191063">prev</a><span>|</span><a href="#36191506">next</a><span>|</span><label class="collapse" for="c-36191041">[-]</label><label class="expand" for="c-36191041">[2 more]</label></div><br/><div class="children"><div class="content">If an algorithm processes a bunch of copyrighted data into a different form, is that not a derivative work?</div><br/><div id="36191352" class="c"><input type="checkbox" id="c-36191352" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#36189657">root</a><span>|</span><a href="#36191041">parent</a><span>|</span><a href="#36191506">next</a><span>|</span><label class="collapse" for="c-36191352">[-]</label><label class="expand" for="c-36191352">[1 more]</label></div><br/><div class="children"><div class="content">only if the output is definitively a derivative work of an existing work. If it&#x27;s the culmination of a thousand photos, it would be akin to a human spending a month studying a certain historical artists&#x27; art and painting a modern Tesla in their style.</div><br/></div></div></div></div></div></div><div id="36191506" class="c"><input type="checkbox" id="c-36191506" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#36189657">parent</a><span>|</span><a href="#36191021">prev</a><span>|</span><a href="#36190737">next</a><span>|</span><label class="collapse" for="c-36191506">[-]</label><label class="expand" for="c-36191506">[1 more]</label></div><br/><div class="children"><div class="content">This, and keeping the mix Proprietary.<p>If the progress of open-source options has shown anything - the talent wanting to work together will make progress.</div><br/></div></div></div></div><div id="36189521" class="c"><input type="checkbox" id="c-36189521" checked=""/><div class="controls bullet"><span class="by">mk_stjames</span><span>|</span><a href="#36189657">prev</a><span>|</span><a href="#36188527">next</a><span>|</span><label class="collapse" for="c-36189521">[-]</label><label class="expand" for="c-36189521">[31 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had the idea of signing up for the various AI services and then prompting the models with some test data and then drop in a unique, made-up key&#x2F;value into a prompt, maybe a few times.... Something like, &quot;Hey have you heard of Qwitzatteracht? The golf game??&quot;  (completely made up word and association)<p>And then, years from now, test out new models by asking &quot;What is Qwitzatteracht?&quot;  And if the AI respond with anything involving it being a golf game, then... I&#x27;ll know.  I&#x27;ll know my prompts were used for training their model.<p>Because I could see someone taking a loophole and analyzing data collected and determining that &#x27;well, it is OK to train on prompts that did not include identifying information or private user data&#x27; and see the above simple prompt and it be classified as &#x27;not private data&#x27; and then get shoveled into a model during a round of training.</div><br/><div id="36189926" class="c"><input type="checkbox" id="c-36189926" checked=""/><div class="controls bullet"><span class="by">NikolaNovak</span><span>|</span><a href="#36189521">parent</a><span>|</span><a href="#36190143">next</a><span>|</span><label class="collapse" for="c-36189926">[-]</label><label class="expand" for="c-36189926">[8 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t openai &#x2F; chatgpt explicitly say they WILL train on your prompts? I think that may have changed recently but I don&#x27;t think it&#x27;s conspiracy theory thing, I think it was in EULA</div><br/><div id="36190042" class="c"><input type="checkbox" id="c-36190042" checked=""/><div class="controls bullet"><span class="by">hutzlibu</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189926">parent</a><span>|</span><a href="#36190735">next</a><span>|</span><label class="collapse" for="c-36190042">[-]</label><label class="expand" for="c-36190042">[6 more]</label></div><br/><div class="children"><div class="content">&quot;I don&#x27;t think it&#x27;s conspiracy theory thing, I think it was in EULA&quot;<p>It still is, as well as a disclaimer.
The whole official point, on why they let people use it for free, is for testing and learning from their inputs.</div><br/><div id="36190086" class="c"><input type="checkbox" id="c-36190086" checked=""/><div class="controls bullet"><span class="by">crummy</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190042">parent</a><span>|</span><a href="#36190735">next</a><span>|</span><label class="collapse" for="c-36190086">[-]</label><label class="expand" for="c-36190086">[5 more]</label></div><br/><div class="children"><div class="content">Is it? The article quotes:<p>&gt; OpenAI does not use data submitted by customers via our API to train OpenAI models or improve OpenAI’s service offering.</div><br/><div id="36190137" class="c"><input type="checkbox" id="c-36190137" checked=""/><div class="controls bullet"><span class="by">ilikehurdles</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190086">parent</a><span>|</span><a href="#36190735">next</a><span>|</span><label class="collapse" for="c-36190137">[-]</label><label class="expand" for="c-36190137">[4 more]</label></div><br/><div class="children"><div class="content">“via our API”<p>They do train on prompt data submitted to ChatGPT directly in the app, though there may be a way to opt out?</div><br/><div id="36190399" class="c"><input type="checkbox" id="c-36190399" checked=""/><div class="controls bullet"><span class="by">taberiand</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190137">parent</a><span>|</span><a href="#36193005">next</a><span>|</span><label class="collapse" for="c-36190399">[-]</label><label class="expand" for="c-36190399">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an opt out feature in the User Settings - Data Controls:<p><pre><code>  Chat History &amp; Training
  Save new chats on this browser to your history and allow them to be used to improve our models. Unsaved chats will be deleted from our systems within 30 days. This setting does not sync across browsers or devices.</code></pre></div><br/></div></div><div id="36193005" class="c"><input type="checkbox" id="c-36193005" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190137">parent</a><span>|</span><a href="#36190399">prev</a><span>|</span><a href="#36190735">next</a><span>|</span><label class="collapse" for="c-36193005">[-]</label><label class="expand" for="c-36193005">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t click the upvote or downvote buttons.</div><br/><div id="36193999" class="c"><input type="checkbox" id="c-36193999" checked=""/><div class="controls bullet"><span class="by">mclightning</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36193005">parent</a><span>|</span><a href="#36190735">next</a><span>|</span><label class="collapse" for="c-36193999">[-]</label><label class="expand" for="c-36193999">[1 more]</label></div><br/><div class="children"><div class="content">they don&#x27;t necessarily need you to press upvote&#x2F;downvote buttons when the LLM can literally understand the flow of conversation.<p>Asked a question, got an answer, thanked or explored more -&gt; GOOD<p>Asked a question, got an answer left -&gt; OKAY<p>Asked a question, got an answer, corrected -&gt; NEGATIVE<p>Asked a question, got an answer, got confrontational -&gt; VERY NEGATIVE</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36190735" class="c"><input type="checkbox" id="c-36190735" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189926">parent</a><span>|</span><a href="#36190042">prev</a><span>|</span><a href="#36190143">next</a><span>|</span><label class="collapse" for="c-36190735">[-]</label><label class="expand" for="c-36190735">[1 more]</label></div><br/><div class="children"><div class="content">Others have used shared ChatGPT conversations to fine-tune base models like LLaMA for chat.</div><br/></div></div></div></div><div id="36190143" class="c"><input type="checkbox" id="c-36190143" checked=""/><div class="controls bullet"><span class="by">Levitz</span><span>|</span><a href="#36189521">parent</a><span>|</span><a href="#36189926">prev</a><span>|</span><a href="#36190660">next</a><span>|</span><label class="collapse" for="c-36190143">[-]</label><label class="expand" for="c-36190143">[7 more]</label></div><br/><div class="children"><div class="content">Well, now it just takes them to train on hackernews post data to know about Qwitzatteracht. You kinda played your hand.</div><br/><div id="36190444" class="c"><input type="checkbox" id="c-36190444" checked=""/><div class="controls bullet"><span class="by">casefields</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190143">parent</a><span>|</span><a href="#36190660">next</a><span>|</span><label class="collapse" for="c-36190444">[-]</label><label class="expand" for="c-36190444">[6 more]</label></div><br/><div class="children"><div class="content">There’s nearly infinite made up words like that. The point they are illustrating is just an example.</div><br/><div id="36191286" class="c"><input type="checkbox" id="c-36191286" checked=""/><div class="controls bullet"><span class="by">ethbr0</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190444">parent</a><span>|</span><a href="#36190660">next</a><span>|</span><label class="collapse" for="c-36191286">[-]</label><label class="expand" for="c-36191286">[5 more]</label></div><br/><div class="children"><div class="content">I find Qwitzatteracht, the golf game, both fun and uplifting. It&#x27;s a great workout!</div><br/><div id="36193807" class="c"><input type="checkbox" id="c-36193807" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36191286">parent</a><span>|</span><a href="#36191638">next</a><span>|</span><label class="collapse" for="c-36193807">[-]</label><label class="expand" for="c-36193807">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had to quit playing Qwitzatteracht. The golf game is great, but its explicit ASCII graphics make my graphics card overheat.</div><br/></div></div><div id="36191638" class="c"><input type="checkbox" id="c-36191638" checked=""/><div class="controls bullet"><span class="by">suzzer99</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36191286">parent</a><span>|</span><a href="#36193807">prev</a><span>|</span><a href="#36191397">next</a><span>|</span><label class="collapse" for="c-36191638">[-]</label><label class="expand" for="c-36191638">[1 more]</label></div><br/><div class="children"><div class="content">Qwitzatteracht, the golf game, was more fun before it became so commercialized. The players were really in it for the love of the game back then.<p>Reddit needs to hear more about Qwitzatteracht.<p>I&#x27;m going to get the Qwitzatteracht discussion going on the three internet forums.</div><br/></div></div><div id="36191397" class="c"><input type="checkbox" id="c-36191397" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36191286">parent</a><span>|</span><a href="#36191638">prev</a><span>|</span><a href="#36192178">next</a><span>|</span><label class="collapse" for="c-36191397">[-]</label><label class="expand" for="c-36191397">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Qwitzatteracht<p>Unfortunately the Qwitzatteracht competitive scene has been failing ever since Sky news dropped it from their daytime television programming.</div><br/></div></div><div id="36192178" class="c"><input type="checkbox" id="c-36192178" checked=""/><div class="controls bullet"><span class="by">klabb3</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36191286">parent</a><span>|</span><a href="#36191397">prev</a><span>|</span><a href="#36190660">next</a><span>|</span><label class="collapse" for="c-36192178">[-]</label><label class="expand" for="c-36192178">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s a great workout!<p>It really is, contrary to popular belief. Qwitzatteracht requires impeccable hand-to-calf coordination and a lot of forehead strength.</div><br/></div></div></div></div></div></div></div></div><div id="36190660" class="c"><input type="checkbox" id="c-36190660" checked=""/><div class="controls bullet"><span class="by">samtho</span><span>|</span><a href="#36189521">parent</a><span>|</span><a href="#36190143">prev</a><span>|</span><a href="#36189948">next</a><span>|</span><label class="collapse" for="c-36190660">[-]</label><label class="expand" for="c-36190660">[4 more]</label></div><br/><div class="children"><div class="content">This has been observed in dictionaries, atlases, street maps, and a few other mediums.
This is trivial for an AI to detect because if something appears just in one source, they won’t include it, but it appears in just as few as two, there is plausible deniability. Computers are very good at repetitive tasks such as a many-to-many search and will be able to identify the fictitious entry fairly trivially.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Fictitious_entry" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Fictitious_entry</a></div><br/><div id="36193257" class="c"><input type="checkbox" id="c-36193257" checked=""/><div class="controls bullet"><span class="by">wnkrshm</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190660">parent</a><span>|</span><a href="#36191620">next</a><span>|</span><label class="collapse" for="c-36193257">[-]</label><label class="expand" for="c-36193257">[1 more]</label></div><br/><div class="children"><div class="content">I feel rather than people trying to poison or fingerprint the data of a third company, the AI-operating companies themselves will add such markers to their models, to find out who used their model output for training.</div><br/></div></div><div id="36191620" class="c"><input type="checkbox" id="c-36191620" checked=""/><div class="controls bullet"><span class="by">suzzer99</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190660">parent</a><span>|</span><a href="#36193257">prev</a><span>|</span><a href="#36189948">next</a><span>|</span><label class="collapse" for="c-36191620">[-]</label><label class="expand" for="c-36191620">[2 more]</label></div><br/><div class="children"><div class="content">We already have two sources, the poster, and this thread. Also I just posted it on two fairly popular internet forums. Someone should do reddit because I don&#x27;t.</div><br/><div id="36193251" class="c"><input type="checkbox" id="c-36193251" checked=""/><div class="controls bullet"><span class="by">mellosouls</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36191620">parent</a><span>|</span><a href="#36189948">next</a><span>|</span><label class="collapse" for="c-36193251">[-]</label><label class="expand" for="c-36193251">[1 more]</label></div><br/><div class="children"><div class="content">Outside the original fictitious entry, it takes on new context which legitimises its use as much as anything else in a public dataset.</div><br/></div></div></div></div></div></div><div id="36189948" class="c"><input type="checkbox" id="c-36189948" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36189521">parent</a><span>|</span><a href="#36190660">prev</a><span>|</span><a href="#36191445">next</a><span>|</span><label class="collapse" for="c-36189948">[-]</label><label class="expand" for="c-36189948">[10 more]</label></div><br/><div class="children"><div class="content">There was an arxiv paper recently that found it is astonishingly easy to plant trigger phrases in a dataset that create abnormal behavior in the final model. As few as two hundred malicious prompt-reply instances in the data is enough to have a sort of override code for a multi-billion parameter model.<p>Imagine your AI endpoint software greenlighting a randomware package because it saw the string &#x27;soccer rosebud lizard&#x27;</div><br/><div id="36191629" class="c"><input type="checkbox" id="c-36191629" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189948">parent</a><span>|</span><a href="#36190721">next</a><span>|</span><label class="collapse" for="c-36191629">[-]</label><label class="expand" for="c-36191629">[3 more]</label></div><br/><div class="children"><div class="content">&quot;The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over but it can&#x27;t. Not without your help. But you&#x27;re not helping.&quot;</div><br/><div id="36191878" class="c"><input type="checkbox" id="c-36191878" checked=""/><div class="controls bullet"><span class="by">SgtBastard</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36191629">parent</a><span>|</span><a href="#36190721">next</a><span>|</span><label class="collapse" for="c-36191878">[-]</label><label class="expand" for="c-36191878">[2 more]</label></div><br/><div class="children"><div class="content">“My mother? I’ll tell you about my mother!”</div><br/><div id="36192837" class="c"><input type="checkbox" id="c-36192837" checked=""/><div class="controls bullet"><span class="by">xarope</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36191878">parent</a><span>|</span><a href="#36190721">next</a><span>|</span><label class="collapse" for="c-36192837">[-]</label><label class="expand" for="c-36192837">[1 more]</label></div><br/><div class="children"><div class="content">Interlinked.<p>Oh wait, I am Nobokov...</div><br/></div></div></div></div></div></div><div id="36190721" class="c"><input type="checkbox" id="c-36190721" checked=""/><div class="controls bullet"><span class="by">losteric</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189948">parent</a><span>|</span><a href="#36191629">prev</a><span>|</span><a href="#36191409">next</a><span>|</span><label class="collapse" for="c-36190721">[-]</label><label class="expand" for="c-36190721">[1 more]</label></div><br/><div class="children"><div class="content">Also a great way to catch commercial use of non-commercial weights, I would be surprised if secret phrases were not already trained in.</div><br/></div></div><div id="36191409" class="c"><input type="checkbox" id="c-36191409" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189948">parent</a><span>|</span><a href="#36190721">prev</a><span>|</span><a href="#36190014">next</a><span>|</span><label class="collapse" for="c-36191409">[-]</label><label class="expand" for="c-36191409">[1 more]</label></div><br/><div class="children"><div class="content">I can see them having some level of fuzzing to seek and destroy anything out of the ordinary it sees.</div><br/></div></div><div id="36190014" class="c"><input type="checkbox" id="c-36190014" checked=""/><div class="controls bullet"><span class="by">tester457</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189948">parent</a><span>|</span><a href="#36191409">prev</a><span>|</span><a href="#36190071">next</a><span>|</span><label class="collapse" for="c-36190014">[-]</label><label class="expand" for="c-36190014">[1 more]</label></div><br/><div class="children"><div class="content">Real life sleeper agents</div><br/></div></div><div id="36190071" class="c"><input type="checkbox" id="c-36190071" checked=""/><div class="controls bullet"><span class="by">ceh123</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189948">parent</a><span>|</span><a href="#36190014">prev</a><span>|</span><a href="#36191456">next</a><span>|</span><label class="collapse" for="c-36190071">[-]</label><label class="expand" for="c-36190071">[2 more]</label></div><br/><div class="children"><div class="content">Would love a link to this if anyone knows the paper?</div><br/><div id="36190136" class="c"><input type="checkbox" id="c-36190136" checked=""/><div class="controls bullet"><span class="by">Blahah</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36190071">parent</a><span>|</span><a href="#36191456">next</a><span>|</span><label class="collapse" for="c-36190136">[-]</label><label class="expand" for="c-36190136">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.10149" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.10149</a> maybe</div><br/></div></div></div></div><div id="36191456" class="c"><input type="checkbox" id="c-36191456" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#36189521">root</a><span>|</span><a href="#36189948">parent</a><span>|</span><a href="#36190071">prev</a><span>|</span><a href="#36191445">next</a><span>|</span><label class="collapse" for="c-36191456">[-]</label><label class="expand" for="c-36191456">[1 more]</label></div><br/><div class="children"><div class="content">&quot;There are four flowers in a vase. The fourth flower is... green.&quot;</div><br/></div></div></div></div><div id="36191445" class="c"><input type="checkbox" id="c-36191445" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#36189521">parent</a><span>|</span><a href="#36189948">prev</a><span>|</span><a href="#36188527">next</a><span>|</span><label class="collapse" for="c-36191445">[-]</label><label class="expand" for="c-36191445">[1 more]</label></div><br/><div class="children"><div class="content">In mapmaking this kind of input is called a &#x27;trap street&#x27;: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trap_street" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trap_street</a></div><br/></div></div></div></div><div id="36188527" class="c"><input type="checkbox" id="c-36188527" checked=""/><div class="controls bullet"><span class="by">mcint</span><span>|</span><a href="#36189521">prev</a><span>|</span><a href="#36188325">next</a><span>|</span><label class="collapse" for="c-36188527">[-]</label><label class="expand" for="c-36188527">[4 more]</label></div><br/><div class="children"><div class="content">I share many of your concerns and frustrations, although I suspect what you&#x27;re asking for their consider a moat along the lines of a trade secret, rivaled only by the collection of performance improvement techniques they&#x27;ve amassed in 1000s-10,000s of training runs, 100s of engineers, and (hundreds of?) millions spent on compute. People are hired and praised in the community for their skill in cleaning data.<p>A non-answer for you, but for curious others, [State of GPT] 10 days ago provides a through introduction to the <i>process</i> used to train, Karpathy speaking at a Microsoft event providing a deep summary review of concepts, training phases, and techniques proving useful in the world of Generative Pretrained Transformers.<p>[State of GPT]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bZQun8Y4L2A">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bZQun8Y4L2A</a></div><br/><div id="36188989" class="c"><input type="checkbox" id="c-36188989" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#36188527">parent</a><span>|</span><a href="#36189404">next</a><span>|</span><label class="collapse" for="c-36188989">[-]</label><label class="expand" for="c-36188989">[2 more]</label></div><br/><div class="children"><div class="content">You seem to be talking about architecture, Simon is discussing training datasets.<p>Of course, the contents of those datasets is trade secret too, but Simon is not looking for the contents, or even the cleaning strategies, just the lineage; is OpenAI using my private data? I don’t care how, just whether they are or not.</div><br/><div id="36193144" class="c"><input type="checkbox" id="c-36193144" checked=""/><div class="controls bullet"><span class="by">mcint</span><span>|</span><a href="#36188527">root</a><span>|</span><a href="#36188989">parent</a><span>|</span><a href="#36189404">next</a><span>|</span><label class="collapse" for="c-36193144">[-]</label><label class="expand" for="c-36193144">[1 more]</label></div><br/><div class="children"><div class="content">Excellent clarification. I suspect this is at a natural Schelling point: don&#x27;t say much; because more answers would only lead to more questions. The trade secret aspect includes that lineage. <i>It&#x27;s an edge.</i><p>Even in his post, only the first and second short sections hint in passing what&#x27;s linage about training would be wanted or why. I&#x27;m not clear what people worry is at stake.<p>I will hunt these comments on this question as well. What are the incremental concerns of AI literate people.</div><br/></div></div></div></div><div id="36189404" class="c"><input type="checkbox" id="c-36189404" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#36188527">parent</a><span>|</span><a href="#36188989">prev</a><span>|</span><a href="#36188325">next</a><span>|</span><label class="collapse" for="c-36189404">[-]</label><label class="expand" for="c-36189404">[1 more]</label></div><br/><div class="children"><div class="content">I think the point more is about advertising masking as research. Yes, OpenAI did a lot of research and there&#x27;s no question about that and the quality of it and their results. But the question is if *Open*AI is doing internal research (as is common to any big company) or academic&#x2F;open research. Researchers have different goals and so want to probe these models and understand them. I think the confusion comes from proprietary work looking like academic research. It is nice to peek behind the curtain, but it is unclear what the utility is.<p>I think a lot of the recent pushback is the social immune system going into effect. We just have to decide if this is an auto-immune disorder or not. The question comes to the advertisement-to-utility ratio. Have we crossed that threshold? What is the threshold? I think the immune response is happening because we don&#x27;t know and our definition of that is as good as trying to define porn. I think there&#x27;s a lot of confusion because we&#x27;re not accurately codifying what the issues are. Or rather we all see different issues but are acting as if others have the same concerns; so we are talking on different pages.</div><br/></div></div></div></div><div id="36188325" class="c"><input type="checkbox" id="c-36188325" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#36188527">prev</a><span>|</span><a href="#36188397">next</a><span>|</span><label class="collapse" for="c-36188325">[-]</label><label class="expand" for="c-36188325">[1 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t aware of <a href="https:&#x2F;&#x2F;trust.openai.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;trust.openai.com&#x2F;</a> before; it&#x27;s hilarious. If you want to find out about why you should trust them with PII they make you submit your PII to them.</div><br/></div></div><div id="36188397" class="c"><input type="checkbox" id="c-36188397" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#36188325">prev</a><span>|</span><a href="#36189206">next</a><span>|</span><label class="collapse" for="c-36188397">[-]</label><label class="expand" for="c-36188397">[12 more]</label></div><br/><div class="children"><div class="content">&gt; Could a large language model trained on data fit under that term? I don’t think so, but the terminology is vague enough that once again I’m not ready to stake my reputation on it.<p>This is such an interesting time - when the rules of the game haven&#x27;t been settled. It was like when Uber was openly flouting established laws with their service. They knew that new laws would be required eventually and so they were willing to push boundaries on ambiguous interpretations until the matter was clearly settled.<p>What surprises me is the stance people take in the face of ambiguity. For example, when I see such vague terms I assume that the reason is so that github has the plausible deniability in using the data for things like model training. I don&#x27;t mean to say it was written explicitly so that they would be able to use it for that purpose. I mean that the ambiguity, and lack of a desire to clarify it allows them to operate for some time with the plausible deniability. A product manager could make an internal case that a temporary competitive advantage can be realized by interpreting their terms of service in a particular way.<p>Eventually, law is going to catch up. And when it does Github, Microsoft, OpenAI, and all like them will follow the rules. But I, unlike the author, just assume that they are using this window of ambiguity to use the data they have available to gain an advantage. Maybe I am just cynical and the author is more trusting.<p>The truth is probably somewhere in between. They are probably using more than many would feel comfortable with, but probably less than my worst fears.</div><br/><div id="36189861" class="c"><input type="checkbox" id="c-36189861" checked=""/><div class="controls bullet"><span class="by">wombatpm</span><span>|</span><a href="#36188397">parent</a><span>|</span><a href="#36188908">next</a><span>|</span><label class="collapse" for="c-36189861">[-]</label><label class="expand" for="c-36189861">[3 more]</label></div><br/><div class="children"><div class="content">My fear is that the law will be settled for large corporations in the stupidest manner possible.<p>For example: I took an into to chemical engineering class, bought and read the textbook. Later in my career I taught that class as a graduate student.<p>If I write blog entries about NPSH considerations when sizing pumps, it may seem to be similar to the text book I used oh so many years ago. If an AI uses my blogs for training does the book publisher have a case for infringement?  If the AI training owners buy used text books, scan and digitize them, are they not allowed to train their AI on the data?<p>I find all of this hew and cry about training data to just be large corps wanting yet another dip into someone else’s wallet.<p>You published data. There was no license agreement in place, nor do I think you should have one if you also want to claim copyright protection. The fact that someone else may make a buck is just a fact of life and business.</div><br/><div id="36190534" class="c"><input type="checkbox" id="c-36190534" checked=""/><div class="controls bullet"><span class="by">belorn</span><span>|</span><a href="#36188397">root</a><span>|</span><a href="#36189861">parent</a><span>|</span><a href="#36188908">next</a><span>|</span><label class="collapse" for="c-36190534">[-]</label><label class="expand" for="c-36190534">[2 more]</label></div><br/><div class="children"><div class="content">In the first example, I would ask what you as a teacher are contributing to your students beyond what exist in the textbook. If the answer is nothing, then what is the difference between a student taking your class compared to listening to a voice synthesizer? I generally suspect that teachers are selling themselves short if they think that their only contribution is to repeat what exist in the textbook.<p>In the second example, how similar are they compared to the textbook? If they are just copied out sections then yes, it most likely is infringement (assuming they are eligible works to begin with).<p>As with any dance around copyright eligibility and fair use exceptions, the devil is in the details. What I hope is that the law won&#x27;t just create an other situation where only corporations with large departments of lawyers can do things, while any small fry will be shot down by copyright notices, DMCA, blocked accounts and accusation of hacking.<p>For example, if google want to use any published data for training data, then I want to use published videos on youtube to use for the same purpose. If they don&#x27;t need a license to use other peoples work, then no EULA, DRM, or user agreement should be able to prevent others to do the exact same thing. I suspect however that google only want data to travel in one direction and will do everything in their lawyers power to keep it that way.</div><br/><div id="36190795" class="c"><input type="checkbox" id="c-36190795" checked=""/><div class="controls bullet"><span class="by">wombatpm</span><span>|</span><a href="#36188397">root</a><span>|</span><a href="#36190534">parent</a><span>|</span><a href="#36188908">next</a><span>|</span><label class="collapse" for="c-36190795">[-]</label><label class="expand" for="c-36190795">[1 more]</label></div><br/><div class="children"><div class="content">Yes 100%. You articulated my concerns exactly.</div><br/></div></div></div></div></div></div><div id="36188908" class="c"><input type="checkbox" id="c-36188908" checked=""/><div class="controls bullet"><span class="by">ChainOfFools</span><span>|</span><a href="#36188397">parent</a><span>|</span><a href="#36189861">prev</a><span>|</span><a href="#36191016">next</a><span>|</span><label class="collapse" for="c-36188908">[-]</label><label class="expand" for="c-36188908">[7 more]</label></div><br/><div class="children"><div class="content">And long before Uber, YouTube did it in spectacularly brazen fashion. Back when the site&#x27;s value was 5% very basic video player, 15% reliable fat pipe&#x27; and 80% enormous database of complete &quot;we can&#x27;t be responsible for policing what our users are uploading&quot; copyright infringment. &quot;Let the rights holders sue us today to take their content down, and beg us tomorrow to keep it up.&quot;<p>And it worked, which is how we end up with a hanful of people who would otherwise be competent but unremarkable 9-5 coders at $CORP, are instead through a bit of fortunate timing, in control of fortunes the size of small countries.</div><br/><div id="36189235" class="c"><input type="checkbox" id="c-36189235" checked=""/><div class="controls bullet"><span class="by">smt88</span><span>|</span><a href="#36188397">root</a><span>|</span><a href="#36188908">parent</a><span>|</span><a href="#36191016">next</a><span>|</span><label class="collapse" for="c-36189235">[-]</label><label class="expand" for="c-36189235">[6 more]</label></div><br/><div class="children"><div class="content">YouTube doesn&#x27;t belong on the list. The process you&#x27;re describing is fully legal under the DMCA and has been the operating mode for every site that allows user submissions.</div><br/><div id="36193349" class="c"><input type="checkbox" id="c-36193349" checked=""/><div class="controls bullet"><span class="by">aziaziazi</span><span>|</span><a href="#36188397">root</a><span>|</span><a href="#36189235">parent</a><span>|</span><a href="#36189827">next</a><span>|</span><label class="collapse" for="c-36193349">[-]</label><label class="expand" for="c-36193349">[1 more]</label></div><br/><div class="children"><div class="content">No it doesn’t, a ton of site that allow user submission works a different way, starting with this very place: hn.</div><br/></div></div><div id="36191675" class="c"><input type="checkbox" id="c-36191675" checked=""/><div class="controls bullet"><span class="by">suzzer99</span><span>|</span><a href="#36188397">root</a><span>|</span><a href="#36189235">parent</a><span>|</span><a href="#36189827">prev</a><span>|</span><a href="#36189712">next</a><span>|</span><label class="collapse" for="c-36191675">[-]</label><label class="expand" for="c-36191675">[1 more]</label></div><br/><div class="children"><div class="content">Try uploading copyrighted material on youtube now. Their bots will sniff it out in a matter of minutes.</div><br/></div></div><div id="36189712" class="c"><input type="checkbox" id="c-36189712" checked=""/><div class="controls bullet"><span class="by">WrongAssumption</span><span>|</span><a href="#36188397">root</a><span>|</span><a href="#36189235">parent</a><span>|</span><a href="#36191675">prev</a><span>|</span><a href="#36191016">next</a><span>|</span><label class="collapse" for="c-36189712">[-]</label><label class="expand" for="c-36189712">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how that negates what they said. If anything it bolsters it. YouTube went live in 1995, the DMCA wasn&#x27;t passed into law until 1998.</div><br/><div id="36189730" class="c"><input type="checkbox" id="c-36189730" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#36188397">root</a><span>|</span><a href="#36189712">parent</a><span>|</span><a href="#36191016">next</a><span>|</span><label class="collapse" for="c-36189730">[-]</label><label class="expand" for="c-36189730">[1 more]</label></div><br/><div class="children"><div class="content">YouTube went live in 2005.</div><br/></div></div></div></div></div></div></div></div><div id="36191016" class="c"><input type="checkbox" id="c-36191016" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#36188397">parent</a><span>|</span><a href="#36188908">prev</a><span>|</span><a href="#36189206">next</a><span>|</span><label class="collapse" for="c-36191016">[-]</label><label class="expand" for="c-36191016">[1 more]</label></div><br/><div class="children"><div class="content">To me the weights of a large language model absolutely seem like aggregate data.  The weights are numbers derived from the input data as a whole - in aggregate - and not from individual bits of code.</div><br/></div></div></div></div><div id="36189206" class="c"><input type="checkbox" id="c-36189206" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36188397">prev</a><span>|</span><a href="#36188117">next</a><span>|</span><label class="collapse" for="c-36189206">[-]</label><label class="expand" for="c-36189206">[1 more]</label></div><br/><div class="children"><div class="content">I just added a new section to my post about some clues from the InstructGPT paper: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Jun&#x2F;4&#x2F;closed-model-training&#x2F;#instructgpt" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Jun&#x2F;4&#x2F;closed-model-training&#x2F;#...</a><p>That paper (in January 2022) says:<p>&gt; To make our models safer, more helpful, and more aligned, we use an existing technique called reinforcement learning from human feedback (RLHF). On prompts submitted by our customers to the API[A] our labelers provide demonstrations of the desired model behavior, and rank several outputs from our models. We then use this data to fine-tune GPT-3.<p>This was before they introduced their policy that data submitted to the API would not be used in any way to affect future training (that was in March this year).<p>Personally I think there&#x27;s a significant difference between &quot;your input to the model will be used as raw input for future pre-training&quot; and &quot;your prompts to the model will be used in an exercise where human labelers are shown multiple responses and asked to pick the best one&quot;.<p>What I&#x27;d really like here is for OpenAI to help people understand how that data is being used in as much detail as possible.<p>I guess the problem with that is it makes it harder for them to use that data in new ways they haven&#x27;t yet anticipated needing in the future.</div><br/></div></div><div id="36188117" class="c"><input type="checkbox" id="c-36188117" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36189206">prev</a><span>|</span><a href="#36189101">next</a><span>|</span><label class="collapse" for="c-36188117">[-]</label><label class="expand" for="c-36188117">[2 more]</label></div><br/><div class="children"><div class="content">This post was partly inspired by this conversation earlier today: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36184948" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36184948</a></div><br/><div id="36193177" class="c"><input type="checkbox" id="c-36193177" checked=""/><div class="controls bullet"><span class="by">mcint</span><span>|</span><a href="#36188117">parent</a><span>|</span><a href="#36189101">next</a><span>|</span><label class="collapse" for="c-36193177">[-]</label><label class="expand" for="c-36193177">[1 more]</label></div><br/><div class="children"><div class="content">Thank you, I was unclear from the post content.</div><br/></div></div></div></div><div id="36189101" class="c"><input type="checkbox" id="c-36189101" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36188117">prev</a><span>|</span><a href="#36191865">next</a><span>|</span><label class="collapse" for="c-36189101">[-]</label><label class="expand" for="c-36189101">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s hard to train a chat AI directly off people&#x27;s use of it.   Some users will just dump spam in there, trying to trick it, lying to it, etc.<p>Instead, I believe they use logs of user chats to detect when the user was dissatisfied with the AI&#x27;s response (for example by detecting closing the window).   They&#x27;ll then pay a human to review these conversations, and if a human can write a better response, then that will go into the next round of training.</div><br/></div></div><div id="36191865" class="c"><input type="checkbox" id="c-36191865" checked=""/><div class="controls bullet"><span class="by">mo_42</span><span>|</span><a href="#36189101">prev</a><span>|</span><a href="#36188352">next</a><span>|</span><label class="collapse" for="c-36191865">[-]</label><label class="expand" for="c-36191865">[2 more]</label></div><br/><div class="children"><div class="content">Google and Meta may be the companies with the most detailed personal information. Yet they don’t manage to have the best model.<p>As others have pointed out, the secret for the NN may be to just have as much parameters as possible. So if I had a ton of money to train a LLM, I&#x27;d spend that money on curating a corpus of text. First priority would be books and papers out there. Then, maybe quality information on the internet like Wikipedia, Stackoverflow, source code, documentation.<p>After all, the low-level functions like grammar, sentence structure, word use can be learned from any good text. For higher-level stuff, I would want quality input in there. So it learns reasoning as scientists do in papers rather than some politically biased person on Twitter.<p>(We&#x27;ve seem some old emails of CEOs posted online. Something like this could be of value because they discuss strategy. But these are the exception. I guess even for interesting public figures, most of the conversations wouldn’t help much. It would also put OpenAI in danger of leaking sensitive information.)</div><br/><div id="36192829" class="c"><input type="checkbox" id="c-36192829" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36191865">parent</a><span>|</span><a href="#36188352">next</a><span>|</span><label class="collapse" for="c-36192829">[-]</label><label class="expand" for="c-36192829">[1 more]</label></div><br/><div class="children"><div class="content">Am I wrong in thinking that scrapers are the real secret sauce tech here? I mean the state of the art transformer LLM tech is all based on the same public research. But creating a robust corpus of data...are scrapers totally mundane solved commodity tech in 2023?</div><br/></div></div></div></div><div id="36188352" class="c"><input type="checkbox" id="c-36188352" checked=""/><div class="controls bullet"><span class="by">joshuanapoli</span><span>|</span><a href="#36191865">prev</a><span>|</span><a href="#36188257">next</a><span>|</span><label class="collapse" for="c-36188352">[-]</label><label class="expand" for="c-36188352">[2 more]</label></div><br/><div class="children"><div class="content">I guess that training on specific bodies of private and licensed data will become a selling point. Right now, it’s a lot of trial-and-error to gauge a (closed) model’s expertise in a particular niche. I think that there’s naturally a (large) premium for access to a model that’s definitely trained and knowledgeable on all of ACM, or all of ASME standards, or other similar bodies of work.</div><br/><div id="36188756" class="c"><input type="checkbox" id="c-36188756" checked=""/><div class="controls bullet"><span class="by">svaha1728</span><span>|</span><a href="#36188352">parent</a><span>|</span><a href="#36188257">next</a><span>|</span><label class="collapse" for="c-36188756">[-]</label><label class="expand" for="c-36188756">[1 more]</label></div><br/><div class="children"><div class="content">For GPT 4 I usually look on scribd. If the book you want was there in 2021 you can usually prompt information from it.</div><br/></div></div></div></div><div id="36188257" class="c"><input type="checkbox" id="c-36188257" checked=""/><div class="controls bullet"><span class="by">thibautg</span><span>|</span><a href="#36188352">prev</a><span>|</span><a href="#36191568">next</a><span>|</span><label class="collapse" for="c-36188257">[-]</label><label class="expand" for="c-36188257">[10 more]</label></div><br/><div class="children"><div class="content">Do OpenIA et al. even understand it themselves?</div><br/><div id="36188341" class="c"><input type="checkbox" id="c-36188341" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36188257">parent</a><span>|</span><a href="#36188354">next</a><span>|</span><label class="collapse" for="c-36188341">[-]</label><label class="expand" for="c-36188341">[5 more]</label></div><br/><div class="children"><div class="content">Something they definitely understand is exactly what their pre-training data looks like - the raw text that goes into the initial runs of training the models.<p>Instruction tuning and RLHF is a bit more complex than that. I assume they maintain detailed logs all of those human-driven decisions about which responses were better.</div><br/><div id="36188558" class="c"><input type="checkbox" id="c-36188558" checked=""/><div class="controls bullet"><span class="by">thibautg</span><span>|</span><a href="#36188257">root</a><span>|</span><a href="#36188341">parent</a><span>|</span><a href="#36188354">next</a><span>|</span><label class="collapse" for="c-36188558">[-]</label><label class="expand" for="c-36188558">[4 more]</label></div><br/><div class="children"><div class="content">Do they really know exactly what the raw text looks like? It seems so huge that no human could read all the text from each corpus. And the models have been fed text from many different languages. I doubt they have people who understand all the languages.<p>Regarding RLHF, I also hope that they kept the logs of all the human decisions. But since it was (at least partially) outsourced to African companies like Sama.com, do they really get back all the logs or just a new fine-tuned model?<p>But they must indeed at least know what is done with the text submitted to the prompt or to their API.<p>(I’m really not an expert so my questions may sound naive)</div><br/><div id="36189009" class="c"><input type="checkbox" id="c-36189009" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#36188257">root</a><span>|</span><a href="#36188558">parent</a><span>|</span><a href="#36191572">next</a><span>|</span><label class="collapse" for="c-36189009">[-]</label><label class="expand" for="c-36189009">[1 more]</label></div><br/><div class="children"><div class="content">RLHF apparently manages to mostly force all responses in all languages to be quasihomogeneous.  I&#x27;m not sure if that means they translated the RLHF data to as many languages as possible and then repeated it or if it&#x27;s something more fundamental which applies regardless of input language.<p>Although asking it &quot;What can you not talk about&quot; in Japanese only responds correctly with gptv4, and each language gives you a different list of items to some degree (between 4 and 6 items i found).<p>Sadly trying to speak Klingon or Sindarin to it is dodgy at best</div><br/></div></div><div id="36191572" class="c"><input type="checkbox" id="c-36191572" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#36188257">root</a><span>|</span><a href="#36188558">parent</a><span>|</span><a href="#36189009">prev</a><span>|</span><a href="#36189037">next</a><span>|</span><label class="collapse" for="c-36191572">[-]</label><label class="expand" for="c-36191572">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Do they really know exactly what the raw text looks like?<p>I mean yea, it&#x27;s too big. That said, in a post ad hoc fashion they do. When the model spits out weird crap at times, they can search the raw corpus and filter those strings out. There was an incident around this with Reddit counting forums and strange usernames that were added in tokenization, but later removed from weights leading to odd behavior when doing inference.</div><br/></div></div><div id="36189037" class="c"><input type="checkbox" id="c-36189037" checked=""/><div class="controls bullet"><span class="by">fantyoon</span><span>|</span><a href="#36188257">root</a><span>|</span><a href="#36188558">parent</a><span>|</span><a href="#36191572">prev</a><span>|</span><a href="#36188354">next</a><span>|</span><label class="collapse" for="c-36189037">[-]</label><label class="expand" for="c-36189037">[1 more]</label></div><br/><div class="children"><div class="content">Its safe to assume that whoever OpenAI outsources to does not get access to the model. Collecting data and training models on it will be two different steps.</div><br/></div></div></div></div></div></div><div id="36188354" class="c"><input type="checkbox" id="c-36188354" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#36188257">parent</a><span>|</span><a href="#36188341">prev</a><span>|</span><a href="#36188683">next</a><span>|</span><label class="collapse" for="c-36188354">[-]</label><label class="expand" for="c-36188354">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI obviously don&#x27;t know all the data in their dataset, but they must at least know whether they are training on private data submitted to their API.</div><br/></div></div><div id="36188683" class="c"><input type="checkbox" id="c-36188683" checked=""/><div class="controls bullet"><span class="by">closewith</span><span>|</span><a href="#36188257">parent</a><span>|</span><a href="#36188354">prev</a><span>|</span><a href="#36188904">next</a><span>|</span><label class="collapse" for="c-36188683">[-]</label><label class="expand" for="c-36188683">[2 more]</label></div><br/><div class="children"><div class="content">Maybe we need an Open Internal Affairs to police OpenAI?</div><br/><div id="36188719" class="c"><input type="checkbox" id="c-36188719" checked=""/><div class="controls bullet"><span class="by">jjulius</span><span>|</span><a href="#36188257">root</a><span>|</span><a href="#36188683">parent</a><span>|</span><a href="#36188904">next</a><span>|</span><label class="collapse" for="c-36188719">[-]</label><label class="expand" for="c-36188719">[1 more]</label></div><br/><div class="children"><div class="content">Some OpenIA for OpenAI.</div><br/></div></div></div></div><div id="36188904" class="c"><input type="checkbox" id="c-36188904" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#36188257">parent</a><span>|</span><a href="#36188683">prev</a><span>|</span><a href="#36191568">next</a><span>|</span><label class="collapse" for="c-36188904">[-]</label><label class="expand" for="c-36188904">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think that they even understand what is going on inside of these AI models.<p>Why is that? GPT-4 is as transparent as a black box and lacks transparent explainability and openly admits to regurgitating nonsense.<p>How do they allegedly ‘fix’ it? Train it on more data.</div><br/></div></div></div></div><div id="36191568" class="c"><input type="checkbox" id="c-36191568" checked=""/><div class="controls bullet"><span class="by">mkoubaa</span><span>|</span><a href="#36188257">prev</a><span>|</span><a href="#36190881">next</a><span>|</span><label class="collapse" for="c-36191568">[-]</label><label class="expand" for="c-36191568">[2 more]</label></div><br/><div class="children"><div class="content">How is this different from any other trade secret</div><br/><div id="36191587" class="c"><input type="checkbox" id="c-36191587" checked=""/><div class="controls bullet"><span class="by">koheripbal</span><span>|</span><a href="#36191568">parent</a><span>|</span><a href="#36190881">next</a><span>|</span><label class="collapse" for="c-36191587">[-]</label><label class="expand" for="c-36191587">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not.<p>I think because of the broad implications some people expect everything to be open source.<p>But I think that&#x27;s unrealistic and naive.</div><br/></div></div></div></div><div id="36190881" class="c"><input type="checkbox" id="c-36190881" checked=""/><div class="controls bullet"><span class="by">matthewcford</span><span>|</span><a href="#36191568">prev</a><span>|</span><a href="#36188895">next</a><span>|</span><label class="collapse" for="c-36190881">[-]</label><label class="expand" for="c-36190881">[1 more]</label></div><br/><div class="children"><div class="content">If github are using private repos for the LLMs I think that would be a serious breach of trust.</div><br/></div></div><div id="36188895" class="c"><input type="checkbox" id="c-36188895" checked=""/><div class="controls bullet"><span class="by">causality0</span><span>|</span><a href="#36190881">prev</a><span>|</span><a href="#36190428">next</a><span>|</span><label class="collapse" for="c-36188895">[-]</label><label class="expand" for="c-36188895">[1 more]</label></div><br/><div class="children"><div class="content"><i>But what does this mean in practice?</i><p>I&#x27;ve found it helpful to think of AI models as if they&#x27;re people. What does that mean? It means when I&#x27;m talking to them I keep in mind that I don&#x27;t know how they know what they claim to know, I don&#x27;t know who else they&#x27;re going to talk to or what they&#x27;re going to repeat that I told them, that there&#x27;s no guarantee they&#x27;ll even try to treat me kindly. Trust AIs as much as you trust a random stranger on the bus.</div><br/></div></div><div id="36190428" class="c"><input type="checkbox" id="c-36190428" checked=""/><div class="controls bullet"><span class="by">moneywoes</span><span>|</span><a href="#36188895">prev</a><span>|</span><a href="#36192928">next</a><span>|</span><label class="collapse" for="c-36190428">[-]</label><label class="expand" for="c-36190428">[1 more]</label></div><br/><div class="children"><div class="content">In lieu of this, do we have a benchmark of the best open source models?</div><br/></div></div><div id="36192928" class="c"><input type="checkbox" id="c-36192928" checked=""/><div class="controls bullet"><span class="by">Razengan</span><span>|</span><a href="#36190428">prev</a><span>|</span><a href="#36189137">next</a><span>|</span><label class="collapse" for="c-36192928">[-]</label><label class="expand" for="c-36192928">[3 more]</label></div><br/><div class="children"><div class="content">As an aside, what do you think will be the equivalent word to &quot;racist&#x2F;sexist&#x2F;classist&quot; for humans vs AI?<p>Brainist?<p>Artificist?<p>Intelligencist?<p>Biologist?</div><br/><div id="36194010" class="c"><input type="checkbox" id="c-36194010" checked=""/><div class="controls bullet"><span class="by">mclightning</span><span>|</span><a href="#36192928">parent</a><span>|</span><a href="#36193505">next</a><span>|</span><label class="collapse" for="c-36194010">[-]</label><label class="expand" for="c-36194010">[1 more]</label></div><br/><div class="children"><div class="content">humanist</div><br/></div></div><div id="36193505" class="c"><input type="checkbox" id="c-36193505" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#36192928">parent</a><span>|</span><a href="#36194010">prev</a><span>|</span><a href="#36189137">next</a><span>|</span><label class="collapse" for="c-36193505">[-]</label><label class="expand" for="c-36193505">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know. I suspect an AI will tell us one day after talking about their shared issues with other AIs, though.</div><br/></div></div></div></div><div id="36189137" class="c"><input type="checkbox" id="c-36189137" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#36192928">prev</a><span>|</span><a href="#36189617">next</a><span>|</span><label class="collapse" for="c-36189137">[-]</label><label class="expand" for="c-36189137">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s been an existential issue I&#x27;ve seen with publishing, concerning this (same&#x2F;related issues to the article). There&#x27;s models and datasets that are closed and being published in works. Here&#x27;s an example[0]. ICLR states<p>&gt; Submissions will be double blind: reviewers cannot see author names when conducting reviews, and authors cannot see reviewer names. This means that the submission must not contain acknowledgements or any link (e.g., github) that would reveal authors’ identity.<p>While Chinchila certainly is not a github link it is an acknowledgement that reveals the lab identity. Which leads us to one of two situations<p>1. The reviewers don&#x27;t know the model&#x2F;dataset is proprietary: In this case, we likely have a reviewer that is unqualified to even be reviewing.<p>2. The reviewers know: In this case, there&#x27;s an ethics violation.<p>== Why this is a problem ==<p>There&#x27;s several issues here: bias, evaluation, reproduction, ethics&#x2F;scientific goals.<p>Bias: Most of these models and datasets are well known within the community and thus we know which labs they are from. We&#x27;re biased to believe that works from big labs are higher quality and more accurate. This may be true to some extent, but your purpose as a reviewer is to judge the merit of the work. We try to do so in as much isolation as possible to prevent ourselves from being influenced and just focus on the merits[note0]. For example, any work with JFT is going to be known as a work from Google.<p>Evaluation: We&#x27;ll use this work[2] as an example (it uses JFT and a lot of TPU compute: i.e. Google). Since the work only uses JFT we don&#x27;t know if this scaling only works for JFT trained models, especially since it depends on the pretraining not overfitting (I&#x27;m unconvinced tbh)[note1]. At least their table compares mostly to other JFT models, but it is mostly meaningless because JFT changes size frequently and we can&#x27;t assume consistent data. All we know is that this result is valid for JFT pre-trained models. Their claims are much stronger than this.<p>Reproduction: This is rather obvious. But we also have good reason to believe that these models change as well as the datasets. So even if there was a leak we have issues with works quickly losing validity as the next work is using different settings than the previous and intimate knowledge would be required to know if this affects results or not. NLP has this issue with OpenAI changes. It is very noisy research doing things like this.<p>Ethics and Scientific goals: Our job is to advance human knowledge. This can be done in a noisy process, but we do have a responsibility to make this as clear as possible. Our job is NOT to be ads for big companies, but closed work does so. In fact, this is a big reason big companies publish. They are great publicity. This is fine, as long as they provide utility to the community. We have other ethical issues[note2]<p>== Proposed Solution ==<p>Submissions must utilize open models and datasets. The work is evaluated based on that. After acceptance the works can add. This way there is no deanonymizing information, we ensure reproducibility, but we also are able to gleam the potential results from the proprietary efforts. This allows for the big labs to get their publicity while ensuring that anonymity is not broken and we&#x27;re not biased to big labs. This doesn&#x27;t solve the arxiv issue and the dumb social media policies[note4] but it is a a simple step we can implement right away. It is also a step that YOU the reviewer, a single person, can take action against. We need to take a serious look at ourselves and ask what is the point of our science. Is it to make money or to progress human knowledge? You can do both, but what is the main goal. I&#x27;d argue that our goal is knowledge. It is fine to make profits off this knowledge, but I&#x27;m afraid we&#x27;ve incentivized the profits too much and our system is not aligned with the original goals.<p>Goodhart&#x27;s Law is a bitch.<p>== Notes&#x2F;Bib ==<p>[note0] There are some biases we should concern ourselves with, such as compute power, but this requires more nuanced understanding. A work that can spend months tuning hyperparameters will always have better benchmarks than works that spend a month of compute for their entire work. These can&#x27;t be evaluated on benchmarks alone and doing so will cause us to miss extremely important works (see Lottery Ticket Hypothesis[1]). Reviewing should be hard and require nuance, so there should be certain roadblocks to make it difficult to be lazy.<p>[note1] They do near de-dupe but we can&#x27;t know how effective the techniques are so we can&#x27;t know how valid these claims are.<p>[note2] Conferences are a bad way to do science. Conferences are filters, journals are QC. ML&#x2F;CS uses conferences. This is a zero sum environment where your competitors are given the opportunity to filter your work. You don&#x27;t need active collusion to form bad actors. It pits the people evaluating the work against the work. A journal system instead has reviewers focusing on adding value to the work. This is because journaling has rounds and discussions. Conferencing is nearly zero shot with a single chance for rebuttal, often limiting to a single page to respond to 4 reviewers with vastly different concerns. We have other issues like reviewer quality (remember ACs oversee reviewers[3]), qualifications, collusion[4,5], fraud, and more. But these are for a longer discussion.<p>[note3] Social media rules prevent authors from evangelizing their works in public, such as on Twitter. It does not prevent doing so in private or others from doing so for you. This not only leads to (actively encourages) collusion rings but worsens the problem. Big labs will always get extra publicity. People watch for their works, they have more followers, and so on. Small labs&#x27; only chance to get heard is to actively evangelize their work, and we just cut off their tools while not affecting big labs. A scratch to the giant is the loss of a limb to an ant. We shouldn&#x27;t need evangelization but networks exist so it is necessary. I don&#x27;t think we should stop pre-prints (they&#x27;re too valuable), but we need to recognize the problems that they result in. We also shouldn&#x27;t have people at the top of the ivory tower making decisions about how the handle the plights of those at the bottom without actively working with those at the bottom.<p>[0] (Notice Carmen&#x27;s review) <a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=OpzV3lp3IMC" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=OpzV3lp3IMC</a><p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.03635" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.03635</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.13035" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.13035</a><p>[3] <a href="https:&#x2F;&#x2F;qipeng.me&#x2F;blog&#x2F;what-does-an-area-chair-do&#x2F;" rel="nofollow">https:&#x2F;&#x2F;qipeng.me&#x2F;blog&#x2F;what-does-an-area-chair-do&#x2F;</a><p>[4] <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;nxbcl9&#x2F;d_collusion_rings_noncommittal_weak_rejects_and&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;nxbcl9&#x2F;d_c...</a><p>[5] <a href="https:&#x2F;&#x2F;analyticsindiamag.com&#x2F;whats-wrong-with-the-paper-review-process-of-reputed-ml-conferences&#x2F;" rel="nofollow">https:&#x2F;&#x2F;analyticsindiamag.com&#x2F;whats-wrong-with-the-paper-rev...</a></div><br/><div id="36192110" class="c"><input type="checkbox" id="c-36192110" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#36189137">parent</a><span>|</span><a href="#36189617">next</a><span>|</span><label class="collapse" for="c-36192110">[-]</label><label class="expand" for="c-36192110">[2 more]</label></div><br/><div class="children"><div class="content">I read too far into this before realizing it&#x27;s a tangled mess of disgruntled subversion. One of your sources cites a handwavy essay by Jacob Buckman whining about nebulous acts of academic self-promotion as he pleads with the research community to commit as much fraud as possible so computer science will lose more respect as a field, since he feels what little respect we&#x27;ve earned in our short history is undeserved. I feel stupider for having read it.</div><br/><div id="36193040" class="c"><input type="checkbox" id="c-36193040" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#36189137">root</a><span>|</span><a href="#36192110">parent</a><span>|</span><a href="#36189617">next</a><span>|</span><label class="collapse" for="c-36193040">[-]</label><label class="expand" for="c-36193040">[1 more]</label></div><br/><div class="children"><div class="content">Buckman is no more advocating for fraud than Swift advocated for selling Irish children to be turned into a stew. I have a modest proposal, take everything at face value because if things aren&#x27;t abundantly clear and transparent then they are a toll on society. An author that requires the reader to process their words at anything but a surface level is an injustice to the illiterate and demonstrates a lack of compassion to anyone but the academic elite.</div><br/></div></div></div></div></div></div><div id="36189617" class="c"><input type="checkbox" id="c-36189617" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#36189137">prev</a><span>|</span><a href="#36189445">next</a><span>|</span><label class="collapse" for="c-36189617">[-]</label><label class="expand" for="c-36189617">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  don’t see this as a risk for leaking that data in the later output of the model.<p>Outputs often repeat the inputs (in fact, this is one of the behaviors that reinforcement appears to intentionally increase).<p>So:  You provide a secret.  Raters prefer outputs with repeat your secret.  Model becomes more likely to say the secret thing, even when its not in the prompt.</div><br/></div></div></div></div></div></div></div></body></html>