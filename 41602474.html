<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726909253206" as="style"/><link rel="stylesheet" href="styles.css?v=1726909253206"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/qhjqhj00/MemoRAG">MemoRAG – Enhance RAG with memory-based knowledge discovery for long contexts</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>taikon</span> | <span>28 comments</span></div><br/><div><div id="41605472" class="c"><input type="checkbox" id="c-41605472" checked=""/><div class="controls bullet"><span class="by">bogwog</span><span>|</span><a href="#41603631">next</a><span>|</span><label class="collapse" for="c-41605472">[-]</label><label class="expand" for="c-41605472">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know how this is different from regular rag yet, but that harry potter example sucks. The &quot;inferior answer&quot; seems much more accurate to the prompt with much higher information density, and the &quot;good answer&quot; just seems like the type of generic slop any old LLM would produce if you asked it to summarize harry potter.<p>Also, the prompt itself is in semi-broken english and it&#x27;s not clear what exactly is being asked.</div><br/></div></div><div id="41603631" class="c"><input type="checkbox" id="c-41603631" checked=""/><div class="controls bullet"><span class="by">simpaticoder</span><span>|</span><a href="#41605472">prev</a><span>|</span><a href="#41603783">next</a><span>|</span><label class="collapse" for="c-41603631">[-]</label><label class="expand" for="c-41603631">[7 more]</label></div><br/><div class="children"><div class="content">I am naive about LLM technology, in particular the relationship between base models, fine-tuning, and RAG. This particular branch of effort seems aimed at something that is of great interest to me (and I&#x27;m sure many others) which is to specialize a more general base model to know a particular domain in great detail and so improve it&#x27;s responses within that domain. In the past, this might have been called an &quot;expert system&quot;. For example, you might want to train an LLM on your project codebase and documentation such that subsequent code suggestions prioritize the use of internal libraries or code conventions over those represented by the public sources encoded in the base model.<p>I found the Google Colab notebook of MemoRag[1] to be of great use in understanding roughly the scope and workflow of this work. The interesting step is when you submit your domain as text to encode a new thing that requires a GPU, a process they call &quot;forming memory&quot;[2]. Perhaps there is some sort of back-and-forth between the base model and your data that results in new weights added to the base model. As I said, I am naive about LLM technology so I&#x27;m not sure about the details or the nomenclature. However, if this is even partially correct I&#x27;d like to understand how the &quot;formed memory&quot; and the base model cohabitate during inference, because this would create memory pressure on the GPU. If the memory required for the base model is M, and the formed memory is N, it&#x27;s reasonable to assume you&#x27;d need M+N memory to use both.<p>1 - <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1fPMXKyi4AwWSBkC7Xr5vBdpPpx9gDeFX" rel="nofollow">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1fPMXKyi4AwWSBkC7Xr5...</a><p>2 - <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1fPMXKyi4AwWSBkC7Xr5vBdpPpx9gDeFX#scrollTo=T3bQcPukXztR" rel="nofollow">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1fPMXKyi4AwWSBkC7Xr5...</a></div><br/><div id="41607900" class="c"><input type="checkbox" id="c-41607900" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#41603631">parent</a><span>|</span><a href="#41603992">next</a><span>|</span><label class="collapse" for="c-41607900">[-]</label><label class="expand" for="c-41607900">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, if this is even partially correct I&#x27;d like to understand how the &quot;formed memory&quot; and the base model cohabitate during inference, because this would create memory pressure on the GPU.<p>Not really. RAG loads selected data into the neural network which changes the state of the existing &quot;neurons&quot; (aka parameters), so the memory usage on GPU is only the size of the neural network.<p>You will hear about &quot;context size&quot; a lot. This means the amount of tokens a particular model can have loaded without becoming saturated and starting to lose things that were previously loaded.</div><br/></div></div><div id="41603992" class="c"><input type="checkbox" id="c-41603992" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#41603631">parent</a><span>|</span><a href="#41607900">prev</a><span>|</span><a href="#41603783">next</a><span>|</span><label class="collapse" for="c-41603992">[-]</label><label class="expand" for="c-41603992">[5 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>   In the past, this might have been called an &quot;expert system&quot;. 
</code></pre>
Heh, it comes full circle... After ~50 years of Expert Systems winter, we&#x27;re training our new AGIs to become more specialized! This is a memorable lesson that binaries must always be deconstructed, at least to some extent -- kinda like the endless dance we&#x27;re doing between monoliths and microservices as each new generation of tools runs into the problems inherent in each.<p><pre><code>  I am naive about LLM technology so I&#x27;m not sure about the details or the nomenclature
</code></pre>
You&#x27;ve got all the details right though, so that&#x27;s pretty impressive :). AFAICT from a quick glance at the code (<a href="https:&#x2F;&#x2F;github.com&#x2F;qhjqhj00&#x2F;MemoRAG&#x2F;blob&#x2F;main&#x2F;memorag&#x2F;memorag.py#L83-L86">https:&#x2F;&#x2F;github.com&#x2F;qhjqhj00&#x2F;MemoRAG&#x2F;blob&#x2F;main&#x2F;memorag&#x2F;memora...</a>), it is indeed &quot;fine tuning&quot; (jargon!) a model on your chosen book, presumably in the most basic&#x2F;direct sense: asking it reproduce sections of text at random from the book given their surrounding context, and rewarding&#x2F;penalizing the neural network based on how well it did. The comment mentions GPU memory in the Colab Notebook merely because this process is expensive -- &quot;fine tuning&quot; is the same thing as &quot;training&quot;, just with a nearly-complete starting point. Thus the call to `AutoModelForCausalLM.from_pretrained()`.<p>To answer your question explicitly: the fine-tuning step creates a modified version of the base model as an &quot;offline&quot; step, so the memory requirements during inference (aka &quot;online&quot; operation) are unaffected. Both in terms of storage and in terms of GPU VRAM. I&#x27;m not the dev tho so obv apologies if I&#x27;m off base!<p>I would passionately argue that that step is more of a small addition to the overall pipeline than a core necessity, though. Fine-tuning is really good for teaching a model to recreate style, tone, structure, and other linguistic details, but it&#x27;s not a very feasible way to teach it <i>facts</i>. That&#x27;s what &quot;RAG&quot; is for: making up for this deficiency in fine-tuning.<p>In other words, this repo is basically like that post from a few weeks back that was advocating for &quot;modular monoliths&quot; that employ both strategies (monolith vs. microservices) in a deeply collaborative way. And my reaction is the same: I&#x27;m not convinced the details of this meshing will be very revolutionary, but the idea itself is deceptively clever!</div><br/><div id="41604300" class="c"><input type="checkbox" id="c-41604300" checked=""/><div class="controls bullet"><span class="by">spmurrayzzz</span><span>|</span><a href="#41603631">root</a><span>|</span><a href="#41603992">parent</a><span>|</span><a href="#41603783">next</a><span>|</span><label class="collapse" for="c-41604300">[-]</label><label class="expand" for="c-41604300">[4 more]</label></div><br/><div class="children"><div class="content">&gt; AFAICT from a quick glance at the code (<a href="https:&#x2F;&#x2F;github.com&#x2F;qhjqhj00&#x2F;MemoRAG&#x2F;blob&#x2F;main&#x2F;memorag&#x2F;memora">https:&#x2F;&#x2F;github.com&#x2F;qhjqhj00&#x2F;MemoRAG&#x2F;blob&#x2F;main&#x2F;memorag&#x2F;memora</a>...), it is indeed &quot;fine tuning&quot; (jargon!) a model on your chosen book, presumably in the most basic&#x2F;direct sense: asking it reproduce sections of text at random from the book given their surrounding context, and rewarding&#x2F;penalizing the neural network based on how well it did.<p>Maybe your use of quotes is intentional here, but for posterity&#x27;s sake there is no actual fine-tuning happening using user input in the code you linked, insofar as the weights of the model aren&#x27;t being touched at all, nor are they modifying anything else that could impact the original weights (like a LoRA adapter). You touch on this, I think (?), in some of your subsequent language but it read as a little confusing to me at first glance. Or maybe I&#x27;ve been too deep in the ML weeds for too many years at this point.<p>The paper details the actual process, but the TL;DR is that the memory module they use, basically a draft model, does go through a pretraining phase using the redpajama dataset, and then an SFT phase with a different objective. This all happens before and irrespective of the inference-time task (i.e. asking questions about a given text). Also, as has been pointed out in other comments, the draft model could really be any model that supports long context and has decent retrieval performance. So the actual training phases here may be non-essential depending on your infra&#x2F;cost constraints.</div><br/><div id="41604548" class="c"><input type="checkbox" id="c-41604548" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#41603631">root</a><span>|</span><a href="#41604300">parent</a><span>|</span><a href="#41603783">next</a><span>|</span><label class="collapse" for="c-41604548">[-]</label><label class="expand" for="c-41604548">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for the corrections! I’m very much not an expert on LLM usage in the real world. But I’m a bit confused:<p><pre><code>  does go through a pretraining phase using the redpajama dataset, and then an SFT phase with a different objective
</code></pre>
Isn’t that equivalent to what I said, since “SFT” seems to stand for “supervised fine-tuning”? That it starts with a pre trained model, and then modifies that model according to your corpus?<p>Perhaps the confusion here is my ambiguity with “model”; I now see that there’s really two models-one for generating a draft + clues and one for constructing the final output—and this library only concerns&#x2F;modifies the former. Maybe?</div><br/><div id="41604637" class="c"><input type="checkbox" id="c-41604637" checked=""/><div class="controls bullet"><span class="by">spmurrayzzz</span><span>|</span><a href="#41603631">root</a><span>|</span><a href="#41604548">parent</a><span>|</span><a href="#41603783">next</a><span>|</span><label class="collapse" for="c-41604637">[-]</label><label class="expand" for="c-41604637">[2 more]</label></div><br/><div class="children"><div class="content">I should have quoted you more specifically, my apologies. I was responding to the comment that there was some training of the &quot;model on your chosen book&quot;.<p>There is no fine-tuning done specific to the corpus you own. I noted this in a sibling comment, but both the pretraining and fine-tuning objective uses a generic dataset (redpajama) which &quot;aims to maximize the generation probability of the next token given the KV cache of the previous memory tokens&quot; (quote from section 2.2 of the paper).<p>This is why I noted you could really use any long-context model that also has good retrieval performance. They&#x27;re training their own draft model in lieu of using an existing model, but you could get similar&#x2F;better outcomes using something like claude sonnet 3.5.</div><br/><div id="41606700" class="c"><input type="checkbox" id="c-41606700" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#41603631">root</a><span>|</span><a href="#41604637">parent</a><span>|</span><a href="#41603783">next</a><span>|</span><label class="collapse" for="c-41606700">[-]</label><label class="expand" for="c-41606700">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for taking the time, that makes sense. This is not the first time I&#x27;ve misunderstood something by having opinions about what it <i>should</i> be doing in my opinion, haha. I absolutely agree with your last point, too.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41603783" class="c"><input type="checkbox" id="c-41603783" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41603631">prev</a><span>|</span><a href="#41603214">next</a><span>|</span><label class="collapse" for="c-41603783">[-]</label><label class="expand" for="c-41603783">[9 more]</label></div><br/><div class="children"><div class="content">The overview paragraph needs to be expanded quite a bit. The only operative phrase about how this thing works is &quot;By recalling query-specific clues&quot;. I think people need a bit more knowledge about what this is and how this works, in an overview, to get them interested in trying it. Surely we can be a bit more specific.</div><br/><div id="41604990" class="c"><input type="checkbox" id="c-41604990" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#41603783">parent</a><span>|</span><a href="#41603844">next</a><span>|</span><label class="collapse" for="c-41604990">[-]</label><label class="expand" for="c-41604990">[5 more]</label></div><br/><div class="children"><div class="content">It reads like an LLM wrote it. Word salad that waffles on without any substance. In fact I think an LLM wrote most of the README. There are the telltale bullet points with bold starting words for example.</div><br/><div id="41605189" class="c"><input type="checkbox" id="c-41605189" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41603783">root</a><span>|</span><a href="#41604990">parent</a><span>|</span><a href="#41605131">next</a><span>|</span><label class="collapse" for="c-41605189">[-]</label><label class="expand" for="c-41605189">[2 more]</label></div><br/><div class="children"><div class="content">I think lots of modern writing apps allow people to let AI &quot;reword&quot; their own content, into better sentence structures, etc. I&#x27;m fine with that actually. Doesn&#x27;t mean the AI invented the content itself.</div><br/><div id="41607635" class="c"><input type="checkbox" id="c-41607635" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#41603783">root</a><span>|</span><a href="#41605189">parent</a><span>|</span><a href="#41605131">next</a><span>|</span><label class="collapse" for="c-41607635">[-]</label><label class="expand" for="c-41607635">[1 more]</label></div><br/><div class="children"><div class="content">If only the AI could get to the point quickly instead of running its mouth on and on...</div><br/></div></div></div></div><div id="41605131" class="c"><input type="checkbox" id="c-41605131" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#41603783">root</a><span>|</span><a href="#41604990">parent</a><span>|</span><a href="#41605189">prev</a><span>|</span><a href="#41603844">next</a><span>|</span><label class="collapse" for="c-41605131">[-]</label><label class="expand" for="c-41605131">[2 more]</label></div><br/><div class="children"><div class="content">&gt; There are the telltale bullet points with bold starting words for example.<p>Is this where we&#x27;re at now, really? Basic markdown formatting is a telltale sign that something was written by AI?</div><br/><div id="41607395" class="c"><input type="checkbox" id="c-41607395" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#41603783">root</a><span>|</span><a href="#41605131">parent</a><span>|</span><a href="#41603844">next</a><span>|</span><label class="collapse" for="c-41607395">[-]</label><label class="expand" for="c-41607395">[1 more]</label></div><br/><div class="children"><div class="content">Basic markdown formatting, no. But using bullet points with bold starting words, after a really waffly introduction makes it more likely they just asked an llm to write it.</div><br/></div></div></div></div></div></div><div id="41603844" class="c"><input type="checkbox" id="c-41603844" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#41603783">parent</a><span>|</span><a href="#41604990">prev</a><span>|</span><a href="#41604380">next</a><span>|</span><label class="collapse" for="c-41603844">[-]</label><label class="expand" for="c-41603844">[1 more]</label></div><br/><div class="children"><div class="content">This comment brought back academic paper reviewer associated ptsd</div><br/></div></div><div id="41604380" class="c"><input type="checkbox" id="c-41604380" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#41603783">parent</a><span>|</span><a href="#41603844">prev</a><span>|</span><a href="#41603214">next</a><span>|</span><label class="collapse" for="c-41604380">[-]</label><label class="expand" for="c-41604380">[2 more]</label></div><br/><div class="children"><div class="content">I think leaving just an overview in the repository is fine considering they&#x27;ve released a paper describing it in detail (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2409.05591" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2409.05591</a>, linked in the README).</div><br/><div id="41605027" class="c"><input type="checkbox" id="c-41605027" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41603783">root</a><span>|</span><a href="#41604380">parent</a><span>|</span><a href="#41603214">next</a><span>|</span><label class="collapse" for="c-41605027">[-]</label><label class="expand" for="c-41605027">[1 more]</label></div><br/><div class="children"><div class="content">Sure an &quot;overview&quot; is fine. However 4 words of meaningful content isn&#x27;t an overview. The overview contained no meaningful content regarding whatever it is they claim to have done.</div><br/></div></div></div></div></div></div><div id="41603214" class="c"><input type="checkbox" id="c-41603214" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41603783">prev</a><span>|</span><label class="collapse" for="c-41603214">[-]</label><label class="expand" for="c-41603214">[10 more]</label></div><br/><div class="children"><div class="content">I don’t understand what the memory is or does from the README. Can anyone explain how it works differently from vector database results in vanilla RAG applications?</div><br/><div id="41603525" class="c"><input type="checkbox" id="c-41603525" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41603214">parent</a><span>|</span><label class="collapse" for="c-41603525">[-]</label><label class="expand" for="c-41603525">[9 more]</label></div><br/><div class="children"><div class="content">Ok, I think I get it now from scanning the paper and reading Eq. 1 and 2.<p>Normally RAG just sends your query `q` to a information retrieval function which searches a database of documents using full-text search or vector search. Those documents are then passed to a generative model along with your query to give you your final answer.<p>MemoRAG instead immediately passes `q` to a generative model to generate some uninformed response `y`. `y` is then passed to the information retrieval function. Then, just like vanilla RAG, `q` and the retrieved documents are sent to a generative model to give you your final answer.<p>Not sure how this is any more &quot;memory-based&quot; than regular RAG, but it seems interesting.<p>Def check out the pre-print, especially eq. 1 and 2. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.05591" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.05591</a><p>EDIT: The &quot;memory&quot; part comes from the first generative model being able to handle larger context, covered in Section 2.1</div><br/><div id="41603717" class="c"><input type="checkbox" id="c-41603717" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603525">parent</a><span>|</span><a href="#41603679">next</a><span>|</span><label class="collapse" for="c-41603717">[-]</label><label class="expand" for="c-41603717">[3 more]</label></div><br/><div class="children"><div class="content">I wonder how this different from HyDE. <a href="https:&#x2F;&#x2F;docs.haystack.deepset.ai&#x2F;docs&#x2F;hypothetical-document-embeddings-hyde" rel="nofollow">https:&#x2F;&#x2F;docs.haystack.deepset.ai&#x2F;docs&#x2F;hypothetical-document-...</a></div><br/><div id="41603767" class="c"><input type="checkbox" id="c-41603767" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603717">parent</a><span>|</span><a href="#41603679">next</a><span>|</span><label class="collapse" for="c-41603767">[-]</label><label class="expand" for="c-41603767">[2 more]</label></div><br/><div class="children"><div class="content">It seems to be fundamentally the same deal except instead of passing `q` to GPT-4, they have some long-context &quot;Memory Model&quot; (whose details I&#x27;ve yet to fully understand). Also, MemoRAG uses a more conventional Retrieve&#x2F;Generate pipeline downstream of the generated queries than &quot;Contriever&quot; (whose details I similarly haven&#x27;t informed myself on).<p>It would be interesting to see a performance comparison, it certainly seems the most relevant one (that or an ablation of their &quot;memory model&quot; with the LLMs upon which they are based).</div><br/><div id="41604558" class="c"><input type="checkbox" id="c-41604558" checked=""/><div class="controls bullet"><span class="by">spmurrayzzz</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603767">parent</a><span>|</span><a href="#41603679">next</a><span>|</span><label class="collapse" for="c-41604558">[-]</label><label class="expand" for="c-41604558">[1 more]</label></div><br/><div class="children"><div class="content">&gt; they have some long-context &quot;Memory Model&quot; (whose details I&#x27;ve yet to fully understand)<p>Section 2.2 of the paper[1] goes into this in more detail. They pretrain the draft model using the redpajama dataset, followed by a supervised fine-tuning step. The training objective &quot;aims to maximize the generation probability of the next token given the KV cache of the previous memory tokens&quot;.<p>This suggests that any model with long context and good retrieval performance could do the same job (and maybe better in the case of the SOTA frontier models).<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2409.05591" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2409.05591</a></div><br/></div></div></div></div></div></div><div id="41603679" class="c"><input type="checkbox" id="c-41603679" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603525">parent</a><span>|</span><a href="#41603717">prev</a><span>|</span><a href="#41604034">next</a><span>|</span><label class="collapse" for="c-41603679">[-]</label><label class="expand" for="c-41603679">[4 more]</label></div><br/><div class="children"><div class="content">thanks for boiling it down to the most salient point... to me, their approach is just query rewriting, which is pretty standard when doing RAG.</div><br/><div id="41605434" class="c"><input type="checkbox" id="c-41605434" checked=""/><div class="controls bullet"><span class="by">fraboniface</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603679">parent</a><span>|</span><a href="#41604533">next</a><span>|</span><label class="collapse" for="c-41605434">[-]</label><label class="expand" for="c-41605434">[1 more]</label></div><br/><div class="children"><div class="content">Not exactly, they use a small but long-context model that has the whole dataset in its context (or a large part of it) to generate the chunks as elements of the reply, before passing those to the final model. So the retrieval itself is different, there is no embeddeding model or vector db.</div><br/></div></div><div id="41604533" class="c"><input type="checkbox" id="c-41604533" checked=""/><div class="controls bullet"><span class="by">opdahl</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603679">parent</a><span>|</span><a href="#41605434">prev</a><span>|</span><a href="#41603743">next</a><span>|</span><label class="collapse" for="c-41604533">[-]</label><label class="expand" for="c-41604533">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. In the RAG space there are a million Open Source projects on GitHub all calling it memory, recreating the same thing over and over again.</div><br/></div></div><div id="41603743" class="c"><input type="checkbox" id="c-41603743" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603679">parent</a><span>|</span><a href="#41604533">prev</a><span>|</span><a href="#41604034">next</a><span>|</span><label class="collapse" for="c-41603743">[-]</label><label class="expand" for="c-41603743">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot there about the generative model (&quot;Memory Models&quot;) in the paper, so perhaps I&#x27;ve misrepresented it, but generally speaking yah I agree with you. It doesn&#x27;t sound like a fundamental change to how we think about RAG, but it might be a nice formalization of an incremental improvement :)</div><br/></div></div></div></div><div id="41604034" class="c"><input type="checkbox" id="c-41604034" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#41603214">root</a><span>|</span><a href="#41603525">parent</a><span>|</span><a href="#41603679">prev</a><span>|</span><label class="collapse" for="c-41604034">[-]</label><label class="expand" for="c-41604034">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  Not sure how this is any more &quot;memory-based&quot; than regular RAG, but it seems interesting.
</code></pre>
I can&#x27;t remember where I read this joke, but as a self-proclaimed Cognitive Engineer I think about it every day: &quot;An AI startup&#x27;s financial evaluation is directly proportional to how many times they can cram &#x27;mind&#x27; into their pitch deck!&quot;</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>