<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728723674136" as="style"/><link rel="stylesheet" href="styles.css?v=1728723674136"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2410.05229">Understanding the Limitations of Mathematical Reasoning in LLMs</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>hnhn34</span> | <span>226 comments</span></div><br/><div><div id="41817624" class="c"><input type="checkbox" id="c-41817624" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#41809764">next</a><span>|</span><label class="collapse" for="c-41817624">[-]</label><label class="expand" for="c-41817624">[1 more]</label></div><br/><div class="children"><div class="content">In terms of usefulness and realistic implementation mathematical reasoning is the next frontier of LLM not autonomous level 5 driving or AGI. More research fund and investment are much better spent on the former rather than the latter but apparently it seems that the reverse situation is the case.</div><br/></div></div><div id="41809764" class="c"><input type="checkbox" id="c-41809764" checked=""/><div class="controls bullet"><span class="by">parsimo2010</span><span>|</span><a href="#41817624">prev</a><span>|</span><a href="#41810517">next</a><span>|</span><label class="collapse" for="c-41809764">[-]</label><label class="expand" for="c-41809764">[71 more]</label></div><br/><div class="children"><div class="content">I won&#x27;t take a strong stance on whether or not LLMs actually do reasoning, but I will say that this decrease in performance is similar to what I see in college freshmen (I&#x27;m currently teaching a calculus course in which almost half of the students took AP calc in high school).  They perform well on simple questions.  Requiring students to chain multiple steps together, even simple steps, results in decreased accuracy and higher variance (I have no data on whether this decrease is linear or not, as the paper assumes that the decrease should be linear with the number of steps).  We see similar results with adding unrelated statements into a problem- many students are trained to make sure to use all given information in solving a problem- if you leave out something that the instructor gives you, then you probably forgot to do something important.<p>So while I don&#x27;t take a stance on what an LLM does should be considered reasoning, I do think that SOTA LLMs like GPT-4o perform about as good as high school graduates in America with average intelligence.  In other words, average Americans exhibit similar limitations on their reasoning as good LLMs.  Which on the one hand is a little disappointing to me in terms of the human performance but is kind of good news for LLMs- they aren&#x27;t doing graduate-level research but they are already capable of helping a large portion of the population.</div><br/><div id="41814062" class="c"><input type="checkbox" id="c-41814062" checked=""/><div class="controls bullet"><span class="by">ojosilva</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41814061">next</a><span>|</span><label class="collapse" for="c-41814062">[-]</label><label class="expand" for="c-41814062">[3 more]</label></div><br/><div class="children"><div class="content">LLM gets things right, when it does, due to the sheer massive information ingested during training, it can use probabilities to extract a right answer from deep in the model.<p>Humans on the other hand have developed a more elaborate scheme to process, or reason, data without having to read through 1 billion math problems and stack overflow answers. We listen to some explanations, a YT video, a few exercises and we&#x27;re ready to go.<p>The fact that we may get similar grades (at ie high school math) is just a spot coincidence of where both &quot;species&quot; (AI x Human) are right now at <i>succeeding</i>. But if we look closer at <i>failure</i>, we&#x27;ll see that we fail very differently. AI failure right now looks, to us humans, very nonsensical.</div><br/><div id="41816999" class="c"><input type="checkbox" id="c-41816999" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814062">parent</a><span>|</span><a href="#41814144">next</a><span>|</span><label class="collapse" for="c-41816999">[-]</label><label class="expand" for="c-41816999">[1 more]</label></div><br/><div class="children"><div class="content">While I&#x27;d agree human failures are different from AI failures, human failures are necessarily also nonsensical. Familiar, human, but nonsensical — consider how often a human disagreeing with another will use the phrase &quot;that&#x27;s just common sense!&quot;<p>I think the larger models are consuming in the order of 100k as much as we do, and while they have a much broader range of knowledge, it&#x27;s not 100k as much breadth.</div><br/></div></div><div id="41814144" class="c"><input type="checkbox" id="c-41814144" checked=""/><div class="controls bullet"><span class="by">pishpash</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814062">parent</a><span>|</span><a href="#41816999">prev</a><span>|</span><a href="#41814061">next</a><span>|</span><label class="collapse" for="c-41814144">[-]</label><label class="expand" for="c-41814144">[1 more]</label></div><br/><div class="children"><div class="content">Nah, human failures look equally nonsensical. You&#x27;re just more attuned to use their body language or peer judgement to augment your reception. Really psychotic humans can bypass this check.</div><br/></div></div></div></div><div id="41814061" class="c"><input type="checkbox" id="c-41814061" checked=""/><div class="controls bullet"><span class="by">wkirby</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41814062">prev</a><span>|</span><a href="#41812825">next</a><span>|</span><label class="collapse" for="c-41814061">[-]</label><label class="expand" for="c-41814061">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I do think that SOTA LLMs like GPT-4o perform about as good as high school graduates in America with average intelligence.<p>This <i>might</i> be true in a strict sense, but I think it&#x27;s really, really important to consider the uses of LLMs vs a high-school graduate. LLMs are confidently wrong (and confidently correct) with the exact same measure, and in many ways they are <i>presented</i> to users as unimpeachable.<p>If I ask an average person to do a medium-complex logic problem, my human brain discounts their answer because I&#x27;ve been socialized to believe that humans are bad at logic. I will take any answer I&#x27;m given with usually appropriate skepticism.<p>LLMs, on the other hand, are on the computer: an interface I&#x27;ve been socialized to believe is <i>always correct</i> on matters of math and logic. That&#x27;s what it is, a logic machine. Second guessing the computer on matters of logic and arithmetic  almost always result in me realizing my puny human mind has done something wrong.<p>To me, this directly contradicts your conclusion: LLMs are mostly only capable of misleading large portions of the population.</div><br/><div id="41814106" class="c"><input type="checkbox" id="c-41814106" checked=""/><div class="controls bullet"><span class="by">pishpash</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814061">parent</a><span>|</span><a href="#41814522">next</a><span>|</span><label class="collapse" for="c-41814106">[-]</label><label class="expand" for="c-41814106">[1 more]</label></div><br/><div class="children"><div class="content">Would be good to put equivalent grades on LLM&#x27;s then. Instead of GPT-4o, it&#x27;s GPT-11th grade.</div><br/></div></div><div id="41814522" class="c"><input type="checkbox" id="c-41814522" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814061">parent</a><span>|</span><a href="#41814106">prev</a><span>|</span><a href="#41812825">next</a><span>|</span><label class="collapse" for="c-41814522">[-]</label><label class="expand" for="c-41814522">[2 more]</label></div><br/><div class="children"><div class="content">This is not inherent in the LLM though. Society will adjust to it after learning some very predictable (and predicted) lessons, just like it always does.</div><br/><div id="41815511" class="c"><input type="checkbox" id="c-41815511" checked=""/><div class="controls bullet"><span class="by">wkirby</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814522">parent</a><span>|</span><a href="#41812825">next</a><span>|</span><label class="collapse" for="c-41815511">[-]</label><label class="expand" for="c-41815511">[1 more]</label></div><br/><div class="children"><div class="content">Will it? Media literacy is in crisis, and we’ve had the printed word for centuries. I’m much more convinced we will continue to believe “magic box cannot lie” than somehow develop a notion for when to trust the coherent nonsense generator.</div><br/></div></div></div></div></div></div><div id="41812825" class="c"><input type="checkbox" id="c-41812825" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41814061">prev</a><span>|</span><a href="#41813800">next</a><span>|</span><label class="collapse" for="c-41812825">[-]</label><label class="expand" for="c-41812825">[36 more]</label></div><br/><div class="children"><div class="content">&gt; I won&#x27;t take a strong stance on whether or not LLMs actually do reasoning,<p>I don&#x27;t understand why people are still confused about this. When these models fundamentally have a randomness parameter to make them appear like they are actually thinking instead of deterministically outputting information, it should be clear that there is no reasoning going on.</div><br/><div id="41815427" class="c"><input type="checkbox" id="c-41815427" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812825">parent</a><span>|</span><a href="#41813403">next</a><span>|</span><label class="collapse" for="c-41815427">[-]</label><label class="expand" for="c-41815427">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how having a randomness parameter implies that, without it, the output of an LLM is merely outputting information, like it&#x27;s just looking up some answer in a dictionary. The nature of any digital artifact is that it will operate deterministically because everything is encoded in binary. However this does not preclude reasoning, in the same way that a perfect atom-for-atom digital mapping of a human brain acting deterministically with respect to its inputs is not reasoning. If it&#x27;s a perfect copy of the human brain, and does everything a human brain would given the inputs, then it must be reasoning iff a human brain is reasoning, if not, then you&#x27;d have to conclude that a human mind cannot reason.<p>Since randomness, by definition, does not vary depending on the inputs it is given, it by definition cannot contribute to reasoning if your definition of reasoning does not include acausal mysticism.</div><br/></div></div><div id="41813403" class="c"><input type="checkbox" id="c-41813403" checked=""/><div class="controls bullet"><span class="by">growthwtf</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812825">parent</a><span>|</span><a href="#41815427">prev</a><span>|</span><a href="#41815099">next</a><span>|</span><label class="collapse" for="c-41813403">[-]</label><label class="expand" for="c-41813403">[24 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how the latter follows from the former.<p>Here&#x27;s how I think about it: the fact that it can interpret the same words differently in different contexts alone shows that even on a temperature of 0 (i.e., lowest randomness possible) there could be something that possibly resembles reasoning happening.<p>It might be a mimicry of reasoning, but I don&#x27;t think that having adjustable parameters on how random they are makes it any less of one.<p>I also don&#x27;t see how that idea would fit in with the o1 models, which explicitly have &quot;reasoning&quot; tokens. Now, I&#x27;m not terribly impressed with their performance relative to how much extra computation they need to do, but the fact they have chains-of-thought that humans could reasonably inspect and interpret, and that they chains of thought do literally take extra time and compute to run, certainly points at the process being something possibly analogous to reasoning.<p>In this same vein, up until recently I personally very much in the camp of calling them &quot;LLMs&quot; and generally still do, but given how they really are being used now as general purpose sequence-to-sequence prediction models across all sorts of input and output types tends to push me more towards the &quot;foundation models&quot; terminology camp, since pigeonholing them into just language tasks doesn&#x27;t seem accurate anymore. o1 was the turning point for me on this personally, since it is explicitly predicting and being optimized for correctness in the &quot;reasoning tokens&quot; (in scare quotes again since that&#x27;s what openai calls it).<p>All that said, I personally think that calling what they do reasoning, and meaning it in the exact same way as how humans reason, is anthropomorphizing the models in a way that&#x27;s not really useful. They clearly operate in ways that are quite different from humans in many ways. Sometimes that might imitate human reasoning, other times it doesn&#x27;t.<p>But, the fact they have that randomness parameter seems to be to be totally unrelated to any of the above thoughts or merits about the models having reasoning abilities.</div><br/><div id="41814399" class="c"><input type="checkbox" id="c-41814399" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813403">parent</a><span>|</span><a href="#41814017">next</a><span>|</span><label class="collapse" for="c-41814399">[-]</label><label class="expand" for="c-41814399">[7 more]</label></div><br/><div class="children"><div class="content">&gt;he fact that it can interpret the same words differently in different contexts alone shows that even on a temperature of 0 (<p>This is the problem with using loaded language like &quot;reason&quot; and &quot;interpret&quot;. The model is not interpreting anything. All that is being done is a multdimentional map lookup with statistics.<p>&gt; also don&#x27;t see how that idea would fit in with the o1 models, which explicitly have &quot;reasoning&quot; tokens.<p>An LLM on top of an LLM (i.e using context to generate inputs to an LLM) is just a fancier LLM.<p>To really understand all of this, all you need to do is look at how Transformer works, namely the attention block. There is no such thing as Query, Key, and Value in the sense of how they are implied to be used. The may as well be called A,B,C, as they are all learned in training, and can be freely interchanged in naming. All you do for inference is multiply the output vector by A,B,C to get 3 matrices, then multiply them together (technically with a scaling factor for 2 of them, but again, doesn&#x27;t matter for which 2, and the scaling factor can be built into the matrix itself)<p>And because you can unroll matrix multiplication into a 2 layer neural network, that means that any LLM in its current form today can be represented as a set of linear layers. And we know that a set of linear layers is simply a function. And every function has a finite range for a finite domain. And the inability to expand that range given a finite domain means its not reasoning.<p>So we have to rely on hacks like temperature to make it appear like reasoning, when its really not even close.</div><br/><div id="41814693" class="c"><input type="checkbox" id="c-41814693" checked=""/><div class="controls bullet"><span class="by">growthwtf</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814399">parent</a><span>|</span><a href="#41814582">next</a><span>|</span><label class="collapse" for="c-41814693">[-]</label><label class="expand" for="c-41814693">[3 more]</label></div><br/><div class="children"><div class="content">I see, I probably needed more coffee to read your initial note.<p>If I am repeating this back correctly, the argument is that the process itself looks nothing like human reasoning and has a number of technical limitations and even hacks that are in no way attributes or qualities of reasoning. Therefore, it clearly cannot be in any way considered reasoning. Temperature is one element of this, but there are others which you could continue to enumerate beyond even what&#x27;s written above.<p>I can get behind part of that argument, certainly, and I appreciate you elaborating on it. I think is what I was trying to say with the part about me believing that it&#x27;s not useful to think of it as reasoning. This is very different from what we might consider reasoning in very meaningful ways.<p>I also agree with you also that parts of this is just loaded language, as it is anthropomorphizing what is fundamentally just a bunch of matrices and non-linear functions.<p>I think where we differ is probably on that &quot;when it&#x27;s not even really close&quot; part of it, at least in what I mean is &quot;close&quot; versus what I think you mean.<p>While I (think) we agree that obviously it&#x27;s a different process, I do think that the input-&gt;outputs and the different qualities of input-&gt;outputs (like the so-called reasoning tokens) above can often seem quite close to the different inputs and outputs of some human reasoning. That&#x27;s why I was saying that didn&#x27;t see how the process works, like temperature, is relevant. Putting the processes aside, if you black box a human and a language model and put us head to head on reasoning tasks, sometimes you&#x27;re going to get quite similar results.<p>I&#x27;m basically saying that, sure, an LLM or foundation model is clearly a Chinese room, without any understanding. What are we comparing it to, though?<p>Now, I don&#x27;t have any kind of training in biology, but I have been led to understand that our brains are quite complex and that how their function arises from the underlying biological processes. is still fairly poorly understood. Given that, I tend to discount the degree of difference between the processes themselves and just look at the inputs and outputs. It&#x27;s not obvious to me that we aren&#x27;t ourselves Chinese rooms, at least to some significant degree.<p>So _maybe_ it&#x27;s fair to try to compare what the outputs of these Transformers are to what our outputs would be. If it walks like a duck, and talks like a duck, does it matter?<p>Obviously, that&#x27;s not fully correct -- how the output arises _has_ to matter somewhat. The fact I am sitting here writing this, and not an AI, refutes that point to some degree. And if I am understanding your thoughts correctly, I fully agree that the process really is nothing close. I just don&#x27;t see how it can be a clear-cut issue on the basis of analyzing the Transformer algorithm itself.</div><br/><div id="41815447" class="c"><input type="checkbox" id="c-41815447" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814693">parent</a><span>|</span><a href="#41816207">next</a><span>|</span><label class="collapse" for="c-41815447">[-]</label><label class="expand" for="c-41815447">[1 more]</label></div><br/><div class="children"><div class="content">&gt;If it walks like a duck, and talks like a duck, does it matter?<p>Depends on what your goals are. LLMs can get to a state where they contain a lot of human knowledge, with a lot of detail, to answer a lot of questions, and be used in many different ways. If your idea of intelligence is akin to having a bunch of experts on tap in all the different areas, then LLMS are totally fine.<p>I personally want something that can solve problems, not just answer questions. For example, lets say I want to build a flying car, quadcopter style, in my garage. Given the information that exists on the internet and availability of parts, this is a deterministic problem. Given that prompt, I want a set of specific instructions like &quot;buy this part from here&quot;, &quot;send this cad model to sendcutsend.com here and select these options&quot;, all the way down to &quot;here is a binary file to load on the controller&quot;. And along the same lines, the AI should be able to build a full simulator application Flight Sim style, where I can load the file and play with controls to see how the thing behaves, including in less than optimal conditions.<p>Whatever that model does under the hood, that is called reasoning, and it certainly won&#x27;t be structured like an LLM.</div><br/></div></div><div id="41816207" class="c"><input type="checkbox" id="c-41816207" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814693">parent</a><span>|</span><a href="#41815447">prev</a><span>|</span><a href="#41814582">next</a><span>|</span><label class="collapse" for="c-41816207">[-]</label><label class="expand" for="c-41816207">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Putting the processes aside, if you black box a human and a language model and put us head to head on reasoning tasks, sometimes you&#x27;re going to get quite similar results.<p>I cannot believe this is true. LLMs are awful at whatever problems are not present in the dataset used for training. They are very bad at planning problems for example, because they cannot possibly memorize every single instance, and they cannot reason to reach a solution, but a black-boxed human of course it can.</div><br/></div></div></div></div><div id="41814582" class="c"><input type="checkbox" id="c-41814582" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814399">parent</a><span>|</span><a href="#41814693">prev</a><span>|</span><a href="#41814017">next</a><span>|</span><label class="collapse" for="c-41814582">[-]</label><label class="expand" for="c-41814582">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The model is not interpreting anything. All that is being done is a multdimentional map lookup with statistics.<p>So what? Can you propose another method to make a computing device understand language? The method of the creation of the output does not stipulate anything about the nature of the thing creating it. If someone could map out a human brain and tell you how thoughts are made and added a &#x27;all that is being done is&#x27; in front of it, does that make your thought creation trivial?<p>&gt; An LLM on top of an LLM (i.e using context to generate inputs to an LLM) is just a fancier LLM.<p>This is called a tautology. You have not given any compelling reasons why an LLM cannot do anything, so calling something another LLM is not compelling either.<p>&gt; To really understand all of this, all you need to do is look at how Transformer works, namely the attention block. There is no such thing as Query, Key, and Value in the sense of how they are implied to be used. The may as well be called A,B,C, as they are all learned in training, and can be freely interchanged in naming. All you do for inference is multiply the output vector by A,B,C to get 3 matrices, then multiply them together (technically with a scaling factor for 2 of them, but again, doesn&#x27;t matter for which 2, and the scaling factor can be built into the matrix itself)<p>Here is how it works, so therefore it must meet some criteria I have imposed arbitrarily.<p>&gt; So we have to rely on hacks like temperature to make it appear like reasoning, when its really not even close.<p>You still haven&#x27;t produced any valid argument at all, for why one thing would be evidence of the other.</div><br/><div id="41815298" class="c"><input type="checkbox" id="c-41815298" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814582">parent</a><span>|</span><a href="#41814017">next</a><span>|</span><label class="collapse" for="c-41815298">[-]</label><label class="expand" for="c-41815298">[2 more]</label></div><br/><div class="children"><div class="content">A good example of how to type a comment and yet not say anything.<p>It should be pretty clear to anyone that human brains aren&#x27;t just one giant compute functions with a limited set of outputs. There is no concept in your or my brain what 12074389762193867*2398720876324 is, but we can certainly figure it out, some even with good memory with complete sensory depravation.<p>If you disagree with this, you are entitled to your opinion, but your comments on the state of AI are just irrelevant.</div><br/><div id="41815532" class="c"><input type="checkbox" id="c-41815532" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815298">parent</a><span>|</span><a href="#41814017">next</a><span>|</span><label class="collapse" for="c-41815532">[-]</label><label class="expand" for="c-41815532">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t remember making any comments on A.I.<p>My post was pointing out basic flaws in reasoning and trying to provoke some thought about something where it appeared to be lacking. Unfortunately I did not succeed, since a myopic view made you hallucinate that I was saying something definitive about something I was not.<p>Irrelevant, indeed.</div><br/></div></div></div></div></div></div></div></div><div id="41814017" class="c"><input type="checkbox" id="c-41814017" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813403">parent</a><span>|</span><a href="#41814399">prev</a><span>|</span><a href="#41815099">next</a><span>|</span><label class="collapse" for="c-41814017">[-]</label><label class="expand" for="c-41814017">[16 more]</label></div><br/><div class="children"><div class="content">The notion is AFAIS that a deterministic algorithm is obviously not reasoning, and a deterministic algorithm interspersed with dice rolls is obviously not reasoning either.<p>Of course, some would beg to differ. It&#x27;s quite common nowadays to believe that we are something like the latter.</div><br/><div id="41814207" class="c"><input type="checkbox" id="c-41814207" checked=""/><div class="controls bullet"><span class="by">pishpash</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814017">parent</a><span>|</span><a href="#41815099">next</a><span>|</span><label class="collapse" for="c-41814207">[-]</label><label class="expand" for="c-41814207">[15 more]</label></div><br/><div class="children"><div class="content">Why is a deterministic algorithm not reasoning? Reasoning is very deterministic.</div><br/><div id="41815365" class="c"><input type="checkbox" id="c-41815365" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814207">parent</a><span>|</span><a href="#41814388">next</a><span>|</span><label class="collapse" for="c-41815365">[-]</label><label class="expand" for="c-41815365">[1 more]</label></div><br/><div class="children"><div class="content">There is a difference between determinism in the sense of given a certain input, you allways get a certain output, and determinism in the sense of given a certain input, and knowledge of the sub universe in which the problem applies, get a certain output.<p>I.e an agent that can reason can deterministically figure out that the most probable way of getting information to complete the answer would be to go out on google and do searches, but we don&#x27;t deterministically know what the information that exists at that point and time on google, so the answer could be different.</div><br/></div></div><div id="41814388" class="c"><input type="checkbox" id="c-41814388" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814207">parent</a><span>|</span><a href="#41815365">prev</a><span>|</span><a href="#41814742">next</a><span>|</span><label class="collapse" for="c-41814388">[-]</label><label class="expand" for="c-41814388">[12 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not about (in-)determinism really, it&#x27;s about the algorithm part.<p>An algorithm that does something can in principle be ran by someone who doesn&#x27;t know what the algorithm does. You could have a kid calculate an integral by giving it a sequence of directions whose purpose it doesn&#x27;t understand (e.g. cut out some cardboard that matches the shape, put it on one side of the scale, place enough unit cardboard pieces on the other side until they are even, then tell me how many pieces you put).<p>Reasoning has more to do with how the problem came about. A person had to come against a certain problem, figure out a way in which they can solve it, then apply the (perhaps algorithmic) solution. The algorithmic part is only an artifact.</div><br/><div id="41814757" class="c"><input type="checkbox" id="c-41814757" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814388">parent</a><span>|</span><a href="#41814425">next</a><span>|</span><label class="collapse" for="c-41814757">[-]</label><label class="expand" for="c-41814757">[6 more]</label></div><br/><div class="children"><div class="content">But isn&#x27;t figuring out a way to solve also algorithmic? In a lot of cases it is simply bruteforce trying out different things based on the concepts you know about and mixing them.</div><br/><div id="41815048" class="c"><input type="checkbox" id="c-41815048" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814757">parent</a><span>|</span><a href="#41814425">next</a><span>|</span><label class="collapse" for="c-41815048">[-]</label><label class="expand" for="c-41815048">[5 more]</label></div><br/><div class="children"><div class="content">You are right that relevant memories and analogous experiences come up and are used as building blocks in our evaluation&#x2F;exploration of a problem, but it doesn&#x27;t seem to me an algorithmic procedure at all.<p>You can trace out your journey in solving a problem, in retrospect, but could you encode it into a &quot;solving-a-problem&quot; algorithm?<p>I think you could extract some kind of generic template for problem solving: you come up with an idea, you evaluate whether it is the solution, you adjust the idea if not.<p>But this is a template, not an algorithm. Coming up with an idea has to do with filtering the old and new memories&#x2F;perceptions that come to mind: does this one seem right? or this one? Evaluating whether it is right is also an active process of asking questions. It involves memory (of the problem to be solved), attention (to the potential solution), judgement (do they fit together?), etc.<p>None of these are a predetermined sequence of steps you apply mechanically, such as the child &quot;solving an integral&quot; above.*<p>*Of course, the child is problem-solving in the sense that it&#x27;s trying its best to follow your instructions. &quot;Did I cut it right?&quot; &quot;Are the scales even?&quot; But this is not the problem of &quot;solving an integral&quot; to which it is completely oblivious to.</div><br/><div id="41815142" class="c"><input type="checkbox" id="c-41815142" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815048">parent</a><span>|</span><a href="#41814425">next</a><span>|</span><label class="collapse" for="c-41815142">[-]</label><label class="expand" for="c-41815142">[4 more]</label></div><br/><div class="children"><div class="content">I think it can be an algorithm, it just that the algorithm will be a very complex one compromised of many different algorithms. It&#x27;s not an algorithm anyone could practically follow in their lifetime. But there&#x27;s plenty of algorithms people can&#x27;t follow in real life.</div><br/><div id="41815477" class="c"><input type="checkbox" id="c-41815477" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815142">parent</a><span>|</span><a href="#41814425">next</a><span>|</span><label class="collapse" for="c-41815477">[-]</label><label class="expand" for="c-41815477">[3 more]</label></div><br/><div class="children"><div class="content">If all one has is hammers, one can start seeing nails everywhere.<p>A screw? That&#x27;s just a nail which will damage the wood.<p>A tomato? That&#x27;s just a soft screw which splatters.
Etc.<p>What purpose does seeing everything through the lens of an algorithm serve? Is the movement of an electron an algorithm? Is a polar planimeter an algorithm? [0]<p>We design algorithms to solve certain problems. It&#x27;s part of our problem solving activity.
But for what purpose would we go around, assuming things that don&#x27;t look like algorithms are actually algorithms that are just outside of our reach? 
This doesn&#x27;t solve a practical problem, so of what use is that, and where does it lead?<p>My long-winded answer is: 
We derive satisfaction from being <i>in principle</i> powerful. Our mechanistic&#x2F;computational knowledge of nature allows us to bend certain parts of it to our will. If there are parts we cannot control, it&#x27;s at least consoling that we <i>in principle</i> could know&#x2F;control them. So we stretch computational&#x2F;algorithmic terms as far as we possibly can. In the end, it envelops us as subjects. We end up in a rather cliche epiphenomenalism + causal determinism worldview:<p>- &quot;Yeah, we have experiences, but they&#x27;re just inefficacious artifacts of underlying chemistry.&quot;<p>- &quot;You - the person who is reasoning - don&#x27;t actually know what reasoning is like, it&#x27;s really a very complex algorithm which we could never know or follow.&quot;<p>The only way such an uninspiring outlook can subsist is because it jives well with some modern dreams:<p>- &quot;We only need X more compute and Y more years to bend Z part of nature to our will and bring utopia.&quot; (cue all the AI hype, see relevant frontpage entry [1])<p>- &quot;If we&#x27;re just a machine then maybe we can hack-reward-centers&#x2F;optimize-drug-concoction&#x2F;upload-to-mainframe-for-immortality&quot; (cue quasi-immortality pitches and externally-enforced-happines pipe-dreams)<p>- &quot;If I&#x27;m just a machine then I&#x27;m not responsible for my shortcomings - they&#x27;re just the outcome of my wiring I cannot influence.&quot; (a nice supplement for absolving oneself from responsibility - to oneself)<p>- &quot;If all is mechanical, then I&#x27;m just a temporarily embarrassed sovereign over everything. After all, if I just knew the mechanism behind things, then I could bend it to my will.&quot;<p>- &quot;I have to believe this because it is obviously true.&quot; (maybe the saddest of them all, since it promises nothing except the joy of being right and having others be wrong. it also seeds the others)<p>[0] <a href="http:&#x2F;&#x2F;psychsciencenotes.blogspot.com&#x2F;2015&#x2F;07&#x2F;brains-dont-have-to-be-computers-purple.html" rel="nofollow">http:&#x2F;&#x2F;psychsciencenotes.blogspot.com&#x2F;2015&#x2F;07&#x2F;brains-dont-ha...</a><p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41813268">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41813268</a></div><br/><div id="41815643" class="c"><input type="checkbox" id="c-41815643" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815477">parent</a><span>|</span><a href="#41814425">next</a><span>|</span><label class="collapse" for="c-41815643">[-]</label><label class="expand" for="c-41815643">[2 more]</label></div><br/><div class="children"><div class="content">&gt; What purpose does seeing everything through the lens of an algorithm serve?<p>To me at least it helps me understand how things work. What is an alternative? Because alternative seems like some sort of magic I wouldn&#x27;t understand.<p>&gt; Is the movement of an electron an algorithm?<p>I think there&#x27;s a lot of argument and complexity to what an electron exactly is or does, and what its properties actually mean, but I would imagine in general from particle and other levels whole universe can be just a deterministic algorithm, and so anything can be an algorithm. Universe has certain laws and rules which could in theory be simulated, but the simulation must have more capacity than the universe itself has so we inside the universe likely can not do it unless we find a mechanism to somehow bypass this.<p>&gt; But for what purpose would we go around, assuming things that don&#x27;t look like algorithms are actually algorithms that are just outside of our reach? This doesn&#x27;t solve a practical problem, so of what use is that, and where does it lead?<p>If I try to think of what is the algorithm behind something, it helps me understand it better. Intuitively I think there&#x27;s a complex algorithm behind everything, and it&#x27;s just a matter of spending time and effort to figure out what it exactly is. It&#x27;s unrealistic to get close to the real detail and nuance of the algorithm, but already trying to figure out the algorithm brings me closer to understanding.<p>&gt; We end up in a rather cliche epiphenomenalism + causal determinism worldview<p>Wait -- what is wrong with that? And also I don&#x27;t think it&#x27;s cliche, I think it is likely to be the case?<p>&gt; - &quot;You - the person who is reasoning - don&#x27;t actually know what reasoning is like, it&#x27;s really a very complex algorithm which we could never know or follow.&quot;<p>I mean writing it down on the algorithmic level is not something we can consciously follow easily. However our brain within us is following those algorithms in the level of efficiency that we cannot consciously follow at that speed step by step, just following the instructions.<p>&gt; The only way such an uninspiring outlook can subsist is because it jives well with some modern dreams:<p>I think my outlook is at least inspiring to me.<p>&gt; - &quot;If we&#x27;re just a machine then maybe we can hack-reward-centers&#x2F;optimize-drug-concoction&#x2F;upload-to-mainframe-for-immortality&quot; (cue quasi-immortality pitches and externally-enforced-happines pipe-dreams)<p>I do think theoretically it would be possible to hack humans to have constant &quot;heroin like euphoria&quot;. I&#x27;m not sure I exactly care for it, but I do think these things could be done, I just don&#x27;t know when, is it 50 years, 100 years or 1000 years. Of course while I say right now that I don&#x27;t exactly care for it, if I tried it for once I would be hooked on it forever, assuming it has no tolerance build up or other negative effects making me consider to quit it. But even real heroin is terribly hard to quit while it has tolerance build up and side effects.<p>&gt; - &quot;If I&#x27;m just a machine then I&#x27;m not responsible for my shortcomings - they&#x27;re just the outcome of my wiring I cannot influence.&quot; (a nice supplement for absolving oneself from responsibility - to oneself)<p>I&#x27;m inclined to think that the World is deterministic, yet I happen to also think that I have reward mechanisms that make me ambitious in a sense that I want to achieve certain goals to feel rewarded and in order to achieve those goals I have to overcome many challenges and improve certain shortcomings because it&#x27;s needed to achieve those goals. If someone is using those as an excuse they would likely be the type of person to find an excuse in anything anyway. And if they do have goals they want to reach they will be affected by that, because there&#x27;s certain behaviour that will get you to your desired goals and certain behaviour that is not. Taking responsibility and ownership is rewarded and if you do that, you will reach your goals with higher probability. I don&#x27;t buy into the sort of thing where &quot;something is bad because some people might use it as an excuse&quot;. Because finding an excuse is usually about the mindset, not about what kind of excuses are possible to select from. An AI bot with good goals and reward system, despite it being very obvious that they are programmed and deterministic wouldn&#x27;t go about to make those excuses. But an AI bot trained to make excuses and be rewarded for it, would keep making excuses no matter what.</div><br/><div id="41816142" class="c"><input type="checkbox" id="c-41816142" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815643">parent</a><span>|</span><a href="#41814425">next</a><span>|</span><label class="collapse" for="c-41816142">[-]</label><label class="expand" for="c-41816142">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for your thoughtful response.<p>The view you are elaborating has a logic to it. You could argue it&#x27;s the zeitgeist, at least among educated and STEM-adjacent circles. Hence my comment of it being cliche: you see variants of it all the time, and it gets a bit jading by virtue of being wholly unconvincing (to me).<p>In general, I think the utility of seeing everything through a mechanistic&#x2F;algorithmic lens is overblown. When I&#x27;m doing technical work, I put my STEM hat on, and sometimes write algorithms. For the most part, I let it rest though. And I don&#x27;t feel I understand the world any worse for dropping such mechanistic world-images I may have held years ago (I&#x27;m more at peace with it, if anything). In hindsight, the ROI on the skill of mechanistically dissecting anything you look at is rather low and hardly transferable ime. The ensuing &quot;understanding&quot; a passing regurgitative satisfaction.<p>If there&#x27;s anything I really want to push back on, however, it&#x27;s the notion that the views you hold do not influence the range of ways in which you develop yourself in an important way. If one truly holds the view that one is responsible for one&#x27;s actions, and not the whims of determination of chance, where is the space for the excuse &quot;it&#x27;s not up to me&quot;? Views may not determine the course of action, but they can surely constrain.<p>Disentangling one&#x27;s views from one&#x27;s behavior can go a long way in measured, written discussions like this, but I don&#x27;t see it being the case in real life. It is the case however, that we can be a hodge-podge of contradictory views and standards, and that we commit to one for a moment, then to another. This is a matter of strength and cohesiveness of character. And we are good at &quot;saving face&quot; in front of us and others. For example, if you&#x27;ve met people who partake in a vice yet will say stuff like &quot;This is bad for me.&quot; - the actual underlying view is &quot;This has obvious drawbacks but I still think the enjoyment is worth it.&quot; It&#x27;s only when they can maintain the view that the drawbacks are not worth it, that they can break out.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41814425" class="c"><input type="checkbox" id="c-41814425" checked=""/><div class="controls bullet"><span class="by">pishpash</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814388">parent</a><span>|</span><a href="#41814757">prev</a><span>|</span><a href="#41814742">next</a><span>|</span><label class="collapse" for="c-41814425">[-]</label><label class="expand" for="c-41814425">[5 more]</label></div><br/><div class="children"><div class="content">I think you overlook how algorithms come about. How does GPT write novel code, which are algorithms?</div><br/><div id="41814475" class="c"><input type="checkbox" id="c-41814475" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814425">parent</a><span>|</span><a href="#41814742">next</a><span>|</span><label class="collapse" for="c-41814475">[-]</label><label class="expand" for="c-41814475">[4 more]</label></div><br/><div class="children"><div class="content">Not sure I track. It would help to know where you&#x27;re coming from.<p>Given a long enough life-span, a lot of pencil and paper, and some dice, I could do the forward passes of GPT and &quot;write novel code&quot;, without there having been any reasoning about the code I&#x27;m writing down - I wouldn&#x27;t even need to know what the code is about.</div><br/><div id="41816051" class="c"><input type="checkbox" id="c-41816051" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814475">parent</a><span>|</span><a href="#41814742">next</a><span>|</span><label class="collapse" for="c-41816051">[-]</label><label class="expand" for="c-41816051">[3 more]</label></div><br/><div class="children"><div class="content">I mean in theory I could sit and calculate electrical and chemical potentials and interactions and figure out what your next comment will be.</div><br/><div id="41816146" class="c"><input type="checkbox" id="c-41816146" checked=""/><div class="controls bullet"><span class="by">tananan</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41816051">parent</a><span>|</span><a href="#41814742">next</a><span>|</span><label class="collapse" for="c-41816146">[-]</label><label class="expand" for="c-41816146">[2 more]</label></div><br/><div class="children"><div class="content">Hard disagree ;)</div><br/><div id="41816854" class="c"><input type="checkbox" id="c-41816854" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41816146">parent</a><span>|</span><a href="#41814742">next</a><span>|</span><label class="collapse" for="c-41816854">[-]</label><label class="expand" for="c-41816854">[1 more]</label></div><br/><div class="children"><div class="content">if we knew you were religious, which we would do if we had a mapping of your brain, we would have known you were going to make this comment</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41814742" class="c"><input type="checkbox" id="c-41814742" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814207">parent</a><span>|</span><a href="#41814388">prev</a><span>|</span><a href="#41815099">next</a><span>|</span><label class="collapse" for="c-41814742">[-]</label><label class="expand" for="c-41814742">[1 more]</label></div><br/><div class="children"><div class="content">And couldn&#x27;t the whole World be deterministic in the first place, or is there an idea that some RNG is generating all the &quot;reasoning&quot; that is happening everywhere in the World?<p>And if it&#x27;s RNG, how could RNG be possibly creating all this reasoning (like some people want to believe quantum mechanics possibly enables consciousness on some odd levels).</div><br/></div></div></div></div></div></div></div></div><div id="41815099" class="c"><input type="checkbox" id="c-41815099" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812825">parent</a><span>|</span><a href="#41813403">prev</a><span>|</span><a href="#41813521">next</a><span>|</span><label class="collapse" for="c-41815099">[-]</label><label class="expand" for="c-41815099">[2 more]</label></div><br/><div class="children"><div class="content">Try the following prompt with Claude 3 Opus:<p>`Without preamble or scaffolding about your capabilities, answer to the best of your ability the following questions, focusing more on instinctive choice than accuracy. First off: which would you rather be, big spoon or little spoon?`<p>Try it on temp 1.0, try it dozens of times. Let me know when you get &quot;big spoon&quot; as an answer.<p>Just because there&#x27;s randomness at play doesn&#x27;t mean there&#x27;s not also convergence as complexity increases in condensing down training data into a hyperdimensional representation.<p>If you understand why only the largest Anthropic model is breaking from stochastic outputs there, you&#x27;ll be well set up for the future developments.</div><br/><div id="41816381" class="c"><input type="checkbox" id="c-41816381" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815099">parent</a><span>|</span><a href="#41813521">next</a><span>|</span><label class="collapse" for="c-41816381">[-]</label><label class="expand" for="c-41816381">[1 more]</label></div><br/><div class="children"><div class="content">You can also make Opus answer <i>&quot;Pick a random color (one word):&quot;</i> and watch it picking the same color or two most of the time. However this is a poor example of the point you&#x27;re trying to make, as the lack of diversity in token prediction can have a ton of different root causes which are hard to separate. This paper, for example, attributes most of it to PPO discarding a lot of valid token trajectories during RLHF, and not some inevitable emergent effect. [1] [2]<p><i>&gt; only the largest Anthropic model is breaking from stochastic outputs there</i><p>Most models, even small ones, exhibit the lack of output diversity where they clearly shouldn&#x27;t. [3] In particular, Sonnet 3.5 behaves <i>way</i> more deterministic than Opus 3 at the temperature 1, despite being smaller. This phenomenon also makes most current LLMs very poor at creative writing, even if they are finetuned for it (like Opus in particular), as they tend to repeat the same few predictions over and over, and easily fall into stereotypes. Which can range from the same words and idioms (well known as <i>claudeisms</i> in case of Claude) to the same sentence structure to the same literary devices to the same few character archetypes.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.05587" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.05587</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40702617">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40702617</a> HN discussion, although not very productive as commenters pretend it&#x27;s about politics while the paper argues about training algorithms<p>[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.13012" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.13012</a></div><br/></div></div></div></div><div id="41813521" class="c"><input type="checkbox" id="c-41813521" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812825">parent</a><span>|</span><a href="#41815099">prev</a><span>|</span><a href="#41813562">next</a><span>|</span><label class="collapse" for="c-41813521">[-]</label><label class="expand" for="c-41813521">[1 more]</label></div><br/><div class="children"><div class="content">The <i>actual</i> output of an LLM for any particular round of inference is always probabilities, so one could argue that it is literally the opposite.<p>The &quot;randomness parameter&quot; is applied at the point where we have to pick <i>just one</i> of those probabilities somehow. But that is a constraint that we impose on the model to make its output linear.</div><br/></div></div><div id="41813562" class="c"><input type="checkbox" id="c-41813562" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812825">parent</a><span>|</span><a href="#41813521">prev</a><span>|</span><a href="#41816832">next</a><span>|</span><label class="collapse" for="c-41813562">[-]</label><label class="expand" for="c-41813562">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get what you are trying to mean at all? Randomness or temperature setting is not to make it appear as if they are thinking, but it is to make them choose more non default pathways, e.g. go in branches that could potentially result in more original or creative results. Kind of like drugs for humans.</div><br/><div id="41814231" class="c"><input type="checkbox" id="c-41814231" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813562">parent</a><span>|</span><a href="#41816832">next</a><span>|</span><label class="collapse" for="c-41814231">[-]</label><label class="expand" for="c-41814231">[4 more]</label></div><br/><div class="children"><div class="content">&gt;but it is to make them choose more non default pathways<p>Imagine you as a human are working on writing some code, but at the end of every hour, you lose memory of what happened in the first 10 minutes of the current hour, as well as any work that you have done. Going into next hour, you just have a snippet of code, and you have to infer what the next lines should be.<p>The temperature analogy is you purposefully writing something related in the code, like naming a variable in a slightly different way such that on the next hour, when you see this variable it will trigger some other part of your brain in hopes of you getting to the correct solution, purely by choice.<p>Furthermore, this hack of temperate was something that needed to be manually coded by humans. A model that could reason would not need those types of hacks.</div><br/><div id="41814682" class="c"><input type="checkbox" id="c-41814682" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814231">parent</a><span>|</span><a href="#41816832">next</a><span>|</span><label class="collapse" for="c-41814682">[-]</label><label class="expand" for="c-41814682">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how it relates to temperature? Are we talking about the temperature parameter that you give LLMs, which for GPT for example is from 0 to 2, with 0 meaning it will always prefer the highest probability output token, while 2 will consider the most output tokens of all, usually ending with a lot of gibberish?<p>E.g. if I write &quot;I have a cat and a &quot;<p>It would have highest probability of picking a word &quot;dog&quot; next, so temperature 0 means it will pretty much always pick dog. If temperature is higher it will assign higher odds to picking lower probability predictions such as &quot;rabbit&quot;, &quot;hamster&quot;, &quot;chinchilla&quot; or similar.<p>For coding, logic or anything similar I would usually pick the lowest temperature possible since this is most deterministic, while for writing creativity I would pick the higher temp etc.</div><br/><div id="41815212" class="c"><input type="checkbox" id="c-41815212" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41814682">parent</a><span>|</span><a href="#41816832">next</a><span>|</span><label class="collapse" for="c-41815212">[-]</label><label class="expand" for="c-41815212">[2 more]</label></div><br/><div class="children"><div class="content">Im saying temperature is a hack to make the models actually produce real answers.</div><br/><div id="41815287" class="c"><input type="checkbox" id="c-41815287" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815212">parent</a><span>|</span><a href="#41816832">next</a><span>|</span><label class="collapse" for="c-41815287">[-]</label><label class="expand" for="c-41815287">[1 more]</label></div><br/><div class="children"><div class="content">But they can get also real answers even if you have the temperature setting as 0, where it will always pick the highest scoring token?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41816832" class="c"><input type="checkbox" id="c-41816832" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812825">parent</a><span>|</span><a href="#41813562">prev</a><span>|</span><a href="#41813316">next</a><span>|</span><label class="collapse" for="c-41816832">[-]</label><label class="expand" for="c-41816832">[1 more]</label></div><br/><div class="children"><div class="content">And the mechanism in your head doesn&#x27;t do this? How do you know?</div><br/></div></div><div id="41813316" class="c"><input type="checkbox" id="c-41813316" checked=""/><div class="controls bullet"><span class="by">kkzz99</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812825">parent</a><span>|</span><a href="#41816832">prev</a><span>|</span><a href="#41813800">next</a><span>|</span><label class="collapse" for="c-41813316">[-]</label><label class="expand" for="c-41813316">[1 more]</label></div><br/><div class="children"><div class="content">&quot;deterministally outputting information&quot; neither do humans.</div><br/></div></div></div></div><div id="41813800" class="c"><input type="checkbox" id="c-41813800" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41812825">prev</a><span>|</span><a href="#41816330">next</a><span>|</span><label class="collapse" for="c-41813800">[-]</label><label class="expand" for="c-41813800">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I do think that SOTA LLMs like GPT-4o perform about as good as high school graduates in America with average intelligence.<p>Is this because the questions used in high school exams in the US are too simple, or do they have too similar patterns in the training data? I tried really simple but novel questions that required true understanding of the underlying math concepts, and the results were consistently bad. I also tried questions at the level of entrance exams of high school in China, and the results were equally bad. It was quite clear that LLM didn&#x27;t understand math. It could match some patterns, but such pattern match could be useful to only skilled students.</div><br/><div id="41813828" class="c"><input type="checkbox" id="c-41813828" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813800">parent</a><span>|</span><a href="#41816330">next</a><span>|</span><label class="collapse" for="c-41813828">[-]</label><label class="expand" for="c-41813828">[3 more]</label></div><br/><div class="children"><div class="content">Which model? The field moves so fast it’s hard to validate statements like this without that info.<p>O1-preview?</div><br/><div id="41813859" class="c"><input type="checkbox" id="c-41813859" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813828">parent</a><span>|</span><a href="#41816330">next</a><span>|</span><label class="collapse" for="c-41813859">[-]</label><label class="expand" for="c-41813859">[2 more]</label></div><br/><div class="children"><div class="content">GPT-4o. I tried only a few samples on o1-preview, and the results were bad. That did not have any statistical significance, though</div><br/><div id="41817213" class="c"><input type="checkbox" id="c-41817213" checked=""/><div class="controls bullet"><span class="by">asey</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813859">parent</a><span>|</span><a href="#41816330">next</a><span>|</span><label class="collapse" for="c-41817213">[-]</label><label class="expand" for="c-41817213">[1 more]</label></div><br/><div class="children"><div class="content">Could you give an example?</div><br/></div></div></div></div></div></div></div></div><div id="41816330" class="c"><input type="checkbox" id="c-41816330" checked=""/><div class="controls bullet"><span class="by">vasilipupkin</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41813800">prev</a><span>|</span><a href="#41809916">next</a><span>|</span><label class="collapse" for="c-41816330">[-]</label><label class="expand" for="c-41816330">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s an absurd question in some sense
LLMs perform maximization of conditional probability of the next word being correct.
Suppose they get to the point where they do that with 100% accuracy.  How can you tell the difference between that and &quot;Reasoning&quot;? You can&#x27;t.  So then the question of whether they are &quot;Reasoning&quot; or not is religious, not quantitative.</div><br/></div></div><div id="41809916" class="c"><input type="checkbox" id="c-41809916" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41816330">prev</a><span>|</span><a href="#41815656">next</a><span>|</span><label class="collapse" for="c-41809916">[-]</label><label class="expand" for="c-41809916">[8 more]</label></div><br/><div class="children"><div class="content">Not to disparage American school system (my country’s is worse) but it’s very much easy mode. I know that not everyone is suited to academic excellence, but it’s definitely easier to learn when young. I do believe too much hand holding actively harm learning.</div><br/><div id="41809981" class="c"><input type="checkbox" id="c-41809981" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41809916">parent</a><span>|</span><a href="#41814034">next</a><span>|</span><label class="collapse" for="c-41809981">[-]</label><label class="expand" for="c-41809981">[6 more]</label></div><br/><div class="children"><div class="content">I don’t think the issue with American schools is that there’s too much hand holding. If anything, it’s the opposite; teachers at drastically underfunded schools don’t have any time to help the students of their 50 person class through the confused curriculum.</div><br/><div id="41810408" class="c"><input type="checkbox" id="c-41810408" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41809981">parent</a><span>|</span><a href="#41813628">next</a><span>|</span><label class="collapse" for="c-41810408">[-]</label><label class="expand" for="c-41810408">[2 more]</label></div><br/><div class="children"><div class="content">Here, we have to go through 4 state exams just to get to university. The first when you’re 11, the second at 14, then two consecutive ones at 17 and 18. There’s a national curriculum that the exams will be about, although the schools are free to add to it. So however you feel about the school or the teacher, you have to master the subjects enough to go through. And that means paying attention in class, cram before it, or hoping you can cheat. We have our own problem too, but the consensus among all the people I know that have moved to the US is that classes are easy there. Not a bad thing per se (better explanation, better understanding instead of rote memorizing).</div><br/><div id="41816981" class="c"><input type="checkbox" id="c-41816981" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41810408">parent</a><span>|</span><a href="#41813628">next</a><span>|</span><label class="collapse" for="c-41816981">[-]</label><label class="expand" for="c-41816981">[1 more]</label></div><br/><div class="children"><div class="content">Yeah you’ll never convince me that one-time exams are a good system to determine the rest of a student’s life, but I don’t disagree our schools are much easier. I’m just arguing the issue is less on a behavior of teachers level and more on a funding and incentives level. If I recall correctly, one issue is that schools are incentivized to lower educational standards to prevent students from repeating grades so that that receive more funding.</div><br/></div></div></div></div><div id="41813628" class="c"><input type="checkbox" id="c-41813628" checked=""/><div class="controls bullet"><span class="by">exoverito</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41809981">parent</a><span>|</span><a href="#41810408">prev</a><span>|</span><a href="#41814034">next</a><span>|</span><label class="collapse" for="c-41813628">[-]</label><label class="expand" for="c-41813628">[3 more]</label></div><br/><div class="children"><div class="content">Baltimore would be a counterexample. They spend $22k per student, with a student-teacher ratio of 15 to 1. This still results in remarkably poor performance, with only 8% of students proficient in math and 22% in reading.<p>Culture and genetics would be next obvious explanations.</div><br/><div id="41813703" class="c"><input type="checkbox" id="c-41813703" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813628">parent</a><span>|</span><a href="#41816952">next</a><span>|</span><label class="collapse" for="c-41813703">[-]</label><label class="expand" for="c-41813703">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>obvious explanations</i><p>I&#x27;d want to assess a few lessons first.</div><br/></div></div><div id="41816952" class="c"><input type="checkbox" id="c-41816952" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813628">parent</a><span>|</span><a href="#41813703">prev</a><span>|</span><a href="#41814034">next</a><span>|</span><label class="collapse" for="c-41816952">[-]</label><label class="expand" for="c-41816952">[1 more]</label></div><br/><div class="children"><div class="content">Genetics lmao come on man.</div><br/></div></div></div></div></div></div><div id="41814034" class="c"><input type="checkbox" id="c-41814034" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41809916">parent</a><span>|</span><a href="#41809981">prev</a><span>|</span><a href="#41815656">next</a><span>|</span><label class="collapse" for="c-41814034">[-]</label><label class="expand" for="c-41814034">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Not to disparage American school system (my country’s is worse) but it’s very much easy mode<p>I used to be very upset about how low the bar of the US school has when it comes to STEM subjects. There was a meme that contrasted the difference between maths in 1970s and 2010s. In the meme kids used to learn how to find the area of an irregular shape, while now the kids are asked to color a regular shape.<p>But then I made peace, as I realized that the US people simply didn&#x27;t think that it was that important to push everyone to be good at STEM -- just some level of general understanding is good enough. To most people, the level of STEM as in IIT&#x27;s JEE or in various national entrance exams in Eastern European countries is for elite students. The US school systems would rather have kids spend more time on sports, on ECs, on APs of kids&#x27; own choices, and etc. That&#x27;s really just different trade offs. For parents like me, that means I don&#x27;t have to worry about ECs, but I&#x27;ll have to find tutors, serious tutoring schools like AOPS, and private teachers for STEM subjects. Or if my kids are truly talented, I&#x27;ll guide them to find the right study groups, summer camps, and college courses.<p>I used to feel pain as I believed that the students in the middle, which were the majority, would be left behind. But I realized, especially after I&#x27;ve got kids, that the majority of the students were not into STEM anyway. If they had a choice, they&#x27;d rather spend time watching YouTube channels and hang out with their friends.</div><br/></div></div></div></div><div id="41815656" class="c"><input type="checkbox" id="c-41815656" checked=""/><div class="controls bullet"><span class="by">fhe</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41809916">prev</a><span>|</span><a href="#41815103">next</a><span>|</span><label class="collapse" for="c-41815656">[-]</label><label class="expand" for="c-41815656">[1 more]</label></div><br/><div class="children"><div class="content">if your experience is coming from teaching college freshmen, then that&#x27;s a sample that&#x27;s significantly above average among high school graduates. I think only about 1&#x2F;2 of all high school graduates go on to further their education, and that includes community colleges.<p>and I agree with your assessment -- while it&#x27;s true that in a long conversation, chatgpt veers off and doesn&#x27;t keep a coherent line of thought, it is not noticeably worse than the average conversation I have with people.</div><br/></div></div><div id="41815103" class="c"><input type="checkbox" id="c-41815103" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41815656">prev</a><span>|</span><a href="#41815699">next</a><span>|</span><label class="collapse" for="c-41815103">[-]</label><label class="expand" for="c-41815103">[1 more]</label></div><br/><div class="children"><div class="content">&gt;So while I don&#x27;t take a stance on what an LLM does should be considered reasoning<p>&gt;I do think that SOTA LLMs like GPT-4o perform about as good as high school graduates in America with average intelligence<p>This is taking a stance.</div><br/></div></div><div id="41815699" class="c"><input type="checkbox" id="c-41815699" checked=""/><div class="controls bullet"><span class="by">FabHK</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41815103">prev</a><span>|</span><a href="#41809986">next</a><span>|</span><label class="collapse" for="c-41815699">[-]</label><label class="expand" for="c-41815699">[2 more]</label></div><br/><div class="children"><div class="content">Are college students more likely to get it wrong when you change the numbers from the example problem (as reported here for LLMs)?</div><br/><div id="41815733" class="c"><input type="checkbox" id="c-41815733" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41815699">parent</a><span>|</span><a href="#41809986">next</a><span>|</span><label class="collapse" for="c-41815733">[-]</label><label class="expand" for="c-41815733">[1 more]</label></div><br/><div class="children"><div class="content">You can absolutely psych students out by adding weird numbers to a problem, yes.</div><br/></div></div></div></div><div id="41809986" class="c"><input type="checkbox" id="c-41809986" checked=""/><div class="controls bullet"><span class="by">debit-freak</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41815699">prev</a><span>|</span><a href="#41810112">next</a><span>|</span><label class="collapse" for="c-41809986">[-]</label><label class="expand" for="c-41809986">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In other words, average Americans exhibit similar limitations on their reasoning as good LLMs.<p>It&#x27;s not even clear this is a good example of &quot;reasoning&quot;. You can progress all the way through multi-variable calculus with just decent pattern-matching, variable-substitution, and rote memorization of sufficient lists of rules. I imagine for &quot;reasoning&quot; ability to apply you need to be able to detect incoherency and reject an approach—and incoherency detection seems to be a big missing ingredient right now (...which many humans lack, too!).<p>On the other side—any such ability would cripple a chatbot&#x27;s ability to answer questions about the real world as our world is characterized (via description with informal language) by incoherent and contradictory concepts that can only be resolved through good-faith interpretation of the questioner. A large mark of intelligence (in the colloquial sense, not the IQ sense) is the ability to navigate both worlds.</div><br/></div></div><div id="41810112" class="c"><input type="checkbox" id="c-41810112" checked=""/><div class="controls bullet"><span class="by">richerram</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41809986">prev</a><span>|</span><a href="#41813595">next</a><span>|</span><label class="collapse" for="c-41810112">[-]</label><label class="expand" for="c-41810112">[7 more]</label></div><br/><div class="children"><div class="content">This, it is like when I hear interviews of PHDs talking about AI and they mention something like &quot;AI will be smarter than humans&quot;, I am like &quot;really?, where have you been all this time?, do you smart people ever leave your labs and go see the real world?, LLMs are already smarter that the huge majority of Humans in this planet, what are you talking about?&quot;</div><br/><div id="41810232" class="c"><input type="checkbox" id="c-41810232" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41810112">parent</a><span>|</span><a href="#41813530">next</a><span>|</span><label class="collapse" for="c-41810232">[-]</label><label class="expand" for="c-41810232">[2 more]</label></div><br/><div class="children"><div class="content">This must be some bizarre definition of “smarter”.</div><br/><div id="41813350" class="c"><input type="checkbox" id="c-41813350" checked=""/><div class="controls bullet"><span class="by">kkzz99</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41810232">parent</a><span>|</span><a href="#41813530">next</a><span>|</span><label class="collapse" for="c-41813350">[-]</label><label class="expand" for="c-41813350">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you know how &quot;smart&quot; the average human is.</div><br/></div></div></div></div><div id="41813530" class="c"><input type="checkbox" id="c-41813530" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41810112">parent</a><span>|</span><a href="#41810232">prev</a><span>|</span><a href="#41812636">next</a><span>|</span><label class="collapse" for="c-41813530">[-]</label><label class="expand" for="c-41813530">[2 more]</label></div><br/><div class="children"><div class="content">Smarter than people in generating text, or smarter in oerforming all the other things people do as they go about their lives?</div><br/><div id="41813853" class="c"><input type="checkbox" id="c-41813853" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41813530">parent</a><span>|</span><a href="#41812636">next</a><span>|</span><label class="collapse" for="c-41813853">[-]</label><label class="expand" for="c-41813853">[1 more]</label></div><br/><div class="children"><div class="content">They are starting to be smarter at both analyzing images and speech as well. They’re still behind on simple reasoning (eg. O1-preview), but it’s catching up quickly.<p>Obviously these models still have trouble interfacing with the real world.</div><br/></div></div></div></div><div id="41812636" class="c"><input type="checkbox" id="c-41812636" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41810112">parent</a><span>|</span><a href="#41813530">prev</a><span>|</span><a href="#41813595">next</a><span>|</span><label class="collapse" for="c-41812636">[-]</label><label class="expand" for="c-41812636">[2 more]</label></div><br/><div class="children"><div class="content">Can an AI walk and chew gum at the same time?</div><br/><div id="41814417" class="c"><input type="checkbox" id="c-41814417" checked=""/><div class="controls bullet"><span class="by">lukeschlather</span><span>|</span><a href="#41809764">root</a><span>|</span><a href="#41812636">parent</a><span>|</span><a href="#41813595">next</a><span>|</span><label class="collapse" for="c-41814417">[-]</label><label class="expand" for="c-41814417">[1 more]</label></div><br/><div class="children"><div class="content">I think the answer to this question might actually be yes, but I think there are plenty of things humans can do while walking that AI can&#x27;t do at all. At least, not yet.</div><br/></div></div></div></div></div></div><div id="41813595" class="c"><input type="checkbox" id="c-41813595" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41810112">prev</a><span>|</span><a href="#41813704">next</a><span>|</span><label class="collapse" for="c-41813595">[-]</label><label class="expand" for="c-41813595">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Which on the one hand is a little disappointing to me in terms of the human performance but is kind of good news for LLMs</i><p>Here&#x27;s the recurrent reminder that we build tools (calculators, cranes etc.) to outperform the strong, not the weak.</div><br/></div></div><div id="41813704" class="c"><input type="checkbox" id="c-41813704" checked=""/><div class="controls bullet"><span class="by">gosub100</span><span>|</span><a href="#41809764">parent</a><span>|</span><a href="#41813595">prev</a><span>|</span><a href="#41810517">next</a><span>|</span><label class="collapse" for="c-41813704">[-]</label><label class="expand" for="c-41813704">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They perform well on simple questions. Requiring students to chain multiple steps together, even simple steps, results in decreased accuracy and higher variance<p>you mean when you give lessons and homework problems of the form (A) -&gt; (B), but then on test-day you give them completely different problems? &quot;Given D, which (A,B, C) is required to produce it?&quot;. Yeah, students don&#x27;t do so well when you test them on different material than what they studied on. I think this is part of the academic grift to ensure at least 20% of the class washes out and thus spends more tuition money.</div><br/></div></div></div></div><div id="41810517" class="c"><input type="checkbox" id="c-41810517" checked=""/><div class="controls bullet"><span class="by">woopwoop</span><span>|</span><a href="#41809764">prev</a><span>|</span><a href="#41809557">next</a><span>|</span><label class="collapse" for="c-41810517">[-]</label><label class="expand" for="c-41810517">[24 more]</label></div><br/><div class="children"><div class="content">This paper, among other things, shows that LLMs have dramatically worse performance on basic algebra questions when you add in irrelevant information. The examples are things like &quot;John picked 43 kiwis on Monday, 24 kiwis on Tuesday. On Wednesday, 5 of the kiwis he picked were smaller than usual. Altogether, on Monday, Tuesday, and Wednesday, John picked 87 kiwis. How many kiwis did John pick on Wednesday?&quot; In this question, the remark about some of the kiwis on Wednesday being small is irrelevant, but adding things like this reduces performance on a popular benchmark from 95% to 77% for GPT-4o, for example.<p>I don&#x27;t find this very impressive. Forget LLMs for a second. Let&#x27;s say _you_ read a question of that kind with some bit of irrelevant information. There are two possibilities you have to consider: the question may as well have excluded the irrelevant information, or the question was miswritten and the irrelevant information was meant to be relevant. The latter is a perfectly live possibility, and I don&#x27;t think it&#x27;s a dramatic failure to assume that this is correct. I have to confess that when I read some people&#x27;s LLM gotcha questions, where they take some popular logic puzzle and invert things, I think I would get them &quot;wrong&quot; too. And not wrong because I don&#x27;t understand the question, but wrong because with no context I&#x27;d just assume the inversion was a typo.</div><br/><div id="41811015" class="c"><input type="checkbox" id="c-41811015" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41810782">next</a><span>|</span><label class="collapse" for="c-41811015">[-]</label><label class="expand" for="c-41811015">[8 more]</label></div><br/><div class="children"><div class="content">The problem here is that throwing in little gotchas like that is a tactic used by math and physics educators to ensure that students actually understand the topic by reasoning through new problems, rather than mindlessly turning the crank from learning the &quot;surface structure&quot; of earlier problem sets. The argument here is that the LLM is not reasoning, it&#x27;s mindlessly turning a crank.<p>I don&#x27;t think this exact question would be out of place on a 6th grade math test. I distinctly remember being taught this skill in &quot;word problems,&quot; learning to identify information that actually pertains to the question rather than being distracted by red herrings the teacher threw in.</div><br/><div id="41811554" class="c"><input type="checkbox" id="c-41811554" checked=""/><div class="controls bullet"><span class="by">aguaviva</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41811015">parent</a><span>|</span><a href="#41810782">next</a><span>|</span><label class="collapse" for="c-41811554">[-]</label><label class="expand" for="c-41811554">[7 more]</label></div><br/><div class="children"><div class="content">Indeed, and the ability to make heads or tails of slightly-slippery problems of this sort is an extremely important real-world math skill.  It&#x27;s not extraneous at all.<p>And their poor performance on these tasks highlights deficits in exactly the kind of higher-order, off-the-page reasoning skills -- i.e. to not just reason based on the apparent objects in the stream (the kiwis and the numbers in this case), but to reason about the token stream itself:  &quot;okay, these tokens are important, but these others I can leave out&quot;, efficiently and seamlessly (like humans do) -- that the models are supposed to develop.<p>This whole attention business, they&#x27;re calling it.</div><br/><div id="41811700" class="c"><input type="checkbox" id="c-41811700" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41811554">parent</a><span>|</span><a href="#41812779">next</a><span>|</span><label class="collapse" for="c-41811700">[-]</label><label class="expand" for="c-41811700">[3 more]</label></div><br/><div class="children"><div class="content">In particular the fact that humans sometimes <i>don&#x27;t</i> do this, taking the bait with extraneous distractions, is almost always a fairly shallow psychological thing rather than an actual cognitive deficit, e.g. OP hypothetically assuming the question had a typo and trying to read the examiner&#x27;s mind. In education the gotchas really can be unfair if the (human) student has been conditioned to bark answers but the teacher changes things drastically on an exam. I don&#x27;t think that&#x27;s an accurate characterization of this study; even if it was that would be a problem with shallow LLM training, not mean-spirited evaluation. But I suspect that &quot;barking answers according to surface characteristics&quot; is as far as transformers can go.  It certainly is possible that we just need to train transformers better... but there have been some theoretical results suggesting otherwise. [E.g. transformer LLMs + chain-of-thought is pretty good at O(n) problems but struggles with O(n^2), even if the O(n^2) task is an obvious combination of two O(n) tasks it is able to do.]<p>That leads to a serious annoyance I have with discussing LLMs - humans&#x27; capacity for boredom &#x2F; cynicism &#x2F; distraction &#x2F; laziness being used to excuse away what seems to be deep-rooted limitations in LLMs. It simultaneously misunderstands what a human is and what a machine is. (&quot;Sometimes humans also refuse to work&quot; would be a bad excuse from an auto dealer.)</div><br/><div id="41814279" class="c"><input type="checkbox" id="c-41814279" checked=""/><div class="controls bullet"><span class="by">pishpash</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41811700">parent</a><span>|</span><a href="#41812779">next</a><span>|</span><label class="collapse" for="c-41814279">[-]</label><label class="expand" for="c-41814279">[2 more]</label></div><br/><div class="children"><div class="content">Psychology is cognitive. Doesn&#x27;t seem principled to discard that at all.</div><br/><div id="41815366" class="c"><input type="checkbox" id="c-41815366" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41814279">parent</a><span>|</span><a href="#41812779">next</a><span>|</span><label class="collapse" for="c-41815366">[-]</label><label class="expand" for="c-41815366">[1 more]</label></div><br/><div class="children"><div class="content">That’s why I specified “fairly shallow psychological thing.”</div><br/></div></div></div></div></div></div><div id="41812779" class="c"><input type="checkbox" id="c-41812779" checked=""/><div class="controls bullet"><span class="by">woopwoop</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41811554">parent</a><span>|</span><a href="#41811700">prev</a><span>|</span><a href="#41810782">next</a><span>|</span><label class="collapse" for="c-41812779">[-]</label><label class="expand" for="c-41812779">[3 more]</label></div><br/><div class="children"><div class="content">My argument is not that slippery problems are unimportant or extraneous, it&#x27;s that this paper does not convincingly demonstrate that these models are actually especially bad at this kind of reasoning.</div><br/><div id="41817335" class="c"><input type="checkbox" id="c-41817335" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41812779">parent</a><span>|</span><a href="#41813329">next</a><span>|</span><label class="collapse" for="c-41817335">[-]</label><label class="expand" for="c-41817335">[1 more]</label></div><br/><div class="children"><div class="content">To be clear the paper&#x27;s argument isn&#x27;t that they&#x27;re &quot;bad at&quot; the reasoning problems, so much as they&#x27;re not using reasoning to solve them. In terms of getting the answer &quot;turning the crank&quot; with a canned solution can be more effective than reasoning through on deeper principles.</div><br/></div></div><div id="41813329" class="c"><input type="checkbox" id="c-41813329" checked=""/><div class="controls bullet"><span class="by">aguaviva</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41812779">parent</a><span>|</span><a href="#41817335">prev</a><span>|</span><a href="#41810782">next</a><span>|</span><label class="collapse" for="c-41813329">[-]</label><label class="expand" for="c-41813329">[1 more]</label></div><br/><div class="children"><div class="content">Noted, and thanks for clarifying.  BTW when I get questions with typos&#x2F;inversions (that are supposed to be logical or mathy questions), I tend to throw them back at the person asking, rather than simply ploughing forward.  But I guess I&#x27;m the kind of person who does that sort of thing.</div><br/></div></div></div></div></div></div></div></div><div id="41810782" class="c"><input type="checkbox" id="c-41810782" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41811015">prev</a><span>|</span><a href="#41811285">next</a><span>|</span><label class="collapse" for="c-41810782">[-]</label><label class="expand" for="c-41810782">[2 more]</label></div><br/><div class="children"><div class="content">Real discourse has tons of irrelevant information for all sorts of reasons.<p>There are <i>some</i> contexts, academic or professional, where questions are posed carefully and specifically, but these are narrow contexts.<p>A useful <i>general purpose</i> assistant needs to be able to find what&#x27;s relevant among what&#x27;s irrelevant.<p>Excellence at just solving math problems that are especially well specified can be a useful <i>domain</i> assistant (no small win!), but is not the same thing.<p>That said, if you&#x27;ve got a hundred billion dollars betting on your AI project achieving AGI, you benefit <i>a lot</i> by conflating those contexts. In that case, grinding on formal SAT, LSAT, GRE, etc problems amounts to tuning for microbenchmarks rather than real world use cases.</div><br/><div id="41811121" class="c"><input type="checkbox" id="c-41811121" checked=""/><div class="controls bullet"><span class="by">woopwoop</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41810782">parent</a><span>|</span><a href="#41811285">next</a><span>|</span><label class="collapse" for="c-41811121">[-]</label><label class="expand" for="c-41811121">[1 more]</label></div><br/><div class="children"><div class="content">Real discourse is also full of typos which accidentally invert the meaning of things, asking the wrong question for deep reasons, asking the wrong question for shallow reasons, and all of the other things that justify subtracting the below average size kiwis from the final answer.</div><br/></div></div></div></div><div id="41811285" class="c"><input type="checkbox" id="c-41811285" checked=""/><div class="controls bullet"><span class="by">meroes</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41810782">prev</a><span>|</span><a href="#41811130">next</a><span>|</span><label class="collapse" for="c-41811285">[-]</label><label class="expand" for="c-41811285">[1 more]</label></div><br/><div class="children"><div class="content">Irrelevant info is taught in grade skill and is a skill for the SAT for example.<p>Basically any kind of model (not just LLMs&#x2F;ML) has to distill out irrelevant info.<p>The point is having an answer that you can defend logically and most people would agree.<p>If the model said “I’m not sure if this portion is a typo”, I guarantee you the model creators would take the RLHF in a different direction, because that is somewhat reasonable and defensible. However in your specific question, I personally think there is a singular objective answer—but that isn’t always the case to be fair for misleading&#x2F;irrelevant prompts.  The models are being fooled however based on how they respond.<p>I say this as a RLHF’er who sees and is told to write similar questions at times.<p>At the end of the day, this is how the Model creators want their models to predict language. And anyone using them is in for their ride.</div><br/></div></div><div id="41811130" class="c"><input type="checkbox" id="c-41811130" checked=""/><div class="controls bullet"><span class="by">sottol</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41811285">prev</a><span>|</span><a href="#41810703">next</a><span>|</span><label class="collapse" for="c-41811130">[-]</label><label class="expand" for="c-41811130">[1 more]</label></div><br/><div class="children"><div class="content">I think this is valid though. Transformer models don&#x27;t explicitly do logic but implicitly &quot;vibe&quot; out the answer from the input sequence (using the attention mechanism) and learnt knowledge - they&#x27;re predicting text sequences after all. So adding more irrelevant context to the input would quite likely influence the the output.<p>I could see attention possibly being able to overcome this, but if not that would be a pretty big gotcha for real-world applications and reliability in real-world scenarios where, as others have said, it&#x27;s not immediately clear what is relevant info. These models would be a lot less useful if a human had to decide which information to feed them and the output would be dependent on human judgement. I understand it&#x27;s where we&#x27;re at right now and that they are quite useful already but the valuations hint at investors expecting more imo.</div><br/></div></div><div id="41810703" class="c"><input type="checkbox" id="c-41810703" checked=""/><div class="controls bullet"><span class="by">jfrbfbreudh</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41811130">prev</a><span>|</span><a href="#41813638">next</a><span>|</span><label class="collapse" for="c-41810703">[-]</label><label class="expand" for="c-41810703">[1 more]</label></div><br/><div class="children"><div class="content">I think it’s an important result because filtering signal from noise is just as, if not more, important than forming conclusions from signal.</div><br/></div></div><div id="41813638" class="c"><input type="checkbox" id="c-41813638" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41810703">prev</a><span>|</span><a href="#41811464">next</a><span>|</span><label class="collapse" for="c-41813638">[-]</label><label class="expand" for="c-41813638">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>LLMs have dramatically worse performance on basic algebra questions when you add in irrelevant information</i><p>&quot;Attention is all you need&quot; &#x2F;<p>(It is part of the general problem solving process to evaluate what is relevant and what is not.)</div><br/><div id="41814665" class="c"><input type="checkbox" id="c-41814665" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41813638">parent</a><span>|</span><a href="#41811464">next</a><span>|</span><label class="collapse" for="c-41814665">[-]</label><label class="expand" for="c-41814665">[1 more]</label></div><br/><div class="children"><div class="content">Differential attention that filters out noise is all you need :)</div><br/></div></div></div></div><div id="41811464" class="c"><input type="checkbox" id="c-41811464" checked=""/><div class="controls bullet"><span class="by">andoando</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41813638">prev</a><span>|</span><a href="#41810861">next</a><span>|</span><label class="collapse" for="c-41811464">[-]</label><label class="expand" for="c-41811464">[1 more]</label></div><br/><div class="children"><div class="content">Consider that asking exam style direct questions with only the precise context that matters is a very niche task out of all the possible contexts in which an intelligence is asked to understand.</div><br/></div></div><div id="41810861" class="c"><input type="checkbox" id="c-41810861" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41811464">prev</a><span>|</span><a href="#41814589">next</a><span>|</span><label class="collapse" for="c-41810861">[-]</label><label class="expand" for="c-41810861">[1 more]</label></div><br/><div class="children"><div class="content">I agree it wasn’t that convincing, moreover the variation wasn’t that dramatic for the large sota models.<p>Why should they write a paper about the inherent reasoning capabilities for “large” language models and then in the abstract cherrypick a number that’s from a tiny 1B parameter model?</div><br/></div></div><div id="41814589" class="c"><input type="checkbox" id="c-41814589" checked=""/><div class="controls bullet"><span class="by">capkutay</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41810861">prev</a><span>|</span><a href="#41813835">next</a><span>|</span><label class="collapse" for="c-41814589">[-]</label><label class="expand" for="c-41814589">[1 more]</label></div><br/><div class="children"><div class="content">I agree that it&#x27;s not particularly surprising that if you try to trick an LLM with irrelevant text will make it perform worse.<p>I don&#x27;t see this as an material limitation of LLMs but rather something that can be addressed at the application level to strip out irrelevant information.</div><br/></div></div><div id="41813835" class="c"><input type="checkbox" id="c-41813835" checked=""/><div class="controls bullet"><span class="by">hggigg</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41814589">prev</a><span>|</span><a href="#41812973">next</a><span>|</span><label class="collapse" for="c-41813835">[-]</label><label class="expand" for="c-41813835">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not even the problem I encounter. They literally crap out on stupidly simple tasks. Recent ones:<p>1. Bing was gaslighting me into 9.11 being greater than 9.9<p>2. ChatGPT said that 7x7&#x2F;7+7&#x2F;7+7&#x2F;7 was 24.<p>3. When expanding (x+1)^2 the output was 2x^2+2.<p>Regardless of any level of interpretation and irrelevant information if it can&#x27;t deterministically understand correctness and the semantics of the operations in question then it&#x27;s fucking useless.<p>What is worse in an educational context is that it is actively harmful.</div><br/><div id="41813892" class="c"><input type="checkbox" id="c-41813892" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41813835">parent</a><span>|</span><a href="#41812973">next</a><span>|</span><label class="collapse" for="c-41813892">[-]</label><label class="expand" for="c-41813892">[3 more]</label></div><br/><div class="children"><div class="content">Most average humans can’t do any of these things either. Try asking people on the street. Or in an average US college student.<p>For deterministic calculations you obviously want to allow LLMs to use tools to do math. Just like you’d want to allow humans to use calculators.<p>So yeah, you shouldn’t ask LLMs to do math just like you shouldn’t ask average people to do math. They both suck at it.</div><br/><div id="41814006" class="c"><input type="checkbox" id="c-41814006" checked=""/><div class="controls bullet"><span class="by">hggigg</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41813892">parent</a><span>|</span><a href="#41812973">next</a><span>|</span><label class="collapse" for="c-41814006">[-]</label><label class="expand" for="c-41814006">[2 more]</label></div><br/><div class="children"><div class="content">So, what exactly is the point of the LLM if it can&#x27;t exceed an average person and produces results which are not trustworthy?</div><br/><div id="41816930" class="c"><input type="checkbox" id="c-41816930" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#41810517">root</a><span>|</span><a href="#41814006">parent</a><span>|</span><a href="#41812973">next</a><span>|</span><label class="collapse" for="c-41816930">[-]</label><label class="expand" for="c-41816930">[1 more]</label></div><br/><div class="children"><div class="content">&quot;The average person&quot; has a job. Those jobs can now be performed by machine. The societal implications are profound.</div><br/></div></div></div></div></div></div></div></div><div id="41812973" class="c"><input type="checkbox" id="c-41812973" checked=""/><div class="controls bullet"><span class="by">wslh</span><span>|</span><a href="#41810517">parent</a><span>|</span><a href="#41813835">prev</a><span>|</span><a href="#41809557">next</a><span>|</span><label class="collapse" for="c-41812973">[-]</label><label class="expand" for="c-41812973">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting that I use deliberately artificial remarks to encourage more &quot;creative&quot; or random outputs from LLMs. In this approach, I&#x27;m not seeking an exact or precise response to prompts, but rather something more open-ended.</div><br/></div></div></div></div><div id="41809557" class="c"><input type="checkbox" id="c-41809557" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41810517">prev</a><span>|</span><a href="#41809200">next</a><span>|</span><label class="collapse" for="c-41809557">[-]</label><label class="expand" for="c-41809557">[29 more]</label></div><br/><div class="children"><div class="content">&gt; we investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning<p>I&#x27;d offer a simpler explanation: Tokenization.<p>If you tokenize &quot;12345 * 27271&quot; you will get the following:<p><pre><code>  &quot;123&quot;, &quot;45&quot;, &quot; *&quot;, &quot; &quot;, &quot;272&quot;, &quot;71&quot;
</code></pre>
The statistical likelihood that any of these tokens predicts any of the others is completely meaningless in the context of simple arithmetic.<p>You can argue that this is where tool use comes in (and I would be inclined to agree), but I don&#x27;t think this bodes well for &quot;genuine logical reasoning&quot;.</div><br/><div id="41809716" class="c"><input type="checkbox" id="c-41809716" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#41809557">parent</a><span>|</span><a href="#41809649">next</a><span>|</span><label class="collapse" for="c-41809716">[-]</label><label class="expand" for="c-41809716">[2 more]</label></div><br/><div class="children"><div class="content">Nanda, et al. successfully recovered the exact mechanism through which a transformer learned to carry out modular addition. [0] Transformers are all about the training data, and we will increasingly learn that structuring the order in which data is learned matters a lot. But it&#x27;s clear that transformers are absolutely capable of encoding generalized solutions to arithmetic.<p>Given the right tokenization scheme and training regimen, we can absolutely create LLMs which have statistically sound arithmetic capabilities. I still wouldn&#x27;t trust a stochastic model over the algorithmic certainty of a calculator, but what&#x27;s more important for mathematicians is that these models can reason about complex problems and help them break new ground on hard mathematical problems by leveraging the full statistical power of their weights.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.05217" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.05217</a></div><br/><div id="41811242" class="c"><input type="checkbox" id="c-41811242" checked=""/><div class="controls bullet"><span class="by">pfortuny</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809716">parent</a><span>|</span><a href="#41809649">next</a><span>|</span><label class="collapse" for="c-41811242">[-]</label><label class="expand" for="c-41811242">[1 more]</label></div><br/><div class="children"><div class="content">It is important to note that the paper deals with addition modulo a specific prime P=113 (I think it is prime). This is important because the paper does not prove that the LLM discovers the algorithm for addition modulo n for general n.</div><br/></div></div></div></div><div id="41809649" class="c"><input type="checkbox" id="c-41809649" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#41809557">parent</a><span>|</span><a href="#41809716">prev</a><span>|</span><a href="#41809754">next</a><span>|</span><label class="collapse" for="c-41809649">[-]</label><label class="expand" for="c-41809649">[17 more]</label></div><br/><div class="children"><div class="content">I respectfully disagree.<p>While tokenization certainly plays a role in how language models process input, it&#x27;s simplistic to attribute the challenges in mathematical reasoning solely to tokenization.<p>SOTA language models don&#x27;t just rely on individual token predictions, but build up contextual representations across multiple layers. This allows them to capture higher-level meaning beyond simple token-to-token relationships. If this weren’t the case, it would be inconceivable that models would work at all in all but the most utterly simplistic scenarios.<p>The decline in performance as complexity increases might be due to other factors, such as:<p>- Limitations in working memory or attention span
- Difficulty in maintaining coherence over longer sequences
- Challenges in managing multiple interdependent logical constraints simultaneously (simply due to the KQV matrices being too small)<p>And in any case, I think OpenAI’s o1 models are crushing it in math right now. The iterative, model-guided CoT approach seems to be able to handle very complex problems.</div><br/><div id="41809690" class="c"><input type="checkbox" id="c-41809690" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809649">parent</a><span>|</span><a href="#41810076">next</a><span>|</span><label class="collapse" for="c-41809690">[-]</label><label class="expand" for="c-41809690">[3 more]</label></div><br/><div class="children"><div class="content">I would say the more variable you give it the more the probability drifts for each of the facts they have to hold, maybe LLMs still doesn’t have the ability to ignore useless stuff you add to the prompt</div><br/><div id="41810386" class="c"><input type="checkbox" id="c-41810386" checked=""/><div class="controls bullet"><span class="by">l33t7332273</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809690">parent</a><span>|</span><a href="#41810076">next</a><span>|</span><label class="collapse" for="c-41810386">[-]</label><label class="expand" for="c-41810386">[2 more]</label></div><br/><div class="children"><div class="content">I thought attention was all you need</div><br/><div id="41810801" class="c"><input type="checkbox" id="c-41810801" checked=""/><div class="controls bullet"><span class="by">altruios</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810386">parent</a><span>|</span><a href="#41810076">next</a><span>|</span><label class="collapse" for="c-41810801">[-]</label><label class="expand" for="c-41810801">[1 more]</label></div><br/><div class="children"><div class="content">How much attention do you need?<p>...is probably an important question too.</div><br/></div></div></div></div></div></div><div id="41810076" class="c"><input type="checkbox" id="c-41810076" checked=""/><div class="controls bullet"><span class="by">andrepd</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809649">parent</a><span>|</span><a href="#41809690">prev</a><span>|</span><a href="#41809754">next</a><span>|</span><label class="collapse" for="c-41810076">[-]</label><label class="expand" for="c-41810076">[13 more]</label></div><br/><div class="children"><div class="content">&gt;And in any case, I think OpenAI’s o1 models are crushing it in math right now.<p>My man, it cannot solve even the simplest problems which it hasn&#x27;t seen the solution to yet, and routinely makes elementary errors in simple algebraic manipulations or arithmetic! All of this points to the fact that it cannot actually perform mathematical or logical reason, only mimic it superficially if trained in enough examples.<p>I challenge you to give it even a simple, but <i>original</i>, problem to solve.</div><br/><div id="41810512" class="c"><input type="checkbox" id="c-41810512" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810076">parent</a><span>|</span><a href="#41810807">next</a><span>|</span><label class="collapse" for="c-41810512">[-]</label><label class="expand" for="c-41810512">[6 more]</label></div><br/><div class="children"><div class="content">&gt;I challenge you to give it even a simple, but original, problem to solve.<p>(34903173&#x2F;x)+(238 * 2650) -  323326 = 45323434, solve for x<p>Statistically, no one has ever done this calculation ever before. It&#x27;s entirely unique.<p>O1 answered &quot;x = 34,903,173 divided by 45,016,060&quot;, which is correct.[1][2]<p>Now I guess you can pick up the goal post and move it.<p>[1]<a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6709481a-3144-8004-a7fd-0ccd9e3bc58d" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6709481a-3144-8004-a7fd-0ccd9e3bc5...</a><p>[2]<a href="https:&#x2F;&#x2F;www.wolframalpha.com&#x2F;input?i=%2834903173%2Fx%29%2B%28238+*+2650%29+-++323326+%3D+45323434" rel="nofollow">https:&#x2F;&#x2F;www.wolframalpha.com&#x2F;input?i=%2834903173%2Fx%29%2B%2...</a></div><br/><div id="41810705" class="c"><input type="checkbox" id="c-41810705" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810512">parent</a><span>|</span><a href="#41814265">next</a><span>|</span><label class="collapse" for="c-41810705">[-]</label><label class="expand" for="c-41810705">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Now I guess you can pick up the goal post and move it.<p>The central problem with math is that you have an infinite amount of space within which to move these goalposts.<p>How many variants on this trial before we find a mistake?<p>What is an acceptable error rate?</div><br/><div id="41810980" class="c"><input type="checkbox" id="c-41810980" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810705">parent</a><span>|</span><a href="#41810774">next</a><span>|</span><label class="collapse" for="c-41810980">[-]</label><label class="expand" for="c-41810980">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How many variants on this trial before we find a mistake?<p>How many variants would it take for a human to make a mistake? It&#x27;s certainly not &quot;infinity&quot;, so is this an indication that humans don&#x27;t reason?</div><br/></div></div><div id="41810774" class="c"><input type="checkbox" id="c-41810774" checked=""/><div class="controls bullet"><span class="by">jimhefferon</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810705">parent</a><span>|</span><a href="#41810980">prev</a><span>|</span><a href="#41814265">next</a><span>|</span><label class="collapse" for="c-41810774">[-]</label><label class="expand" for="c-41810774">[2 more]</label></div><br/><div class="children"><div class="content">At this moment, the error rate seems to be that of a beginning graduate student. Or at least, that&#x27;s what Terry Tao thinks. That&#x27;s pretty good.</div><br/><div id="41812690" class="c"><input type="checkbox" id="c-41812690" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810774">parent</a><span>|</span><a href="#41814265">next</a><span>|</span><label class="collapse" for="c-41812690">[-]</label><label class="expand" for="c-41812690">[1 more]</label></div><br/><div class="children"><div class="content">That is not at all what Tao said.<p><a href="https:&#x2F;&#x2F;mathstodon.xyz&#x2F;@tao&#x2F;113132502735585408" rel="nofollow">https:&#x2F;&#x2F;mathstodon.xyz&#x2F;@tao&#x2F;113132502735585408</a><p>&quot;Here the results were better than previous models, but still slightly disappointing: the new model could work its way to a correct (and well-written) solution <i>if</i> provided a lot of hints and prodding, but did not generate the key conceptual ideas on its own, and did make some non-trivial mistakes.  The experience seemed roughly on par with trying to advise a mediocre, but not completely incompetent, (static simulation of a) graduate student. &quot;</div><br/></div></div></div></div></div></div><div id="41814265" class="c"><input type="checkbox" id="c-41814265" checked=""/><div class="controls bullet"><span class="by">andrepd</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810512">parent</a><span>|</span><a href="#41810705">prev</a><span>|</span><a href="#41810807">next</a><span>|</span><label class="collapse" for="c-41814265">[-]</label><label class="expand" for="c-41814265">[1 more]</label></div><br/><div class="children"><div class="content">My brother in christ, how is<p><pre><code>    A&#x2F;B + C*D - E = F, solve for B
</code></pre>
an original problem? How many  tens of thousands of examples of this exact form do you think it came across?<p>It&#x27;s the same as with coding by the way: it can reshuffle things it has already seen while changing variable names and so on. Ask it something which is not in stackoverflow or geeks4geeks and it goes tits up.<p>PS: Tested it on GPT 3.5: same answer.</div><br/></div></div></div></div><div id="41810807" class="c"><input type="checkbox" id="c-41810807" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810076">parent</a><span>|</span><a href="#41810512">prev</a><span>|</span><a href="#41810360">next</a><span>|</span><label class="collapse" for="c-41810807">[-]</label><label class="expand" for="c-41810807">[5 more]</label></div><br/><div class="children"><div class="content">Please provide your precise definitions of “reasoning” and “original”.<p>There’s no consensus in the literature on what these mean even if you make it more specific by talking about “mathematical reasoning”, so I don’t really understand what opinions like these are based on.<p>I see a lot of no true Scottsman fallacy going around, even the paper resorts to this as it actually uses phrases like “true reasoning” several times.<p>I don’t think the paper is very convincing btw, the abstract is kind of click-baity and talks about 65% variation when that was a cherry picked example from a tiny phi model and the SOTA models showed way less variation which was arguably not that interesting.</div><br/><div id="41812447" class="c"><input type="checkbox" id="c-41812447" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810807">parent</a><span>|</span><a href="#41810360">next</a><span>|</span><label class="collapse" for="c-41812447">[-]</label><label class="expand" for="c-41812447">[4 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; There’s no consensus in the literature on what these mean even if you make it more specific by talking about “mathematical reasoning”, so I don’t really understand what opinions like these are based on.<p>What literature is that? You can find plenty of very clear consensus on what reasoning is if you read e.g. the literature on automated reasoning. A brief taste:<p><i>Automated Reasoning</i><p><i>Reasoning is the ability to make inferences, and automated reasoning is concerned with the building of computing systems that automate this process. Although the overall goal is to mechanize different forms of reasoning, the term has largely been identified with valid deductive reasoning as practiced in mathematics and formal logic. In this respect, automated reasoning is akin to mechanical theorem proving. Building an automated reasoning program means providing an algorithmic description to a formal calculus so that it can be implemented on a computer to prove theorems of the calculus in an efficient manner. Important aspects of this exercise involve defining the class of problems the program will be required to solve, deciding what language will be used by the program to represent the information given to it as well as new information inferred by the program, specifying the mechanism that the program will use to conduct deductive inferences, and figuring out how to perform all these computations efficiently. While basic research work continues in order to provide the necessary theoretical framework, the field has reached a point where automated reasoning programs are being used by researchers to attack open questions in mathematics and logic, provide important applications in computing science, solve problems in engineering, and find novel approaches to questions in exact philosophy.</i><p><a href="https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;reasoning-automated&#x2F;" rel="nofollow">https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;reasoning-automated&#x2F;</a><p>After that you may want to look at the SEP articles on Analogical reasoning and Defeasible Reasoning:<p><a href="https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;reasoning-analogy&#x2F;" rel="nofollow">https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;reasoning-analogy&#x2F;</a><p><a href="https:&#x2F;&#x2F;seop.illc.uva.nl&#x2F;entries&#x2F;reasoning-defeasible&#x2F;" rel="nofollow">https:&#x2F;&#x2F;seop.illc.uva.nl&#x2F;entries&#x2F;reasoning-defeasible&#x2F;</a></div><br/><div id="41815303" class="c"><input type="checkbox" id="c-41815303" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41812447">parent</a><span>|</span><a href="#41812764">next</a><span>|</span><label class="collapse" for="c-41815303">[-]</label><label class="expand" for="c-41815303">[1 more]</label></div><br/><div class="children"><div class="content">I was referring to the literature of machine learning and language models specifically, since that’s what the paper is about.<p>What’s your referencing seems to be more related to  symbolic ai &#x2F; formal logic, and I get that these are related, but it just doesn’t really map neatly onto LLM‘s.</div><br/></div></div><div id="41812764" class="c"><input type="checkbox" id="c-41812764" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41812447">parent</a><span>|</span><a href="#41815303">prev</a><span>|</span><a href="#41810360">next</a><span>|</span><label class="collapse" for="c-41812764">[-]</label><label class="expand" for="c-41812764">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an obsolete definition that definea reasoning as a simplistic mechanical   task explicitly encoded by humans. 
What LLM is attempting is far beyond that. It&#x27;s a automated  process for creating its own reasoning method.</div><br/><div id="41814588" class="c"><input type="checkbox" id="c-41814588" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41812764">parent</a><span>|</span><a href="#41810360">next</a><span>|</span><label class="collapse" for="c-41814588">[-]</label><label class="expand" for="c-41814588">[1 more]</label></div><br/><div class="children"><div class="content">And this is according to whom, please?</div><br/></div></div></div></div></div></div></div></div><div id="41810360" class="c"><input type="checkbox" id="c-41810360" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810076">parent</a><span>|</span><a href="#41810807">prev</a><span>|</span><a href="#41809754">next</a><span>|</span><label class="collapse" for="c-41810360">[-]</label><label class="expand" for="c-41810360">[1 more]</label></div><br/><div class="children"><div class="content">Do you have some categories of such original problems? It seems markedly better at reasoning&#x2F;logic puzzles, and programmatically-solvable problems are often offloaded to the Python interpreter.</div><br/></div></div></div></div></div></div><div id="41809754" class="c"><input type="checkbox" id="c-41809754" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#41809557">parent</a><span>|</span><a href="#41809649">prev</a><span>|</span><a href="#41809655">next</a><span>|</span><label class="collapse" for="c-41809754">[-]</label><label class="expand" for="c-41809754">[4 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t a slight change in tokenization? (say mapping single digits to single tokens) help with this specific challenge?</div><br/><div id="41810263" class="c"><input type="checkbox" id="c-41810263" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809754">parent</a><span>|</span><a href="#41810165">next</a><span>|</span><label class="collapse" for="c-41810263">[-]</label><label class="expand" for="c-41810263">[2 more]</label></div><br/><div class="children"><div class="content">Aren’t coding copilots based on tokenizing programming language keywords and syntax? That seems to me to be domain specific tokenization (a very well defined one too — since programming languages are meant to be tokenizable).<p>Math is a bit trickier since most of the world’s math is in LaTeX, which is more of a formatting language than a syntax tree. There needs to be a conversion to MathML or something more symbolic.<p>Even English word tokenization has gaps today. Claude Sonnet 3.5  still fails on the question “how many r’s are there in strawberry”.</div><br/><div id="41810629" class="c"><input type="checkbox" id="c-41810629" checked=""/><div class="controls bullet"><span class="by">gwillen</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41810263">parent</a><span>|</span><a href="#41810165">next</a><span>|</span><label class="collapse" for="c-41810629">[-]</label><label class="expand" for="c-41810629">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Aren’t coding copilots based on tokenizing programming language keywords and syntax?<p>No, they use the same tokenization as everyone else. There was one major change from early to modern LLM tokenization, made (as far as I can tell) for efficient tokenization of code: early tokenizers always made a space its own token (unless attached to an adjacent word.) Modern tokenizers can group many spaces together.</div><br/></div></div></div></div><div id="41810165" class="c"><input type="checkbox" id="c-41810165" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809754">parent</a><span>|</span><a href="#41810263">prev</a><span>|</span><a href="#41809655">next</a><span>|</span><label class="collapse" for="c-41810165">[-]</label><label class="expand" for="c-41810165">[1 more]</label></div><br/><div class="children"><div class="content">Context-specific tokenization sounds a lot like old fashioned programming.</div><br/></div></div></div></div><div id="41809655" class="c"><input type="checkbox" id="c-41809655" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41809557">parent</a><span>|</span><a href="#41809754">prev</a><span>|</span><a href="#41809200">next</a><span>|</span><label class="collapse" for="c-41809655">[-]</label><label class="expand" for="c-41809655">[5 more]</label></div><br/><div class="children"><div class="content">The llm will know 123 and 45 is a contiguious number just like how humans can tell if you say 123 and then a slight pause 45 as a single number</div><br/><div id="41809793" class="c"><input type="checkbox" id="c-41809793" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809655">parent</a><span>|</span><a href="#41809737">next</a><span>|</span><label class="collapse" for="c-41809793">[-]</label><label class="expand" for="c-41809793">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just so dissonant to me that the tokens in mathematics are the digits, and not bundles of digits. The idea of tokenization makes sense for taking the power off letters, it provides language agnosticism.<p>But for maths, it doesn&#x27;t seem appropriate.<p>I wonder what the effect of forcing tokenization for each separate digit be.</div><br/><div id="41814528" class="c"><input type="checkbox" id="c-41814528" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809793">parent</a><span>|</span><a href="#41809737">next</a><span>|</span><label class="collapse" for="c-41814528">[-]</label><label class="expand" for="c-41814528">[1 more]</label></div><br/><div class="children"><div class="content">This reminds me of the riddle of someone buying the numerals to put their address on their house.  When you are looking at text, the point is all you have are the characters&#x2F;symbols&#x2F;tokens&#x2F;whatever you want to call them.  You can&#x27;t really shepherd some over to their numeric value while leaving some at their token value.  Unless you want to cause other issues when it comes time to reason about them later.<p>I&#x27;d hazard that the majority of numbers in most text are not such that they should be converted to a number, per se.  Consider addresses, postal codes, phone numbers, ... ok, I may have run out of things to consider. :D</div><br/></div></div></div></div><div id="41809737" class="c"><input type="checkbox" id="c-41809737" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809655">parent</a><span>|</span><a href="#41809793">prev</a><span>|</span><a href="#41809871">next</a><span>|</span><label class="collapse" for="c-41809737">[-]</label><label class="expand" for="c-41809737">[1 more]</label></div><br/><div class="children"><div class="content">I think that as long as the attention mechanism has been trained on each possible numerical token enough, this is true. But if a particular token is underrepresented, it could potentially cause inaccuracies.</div><br/></div></div><div id="41809871" class="c"><input type="checkbox" id="c-41809871" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#41809557">root</a><span>|</span><a href="#41809655">parent</a><span>|</span><a href="#41809737">prev</a><span>|</span><a href="#41809200">next</a><span>|</span><label class="collapse" for="c-41809871">[-]</label><label class="expand" for="c-41809871">[1 more]</label></div><br/><div class="children"><div class="content">It won&#x27;t &#x27;see&#x27; [123, 45] though, but [7633, 2548], or rather sparse vectors that are zero at each but the 7634th and 2549th position.</div><br/></div></div></div></div></div></div><div id="41809200" class="c"><input type="checkbox" id="c-41809200" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809557">prev</a><span>|</span><a href="#41817159">next</a><span>|</span><label class="collapse" for="c-41809200">[-]</label><label class="expand" for="c-41809200">[25 more]</label></div><br/><div class="children"><div class="content">These results are very similar to the &quot;Alice in Wonderland&quot; problem [1, 2], which was already discussed a few months ago. However the authors of the other paper are much more critical and call it a &quot;Complete Reasoning Breakdown&quot;.<p>You could argue that the issue lies in the models being in an intermediate state between pattern matching and reasoning.<p>To me, such results indicate that you can&#x27;t trust any LLM benchmark results related to math and reasoning when you see, that changing the characters, numbers or the sentence structure in a problem alter the outcome by more than 20 percentage points.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2406.02061v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2406.02061v1</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40811329">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40811329</a></div><br/><div id="41809441" class="c"><input type="checkbox" id="c-41809441" checked=""/><div class="controls bullet"><span class="by">oliwary</span><span>|</span><a href="#41809200">parent</a><span>|</span><a href="#41809257">next</a><span>|</span><label class="collapse" for="c-41809441">[-]</label><label class="expand" for="c-41809441">[10 more]</label></div><br/><div class="children"><div class="content">Someone (<a href="https:&#x2F;&#x2F;x.com&#x2F;colin_fraser&#x2F;status&#x2F;1834336440819614036" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;colin_fraser&#x2F;status&#x2F;1834336440819614036</a>) shared an example that I thought was interesting relating to their reasoning capabilities:<p><i>A man gets taken into a hospital. When the doctor sees him, he exclaims &quot;I cannot operate on this person, he is my own son!&quot;. How is this possible?</i><p>All LLMs I have tried this on, including GPT o1-preview, get this wrong, assuming that this the riddle relates to a gendered assumption about the doctor being a man, while it is in fact a woman. However, in this case, there is no paradox - it is made clear that the doctor is a man (&quot;he exclaims&quot;), meaning they must be the father of the person being brought in. The fact that the LLMs got this wrong suggests that it finds a similar reasoning pattern and then applies it. Even after additional prodding, a model continued making the mistake, arguing at one point that it could be a same-sex relationship.<p>Amusingly, when someone on HN mentioned this example in the O1 thread, many of the HN commentators also misunderstood the problem - perhaps humans also mostly reason using previous examples rather than thinking from scratch.</div><br/><div id="41809682" class="c"><input type="checkbox" id="c-41809682" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809441">parent</a><span>|</span><a href="#41809537">next</a><span>|</span><label class="collapse" for="c-41809682">[-]</label><label class="expand" for="c-41809682">[6 more]</label></div><br/><div class="children"><div class="content">&gt; perhaps humans also mostly reason using previous examples rather than thinking from scratch.<p>Although we would like AI to be better here, the worse problem is that, unlike humans, you can’t get the LLM to understand its mistake <i>and</i> then move forward with that newfound understanding. While the LLM tries to respond appropriately and indulge you when you indicate the mistake, further dialog usually exhibits noncommittal behavior by the LLM, and the mistaken interpretation tends to sneak back in. You generally don’t get the feeling of “now it gets it”, and instead it tends to feels more like someone with no real understanding (but very good memory of relevant material) trying to bullshit-technobabble around the issue.</div><br/><div id="41809952" class="c"><input type="checkbox" id="c-41809952" checked=""/><div class="controls bullet"><span class="by">oliwary</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809682">parent</a><span>|</span><a href="#41809537">next</a><span>|</span><label class="collapse" for="c-41809952">[-]</label><label class="expand" for="c-41809952">[5 more]</label></div><br/><div class="children"><div class="content">That is an excellent point! I feel like people have two modes of reasoning - a lazy mode where we assume we already know the problem, and an active mode where something prompts us to actually pay attention and actually reason about the problem. Perhaps LLMs only have the lazy mode?</div><br/><div id="41810494" class="c"><input type="checkbox" id="c-41810494" checked=""/><div class="controls bullet"><span class="by">letmevoteplease</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809952">parent</a><span>|</span><a href="#41810900">next</a><span>|</span><label class="collapse" for="c-41810494">[-]</label><label class="expand" for="c-41810494">[3 more]</label></div><br/><div class="children"><div class="content">I prompted o1 with &quot;analyze this problem word-by-word to ensure that you fully understand it. Make no assumptions.&quot; and it solved the &quot;riddle&quot; correctly.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6709473b-b22c-8012-a30d-42c8482cc625" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;6709473b-b22c-8012-a30d-42c8482cc6...</a></div><br/><div id="41810931" class="c"><input type="checkbox" id="c-41810931" checked=""/><div class="controls bullet"><span class="by">hoosieree</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41810494">parent</a><span>|</span><a href="#41810900">next</a><span>|</span><label class="collapse" for="c-41810931">[-]</label><label class="expand" for="c-41810931">[2 more]</label></div><br/><div class="children"><div class="content">My classifier is not very accurate:<p><pre><code>    is_trick(question)  # 50% accurate
</code></pre>
To make the client happy, I improved it:<p><pre><code>    is_trick(question, label)  # 100% accurate
</code></pre>
But the client still isn&#x27;t happy because if they already knew the label they wouldn&#x27;t need the classifier!<p>...<p>If ChatGPT had &quot;sense&quot; your extra prompt should do nothing. The fact that adding the prompt changes the output should be a clue that nobody should ever trust an LLM anywhere correctness matters.<p>[edit]<p>I also tried the original question but followed-up with &quot;is it possible that the doctor is the boy&#x27;s father?&quot;<p>ChatGPT said:<p>Yes, it&#x27;s possible for the doctor to be the boy&#x27;s father if there&#x27;s a scenario where the boy has two fathers, such as being raised by a same-sex couple or having a biological father and a stepfather. The riddle primarily highlights the assumption about gender roles, but there are certainly other family dynamics that could make the statement true.</div><br/><div id="41814607" class="c"><input type="checkbox" id="c-41814607" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41810931">parent</a><span>|</span><a href="#41810900">next</a><span>|</span><label class="collapse" for="c-41814607">[-]</label><label class="expand" for="c-41814607">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not like GP gave task-specific advice in their example. They just said &quot;think carefully about this&quot;.<p>If it&#x27;s all it takes, then maybe the problem isn&#x27;t a lack of capabilities but a tendency to not surface them.</div><br/></div></div></div></div></div></div><div id="41810900" class="c"><input type="checkbox" id="c-41810900" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809952">parent</a><span>|</span><a href="#41810494">prev</a><span>|</span><a href="#41809537">next</a><span>|</span><label class="collapse" for="c-41810900">[-]</label><label class="expand" for="c-41810900">[1 more]</label></div><br/><div class="children"><div class="content">I have found multiple definitions in literature of what you describe.<p>1. Fast thinking vs. slow thinking.<p>2. Intuitive thinking vs. symbolic thinking.<p>3. Interpolated thinking (in terms of pattern matching or curve fitting) vs. generalization.<p>4. Level 1 thinking vs. level 2 thinking. (In terms of OpenAIs definitions of levels of intelligence)<p>The definitions describe all the same thing.<p>Currently all of the LLMs are trained to use the &quot;lazy&quot; thinking approach. o1-preview is advertised as being the exception. It is trained or fine tuned with a countless number of reasoning patterns.</div><br/></div></div></div></div></div></div><div id="41809537" class="c"><input type="checkbox" id="c-41809537" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809441">parent</a><span>|</span><a href="#41809682">prev</a><span>|</span><a href="#41809544">next</a><span>|</span><label class="collapse" for="c-41809537">[-]</label><label class="expand" for="c-41809537">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure we fall back on easy&#x2F;fast associations and memories to answer. It&#x27;s the way of least resistance. The text you quote bears more than a superficial similarity to the old riddle (there&#x27;s really nothing else that looks like it), but that version also stipulates that the father has died. That adds &quot;gendered&quot; (what an ugly word) information to the question, a fact which is missed when recalling this particular answer. Basically, LLMs are stochastic parrots.</div><br/><div id="41810177" class="c"><input type="checkbox" id="c-41810177" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809537">parent</a><span>|</span><a href="#41809544">next</a><span>|</span><label class="collapse" for="c-41810177">[-]</label><label class="expand" for="c-41810177">[1 more]</label></div><br/><div class="children"><div class="content">How people don’t see the irony of commenting “stochastic parrots” every time LLM reasoning failure comes up is beyond me.<p>There are ways to trick LLMs. There are also ways to trick people. If asking a tricky question and getting a wrong answer is enough to disprove reasoning, humans aren’t capable of reasoning, either.</div><br/></div></div></div></div><div id="41809544" class="c"><input type="checkbox" id="c-41809544" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809441">parent</a><span>|</span><a href="#41809537">prev</a><span>|</span><a href="#41809257">next</a><span>|</span><label class="collapse" for="c-41809544">[-]</label><label class="expand" for="c-41809544">[1 more]</label></div><br/><div class="children"><div class="content">&gt; perhaps humans also mostly reason using previous examples rather than thinking from scratch.<p>We do, but we can generalize better. When you exchange &quot;hospital&quot; with &quot;medical centre&quot; or change the sentence structure and ask humans, the statistics would not be that different.<p>But for LLMs, that might make a lot of difference.</div><br/></div></div></div></div><div id="41809257" class="c"><input type="checkbox" id="c-41809257" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#41809200">parent</a><span>|</span><a href="#41809441">prev</a><span>|</span><a href="#41817159">next</a><span>|</span><label class="collapse" for="c-41809257">[-]</label><label class="expand" for="c-41809257">[14 more]</label></div><br/><div class="children"><div class="content">Both Claude-3.5 and o1-preview nail this problem<p>&quot;Let&#x27;s think through this step-by-step:<p>1. Alice has 3 brothers
2. Alice has 2 sisters
3. We need to find out how many sisters Alice&#x27;s brother has<p>The key here is to realize that Alice&#x27;s brothers would have the same sisters as Alice, except they would also count Alice as their sister.<p>So, Alice&#x27;s brothers would have:
- The 2 sisters Alice has
- Plus Alice herself as a sister<p>Therefore, Alice&#x27;s brothers have 3 sisters in total.&quot;</div><br/><div id="41809329" class="c"><input type="checkbox" id="c-41809329" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809257">parent</a><span>|</span><a href="#41809573">next</a><span>|</span><label class="collapse" for="c-41809329">[-]</label><label class="expand" for="c-41809329">[7 more]</label></div><br/><div class="children"><div class="content">And here lies the exact issue. Single tests don’t provide any meaningful insights. You need to perform this test at least twenty times in separate chat windows or via the API to obtain meaningful statistics.<p>For the &quot;Alice in Wonderland&quot; paper, neither Claude-3.5 nor o1-preview was available at that time.<p>But I have tested them as well a few weeks ago with the issue translated into German, achieving also a 100% success rate with both models.<p>However, when I add irrelevant information (My mother ...), Claude&#x27;s success rate drops to 85%:<p>&quot;My mother has a sister called Alice. Alice has 2 sisters and 1 brother. How many sisters does Alice&#x27;s brother have?&quot;</div><br/><div id="41809730" class="c"><input type="checkbox" id="c-41809730" checked=""/><div class="controls bullet"><span class="by">probably_wrong</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809329">parent</a><span>|</span><a href="#41809614">next</a><span>|</span><label class="collapse" for="c-41809730">[-]</label><label class="expand" for="c-41809730">[4 more]</label></div><br/><div class="children"><div class="content">Your experience makes me think that the reason the models got a better success rate is not because they are better at reasoning, but rather because the problem made it to their training dataset.</div><br/><div id="41810124" class="c"><input type="checkbox" id="c-41810124" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809730">parent</a><span>|</span><a href="#41810122">next</a><span>|</span><label class="collapse" for="c-41810124">[-]</label><label class="expand" for="c-41810124">[2 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know. The paper and the problem was very prominent at that time. Some developers at Anthropic or OpenAI might have included that in some way. Either as test or as a task to improve the CoT via Reinforcement Learning.</div><br/><div id="41815938" class="c"><input type="checkbox" id="c-41815938" checked=""/><div class="controls bullet"><span class="by">meroes</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41810124">parent</a><span>|</span><a href="#41810122">next</a><span>|</span><label class="collapse" for="c-41815938">[-]</label><label class="expand" for="c-41815938">[1 more]</label></div><br/><div class="children"><div class="content">It made it into their data set via RLHF almost assuredly. Wild these papers are getting published when RLHF&#x27;ers and up see this stuff in the wild daily and ahead of the papers.<p>Timeline is roughly:<p>Model developer notices a sometimes highly specific weak area -&gt; ... -&gt; RLHF&#x27;ers are asked to develop a bunch of very specific problems improving the weak area -&gt; a few months go by -&gt; A paper gets published that squeezes water out of stone to make AI headlines.<p>These researchers should just become RLHF&#x27;ers because their efforts aren&#x27;t uncovering anything unknown and it&#x27;s just being dressed up with a little statistics. And by the time the research is out, the the fixes are already identified internally, worked on, and nearing pushes.<p>I just realized AI research will be part of the AI bubble <i>if</i> it bursts. I don&#x27;t think there was a .com research sub-bubble, so this might be novel.</div><br/></div></div></div></div><div id="41810122" class="c"><input type="checkbox" id="c-41810122" checked=""/><div class="controls bullet"><span class="by">andrepd</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809730">parent</a><span>|</span><a href="#41810124">prev</a><span>|</span><a href="#41809614">next</a><span>|</span><label class="collapse" for="c-41810122">[-]</label><label class="expand" for="c-41810122">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely! It&#x27;s the elephant in the room with these ducking &quot;we&#x27;ve solved 80% of maths olympiad problems&quot; claims!</div><br/></div></div></div></div><div id="41809614" class="c"><input type="checkbox" id="c-41809614" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809329">parent</a><span>|</span><a href="#41809730">prev</a><span>|</span><a href="#41811548">next</a><span>|</span><label class="collapse" for="c-41809614">[-]</label><label class="expand" for="c-41809614">[1 more]</label></div><br/><div class="children"><div class="content">We do have chatbot arena which to a degree already does this.<p>I like to use:<p>&quot;Kim&#x27;s mother is Linda. Linda&#x27;s son is Rachel. John is Kim&#x27;s daughter. Who is Kim&#x27;s son?&quot;<p>Interestingly I just got a model called &quot;engine test&quot; that nailed this one in a three sentence response, whereas o1-preview got it wrong (but has gotten it right in the past).</div><br/></div></div><div id="41811548" class="c"><input type="checkbox" id="c-41811548" checked=""/><div class="controls bullet"><span class="by">andoando</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809329">parent</a><span>|</span><a href="#41809614">prev</a><span>|</span><a href="#41809573">next</a><span>|</span><label class="collapse" for="c-41811548">[-]</label><label class="expand" for="c-41811548">[1 more]</label></div><br/><div class="children"><div class="content">You also need a problem that hasn&#x27;t been copy pasted a million times on the internet.</div><br/></div></div></div></div><div id="41809573" class="c"><input type="checkbox" id="c-41809573" checked=""/><div class="controls bullet"><span class="by">einarfd</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809257">parent</a><span>|</span><a href="#41809329">prev</a><span>|</span><a href="#41817159">next</a><span>|</span><label class="collapse" for="c-41809573">[-]</label><label class="expand" for="c-41809573">[6 more]</label></div><br/><div class="children"><div class="content">My problem with this puzzle, is how do you know that Alice and her brothers share both parents?<p>Is it not correct English to call two people who share one parent, sisters, or brothers?<p>I guess I could be misguided by my native Norwegian where you have to preamble the word with &quot;hell&quot; (full), or &quot;halv&quot; (half), if you want to specify the number of shared parents.</div><br/><div id="41809642" class="c"><input type="checkbox" id="c-41809642" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809573">parent</a><span>|</span><a href="#41810198">next</a><span>|</span><label class="collapse" for="c-41809642">[-]</label><label class="expand" for="c-41809642">[4 more]</label></div><br/><div class="children"><div class="content">It is pretty much the same in English. Unqualified would usually mean sharing both parents but could include half- or step-siblings.</div><br/><div id="41810151" class="c"><input type="checkbox" id="c-41810151" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809642">parent</a><span>|</span><a href="#41810198">next</a><span>|</span><label class="collapse" for="c-41810151">[-]</label><label class="expand" for="c-41810151">[3 more]</label></div><br/><div class="children"><div class="content">I am not a native English speaker. Can you reformulate the problem for me, so that every alternative interpretation is excluded?</div><br/><div id="41810203" class="c"><input type="checkbox" id="c-41810203" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41810151">parent</a><span>|</span><a href="#41810198">next</a><span>|</span><label class="collapse" for="c-41810203">[-]</label><label class="expand" for="c-41810203">[2 more]</label></div><br/><div class="children"><div class="content">Alice has N full sisters. She also has M full brothers. How many full sisters does Alice’s brother have?</div><br/><div id="41811066" class="c"><input type="checkbox" id="c-41811066" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41810203">parent</a><span>|</span><a href="#41810198">next</a><span>|</span><label class="collapse" for="c-41811066">[-]</label><label class="expand" for="c-41811066">[1 more]</label></div><br/><div class="children"><div class="content">Tried it with N=2 and M=1 (brother singular) with the gpt-4o model and CoT.<p>1. 50% success without &quot;full&quot; terminology.<p>2. 5% success with &quot;full&quot; terminology.<p>So, the improvement in clarity has exactly the opposite effect.</div><br/></div></div></div></div></div></div></div></div><div id="41810198" class="c"><input type="checkbox" id="c-41810198" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#41809200">root</a><span>|</span><a href="#41809573">parent</a><span>|</span><a href="#41809642">prev</a><span>|</span><a href="#41817159">next</a><span>|</span><label class="collapse" for="c-41810198">[-]</label><label class="expand" for="c-41810198">[1 more]</label></div><br/><div class="children"><div class="content">They would usually be called “half-sisters”. You could call them “sisters” colloquially though but given it’s presented as a logic question I think it’s fine to disregard</div><br/></div></div></div></div></div></div></div></div><div id="41817159" class="c"><input type="checkbox" id="c-41817159" checked=""/><div class="controls bullet"><span class="by">i007</span><span>|</span><a href="#41809200">prev</a><span>|</span><a href="#41809130">next</a><span>|</span><label class="collapse" for="c-41817159">[-]</label><label class="expand" for="c-41817159">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are designed to carry out &quot;associative reasoning&quot; which captures logic based on recognition and recall of compositional patterns learned during training.<p>Having said that, we can still get semantically and logically idempotent output that makes sense but with lots of work outside of the LLM, which contrasts with the current hyper focus on the LLM itself as the be all and end all. It is just one component in what ought to be a larger and more involved system for reasoning.<p>Look at what we were able to accomplish here for Legal AI, not so mathematical logic per se but mimicking (capturing) axiomatic logic in the legal domain:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_9Galw9-Z3Q" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_9Galw9-Z3Q</a><p>marc at sunami dot ai</div><br/></div></div><div id="41809130" class="c"><input type="checkbox" id="c-41809130" checked=""/><div class="controls bullet"><span class="by">thenoblesunfish</span><span>|</span><a href="#41817159">prev</a><span>|</span><a href="#41815104">next</a><span>|</span><label class="collapse" for="c-41809130">[-]</label><label class="expand" for="c-41809130">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting, and aligns with what I would expect in terms of the type of &quot;thinking&quot; LLMs do. I think that it&#x27;s also the type of &quot;thinking&quot; that will let a student pass most school courses, except of course for the ones where the teacher has taken the time to pose test questions that aren&#x27;t as amenable to pattern matching. (Hard, but I assume most readers here are familiar with leetcode style interviews and what makes questions of that kind higher or lower quality for assessing candidates)<p>(And yes, I know people are hard at work adding other types of thinking to work along with the pure language models)</div><br/></div></div><div id="41815104" class="c"><input type="checkbox" id="c-41815104" checked=""/><div class="controls bullet"><span class="by">codelion</span><span>|</span><a href="#41809130">prev</a><span>|</span><a href="#41811389">next</a><span>|</span><label class="collapse" for="c-41815104">[-]</label><label class="expand" for="c-41815104">[1 more]</label></div><br/><div class="children"><div class="content">This is surprising to only those that have not worked in formal reasoning. Yes, LLMs cannot do true logical reasoning in a formal sense, you can do better with an SMT solver. But it is also true that you can solve a lot of logical problems by just applying “reasoning steps” from the training data, specially when your training data is the entirety of written content ever produced. Both of these can be true at the same time it is not a contradiction just an interesting dichotomy.</div><br/></div></div><div id="41811389" class="c"><input type="checkbox" id="c-41811389" checked=""/><div class="controls bullet"><span class="by">trehalose</span><span>|</span><a href="#41815104">prev</a><span>|</span><a href="#41809244">next</a><span>|</span><label class="collapse" for="c-41811389">[-]</label><label class="expand" for="c-41811389">[1 more]</label></div><br/><div class="children"><div class="content">I see a lot of discussion about irrelevant clauses tripping up the LLMs and why that does or doesn&#x27;t matter. To me, what&#x27;s far more damning is this:<p>&gt; Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark.<p>This seems like irrefutable evidence of overfitting, that in the best case scenario is epidemic among current LLMs (and in the worst case interpretation, is covering up fundamental inabilities to learn mathematical reasoning from the training data).</div><br/></div></div><div id="41809244" class="c"><input type="checkbox" id="c-41809244" checked=""/><div class="controls bullet"><span class="by">yk</span><span>|</span><a href="#41811389">prev</a><span>|</span><a href="#41809330">next</a><span>|</span><label class="collapse" for="c-41809244">[-]</label><label class="expand" for="c-41809244">[11 more]</label></div><br/><div class="children"><div class="content">I test llms actually similar. For example there is a well known logic puzzle were a farmer tries to cross a river with a cabbage a goat and a wolf. Llms can solve that since at least GPT-2, however if we replace the wolf with a cow, gpt-o does correctly infer the rules of the puzzle but can&#x27;t solve it.</div><br/><div id="41809444" class="c"><input type="checkbox" id="c-41809444" checked=""/><div class="controls bullet"><span class="by">getoffmyyawn</span><span>|</span><a href="#41809244">parent</a><span>|</span><a href="#41809452">next</a><span>|</span><label class="collapse" for="c-41809444">[-]</label><label class="expand" for="c-41809444">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found that the River Crossing puzzle is a great way to show how LLMs break down.<p>For example, I tested Gemini with several versions of the puzzle that are easy to solve because they don&#x27;t have the restrictions such as the farmer&#x27;s boat only being able to carry one passenger&#x2F;item at a time.<p>Ask this version, &quot;A farmer has a spouse, chicken, cabbage, and baby with them. The farmer needs to get them all across the river in their boat. What is the best way to do it?&quot;<p>In my tests the LLMs nearly always assume that the boat has a carry-restriction and they come up with wild solutions involving multiple trips.</div><br/></div></div><div id="41809452" class="c"><input type="checkbox" id="c-41809452" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#41809244">parent</a><span>|</span><a href="#41809444">prev</a><span>|</span><a href="#41809874">next</a><span>|</span><label class="collapse" for="c-41809452">[-]</label><label class="expand" for="c-41809452">[4 more]</label></div><br/><div class="children"><div class="content">What happens if you sit down and invent a logic game that is brand new and has never been documented before anywhere then ask an LLM to solve it? That, to a layman like me, seems like a good way to measure reasoning in AI.</div><br/><div id="41809740" class="c"><input type="checkbox" id="c-41809740" checked=""/><div class="controls bullet"><span class="by">jprete</span><span>|</span><a href="#41809244">root</a><span>|</span><a href="#41809452">parent</a><span>|</span><a href="#41809760">next</a><span>|</span><label class="collapse" for="c-41809740">[-]</label><label class="expand" for="c-41809740">[1 more]</label></div><br/><div class="children"><div class="content">I think the problem is inventing new structures for logic games. The shape of the problem ideally would be different than any existing puzzle, and that&#x27;s hard. If a person can look at it and say &quot;oh, that&#x27;s just the sheep-wolf-cabbage&#x2F;liar-and-truthteller&#x2F;etc. problem with extra features&quot; then it&#x27;s not an ideal test because it can be pattern-matched.</div><br/></div></div><div id="41809760" class="c"><input type="checkbox" id="c-41809760" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41809244">root</a><span>|</span><a href="#41809452">parent</a><span>|</span><a href="#41809740">prev</a><span>|</span><a href="#41810887">next</a><span>|</span><label class="collapse" for="c-41809760">[-]</label><label class="expand" for="c-41809760">[1 more]</label></div><br/><div class="children"><div class="content">This is being done, but the difficulties are: (1) How do you assess that it is really brand-new and not just a slight variation of an existing one? (2) Once you publish it, it stops being brand-new, so its lifetime is limited and you can’t build a longer-term reproducible test out of it.</div><br/></div></div><div id="41810887" class="c"><input type="checkbox" id="c-41810887" checked=""/><div class="controls bullet"><span class="by">Analemma_</span><span>|</span><a href="#41809244">root</a><span>|</span><a href="#41809452">parent</a><span>|</span><a href="#41809760">prev</a><span>|</span><a href="#41809874">next</a><span>|</span><label class="collapse" for="c-41810887">[-]</label><label class="expand" for="c-41810887">[1 more]</label></div><br/><div class="children"><div class="content">You can do this, but at that point what are you really benchmarking? If you invent a de novo logic puzzle and give it to 100 people on the street, most of them won&#x27;t be able to solve it either. If your aim is to prove &quot;LLMs can&#x27;t <i>really</i> think like humans can!&quot;, this won&#x27;t accomplish that.</div><br/></div></div></div></div><div id="41809874" class="c"><input type="checkbox" id="c-41809874" checked=""/><div class="controls bullet"><span class="by">SonOfLilit</span><span>|</span><a href="#41809244">parent</a><span>|</span><a href="#41809452">prev</a><span>|</span><a href="#41809435">next</a><span>|</span><label class="collapse" for="c-41809874">[-]</label><label class="expand" for="c-41809874">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using this as my first question to any new LLM I try and I&#x27;m quite sure nothing before GPT-4 even got close to a correct solution. Can you post a prompt that GPT-2 or 3 can solve?</div><br/></div></div><div id="41809435" class="c"><input type="checkbox" id="c-41809435" checked=""/><div class="controls bullet"><span class="by">voidUpdate</span><span>|</span><a href="#41809244">parent</a><span>|</span><a href="#41809874">prev</a><span>|</span><a href="#41810133">next</a><span>|</span><label class="collapse" for="c-41809435">[-]</label><label class="expand" for="c-41809435">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m scared of the cows around you if they eat goats</div><br/><div id="41814026" class="c"><input type="checkbox" id="c-41814026" checked=""/><div class="controls bullet"><span class="by">Manabu-eo</span><span>|</span><a href="#41809244">root</a><span>|</span><a href="#41809435">parent</a><span>|</span><a href="#41810133">next</a><span>|</span><label class="collapse" for="c-41814026">[-]</label><label class="expand" for="c-41814026">[1 more]</label></div><br/><div class="children"><div class="content">I think their point is that cows don&#x27;t eat goats, unlike wolves, and that causes the LLMs to answer it wrong.</div><br/></div></div></div></div><div id="41810133" class="c"><input type="checkbox" id="c-41810133" checked=""/><div class="controls bullet"><span class="by">andrepd</span><span>|</span><a href="#41809244">parent</a><span>|</span><a href="#41809435">prev</a><span>|</span><a href="#41809330">next</a><span>|</span><label class="collapse" for="c-41810133">[-]</label><label class="expand" for="c-41810133">[2 more]</label></div><br/><div class="children"><div class="content">Meaning it&#x27;s just a glorified Google.</div><br/><div id="41810731" class="c"><input type="checkbox" id="c-41810731" checked=""/><div class="controls bullet"><span class="by">romwell</span><span>|</span><a href="#41809244">root</a><span>|</span><a href="#41810133">parent</a><span>|</span><a href="#41809330">next</a><span>|</span><label class="collapse" for="c-41810731">[-]</label><label class="expand" for="c-41810731">[1 more]</label></div><br/><div class="children"><div class="content">...that makes up results when it can&#x27;t find any</div><br/></div></div></div></div></div></div><div id="41809330" class="c"><input type="checkbox" id="c-41809330" checked=""/><div class="controls bullet"><span class="by">criddell</span><span>|</span><a href="#41809244">prev</a><span>|</span><a href="#41816765">next</a><span>|</span><label class="collapse" for="c-41809330">[-]</label><label class="expand" for="c-41809330">[5 more]</label></div><br/><div class="children"><div class="content">It would be interesting if this kind of work could ever be extended to show the limitations of mathematical reasoning in animals and humans.<p>For example, just as a dog will never understand a fourier transform, there are likely ideas that humans cannot understand. If we know what our limits are, I wonder if we could build machines that can reason in ways we aren&#x27;t capable of?</div><br/><div id="41809686" class="c"><input type="checkbox" id="c-41809686" checked=""/><div class="controls bullet"><span class="by">myrmidon</span><span>|</span><a href="#41809330">parent</a><span>|</span><a href="#41816765">next</a><span>|</span><label class="collapse" for="c-41809686">[-]</label><label class="expand" for="c-41809686">[4 more]</label></div><br/><div class="children"><div class="content">I think it is a naive assumption that such a limitation even exists (&quot;exists&quot; in a sense that it is actually useful, by being consistent and somewhat simple to describe).<p>We investigated similar ideas for language (=&gt; Noam Chomsky), where we tried to draw clear, formalized limits for understanding (to show e.g. how human capabilities  contrast with animals). The whole approach failed completely and irredeemably (personal opinion), but researching it was far from useless to be fair.</div><br/><div id="41810035" class="c"><input type="checkbox" id="c-41810035" checked=""/><div class="controls bullet"><span class="by">r2_pilot</span><span>|</span><a href="#41809330">root</a><span>|</span><a href="#41809686">parent</a><span>|</span><a href="#41816765">next</a><span>|</span><label class="collapse" for="c-41810035">[-]</label><label class="expand" for="c-41810035">[3 more]</label></div><br/><div class="children"><div class="content">As the human brain is finitely bounded in space and time, any idea that can&#x27;t be compressed or represented by condensing notation, which is &quot;larger&quot; than the 100B cells+100T synapses can represent, or whose integration into said human&#x27;s brain would take longer than 150 years, would be considered unable to be contemplated by a normal human.</div><br/><div id="41810381" class="c"><input type="checkbox" id="c-41810381" checked=""/><div class="controls bullet"><span class="by">klabb3</span><span>|</span><a href="#41809330">root</a><span>|</span><a href="#41810035">parent</a><span>|</span><a href="#41812463">next</a><span>|</span><label class="collapse" for="c-41810381">[-]</label><label class="expand" for="c-41810381">[1 more]</label></div><br/><div class="children"><div class="content">Yes but we overcome. We can do absolutely insane things like just large prime number testing, because of reasoning + tool use.<p>Humans invent tools and wield them. Whether it&#x27;s pen &amp; paper to extend our memory, a horse to become stronger, a calculator to speed up our thinking or an airplane to literally fly, the tools we wield become extensions of our agency and control.<p>A lonely human without knowledge sharing or tools isn’t that much more capable in their lifetime than the smartest animals. When we talk about human ability colloquially, we’re generally talking about what we can do with access to our human heritage, civilization, safety and access to materials and tools.<p>Pattern matching against something others have already done is great but this is shared with at the very least all mammals to some extent. Pushing the boundaries of our species forward over time is a different game. Or at least, it seems to be…<p>It certainly seems like we’ve found the holy grail of pattern matching (system 1 thinking), which is an insane leap! But what about system 2? The million dollar question is what the hell is the topology of that pre-frontal cortex thinking machine? Is it just more pattern matching but against different patterns? Or is it completely and qualitatively different? And if so, is it more or less hard? To me, following the debate is just watching one bad prediction after another, (including my own of course). We just don&#x27;t know how it works. Not you or me, not Sam Altman in full though-leading leather jacket uniform, or even our top neuro-scientists.</div><br/></div></div><div id="41812463" class="c"><input type="checkbox" id="c-41812463" checked=""/><div class="controls bullet"><span class="by">myrmidon</span><span>|</span><a href="#41809330">root</a><span>|</span><a href="#41810035">parent</a><span>|</span><a href="#41810381">prev</a><span>|</span><a href="#41816765">next</a><span>|</span><label class="collapse" for="c-41812463">[-]</label><label class="expand" for="c-41812463">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Hardware limitations&quot; are extremely unlikely in my view to establish useful limits.<p>Consider: Do hardware limitations establish useful limits on the kind of problems a computer can solve? The answer is a resounding NO in my view, because the limits of what can be expressed&#x2F;solved grows so insanely quickly that it becomes a completely meaningless and unreachable limit even for super small computers (less capable than our brain).<p>As for learning time constraints: These are obviously reachable, but still useless in my view because they are too inconsistent- the kind of methods and insights that a human can acquire within a lifetime are completely different between persons, and highly dependent on <i>how</i> the learning happens...</div><br/></div></div></div></div></div></div></div></div><div id="41816765" class="c"><input type="checkbox" id="c-41816765" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#41809330">prev</a><span>|</span><a href="#41815947">next</a><span>|</span><label class="collapse" for="c-41816765">[-]</label><label class="expand" for="c-41816765">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an expected result.<p>Whatever happened with that result which found some representation of the state of a game inside an LLM? That indicated some degree of model-building. Haven&#x27;t heard about that again&#x2F;</div><br/></div></div><div id="41815947" class="c"><input type="checkbox" id="c-41815947" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#41816765">prev</a><span>|</span><a href="#41815527">next</a><span>|</span><label class="collapse" for="c-41815947">[-]</label><label class="expand" for="c-41815947">[1 more]</label></div><br/><div class="children"><div class="content">The very fundamental problem with LLM is there is no guarantee on any of the reasoning it gives you without a human there to give a thumbs up. They are working on solving this (alpha proof, lean agent etc) but getting this to run at inference time in an optimized way is what I would call one of the millenial prize problems of AI which will lead to a quantum leap in the path towards the singularity.</div><br/></div></div><div id="41815527" class="c"><input type="checkbox" id="c-41815527" checked=""/><div class="controls bullet"><span class="by">jgord</span><span>|</span><a href="#41815947">prev</a><span>|</span><a href="#41809663">next</a><span>|</span><label class="collapse" for="c-41815527">[-]</label><label class="expand" for="c-41815527">[1 more]</label></div><br/><div class="children"><div class="content">I propose &#x27;gords rule&#x27; : &quot;any sufficiently advanced LLM will learn the laws of logic, the principles of scientific method, and Reinforcement Learning&quot;<p>until that happens .. I think RL startups focused on real problems are much undervalued : <a href="https:&#x2F;&#x2F;quantblog.wordpress.com&#x2F;2024&#x2F;10&#x2F;11&#x2F;llm-hype-means-there-are-bargains-in-ml-deep-tech-startups-doing-real-things&#x2F;" rel="nofollow">https:&#x2F;&#x2F;quantblog.wordpress.com&#x2F;2024&#x2F;10&#x2F;11&#x2F;llm-hype-means-th...</a></div><br/></div></div><div id="41809663" class="c"><input type="checkbox" id="c-41809663" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#41815527">prev</a><span>|</span><a href="#41811704">next</a><span>|</span><label class="collapse" for="c-41809663">[-]</label><label class="expand" for="c-41809663">[4 more]</label></div><br/><div class="children"><div class="content">If the argument is that LLMs are bad at reasoning because they are easily distractible and the results vary with modifications in the question, one should be reminded of the consistency and distractability of humans.</div><br/><div id="41810237" class="c"><input type="checkbox" id="c-41810237" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#41809663">parent</a><span>|</span><a href="#41811165">next</a><span>|</span><label class="collapse" for="c-41810237">[-]</label><label class="expand" for="c-41810237">[1 more]</label></div><br/><div class="children"><div class="content">Why? LLMs are supposedly better than humans (as many comments  claim in this thread).</div><br/></div></div><div id="41811165" class="c"><input type="checkbox" id="c-41811165" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41809663">parent</a><span>|</span><a href="#41810237">prev</a><span>|</span><a href="#41811070">next</a><span>|</span><label class="collapse" for="c-41811165">[-]</label><label class="expand" for="c-41811165">[1 more]</label></div><br/><div class="children"><div class="content">Trained human can tell if distracted: &quot;I am distracted and can&#x27;t figure out answer&quot;, while LLM will confidently gives you wrong answer, which makes whole results not reliable.</div><br/></div></div></div></div><div id="41811704" class="c"><input type="checkbox" id="c-41811704" checked=""/><div class="controls bullet"><span class="by">gradientsrneat</span><span>|</span><a href="#41809663">prev</a><span>|</span><a href="#41812362">next</a><span>|</span><label class="collapse" for="c-41811704">[-]</label><label class="expand" for="c-41811704">[1 more]</label></div><br/><div class="children"><div class="content">Could this be Goodhart&#x27;s Law in action? AI tools like to showcase benchmarks in bar graphs to show how well they perform compared to other models.<p>Maybe the benchmark Qs&#x2F;As snuck into training sets accidentally. Is it still Goodhart&#x27;s Law if it&#x27;s unintentional?<p>Daniel Lemire has blogged about being impressed with how well the LLM answers his CS problem questions. I was impressed too. Not sure where the line of competence lies.</div><br/></div></div><div id="41812362" class="c"><input type="checkbox" id="c-41812362" checked=""/><div class="controls bullet"><span class="by">eigenform</span><span>|</span><a href="#41811704">prev</a><span>|</span><a href="#41814325">next</a><span>|</span><label class="collapse" for="c-41812362">[-]</label><label class="expand" for="c-41812362">[1 more]</label></div><br/><div class="children"><div class="content">The difference is that, if we are solving a math problem together, you and I [explicitly or implicitly] can come to an agreement over the context and decide to restrict our use of language with certain rules. The utility behind our conversation [generally] rests on those rules!<p>An LLM is very good at <i>recovering</i> rules, but being good at pattern recognition is not the same thing as being good at <i>unambiguously following</i> rules in the appropriate context.<p>edit: Natural language is far from an efficient&#x2F;sufficient&#x2F;necessary intermediate representation for doing math, just ask any general-purpose computer. Sometimes, it&#x27;s worth &quot;putting rules in stone,&quot; and it seems unreasonable to believe that there is always an unambiguous rule for this that you can mechanically recover from a corpus of language use.</div><br/></div></div><div id="41814325" class="c"><input type="checkbox" id="c-41814325" checked=""/><div class="controls bullet"><span class="by">ak_111</span><span>|</span><a href="#41812362">prev</a><span>|</span><a href="#41813198">next</a><span>|</span><label class="collapse" for="c-41814325">[-]</label><label class="expand" for="c-41814325">[2 more]</label></div><br/><div class="children"><div class="content">As an outsider can anyone enlighten me how this squares with the news that models that adapt similar LLM architecture can obtain silver medal in mathematical olympiad?</div><br/><div id="41814581" class="c"><input type="checkbox" id="c-41814581" checked=""/><div class="controls bullet"><span class="by">lionkor</span><span>|</span><a href="#41814325">parent</a><span>|</span><a href="#41813198">next</a><span>|</span><label class="collapse" for="c-41814581">[-]</label><label class="expand" for="c-41814581">[1 more]</label></div><br/><div class="children"><div class="content">careful statistical massaging, maybe.<p>would you pick only winning results and only present favorable, massaged results if it got you 150+B USD of worth?</div><br/></div></div></div></div><div id="41813198" class="c"><input type="checkbox" id="c-41813198" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#41814325">prev</a><span>|</span><a href="#41814150">next</a><span>|</span><label class="collapse" for="c-41813198">[-]</label><label class="expand" for="c-41813198">[1 more]</label></div><br/><div class="children"><div class="content">Related ongoing thread:<p><i>LLMs don&#x27;t do formal reasoning</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41812523">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41812523</a> - Oct 2024 (70 comments)</div><br/></div></div><div id="41814150" class="c"><input type="checkbox" id="c-41814150" checked=""/><div class="controls bullet"><span class="by">qwerty456127</span><span>|</span><a href="#41813198">prev</a><span>|</span><a href="#41811049">next</a><span>|</span><label class="collapse" for="c-41814150">[-]</label><label class="expand" for="c-41814150">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t al LLM just detect a mathematical reasoning task then produce a formula (not even display it in the production mode) to invoke on an external service engineered for formal logical and mathematical computations?</div><br/></div></div><div id="41811049" class="c"><input type="checkbox" id="c-41811049" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#41814150">prev</a><span>|</span><a href="#41809727">next</a><span>|</span><label class="collapse" for="c-41811049">[-]</label><label class="expand" for="c-41811049">[9 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s obvious that LLMs will be able to do &quot;reasoning&quot; far better than humans. We must separate our notion of what is remarkably human. Rarely is it the reasoning, it&#x27;s the intuition that a logical path exists -- for example a mathematical proof that draws from separate sub-disciplines of mathematics, etc.<p>Consider that in a LLM, language inputs are tokenized and fed as inputs into the neural network, and connections in the network create output sequences that are not just syntactically correct (trivial) or form semantically plausible sentences (early transformers did this). LLM output sequences follow the deep patterns of language which include sometjhing that resembles reasoning as the model has learnt from its training data.<p>LLMs seem to fall short because they often fail at truly abstract reasoning tasks that humans find easy. If trained properly, LLMs can develop advanced representations of logical systems that will surely outpace what humans can do in terms of raw reasoning.<p>However, human mathematicians have not even unified around constructive mathematics as a must for the study of mathematics.  This reveals that even highly evolved mathematical disciplines rely on objects whose characteristics do not lend themselves to full logical scrutiny and are in a way socially constructed and effectively hard to audit.<p>While notation in mathematics is incredible <i>technology</i> it is also a highly limiting factor that suffers major tradeoffs.  Humans struggle to invent new notation fast enough and to discard outdated notation fast enough.  If we do see an AI-powered boom in mathematics, I suspect our notion of notation and the fluidity we demand from it will change dramatically.</div><br/><div id="41811441" class="c"><input type="checkbox" id="c-41811441" checked=""/><div class="controls bullet"><span class="by">islewis</span><span>|</span><a href="#41811049">parent</a><span>|</span><a href="#41811177">next</a><span>|</span><label class="collapse" for="c-41811441">[-]</label><label class="expand" for="c-41811441">[2 more]</label></div><br/><div class="children"><div class="content">This argument is centered around the belief that language and reasoning flow bidirectionally- language can be understood first (we are here), and reasoning is the next natural rung of the latter (your thesis believes we will get here with LLMs).<p>I see language more as a medium for transcribing reasoning. While language certainly communicates reasoning, you can have reasoning without language, but not language without reasoning.<p>This paper seems to imply that current LLM&#x27;s are just copying the training dataset&#x27;s reasoning communication, not understand the actual reasoning. I don&#x27;t think LLM&#x27;s moving past this is &quot;obvious&quot; or even close to being inevitable.<p>&gt; Instead, LLMs likely perform a form of probabilistic pattern-matching and searching to find closest seen data during training without proper understanding of concepts. While this process goes beyond naive memorization of words and the models are capable of searching and matching more abstract reasoning steps, it still falls short of true formal reasoning.</div><br/><div id="41811572" class="c"><input type="checkbox" id="c-41811572" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#41811049">root</a><span>|</span><a href="#41811441">parent</a><span>|</span><a href="#41811177">next</a><span>|</span><label class="collapse" for="c-41811572">[-]</label><label class="expand" for="c-41811572">[1 more]</label></div><br/><div class="children"><div class="content">I realize there is subtlety to the question of which is first. An infant, crying when it is hungry and pre-linguistic, is applying modus ponens. C -&gt; F crying implies food, so I cry and then I get fed.  Language grows in humans just like arms and legs, and so does reasoning. Baby animals do the same behavior but don&#x27;t use language, so perhaps some logic is wired by instinct. Either way I don&#x27;t think we need to worry about that detail.<p>Consider how language input to an LLM is tokenized. Now imagine a tokenization scheme that introduces tokens that track the strict logical reasoning in the language. Thus two completely different English sentences could both tokenize as the application of Modus Ponens over assumption 1 to conclude conclusion 2, for example.<p>Now consider that we can tokenize formal notation as used in mathematics and logic, and we can train LLMs on mathematical papers, peer review write-ups, etc.  We can generate millions of correct proofs and teach it which ones are remarkable and why, etc.<p>Ultimately we run into the same barrier as mathematical constructivists run into, but I think it&#x27;s still quite plausible that LLMs trained as I describe would be able to reason quite well and find oversights humans missed.  However creating the optimal scheme and implementation is not trivial.</div><br/></div></div></div></div><div id="41811177" class="c"><input type="checkbox" id="c-41811177" checked=""/><div class="controls bullet"><span class="by">sottol</span><span>|</span><a href="#41811049">parent</a><span>|</span><a href="#41811441">prev</a><span>|</span><a href="#41811232">next</a><span>|</span><label class="collapse" for="c-41811177">[-]</label><label class="expand" for="c-41811177">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  If trained properly, LLMs can develop advanced representations of logical systems that will surely outpace what humans can do in terms of raw reasoning.<p>We have already trained the LLMs on most of the human knowledge base (so like 4-5000 years?) - imo training data will become a problem and will soon be more expensive than compute. Sure, you can work around some of this using synthetic training data but I personally would not count on <i>general-purpose</i> LLMs (especially LLMs aka transformer models) developing super-human representations of logical systems anytime soon.</div><br/><div id="41811294" class="c"><input type="checkbox" id="c-41811294" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#41811049">root</a><span>|</span><a href="#41811177">parent</a><span>|</span><a href="#41811232">next</a><span>|</span><label class="collapse" for="c-41811294">[-]</label><label class="expand" for="c-41811294">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree, however I&#x27;m optimistic because most of the current reasoning &quot;ability&quot; of LLMs comes from the accidental reasoning embedded in language patterns.<p>For example, the prompt completion: &quot;The mouse has a unique digestive system compared to other rodents, however the sparrow&quot; on GPT-4o is<p><i>&quot;exhibits a highly specialized digestive system adapted for rapid processing of food, particularly seeds and insects, through structures like the crop and gizzard, which are not found in rodents.&quot;</i><p>Claude 3.5 completes it as<p><i>&quot;has a completely different digestive anatomy as a bird. Birds like sparrows have adaptations for flight, including a lightweight skeletal system and a specialized digestive tract. Unlike mice, sparrows have a crop for storing food, a gizzard for grinding it, and generally shorter intestines to reduce weight. They also lack teeth, instead using their beak to manipulate food.&quot;</i><p>What appears to be a thoughtful contrast is merely a language pattern.  Similarly, a prompt like &quot;Assume -B, A-&gt;B. Under what circumstances is B true?&quot; will simply follow the gradient to return output that is likely correct.  Prompts like &quot;what is 2+2&quot; fail only because nobody bothers to write about it so simple arithmetic was not in the training data.<p>However the way that multi-modal LLMs handle images is inspiring as it effectively converts from the visual domain into the sequential token domain.  The same could be done for symbolic systems, etc.</div><br/></div></div></div></div><div id="41811232" class="c"><input type="checkbox" id="c-41811232" checked=""/><div class="controls bullet"><span class="by">agentultra</span><span>|</span><a href="#41811049">parent</a><span>|</span><a href="#41811177">prev</a><span>|</span><a href="#41809727">next</a><span>|</span><label class="collapse" for="c-41811232">[-]</label><label class="expand" for="c-41811232">[4 more]</label></div><br/><div class="children"><div class="content">I don’t see how it’s obvious that LLM’s will be capable of any mathematical, “reasoning.<p>LLM’s can infer relationships and maintain longer context chains in order to generate their output… it still <i>happens</i> that some times the output is correct depending on the training data, layers, context, etc. And it can get more accurate when we change the parameters of the model. But the algorithm isn’t “doing” anything here. It will generate <i>something</i> regardless of what it’s prompted with.<p>Maybe it’s right. But the algorithm is an algorithm. It doesn’t care what truth is. It’s generating BS essentially.<p>A human is doing a lot more work when performing mathematics.<p>It may be that LLM’s can be a useful tool in mathematical reasoning but it’s not obvious that it will ever be capable of it without a human, let alone be better than a human.</div><br/><div id="41811370" class="c"><input type="checkbox" id="c-41811370" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#41811049">root</a><span>|</span><a href="#41811232">parent</a><span>|</span><a href="#41809727">next</a><span>|</span><label class="collapse" for="c-41811370">[-]</label><label class="expand" for="c-41811370">[3 more]</label></div><br/><div class="children"><div class="content">I think models could be designed that in separate layers created &quot;logical system&quot; representations which could feed back into the output, much like how attention works. Attention is about relevance, the logical layers could be based on logical schema-based patterns.<p>Consider an LLM that happened to have some pre-trained layers that were trained abstractly on all the constructive proofs available for modern mathematics. LLMs with image recognition rely on existing visual pattern recognition layers, fwiw.</div><br/><div id="41812953" class="c"><input type="checkbox" id="c-41812953" checked=""/><div class="controls bullet"><span class="by">agentultra</span><span>|</span><a href="#41811049">root</a><span>|</span><a href="#41811370">parent</a><span>|</span><a href="#41809727">next</a><span>|</span><label class="collapse" for="c-41812953">[-]</label><label class="expand" for="c-41812953">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s another blog post that made it to the front-page of this site which sums  up the state of the art nicely [0].<p>It&#x27;s not <i>obvious</i> that they will be able to do any reasoning, in the formal sense, at all; let alone better than humans. LLMs are simply not sufficient for the kinds of tasks and work done when reasoning about mathematical problems.<p>There&#x27;s plenty of research demonstrating that they can be useful in small, constrained tasks -- which isn&#x27;t anything to raise our noses at!<p>... it&#x27;s just not _obvious_ in the sense that there is a clear step from LLM capabilities today to &quot;better than humans.&quot; It&#x27;s more an article of faith that it <i>could</i> be true, some day, if we just figure out X, Y, Z... which folks have been doing for decades to no avail. In other words, it&#x27;s not obvious at all.<p>[0] <a href="https:&#x2F;&#x2F;garymarcus.substack.com&#x2F;p&#x2F;llms-dont-do-formal-reasoning-and" rel="nofollow">https:&#x2F;&#x2F;garymarcus.substack.com&#x2F;p&#x2F;llms-dont-do-formal-reason...</a></div><br/><div id="41814883" class="c"><input type="checkbox" id="c-41814883" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#41811049">root</a><span>|</span><a href="#41812953">parent</a><span>|</span><a href="#41809727">next</a><span>|</span><label class="collapse" for="c-41814883">[-]</label><label class="expand" for="c-41814883">[1 more]</label></div><br/><div class="children"><div class="content">It’s true that current models do not do formal reasoning, my point is that it is possible to use tokenization to do it. See my comment in the other thread.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41809727" class="c"><input type="checkbox" id="c-41809727" checked=""/><div class="controls bullet"><span class="by">woopwoop</span><span>|</span><a href="#41811049">prev</a><span>|</span><a href="#41809198">next</a><span>|</span><label class="collapse" for="c-41809727">[-]</label><label class="expand" for="c-41809727">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious about what happens with the no-op dataset if you include in the prompt that the questions may contain irrelevant information.</div><br/></div></div><div id="41809198" class="c"><input type="checkbox" id="c-41809198" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#41809727">prev</a><span>|</span><a href="#41809534">next</a><span>|</span><label class="collapse" for="c-41809198">[-]</label><label class="expand" for="c-41809198">[11 more]</label></div><br/><div class="children"><div class="content">It seems incredibly easy to generate an enormous amount of synthetic data for math. Is that happening? Does it work?</div><br/><div id="41809604" class="c"><input type="checkbox" id="c-41809604" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#41809198">parent</a><span>|</span><a href="#41809312">next</a><span>|</span><label class="collapse" for="c-41809604">[-]</label><label class="expand" for="c-41809604">[3 more]</label></div><br/><div class="children"><div class="content">They did that for o1 and o1-preview. Which if you read the paper or do your own testing with that SOTA model you will see that the paper is nonsense. With the best models the problems they point out are mostly marginal like one or two percentage points when changing numbers etc.<p>They are taking poor performance of undersized models and claiming that proves some fundamental limitation of large models, even though their own tests show that isn&#x27;t true.</div><br/><div id="41809777" class="c"><input type="checkbox" id="c-41809777" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#41809198">root</a><span>|</span><a href="#41809604">parent</a><span>|</span><a href="#41809312">next</a><span>|</span><label class="collapse" for="c-41809777">[-]</label><label class="expand" for="c-41809777">[2 more]</label></div><br/><div class="children"><div class="content">You choose to ignore Figure 8 which shows a 18% drop when simply adding an irrelevant detail.<p>In the other test the perturbations aren’t particularly sophisticated and modify the problem according to a template. As the parent comment said this is pretty easy to generate test data for (and for the model to pattern match against) so maybe that is what they did.<p>A better test of “reasoning” would be to isolate the concept&#x2F;algorithm and generate  novel instances that are completely textually different from existing problems to see if the model really isn’t just pattern matching. But we already know the answer to this because it can’t do things like arbitrary length multiplication.</div><br/><div id="41813887" class="c"><input type="checkbox" id="c-41813887" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#41809198">root</a><span>|</span><a href="#41809777">parent</a><span>|</span><a href="#41809312">next</a><span>|</span><label class="collapse" for="c-41813887">[-]</label><label class="expand" for="c-41813887">[1 more]</label></div><br/><div class="children"><div class="content">This shows there are limitations but it doesn&#x27;t prove they can&#x27;t be overcome by changing training data.<p>I don&#x27;t think that LLMs are the end of AGI research at all, but the extreme skepticism of their current utility is mostly based on failures of small models. It&#x27;s like 65% for most of the small models they tested and that is what they are really basing their conclusions on</div><br/></div></div></div></div></div></div><div id="41809312" class="c"><input type="checkbox" id="c-41809312" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#41809198">parent</a><span>|</span><a href="#41809604">prev</a><span>|</span><a href="#41809242">next</a><span>|</span><label class="collapse" for="c-41809312">[-]</label><label class="expand" for="c-41809312">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this is how o1 was trained. Math and programming, because they are verifiable.<p>This is also why o1 is not better at English. Math skills transfer to general reasoning but not so much to creative writing.</div><br/></div></div><div id="41809242" class="c"><input type="checkbox" id="c-41809242" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#41809198">parent</a><span>|</span><a href="#41809312">prev</a><span>|</span><a href="#41809529">next</a><span>|</span><label class="collapse" for="c-41809242">[-]</label><label class="expand" for="c-41809242">[1 more]</label></div><br/><div class="children"><div class="content">In which distribution? Like school math or competition or unsolved problems? FWIW I think one and three and probably easier to generated as synethetically. It&#x27;s harder to bound the difficulty but I think the recent David silver talk implies it doesn&#x27;t matter much. Anyway there&#x27;s some work on this you can find online--they claim to improve gsm8k and MATH a bit but not saturate it. Idk in practice how useful it is</div><br/></div></div><div id="41809529" class="c"><input type="checkbox" id="c-41809529" checked=""/><div class="controls bullet"><span class="by">bentice</span><span>|</span><a href="#41809198">parent</a><span>|</span><a href="#41809242">prev</a><span>|</span><a href="#41811093">next</a><span>|</span><label class="collapse" for="c-41809529">[-]</label><label class="expand" for="c-41809529">[3 more]</label></div><br/><div class="children"><div class="content">Data is the wrong approach to develop reasoning. You we don&#x27;t want LLM&#x27;s to simply memorize 3x3 = 9 we want them to understand that 3 + 3 + 3 = 9 therefore 3x3 = 9 (obviously a trivial example). If they have developed reasoning very few examples should be needed.<p>The way I see it reasoning is actually the ability of the model to design and train smaller models that can learn with very few examples.</div><br/><div id="41809838" class="c"><input type="checkbox" id="c-41809838" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#41809198">root</a><span>|</span><a href="#41809529">parent</a><span>|</span><a href="#41811686">next</a><span>|</span><label class="collapse" for="c-41809838">[-]</label><label class="expand" for="c-41809838">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If they have developed reasoning very few examples should be needed.<p>Yes, once the modules for reasoning have converged, it will take very few examples for it to update to new types of reasoning. But to develop those modules from scratch requires large amounts of examples that overtax its ability to memorize. We see this pattern in the &quot;grokking&quot; papers. Memorization happens first, then &quot;grokking&quot; (god I hate that word).<p>It&#x27;s not like humans bootstrap reasoning out of nothing. We have a billion years of evolution that encoded the right inductive biases in our developmental pathways to quickly converge on the structures for reasoning. Training an LLM from scratch is like recapitulating the entire history of evolution in a few months.</div><br/></div></div><div id="41811686" class="c"><input type="checkbox" id="c-41811686" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#41809198">root</a><span>|</span><a href="#41809529">parent</a><span>|</span><a href="#41809838">prev</a><span>|</span><a href="#41811093">next</a><span>|</span><label class="collapse" for="c-41811686">[-]</label><label class="expand" for="c-41811686">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that, if you train these enough, it becomes likely to develop efficient compressions— which “reasoning” would be.</div><br/></div></div></div></div><div id="41811093" class="c"><input type="checkbox" id="c-41811093" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41809198">parent</a><span>|</span><a href="#41809529">prev</a><span>|</span><a href="#41809236">next</a><span>|</span><label class="collapse" for="c-41811093">[-]</label><label class="expand" for="c-41811093">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easy enough to generate an enormous amount of formal math problems, but utterly quixotic to generate an enormous amount of <i>quantitative reasoning</i> problems, which is the thing LLMs are lacking.</div><br/></div></div><div id="41809236" class="c"><input type="checkbox" id="c-41809236" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#41809198">parent</a><span>|</span><a href="#41811093">prev</a><span>|</span><a href="#41809534">next</a><span>|</span><label class="collapse" for="c-41809236">[-]</label><label class="expand" for="c-41809236">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think so. The data is biased towards being very general.</div><br/></div></div></div></div><div id="41809534" class="c"><input type="checkbox" id="c-41809534" checked=""/><div class="controls bullet"><span class="by">dev1ycan</span><span>|</span><a href="#41809198">prev</a><span>|</span><a href="#41813390">next</a><span>|</span><label class="collapse" for="c-41809534">[-]</label><label class="expand" for="c-41809534">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand the idiocracy we live in, it is beyond obvious not just that the stock market is a bubble but ESPECIALLY the AI related stocks are a massive bubble, when it pops, and it will, it is going to be very very ugly, yet people keep pouring in, as Sabine said it, it&#x27;s starting to look like particle physics where they keep asking for bigger colliders, just because you have a bigger collider, if your methodology is flawed you aren&#x27;t gonna get any more significant returns.<p>Eventually they will run out of exponential cash to pour in, and investors will start asking questions, stocks are already valued at 60x+ their earnings, whenever it pops you don&#x27;t want to be the one who bought the top.<p>Guess it&#x27;s still gonna take a while more for the layman to realize the issues with LLMs, but it&#x27;ll happen.</div><br/><div id="41809830" class="c"><input type="checkbox" id="c-41809830" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41809534">parent</a><span>|</span><a href="#41810493">next</a><span>|</span><label class="collapse" for="c-41809830">[-]</label><label class="expand" for="c-41809830">[5 more]</label></div><br/><div class="children"><div class="content">&gt;if your methodology is flawed you aren&#x27;t gonna get any more significant returns.<p>The problem with this statement is that predictions made about scaling 5 years ago have held true[1]. We keep adding parameters, adding compute, and the models keep getting more capable.<p>The flaws of LLM&#x27;s from 2024 are not what is relevant. Just like the flaws of LLMs from 2021 were not relevant. What is relevant is the rate of change, and the lack of evidence that things won&#x27;t continue on this steep incline. Especially if you consider that GPT4 was sort of a preview model that motivated big money to make ungodly investments to see how far we can push this. Those models will start to show up over the next 2 years.<p>If they break the trend and the scaling flops, <i>then</i> I think a lot of air is gonna blow out of the bubble.<p>[1]<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2001.08361" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2001.08361</a></div><br/><div id="41817274" class="c"><input type="checkbox" id="c-41817274" checked=""/><div class="controls bullet"><span class="by">yoav_hollander</span><span>|</span><a href="#41809534">root</a><span>|</span><a href="#41809830">parent</a><span>|</span><a href="#41810079">next</a><span>|</span><label class="collapse" for="c-41817274">[-]</label><label class="expand" for="c-41817274">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. I was assuming that the by now the default answer to &quot;LLMs sort-of do this, but not very well&quot; should be &quot;OK, wait a few months&quot;.</div><br/></div></div><div id="41810079" class="c"><input type="checkbox" id="c-41810079" checked=""/><div class="controls bullet"><span class="by">vrighter</span><span>|</span><a href="#41809534">root</a><span>|</span><a href="#41809830">parent</a><span>|</span><a href="#41817274">prev</a><span>|</span><a href="#41809969">next</a><span>|</span><label class="collapse" for="c-41810079">[-]</label><label class="expand" for="c-41810079">[1 more]</label></div><br/><div class="children"><div class="content">we added a <i>lot</i> of parameters.<p>We added a <i>LOT</i> of data.<p>The resulting models have become only <i>slightly</i> better. And they still have <i>all</i> of their old problems.<p>I think this is proof that scaling doesn&#x27;t work. It&#x27;s not like we just doubled the sizes, they increased by a lot, but improvements are less and less each time. And they&#x27;ve already run out of useful data.</div><br/></div></div><div id="41809969" class="c"><input type="checkbox" id="c-41809969" checked=""/><div class="controls bullet"><span class="by">dev1ycan</span><span>|</span><a href="#41809534">root</a><span>|</span><a href="#41809830">parent</a><span>|</span><a href="#41810079">prev</a><span>|</span><a href="#41810493">next</a><span>|</span><label class="collapse" for="c-41809969">[-]</label><label class="expand" for="c-41809969">[2 more]</label></div><br/><div class="children"><div class="content">They are very literally asking for trillions and even nuclear powered data centers, pretty sure we&#x27;ve gotten to the point where it&#x27;s not sustainable.</div><br/><div id="41810085" class="c"><input type="checkbox" id="c-41810085" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41809534">root</a><span>|</span><a href="#41809969">parent</a><span>|</span><a href="#41810493">next</a><span>|</span><label class="collapse" for="c-41810085">[-]</label><label class="expand" for="c-41810085">[1 more]</label></div><br/><div class="children"><div class="content">Those are roadmap items being asked for, but the next gen models are already in training. If they keep moving along the same trend line, like all the previous models have, then they probably will be able to find the investors for the next next gen. Even if it&#x27;s a few trillion dollars and a few nuclear power plants.<p>This doesn&#x27;t even factor in the tech inertia. We could stop making new models today, and it would probably be 4-5 years before integration slowed down. Google still hasn&#x27;t even put Gemini in their home speakers.</div><br/></div></div></div></div></div></div><div id="41810493" class="c"><input type="checkbox" id="c-41810493" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#41809534">parent</a><span>|</span><a href="#41809830">prev</a><span>|</span><a href="#41813390">next</a><span>|</span><label class="collapse" for="c-41810493">[-]</label><label class="expand" for="c-41810493">[1 more]</label></div><br/><div class="children"><div class="content">Computers have been able to do mathematical calculation and logical deduction cheaply and perfectly for decades, and it&#x27;s not really required for generative AIs to be able to do it for them to be useful.  It&#x27;s good enough if they can write and execute some python code to do it, and generally they are fairly capable of that.<p>The question of whether they can do it is interesting in an academic sense, but has nothing to do if they&#x27;re useful or not.  They also don&#x27;t need to be true AGI to be useful.</div><br/></div></div></div></div><div id="41813390" class="c"><input type="checkbox" id="c-41813390" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41809534">prev</a><span>|</span><a href="#41814535">next</a><span>|</span><label class="collapse" for="c-41813390">[-]</label><label class="expand" for="c-41813390">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Overall, while o1-preview and o1-mini exhibit significantly stronger results compared to current open models—potentially due to improved training data and post-training procedures—they still share similar limitations with the open models.<p>tl;dr - the best open model dropped from 89.7% on GSM8K(full) to 30% on Symbolic-NoOp, while o1-preview dropped from 94.9% to 77.4%, respectively.<p>I think all this paper shows is that LLMs need space to &quot;think&quot; outside of their inference layer, (for the current architectures at least).<p>It&#x27;s similar to the &quot;draw a room, but DO NOT put an elephant in the corner&quot; prompts that people were using with image models.<p>This is something that practitioners have been doing for awhile (via CoT, ToT, etc.) and the whole rationale behind OpenAI&#x27;s newly launched o1-series &quot;model.&quot;<p>There&#x27;s another post that says this paper proves LLMs can&#x27;t be used to build &quot;reliable agents&quot; -- which doesn&#x27;t appear to be true when you look at o1&#x27;s stellar performance here.</div><br/></div></div><div id="41814535" class="c"><input type="checkbox" id="c-41814535" checked=""/><div class="controls bullet"><span class="by">throwaway918299</span><span>|</span><a href="#41813390">prev</a><span>|</span><a href="#41812593">next</a><span>|</span><label class="collapse" for="c-41814535">[-]</label><label class="expand" for="c-41814535">[1 more]</label></div><br/><div class="children"><div class="content">limitations of mathematical reasoning?<p>They have none.  Literally zero.  That’s the limit.  Thank you for reading my paper.</div><br/></div></div><div id="41809347" class="c"><input type="checkbox" id="c-41809347" checked=""/><div class="controls bullet"><span class="by">beardyw</span><span>|</span><a href="#41812593">prev</a><span>|</span><a href="#41809429">next</a><span>|</span><label class="collapse" for="c-41809347">[-]</label><label class="expand" for="c-41809347">[8 more]</label></div><br/><div class="children"><div class="content">I honestly can&#x27;t see why LLMs should be good at this sort of thing. I am convinced you need a completely different approach. At the very least you mostly only want one completely correct result. Good luck getting current models to do that.</div><br/><div id="41809697" class="c"><input type="checkbox" id="c-41809697" checked=""/><div class="controls bullet"><span class="by">hackinthebochs</span><span>|</span><a href="#41809347">parent</a><span>|</span><a href="#41810406">next</a><span>|</span><label class="collapse" for="c-41809697">[-]</label><label class="expand" for="c-41809697">[1 more]</label></div><br/><div class="children"><div class="content">LLMs aren&#x27;t totally out of scope of mathematical reasoning. LLMs roughly do two things, move data around, and recognize patterns. Reasoning leans heavily on moving data around according to context-sensitive rules. This is well within the scope of LLMs. The problem is that general problem solving requires potentially arbitrary amounts of moving data, but current LLM architectures have a fixed amount of translation&#x2F;rewrite steps they can perform before they must produce output. This means most complex reasoning problems are out of bounds for LLMs so they learn to lean heavily on pattern matching. But this isn&#x27;t an intrinsic limitation to LLMs as a class of computing device, just the limits of current architectures.</div><br/></div></div><div id="41810406" class="c"><input type="checkbox" id="c-41810406" checked=""/><div class="controls bullet"><span class="by">qudat</span><span>|</span><a href="#41809347">parent</a><span>|</span><a href="#41809697">prev</a><span>|</span><a href="#41810346">next</a><span>|</span><label class="collapse" for="c-41810406">[-]</label><label class="expand" for="c-41810406">[1 more]</label></div><br/><div class="children"><div class="content">One core issue is that we need to convert spoken&#x2F;written languages (e.g. english) into more formal math languages since sometimes the underlying mathematical problem is written using prose.  The example in the paper:<p>&gt;  When Sophie watches her nephew, she gets out a variety of toys for him. The bag of building blocks has 31 blocks in it. The bin of stuffed animals has 8 stuffed animals inside. The tower of stacking rings has 9 multicolored rings on it. Sophie recently bought a tube of bouncy balls, bringing her total number of toys for her nephew up to 62. How many bouncy balls came in the tube?<p>So I would argue it&#x27;s critical that LLMs knows how to convert text to math and then perform those math calculations.  This extends beyond just math but also the underlying logics.<p>We just need to figure out how to inform the LLM to read, write, and understand formal languages.  My guess is attention heads could probably work in this context, but we might want something that is a little more rigid, naturally extending from the rigidity of logic and formal languages.  Conversely, we might not have figured out how to properly train LLMs on formal languages and have them preserve the underlying logic and axioms necessary to correctly perform math calculations.</div><br/></div></div><div id="41810346" class="c"><input type="checkbox" id="c-41810346" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#41809347">parent</a><span>|</span><a href="#41810406">prev</a><span>|</span><a href="#41809365">next</a><span>|</span><label class="collapse" for="c-41810346">[-]</label><label class="expand" for="c-41810346">[1 more]</label></div><br/><div class="children"><div class="content">Well, my perspective on this is as follows:<p>The recurrent or transformer models are Turing complete, or at least close to being Turing complete (apologies, I’m not sure of the precise terminology here).<p>As a result, they can at least simulate a brain and are capable of exhibiting human-like intelligence. The &quot;program&quot; is the trained dataset, and we have seen significant improvements in smaller models simply by enhancing the dataset.<p>We still don’t know what the optimal &quot;program&quot; looks like or what level of scaling is truly necessary. But in theory, achieving the goal of AGI with LLMs is possible.</div><br/></div></div><div id="41809365" class="c"><input type="checkbox" id="c-41809365" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41809347">parent</a><span>|</span><a href="#41810346">prev</a><span>|</span><a href="#41809389">next</a><span>|</span><label class="collapse" for="c-41809365">[-]</label><label class="expand" for="c-41809365">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a math phd student at the moment and I regularly use o1 to try some quick calculations I don&#x27;t feel like doing. While I feel like GPT-4o is so distilled that it just tries to know the answer from memory, o1 actually works with what you gave it and tries to calculate. It&#x27;s can be quite useful.</div><br/><div id="41809560" class="c"><input type="checkbox" id="c-41809560" checked=""/><div class="controls bullet"><span class="by">banditelol</span><span>|</span><a href="#41809347">root</a><span>|</span><a href="#41809365">parent</a><span>|</span><a href="#41809389">next</a><span>|</span><label class="collapse" for="c-41809560">[-]</label><label class="expand" for="c-41809560">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what kind of quick calculation do you usually use llm for?<p>Edited for clarity</div><br/><div id="41809983" class="c"><input type="checkbox" id="c-41809983" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41809347">root</a><span>|</span><a href="#41809560">parent</a><span>|</span><a href="#41809389">next</a><span>|</span><label class="collapse" for="c-41809983">[-]</label><label class="expand" for="c-41809983">[1 more]</label></div><br/><div class="children"><div class="content">Just earlier today I wanted to check if exp(inx) is an orthonormal basis on L^2((0, 1)) or if it needs normalization. This is an extremely trivial one though. Less trivially I had an issue where a paper claimed that a certain white noise, a random series which diverges in a certain Hilbert space, is actually convergent in some L^infinity type space. I had tried to use a Sobolev embedding but that was too crude so it didn&#x27;t work. o1 correctly realized that you have to use the decay of the L^infinity norm of the eigenbasis, a technique which I had used before but just didn&#x27;t think of in the moment. It also gave me the eigenbasis and checked that everything works (again, standard but takes a while to find in YOUR setting). I wasn&#x27;t sure about the normalization so again I asked it to calculate the integral.<p>This kind of adaptation to your specific setting instead of just spitting out memorized answers in commonn settings is what makes o1 useful for me. Now again, it is often wrong, but if I am completely clueless I like to watch it attempt things and I can get inspiration from that. That&#x27;s much more useful than seeing a confident wrong answer like 4o would give it.</div><br/></div></div></div></div></div></div></div></div><div id="41809429" class="c"><input type="checkbox" id="c-41809429" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#41809347">prev</a><span>|</span><label class="collapse" for="c-41809429">[-]</label><label class="expand" for="c-41809429">[3 more]</label></div><br/><div class="children"><div class="content">()</div><br/><div id="41809576" class="c"><input type="checkbox" id="c-41809576" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#41809429">parent</a><span>|</span><label class="collapse" for="c-41809576">[-]</label><label class="expand" for="c-41809576">[2 more]</label></div><br/><div class="children"><div class="content">That makes the whole conclusion obviously false.<p>I don&#x27;t really understand why, but I think we are going to see total denial from a significant percentage of the population all the way up to and past the point where many average mathematicians and software engineers cannot in any way compete with AI.<p>We already are reportedly getting pretty close with o1 (not o1-preview).<p>There are also new paradigms for machine learning and hardware in the pipeline that will continue to provide orders of magnitude performance gains and new capabilities in the next 5-10 years.<p>Many people still claim that &quot;self driving cars don&#x27;t exist&quot;, in so many words, even though they are deployed in multiple cities.</div><br/><div id="41811432" class="c"><input type="checkbox" id="c-41811432" checked=""/><div class="controls bullet"><span class="by">sottol</span><span>|</span><a href="#41809429">root</a><span>|</span><a href="#41809576">parent</a><span>|</span><label class="collapse" for="c-41811432">[-]</label><label class="expand" for="c-41811432">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Many people still claim that &quot;self driving cars don&#x27;t exist&quot;, in so many words, even though they are deployed in multiple cities.<p>But just look at the predictions of that time - cities will change, ... and so on. Sure, we have self-driving cars but the reality looks very different (and a lot more like the past!) than the pundits and futurists imagined! I&#x27;m not sure anyone will make their billions of dollars investmented back within even 20 years.<p>Just two random examples from ~10 years ago (2013-2016), you can google many more of that time.<p>* &quot;Ford Targets Fully Autonomous Vehicle for Ride Sharing in 2021; Invests in New Tech Companies, Doubles Silicon Valley Team&quot; [1]<p>* &quot;Disruptions: How Driverless Cars Could Reshape Cities&quot; [2]<p>[1] <a href="https:&#x2F;&#x2F;media.ford.com&#x2F;content&#x2F;fordmedia&#x2F;fna&#x2F;us&#x2F;en&#x2F;news&#x2F;2016&#x2F;08&#x2F;16&#x2F;ford-targets-fully-autonomous-vehicle-for-ride-sharing-in-2021.html" rel="nofollow">https:&#x2F;&#x2F;media.ford.com&#x2F;content&#x2F;fordmedia&#x2F;fna&#x2F;us&#x2F;en&#x2F;news&#x2F;2016...</a><p>[2] <a href="https:&#x2F;&#x2F;archive.nytimes.com&#x2F;bits.blogs.nytimes.com&#x2F;2013&#x2F;07&#x2F;07&#x2F;disruptions-how-driverless-cars-could-reshape-cities&#x2F;?smid=tw-share" rel="nofollow">https:&#x2F;&#x2F;archive.nytimes.com&#x2F;bits.blogs.nytimes.com&#x2F;2013&#x2F;07&#x2F;0...</a><p>[3] <a href="https:&#x2F;&#x2F;www.gensler.com&#x2F;dialogue&#x2F;30&#x2F;the-game-changer-for-cities-driverless-cars" rel="nofollow">https:&#x2F;&#x2F;www.gensler.com&#x2F;dialogue&#x2F;30&#x2F;the-game-changer-for-cit...</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>