<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707901257900" as="style"/><link rel="stylesheet" href="styles.css?v=1707901257900"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/Stability-AI/StableCascade">Stable Cascade</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>davidbarker</span> | <span>139 comments</span></div><br/><div><div id="39360297" class="c"><input type="checkbox" id="c-39360297" checked=""/><div class="controls bullet"><span class="by">obviyus</span><span>|</span><a href="#39360630">next</a><span>|</span><label class="collapse" for="c-39360297">[-]</label><label class="expand" for="c-39360297">[28 more]</label></div><br/><div class="children"><div class="content">Been using it for a couple of hours and it seems it’s much better at following the prompt. Right away it seems the quality is worse compared to some SDXL models but I’ll reserve judgement until a couple more days of testing.<p>It’s fast too! I would reckon about 2-3x faster than non-turbo SDXL.</div><br/><div id="39363024" class="c"><input type="checkbox" id="c-39363024" checked=""/><div class="controls bullet"><span class="by">vergessenmir</span><span>|</span><a href="#39360297">parent</a><span>|</span><a href="#39361179">next</a><span>|</span><label class="collapse" for="c-39363024">[-]</label><label class="expand" for="c-39363024">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll take prompt adherence over quality any day. The machinery otherwise isn&#x27;t worth it i.e the controlnets, openpose, depthmaps just to force a particular look or to achieve depth. Th solution becomes bespoke for each generation.<p>Had a test of it and my option is it&#x27;s an improvement when it comes to following prompts and I do find the images more visually appealing.</div><br/><div id="39363299" class="c"><input type="checkbox" id="c-39363299" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39363024">parent</a><span>|</span><a href="#39361179">next</a><span>|</span><label class="collapse" for="c-39363299">[-]</label><label class="expand" for="c-39363299">[3 more]</label></div><br/><div class="children"><div class="content">Can we use its output as input to SDXL? Presumably it would just fill in the details, and not create whole new images.</div><br/><div id="39364365" class="c"><input type="checkbox" id="c-39364365" checked=""/><div class="controls bullet"><span class="by">RIMR</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39363299">parent</a><span>|</span><a href="#39361179">next</a><span>|</span><label class="collapse" for="c-39364365">[-]</label><label class="expand" for="c-39364365">[2 more]</label></div><br/><div class="children"><div class="content">I was thinking that exactly. You could use the same trick as the hires-fix for an adherence-fix.</div><br/><div id="39366310" class="c"><input type="checkbox" id="c-39366310" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39364365">parent</a><span>|</span><a href="#39361179">next</a><span>|</span><label class="collapse" for="c-39366310">[-]</label><label class="expand" for="c-39366310">[1 more]</label></div><br/><div class="children"><div class="content">Yeah chain it in comfy to a turbo model for detail</div><br/></div></div></div></div></div></div></div></div><div id="39361179" class="c"><input type="checkbox" id="c-39361179" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39360297">parent</a><span>|</span><a href="#39363024">prev</a><span>|</span><a href="#39360497">next</a><span>|</span><label class="collapse" for="c-39361179">[-]</label><label class="expand" for="c-39361179">[10 more]</label></div><br/><div class="children"><div class="content">How much VRAM does it need? They mention that the largest model uses 1.4 billion parameters more than SDXL, which in turn need a lot of VRAM.</div><br/><div id="39361834" class="c"><input type="checkbox" id="c-39361834" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361179">parent</a><span>|</span><a href="#39361431">next</a><span>|</span><label class="collapse" for="c-39361834">[-]</label><label class="expand" for="c-39361834">[4 more]</label></div><br/><div class="children"><div class="content">Should use no more than 6GiB for FP16 models at each stage. The current implementation is not RAM optimized.</div><br/><div id="39361885" class="c"><input type="checkbox" id="c-39361885" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361834">parent</a><span>|</span><a href="#39364441">next</a><span>|</span><label class="collapse" for="c-39361885">[-]</label><label class="expand" for="c-39361885">[2 more]</label></div><br/><div class="children"><div class="content">The large C model uses 3.6 billion parameters which is 6.7 GiB if each parameter is 16 bits.</div><br/><div id="39361932" class="c"><input type="checkbox" id="c-39361932" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361885">parent</a><span>|</span><a href="#39364441">next</a><span>|</span><label class="collapse" for="c-39361932">[-]</label><label class="expand" for="c-39361932">[1 more]</label></div><br/><div class="children"><div class="content">The large C model have fair bit of parameters tied to text-conditioning, not to the main denoising process. Similar to how we split the network for SDXL Base, I am pretty confident we can split non-trivial amount of parameters to text-conditioning hence during denoising process, loading less than 3.6B parameters.</div><br/></div></div></div></div><div id="39364441" class="c"><input type="checkbox" id="c-39364441" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361834">parent</a><span>|</span><a href="#39361885">prev</a><span>|</span><a href="#39361431">next</a><span>|</span><label class="collapse" for="c-39364441">[-]</label><label class="expand" for="c-39364441">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s more, they can presumably be swapped in and out like the SDXL base + refiner, right?</div><br/></div></div></div></div><div id="39361431" class="c"><input type="checkbox" id="c-39361431" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361179">parent</a><span>|</span><a href="#39361834">prev</a><span>|</span><a href="#39360497">next</a><span>|</span><label class="collapse" for="c-39361431">[-]</label><label class="expand" for="c-39361431">[5 more]</label></div><br/><div class="children"><div class="content">There was a leak from Japan yesterday, prior to this release, and in that it was suggested 20gb for the largest model.<p>This text was part of the Stability Japan leak (the 20gb VRAM reference was dropped in the release today):<p>&quot;Stages C and B will be released in two different models. Stage C uses parameters of 1B and 3.6B, and Stage B uses parameters of 700M and 1.5B. However, if you want to minimize your hardware needs, you can also use the 1B parameter version. In Stage B, both give great results, but 1.5 billion is better at reconstructing finer details. Thanks to Stable Cascade&#x27;s modular approach, the expected amount of VRAM required for inference can be kept at around 20GB, but can be reduced even further by using smaller variations (as mentioned earlier, this (which may reduce the final output quality).&quot;</div><br/><div id="39361590" class="c"><input type="checkbox" id="c-39361590" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361431">parent</a><span>|</span><a href="#39360497">next</a><span>|</span><label class="collapse" for="c-39361590">[-]</label><label class="expand" for="c-39361590">[4 more]</label></div><br/><div class="children"><div class="content">Thanks. I guess this means that fewer people will be able to use it on their own computer, but the improved efficiency makes it cheaper to run on servers with enough VRAM.<p>Maybe running stage C first, unloading it from VRAM, and then do B and A would make it fit in 12 or even 8 GB, but I wonder if the memory transfers would negate any time saving. Might still be worth it if it produces better images though.</div><br/><div id="39361787" class="c"><input type="checkbox" id="c-39361787" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361590">parent</a><span>|</span><a href="#39361743">next</a><span>|</span><label class="collapse" for="c-39361787">[-]</label><label class="expand" for="c-39361787">[2 more]</label></div><br/><div class="children"><div class="content">Sequential model offloading isn’t too bad. It adds about a second or less to inference, assuming it still fits in main memory.</div><br/><div id="39361941" class="c"><input type="checkbox" id="c-39361941" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361787">parent</a><span>|</span><a href="#39361743">next</a><span>|</span><label class="collapse" for="c-39361941">[-]</label><label class="expand" for="c-39361941">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes I forget how fast modern computers are. PCIe v4 x16 has a transfer speed of 31.5 GB&#x2F;s, so theoretically it should take less than 100 ms to transfer stage B and A. Maybe it&#x27;s not so bad after all, it will be interesting to see what happens.</div><br/></div></div></div></div><div id="39361743" class="c"><input type="checkbox" id="c-39361743" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361590">parent</a><span>|</span><a href="#39361787">prev</a><span>|</span><a href="#39360497">next</a><span>|</span><label class="collapse" for="c-39361743">[-]</label><label class="expand" for="c-39361743">[1 more]</label></div><br/><div class="children"><div class="content">If it worked I imagine large batching could make it worth the load&#x2F;unload time cost.</div><br/></div></div></div></div></div></div></div></div><div id="39360497" class="c"><input type="checkbox" id="c-39360497" checked=""/><div class="controls bullet"><span class="by">kimoz</span><span>|</span><a href="#39360297">parent</a><span>|</span><a href="#39361179">prev</a><span>|</span><a href="#39360630">next</a><span>|</span><label class="collapse" for="c-39360497">[-]</label><label class="expand" for="c-39360497">[13 more]</label></div><br/><div class="children"><div class="content">Can one run it on CPU?</div><br/><div id="39360704" class="c"><input type="checkbox" id="c-39360704" checked=""/><div class="controls bullet"><span class="by">rwmj</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360497">parent</a><span>|</span><a href="#39360824">next</a><span>|</span><label class="collapse" for="c-39360704">[-]</label><label class="expand" for="c-39360704">[10 more]</label></div><br/><div class="children"><div class="content">Stable Diffusion on a 16 core AMD CPU takes for me about 2-3 hours to generate an image, just to give you a rough idea of the performance.  (On the same AMD&#x27;s iGPU it takes 2 minutes or so).</div><br/><div id="39367937" class="c"><input type="checkbox" id="c-39367937" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360704">parent</a><span>|</span><a href="#39360911">next</a><span>|</span><label class="collapse" for="c-39367937">[-]</label><label class="expand" for="c-39367937">[1 more]</label></div><br/><div class="children"><div class="content">Which AMD CPU&#x2F;iGPU are these timings for?</div><br/></div></div><div id="39360911" class="c"><input type="checkbox" id="c-39360911" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360704">parent</a><span>|</span><a href="#39367937">prev</a><span>|</span><a href="#39361063">next</a><span>|</span><label class="collapse" for="c-39360911">[-]</label><label class="expand" for="c-39360911">[5 more]</label></div><br/><div class="children"><div class="content">Even older GPUs are worth using then I take it?<p>For example I pulled a (2GB I think, 4 tops) 6870 out of my desktop because it&#x27;s a beast (in physical size, and power consumption) and I wasn&#x27;t using it for gaming or anything, figured I&#x27;d be fine just with the Intel integrated graphics. But if I wanted to play around with some models locally, it&#x27;d be worth putting it back &amp; figuring out how to use it as a secondary card?</div><br/><div id="39361026" class="c"><input type="checkbox" id="c-39361026" checked=""/><div class="controls bullet"><span class="by">rwmj</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360911">parent</a><span>|</span><a href="#39361109">next</a><span>|</span><label class="collapse" for="c-39361026">[-]</label><label class="expand" for="c-39361026">[2 more]</label></div><br/><div class="children"><div class="content">One counterintuitive advantage of the integrated GPU is it has access to system RAM (instead of using a dedicated and fixed amount of VRAM).  That means I&#x27;m able to give the iGPU 16 GB of RAM.  For me SD takes 8-9 GB of RAM when running.  The system RAM is slower than VRAM which is the trade-off here.</div><br/><div id="39361169" class="c"><input type="checkbox" id="c-39361169" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361026">parent</a><span>|</span><a href="#39361109">next</a><span>|</span><label class="collapse" for="c-39361169">[-]</label><label class="expand" for="c-39361169">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I did wonder about that as I typed, which is why I mentioned the low amount (by modern standards anyway) on the card. OK, thanks!</div><br/></div></div></div></div><div id="39361109" class="c"><input type="checkbox" id="c-39361109" checked=""/><div class="controls bullet"><span class="by">mat0</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360911">parent</a><span>|</span><a href="#39361026">prev</a><span>|</span><a href="#39362520">next</a><span>|</span><label class="collapse" for="c-39361109">[-]</label><label class="expand" for="c-39361109">[1 more]</label></div><br/><div class="children"><div class="content">No, I don&#x27;t think so. I think you would need more VRAM to start with.</div><br/></div></div><div id="39362520" class="c"><input type="checkbox" id="c-39362520" checked=""/><div class="controls bullet"><span class="by">purpleflame1257</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360911">parent</a><span>|</span><a href="#39361109">prev</a><span>|</span><a href="#39361063">next</a><span>|</span><label class="collapse" for="c-39362520">[-]</label><label class="expand" for="c-39362520">[1 more]</label></div><br/><div class="children"><div class="content">2GB is really low. I&#x27;ve been able to use A111 stable diffusion on my old gaming laptop&#x27;s 1060 (6GB VRAM) and it takes a little bit less than a minute to generate an image. You would probably need to try the --lowvram flag on startup.</div><br/></div></div></div></div><div id="39361063" class="c"><input type="checkbox" id="c-39361063" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360704">parent</a><span>|</span><a href="#39360911">prev</a><span>|</span><a href="#39361544">next</a><span>|</span><label class="collapse" for="c-39361063">[-]</label><label class="expand" for="c-39361063">[2 more]</label></div><br/><div class="children"><div class="content">SDXL Turbo is much better, albeit kinda fuzzy and distorted. I was able to get decent single-sample response times (~80-100s) from my 4 core ARM Ampere instance, good enough for a Discord bot with friends.</div><br/><div id="39362570" class="c"><input type="checkbox" id="c-39362570" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39361063">parent</a><span>|</span><a href="#39361544">next</a><span>|</span><label class="collapse" for="c-39362570">[-]</label><label class="expand" for="c-39362570">[1 more]</label></div><br/><div class="children"><div class="content">Sd turbo runs nicely on a m2 MacBook Air (as does stable lm 2!)<p>Much faster models will come</div><br/></div></div></div></div><div id="39361544" class="c"><input type="checkbox" id="c-39361544" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360704">parent</a><span>|</span><a href="#39361063">prev</a><span>|</span><a href="#39360824">next</a><span>|</span><label class="collapse" for="c-39361544">[-]</label><label class="expand" for="c-39361544">[1 more]</label></div><br/><div class="children"><div class="content">If that is true, then the CPU variant must be a much worse implementation of the algorithm than the GPU variant, because the true ratio of the GPU and CPU performances is many times less than that.</div><br/></div></div></div></div><div id="39360824" class="c"><input type="checkbox" id="c-39360824" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360497">parent</a><span>|</span><a href="#39360704">prev</a><span>|</span><a href="#39360636">next</a><span>|</span><label class="collapse" for="c-39360824">[-]</label><label class="expand" for="c-39360824">[1 more]</label></div><br/><div class="children"><div class="content">Not if you want to finish the generation before you have stopped caring about the results.</div><br/></div></div><div id="39360636" class="c"><input type="checkbox" id="c-39360636" checked=""/><div class="controls bullet"><span class="by">ghurtado</span><span>|</span><a href="#39360297">root</a><span>|</span><a href="#39360497">parent</a><span>|</span><a href="#39360824">prev</a><span>|</span><a href="#39360630">next</a><span>|</span><label class="collapse" for="c-39360636">[-]</label><label class="expand" for="c-39360636">[1 more]</label></div><br/><div class="children"><div class="content">You can run any ML model on CPU. The question is the performance</div><br/></div></div></div></div></div></div><div id="39360630" class="c"><input type="checkbox" id="c-39360630" checked=""/><div class="controls bullet"><span class="by">yogorenapan</span><span>|</span><a href="#39360297">prev</a><span>|</span><a href="#39360722">next</a><span>|</span><label class="collapse" for="c-39360630">[-]</label><label class="expand" for="c-39360630">[42 more]</label></div><br/><div class="children"><div class="content">Very impressive.<p>From what I understand, Stability AI is currently VC funded. It’s bound to burn through tons of money and it’s not clear whether the business model (if any) is sustainable. Perhaps worthy of government funding.</div><br/><div id="39360749" class="c"><input type="checkbox" id="c-39360749" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39360630">parent</a><span>|</span><a href="#39361197">next</a><span>|</span><label class="collapse" for="c-39360749">[-]</label><label class="expand" for="c-39360749">[34 more]</label></div><br/><div class="children"><div class="content">Stability AI has been burning through tons of money for awhile now, which is the reason newer models like Stable Cascade are not commercially-friendly-licensed open source anymore.<p>&gt; The company is spending significant amounts of money to grow its business. At the time of its deal with Intel, Stability was spending roughly $8 million a month on bills and payroll and earning a fraction of that in revenue, two of the people familiar with the matter said.<p>&gt; It made $1.2 million in revenue in August and was on track to make $3 million this month from software and services, according to a post Mostaque wrote on Monday on X, the platform formerly known as Twitter. The post has since been deleted.<p><a href="https:&#x2F;&#x2F;fortune.com&#x2F;2023&#x2F;11&#x2F;29&#x2F;stability-ai-sale-intel-ceo-resign&#x2F;" rel="nofollow">https:&#x2F;&#x2F;fortune.com&#x2F;2023&#x2F;11&#x2F;29&#x2F;stability-ai-sale-intel-ceo-r...</a></div><br/><div id="39361596" class="c"><input type="checkbox" id="c-39361596" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360749">parent</a><span>|</span><a href="#39360896">next</a><span>|</span><label class="collapse" for="c-39361596">[-]</label><label class="expand" for="c-39361596">[2 more]</label></div><br/><div class="children"><div class="content">I get the impression that a lot of open source adjacent AI companies, including Stability AI, are in the &quot;???&quot; phase of execution, hoping the &quot;Profit&quot; phase comes next.<p>Given how much VC money is chasing the AI space, this isn&#x27;t necessarily a bad plan.  Give stuff away for free while developing deep expertise, then either figure out something to sell, or pivot to proprietary, or get aquihired by a tech giant.</div><br/><div id="39361691" class="c"><input type="checkbox" id="c-39361691" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361596">parent</a><span>|</span><a href="#39360896">next</a><span>|</span><label class="collapse" for="c-39361691">[-]</label><label class="expand" for="c-39361691">[1 more]</label></div><br/><div class="children"><div class="content">That is indeed the case, hence the more recent pushes toward building moats by every AI company.</div><br/></div></div></div></div><div id="39360896" class="c"><input type="checkbox" id="c-39360896" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360749">parent</a><span>|</span><a href="#39361596">prev</a><span>|</span><a href="#39361197">next</a><span>|</span><label class="collapse" for="c-39360896">[-]</label><label class="expand" for="c-39360896">[31 more]</label></div><br/><div class="children"><div class="content">&gt;  which is the reason newer models like Stable Cascade are not commercially-friendly-licensed open source anymore.<p>The main reason is probably Mid journey and OpenAi using their tech without any kind of contribution back. AI desperately needs a GPL equivalent…</div><br/><div id="39360977" class="c"><input type="checkbox" id="c-39360977" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360896">parent</a><span>|</span><a href="#39361620">next</a><span>|</span><label class="collapse" for="c-39360977">[-]</label><label class="expand" for="c-39360977">[18 more]</label></div><br/><div class="children"><div class="content">It&#x27;s highly doubtful that Midjourney and OpenAI use Stable Diffusion or other Stability models.</div><br/><div id="39361159" class="c"><input type="checkbox" id="c-39361159" checked=""/><div class="controls bullet"><span class="by">cthalupa</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360977">parent</a><span>|</span><a href="#39361884">next</a><span>|</span><label class="collapse" for="c-39361159">[-]</label><label class="expand" for="c-39361159">[2 more]</label></div><br/><div class="children"><div class="content">Midjourney 100% at least used to use Stable Diffusion: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;EMostaque&#x2F;status&#x2F;1561917541743841280" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;EMostaque&#x2F;status&#x2F;1561917541743841280</a><p>I am not sure if that is still the case.</div><br/><div id="39361188" class="c"><input type="checkbox" id="c-39361188" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361159">parent</a><span>|</span><a href="#39361884">next</a><span>|</span><label class="collapse" for="c-39361188">[-]</label><label class="expand" for="c-39361188">[1 more]</label></div><br/><div class="children"><div class="content">It trialled it as an explicitly optional model for a moment a couple years ago. (or only a year? time moves so fast. somewhere in v2&#x2F;v3 timeframe and around when SD came out). I am sure it is no longer the case.</div><br/></div></div></div></div><div id="39361884" class="c"><input type="checkbox" id="c-39361884" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360977">parent</a><span>|</span><a href="#39361159">prev</a><span>|</span><a href="#39361080">next</a><span>|</span><label class="collapse" for="c-39361884">[-]</label><label class="expand" for="c-39361884">[1 more]</label></div><br/><div class="children"><div class="content">DALL-E shares the same autoencoders as SD v1.x. It is probably similar to how Meta&#x27;s Emu-class models work though. They tweaked the architecture quite a bit, trained on their own dataset, reused some components (or in Emu case, trained all the components from scratch but reused the same arch).</div><br/></div></div><div id="39361080" class="c"><input type="checkbox" id="c-39361080" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360977">parent</a><span>|</span><a href="#39361884">prev</a><span>|</span><a href="#39361620">next</a><span>|</span><label class="collapse" for="c-39361080">[-]</label><label class="expand" for="c-39361080">[14 more]</label></div><br/><div class="children"><div class="content">How do you know though?</div><br/><div id="39361140" class="c"><input type="checkbox" id="c-39361140" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361080">parent</a><span>|</span><a href="#39361620">next</a><span>|</span><label class="collapse" for="c-39361140">[-]</label><label class="expand" for="c-39361140">[13 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t use off-the-shelf models to get the results Midjourney and DALL-E generate, even with strong finetuning.</div><br/><div id="39361353" class="c"><input type="checkbox" id="c-39361353" checked=""/><div class="controls bullet"><span class="by">cthalupa</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361140">parent</a><span>|</span><a href="#39365462">next</a><span>|</span><label class="collapse" for="c-39361353">[-]</label><label class="expand" for="c-39361353">[10 more]</label></div><br/><div class="children"><div class="content">I pay for both MJ and DALL-E (though OpenAI mostly gets my money for GPT) and don&#x27;t find them to produce significantly better images than popular checkpoints on CivitAI. What I do find is that they are significantly easier to work with. (Actually, my experience with hundreds of DALL-E generations is that it&#x27;s actually quite poor in quality. I&#x27;m in several IRC channels where it&#x27;s the image generator of choice for some IRC bots, and I&#x27;m never particularly impressed with the visual quality.)<p>For MJ in particular, knowing that they at least used to use Stable Diffusion under the hood, it would not surprise me if the majority of the secret sauce is actually a middle layer that processes the prompt and converts it to one that is better for working with SD. Prompting SD to get output at the MJ quality level takes significantly more tokens, lots of refinement, heavy tweaking of negative prompting, etc. Also a stack of embeddings and LoRAs, though I would place those more in the category of finetuning like you had mentioned.</div><br/><div id="39361575" class="c"><input type="checkbox" id="c-39361575" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361353">parent</a><span>|</span><a href="#39362404">next</a><span>|</span><label class="collapse" for="c-39361575">[-]</label><label class="expand" for="c-39361575">[2 more]</label></div><br/><div class="children"><div class="content">If you try diffusionGPT with regional prompting added and a GAN corrector you can get a good idea of what is possible <a href="https:&#x2F;&#x2F;diffusiongpt.github.io" rel="nofollow">https:&#x2F;&#x2F;diffusiongpt.github.io</a></div><br/><div id="39362947" class="c"><input type="checkbox" id="c-39362947" checked=""/><div class="controls bullet"><span class="by">euazOn</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361575">parent</a><span>|</span><a href="#39362404">next</a><span>|</span><label class="collapse" for="c-39362947">[-]</label><label class="expand" for="c-39362947">[1 more]</label></div><br/><div class="children"><div class="content">That looks very impressive unless the demo is cherrypicked, would be great if this could be implemented into a frontend like Fooocus <a href="https:&#x2F;&#x2F;github.com&#x2F;lllyasviel&#x2F;Fooocus">https:&#x2F;&#x2F;github.com&#x2F;lllyasviel&#x2F;Fooocus</a></div><br/></div></div></div></div><div id="39362404" class="c"><input type="checkbox" id="c-39362404" checked=""/><div class="controls bullet"><span class="by">millgrove</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361353">parent</a><span>|</span><a href="#39361575">prev</a><span>|</span><a href="#39362541">next</a><span>|</span><label class="collapse" for="c-39362404">[-]</label><label class="expand" for="c-39362404">[5 more]</label></div><br/><div class="children"><div class="content">What do you use it for? I haven&#x27;t found a great use for it myself (outside of generating assets for landing pages &#x2F; apps, where it&#x27;s really really good). But I have seen endless subreddits &#x2F; instagram pages dedicated to various forms of AI content, so it seems lots of people are using it for fun?</div><br/><div id="39362969" class="c"><input type="checkbox" id="c-39362969" checked=""/><div class="controls bullet"><span class="by">cthalupa</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39362404">parent</a><span>|</span><a href="#39362541">next</a><span>|</span><label class="collapse" for="c-39362969">[-]</label><label class="expand" for="c-39362969">[4 more]</label></div><br/><div class="children"><div class="content">Nothing professional. I run a variety of tabletop RPGs for friends, so I mostly use it for making visual aids there. I&#x27;ve also got a large format printer that I was no longer using for it&#x27;s original purpose, so I bought a few front-loading art frames that I generate art for and rotate through periodically.<p>I&#x27;ve also used it to generate art for deskmats I got printed at <a href="https:&#x2F;&#x2F;specterlabs.co&#x2F;" rel="nofollow">https:&#x2F;&#x2F;specterlabs.co&#x2F;</a><p>For commercial stuff I still pay human artists.</div><br/><div id="39364562" class="c"><input type="checkbox" id="c-39364562" checked=""/><div class="controls bullet"><span class="by">throwanem</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39362969">parent</a><span>|</span><a href="#39362541">next</a><span>|</span><label class="collapse" for="c-39364562">[-]</label><label class="expand" for="c-39364562">[3 more]</label></div><br/><div class="children"><div class="content">Whose frames do you use? Do you like them? I print my photos to frame and hang, and wouldn&#x27;t at all mind being able to rotate them more conveniently and inexpensively than dedicating a frame to each allows.</div><br/><div id="39366221" class="c"><input type="checkbox" id="c-39366221" checked=""/><div class="controls bullet"><span class="by">cthalupa</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39364562">parent</a><span>|</span><a href="#39362541">next</a><span>|</span><label class="collapse" for="c-39366221">[-]</label><label class="expand" for="c-39366221">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.spotlightdisplays.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.spotlightdisplays.com&#x2F;</a><p>I like them quite a bit, and you can get basically any size cut to fit your needs even if they don&#x27;t directly offer it on the site.</div><br/><div id="39366314" class="c"><input type="checkbox" id="c-39366314" checked=""/><div class="controls bullet"><span class="by">throwanem</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39366221">parent</a><span>|</span><a href="#39362541">next</a><span>|</span><label class="collapse" for="c-39366314">[-]</label><label class="expand" for="c-39366314">[1 more]</label></div><br/><div class="children"><div class="content">Perfectly suited to go alongside the style of frame I already have lots of, and very reasonably priced off the shelf for the 13x19 my printer tops out at. Thanks so much! It&#x27;ll be easier to fill that one blank wall now.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39362541" class="c"><input type="checkbox" id="c-39362541" checked=""/><div class="controls bullet"><span class="by">soultrees</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361353">parent</a><span>|</span><a href="#39362404">prev</a><span>|</span><a href="#39365462">next</a><span>|</span><label class="collapse" for="c-39362541">[-]</label><label class="expand" for="c-39362541">[2 more]</label></div><br/><div class="children"><div class="content">What IRC Channels do you frequent?</div><br/><div id="39362915" class="c"><input type="checkbox" id="c-39362915" checked=""/><div class="controls bullet"><span class="by">cthalupa</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39362541">parent</a><span>|</span><a href="#39365462">next</a><span>|</span><label class="collapse" for="c-39362915">[-]</label><label class="expand" for="c-39362915">[1 more]</label></div><br/><div class="children"><div class="content">Largely some old channels from the 90s&#x2F;00s that really only exist as vestiges of their former selves - not really related to their original purpose, just rooms for hanging out with friends made there back when they had a point besides being a group chat.</div><br/></div></div></div></div></div></div><div id="39365462" class="c"><input type="checkbox" id="c-39365462" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361140">parent</a><span>|</span><a href="#39361353">prev</a><span>|</span><a href="#39362946">next</a><span>|</span><label class="collapse" for="c-39365462">[-]</label><label class="expand" for="c-39365462">[1 more]</label></div><br/><div class="children"><div class="content">Midjourney has absolutely nothing to offer compared to proper finetunes. DALL-E has: it generalizes well (can make objects interact properly for example) and has great prompt adherence. But it can also be unpredictable as hell because it rewrites the prompts. DALL-E&#x27;s quality is meh - it has terrible artifacts on all pixel-sized details, hallucinations on small details, and limited resolution. Controlnets, finetuning&#x2F;zero-shot reference transfer, and open tooling would have made a beast of a model of it, but they aren&#x27;t available.</div><br/></div></div><div id="39362946" class="c"><input type="checkbox" id="c-39362946" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361140">parent</a><span>|</span><a href="#39365462">prev</a><span>|</span><a href="#39361620">next</a><span>|</span><label class="collapse" for="c-39362946">[-]</label><label class="expand" for="c-39362946">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not really true, MJ and DALL-E are just more beginner friendly.</div><br/></div></div></div></div></div></div></div></div><div id="39361620" class="c"><input type="checkbox" id="c-39361620" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360896">parent</a><span>|</span><a href="#39360977">prev</a><span>|</span><a href="#39361003">next</a><span>|</span><label class="collapse" for="c-39361620">[-]</label><label class="expand" for="c-39361620">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;d be interesting to have a non-profit &quot;model sharing&quot; platform, where people can buy&#x2F;sell compute. When you run someone&#x27;s model, they get royalties on the compute you buy.</div><br/></div></div><div id="39361003" class="c"><input type="checkbox" id="c-39361003" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360896">parent</a><span>|</span><a href="#39361620">prev</a><span>|</span><a href="#39363246">next</a><span>|</span><label class="collapse" for="c-39361003">[-]</label><label class="expand" for="c-39361003">[1 more]</label></div><br/><div class="children"><div class="content">More specifically, it&#x27;s so Stability AI can theoretically make a business on selling commercial access to those models through a membership: <a href="https:&#x2F;&#x2F;stability.ai&#x2F;news&#x2F;introducing-stability-ai-membership" rel="nofollow">https:&#x2F;&#x2F;stability.ai&#x2F;news&#x2F;introducing-stability-ai-membershi...</a></div><br/></div></div><div id="39363246" class="c"><input type="checkbox" id="c-39363246" checked=""/><div class="controls bullet"><span class="by">thatguysaguy</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360896">parent</a><span>|</span><a href="#39361003">prev</a><span>|</span><a href="#39360963">next</a><span>|</span><label class="collapse" for="c-39363246">[-]</label><label class="expand" for="c-39363246">[1 more]</label></div><br/><div class="children"><div class="content">The net flow of knowledge about text-to-image generation from OpenAI has definitely been outward. The early open source methods used CLIP, which OpenAI came up with. Dall-e (1) was also the first demonstration that we could do text to image at all. (There were some earlier papers which could give you a red splotch if you said stop sign or something years earlier).</div><br/></div></div><div id="39360963" class="c"><input type="checkbox" id="c-39360963" checked=""/><div class="controls bullet"><span class="by">yogorenapan</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360896">parent</a><span>|</span><a href="#39363246">prev</a><span>|</span><a href="#39361197">next</a><span>|</span><label class="collapse" for="c-39360963">[-]</label><label class="expand" for="c-39360963">[9 more]</label></div><br/><div class="children"><div class="content">&gt; AI desperately needs a GPL equivalent<p>Why not just the GPL then?</div><br/><div id="39361408" class="c"><input type="checkbox" id="c-39361408" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39360963">parent</a><span>|</span><a href="#39361197">next</a><span>|</span><label class="collapse" for="c-39361408">[-]</label><label class="expand" for="c-39361408">[8 more]</label></div><br/><div class="children"><div class="content">The GPL was intended for computer code that gets compiled to a binary form.  You can share the binary, but you also have to share the code that the binary is compiled from.  Pre-trained model weights might be thought of as analogous to compiled code, and the training data may be analogous to program code, but they&#x27;re not the same thing.<p>The model weights are shared openly, but the training data used to create these models isn&#x27;t.  This is at least partly because all these models, including OpenAI&#x27;s, are trained on copyrighted data, so the copyright status of the models themselves is somewhat murky.<p>In the future we may see models that are 100% trained in the open, but foundational models are currently very expensive to train from scratch.  Either prices would need to come down, or enthusiasts will need some way to share radically distributed GPU resources.</div><br/><div id="39361559" class="c"><input type="checkbox" id="c-39361559" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361408">parent</a><span>|</span><a href="#39362790">next</a><span>|</span><label class="collapse" for="c-39361559">[-]</label><label class="expand" for="c-39361559">[6 more]</label></div><br/><div class="children"><div class="content">Tbh I think these models will largely be trained on synthetic datasets in the future. They are mostly trained on garbage now. We have been doing opt outs on these, has been interesting to see quality differential (or lack thereof), eg removing books3 from stableLM 3b zephyr <a href="https:&#x2F;&#x2F;stability.wandb.io&#x2F;stability-llm&#x2F;stable-lm&#x2F;reports&#x2F;StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo" rel="nofollow">https:&#x2F;&#x2F;stability.wandb.io&#x2F;stability-llm&#x2F;stable-lm&#x2F;reports&#x2F;S...</a></div><br/><div id="39363264" class="c"><input type="checkbox" id="c-39363264" checked=""/><div class="controls bullet"><span class="by">keenmaster</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361559">parent</a><span>|</span><a href="#39363896">next</a><span>|</span><label class="collapse" for="c-39363264">[-]</label><label class="expand" for="c-39363264">[2 more]</label></div><br/><div class="children"><div class="content">Why aren’t the big models trained on synthetic datasets now? What’s the bottleneck? And how do you avoid amplifying the weaknesses of LLMs when you train on LLM output vs. novel material from the comparatively very intelligent members of the human species. Would be interesting to see your take on this.</div><br/><div id="39366336" class="c"><input type="checkbox" id="c-39366336" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39363264">parent</a><span>|</span><a href="#39363896">next</a><span>|</span><label class="collapse" for="c-39366336">[-]</label><label class="expand" for="c-39366336">[1 more]</label></div><br/><div class="children"><div class="content">We are starting to see that, see phi2 for example<p>There are approaches to get the right type of augmented and generated data to feed these models right, check out our QDAIF paper we worked on for example<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.13032.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.13032.pdf</a></div><br/></div></div></div></div><div id="39363896" class="c"><input type="checkbox" id="c-39363896" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361559">parent</a><span>|</span><a href="#39363264">prev</a><span>|</span><a href="#39362790">next</a><span>|</span><label class="collapse" for="c-39363896">[-]</label><label class="expand" for="c-39363896">[3 more]</label></div><br/><div class="children"><div class="content">I’ve wondered whether books3 makes a difference, and how much. If you ever train a model with a proper books3 ablation I’d be curious to know how it does. Books are an important data source, but if users find the model useful without them then that’s a good datapoint.</div><br/><div id="39364672" class="c"><input type="checkbox" id="c-39364672" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39363896">parent</a><span>|</span><a href="#39362790">next</a><span>|</span><label class="collapse" for="c-39364672">[-]</label><label class="expand" for="c-39364672">[2 more]</label></div><br/><div class="children"><div class="content">We did try stableLM 3b4 with books3 and it got worse in general and benchmarks<p>Just did some pes2o ablations too which were eh</div><br/><div id="39364897" class="c"><input type="checkbox" id="c-39364897" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39364672">parent</a><span>|</span><a href="#39362790">next</a><span>|</span><label class="collapse" for="c-39364897">[-]</label><label class="expand" for="c-39364897">[1 more]</label></div><br/><div class="children"><div class="content">What I mean is, it’s important to train a model with <i>and</i> without books3. That’s the only way to know whether it was books3 itself causing the issue, or some artifact of the training process.<p>One thing that’s hard to measure is the knowledge contained in books3. If someone asks about certain books, it won’t be able to give an answer unless the knowledge is there in some form. I’ve often wondered whether scraping the internet is enough rather than training on books directly.<p>But be careful about relying too much on evals. Ultimately the only benchmark that matters is whether users find the model useful. The clearest test of this would be to train two models side by side, with and without books3, and then ask some people which they prefer.<p>It’s really tricky to get all of this right. But if there’s more details on the pes2o ablations I’d be curious to see.</div><br/></div></div></div></div></div></div></div></div><div id="39362790" class="c"><input type="checkbox" id="c-39362790" checked=""/><div class="controls bullet"><span class="by">protomikron</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361408">parent</a><span>|</span><a href="#39361559">prev</a><span>|</span><a href="#39361197">next</a><span>|</span><label class="collapse" for="c-39362790">[-]</label><label class="expand" for="c-39362790">[1 more]</label></div><br/><div class="children"><div class="content">What about CC licenses for model weights? It&#x27;s common for files (&quot;images&quot;, &quot;video&quot;, &quot;audio&quot;, ...) So maybe appropriate.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39361197" class="c"><input type="checkbox" id="c-39361197" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#39360630">parent</a><span>|</span><a href="#39360749">prev</a><span>|</span><a href="#39361060">next</a><span>|</span><label class="collapse" for="c-39361197">[-]</label><label class="expand" for="c-39361197">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen Emad (Stability AI founder) commenting here on HN somewhere about this before, what exactly their business model is&#x2F;will be, and similar thoughts.<p>HN search doesn&#x27;t seem to agree with me today though and I cannot find the specific comment&#x2F;s I have in mind, maybe someone else has any luck? This is their user <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;user?id=emadm">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;user?id=emadm</a></div><br/><div id="39361501" class="c"><input type="checkbox" id="c-39361501" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361197">parent</a><span>|</span><a href="#39361060">next</a><span>|</span><label class="collapse" for="c-39361501">[-]</label><label class="expand" for="c-39361501">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;x.com&#x2F;EMostaque&#x2F;status&#x2F;1649152422634221593?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;EMostaque&#x2F;status&#x2F;1649152422634221593?s=20</a><p>We now have top models of every type, sites like www.stableaudio.com, memberships, custom model deals etc so lots of demand<p>We&#x27;re the only AI company that can make a model of any type for anyone from scratch &amp; are the most liked &#x2F; one of the most downloaded on HuggingFace (<a href="https:&#x2F;&#x2F;x.com&#x2F;Jarvis_Data&#x2F;status&#x2F;1730394474285572148?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;Jarvis_Data&#x2F;status&#x2F;1730394474285572148?s=20</a>, <a href="https:&#x2F;&#x2F;x.com&#x2F;EMostaque&#x2F;status&#x2F;1727055672057962634?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;EMostaque&#x2F;status&#x2F;1727055672057962634?s=20</a>)<p>Its going ok, team working hard and shipping good models, the team are accelerating their work on building ComfyUI to bring it all together.<p>My favourite recent model was CheXagent, I think medical models should be open &amp; will really save lives: <a href="https:&#x2F;&#x2F;x.com&#x2F;Kseniase_&#x2F;status&#x2F;1754575702824038717?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;Kseniase_&#x2F;status&#x2F;1754575702824038717?s=20</a></div><br/></div></div></div></div><div id="39361060" class="c"><input type="checkbox" id="c-39361060" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#39360630">parent</a><span>|</span><a href="#39361197">prev</a><span>|</span><a href="#39361165">next</a><span>|</span><label class="collapse" for="c-39361060">[-]</label><label class="expand" for="c-39361060">[2 more]</label></div><br/><div class="children"><div class="content">exactly my thought. stability should be receiving research grants</div><br/><div id="39361538" class="c"><input type="checkbox" id="c-39361538" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361060">parent</a><span>|</span><a href="#39361165">next</a><span>|</span><label class="collapse" for="c-39361538">[-]</label><label class="expand" for="c-39361538">[1 more]</label></div><br/><div class="children"><div class="content">We should, we haven&#x27;t yet...<p>Instead we&#x27;ve given 10m+ supercomputer hours in grants to all sorts of projects, now we have our grant team in place &amp; there is a huge increase in available funding for folk that can actually build stuff we can tap into.</div><br/></div></div></div></div><div id="39361165" class="c"><input type="checkbox" id="c-39361165" checked=""/><div class="controls bullet"><span class="by">sveme</span><span>|</span><a href="#39360630">parent</a><span>|</span><a href="#39361060">prev</a><span>|</span><a href="#39361147">next</a><span>|</span><label class="collapse" for="c-39361165">[-]</label><label class="expand" for="c-39361165">[2 more]</label></div><br/><div class="children"><div class="content">None of the researchers are associated with stability.ai, but with universities in Germany and Canada. How does this work? Is this exclusive work for stability.ai?</div><br/><div id="39361426" class="c"><input type="checkbox" id="c-39361426" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360630">root</a><span>|</span><a href="#39361165">parent</a><span>|</span><a href="#39361147">next</a><span>|</span><label class="collapse" for="c-39361426">[-]</label><label class="expand" for="c-39361426">[1 more]</label></div><br/><div class="children"><div class="content">Dom and Pablo both work for Stability AI (Dom finishing his degree).<p>All the original Stable Diffusion researchers (Robin Rombach, Patrick Esser, Dominik Lorenz, Andreas Blattman) also work for Stability AI.</div><br/></div></div></div></div><div id="39361147" class="c"><input type="checkbox" id="c-39361147" checked=""/><div class="controls bullet"><span class="by">downrightmike</span><span>|</span><a href="#39360630">parent</a><span>|</span><a href="#39361165">prev</a><span>|</span><a href="#39360722">next</a><span>|</span><label class="collapse" for="c-39361147">[-]</label><label class="expand" for="c-39361147">[1 more]</label></div><br/><div class="children"><div class="content">Finally a good use to burn VC money!</div><br/></div></div></div></div><div id="39360722" class="c"><input type="checkbox" id="c-39360722" checked=""/><div class="controls bullet"><span class="by">yogorenapan</span><span>|</span><a href="#39360630">prev</a><span>|</span><a href="#39366386">next</a><span>|</span><label class="collapse" for="c-39360722">[-]</label><label class="expand" for="c-39360722">[10 more]</label></div><br/><div class="children"><div class="content">I see in the commits that the license was changed from MIT to their own custom one: <a href="https:&#x2F;&#x2F;github.com&#x2F;Stability-AI&#x2F;StableCascade&#x2F;commit&#x2F;209a52600f35dfe2a205daef54c0ff4068e86bc7">https:&#x2F;&#x2F;github.com&#x2F;Stability-AI&#x2F;StableCascade&#x2F;commit&#x2F;209a526...</a><p>Is it legal to use an older snapshot before the license was changed in accordance with the previous MIT license?</div><br/><div id="39361421" class="c"><input type="checkbox" id="c-39361421" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#39360722">parent</a><span>|</span><a href="#39360805">next</a><span>|</span><label class="collapse" for="c-39361421">[-]</label><label class="expand" for="c-39361421">[4 more]</label></div><br/><div class="children"><div class="content">It seems pretty clear the intent was to use a non-commercial license, so it’s probably something that would go to court, if you really wanted to press the issue.<p>Generally courts are more holistic and look at intent, and understand that clerical errors happen. One exception to this is if a business claims it relied on the previous license and invested a bunch of resources as a result.<p>I believe the timing of commits is pretty important— it would be hard to claim your business made a substantial investment on a pre-announcement repo that was only MIT’ed for a few hours.</div><br/><div id="39364523" class="c"><input type="checkbox" id="c-39364523" checked=""/><div class="controls bullet"><span class="by">RIMR</span><span>|</span><a href="#39360722">root</a><span>|</span><a href="#39361421">parent</a><span>|</span><a href="#39360805">next</a><span>|</span><label class="collapse" for="c-39364523">[-]</label><label class="expand" for="c-39364523">[3 more]</label></div><br/><div class="children"><div class="content">If I clone&#x2F;fork that repo before the license change, and start putting any amount of time into developing my own fork in good faith, they shouldn&#x27;t be allowed to claim a clerical error when they lied to me upon delivery about what I was allowed to do with the code.<p>Licenses are important. If you are going to expose your code to the world, make sure it has the right license. If you publish your code with the wrong license, you shouldn&#x27;t be allowed to take it back. Not for an organization of this size that is going to see a new repo cloned thousands of times upon release.</div><br/><div id="39367483" class="c"><input type="checkbox" id="c-39367483" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39360722">root</a><span>|</span><a href="#39364523">parent</a><span>|</span><a href="#39364609">next</a><span>|</span><label class="collapse" for="c-39367483">[-]</label><label class="expand" for="c-39367483">[1 more]</label></div><br/><div class="children"><div class="content">No, sadly this won’t fly in court.<p>For the same reason you cannot publish a private corporate repo with an MIT license and then have other people claim in “good faith” to be using it.<p>All they need is to assert that the license was published in error, or that the person publishing it did not have the authority to publish it.<p>You can’t “magically” make a license stick by putting it in a repo, any more than putting a “name here” sticker on someone’s car and then claiming to own it.<p>The license file in the repo is simply the <i>notice</i> of the license.<p>It does not indicate a binding legal agreement.<p>You of course, can challenge it in court, and ianal, but I assure you, there is president in incorrectly labelled repos removing and changing their licenses.</div><br/></div></div><div id="39364609" class="c"><input type="checkbox" id="c-39364609" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#39360722">root</a><span>|</span><a href="#39364523">parent</a><span>|</span><a href="#39367483">prev</a><span>|</span><a href="#39360805">next</a><span>|</span><label class="collapse" for="c-39364609">[-]</label><label class="expand" for="c-39364609">[1 more]</label></div><br/><div class="children"><div class="content">There’s no case law here, so if you’re volunteering to find out what a judge thinks we’d surely appreciate it!</div><br/></div></div></div></div></div></div><div id="39360805" class="c"><input type="checkbox" id="c-39360805" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#39360722">parent</a><span>|</span><a href="#39361421">prev</a><span>|</span><a href="#39360807">next</a><span>|</span><label class="collapse" for="c-39360805">[-]</label><label class="expand" for="c-39360805">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you can continue to do what you want with that commit^ in accordance with the MIT licence it was released under. Kind of like if you buy an ebook, and then they publish a second edition but only as a hardback - the first edition ebook is still yours to read.</div><br/></div></div><div id="39360807" class="c"><input type="checkbox" id="c-39360807" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39360722">parent</a><span>|</span><a href="#39360805">prev</a><span>|</span><a href="#39364494">next</a><span>|</span><label class="collapse" for="c-39360807">[-]</label><label class="expand" for="c-39360807">[2 more]</label></div><br/><div class="children"><div class="content">I think the model architecture (training code etc.) itself is still under MIT while the weights (which was the result of training in a huge GPU cluster as well as the dataset they have used [not sure if they publicly talked about it] is under this new license.</div><br/><div id="39361513" class="c"><input type="checkbox" id="c-39361513" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39360722">root</a><span>|</span><a href="#39360807">parent</a><span>|</span><a href="#39364494">next</a><span>|</span><label class="collapse" for="c-39361513">[-]</label><label class="expand" for="c-39361513">[1 more]</label></div><br/><div class="children"><div class="content">Code is MIT, weights are under the NC license for now.</div><br/></div></div></div></div><div id="39364494" class="c"><input type="checkbox" id="c-39364494" checked=""/><div class="controls bullet"><span class="by">RIMR</span><span>|</span><a href="#39360722">parent</a><span>|</span><a href="#39360807">prev</a><span>|</span><a href="#39366386">next</a><span>|</span><label class="collapse" for="c-39364494">[-]</label><label class="expand" for="c-39364494">[2 more]</label></div><br/><div class="children"><div class="content">MIT license is not parasitic like GPL. You can close an MIT licensed codebase, but you cannot retroactively change the license of the old code.<p>Stability&#x27;s initial commit had an MIT license, so you can fork that commit and do whatever you want with it. It&#x27;s MIT licensed.<p>Now, the tricky part here is that they committed a change to the license that changes it from MIT to proprietary, but they didn&#x27;t change any code with it. That is definitely invalid, because they cannot license the exact same codebase with two different contradictory licenses. They can only license the changes made to the codebase after the license change. I wouldn&#x27;t call it &quot;illegal&quot;, but it wouldn&#x27;t stand up in court if they tried to claim that the software is proprietary, because they already distributed it verbatim with an open license.</div><br/><div id="39364620" class="c"><input type="checkbox" id="c-39364620" checked=""/><div class="controls bullet"><span class="by">kruuuder</span><span>|</span><a href="#39360722">root</a><span>|</span><a href="#39364494">parent</a><span>|</span><a href="#39366386">next</a><span>|</span><label class="collapse" for="c-39364620">[-]</label><label class="expand" for="c-39364620">[1 more]</label></div><br/><div class="children"><div class="content">&gt; they didn&#x27;t change any code with it. That is definitely invalid, because they cannot license the exact same codebase with two different contradictory licenses.<p>Why couldn&#x27;t they? Of course they can. If you are the copyright owner, you can publish&#x2F;sell your stuff under as many licenses as you like.</div><br/></div></div></div></div></div></div><div id="39366386" class="c"><input type="checkbox" id="c-39366386" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#39360722">prev</a><span>|</span><a href="#39360800">next</a><span>|</span><label class="collapse" for="c-39366386">[-]</label><label class="expand" for="c-39366386">[2 more]</label></div><br/><div class="children"><div class="content">Like every other image generator I&#x27;ve tried, it can&#x27;t do a piano keyboard [1]. I expect that some different approach is needed to be able to count the black keys groups.<p>[1] <a href="https:&#x2F;&#x2F;fal.ai&#x2F;models&#x2F;stable-cascade?share=13d35b76-d32f-45ca-be08-25d2fcb7349d" rel="nofollow">https:&#x2F;&#x2F;fal.ai&#x2F;models&#x2F;stable-cascade?share=13d35b76-d32f-45c...</a></div><br/><div id="39366966" class="c"><input type="checkbox" id="c-39366966" checked=""/><div class="controls bullet"><span class="by">Agraillo</span><span>|</span><a href="#39366386">parent</a><span>|</span><a href="#39360800">next</a><span>|</span><label class="collapse" for="c-39366966">[-]</label><label class="expand" for="c-39366966">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s more than this. In my case in most of images I made about basketball there were more than one ball. I&#x27;m not an expert, but some fundamental constrains of the human (cultural) life (like all piano keys are the same, there&#x27;s only one ball in a game) are not grasped by the training or grasped partially</div><br/></div></div></div></div><div id="39360800" class="c"><input type="checkbox" id="c-39360800" checked=""/><div class="controls bullet"><span class="by">gorkemyurt</span><span>|</span><a href="#39366386">prev</a><span>|</span><a href="#39367504">next</a><span>|</span><label class="collapse" for="c-39360800">[-]</label><label class="expand" for="c-39360800">[6 more]</label></div><br/><div class="children"><div class="content">we have an optimized playground here: <a href="https:&#x2F;&#x2F;www.fal.ai&#x2F;models&#x2F;stable-cascade" rel="nofollow">https:&#x2F;&#x2F;www.fal.ai&#x2F;models&#x2F;stable-cascade</a></div><br/><div id="39361562" class="c"><input type="checkbox" id="c-39361562" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#39360800">parent</a><span>|</span><a href="#39367504">next</a><span>|</span><label class="collapse" for="c-39361562">[-]</label><label class="expand" for="c-39361562">[5 more]</label></div><br/><div class="children"><div class="content">&quot;sign in to run&quot;<p>That&#x27;s a marketing opportunity being missed, especially given how crowded the space is now. The HN crowd is more likely to run it themselves when presented with signing up just to test out a single generation.</div><br/><div id="39361881" class="c"><input type="checkbox" id="c-39361881" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39360800">root</a><span>|</span><a href="#39361562">parent</a><span>|</span><a href="#39361882">next</a><span>|</span><label class="collapse" for="c-39361881">[-]</label><label class="expand" for="c-39361881">[2 more]</label></div><br/><div class="children"><div class="content">Uh, thanks for noticing it! We generally turn it off for popular models so people can see the underlying inference speed and the results but we forgot about it for this one, it should now be auth-less with a stricter rate limit just like other popular models in the gallery.</div><br/><div id="39364433" class="c"><input type="checkbox" id="c-39364433" checked=""/><div class="controls bullet"><span class="by">RIMR</span><span>|</span><a href="#39360800">root</a><span>|</span><a href="#39361881">parent</a><span>|</span><a href="#39361882">next</a><span>|</span><label class="collapse" for="c-39364433">[-]</label><label class="expand" for="c-39364433">[1 more]</label></div><br/><div class="children"><div class="content">I just got rate-limited on my first generation. The message is &quot;You have exceeded the request limit per minute&quot;. This was after showing me cli output suggesting that my image was being generated.<p>I guess my zero attempts per minute was too much. You really shouldn&#x27;t post your product on HN if you aren&#x27;t prepared for it to work. Reputations are hard to earn, and you&#x27;re losing people&#x27;s interest by directing them to a broken product.</div><br/></div></div></div></div><div id="39361882" class="c"><input type="checkbox" id="c-39361882" checked=""/><div class="controls bullet"><span class="by">MattRix</span><span>|</span><a href="#39360800">root</a><span>|</span><a href="#39361562">parent</a><span>|</span><a href="#39361881">prev</a><span>|</span><a href="#39367504">next</a><span>|</span><label class="collapse" for="c-39361882">[-]</label><label class="expand" for="c-39361882">[2 more]</label></div><br/><div class="children"><div class="content">It uses github auth, it’s not some complex process. I can see why they would need to require accounts so it’s harder to abuse it.</div><br/><div id="39367533" class="c"><input type="checkbox" id="c-39367533" checked=""/><div class="controls bullet"><span class="by">arcanemachiner</span><span>|</span><a href="#39360800">root</a><span>|</span><a href="#39361882">parent</a><span>|</span><a href="#39367504">next</a><span>|</span><label class="collapse" for="c-39367533">[-]</label><label class="expand" for="c-39367533">[1 more]</label></div><br/><div class="children"><div class="content">After all the bellyaching from the HN crowd when PyPI started requiring 2FA, nothing surprises me anymore.</div><br/></div></div></div></div></div></div></div></div><div id="39367504" class="c"><input type="checkbox" id="c-39367504" checked=""/><div class="controls bullet"><span class="by">instagraham</span><span>|</span><a href="#39360800">prev</a><span>|</span><a href="#39365271">next</a><span>|</span><label class="collapse" for="c-39367504">[-]</label><label class="expand" for="c-39367504">[1 more]</label></div><br/><div class="children"><div class="content">Will this work on AMD? Found no mention of support.
Kinda an important feature for such a project, as AMD users running Stable Diffusion will be suffering diminished performance.</div><br/></div></div><div id="39365271" class="c"><input type="checkbox" id="c-39365271" checked=""/><div class="controls bullet"><span class="by">pxoe</span><span>|</span><a href="#39367504">prev</a><span>|</span><a href="#39366315">next</a><span>|</span><label class="collapse" for="c-39365271">[-]</label><label class="expand" for="c-39365271">[4 more]</label></div><br/><div class="children"><div class="content">the way it&#x27;s written about in Image Reconstruction section like it is just an image compression thing...is kind of interesting. for that stuff and its presented use there to be very much about storing images and reconstructing them. when &quot;it doesn&#x27;t actually store original images&quot; and &quot;it can&#x27;t actually give out original images&quot; are points that get used so often in arguments as a defense for image generators. 
so it is just a multi-image compression file format, just a very efficient one. sure, it&#x27;s &quot;redrawing&quot;&#x2F;&quot;rendering&quot; its output and makes things look kinda fuzzy, but any other compressed image format does that as well.
what was all that &#x27;well it doesn&#x27;t do those things&#x27; nonsense about then? clearly it can do that.</div><br/><div id="39365460" class="c"><input type="checkbox" id="c-39365460" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#39365271">parent</a><span>|</span><a href="#39365334">next</a><span>|</span><label class="collapse" for="c-39365460">[-]</label><label class="expand" for="c-39365460">[1 more]</label></div><br/><div class="children"><div class="content">Ultimately this is abstraction not compression.</div><br/></div></div><div id="39365334" class="c"><input type="checkbox" id="c-39365334" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39365271">parent</a><span>|</span><a href="#39365460">prev</a><span>|</span><a href="#39366315">next</a><span>|</span><label class="collapse" for="c-39365334">[-]</label><label class="expand" for="c-39365334">[2 more]</label></div><br/><div class="children"><div class="content">In a way it&#x27;s just an algorithm than can compress either text or an image. The neat trick is that if you compress the text &quot;brown bear hitting Vladimir Putin&quot; and then decompress it as an image, you get an image of a bear hitting Vladimir Putin.<p>This principle is the idea behind all Stable Diffusion models, this one &quot;just&quot; achieved a much better compression ratio</div><br/><div id="39365386" class="c"><input type="checkbox" id="c-39365386" checked=""/><div class="controls bullet"><span class="by">pxoe</span><span>|</span><a href="#39365271">root</a><span>|</span><a href="#39365334">parent</a><span>|</span><a href="#39366315">next</a><span>|</span><label class="collapse" for="c-39365386">[-]</label><label class="expand" for="c-39365386">[1 more]</label></div><br/><div class="children"><div class="content">well yeah. but it&#x27;s not so much about what it actually does, but how they talk about it. maybe (probably) i missed them putting out something that&#x27;s described like that before, but it&#x27;s just the open admission in demonstration of it. i guess they&#x27;re getting more brazen, given than they&#x27;re not really getting punished for what they&#x27;re doing, be it piracy or infringement or whatever.</div><br/></div></div></div></div></div></div><div id="39366315" class="c"><input type="checkbox" id="c-39366315" checked=""/><div class="controls bullet"><span class="by">lqcfcjx</span><span>|</span><a href="#39365271">prev</a><span>|</span><a href="#39363363">next</a><span>|</span><label class="collapse" for="c-39366315">[-]</label><label class="expand" for="c-39366315">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m very impressed by the recent AI progress on making models smaller and more efficient. I just have the feeling that every week there&#x27;s something big on this space (like what we saw previously from ollama, llava, mixtral...). Apparently the space for on-device models are not fully discovered yet. Very excited to see future products on that direction.</div><br/><div id="39366491" class="c"><input type="checkbox" id="c-39366491" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#39366315">parent</a><span>|</span><a href="#39363363">next</a><span>|</span><label class="collapse" for="c-39366491">[-]</label><label class="expand" for="c-39366491">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m very impressed by the recent AI progress on making models smaller and more efficient.<p>That&#x27;s an odd comment to place in a thread about an image generation model that is bigger than SDXL. Yes, it works in a smaller latent space, yes its faster in the hardware configuration they&#x27;ve used, but its not <i>smaller</i>.</div><br/></div></div></div></div><div id="39363363" class="c"><input type="checkbox" id="c-39363363" checked=""/><div class="controls bullet"><span class="by">hncomb</span><span>|</span><a href="#39366315">prev</a><span>|</span><a href="#39365465">next</a><span>|</span><label class="collapse" for="c-39363363">[-]</label><label class="expand" for="c-39363363">[5 more]</label></div><br/><div class="children"><div class="content">Is there any way this can be used to generate multiple images of the same model? e.g. a car model rotated around (but all images are of the same generated car)</div><br/><div id="39363949" class="c"><input type="checkbox" id="c-39363949" checked=""/><div class="controls bullet"><span class="by">matroid</span><span>|</span><a href="#39363363">parent</a><span>|</span><a href="#39364176">next</a><span>|</span><label class="collapse" for="c-39363949">[-]</label><label class="expand" for="c-39363949">[2 more]</label></div><br/><div class="children"><div class="content">Someone with resources will have to train Zero123 [1] with this backbone.<p>[1] <a href="https:&#x2F;&#x2F;zero123.cs.columbia.edu&#x2F;" rel="nofollow">https:&#x2F;&#x2F;zero123.cs.columbia.edu&#x2F;</a></div><br/><div id="39366356" class="c"><input type="checkbox" id="c-39366356" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39363363">root</a><span>|</span><a href="#39363949">parent</a><span>|</span><a href="#39364176">next</a><span>|</span><label class="collapse" for="c-39366356">[-]</label><label class="expand" for="c-39366356">[1 more]</label></div><br/><div class="children"><div class="content">Heh <a href="https:&#x2F;&#x2F;stability.ai&#x2F;news&#x2F;stable-zero123-3d-generation" rel="nofollow">https:&#x2F;&#x2F;stability.ai&#x2F;news&#x2F;stable-zero123-3d-generation</a><p>Better coming</div><br/></div></div></div></div><div id="39364176" class="c"><input type="checkbox" id="c-39364176" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39363363">parent</a><span>|</span><a href="#39363949">prev</a><span>|</span><a href="#39365465">next</a><span>|</span><label class="collapse" for="c-39364176">[-]</label><label class="expand" for="c-39364176">[2 more]</label></div><br/><div class="children"><div class="content">Yes, input image =&gt; embedding =&gt; N images, and if you&#x27;re thinking 3D perspectives for rendering, you&#x27;d ControlNet the N.<p>ref.: &quot;The model can also understand image embeddings, which makes it possible to generate variations of a given image (left). There was no prompt given here.&quot;</div><br/><div id="39366283" class="c"><input type="checkbox" id="c-39366283" checked=""/><div class="controls bullet"><span class="by">taejavu</span><span>|</span><a href="#39363363">root</a><span>|</span><a href="#39364176">parent</a><span>|</span><a href="#39365465">next</a><span>|</span><label class="collapse" for="c-39366283">[-]</label><label class="expand" for="c-39366283">[1 more]</label></div><br/><div class="children"><div class="content">The model looks different in each of those variations though. Which seems to be intentional, but the post you&#x27;re responding to is asking whether it&#x27;s possible to keep the model exactly the same in each render, varying only by perspective.</div><br/></div></div></div></div></div></div><div id="39365465" class="c"><input type="checkbox" id="c-39365465" checked=""/><div class="controls bullet"><span class="by">mise_en_place</span><span>|</span><a href="#39363363">prev</a><span>|</span><a href="#39365923">next</a><span>|</span><label class="collapse" for="c-39365465">[-]</label><label class="expand" for="c-39365465">[1 more]</label></div><br/><div class="children"><div class="content">Was anyone able to get this running on Colab? I got as far as loading extras in text-to-inference, but it was complaining about a dependency.</div><br/></div></div><div id="39365923" class="c"><input type="checkbox" id="c-39365923" checked=""/><div class="controls bullet"><span class="by">SECourses</span><span>|</span><a href="#39365465">prev</a><span>|</span><a href="#39361722">next</a><span>|</span><label class="collapse" for="c-39365923">[-]</label><label class="expand" for="c-39365923">[1 more]</label></div><br/><div class="children"><div class="content">It is pretty good I shared a comparison on medium<p><a href="https:&#x2F;&#x2F;medium.com&#x2F;@furkangozukara&#x2F;stable-cascade-prompt-following-is-amazing-this-model-has-huge-potential-high-resolutions-uses-29088c4ad171" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@furkangozukara&#x2F;stable-cascade-prompt-fol...</a><p>My Gradio APP even works amazing on 8 GB gpu with CPU offloading</div><br/></div></div><div id="39361722" class="c"><input type="checkbox" id="c-39361722" checked=""/><div class="controls bullet"><span class="by">joshelgar</span><span>|</span><a href="#39365923">prev</a><span>|</span><a href="#39366687">next</a><span>|</span><label class="collapse" for="c-39361722">[-]</label><label class="expand" for="c-39361722">[4 more]</label></div><br/><div class="children"><div class="content">Why are they benchmarking it with 20+10 steps vs. 50 steps for the other models?</div><br/><div id="39361822" class="c"><input type="checkbox" id="c-39361822" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39361722">parent</a><span>|</span><a href="#39362809">next</a><span>|</span><label class="collapse" for="c-39361822">[-]</label><label class="expand" for="c-39361822">[2 more]</label></div><br/><div class="children"><div class="content">prior generations usually take fewer steps than vanilla SDXL to reach the same quality.<p>But yeah, the inference speed improvement is mediocre (until I take a look at exactly what computation performed to have more informed opinion on whether it is implementation issue or model issue).<p>The prompt alignment should be better though. It looks like the model have more parameters to work with text conditioning.</div><br/><div id="39361967" class="c"><input type="checkbox" id="c-39361967" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39361722">root</a><span>|</span><a href="#39361822">parent</a><span>|</span><a href="#39362809">next</a><span>|</span><label class="collapse" for="c-39361967">[-]</label><label class="expand" for="c-39361967">[1 more]</label></div><br/><div class="children"><div class="content">in my observation, it yields amazing perf at higher batch sizes (4 or better 8). i assume it is due to memory bandwith and the constrained latent space helping.</div><br/></div></div></div></div><div id="39362809" class="c"><input type="checkbox" id="c-39362809" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39361722">parent</a><span>|</span><a href="#39361822">prev</a><span>|</span><a href="#39366687">next</a><span>|</span><label class="collapse" for="c-39362809">[-]</label><label class="expand" for="c-39362809">[1 more]</label></div><br/><div class="children"><div class="content">I think that this model used consistency loss during training so that it can yield better results with less steps.</div><br/></div></div></div></div><div id="39366687" class="c"><input type="checkbox" id="c-39366687" checked=""/><div class="controls bullet"><span class="by">sanroot99</span><span>|</span><a href="#39361722">prev</a><span>|</span><a href="#39362378">next</a><span>|</span><label class="collapse" for="c-39366687">[-]</label><label class="expand" for="c-39366687">[1 more]</label></div><br/><div class="children"><div class="content">What is the system requirements needed to run this, particularly how much vram it would take?</div><br/></div></div><div id="39362378" class="c"><input type="checkbox" id="c-39362378" checked=""/><div class="controls bullet"><span class="by">k2enemy</span><span>|</span><a href="#39366687">prev</a><span>|</span><a href="#39360975">next</a><span>|</span><label class="collapse" for="c-39362378">[-]</label><label class="expand" for="c-39362378">[8 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t been following the image generation space since the initial excitement around stable diffusion.  Is there an easy to use interface for the new models coming out?<p>I remember setting up the python env for stable diffusion, but then shortly after there were a host of nice GUIs.  Are there some popular GUIs that can be used to try out newer models?  Similarly, what&#x27;s the best GUI for some of the older models?  Preferably for macos.</div><br/><div id="39364482" class="c"><input type="checkbox" id="c-39364482" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39362378">parent</a><span>|</span><a href="#39364087">next</a><span>|</span><label class="collapse" for="c-39364482">[-]</label><label class="expand" for="c-39364482">[3 more]</label></div><br/><div class="children"><div class="content">Fooocus is the fastest way to try SDXL&#x2F;SDXL turbo with good quality.<p>ComfyUI is cool but very DIY. You don&#x27;t get good results unless you wrap your head around all the augmentations and defaults.<p>No idea if it will support cascade.</div><br/><div id="39365211" class="c"><input type="checkbox" id="c-39365211" checked=""/><div class="controls bullet"><span class="by">SpliffnCola</span><span>|</span><a href="#39362378">root</a><span>|</span><a href="#39364482">parent</a><span>|</span><a href="#39364087">next</a><span>|</span><label class="collapse" for="c-39365211">[-]</label><label class="expand" for="c-39365211">[2 more]</label></div><br/><div class="children"><div class="content">ComfyUI is similar to Houdini in complexity, but immensely powerful. It&#x27;s a joy to use.<p>There are also a large amount of resources available for it on YouTube, GitHub (<a href="https:&#x2F;&#x2F;github.com&#x2F;comfyanonymous&#x2F;ComfyUI_examples">https:&#x2F;&#x2F;github.com&#x2F;comfyanonymous&#x2F;ComfyUI_examples</a>), reddit (<a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;comfyui" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;comfyui</a>), CivitAI, Comfy Workflows (<a href="https:&#x2F;&#x2F;comfyworkflows.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;comfyworkflows.com&#x2F;</a>), and OpenArt Flow (<a href="https:&#x2F;&#x2F;openart.ai&#x2F;workflows&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openart.ai&#x2F;workflows&#x2F;</a>).<p>I still use AUTO1111 (<a href="https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui">https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui</a>) and the recently released and heavily modified fork of AUTO1111 called Forge (<a href="https:&#x2F;&#x2F;github.com&#x2F;lllyasviel&#x2F;stable-diffusion-webui-forge">https:&#x2F;&#x2F;github.com&#x2F;lllyasviel&#x2F;stable-diffusion-webui-forge</a>).</div><br/><div id="39366367" class="c"><input type="checkbox" id="c-39366367" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39362378">root</a><span>|</span><a href="#39365211">parent</a><span>|</span><a href="#39364087">next</a><span>|</span><label class="collapse" for="c-39366367">[-]</label><label class="expand" for="c-39366367">[1 more]</label></div><br/><div class="children"><div class="content">Our team at Stability AI build ComfyUI so yeah is supported</div><br/></div></div></div></div></div></div><div id="39364087" class="c"><input type="checkbox" id="c-39364087" checked=""/><div class="controls bullet"><span class="by">yokto</span><span>|</span><a href="#39362378">parent</a><span>|</span><a href="#39364482">prev</a><span>|</span><a href="#39362429">next</a><span>|</span><label class="collapse" for="c-39364087">[-]</label><label class="expand" for="c-39364087">[1 more]</label></div><br/><div class="children"><div class="content">fal.ai is nice and fast: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39360800">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39360800</a>
Both in performance and for how quickly they integrate new models apparently: they already support Stable Cascade.</div><br/></div></div><div id="39362429" class="c"><input type="checkbox" id="c-39362429" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#39362378">parent</a><span>|</span><a href="#39364087">prev</a><span>|</span><a href="#39360975">next</a><span>|</span><label class="collapse" for="c-39362429">[-]</label><label class="expand" for="c-39362429">[3 more]</label></div><br/><div class="children"><div class="content">Auto1111 and Comfy both get updated pretty quickly to support most of the new models coming out. I expect they&#x27;ll both support this soon.</div><br/><div id="39363144" class="c"><input type="checkbox" id="c-39363144" checked=""/><div class="controls bullet"><span class="by">stereobit</span><span>|</span><a href="#39362378">root</a><span>|</span><a href="#39362429">parent</a><span>|</span><a href="#39360975">next</a><span>|</span><label class="collapse" for="c-39363144">[-]</label><label class="expand" for="c-39363144">[2 more]</label></div><br/><div class="children"><div class="content">Check out invoke.com</div><br/><div id="39364263" class="c"><input type="checkbox" id="c-39364263" checked=""/><div class="controls bullet"><span class="by">sophrocyne</span><span>|</span><a href="#39362378">root</a><span>|</span><a href="#39363144">parent</a><span>|</span><a href="#39360975">next</a><span>|</span><label class="collapse" for="c-39364263">[-]</label><label class="expand" for="c-39364263">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for calling us out - I&#x27;m one of the maintainers.<p>Not entirely sure we&#x27;ll be in the Stable Cascade race quite yet. Since Auto&#x2F;Comfy aren&#x27;t really built for businesses, they&#x27;ll get it incorporated sooner vs later.<p>Invoke&#x27;s main focus is building open-source tools for the pros using this for work that are getting disrupted, and non-commercial licenses don&#x27;t really help the ones that are trying to follow the letter of the license.<p>Theoretically, since we&#x27;re just a deployment solution, it might come up with our larger customers who want us to run something they license from Stability, but we&#x27;ve had zero interest on any of the closed-license stuff so far.</div><br/></div></div></div></div></div></div></div></div><div id="39360696" class="c"><input type="checkbox" id="c-39360696" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#39360975">prev</a><span>|</span><a href="#39364891">next</a><span>|</span><label class="collapse" for="c-39360696">[-]</label><label class="expand" for="c-39360696">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say I&#x27;m most impressed by the compression.  Being able to compress an image 42x is huge for portable devices or bad internet connectivity (or both!).</div><br/><div id="39362347" class="c"><input type="checkbox" id="c-39362347" checked=""/><div class="controls bullet"><span class="by">seanalltogether</span><span>|</span><a href="#39360696">parent</a><span>|</span><a href="#39360812">next</a><span>|</span><label class="collapse" for="c-39362347">[-]</label><label class="expand" for="c-39362347">[3 more]</label></div><br/><div class="children"><div class="content">I have to imagine at this point someone is working toward a fast AI based video codec that comes with a small pretrained model and can operate in a limited memory environment like a tv to offer 8k resolution with low bandwidth.</div><br/><div id="39364861" class="c"><input type="checkbox" id="c-39364861" checked=""/><div class="controls bullet"><span class="by">Lord-Jobo</span><span>|</span><a href="#39360696">root</a><span>|</span><a href="#39362347">parent</a><span>|</span><a href="#39362409">next</a><span>|</span><label class="collapse" for="c-39364861">[-]</label><label class="expand" for="c-39364861">[1 more]</label></div><br/><div class="children"><div class="content">I am 65% sure this is already extremely similar to LGs upscaling approach in their most recent flagship</div><br/></div></div><div id="39362409" class="c"><input type="checkbox" id="c-39362409" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#39360696">root</a><span>|</span><a href="#39362347">parent</a><span>|</span><a href="#39364861">prev</a><span>|</span><a href="#39360812">next</a><span>|</span><label class="collapse" for="c-39362409">[-]</label><label class="expand" for="c-39362409">[1 more]</label></div><br/><div class="children"><div class="content">I would be shocked if Netflix was <i>not</i> working on that.</div><br/></div></div></div></div><div id="39360812" class="c"><input type="checkbox" id="c-39360812" checked=""/><div class="controls bullet"><span class="by">incrudible</span><span>|</span><a href="#39360696">parent</a><span>|</span><a href="#39362347">prev</a><span>|</span><a href="#39361058">next</a><span>|</span><label class="collapse" for="c-39360812">[-]</label><label class="expand" for="c-39360812">[3 more]</label></div><br/><div class="children"><div class="content">That is 42x <i>spatial</i> compression, but it needs 16 channels instead of 3 for RGB.</div><br/><div id="39362560" class="c"><input type="checkbox" id="c-39362560" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#39360696">root</a><span>|</span><a href="#39360812">parent</a><span>|</span><a href="#39361927">next</a><span>|</span><label class="collapse" for="c-39362560">[-]</label><label class="expand" for="c-39362560">[1 more]</label></div><br/><div class="children"><div class="content">Even assuming 32 bit floats (the extra 4 on the end):<p>4*16*24*24*4 = 147,456<p>vs (removing the alpha channel as it&#x27;s unused here)<p>3*3*1024*1024 = 9,437,184<p>Or 1&#x2F;64 raw size, assuming I haven&#x27;t fucked up the math&#x2F;understanding somewhere (very possible at the moment).</div><br/></div></div><div id="39361927" class="c"><input type="checkbox" id="c-39361927" checked=""/><div class="controls bullet"><span class="by">ansk</span><span>|</span><a href="#39360696">root</a><span>|</span><a href="#39360812">parent</a><span>|</span><a href="#39362560">prev</a><span>|</span><a href="#39361058">next</a><span>|</span><label class="collapse" for="c-39361927">[-]</label><label class="expand" for="c-39361927">[1 more]</label></div><br/><div class="children"><div class="content">Furthermore, each of those 16 channels would typically be mutibyte floats as opposed to single byte RGB channels.  (speaking generally, haven&#x27;t read the paper)</div><br/></div></div></div></div><div id="39361058" class="c"><input type="checkbox" id="c-39361058" checked=""/><div class="controls bullet"><span class="by">flgstnd</span><span>|</span><a href="#39360696">parent</a><span>|</span><a href="#39360812">prev</a><span>|</span><a href="#39364891">next</a><span>|</span><label class="collapse" for="c-39361058">[-]</label><label class="expand" for="c-39361058">[1 more]</label></div><br/><div class="children"><div class="content">a 42x compression is also impressive as it matches the answer to the ultimate question of life, the universe, and everything, maybe there is some deep universal truth within this model.</div><br/></div></div></div></div><div id="39364891" class="c"><input type="checkbox" id="c-39364891" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#39360696">prev</a><span>|</span><a href="#39361069">next</a><span>|</span><label class="collapse" for="c-39364891">[-]</label><label class="expand" for="c-39364891">[2 more]</label></div><br/><div class="children"><div class="content">Does anyone have a link to a demo online?</div><br/><div id="39365709" class="c"><input type="checkbox" id="c-39365709" checked=""/><div class="controls bullet"><span class="by">martin82</span><span>|</span><a href="#39364891">parent</a><span>|</span><a href="#39361069">next</a><span>|</span><label class="collapse" for="c-39365709">[-]</label><label class="expand" for="c-39365709">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;multimodalart&#x2F;stable-cascade" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;multimodalart&#x2F;stable-cascade</a></div><br/></div></div></div></div><div id="39361069" class="c"><input type="checkbox" id="c-39361069" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#39364891">prev</a><span>|</span><a href="#39360628">next</a><span>|</span><label class="collapse" for="c-39361069">[-]</label><label class="expand" for="c-39361069">[1 more]</label></div><br/><div class="children"><div class="content">Wow like the compression part. 42 fixed times compression. That is really nice. Slow to unpack on the fly. But the future is waiting.</div><br/></div></div><div id="39360628" class="c"><input type="checkbox" id="c-39360628" checked=""/><div class="controls bullet"><span class="by">ttpphd</span><span>|</span><a href="#39361069">prev</a><span>|</span><a href="#39362162">next</a><span>|</span><label class="collapse" for="c-39360628">[-]</label><label class="expand" for="c-39360628">[1 more]</label></div><br/><div class="children"><div class="content">That is a very tiny latent space. Wow!</div><br/></div></div><div id="39362162" class="c"><input type="checkbox" id="c-39362162" checked=""/><div class="controls bullet"><span class="by">gajnadsgjoas</span><span>|</span><a href="#39360628">prev</a><span>|</span><a href="#39361339">next</a><span>|</span><label class="collapse" for="c-39362162">[-]</label><label class="expand" for="c-39362162">[2 more]</label></div><br/><div class="children"><div class="content">Where can I run it if I don&#x27;t have a GPU? Colab didn&#x27;t work</div><br/><div id="39362325" class="c"><input type="checkbox" id="c-39362325" checked=""/><div class="controls bullet"><span class="by">detolly</span><span>|</span><a href="#39362162">parent</a><span>|</span><a href="#39361339">next</a><span>|</span><label class="collapse" for="c-39362325">[-]</label><label class="expand" for="c-39362325">[1 more]</label></div><br/><div class="children"><div class="content">runpod, kaggle, lambda labs, or pretty much any other server provider that gives you one or more gpus.</div><br/></div></div></div></div><div id="39361339" class="c"><input type="checkbox" id="c-39361339" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39362162">prev</a><span>|</span><a href="#39362422">next</a><span>|</span><label class="collapse" for="c-39361339">[-]</label><label class="expand" for="c-39361339">[1 more]</label></div><br/><div class="children"><div class="content">I remember doing some random experiments with these two researchers to find the best way to condition the stage B on the latent, my very fancy cross-attn with relative 2D positional embeddings didn&#x27;t work as well as just concatenating the channels of the input with the nearest upsample of the latent, so I just gave up ahah.<p>This model used to be known as Würstchen v3.</div><br/></div></div><div id="39362422" class="c"><input type="checkbox" id="c-39362422" checked=""/><div class="controls bullet"><span class="by">cybereporter</span><span>|</span><a href="#39361339">prev</a><span>|</span><a href="#39364626">next</a><span>|</span><label class="collapse" for="c-39362422">[-]</label><label class="expand" for="c-39362422">[2 more]</label></div><br/><div class="children"><div class="content">Will this get integrated into Stable Diffusion Web UI?</div><br/><div id="39366253" class="c"><input type="checkbox" id="c-39366253" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#39362422">parent</a><span>|</span><a href="#39364626">next</a><span>|</span><label class="collapse" for="c-39366253">[-]</label><label class="expand" for="c-39366253">[1 more]</label></div><br/><div class="children"><div class="content">Surely within days. ComfyUI’s maintainer said he is readying the node for release perhaps by this weekend. The Stable Cascade model is otherwise known as Würschten v3 and has been floating around the open source generative image space since fall.</div><br/></div></div></div></div></div></div></div></div></div></body></html>