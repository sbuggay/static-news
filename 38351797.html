<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700643667669" as="style"/><link rel="stylesheet" href="styles.css?v=1700643667669"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://questdb.io/blog/solving-duplicate-data-performant-deduplication/">Solving duplicate data with performant deduplication</a> <span class="domain">(<a href="https://questdb.io">questdb.io</a>)</span></div><div class="subtext"><span>goodroot</span> | <span>16 comments</span></div><br/><div><div id="38376706" class="c"><input type="checkbox" id="c-38376706" checked=""/><div class="controls bullet"><span class="by">goenning</span><span>|</span><a href="#38371498">next</a><span>|</span><label class="collapse" for="c-38376706">[-]</label><label class="expand" for="c-38376706">[1 more]</label></div><br/><div class="children"><div class="content">If your ClickHouse ReplacingMergeTree returns twice the expected row count is because your query is wrong. You don’t need to FINAL it, just use aggregation on your queries as per their docs</div><br/></div></div><div id="38371498" class="c"><input type="checkbox" id="c-38371498" checked=""/><div class="controls bullet"><span class="by">goodroot</span><span>|</span><a href="#38376706">prev</a><span>|</span><a href="#38371631">next</a><span>|</span><label class="collapse" for="c-38371498">[-]</label><label class="expand" for="c-38371498">[3 more]</label></div><br/><div class="children"><div class="content">Hey! Thanks for upvoting.<p>Happy to answer any questions about deduplication. One thing that&#x27;s not included in the write-up is that we also address out-of-order indexing alongside deduplication.</div><br/><div id="38371764" class="c"><input type="checkbox" id="c-38371764" checked=""/><div class="controls bullet"><span class="by">CommanderHux</span><span>|</span><a href="#38371498">parent</a><span>|</span><a href="#38371631">next</a><span>|</span><label class="collapse" for="c-38371764">[-]</label><label class="expand" for="c-38371764">[2 more]</label></div><br/><div class="children"><div class="content">The dataset link seems to be dead. Do you have a mirror?</div><br/><div id="38371947" class="c"><input type="checkbox" id="c-38371947" checked=""/><div class="controls bullet"><span class="by">goodroot</span><span>|</span><a href="#38371498">root</a><span>|</span><a href="#38371764">parent</a><span>|</span><a href="#38371631">next</a><span>|</span><label class="collapse" for="c-38371947">[-]</label><label class="expand" for="c-38371947">[1 more]</label></div><br/><div class="children"><div class="content">Edit: Updated!<p><a href="https:&#x2F;&#x2F;mega.nz&#x2F;folder&#x2F;A1BjnSYQ#NQe5qhYLVBqiRwhWRmcVtg" rel="nofollow noreferrer">https:&#x2F;&#x2F;mega.nz&#x2F;folder&#x2F;A1BjnSYQ#NQe5qhYLVBqiRwhWRmcVtg</a><p>Article is updating too.</div><br/></div></div></div></div></div></div><div id="38371631" class="c"><input type="checkbox" id="c-38371631" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38371498">prev</a><span>|</span><a href="#38375099">next</a><span>|</span><label class="collapse" for="c-38371631">[-]</label><label class="expand" for="c-38371631">[8 more]</label></div><br/><div class="children"><div class="content">Can anyone comment on QuestDB vs Clickhouse vs TimescaleDB? Real world experience around ergonomics, ops, etc.<p>Currently using BigQuery for a lot of this (ingesting ~5-10TB monthly) but would like to begin exploring in-house tooling.<p>On the flip side, we still use PSQL&#x2F;RDS a lot and I enjoy it for the low operations burden - but we&#x27;re doing some time series stuff with it now that is starting to fall over. TimescaleDB is nice because it <i>is</i> postgres, but afaik cannot work inside RDS. Clickhouse is next on my list for a test deployment, but QuestDB looks pretty neat too.</div><br/><div id="38376363" class="c"><input type="checkbox" id="c-38376363" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#38371631">parent</a><span>|</span><a href="#38372361">next</a><span>|</span><label class="collapse" for="c-38376363">[-]</label><label class="expand" for="c-38376363">[1 more]</label></div><br/><div class="children"><div class="content">I looked at it every year for a few years, most recently 1-2 yrs ago. It really is 20-50x faster at a lot of workloads than something like postgres, and it really does have a relational core and decent time-series functionality. I&#x27;d love to use it more, but IMO it&#x27;s far too buggy. However it got that way, nothing about query execution felt cleanly composable.<p>No single query directly corrupted any persistent data, but &quot;complicated&quot; queries (any more nesting than the classic &quot;select from join where&quot; SQL backbone) were prone to returning no results, wrong results, error conditions, or slowly. For a bit I worked around that by wrapping right-sized queries (big enough to do something meaningful and avoid comms overhead, small&#x2F;simple enough to work as expected) with a normal turing-complete language, but it was painful. Plus the defaults were quirky (broken telemetry opt-out, connections time out, RAM limits being per-arcane-questdb-subcomponent rather than per-questdb, ...) IMO, not that I would have minded enough to look elsewhere if the queries were correct.<p>Colored by that experience, I&#x27;d caution to thoroughly go through the docs and get it configured exactly as you want before deploying to prod, testing that the features you need behave correctly (best practice for most software, but IME not strictly necessary for a lot of products if your application code is sane), and also do something to probe reliability like spending a day hammering it with a toy project (maybe a few 10M row tables so that it&#x27;s a fast experiment but anything fishy still stinks appropriately) and checking the query results. The current GitHub issues look fairly tame, so maybe it&#x27;s better by now.</div><br/></div></div><div id="38372361" class="c"><input type="checkbox" id="c-38372361" checked=""/><div class="controls bullet"><span class="by">anonacct37</span><span>|</span><a href="#38371631">parent</a><span>|</span><a href="#38376363">prev</a><span>|</span><a href="#38372066">next</a><span>|</span><label class="collapse" for="c-38372361">[-]</label><label class="expand" for="c-38372361">[2 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t answer your question directly but I&#x27;ve touched a few of those here and there.<p>Clickhouse was deployed to replace a home grown distributed storage system that at one point in time a long time ago was much cheaper and faster than the results the team was getting with BQ.<p>We evaluated clickhouse and druid for a few data stores for doing interactive queries on a fairly high throughput clickstream data pipeline.<p>Clickhouse won in terms of performance. We did some chaos testing on it in the form of simulating node outages and network partitions and we were happy with the results. My only complaint is that there are some other database options which don&#x27;t require me to run a database and that&#x27;s nice if I can get away with it.<p>One of the things you might try is dumping your data into parquet files on gcs. There are quite a few  databases you can query that with and get good results depending on your indexing and partitioning needs. It&#x27;s tough to get lower operational burden than &quot;stick it in cloud storage and sometimes spin up some stateless compute to query it&quot;.<p>I think duckdb is super cool but for me at the moment it&#x27;s a solution that I&#x27;m still in search of a problem for.</div><br/><div id="38372649" class="c"><input type="checkbox" id="c-38372649" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38371631">root</a><span>|</span><a href="#38372361">parent</a><span>|</span><a href="#38372066">next</a><span>|</span><label class="collapse" for="c-38372649">[-]</label><label class="expand" for="c-38372649">[1 more]</label></div><br/><div class="children"><div class="content">We’re also using duckdb in prod and the performance is phenomenal.<p>Haven’t tried using it to read parquet direct from cloud storage but that’s on our todo list. We currently use it to generate a massive table once a day and then throughout the day that’s queried to serve prod requests.</div><br/></div></div></div></div><div id="38372066" class="c"><input type="checkbox" id="c-38372066" checked=""/><div class="controls bullet"><span class="by">nhourcard</span><span>|</span><a href="#38371631">parent</a><span>|</span><a href="#38372361">prev</a><span>|</span><a href="#38371718">next</a><span>|</span><label class="collapse" for="c-38372066">[-]</label><label class="expand" for="c-38372066">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be curious to hear how RDS is starting to fall over with time series data, is it a bottleneck on ingestion, queries, or both?</div><br/><div id="38372689" class="c"><input type="checkbox" id="c-38372689" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38371631">root</a><span>|</span><a href="#38372066">parent</a><span>|</span><a href="#38371718">next</a><span>|</span><label class="collapse" for="c-38372689">[-]</label><label class="expand" for="c-38372689">[1 more]</label></div><br/><div class="children"><div class="content">It’s working okay with a 30 day rolling average… (every day we truncate older rows) we read from it to generate on demand status for per-second performance of tasks in a big job processing engine. But long term we want to have all the data available for historical analysis, trend analysis etc.</div><br/></div></div></div></div><div id="38371718" class="c"><input type="checkbox" id="c-38371718" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#38371631">parent</a><span>|</span><a href="#38372066">prev</a><span>|</span><a href="#38375099">next</a><span>|</span><label class="collapse" for="c-38371718">[-]</label><label class="expand" for="c-38371718">[2 more]</label></div><br/><div class="children"><div class="content">What about iceberg tables and a lake approach on GCS and then picking a querying engine?</div><br/><div id="38372701" class="c"><input type="checkbox" id="c-38372701" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38371631">root</a><span>|</span><a href="#38371718">parent</a><span>|</span><a href="#38375099">next</a><span>|</span><label class="collapse" for="c-38372701">[-]</label><label class="expand" for="c-38372701">[1 more]</label></div><br/><div class="children"><div class="content">These are terms I’m sorta familiar with but not sure. Data lake = bunch of noise (everything), iceberg = generated tables or views to read relevant&#x2F;hot data from the lake?</div><br/></div></div></div></div></div></div><div id="38375099" class="c"><input type="checkbox" id="c-38375099" checked=""/><div class="controls bullet"><span class="by">jimsimmons</span><span>|</span><a href="#38371631">prev</a><span>|</span><label class="collapse" for="c-38375099">[-]</label><label class="expand" for="c-38375099">[3 more]</label></div><br/><div class="children"><div class="content">What is the best way to deduplicate a corpus of documents</div><br/><div id="38376356" class="c"><input type="checkbox" id="c-38376356" checked=""/><div class="controls bullet"><span class="by">OnlyMortal</span><span>|</span><a href="#38375099">parent</a><span>|</span><a href="#38375301">next</a><span>|</span><label class="collapse" for="c-38376356">[-]</label><label class="expand" for="c-38376356">[1 more]</label></div><br/><div class="children"><div class="content">A Rabin finger printing algorithm and a hash of the data it generates.<p>Reference count the hashes.</div><br/></div></div><div id="38375301" class="c"><input type="checkbox" id="c-38375301" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#38375099">parent</a><span>|</span><a href="#38376356">prev</a><span>|</span><label class="collapse" for="c-38375301">[-]</label><label class="expand" for="c-38375301">[1 more]</label></div><br/><div class="children"><div class="content">If you mean in the sense of dealing with documents that are very similar but not binary identical, a locality sensitive hash would do the job.</div><br/></div></div></div></div></div></div></div></div></div></body></html>