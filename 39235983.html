<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706950856991" as="style"/><link rel="stylesheet" href="styles.css?v=1706950856991"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html">A decoder-only foundation model for time-series forecasting</a>Â <span class="domain">(<a href="https://blog.research.google">blog.research.google</a>)</span></div><div class="subtext"><span>jasondavies</span> | <span>33 comments</span></div><br/><div><div id="39237591" class="c"><input type="checkbox" id="c-39237591" checked=""/><div class="controls bullet"><span class="by">frogamel</span><span>|</span><a href="#39237071">next</a><span>|</span><label class="collapse" for="c-39237591">[-]</label><label class="expand" for="c-39237591">[3 more]</label></div><br/><div class="children"><div class="content">The research in this space is very conflicting about what methods actually work. In the graph on the page, the ETS model (basically just a weighted moving average) outperforms multiple, recent deep learning models. But the papers for those models claim they outperform ETS and other basic methods by quite a bit.<p>You can find recent papers from researchers about how their new transformers model is the best and SOTA, papers which claim transformers is garbage for time series and claim their own MLP variant is SOTA, other papers which claim deep learning in general underperforms compared to xgboost&#x2F;lightgbm, etc.<p>Realistically I think time series is incredibly diverse, and results are going to be highly dependent on which dataset was cherry-picked for benchmarking. IMO this is why the idea of a time series foundation model is fundamentally flawed - transfer learning is the reason why foundation models work in language models, but most time series are overwhelmingly noise and don&#x27;t provide enough context to figure out what information is actually transferrable between different time series.</div><br/><div id="39238014" class="c"><input type="checkbox" id="c-39238014" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39237591">parent</a><span>|</span><a href="#39238458">next</a><span>|</span><label class="collapse" for="c-39238014">[-]</label><label class="expand" for="c-39238014">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Realistically I think time series is incredibly diverse, and results are going to be highly dependent on which dataset was cherry-picked for benchmarking<p>That&#x27;s exactly right it is bearly one step above playing &quot;guess which number I&#x27;m thinking of&quot; and acting amazed that if you play long enough you&#x27;ll witness an occasional winning streak.<p>My god, model has learned to read your mind! ;)<p>This smacks of when very serious soviet scientists ran Telekinesis experiments and all manner of cold reading and charlatans.
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Telekinesis" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Telekinesis</a><p>Somebody should come up with a decoder-only foundation model for bending-spoons.<p>Interesting reading:
<a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;competitions&#x2F;m5-forecasting-accuracy&#x2F;overview" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;competitions&#x2F;m5-forecasting-accuracy&#x2F;...</a></div><br/></div></div><div id="39238458" class="c"><input type="checkbox" id="c-39238458" checked=""/><div class="controls bullet"><span class="by">rlupi</span><span>|</span><a href="#39237591">parent</a><span>|</span><a href="#39238014">prev</a><span>|</span><a href="#39237071">next</a><span>|</span><label class="collapse" for="c-39238458">[-]</label><label class="expand" for="c-39238458">[1 more]</label></div><br/><div class="children"><div class="content">I think the next jump will come from neurosymbolic approaches, merging timeseries with a description of what they are about as input.<p>You can use that description for system identification, i.e. build a model of how the &quot;world&quot; works. This can be translated into a two-part network architecture, one is essentially a world-model-informed (physics-informed, as its often called in literature) part for the known-unknowns, the other one is a bounded error term  for the unknown unknowns (e.g. dense layers, or maybe dense layers + non-linearities that capture the fundamental modes of the problem space for reservoir computing). The world model is revised with another external cycle of meta-learning, via symbolic regression.<p>The unknown-unkowns bit that I choose is designed as a shallow network that can be trained online (traditional methods, but I&#x27;d like to see if the forward-forward algorithm from Hinton would work well for short-term online adjustments), or by well-known tools like particle filters &#x2F; kalman filters.<p>The non-linearities (and the overall approach in general) resemble physics-informed dynamic mode decomposition (piDMD), which show remarkable resistance to noise (e.g. salt &amp; pepper).<p>If you have simple timeseries, and not very complex hierarchical systems that change over time and show novel modes that you haven&#x27;t encountered before, then piDMD is likely enough for what you need.<p>---<p>Essentially, what I describe is a multi modal model for timeseries + a planning step. (AlphaGeometry to the rescue?)<p>---<p>Like you say, time-series in general have an incredibly complicated domain with comparatively few data available.<p>For example, real-world complex physical systems (industrial plants, but also large-scale software systems) may have replicas of the same components with complex behavior, and no&#x2F;few shared dependencies for reliability or other constraints (e.g. physically apart). These can be captured by transformers. Training will be much faster if you initialize weights like I describe above, and share weights among replicas. The physical structure also creates particular conditions on the covariant matrixes and on the domain of higher-level timeseries (ultrametric spaces, which changes how measurement and frequency behaves there and can lead to great simplifications, but also errors if tools like FFT are applied blindly without proper adjustments; much like in operations research &#x2F; planning problems, symmetries are sought after to reduce complexity).<p>On the other hand, the next level that compose these building blocks often have graph structure and sometimes scale-free networks (e.g. if they represent usage or behaviors, rather than physical systems). I think we&#x27;ll see graph neural networks shine on this front.<p>There are likely other kind of behavior that I haven&#x27;t encountered yet in my work.<p>I think overall, we&#x27;ll see planning&#x2F;neurosymbolic used at the highest-layer, graph neural networks  for scale-free networks and to optimize long-range connections (also when a dense model with dynamic covariant matrices would be too expensive to compute even in sparse form), and transformers or&#x2F;with piDMD-like approaches for dense patches of complex behavior. I.e. graph models as a generalization for spatial locality to arbitrary spatial-like domains, transformers&#x2F;piDMD or similar for sequence-&#x2F;time- locality for arbitrary complex systems.  (I wonder what kind of weights will they implement when trained together on problems that are fundamentally in the middle, where traditionally one would use wavelets... if you look a the GraphCast model by deepmind for weather forecasting, it looks quite similar)</div><br/></div></div></div></div><div id="39237071" class="c"><input type="checkbox" id="c-39237071" checked=""/><div class="controls bullet"><span class="by">snats</span><span>|</span><a href="#39237591">prev</a><span>|</span><a href="#39238448">next</a><span>|</span><label class="collapse" for="c-39237071">[-]</label><label class="expand" for="c-39237071">[3 more]</label></div><br/><div class="children"><div class="content">A little bit of context. 
Basically, all of tabular deep learning has been stuck and SOTA has been tree based algorithms like Catboost and XGBoost. This seems like a big step forward towards getting a generalizable deep learning model besides this &quot;tree models&quot;</div><br/><div id="39238309" class="c"><input type="checkbox" id="c-39238309" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39237071">parent</a><span>|</span><a href="#39237461">next</a><span>|</span><label class="collapse" for="c-39238309">[-]</label><label class="expand" for="c-39238309">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t for tabular data though? Time series transformers have been around for a few years now, see Transformers in Time Series: A Survey
 <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.07125" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.07125</a> and  Are Transformers Effective for Time Series Forecasting?
 <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.13504" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.13504</a></div><br/></div></div><div id="39237461" class="c"><input type="checkbox" id="c-39237461" checked=""/><div class="controls bullet"><span class="by">spyder</span><span>|</span><a href="#39237071">parent</a><span>|</span><a href="#39238309">prev</a><span>|</span><a href="#39238448">next</a><span>|</span><label class="collapse" for="c-39237461">[-]</label><label class="expand" for="c-39237461">[1 more]</label></div><br/><div class="children"><div class="content">Yea, and from this paper it looks like the tree model (Catboost supervised) is still beating their (zero-shot) performance and they don&#x27;t show their supervised version yet, but they write at the end that they plan to do it in a future work. Will be interesting to see.</div><br/></div></div></div></div><div id="39238448" class="c"><input type="checkbox" id="c-39238448" checked=""/><div class="controls bullet"><span class="by">alexey-salmin</span><span>|</span><a href="#39237071">prev</a><span>|</span><a href="#39236784">next</a><span>|</span><label class="collapse" for="c-39238448">[-]</label><label class="expand" for="c-39238448">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure a zero-shot foundational model for time series makes sense at all: if you only look at the numbers it&#x27;s the &quot;continue the sequence&quot; game, nowhere to apply your &quot;foundational knowledge&quot; really.<p>A one-shot model that takes a prompt like &quot;this sequence is a voltage of a PV-cell every hour&quot; will have a chance though.</div><br/></div></div><div id="39236784" class="c"><input type="checkbox" id="c-39236784" checked=""/><div class="controls bullet"><span class="by">thatcherc</span><span>|</span><a href="#39238448">prev</a><span>|</span><a href="#39238241">next</a><span>|</span><label class="collapse" for="c-39236784">[-]</label><label class="expand" for="c-39236784">[10 more]</label></div><br/><div class="children"><div class="content">Sounds like a cool model. I&#x27;d love to try it but it seems like they&#x27;re not releasing it (yet?). I&#x27;ve really gotten spoiled with the recent language models where I can download and run any new model or fine-tune I hear about. It&#x27;s gotten to the point where I don&#x27;t feel a model is very relevant unless I can run it locally. Hopefully a local version of this becomes available because I have plenty of time series data I&#x27;d like to run through it!</div><br/><div id="39236989" class="c"><input type="checkbox" id="c-39236989" checked=""/><div class="controls bullet"><span class="by">gardnr</span><span>|</span><a href="#39236784">parent</a><span>|</span><a href="#39236874">prev</a><span>|</span><a href="#39238241">next</a><span>|</span><label class="collapse" for="c-39236989">[-]</label><label class="expand" for="c-39236989">[8 more]</label></div><br/><div class="children"><div class="content">What time series data do you have that you&#x27;d like to run through it?</div><br/><div id="39237159" class="c"><input type="checkbox" id="c-39237159" checked=""/><div class="controls bullet"><span class="by">mordechai9000</span><span>|</span><a href="#39236784">root</a><span>|</span><a href="#39236989">parent</a><span>|</span><a href="#39237058">next</a><span>|</span><label class="collapse" for="c-39237159">[-]</label><label class="expand" for="c-39237159">[6 more]</label></div><br/><div class="children"><div class="content">Stock market data. But maybe the advantage disappears if other traders have the same model. Perhaps that&#x27;s why they haven&#x27;t released it.</div><br/><div id="39237199" class="c"><input type="checkbox" id="c-39237199" checked=""/><div class="controls bullet"><span class="by">bethekind</span><span>|</span><a href="#39236784">root</a><span>|</span><a href="#39237159">parent</a><span>|</span><a href="#39237058">next</a><span>|</span><label class="collapse" for="c-39237199">[-]</label><label class="expand" for="c-39237199">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tried using TimeGPT to predict the SPDR future price and it couldn&#x27;t make any useful predictions<p>Maybe this one is better<p>I still think humans have better pattern recognition in the stock market than neural nets...at least for now</div><br/><div id="39237320" class="c"><input type="checkbox" id="c-39237320" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#39236784">root</a><span>|</span><a href="#39237199">parent</a><span>|</span><a href="#39237601">next</a><span>|</span><label class="collapse" for="c-39237320">[-]</label><label class="expand" for="c-39237320">[3 more]</label></div><br/><div class="children"><div class="content">I highly doubt a human can beat the market consistently looking at the timeseries of some ticker alone.</div><br/><div id="39237748" class="c"><input type="checkbox" id="c-39237748" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#39236784">root</a><span>|</span><a href="#39237320">parent</a><span>|</span><a href="#39237601">next</a><span>|</span><label class="collapse" for="c-39237748">[-]</label><label class="expand" for="c-39237748">[2 more]</label></div><br/><div class="children"><div class="content">you dont need to beat the market. you just need to beat pure chance to make money</div><br/><div id="39237809" class="c"><input type="checkbox" id="c-39237809" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39236784">root</a><span>|</span><a href="#39237748">parent</a><span>|</span><a href="#39237601">next</a><span>|</span><label class="collapse" for="c-39237809">[-]</label><label class="expand" for="c-39237809">[1 more]</label></div><br/><div class="children"><div class="content">If you aren&#x27;t beating the market you&#x27;re losing money in opportunity cost, because you could&#x27;ve just bought an index fund and made more.<p>(Also, you don&#x27;t just need to beat pure chance, because pure chance is not guaranteed â or I think likely â to result in net zero losses on average, since you are then in an information-asymmetric environment where your trading partner looks at data and you do not. But regardless, even if you &quot;make&quot; money but underperform the market, you are effectively losing money in opportunity cost!)</div><br/></div></div></div></div></div></div><div id="39237601" class="c"><input type="checkbox" id="c-39237601" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39236784">root</a><span>|</span><a href="#39237199">parent</a><span>|</span><a href="#39237320">prev</a><span>|</span><a href="#39237058">next</a><span>|</span><label class="collapse" for="c-39237601">[-]</label><label class="expand" for="c-39237601">[1 more]</label></div><br/><div class="children"><div class="content">What input data did you feed into TimeGPT? Did you fine tune it?</div><br/></div></div></div></div></div></div><div id="39237058" class="c"><input type="checkbox" id="c-39237058" checked=""/><div class="controls bullet"><span class="by">ctrw</span><span>|</span><a href="#39236784">root</a><span>|</span><a href="#39236989">parent</a><span>|</span><a href="#39237159">prev</a><span>|</span><a href="#39238241">next</a><span>|</span><label class="collapse" for="c-39237058">[-]</label><label class="expand" for="c-39237058">[1 more]</label></div><br/><div class="children"><div class="content">Mine of your business. That&#x27;s the point of open source.</div><br/></div></div></div></div></div></div><div id="39238241" class="c"><input type="checkbox" id="c-39238241" checked=""/><div class="controls bullet"><span class="by">Smith42</span><span>|</span><a href="#39236784">prev</a><span>|</span><a href="#39238025">next</a><span>|</span><label class="collapse" for="c-39238241">[-]</label><label class="expand" for="c-39238241">[1 more]</label></div><br/><div class="children"><div class="content">If you are interested in this also check out EarthPT, which is also a time series decoding transformer (and has the code and weights released under the MIT licence): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07207" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07207</a></div><br/></div></div><div id="39238025" class="c"><input type="checkbox" id="c-39238025" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#39238241">prev</a><span>|</span><a href="#39237048">next</a><span>|</span><label class="collapse" for="c-39238025">[-]</label><label class="expand" for="c-39238025">[1 more]</label></div><br/><div class="children"><div class="content">I like tabular ML, but very skeptical about this research direction.<p>Language models trained on a broad corpus make sense, as the language is similar across different domains. Time-series, however, are extremely different... Stock prices, heart-rates, brain-waves, digital signals... Patterns learned on a broad dataset should only introduce uninterpretable noise.</div><br/></div></div><div id="39237048" class="c"><input type="checkbox" id="c-39237048" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#39238025">prev</a><span>|</span><a href="#39237297">next</a><span>|</span><label class="collapse" for="c-39237048">[-]</label><label class="expand" for="c-39237048">[2 more]</label></div><br/><div class="children"><div class="content">This is cool, but I&#x27;m amused that CatBoost still beats it.</div><br/><div id="39237429" class="c"><input type="checkbox" id="c-39237429" checked=""/><div class="controls bullet"><span class="by">old_bayes</span><span>|</span><a href="#39237048">parent</a><span>|</span><a href="#39237297">next</a><span>|</span><label class="collapse" for="c-39237429">[-]</label><label class="expand" for="c-39237429">[1 more]</label></div><br/><div class="children"><div class="content">Fraction of the cost, way more interpretable.<p>Still, I suppose we gotta keep an eye on this kind of work in case there&#x27;s a tipping point.</div><br/></div></div></div></div><div id="39237297" class="c"><input type="checkbox" id="c-39237297" checked=""/><div class="controls bullet"><span class="by">neodypsis</span><span>|</span><a href="#39237048">prev</a><span>|</span><a href="#39237103">next</a><span>|</span><label class="collapse" for="c-39237297">[-]</label><label class="expand" for="c-39237297">[7 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Synthetic data helps with the basics. Meaningful synthetic time-series data can be generated using statistical models or physical simulations. These basic temporal patterns can teach the model the grammar of time series forecasting.&quot;<p>Can someone elaborate on what a grammar means in the context of time series forecasting?</div><br/><div id="39237408" class="c"><input type="checkbox" id="c-39237408" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#39237297">parent</a><span>|</span><a href="#39237388">next</a><span>|</span><label class="collapse" for="c-39237408">[-]</label><label class="expand" for="c-39237408">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a bit of an anthropomorphism ation. I don&#x27;t believe it has any formal meaning here. The idea is that there are certain kinds of underlying signals and patterns which are common to a wide range of time series data. So if a model is able to learn those signals and patterns, it can look at any time series and, with enough historical data, predict future data, without actually updating any model weights. Those signals constitute a &quot;grammar&quot; of sorts.</div><br/></div></div><div id="39237388" class="c"><input type="checkbox" id="c-39237388" checked=""/><div class="controls bullet"><span class="by">remontoire</span><span>|</span><a href="#39237297">parent</a><span>|</span><a href="#39237408">prev</a><span>|</span><a href="#39237300">next</a><span>|</span><label class="collapse" for="c-39237388">[-]</label><label class="expand" for="c-39237388">[2 more]</label></div><br/><div class="children"><div class="content">Probably drawing an analogy to how causal pretrained models go through stages of understanding language, words -&gt; grammar -&gt; meaning.  Gwen mentions this experience when training character level RNNs. <a href="https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis#why-does-pretraining-work" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis#why-does-pretraining-wo...</a></div><br/><div id="39237471" class="c"><input type="checkbox" id="c-39237471" checked=""/><div class="controls bullet"><span class="by">horacemorace</span><span>|</span><a href="#39237297">root</a><span>|</span><a href="#39237388">parent</a><span>|</span><a href="#39237300">next</a><span>|</span><label class="collapse" for="c-39237471">[-]</label><label class="expand" for="c-39237471">[1 more]</label></div><br/><div class="children"><div class="content">Totally. Also basic temporal scale or cyclic properties. Itâs kind of mind blowing that the shape of most recorded human patterns is reducible in this way.</div><br/></div></div></div></div><div id="39237300" class="c"><input type="checkbox" id="c-39237300" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#39237297">parent</a><span>|</span><a href="#39237388">prev</a><span>|</span><a href="#39237367">next</a><span>|</span><label class="collapse" for="c-39237300">[-]</label><label class="expand" for="c-39237300">[2 more]</label></div><br/><div class="children"><div class="content">They probably mean the manifold of the universe</div><br/><div id="39238156" class="c"><input type="checkbox" id="c-39238156" checked=""/><div class="controls bullet"><span class="by">mnoronha</span><span>|</span><a href="#39237297">root</a><span>|</span><a href="#39237300">parent</a><span>|</span><a href="#39237367">next</a><span>|</span><label class="collapse" for="c-39238156">[-]</label><label class="expand" for="c-39238156">[1 more]</label></div><br/><div class="children"><div class="content">ah yes, the manifold of the universe (god) :)</div><br/></div></div></div></div><div id="39237367" class="c"><input type="checkbox" id="c-39237367" checked=""/><div class="controls bullet"><span class="by">alextheparrot</span><span>|</span><a href="#39237297">parent</a><span>|</span><a href="#39237300">prev</a><span>|</span><a href="#39237103">next</a><span>|</span><label class="collapse" for="c-39237367">[-]</label><label class="expand" for="c-39237367">[1 more]</label></div><br/><div class="children"><div class="content">You could probably consider learning a sign wave to be a âgrammarâ related to periodic variations. Grammar in this context feels like âWhat are the core conceptual heuristics that help guide towards faster understandingâ</div><br/></div></div></div></div><div id="39237103" class="c"><input type="checkbox" id="c-39237103" checked=""/><div class="controls bullet"><span class="by">j7ake</span><span>|</span><a href="#39237297">prev</a><span>|</span><a href="#39237633">next</a><span>|</span><label class="collapse" for="c-39237103">[-]</label><label class="expand" for="c-39237103">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting how they say that their model has high &quot;zero-shot&quot; performance, even though it was trained on millions of examples.<p>To me, zero-shot would mean a model whose parameters was never tuned through training examples at all...</div><br/><div id="39237135" class="c"><input type="checkbox" id="c-39237135" checked=""/><div class="controls bullet"><span class="by">breakds</span><span>|</span><a href="#39237103">parent</a><span>|</span><a href="#39237132">next</a><span>|</span><label class="collapse" for="c-39237135">[-]</label><label class="expand" for="c-39237135">[1 more]</label></div><br/><div class="children"><div class="content">In this context, &quot;zero-shot&quot; capability usually means &quot;how well it performs without fine-tuning or prompting with in-context examples&quot; after the model is trained and parameters frozen.</div><br/></div></div><div id="39237132" class="c"><input type="checkbox" id="c-39237132" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39237103">parent</a><span>|</span><a href="#39237135">prev</a><span>|</span><a href="#39237633">next</a><span>|</span><label class="collapse" for="c-39237132">[-]</label><label class="expand" for="c-39237132">[1 more]</label></div><br/><div class="children"><div class="content">Zero shot means a model trained one one dataset, or for one task, performs well on another dataset, or on another task - without any finetuning.</div><br/></div></div></div></div><div id="39237633" class="c"><input type="checkbox" id="c-39237633" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39237103">prev</a><span>|</span><label class="collapse" for="c-39237633">[-]</label><label class="expand" for="c-39237633">[1 more]</label></div><br/><div class="children"><div class="content">Are they ever going to release the weights?</div><br/></div></div></div></div></div></div></div></body></html>