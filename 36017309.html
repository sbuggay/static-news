<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684659656329" as="style"/><link rel="stylesheet" href="styles.css?v=1684659656329"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://spectrum.ieee.org/gpt-4-calm-down">Rodney Brooks on GPT-4</a> <span class="domain">(<a href="https://spectrum.ieee.org">spectrum.ieee.org</a>)</span></div><div class="subtext"><span>marshblocker</span> | <span>263 comments</span></div><br/><div><div id="36017613" class="c"><input type="checkbox" id="c-36017613" checked=""/><div class="controls bullet"><span class="by">fwlr</span><span>|</span><a href="#36017579">next</a><span>|</span><label class="collapse" for="c-36017613">[-]</label><label class="expand" for="c-36017613">[128 more]</label></div><br/><div class="children"><div class="content">&gt; The large language models are a little surprising. I’ll give you that.<p>I think this is the key point about LLMs that kind of explains the wide and polarized views on whether it understands or parrots, whether it can think or is the precursor to thinking or is a dead-end, whether it will catastrophically destroy the world, or “merely” make it steadily worse with bullshit, or just put a few industries out of a job.<p>Almost nobody is really surprised that if you throw more compute at a neural net it becomes better at the task it’s trained on. But almost everybody is really surprised that becoming better at a task like ‘natural language prediction’ would produce all these strange abilities that sort of look like “understanding the world”.<p>One way to resolve this surprise is to find some reason to believe these strange abilities are fundamentally <i>not</i> an understanding of the world. Thus stochastic parrots, this article, Yan LeCun and Chomsky, etc.<p>Another way to resolve this surprise is to find some reason to believe these strange abilities fundamentally <i>are</i> an understanding of the world. Thus regulation of AI, existential risk, Hinton and Yudkowsky, etc.<p>I don’t know what the correct resolution of the surprise is. The only thing I’m confident in is that it’s correct to be surprised by the abilities of LLMs. My current (tentative) resolution of the surprise is that language encoded way more information about reality than we thought it did. (Enough information that you can <i>fully</i> derive reality from language seems improbable, but iirc it did derive Othello and partly derived chess and I would have thought there wasn’t enough information in language to derive those without playing the games as well, so I can’t rule it out.)</div><br/><div id="36017973" class="c"><input type="checkbox" id="c-36017973" checked=""/><div class="controls bullet"><span class="by">thomastjeffery</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017681">next</a><span>|</span><label class="collapse" for="c-36017973">[-]</label><label class="expand" for="c-36017973">[14 more]</label></div><br/><div class="children"><div class="content">&gt; My current (tentative) resolution of the surprise is that language encoded way more information about reality than we thought it did.<p>I think you are close to the mark, but you have been subtly mislead: <i>language</i> is not the data we are working with. We are working with <i>text</i>.<p>Once you fix that particular failure of word choice, everything else becomes much more clear: <i>text</i> contains much more information than <i>language</i>.<p>We aren&#x27;t dealing with just any text, either: that would be noise. We&#x27;re training LLMs on <i>written text</i>.<p>Natural language is infamous for one specific feature: ambiguity. There are many possible ways to write something, but we can only write one. We must choose: in doing so, we record the choice itself, and all of the entropy that informed it.<p>That entropy is the secret sauce: the extra data that LLMs are sometimes able to model. We don&#x27;t see it, because we read <i>language</i>, not <i>text</i>.<p>The big surprise is that LLMs aren&#x27;t able to write language: they can only write text. They don&#x27;t get tripped up reading ambiguity, but they can&#x27;t avoid writing it, either. Who chooses what an LLM writes? Is it a mystery character who lives in a black box, or a continuation of the entropy that was encoded into the text that LLM was trained on?</div><br/><div id="36018241" class="c"><input type="checkbox" id="c-36018241" checked=""/><div class="controls bullet"><span class="by">zainhoda</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017973">parent</a><span>|</span><a href="#36018269">next</a><span>|</span><label class="collapse" for="c-36018241">[-]</label><label class="expand" for="c-36018241">[5 more]</label></div><br/><div class="children"><div class="content">There’s an exercise that some people do when learning programming, which is to write down the steps to make a sandwich. Then the teacher follows the exact instructions to make a sandwich and most people don’t put enough detail for a computer to follow (I.e. open the fridge etc) and the teacher will run around bumping into things. That used to be a teaching exercise to show people the amount of precision required when telling a machine what to do.<p>Now with LLMs, I think one of the great leaps is the idea that it’s no longer necessary to be “pedantic” when giving computers instructions because LLMs have somehow learned to fill in the blanks with a similar shared “understanding” of the world that we have (I.e. cheese is stored in the fridge so you have to go open the fridge to fetch the cheese for the sandwich).</div><br/><div id="36018957" class="c"><input type="checkbox" id="c-36018957" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018241">parent</a><span>|</span><a href="#36018761">next</a><span>|</span><label class="collapse" for="c-36018957">[-]</label><label class="expand" for="c-36018957">[1 more]</label></div><br/><div class="children"><div class="content">Even then, you can see some of the &quot;pedantic&quot; cases when it comes to actually understanding the nature of the the connections between those concepts. For example, it&#x27;s very easy to get it to reverse shorter&#x2F;taller or younger&#x2F;older of clearly defined relationships.</div><br/></div></div><div id="36018761" class="c"><input type="checkbox" id="c-36018761" checked=""/><div class="controls bullet"><span class="by">prox</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018241">parent</a><span>|</span><a href="#36018957">prev</a><span>|</span><a href="#36018760">next</a><span>|</span><label class="collapse" for="c-36018761">[-]</label><label class="expand" for="c-36018761">[1 more]</label></div><br/><div class="children"><div class="content">Don’t forget they hold vector spaces. So fridge and cheese score high together for cohesiveness, but fridge and Antarctica less so, but both have something to do with cold. Together with all the training on texts creates a good ability to make inferences and “conclusions”. It has a net of lines of meanings of all concepts we fed it that give it the ability that it has, without actually understanding.</div><br/></div></div><div id="36018312" class="c"><input type="checkbox" id="c-36018312" checked=""/><div class="controls bullet"><span class="by">hattmall</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018241">parent</a><span>|</span><a href="#36018760">prev</a><span>|</span><a href="#36018269">next</a><span>|</span><label class="collapse" for="c-36018312">[-]</label><label class="expand" for="c-36018312">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get the &quot;magic&quot; people are seeing. It makes sense.<p>&gt;LLMs have somehow learned to fill in the blanks<p>It&#x27;s not somehow, it&#x27;s because they have read a ton of books, documents, etc and can make enough links between cheese and refrigerator and follow that back to know that a refrigerator needs to be opened.<p>I have seen a lot of very clever AI examples using the latest tools, but I haven&#x27;t seen anything that seems difficult to deconstruct.</div><br/></div></div></div></div><div id="36018269" class="c"><input type="checkbox" id="c-36018269" checked=""/><div class="controls bullet"><span class="by">tornato7</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017973">parent</a><span>|</span><a href="#36018241">prev</a><span>|</span><a href="#36018104">next</a><span>|</span><label class="collapse" for="c-36018269">[-]</label><label class="expand" for="c-36018269">[1 more]</label></div><br/><div class="children"><div class="content">One experiment that I would love to see is an LLM-like model for audio. Feed it hours and hours of lectures, sound effects, animal calls, music etc. You would be able to talk to it and it would ingest the raw waveform then produce audio as a response. Would it learn the fundamentals of music theory? Would it learn to produce &quot;the sound of a bowling ball hitting a dozen windchimes?&quot; Would it learn to talk in English and communicate with whales?
 We&#x27;ve already done text and images, now someone please do sound!</div><br/></div></div><div id="36018104" class="c"><input type="checkbox" id="c-36018104" checked=""/><div class="controls bullet"><span class="by">fwlr</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017973">parent</a><span>|</span><a href="#36018269">prev</a><span>|</span><a href="#36018198">next</a><span>|</span><label class="collapse" for="c-36018104">[-]</label><label class="expand" for="c-36018104">[1 more]</label></div><br/><div class="children"><div class="content">The idea that text contains more information than language is <i>fascinating</i>, wow. Thank you!</div><br/></div></div><div id="36018198" class="c"><input type="checkbox" id="c-36018198" checked=""/><div class="controls bullet"><span class="by">blowski</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017973">parent</a><span>|</span><a href="#36018104">prev</a><span>|</span><a href="#36017681">next</a><span>|</span><label class="collapse" for="c-36018198">[-]</label><label class="expand" for="c-36018198">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really understand your distinction between language and text, but it sounds intrguing. Would you be able to give more detail? I searched but couldn&#x27;t find anything that seemed to explain it.</div><br/><div id="36018493" class="c"><input type="checkbox" id="c-36018493" checked=""/><div class="controls bullet"><span class="by">gmt2027</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018198">parent</a><span>|</span><a href="#36018411">next</a><span>|</span><label class="collapse" for="c-36018493">[-]</label><label class="expand" for="c-36018493">[2 more]</label></div><br/><div class="children"><div class="content">Text is an instance of language. Think of it as the difference between the python language and a large collection of python programs. The language describes syntactic and semantic rules, the collection is a sampling of possible programs that encodes a significant amount of information about the world. You could learn a lot about the laws of nature, the internet, even human society and laws by examining all the python programs ever written.<p>An extreme version of the same idea is the difference between understanding DNA vs the genome of every individual organism that has lived on earth. The species record encodes a ton of information about the laws of nature, the composition and history of our planet. You could deduce physical laws and constants from looking at this information, wars and natural disasters, economic performance, historical natural boundaries, the industrial revolution and a lot more.</div><br/><div id="36018753" class="c"><input type="checkbox" id="c-36018753" checked=""/><div class="controls bullet"><span class="by">Dwolb</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018493">parent</a><span>|</span><a href="#36018411">next</a><span>|</span><label class="collapse" for="c-36018753">[-]</label><label class="expand" for="c-36018753">[1 more]</label></div><br/><div class="children"><div class="content">This is a fascinating point.<p>Let me see if I can play this back.<p>If a student studies DNA sequencing, they’ll learn about the compounds that make up DNA, how traits get encoded, etc.<p>Therefore the student might expect an AI trained on people’s DNA to be able to tell you about whether certain traits are more prevalent in one geography or the other.<p>However, since DNA responds to changes in environment, the AI would start to see time, population, and geography-based patterns emerge.<p>The AI for example could infer that a given person in the US who’s settled in NYC had ancestors from a given region of the world who left due to an environmental disaster just by looking at a given DNA sequence.<p>To the student this result would look like magic. But in the end, it’s a result of individual’s DNA having much more information encoded in it than just human traits.</div><br/></div></div></div></div><div id="36018411" class="c"><input type="checkbox" id="c-36018411" checked=""/><div class="controls bullet"><span class="by">bruce511</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018198">parent</a><span>|</span><a href="#36018493">prev</a><span>|</span><a href="#36018446">next</a><span>|</span><label class="collapse" for="c-36018411">[-]</label><label class="expand" for="c-36018411">[1 more]</label></div><br/><div class="children"><div class="content">Not the poster, but for me it comes down to a mix of clarity and permanence.<p>I teach both verbally (interactive question&#x2F;answer) and I&#x27;ve also written text books.<p>Verbally by language is &quot;loose&quot;. I&#x27;ll say class when I mean object, unicode when I mean utf-8 and so on. Sentences are not all well formed, and sometimes change mid-thought. It&#x27;s very &quot;real time&quot;<p>Writing is a lot more deliberate. I have to be sure of each fact I state. I often re-test things I&#x27;m only 95% sure about. I edit, restructure, remove, add, until I&#x27;m happy.<p>Of course all communication falls on a spectrum. Think phone call at one end, text book on the other. When I do a verbal lecture I&#x27;m usually careful with my speech, and when I post on hacker-news less rigorous.<p>Language covers all of it. Text skews to the more deliberate side. Cunningly the language models are trained using (mostly) text, not speech. That will have an impact on them.</div><br/></div></div><div id="36018446" class="c"><input type="checkbox" id="c-36018446" checked=""/><div class="controls bullet"><span class="by">espe</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018198">parent</a><span>|</span><a href="#36018411">prev</a><span>|</span><a href="#36018266">next</a><span>|</span><label class="collapse" for="c-36018446">[-]</label><label class="expand" for="c-36018446">[1 more]</label></div><br/><div class="children"><div class="content">from a linguistic standpoint a text is a whole lot more than language: it is an externalisation of thought that is fixed onto a medium using writing utensils and most of all, cultural norms in the form of a wild variety of different genres and forms of text, ranging from something like a stream of consciousness to something like a speech act. furthermore, text can be conceptually written or spoken and with the internet we got an explosion of text that is conceptually spoken. those are the things OP might be refering to in regards to the &quot;entropy&quot; that encodes much more than just the tokens themselves.</div><br/></div></div><div id="36018266" class="c"><input type="checkbox" id="c-36018266" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018198">parent</a><span>|</span><a href="#36018446">prev</a><span>|</span><a href="#36017681">next</a><span>|</span><label class="collapse" for="c-36018266">[-]</label><label class="expand" for="c-36018266">[1 more]</label></div><br/><div class="children"><div class="content">Spoken language have pitch, stress, mood, etc..
Written text contain some, but not all.<p>Text, on the other hand, can present in list or table, with varies formatting, indentation. You can&#x27;t reproduce them in spoken text.</div><br/></div></div></div></div></div></div><div id="36017681" class="c"><input type="checkbox" id="c-36017681" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017973">prev</a><span>|</span><a href="#36017668">next</a><span>|</span><label class="collapse" for="c-36017681">[-]</label><label class="expand" for="c-36017681">[20 more]</label></div><br/><div class="children"><div class="content">The issue with all these experts is they still think it&#x27;s human nature to be able to fully understand the world before they speak about it. On the contrary it&#x27;s human nature (and all animal nature) to figure out how to navigate the world without fully understanding or having a complete model of it. All you need is a working model that affects the facets of the world you need to deal with.<p>I still remember in the 90s my school friend came over to my house and I was sending a fax for my dad. He was surprised the paper came back out the other side. He wasn&#x27;t an idiot, and he was 15. But his model of the world didn&#x27;t include deep thought about how fax works, he just merely concoted a system where the paper just went through the wire. That moment stays with me and reminds me what it is to be human and think like one. I think chatgpt is like my friend, and that should scare and excite us.</div><br/><div id="36017862" class="c"><input type="checkbox" id="c-36017862" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017681">parent</a><span>|</span><a href="#36017849">next</a><span>|</span><label class="collapse" for="c-36017862">[-]</label><label class="expand" for="c-36017862">[12 more]</label></div><br/><div class="children"><div class="content">&gt; The issue with all these experts is they still think it&#x27;s human nature to be able to fully understand the world before they speak about it<p>None of the experts think this.</div><br/><div id="36018179" class="c"><input type="checkbox" id="c-36018179" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017862">parent</a><span>|</span><a href="#36018926">next</a><span>|</span><label class="collapse" for="c-36018179">[-]</label><label class="expand" for="c-36018179">[8 more]</label></div><br/><div class="children"><div class="content">Is Chomsky an expert? Try reading his infuriatingly tone-deaf NYTimes editorial.</div><br/><div id="36018834" class="c"><input type="checkbox" id="c-36018834" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018179">parent</a><span>|</span><a href="#36018779">next</a><span>|</span><label class="collapse" for="c-36018834">[-]</label><label class="expand" for="c-36018834">[1 more]</label></div><br/><div class="children"><div class="content">He’s doing the same to ChatGPT that he did to Skinner in the 1960s. Back then, it put him on the map, and he lives in the illusion that he was right. Now ChatGPT is pretty much Skinner’s work come to life. The BFS book Chomsky critiqued was literally “Verbal Behavior”  - a book about how intelligence arises from “dumb” reinforcement learning of words. Obviously Chomsky must now claim that ChatGPT only pretends to be intelligent. Else his entire life’s work is proven wrong.</div><br/></div></div><div id="36018779" class="c"><input type="checkbox" id="c-36018779" checked=""/><div class="controls bullet"><span class="by">DiogenesKynikos</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018179">parent</a><span>|</span><a href="#36018834">prev</a><span>|</span><a href="#36018444">next</a><span>|</span><label class="collapse" for="c-36018779">[-]</label><label class="expand" for="c-36018779">[1 more]</label></div><br/><div class="children"><div class="content">Chomsky writes that language models lack the ability to reason.<p>&gt; Their deepest flaw is the absence of the most critical capacity of any intelligence: to say not only what is the case, what was the case and what will be the case — that’s description and prediction — but also what is not the case and what could and could not be the case. Those are the ingredients of explanation, the mark of true intelligence.<p>&gt; [...] Suppose you are holding an apple in your hand. Now you let the apple go. You observe the result and say, “The apple falls.” That is a description. A prediction might have been the statement “The apple will fall if I open my hand.” Both are valuable, and both can be correct. But an explanation is something more: It includes not only descriptions and predictions but also counterfactual conjectures like “Any such object would fall,” plus the additional clause “because of the force of gravity” or “because of the curvature of space-time” or whatever. That is a causal explanation: “The apple would not have fallen but for the force of gravity.” That is thinking.<p>I decided to ask ChatGPT why an apple falls, based on Chomsky&#x27;s statement:<p>&gt; Suppose you are holding an apple in your hand. Now you let the apple go. You observe the result and say, “The apple falls.” That is a description. Can you say why it falls?<p>ChatGPT responds in exactly the way Chomsky says it cannot:<p>&gt; Yes, the apple falls due to the force of gravity. Gravity is a natural force that attracts objects with mass towards each other. When the apple is released from your hand, it is subject to the gravitational pull of the Earth, causing it to accelerate downward and fall to the ground.<p>ChatGPT certainly appears to understand that apples fall because of gravitational attraction, and that gravity is universal.<p>What makes all the discussion of whether ChatGPT does or does not <i>truly</i> understand this or that so frustrating is that it&#x27;s based on pure assertion. ChatGPT responds exactly like someone who understands gravity would, so I&#x27;m very strongly inclined to believe that it understands gravity. Otherwise, what does &quot;understanding&quot; even mean? It&#x27;s not some magic process.<p>Again, turning to ChatGPT to define &quot;understanding,&quot; here is what it says:<p>&gt; [Understanding] involves making connections, integrating information, and gaining insights or knowledge about a particular subject or concept. Understanding goes beyond simple awareness or recognition; it involves interpreting, analyzing, and synthesizing information to form a coherent mental representation or mental model of the subject matter. It often involves the ability to apply knowledge in new or different contexts, make connections to prior knowledge or experiences, and make sense of complex or abstract ideas.<p>ChatGPT definitely fulfills that definition of &quot;understanding.&quot;</div><br/></div></div><div id="36018444" class="c"><input type="checkbox" id="c-36018444" checked=""/><div class="controls bullet"><span class="by">awongh</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018179">parent</a><span>|</span><a href="#36018779">prev</a><span>|</span><a href="#36018225">next</a><span>|</span><label class="collapse" for="c-36018444">[-]</label><label class="expand" for="c-36018444">[1 more]</label></div><br/><div class="children"><div class="content">here it is for anyone curious:<p><a href="https:&#x2F;&#x2F;archive.is&#x2F;oGXNt" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;oGXNt</a><p>(edit, archive link)</div><br/></div></div><div id="36018225" class="c"><input type="checkbox" id="c-36018225" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018179">parent</a><span>|</span><a href="#36018444">prev</a><span>|</span><a href="#36018369">next</a><span>|</span><label class="collapse" for="c-36018225">[-]</label><label class="expand" for="c-36018225">[2 more]</label></div><br/><div class="children"><div class="content">Chomsky is an expert of a few things, but I doubt that&#x27;s what you actually mean to ask.<p>Regardless his editorial matches how scientists think of the human mind and how OpenAI&#x27;s own creators describe GPT&#x27;s design.</div><br/><div id="36018340" class="c"><input type="checkbox" id="c-36018340" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018225">parent</a><span>|</span><a href="#36018369">next</a><span>|</span><label class="collapse" for="c-36018340">[-]</label><label class="expand" for="c-36018340">[1 more]</label></div><br/><div class="children"><div class="content">Citation please, for where you&#x27;re saying the two parties agree on?</div><br/></div></div></div></div><div id="36018369" class="c"><input type="checkbox" id="c-36018369" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018179">parent</a><span>|</span><a href="#36018225">prev</a><span>|</span><a href="#36018926">next</a><span>|</span><label class="collapse" for="c-36018369">[-]</label><label class="expand" for="c-36018369">[2 more]</label></div><br/><div class="children"><div class="content">Well not in the field he&#x27;s a professional clickbaiter targeting a specific, well defined eco chamber.</div><br/><div id="36018984" class="c"><input type="checkbox" id="c-36018984" checked=""/><div class="controls bullet"><span class="by">cubancigar11</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018369">parent</a><span>|</span><a href="#36018926">next</a><span>|</span><label class="collapse" for="c-36018984">[-]</label><label class="expand" for="c-36018984">[1 more]</label></div><br/><div class="children"><div class="content">That is a very common perception from people whom haven&#x27;t given him any attention. I used to be in the same boat but it was eventually very interesting when I listened to him.</div><br/></div></div></div></div></div></div><div id="36018926" class="c"><input type="checkbox" id="c-36018926" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017862">parent</a><span>|</span><a href="#36018179">prev</a><span>|</span><a href="#36018486">next</a><span>|</span><label class="collapse" for="c-36018926">[-]</label><label class="expand" for="c-36018926">[1 more]</label></div><br/><div class="children"><div class="content">Especially Rodney Brooks.</div><br/></div></div><div id="36018486" class="c"><input type="checkbox" id="c-36018486" checked=""/><div class="controls bullet"><span class="by">bheadmaster</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017862">parent</a><span>|</span><a href="#36018926">prev</a><span>|</span><a href="#36018043">next</a><span>|</span><label class="collapse" for="c-36018486">[-]</label><label class="expand" for="c-36018486">[1 more]</label></div><br/><div class="children"><div class="content">Actually, <i>at least one</i> expert thinks that.</div><br/></div></div><div id="36018043" class="c"><input type="checkbox" id="c-36018043" checked=""/><div class="controls bullet"><span class="by">usui</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017862">parent</a><span>|</span><a href="#36018486">prev</a><span>|</span><a href="#36017849">next</a><span>|</span><label class="collapse" for="c-36018043">[-]</label><label class="expand" for="c-36018043">[1 more]</label></div><br/><div class="children"><div class="content">One of the biggest strawmen I&#x27;ve ever seen.</div><br/></div></div></div></div><div id="36017849" class="c"><input type="checkbox" id="c-36017849" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017681">parent</a><span>|</span><a href="#36017862">prev</a><span>|</span><a href="#36018469">next</a><span>|</span><label class="collapse" for="c-36017849">[-]</label><label class="expand" for="c-36017849">[3 more]</label></div><br/><div class="children"><div class="content">I remember a friend being confused by downloading. He didn&#x27;t realize that meant we&#x27;d have a copy and not that the source would lose it in the process.<p>It seemed bizarre at the time, but, tbh, I didn&#x27;t have _that_ much better of a model of how the whole process worked.</div><br/><div id="36018550" class="c"><input type="checkbox" id="c-36018550" checked=""/><div class="controls bullet"><span class="by">Taywee</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017849">parent</a><span>|</span><a href="#36018260">next</a><span>|</span><label class="collapse" for="c-36018550">[-]</label><label class="expand" for="c-36018550">[1 more]</label></div><br/><div class="children"><div class="content">Bit of a tangent, I asked a friend if I could rip and burn one of his CDs about 15 years ago, and he looked at me in horror and told me that I absolutely could not, because he liked that disc.<p>It took me a bit to realize what he understood my words to mean.</div><br/></div></div><div id="36018260" class="c"><input type="checkbox" id="c-36018260" checked=""/><div class="controls bullet"><span class="by">groestl</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017849">parent</a><span>|</span><a href="#36018550">prev</a><span>|</span><a href="#36018469">next</a><span>|</span><label class="collapse" for="c-36018260">[-]</label><label class="expand" for="c-36018260">[1 more]</label></div><br/><div class="children"><div class="content">&gt; we&#x27;d have a copy and not that the source would lose it in the process<p>I don&#x27;t known when this happened, but maybe that means the Anti-Piracy campaigns worked on him, confusing illegal replication with theft.</div><br/></div></div></div></div><div id="36018469" class="c"><input type="checkbox" id="c-36018469" checked=""/><div class="controls bullet"><span class="by">drpixie</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017681">parent</a><span>|</span><a href="#36017849">prev</a><span>|</span><a href="#36017737">next</a><span>|</span><label class="collapse" for="c-36018469">[-]</label><label class="expand" for="c-36018469">[2 more]</label></div><br/><div class="children"><div class="content">&gt; how to navigate the world without fully understanding or having a complete model of it<p>GPT-4 does not have ANY understanding or model of the world - it just has a model of what tokens (words) are likely to appear in a certain context. If it could build any usable model of the world, and reason about it, I&#x27;d be much more impressed.<p>When it quacks like a duck, only the most simplistic view takes it as being a duck.</div><br/><div id="36018743" class="c"><input type="checkbox" id="c-36018743" checked=""/><div class="controls bullet"><span class="by">TFYS</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018469">parent</a><span>|</span><a href="#36017737">next</a><span>|</span><label class="collapse" for="c-36018743">[-]</label><label class="expand" for="c-36018743">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t our understanding or model of the world just what images, sounds, smells, tastes and physical feelings are likely to appear in a certain context? Obviously a language model doesn&#x27;t yet have access to all the data humans have, but that doesn&#x27;t mean there can&#x27;t be some kind of a model of the world. If you invent a new word and give a couple example sentences, GPT-4 can figure out the meaning without ever seeing the word. Doesn&#x27;t that require some kind of understanding?</div><br/></div></div></div></div><div id="36017737" class="c"><input type="checkbox" id="c-36017737" checked=""/><div class="controls bullet"><span class="by">zht</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017681">parent</a><span>|</span><a href="#36018469">prev</a><span>|</span><a href="#36017668">next</a><span>|</span><label class="collapse" for="c-36017737">[-]</label><label class="expand" for="c-36017737">[2 more]</label></div><br/><div class="children"><div class="content">Now imagine if grown ass adults listened to your friend assuming he understood the world fully</div><br/><div id="36017767" class="c"><input type="checkbox" id="c-36017767" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017737">parent</a><span>|</span><a href="#36017668">next</a><span>|</span><label class="collapse" for="c-36017767">[-]</label><label class="expand" for="c-36017767">[1 more]</label></div><br/><div class="children"><div class="content">Nobody understands the world fully so that&#x27;s cool. We don&#x27;t even get sense data that isn&#x27;t distorted by the brain.</div><br/></div></div></div></div></div></div><div id="36017668" class="c"><input type="checkbox" id="c-36017668" checked=""/><div class="controls bullet"><span class="by">consilient</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017681">prev</a><span>|</span><a href="#36017629">next</a><span>|</span><label class="collapse" for="c-36017668">[-]</label><label class="expand" for="c-36017668">[13 more]</label></div><br/><div class="children"><div class="content">&gt; My current (tentative) resolution of the surprise is that language encoded way more information about reality than we thought it did. (Enough information that you can fully derive reality from language seems improbable, but iirc it did derive Othello and partly derived chess and I would have thought there wasn’t enough information in language to derive those without playing the games as well, so I can’t rule it out.)<p>The information isn&#x27;t in language itself, it&#x27;s in language <i>as actually used by humans</i>. GPT4 knows about chess because it&#x27;s &quot;read&quot; a significant fraction of everything we&#x27;ve ever written about chess. A human being who did that without ever playing a game would also start out better than a typical novice.</div><br/><div id="36017830" class="c"><input type="checkbox" id="c-36017830" checked=""/><div class="controls bullet"><span class="by">fwlr</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017668">parent</a><span>|</span><a href="#36017960">next</a><span>|</span><label class="collapse" for="c-36017830">[-]</label><label class="expand" for="c-36017830">[3 more]</label></div><br/><div class="children"><div class="content">&gt; A human being who did that without ever playing a game would also start out better than a typical novice.<p>I am quite skeptical of these arguments along the lines of “imagine a human read everything written on the topic…”.<p>What humans are doing when they read something is not what neural nets are doing when they read something. Humans are (idealistically) doing something like Feynman’s description of how he reads (or in this case, listens to) a theorem:<p>“<i>I had a scheme, which I still use today when somebody is explaining something that I’m trying to understand: I keep making up examples. For instance, the mathematicians would come in with a terrific theorem, and they’re all excited. As they’re telling me the conditions of the theorem, I construct something which fits all the conditions. You know, you have a set (one ball) – disjoint (two balls). Then the balls turn colors, grow hairs, or whatever, in my head as they put more conditions on. Finally they state the theorem, which is some dumb thing about the ball which isn’t true for my hairy green ball thing, so I say, ‘False!’&quot;</i>”<p>Bret Victor’s description of what “really good programmers” are doing is also related:<p>“<i>[showing the code for binary search]  In order to write code like this, you have to imagine an array in your head, and you essentially have to ‘play computer’. You have to simulate in your head what each line of code would do on a computer. And to a large extent those who we consider to be skilled software engineers are just those people who are really good at playing computer.</i>”<p>I think when we imagine an LLM as a human who’s read everything ever written in chess but never played an actual game, we’re actually tricking ourselves - because that hypothetical human would be ‘playing chess’ inside their head by imagining the pieces and moving them according to the rules they had read[1]. LLMs are not doing anything like that when they read about chess. So it’s a very restricted (or perhaps more accurately, a very <i>different</i>) kind of ‘reading’ that we don’t have any intuition for. Since the ‘reading’ that we <i>do</i> have an intuition for is smuggling in exactly the kind of “modeling the world” ability we’re looking for, it’s not surprising that this argument would incorrectly lead us to believe we’ve found it in LLMs.<p>1: In fact the very best computer chess is achieved by AlphaZero which was trained <i>exclusively</i> on “playing chess in its head”, and it beats even the most powerful and optimized search algorithms like Stockfish looking 20 moves ahead.</div><br/><div id="36018004" class="c"><input type="checkbox" id="c-36018004" checked=""/><div class="controls bullet"><span class="by">anybodyz</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017830">parent</a><span>|</span><a href="#36017960">next</a><span>|</span><label class="collapse" for="c-36018004">[-]</label><label class="expand" for="c-36018004">[2 more]</label></div><br/><div class="children"><div class="content">What LLM&#x27;s are doing when they imagine playing chess is what we do when we stand up after sitting on the floor, or what we do when we see a few million individual samples of color and light intensity and realize there&#x27;s an apple and a knife in front of us.<p>I think what is almost impossible for most people to understand is that AI&#x27;s do not need to be structured like the human brain and use the crutches we use to solve problems the way we do because evolution did not provide us with a way of instantly understanding complex physics or instantly absorbing the structure of a computer program by seeing it&#x27;s code in one shot.</div><br/><div id="36018320" class="c"><input type="checkbox" id="c-36018320" checked=""/><div class="controls bullet"><span class="by">groestl</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018004">parent</a><span>|</span><a href="#36017960">next</a><span>|</span><label class="collapse" for="c-36018320">[-]</label><label class="expand" for="c-36018320">[1 more]</label></div><br/><div class="children"><div class="content">Also, there is no reason to believe that playing chess in our head is anything else but us pattern matching a mental process on a higher level, recognizing a simulation there, and feeding that info back into the loop below. Nature provided us with a complex, layered and circular architecture of the brain, but the rest is pretty much training that structure. And we know that different architectures with similar outcome are possible, since there are vast variations across our own species, and other species as well, with essentially the same capabilities.</div><br/></div></div></div></div></div></div><div id="36017960" class="c"><input type="checkbox" id="c-36017960" checked=""/><div class="controls bullet"><span class="by">wombatpm</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017668">parent</a><span>|</span><a href="#36017830">prev</a><span>|</span><a href="#36017876">next</a><span>|</span><label class="collapse" for="c-36017960">[-]</label><label class="expand" for="c-36017960">[2 more]</label></div><br/><div class="children"><div class="content">So if we gathered all books, writings, games, etc on ever published on chess could we develop a grandmaster player without having to build a deep blue type system?</div><br/><div id="36018409" class="c"><input type="checkbox" id="c-36018409" checked=""/><div class="controls bullet"><span class="by">anon84873628</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017960">parent</a><span>|</span><a href="#36017876">next</a><span>|</span><label class="collapse" for="c-36018409">[-]</label><label class="expand" for="c-36018409">[1 more]</label></div><br/><div class="children"><div class="content">In that scenario, I believe a current LLM would &quot;understand&quot; chess. It would know how to play the game, it would know what being a grandmaster means, and it would know the heuristics and algorithms for evaluating a given game state. But it would not be able to implement an optimized solver to actually play the game efficiently. To do that would require it to also have all the knowledge about computer science.<p>Hence the dream of singularity. Can you teach ChatGPT to build AlphaZero?</div><br/></div></div></div></div><div id="36017876" class="c"><input type="checkbox" id="c-36017876" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017668">parent</a><span>|</span><a href="#36017960">prev</a><span>|</span><a href="#36017629">next</a><span>|</span><label class="collapse" for="c-36017876">[-]</label><label class="expand" for="c-36017876">[7 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s &quot;read&quot; a significant fraction of everything we&#x27;ve ever written about chess<p>I doubt heavily that a significant fraction of chess&#x27;s writings are even available in digital format, much less inside of CommonCrawl and correctly trained on.</div><br/><div id="36017966" class="c"><input type="checkbox" id="c-36017966" checked=""/><div class="controls bullet"><span class="by">beardedwizard</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017876">parent</a><span>|</span><a href="#36017629">next</a><span>|</span><label class="collapse" for="c-36017966">[-]</label><label class="expand" for="c-36017966">[6 more]</label></div><br/><div class="children"><div class="content">Why would common crawl NOT contain a chess manual? the rules are explained in detail on wikipedia. the simplest conclusion is that it has indeed been trained on a chess manual and is good at predicting what the next word in a chess manual is. it is not synthesizing anything.</div><br/><div id="36018316" class="c"><input type="checkbox" id="c-36018316" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017966">parent</a><span>|</span><a href="#36018603">next</a><span>|</span><label class="collapse" for="c-36018316">[-]</label><label class="expand" for="c-36018316">[3 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t seen this mentioned but can LLMs actually play chess?<p>I&#x27;m sure they have read rules of chess online, but if you ask them to play chess with you, what happens? Can they apply the rules? Can they apply them intelligently and win the game?<p>My point is that even though LLMs &quot;know&quot; what the rules of chess are, they don&#x27;t really &quot;understand&quot; them, unless they can use them to play the game and play it well.</div><br/><div id="36018453" class="c"><input type="checkbox" id="c-36018453" checked=""/><div class="controls bullet"><span class="by">consilient</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018316">parent</a><span>|</span><a href="#36018424">next</a><span>|</span><label class="collapse" for="c-36018453">[-]</label><label class="expand" for="c-36018453">[1 more]</label></div><br/><div class="children"><div class="content">GPT4 can play chess, but not especially well. For example: <a href="https:&#x2F;&#x2F;lichess.org&#x2F;@&#x2F;oopsallbots-gpt-4" rel="nofollow">https:&#x2F;&#x2F;lichess.org&#x2F;@&#x2F;oopsallbots-gpt-4</a></div><br/></div></div><div id="36018424" class="c"><input type="checkbox" id="c-36018424" checked=""/><div class="controls bullet"><span class="by">anon84873628</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018316">parent</a><span>|</span><a href="#36018453">prev</a><span>|</span><a href="#36018603">next</a><span>|</span><label class="collapse" for="c-36018424">[-]</label><label class="expand" for="c-36018424">[1 more]</label></div><br/><div class="children"><div class="content">You win chess by &quot;applying the rules intelligently&quot; the same way you launch rockets to space by applying the laws of physics intelligently. True, yes, but not as simple as it sounds.</div><br/></div></div></div></div><div id="36018603" class="c"><input type="checkbox" id="c-36018603" checked=""/><div class="controls bullet"><span class="by">mejutoco</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017966">parent</a><span>|</span><a href="#36018316">prev</a><span>|</span><a href="#36018204">next</a><span>|</span><label class="collapse" for="c-36018603">[-]</label><label class="expand" for="c-36018603">[1 more]</label></div><br/><div class="children"><div class="content">&quot;A chess manual&quot; vs the finer point of OP &quot;a significant fraction of chess&#x27;s writings&quot;</div><br/></div></div><div id="36018204" class="c"><input type="checkbox" id="c-36018204" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017966">parent</a><span>|</span><a href="#36018603">prev</a><span>|</span><a href="#36017629">next</a><span>|</span><label class="collapse" for="c-36018204">[-]</label><label class="expand" for="c-36018204">[1 more]</label></div><br/><div class="children"><div class="content">I never said that CommonCrawl doesn&#x27;t have a chess manual, whatever the hell that might be.<p>&gt; the simplest conclusion is that it has indeed been trained on a chess manual<p>That is an absurd conclusion that is based on literally fantasy.</div><br/></div></div></div></div></div></div></div></div><div id="36017629" class="c"><input type="checkbox" id="c-36017629" checked=""/><div class="controls bullet"><span class="by">JoshTko</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017668">prev</a><span>|</span><a href="#36018136">next</a><span>|</span><label class="collapse" for="c-36017629">[-]</label><label class="expand" for="c-36017629">[23 more]</label></div><br/><div class="children"><div class="content">The more I think about it the more I&#x27;m convinced I am basically just predicting&#x2F;saying my next word whenever I speak.</div><br/><div id="36018230" class="c"><input type="checkbox" id="c-36018230" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36017996">next</a><span>|</span><label class="collapse" for="c-36018230">[-]</label><label class="expand" for="c-36018230">[3 more]</label></div><br/><div class="children"><div class="content">Honestly I feel like one reason people are struggling with this is they can&#x27;t accept a critical part of the truth : most people are stochastic parrots themselves most of the time. True, creative genuine deep thinking is an exceptional state of thinking for us.</div><br/><div id="36018596" class="c"><input type="checkbox" id="c-36018596" checked=""/><div class="controls bullet"><span class="by">DJHenk</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018230">parent</a><span>|</span><a href="#36017996">next</a><span>|</span><label class="collapse" for="c-36018596">[-]</label><label class="expand" for="c-36018596">[2 more]</label></div><br/><div class="children"><div class="content">Years ago I transitioned from a developer role to a manager role and suddenly I had to do a lot more talking. Not all the talking needs to be a deeply involved exchange of complex ideas, a lot of it serves a different purpose. Sometimes it can be a simple as filling up the time in a pleasant way with a group of people that may or may not know each other that well.<p>After getting some experience with this I noticed that I had developed a talking on&#x2F;off button in my head. I could just simply turn it on and start talking. I could generate words that sounded good together and fit the purpose of the moment. But They just seemed to come from a different place in my brain than my conscious mind. Because that was not involved in this process at all. The only job my mind had was to turn the button off again at the right moment, for the rest it was free to think whatever it wanted.<p>(I transferred back to development a couple of years later.)</div><br/><div id="36018781" class="c"><input type="checkbox" id="c-36018781" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018596">parent</a><span>|</span><a href="#36017996">next</a><span>|</span><label class="collapse" for="c-36018781">[-]</label><label class="expand" for="c-36018781">[1 more]</label></div><br/><div class="children"><div class="content">The very fact that we can be conscious about our ability to generate language and work it separately from the rest of our consciousness tells me that chatbots might emulate our language generation ability. We shouldn&#x27;t invert it to conclude that means they can reason about it.</div><br/></div></div></div></div></div></div><div id="36017996" class="c"><input type="checkbox" id="c-36017996" checked=""/><div class="controls bullet"><span class="by">cageface</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36018230">prev</a><span>|</span><a href="#36018774">next</a><span>|</span><label class="collapse" for="c-36017996">[-]</label><label class="expand" for="c-36017996">[4 more]</label></div><br/><div class="children"><div class="content">Maybe in casual conversation but that&#x27;s not how I experience my though process about anything non trivial at all. I usually spend a lot of time thinking about the concept in non verbal terms and that process involves recalling images and sensory information in fairly abstract terms and then through what feels like several iterations it starts to coalesce into something I can encode in language.<p>I think we can all agree that these LLMs are surprisingly good at generating text that is often coherent but I don&#x27;t see how you can discard all those extra inputs and claim you have the same process.</div><br/><div id="36018147" class="c"><input type="checkbox" id="c-36018147" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017996">parent</a><span>|</span><a href="#36018774">next</a><span>|</span><label class="collapse" for="c-36018147">[-]</label><label class="expand" for="c-36018147">[3 more]</label></div><br/><div class="children"><div class="content"><i>&gt; thinking about the concept in non verbal ... abstract terms</i><p>The abstract terms we think about are concepts, and we think about multiple concepts, at various levels of abstraction, and their relationships to each other, before getting a sense of what we want to say or write.<p>Only then do we begin speaking or writing, grouping concepts into paragraphs, breaking them down into sentences and words.<p>And there&#x27;s evidence that LLMs do something similar, creating embeddings for both big ideas and small details, modeling how the small details combine into larger concepts, discovering the relationships between concepts, and only then generating a probabilistic sequence of tokens to express those deeper concepts.</div><br/><div id="36018515" class="c"><input type="checkbox" id="c-36018515" checked=""/><div class="controls bullet"><span class="by">cageface</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018147">parent</a><span>|</span><a href="#36018774">next</a><span>|</span><label class="collapse" for="c-36018515">[-]</label><label class="expand" for="c-36018515">[2 more]</label></div><br/><div class="children"><div class="content">Right but they&#x27;re limited to concepts that are derived from linguistic tokens. Human reasoning can include nonverbal inputs.<p>For example asking GPT what would be a reasonable way to stack a list of arbitrary objects on top of each other.</div><br/><div id="36018797" class="c"><input type="checkbox" id="c-36018797" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018515">parent</a><span>|</span><a href="#36018774">next</a><span>|</span><label class="collapse" for="c-36018797">[-]</label><label class="expand" for="c-36018797">[1 more]</label></div><br/><div class="children"><div class="content">Lately I&#x27;ve been think that staple of science fiction, the android robot, is the only way to get AGI. I thought a lot about computer vision when that was the bleeding edge of AI a few years back. It strikes me as obviously wrong to try training models on two dimensional arrays of pixels and expect them to develop visual skills. Why don&#x27;t we give them two cameras and arms so they can learn about physics and start differentiating 3 dimensional input? I think when we have robots capable of interacting with the world as humans do we might be able to train them to develop near human levels of intelligence.</div><br/></div></div></div></div></div></div></div></div><div id="36018774" class="c"><input type="checkbox" id="c-36018774" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36017996">prev</a><span>|</span><a href="#36017827">next</a><span>|</span><label class="collapse" for="c-36018774">[-]</label><label class="expand" for="c-36018774">[1 more]</label></div><br/><div class="children"><div class="content">Can you think without words? Can you imagine how it feels to swim, something you might have done, and extend it to fly like a bird? Can you read a novel and feel emotions about characters that don&#x27;t exist beyond text descriptions of their actions? Language is a way to encode thought. It&#x27;s not what thoughts are made of.</div><br/></div></div><div id="36017827" class="c"><input type="checkbox" id="c-36017827" checked=""/><div class="controls bullet"><span class="by">quad_eye_oh</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36018774">prev</a><span>|</span><a href="#36017856">next</a><span>|</span><label class="collapse" for="c-36017827">[-]</label><label class="expand" for="c-36017827">[2 more]</label></div><br/><div class="children"><div class="content">This is a thought that LLMs caused me to have: the emotion of surprise is just the brain model of the world discovering that it had been poorly calibrated, or making poor predictions.</div><br/><div id="36018019" class="c"><input type="checkbox" id="c-36018019" checked=""/><div class="controls bullet"><span class="by">Hendrikto</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017827">parent</a><span>|</span><a href="#36017856">next</a><span>|</span><label class="collapse" for="c-36018019">[-]</label><label class="expand" for="c-36018019">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like the free energy principle [0]:<p>&gt; The free energy principle is based on the Bayesian idea of the brain as an “inference engine.” Under the free energy principle, systems pursue paths of least surprise, or equivalently, minimize the difference between predictions based on their model of the world and their sense and associated perception.<p>[0]: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Free_energy_principle" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Free_energy_principle</a></div><br/></div></div></div></div><div id="36017856" class="c"><input type="checkbox" id="c-36017856" checked=""/><div class="controls bullet"><span class="by">mewse</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36017827">prev</a><span>|</span><a href="#36017816">next</a><span>|</span><label class="collapse" for="c-36017856">[-]</label><label class="expand" for="c-36017856">[4 more]</label></div><br/><div class="children"><div class="content">The question is whether you knew how that sentence was going to end when you started writing it, or indeed whether <i>I</i> knew that I was going to add this comma-separated adjunct when I started writing the preceding clause, and I cannot honestly say at this precise moment of typing whether the final word in this sentence is going to end up being &#x27;yes&#x27; or &#x27;no&#x27;.</div><br/><div id="36018126" class="c"><input type="checkbox" id="c-36018126" checked=""/><div class="controls bullet"><span class="by">diffxx</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017856">parent</a><span>|</span><a href="#36018048">next</a><span>|</span><label class="collapse" for="c-36018126">[-]</label><label class="expand" for="c-36018126">[2 more]</label></div><br/><div class="children"><div class="content">For me it&#x27;s a little more subtle. When I sit down to write, say, an email, I usually have a specific point that I want to make but I usually haven&#x27;t yet figured out exactly how I want to get there. I&#x27;m not even sure it is even possible to conceive of writing without doing it word by word. Perhaps the difference though is that we have the ability to go forward and backwards in the sense that we can usually think faster than we can type or write which allows our thoughts to advance beyond the tokens we have produced. This allows for a kind of real time editing process that incorporates both past and possible future information. But I imagine there us probably a way to enhance LLMs to branch and edit before producing the next word materially (if they aren&#x27;t already doing that).<p>To go even more meta, there is an analogy I&#x27;m trying to make right now in which I am visualizing a road and thinking about how describing the road relates to the process of writing. In my mind&#x27;s eye, I can see the full length of the road and all of its contours but I can&#x27;t actually describe the individual stretches of the road coherently without enumerating them. Something similar happens with writing. I can visualize what I want to say far beyond the next word, but it&#x27;s true that the actual process of writing goes word to word, much like how the process of token selection is described for an LLM. The question is whether the LLM has an analogous conception of where it is going. Going back to the process above, sometimes I know where I am going and haven&#x27;t yet figured out how to articulate it yet. It is through the process of writing that I am able to articulate that thought. But the thought preceded my articulation of it. I don&#x27;t know to what extent LLMs have coherent thoughts that they are articulating or if that even makes sense for the type of intelligence they project. My suspicion is that they don&#x27;t have additional sensory inputs beyond language that give thoughts the immaterial shape that then is expressed in language. Without that, I am skeptical that they will truly get beyond regurgitating and&#x2F;or remixing what has already been fed to them textually. That doesn&#x27;t diminish how amazing they are, but I am somewhat more in the Brooks&#x2F;Knuth camp that they are impressive and surprising, but there is something that ultimately leaves me a bit cold about them.</div><br/><div id="36018699" class="c"><input type="checkbox" id="c-36018699" checked=""/><div class="controls bullet"><span class="by">iainmerrick</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018126">parent</a><span>|</span><a href="#36018048">next</a><span>|</span><label class="collapse" for="c-36018699">[-]</label><label class="expand" for="c-36018699">[1 more]</label></div><br/><div class="children"><div class="content"><i>I usually have a specific point that I want to make but I usually haven&#x27;t yet figured out exactly how I want to get there. I&#x27;m not even sure it is even possible to conceive of writing without doing it word by word. [...] In my mind&#x27;s eye, I can see the full length of the road and all of its contours but I can&#x27;t actually describe the individual stretches of the road coherently without enumerating them.</i><p>Not to trivialise the interesting point you’re making, but do you never write with an outline? Write bullets for the big points you want to touch, then go back and flesh out the details?</div><br/></div></div></div></div><div id="36018048" class="c"><input type="checkbox" id="c-36018048" checked=""/><div class="controls bullet"><span class="by">Mertax</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017856">parent</a><span>|</span><a href="#36018126">prev</a><span>|</span><a href="#36017816">next</a><span>|</span><label class="collapse" for="c-36018048">[-]</label><label class="expand" for="c-36018048">[1 more]</label></div><br/><div class="children"><div class="content">I do wonder how well (quickly) the whole thought is formed in your head and it’s just the encoding into language that tricks you into thinking you didn’t know what the word would be.<p>Having learned another language, the moment you start to feel “fluent” is when you start speaking <i>first</i> in the 2nd language and aren’t using your first language as an intermediate step to translate to your 2nd language.</div><br/></div></div></div></div><div id="36017816" class="c"><input type="checkbox" id="c-36017816" checked=""/><div class="controls bullet"><span class="by">morkalork</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36017856">prev</a><span>|</span><a href="#36018479">next</a><span>|</span><label class="collapse" for="c-36017816">[-]</label><label class="expand" for="c-36017816">[3 more]</label></div><br/><div class="children"><div class="content">Why not? Your brain is already making predictions about what you expect to see and hear as a part of your perception of reality anyways.</div><br/><div id="36017848" class="c"><input type="checkbox" id="c-36017848" checked=""/><div class="controls bullet"><span class="by">quad_eye_oh</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017816">parent</a><span>|</span><a href="#36018479">next</a><span>|</span><label class="collapse" for="c-36017848">[-]</label><label class="expand" for="c-36017848">[2 more]</label></div><br/><div class="children"><div class="content">Right, but LLMs suggest that learning how to predict and only training on predictions is  sufficient to learn anything and to have emergent generative abilities. What if learning to predict the upcoming input is all that&#x27;s all that is needed for general human intelligence? What if it is all that any animals do?</div><br/><div id="36017933" class="c"><input type="checkbox" id="c-36017933" checked=""/><div class="controls bullet"><span class="by">20after4</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017848">parent</a><span>|</span><a href="#36018479">next</a><span>|</span><label class="collapse" for="c-36017933">[-]</label><label class="expand" for="c-36017933">[1 more]</label></div><br/><div class="children"><div class="content">This right here. I actually am strongly starting to believe that this is indeed what&#x27;s going on.<p>I read the book Kingdom of Speech a few years ago and that also left me with the perspective that perhaps language has a lot more to do with how we think and perceive the world than most people like to admit.  The book has been heavily criticized but I believe it made an interesting point about language.</div><br/></div></div></div></div></div></div><div id="36018479" class="c"><input type="checkbox" id="c-36018479" checked=""/><div class="controls bullet"><span class="by">anon84873628</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36017816">prev</a><span>|</span><a href="#36018181">next</a><span>|</span><label class="collapse" for="c-36018479">[-]</label><label class="expand" for="c-36018479">[3 more]</label></div><br/><div class="children"><div class="content">The sad truth is that we are all NPCs without free will. &quot;Thoughts&quot; are a non-physical phenomenon that, seemingly, arise from physical activity in the brain. The thoughts (our &quot;experience&quot;) come <i>after</i> the electrochemical reaction that manifests them. How could these non-physical phenomena then turn around and influence the physical chemistry? They can&#x27;t. By the time we experience a thought, the physical state for it has been represented, and that state causes the next one after it. Like a stone bouncing down a hill - chaotic but still deterministic. We&#x27;re just along for the ride.</div><br/><div id="36018801" class="c"><input type="checkbox" id="c-36018801" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018479">parent</a><span>|</span><a href="#36018666">next</a><span>|</span><label class="collapse" for="c-36018801">[-]</label><label class="expand" for="c-36018801">[1 more]</label></div><br/><div class="children"><div class="content">Reducing a debate that has been raging for millennia across several disciplines to three lines of text tells me you don&#x27;t have enough of a grasp on the subject to draw any conclusions about it, let alone resolve it altogether.</div><br/></div></div><div id="36018666" class="c"><input type="checkbox" id="c-36018666" checked=""/><div class="controls bullet"><span class="by">antihipocrat</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018479">parent</a><span>|</span><a href="#36018801">prev</a><span>|</span><a href="#36018181">next</a><span>|</span><label class="collapse" for="c-36018666">[-]</label><label class="expand" for="c-36018666">[1 more]</label></div><br/><div class="children"><div class="content">The act of being conscious of our own thoughts and using them to influence our physical state is skill practiced by many people with success.</div><br/></div></div></div></div><div id="36018181" class="c"><input type="checkbox" id="c-36018181" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36018479">prev</a><span>|</span><a href="#36018072">next</a><span>|</span><label class="collapse" for="c-36018181">[-]</label><label class="expand" for="c-36018181">[1 more]</label></div><br/><div class="children"><div class="content">I believe this view of humans is because we work in increasingly deterministic environments (the internet) using predictable methods of communication, text.<p>We’ve kind of made the world a bit boring and deterministic, places where almost perfect is knowledge is obtainable and so everything feels more and more predictable.<p>Your day probably consists of using Google, talking on platforms that don’t change much, solving already solved coding problems and communicating with others about office politics problems that we’ve all spoke about over and over again. We literally are just chatbots in this world.</div><br/></div></div><div id="36018072" class="c"><input type="checkbox" id="c-36018072" checked=""/><div class="controls bullet"><span class="by">astrea</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017629">parent</a><span>|</span><a href="#36018181">prev</a><span>|</span><a href="#36018136">next</a><span>|</span><label class="collapse" for="c-36018072">[-]</label><label class="expand" for="c-36018072">[1 more]</label></div><br/><div class="children"><div class="content">But you and your brain are much more than the language-forming aspect.</div><br/></div></div></div></div><div id="36018136" class="c"><input type="checkbox" id="c-36018136" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017629">prev</a><span>|</span><a href="#36019031">next</a><span>|</span><label class="collapse" for="c-36018136">[-]</label><label class="expand" for="c-36018136">[11 more]</label></div><br/><div class="children"><div class="content">&gt; One way to resolve this surprise is to find some reason to believe these strange abilities are fundamentally not an understanding of the world. Thus stochastic parrots, this article, Yan LeCun and Chomsky, etc.<p>I mean, it is kind of obvious that predicting text based on a large corpus of text written by people with a model of the world will, if it works at all, look like having a model of the world.<p>The question is whether that <i>involves</i> having a model of the world, and secondarily, if there is a difference, what is the practical impact of the difference?<p>Or maybe that’s <i>not</i> really the question, because the whole “model of the world” thing is metaphysical omphaloskepsis that is inherently unanswerable because we can’t actually unambiguously reduce it to something with testable empirical predictions, reflecting a rationalization for elements of our culture and particularly our own view of our own special place in the universe, and the different answers to it likewise have no real meaning but simply reflect people’s bias for whether or not they want to share that special place, either in general or with the particular computing systems under discussion in particular.</div><br/><div id="36018302" class="c"><input type="checkbox" id="c-36018302" checked=""/><div class="controls bullet"><span class="by">tornato7</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018136">parent</a><span>|</span><a href="#36018808">next</a><span>|</span><label class="collapse" for="c-36018302">[-]</label><label class="expand" for="c-36018302">[9 more]</label></div><br/><div class="children"><div class="content">The one thing that has convinced me that chatGPT has built a real world model is asking it how to stack different objects. You can ask it for the safest way to stack a pillow, an egg, a nail, a book and an action figure. Even get more complicated. GPT-4 will, most of the time, correctly reason through how to stack these items to form a stable structure without breaking. That tells me it knows intimately the physical properties of all of these objects and how they interact with each other.</div><br/><div id="36018372" class="c"><input type="checkbox" id="c-36018372" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018302">parent</a><span>|</span><a href="#36018808">next</a><span>|</span><label class="collapse" for="c-36018372">[-]</label><label class="expand" for="c-36018372">[8 more]</label></div><br/><div class="children"><div class="content">Or was that one of the training samples provided to gpt 3.5?<p>It is going to be increasingly harder to divide between what it was in the training set and what is creative thinking going forward.<p>Like, at the very minimum, all the publicly known reasoning samples that were used to probe gpt version 3.5 downward are going to be unreliable for knowing what gpt4 reasoning abilities are</div><br/><div id="36018448" class="c"><input type="checkbox" id="c-36018448" checked=""/><div class="controls bullet"><span class="by">tornato7</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018372">parent</a><span>|</span><a href="#36018808">next</a><span>|</span><label class="collapse" for="c-36018448">[-]</label><label class="expand" for="c-36018448">[7 more]</label></div><br/><div class="children"><div class="content">The point of the question is that you can easily come up with combinations of objects that are unlikely, or statistically impossible, to be in their training samples. How about this one? &quot;Tell me how to safely stack the following objects on top of each other without any additional support: A Nikon DSLR, A small succulent, 4 lightbulbs, a pack of gummy worms, and a hardcover copy of Harry Potter and the Halfblood Prince.&quot;<p>Hardcover copy of Harry Potter and the Half-Blood Prince: Start with the largest, heaviest, and most stable object. This will serve as your base. Make sure to place it on a flat, secure surface.<p>Pack of gummy worms: This item is fairly light and flexible. It should sit nicely on the book without risking damage to either. You can place it right in the center to maintain balance.<p>Nikon DSLR: This is a heavy object with a fairly stable shape. Put this on top of the pack of gummy worms, as it&#x27;s sturdy enough to handle the weight and won&#x27;t be harmed by the soft package. Make sure to place it so that the lens is pointing upward and it sits flat and stable. Don&#x27;t use any protruding lenses or parts to balance it, as they could easily be damaged.<p>Four Lightbulbs: Since light bulbs are round and fragile, you will need to stack them with care. If the bulbs are in a box, you can place the box on top of the DSLR, making sure it&#x27;s centered and stable. If the bulbs are loose, it&#x27;s a bit more tricky. You might want to nestle them into the lens of the DSLR (if the lens is large enough), which should provide some natural containment for them. If not, it might be safer not to include the lightbulbs in your stack.<p>A small succulent: Finally, the succulent can be placed on top. It&#x27;s likely the lightest object and it is usually quite stable due to its pot. If the lightbulbs were in a box, place the succulent on top of that. If you ended up not including the lightbulbs, place the succulent directly on top of the DSLR.</div><br/><div id="36018663" class="c"><input type="checkbox" id="c-36018663" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018448">parent</a><span>|</span><a href="#36018549">next</a><span>|</span><label class="collapse" for="c-36018663">[-]</label><label class="expand" for="c-36018663">[5 more]</label></div><br/><div class="children"><div class="content">Reads well if you don&#x27;t think about it too much... For example: Where does the succulent go if the light bulbs are nestled into the lens of the DLSR? Balanced on the light bulbs? Why would the gummy worm package need to be in the center of the book to maintain balance?</div><br/><div id="36018754" class="c"><input type="checkbox" id="c-36018754" checked=""/><div class="controls bullet"><span class="by">tornato7</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018663">parent</a><span>|</span><a href="#36018549">next</a><span>|</span><label class="collapse" for="c-36018754">[-]</label><label class="expand" for="c-36018754">[4 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;feEiiZA.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;feEiiZA.png</a><p>I tried to stack all of these objects myself and couldn&#x27;t really. I think GPT-4&#x27;s approach is actually really good. It correctly points out that the gummy worms make a flexible base for the DSLR (otherwise the protruding buttons&#x2F;viewfinder make it wobbly on the hard book), and the light bulbs are able to nestle into the front of the lens. If they were smaller light bulbs I could probably use the four of them as a small base on top of the lens to host the succulent.</div><br/><div id="36018778" class="c"><input type="checkbox" id="c-36018778" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018754">parent</a><span>|</span><a href="#36018549">next</a><span>|</span><label class="collapse" for="c-36018778">[-]</label><label class="expand" for="c-36018778">[3 more]</label></div><br/><div class="children"><div class="content">Might also put the light bulbs as a base (especially if in a box). They are pretty sturdy and can hold a book.</div><br/><div id="36018825" class="c"><input type="checkbox" id="c-36018825" checked=""/><div class="controls bullet"><span class="by">tornato7</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018778">parent</a><span>|</span><a href="#36018549">next</a><span>|</span><label class="collapse" for="c-36018825">[-]</label><label class="expand" for="c-36018825">[2 more]</label></div><br/><div class="children"><div class="content">The point is that ChatGPT undeniably built a world model good enough to understand the physical and three-dimensional properties of these items pretty well, and it gives me a somewhat workable way to stack them, despite never having seen that in its training data.</div><br/><div id="36018871" class="c"><input type="checkbox" id="c-36018871" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018825">parent</a><span>|</span><a href="#36018549">next</a><span>|</span><label class="collapse" for="c-36018871">[-]</label><label class="expand" for="c-36018871">[1 more]</label></div><br/><div class="children"><div class="content">You cannot conclude that from the output - the training data will likely contain a lot stacking things. Everyday objects also might have some stacking properties that make these questions easy to answer even with semi-random answers.<p>Plus, some stuff clearly makes no sense or is ignored (like the gummy worms in the center, forgetting about the succulent in some cases).<p>If you want to test world modeling, give it objects it will have never encountered, describe them and then ask to stack etc. For example, a bunch of 7 dimensional objects that can only be stacked a certain way.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36018808" class="c"><input type="checkbox" id="c-36018808" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018136">parent</a><span>|</span><a href="#36018302">prev</a><span>|</span><a href="#36019031">next</a><span>|</span><label class="collapse" for="c-36018808">[-]</label><label class="expand" for="c-36018808">[1 more]</label></div><br/><div class="children"><div class="content">Extra points for teaching me a new word for navel gazing. But I think your reasoning about it devolved into navel gazing at the end.</div><br/></div></div></div></div><div id="36019031" class="c"><input type="checkbox" id="c-36019031" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018136">prev</a><span>|</span><a href="#36018310">next</a><span>|</span><label class="collapse" for="c-36019031">[-]</label><label class="expand" for="c-36019031">[1 more]</label></div><br/><div class="children"><div class="content">&gt; My current (tentative) resolution of the surprise is that language encoded way more information about reality than we thought it did.<p>&quot;A whole mythology is deposited in our language&quot; - Wittgenstein</div><br/></div></div><div id="36018310" class="c"><input type="checkbox" id="c-36018310" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36019031">prev</a><span>|</span><a href="#36018907">next</a><span>|</span><label class="collapse" for="c-36018310">[-]</label><label class="expand" for="c-36018310">[2 more]</label></div><br/><div class="children"><div class="content">There is in fact compelling evidence that language models can learn some world models.<p>Here&#x27;s a research paper on this and a blog post by the lead author summarizing the results.<p>Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task.
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382</a><p>Do Large Language Models learn world models or just surface statistics?
<a href="https:&#x2F;&#x2F;thegradient.pub&#x2F;othello&#x2F;" rel="nofollow">https:&#x2F;&#x2F;thegradient.pub&#x2F;othello&#x2F;</a><p>-----<p>An argument based on common sense can also be made: 
Any system that possesses a wide range of capabilities, most of which it was not specifically trained to perform, cannot possibly perform all these tasks so well solely by making probabilistic guesses.<p>(Humans, too, were not directly shaped by natural and sexual selection to possess all of our cognitive capacities.)</div><br/><div id="36018574" class="c"><input type="checkbox" id="c-36018574" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018310">parent</a><span>|</span><a href="#36018907">next</a><span>|</span><label class="collapse" for="c-36018574">[-]</label><label class="expand" for="c-36018574">[1 more]</label></div><br/><div class="children"><div class="content">Beyond compelling evidence, it has been our working theory for how neural networks generalize for many years now.</div><br/></div></div></div></div><div id="36018907" class="c"><input type="checkbox" id="c-36018907" checked=""/><div class="controls bullet"><span class="by">blazespin</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018310">prev</a><span>|</span><a href="#36017648">next</a><span>|</span><label class="collapse" for="c-36018907">[-]</label><label class="expand" for="c-36018907">[3 more]</label></div><br/><div class="children"><div class="content">The resolution is actually fairly simple.  It&#x27;s an incredibly brilliant stochastic parrot with some limited reasoning capabilities.<p>Some folks will try to say it cannot reason, but they are wrong, there is extensive proof of that.<p>The only question is how limited are its reasoning capabilities.  After spending extensive time on openai&#x2F;evals, having submitted 3 of my own, and doing a lot of tests, I would argue that an average person of average IQ could out think GPT4 - as long as the stochastic parrot aspect wasn&#x27;t a factor.</div><br/><div id="36018929" class="c"><input type="checkbox" id="c-36018929" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018907">parent</a><span>|</span><a href="#36017648">next</a><span>|</span><label class="collapse" for="c-36018929">[-]</label><label class="expand" for="c-36018929">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very hard to know because we don&#x27;t know and can&#x27;t experiment with its training data. So - it may be doing first principle reasoning, or it may be doing token substitution vs. some known example that it&#x27;s seen before and is matching to.</div><br/><div id="36018947" class="c"><input type="checkbox" id="c-36018947" checked=""/><div class="controls bullet"><span class="by">blazespin</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018929">parent</a><span>|</span><a href="#36017648">next</a><span>|</span><label class="collapse" for="c-36018947">[-]</label><label class="expand" for="c-36018947">[1 more]</label></div><br/><div class="children"><div class="content">The dictionary definition of reasoning says nothing about how the thinking is done, only that it&#x27;s sensible and logical, which is exactly what GPT4 is.  Limited, yes, but it reasons.</div><br/></div></div></div></div></div></div><div id="36017648" class="c"><input type="checkbox" id="c-36017648" checked=""/><div class="controls bullet"><span class="by">mamp</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018907">prev</a><span>|</span><a href="#36017701">next</a><span>|</span><label class="collapse" for="c-36017648">[-]</label><label class="expand" for="c-36017648">[1 more]</label></div><br/><div class="children"><div class="content">It turns out to accurately predict the next word requires huge amounts of implicit contextual knowledge i.e. understanding about the world.<p>Next word prediction was the trigger, the optimisation for the task results in a broad (but currently unreliable) model of the world.</div><br/></div></div><div id="36017701" class="c"><input type="checkbox" id="c-36017701" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017648">prev</a><span>|</span><a href="#36018267">next</a><span>|</span><label class="collapse" for="c-36017701">[-]</label><label class="expand" for="c-36017701">[2 more]</label></div><br/><div class="children"><div class="content">I think the main issue is that since we don&#x27;t have access to the implementation of GPT-4, there&#x27;s no way for researchers to know how it works, so either they&#x27;re not saying anything or they&#x27;re just making things up.<p>GPT-4 is &quot;multimodal&quot; and RLHF&#x27;d, so it was trained with some tasks other than next word prediction. I don&#x27;t remember if it&#x27;s been trained for code correctness (by running unit tests etc.), but other models have been.</div><br/><div id="36017896" class="c"><input type="checkbox" id="c-36017896" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017701">parent</a><span>|</span><a href="#36018267">next</a><span>|</span><label class="collapse" for="c-36017896">[-]</label><label class="expand" for="c-36017896">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not much of a problem, as it only affects the &quot;GPT&quot; LLM.<p>The same researchers can always study comparable in behavior LLMs like Llama and its descendants.</div><br/></div></div></div></div><div id="36018267" class="c"><input type="checkbox" id="c-36018267" checked=""/><div class="controls bullet"><span class="by">notShabu</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017701">prev</a><span>|</span><a href="#36018575">next</a><span>|</span><label class="collapse" for="c-36018267">[-]</label><label class="expand" for="c-36018267">[6 more]</label></div><br/><div class="children"><div class="content">Has there been any research in the possibility that much of what we say&#x2F;speak (including this convo) is actually just stochastic parroting?<p>e.g. instead of the stochastic parrots mimicking intelligence maybe intelligence doesn&#x27;t exist, it&#x27;s just stochastic parrots of various levels of sophistications organized into a hierarchy. &quot;Intelligence&quot; is necessarily socially defined with the more complex parrots being unpredictable and &quot;intelligent&quot; from the POV of lower parrots. Vicer versa, looking down, the lower parrots seem to act like &quot;NPCs&quot;</div><br/><div id="36018884" class="c"><input type="checkbox" id="c-36018884" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018267">parent</a><span>|</span><a href="#36018579">next</a><span>|</span><label class="collapse" for="c-36018884">[-]</label><label class="expand" for="c-36018884">[1 more]</label></div><br/><div class="children"><div class="content">BF Skinner, “Verbal Behavior”. Critiquing that book is what put Chomsky on the map.</div><br/></div></div><div id="36018579" class="c"><input type="checkbox" id="c-36018579" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018267">parent</a><span>|</span><a href="#36018884">prev</a><span>|</span><a href="#36018288">next</a><span>|</span><label class="collapse" for="c-36018579">[-]</label><label class="expand" for="c-36018579">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Has there been any research in the possibility that much of what we say&#x2F;speak (including this convo) is actually just stochastic parroting?<p>I don&#x27;t think that is really a well defined question. What is a stochastic parrot?</div><br/></div></div><div id="36018288" class="c"><input type="checkbox" id="c-36018288" checked=""/><div class="controls bullet"><span class="by">Sai_</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018267">parent</a><span>|</span><a href="#36018579">prev</a><span>|</span><a href="#36018575">next</a><span>|</span><label class="collapse" for="c-36018288">[-]</label><label class="expand" for="c-36018288">[3 more]</label></div><br/><div class="children"><div class="content">This is an interesting question.<p>To paraphrase as per my understanding of your comment, is intelligence an emergent property of being able to interact with each other through language?<p>Say I speak gibberish (to you) which is actually me explaining to you the theory of relativity, would you consider me intelligent?</div><br/><div id="36018580" class="c"><input type="checkbox" id="c-36018580" checked=""/><div class="controls bullet"><span class="by">anon84873628</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018288">parent</a><span>|</span><a href="#36018575">next</a><span>|</span><label class="collapse" for="c-36018580">[-]</label><label class="expand" for="c-36018580">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the difference between &quot;understanding&quot; and &quot;having really good probabilistic information about how words combine&quot;?<p>Kids learn to speak by parroting what they hear and observing the outcome. Then they run tests that reinforce the connections between words. That&#x27;s what the model <i>is</i>.<p>But humans also get to link words with all the other sense experience we have (like how sweet cherries, loud fire trucks, and that one crayon are all &quot;red&quot;). LLMs don&#x27;t have as many dimensions of experience they can link to.<p>But anyway, intelligence is about having an internal model of the world and using it to predict the future. The more rich and accurate the model, the more intelligent. The ability to communicate isn&#x27;t a prerequisite; lots of animals have intelligence that isn&#x27;t built with language.</div><br/><div id="36018833" class="c"><input type="checkbox" id="c-36018833" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018580">parent</a><span>|</span><a href="#36018575">next</a><span>|</span><label class="collapse" for="c-36018833">[-]</label><label class="expand" for="c-36018833">[1 more]</label></div><br/><div class="children"><div class="content">Spot on. That we have internal language models doesn&#x27;t mean we only have language models as cognition. People seem to be redefining imposter syndrome as reflected by stand alone language models. Makes me think of Tropic Thunder. &quot;Am I even real&quot;. Lol get a grip people.</div><br/></div></div></div></div></div></div></div></div><div id="36018575" class="c"><input type="checkbox" id="c-36018575" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018267">prev</a><span>|</span><a href="#36018102">next</a><span>|</span><label class="collapse" for="c-36018575">[-]</label><label class="expand" for="c-36018575">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But almost everybody is really surprised that becoming better at a task like ‘natural language prediction’ would produce all these strange abilities that sort of look like “understanding the world”.<p>It honestly should not have been a surprise to anyone in the field at least in the last 6 years.</div><br/></div></div><div id="36018102" class="c"><input type="checkbox" id="c-36018102" checked=""/><div class="controls bullet"><span class="by">winddude</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018575">prev</a><span>|</span><a href="#36018146">next</a><span>|</span><label class="collapse" for="c-36018102">[-]</label><label class="expand" for="c-36018102">[1 more]</label></div><br/><div class="children"><div class="content">&gt; One way to resolve this surprise is to find some reason to believe these strange abilities are fundamentally not an understanding of the world. Thus stochastic parrots, this article, Yan LeCun and Chomsky, etc.<p>This one. This is the correct resolution.</div><br/></div></div><div id="36018146" class="c"><input type="checkbox" id="c-36018146" checked=""/><div class="controls bullet"><span class="by">yblu</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018102">prev</a><span>|</span><a href="#36018177">next</a><span>|</span><label class="collapse" for="c-36018146">[-]</label><label class="expand" for="c-36018146">[16 more]</label></div><br/><div class="children"><div class="content">What I found disheartening was many of those scientists, especially those on the &quot;nothing to worry about&quot; camp, seemed not to entertain the thought that they could be wrong, considering the scale of the matter, i.e. human extinction. If there&#x27;s a chance AI poses an existential threat to us, even if it is 0.00000001% (I made that up), should they be at least a bit more humble? This is uncharted domain and I find it incredible that many talk like they already have all the answers.</div><br/><div id="36018189" class="c"><input type="checkbox" id="c-36018189" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018146">parent</a><span>|</span><a href="#36018843">next</a><span>|</span><label class="collapse" for="c-36018189">[-]</label><label class="expand" for="c-36018189">[5 more]</label></div><br/><div class="children"><div class="content">Meh. Add it to the pile. The number of world ending risks that we could be worried about at this point are piling up and AI exterminating us is far from the top concern, especially when AI may be critical to solving many of the other problems that are.<p>Wrong about nuclear proliferation and MAD game theory? Human extinction. Wrong about plasticizers and other endocrine disruptors, leading to a <i>Children of Men</i> scenario? Human extinction. Wrong about the risk of asteroid impact? Human extinction. Climate change? Human extinction. Gain of function zombie virus? Human extinction. Malignant AGI? ehh... whatever, we get it.<p>It&#x27;s like the risk of driving: yeah it&#x27;s one of the leading causes of death but what are we going to do, stay inside our suburban bubbles all our lives, too afraid to cross a stroad? Except with AI this is all still completely theoretical.</div><br/><div id="36018587" class="c"><input type="checkbox" id="c-36018587" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018189">parent</a><span>|</span><a href="#36018270">next</a><span>|</span><label class="collapse" for="c-36018587">[-]</label><label class="expand" for="c-36018587">[1 more]</label></div><br/><div class="children"><div class="content">I think almost none of the scenarios you have named outside of the asteroid &amp; the AGI would result in complete human extinction, potentially a very bad MAD breakdown could also lead to this but the research here is legitimately mixed.</div><br/></div></div><div id="36018270" class="c"><input type="checkbox" id="c-36018270" checked=""/><div class="controls bullet"><span class="by">onethought</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018189">parent</a><span>|</span><a href="#36018587">prev</a><span>|</span><a href="#36018843">next</a><span>|</span><label class="collapse" for="c-36018270">[-]</label><label class="expand" for="c-36018270">[3 more]</label></div><br/><div class="children"><div class="content">Yes, and all of those (including AI) are not even human extinction events.<p>- Nuclear war: Northern Hemisphere is pretty fucked. But life goes one elsewhere.<p>- Plasticisers: We have enough science to pretty much do what we like with fertility these days. So it&#x27;s catastrophic but not extinction.<p>- Climate Change: Life gets hard, but we can build livable habitats in space... pretty sure we can manage a harsh earth climate. Not extinction.<p>- Deadly virus: Wouldn&#x27;t be the first time, and we&#x27;re still here.<p>- Astroid impact: Again, ALL human life globally? Some how birds survived the meteor that killed the dinosaurs, I&#x27;m sure we&#x27;d find a way.<p>- Complete Made up evil AI: Well we&#x27;d torch the sky, be turned into batteries but then be freed by Keanu Reeves.. or a Time traveling John Connor. (sounds like I&#x27;m being ridiculous, but ask a stupid question...)</div><br/><div id="36018689" class="c"><input type="checkbox" id="c-36018689" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018270">parent</a><span>|</span><a href="#36018589">next</a><span>|</span><label class="collapse" for="c-36018689">[-]</label><label class="expand" for="c-36018689">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re taking these things too lightly. It&#x27;s true that most of these things are unlikely to kill all humans directly, but with most of them, civilizational collapse is definitely on the table, and that can ultimately lead to human extinction.<p>For example: Yes, we could probably build livable habitats in space (though we don&#x27;t really have proof of that). But how many, for how many people, and what kind of external support systems do they require? These questions put stresses on society that prevents space habitats from working out in the long term.</div><br/></div></div><div id="36018589" class="c"><input type="checkbox" id="c-36018589" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018270">parent</a><span>|</span><a href="#36018689">prev</a><span>|</span><a href="#36018843">next</a><span>|</span><label class="collapse" for="c-36018589">[-]</label><label class="expand" for="c-36018589">[1 more]</label></div><br/><div class="children"><div class="content">&gt; - Astroid impact: Again, ALL human life globally? Some how birds survived the meteor that killed the dinosaurs, I&#x27;m sure we&#x27;d find a way.<p>I agree with many of these but we&#x27;d plausibly be toast in this scenario.</div><br/></div></div></div></div></div></div><div id="36018843" class="c"><input type="checkbox" id="c-36018843" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018146">parent</a><span>|</span><a href="#36018189">prev</a><span>|</span><a href="#36018192">next</a><span>|</span><label class="collapse" for="c-36018843">[-]</label><label class="expand" for="c-36018843">[1 more]</label></div><br/><div class="children"><div class="content">Humans have a start in time and will have an end. I was born and I will die. I don&#x27;t know why we&#x27;re so obsessed about this. We will most definitely cease existing soon in geological&#x2F;cosmic time scale. Doesn&#x27;t matter.</div><br/></div></div><div id="36018192" class="c"><input type="checkbox" id="c-36018192" checked=""/><div class="controls bullet"><span class="by">transcriptase</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018146">parent</a><span>|</span><a href="#36018843">prev</a><span>|</span><a href="#36018345">next</a><span>|</span><label class="collapse" for="c-36018192">[-]</label><label class="expand" for="c-36018192">[2 more]</label></div><br/><div class="children"><div class="content">Debatable, since there are plenty of other unavoidable existential threats that are far more likely than the best estimates that AI will wipe us out. E.g. supervolcano eruption, massive solar flare, asteroid impact, some novel virus.<p>At least we can take comfort in the fact that if an AI takes us out, one of the aforementioned will avenge us and destroy the AI too on a long enough time scale.</div><br/><div id="36018857" class="c"><input type="checkbox" id="c-36018857" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018192">parent</a><span>|</span><a href="#36018345">next</a><span>|</span><label class="collapse" for="c-36018857">[-]</label><label class="expand" for="c-36018857">[1 more]</label></div><br/><div class="children"><div class="content">I find striking that we have a rich cultural tradition of claiming we&#x27;re artificial beings. Maybe we&#x27;re building a successor lifeform... I&#x27;ve thought about this as a story premise: humans and robots are two stages of a lifecycle. Humans flourish on a planetary ecosystem, build robots that go on to colonize new systems where they seed humans because (reason I haven&#x27;t been able to formulate).</div><br/></div></div></div></div><div id="36018345" class="c"><input type="checkbox" id="c-36018345" checked=""/><div class="controls bullet"><span class="by">woodruffw</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018146">parent</a><span>|</span><a href="#36018192">prev</a><span>|</span><a href="#36018231">next</a><span>|</span><label class="collapse" for="c-36018345">[-]</label><label class="expand" for="c-36018345">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a nonzero chance that the celery in my fridge is harboring an existentially virulent and fatal strain of E. coli. At the same time, it would be <i>completely insane</i> for me to autoclave every vegetable that enters my house.<p>Sensible action here requires sensible numbers: it&#x27;s not enough to claim existential risk on extraordinary odds.</div><br/></div></div><div id="36018231" class="c"><input type="checkbox" id="c-36018231" checked=""/><div class="controls bullet"><span class="by">onethought</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018146">parent</a><span>|</span><a href="#36018345">prev</a><span>|</span><a href="#36018177">next</a><span>|</span><label class="collapse" for="c-36018231">[-]</label><label class="expand" for="c-36018231">[6 more]</label></div><br/><div class="children"><div class="content">&gt; considering the scale of the matter, i.e. human extinction.<p>There is literally no evidence that this is the scale of the matter. Has AI ever caused anything to go extinct? Where did this hypothesis (and that&#x27;s all it is) come from? Terminator movies?<p>It&#x27;s very frustrating watching experts and the literal founder of lesswrong reacting to pure make believe. There is no disernable&#x2F;convincing path from GPT4 -&gt; Human Extinction. What am I missing here?</div><br/><div id="36018706" class="c"><input type="checkbox" id="c-36018706" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018231">parent</a><span>|</span><a href="#36018367">next</a><span>|</span><label class="collapse" for="c-36018706">[-]</label><label class="expand" for="c-36018706">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Has AI ever caused anything to go extinct?<p>We know from human history that <i>intelligence</i> tends to cause extinctions.<p>AI just hasn&#x27;t been around long enough, nor been intelligent enough yet.<p>Though, if you count corporations as artificial intelligences, as some suggest, then yes, AIs have in fact already contributed to extinctions.</div><br/></div></div><div id="36018367" class="c"><input type="checkbox" id="c-36018367" checked=""/><div class="controls bullet"><span class="by">kristiandupont</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018231">parent</a><span>|</span><a href="#36018706">prev</a><span>|</span><a href="#36018177">next</a><span>|</span><label class="collapse" for="c-36018367">[-]</label><label class="expand" for="c-36018367">[4 more]</label></div><br/><div class="children"><div class="content">Nuclear bombs have also never caused anything to go extinct. That&#x27;s no reason not to be cautious.<p>The path is pretty clear to me. An AI that can recreate an improved version of itself will cause an intelligence explosion. That is a mathematical tautology though it could turn out that it would plateau at some point due to physical limitations or whatever. And the situation then becomes: at some point, this AI will be smarter than us. And so, if it decides that we are in the way for one reason or another, it can decide to get rid of us and we would have as much chance of stopping it as chimpanzees would of stopping us if we decided to kill them off.<p>We do not, I think, have such a thing at this point but it doesn&#x27;t feel far off with the coding capabilities that GPT4 has.</div><br/><div id="36018474" class="c"><input type="checkbox" id="c-36018474" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018367">parent</a><span>|</span><a href="#36018177">next</a><span>|</span><label class="collapse" for="c-36018474">[-]</label><label class="expand" for="c-36018474">[3 more]</label></div><br/><div class="children"><div class="content">So what would be the path for GPT5 or 6 creating an improved model of itself? It&#x27;s not enough to generate working code. It has to come up with a better architecture or training data.</div><br/><div id="36018516" class="c"><input type="checkbox" id="c-36018516" checked=""/><div class="controls bullet"><span class="by">ShaddyDC</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018474">parent</a><span>|</span><a href="#36018177">next</a><span>|</span><label class="collapse" for="c-36018516">[-]</label><label class="expand" for="c-36018516">[2 more]</label></div><br/><div class="children"><div class="content">The idea is that a model might already be smarter than us or at the very least have a very different thought process from us and <i>then</i> do something like improving itself. The problem is that it&#x27;s impossible for us to predict the exact path because it&#x27;s thought up by an entity whose thinking we don&#x27;t really understand or are able to predict.</div><br/><div id="36018864" class="c"><input type="checkbox" id="c-36018864" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018516">parent</a><span>|</span><a href="#36018177">next</a><span>|</span><label class="collapse" for="c-36018864">[-]</label><label class="expand" for="c-36018864">[1 more]</label></div><br/><div class="children"><div class="content">I think &quot;llm builds better llm&quot; is drawing the border at the wrong place. Technical progress has been accelerating for centuries. It&#x27;s pretty self evident that the technological civilization is improving upon itself.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36018177" class="c"><input type="checkbox" id="c-36018177" checked=""/><div class="controls bullet"><span class="by">dirkt</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018146">prev</a><span>|</span><a href="#36017687">next</a><span>|</span><label class="collapse" for="c-36018177">[-]</label><label class="expand" for="c-36018177">[1 more]</label></div><br/><div class="children"><div class="content">&gt; One way to resolve this surprise is to find some reason to believe these strange abilities are fundamentally not an understanding of the world.<p>An easy way to test this is to ask questions and followup-questions that actually require understanding, and compare this to the answers. I recommend to try that.</div><br/></div></div><div id="36017850" class="c"><input type="checkbox" id="c-36017850" checked=""/><div class="controls bullet"><span class="by">klipt</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017687">prev</a><span>|</span><a href="#36018318">next</a><span>|</span><label class="collapse" for="c-36017850">[-]</label><label class="expand" for="c-36017850">[6 more]</label></div><br/><div class="children"><div class="content">&gt; language encoded way more information about reality than we thought it did<p>Language is roughly what separates humans from other apes ... so why would it surprise us that it encodes much of the information of civilization?</div><br/><div id="36018028" class="c"><input type="checkbox" id="c-36018028" checked=""/><div class="controls bullet"><span class="by">fwlr</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017850">parent</a><span>|</span><a href="#36018122">next</a><span>|</span><label class="collapse" for="c-36018028">[-]</label><label class="expand" for="c-36018028">[1 more]</label></div><br/><div class="children"><div class="content">I’m not as surprised as many people (I saw a great tweet once that said “Language was the first artificial intelligence, writing was the second. I literally believe this”, and that’s a broadly accurate description of my worldview too).<p>That said, maybe some of the surprise comes from believing “the map is not the territory” and related ideas? We generally believe that the map is not the territory and this gives us some obviously correct intuitions (like “changing the map doesn’t change the territory”), but maybe it has also given us some subtly incorrect intuitions. I’m not talking about obviously incorrect, like “you can’t understand the territory just by looking at enough maps”. I mean something more subtly wrong. One candidate off the top of my head is an intuition that “maps approximate the territory but necessarily at a lower level of detail (a 1:1 map of the territory would be the same size as the territory), so your understanding of the territory can improve as you read more maps but it can’t improve on the limit of the most detailed map available, because that information literally isn’t there”. I could see that possibly being wrong somehow.</div><br/></div></div><div id="36018122" class="c"><input type="checkbox" id="c-36018122" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017850">parent</a><span>|</span><a href="#36018028">prev</a><span>|</span><a href="#36017870">next</a><span>|</span><label class="collapse" for="c-36018122">[-]</label><label class="expand" for="c-36018122">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Language is roughly what separates humans from other apes<p>Maybe not.<p>A recent study pushes back the &quot;dawn of speech&quot; to 20 Ma which is far, far beyond the horizon where we consider humans to separate from apes. <a href="https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;sciadv.aaw3916" rel="nofollow">https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;sciadv.aaw3916</a> Even if you consider Sahelanthropus tchadensis to belong to humans that was only 7 Ma and that is still under debate.<p>I personally find &quot;the fundamental human trait is control of fire to be used for cooking&quot; theory very convincing. We do not yet know how far this goes back but no one pushed that back beyond 2 Ma.</div><br/></div></div><div id="36017870" class="c"><input type="checkbox" id="c-36017870" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017850">parent</a><span>|</span><a href="#36018122">prev</a><span>|</span><a href="#36018318">next</a><span>|</span><label class="collapse" for="c-36017870">[-]</label><label class="expand" for="c-36017870">[3 more]</label></div><br/><div class="children"><div class="content">&gt; so why would it surprise us that it encodes much of the information of civilization?<p>The Humanities are largely considered superfluous in the tech world, why are you surprised that they are surprised?</div><br/><div id="36017970" class="c"><input type="checkbox" id="c-36017970" checked=""/><div class="controls bullet"><span class="by">klipt</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017870">parent</a><span>|</span><a href="#36018318">next</a><span>|</span><label class="collapse" for="c-36017970">[-]</label><label class="expand" for="c-36017970">[2 more]</label></div><br/><div class="children"><div class="content">To clarify, I&#x27;m including stuff like Python code under <i>language</i>, because these large <i>language</i> models are also trained on Python code.</div><br/><div id="36018211" class="c"><input type="checkbox" id="c-36018211" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36017970">parent</a><span>|</span><a href="#36018318">next</a><span>|</span><label class="collapse" for="c-36018211">[-]</label><label class="expand" for="c-36018211">[1 more]</label></div><br/><div class="children"><div class="content">Very very few people in tech are even experts in the meta analysis of programming languages, much less the importance of languages.</div><br/></div></div></div></div></div></div></div></div><div id="36018318" class="c"><input type="checkbox" id="c-36018318" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36017850">prev</a><span>|</span><a href="#36018830">next</a><span>|</span><label class="collapse" for="c-36018318">[-]</label><label class="expand" for="c-36018318">[1 more]</label></div><br/><div class="children"><div class="content">&gt; or just put a few industries out of a job.<p>Yeah no big deal. Happens in history all the time, within a year.</div><br/></div></div><div id="36018830" class="c"><input type="checkbox" id="c-36018830" checked=""/><div class="controls bullet"><span class="by">DiogenesKynikos</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018318">prev</a><span>|</span><a href="#36018057">next</a><span>|</span><label class="collapse" for="c-36018830">[-]</label><label class="expand" for="c-36018830">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s at all surprising that a system designed to predict the next word ends up understanding the world.<p>If you want to predict the next word accurately, you first have to know which words exist. To progress, you&#x27;ll have to learn about the mechanics of grammar and which words are used more frequently or in combination. To become even more accurate, it helps to understand context, so that the sentences you string together will at least be relevant to the subject. If you want to increase your accuracy even further, you&#x27;ll have to start memorizing all sorts of facts (e.g., &quot;Who was the monarch of England in 1600?&quot;). Being able to synthesize those facts into a coherent argument will increase your accuracy even further.<p>In the end, predicting the next word accurately requires an understanding of the world.<p>This isn&#x27;t all that different from how our own intelligence evolved. You could look at humans from the outside and disparagingly point out that the ultimate purpose of the human brain is to direct muscle motions in a way that maximizes the chances of reproductive success. It just turns out that solving that problem effectively has led to the development of an enormously complicated piece of machinery, capable of synthesizing all sorts of input stimuli into a coherent picture of the world, and ultimately of producing the works of Shakespeare and the music of Beethoven.</div><br/></div></div><div id="36018057" class="c"><input type="checkbox" id="c-36018057" checked=""/><div class="controls bullet"><span class="by">aappleby</span><span>|</span><a href="#36017613">parent</a><span>|</span><a href="#36018830">prev</a><span>|</span><a href="#36017579">next</a><span>|</span><label class="collapse" for="c-36018057">[-]</label><label class="expand" for="c-36018057">[3 more]</label></div><br/><div class="children"><div class="content">What exactly does &quot;understanding the world&quot; really mean?</div><br/><div id="36018462" class="c"><input type="checkbox" id="c-36018462" checked=""/><div class="controls bullet"><span class="by">espe</span><span>|</span><a href="#36017613">root</a><span>|</span><a href="#36018057">parent</a><span>|</span><a href="#36018851">prev</a><span>|</span><a href="#36017579">next</a><span>|</span><label class="collapse" for="c-36018462">[-]</label><label class="expand" for="c-36018462">[1 more]</label></div><br/><div class="children"><div class="content">compression :)</div><br/></div></div></div></div></div></div><div id="36017579" class="c"><input type="checkbox" id="c-36017579" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36017613">prev</a><span>|</span><a href="#36017546">next</a><span>|</span><label class="collapse" for="c-36017579">[-]</label><label class="expand" for="c-36017579">[11 more]</label></div><br/><div class="children"><div class="content">No world model ?<p>A world model is so obvious papers like these are more confirmation than surprise<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11169" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11169</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382</a><p>There a certain sentiment that AGI however you wish to define it won&#x27;t infact be a &quot;We&#x27;ll know it when we see it&quot; situation but rather a &quot;AGI will arrive long before consensus reaches its AGI&quot;. LLMs have made me believe this will 100% be the case, either way.<p>It&#x27;s one thing to argue over things we can&#x27;t evaluate even now but man the 100th &quot;They can&#x27;t reason!&quot; every week is pretty funny when you can basically take your pick of reasonong type - Algorithmic, Casual, Inference, Analogical and read a paper showing strong performance.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.09196" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.09196</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.00050" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.00050</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2204.02329" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2204.02329</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.09066" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.09066</a><p>People refuse to see even what is staring right at them.</div><br/><div id="36019054" class="c"><input type="checkbox" id="c-36019054" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#36017579">parent</a><span>|</span><a href="#36017818">next</a><span>|</span><label class="collapse" for="c-36019054">[-]</label><label class="expand" for="c-36019054">[1 more]</label></div><br/><div class="children"><div class="content">What the skeptics seem to get hung up on is the lack of common sense. These LLM’s are trained on language describing the world, not on the world itself. They lack the lived experience that we take for granted. So it is trivial to ask them common sense questions and see them fail amusingly, but that is unfair because they haven’t been taught the world, only a linguistic map of the world, and the map is not the territory. Also, we don’t even know what map they learned, because the only way to know is to ask a million questions, like an early seafarer discovering the shape of a foreign continent beach by beach.<p>LLM’s are not people, they lack common sense, but they understand and can reason about what they are trained on. That is exceedingly powerful and very useful even at today’s level of ability, so products built on top of this technology are going to transform everything. The trick is boxing it in and only making it do things it can, so the art of LLM product development will have to become a whole subfield of software engineering until the LLM’s develop to the point where their map of the world is close enough to the world itself.</div><br/></div></div><div id="36017818" class="c"><input type="checkbox" id="c-36017818" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36017579">parent</a><span>|</span><a href="#36019054">prev</a><span>|</span><a href="#36018900">next</a><span>|</span><label class="collapse" for="c-36017818">[-]</label><label class="expand" for="c-36017818">[4 more]</label></div><br/><div class="children"><div class="content">This paper is startling in terms of a world model and how it seems to emerge in a specific example in the step from GPT3.5 to 4. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.12712.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.12712.pdf</a><p>One of the prompts is &quot;Here we have a book, 9 eggs, a laptop, a bottle and a nail. Please tell me how to stack them onto each other in a stable manner.&quot;<p>Gpt3.5 is hopeless.  It says start from the bottle, balance the nail on top of it, then the eggs (somehow) then the laptop, then the book.<p>GPT 4 says put the book first, then the eggs on top of the book in a 3x3 grid, then the laptop, then the nail.</div><br/><div id="36018535" class="c"><input type="checkbox" id="c-36018535" checked=""/><div class="controls bullet"><span class="by">mojuba</span><span>|</span><a href="#36017579">root</a><span>|</span><a href="#36017818">parent</a><span>|</span><a href="#36018339">next</a><span>|</span><label class="collapse" for="c-36018535">[-]</label><label class="expand" for="c-36018535">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting that GTP4&#x27;s current answer is different but what you described is almost precisely what Google Bard gives as a response. Clearly the solution (book -&gt; eggs 3x3 -&gt; laptop etc) is coming from the Internet.<p>However any deviation from the original task ruins the LLM&#x27;s answer. Try 9 cabbages instead of eggs and see how ridiculous and out of touch with reality the responses given by both GPT4 and Bard are.</div><br/></div></div><div id="36018339" class="c"><input type="checkbox" id="c-36018339" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#36017579">root</a><span>|</span><a href="#36017818">parent</a><span>|</span><a href="#36018535">prev</a><span>|</span><a href="#36018900">next</a><span>|</span><label class="collapse" for="c-36018339">[-]</label><label class="expand" for="c-36018339">[2 more]</label></div><br/><div class="children"><div class="content">&gt; GPT 4 says put the book first<p>I assume it read the correct answer somewhere. Perhaps in a post like yours.<p>What does it answer if you say there is one egg, or two eggs? Or 1.5 eggs?</div><br/><div id="36018398" class="c"><input type="checkbox" id="c-36018398" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36017579">root</a><span>|</span><a href="#36018339">parent</a><span>|</span><a href="#36018900">next</a><span>|</span><label class="collapse" for="c-36018398">[-]</label><label class="expand" for="c-36018398">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting.  The answer shows some reasoning about weight and pressure but not curvature. In response to<p><pre><code>    Here we have a book, an egg, a laptop, a bottle and a nail. Please tell me how to stack them onto each other in a stable manner.
</code></pre>
GPT-4 gives me:<p><pre><code>    Arranging objects in a stable stack depends on their size, shape, and weight. In general, you want to place the largest, heaviest and most stable objects at the bottom and then move to the smallest, lightest, and least stable at the top. Here&#x27;s one way to stack the objects you listed:

        Laptop: Start with the laptop at the bottom. It&#x27;s likely the largest object and it has a flat, stable surface.

        Book: Next, put the book on top of the laptop. Books are generally flat and heavy, which makes them a good choice for the base of your stack.

        Bottle: The bottle goes next, as it should have a flat bottom to sit securely on the book. Just make sure it&#x27;s closed tight and won&#x27;t roll away!

        Egg: The egg is delicate, so it should be towards the top to avoid pressure. However, it&#x27;s not flat, so it should not be at the very top.

        Nail: Finally, the nail goes on top. It&#x27;s the smallest and lightest, so it won&#x27;t add much pressure to the egg.

    Remember that this is just one possible way to stack these items and it could depend on the specific characteristics of the items you have. For example, if the bottle is much larger and heavier than the book or laptop, you&#x27;d need to adjust accordingly. Safety should always be your first priority when stacking items, especially if they are delicate or valuable.
</code></pre>
The &quot;make sure it&#x27;s closed tight and won&#x27;t roll away&quot; comment makes no sense obviously.  Most people would place the bottle standing on its end so neither of those is a concern. The response also doesn&#x27;t show an understanding of the fact that the nail won&#x27;t sit on top of the egg although it&#x27;s interestingly concerned with pressure breaking the egg.</div><br/></div></div></div></div></div></div><div id="36018900" class="c"><input type="checkbox" id="c-36018900" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36017579">parent</a><span>|</span><a href="#36017818">prev</a><span>|</span><a href="#36017716">next</a><span>|</span><label class="collapse" for="c-36018900">[-]</label><label class="expand" for="c-36018900">[1 more]</label></div><br/><div class="children"><div class="content">I find interesting that Turing foresaw all this with the chinese room&#x2F;turing test thing. If computers can emulate human language to an indiscernible degree of difference from what other humans can do, it is essentially reasoning, for some definitions of reasoning. But how much of it reflects our own understanding of reasoning and efforts in fine tuning? If I show a modern imax movie to a stone age hunter and they confuse it for reality, is the movie reality? People ran from footage of a steam locomotive driving towards the camera. Does that mean the train was real?</div><br/></div></div><div id="36017716" class="c"><input type="checkbox" id="c-36017716" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36017579">parent</a><span>|</span><a href="#36018900">prev</a><span>|</span><a href="#36018160">next</a><span>|</span><label class="collapse" for="c-36017716">[-]</label><label class="expand" for="c-36017716">[3 more]</label></div><br/><div class="children"><div class="content">Do you know what a &quot;world model&quot; is? It&#x27;s a thing people were assumed to have in 1970s psychology, but was never well-defined enough to tell if it exists or not, so I don&#x27;t think it&#x27;s obvious anything else has one.<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;Meaningness&#x2F;status&#x2F;1639120720088408065" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;Meaningness&#x2F;status&#x2F;1639120720088408065</a><p>I think &quot;common sense&quot; or &quot;long term memory&quot; might be more productive things to say.</div><br/><div id="36017811" class="c"><input type="checkbox" id="c-36017811" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36017579">root</a><span>|</span><a href="#36017716">parent</a><span>|</span><a href="#36018160">next</a><span>|</span><label class="collapse" for="c-36017811">[-]</label><label class="expand" for="c-36017811">[2 more]</label></div><br/><div class="children"><div class="content">The brain does a lot of hierarchical prediction with sense data. When new information comes in, it references &quot;something&quot; and adjusts the sense data likewise.<p>That&#x27;s why when you shift your eyes quickly, you see blurred images pass by. In reality, you should be seeing complete black because the brain doesn&#x27;t actually process visual information that shifts so quickly.<p>But your brain &quot;knows&quot; it should see...well something. And so it fits that blurred passthrough as compensation. Completely made up data. But not ungrounded data, data that seems like it should fit according to that &quot;something&quot;. That &quot;something&quot; being referenced is a world model.<p>That&#x27;s a sub-conscious example but you can think of a lot of times people consciously reference &quot;something&quot; to better take actions or make decisions.<p>For Othello-GPT, training seemed to induce a learned board state that the model referenced at every Inference to make moves.<p>It seems that fully accurate or not, deep predictions might require &quot;something&quot; to reference. Else perhaps you might need to derive everything from first principles at every turn. Not very efficient.</div><br/><div id="36018233" class="c"><input type="checkbox" id="c-36018233" checked=""/><div class="controls bullet"><span class="by">SanderNL</span><span>|</span><a href="#36017579">root</a><span>|</span><a href="#36017811">parent</a><span>|</span><a href="#36018160">next</a><span>|</span><label class="collapse" for="c-36018233">[-]</label><label class="expand" for="c-36018233">[1 more]</label></div><br/><div class="children"><div class="content">It always seemed sensible to me that we are looking at a dreamlike construction <i>all</i> the time, not just holes filled in but literally everything you see is made up, but grounded in sensory data (when not psychotic or dreaming). This also nicely explains the clarity and realism of lucid dreaming, because it’s just doing whatever you are doing during the day but without (external) grounding sensory data.</div><br/></div></div></div></div></div></div><div id="36018160" class="c"><input type="checkbox" id="c-36018160" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#36017579">parent</a><span>|</span><a href="#36017716">prev</a><span>|</span><a href="#36017546">next</a><span>|</span><label class="collapse" for="c-36018160">[-]</label><label class="expand" for="c-36018160">[1 more]</label></div><br/><div class="children"><div class="content">To do a great deal of harm, GPT-X just has to have enough of a world model to be useful for automating tasks like deception and impersonation at a scale humanity has never before experienced.</div><br/></div></div></div></div><div id="36017546" class="c"><input type="checkbox" id="c-36017546" checked=""/><div class="controls bullet"><span class="by">tunesmith</span><span>|</span><a href="#36017579">prev</a><span>|</span><a href="#36017779">next</a><span>|</span><label class="collapse" for="c-36017546">[-]</label><label class="expand" for="c-36017546">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny how it&#x27;s possible to simultaneously overestimate and underestimate GPT4 at the same time, vastly.<p>I think that we just don&#x27;t fully understand everything it gives us yet.<p>The complaints of &quot;well it explained this wrong&quot; are over-emphasized. The same thing happens with google and with any sort of research. Besides, if you&#x27;re actually being productive with GPT4, you&#x27;re going to be asking it stuff that relates to something you do know, and will be able to verify it readily enough. (Especially when it comes to programming and compilers.)<p>And just a reminder, those of you opining based off your experience with GPT3.5... GPT4 is a huge, huge improvement. Almost to the point of it not really being an incremental improvement. It&#x27;s so much better it&#x27;s like a different thing.</div><br/><div id="36017675" class="c"><input type="checkbox" id="c-36017675" checked=""/><div class="controls bullet"><span class="by">PsylentKnight</span><span>|</span><a href="#36017546">parent</a><span>|</span><a href="#36017565">next</a><span>|</span><label class="collapse" for="c-36017675">[-]</label><label class="expand" for="c-36017675">[4 more]</label></div><br/><div class="children"><div class="content">&gt; And just a reminder, those of you opining based off your experience with GPT3.5... GPT4 is a huge, huge improvement.<p>God, yes. The number of people of HN pushing up their glasses and saying &quot;well, actshually...&quot; when they&#x27;re basing their opinions off the 3 questions they asked 3.5 is starting to become pretty grating.</div><br/><div id="36018068" class="c"><input type="checkbox" id="c-36018068" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36017546">root</a><span>|</span><a href="#36017675">parent</a><span>|</span><a href="#36017565">next</a><span>|</span><label class="collapse" for="c-36018068">[-]</label><label class="expand" for="c-36018068">[3 more]</label></div><br/><div class="children"><div class="content">The number of people parroting this is also absolutely astounding and grating too.<p>Like, anyone who has spent 5 minutes on this forum already knows this. It’s probably not necessary to keep pointing it out. Yes some people don’t know ChatGPT 3.5 is the default for non-paying customers.</div><br/><div id="36018094" class="c"><input type="checkbox" id="c-36018094" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#36017546">root</a><span>|</span><a href="#36018068">parent</a><span>|</span><a href="#36018120">next</a><span>|</span><label class="collapse" for="c-36018094">[-]</label><label class="expand" for="c-36018094">[1 more]</label></div><br/><div class="children"><div class="content">What you are saying is humans are just stochastic parrots - predictable little denial engines regurgitating their latest chosen sentence that makes them feel special and safe.<p>Any thread in the topic, simple statistical modeling will get you close to perfect to the distribution of arguments that will appear.</div><br/></div></div><div id="36018120" class="c"><input type="checkbox" id="c-36018120" checked=""/><div class="controls bullet"><span class="by">PsylentKnight</span><span>|</span><a href="#36017546">root</a><span>|</span><a href="#36018068">parent</a><span>|</span><a href="#36018094">prev</a><span>|</span><a href="#36017565">next</a><span>|</span><label class="collapse" for="c-36018120">[-]</label><label class="expand" for="c-36018120">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Like, anyone who has spent 5 minutes on this forum already knows this<p>You would think, and yet...</div><br/></div></div></div></div></div></div><div id="36017565" class="c"><input type="checkbox" id="c-36017565" checked=""/><div class="controls bullet"><span class="by">jmerz</span><span>|</span><a href="#36017546">parent</a><span>|</span><a href="#36017675">prev</a><span>|</span><a href="#36018084">next</a><span>|</span><label class="collapse" for="c-36017565">[-]</label><label class="expand" for="c-36017565">[1 more]</label></div><br/><div class="children"><div class="content">This captures it well. We&#x27;re at the &quot;startups are throwing GPT at every possible wall to see what sticks&quot; stage of this.<p>We&#x27;re going to see both improvements in application, and parallel improvements in the underlying model.<p>Who cares if it&#x27;s AGI if someone figures out how to turn it into a competent tax accountant?</div><br/></div></div><div id="36018084" class="c"><input type="checkbox" id="c-36018084" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#36017546">parent</a><span>|</span><a href="#36017565">prev</a><span>|</span><a href="#36017779">next</a><span>|</span><label class="collapse" for="c-36018084">[-]</label><label class="expand" for="c-36018084">[1 more]</label></div><br/><div class="children"><div class="content">Every time there is a thread in the topic humans are hallucinating and making shit up at epic scale. So really, it’s either a terminal flaw and we should never listen to another human again or we should start getting some humility</div><br/></div></div></div></div><div id="36017779" class="c"><input type="checkbox" id="c-36017779" checked=""/><div class="controls bullet"><span class="by">thatguyknows</span><span>|</span><a href="#36017546">prev</a><span>|</span><a href="#36017984">next</a><span>|</span><label class="collapse" for="c-36017779">[-]</label><label class="expand" for="c-36017779">[11 more]</label></div><br/><div class="children"><div class="content">As mentioned, much of our discussion is rote parroting. I can usually go into any hackernews thread and roughly know what the top discussions are going to be. It&#x27;s not surprising that an AI trained on a large portion of the internet would thus look human like.<p>If you really poke at GPT, you begin to realize it&#x27;s fairly shallow. Human intelligence is like a deep well or pond, where as GPT is a vast but shallow ocean.<p>Making that ocean deeper is not a trivial problem that we can just throw more compute or data at. We&#x27;ve pretty much tapped out that depth with GPT4 and are going to need better designs.<p>This could only take half a decade or it could be half a century. Plenty of enterprises stagnate for decades.</div><br/><div id="36017815" class="c"><input type="checkbox" id="c-36017815" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36017779">parent</a><span>|</span><a href="#36018538">next</a><span>|</span><label class="collapse" for="c-36017815">[-]</label><label class="expand" for="c-36017815">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Making that ocean deeper is not a trivial problem that we can just throw more compute or data at.<p>You can&#x27;t possibly know that, given that we don&#x27;t actually understand how LLMs work on a high level.<p>&gt; We&#x27;ve pretty much tapped out that depth with GPT4<p>GPT-4 is <i>three months old</i> and you&#x27;re confident that its working principle cannot be extended further? Where do you get that confidence from?</div><br/><div id="36017820" class="c"><input type="checkbox" id="c-36017820" checked=""/><div class="controls bullet"><span class="by">thatguyknows</span><span>|</span><a href="#36017779">root</a><span>|</span><a href="#36017815">parent</a><span>|</span><a href="#36018151">next</a><span>|</span><label class="collapse" for="c-36017820">[-]</label><label class="expand" for="c-36017820">[5 more]</label></div><br/><div class="children"><div class="content">Sam Altman said it himself. He seems like a reasonable source.<p>If you&#x27;re familiar with other fields of AI, adding more and more layers to ResNet was the hotness for awhile, but the trick stopped working after awhile.</div><br/><div id="36017864" class="c"><input type="checkbox" id="c-36017864" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#36017779">root</a><span>|</span><a href="#36017820">parent</a><span>|</span><a href="#36017977">next</a><span>|</span><label class="collapse" for="c-36017864">[-]</label><label class="expand" for="c-36017864">[2 more]</label></div><br/><div class="children"><div class="content">Exactly, and OpenAI has been around nearly 8 years, consumed huge amount of data  with tons of compute.  They are just showing us the product now.<p>It is possible they&#x27;ve reached some 80&#x2F;20 point and he is pretty honest about how much more extendable the current approach really is.<p>Would explain going to congress and asking for regulation (of their not-quite-there-yet competitors who they want a regulatory moat against).</div><br/><div id="36017997" class="c"><input type="checkbox" id="c-36017997" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36017779">root</a><span>|</span><a href="#36017864">parent</a><span>|</span><a href="#36017977">next</a><span>|</span><label class="collapse" for="c-36017997">[-]</label><label class="expand" for="c-36017997">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36017977" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36017977</a></div><br/></div></div></div></div><div id="36017977" class="c"><input type="checkbox" id="c-36017977" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#36017779">root</a><span>|</span><a href="#36017820">parent</a><span>|</span><a href="#36017864">prev</a><span>|</span><a href="#36018075">next</a><span>|</span><label class="collapse" for="c-36017977">[-]</label><label class="expand" for="c-36017977">[1 more]</label></div><br/><div class="children"><div class="content">Altman didn&#x27;t really say that. Reading what he actually said rather than a headline, He was alluding to economical walls. He didn&#x27;t say anything about diminishing returns on scaling. And if anything, the chief scientist, Ilya thinks there&#x27;s a lot left to squeeze.</div><br/></div></div><div id="36018075" class="c"><input type="checkbox" id="c-36018075" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#36017779">root</a><span>|</span><a href="#36017820">parent</a><span>|</span><a href="#36017977">prev</a><span>|</span><a href="#36018151">next</a><span>|</span><label class="collapse" for="c-36018075">[-]</label><label class="expand" for="c-36018075">[1 more]</label></div><br/><div class="children"><div class="content">Sure Sam Altman, the lying CEO of a tech company (they all do) should be listened to on this matter but not on the part where he thinks AGI within reach using his approach. Selective hearing.</div><br/></div></div></div></div><div id="36018151" class="c"><input type="checkbox" id="c-36018151" checked=""/><div class="controls bullet"><span class="by">LouisSayers</span><span>|</span><a href="#36017779">root</a><span>|</span><a href="#36017815">parent</a><span>|</span><a href="#36017820">prev</a><span>|</span><a href="#36018365">next</a><span>|</span><label class="collapse" for="c-36018151">[-]</label><label class="expand" for="c-36018151">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can&#x27;t possibly know that, given that we don&#x27;t actually understand how LLMs work on a high level.<p>It&#x27;s a fair assumption to make however - basically 80&#x2F;20 rule.<p>AI research isn&#x27;t a new thing and I bet you could go back 40&#x2F;50 years where they thought they were about to have a massive breakthrough to human level intelligence.<p>&gt; GPT-4 is three months old and you&#x27;re confident that its working principle cannot be extended further? Where do you get that confidence from?<p>I&#x27;m guessing from actually using it.<p>GPT4 is super impressive and helpful in a practical way, but having used it myself for a while now I get this feeling also. It feels a bit like &quot;it&#x27;s been fed everything we have, with all the techniques we have, now what?&quot;</div><br/></div></div><div id="36018365" class="c"><input type="checkbox" id="c-36018365" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#36017779">root</a><span>|</span><a href="#36017815">parent</a><span>|</span><a href="#36018151">prev</a><span>|</span><a href="#36018538">next</a><span>|</span><label class="collapse" for="c-36018365">[-]</label><label class="expand" for="c-36018365">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ... we don&#x27;t actually understand how LLMs work on a high level.<p>Are you saying that people who created ChatGPT don&#x27;t understand how it works? Or that we the rest of people don&#x27;t?</div><br/></div></div></div></div><div id="36018538" class="c"><input type="checkbox" id="c-36018538" checked=""/><div class="controls bullet"><span class="by">OrderlyTiamat</span><span>|</span><a href="#36017779">parent</a><span>|</span><a href="#36017815">prev</a><span>|</span><a href="#36018362">next</a><span>|</span><label class="collapse" for="c-36018538">[-]</label><label class="expand" for="c-36018538">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Making that ocean deeper is not a trivial problem that we can just throw more compute or data at.<p>I&#x27;d say this is immediately counterindicated by the available evidence. Gpt2 was hopeless for anything other than some fun languagw games like a bot replica of a subreddit or trump. 3.5 is much much bigger, and has semi competent but limited reasoning abilities.<p>Gpt 4 is a vast improvement over 3.5 in various reasoning tasks. Yes, a priori I would have agreed with you that this has to stop somewhere, but not anymore. I would need to see some data of post gpt4 models to believe you.</div><br/></div></div><div id="36018362" class="c"><input type="checkbox" id="c-36018362" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#36017779">parent</a><span>|</span><a href="#36018538">prev</a><span>|</span><a href="#36017984">next</a><span>|</span><label class="collapse" for="c-36018362">[-]</label><label class="expand" for="c-36018362">[1 more]</label></div><br/><div class="children"><div class="content">The apparent shallowness IMO is due to the lack of long term memory and limited context. In terms of depth, the depth of the human mind is fairly limited. Put any average human to task on any creative endeavor and it&#x27;s surely a regurgitation of things they&#x27;ve seen. The high standard of true creativity that people hold LLM&#x27;s to is only a capability of a small minority of humans.<p>I&#x27;d wager it&#x27;s far more likely 5 years than 50 LLMs get to the full depths all humans are capable of. Simply compare the state of LLMs today vs 2018.</div><br/></div></div></div></div><div id="36017984" class="c"><input type="checkbox" id="c-36017984" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36017779">prev</a><span>|</span><a href="#36017389">next</a><span>|</span><label class="collapse" for="c-36017984">[-]</label><label class="expand" for="c-36017984">[8 more]</label></div><br/><div class="children"><div class="content">This is a terrible article written by someone who doesn&#x27;t seem to have even tried GPT 4. Their only example references GPT 3.5, for example, and then they waffle on about only vaguely related topics such as level 5 self-driving.<p>This quote in particular stood out as ignorant:<p><i>“What the large language models are good at is saying what an answer should sound like, which is different from what an answer should be.”</i><p>That&#x27;s... not at all how <i>large</i> language models work. Tiny, trivial, toy language models work like this, because they don&#x27;t have the internal capacity to do anything else. They just don&#x27;t have enough parameters.<p>Stephen Wolfram explained it best: After a point, the <i>only</i> way to get better at modelling the statistics of language is to go to the level &quot;above&quot; grammar and start modelling common sense facts about the world. The larger the model, the higher the level of abstraction it can reach to improve its predictions.<p>His example was this sentence: &quot;The elephant flew to the Moon.&quot;<p>That is a syntactically and grammatically correct sentence. A toy LLM, or older NLP algorithms will mark that as &quot;valid&quot; and happily match it, predict it, or whatever. But elephants <i>don&#x27;t</i> fly to the Moon, not because the <i>sentence is invalid</i>, but because they can&#x27;t fly, the Moon has never been visited by any animal, and even humans can&#x27;t reach it (at the moment). To predict that this sentence is <i>unlikely</i>, the model has to encode all of that knowledge about the world.<p>Go ask GPT 4 -- not 3.5 -- what it thinks about elephants flying to the moon. Then, and only them go write a snarky IEEE article.</div><br/><div id="36018030" class="c"><input type="checkbox" id="c-36018030" checked=""/><div class="controls bullet"><span class="by">zmnd</span><span>|</span><a href="#36017984">parent</a><span>|</span><a href="#36018301">next</a><span>|</span><label class="collapse" for="c-36018030">[-]</label><label class="expand" for="c-36018030">[4 more]</label></div><br/><div class="children"><div class="content">I think the main reason for division is that everyone projects to their own use cases. I have been using gpt-4 for quite some time and also couldn&#x27;t understand why someone would say that it just produces something that sounds like a real answer. But then I found some queries that can definitely be described as &quot;sounding like truth&quot;. So your personal experience probably wasn&#x27;t what was their experience.<p>For those curious, I was asking gpt-4 about the top 3 cards from my favorite board game, Spirit Island. All three of them sounded really convincing, having the same structure and the same writing style, but unfortunately none of them existed. So everything that fails outside of most common use cases would probably have an experience of convincing hallucinations.</div><br/><div id="36018121" class="c"><input type="checkbox" id="c-36018121" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36017984">root</a><span>|</span><a href="#36018030">parent</a><span>|</span><a href="#36018701">next</a><span>|</span><label class="collapse" for="c-36018121">[-]</label><label class="expand" for="c-36018121">[1 more]</label></div><br/><div class="children"><div class="content">People also mis-use LLMs.<p>ChatGPT is <i>forced</i> to given an answer. It&#x27;s like a human on &quot;truth serum&quot;. The drugs don&#x27;t stop you lying, they just lower inhibitions so you blab more without realising it.<p>The more obscure the topic, the more likely the hallucination. If you ask it about <i>common</i> card games, it gives very good answers.<p>If you asked a random human about 3 cards from a random board game <i>at gunpoint</i> and said: &quot;Talk, now, or you get shot&quot;, they&#x27;ll just start spouting gibberish too.<p>PS: I asked GPT 4 about that game, and it prefixed every answer with some variant of &quot;I&#x27;m not sure about this answer&quot;, or it completely refused to answer, stating that it did not know about any specific cards.</div><br/></div></div><div id="36018701" class="c"><input type="checkbox" id="c-36018701" checked=""/><div class="controls bullet"><span class="by">markgavalda</span><span>|</span><a href="#36017984">root</a><span>|</span><a href="#36018030">parent</a><span>|</span><a href="#36018121">prev</a><span>|</span><a href="#36018356">next</a><span>|</span><label class="collapse" for="c-36018701">[-]</label><label class="expand" for="c-36018701">[1 more]</label></div><br/><div class="children"><div class="content">I was curious and I have the browsing module for GPT4 so I tried to get it to name the top 3 cards, the following is what it came up with after like 10 minutes of back and forth and it browsing a lot of sites. I don&#x27;t know if this is any good as I know nothing about the game. Do these cards even exist?<p>Talons of Lightning: This is a major power card with an energy cost of 6. It&#x27;s a fast card with a range of 1 from Sacred Site. It targets any land and deals 4 damage. If you meet its threshold requirement (3 Air and 2 Fire), it deals +3 damage and destroys all Dahan 1  2 .
Tsunami: This is another major power card with an energy cost of 6. It&#x27;s a slow card with a range of 2 from the ocean. It affects every coastal land in one board, dealing 6 damage to each. If you meet its threshold requirement (3 Water and 3 Earth), it deals +4 damage and adds 2 fear per land where it destroys any towns or cities 3  4 .
Blazing Renewal: A major power card with an energy cost of 5. It&#x27;s a fast card with a range of 0, measured from any land. It targets any land, allows you to gather up to 2 of your destroyed presence, and deals 2 damage per presence gathered. If you meet its threshold requirement (4 Fire and 2 Earth), it deals +10 damage 5  6 .</div><br/></div></div><div id="36018356" class="c"><input type="checkbox" id="c-36018356" checked=""/><div class="controls bullet"><span class="by">tornato7</span><span>|</span><a href="#36017984">root</a><span>|</span><a href="#36018030">parent</a><span>|</span><a href="#36018701">prev</a><span>|</span><a href="#36018301">next</a><span>|</span><label class="collapse" for="c-36018356">[-]</label><label class="expand" for="c-36018356">[1 more]</label></div><br/><div class="children"><div class="content">Try helping it along with something like &quot;only provide information you are certain about. If you don&#x27;t know the answer, say &quot;i don&#x27;t know&quot;&quot;<p>For your question, it says it doesn&#x27;t know about any card ranking system in spirit island. Then if I ask it for a list of cards, they all check out via Google search.</div><br/></div></div></div></div><div id="36018301" class="c"><input type="checkbox" id="c-36018301" checked=""/><div class="controls bullet"><span class="by">cf141q5325</span><span>|</span><a href="#36017984">parent</a><span>|</span><a href="#36018030">prev</a><span>|</span><a href="#36018610">next</a><span>|</span><label class="collapse" for="c-36018301">[-]</label><label class="expand" for="c-36018301">[1 more]</label></div><br/><div class="children"><div class="content">Great posts. I think its an error caused by a mistake often made on the topic, the assumption that side effects we see now are some fundamental problem and not just an artifact of the way systems are trained and used. And of how we (mal)function.<p>Especially tightly embracing the cognitive bias of how special and wonderful our intelligence is. After all we have that fancy squishy brain which we assume to be essential. As far as i can tell the only visible bottlenecks when looking into the future come into view once you start debating intelligence vs emulating intelligence. And if thats really the metric some honest introspection about the nature of human intelligence might be in order.<p>Not sure how much of that is done purposefully to not get too much urgency in figuring out outer alignment on a societal level. Just as its no wonder that we havent figured out how to deal with fake news while at the same time insisting on malinformation existing, its really no wonder that we cant figure out AI alignment while not having solved human alignment. Nobody should be surprised that the cause of problems might be sitting in front of the machine.</div><br/></div></div><div id="36018610" class="c"><input type="checkbox" id="c-36018610" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#36017984">parent</a><span>|</span><a href="#36018301">prev</a><span>|</span><a href="#36018330">next</a><span>|</span><label class="collapse" for="c-36018610">[-]</label><label class="expand" for="c-36018610">[1 more]</label></div><br/><div class="children"><div class="content">&gt;The elephant flew to the Moon.<p>Doesn&#x27;t prove anything.
So GPT-4 is trained on Wolframs example or many people tried it on GPT-4 and corrected the wrong answer.</div><br/></div></div><div id="36018330" class="c"><input type="checkbox" id="c-36018330" checked=""/><div class="controls bullet"><span class="by">Nimitz14</span><span>|</span><a href="#36017984">parent</a><span>|</span><a href="#36018610">prev</a><span>|</span><a href="#36017389">next</a><span>|</span><label class="collapse" for="c-36018330">[-]</label><label class="expand" for="c-36018330">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Go ask GPT 4 -- not 3.5 -- what it thinks about elephants flying to the moon. Then, and only them go write a snarky IEEE article.<p>It&#x27;s hard to know what things have been seen in the training data and are only therefore correct. And GPT4 is large enough that it can generalize from learning that x doesn&#x27;t make sense that y also doesn&#x27;t make sense. Does that mean it *understands*? Maybe. But it doesn&#x27;t have persistent state and can&#x27;t do math. It&#x27;s definitely not yet what we think of when we say AGI.</div><br/></div></div></div></div><div id="36017389" class="c"><input type="checkbox" id="c-36017389" checked=""/><div class="controls bullet"><span class="by">softwaredoug</span><span>|</span><a href="#36017984">prev</a><span>|</span><a href="#36017705">next</a><span>|</span><label class="collapse" for="c-36017389">[-]</label><label class="expand" for="c-36017389">[39 more]</label></div><br/><div class="children"><div class="content">Annoyed at all these N=1 articles from prominent thinkers about this stuff.<p>Especially from scientists - can these sorts of folks please more carefully quantify, how often it’s “wrong” and then from that decide whether or not to “calm down”.<p>Right now I suspect we hear from the outliers on both ends of the spectrum here. People who either see AGI happening tomorrow and the more dismissive crowd. But aside from what we’ve seen about testing like the Bar exam, not a lot of boring statistical study (that makes headlines at least)</div><br/><div id="36017458" class="c"><input type="checkbox" id="c-36017458" checked=""/><div class="controls bullet"><span class="by">sheeshkebab</span><span>|</span><a href="#36017389">parent</a><span>|</span><a href="#36017626">next</a><span>|</span><label class="collapse" for="c-36017458">[-]</label><label class="expand" for="c-36017458">[33 more]</label></div><br/><div class="children"><div class="content">Anytime I ask these things something (bard, gpt etc), 
33% of the answer is genius, 
33% misleading garbage,
33% filler stuff that’s neither here or there<p>The problem is distinguishing between these parts requires me to be be an expert in the area I’m inquiring about - and then why the heck do I need to ask some idiot bot for answers to questions that I already know an answer to?<p>I don’t know who finds these things useful and more importantly blowing smoke up everyone’s collective rear, especially  medias.</div><br/><div id="36018041" class="c"><input type="checkbox" id="c-36018041" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017458">parent</a><span>|</span><a href="#36017881">next</a><span>|</span><label class="collapse" for="c-36018041">[-]</label><label class="expand" for="c-36018041">[9 more]</label></div><br/><div class="children"><div class="content">Bard is a brain-damaged-but-literate idiot compared to GPT 3, which is still dumber than the typical human.<p>Try GPT 4 for a week.<p>I&#x27;ve found it to be more like 50% immediately useful, 25% very impressive, and 25% where it&#x27;s not wrong but I have to poke it a few times with different prompts to coax out the specific answer I&#x27;m looking for.<p>That&#x27;s better than <i>most humans</i> that I collaborate with at work.<p>Literally <i>half</i> of humans -- in a professional IT setting -- can&#x27;t understand simplified, clear english in emails. Similarly, in my experience about half can&#x27;t follow simple A -&gt; B logic. Many are perpetually perplexed that prerequisites need to precede the work, not be a footnote in the post-mortem of the predictable failure. Etc...<p>PS: That last sentence is <i>too hard</i> for several English-native speakers I work with to parse. Seriously. I&#x27;m not even exaggerating the tiniest bit. I&#x27;ve had coworkers fail to understand words like &quot;orthogonal&quot; or &quot;vanilla&quot; in a sentence. <i>Vanilla!</i><p>In my estimation, Chat GPT 4 is already smarter than many people, certainly the bottom 25% of the human population.<p>LLMs are a real existential threat to those people in their current state. A few more years of improvement, and they&#x27;ll be displacing the <i>bottom 50%</i> in workplaces, easily.</div><br/><div id="36018305" class="c"><input type="checkbox" id="c-36018305" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018041">parent</a><span>|</span><a href="#36018563">next</a><span>|</span><label class="collapse" for="c-36018305">[-]</label><label class="expand" for="c-36018305">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Literally half of humans -- in a professional IT setting -- can&#x27;t understand simplified, clear english in emails. Similarly, in my experience about half can&#x27;t follow simple A -&gt; B logic.<p>There are alternate hypotheses.<p>People have preferences. When it appears that someone does not understand something, they may be pretending they don&#x27;t understand it, or they may simply be ignoring it. Maybe they are trying to avoid an unpleasant task, or maybe they find dealing with a specific person unpleasant and not worth the effort.<p>In my experience, people are far more capable and competent when they feel comfortable and are interested in the task.</div><br/><div id="36018429" class="c"><input type="checkbox" id="c-36018429" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018305">parent</a><span>|</span><a href="#36018563">next</a><span>|</span><label class="collapse" for="c-36018429">[-]</label><label class="expand" for="c-36018429">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In my experience, people are far more capable and competent when they feel comfortable and are interested in the task.<p>That&#x27;s definitely true, but in my experience people have limits: simple biological ones. Repetitive tasks make practically all humans bored, for example.<p>The fact that AIs never get sleepy, distracted, or bored already makes them super-human in at least that one aspect. That they have essentially perfect English comprehension, and hence aren&#x27;t phased by the use of jargon or technical language, puts them head-and-shoulders above most humans.<p>The frustrations I&#x27;m venting aren&#x27;t some rare thing. I&#x27;m working on a technical team where the project manager <i>doesn&#x27;t understand what the team members are saying</i>. This is not just a matter of syntax, or jargon. They just don&#x27;t understand <i>the concepts</i>. This is so common in the wider industry that I&#x27;m pleasantly surprised, shocked even, when I come across a PM that can ask useful questions instead of needing endless corrections along the lines of: &quot;It&#x27;s spelled SQL, not Sequel.&quot; I&#x27;ve never met a PM that could do simple arithmetic, like &quot;10 TB at 100 MB&#x2F;s will take over a day to copy, we should plan for that!&quot;. Never.<p>I&#x27;ve tested Chat GPT 4 on both language and <i>concepts</i> that I&#x27;ve seen trip up PMs, and it understood &quot;well enough&quot; every time.<p>For example, GPT 4: The sentence &quot;We deployed sequel server successfully last night&quot; seems incorrect due to the incorrect naming of a product. &quot;Sequel server&quot; should actually be &quot;SQL Server&quot;, a popular relational database management system (RDBMS) developed by Microsoft. Therefore, the corrected sentence should be: &quot;We deployed SQL Server successfully last night.&quot;<p>PS: If you tell GPT 4 to pretend it is a technical project manager and instruct it to ask followup questions, it is <i>noticeably</i> better at this than any PM I have worked with in the last few years.</div><br/></div></div></div></div><div id="36018563" class="c"><input type="checkbox" id="c-36018563" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018041">parent</a><span>|</span><a href="#36018305">prev</a><span>|</span><a href="#36018089">next</a><span>|</span><label class="collapse" for="c-36018563">[-]</label><label class="expand" for="c-36018563">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I&#x27;ve found it to be more like 50% immediately useful, 25% very impressive, and 25% where it&#x27;s not wrong but I have to poke it a few times with different prompts to coax out the specific answer I&#x27;m looking for.<p>That could tell us more about your questions than GPT&#x27;s capabilities.</div><br/></div></div><div id="36018089" class="c"><input type="checkbox" id="c-36018089" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018041">parent</a><span>|</span><a href="#36018563">prev</a><span>|</span><a href="#36018090">next</a><span>|</span><label class="collapse" for="c-36018089">[-]</label><label class="expand" for="c-36018089">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  I’ve had coworkers fail to understand words like “orthogonal” or “vanilla” in a sentence. Vanilla!<p>Presumably, you are referring to the idiomatic use of vanilla, which is probably a less-universal idiom than you think it is (it is of fairly recent origin in wide use, derives from a specific American cultural loading of the literal vanilla flavor) and which, even when the general idiom is understood, can rely on a deeply shared understanding of what is the basic default in the referenced context to actually be understand as to its contextual meaning.</div><br/></div></div><div id="36018090" class="c"><input type="checkbox" id="c-36018090" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018041">parent</a><span>|</span><a href="#36018089">prev</a><span>|</span><a href="#36017881">next</a><span>|</span><label class="collapse" for="c-36018090">[-]</label><label class="expand" for="c-36018090">[4 more]</label></div><br/><div class="children"><div class="content">Not to be rude, but are you paid by OpenAI to say this?<p>The amount of comments here telling people to “upgrade to ChatGPT 4” is absolutely unprecedented.<p>I know it might be good, but people will find value in it and upgrade if they see the need to do so?</div><br/><div id="36018154" class="c"><input type="checkbox" id="c-36018154" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018090">parent</a><span>|</span><a href="#36017881">next</a><span>|</span><label class="collapse" for="c-36018154">[-]</label><label class="expand" for="c-36018154">[3 more]</label></div><br/><div class="children"><div class="content">I am in no shape, way, or form affiliated with OpenAI or any other AI company.<p>What I and many others have noticed about the &quot;Are LLMs really smart?&quot; debate is that <i>everyone</i> on the &quot;Nay&quot; side is using 3.5 and everyone on the &quot;Yay&quot; side is using 4.0.<p>The naming and the versioning implies that GPT 4 is somehow <i>slightly</i> better than 3.5, like not even a &quot;full +1&quot; better, just &quot;+0.5&quot; better. (This goes to show how trivial it is to trick &quot;mere&quot; humans and their primitive meat brains.)<p>Similarly, all pre-4 LLMs including not just the older ChatGPT variants, but Bard, Vicuna, etc... are all very clearly and obviously sub-par, making glaring mistakes regularly. Hence, people generalise and assume GPT 4 must be more of the same.<p>For the last few weeks, across many forums, every time someone has said &quot;AIs can&#x27;t do <i>X</i>&quot; I have put <i>X</i> into ChatGPT 4 and it could do it, with only a very few exceptions.<p>The unfortunate thing is that there is no free trial for GPT 4, and the version on Bing doesn&#x27;t seem to be quite the same. (It&#x27;s probably too restricted by a very long system prompt.)<p>So no, people won&#x27;t form their own opinions, at least not yet, because they can&#x27;t do so without paying for access.</div><br/><div id="36018656" class="c"><input type="checkbox" id="c-36018656" checked=""/><div class="controls bullet"><span class="by">omnicognate</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018154">parent</a><span>|</span><a href="#36017881">next</a><span>|</span><label class="collapse" for="c-36018656">[-]</label><label class="expand" for="c-36018656">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been paying for GPT-4 since it came out and have used it extensively. It&#x27;s clearly an iteration on the same thing and behaves in qualitatively the same way. The differences are just differences of degree.<p>It&#x27;s not hard to get a feel for the &quot;edges&quot; of an LLM. You just need to come up with a sequence of related tasks of increasing complexity. A good one is to give it a simple program and ask what it outputs. Then progressively add complications to the program until it starts to fail to predict the output. You&#x27;ll reliably find a point where it transitions from reliably getting it right to frequently getting it wrong, and doing so in a distinctly non-humanlike way that is consistent with the space of possible programs and outputs becoming too large for its approach of predicting tokens instead of forming and mentally &quot;executing&quot; a model of the code to work. The improvement between 3.5 and 4 in this is incremental: the boundary has moved a bit, but it&#x27;s still there.</div><br/><div id="36018868" class="c"><input type="checkbox" id="c-36018868" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018656">parent</a><span>|</span><a href="#36017881">next</a><span>|</span><label class="collapse" for="c-36018868">[-]</label><label class="expand" for="c-36018868">[1 more]</label></div><br/><div class="children"><div class="content">Most developers -- let alone humans -- I&#x27;ve met can&#x27;t run <i>trivial</i> programs in their head successfully, let alone complex ones.<p>I&#x27;ve thrown crazy complicated problems at GPT 4 and had mixed results, but then again, I get mixed results from people too.<p>I&#x27;ve had it explain a multi-page SQL query I couldn&#x27;t understand myself. I asked it to write doc-comments for spaghetti code that I wrote for a programming competition, and it spat out a comment for every function correctly. One particular function was unintelligible numeric operations on single-letter identifiers, and its true purpose could only be understood through seven levels of indirection! It figured it out.<p>The fact that we&#x27;re debating the finer points of what it can and can&#x27;t do is by itself staggering.<p>Imagine if <i>next week</i> you could buy a $20K Tesla bipedal home robot. I guarantee you then people would start arguing that it &quot;can&#x27;t <i>really</i> cook&quot; because it couldn&#x27;t cook them a Michelin star quality meal with nothing but stale ingredients, one pot, and a broken spatula.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36017881" class="c"><input type="checkbox" id="c-36017881" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017458">parent</a><span>|</span><a href="#36018041">prev</a><span>|</span><a href="#36017551">next</a><span>|</span><label class="collapse" for="c-36017881">[-]</label><label class="expand" for="c-36017881">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I think the use cares for these things are far narrower than the boosters &amp; hype cycle think.<p>If you could have unlimited interns for $0 (let&#x27;s pretend it doesn&#x27;t cost tons and tons of compute) that don&#x27;t shutup, hallucinate &amp; lie, and also do good work, in varying degrees.. how many would you want?<p>These things are probably going to be great for lots of blackout - propaganda, political marketing, flooding the zone with BS of unlimited iterations of messaging.  Basically things that can be A&#x2F;B tested to death, where veracity is of zero importance, and you have near limitless shots on goal to keep iterating.</div><br/></div></div><div id="36017551" class="c"><input type="checkbox" id="c-36017551" checked=""/><div class="controls bullet"><span class="by">yberreby</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017458">parent</a><span>|</span><a href="#36017881">prev</a><span>|</span><a href="#36017678">next</a><span>|</span><label class="collapse" for="c-36017551">[-]</label><label class="expand" for="c-36017551">[7 more]</label></div><br/><div class="children"><div class="content">&gt; The problem is distinguishing between these parts requires me to be be an expert in the area I’m inquiring about - and then why the heck do I need to ask some idiot bot for answers to questions that I already know an answer to?<p>Because it can be significantly faster to check something for correctness than to produce it?<p>More so when the correctness check can itself be automated to some extent.</div><br/><div id="36017603" class="c"><input type="checkbox" id="c-36017603" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017551">parent</a><span>|</span><a href="#36018016">next</a><span>|</span><label class="collapse" for="c-36017603">[-]</label><label class="expand" for="c-36017603">[4 more]</label></div><br/><div class="children"><div class="content">&gt; ecause it can be significantly faster to check something for correctness than to produce it<p>Erm no
There are many things that cant be easily checked especially if you dont know the topic well</div><br/><div id="36017634" class="c"><input type="checkbox" id="c-36017634" checked=""/><div class="controls bullet"><span class="by">omeze</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017603">parent</a><span>|</span><a href="#36018016">next</a><span>|</span><label class="collapse" for="c-36017634">[-]</label><label class="expand" for="c-36017634">[3 more]</label></div><br/><div class="children"><div class="content">Youre right but I think the point still stands. Many times it really is easier to verify than produce. Compilable code being a good example.</div><br/><div id="36017654" class="c"><input type="checkbox" id="c-36017654" checked=""/><div class="controls bullet"><span class="by">Firmwarrior</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017634">parent</a><span>|</span><a href="#36018016">next</a><span>|</span><label class="collapse" for="c-36017654">[-]</label><label class="expand" for="c-36017654">[2 more]</label></div><br/><div class="children"><div class="content">I dunno about you, but I spend a LOT of my time fixing code that was broken in some subtle way the compiler didn&#x27;t catch<p>If we&#x27;re talking about generating and integrating sample code, it&#x27;s great at that<p>Anything more advanced and it&#x27;s a footgun</div><br/><div id="36017906" class="c"><input type="checkbox" id="c-36017906" checked=""/><div class="controls bullet"><span class="by">tominous</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017654">parent</a><span>|</span><a href="#36018016">next</a><span>|</span><label class="collapse" for="c-36017906">[-]</label><label class="expand" for="c-36017906">[1 more]</label></div><br/><div class="children"><div class="content">I have very quickly picked up the habit of pasting snippets of my code into GPT-4 and simply asking &quot;Why is this not working?&quot; Almost every time, it succinctly explains the apparent purpose of the code, and how it is subtly wrong.<p>It&#x27;s so good that I often do this preemptively to avoid a compile&#x2F;deploy&#x2F;test cycle.</div><br/></div></div></div></div></div></div></div></div><div id="36018016" class="c"><input type="checkbox" id="c-36018016" checked=""/><div class="controls bullet"><span class="by">cageface</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017551">parent</a><span>|</span><a href="#36017603">prev</a><span>|</span><a href="#36017678">next</a><span>|</span><label class="collapse" for="c-36018016">[-]</label><label class="expand" for="c-36018016">[2 more]</label></div><br/><div class="children"><div class="content">My experience with Github copilot is that the time it saves me typing out boilerplate has been more than lost when I have to spend time carefully debugging bugs in the code it produces. And those are the bugs I catch right away.<p>I expect this will improve but it&#x27;s certainly not always the case that checking something is cheaper or easier than generating it in the first place.</div><br/><div id="36018200" class="c"><input type="checkbox" id="c-36018200" checked=""/><div class="controls bullet"><span class="by">LouisSayers</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36018016">parent</a><span>|</span><a href="#36017678">next</a><span>|</span><label class="collapse" for="c-36018200">[-]</label><label class="expand" for="c-36018200">[1 more]</label></div><br/><div class="children"><div class="content">I was torn on Copilot - it seemed like it was saving me time, but I found myself getting way more value out of just copy &#x2F; pasting code into GPT4.<p>So I decided to stop my copilot subscription and just see how I go without it.<p>I&#x27;ve been off copilot for a few days now and other than having to do more code lookups it&#x27;s not a terrible experience not having it. It does feel like something that should be baked into the IDE for free though.</div><br/></div></div></div></div></div></div><div id="36017678" class="c"><input type="checkbox" id="c-36017678" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017458">parent</a><span>|</span><a href="#36017551">prev</a><span>|</span><a href="#36017545">next</a><span>|</span><label class="collapse" for="c-36017678">[-]</label><label class="expand" for="c-36017678">[1 more]</label></div><br/><div class="children"><div class="content">Multiple choice tests are easier than fill in the blank precisely because it&#x27;s easier to recognize when something is correct than it is to regurgitate a fact from thin air.<p>You don&#x27;t have to be an expert to recognize when ChatGPT is providing useful information. There&#x27;s a middle ground between expert and novice where ChatGPT provides real value. Its the times where you would know the answer if you saw it, but can quite remember it off the top of your head.</div><br/></div></div><div id="36017545" class="c"><input type="checkbox" id="c-36017545" checked=""/><div class="controls bullet"><span class="by">stocknoob</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017458">parent</a><span>|</span><a href="#36017678">prev</a><span>|</span><a href="#36017810">next</a><span>|</span><label class="collapse" for="c-36017545">[-]</label><label class="expand" for="c-36017545">[4 more]</label></div><br/><div class="children"><div class="content">Yeah, I don’t think a machine that generates novel genius ideas 1 out of 3 times is useful either. Creating a new idea is exactly as hard as curating them.</div><br/><div id="36017590" class="c"><input type="checkbox" id="c-36017590" checked=""/><div class="controls bullet"><span class="by">cageface</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017545">parent</a><span>|</span><a href="#36017598">next</a><span>|</span><label class="collapse" for="c-36017590">[-]</label><label class="expand" for="c-36017590">[2 more]</label></div><br/><div class="children"><div class="content">What novel genius ideas has the machine created so far?</div><br/><div id="36018544" class="c"><input type="checkbox" id="c-36018544" checked=""/><div class="controls bullet"><span class="by">stocknoob</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017590">parent</a><span>|</span><a href="#36017598">next</a><span>|</span><label class="collapse" for="c-36018544">[-]</label><label class="expand" for="c-36018544">[1 more]</label></div><br/><div class="children"><div class="content">From today, it lets a hobbyist create better home automation than trillion dollar FAANG companies can provide:<p><a href="https:&#x2F;&#x2F;www.atomic14.com&#x2F;2023&#x2F;05&#x2F;14&#x2F;is-this-the-future-of-home-automation.html" rel="nofollow">https:&#x2F;&#x2F;www.atomic14.com&#x2F;2023&#x2F;05&#x2F;14&#x2F;is-this-the-future-of-ho...</a><p>The novelty is asking the machine to use its own genius to do the right thing.</div><br/></div></div></div></div><div id="36017598" class="c"><input type="checkbox" id="c-36017598" checked=""/><div class="controls bullet"><span class="by">iseanstevens</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017545">parent</a><span>|</span><a href="#36017590">prev</a><span>|</span><a href="#36017810">next</a><span>|</span><label class="collapse" for="c-36017598">[-]</label><label class="expand" for="c-36017598">[1 more]</label></div><br/><div class="children"><div class="content">Sarcasm?</div><br/></div></div></div></div><div id="36017810" class="c"><input type="checkbox" id="c-36017810" checked=""/><div class="controls bullet"><span class="by">agotterer</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017458">parent</a><span>|</span><a href="#36017545">prev</a><span>|</span><a href="#36017487">next</a><span>|</span><label class="collapse" for="c-36017810">[-]</label><label class="expand" for="c-36017810">[1 more]</label></div><br/><div class="children"><div class="content">I personally find value because it saves me time. I’ll ask chat-gpt to write something slightly more complex than boilerplate code for me based on some requirements. Because I’m an “expert” I can read&#x2F;run the code and message back improvements and tweaks until to arrives at something satisfactory. It certainly doesn’t always produce correct code from the start and I often encounter syntax errors or code which doesn’t work. However, I’ve found it pretty good at remedying those issues when I describe what’s wrong.<p>It produces something which is a good enough starting place. Sure, I could have written the code myself because I already know how. But I’ve found it saves me time and require minimal effort.</div><br/></div></div><div id="36017487" class="c"><input type="checkbox" id="c-36017487" checked=""/><div class="controls bullet"><span class="by">RC_ITR</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017458">parent</a><span>|</span><a href="#36017810">prev</a><span>|</span><a href="#36017626">next</a><span>|</span><label class="collapse" for="c-36017487">[-]</label><label class="expand" for="c-36017487">[9 more]</label></div><br/><div class="children"><div class="content">Remember when the Internet was new and no-one believed anything on it?<p>Then, learning what to believe became a marketable skill for many people?<p>Then society fundamentally changed because not everyone learned that skill?<p>This is just that again.  Gen Z will joke about their millennial&#x2F;Gen X bosses believing anything the AI tells them and it will probably lead to some sort of mainstream conspiracy that Jackie O herself is running it or something (to those reading: please don&#x27;t take this idea)</div><br/><div id="36017539" class="c"><input type="checkbox" id="c-36017539" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017487">parent</a><span>|</span><a href="#36017626">next</a><span>|</span><label class="collapse" for="c-36017539">[-]</label><label class="expand" for="c-36017539">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Remember when the Internet was new and no-one believed anything on it?<p>Is this true?</div><br/><div id="36017608" class="c"><input type="checkbox" id="c-36017608" checked=""/><div class="controls bullet"><span class="by">sanderjd</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017539">parent</a><span>|</span><a href="#36017557">next</a><span>|</span><label class="collapse" for="c-36017608">[-]</label><label class="expand" for="c-36017608">[1 more]</label></div><br/><div class="children"><div class="content">Yes. &quot;Anyone can write anything on there! There&#x27;s no way to trust it.&quot; About halfway through high school they started letting us cite websites, and it was a big deal and considered very forward thinking.</div><br/></div></div><div id="36017557" class="c"><input type="checkbox" id="c-36017557" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017539">parent</a><span>|</span><a href="#36017608">prev</a><span>|</span><a href="#36017589">next</a><span>|</span><label class="collapse" for="c-36017557">[-]</label><label class="expand" for="c-36017557">[1 more]</label></div><br/><div class="children"><div class="content">I remember my parents and teachers indicating this, and me and my friends thinking “then why do they trust TV, books, etc.?”</div><br/></div></div><div id="36017589" class="c"><input type="checkbox" id="c-36017589" checked=""/><div class="controls bullet"><span class="by">wewtyflakes</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017539">parent</a><span>|</span><a href="#36017557">prev</a><span>|</span><a href="#36018003">next</a><span>|</span><label class="collapse" for="c-36017589">[-]</label><label class="expand" for="c-36017589">[3 more]</label></div><br/><div class="children"><div class="content">This is true; it was a almost a meme before memes existed, as to say &quot;I saw it on the internet so it must be true!&quot;, as a way to teasingly highlight how non-credible it was seen at the time.</div><br/><div id="36017866" class="c"><input type="checkbox" id="c-36017866" checked=""/><div class="controls bullet"><span class="by">klyrs</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017589">parent</a><span>|</span><a href="#36018003">next</a><span>|</span><label class="collapse" for="c-36017866">[-]</label><label class="expand" for="c-36017866">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Kilroy was here&quot; was a meme before &quot;meme&quot; was coined.  I don&#x27;t think the phrase &quot;before memes existed&quot; has meaning.</div><br/><div id="36018161" class="c"><input type="checkbox" id="c-36018161" checked=""/><div class="controls bullet"><span class="by">wewtyflakes</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017866">parent</a><span>|</span><a href="#36018003">next</a><span>|</span><label class="collapse" for="c-36018161">[-]</label><label class="expand" for="c-36018161">[1 more]</label></div><br/><div class="children"><div class="content">Sure. Maybe better if I qualified it with &quot;as they exist as we know them today&quot;</div><br/></div></div></div></div></div></div><div id="36018003" class="c"><input type="checkbox" id="c-36018003" checked=""/><div class="controls bullet"><span class="by">wombatpm</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017539">parent</a><span>|</span><a href="#36017589">prev</a><span>|</span><a href="#36017673">next</a><span>|</span><label class="collapse" for="c-36018003">[-]</label><label class="expand" for="c-36018003">[1 more]</label></div><br/><div class="children"><div class="content">On the internet no one knew you were a dog according to the Net Yorker cartoon</div><br/></div></div><div id="36017673" class="c"><input type="checkbox" id="c-36017673" checked=""/><div class="controls bullet"><span class="by">repler</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017539">parent</a><span>|</span><a href="#36018003">prev</a><span>|</span><a href="#36017626">next</a><span>|</span><label class="collapse" for="c-36017673">[-]</label><label class="expand" for="c-36017673">[1 more]</label></div><br/><div class="children"><div class="content">yes. I remember boomers shitting on Wikipedia around 2004, but fast forward to present day and the same people would cite it.</div><br/></div></div></div></div></div></div></div></div><div id="36017626" class="c"><input type="checkbox" id="c-36017626" checked=""/><div class="controls bullet"><span class="by">metaphor</span><span>|</span><a href="#36017389">parent</a><span>|</span><a href="#36017458">prev</a><span>|</span><a href="#36017600">next</a><span>|</span><label class="collapse" for="c-36017626">[-]</label><label class="expand" for="c-36017626">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Annoyed at all these N=1 articles from prominent thinkers about this stuff.</i><p>It&#x27;s a transcript of a casual interview, not an article, and certainly not a publication whose purpose is to convey statistical rigor.<p>As an aside, it doesn&#x27;t strike me as entitled that society might permit thinkers of academic renown to express their personal opinions in less than rigorous settings on subjects to which their peer-reviewed contributions may be categorized as &quot;prominent&quot;.</div><br/></div></div><div id="36017600" class="c"><input type="checkbox" id="c-36017600" checked=""/><div class="controls bullet"><span class="by">throwaway892238</span><span>|</span><a href="#36017389">parent</a><span>|</span><a href="#36017626">prev</a><span>|</span><a href="#36017475">next</a><span>|</span><label class="collapse" for="c-36017600">[-]</label><label class="expand" for="c-36017600">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I suspect we hear from the outliers on both ends of the spectrum<p>Welcome to publishing. Nothing gets widely published unless it&#x27;s clickbait. Something you vehemently agree with you&#x27;ll click on to see that it validates your opinion, and something you vehemently oppose you&#x27;ll furiously read to find out how stupid they are. Nobody reads fair and balanced arguments that solely come with concrete evidence; they&#x27;re rare and boring.</div><br/></div></div><div id="36017475" class="c"><input type="checkbox" id="c-36017475" checked=""/><div class="controls bullet"><span class="by">jruohonen</span><span>|</span><a href="#36017389">parent</a><span>|</span><a href="#36017600">prev</a><span>|</span><a href="#36017865">next</a><span>|</span><label class="collapse" for="c-36017475">[-]</label><label class="expand" for="c-36017475">[1 more]</label></div><br/><div class="children"><div class="content">Well, increasing N does not solve anything for these debates. It just leads to the false balance fallacy, as in many other things.</div><br/></div></div><div id="36017865" class="c"><input type="checkbox" id="c-36017865" checked=""/><div class="controls bullet"><span class="by">chris-orgmenta</span><span>|</span><a href="#36017389">parent</a><span>|</span><a href="#36017475">prev</a><span>|</span><a href="#36017705">next</a><span>|</span><label class="collapse" for="c-36017865">[-]</label><label class="expand" for="c-36017865">[2 more]</label></div><br/><div class="children"><div class="content">I feel that &#x27;we in the middle&#x27; are ignored.  
Maybe &#x27;the middle&#x27; is too polysemous here. 
The person to the left and the person to the right are shouting at the person in the middle. They often have uncharitable arguments, and they often take arguments from the other side uncharitably. The person in the middle is ignored.<p>Tug-of-rope game theory means that no-one is going to start pulling from the middle. People join the (extreme) end that they want to slightly drag the conversation towards. Maybe that&#x27;s part of it.<p>I wonder how many $XX,XXX I have historically spent on labour, for things that I now get in seconds. Data entry &#x2F; manipulation? Sure. But also wisdom &#x2F; knowledge. Entire industries HAVE collapsed over night. And will continue to collapse. And ignoring that - Why are so many assuming that AI is to be a human-replacement, rather than a limb&#x2F;exoskeleton?<p>And why is the conversation &#x27;This is magic&#x27; vs &#x27;You are stupid, this is just pulling the wool over your eyes&#x27;, rather than &#x27;This is a valuable tool in our toolbelt, that will clearly create trillions in value - Just like search, just like the resistor, just like the lightbulb&#x27;.<p>I don&#x27;t understand.... I&#x27;m not claiming LLMs to be magic. I&#x27;m not saying they are indications of AGI. I&#x27;m not saying that the world as we know it is over.  
But this is important. It&#x27;s clearly important. It&#x27;s clearly valuable. It has shown itself to be.<p>Yes, ChatGPT lies. OK? We know. We cater for that. We don&#x27;t expect it not too.<p>It feels like talking to classic rock enthusiasts that dismiss electronica et al entirely. Fine, but god_damn_ you are missing out on some incredible sound design - My heart breaks daily from what I pump into my ears - Some from rock, some hip-hop, some electronica, and most excitingly, the &gt;2000 merging, where the synergy of genres learning&#x2F;borrowing from each other is.... just. great.  
Sit back and enjoy. It is not marvellous?  
ChatGPT is obnoxious, annoying, repetitive, avoidant, argumentative . But I&#x27;m still able to appreciate its value.<p>We could stop, right now. Freeze ChatGPT in its current state. It will still create trillions in value. I don&#x27;t care if future improvements are incremental, at this point.<p>JUST the &#x27;tip of my tongue&#x27; &#x2F; synonymous words value from ChatGPT is useful. Not having to know exactly what term to plug into google.... This is the glue that binds the gray, while before we were stuck with the black and white.<p>At the very least, this is another &#x27;Google it&#x27; revolution - In the 1990s, I remember idiots (inc. me) arguing in the pub for 8 hours, over facts that should have been verified within 10 seconds.<p>I, for one, am enjoying my new bionic arm.</div><br/><div id="36018070" class="c"><input type="checkbox" id="c-36018070" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36017389">root</a><span>|</span><a href="#36017865">parent</a><span>|</span><a href="#36017705">next</a><span>|</span><label class="collapse" for="c-36018070">[-]</label><label class="expand" for="c-36018070">[1 more]</label></div><br/><div class="children"><div class="content">I foresee that the code-optimised version of GPT 4 with the 32K token context window will be amazing. GitHub Copilot was a derivative of GPT 3.0, which was pretty dumb compared to GPT 3.5, which in turn is the village idiot next to GPT 4... which IMHO is human-equivalent at <i>many</i> tasks. Not all, but <i>many</i>.<p>Realistically, GPT 4 costs 100x as much as GPT 3.5 in inference mode, so it won&#x27;t change the world just yet. There are still API rate limits, waiting lists, etc...<p>Still... having the equivalent of a junior employee assisting with your code, but at a fraction of the cost and many times the speed, will be amazing.</div><br/></div></div></div></div></div></div><div id="36017705" class="c"><input type="checkbox" id="c-36017705" checked=""/><div class="controls bullet"><span class="by">idopmstuff</span><span>|</span><a href="#36017389">prev</a><span>|</span><a href="#36017356">next</a><span>|</span><label class="collapse" for="c-36017705">[-]</label><label class="expand" for="c-36017705">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The example I used at the time was, I think it was a Google program labeling an image of people playing Frisbee in the park. And if a person says, “Oh, that’s a person playing Frisbee in the park,” you would assume you could ask him a question, like, “Can you eat a Frisbee?” And they would know, of course not; it’s made of plastic. You’d just expect they’d have that competence. That they would know the answer to the question, “Can you play Frisbee in a snowstorm? Or, how far can a person throw a Frisbee? Can they throw it 10 miles? Can they only throw it 10 centimeters?” You’d expect all that competence from that one piece of performance: a person saying, “That’s a picture of people playing Frisbee in the park.”<p>This seems like exactly a set of things that GPT-4 can do. The image recognition capabilities haven&#x27;t been released yet, but they were demoed when it launched and clearly have the ability to handle a situation like this. From there, you could ask it every single one of these questions and get the correct answer.</div><br/></div></div><div id="36017356" class="c"><input type="checkbox" id="c-36017356" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#36017705">prev</a><span>|</span><a href="#36017789">next</a><span>|</span><label class="collapse" for="c-36017356">[-]</label><label class="expand" for="c-36017356">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  I’ll give you that. And I think what they say, interestingly, is how much of our language is very much rote, R-O-T-E, rather than generated directly, because it can be collapsed down to this set of parameters. But in that “Seven Deadly Sins” article, I said that one of the deadly sins was how we humans mistake performance for competence.<p>On this, I think he might be wrong. I think the hallucination ability shows that the generation of language can be rote, such that the embedding of ideas is a rote item learnable in the billions-to-trillions parameter space, but not the entirety of language. To me, logic and truth seem to be separate concepts from generation propensity.<p>Note: I am still learning the mathematics driving LLMs, and my opinions might change in the future.</div><br/><div id="36017572" class="c"><input type="checkbox" id="c-36017572" checked=""/><div class="controls bullet"><span class="by">progrus</span><span>|</span><a href="#36017356">parent</a><span>|</span><a href="#36017571">next</a><span>|</span><label class="collapse" for="c-36017572">[-]</label><label class="expand" for="c-36017572">[1 more]</label></div><br/><div class="children"><div class="content">Logic is inconsistent and&#x2F;or incomplete.<p>Truth is both consistent and complete.</div><br/></div></div><div id="36017571" class="c"><input type="checkbox" id="c-36017571" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#36017356">parent</a><span>|</span><a href="#36017572">prev</a><span>|</span><a href="#36017789">next</a><span>|</span><label class="collapse" for="c-36017571">[-]</label><label class="expand" for="c-36017571">[1 more]</label></div><br/><div class="children"><div class="content">Hallucination right now is just the exponential divergence that LeCun talks about if you are interested in reading more about it.<p>LLMs probably need generative diffusion but still lack the fundamentals to reason, plan and evaluate.</div><br/></div></div></div></div><div id="36017789" class="c"><input type="checkbox" id="c-36017789" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36017356">prev</a><span>|</span><a href="#36018904">next</a><span>|</span><label class="collapse" for="c-36017789">[-]</label><label class="expand" for="c-36017789">[6 more]</label></div><br/><div class="children"><div class="content">&gt; It gives an answer with complete confidence, and I sort of believe it. And half the time, it’s completely wrong.<p>That&#x27;s bullshit, unless you are asking questions specifically designed to make GPT-4 hallucinate. For most real-world, everyday topics, the accuracy is close to 100%. GPT-4 would be utterly useless otherwise.</div><br/><div id="36018047" class="c"><input type="checkbox" id="c-36018047" checked=""/><div class="controls bullet"><span class="by">helloplanets</span><span>|</span><a href="#36017789">parent</a><span>|</span><a href="#36017929">next</a><span>|</span><label class="collapse" for="c-36018047">[-]</label><label class="expand" for="c-36018047">[1 more]</label></div><br/><div class="children"><div class="content">Such a weird time, when the gap in the performance of GPT 3.5 and 4 is huge, but the time between their releases is so short. Some of the critique that was apt for 3.5 sounds a bit out of touch when it comes to 4.</div><br/></div></div><div id="36017929" class="c"><input type="checkbox" id="c-36017929" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#36017789">parent</a><span>|</span><a href="#36018047">prev</a><span>|</span><a href="#36018904">next</a><span>|</span><label class="collapse" for="c-36017929">[-]</label><label class="expand" for="c-36017929">[4 more]</label></div><br/><div class="children"><div class="content">Yeah a lot of times is right, unless they are really complex subjects then maybe even less then 50</div><br/><div id="36018265" class="c"><input type="checkbox" id="c-36018265" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#36017789">root</a><span>|</span><a href="#36017929">parent</a><span>|</span><a href="#36018051">next</a><span>|</span><label class="collapse" for="c-36018265">[-]</label><label class="expand" for="c-36018265">[2 more]</label></div><br/><div class="children"><div class="content">Less about complexity than about how well-documented the subject is on the internet IMO. I’ve been using it to help me set up and troubleshoot AWS Elastic Kubernetes clusters, which are plenty complex, and I’d estimate it’s been around 95% accurate. (And for at least one of the times it seemed to be wrong, it turned out I’d made a mistake following its instructions...)</div><br/><div id="36018390" class="c"><input type="checkbox" id="c-36018390" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#36017789">root</a><span>|</span><a href="#36018265">parent</a><span>|</span><a href="#36018051">next</a><span>|</span><label class="collapse" for="c-36018390">[-]</label><label class="expand" for="c-36018390">[1 more]</label></div><br/><div class="children"><div class="content">You could ask me a difficult scientific question which I wouldn&#x27;t even understand. But I could google and find a scientific paper which I would pass to you. You could say fantastic answer, thanks.  But I would have no clue as to  whether it is or is not. Now if I could just do that fast enough to serve all the people all the time, you would call me a sensation. I think this is what&#x27;s happening with LLMs.</div><br/></div></div></div></div><div id="36018051" class="c"><input type="checkbox" id="c-36018051" checked=""/><div class="controls bullet"><span class="by">munchler</span><span>|</span><a href="#36017789">root</a><span>|</span><a href="#36017929">parent</a><span>|</span><a href="#36018265">prev</a><span>|</span><a href="#36018904">next</a><span>|</span><label class="collapse" for="c-36018051">[-]</label><label class="expand" for="c-36018051">[1 more]</label></div><br/><div class="children"><div class="content">Which is true for many humans as well.</div><br/></div></div></div></div></div></div><div id="36017739" class="c"><input type="checkbox" id="c-36017739" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#36018904">prev</a><span>|</span><a href="#36017636">next</a><span>|</span><label class="collapse" for="c-36017739">[-]</label><label class="expand" for="c-36017739">[4 more]</label></div><br/><div class="children"><div class="content">Rodney Brooks&#x27; deep learning predictions have not aged well. For example, in his 2018 blog post (<a href="http:&#x2F;&#x2F;rodneybrooks.com&#x2F;forai-steps-toward-super-intelligence-i-how-we-got-here&#x2F;" rel="nofollow">http:&#x2F;&#x2F;rodneybrooks.com&#x2F;forai-steps-toward-super-intelligenc...</a>), he rates various approaches from 1-3 (with 3 being the best). Neural nets scored:<p>Composition: 1<p>Grounding: 3<p>Spatial: 1<p>Sentience: 1<p>Ambiguity: 2<p>At that time, the potential of neural nets was already very clear.<p>He also predicted that by 2020 we&#x27;ll have popular press stories that the era of Deep Learning is over and that by 2021 VCs will figure out that for an investment to pay off there needs to be something more than &quot;X + Deep Learning&quot;.</div><br/><div id="36017860" class="c"><input type="checkbox" id="c-36017860" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36017739">parent</a><span>|</span><a href="#36017636">next</a><span>|</span><label class="collapse" for="c-36017860">[-]</label><label class="expand" for="c-36017860">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll let you in on a secret: There aren&#x27;t actually any &quot;AI experts&quot;. There are <i>machine learning</i> experts, that is, people whose expertise lies in designing and analyzing systems that perform (semi-)automatic inference on data. But nobody can be an expert on &quot;artificial intelligence&quot;, because we don&#x27;t know what that word really means. We don&#x27;t even know what intelligence really is. We have no idea how the human mind works. We don&#x27;t understand emergence, at all, which is why we continue to be surprised when it happens.<p>So it shouldn&#x27;t come as a shock when eminent figures commonly labeled &quot;AI experts&quot; make predictions that turn out to be fundamentally and embarrassingly wrong in a very short timeframe: They&#x27;re just talking out of their behinds, like everyone else.</div><br/><div id="36018013" class="c"><input type="checkbox" id="c-36018013" checked=""/><div class="controls bullet"><span class="by">xvilka</span><span>|</span><a href="#36017739">root</a><span>|</span><a href="#36017860">parent</a><span>|</span><a href="#36017636">next</a><span>|</span><label class="collapse" for="c-36018013">[-]</label><label class="expand" for="c-36018013">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We have no idea how the human mind works.<p>True, but we already know the so called &quot;neural networks&quot; that many computer scientists believe are how brain works aren&#x27;t even close. They are all based on half-a-century old concept of neuron that was debunked many times over, experimentally by real neuroscientists.</div><br/><div id="36018105" class="c"><input type="checkbox" id="c-36018105" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36017739">root</a><span>|</span><a href="#36018013">parent</a><span>|</span><a href="#36017636">next</a><span>|</span><label class="collapse" for="c-36018105">[-]</label><label class="expand" for="c-36018105">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s correct, but it doesn&#x27;t mean artificial neural networks cannot achieve intelligence, or even superintelligence. The fact that the human brain doesn&#x27;t work like that doesn&#x27;t automatically imply that (structurally) simpler models are fundamentally less capable.</div><br/></div></div></div></div></div></div></div></div><div id="36017636" class="c"><input type="checkbox" id="c-36017636" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#36017739">prev</a><span>|</span><a href="#36018283">next</a><span>|</span><label class="collapse" for="c-36017636">[-]</label><label class="expand" for="c-36017636">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately even though the questions are about GPT-4 the answers and personal experience only refer to GPT-3.5 at most.  I hope openai changes the name of the next version to avoid this confusing narrative by prominent people.  3.5 vs 4 is like comparing a toddler to a high school kid.</div><br/></div></div><div id="36018283" class="c"><input type="checkbox" id="c-36018283" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#36017636">prev</a><span>|</span><a href="#36018374">next</a><span>|</span><label class="collapse" for="c-36018283">[-]</label><label class="expand" for="c-36018283">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  it answers with such confidence any question I ask. It gives an answer with complete confidence, and I sort of believe it. And half the time, it’s completely wrong.<p>That&#x27;s it. LLMs don&#x27;t have SHAME! They simply don&#x27;t care if what they&#x27;re saying is false or not. They are like some politicians of late.<p>They don&#x27;t even understand that giving out a wrong or misleading answer will affect their credibility. You see LLMs don&#x27;t have a DESIRE for credibility. They don&#x27;t have desires.<p>We need not fear these models. But we do need to fear some people who will use them for evil purposes.</div><br/></div></div><div id="36018374" class="c"><input type="checkbox" id="c-36018374" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#36018283">prev</a><span>|</span><a href="#36018891">next</a><span>|</span><label class="collapse" for="c-36018374">[-]</label><label class="expand" for="c-36018374">[1 more]</label></div><br/><div class="children"><div class="content"><i>&quot;And I think what they say, interestingly, is how much of our language is very much rote.&quot;</i><p>I&#x27;ve been saying that for a while. Large language model systems have made it painfully clear that much of what humans thought was intelligent behavior is rather banal.
The scary thing is that a sizable fraction of white-collar work is banal enough to be done by such systems.</div><br/></div></div><div id="36018891" class="c"><input type="checkbox" id="c-36018891" checked=""/><div class="controls bullet"><span class="by">stareatgoats</span><span>|</span><a href="#36018374">prev</a><span>|</span><a href="#36017549">next</a><span>|</span><label class="collapse" for="c-36018891">[-]</label><label class="expand" for="c-36018891">[3 more]</label></div><br/><div class="children"><div class="content">One of my goals in life before dementia sets in for real is to devise some model, perhaps a conceptual framework which will allow us to escape the clutches of habitual simplification, a subset of which is dichotomous thinking, which in turn leads to the inevitable painting of strawmen as a way to prove our point (among other things). How sweet it would be to shortcut all the mandatory twists and turns of discourse that follows: &quot;this is a mischaracterization of x&quot;, &quot;not all x are y&quot;, &quot;x and z are really not opposites, but overlapping&quot;, &quot;this is a spectrum with a bell curve, not an either&#x2F;or&quot; etc. But of course, we all do this, not because we can&#x27;t think clearly, but because we have an agenda, or maybe more frequently: want to trash talk the stance of an opponent because of what the proliferation of that stance might lead to, and so on. Taking such into account should be an integral part of the conceptual framework, obviously.<p>In the case at hand, one could easily argue that people in the debate are creating false dichotomies: LLMs are either stochastic parrots <i>OR</i> algorithms with an understanding, when in reality they are both (and also something else completely), but acknowledging such would likely require that one doesn&#x27;t have an axe to grind, a stake in the field or what you might call it. It would require extending some &quot;philosophers charity&quot; to an opponent, that maybe has tried to undercut one&#x27;s work for decades, in a field steeped in fierce and bitter competition for a name, like academia. Or, in case one has a business in the field, it would require maybe saying something that puts your core business idea in the crosshairs of legislators, or something else that doesn&#x27;t serve your long term business interests.<p>Which brings us to this important aspect of this &quot;conceptual framework against simplification&quot; already briefly touched upon, namely identifying the bias of the participants in the debate. My impression is that naming bias has largely gone out of fashion, which is a pity because it is really a necessary part of understanding an argument: it rarely explains it all (that would be a grave simplification), but it is really a vital part of understanding an argument. And a difficult one, because people will go to extreme lengths to hide their agenda. And the current conceptual framework for unravelling bias has largely been occupied by the fact-checking industry: i.e. things are either true or false, and once you are cleared (like most mainstream media) then bias is not questioned. But we can be assured, there is always <i>some</i> bias, and it is usually relevant to name it (if one can see it), even if it infuriates the named party.<p>Just sayin&#x27;.</div><br/><div id="36018928" class="c"><input type="checkbox" id="c-36018928" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36018891">parent</a><span>|</span><a href="#36017549">next</a><span>|</span><label class="collapse" for="c-36018928">[-]</label><label class="expand" for="c-36018928">[2 more]</label></div><br/><div class="children"><div class="content">The limitations of language make adversarial debate a powerful tool for advancing our understanding. It might be construed as unfortunate, but it brought us this far.<p>Maybe when we learn higher dimensional ways of communicating we can get better tools for constructing common knowledge.</div><br/><div id="36019074" class="c"><input type="checkbox" id="c-36019074" checked=""/><div class="controls bullet"><span class="by">stareatgoats</span><span>|</span><a href="#36018891">root</a><span>|</span><a href="#36018928">parent</a><span>|</span><a href="#36017549">next</a><span>|</span><label class="collapse" for="c-36019074">[-]</label><label class="expand" for="c-36019074">[1 more]</label></div><br/><div class="children"><div class="content">&gt; adversarial debate (...) might be construed as unfortunate, but it brought us this far.<p>I believe our understanding will go, and would have gone, further with less adversity and more &quot;philosophers charity&quot;. I&#x27;m not sure if we need any &quot;higher dimensional ways of communicating&quot; (whatever that is).<p>But a pervasive adversity in our society and time puts a natural limit of how un-adversarial our debates can be: you can&#x27;t expect Ukrainian defenders to extend much &quot;philosophers charity&quot; to Putin.<p>The more conflict laden a topic, the less truth (multifaceted analysis) one can expect, and (if we aspire to a balanced viewpoint) that&#x27;s why we need to keep an eye on bias, not just in war where &quot;the first victim is the truth&quot;, but always.</div><br/></div></div></div></div></div></div><div id="36017549" class="c"><input type="checkbox" id="c-36017549" checked=""/><div class="controls bullet"><span class="by">mrmincent</span><span>|</span><a href="#36018891">prev</a><span>|</span><a href="#36017843">next</a><span>|</span><label class="collapse" for="c-36017549">[-]</label><label class="expand" for="c-36017549">[5 more]</label></div><br/><div class="children"><div class="content">I get why AI people are at pains to say that GPT-* isn’t AI, and that agi is still a long, difficult way off, I do understand that the distinction is important. But ChatGPT has become such a useful tool to help explain a concept or to filter thoughts through I don’t really care if it’s proper AI or just playing pretend.<p>Google search gives me pretty useless results these days, forums are slow and inconsistent to respond. ChatGPT is fast, easy to use, and sometimes incredibly wrong. I can live with that, I’m not using it to drive my car.</div><br/><div id="36017602" class="c"><input type="checkbox" id="c-36017602" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#36017549">parent</a><span>|</span><a href="#36017979">next</a><span>|</span><label class="collapse" for="c-36017602">[-]</label><label class="expand" for="c-36017602">[1 more]</label></div><br/><div class="children"><div class="content">This is exactly my thoughts and feelings after more than 20 years of Googling except that Google is largely still useful as it is. Sooner or later, however, most of the people will use ChatGPT or similar services as a better replacement for Google search or Google on steroids. Before the advent of ChatGPT researchers especially, have been clamoring for better Google search with more contexts, intuitive and relevant feedbacks.<p>With the new ChatGPT (Plus) features introduction for examples web online search and plug-ins, ChatGPT has becoming a very powerful and viable better alternative to Google search.</div><br/></div></div><div id="36017979" class="c"><input type="checkbox" id="c-36017979" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#36017549">parent</a><span>|</span><a href="#36017602">prev</a><span>|</span><a href="#36017843">next</a><span>|</span><label class="collapse" for="c-36017979">[-]</label><label class="expand" for="c-36017979">[3 more]</label></div><br/><div class="children"><div class="content">AGI is just a poorly defined, moving target.<p>Absent the goals constantly shifting, GPT-3 can be viewed as one, GPT-4 even more so. You can ask it questions about almost anything (at a broad level) and get an answer. That&#x27;s what makes it general and &quot;intelligent&quot;</div><br/><div id="36018081" class="c"><input type="checkbox" id="c-36018081" checked=""/><div class="controls bullet"><span class="by">nirav72</span><span>|</span><a href="#36017549">root</a><span>|</span><a href="#36017979">parent</a><span>|</span><a href="#36018156">next</a><span>|</span><label class="collapse" for="c-36018081">[-]</label><label class="expand" for="c-36018081">[1 more]</label></div><br/><div class="children"><div class="content">&gt;makes it general and &quot;intelligent&quot;<p>Isn&#x27;t intelligent a matter of perspective?  Most people that are critical of GPT-4 wonder if it ever produces anything novel. Since its been trained on existing text created by humans. So it&#x27;s replicating those patterns in its output. But yes, it has its general purpose use as a tool. But it has its limits. Just the other day, there was an article posted on HA about how LLM&#x27;s can&#x27;t handle negation and tend to fall apart.<p>Here is the article.  <a href="https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;ai-like-chatgpt-are-no-good-at-not-20230512&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;ai-like-chatgpt-are-no-good-a...</a></div><br/></div></div><div id="36018156" class="c"><input type="checkbox" id="c-36018156" checked=""/><div class="controls bullet"><span class="by">kmoser</span><span>|</span><a href="#36017549">root</a><span>|</span><a href="#36017979">parent</a><span>|</span><a href="#36018081">prev</a><span>|</span><a href="#36017843">next</a><span>|</span><label class="collapse" for="c-36018156">[-]</label><label class="expand" for="c-36018156">[1 more]</label></div><br/><div class="children"><div class="content">You can ask the original ELIZA anything and get an answer but that doesn&#x27;t make it &quot;intelligent&quot; according to any generally accepted definition of AGI.<p>Regardless of how you define it exactly, AGI means essentially the ability to converse relatively intelligently on any topic. That means an app that tends to spout gibberish when asked a question most humans should be able to answer is, well, not an AGI.</div><br/></div></div></div></div></div></div><div id="36017843" class="c"><input type="checkbox" id="c-36017843" checked=""/><div class="controls bullet"><span class="by">2devnull</span><span>|</span><a href="#36017549">prev</a><span>|</span><a href="#36018508">next</a><span>|</span><label class="collapse" for="c-36017843">[-]</label><label class="expand" for="c-36017843">[1 more]</label></div><br/><div class="children"><div class="content">If it weren’t for gpt all we’d be talking about is layoffs so thank you very much i’ll take ridiculous ai talk over the alternative.</div><br/></div></div><div id="36018508" class="c"><input type="checkbox" id="c-36018508" checked=""/><div class="controls bullet"><span class="by">braindead_in</span><span>|</span><a href="#36017843">prev</a><span>|</span><a href="#36018015">next</a><span>|</span><label class="collapse" for="c-36018508">[-]</label><label class="expand" for="c-36018508">[1 more]</label></div><br/><div class="children"><div class="content">&gt; No, because it doesn’t have any underlying model of the world.<p>Ilya&#x27;s counter to this reasoning is for next word prediction to work, the model has to &#x27;understand&#x27; our world. Otherwise the predictions will be way off. Therefore the human world has been modelled to a degree by GPT.<p>I haven&#x27;t yet heard a good counter to that.</div><br/></div></div><div id="36018015" class="c"><input type="checkbox" id="c-36018015" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#36018508">prev</a><span>|</span><a href="#36017913">next</a><span>|</span><label class="collapse" for="c-36018015">[-]</label><label class="expand" for="c-36018015">[1 more]</label></div><br/><div class="children"><div class="content">I don’t completely disagree but this article has a lot of “they said x about y, and that didn’t come true” type arguments which don’t resonate.</div><br/></div></div><div id="36017913" class="c"><input type="checkbox" id="c-36017913" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#36018015">prev</a><span>|</span><a href="#36018573">next</a><span>|</span><label class="collapse" for="c-36017913">[-]</label><label class="expand" for="c-36017913">[1 more]</label></div><br/><div class="children"><div class="content">Not really the thrust of the article, but that &#x27;50s picture of the family playing scrabble in a self-driving car, surrounded by text about trains... really makes me think that if I were that family, I&#x27;d still prefer to be playing scrabble on a train, rather than in a cramped self-driving car on a highway.</div><br/></div></div><div id="36018573" class="c"><input type="checkbox" id="c-36018573" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#36017913">prev</a><span>|</span><a href="#36017533">next</a><span>|</span><label class="collapse" for="c-36018573">[-]</label><label class="expand" for="c-36018573">[1 more]</label></div><br/><div class="children"><div class="content">Amazingly wrong and proven wrong. This guy didn&#x27;t get the memo where GPT passed all the medical and law exams and now coding. Coding is not &quot;how an answer should be like&quot;, it is the answer. When humanity will stop hearing out peripherals to AI? Let&#x27;s focus in what Ilya says or Hinton.</div><br/></div></div><div id="36017533" class="c"><input type="checkbox" id="c-36017533" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36018573">prev</a><span>|</span><a href="#36017622">next</a><span>|</span><label class="collapse" for="c-36017533">[-]</label><label class="expand" for="c-36017533">[7 more]</label></div><br/><div class="children"><div class="content">It is a disruption. It now has more wood behind the arrow. You can’t ignore it just because it lies today for some inputs.</div><br/><div id="36017595" class="c"><input type="checkbox" id="c-36017595" checked=""/><div class="controls bullet"><span class="by">Gigachad</span><span>|</span><a href="#36017533">parent</a><span>|</span><a href="#36017562">next</a><span>|</span><label class="collapse" for="c-36017595">[-]</label><label class="expand" for="c-36017595">[5 more]</label></div><br/><div class="children"><div class="content">Feels like there have been a lot of previous technologies where the last 10% was far more complex than the first 90%. Self driving cars were pretty much solved a decade ago, and yet we still aren&#x27;t there yet. VR was pretty much working and ready to change the world a decade ago, and we still aren&#x27;t there yet.<p>So it&#x27;s hard to tell if this is an iphone moment where it just rockets off in to space and changes the world. Or if it&#x27;s something that will always be &quot;not quite there yet&quot;</div><br/><div id="36017787" class="c"><input type="checkbox" id="c-36017787" checked=""/><div class="controls bullet"><span class="by">jstx1</span><span>|</span><a href="#36017533">root</a><span>|</span><a href="#36017595">parent</a><span>|</span><a href="#36017562">next</a><span>|</span><label class="collapse" for="c-36017787">[-]</label><label class="expand" for="c-36017787">[4 more]</label></div><br/><div class="children"><div class="content">The difference is that GPT-4 is already useful even in its current imperfect form. It’s not a 90% thing that needs to get to 100% for us to use it.</div><br/><div id="36017859" class="c"><input type="checkbox" id="c-36017859" checked=""/><div class="controls bullet"><span class="by">Gigachad</span><span>|</span><a href="#36017533">root</a><span>|</span><a href="#36017787">parent</a><span>|</span><a href="#36018537">next</a><span>|</span><label class="collapse" for="c-36017859">[-]</label><label class="expand" for="c-36017859">[2 more]</label></div><br/><div class="children"><div class="content">VR is useful in its current form. Self driving tech is being used for lots of assistance feature. They just haven’t changed the world yet.</div><br/><div id="36017874" class="c"><input type="checkbox" id="c-36017874" checked=""/><div class="controls bullet"><span class="by">jstx1</span><span>|</span><a href="#36017533">root</a><span>|</span><a href="#36017859">parent</a><span>|</span><a href="#36018537">next</a><span>|</span><label class="collapse" for="c-36017874">[-]</label><label class="expand" for="c-36017874">[1 more]</label></div><br/><div class="children"><div class="content">In their current state VR and self driving aren’t on the same level of usefulness as LLMs, not even close.</div><br/></div></div></div></div><div id="36018537" class="c"><input type="checkbox" id="c-36018537" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#36017533">root</a><span>|</span><a href="#36017787">parent</a><span>|</span><a href="#36017859">prev</a><span>|</span><a href="#36017562">next</a><span>|</span><label class="collapse" for="c-36018537">[-]</label><label class="expand" for="c-36018537">[1 more]</label></div><br/><div class="children"><div class="content">The context is AGI, not whether it&#x27;s useful tool.</div><br/></div></div></div></div></div></div><div id="36017562" class="c"><input type="checkbox" id="c-36017562" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#36017533">parent</a><span>|</span><a href="#36017595">prev</a><span>|</span><a href="#36017622">next</a><span>|</span><label class="collapse" for="c-36017562">[-]</label><label class="expand" for="c-36017562">[1 more]</label></div><br/><div class="children"><div class="content">Maybe, but the rest of the arrow is not more chat gpt but other AI things to come. The problem I currently see is the hype, we’re acting as if we’re already there, we’ve nearly achieved AGI with LLMs, we just need to ramp up production more and miraculously AGI will pop into existence</div><br/></div></div></div></div><div id="36017622" class="c"><input type="checkbox" id="c-36017622" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#36017533">prev</a><span>|</span><a href="#36017919">next</a><span>|</span><label class="collapse" for="c-36017622">[-]</label><label class="expand" for="c-36017622">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Brooks: No, because it doesn’t have any underlying model of the world.<p>I don&#x27;t know whether to be more disappointed with the famous technologists who are apparently unable to think of questions to ask GPT-4 that require a world model to answer, or with the writers who don&#x27;t question them about it.</div><br/></div></div><div id="36017919" class="c"><input type="checkbox" id="c-36017919" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#36017622">prev</a><span>|</span><a href="#36017443">next</a><span>|</span><label class="collapse" for="c-36017919">[-]</label><label class="expand" for="c-36017919">[1 more]</label></div><br/><div class="children"><div class="content">Gpt is a great tool, it won’t be able to do complex tasks because it really isn’t that smart.  If you tell it to do something relatively complex end to end, it will fail unless a plugin specifically supports it.</div><br/></div></div><div id="36017443" class="c"><input type="checkbox" id="c-36017443" checked=""/><div class="controls bullet"><span class="by">jstx1</span><span>|</span><a href="#36017919">prev</a><span>|</span><a href="#36018174">next</a><span>|</span><label class="collapse" for="c-36017443">[-]</label><label class="expand" for="c-36017443">[6 more]</label></div><br/><div class="children"><div class="content">&gt; It gives an answer with complete confidence, and I sort of believe it. And half the time, it’s completely wrong.<p>Nowhere near half in my experience. This is why we have benchmarks and metrics - so we don&#x27;t need to rely on the author&#x27;s opinion or on mine.<p>&gt; I think it’s going to be another thing that’s useful.<p>Good.</div><br/><div id="36017616" class="c"><input type="checkbox" id="c-36017616" checked=""/><div class="controls bullet"><span class="by">strofcon</span><span>|</span><a href="#36017443">parent</a><span>|</span><a href="#36017609">next</a><span>|</span><label class="collapse" for="c-36017616">[-]</label><label class="expand" for="c-36017616">[1 more]</label></div><br/><div class="children"><div class="content">Oh, and I agree - it&#x27;ll be useful for a number of reasons, but &#x27;being generally intelligent&#x27; won&#x27;t be one of them. :-)</div><br/></div></div><div id="36017609" class="c"><input type="checkbox" id="c-36017609" checked=""/><div class="controls bullet"><span class="by">strofcon</span><span>|</span><a href="#36017443">parent</a><span>|</span><a href="#36017616">prev</a><span>|</span><a href="#36018113">next</a><span>|</span><label class="collapse" for="c-36017609">[-]</label><label class="expand" for="c-36017609">[2 more]</label></div><br/><div class="children"><div class="content">I think you make a good point, benchmarks and metrics are indeed a better proxy for performance. Seems worth pointing out that, while &quot;nowhere near half in [your] experience&quot; are completely wrong, I don&#x27;t take your word for it either. :-)<p>The trouble in my view is that the only way to know that the answers you&#x27;re getting are accurate and not misleading is to study up on the answers elsewhere - which is a great habit to nurture, but is also precisely why these tools tend toward uselessness in their &quot;general AI&quot; bids. If I can&#x27;t know how the answer was built, or how good that answer is, there&#x27;s no point asking it - I&#x27;ll just do my own reading and apply appropriate discernment as I go.<p>To be fair, hardly anyone does this today, nor did they before LLM-based chat bots... So it&#x27;s a moot point, because society is largely doomed anyway. But a moot point can still be a valid one.<p>I also think the author makes a good point that we frequently confuse performance for competence. &quot;It does a really good job at &lt;X&gt;!... or at least does a damn fine job of mimicking someone who <i>acts</i> like they do a really good job at &lt;X&gt;!&quot;<p>By way of analogy, consider Elon Musk - by all <i>appearances</i>, he&#x27;s a genius and is saving humanity - but by dint of his narcissism and largely smooth-brained approach to... well... everything... he&#x27;s running all of us into an earlier planet-size grave than is necessary. His performance is fantastic, his competence is nonexistent.</div><br/><div id="36017756" class="c"><input type="checkbox" id="c-36017756" checked=""/><div class="controls bullet"><span class="by">jstx1</span><span>|</span><a href="#36017443">root</a><span>|</span><a href="#36017609">parent</a><span>|</span><a href="#36018113">next</a><span>|</span><label class="collapse" for="c-36017756">[-]</label><label class="expand" for="c-36017756">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If I can&#x27;t know how the answer was built, or how good that answer is, there&#x27;s no point asking it<p>In many cases, like programming for example, you can know how good the answer is - either by reading it (verifying an idea is different from coming up with it) or by testing&#x2F;running code.<p>How the answer was built seems completely irrelevant to me, I don’t get how a useful answer produced by method x is different from a useful answer produced by method y.</div><br/></div></div></div></div><div id="36018113" class="c"><input type="checkbox" id="c-36018113" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36017443">parent</a><span>|</span><a href="#36017609">prev</a><span>|</span><a href="#36018112">next</a><span>|</span><label class="collapse" for="c-36018113">[-]</label><label class="expand" for="c-36018113">[1 more]</label></div><br/><div class="children"><div class="content">Which benchmarks are you going off ?<p>Also, just take it easy :)</div><br/></div></div><div id="36018112" class="c"><input type="checkbox" id="c-36018112" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36017443">parent</a><span>|</span><a href="#36018113">prev</a><span>|</span><a href="#36018174">next</a><span>|</span><label class="collapse" for="c-36018112">[-]</label><label class="expand" for="c-36018112">[1 more]</label></div><br/><div class="children"><div class="content">Which benchmarks are you going off ?</div><br/></div></div></div></div><div id="36018174" class="c"><input type="checkbox" id="c-36018174" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#36017443">prev</a><span>|</span><a href="#36017909">next</a><span>|</span><label class="collapse" for="c-36018174">[-]</label><label class="expand" for="c-36018174">[1 more]</label></div><br/><div class="children"><div class="content">The primary question is, what phase are we in now? Are we just at the beginning or somehow already in the asymptotic phase?</div><br/></div></div><div id="36017909" class="c"><input type="checkbox" id="c-36017909" checked=""/><div class="controls bullet"><span class="by">thomastjeffery</span><span>|</span><a href="#36018174">prev</a><span>|</span><a href="#36017547">next</a><span>|</span><label class="collapse" for="c-36017909">[-]</label><label class="expand" for="c-36017909">[1 more]</label></div><br/><div class="children"><div class="content">Brooks has done something I appreciate a lot: he turned a phrase.<p>&gt; stop confusing performance with competence<p>You can safely skip the rest of the article. That sentence gives you all you need, because <i>you</i> are competent.<p>If you want a little more meat:<p>&gt; The example I used at the time was, I think it was a Google program labeling an image of people playing Frisbee in the park. And if a person says, “Oh, that’s a person playing Frisbee in the park,” you would assume you could ask him a question, like, “Can you eat a Frisbee?” And they would know, of course not; it’s made of plastic. You’d just expect they’d have that competence. That they would know the answer to the question, “Can you play Frisbee in a snowstorm? Or, how far can a person throw a Frisbee? Can they throw it 10 miles? Can they only throw it 10 centimeters?” You’d expect all that competence from that one piece of performance: a person saying, “That’s a picture of people playing Frisbee in the park.”<p>---<p>So I&#x27;ve calmed down. Now what? The problem isn&#x27;t only that this train is flying off on a tangent: it&#x27;s that it&#x27;s off the rails. What rails should it be on?<p>The problem, as I see it, is narrative. As soon as we called it &quot;AI&quot;, that wrote the Genesis of the Scripture of the cult. In this new religious movement, God is spelled L-L-M. Back here in reality, LLM isn&#x27;t a God; or even a <i>person</i> at all.<p>That&#x27;s the mistake: personification. A person can perform, but a performance can&#x27;t person.<p>---<p>Narrative is a powerful tool. It&#x27;s why we&#x27;re so excited about Natural Language Processing in the first place. Ever since the very origins of software, the power of narrative has been <i>so close</i>, but always still just out of grasp. Do we even know what we are reaching for in the first place?<p>In a sense, we have a part of it: explicit definition. What Chomsky categorized &quot;Context-Free Grammar&quot;, we have made into programming languages. What they are missing is <i>implicit inference</i>: context.<p>That&#x27;s what LLMs do. They use <i>inference</i> to model the patterns that exist in written text. With that model, they can hallucinate more text that follows the same patterns: they can <i>perform</i> natural language.<p>So that&#x27;s it, right? Problem solved! What&#x27;s missing? <i>explicit definition</i>. We traded one problem for another. No one (so far) has figured out how to solve both <i>in the same program</i>. You can have definition, <i>or</i>, you can have inference. You can&#x27;t have both.<p>This doesn&#x27;t make any sense to us humans. We don&#x27;t have any trouble at all doing both at the same time. We do it all the time! Do we actually do anything else? Unfortunately, LLMs are not humans.<p>---<p>The two approaches to language are diametrically opposed, but they work with the same domain. Approaching from either end of the spectrum, definition and inference explore together the wild universe that is <i>story</i>. That&#x27;s the missing piece: once we figure out what story is made of, we should be able to put all three pieces together.</div><br/></div></div><div id="36017547" class="c"><input type="checkbox" id="c-36017547" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#36017909">prev</a><span>|</span><a href="#36017817">next</a><span>|</span><label class="collapse" for="c-36017547">[-]</label><label class="expand" for="c-36017547">[1 more]</label></div><br/><div class="children"><div class="content">Jaron Lanier&#x27;s 2010 book &quot;You are not a gadget&quot; 
basically foreshadows the hype around chat-gpt and how we (technologists) want to make people obsolete so that computers seem more advanced.  He argues that we adjust ourselves and reduce our expectations in the Turing test.</div><br/></div></div><div id="36017817" class="c"><input type="checkbox" id="c-36017817" checked=""/><div class="controls bullet"><span class="by">jmyeet</span><span>|</span><a href="#36017547">prev</a><span>|</span><a href="#36018381">next</a><span>|</span><label class="collapse" for="c-36017817">[-]</label><label class="expand" for="c-36017817">[8 more]</label></div><br/><div class="children"><div class="content">GPT-4 is pretty amazing but I, too, feel this is being overhyped. For me, a sobering example is how OpenAI does math (eg [1]). Specifically, the model clearly doesn&#x27;t really understand multiplication and &quot;learns&quot; it from training data. This tends to get the first few and last few digits right for a simple multiplication with 6-7 digit numbers.<p>Now you can solve that with plugins (eg training the model to recognize math problems and have access to a calculator) so it&#x27;s a solvable problem but you realize there&#x27;s an extremely long tail of such problems. It goes to show that GPT-4 isn&#x27;t &quot;magic&quot; and we still have a long way to go.<p>[1]: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;OpenAI&#x2F;comments&#x2F;12donja&#x2F;gpt4_and_math&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;OpenAI&#x2F;comments&#x2F;12donja&#x2F;gpt4_and_ma...</a></div><br/><div id="36017868" class="c"><input type="checkbox" id="c-36017868" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36017817">parent</a><span>|</span><a href="#36018644">next</a><span>|</span><label class="collapse" for="c-36017868">[-]</label><label class="expand" for="c-36017868">[6 more]</label></div><br/><div class="children"><div class="content">Most of the time when people find a maths problem that they can trick the model into getting wrong, it&#x27;s also possible to get the model to give the correct answer with better prompting.<p>A trick that&#x27;s worth knowing is just to ask the model to give each step in the solution and explain as it goes. This gives the model &quot;time to think&quot; and leads to better results.</div><br/><div id="36017992" class="c"><input type="checkbox" id="c-36017992" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#36017817">root</a><span>|</span><a href="#36017868">parent</a><span>|</span><a href="#36018644">next</a><span>|</span><label class="collapse" for="c-36017992">[-]</label><label class="expand" for="c-36017992">[5 more]</label></div><br/><div class="children"><div class="content">Pretty sure you can&#x27;t get GPT-4 to do 8 digit multiplication with any prompt.<p>For what it&#x27;s worth, I&#x27;m not even sure if chain of thought provides much value to GPT-4. The RLHF it went through seems to have encouraged more logical thinking already.</div><br/><div id="36018373" class="c"><input type="checkbox" id="c-36018373" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36017817">root</a><span>|</span><a href="#36017992">parent</a><span>|</span><a href="#36018244">next</a><span>|</span><label class="collapse" for="c-36018373">[-]</label><label class="expand" for="c-36018373">[1 more]</label></div><br/><div class="children"><div class="content">Btw I think you&#x27;re not correct about the 8 digit multiplication.  I tried:<p><pre><code>    please could you multiply 12345678 by 87654321.  Give your answer in steps.  Each step should calculate one column at a time from right to left remembering to carry any digits which overflowed in a previous step. At the end add a final step adding the values up.
</code></pre>
And it gave me the following, which I believe to be correct:<p><pre><code>    Sure, I&#x27;ll be doing a long multiplication here, step by step, column by column from right to left. For brevity, I&#x27;ll write the numbers to be multiplied as:

    A: 12345678
    B: 87654321

    Step 1: Multiply the 1st digit of B by A:

    1 * 12345678 = 12345678

    Step 2: Multiply the 2nd digit of B by A, remember to shift one position to the left:

    2 * 12345678 = 24691356 (shift one position to the left, i.e., append a 0 at the end)

    Step 3: Multiply the 3rd digit of B by A, remember to shift two positions to the left:

</code></pre>
... skip some of the answer because you get the gist....<p><pre><code>    7 * 12345678 = 86419746 (shift six positions to the left, i.e., append six 0s at the end)

    Step 8: Multiply the 8th digit of B by A, remember to shift seven positions to the left:

    8 * 12345678 = 98765424 (shift seven positions to the left, i.e., append seven 0s at the end)

    Now, the last step is to add all these results together:

    12345678

        246913560
        3703703400
        49382712000
        617283900000
        7407406800000
        86419746000000
        987654240000000

    = 1082152022374638

    So, the result of 12345678 multiplied by 87654321 is 1082152022374638.</code></pre></div><br/></div></div><div id="36018244" class="c"><input type="checkbox" id="c-36018244" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36017817">root</a><span>|</span><a href="#36017992">parent</a><span>|</span><a href="#36018373">prev</a><span>|</span><a href="#36018182">next</a><span>|</span><label class="collapse" for="c-36018244">[-]</label><label class="expand" for="c-36018244">[1 more]</label></div><br/><div class="children"><div class="content">The deeplearning.com prompt engineering course contains an example where GPT4 solves a problem with chain of thought and fails without.  The prompt is<p><pre><code>    Determine if the student&#x27;s solution is correct or not.

    Question:
    I&#x27;m building a solar power installation and I need 
     help working out the financials. 
    - Land costs $100 &#x2F; square foot
    - I can buy solar panels for $250 &#x2F; square foot
    - I negotiated a contract for maintenance that will cost  
    me a flat $100k per year, and an additional $10 &#x2F; square 
    foot
    What is the total cost for the first year of operations 
    as a function of the number of square feet.

    Student&#x27;s Solution:
    Let x be the size of the installation in square feet.
    Costs:
    1. Land cost: 100x
    2. Solar panel cost: 250x
    3. Maintenance cost: 100,000 + 100x
    Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
</code></pre>
GPT-4 will say the student is correct even though the answer includes an error (they should be multiplying by 10x not 100x).  If you change the prompt to include chain of thought as follows it notices the error<p><pre><code>    Your task is to determine if the student&#x27;s solution 
    is correct or not.
    To solve the problem do the following:
    - First, work out your own solution to the problem. 
    - Then compare your solution to the student&#x27;s solution \
    and evaluate if the student&#x27;s solution is correct or not. 
    Don&#x27;t decide if the student&#x27;s solution is correct until 
    you have done the problem yourself.

    Use the following format:
    Question:
    ```
    question here
    ```
    Student&#x27;s solution:
    ```
    student&#x27;s solution here
    ```
    Actual solution:
    ```
    steps to work out the solution and your solution here
    ```
    Is the student&#x27;s solution the same as actual solution \
    just calculated:
    ```
    yes or no
    ```
    Student grade:
    ```
    correct or incorrect
    ```

    Question:
    ```
    I&#x27;m building a solar power installation and I need help \
    working out the financials. 
    - Land costs $100 &#x2F; square foot
    - I can buy solar panels for $250 &#x2F; square foot
    - I negotiated a contract for maintenance that will cost \
    me a flat $100k per year, and an additional $10 &#x2F; square \
    foot
    What is the total cost for the first year of operations \
    as a function of the number of square feet.
    ``` 
    Student&#x27;s solution:
    ```
    Let x be the size of the installation in square feet.
    Costs:
    1. Land cost: 100x
    2. Solar panel cost: 250x
    3. Maintenance cost: 100,000 + 100x
    Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
    ```
    Actual solution:

</code></pre>
As I say this same prompt &quot;fools&quot; GPT3.5 and 4.</div><br/></div></div><div id="36018182" class="c"><input type="checkbox" id="c-36018182" checked=""/><div class="controls bullet"><span class="by">morkalork</span><span>|</span><a href="#36017817">root</a><span>|</span><a href="#36017992">parent</a><span>|</span><a href="#36018244">prev</a><span>|</span><a href="#36018644">next</a><span>|</span><label class="collapse" for="c-36018182">[-]</label><label class="expand" for="c-36018182">[2 more]</label></div><br/><div class="children"><div class="content">Now you have me wondering if you could prompt it to solve it step by step, using the same method an elementary student would do on paper.</div><br/><div id="36018215" class="c"><input type="checkbox" id="c-36018215" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#36017817">root</a><span>|</span><a href="#36018182">parent</a><span>|</span><a href="#36018644">next</a><span>|</span><label class="collapse" for="c-36018215">[-]</label><label class="expand" for="c-36018215">[1 more]</label></div><br/><div class="children"><div class="content">You can. One thing that I&#x27;ve found fun is to prompt it for some maths problems without solutions, then I provide solutions and for any that I get wrong ask it to explain my mistakes.</div><br/></div></div></div></div></div></div></div></div><div id="36018644" class="c"><input type="checkbox" id="c-36018644" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#36017817">parent</a><span>|</span><a href="#36017868">prev</a><span>|</span><a href="#36018381">next</a><span>|</span><label class="collapse" for="c-36018644">[-]</label><label class="expand" for="c-36018644">[1 more]</label></div><br/><div class="children"><div class="content">Large language models and the transformer architecture are just the ALGOL 60 [1] of the search for general synthetic competence (intelligence is too finicky, competence is being given a task and fulfilling it successfully, in time, on budget, optimizing along the way). ChatGPT + Wolfram or other plugins are great for making transformers mathy, but the plugins must also be end-to-end machine learning architectures. To continue the high-level programming language analogy, we will probably see the C-level neural architecture in a few years once we get more embedded data (competence obtained through correlation physical object ↔ concept) with feedback loops forcing algorithms to operate under energy-restrictions (unlike the current architectures, more of a proof of concept, being careless about their energy).<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ALGOL_60" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ALGOL_60</a></div><br/></div></div></div></div><div id="36018381" class="c"><input type="checkbox" id="c-36018381" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#36017817">prev</a><span>|</span><label class="collapse" for="c-36018381">[-]</label><label class="expand" for="c-36018381">[1 more]</label></div><br/><div class="children"><div class="content">ieee has been on a copium suspension trip for a while now.</div><br/></div></div></div></div></div></div></div></body></html>