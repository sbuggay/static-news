<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714899712614" as="style"/><link rel="stylesheet" href="styles.css?v=1714899712614"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.00436">Porting HPC Applications to AMD Instinct MI300A Using Unified Memory and OpenMP</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>arcanus</span> | <span>28 comments</span></div><br/><div><div id="40260079" class="c"><input type="checkbox" id="c-40260079" checked=""/><div class="controls bullet"><span class="by">curt15</span><span>|</span><a href="#40262886">next</a><span>|</span><label class="collapse" for="c-40260079">[-]</label><label class="expand" for="c-40260079">[14 more]</label></div><br/><div class="children"><div class="content">I was talking with a friend in HPC lately who said that AMD is actually quite competitive in the HPC space these days. For example, Frontier (<a href="https:&#x2F;&#x2F;docs.olcf.ornl.gov&#x2F;systems&#x2F;frontier_user_guide.html" rel="nofollow">https:&#x2F;&#x2F;docs.olcf.ornl.gov&#x2F;systems&#x2F;frontier_user_guide.html</a>) is an all-AMD installation. Do scientists actually use ROCm in their code or does AMD have another programming framework for their Instinct chips?</div><br/><div id="40260103" class="c"><input type="checkbox" id="c-40260103" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#40260079">parent</a><span>|</span><a href="#40261795">next</a><span>|</span><label class="collapse" for="c-40260103">[-]</label><label class="expand" for="c-40260103">[4 more]</label></div><br/><div class="children"><div class="content">I currently have a project with ORNL OLCF (on Frontier).
 The short answer is yes. Happy to answer any questions I can.</div><br/><div id="40260292" class="c"><input type="checkbox" id="c-40260292" checked=""/><div class="controls bullet"><span class="by">ysleepy</span><span>|</span><a href="#40260079">root</a><span>|</span><a href="#40260103">parent</a><span>|</span><a href="#40261795">next</a><span>|</span><label class="collapse" for="c-40260292">[-]</label><label class="expand" for="c-40260292">[3 more]</label></div><br/><div class="children"><div class="content">ROCm or HIP? Does it start out with porting a lot from CUDA etc. or starting fresh on top of the AMD APIs?<p>How much of the project time is spent on that compute API stuff in comparison to &quot;payload&quot; work?</div><br/><div id="40261160" class="c"><input type="checkbox" id="c-40261160" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#40260079">root</a><span>|</span><a href="#40260292">parent</a><span>|</span><a href="#40262130">next</a><span>|</span><label class="collapse" for="c-40261160">[-]</label><label class="expand" for="c-40261160">[1 more]</label></div><br/><div class="children"><div class="content">My project is ROCm (torch, more or less) and working with OLCF staff I&#x27;ve never heard of HIP in use but based on their training series it is supported[0].<p>Of course my personal experience isn&#x27;t exhaustive and it can be inferred from the ongoing training series that it is in use in some cases.<p>Speaking from personal experience ROCm itself is... Challenging (which I already knew from prior endeavors). We&#x27;ve taken to dev and staging workloads on more typical MI2xx hardware and then working it over to Frontier.<p>We currently have 20k node hours on Frontier via a Director&#x27;s Discretion Project[1]. It&#x27;s a relatively simple application and at the end of the day you have access to significant compute so depending on workload the extra effort for ROCm, etc is still worth it.<p>[0] - <a href="https:&#x2F;&#x2F;www.olcf.ornl.gov&#x2F;hip-training-series&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.olcf.ornl.gov&#x2F;hip-training-series&#x2F;</a><p>[1] - <a href="https:&#x2F;&#x2F;www.olcf.ornl.gov&#x2F;for-users&#x2F;documents-forms&#x2F;olcf-directors-discretion-project-application&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.olcf.ornl.gov&#x2F;for-users&#x2F;documents-forms&#x2F;olcf-dir...</a></div><br/></div></div><div id="40262130" class="c"><input type="checkbox" id="c-40262130" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#40260079">root</a><span>|</span><a href="#40260292">parent</a><span>|</span><a href="#40261160">prev</a><span>|</span><a href="#40261795">next</a><span>|</span><label class="collapse" for="c-40262130">[-]</label><label class="expand" for="c-40262130">[1 more]</label></div><br/><div class="children"><div class="content">&gt;ROCm or HIP?<p>I&#x27;m not sure that&#x27;s the right question to ask. Afaik ROCm is the name of that entire tech stack and HIP is AMD&#x27;s equivalent to CUDA C++ (they basically replicated the API and replaced every &quot;CUDA&quot; by &quot;hip&quot;, they have functions called &quot;hipmalloc&quot; and &quot;hipmemcpy&quot;).<p>The repository is located at <a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;HIP">https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;HIP</a>.</div><br/></div></div></div></div></div></div><div id="40261795" class="c"><input type="checkbox" id="c-40261795" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#40260079">parent</a><span>|</span><a href="#40260103">prev</a><span>|</span><a href="#40262015">next</a><span>|</span><label class="collapse" for="c-40261795">[-]</label><label class="expand" for="c-40261795">[1 more]</label></div><br/><div class="children"><div class="content">An interesting alternative question: &quot;how necessary is ROCm when working with APU?&quot;.<p>CUDA&#x27;s advantage seemed to me to come mostly from memory management and task scheduling being so poor on AMD cards. If AMD has engineered that problem out of the system, we might be able to get away with using 3rd party libraries instead of these vendor-promoted frameworks.</div><br/></div></div><div id="40262015" class="c"><input type="checkbox" id="c-40262015" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#40260079">parent</a><span>|</span><a href="#40261795">prev</a><span>|</span><a href="#40260204">next</a><span>|</span><label class="collapse" for="c-40262015">[-]</label><label class="expand" for="c-40262015">[1 more]</label></div><br/><div class="children"><div class="content">AMD had pretty much always been competitive in HPC, AI not so much because of software.</div><br/></div></div><div id="40260204" class="c"><input type="checkbox" id="c-40260204" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#40260079">parent</a><span>|</span><a href="#40262015">prev</a><span>|</span><a href="#40262886">next</a><span>|</span><label class="collapse" for="c-40260204">[-]</label><label class="expand" for="c-40260204">[7 more]</label></div><br/><div class="children"><div class="content">National labs sign &quot;cost-effective&quot; deals. NVIDIA isn&#x27;t cost-effective. Aurora (at Argonne) is all Intel GPU. Aurora is also a clusterfuck so that just tells you these decisions aren&#x27;t made by the most competent people.</div><br/><div id="40261608" class="c"><input type="checkbox" id="c-40261608" checked=""/><div class="controls bullet"><span class="by">atrettel</span><span>|</span><a href="#40260079">root</a><span>|</span><a href="#40260204">parent</a><span>|</span><a href="#40260527">next</a><span>|</span><label class="collapse" for="c-40261608">[-]</label><label class="expand" for="c-40261608">[1 more]</label></div><br/><div class="children"><div class="content">LANL might disagree given that they just unveiled a new supercomputer with NVIDIA chips [1].  NVIDIA CEO Jensen Huang was even at the unveiling.<p>[1] <a href="https:&#x2F;&#x2F;ladailypost.com&#x2F;los-alamos-national-laboratory-unveils-venado-supercomputer-and-opens-door-for-ai-applications&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ladailypost.com&#x2F;los-alamos-national-laboratory-unvei...</a></div><br/></div></div><div id="40260527" class="c"><input type="checkbox" id="c-40260527" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#40260079">root</a><span>|</span><a href="#40260204">parent</a><span>|</span><a href="#40261608">prev</a><span>|</span><a href="#40260328">next</a><span>|</span><label class="collapse" for="c-40260527">[-]</label><label class="expand" for="c-40260527">[1 more]</label></div><br/><div class="children"><div class="content">Both Frontier and Aurora bet on unproven future chips. Sometimes it pays off and sometimes it doesn&#x27;t.</div><br/></div></div><div id="40260328" class="c"><input type="checkbox" id="c-40260328" checked=""/><div class="controls bullet"><span class="by">jfkfif</span><span>|</span><a href="#40260079">root</a><span>|</span><a href="#40260204">parent</a><span>|</span><a href="#40260527">prev</a><span>|</span><a href="#40261516">next</a><span>|</span><label class="collapse" for="c-40260328">[-]</label><label class="expand" for="c-40260328">[2 more]</label></div><br/><div class="children"><div class="content">nvidia absolutely gives deals to national labs and universities. See Crossroads @ LANL, Isambard in the UK, Perlmutter @ LBL. While AMD is being deployed at LLNL and ORNL, Nvidia isn’t done with their HPC game. Maybe not at the leadership level, but we’ll see how Oak Ridge and LANL decide their next round of procurements</div><br/></div></div><div id="40260637" class="c"><input type="checkbox" id="c-40260637" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#40260079">root</a><span>|</span><a href="#40260204">parent</a><span>|</span><a href="#40261516">prev</a><span>|</span><a href="#40262886">next</a><span>|</span><label class="collapse" for="c-40260637">[-]</label><label class="expand" for="c-40260637">[1 more]</label></div><br/><div class="children"><div class="content">They are competent people, just not in the fields techies want.<p>When you&#x27;re a national laboratory and your wallet is taxes from fellow Americans, it is very important that you find a balance between bang and buck. Lest you get your budget slashed or worse.</div><br/></div></div></div></div></div></div><div id="40262886" class="c"><input type="checkbox" id="c-40262886" checked=""/><div class="controls bullet"><span class="by">Agingcoder</span><span>|</span><a href="#40260079">prev</a><span>|</span><a href="#40259545">next</a><span>|</span><label class="collapse" for="c-40262886">[-]</label><label class="expand" for="c-40262886">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been waiting for something like that in the HPC space for years - that’s what I wanted when HSA first came out.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Heterogeneous_System_Architecture" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Heterogeneous_System_Architect...</a></div><br/></div></div><div id="40259545" class="c"><input type="checkbox" id="c-40259545" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#40262886">prev</a><span>|</span><a href="#40260324">next</a><span>|</span><label class="collapse" for="c-40259545">[-]</label><label class="expand" for="c-40259545">[7 more]</label></div><br/><div class="children"><div class="content">APU’s for HPC are going to be a wild ride. Accelerated computing in shared memory. Get CPU-focused folks will actually get access to some high throughput compute accessible on the sort of timescales that we can actually reason about (the GPU is so far away).</div><br/><div id="40260133" class="c"><input type="checkbox" id="c-40260133" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#40259545">parent</a><span>|</span><a href="#40259913">next</a><span>|</span><label class="collapse" for="c-40260133">[-]</label><label class="expand" for="c-40260133">[2 more]</label></div><br/><div class="children"><div class="content">APUs are very cool for GPU programming in general. Explicitly copying data to&#x2F;from GPUs is a definite nuisance. I&#x27;m hopeful that the MI300A will have a positive knock on effect on the low power APUs in laptops and similar.</div><br/><div id="40260765" class="c"><input type="checkbox" id="c-40260765" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40259545">root</a><span>|</span><a href="#40260133">parent</a><span>|</span><a href="#40259913">next</a><span>|</span><label class="collapse" for="c-40260765">[-]</label><label class="expand" for="c-40260765">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Explicitly copying data to&#x2F;from GPUs is a definite nuisance.<p>CXL allows fine grained shared memory, but people look at the shiny high bandwidth NVLink and talk about how much better it is for... AI.</div><br/></div></div></div></div><div id="40259913" class="c"><input type="checkbox" id="c-40259913" checked=""/><div class="controls bullet"><span class="by">djmips</span><span>|</span><a href="#40259545">parent</a><span>|</span><a href="#40260133">prev</a><span>|</span><a href="#40260324">next</a><span>|</span><label class="collapse" for="c-40259913">[-]</label><label class="expand" for="c-40259913">[4 more]</label></div><br/><div class="children"><div class="content">All video game consoles use APUs and it does make memory related operations potentially faster but at least for video games it&#x27;s not the bottleneck. I suppose for HPC it might have more significance.</div><br/><div id="40260030" class="c"><input type="checkbox" id="c-40260030" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#40259545">root</a><span>|</span><a href="#40259913">parent</a><span>|</span><a href="#40260360">next</a><span>|</span><label class="collapse" for="c-40260030">[-]</label><label class="expand" for="c-40260030">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re doing simulations, or poking big matrices continuously on CPUs, you can saturate the memory controller pretty easily. If you know what you&#x27;re doing, your FPU or vector units are saturated at the same time, so &quot;whole system&quot; becomes the bottleneck while it tries to keep itself cool.<p>Games move that kind of data in the beginning and doesn&#x27;t stream new data that much after the initial texture and model data. If you are working on HPC with GPUs, you may need to constantly stream in new data to the GPU while streaming out the results. This is why datacenter&#x2F;compute GPUs have multiple independent DMA engines.</div><br/></div></div><div id="40260360" class="c"><input type="checkbox" id="c-40260360" checked=""/><div class="controls bullet"><span class="by">crest</span><span>|</span><a href="#40259545">root</a><span>|</span><a href="#40259913">parent</a><span>|</span><a href="#40260030">prev</a><span>|</span><a href="#40260324">next</a><span>|</span><label class="collapse" for="c-40260360">[-]</label><label class="expand" for="c-40260360">[2 more]</label></div><br/><div class="children"><div class="content">Afaik those unified memory architectures are mostly neither cache coherent nor do they support virtual addresses efficiently (you have to trap into privileged code to pin&#x2F;unpin the mappings) which means that the relative cost is lower than a dedicated GPU accessed via PCIe slots, but still to high. Only the &quot;boring&quot; old Bobcat based AMD APUs supported accessing unpinned virtual memory from the L3 (aka system level) cache and nobody bothered with porting code to them.</div><br/><div id="40262322" class="c"><input type="checkbox" id="c-40262322" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#40259545">root</a><span>|</span><a href="#40260360">parent</a><span>|</span><a href="#40260324">next</a><span>|</span><label class="collapse" for="c-40262322">[-]</label><label class="expand" for="c-40262322">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Afaik those unified memory architectures are mostly neither cache coherent nor do they support virtual addresses efficiently (you have to trap into privileged code to pin&#x2F;unpin the mappings) which means that the relative cost is lower than a dedicated GPU accessed via PCIe slots, but still to high. Only the &quot;boring&quot; old Bobcat based AMD APUs supported accessing unpinned virtual memory from the L3 (aka system level) cache and nobody bothered with porting code to them.<p>Other way around, bobcat was the era of “onion bus”&#x2F;“garlic bus” and today things like apple silicon don’t need to be explicitly accessed in certain ways afaik.<p><a href="https:&#x2F;&#x2F;www.realworldtech.com&#x2F;fusion-llano&#x2F;3&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.realworldtech.com&#x2F;fusion-llano&#x2F;3&#x2F;</a><p><a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;16226&#x2F;apple-silicon-m1-a14-deep-dive&#x2F;2" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;16226&#x2F;apple-silicon-m1-a14-de...</a></div><br/></div></div></div></div></div></div></div></div><div id="40260324" class="c"><input type="checkbox" id="c-40260324" checked=""/><div class="controls bullet"><span class="by">mathiasgredal</span><span>|</span><a href="#40259545">prev</a><span>|</span><label class="collapse" for="c-40260324">[-]</label><label class="expand" for="c-40260324">[5 more]</label></div><br/><div class="children"><div class="content">Having looked briefly at the code I still think C++17 parallel algorithms are more ergonomic compared to OpenMP: <a href="https:&#x2F;&#x2F;rocm.blogs.amd.com&#x2F;software-tools-optimization&#x2F;hipstdpar&#x2F;README.html" rel="nofollow">https:&#x2F;&#x2F;rocm.blogs.amd.com&#x2F;software-tools-optimization&#x2F;hipst...</a></div><br/><div id="40260563" class="c"><input type="checkbox" id="c-40260563" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#40260324">parent</a><span>|</span><a href="#40260386">next</a><span>|</span><label class="collapse" for="c-40260563">[-]</label><label class="expand" for="c-40260563">[3 more]</label></div><br/><div class="children"><div class="content">Is language support why people like OpenMP?<p>I think it is nice because it supports both C and Fortran, and they use the same runtime, so you can do things like pin threads to cores or avoid oversubscription. Stuff like calling a Fortran library that uses OpenMP, from a C code that also uses OpenMP, doesn’t require anything clever.</div><br/><div id="40261395" class="c"><input type="checkbox" id="c-40261395" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#40260324">root</a><span>|</span><a href="#40260563">parent</a><span>|</span><a href="#40260779">next</a><span>|</span><label class="collapse" for="c-40261395">[-]</label><label class="expand" for="c-40261395">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Is language support why people like OpenMP?<p>I use it sometimes with C++ because it is super easy to make &quot;embarrassingly parallel&quot; code actually run in parallel. And by using nothing but #pragma statements it will still compile single threaded if you <i>don&#x27;t</i> have OMP as the pragmas will be ignored.</div><br/></div></div><div id="40260779" class="c"><input type="checkbox" id="c-40260779" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#40260324">root</a><span>|</span><a href="#40260563">parent</a><span>|</span><a href="#40261395">prev</a><span>|</span><a href="#40260386">next</a><span>|</span><label class="collapse" for="c-40260779">[-]</label><label class="expand" for="c-40260779">[1 more]</label></div><br/><div class="children"><div class="content">OpenMP has been around for a long time. People know how to use it, and it has gained many features that are useful for scientific computing.<p>The consortium behind OpenMP consists mostly of hardware companies and organizations doing scientific computing. Software companies are largely missing. That may contribute to the popularity of OpenMP, as the interests of scientific computing and software development are often different.</div><br/></div></div></div></div><div id="40260386" class="c"><input type="checkbox" id="c-40260386" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#40260324">parent</a><span>|</span><a href="#40260563">prev</a><span>|</span><label class="collapse" for="c-40260386">[-]</label><label class="expand" for="c-40260386">[1 more]</label></div><br/><div class="children"><div class="content">funny how we only get LoC between the different versions, but not the performance...<p>Of course the parallel algorithms are shorter, it&#x27;s a more high-level interface. But being explicit gives you more control and potentially more performance.</div><br/></div></div></div></div></div></div></div></div></div></body></html>