<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696755677368" as="style"/><link rel="stylesheet" href="styles.css?v=1696755677368"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/index/decomposing-language-models-into-understandable-components">Decomposing language models into understandable components</a> <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>tompark</span> | <span>33 comments</span></div><br/><div><div id="37807310" class="c"><input type="checkbox" id="c-37807310" checked=""/><div class="controls bullet"><span class="by">kalkin</span><span>|</span><a href="#37807615">next</a><span>|</span><label class="collapse" for="c-37807310">[-]</label><label class="expand" for="c-37807310">[2 more]</label></div><br/><div class="children"><div class="content">Just ran across this useful comparison with another very recent paper that effectively corroborates some of the core findings, I believe by an author of the other paper: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;F4iogK5xdNd7jDNyw&#x2F;comparing-anthropic-s-dictionary-learning-to-ours" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;F4iogK5xdNd7jDNyw&#x2F;comparing-...</a></div><br/><div id="37807444" class="c"><input type="checkbox" id="c-37807444" checked=""/><div class="controls bullet"><span class="by">pabo</span><span>|</span><a href="#37807310">parent</a><span>|</span><a href="#37807615">next</a><span>|</span><label class="collapse" for="c-37807444">[-]</label><label class="expand" for="c-37807444">[1 more]</label></div><br/><div class="children"><div class="content">What a great post, thanks for sharing.</div><br/></div></div></div></div><div id="37807615" class="c"><input type="checkbox" id="c-37807615" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#37807310">prev</a><span>|</span><a href="#37807599">next</a><span>|</span><label class="collapse" for="c-37807615">[-]</label><label class="expand" for="c-37807615">[3 more]</label></div><br/><div class="children"><div class="content">Oh dang, I am quite literally working on this as a side project (out of mere curiosity).<p>Well, sort of ..., I&#x27;m refining an algo that takes several (carefully calibrated) outputs from a given LLM and infers the most plausible set of parameters behind it. I was expecting to find clusters of parameters very much alike to what they observe.<p>I informally call this problem <i>inverting</i> an LLM, and obv., it turns out to be non-trivial to solve. Not completely impossible, tho! as so far I&#x27;ve found some good approximations to it.<p>Anyway, quite an interesting read, def. will keep an eye on what they publish in the future.<p>Also, from the linked manuscript at the end,<p>&gt;Another hypothesis is that some features are actually higher-dimensional feature manifolds which dictionary learning is approximating.<p>Well, you have something that behaves like a continuous, smooth space so you could define as many manifolds as you&#x27;d need to suit your needs, so yes :^). But, pedantry off, I get the idea and IMO that&#x27;s definitely what&#x27;s going on and the right framework to approach this problem from.<p>One amazing realization one can get from this is, what is the conceptual equivalent of the transition functions that connect all different manifolds in this LLM space? When you see it your mind will be blown, not because of its complexity, but rather because of its exceptional simplicity.</div><br/><div id="37808589" class="c"><input type="checkbox" id="c-37808589" checked=""/><div class="controls bullet"><span class="by">codethief</span><span>|</span><a href="#37807615">parent</a><span>|</span><a href="#37808191">next</a><span>|</span><label class="collapse" for="c-37808589">[-]</label><label class="expand" for="c-37808589">[1 more]</label></div><br/><div class="children"><div class="content">&gt; One amazing realization one can get from this is, what is the conceptual equivalent of the transition functions that connect all different manifolds in this LLM space?<p>Could you elaborate on what you mean by &quot;transition functions&quot; here?</div><br/></div></div><div id="37808191" class="c"><input type="checkbox" id="c-37808191" checked=""/><div class="controls bullet"><span class="by">herodoturtle</span><span>|</span><a href="#37807615">parent</a><span>|</span><a href="#37808589">prev</a><span>|</span><a href="#37807599">next</a><span>|</span><label class="collapse" for="c-37808191">[-]</label><label class="expand" for="c-37808191">[1 more]</label></div><br/><div class="children"><div class="content">At first I thought this was an ode to dang.</div><br/></div></div></div></div><div id="37807599" class="c"><input type="checkbox" id="c-37807599" checked=""/><div class="controls bullet"><span class="by">zyxin</span><span>|</span><a href="#37807615">prev</a><span>|</span><a href="#37807116">next</a><span>|</span><label class="collapse" for="c-37807599">[-]</label><label class="expand" for="c-37807599">[6 more]</label></div><br/><div class="children"><div class="content">This makes me wonder what would happen if neural networks contain manually programmed components. It seems like trivial components such as detecting DNA sequences could be programmed in by manually setting the weights. The same thing could be done for example to give neural networks a maths component. Would the network when training discover and make use of these predefined components, or would it ignore them and make up its own ways of detecting DNA sequences?</div><br/><div id="37808770" class="c"><input type="checkbox" id="c-37808770" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#37807599">parent</a><span>|</span><a href="#37808419">next</a><span>|</span><label class="collapse" for="c-37808770">[-]</label><label class="expand" for="c-37808770">[1 more]</label></div><br/><div class="children"><div class="content">This is called feature engineering if you want to look up more of a history and use of this idea.<p>Edit - tokenising is a form of this, you&#x27;re pre-transforming the data to save it having to learn patterns you know are important.</div><br/></div></div><div id="37808419" class="c"><input type="checkbox" id="c-37808419" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#37807599">parent</a><span>|</span><a href="#37808770">prev</a><span>|</span><a href="#37808565">next</a><span>|</span><label class="collapse" for="c-37808419">[-]</label><label class="expand" for="c-37808419">[1 more]</label></div><br/><div class="children"><div class="content">In a way, this could be considered adding a speculative transformation of the input as part of the input to some layer, and the network deciding whether or not to use that transformation. It would be akin to a convolution layer in a CNN, albeit far more domain-specific. But I’m not sure how much research has been done on weird layers like this!</div><br/></div></div><div id="37808565" class="c"><input type="checkbox" id="c-37808565" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37807599">parent</a><span>|</span><a href="#37808419">prev</a><span>|</span><a href="#37808027">next</a><span>|</span><label class="collapse" for="c-37808565">[-]</label><label class="expand" for="c-37808565">[1 more]</label></div><br/><div class="children"><div class="content">You can manually program transformers:<p><a href="https:&#x2F;&#x2F;srush.github.io&#x2F;raspy&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;srush.github.io&#x2F;raspy&#x2F;</a><p>I don&#x27;t know if you can integrate them into a model. I think you might run out of space, since these aren&#x27;t polysemantic and so would take up a lot more &quot;room&quot; than learned neurons.</div><br/></div></div><div id="37808027" class="c"><input type="checkbox" id="c-37808027" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#37807599">parent</a><span>|</span><a href="#37808565">prev</a><span>|</span><a href="#37807116">next</a><span>|</span><label class="collapse" for="c-37808027">[-]</label><label class="expand" for="c-37808027">[2 more]</label></div><br/><div class="children"><div class="content">This is indeed interesting. In certain use cases where precision is paramount, we might opt for manually crafted code for the computations. This allows us to be confident in the efficiency of our manual method, rather than relying on LLM for such a specific task. However, it remains unclear whether this would be directly integrated with the network or simply be a tool at LLM&#x27;s disposal. Interestingly, this situation seems to parallel the choice between enhancing the human brain with something like Neuralink and simply equipping with a calculator.</div><br/><div id="37808097" class="c"><input type="checkbox" id="c-37808097" checked=""/><div class="controls bullet"><span class="by">drsopp</span><span>|</span><a href="#37807599">root</a><span>|</span><a href="#37808027">parent</a><span>|</span><a href="#37807116">next</a><span>|</span><label class="collapse" for="c-37808097">[-]</label><label class="expand" for="c-37808097">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what the limitations are. Do LLM&#x27;s have Turing completeness?</div><br/></div></div></div></div></div></div><div id="37807116" class="c"><input type="checkbox" id="c-37807116" checked=""/><div class="controls bullet"><span class="by">DennisP</span><span>|</span><a href="#37807599">prev</a><span>|</span><a href="#37807195">next</a><span>|</span><label class="collapse" for="c-37807116">[-]</label><label class="expand" for="c-37807116">[3 more]</label></div><br/><div class="children"><div class="content">This looks like a big advance in alignment research. A big problem has been that LLMs were just a giant set of inscrutable numbers, and we had no idea what was going on inside.<p>But if this technique scales up, then Anthropic has fixed that. They can figure out what different groups of neurons are actually doing, and use that to control the LLM&#x27;s behavior. That could help with preventing accidentally misaligned AIs.</div><br/><div id="37808583" class="c"><input type="checkbox" id="c-37808583" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#37807116">parent</a><span>|</span><a href="#37807837">next</a><span>|</span><label class="collapse" for="c-37808583">[-]</label><label class="expand" for="c-37808583">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We find that the features that are learned are largely universal between different models, so the lessons learned by studying the features in one model may generalize to others.<p>Hm. I wish they&#x27;d said more about that. Does that mean they found the same feature recognizers when training with the same training set? Or what? This tells us something, but what does it tell us?</div><br/></div></div><div id="37807837" class="c"><input type="checkbox" id="c-37807837" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37807116">parent</a><span>|</span><a href="#37808583">prev</a><span>|</span><a href="#37807195">next</a><span>|</span><label class="collapse" for="c-37807837">[-]</label><label class="expand" for="c-37807837">[1 more]</label></div><br/><div class="children"><div class="content">To me, it sounds more like a good lead for pruning.</div><br/></div></div></div></div><div id="37807195" class="c"><input type="checkbox" id="c-37807195" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37807116">prev</a><span>|</span><a href="#37807077">next</a><span>|</span><label class="collapse" for="c-37807195">[-]</label><label class="expand" for="c-37807195">[3 more]</label></div><br/><div class="children"><div class="content">I am hoping that this type of research leads into ways to create highly tuned and steerable models that are also much smaller and more efficient.<p>Because if you can see what each part is doing, then theoretically you can find ways to create just the set of features you want. Or maybe tune features that have redundant capacity or something.<p>Maybe by studying the features they will get to the point where the knowledge can be distilled into something more like a very rich and finely defined knowledge graph.</div><br/><div id="37807864" class="c"><input type="checkbox" id="c-37807864" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37807195">parent</a><span>|</span><a href="#37807077">next</a><span>|</span><label class="collapse" for="c-37807864">[-]</label><label class="expand" for="c-37807864">[2 more]</label></div><br/><div class="children"><div class="content">Anthropic must be walking on multi-dimensional tightropes. They want AI
safety, and probably want to avoid every Tom, Dick and Harry having a powerful model. But research output picked up by Meta and various discord group could turn the wooly LLMs into powerful contenders and then you have access to the power for all. I don’t have a strong opinion on what is better,
but I lean slightly towards models in the open.<p>After all us plebs are allow to use
computers and latest CPUs and internet and stuff already! Yes there is
shit happening like scams, and worse but it is better than limiting what people can do.</div><br/><div id="37808340" class="c"><input type="checkbox" id="c-37808340" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#37807195">root</a><span>|</span><a href="#37807864">parent</a><span>|</span><a href="#37807077">next</a><span>|</span><label class="collapse" for="c-37808340">[-]</label><label class="expand" for="c-37808340">[1 more]</label></div><br/><div class="children"><div class="content">On the other hand GPS is precedent for intentional nerfing for civilians.</div><br/></div></div></div></div></div></div><div id="37807077" class="c"><input type="checkbox" id="c-37807077" checked=""/><div class="controls bullet"><span class="by">r3trohack3r</span><span>|</span><a href="#37807195">prev</a><span>|</span><a href="#37808492">next</a><span>|</span><label class="collapse" for="c-37807077">[-]</label><label class="expand" for="c-37807077">[3 more]</label></div><br/><div class="children"><div class="content">One large model is not how the brain works. It’s not how org charts work.<p>That LLMs are capable of what they are at the compute density they are strongly signals to me that the task of making a productive knowledge worker is in overhang territory.<p>The missing piece isn’t LLM advancement, it’s LLM management.<p>Building trust in an inwardly-adversarial LLM org chart that reports to you.</div><br/><div id="37807591" class="c"><input type="checkbox" id="c-37807591" checked=""/><div class="controls bullet"><span class="by">PBnFlash</span><span>|</span><a href="#37807077">parent</a><span>|</span><a href="#37808492">next</a><span>|</span><label class="collapse" for="c-37807591">[-]</label><label class="expand" for="c-37807591">[2 more]</label></div><br/><div class="children"><div class="content">The way these systems work feel massively inefficient.<p>We don&#x27;t re-evaluate our astrophysics models when reading a cooking book.</div><br/><div id="37807713" class="c"><input type="checkbox" id="c-37807713" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#37807077">root</a><span>|</span><a href="#37807591">parent</a><span>|</span><a href="#37808492">next</a><span>|</span><label class="collapse" for="c-37807713">[-]</label><label class="expand" for="c-37807713">[1 more]</label></div><br/><div class="children"><div class="content">Neither does GPT-4 or other sparse mixtures of experts, such as e.g. switch transformers [1].<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.03961" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.03961</a></div><br/></div></div></div></div></div></div><div id="37808492" class="c"><input type="checkbox" id="c-37808492" checked=""/><div class="controls bullet"><span class="by">gorgoiler</span><span>|</span><a href="#37807077">prev</a><span>|</span><a href="#37807822">next</a><span>|</span><label class="collapse" for="c-37808492">[-]</label><label class="expand" for="c-37808492">[1 more]</label></div><br/><div class="children"><div class="content">I am a lay person.  To me, I understand a trained model describes transitions from one symbol to the next with probabilities between nodes.  There is a structure to this graph — after all if there weren’t then training would be impossible — but this structure is as if it is all written on one sheet of paper with the definitions of each node all inked on top of each other in differed colors.<p>This research (and it’s parent and sibling papers, from the LW article) seem to be about picking out those colored graph components from the floating point soup?</div><br/></div></div><div id="37807822" class="c"><input type="checkbox" id="c-37807822" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#37808492">prev</a><span>|</span><a href="#37807020">next</a><span>|</span><label class="collapse" for="c-37807822">[-]</label><label class="expand" for="c-37807822">[8 more]</label></div><br/><div class="children"><div class="content">So, I came up with a pretty decent neural net from scratch about 20 years ago - it ran in the browser in Flash. It basically had a 10x10 bitmap input and an output of the same size, and lots of &quot;neurons&quot; in between that strengthened or weakened their connections based on feedback from the end result. And at a certain point they randomly mutated how they processed the input.<p>I don&#x27;t see anything wildly different now, other than scale and youth and the hubris that accompanies those things.</div><br/><div id="37808839" class="c"><input type="checkbox" id="c-37808839" checked=""/><div class="controls bullet"><span class="by">dhoe</span><span>|</span><a href="#37807822">parent</a><span>|</span><a href="#37807850">next</a><span>|</span><label class="collapse" for="c-37808839">[-]</label><label class="expand" for="c-37808839">[1 more]</label></div><br/><div class="children"><div class="content">As a fellow old person, the way I think about it is that every time I have a thought like that it&#x27;s because the neural networks inside my head have stopped being updated and are resulting in wildly outdated pattern matching. &quot;So this car thing is just like a horse but this time with circular legs? Nothing new under the sun, I swear&quot;.</div><br/></div></div><div id="37807850" class="c"><input type="checkbox" id="c-37807850" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37807822">parent</a><span>|</span><a href="#37808839">prev</a><span>|</span><a href="#37807874">next</a><span>|</span><label class="collapse" for="c-37807850">[-]</label><label class="expand" for="c-37807850">[5 more]</label></div><br/><div class="children"><div class="content">Except the emergent properties at scale? At some point you go from making word like sentences, upping the neurons&#x2F;architecture you get real sounding sentences and then upping again with RLHF loops you get impressive emergent intelligence and ability to solve tasks that were not forseen. It is a rare bird that’s not impressed with 2020s AI.</div><br/><div id="37807869" class="c"><input type="checkbox" id="c-37807869" checked=""/><div class="controls bullet"><span class="by">nwienert</span><span>|</span><a href="#37807822">root</a><span>|</span><a href="#37807850">parent</a><span>|</span><a href="#37807874">next</a><span>|</span><label class="collapse" for="c-37807869">[-]</label><label class="expand" for="c-37807869">[4 more]</label></div><br/><div class="children"><div class="content">&gt; emergent intelligence and ability to solve tasks that were not forseen<p>What&#x27;s your best examples of this? Some of the most impressive examples I&#x27;ve seen ended up being likely in the dataset, or very close to being so. I&#x27;ve yet to see something where it definitely wasn&#x27;t approximately in the dataset and was solved in a way that seemed to use some sort of novel process, but open to being wrong.</div><br/><div id="37807881" class="c"><input type="checkbox" id="c-37807881" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#37807822">root</a><span>|</span><a href="#37807869">parent</a><span>|</span><a href="#37807936">next</a><span>|</span><label class="collapse" for="c-37807881">[-]</label><label class="expand" for="c-37807881">[1 more]</label></div><br/><div class="children"><div class="content">A good example is the 100s of conversation histories I have with GPT-4 where it does everything from help me code entirely novel and original ideas, or develop more abstract ideas.<p>Every single day, I get immense use out of modern language models. Even if an output is <i>similar</i> to something it&#x27;s already processed, that&#x27;s fine! Such is the nature of synthesis.</div><br/></div></div><div id="37807936" class="c"><input type="checkbox" id="c-37807936" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37807822">root</a><span>|</span><a href="#37807869">parent</a><span>|</span><a href="#37807881">prev</a><span>|</span><a href="#37807995">next</a><span>|</span><label class="collapse" for="c-37807936">[-]</label><label class="expand" for="c-37807936">[1 more]</label></div><br/><div class="children"><div class="content">Maybe not novel novel but you can get it to write code in an application’s automation language and assist users using that application but with it’s general intelligence too (so it can figure out what the user intends, what to do in the app and generate the code to do that stuff). With a good UI that passes and executes automation code automatically, you now have magic in your app.</div><br/></div></div></div></div></div></div><div id="37807874" class="c"><input type="checkbox" id="c-37807874" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#37807822">parent</a><span>|</span><a href="#37807850">prev</a><span>|</span><a href="#37807020">next</a><span>|</span><label class="collapse" for="c-37807874">[-]</label><label class="expand" for="c-37807874">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re describing genetic programming and a very simple neural net, which is cool. However, the utility of transformer models should not be discounted, and if that interested you 20 years ago, you would be blown away by what&#x27;s possible today.</div><br/></div></div></div></div><div id="37807020" class="c"><input type="checkbox" id="c-37807020" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#37807822">prev</a><span>|</span><a href="#37807828">next</a><span>|</span><label class="collapse" for="c-37807020">[-]</label><label class="expand" for="c-37807020">[1 more]</label></div><br/><div class="children"><div class="content">This is kind of really cool.<p>All these LLMs appear to be converging around these features.</div><br/></div></div><div id="37807828" class="c"><input type="checkbox" id="c-37807828" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#37807020">prev</a><span>|</span><label class="collapse" for="c-37807828">[-]</label><label class="expand" for="c-37807828">[2 more]</label></div><br/><div class="children"><div class="content">All machine learning is just renormalization which in turn is a convolution in Hopf algebra. That&#x27;s why you see superposition<p>&quot;In physics, wherever there is a linear system with a &quot;superposition principle&quot;, a convolution operation makes an appearance.&quot;<p>I&#x27;m working this out in more details but it is uncanny how much it works out.<p>I have a discord if you want to discuss this further<p><a href="https:&#x2F;&#x2F;discord.cofunctional.ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;discord.cofunctional.ai</a></div><br/><div id="37808053" class="c"><input type="checkbox" id="c-37808053" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#37807828">parent</a><span>|</span><label class="collapse" for="c-37808053">[-]</label><label class="expand" for="c-37808053">[1 more]</label></div><br/><div class="children"><div class="content">Do you mean all ML or just large neural networks? Where is renormalization in a tree model? What superposition are you referring to?</div><br/></div></div></div></div></div></div></div></div></div></body></html>