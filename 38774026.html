<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703667654884" as="style"/><link rel="stylesheet" href="styles.css?v=1703667654884"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://vram.asmirnov.xyz/">Show HN: I made a GPU VRAM calculator for transformer-based models</a> <span class="domain">(<a href="https://vram.asmirnov.xyz">vram.asmirnov.xyz</a>)</span></div><div class="subtext"><span>furiousteabag</span> | <span>36 comments</span></div><br/><div><div id="38776493" class="c"><input type="checkbox" id="c-38776493" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38777413">next</a><span>|</span><label class="collapse" for="c-38776493">[-]</label><label class="expand" for="c-38776493">[3 more]</label></div><br/><div class="children"><div class="content">Does this have an option for quantization levels? Don&#x27;t think I saw it.</div><br/><div id="38778020" class="c"><input type="checkbox" id="c-38778020" checked=""/><div class="controls bullet"><span class="by">furiousteabag</span><span>|</span><a href="#38776493">parent</a><span>|</span><a href="#38776743">next</a><span>|</span><label class="collapse" for="c-38778020">[-]</label><label class="expand" for="c-38778020">[1 more]</label></div><br/><div class="children"><div class="content">There is no option to select quantized version yet. Will work on that!</div><br/></div></div><div id="38776743" class="c"><input type="checkbox" id="c-38776743" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#38776493">parent</a><span>|</span><a href="#38778020">prev</a><span>|</span><a href="#38777413">next</a><span>|</span><label class="collapse" for="c-38776743">[-]</label><label class="expand" for="c-38776743">[1 more]</label></div><br/><div class="children"><div class="content">I second the request for quantization, eg for exl2.</div><br/></div></div></div></div><div id="38777413" class="c"><input type="checkbox" id="c-38777413" checked=""/><div class="controls bullet"><span class="by">samspenc</span><span>|</span><a href="#38776493">prev</a><span>|</span><a href="#38776662">next</a><span>|</span><label class="collapse" for="c-38777413">[-]</label><label class="expand" for="c-38777413">[10 more]</label></div><br/><div class="children"><div class="content">Consumer grade GPUs like NVidia&#x27;s 3090 and 4090 max out at 24 GB VRAM, and those cost $1000-2000 each. You can get higher VRAM but need enterprise GPUs which are in the five figures, easily starting at $30K a pop.<p>Per this calculator, for training, only gpt2-large and gpt2-medium would work with those two top-of-the-line GPUs.<p>For inference it&#x27;s certainly a bit better, only the Llama-2-70b-hf and Llama-2-13b-hf don&#x27;t fit in that much VRAM, all the other models do.</div><br/><div id="38777521" class="c"><input type="checkbox" id="c-38777521" checked=""/><div class="controls bullet"><span class="by">alexhutcheson</span><span>|</span><a href="#38777413">parent</a><span>|</span><a href="#38779925">next</a><span>|</span><label class="collapse" for="c-38777521">[-]</label><label class="expand" for="c-38777521">[6 more]</label></div><br/><div class="children"><div class="content">Nvidia’s workstation cards are available with more RAM than the consumer cards, at a lower price than the datacenter cards. RTX 6000 Ada has 48 GB VRAM and retails for $6800, and RTX 5000 Ada has 32 GB VRAM and retails for $4000[1].<p>Very large models have to be distributed across multiple GPUs though, even if you’re using datacenter chips like H100s.<p>[1] <a href="https:&#x2F;&#x2F;store.nvidia.com&#x2F;en-us&#x2F;nvidia-rtx&#x2F;store&#x2F;" rel="nofollow">https:&#x2F;&#x2F;store.nvidia.com&#x2F;en-us&#x2F;nvidia-rtx&#x2F;store&#x2F;</a></div><br/><div id="38777841" class="c"><input type="checkbox" id="c-38777841" checked=""/><div class="controls bullet"><span class="by">slabity</span><span>|</span><a href="#38777413">root</a><span>|</span><a href="#38777521">parent</a><span>|</span><a href="#38779925">next</a><span>|</span><label class="collapse" for="c-38777841">[-]</label><label class="expand" for="c-38777841">[5 more]</label></div><br/><div class="children"><div class="content">Other than power consumption, is there any reason to prefer a single workstation card over multiple consumer cards then?<p>A single $6800 RTX 6000 Ada with 48GB of VRAM vs 6x 7900XTX with a combined total of 144GB of VRAM honestly makes this seem like a no brainer to me.</div><br/><div id="38778138" class="c"><input type="checkbox" id="c-38778138" checked=""/><div class="controls bullet"><span class="by">alexhutcheson</span><span>|</span><a href="#38777413">root</a><span>|</span><a href="#38777841">parent</a><span>|</span><a href="#38778097">next</a><span>|</span><label class="collapse" for="c-38778138">[-]</label><label class="expand" for="c-38778138">[3 more]</label></div><br/><div class="children"><div class="content">You can only fit 1-2 graphics cards in a “normal” ATX case (each card takes 2-3 “slots”). If you want 4 cards on one machine, you need a bigger&#x2F;more expensive motherboard, case, PSU, etc. I haven’t personally seen anyone put 6 cards in a workstation.</div><br/><div id="38779542" class="c"><input type="checkbox" id="c-38779542" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#38777413">root</a><span>|</span><a href="#38778138">parent</a><span>|</span><a href="#38778097">next</a><span>|</span><label class="collapse" for="c-38779542">[-]</label><label class="expand" for="c-38779542">[2 more]</label></div><br/><div class="children"><div class="content">In a water cooled config the cards only take 1 slot. I’ve got 2 3090s and am buying another two shortly. Preemtively upgraded the power to 220v, found a 2kw PSU, and installed a dedicated mini split. I’m also undervolting the cards to keep power and heat down, because even 2000w is not enough to run 4 and a server grade CPU without tripping. When you start accumulating GPUs you also run into all kinds of thermal and power problems for the room, too.</div><br/><div id="38779696" class="c"><input type="checkbox" id="c-38779696" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#38777413">root</a><span>|</span><a href="#38779542">parent</a><span>|</span><a href="#38778097">next</a><span>|</span><label class="collapse" for="c-38779696">[-]</label><label class="expand" for="c-38779696">[1 more]</label></div><br/><div class="children"><div class="content">This is impressive.<p>I was fortunate enough to scoop up a bunch of Gigabyte RTX 3090 Turbos. Cheap used eight slot SuperMicro (or whatever), a cabling kit, four 3090s, boot.<p>Those were the days!</div><br/></div></div></div></div></div></div><div id="38778097" class="c"><input type="checkbox" id="c-38778097" checked=""/><div class="controls bullet"><span class="by">ttt3ts</span><span>|</span><a href="#38777413">root</a><span>|</span><a href="#38777841">parent</a><span>|</span><a href="#38778138">prev</a><span>|</span><a href="#38779925">next</a><span>|</span><label class="collapse" for="c-38778097">[-]</label><label class="expand" for="c-38778097">[1 more]</label></div><br/><div class="children"><div class="content">You have to pass the context between GPUs for large models that don&#x27;t fit in VRAM. Often ends up slower. Also, tooling around AMD GPUs is still poor in comparison.</div><br/></div></div></div></div></div></div><div id="38779925" class="c"><input type="checkbox" id="c-38779925" checked=""/><div class="controls bullet"><span class="by">mciancia</span><span>|</span><a href="#38777413">parent</a><span>|</span><a href="#38777521">prev</a><span>|</span><a href="#38780097">next</a><span>|</span><label class="collapse" for="c-38779925">[-]</label><label class="expand" for="c-38779925">[2 more]</label></div><br/><div class="children"><div class="content">Used 3090 are going for ~600usd these days (at least in Europe) thanks to crypto mining crash - building a workstation with 2 of these is fairly easy for 48GB of vram, with 4 a bit more tricky but still doable and affordable IMO</div><br/><div id="38780012" class="c"><input type="checkbox" id="c-38780012" checked=""/><div class="controls bullet"><span class="by">ngoro7bd</span><span>|</span><a href="#38777413">root</a><span>|</span><a href="#38779925">parent</a><span>|</span><a href="#38780097">next</a><span>|</span><label class="collapse" for="c-38780012">[-]</label><label class="expand" for="c-38780012">[1 more]</label></div><br/><div class="children"><div class="content">Recently bought 24GB 3090 for $700 in US. Used but never tweaked, runs stable for 6 months despite heavy workloads.<p>nVidias play seems obvious. Game graphics don’t move <i>that</i> fast these days. Used market flush with 3090s and down is fine to them while they focus on extracting top dollar from fast moving AI researchers&#x2F;VCs</div><br/></div></div></div></div><div id="38780097" class="c"><input type="checkbox" id="c-38780097" checked=""/><div class="controls bullet"><span class="by">nox100</span><span>|</span><a href="#38777413">parent</a><span>|</span><a href="#38779925">prev</a><span>|</span><a href="#38776662">next</a><span>|</span><label class="collapse" for="c-38780097">[-]</label><label class="expand" for="c-38780097">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t a clue how they compare but a Studio Mac with an M2 Ultra can get 192GB of unified ram for $5700  (PS: not a mac fan, a curious)</div><br/></div></div></div></div><div id="38776662" class="c"><input type="checkbox" id="c-38776662" checked=""/><div class="controls bullet"><span class="by">roseway4</span><span>|</span><a href="#38777413">prev</a><span>|</span><a href="#38775709">next</a><span>|</span><label class="collapse" for="c-38776662">[-]</label><label class="expand" for="c-38776662">[5 more]</label></div><br/><div class="children"><div class="content">While not as pretty (and mobile-friendly) as the original link, the calculators below support modeling LoRA-based training, alongside full finetuning.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;Vokturz&#x2F;can-it-run-llm" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;Vokturz&#x2F;can-it-run-llm</a><p><a href="https:&#x2F;&#x2F;rahulschand.github.io&#x2F;gpu_poor&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rahulschand.github.io&#x2F;gpu_poor&#x2F;</a></div><br/><div id="38778195" class="c"><input type="checkbox" id="c-38778195" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#38776662">parent</a><span>|</span><a href="#38776802">next</a><span>|</span><label class="collapse" for="c-38778195">[-]</label><label class="expand" for="c-38778195">[1 more]</label></div><br/><div class="children"><div class="content">Been looking for something like thos for a while! I googled a lot, and this link never popped up. I feel google search is regressing.</div><br/></div></div><div id="38776802" class="c"><input type="checkbox" id="c-38776802" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#38776662">parent</a><span>|</span><a href="#38778195">prev</a><span>|</span><a href="#38777008">next</a><span>|</span><label class="collapse" for="c-38776802">[-]</label><label class="expand" for="c-38776802">[1 more]</label></div><br/><div class="children"><div class="content">They seem to be broken when I try any HF ids besides what came preconfigured. e.g. just tried brucethemoose&#x2F;Yi-34B-200K-DARE-merge-v5-3.1bpw-exl2-fiction or LoneStriker&#x2F;shisa-7b-v1-3.0bpw-h6-exl2</div><br/></div></div><div id="38777008" class="c"><input type="checkbox" id="c-38777008" checked=""/><div class="controls bullet"><span class="by">icelancer</span><span>|</span><a href="#38776662">parent</a><span>|</span><a href="#38776802">prev</a><span>|</span><a href="#38779612">next</a><span>|</span><label class="collapse" for="c-38777008">[-]</label><label class="expand" for="c-38777008">[1 more]</label></div><br/><div class="children"><div class="content">Second link hasn&#x27;t been working for awhile.</div><br/></div></div></div></div><div id="38775709" class="c"><input type="checkbox" id="c-38775709" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#38776662">prev</a><span>|</span><a href="#38775946">next</a><span>|</span><label class="collapse" for="c-38775709">[-]</label><label class="expand" for="c-38775709">[1 more]</label></div><br/><div class="children"><div class="content">Very nice, would be cool to have a little i next to each spot to explain what each thing is for newer users (batch size, etc)</div><br/></div></div><div id="38775946" class="c"><input type="checkbox" id="c-38775946" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#38775709">prev</a><span>|</span><a href="#38777950">next</a><span>|</span><label class="collapse" for="c-38775946">[-]</label><label class="expand" for="c-38775946">[2 more]</label></div><br/><div class="children"><div class="content">I noticed the default parameter count value is 1.418 billion but if you erase it you can&#x27;t actually enter it back because you can&#x27;t type a decimal point in the input area. Also, you can&#x27;t enter parameter counts smaller than 1 billion</div><br/><div id="38776566" class="c"><input type="checkbox" id="c-38776566" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#38775946">parent</a><span>|</span><a href="#38777950">next</a><span>|</span><label class="collapse" for="c-38776566">[-]</label><label class="expand" for="c-38776566">[1 more]</label></div><br/><div class="children"><div class="content">It works if you type the digits first and then insert the decimal point after.</div><br/></div></div></div></div><div id="38777950" class="c"><input type="checkbox" id="c-38777950" checked=""/><div class="controls bullet"><span class="by">thatguysaguy</span><span>|</span><a href="#38775946">prev</a><span>|</span><a href="#38777558">next</a><span>|</span><label class="collapse" for="c-38777950">[-]</label><label class="expand" for="c-38777950">[2 more]</label></div><br/><div class="children"><div class="content">This only lists first moments, but Adam stores estimates of first and second moments.</div><br/><div id="38777978" class="c"><input type="checkbox" id="c-38777978" checked=""/><div class="controls bullet"><span class="by">furiousteabag</span><span>|</span><a href="#38777950">parent</a><span>|</span><a href="#38777558">next</a><span>|</span><label class="collapse" for="c-38777978">[-]</label><label class="expand" for="c-38777978">[1 more]</label></div><br/><div class="children"><div class="content">By default, SGD w momentum is enabled as optimizer. You may try selecting Adam and it will list second moments as well.</div><br/></div></div></div></div><div id="38777558" class="c"><input type="checkbox" id="c-38777558" checked=""/><div class="controls bullet"><span class="by">_giorgio_</span><span>|</span><a href="#38777950">prev</a><span>|</span><a href="#38778424">next</a><span>|</span><label class="collapse" for="c-38777558">[-]</label><label class="expand" for="c-38777558">[1 more]</label></div><br/><div class="children"><div class="content">What is the usual way to do it inside the python file that defined the model?</div><br/></div></div><div id="38778424" class="c"><input type="checkbox" id="c-38778424" checked=""/><div class="controls bullet"><span class="by">twayt</span><span>|</span><a href="#38777558">prev</a><span>|</span><a href="#38776912">next</a><span>|</span><label class="collapse" for="c-38778424">[-]</label><label class="expand" for="c-38778424">[1 more]</label></div><br/><div class="children"><div class="content">This is actually pretty useful</div><br/></div></div><div id="38776912" class="c"><input type="checkbox" id="c-38776912" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#38778424">prev</a><span>|</span><label class="collapse" for="c-38776912">[-]</label><label class="expand" for="c-38776912">[10 more]</label></div><br/><div class="children"><div class="content">Are people still rawdoggin&#x27; 16-bit models? I almost exclusively use 5-bit inference quants (or 8-bit natives like Yi-34b) on my MacBook Pro. Tiny accuracy loss, runs fast, and leave plenty of (V)RAM on the table. Mixtral 8x7 is my new daily driver, and only takes like 40GB to run! I wonder if I could run two of them talking to each other...</div><br/><div id="38778679" class="c"><input type="checkbox" id="c-38778679" checked=""/><div class="controls bullet"><span class="by">chrsig</span><span>|</span><a href="#38776912">parent</a><span>|</span><a href="#38777605">next</a><span>|</span><label class="collapse" for="c-38778679">[-]</label><label class="expand" for="c-38778679">[1 more]</label></div><br/><div class="children"><div class="content">How does one rawdog a 16-bit model?</div><br/></div></div><div id="38777605" class="c"><input type="checkbox" id="c-38777605" checked=""/><div class="controls bullet"><span class="by">rubatuga</span><span>|</span><a href="#38776912">parent</a><span>|</span><a href="#38778679">prev</a><span>|</span><a href="#38779608">next</a><span>|</span><label class="collapse" for="c-38777605">[-]</label><label class="expand" for="c-38777605">[7 more]</label></div><br/><div class="children"><div class="content">Pure 16bit is horrible for training, sorry.</div><br/><div id="38777753" class="c"><input type="checkbox" id="c-38777753" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#38776912">root</a><span>|</span><a href="#38777605">parent</a><span>|</span><a href="#38778281">next</a><span>|</span><label class="collapse" for="c-38777753">[-]</label><label class="expand" for="c-38777753">[5 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t using bf16 alleviate the problem? At least I&#x27;ve had success training a Bert like model from scratch</div><br/><div id="38778367" class="c"><input type="checkbox" id="c-38778367" checked=""/><div class="controls bullet"><span class="by">furiousteabag</span><span>|</span><a href="#38776912">root</a><span>|</span><a href="#38777753">parent</a><span>|</span><a href="#38778038">next</a><span>|</span><label class="collapse" for="c-38778367">[-]</label><label class="expand" for="c-38778367">[1 more]</label></div><br/><div class="children"><div class="content">Mixed precision is a default method to pretrain and full fine tune right now. It is especially good in transformers, because they have memory bottleneck in activations (outputs of intermediate layers stored for backprop), and running forward pass in fp16&#x2F;bf16 reduces VRAM by almost half (speeds up forward pass as well).</div><br/></div></div><div id="38778038" class="c"><input type="checkbox" id="c-38778038" checked=""/><div class="controls bullet"><span class="by">shikon7</span><span>|</span><a href="#38776912">root</a><span>|</span><a href="#38777753">parent</a><span>|</span><a href="#38778367">prev</a><span>|</span><a href="#38778281">next</a><span>|</span><label class="collapse" for="c-38778038">[-]</label><label class="expand" for="c-38778038">[3 more]</label></div><br/><div class="children"><div class="content">I wonder about that too. With the small precision, parameter updates might be too small to have an effect (is it possible to use some sort of probabilistic update in that case?) Unfortunately, I haven’t found any resources describing the feasibility of full fp16 or bf16 training.</div><br/><div id="38778454" class="c"><input type="checkbox" id="c-38778454" checked=""/><div class="controls bullet"><span class="by">furiousteabag</span><span>|</span><a href="#38776912">root</a><span>|</span><a href="#38778038">parent</a><span>|</span><a href="#38778243">next</a><span>|</span><label class="collapse" for="c-38778454">[-]</label><label class="expand" for="c-38778454">[1 more]</label></div><br/><div class="children"><div class="content">You are correct, training sorely in fp16&#x2F;bf16 can lead to imprecise weight updates or even gradients turning to zero. Because of that, mixed precision is used. In mixed precision training, we keep a copy of the weights in fp32 (master model) and the training loop looks like this:
 compute the output with the fp16 model, then the loss
-&gt; back-propagate the gradients in half-precision
-&gt; copy the gradients in fp32 precision
-&gt; do the update on the master model (in fp32 precision)
-&gt; copy the master model in the fp16 model.
We also do loss scaling which means multiplying the output of the loss function by some scalar number before backprop (necessary in fp16 but not required in bf16).<p>Check out the fastai docs for more details: <a href="https:&#x2F;&#x2F;docs.fast.ai&#x2F;callback.fp16.html" rel="nofollow">https:&#x2F;&#x2F;docs.fast.ai&#x2F;callback.fp16.html</a></div><br/></div></div><div id="38778243" class="c"><input type="checkbox" id="c-38778243" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#38776912">root</a><span>|</span><a href="#38778038">parent</a><span>|</span><a href="#38778454">prev</a><span>|</span><a href="#38778281">next</a><span>|</span><label class="collapse" for="c-38778243">[-]</label><label class="expand" for="c-38778243">[1 more]</label></div><br/><div class="children"><div class="content">Ah my bad. I am using mixed precision training in the my previous comment.<p>You might find this paper interesting:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2010.06192.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2010.06192.pdf</a></div><br/></div></div></div></div></div></div><div id="38778281" class="c"><input type="checkbox" id="c-38778281" checked=""/><div class="controls bullet"><span class="by">bigdict</span><span>|</span><a href="#38776912">root</a><span>|</span><a href="#38777605">parent</a><span>|</span><a href="#38777753">prev</a><span>|</span><a href="#38779608">next</a><span>|</span><label class="collapse" for="c-38778281">[-]</label><label class="expand" for="c-38778281">[1 more]</label></div><br/><div class="children"><div class="content">Hmm, what do you mean? I thought bf16 is used extensively for LLM training.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>