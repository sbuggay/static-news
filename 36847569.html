<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690275648550" as="style"/><link rel="stylesheet" href="styles.css?v=1690275648550"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Launch HN: Artie (YC S23) – Real time data replication to data warehouses</a> </div><div class="subtext"><span>tang8330</span> | <span>45 comments</span></div><br/><div><div id="36859362" class="c"><input type="checkbox" id="c-36859362" checked=""/><div class="controls bullet"><span class="by">scapecast</span><span>|</span><a href="#36852454">next</a><span>|</span><label class="collapse" for="c-36859362">[-]</label><label class="expand" for="c-36859362">[1 more]</label></div><br/><div class="children"><div class="content">In 2016, I wrote a pretty detailed answer for &quot;Spark vs. Redshift&quot; question. This was in the very early days of what today I guess is called &quot;the modern data stack&quot;<p>The core of the answer was that cloud warehouses are not suitable for real-time use cases, because the batch processing and transformations take too long. If you want real-time, you need to pay up - hence Databricks &#x2F; Spark. I did call out the fraud use case in that answer.<p>There were 1st generation ETL tools like Alooma that tried to go into the direction of streaming, and they pushed the limits.<p>Back then (we had built cloud warehouse monitoring tool), the closest to real-time I&#x27;ve ever seen any company get was IronSource. The time between an event and until that event was available in a dashboard was five minutes (they were using Redshift).<p>I&#x27;ll stick my neck out and say that certain industries will be all over Artie, whereas others will shrug their shoulders.<p>The industries that I think will be all over Artie:<p>- FinTech
- Insurance
- AdTech
- Gaming
- Publishing
- Logistics &#x2F; Delivery<p>These are industries where a couple of minutes of difference in data recency can make a difference of millions of dollars. And you&#x27;ll probably cost less than existing streaming solutions, which is obviously nice. But I think the real advantage will be simplicity.<p>I know you can&#x27;t support all destinations at once, and need to go with where demand is. But I would expect that the Materialize and Clickhouse crowds are good target users for you.<p>Good luck, this is an exciting product!</div><br/></div></div><div id="36852454" class="c"><input type="checkbox" id="c-36852454" checked=""/><div class="controls bullet"><span class="by">slotrans</span><span>|</span><a href="#36859362">prev</a><span>|</span><a href="#36859492">next</a><span>|</span><label class="collapse" for="c-36852454">[-]</label><label class="expand" for="c-36852454">[9 more]</label></div><br/><div class="children"><div class="content">(None of the below is meant to diminish the work done by the author&#x2F;poster.)<p>&gt; This means that when companies aggregate production data into their data warehouse, the underlying data is always stale.<p>This is intentional and desirable.<p>The classic piece on this is this one by Dan McKinley <a href="https:&#x2F;&#x2F;mcfunley.com&#x2F;whom-the-gods-would-destroy-they-first-give-real-time-analytics" rel="nofollow noreferrer">https:&#x2F;&#x2F;mcfunley.com&#x2F;whom-the-gods-would-destroy-they-first-...</a><p>Something McKinley doesn&#x27;t address is that it&#x27;s quite advantageous if the values in your data warehouse <i>don&#x27;t change intra-day</i> because this lets business users reach consensus. Whereas if Bob runs a report and gets $X, and Alice runs the same report 5 minutes later and gets $Y, that creates confusion (much more than you would expect). I recall a particular system I built that refreshed every 6 hours (limited by upstream), that eventually Marketing asked me to dial back to every 24 hours because they couldn&#x27;t stand things changing in the middle of the day.<p>Now of course I see you&#x27;re targeting more real-time use cases like fraud detection. That&#x27;s great! But why you would run a fraud detection process out of your data warehouse, which likely doesn&#x27;t even have a production-grade uptime SLA? Run it out of your production database, that&#x27;s what it&#x27;s for!</div><br/><div id="36856169" class="c"><input type="checkbox" id="c-36856169" checked=""/><div class="controls bullet"><span class="by">DenisM</span><span>|</span><a href="#36852454">parent</a><span>|</span><a href="#36852598">next</a><span>|</span><label class="collapse" for="c-36856169">[-]</label><label class="expand" for="c-36856169">[1 more]</label></div><br/><div class="children"><div class="content">Snowflake and Big Query support as-of queries. Redshift supports snapshots. One could easily get a report that&#x27;s consistent over the course of time.</div><br/></div></div><div id="36852598" class="c"><input type="checkbox" id="c-36852598" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36852454">parent</a><span>|</span><a href="#36856169">prev</a><span>|</span><a href="#36855528">next</a><span>|</span><label class="collapse" for="c-36852598">[-]</label><label class="expand" for="c-36852598">[6 more]</label></div><br/><div class="children"><div class="content">Thanks for your feedback!<p>&gt; Something McKinley doesn&#x27;t address is that it&#x27;s quite advantageous if the values in your data warehouse don&#x27;t change intra-day because this lets business users reach consensus. Whereas if Bob runs a report and gets $X, and Alice runs the same report 5 minutes later and gets $Y, that creates confusion (much more than you would expect). I recall a particular system I built that refreshed every 6 hours (limited by upstream), that eventually Marketing asked me to dial back to every 24 hours because they couldn&#x27;t stand things changing in the middle of the day.<p>If they want to see a consistent view of the report, you could bound this.<p>1&#x2F; SELECT * FROM FOO WHERE DATE_TRUNC(&#x27;day&#x27;, updated_at) &lt; DATE_TRUNC(&#x27;day&#x27;, DATEADD(day, -1, CURRENT_DATE()));<p>If your dataset doesn&#x27;t contain kv, you can turn on include `artie_updated_at` which will provide an additional column with the updated_at field to support incremental ingestion.<p>2&#x2F; If you had stateful data, you could also explore creating a Snowflake task and leveraging the time travel f(x) to create a &quot;snapshot&quot; if your workload depended on it.<p>3&#x2F; Also, if you _did_ want this to be more lagged, you can actually increase the flushIntervalSeconds [1] to 6h, 24h, whichever time interval you fancy. You as the customer should have maximum flexibility when it comes to when to flush to DWH.<p>4&#x2F; You can also choose to refresh the analytical report on Looker &#x2F; Mode to be daily. [2]<p>&gt; Now of course I see you&#x27;re targeting more real-time use cases like fraud detection. That&#x27;s great! But why you would run a fraud detection process out of your data warehouse, which likely doesn&#x27;t even have a production-grade uptime SLA? Run it out of your production database, that&#x27;s what it&#x27;s for!<p>You can certainly do this in production db (that was our original hypothesis as well!), however, after talking to more companies...it has become more obvious to us that folks that are running fraud algos actually want to join this across various data sets. Further, by using a DWH - it provides a nice visualization layer on top.<p>Of course, you could go with something even more bespoke by utilizing real-time DBs such as Materialize &#x2F; Rockset &#x2F; RisingWave. Just comes with trade offs such as increase in architectural complexity.<p>There are also plenty of additional use cases this can unlock given that DWH is a platform, any post-DWH application can benefit from less lag, such as reverse ETLs.<p>[1] <a href="https:&#x2F;&#x2F;docs.artie.so&#x2F;running-transfer&#x2F;options">https:&#x2F;&#x2F;docs.artie.so&#x2F;running-transfer&#x2F;options</a><p>[2] <a href="https:&#x2F;&#x2F;mode.com&#x2F;help&#x2F;articles&#x2F;report-scheduling-and-sharing&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;mode.com&#x2F;help&#x2F;articles&#x2F;report-scheduling-and-sharing...</a></div><br/><div id="36854632" class="c"><input type="checkbox" id="c-36854632" checked=""/><div class="controls bullet"><span class="by">debarshri</span><span>|</span><a href="#36852454">root</a><span>|</span><a href="#36852598">parent</a><span>|</span><a href="#36852881">next</a><span>|</span><label class="collapse" for="c-36854632">[-]</label><label class="expand" for="c-36854632">[1 more]</label></div><br/><div class="children"><div class="content">I agree with the parent point. I also don&#x27;t think DWH is the primary usecase for your platform.<p>I have seen architectures where databases are siloed within departments and data has to be replicated across department physical databases in the same network or different, mostly in banks, insurances and old school industries. In this scenario, a daily batch would run that would replicate and populate the tables and kick start business processes. A platform like this would make sense. Another usecase, i can think of is reverse ETL, but there are many tools custom made for that.<p>As for fraud analysis, there are many vendor tools that does exactly that, asking people to visualize and implement a full blown usecase is hard.<p>I might be naive I don&#x27;t see the USP between artie and Airbyte, hevodata, fivetran, stitch etc. and others from a distance.</div><br/></div></div><div id="36852881" class="c"><input type="checkbox" id="c-36852881" checked=""/><div class="controls bullet"><span class="by">mbesto</span><span>|</span><a href="#36852454">root</a><span>|</span><a href="#36852598">parent</a><span>|</span><a href="#36854632">prev</a><span>|</span><a href="#36855528">next</a><span>|</span><label class="collapse" for="c-36852881">[-]</label><label class="expand" for="c-36852881">[4 more]</label></div><br/><div class="children"><div class="content">I think you missed the parent&#x27;s point - your USP is real-time replication. So everything you&#x27;re proposing makes it <i>not</i> real time. Your USP is now worthless (in that context) and you&#x27;re competitors are numerous.</div><br/><div id="36852949" class="c"><input type="checkbox" id="c-36852949" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36852454">root</a><span>|</span><a href="#36852881">parent</a><span>|</span><a href="#36855528">next</a><span>|</span><label class="collapse" for="c-36852949">[-]</label><label class="expand" for="c-36852949">[3 more]</label></div><br/><div class="children"><div class="content">Hm, perhaps I wasn&#x27;t being clear, apologies for that.<p>What I am proposing above is ways to provide a view to teams that do not want real-time data while keeping your underlying dataset in real-time.</div><br/><div id="36856834" class="c"><input type="checkbox" id="c-36856834" checked=""/><div class="controls bullet"><span class="by">mbesto</span><span>|</span><a href="#36852454">root</a><span>|</span><a href="#36852949">parent</a><span>|</span><a href="#36855528">next</a><span>|</span><label class="collapse" for="c-36856834">[-]</label><label class="expand" for="c-36856834">[2 more]</label></div><br/><div class="children"><div class="content">Huh? The parent&#x27;s point was your underlying dataset is <i>always</i> in real-time. There&#x27;s no issue querying a data warehouse when all you&#x27;re doing is looking for a simple transactional report.</div><br/><div id="36859432" class="c"><input type="checkbox" id="c-36859432" checked=""/><div class="controls bullet"><span class="by">shrimpx</span><span>|</span><a href="#36852454">root</a><span>|</span><a href="#36856834">parent</a><span>|</span><a href="#36855528">next</a><span>|</span><label class="collapse" for="c-36859432">[-]</label><label class="expand" for="c-36859432">[1 more]</label></div><br/><div class="children"><div class="content">I think their point is they have a real-time warehouse that can also be used in “stale snapshot” mode.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36859492" class="c"><input type="checkbox" id="c-36859492" checked=""/><div class="controls bullet"><span class="by">justinsoong</span><span>|</span><a href="#36852454">prev</a><span>|</span><a href="#36855219">next</a><span>|</span><label class="collapse" for="c-36859492">[-]</label><label class="expand" for="c-36859492">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve previously used Stitch data <a href="https:&#x2F;&#x2F;www.stitchdata.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.stitchdata.com&#x2F;</a> and found it super easy to stream cdc to redshift and snowflake<p>Will give artie a go</div><br/></div></div><div id="36855219" class="c"><input type="checkbox" id="c-36855219" checked=""/><div class="controls bullet"><span class="by">cgio</span><span>|</span><a href="#36859492">prev</a><span>|</span><a href="#36848350">next</a><span>|</span><label class="collapse" for="c-36855219">[-]</label><label class="expand" for="c-36855219">[5 more]</label></div><br/><div class="children"><div class="content">I’ve been in this space and can appreciate your design decisions. They are meaningful, but a couple of comments on the differentiating factors. DML is tackled in traditional pipelines very similarly, I.e. execution of the log at the target, whether that is materialised or as a view. DDL, it is a nice to have from a technical achievement perspective, but changes to a data schema are not live. They have to be deployed, approved etc. so the flexibility of accommodating them on the pipeline is a benefit from an ease of deployment perspective but could also be alarming for data officers who may feel like they are losing a control lever by not getting the pipeline to block when a schema changes outside normal deployment processes. Finally, the biggest issue with CDC always ends up being the seed loads, recoveries and the incremental snapshot strategies.</div><br/><div id="36855338" class="c"><input type="checkbox" id="c-36855338" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36855219">parent</a><span>|</span><a href="#36855463">next</a><span>|</span><label class="collapse" for="c-36855338">[-]</label><label class="expand" for="c-36855338">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the comment!<p>Your comment regarding DDL is interesting.<p>Today, this is what happens:<p>1&#x2F; Column doesn&#x27;t exist in the destination, we&#x27;ll create it based on our typing inference from the data type (important: not the data value).<p>2&#x2F; Certain tools will handle automatic column data type conversion if a change like this was detected at the source. We do not do this. We will simply hard fail and cause head-of-line blocking reasons being: this is anti-pattern and should be rare, in which case - it&#x27;s okay to cause an err and require manual intervention for this breaking change.<p>3&#x2F; If the column has been dropped from the source, you as the end user can decide whether this column should be also dropped in the destination, or not. The default is not to drop it.<p>^ We hear more customers explicitly don&#x27;t want columns to be dropped because it could cause downstream errors, such as other views &#x2F; tables not compiling due to referencing a non-existent column.<p>We haven&#x27;t heard much from folks that don&#x27;t even want columns to be added. If there is a need, we can definitely add that as a config option to provide maximum configurability.<p>&gt; Finally, the biggest issue with CDC always ends up being the seed loads, recoveries and the incremental snapshot strategies.<p>Yep totally. On the recovery bit, this is exactly why we are leveraging Kafka. If there are any particular issues, we simply don&#x27;t commit the offset and cause head-of-line blocking.<p>On the incremental snapshot and recoveries bit, we primarily leverage Debezium&#x27;s DDD-3 high watermark strategy [1] for MySQL and MongoDB. Postgres has a different issue in that replication slots can grow really fast, esp on AWS! [2]. We ended up writing our own custom snapshotter for Postgres that is Debezium compatible to onboard customers that have a massive dataset and cannot afford to have a read lock on their WAL.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;debezium&#x2F;debezium-design-documents&#x2F;blob&#x2F;main&#x2F;DDD-3.md">https:&#x2F;&#x2F;github.com&#x2F;debezium&#x2F;debezium-design-documents&#x2F;blob&#x2F;m...</a>
[2] <a href="https:&#x2F;&#x2F;www.morling.dev&#x2F;blog&#x2F;insatiable-postgres-replication-slot&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.morling.dev&#x2F;blog&#x2F;insatiable-postgres-replication...</a></div><br/><div id="36856070" class="c"><input type="checkbox" id="c-36856070" checked=""/><div class="controls bullet"><span class="by">cgio</span><span>|</span><a href="#36855219">root</a><span>|</span><a href="#36855338">parent</a><span>|</span><a href="#36855463">next</a><span>|</span><label class="collapse" for="c-36856070">[-]</label><label class="expand" for="c-36856070">[1 more]</label></div><br/><div class="children"><div class="content">The custom snapshotter sounds interesting, potentially a good selling point. On recovery end, in my designs I have also found it useful to have synthetic events so in breakage of CDC I can stitch logs together and not just start from scratch and lose history. I can see you are in the depths of it, more than I’ve been for a while. Wish you the best.</div><br/></div></div></div></div><div id="36855463" class="c"><input type="checkbox" id="c-36855463" checked=""/><div class="controls bullet"><span class="by">LeonB</span><span>|</span><a href="#36855219">parent</a><span>|</span><a href="#36855338">prev</a><span>|</span><a href="#36848350">next</a><span>|</span><label class="collapse" for="c-36855463">[-]</label><label class="expand" for="c-36855463">[2 more]</label></div><br/><div class="children"><div class="content">I found in integration&#x2F;warehousing that if a source system “suddenly” has a new column  — it’s best if we can automatically just bring it in. We would tend to do this in a way that it can’t break the warehouse, doesn’t affect people downstream of us. We can then choose to make it available to others, or not — but the moment that data is available we start hoovering it in.</div><br/><div id="36856127" class="c"><input type="checkbox" id="c-36856127" checked=""/><div class="controls bullet"><span class="by">cgio</span><span>|</span><a href="#36855219">root</a><span>|</span><a href="#36855463">parent</a><span>|</span><a href="#36848350">next</a><span>|</span><label class="collapse" for="c-36856127">[-]</label><label class="expand" for="c-36856127">[1 more]</label></div><br/><div class="children"><div class="content">That’s the logical thing in the context of this domain. In a broader data domain there are other considerations, not always logical, so if that ends up configurable it’s a feature that broadens application scope.</div><br/></div></div></div></div></div></div><div id="36848350" class="c"><input type="checkbox" id="c-36848350" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#36855219">prev</a><span>|</span><a href="#36848273">next</a><span>|</span><label class="collapse" for="c-36848350">[-]</label><label class="expand" for="c-36848350">[3 more]</label></div><br/><div class="children"><div class="content">Hey, brief feedback, but &quot;5,000,000,000+&quot; rows processed might be a red flag for some. Many individual customers might do this per day, so to say that&#x27;s all the company has done so far might put them off.<p>You&#x27;ve said that initial imports are free, but anecdotally, initial imports ended up being a somewhat regular occurrence as we found issues required a re-import, table rewrites, breaking schema changes, and so on. Do you pay full cost for subsequent full-imports?<p>Lastly, pricing. I feel like you really need a dollar figure on this even just to start the discussion. &quot;Call us for pricing&quot; is fine for enterprise plans, but in a ~100 person startup I would have just passed immediately on to others that provide pricing details. I&#x27;m not going to spend time trialling something without knowing if the pricing is in the right ballpark as it&#x27;s a waste of my time if it isn&#x27;t. My perception is that this would be substantially more expensive than batch replication competitors such as Stitch, but I don&#x27;t know.</div><br/><div id="36848918" class="c"><input type="checkbox" id="c-36848918" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36848350">parent</a><span>|</span><a href="#36848273">next</a><span>|</span><label class="collapse" for="c-36848918">[-]</label><label class="expand" for="c-36848918">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Hey, brief feedback, but &quot;5,000,000,000+&quot; rows processed might be a red flag for some. Many individual customers might do this per day, so to say that&#x27;s all the company has done so far might put them off.<p>Appreciate the feedback! Totally agree and it&#x27;s something we are working towards :). We learned a ton from going from nothing -&gt; billions, certain functions, backfill strategies, etc just doesn&#x27;t work at that scale. I&#x27;m sure we&#x27;ll have another equally exciting learning curve when scaling from B&#x2F;month -&gt; B&#x2F;day!<p>&gt; You&#x27;ve said that initial imports are free, but anecdotally, initial imports ended up being a somewhat regular occurrence as we found issues required a re-import, table rewrites, breaking schema changes, and so on. Do you pay full cost for subsequent full-imports?<p>Thanks for pointing this out. We plan to make all backfills free of charge.<p>&gt; Lastly, pricing. I feel like you really need a dollar figure on this even just to start the discussion. &quot;Call us for pricing&quot; is fine for enterprise plans, but in a ~100 person startup I would have just passed immediately on to others that provide pricing details. I&#x27;m not going to spend time trialling something without knowing if the pricing is in the right ballpark as it&#x27;s a waste of my time if it isn&#x27;t. My perception is that this would be substantially more expensive than batch replication competitors such as Stitch, but I don&#x27;t know.<p>Makes sense. A lot of folks think that streaming is more expensive than batched, what we&#x27;ve found with our initial customers is the opposite. Streaming is able to distribute the load to DWH and as a result, customers on Snowflake can use a much smaller vDWH when working with Artie. On the &quot;call us for pricing&quot; front, we are planning to provide a calculator &#x2F; graph to estimate costs before talking to a human in the future. However, we are learning that a lot of companies have bespoke 1yr&#x2F; 2yr contracts with their data provider and are trying to figure out our own pricing and packaging at the moment.</div><br/><div id="36852376" class="c"><input type="checkbox" id="c-36852376" checked=""/><div class="controls bullet"><span class="by">GordonS</span><span>|</span><a href="#36848350">root</a><span>|</span><a href="#36848918">parent</a><span>|</span><a href="#36848273">next</a><span>|</span><label class="collapse" for="c-36852376">[-]</label><label class="expand" for="c-36852376">[1 more]</label></div><br/><div class="children"><div class="content">I just wanted to second the GP&#x27;s point on pricing - like them, &quot;call me pricing&quot; is a hard no from me. IMO it&#x27;s fine for the highest tier (Enterprise or whatever), but lower tiers should have fixed and public pricing.</div><br/></div></div></div></div></div></div><div id="36848273" class="c"><input type="checkbox" id="c-36848273" checked=""/><div class="controls bullet"><span class="by">EDEdDNEdDYFaN</span><span>|</span><a href="#36848350">prev</a><span>|</span><a href="#36849087">next</a><span>|</span><label class="collapse" for="c-36848273">[-]</label><label class="expand" for="c-36848273">[4 more]</label></div><br/><div class="children"><div class="content">How many of these tools are out there? This is like the 4th CDC to Data warehouse tool I&#x27;ve seen this year and I&#x27;m struggling to understand why they all exist in parallel.<p>I just found estuary with a colleague yesterday which didn&#x27;t work and previously have seen at least two other tools promising the same CDC to Data Warehouse in-a-box pipelines. What makes this tool different?</div><br/><div id="36851084" class="c"><input type="checkbox" id="c-36851084" checked=""/><div class="controls bullet"><span class="by">jgraettinger1</span><span>|</span><a href="#36848273">parent</a><span>|</span><a href="#36849172">next</a><span>|</span><label class="collapse" for="c-36851084">[-]</label><label class="expand" for="c-36851084">[1 more]</label></div><br/><div class="children"><div class="content">Hi, I&#x27;m Estuary&#x27;s CTO (<a href="https:&#x2F;&#x2F;estuary.dev" rel="nofollow noreferrer">https:&#x2F;&#x2F;estuary.dev</a>). Mind speaking a bit more about what didn&#x27;t work?<p>We put quite a bit of effort into our CDC connectors, as it&#x27;s a core competency. We have numerous customers using them at scale successfully, but they can be a bit nuanced to get configured. We&#x27;re constantly trying to make our onboarding experience more intuitive and seamless... it&#x27;s a hard problem.</div><br/></div></div><div id="36849172" class="c"><input type="checkbox" id="c-36849172" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36848273">parent</a><span>|</span><a href="#36851084">prev</a><span>|</span><a href="#36848467">next</a><span>|</span><label class="collapse" for="c-36849172">[-]</label><label class="expand" for="c-36849172">[1 more]</label></div><br/><div class="children"><div class="content">For us, what we are striving to do differently is:<p>1&#x2F; DFS vs BFS. We are planning on rolling out connectors slowly vs. building tens &#x2F; hundreds &#x2F; thousands of connectors to try to attract a broader set of audiences. Those that have tried to replicate data between OLTP and OLAP know how painful it is and we really want to solve this pain point before we move on to new sources. In addition, we&#x27;re planning on providing more value than just replicating data from source to destination. We&#x27;re planning on integrating our telemetry library [1] with Datadog such that customers can:
 * Centralize all metrics. See Artie metrics without coming to our dashboard, instead it&#x27;s integrated with your existing tools.
 * Help provide cookie cutter monitors for anomaly detection
 * We also want to provide better table quality checks<p>2&#x2F; We do not want to change user behavior. We are using pretty standard tools to solve this problem such as Kafka, Pub&#x2F;Sub and Red Hat Debezium. If you are already using one of these tools, we can just integrate vs. standing up a whole suite of services just for the data pipeline.
 * If you have CDC events being emitted for event-driven architecture already, we&#x27;ll skip deploying Debezium and just deploy our consumer
 * If you have Kafka enabled and want to also consume CDC events, we&#x27;ll just deploy Debezium to publish to your Kafka vs. ours.<p>3&#x2F; Ease of use. This goes without saying, but plenty of tools out there have broken links in their documentation, extremely confusing UI, etc. We really look up to Fivetran in this regard and we try to make the onboarding process for new connectors as simple as possible.<p>Do you think there are anything missing with the other CDC or data replication tools out in the market? Let me know and happy to see how we can help!<p>[1] <a href="https:&#x2F;&#x2F;docs.artie.so&#x2F;telemetry&#x2F;overview">https:&#x2F;&#x2F;docs.artie.so&#x2F;telemetry&#x2F;overview</a></div><br/></div></div><div id="36848467" class="c"><input type="checkbox" id="c-36848467" checked=""/><div class="controls bullet"><span class="by">joelhaasnoot</span><span>|</span><a href="#36848273">parent</a><span>|</span><a href="#36849172">prev</a><span>|</span><a href="#36849087">next</a><span>|</span><label class="collapse" for="c-36848467">[-]</label><label class="expand" for="c-36848467">[1 more]</label></div><br/><div class="children"><div class="content">Many of them really just suck (depending on your workload)</div><br/></div></div></div></div><div id="36849087" class="c"><input type="checkbox" id="c-36849087" checked=""/><div class="controls bullet"><span class="by">PeterZaitsev</span><span>|</span><a href="#36848273">prev</a><span>|</span><a href="#36855159">next</a><span>|</span><label class="collapse" for="c-36849087">[-]</label><label class="expand" for="c-36849087">[1 more]</label></div><br/><div class="children"><div class="content">Hi Robin, This looks like very cool project<p>Note thought Elastic Licence 2.0 is not Open Source License so if this is your license choice I&#x27;d avoid calling the code you have on gitHub OSS</div><br/></div></div><div id="36855159" class="c"><input type="checkbox" id="c-36855159" checked=""/><div class="controls bullet"><span class="by">RyanHamilton</span><span>|</span><a href="#36849087">prev</a><span>|</span><a href="#36850821">next</a><span>|</span><label class="collapse" for="c-36855159">[-]</label><label class="expand" for="c-36855159">[1 more]</label></div><br/><div class="children"><div class="content">Good to see more entrants in the real time analytics space. I&#x27;ve been waiting years for most database to embrace streaming. I&#x27;m betting on these real time data platforms needing a faster dashboard than tableau... That&#x27;s what I&#x27;m working on <a href="https:&#x2F;&#x2F;www.timestored.com&#x2F;pulse&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.timestored.com&#x2F;pulse&#x2F;</a>. You may find it useful to show off demos. We are partnering with some startups like yourself. Email me if interested. And good luck!</div><br/></div></div><div id="36850821" class="c"><input type="checkbox" id="c-36850821" checked=""/><div class="controls bullet"><span class="by">BenderV</span><span>|</span><a href="#36855159">prev</a><span>|</span><a href="#36849163">next</a><span>|</span><label class="collapse" for="c-36850821">[-]</label><label class="expand" for="c-36850821">[1 more]</label></div><br/><div class="children"><div class="content">Congrats on your project. Reliable CDC is hard to set up.<p>I worked a bit on the space, focusing on api instead of DB, which is in the end a quite different problem.
My goal was to leverage LLM to auto-build connector on the fly.<p>I started by building a lightweight ETL, that I open-sourced (<a href="https:&#x2F;&#x2F;github.com&#x2F;BenderV&#x2F;universal-data">https:&#x2F;&#x2F;github.com&#x2F;BenderV&#x2F;universal-data</a>).<p>I left the space because I realize that I didn&#x27;t want to work on this problem, even though I believe in the &quot;AI&quot; approach, and think simplifying data transfer (or distributing compute) is one the key factor to scale data usage.</div><br/></div></div><div id="36849163" class="c"><input type="checkbox" id="c-36849163" checked=""/><div class="controls bullet"><span class="by">bpicolo</span><span>|</span><a href="#36850821">prev</a><span>|</span><a href="#36850805">next</a><span>|</span><label class="collapse" for="c-36849163">[-]</label><label class="expand" for="c-36849163">[4 more]</label></div><br/><div class="children"><div class="content">&gt;  the partition key is the primary key(s) to ensure no out of order writes.<p>Can you get out of order writes if the pk for a row changes?</div><br/><div id="36849280" class="c"><input type="checkbox" id="c-36849280" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36849163">parent</a><span>|</span><a href="#36850805">next</a><span>|</span><label class="collapse" for="c-36849280">[-]</label><label class="expand" for="c-36849280">[3 more]</label></div><br/><div class="children"><div class="content">Great question! Say this happened:<p>* CREATE TABLE user_test (id int primary key, name text);<p>* INSERT INTO user_test (id, name) VALUES (1, &#x27;foo&#x27;);<p>* UPDATE user_test set id = 2 where id = 1;<p>When the UPDATE call is invoked, there will be 2 events emitted to Kafka.
1&#x2F; A DELETE event for id = 1<p>2&#x2F; A CREATE event for id = 2<p>Here&#x27;s the detailed events that get emitted: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Tang8330&#x2F;7a9450f95fbae486a4393abdd4990d7e" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Tang8330&#x2F;7a9450f95fbae486a4393abdd49...</a></div><br/><div id="36849460" class="c"><input type="checkbox" id="c-36849460" checked=""/><div class="controls bullet"><span class="by">bpicolo</span><span>|</span><a href="#36849163">root</a><span>|</span><a href="#36849280">parent</a><span>|</span><a href="#36850805">next</a><span>|</span><label class="collapse" for="c-36849460">[-]</label><label class="expand" for="c-36849460">[2 more]</label></div><br/><div class="children"><div class="content">Interesting! I&#x27;m both surprised and also not surprised it&#x27;s modeled that way.</div><br/><div id="36849581" class="c"><input type="checkbox" id="c-36849581" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36849163">root</a><span>|</span><a href="#36849460">parent</a><span>|</span><a href="#36850805">next</a><span>|</span><label class="collapse" for="c-36849581">[-]</label><label class="expand" for="c-36849581">[1 more]</label></div><br/><div class="children"><div class="content">Eventual consistency FTW!</div><br/></div></div></div></div></div></div></div></div><div id="36850805" class="c"><input type="checkbox" id="c-36850805" checked=""/><div class="controls bullet"><span class="by">yevpats</span><span>|</span><a href="#36849163">prev</a><span>|</span><a href="#36852294">next</a><span>|</span><label class="collapse" for="c-36850805">[-]</label><label class="expand" for="c-36850805">[2 more]</label></div><br/><div class="children"><div class="content">Congrats!! Check out also CloudQuery (<a href="https:&#x2F;&#x2F;github.com&#x2F;cloudquery&#x2F;cloudquery">https:&#x2F;&#x2F;github.com&#x2F;cloudquery&#x2F;cloudquery</a>) an open source ELT that support DB-&gt;DB without Kafka or Debezium (Founder here) :)</div><br/><div id="36851247" class="c"><input type="checkbox" id="c-36851247" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36850805">parent</a><span>|</span><a href="#36852294">next</a><span>|</span><label class="collapse" for="c-36851247">[-]</label><label class="expand" for="c-36851247">[1 more]</label></div><br/><div class="children"><div class="content">Thank you!!</div><br/></div></div></div></div><div id="36852294" class="c"><input type="checkbox" id="c-36852294" checked=""/><div class="controls bullet"><span class="by">davej</span><span>|</span><a href="#36850805">prev</a><span>|</span><a href="#36851097">next</a><span>|</span><label class="collapse" for="c-36852294">[-]</label><label class="expand" for="c-36852294">[3 more]</label></div><br/><div class="children"><div class="content">Very cool, are you guys focused on databases as sources or do you plan to add API-based sources too (e.g.. Stripe Customers)? Currently using Airbyte but something more real-time would be beneficial to us.</div><br/><div id="36852490" class="c"><input type="checkbox" id="c-36852490" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36852294">parent</a><span>|</span><a href="#36852759">next</a><span>|</span><label class="collapse" for="c-36852490">[-]</label><label class="expand" for="c-36852490">[1 more]</label></div><br/><div class="children"><div class="content">For now, we&#x27;re super focused on databases as sources. We really want to do this well before we move on to other data sources such as APIs.</div><br/></div></div><div id="36852759" class="c"><input type="checkbox" id="c-36852759" checked=""/><div class="controls bullet"><span class="by">jgraettinger1</span><span>|</span><a href="#36852294">parent</a><span>|</span><a href="#36852490">prev</a><span>|</span><a href="#36851097">next</a><span>|</span><label class="collapse" for="c-36852759">[-]</label><label class="expand" for="c-36852759">[1 more]</label></div><br/><div class="children"><div class="content">estuary.dev may be a fit for you (am CTO).<p>(Competitive product and I feel weird about replying in Artie&#x27;s thread, but tang8330 has said they&#x27;re not serving this segment)</div><br/></div></div></div></div><div id="36851097" class="c"><input type="checkbox" id="c-36851097" checked=""/><div class="controls bullet"><span class="by">Aaronstotle</span><span>|</span><a href="#36852294">prev</a><span>|</span><a href="#36848483">next</a><span>|</span><label class="collapse" for="c-36851097">[-]</label><label class="expand" for="c-36851097">[3 more]</label></div><br/><div class="children"><div class="content">The blocker for testing this out and using it at my place of employment is lack of binary data type in MySQL, any plans to add this?</div><br/><div id="36851143" class="c"><input type="checkbox" id="c-36851143" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36851097">parent</a><span>|</span><a href="#36848483">next</a><span>|</span><label class="collapse" for="c-36851143">[-]</label><label class="expand" for="c-36851143">[2 more]</label></div><br/><div class="children"><div class="content">Definitely. Do you expect the resulting data to also be in binary &#x2F; bytes format in your DWH?<p>I ask because there&#x27;s a workaround by setting `binary.handling.mode` to a STRING type [1].<p>Transfer will then automatically pick this up and write this as a B64 string to the DWH.<p>[1] <a href="https:&#x2F;&#x2F;debezium.io&#x2F;documentation&#x2F;reference&#x2F;stable&#x2F;connectors&#x2F;mysql.html#mysql-property-binary-handling-mode" rel="nofollow noreferrer">https:&#x2F;&#x2F;debezium.io&#x2F;documentation&#x2F;reference&#x2F;stable&#x2F;connector...</a></div><br/><div id="36852465" class="c"><input type="checkbox" id="c-36852465" checked=""/><div class="controls bullet"><span class="by">Aaronstotle</span><span>|</span><a href="#36851097">root</a><span>|</span><a href="#36851143">parent</a><span>|</span><a href="#36848483">next</a><span>|</span><label class="collapse" for="c-36852465">[-]</label><label class="expand" for="c-36852465">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the reply! We&#x27;ll be keeping an eye on it</div><br/></div></div></div></div></div></div><div id="36848483" class="c"><input type="checkbox" id="c-36848483" checked=""/><div class="controls bullet"><span class="by">joelhaasnoot</span><span>|</span><a href="#36851097">prev</a><span>|</span><a href="#36849490">next</a><span>|</span><label class="collapse" for="c-36848483">[-]</label><label class="expand" for="c-36848483">[4 more]</label></div><br/><div class="children"><div class="content">Do you support PostGIS? This seems to be an issue with a lot of solutions I&#x27;ve seen...</div><br/><div id="36848748" class="c"><input type="checkbox" id="c-36848748" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36848483">parent</a><span>|</span><a href="#36849490">next</a><span>|</span><label class="collapse" for="c-36848748">[-]</label><label class="expand" for="c-36848748">[3 more]</label></div><br/><div class="children"><div class="content">Not at the moment, but no reason(s) why we shouldn&#x27;t. Debezium already supports this [1] and it&#x27;d be a minor refactor on our end to support. If that&#x27;s something you&#x27;d want, LMK!<p>[1] <a href="https:&#x2F;&#x2F;debezium.io&#x2F;documentation&#x2F;reference&#x2F;stable&#x2F;connectors&#x2F;postgresql.html#postgresql-postgis-types" rel="nofollow noreferrer">https:&#x2F;&#x2F;debezium.io&#x2F;documentation&#x2F;reference&#x2F;stable&#x2F;connector...</a></div><br/><div id="36849848" class="c"><input type="checkbox" id="c-36849848" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#36848483">root</a><span>|</span><a href="#36848748">parent</a><span>|</span><a href="#36849490">next</a><span>|</span><label class="collapse" for="c-36849848">[-]</label><label class="expand" for="c-36849848">[2 more]</label></div><br/><div class="children"><div class="content">Seconding this! We&#x27;re in the short-term rental space, and while properties to don&#x27;t change locations frequently (though mistakes get corrected, and also <a href="https:&#x2F;&#x2F;arkup.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arkup.com&#x2F;</a> exists...), the polygon definitions and hierarchical relationships of regions-of-interest for search and discovery are often being updated, and being able to have real-time geometry intersections rather than rerunning scripts periodically to rebuild our analyses would be incredibly powerful. And certainly for use cases like trucking fleet management etc., real-time analytics on location data across the entire gamut is vital.</div><br/><div id="36849959" class="c"><input type="checkbox" id="c-36849959" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36848483">root</a><span>|</span><a href="#36849848">parent</a><span>|</span><a href="#36849490">next</a><span>|</span><label class="collapse" for="c-36849959">[-]</label><label class="expand" for="c-36849959">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s fascinating! Thanks for providing more color, support for geometric shapes coming! <a href="https:&#x2F;&#x2F;github.com&#x2F;artie-labs&#x2F;transfer&#x2F;issues&#x2F;155">https:&#x2F;&#x2F;github.com&#x2F;artie-labs&#x2F;transfer&#x2F;issues&#x2F;155</a></div><br/></div></div></div></div></div></div></div></div><div id="36849490" class="c"><input type="checkbox" id="c-36849490" checked=""/><div class="controls bullet"><span class="by">halfcat</span><span>|</span><a href="#36848483">prev</a><span>|</span><label class="collapse" for="c-36849490">[-]</label><label class="expand" for="c-36849490">[2 more]</label></div><br/><div class="children"><div class="content">Is the value here primarily in low latency updates? i.e. the fraud detection scenario<p>My main pain points with ETLs are with systems that don’t provide good CDC metrics to begin with, which are not Postgres&#x2F;etc, which are usually a bolt-on ODBC driver or rate-limited APIs.</div><br/><div id="36849545" class="c"><input type="checkbox" id="c-36849545" checked=""/><div class="controls bullet"><span class="by">tang8330</span><span>|</span><a href="#36849490">parent</a><span>|</span><label class="collapse" for="c-36849545">[-]</label><label class="expand" for="c-36849545">[1 more]</label></div><br/><div class="children"><div class="content">Main value props are low latency OLTP data in your DWH and lower DWH ingestion costs.</div><br/></div></div></div></div></div></div></div></div></div></body></html>