<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720429276708" as="style"/><link rel="stylesheet" href="styles.css?v=1720429276708"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2407.02678">Reasoning in Large Language Models: A Geometric Perspective</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>belter</span> | <span>80 comments</span></div><br/><div><div id="40902548" class="c"><input type="checkbox" id="c-40902548" checked=""/><div class="controls bullet"><span class="by">john-tells-all</span><span>|</span><a href="#40900219">next</a><span>|</span><label class="collapse" for="c-40902548">[-]</label><label class="expand" for="c-40902548">[2 more]</label></div><br/><div class="children"><div class="content">AI has a &quot;bathtub curve&quot; of value. At the low level, it&#x27;s a super-autocomplete, able to write 1-3 lines of code that works good enough. At the high level, it&#x27;s great for explaining high-level concepts that are relevant to a task at hand.<p>In the middle... AI doesn&#x27;t work very well.<p>If an AI writes a multi-step plan, where the pieces have to fit together, I&#x27;ve found it goes off the rails. Parts 1 and 3 of a 4-part plan are fine. So is part 2. However they don&#x27;t fit together! AI has no concept of &quot;these four parts have to be closely connected, building a whole&quot;. It just builds from A to B in four steps... but taking two different paths and stitching the pieces together poorly.</div><br/><div id="40902923" class="c"><input type="checkbox" id="c-40902923" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#40902548">parent</a><span>|</span><a href="#40900219">next</a><span>|</span><label class="collapse" for="c-40902923">[-]</label><label class="expand" for="c-40902923">[1 more]</label></div><br/><div class="children"><div class="content">The low-level high-level spectrum might not be the best scale to gauge AI by. We should kernel-trick our scale so that low and high level is seperable from multi-step planning problems. Or in other words, use a different dimension to separate these three problems.</div><br/></div></div></div></div><div id="40900219" class="c"><input type="checkbox" id="c-40900219" checked=""/><div class="controls bullet"><span class="by">lifeisstillgood</span><span>|</span><a href="#40902548">prev</a><span>|</span><a href="#40903205">next</a><span>|</span><label class="collapse" for="c-40900219">[-]</label><label class="expand" for="c-40900219">[13 more]</label></div><br/><div class="children"><div class="content">But I understand there are two sides to the discussion - that by ingesting huge amounts of text these models have somehow built reasoning capabilities (language then reasoning) or that the reasoning was done by humans and then written down so as long as you ask something like “should romeo find another love after Juliet” there is a set of reasoning reflected in a billion English literature essays and the model just reflects those answers<p>Am I missing something?</div><br/><div id="40902064" class="c"><input type="checkbox" id="c-40902064" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40900566">next</a><span>|</span><label class="collapse" for="c-40902064">[-]</label><label class="expand" for="c-40902064">[1 more]</label></div><br/><div class="children"><div class="content">You should take a look at the more extensive reasoning tests used for LLMs right now, like MuSR, which clearly can&#x27;t be the latter, since the questions are new: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16049" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.16049</a></div><br/></div></div><div id="40900566" class="c"><input type="checkbox" id="c-40900566" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40902064">prev</a><span>|</span><a href="#40900284">next</a><span>|</span><label class="collapse" for="c-40900566">[-]</label><label class="expand" for="c-40900566">[2 more]</label></div><br/><div class="children"><div class="content">To me those seem like to sides of the same coin. LLMs are fundamentally trained to complete text. The training just tries to find the most effective way to do that within the given model architecture and parameter count.<p>Now if we start by &quot;LLMs ingest huge amounts of text&quot;, then a simple model would complete text by simple memorization. But correctly completing &quot;234 * 452 =&quot; is a lot simpler to do by doing math than by having memorized all possible multiplications. Similarly, understanding the world and being able to reason about it helps you correctly completing human-written sentences. Thus a sufficiently well-trained model that has enough parameters to do this but not so many that it simply overfits should be expected to develop some reasoning ability.<p>If you start with &quot;the training set contains a lot of reasoning&quot; you can get something that looks like reasoning in the memorization stage. But the same argument why the model would develop actual reasoning still works and is even stronger: if you have to complete someone&#x27;s argument that&#x27;s a lot easier if you can follow their train of thought.</div><br/><div id="40903336" class="c"><input type="checkbox" id="c-40903336" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#40900219">root</a><span>|</span><a href="#40900566">parent</a><span>|</span><a href="#40900284">next</a><span>|</span><label class="collapse" for="c-40903336">[-]</label><label class="expand" for="c-40903336">[1 more]</label></div><br/><div class="children"><div class="content">I think this is an underappreciated perspective. The simplest model of a reasoning process, at scale, is the reasoning process itself! That said, I haven&#x27;t come across any research directly testing that hypothesis with transformers. Do you know of any?<p>The closest I&#x27;ve seen is a paper on OthelloGPT using linear probes to show that it does in fact learn a predictive model of Othello board states (which can be manipulated at inference time, so it&#x27;s causal on the model&#x27;s behaviour).</div><br/></div></div></div></div><div id="40900284" class="c"><input type="checkbox" id="c-40900284" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40900566">prev</a><span>|</span><a href="#40900462">next</a><span>|</span><label class="collapse" for="c-40900284">[-]</label><label class="expand" for="c-40900284">[2 more]</label></div><br/><div class="children"><div class="content">Glossing through the paper, it seems they&#x27;re noting this issue but kinda skipping over it:<p><i>In fact, it is clear that approximation capabilities and generalization are not equivalent notions. However, it is not yet determined that the reasoning capabilities of LLMs are tied to their generalization. While these notions are still hard to pinpoint, we will focus in this experimental section on the relationship between intrinsic dimension, thus expressive power, and reasoning capabilities.</i></div><br/><div id="40900771" class="c"><input type="checkbox" id="c-40900771" checked=""/><div class="controls bullet"><span class="by">cowsaymoo</span><span>|</span><a href="#40900219">root</a><span>|</span><a href="#40900284">parent</a><span>|</span><a href="#40900462">next</a><span>|</span><label class="collapse" for="c-40900771">[-]</label><label class="expand" for="c-40900771">[1 more]</label></div><br/><div class="children"><div class="content">Right, they never claimed to have found a roadmap to AGI, they just found a cool geometric tool to describe how LLMs reason through approximation. Sounds like a handy tool if you want to discover things about approximation or generalization.</div><br/></div></div></div></div><div id="40900462" class="c"><input type="checkbox" id="c-40900462" checked=""/><div class="controls bullet"><span class="by">nshm</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40900284">prev</a><span>|</span><a href="#40900369">next</a><span>|</span><label class="collapse" for="c-40900462">[-]</label><label class="expand" for="c-40900462">[3 more]</label></div><br/><div class="children"><div class="content">It is actually pretty straightforward why those model &quot;reason&quot; or, to be more exact, can operate on a complex concepts. By processing huge amount of texts they build an internal representation where those concepts are represented as a simple nodes (neurons or groups). So they really distill knowledge. Alternatively you can think about it as a very good principal component analysis that can extract many important aspects. Or like a semantic graph built automatically.<p>Once knowledge is distilled you can build on top of it easily by merging concepts for example.<p>So no secret here.</div><br/><div id="40901162" class="c"><input type="checkbox" id="c-40901162" checked=""/><div class="controls bullet"><span class="by">lifeisstillgood</span><span>|</span><a href="#40900219">root</a><span>|</span><a href="#40900462">parent</a><span>|</span><a href="#40900369">next</a><span>|</span><label class="collapse" for="c-40901162">[-]</label><label class="expand" for="c-40901162">[2 more]</label></div><br/><div class="children"><div class="content">Do they distill knowledge or distill the relationship between words (that describe knowledge)<p>I know it seems dancing on head of pin but …</div><br/><div id="40901296" class="c"><input type="checkbox" id="c-40901296" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40900219">root</a><span>|</span><a href="#40901162">parent</a><span>|</span><a href="#40900369">next</a><span>|</span><label class="collapse" for="c-40901296">[-]</label><label class="expand" for="c-40901296">[1 more]</label></div><br/><div class="children"><div class="content">Well the internal representation is tokens not words so.. the pin is even smaller?<p>They distill relationships between tokens. Multiple tokens together make up a word, and multiple words together make up a <i>label</i> for something we recognize as a &quot;concept&quot;.<p>These &quot;concepts&quot; are <i>not</i> just a label though - they are an area in the latent space inside the neural network which happens to contains those words in the sequence (along with other labels that mean similar things).<p>A simple demonstration of this is how easily multi-modal neural networks build cross modal representations of the same thing, so &quot;cats&quot; end up in the same place in both image and word form but also more complex concepts (&quot;a beautiful country fields with a foreboding thunderstorm forming&quot;) will also align well between the words and the images.</div><br/></div></div></div></div></div></div><div id="40900369" class="c"><input type="checkbox" id="c-40900369" checked=""/><div class="controls bullet"><span class="by">Fripplebubby</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40900462">prev</a><span>|</span><a href="#40901103">next</a><span>|</span><label class="collapse" for="c-40900369">[-]</label><label class="expand" for="c-40900369">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the model just reflects those answers<p>I think there is a lot happening in the word &quot;reflects&quot;! Is it so simple?<p>Does this mean that the model takes on the opinion of a specific lit crit essay it has &quot;read&quot;? Does that mean it takes on some kind of &quot;average&quot; opinion from everything? How would you define the &quot;average&quot; opinion on a topic, anyway?<p>Anyway, although I think this is really interesting stuff and cuts to the core of what an LLM is, this paper isn&#x27;t where you&#x27;re going to get the answer to that, because it is much more focused and narrow.</div><br/></div></div><div id="40901103" class="c"><input type="checkbox" id="c-40901103" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40900369">prev</a><span>|</span><a href="#40903007">next</a><span>|</span><label class="collapse" for="c-40901103">[-]</label><label class="expand" for="c-40901103">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re close enough that the differences probably aren&#x27;t too important. But if you want a bit more nuance, then read on. For disclosure, I&#x27;m in the second camp here. But I&#x27;ll also say that I have a lot of very strong evidence to support this position, and that I do this from the perspective of a researcher.<p>There&#x27;s a few big problems when making any definite claims about either side. First, we need to know what data the machine is processing when training. I think we all understand that if the data is in training, then testing is not actually testing a model&#x27;s ability to generalize, but a model&#x27;s ability to recall. Second, we need to recognize the amount of duplication of data, both exact and semantically.<p>1) We have no idea because these are proprietary. While LLAMA is more open than GPT, we don&#x27;t know all the data that went into it (last I checked). Thus, you can&#x27;t say &quot;this isn&#x27;t in the data.&quot;[0] But we do know some things that are in the data, though we don&#x27;t know exactly what was filtered out. We&#x27;re all pretty online people here and I&#x27;m sure many people have seen some of the depths of places like Reddit, Medium, or even Hacker News. These are all in the (unfiltered) training data! There&#x27;s even a large number of arxiv papers, books, publications, and so much more. So you have to ask yourself this: &quot;Are we confident that what we&#x27;re asking the model to do is not in the data we trained on?&quot; Almost certainly it is, so then the question moves to &quot;Are we confident that what we&#x27;re asking the model to do was adequately filtered out during training so we can have a fair test?&quot; Regardless of what your position is, I think you can see how such a question is incredibly important and how it would be easy to mess up. And only easier the more data we train on, since it&#x27;s so incredibly hard to process that data.[1] I think you can see some concerning issues with this filtering method and how it can create a large number of false negatives. They explicitly ignore answers, which is important for part 2. IIRC the GPT-3 paper also used an ngram model to check for dupes. But the most concerning line to me was this one:<p><pre><code>  &gt; As can be seen in tables 9 and 10, contamination overall has very little effect on the reported results.
</code></pre>
There is a concerning way to read the data here that serves a valid explanation for the results. That the data is so contaminated, the filtering process does not meaningfully remove the contamination and thus does not significantly change the results. If introducing contamination into your data does not change your results you either have a model that has learned the function of the data VERY well and has an extremely impressive form of generalization, OR your data is contaminated in ways you aren&#x27;t aware of (there are other explanations too btw). There&#x27;s a clearly simpler answer here.<p>Second, is about semantic information and contamination[2]. This is when data has the same effective meaning, but uses different ways to express it. &quot;This is a cat&quot; and &quot;este es un gato&quot; are semantically the same but share no similar words. So is &quot;I think there&#x27;s data spoilage&quot; as well as &quot;There is some concerning issues left to be resolved that bring into question the potential for information leakage.&quot; These will not be caught by substrings or ngrams. Yet, training on one will be no different than training on the other once we consider RLHF. The thing here is that in high dimensions, data is very confusing and does not act the way you might expect when operating in 2D and 3D. A mean between two values may or may not be representative depending on the type of distribution (uniform and gaussian, respectively), and we don&#x27;t have a clue what that is (it is intractable!). The curse of dimensionality is about how it is difficult to distinguish a nearest neighboring point from the furthest neighboring point, because our concept of a metric degrades as we increase dimensionality (just like we lose algebraic structure when going from C (complex) -&gt; H (quaternion) -&gt; O (octonions) (commutativity, then associativity)[3]. Some of this may be uninteresting in the mathematical sense but some does matter too. But because of this, we need to rethink our previous questions carefully. Now we need to ask: &quot;Are we confident that we have filtered out data that is not sufficiently meaningfully different from that in the test data?&quot; Given the complexity of semantic similarity and the fact that &quot;sufficiently&quot; is not well defined, I think this should make anybody uneasy. If you are absolutely confident the answer is &quot;yes, we have filtered it&quot; I would think you a fool. It is so incredibly easy to fool ourselves that any good researcher needs to have a constant amount of doubt (though confidence is needed too!). But neither should our lack of a definite answer here stop progress. But it should make us more careful about what claims we do make. And we need to be clear about this or else conmen have an easy time convincing others.<p>To me, the common line of research is wrong. Until we know the data and have processed the data with many looking for means of contamination, results like these are not meaningful. They rely on a shaky foundation and often are more looking for evidence to prove reasoning than to consider it might not.<p>But for me, I think the conversations about a lot of this are quite strange. Does it matter that LLMs can&#x27;t reason? I mean in some sense yes, but the lack of this property does not make them any less powerful of a tool. If all they are is a lossy compression of the majority of human knowledge with a built in human interface, that sounds like an incredible achievement and a very useful tool. Even Google is fuzzy! But this also tells us what the tool is good for and isn&#x27;t. That this puts bounds on what we should rely on it for and what we can trust it to do with and without human intervention. I think some are afraid that if LLMs aren&#x27;t reasoning, then that means we won&#x27;t get AGI. But at the same time, if they don&#x27;t reason, then we need to find out why and how to make machines reason if we are to get there. So ignoring potential pitfalls hinders this progress. I&#x27;m not suggesting that we should stop using or studying LLMs (we should continue to), but rather that we need to stop putting alternatives down. We need to stop comparing alternatives one-to-one to models that took millions of dollars to do a single training and have been studied by thousands of people for several years against things scrambled together by small labs on a shoestring budget. We&#x27;ll never be able to advance if the goalpost is that you can&#x27;t make incremental steps along the way. Otherwise how do you? You got to create something new without testing, convince someone to give you millions of dollars to train it, and then millions more to debug your mistakes and things you&#x27;ve learned along the way? Very inefficient. We can take small steps. I think this goalpost results in obscurification. That because the bar is set so high, that strong claims need to be made for these works to be published. So we have to ask ourselves the deeper questions: &quot;Why are we doing this?&quot;[4]<p>[0] This might seem backwards but the creation of the model implicitly claims that the test data and training data are segregated. &quot;Show me this isn&#x27;t in training&quot; is a request for validation.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.08774" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.08774</a><p>[2] If you&#x27;re interested, Meta put out a work on semantic deduplication last year. They mostly focused on vision, but it still shows the importance of what&#x27;s being argued here. It is probably easier to verify that images are semantically similar than sentences, since language is more abstract. So pixels can be wildly different and the result is visually identical; how does this concept translate with language? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.09540" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.09540</a><p>[3] <a href="https:&#x2F;&#x2F;math.stackexchange.com&#x2F;questions&#x2F;641809&#x2F;what-specific-algebraic-properties-are-broken-at-each-cayley-dickson-stage-beyon" rel="nofollow">https:&#x2F;&#x2F;math.stackexchange.com&#x2F;questions&#x2F;641809&#x2F;what-specifi...</a><p>[4] I think if our answer is just &quot;to make money&quot; (or anything semantically similar like &quot;increase share value&quot;) then we are doomed to mediocrity and will stagnate. But I think if we&#x27;re doing these things to better human lives, to understand the world and how things work (I&#x27;d argue building AI is, even if a bit abstract), or to make useful and meaningful things, then the money will follow. But I think that many of us and many leading teams and businesses have lost focus on the journey that has led to profits and are too focused on the end result. And I do not think this is isolated to CEOs, I think this similar short sighted thinking can be repeated all the way down the corporate ladder. To a manager focusing on what their bosses explicitly ask for (rather than the intent) to the employee who knows that this is not the right thing to do but does it anyways (often because they know the manager will be unhappy. And this repeats all the way up). All life, business, technology, and creation have immense amounts of complexity to them. Ones we obviously want to simplify as much as possible. But when we hyper focus on any set of rules, no matter how complex, we will be doomed to fail because the environment is always changing and you will never be able to instantly adapt (this is the nature of chaos. Where small perturbations have large changes on the outcome). That doesn&#x27;t mean we shouldn&#x27;t try to make rules, but rather it means that rules are to be broken. It&#x27;s just a matter of knowing when. In the end, this is an example of what it means to be able to reason. So we should be careful to ensure that we create AGI by making machines able to reason and think (to make them &quot;more human&quot;) rather than by making humans into unthinking machines. I worry that the latter looks more likely, given that it is a much easier task to accomplish.</div><br/></div></div><div id="40903007" class="c"><input type="checkbox" id="c-40903007" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40901103">prev</a><span>|</span><a href="#40901957">next</a><span>|</span><label class="collapse" for="c-40903007">[-]</label><label class="expand" for="c-40903007">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re missing the fact that the model can only express its capabilities through the token generation mechanism.<p>The annoying &quot;humans are auto complete&quot; crowd really tries their best to obscure this.<p>Consider the following. You are taking notes in French in a choppy way by writing keywords. Then you write the output in English, but you are only allowed to use phrases that you have already seen to express your keywords. Your teacher doesn&#x27;t speak french and only looks at your essay. You are therefore able to do more complicated things in french, since you don&#x27;t lose points for writing things that the teacher hasn&#x27;t taught you. However, the point deduction is so ingrained in you, that even after the teacher is gone, you still decide to not say some of the things you have written in french.</div><br/></div></div><div id="40901957" class="c"><input type="checkbox" id="c-40901957" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#40900219">parent</a><span>|</span><a href="#40903007">prev</a><span>|</span><a href="#40903205">next</a><span>|</span><label class="collapse" for="c-40901957">[-]</label><label class="expand" for="c-40901957">[1 more]</label></div><br/><div class="children"><div class="content">They’re the same thing:<p>A type theory has a corresponding geometric interpretation, per topos theory. And this is bidirectional, since there’s an equivalence.<p>A geometric model of language will correspond to some effective type theory encoded in that language.<p>So LLMs are essentially learning an implicit “internal language” they’re reasoning in — based on their training data of our language and ways of reasoning.</div><br/></div></div></div></div><div id="40903205" class="c"><input type="checkbox" id="c-40903205" checked=""/><div class="controls bullet"><span class="by">omerhac</span><span>|</span><a href="#40900219">prev</a><span>|</span><a href="#40899989">next</a><span>|</span><label class="collapse" for="c-40903205">[-]</label><label class="expand" for="c-40903205">[1 more]</label></div><br/><div class="children"><div class="content">Each time research about LLM and reasoning comes out Yan LeCun gets an itch</div><br/></div></div><div id="40899989" class="c"><input type="checkbox" id="c-40899989" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#40903205">prev</a><span>|</span><a href="#40902639">next</a><span>|</span><label class="collapse" for="c-40899989">[-]</label><label class="expand" for="c-40899989">[20 more]</label></div><br/><div class="children"><div class="content">What does reasoning have to do with geometry? Is this like the idea that different concepts have inherent geometrical forms? A Platonic or noetic take on the geometries of reason? (I struggled to understand much of this paper…)</div><br/><div id="40901800" class="c"><input type="checkbox" id="c-40901800" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40899989">parent</a><span>|</span><a href="#40900224">next</a><span>|</span><label class="collapse" for="c-40901800">[-]</label><label class="expand" for="c-40901800">[1 more]</label></div><br/><div class="children"><div class="content">A follow-up comment after having studied the paper a bit more, since you asked about where the geometry comes into play.<p>One of the references the paper provide is to this[1] paper, which shows how the non-linear layers in modern deep neural networks partitions the input into regions and applies region-dependent affine mappings[2] to generate the output. It also mentions how that connects to vector quantization and k-means clustering.<p>So, the geometric perspective isn&#x27;t referring to your typical high-school geometry, but more abstract concepts like vector spaces[3] and combinatiorial computational geometry[4].<p>The submitted paper shows that this partitioning is directly linked to the approximation power of the neural network. They then show how increasing the approximation power results in better answers to math word problems, and hence that the approximation power correlated to the reasoning ability of LLMs.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1805.06576v2" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1805.06576v2</a><p>[2]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Affine_transformation" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Affine_transformation</a><p>[3]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector_space" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector_space</a><p>[4]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Computational_geometry#Combinatorial_computational_geometry" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Computational_geometry#Combina...</a></div><br/></div></div><div id="40900224" class="c"><input type="checkbox" id="c-40900224" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40899989">parent</a><span>|</span><a href="#40901800">prev</a><span>|</span><a href="#40902598">next</a><span>|</span><label class="collapse" for="c-40900224">[-]</label><label class="expand" for="c-40900224">[3 more]</label></div><br/><div class="children"><div class="content">Modern neural networks make heavy use of linear algebra, in particular the transformer[1] architecture that powers modern LLMs.<p>Since linear algebra is closely related to geometry[2], it seems quite reasonable that there are some geometric aspects that define their capabilities and performance.<p>Specifically, in this paper they&#x27;re considering the intrinsic dimension[3] of the attention layers, and seeing how it correlates with the performance of LLMs.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformer_(deep_learning_architecture)#Scaled_dot-product_attention" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformer_(deep_learning_arc...</a><p>[2]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Linear_algebra#Relationship_with_geometry" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Linear_algebra#Relationship_wi...</a><p>[3]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intrinsic_dimension" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intrinsic_dimension</a></div><br/><div id="40901541" class="c"><input type="checkbox" id="c-40901541" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40900224">parent</a><span>|</span><a href="#40902598">next</a><span>|</span><label class="collapse" for="c-40901541">[-]</label><label class="expand" for="c-40901541">[2 more]</label></div><br/><div class="children"><div class="content">&gt; it seems quite reasonable that there are some geometric aspects that define their capabilities and performance.<p>Sure but this doesn&#x27;t mean terribly much when you can relate either concept to virtually any other concept. &quot;Reasonable&quot; would imply one specific term implies another specific term and you haven&#x27;t filled in those blanks yet.</div><br/></div></div></div></div><div id="40902598" class="c"><input type="checkbox" id="c-40902598" checked=""/><div class="controls bullet"><span class="by">rlupi</span><span>|</span><a href="#40899989">parent</a><span>|</span><a href="#40900224">prev</a><span>|</span><a href="#40901248">next</a><span>|</span><label class="collapse" for="c-40902598">[-]</label><label class="expand" for="c-40902598">[1 more]</label></div><br/><div class="children"><div class="content">&quot;different concepts have inherent geometrical forms&quot;<p>Absolutely, in fact you can build the foundation of mathematics on this concept. You can build proofs and reasoning (for some value of &quot;reasoning&quot;).<p>That&#x27;s how dependent type systems work, search for HoTT and modal homotophy theory. That&#x27;s how lean4, coq, and theorem proofs work.<p>If you remember at the foundation of lambda calculus or boolean algebra, they proceed through a series of transformation of mathematical objects that are organized lattices or semi-lattices, partially ordered sets (e.g. in boolean algebra, where the partial order is provided by the implication).<p>It would be interesting to understand if the density of attention mechanisms follow a similar progression as dependent type systems, and we can find a link between the dependent types involved in a proof and the corresponding spaces in a LLM via some continuous relaxation analogous to a proximal operator + some transformation (from high-level concepts into output tokens).<p>We have found in embeddings that geometry has a meaning. Specific simple concepts correspond to vector directions. I wouldn&#x27;t be surprised at all that we find that reasoning on dependent concepts correspond to complex subspaces in the paths that a LLM takes, and that with enough training this connections becomes closer and closer to the logical structure of corresponding proofs (for self-consistent corpus of input and, like math proofs, and given enough training data).</div><br/></div></div><div id="40901248" class="c"><input type="checkbox" id="c-40901248" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#40899989">parent</a><span>|</span><a href="#40902598">prev</a><span>|</span><a href="#40900052">next</a><span>|</span><label class="collapse" for="c-40901248">[-]</label><label class="expand" for="c-40901248">[4 more]</label></div><br/><div class="children"><div class="content">The paper doesn&#x27;t make this point at all, but one thing you could do here is an AlphaGeometry-style[1] synthetic benchmark, where you have a geometry engine crank out a hundred million word problems, and have an LLM try to solve them.<p>Geometry problems have the nice property that they&#x27;re easy to generate and solve mechanically, but there&#x27;s no particular reason why a vanilla Transformer LLM would be any good at them, and you can have absolutely huge scale. (Unlike, say, the HumanEval benchmark, which only has 164 problems, which resulted in lots of accusations that LLMs can simply memorize the answers)<p>1: <a href="https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;alphageometry-an-olympiad-level-ai-system-for-geometry&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;alphageometry-an-olymp...</a></div><br/><div id="40901526" class="c"><input type="checkbox" id="c-40901526" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40901248">parent</a><span>|</span><a href="#40900052">next</a><span>|</span><label class="collapse" for="c-40901526">[-]</label><label class="expand" for="c-40901526">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;d have the second problem of trying to figure out how to relay geometry as a sequence of tokens when surely how you would encode this would affect what things you might reasonably expect an LLM to draw from it.</div><br/><div id="40902839" class="c"><input type="checkbox" id="c-40902839" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40901526">parent</a><span>|</span><a href="#40902078">next</a><span>|</span><label class="collapse" for="c-40902839">[-]</label><label class="expand" for="c-40902839">[1 more]</label></div><br/><div class="children"><div class="content">Only if your purpose is to create the best geometry solver. If you&#x27;re trying to improve the general intelligence of a frontier LLM, you&#x27;re probably better off feeding in the synthetic data as some combination of raw images and text (as part of its existing tokenisation).</div><br/></div></div><div id="40902078" class="c"><input type="checkbox" id="c-40902078" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40901526">parent</a><span>|</span><a href="#40902839">prev</a><span>|</span><a href="#40900052">next</a><span>|</span><label class="collapse" for="c-40902078">[-]</label><label class="expand" for="c-40902078">[1 more]</label></div><br/><div class="children"><div class="content">Image input.</div><br/></div></div></div></div></div></div><div id="40900052" class="c"><input type="checkbox" id="c-40900052" checked=""/><div class="controls bullet"><span class="by">qorrect</span><span>|</span><a href="#40899989">parent</a><span>|</span><a href="#40901248">prev</a><span>|</span><a href="#40900041">next</a><span>|</span><label class="collapse" for="c-40900052">[-]</label><label class="expand" for="c-40900052">[6 more]</label></div><br/><div class="children"><div class="content">I think they are talking about the word embeddings, where context is embedded into high geometric dimensions (one dimension might capture how &#x27;feminine&#x27; a word is, or how &#x27;blue&#x27; it is).</div><br/><div id="40900608" class="c"><input type="checkbox" id="c-40900608" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40900052">parent</a><span>|</span><a href="#40900675">next</a><span>|</span><label class="collapse" for="c-40900608">[-]</label><label class="expand" for="c-40900608">[3 more]</label></div><br/><div class="children"><div class="content">Which word embeddings get their own dimension though, and which don&#x27;t? (&quot;feminine&quot; and &quot;blue&quot; are words like any other)</div><br/><div id="40902240" class="c"><input type="checkbox" id="c-40902240" checked=""/><div class="controls bullet"><span class="by">willy_k</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40900608">parent</a><span>|</span><a href="#40900686">next</a><span>|</span><label class="collapse" for="c-40902240">[-]</label><label class="expand" for="c-40902240">[1 more]</label></div><br/><div class="children"><div class="content">My extremely naive understanding is that the more useful ones, which also tend to be structures of language like gender or color, get their own dimensions, and other embedding are represented with combinations.<p>A weak illustration of this is this site[1], from an HN post a few months ago[2].<p>[1] <a href="https:&#x2F;&#x2F;neal.fun&#x2F;infinite-craft&#x2F;" rel="nofollow">https:&#x2F;&#x2F;neal.fun&#x2F;infinite-craft&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39205020">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39205020</a></div><br/></div></div><div id="40900686" class="c"><input type="checkbox" id="c-40900686" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40900608">parent</a><span>|</span><a href="#40902240">prev</a><span>|</span><a href="#40900675">next</a><span>|</span><label class="collapse" for="c-40900686">[-]</label><label class="expand" for="c-40900686">[1 more]</label></div><br/><div class="children"><div class="content">maybe it&#x27;s like a PCA&#x2F;Huffman deal, where the more regularly useful ones get to be the eigenvectors.</div><br/></div></div></div></div><div id="40900675" class="c"><input type="checkbox" id="c-40900675" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40900052">parent</a><span>|</span><a href="#40900608">prev</a><span>|</span><a href="#40900041">next</a><span>|</span><label class="collapse" for="c-40900675">[-]</label><label class="expand" for="c-40900675">[2 more]</label></div><br/><div class="children"><div class="content">ooooh what if qualia is just embedding? some philosophers would get their toga in a twist!</div><br/><div id="40901620" class="c"><input type="checkbox" id="c-40901620" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40900675">parent</a><span>|</span><a href="#40900041">next</a><span>|</span><label class="collapse" for="c-40901620">[-]</label><label class="expand" for="c-40901620">[1 more]</label></div><br/><div class="children"><div class="content">Not Wittgenstein :cool:</div><br/></div></div></div></div></div></div><div id="40901375" class="c"><input type="checkbox" id="c-40901375" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#40899989">parent</a><span>|</span><a href="#40900041">prev</a><span>|</span><a href="#40900064">next</a><span>|</span><label class="collapse" for="c-40901375">[-]</label><label class="expand" for="c-40901375">[1 more]</label></div><br/><div class="children"><div class="content">If the curvature metric wasn’t steep to begin with AdamW wouldn’t work. If the regions of interest weren’t roughly Euclidean control vectors wouldn’t work.</div><br/></div></div><div id="40900064" class="c"><input type="checkbox" id="c-40900064" checked=""/><div class="controls bullet"><span class="by">cornholio</span><span>|</span><a href="#40899989">parent</a><span>|</span><a href="#40901375">prev</a><span>|</span><a href="#40902639">next</a><span>|</span><label class="collapse" for="c-40900064">[-]</label><label class="expand" for="c-40900064">[2 more]</label></div><br/><div class="children"><div class="content">I think the connection is that the authors could convincingly write a paper on this connection, thus inflating the AI publication bubble, furthering their academic acumen and improving their chances of getting research grants or selective jobs in the field. Some other interests of the authors seem to be detecting exoplanets using AI and detecting birds through audio analysis.<p>Since nobody can really say what a good AI department does, companies seem to be driven by credentiallism, load up on machine learning PhDs and masters so they can show their board and investors that they are ready for the AI revolution. This creates economic pressure to write such papers, the vast majority of which will amount to nothing.</div><br/><div id="40900406" class="c"><input type="checkbox" id="c-40900406" checked=""/><div class="controls bullet"><span class="by">techbro92</span><span>|</span><a href="#40899989">root</a><span>|</span><a href="#40900064">parent</a><span>|</span><a href="#40902639">next</a><span>|</span><label class="collapse" for="c-40900406">[-]</label><label class="expand" for="c-40900406">[1 more]</label></div><br/><div class="children"><div class="content">I think a lot of the time you would be correct. But this is published to arxiv so it’s not peer reviewed and doesn’t boost the authors credentials. It could be designed to attract attention to the company they work at. Or it could just be a cool idea the author wanted to share.</div><br/></div></div></div></div></div></div><div id="40902639" class="c"><input type="checkbox" id="c-40902639" checked=""/><div class="controls bullet"><span class="by">DrMiaow</span><span>|</span><a href="#40899989">prev</a><span>|</span><a href="#40900482">next</a><span>|</span><label class="collapse" for="c-40902639">[-]</label><label class="expand" for="c-40902639">[1 more]</label></div><br/><div class="children"><div class="content">&quot;just add more dimensions, bro!&quot;</div><br/></div></div><div id="40900482" class="c"><input type="checkbox" id="c-40900482" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40902639">prev</a><span>|</span><a href="#40902467">next</a><span>|</span><label class="collapse" for="c-40900482">[-]</label><label class="expand" for="c-40900482">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not into AI, but I like to watch from the sidelines. Here&#x27;s my non-AI summary of the paper after glossing through (corrections appreciated):<p>The multilayered perceptron[1] layers used in modern neural networks, like LLMs, essentially partitions the input into multiple regions. They show that the number of regions a single MLP layer can partition into depends exponentially on the intrinsic dimension[2] of the input. The number of regions&#x2F;partitions increases the approximation power of the MLP layer.<p>Thus you can significantly increase the approximation power of a MLP layer without increasing the number of neurons, by essentially &quot;distilling&quot; the input to it.<p>In the transformer architecture, the inputs to the MLP layers are the self-attention layers[3]. The authors then show that the graph density of the self-attention layers[3] correlates strongly with the intrinsic dimension of the self-attention layer. Thus a more dense self-attention layer means the MLP can do a better job.<p>One way of increasing the density of the attention layers is to add more context. (edited, see comment) They show that prepending any token as context to a question which increases the intrinsic dimension of the final layer makes the LLM perform better.<p>They also note that the transformer architecture is susceptible to compounding approximation errors, and that the much more precise partitioning provided by the MLP layers when fed with high intrinsic-dimensional input can help with this. However the impact of this on generalization remains to be explored further.<p>If the results hold up it does seem like this paper provides nice insight into how to better optimize LLMs and similar neural networks.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multilayer_perceptron" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multilayer_perceptron</a><p>[2]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intrinsic_dimension" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intrinsic_dimension</a><p>[3]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformer_(deep_learning_architecture)#Multi-head_attention" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformer_(deep_learning_arc...</a></div><br/><div id="40900690" class="c"><input type="checkbox" id="c-40900690" checked=""/><div class="controls bullet"><span class="by">Fripplebubby</span><span>|</span><a href="#40900482">parent</a><span>|</span><a href="#40901467">next</a><span>|</span><label class="collapse" for="c-40900690">[-]</label><label class="expand" for="c-40900690">[3 more]</label></div><br/><div class="children"><div class="content">Awesome summarization by someone who read and actually understood the paper.<p>&gt; One way of increasing the density of the attention layers is to add more context. They show that simply prepending any token as context to a question makes the LLM perform better. Adding relevant context makes it even better.<p>Right, I think a more intuitive way to think about this is to define density: the number of _edges_ in the self-attention graph connecting tokens. Maybe a simpler explanation: the number of times a token had some connection to another token divided by the number of tokens. So, tokens which actually relate to one another and provide information are good, non sequitur tokens don&#x27;t help except that you say<p>&gt; They show that simply prepending any token as context to a question makes the LLM perform better.<p>I think this is not quite right. What they found was:<p>&gt; pre-pending the question at hand
with any type of token does increase the intrinsic dimension at the first layer<p>&gt; however, this increase is not
necessarily correlated with the reasoning capability of the model<p>but it is only<p>&gt; when the pre-pended tokens lead to an increase in the
intrinsic dimension at the *final layer* of the model, the reasoning capabilities of the LLM improve
significantly.<p>(emphasis mine)</div><br/><div id="40900709" class="c"><input type="checkbox" id="c-40900709" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40900482">root</a><span>|</span><a href="#40900690">parent</a><span>|</span><a href="#40902306">prev</a><span>|</span><a href="#40901467">next</a><span>|</span><label class="collapse" for="c-40900709">[-]</label><label class="expand" for="c-40900709">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, good catch, got distracted by the editing flaws at the end there (they rewrote a section without removing the old one).</div><br/></div></div></div></div></div></div><div id="40901461" class="c"><input type="checkbox" id="c-40901461" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40902467">prev</a><span>|</span><a href="#40901341">next</a><span>|</span><label class="collapse" for="c-40901461">[-]</label><label class="expand" for="c-40901461">[7 more]</label></div><br/><div class="children"><div class="content">Okay. So more weights = more parameter space for expression.<p>And?</div><br/><div id="40902305" class="c"><input type="checkbox" id="c-40902305" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40901461">parent</a><span>|</span><a href="#40901474">next</a><span>|</span><label class="collapse" for="c-40902305">[-]</label><label class="expand" for="c-40902305">[5 more]</label></div><br/><div class="children"><div class="content">The way I understood it, it&#x27;s more like the opposite. That is if you feed the non-linear layers &quot;dense&quot; data, ie with higher intrinsic dimension, they perform better. Thus, you could potentially get by using a smaller non-linear layers by &quot;condensing&quot; the input before passing it through the non-linear layers.</div><br/><div id="40902597" class="c"><input type="checkbox" id="c-40902597" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40901461">root</a><span>|</span><a href="#40902305">parent</a><span>|</span><a href="#40901474">next</a><span>|</span><label class="collapse" for="c-40902597">[-]</label><label class="expand" for="c-40902597">[4 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t make any sense. Higher dimension means less dense. Far less dense, actually.</div><br/><div id="40902691" class="c"><input type="checkbox" id="c-40902691" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40901461">root</a><span>|</span><a href="#40902597">parent</a><span>|</span><a href="#40902661">next</a><span>|</span><label class="collapse" for="c-40902691">[-]</label><label class="expand" for="c-40902691">[2 more]</label></div><br/><div class="children"><div class="content">But the point is to focus on the intrinsic dimension[1], not dimensions of the vector itself. I meant dense in the sense that the two are close, relative to another vector where they are not so close. Perhaps a poor choice of words on my part.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intrinsic_dimension" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intrinsic_dimension</a></div><br/><div id="40902783" class="c"><input type="checkbox" id="c-40902783" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40901461">root</a><span>|</span><a href="#40902691">parent</a><span>|</span><a href="#40902661">next</a><span>|</span><label class="collapse" for="c-40902783">[-]</label><label class="expand" for="c-40902783">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I see. Well, the data has an intrinsic dimension of a specific size. You don&#x27;t get to choose that. And, in any case, you want something quite a bit larger than the intrinsic dimension, because deep-learning needs redundancy in its weights in order to train correctly.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40901341" class="c"><input type="checkbox" id="c-40901341" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#40901461">prev</a><span>|</span><a href="#40900467">next</a><span>|</span><label class="collapse" for="c-40901341">[-]</label><label class="expand" for="c-40901341">[2 more]</label></div><br/><div class="children"><div class="content">I read too much of this stuff to dive deep unless someone on HN resoundingly endorses it.<p>From a cursory glance it looks like we’re once again on the verge of realizing that we’re dealing with complex valued weights.<p>Even Anthropic will be publishing that before the year is out.</div><br/></div></div><div id="40900526" class="c"><input type="checkbox" id="c-40900526" checked=""/><div class="controls bullet"><span class="by">bastien2</span><span>|</span><a href="#40900467">prev</a><span>|</span><a href="#40900460">next</a><span>|</span><label class="collapse" for="c-40900526">[-]</label><label class="expand" for="c-40900526">[18 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t &quot;enhance&quot; from zero. LLMs <i>by design</i> are not capable of reason.<p>We can observe LLM-like behaviour in humans: all those reactionaries who just parrot whatever catchphrases mass media programmed into them. LLMs are just the computer version of that uncle who thinks Fox News is true and is the reason your nieces have to wear long pants at family gatherings.<p>He doesn&#x27;t understand the catchphrases he parrots any more than the chatbots do.<p>Actual AI will require a kind of modelling that as yet does not exist.</div><br/><div id="40901337" class="c"><input type="checkbox" id="c-40901337" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40900526">parent</a><span>|</span><a href="#40900549">next</a><span>|</span><label class="collapse" for="c-40901337">[-]</label><label class="expand" for="c-40901337">[4 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs by design are not capable of reason.<p>This isn&#x27;t true.<p>A deep neural network certainly can emulate the logical functions we think of as &quot;reasoning&quot; (ie, AND&#x2F;OR&#x2F;XOR functions).<p>See for example:<p><a href="https:&#x2F;&#x2F;cprimozic.net&#x2F;blog&#x2F;boolean-logic-with-neural-networks&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cprimozic.net&#x2F;blog&#x2F;boolean-logic-with-neural-network...</a><p><a href="https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~axgao&#x2F;cs486686_f21&#x2F;lecture_notes&#x2F;Lecture_08_on_Neural_Networks_1.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~axgao&#x2F;cs486686_f21&#x2F;lecture_notes...</a><p><a href="https:&#x2F;&#x2F;towardsdatascience.com&#x2F;emulating-logical-gates-with-a-neural-network-75c229ec4cc9" rel="nofollow">https:&#x2F;&#x2F;towardsdatascience.com&#x2F;emulating-logical-gates-with-...</a><p><a href="https:&#x2F;&#x2F;medium.com&#x2F;@stanleydukor&#x2F;neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@stanleydukor&#x2F;neural-representation-of-an...</a></div><br/><div id="40902037" class="c"><input type="checkbox" id="c-40902037" checked=""/><div class="controls bullet"><span class="by">amenhotep</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40901337">parent</a><span>|</span><a href="#40900549">next</a><span>|</span><label class="collapse" for="c-40902037">[-]</label><label class="expand" for="c-40902037">[3 more]</label></div><br/><div class="children"><div class="content">What an odd comment. Would you assert also that an 8080 is &quot;capable of reason&quot;?</div><br/><div id="40902332" class="c"><input type="checkbox" id="c-40902332" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40902037">parent</a><span>|</span><a href="#40902380">next</a><span>|</span><label class="collapse" for="c-40902332">[-]</label><label class="expand" for="c-40902332">[1 more]</label></div><br/><div class="children"><div class="content">Usually when people are saying &quot;LLMs can&#x27;t reason&quot; they are claiming they are unable to do logical inference (although the claims are often quite hard to pin down to something specific).<p>Yes, an 8080 is capable of reasoning. Prolog runs well, see for example: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@kenichisasagawa&#x2F;exploring-the-wonders-of-prolog-my-journey-from-the-1980s-31b4df592ef8" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@kenichisasagawa&#x2F;exploring-the-wonders-of...</a></div><br/></div></div><div id="40902380" class="c"><input type="checkbox" id="c-40902380" checked=""/><div class="controls bullet"><span class="by">Kerb_</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40902037">parent</a><span>|</span><a href="#40902332">prev</a><span>|</span><a href="#40900549">next</a><span>|</span><label class="collapse" for="c-40902380">[-]</label><label class="expand" for="c-40902380">[1 more]</label></div><br/><div class="children"><div class="content">I would say integrated circuits in general are not incapable of reason by design, even if some examples may be. Somehow a bunch of meat and fat is capable of reason, even if my steak isn&#x27;t.</div><br/></div></div></div></div></div></div><div id="40900549" class="c"><input type="checkbox" id="c-40900549" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#40900526">parent</a><span>|</span><a href="#40901337">prev</a><span>|</span><a href="#40900663">next</a><span>|</span><label class="collapse" for="c-40900549">[-]</label><label class="expand" for="c-40900549">[6 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs by design are not capable of reason.<p>It is not as clear cut. The argument being, that the patterns they learn in text encodes several layers of abstraction, one of them being <i>some</i> reasoning, as it is encoded in the discourse.</div><br/><div id="40901054" class="c"><input type="checkbox" id="c-40901054" checked=""/><div class="controls bullet"><span class="by">wizzwizz4</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40900549">parent</a><span>|</span><a href="#40900663">next</a><span>|</span><label class="collapse" for="c-40901054">[-]</label><label class="expand" for="c-40901054">[5 more]</label></div><br/><div class="children"><div class="content">They are capable of picking up incredibly crude, noisy versions of first-order symbolic reasoning, and specific, commonly-used arguments, and the context for when those might be applied.<p>Taken together and iterated, you get something vaguely resembling a reasoning algorithm, but your average schoolchild with an NLP library and regular expressions could make a better reasoning algorithm. (While I&#x27;ve been calling these &quot;reasoning algorithms&quot; for analogy&#x27;s sake, they don&#x27;t actually behave how we expect reasoning to behave.)<p>The language model predicts what reasoning might look like. But it doesn&#x27;t actually <i>do the reasoning</i>, so (unless it has something capable of reasoning to guide it), it&#x27;s not going to correctly derive conclusions from premises.</div><br/><div id="40901354" class="c"><input type="checkbox" id="c-40901354" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40901054">parent</a><span>|</span><a href="#40900663">next</a><span>|</span><label class="collapse" for="c-40901354">[-]</label><label class="expand" for="c-40901354">[4 more]</label></div><br/><div class="children"><div class="content">Yes and No. I don&#x27;t entirely disagree with you, but think about when you ask a model to explain step by step a conclusion. It is not doing the reasoning, but in a way abstracted and <i>learned</i> the pattern of doing the reasoning....So it is doing some type of reasoning....and sometimes producing the outcomes that are derived from <i>actual</i> reasoning...Even if defining &quot;actual reasoning&quot; is a whole new challenge.</div><br/><div id="40902117" class="c"><input type="checkbox" id="c-40902117" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40901354">parent</a><span>|</span><a href="#40900663">next</a><span>|</span><label class="collapse" for="c-40902117">[-]</label><label class="expand" for="c-40902117">[3 more]</label></div><br/><div class="children"><div class="content">It took a long time for the limitations of LLMs to &quot;click&quot; for me in my brain.<p>Let&#x27;s say there&#x27;s a student reading 10 books on some topic. They notice that 9 of the books say &quot;A is the answer&quot; and just 1 book says &quot;B is the answer&quot;. From that, the student will conclude and memorise that 90% of authors agree on A and that B is the 10% minority opinion.<p>If you train an LLM on the same data set, then the LLM will learn the same statistical distribution but won&#x27;t be able to articulate it. In other words, if you start off with a generic intro blurb paragraph, it&#x27;ll be able to complete it with the answer &quot;A&quot; 90% of the time and the answer &quot;B&quot; 10% of the time. What it won&#x27;t be able to tell you is what the ratio is between A or B, and it won&#x27;t &quot;know&quot; that B is the minority opinion.<p>Of course, if it reads a &quot;meta review&quot; text during training that talks about A-versus-B and the ratios between them, it&#x27;ll learn <i>that</i>, but it can&#x27;t itself arrive at this conclusion from simply having read the original sources!<p><i>THIS</i> more than anything seems to be the limit of LLM intelligence: they&#x27;re always one level behind humans when trained on the same inputs. They can learn only to reproduce the level of abstraction given to them, they can&#x27;t infer the next level from the inputs.<p>I strongly suspect that this is solvable, but the trillion-dollar question is <i>how?</i> Certainly, vanilla GPT-syle networks cannot do this, something fundamentally new would be required at the training stage. Maybe there needs to be multiple passes over the input data, with secondary passes somehow &quot;meta-training&quot; the model. (If I knew the answer, I&#x27;d be rich!)</div><br/><div id="40903611" class="c"><input type="checkbox" id="c-40903611" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40902117">parent</a><span>|</span><a href="#40902532">next</a><span>|</span><label class="collapse" for="c-40903611">[-]</label><label class="expand" for="c-40903611">[1 more]</label></div><br/><div class="children"><div class="content">But if you give it those 10 books in the prompt, it will be able to spot that 1 of the authors disagreed.</div><br/></div></div><div id="40902532" class="c"><input type="checkbox" id="c-40902532" checked=""/><div class="controls bullet"><span class="by">john-tells-all</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40902117">parent</a><span>|</span><a href="#40903611">prev</a><span>|</span><a href="#40900663">next</a><span>|</span><label class="collapse" for="c-40902532">[-]</label><label class="expand" for="c-40902532">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s really insightful! Thanks.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40900663" class="c"><input type="checkbox" id="c-40900663" checked=""/><div class="controls bullet"><span class="by">bl0rg</span><span>|</span><a href="#40900526">parent</a><span>|</span><a href="#40900549">prev</a><span>|</span><a href="#40901119">next</a><span>|</span><label class="collapse" for="c-40900663">[-]</label><label class="expand" for="c-40900663">[3 more]</label></div><br/><div class="children"><div class="content">Can you explain what it means to reason about something? Since you are so confident I&#x27;m guessing you&#x27;ll find it easy to come up with a non-contrived definition that&#x27;ll clearly include humans and future &quot;actual AI&quot; but exclude LLMs.</div><br/><div id="40901038" class="c"><input type="checkbox" id="c-40901038" checked=""/><div class="controls bullet"><span class="by">stoperaticless</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40900663">parent</a><span>|</span><a href="#40901119">next</a><span>|</span><label class="collapse" for="c-40901038">[-]</label><label class="expand" for="c-40901038">[2 more]</label></div><br/><div class="children"><div class="content">Not the parent, but there are couple of things current AI lack:<p>- learning from single article  &#x2F;book with lasting effect (accumulation of knowledge)<p>- arithmetics without unexpected errors<p>- gauging reliability of information it’s printing<p>BTW. I doubt that you’ll get satisfactory definition of “able to reason” (or “conscious” or “alive” or “chair”). As they define more an end or direction of a spectrum, not an exact cut off point.<p>Current llms are impressive and useful, but given how often they spout nonsense, it is hard to put them into “able to reason” category.</div><br/><div id="40901331" class="c"><input type="checkbox" id="c-40901331" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40901038">parent</a><span>|</span><a href="#40901119">next</a><span>|</span><label class="collapse" for="c-40901331">[-]</label><label class="expand" for="c-40901331">[1 more]</label></div><br/><div class="children"><div class="content">&gt; learning from single article &#x2F;book with lasting effect (accumulation of knowledge)<p>If you mean without training the model, it can be done by using RAG, and allowing LLM to decide what to keep in mind as learnings to later come back to those. There are various techniques for RAG based memory&#x2F;learning. It&#x27;s a combination of querying the memory that is relevant to current goal, as well as method to keep most recent info in memory, as well as compressing, throwing out old info progressively, assigning importance levels to different &quot;memories&quot;. Kind of like humans, honestly.<p>&gt; arithmetics without unexpected errors<p>That&#x27;s a bit handwavy, because humans make very many unexpected errors when doing arithmetics.<p>&gt; gauging reliability of information it’s printing<p>Arguably most people also whatever they output, they are not very good at gauging the reliability. Also you can actually make it do that with proper prompting. You can make it debate itself, and finally let it decide the winning decision and confidence level.</div><br/></div></div></div></div></div></div><div id="40901119" class="c"><input type="checkbox" id="c-40901119" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#40900526">parent</a><span>|</span><a href="#40900663">prev</a><span>|</span><a href="#40900715">next</a><span>|</span><label class="collapse" for="c-40901119">[-]</label><label class="expand" for="c-40901119">[1 more]</label></div><br/><div class="children"><div class="content">Go look at the top comment of this thread: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40900482">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40900482</a><p>That&#x27;s the kind of stuff I want to see when opening a thread on HN, but most of the times we get shallow snark like yours instead. It&#x27;s a shame.</div><br/></div></div><div id="40900715" class="c"><input type="checkbox" id="c-40900715" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40900526">parent</a><span>|</span><a href="#40901119">prev</a><span>|</span><a href="#40900656">next</a><span>|</span><label class="collapse" for="c-40900715">[-]</label><label class="expand" for="c-40900715">[2 more]</label></div><br/><div class="children"><div class="content">LLMs are trained to predict the next word in a sequence. As a result of this training they developed reasoning abilities. Currently these reasoning abilities are roughly at human level, but next gen models (gpt5) should be superior to humans at any reasoning tasks.</div><br/><div id="40901271" class="c"><input type="checkbox" id="c-40901271" checked=""/><div class="controls bullet"><span class="by">soist</span><span>|</span><a href="#40900526">root</a><span>|</span><a href="#40900715">parent</a><span>|</span><a href="#40900656">next</a><span>|</span><label class="collapse" for="c-40901271">[-]</label><label class="expand" for="c-40901271">[1 more]</label></div><br/><div class="children"><div class="content">How did you reach these conclusions and have you validated them by asking these superior artificial agents about whether you&#x27;re correct or not?</div><br/></div></div></div></div><div id="40900656" class="c"><input type="checkbox" id="c-40900656" checked=""/><div class="controls bullet"><span class="by">cowsaymoo</span><span>|</span><a href="#40900526">parent</a><span>|</span><a href="#40900715">prev</a><span>|</span><a href="#40900460">next</a><span>|</span><label class="collapse" for="c-40900656">[-]</label><label class="expand" for="c-40900656">[1 more]</label></div><br/><div class="children"><div class="content">The vocabulary used here doesn&#x27;t have sufficient intrinsic dimension to partition the input into a low loss prediction. Improvement is promising with larger context or denser attention.</div><br/></div></div></div></div><div id="40900460" class="c"><input type="checkbox" id="c-40900460" checked=""/><div class="controls bullet"><span class="by">ChicagoDave</span><span>|</span><a href="#40900526">prev</a><span>|</span><label class="collapse" for="c-40900460">[-]</label><label class="expand" for="c-40900460">[5 more]</label></div><br/><div class="children"><div class="content">LLMs do not have the technology to iteratively solve a complex problem.<p>This is a fact. No graph will change this.<p>You want “reasoning,” then you need to invent a new technology to iterate, validate, experiment, validate, query external expertise, and validate again. When we get that technology, then AI will become resilient in solving complex problems.</div><br/><div id="40900563" class="c"><input type="checkbox" id="c-40900563" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40900460">parent</a><span>|</span><a href="#40902887">next</a><span>|</span><label class="collapse" for="c-40900563">[-]</label><label class="expand" for="c-40900563">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s false. It has been shown that LLMs can perform e.g. gradient descent internally [1], which can explain why they are so good at few shot prompting. The universal approximation theorem already tells us that a single layer is sufficient to approximate any function, so it should come as no surprise that modern deep networks with many layers should be able to perform iterative optimisations.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10559" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10559</a></div><br/><div id="40901502" class="c"><input type="checkbox" id="c-40901502" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40900460">root</a><span>|</span><a href="#40900563">parent</a><span>|</span><a href="#40901319">next</a><span>|</span><label class="collapse" for="c-40901502">[-]</label><label class="expand" for="c-40901502">[1 more]</label></div><br/><div class="children"><div class="content">So you want to encapsulate reason into a simple function?</div><br/></div></div><div id="40901319" class="c"><input type="checkbox" id="c-40901319" checked=""/><div class="controls bullet"><span class="by">ChicagoDave</span><span>|</span><a href="#40900460">root</a><span>|</span><a href="#40900563">parent</a><span>|</span><a href="#40901502">prev</a><span>|</span><a href="#40902887">next</a><span>|</span><label class="collapse" for="c-40901319">[-]</label><label class="expand" for="c-40901319">[1 more]</label></div><br/><div class="children"><div class="content">What’s the success rate once you hit 20 prompts? What’s the success rate if you hit 30 prompts 40 prompts?<p>I’m pretty sure that as you increase the complexity of your questioning, the LLM is just gonna flat out fail and no change to the vector database is going to improve that.</div><br/></div></div></div></div><div id="40902887" class="c"><input type="checkbox" id="c-40902887" checked=""/><div class="controls bullet"><span class="by">ChicagoDave</span><span>|</span><a href="#40900460">parent</a><span>|</span><a href="#40900563">prev</a><span>|</span><label class="collapse" for="c-40902887">[-]</label><label class="expand" for="c-40902887">[1 more]</label></div><br/><div class="children"><div class="content">You have to wonder whose fortunes are tied to the LLM hype.<p>I have no problem pointing out the falsehoods being pushed by these efforts.<p>It may take time for the bubble to burst, but it will.<p>Mark this comment.</div><br/></div></div></div></div></div></div></div></div></div></body></html>