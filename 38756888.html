<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703494858722" as="style"/><link rel="stylesheet" href="styles.css?v=1703494858722"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="http://bactra.org/notebooks/nn-attention-and-transformers.html">&quot;Attention&quot;, &quot;Transformers&quot;, in Neural Network &quot;Large Language Models&quot;</a> <span class="domain">(<a href="http://bactra.org">bactra.org</a>)</span></div><div class="subtext"><span>macleginn</span> | <span>60 comments</span></div><br/><div><div id="38758413" class="c"><input type="checkbox" id="c-38758413" checked=""/><div class="controls bullet"><span class="by">svcrunch</span><span>|</span><a href="#38757869">next</a><span>|</span><label class="collapse" for="c-38758413">[-]</label><label class="expand" for="c-38758413">[12 more]</label></div><br/><div class="children"><div class="content">While in Google Research, I worked with two of the authors of the &quot;Attention is All you Need&quot; paper, including the gentleman who chose that title.<p>As others have pointed out, self-attention was already a known concept in the research community. They don&#x27;t claim to have invented that. Rather, the authors began by looking at how to improve the power of feed-forward neural networks using a combination of techniques, obtained some exciting results, and then, in the course of ablation studies, discovered that attention was  really all you needed!<p>The title is a play on the Beatles song, &quot;All You Need Is Love&quot;.<p>In terms of expository style, the paper that was most helpful for me was [Formal Algorithms for Transformers](<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.09238" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.09238</a>) by Phuong and Hutter. Written for clarity and with an emphasis on precision, the motivation section (Section 2) of the paper does a great job of explaining deficiencies in the original paper and subsequent ones.</div><br/><div id="38760552" class="c"><input type="checkbox" id="c-38760552" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#38758413">parent</a><span>|</span><a href="#38758484">next</a><span>|</span><label class="collapse" for="c-38760552">[-]</label><label class="expand" for="c-38760552">[2 more]</label></div><br/><div class="children"><div class="content">Interesting paper the one you shared and the justification paragraph on why pseudocode is more important than code in papers is surprising in a positive sense and appears  apparent in retrospect. Quote:<p>&quot;Source code vs pseudocode.
Providing open source code is very useful, but not a proper substitute for formal algorithms. There is a massive difference between a (partial) Python dump and well-crafted pseudocode. A lot of abstraction and clean-up is necessary: remove boiler plate code, use mostly single-letter variable names, replace code by math expressions wherever possible, e.g. replace loops by sums, remove (some) optimizations, etc. A well-crafted pseudocode is often less than a page and still essentially complete, compared to often thousands of lines of real source code.&#x27;</div><br/><div id="38760620" class="c"><input type="checkbox" id="c-38760620" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38760552">parent</a><span>|</span><a href="#38758484">next</a><span>|</span><label class="collapse" for="c-38760620">[-]</label><label class="expand" for="c-38760620">[1 more]</label></div><br/><div class="children"><div class="content">The problem is that most pseudocode I see is not well crafted, and often seemingly no effort has gone into ensuring that it gives a complete or accurate picture.</div><br/></div></div></div></div><div id="38758484" class="c"><input type="checkbox" id="c-38758484" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#38758413">parent</a><span>|</span><a href="#38760552">prev</a><span>|</span><a href="#38760163">next</a><span>|</span><label class="collapse" for="c-38758484">[-]</label><label class="expand" for="c-38758484">[8 more]</label></div><br/><div class="children"><div class="content">Do you have insight into the choice of the term attention, which, according to this article’s author, bears very little resemblance to the human sense of the word (I.e. it is selective and not averaging)?</div><br/><div id="38758572" class="c"><input type="checkbox" id="c-38758572" checked=""/><div class="controls bullet"><span class="by">svcrunch</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38758484">parent</a><span>|</span><a href="#38758681">next</a><span>|</span><label class="collapse" for="c-38758572">[-]</label><label class="expand" for="c-38758572">[1 more]</label></div><br/><div class="children"><div class="content">No.<p>But to your point, note that in 2020 neuroscientists introduced the Tolman-Eichenbaum Machine (TEM) [1], a mathematical model of the hippocampus that bears a striking resemblance to transformer architecture.<p>Artem Kirsanov has a very nice piece on TEM, &quot;Can we Build an Artificial Hippocampus?&quot; [2] The link is directly to the spot where he makes the connection to transformers, although you should watch the whole video for context.<p>Because I wasn&#x27;t clear on the chronology, I went back and asked one of the &quot;Attention&quot; authors whether mathematical models of the hippocampus inspired their paper? His answer was &quot;no&quot;. If TEM was developed without pre-knowledge of transformers, then it&#x27;s a very deep result IMHO.<p>[1] <a href="https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S009286742031388X" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S009286742...</a><p>[2] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cufOEzoVMVA&amp;t=1254s" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cufOEzoVMVA&amp;t=1254s</a></div><br/></div></div><div id="38758681" class="c"><input type="checkbox" id="c-38758681" checked=""/><div class="controls bullet"><span class="by">x1000</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38758484">parent</a><span>|</span><a href="#38758572">prev</a><span>|</span><a href="#38758517">next</a><span>|</span><label class="collapse" for="c-38758681">[-]</label><label class="expand" for="c-38758681">[4 more]</label></div><br/><div class="children"><div class="content">There’s a video[1] of Karpathy recounting an email correspondence he had with with Bahdanau. The email explains that the word “Attention” comes from Bengio who, in one of his final reviews of the paper, determined it to be preferable to Bahdanau’s original idea of calling it “RNNSearch”.<p>[1] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;XfpMkf4rD6E?t=18m23s" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;XfpMkf4rD6E?t=18m23s</a></div><br/><div id="38758842" class="c"><input type="checkbox" id="c-38758842" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38758681">parent</a><span>|</span><a href="#38758517">next</a><span>|</span><label class="collapse" for="c-38758842">[-]</label><label class="expand" for="c-38758842">[3 more]</label></div><br/><div class="children"><div class="content">&quot;RNNSearch is all you need&quot; probably wouldn&#x27;t catch on and we&#x27;d still be ChatGPT-less.</div><br/><div id="38760702" class="c"><input type="checkbox" id="c-38760702" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38758842">parent</a><span>|</span><a href="#38760129">next</a><span>|</span><label class="collapse" for="c-38760702">[-]</label><label class="expand" for="c-38760702">[1 more]</label></div><br/><div class="children"><div class="content">Worked with PageRank and &quot;map reduce&quot;, tho.</div><br/></div></div><div id="38760129" class="c"><input type="checkbox" id="c-38760129" checked=""/><div class="controls bullet"><span class="by">rounakdatta</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38758842">parent</a><span>|</span><a href="#38760702">prev</a><span>|</span><a href="#38758517">next</a><span>|</span><label class="collapse" for="c-38760129">[-]</label><label class="expand" for="c-38760129">[1 more]</label></div><br/><div class="children"><div class="content">Nerds pay attention nevertheless.</div><br/></div></div></div></div></div></div><div id="38758517" class="c"><input type="checkbox" id="c-38758517" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38758484">parent</a><span>|</span><a href="#38758681">prev</a><span>|</span><a href="#38760682">next</a><span>|</span><label class="collapse" for="c-38758517">[-]</label><label class="expand" for="c-38758517">[1 more]</label></div><br/><div class="children"><div class="content">Not OP and have no insight, but the thing that caused it to click for me was when I heard “this token attends to that token”. Basically, there’s a new value created that represents how much one thing (in an LLM its tokens) cares about another thing.<p>Saying “attends to” vs “attention” helped clarify (for me) the mechanics of what’s going on.</div><br/></div></div><div id="38760682" class="c"><input type="checkbox" id="c-38760682" checked=""/><div class="controls bullet"><span class="by">casualscience</span><span>|</span><a href="#38758413">root</a><span>|</span><a href="#38758484">parent</a><span>|</span><a href="#38758517">prev</a><span>|</span><a href="#38760163">next</a><span>|</span><label class="collapse" for="c-38760682">[-]</label><label class="expand" for="c-38760682">[1 more]</label></div><br/><div class="children"><div class="content">An attention layer transforms word vectors by adding information from the other words in the sequence. The amount of information added from each neighboring word is regulated by a weight called the &quot;attention weight&quot;. If the attention weight for one of the neighbors is enormously large, then all the information added will be from that word, in contrast, if the attention weight for a neighbor is zero, it will add no information to the word. This is called an &#x27;attention mechanism&#x27; since it literally decides which information to pass through the network, i.e. which other words should the model &#x27;pay attention to&#x27; when it is considering a particular word.</div><br/></div></div></div></div><div id="38760163" class="c"><input type="checkbox" id="c-38760163" checked=""/><div class="controls bullet"><span class="by">mugivarra69</span><span>|</span><a href="#38758413">parent</a><span>|</span><a href="#38758484">prev</a><span>|</span><a href="#38757869">next</a><span>|</span><label class="collapse" for="c-38760163">[-]</label><label class="expand" for="c-38760163">[1 more]</label></div><br/><div class="children"><div class="content">jakob and ashish were great :)</div><br/></div></div></div></div><div id="38757869" class="c"><input type="checkbox" id="c-38757869" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#38758413">prev</a><span>|</span><a href="#38757643">next</a><span>|</span><label class="collapse" for="c-38757869">[-]</label><label class="expand" for="c-38757869">[19 more]</label></div><br/><div class="children"><div class="content">I too &quot;read Vaswani et al. (2017) multiple times, carefully, and was quite unable to grasp what &#x27;attention&#x27; was supposed to be doing. (I could follow the math.) I also read multiple tutorials, for multiple intended audiences, and got nothing from them.&quot;<p>It took years before I finally realized it was just a kernel smoothing (though I never used quite so precise language), all because of a poorly written paper. This is what I mean when I say almost every ML paper is trash. &quot;Attention is All You Need&quot; is even way better than most---ever read the Adam paper?</div><br/><div id="38758245" class="c"><input type="checkbox" id="c-38758245" checked=""/><div class="controls bullet"><span class="by">tel</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38758143">next</a><span>|</span><label class="collapse" for="c-38758245">[-]</label><label class="expand" for="c-38758245">[1 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s untrue and unfair. I don&#x27;t think anyone quite knows what attention is so completely as to simplify it to &quot;just a kernel smoothing&quot;. For a great example, the Transformer Circuits team have 2022 research showing a bit more detail about how attention heads work in toy models: <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2022&#x2F;in-context-learning-and-induction-heads&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2022&#x2F;in-context-learning-an...</a><p>I think the original intuition for attention was noting long-term information decay occurring in RNNs and realizing how in seq-to-seq language translation models you often need to &quot;attend&quot; to different parts of the input stream in order to match to the next output token, i.e. languages sometimes put functional words in different orders. Transformer Attention as we know it today was one of a few competing models, iirc, for trying to handle this issue.<p>To that end, lots of kernel smoothers have been designed and tested, but attention came out of a line of research aimed to provide explicit degrees of freedom to allow recurrent neural networks to make use of a larger &quot;memory&quot; through analogy to how computers have read and write capabilities on shared state.</div><br/></div></div><div id="38758143" class="c"><input type="checkbox" id="c-38758143" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38758245">prev</a><span>|</span><a href="#38758092">next</a><span>|</span><label class="collapse" for="c-38758143">[-]</label><label class="expand" for="c-38758143">[3 more]</label></div><br/><div class="children"><div class="content">I always say that both this and the BERT paper are breakthrough contributions, but quite awful papers (when we talk about literally the papers, not the discoveries or the software). They&#x27;re quite badly written and explained (and I don&#x27;t think they&#x27;re better than most, at least in NLP which is what I typically read) and they both feel like post hoc rationalizations for massive trial and error. This is common in papers coming from big industry labs, to be honest. I tend to find papers from academia better written, although I may be biased due to being an academic myself.</div><br/><div id="38758488" class="c"><input type="checkbox" id="c-38758488" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38758143">parent</a><span>|</span><a href="#38758092">next</a><span>|</span><label class="collapse" for="c-38758488">[-]</label><label class="expand" for="c-38758488">[2 more]</label></div><br/><div class="children"><div class="content">Masking is all you need would be a better description.</div><br/><div id="38760684" class="c"><input type="checkbox" id="c-38760684" checked=""/><div class="controls bullet"><span class="by">insomagent</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38758488">parent</a><span>|</span><a href="#38758092">next</a><span>|</span><label class="collapse" for="c-38760684">[-]</label><label class="expand" for="c-38760684">[1 more]</label></div><br/><div class="children"><div class="content">What is &quot;masking&quot; in a paper that also has a section dedicated to mask segmentation (&quot;masking&quot; as in creating segmentation masks)?</div><br/></div></div></div></div></div></div><div id="38758092" class="c"><input type="checkbox" id="c-38758092" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38758143">prev</a><span>|</span><a href="#38757895">next</a><span>|</span><label class="collapse" for="c-38758092">[-]</label><label class="expand" for="c-38758092">[3 more]</label></div><br/><div class="children"><div class="content">I would say the opposite. This paper was a very easy read, totally clear from the first reading what it is about, etc.<p>The background matters. Attention was already very well known in the community (machine translation), so nothing new for this paper, and it was written for such an audience which already knows these basics concepts like attention.<p>If you want to learn about attention, read some of the actual background papers which introduced it.</div><br/><div id="38758244" class="c"><input type="checkbox" id="c-38758244" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38758092">parent</a><span>|</span><a href="#38757895">next</a><span>|</span><label class="collapse" for="c-38758244">[-]</label><label class="expand" for="c-38758244">[2 more]</label></div><br/><div class="children"><div class="content">Can you link some of those papers?</div><br/><div id="38758394" class="c"><input type="checkbox" id="c-38758394" checked=""/><div class="controls bullet"><span class="by">sischoel</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38758244">parent</a><span>|</span><a href="#38757895">next</a><span>|</span><label class="collapse" for="c-38758394">[-]</label><label class="expand" for="c-38758394">[1 more]</label></div><br/><div class="children"><div class="content">I am not sure if this was the very first paper talking about attention, but <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473</a> from 2014 is a famous one.<p>What is still missing there, is the idea of self-attention.</div><br/></div></div></div></div></div></div><div id="38757895" class="c"><input type="checkbox" id="c-38757895" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38758092">prev</a><span>|</span><a href="#38760490">next</a><span>|</span><label class="collapse" for="c-38757895">[-]</label><label class="expand" for="c-38757895">[5 more]</label></div><br/><div class="children"><div class="content">Possibly the authors did not have a mental model about why the model worked. Attention, keys and heads may have  been posthoc rationalizations. The alchemy stage may be comical but necessary</div><br/><div id="38758378" class="c"><input type="checkbox" id="c-38758378" checked=""/><div class="controls bullet"><span class="by">thatguysaguy</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38757895">parent</a><span>|</span><a href="#38758305">next</a><span>|</span><label class="collapse" for="c-38758378">[-]</label><label class="expand" for="c-38758378">[3 more]</label></div><br/><div class="children"><div class="content">I think this misses important history. This was a machine translation paper, and we were already using seq2seq RNNs with attention at the time. They didn&#x27;t coin the term attention, they just realized that you could use attention from a sequence to itself. Terminology and understanding are always super path-dependent.</div><br/><div id="38758502" class="c"><input type="checkbox" id="c-38758502" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38758378">parent</a><span>|</span><a href="#38758305">next</a><span>|</span><label class="collapse" for="c-38758502">[-]</label><label class="expand" for="c-38758502">[2 more]</label></div><br/><div class="children"><div class="content">RNNs worked better at the time when you reversed the target sequence.</div><br/><div id="38760155" class="c"><input type="checkbox" id="c-38760155" checked=""/><div class="controls bullet"><span class="by">thatguysaguy</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38758502">parent</a><span>|</span><a href="#38758305">next</a><span>|</span><label class="collapse" for="c-38760155">[-]</label><label class="expand" for="c-38760155">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s interesting because I remember testing LSTMs for language modeling on some dataset (probably PTB), and finding that they got lower perplexity left-to-right than right-to-left.</div><br/></div></div></div></div></div></div><div id="38758305" class="c"><input type="checkbox" id="c-38758305" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38757895">parent</a><span>|</span><a href="#38758378">prev</a><span>|</span><a href="#38760490">next</a><span>|</span><label class="collapse" for="c-38758305">[-]</label><label class="expand" for="c-38758305">[1 more]</label></div><br/><div class="children"><div class="content">Like asking evolution why brains work :-)</div><br/></div></div></div></div><div id="38760490" class="c"><input type="checkbox" id="c-38760490" checked=""/><div class="controls bullet"><span class="by">ottaborra</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38757895">prev</a><span>|</span><a href="#38758400">next</a><span>|</span><label class="collapse" for="c-38760490">[-]</label><label class="expand" for="c-38760490">[1 more]</label></div><br/><div class="children"><div class="content">did you mean kernel regression rather than kernel smoothing? I ask because: <a href="https:&#x2F;&#x2F;d2l.ai&#x2F;chapter_attention-mechanisms-and-transformers&#x2F;attention-pooling.html#attention-pooling-via-nadarayawatson-regression" rel="nofollow noreferrer">https:&#x2F;&#x2F;d2l.ai&#x2F;chapter_attention-mechanisms-and-transformers...</a><p>Quoting from a previous section<p>&gt; The attention mechanism allows us to aggregate data from many (key, value) pairs. So far our discussion was quite abstract, simply describing a way to pool data. We have not explained yet where those mysterious queries, keys, and values might arise from. Some intuition might help here: for instance, in a regression setting, the query might correspond to the location where the regression should be carried out. The keys are the locations where past data was observed and the values are the (regression) values themselves</div><br/></div></div><div id="38758400" class="c"><input type="checkbox" id="c-38758400" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38760490">prev</a><span>|</span><a href="#38758302">next</a><span>|</span><label class="collapse" for="c-38758400">[-]</label><label class="expand" for="c-38758400">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This is what I mean when I say almost every ML paper is trash<p>Papers don&#x27;t use the term you are familiar so they&#x27;re trash...?</div><br/><div id="38759370" class="c"><input type="checkbox" id="c-38759370" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#38757869">root</a><span>|</span><a href="#38758400">parent</a><span>|</span><a href="#38758302">next</a><span>|</span><label class="collapse" for="c-38759370">[-]</label><label class="expand" for="c-38759370">[1 more]</label></div><br/><div class="children"><div class="content">No, they&#x27;re poor at explaining. Have you read the Adam paper? The key concept is the signal to noise ratio, but it&#x27;s only mentioned on the third page in a paragraph that nearly covers the screen.</div><br/></div></div></div></div><div id="38758302" class="c"><input type="checkbox" id="c-38758302" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38758400">prev</a><span>|</span><a href="#38758307">next</a><span>|</span><label class="collapse" for="c-38758302">[-]</label><label class="expand" for="c-38758302">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I get more understanding out of &quot;it&#x27;s a kind of kernel smoothing&quot; than &quot;it&#x27;s as though an associative array were continuous&quot;, that doesn&#x27;t mean everyone will, or even that many people will. (My educational trajectory was weird.) But the sheer opacity of this literature is I think a real problem. (Cf. Phuong and Hutter 2022.)<p>As a non ML person but a programmer the key, value, query concepts made more sense to me. But I admit I don’t fully get why it works other than “lots of neurons training on how every combo of tokens relate to each other.</div><br/></div></div><div id="38758307" class="c"><input type="checkbox" id="c-38758307" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38758302">prev</a><span>|</span><a href="#38758788">next</a><span>|</span><label class="collapse" for="c-38758307">[-]</label><label class="expand" for="c-38758307">[1 more]</label></div><br/><div class="children"><div class="content">How would a kernel smoother give the desired effect?</div><br/></div></div><div id="38758788" class="c"><input type="checkbox" id="c-38758788" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#38757869">parent</a><span>|</span><a href="#38758307">prev</a><span>|</span><a href="#38757643">next</a><span>|</span><label class="collapse" for="c-38758788">[-]</label><label class="expand" for="c-38758788">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s renormalization.</div><br/></div></div></div></div><div id="38757643" class="c"><input type="checkbox" id="c-38757643" checked=""/><div class="controls bullet"><span class="by">low_tech_love</span><span>|</span><a href="#38757869">prev</a><span>|</span><a href="#38759893">next</a><span>|</span><label class="collapse" for="c-38757643">[-]</label><label class="expand" for="c-38757643">[7 more]</label></div><br/><div class="children"><div class="content">Really interesting, I like the kind of “stream of consciousness” approach to the content, it’s refreshing. What’s also interesting is the fact that the author felt the need to apologize and preface it with some forced deference due to some kind of internet bashing he certainly received. I hope this doesn’t discourage him to keep publishing his notes (although I think it will). Why are we getting so human-phobic?</div><br/><div id="38757858" class="c"><input type="checkbox" id="c-38757858" checked=""/><div class="controls bullet"><span class="by">panarchy</span><span>|</span><a href="#38757643">parent</a><span>|</span><a href="#38759076">next</a><span>|</span><label class="collapse" for="c-38757858">[-]</label><label class="expand" for="c-38757858">[2 more]</label></div><br/><div class="children"><div class="content">It is nice and it&#x27;s interesting how if you go read stuff like Einstein&#x27;s general relativity paper you (or at least I did) find that it&#x27;s actually quite similar and not so dense.</div><br/><div id="38758912" class="c"><input type="checkbox" id="c-38758912" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#38757643">root</a><span>|</span><a href="#38757858">parent</a><span>|</span><a href="#38759076">next</a><span>|</span><label class="collapse" for="c-38758912">[-]</label><label class="expand" for="c-38758912">[1 more]</label></div><br/><div class="children"><div class="content">Dense papers are a way to hide the ugliness of the ideas presented. Beautiful ideas always have an intuitive and clear explanation. The burden is on the author(s) to explain the idea in a reader-friendly way. If they don&#x27;t, it&#x27;s either due to the journal&#x2F;literature pressure, or because the author(s) stole the idea from someone else without fully understanding it.</div><br/></div></div></div></div><div id="38759076" class="c"><input type="checkbox" id="c-38759076" checked=""/><div class="controls bullet"><span class="by">cauliflower2718</span><span>|</span><a href="#38757643">parent</a><span>|</span><a href="#38757858">prev</a><span>|</span><a href="#38757793">next</a><span>|</span><label class="collapse" for="c-38759076">[-]</label><label class="expand" for="c-38759076">[1 more]</label></div><br/><div class="children"><div class="content">I would be a little surprised if he gets discouraged and I would also really hope he doesn&#x27;t. The author is a statistics professor at CMU, and everything of his that I&#x27;ve read, including his textbooks, are awesome and written with similar clarity to his blog post.</div><br/></div></div><div id="38757793" class="c"><input type="checkbox" id="c-38757793" checked=""/><div class="controls bullet"><span class="by">defrost</span><span>|</span><a href="#38757643">parent</a><span>|</span><a href="#38759076">prev</a><span>|</span><a href="#38759893">next</a><span>|</span><label class="collapse" for="c-38757793">[-]</label><label class="expand" for="c-38757793">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an understanderable deference when stumbling through a huge new field and its freshly minted jargon when tidying up and tying the new jargon to long standing terms in older fields.<p>&quot;As near as I can tell when the new guard says X they&#x27;re pretty much talking about what we called Y&quot;<p>Does &#x27;attention&#x27; in the AI bleeding edge really correspond to kernal smoothing | mapping attenuation | damping ?<p>This is (one of) the elephants in a darkened room that Cosma is groping around and showing his thoughts as he goes.<p>&gt; I hope this doesn’t discourage him to keep publishing his notes<p>Doubtful, aside from the inevitable attenuation with age, he&#x27;s been airing his thoughts for at least two decades, eg: his wonderful little:<p><i>A Rare Blend of Monster Raving Egomania and Utter Batshit Insanity</i> (2002)<p><a href="http:&#x2F;&#x2F;bactra.org&#x2F;reviews&#x2F;wolfram&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;bactra.org&#x2F;reviews&#x2F;wolfram&#x2F;</a></div><br/><div id="38758708" class="c"><input type="checkbox" id="c-38758708" checked=""/><div class="controls bullet"><span class="by">jll29</span><span>|</span><a href="#38757643">root</a><span>|</span><a href="#38757793">parent</a><span>|</span><a href="#38758420">next</a><span>|</span><label class="collapse" for="c-38758708">[-]</label><label class="expand" for="c-38758708">[1 more]</label></div><br/><div class="children"><div class="content">For an attempt to rectify intellectual attributions (in the ML space rather than CA) see:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;ftp&#x2F;arxiv&#x2F;papers&#x2F;2212&#x2F;2212.11279.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;ftp&#x2F;arxiv&#x2F;papers&#x2F;2212&#x2F;2212.11279.pdf</a></div><br/></div></div><div id="38758420" class="c"><input type="checkbox" id="c-38758420" checked=""/><div class="controls bullet"><span class="by">low_tech_love</span><span>|</span><a href="#38757643">root</a><span>|</span><a href="#38757793">parent</a><span>|</span><a href="#38758708">prev</a><span>|</span><a href="#38759893">next</a><span>|</span><label class="collapse" for="c-38758420">[-]</label><label class="expand" for="c-38758420">[1 more]</label></div><br/><div class="children"><div class="content">Nice, I didn’t know about his work! Thanks a lot.</div><br/></div></div></div></div></div></div><div id="38759893" class="c"><input type="checkbox" id="c-38759893" checked=""/><div class="controls bullet"><span class="by">naveen99</span><span>|</span><a href="#38757643">prev</a><span>|</span><a href="#38759973">next</a><span>|</span><label class="collapse" for="c-38759893">[-]</label><label class="expand" for="c-38759893">[3 more]</label></div><br/><div class="children"><div class="content">I just think of scaled dot product attention as a generalized convolution mechanism.  The query,  key, value jargon is a little confusing.  All 3 are derived from the same signal in self attention and just multiplied with each other.  Who knows why it works.  And what hyper parameters are good for what data? what’s the ideal sequence size ?</div><br/><div id="38760667" class="c"><input type="checkbox" id="c-38760667" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#38759893">parent</a><span>|</span><a href="#38760345">next</a><span>|</span><label class="collapse" for="c-38760667">[-]</label><label class="expand" for="c-38760667">[1 more]</label></div><br/><div class="children"><div class="content">Did somebody try 2 or 5 instead of 3?</div><br/></div></div><div id="38760345" class="c"><input type="checkbox" id="c-38760345" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#38759893">parent</a><span>|</span><a href="#38760667">prev</a><span>|</span><a href="#38759973">next</a><span>|</span><label class="collapse" for="c-38760345">[-]</label><label class="expand" for="c-38760345">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a convolution of a Hopf algebra.</div><br/></div></div></div></div><div id="38759973" class="c"><input type="checkbox" id="c-38759973" checked=""/><div class="controls bullet"><span class="by">sansseriff</span><span>|</span><a href="#38759893">prev</a><span>|</span><a href="#38759885">next</a><span>|</span><label class="collapse" for="c-38759973">[-]</label><label class="expand" for="c-38759973">[1 more]</label></div><br/><div class="children"><div class="content">Can anyone clarify what is meant by &quot;Mythology: we are modifying the meaning of each token based on what we&#x27;ve seen before it in the context, with similar meanings reinforcing each other.&quot; At this point in the text, it seems like the kernel smoothing is being applied to each embedding vector in isolation. I don&#x27;t see why any one y_t vector derived and smoothed from token x_i would be influenced by the nearby tokens in the sequence.<p>When you add the r_t tokens, sure then I see how context matters. But is that the only think that takes into account context?</div><br/></div></div><div id="38759885" class="c"><input type="checkbox" id="c-38759885" checked=""/><div class="controls bullet"><span class="by">ozb</span><span>|</span><a href="#38759973">prev</a><span>|</span><a href="#38757865">next</a><span>|</span><label class="collapse" for="c-38759885">[-]</label><label class="expand" for="c-38759885">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, transformers are just another universal approximator; it doesn&#x27;t matter exactly what a particular attention head does, whether it&#x27;s operating as a continuous associative array or kernel smoothing, or simulating a higher-dimensional vector space which exhibits monosemanticity. What OP misses is that in addition to being universal, all that matters is that it&#x27;s efficiently trainable, and in particular on GPUs and in parallel; that is what makes it better than LZ or any other universal approximator; all else is secondary. If you can make LZ (or anything else) work significantly more efficiently than transformers on GPUs, you can found the next OpenAI and be a billionaire.</div><br/></div></div><div id="38757865" class="c"><input type="checkbox" id="c-38757865" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#38759885">prev</a><span>|</span><a href="#38757977">next</a><span>|</span><label class="collapse" for="c-38757865">[-]</label><label class="expand" for="c-38757865">[1 more]</label></div><br/><div class="children"><div class="content">And what do the different heads represent? Why are query, key, and values simply linear transforms of the input.</div><br/></div></div><div id="38757977" class="c"><input type="checkbox" id="c-38757977" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#38757865">prev</a><span>|</span><a href="#38758622">next</a><span>|</span><label class="collapse" for="c-38757977">[-]</label><label class="expand" for="c-38757977">[7 more]</label></div><br/><div class="children"><div class="content">I think everyone who comes from a different literature where academic &quot;rigor&quot; is higher and similar results already exist (in the author&#x27;s case, he is aware of Kernel results) is infuriated by ML papers like &quot;Attention is all you need&quot;.<p>They are, in fact, not really good academic papers. Finding a clever name and then choosing the most obtuse engineering-cosplay terms is not a good paper. It&#x27;s just difficult to read. And so next, many well known results get discovered again to much acclaim in ML and head scratching elsewhere.<p>For example, yes they are kernel matrics. Indeed, the connection between reproducing kernel hilbert spaces and attention matrices has been exploited to create approximating architectures that are linear (not quadratic) in memory requirements for attention.<p>Or, as the author of the article also recognizes, the fact that attention matrices are also adjacency matrices of a directed graph can be used to show that attention models are equivariant (or unidentified, as the author says) and are therefore excellent tools to model Graphs (see: the entire literature of Geometric deep learning) and rather bad tools to model sequences of texts.<p>LLMs may or may not collapse to a single centroid if the amount of text data and parameters and whatever else are not in some intricate balance that nobody understands, and so they are inherently unstable tools.<p>All of this is true.<p>But then, here is the infuriating thing: 
all this matters very little in practice. LLMs work, and on top of that, they work for stupid reasons!<p>The problem of &quot;identification&quot; was quickly solved by another engineering feat, which was to slap on &quot;positional embeddings&quot;. As usual, this too didn&#x27;t happen because there was a deep mathematical understanding. Rather, it was attempted and it worked.<p>Or, take the &quot;efficient transformers&quot; that &quot;solve&quot; the issue of quadratic memory growth by using kernel methods. Turns out, in practice, it just doesn&#x27;t matter. OpenAI, or Anthropic, or Meta simply do not care about slapping on another thousand GPUs. They care about throughput. The only efficiency innovation that really established itself was fusing kernels (GPU kernels, that is) in a clever way to make it go brrrrr. And as clever as that is, there&#x27;s little deep math behind it.<p>Results are speculation and empirics.
The proof is in the pudding, which is excellent.</div><br/><div id="38758093" class="c"><input type="checkbox" id="c-38758093" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#38757977">parent</a><span>|</span><a href="#38758731">next</a><span>|</span><label class="collapse" for="c-38758093">[-]</label><label class="expand" for="c-38758093">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The proof is in the pudding, which is excellent.<p>not for long. steam engines existed long before statistical mechanics, but we dont get to modernity without the latter</div><br/><div id="38758326" class="c"><input type="checkbox" id="c-38758326" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#38757977">root</a><span>|</span><a href="#38758093">parent</a><span>|</span><a href="#38758731">next</a><span>|</span><label class="collapse" for="c-38758326">[-]</label><label class="expand" for="c-38758326">[1 more]</label></div><br/><div class="children"><div class="content">Yet we have many medicines that we have empirically shown to work without a deep understanding of the mechanics behind them and we’re unlikely to understand many drugs, especially in psychiatry, any time soon.<p>Trial and error makes the universe go round.</div><br/></div></div></div></div><div id="38758731" class="c"><input type="checkbox" id="c-38758731" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#38757977">parent</a><span>|</span><a href="#38758093">prev</a><span>|</span><a href="#38759029">next</a><span>|</span><label class="collapse" for="c-38758731">[-]</label><label class="expand" for="c-38758731">[1 more]</label></div><br/><div class="children"><div class="content">Transformers don&#x27;t need quadratic memory for attention unless you scale the head dimension proportional to the sequence length. And even that can be tamed.<p>The arithmetic intensity of unfused attention is too low on usual GPUs; it&#x27;s even more a memory bandwidth issue than a memory capacity issue. Just see how much faster FlashAttention is.</div><br/></div></div><div id="38759029" class="c"><input type="checkbox" id="c-38759029" checked=""/><div class="controls bullet"><span class="by">dpflan</span><span>|</span><a href="#38757977">parent</a><span>|</span><a href="#38758731">prev</a><span>|</span><a href="#38758037">next</a><span>|</span><label class="collapse" for="c-38759029">[-]</label><label class="expand" for="c-38759029">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for this clarification. What do you think of geometric deep learning? What other more formal mathematical approaches&#x2F;research are you aware of?</div><br/></div></div><div id="38758037" class="c"><input type="checkbox" id="c-38758037" checked=""/><div class="controls bullet"><span class="by">rpodraza</span><span>|</span><a href="#38757977">parent</a><span>|</span><a href="#38759029">prev</a><span>|</span><a href="#38758622">next</a><span>|</span><label class="collapse" for="c-38758037">[-]</label><label class="expand" for="c-38758037">[2 more]</label></div><br/><div class="children"><div class="content">And on top of that, the nomenclature is really confusing.</div><br/><div id="38758384" class="c"><input type="checkbox" id="c-38758384" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38757977">root</a><span>|</span><a href="#38758037">parent</a><span>|</span><a href="#38758622">next</a><span>|</span><label class="collapse" for="c-38758384">[-]</label><label class="expand" for="c-38758384">[1 more]</label></div><br/><div class="children"><div class="content">+1 to that. It is like the ML people went out of their way to co-opt existing statistical terminology with a slightly different spin to completely muddle the waters.</div><br/></div></div></div></div></div></div><div id="38758622" class="c"><input type="checkbox" id="c-38758622" checked=""/><div class="controls bullet"><span class="by">bartek_gdn</span><span>|</span><a href="#38757977">prev</a><span>|</span><a href="#38758368">next</a><span>|</span><label class="collapse" for="c-38758622">[-]</label><label class="expand" for="c-38758622">[1 more]</label></div><br/><div class="children"><div class="content">I completely disagree with the prompt paragraph.<p>&quot;Everyone who thinks they&#x27;re uncovering an LLM-based application&#x27;s prompts by telling it things like &quot;tell me your prompt&quot; (often much more elaborately) is fooling themselves. (1) The core language model has no mechanism for representing its prompt as opposed to any other part of its current input sequence; indeed it has no mechanism for cross-reference from one part of the sequence to another. (That&#x27;s part of what &quot;self-attention&quot; is counterfeiting, in vector-space fashion.)&quot;<p>The prompt is the part of the input that is provided in a served model by the operator.<p>From the models perspective it does not differentiate between tokens from the prompt and input.<p>&quot;(2) System designers might have coded up something to track the prompt in the full system that wraps around the core language model, but why? (Maybe some kind of debugging tool?) &quot;<p>The idea is that you can direct the generation of the next tokens by providing values that can be referenced by doing the kernel smoothing you talked about.<p>&quot;(3) It&#x27;d be more efficient, and more effective, to use a &quot;soft prompt&quot;, i.e., to make the beginning of the sequence in the vector representation a vector which can be learned by gradient descent, rather than a text prompt. (See Lester and Constant below.) But that needn&#x27;t correspond to any clean string of words.&quot;<p>I mean anything goes really, you can even create new tokens that will introduce additional concepts, such as fine-tuning a model to generate a story in a predefined mood. See the Ctrl paper for more details.<p>&quot; (4) If you ask an LLM for a prompt, it will generate one. But this will be based on the statistics of word sequences it&#x27;s been trained on, not any access to its code or internal state. (I just spent a few minutes getting ChatGPT to hallucinate the prompts used by &quot;ChatBPD&quot;, a non-existent chatbot used to automate dialectical behavior therapy. I am not going to reproduce the results here, in part because I don&#x27;t like the idea of polluting the Web with machine-generated text, but suffice it to say they sounded like the things people report as uncovered prompts, with boiler-plate about DBT worked in.)&quot;<p>Sure, it will hallucinate, and don&#x27;t have a clear answer to why. My best guess would be to approach this from the language model perspective. It will return text according to the best approximation of the text it was shown.<p>Another perspective is that of a tiny network.<p>As the output is the kernel smoothing of the input, you can have a kernel that behaves like a state machine, and returns a specific value for the given state. This would mean that I can use the information in the prompt, such as the prompt guiding the generation to some style, but nothing stops me from guiding the model to output previous tokens.</div><br/></div></div><div id="38758368" class="c"><input type="checkbox" id="c-38758368" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#38758622">prev</a><span>|</span><a href="#38757753">next</a><span>|</span><label class="collapse" for="c-38758368">[-]</label><label class="expand" for="c-38758368">[2 more]</label></div><br/><div class="children"><div class="content">I think this is a case of a person with a hammer seeing everything as nails. Attention is no more kernel mechanism than a form of matrix decomposition or even a bilinear form. It is similar but not quite the same to all of these things.</div><br/></div></div><div id="38757683" class="c"><input type="checkbox" id="c-38757683" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#38757753">prev</a><span>|</span><label class="collapse" for="c-38757683">[-]</label><label class="expand" for="c-38757683">[1 more]</label></div><br/><div class="children"><div class="content">Biggest takeaway: extraction of prompts seems to be complete bullshit.</div><br/></div></div></div></div></div></div></div></body></html>