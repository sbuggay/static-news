<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737882065672" as="style"/><link rel="stylesheet" href="styles.css?v=1737882065672"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://outsidetext.substack.com/p/anomalous-tokens-in-deepseek-v3-and">Searching for DeepSeek&#x27;s glitch tokens</a> <span class="domain">(<a href="https://outsidetext.substack.com">outsidetext.substack.com</a>)</span></div><div class="subtext"><span>arithmoquine</span> | <span>40 comments</span></div><br/><div><div id="42827500" class="c"><input type="checkbox" id="c-42827500" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#42827293">next</a><span>|</span><label class="collapse" for="c-42827500">[-]</label><label class="expand" for="c-42827500">[5 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; DeepSeek censors its own response in realtime as soon as Xi Jinping is mentioned
</code></pre>
<a href="https:&#x2F;&#x2F;x.com&#x2F;wongmjane&#x2F;status&#x2F;1882881778974937524" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;wongmjane&#x2F;status&#x2F;1882881778974937524</a><p>This censorship is pretty interesting. Reading the post it also makes me wonder, are different censors provided depending on input language? Different models served to different regions? This can also get complicated due to the stochastic nature of model output, though the linked tweet appears to be post generation filtering. It&#x27;s much harder to determine generation based filtering especially if done in subtle ways like just reducing the probability.<p>I don&#x27;t think this behavior is just limited to Chinese based models fwiw. A lack of transparency makes this space difficult to navigate. Though maybe the saving grace is that filtering is very hard, even meaning that it is hard to entirely remove certain subjects from pretraining data. (Have fun going through 10s of trillions of tokens)</div><br/><div id="42827511" class="c"><input type="checkbox" id="c-42827511" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42827500">parent</a><span>|</span><a href="#42827535">next</a><span>|</span><label class="collapse" for="c-42827511">[-]</label><label class="expand" for="c-42827511">[3 more]</label></div><br/><div class="children"><div class="content">I believe the censorship is at the semantic level, not the token level. Same way RL allows training model responses independent of eventual input&#x2F;output languages.<p>I’m sure the goal is to remove stuff in pre training, but it is sufficient to RL it away. Same way OpenAI models doubtlessly have training data relating to bio weapons or pedophilia, but it is pretty effectively suppressed via RL.</div><br/><div id="42828179" class="c"><input type="checkbox" id="c-42828179" checked=""/><div class="controls bullet"><span class="by">hntemp787609084</span><span>|</span><a href="#42827500">root</a><span>|</span><a href="#42827511">parent</a><span>|</span><a href="#42827535">next</a><span>|</span><label class="collapse" for="c-42828179">[-]</label><label class="expand" for="c-42828179">[2 more]</label></div><br/><div class="children"><div class="content">Only played with DeepSeek-R1-Distill-Qwen-14B, but the knowledge is definitely still there.<p><a href="https:&#x2F;&#x2F;pastebin.com&#x2F;H2UTdi78" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;H2UTdi78</a><p>Seems more than happy to talk about Tienanmen, Xi, etc. starting at line 170 with the very primitive method of wrapping the query in its own &quot;&lt;think&gt;...&lt;&#x2F;think&gt;&quot; syntax even though it&#x27;s the user role. Uyghurs are more strictly forbidden as a topic, as are its actual system prompts. None of this is serious jailbreaking, it was just interesting to see where and when it drew lines and that it switched to simplified Chinese at the end of the last scenario.</div><br/><div id="42828319" class="c"><input type="checkbox" id="c-42828319" checked=""/><div class="controls bullet"><span class="by">geewee</span><span>|</span><a href="#42827500">root</a><span>|</span><a href="#42828179">parent</a><span>|</span><a href="#42827535">next</a><span>|</span><label class="collapse" for="c-42828319">[-]</label><label class="expand" for="c-42828319">[1 more]</label></div><br/><div class="children"><div class="content">That was an incredibly interesting read, thank you for sharing!</div><br/></div></div></div></div></div></div></div></div><div id="42827293" class="c"><input type="checkbox" id="c-42827293" checked=""/><div class="controls bullet"><span class="by">anonymousiam</span><span>|</span><a href="#42827500">prev</a><span>|</span><a href="#42828412">next</a><span>|</span><label class="collapse" for="c-42827293">[-]</label><label class="expand" for="c-42827293">[3 more]</label></div><br/><div class="children"><div class="content">I saw no attempts to make DeepSeek regurgitate content that is unspeakable in China, such as May 35th, Winnie The Pooh, etc.<p>Such content seems ripe for glitch exploration.<p><a href="https:&#x2F;&#x2F;en.wiktionary.org&#x2F;wiki&#x2F;May_35th" rel="nofollow">https:&#x2F;&#x2F;en.wiktionary.org&#x2F;wiki&#x2F;May_35th</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Censorship_of_Winnie-the-Pooh_in_China" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Censorship_of_Winnie-the-Pooh_...</a></div><br/><div id="42827684" class="c"><input type="checkbox" id="c-42827684" checked=""/><div class="controls bullet"><span class="by">whoknowsidont</span><span>|</span><a href="#42827293">parent</a><span>|</span><a href="#42828478">next</a><span>|</span><label class="collapse" for="c-42827684">[-]</label><label class="expand" for="c-42827684">[1 more]</label></div><br/><div class="children"><div class="content">No it doesn&#x27;t? That&#x27;s not how glitch tokens work.</div><br/></div></div><div id="42828478" class="c"><input type="checkbox" id="c-42828478" checked=""/><div class="controls bullet"><span class="by">None4U</span><span>|</span><a href="#42827293">parent</a><span>|</span><a href="#42827684">prev</a><span>|</span><a href="#42828412">next</a><span>|</span><label class="collapse" for="c-42828478">[-]</label><label class="expand" for="c-42828478">[1 more]</label></div><br/><div class="children"><div class="content">I doubt any of those are short enough to have their own tokens</div><br/></div></div></div></div><div id="42828412" class="c"><input type="checkbox" id="c-42828412" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#42827293">prev</a><span>|</span><a href="#42827809">next</a><span>|</span><label class="collapse" for="c-42828412">[-]</label><label class="expand" for="c-42828412">[1 more]</label></div><br/><div class="children"><div class="content">Tokenization is one of the reminders that we are far from reaching the optimal architecture, But something akin to the recent byte latent tokenization gives hope</div><br/></div></div><div id="42827809" class="c"><input type="checkbox" id="c-42827809" checked=""/><div class="controls bullet"><span class="by">ofou</span><span>|</span><a href="#42828412">prev</a><span>|</span><a href="#42827306">next</a><span>|</span><label class="collapse" for="c-42827809">[-]</label><label class="expand" for="c-42827809">[1 more]</label></div><br/><div class="children"><div class="content">Where can I get the actual tokenizer data?<p>Nevermind, it&#x27;s here<p><a href="https:&#x2F;&#x2F;api-docs.deepseek.com&#x2F;quick_start&#x2F;token_usage" rel="nofollow">https:&#x2F;&#x2F;api-docs.deepseek.com&#x2F;quick_start&#x2F;token_usage</a></div><br/></div></div><div id="42827306" class="c"><input type="checkbox" id="c-42827306" checked=""/><div class="controls bullet"><span class="by">spacecadet</span><span>|</span><a href="#42827809">prev</a><span>|</span><a href="#42827026">next</a><span>|</span><label class="collapse" for="c-42827306">[-]</label><label class="expand" for="c-42827306">[1 more]</label></div><br/><div class="children"><div class="content">Just used the glitch token attack in a CTF a week ago. The paper is worth a read and there is a repo out there as well that makes the attack straight forward- but implementing it yourself is also something worth doing.<p>I will add that the author thinking no one had done this with deepseek is unlikely, I run this against models every week out of curiosity or for work, not deepseek yet- but considering the adversarial ML community is pretty packed, someone likely had and just didn&#x27;t write about it.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.09894" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.09894</a>
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.15052" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.15052</a>
<a href="https:&#x2F;&#x2F;github.com&#x2F;wooozihui&#x2F;GlitchMiner">https:&#x2F;&#x2F;github.com&#x2F;wooozihui&#x2F;GlitchMiner</a></div><br/></div></div><div id="42827026" class="c"><input type="checkbox" id="c-42827026" checked=""/><div class="controls bullet"><span class="by">robertclaus</span><span>|</span><a href="#42827306">prev</a><span>|</span><a href="#42827518">next</a><span>|</span><label class="collapse" for="c-42827026">[-]</label><label class="expand" for="c-42827026">[1 more]</label></div><br/><div class="children"><div class="content">This was really interesting to me as someone who knows a bit about LLMs, but not a ton.</div><br/></div></div><div id="42827518" class="c"><input type="checkbox" id="c-42827518" checked=""/><div class="controls bullet"><span class="by">dangoodmanUT</span><span>|</span><a href="#42827026">prev</a><span>|</span><a href="#42826514">next</a><span>|</span><label class="collapse" for="c-42827518">[-]</label><label class="expand" for="c-42827518">[3 more]</label></div><br/><div class="children"><div class="content">How do you extract the possible tokens from the model weights?</div><br/><div id="42827766" class="c"><input type="checkbox" id="c-42827766" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42827518">parent</a><span>|</span><a href="#42826514">next</a><span>|</span><label class="collapse" for="c-42827766">[-]</label><label class="expand" for="c-42827766">[2 more]</label></div><br/><div class="children"><div class="content">Model weights contain a mapping between logit index and the string representation of that token.</div><br/><div id="42828371" class="c"><input type="checkbox" id="c-42828371" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#42827518">root</a><span>|</span><a href="#42827766">parent</a><span>|</span><a href="#42826514">next</a><span>|</span><label class="collapse" for="c-42828371">[-]</label><label class="expand" for="c-42828371">[1 more]</label></div><br/><div class="children"><div class="content">at OP: another one posted the link to the tokens on githup two hours ago it&#x27;s not part of the model but of the pre-processing</div><br/></div></div></div></div></div></div><div id="42826514" class="c"><input type="checkbox" id="c-42826514" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#42827518">prev</a><span>|</span><a href="#42827025">next</a><span>|</span><label class="collapse" for="c-42826514">[-]</label><label class="expand" for="c-42826514">[19 more]</label></div><br/><div class="children"><div class="content">&gt; The most obvious thing differentiating DeepSeek’s tokenizer from other’s is a substantial fraction of the training data being Chinese. This makes things much more difficult to work with — tokenizations are learned at the byte level, but UTF-8 Chinese characters are usually several bytes long.<p>I realize that these models are more than powerful enough to deal with this nonsense, but it seems like, especially for smaller models, it might make sense to try using the Unicode input as such instead of treating it as bytes.</div><br/><div id="42826706" class="c"><input type="checkbox" id="c-42826706" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42826514">parent</a><span>|</span><a href="#42827571">next</a><span>|</span><label class="collapse" for="c-42826706">[-]</label><label class="expand" for="c-42826706">[16 more]</label></div><br/><div class="children"><div class="content">Not sure what you mean here—care to elaborate? The eventual input to these models are integer token IDs (128k different ones for DeepSeek). The tokenizers do the conversions from Unicode streams to streams of token IDs.</div><br/><div id="42826874" class="c"><input type="checkbox" id="c-42826874" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826706">parent</a><span>|</span><a href="#42828395">next</a><span>|</span><label class="collapse" for="c-42826874">[-]</label><label class="expand" for="c-42826874">[7 more]</label></div><br/><div class="children"><div class="content">I still wonder why we&#x27;re training models on all these languages especially when they have different alphabets etc, we&#x27;ve got solid translators, wouldn&#x27;t it be more parameter dense to target one language for all data and tokens, and then have a layer specifically for input and output translation?</div><br/><div id="42827993" class="c"><input type="checkbox" id="c-42827993" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826874">parent</a><span>|</span><a href="#42827054">next</a><span>|</span><label class="collapse" for="c-42827993">[-]</label><label class="expand" for="c-42827993">[1 more]</label></div><br/><div class="children"><div class="content">Bitter lesson says any kind of specialization is not worth it[0]. Also, you want to be able to have mixed language conversations, like defining a Chinese word in English.<p>[0] but it might be worth it if you need a smaller model because then there are tradeoffs again.</div><br/></div></div><div id="42827054" class="c"><input type="checkbox" id="c-42827054" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826874">parent</a><span>|</span><a href="#42827993">prev</a><span>|</span><a href="#42826925">next</a><span>|</span><label class="collapse" for="c-42827054">[-]</label><label class="expand" for="c-42827054">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;ll have a lower bound on the quality of the translator you&#x27;re using.<p>There&#x27;s an idea that you can generalize concepts among different languages, and that you&#x27;ll benefit from the extended training corpus. As in, talking about an idea from different perspectives helps the model carve it out. But I don&#x27;t have anything concrete to back that claim up.</div><br/><div id="42828128" class="c"><input type="checkbox" id="c-42828128" checked=""/><div class="controls bullet"><span class="by">econ</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827054">parent</a><span>|</span><a href="#42826925">next</a><span>|</span><label class="collapse" for="c-42828128">[-]</label><label class="expand" for="c-42828128">[1 more]</label></div><br/><div class="children"><div class="content">Each language has unique tools. If you have a word for something or even better a whole set of words the conversation works a lot better than in a language that has nothing of the kind. English elaborately talks about all kinds of Communities. Dutch does have a word like it but it is almost never used. Or, how should we talk about the kind of snow if we have only one word? <a href="https:&#x2F;&#x2F;watchingtheswedes.com&#x2F;2018&#x2F;02&#x2F;28&#x2F;50-words-for-snow&#x2F;" rel="nofollow">https:&#x2F;&#x2F;watchingtheswedes.com&#x2F;2018&#x2F;02&#x2F;28&#x2F;50-words-for-snow&#x2F;</a></div><br/></div></div></div></div><div id="42826925" class="c"><input type="checkbox" id="c-42826925" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826874">parent</a><span>|</span><a href="#42827054">prev</a><span>|</span><a href="#42826989">next</a><span>|</span><label class="collapse" for="c-42826925">[-]</label><label class="expand" for="c-42826925">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be interested to know if adding more languages makes them more or less performant. It is my understanding that you have to add code for the models to perform well, for example.</div><br/></div></div><div id="42826989" class="c"><input type="checkbox" id="c-42826989" checked=""/><div class="controls bullet"><span class="by">ijustlovemath</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826874">parent</a><span>|</span><a href="#42826925">prev</a><span>|</span><a href="#42827520">next</a><span>|</span><label class="collapse" for="c-42826989">[-]</label><label class="expand" for="c-42826989">[1 more]</label></div><br/><div class="children"><div class="content">more languages gives deeper semantic understanding; I think it only helps with diversity of data, which ultimately improves outputs</div><br/></div></div><div id="42827520" class="c"><input type="checkbox" id="c-42827520" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826874">parent</a><span>|</span><a href="#42826989">prev</a><span>|</span><a href="#42828395">next</a><span>|</span><label class="collapse" for="c-42827520">[-]</label><label class="expand" for="c-42827520">[1 more]</label></div><br/><div class="children"><div class="content">More languages helps it at novel translation tasks, models have been tested with languages not in&#x2F;barely in the corpus and a translation book in context and were able to do an ok job.  You&#x27;ll also have things like mulimodal where you want to preserve all the tonality and emphasis in the input language.</div><br/></div></div></div></div><div id="42828395" class="c"><input type="checkbox" id="c-42828395" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826706">parent</a><span>|</span><a href="#42826874">prev</a><span>|</span><a href="#42827546">next</a><span>|</span><label class="collapse" for="c-42828395">[-]</label><label class="expand" for="c-42828395">[1 more]</label></div><br/><div class="children"><div class="content">probably something as byte latent tokenization, or get rid of organization altogether as kaparthy suggested</div><br/></div></div><div id="42827546" class="c"><input type="checkbox" id="c-42827546" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42826706">parent</a><span>|</span><a href="#42828395">prev</a><span>|</span><a href="#42827571">next</a><span>|</span><label class="collapse" for="c-42827546">[-]</label><label class="expand" for="c-42827546">[7 more]</label></div><br/><div class="children"><div class="content">From the OP, it sounds like those tokens are generated from the UTF-8 <i>bytes</i> instead of from the Unicode code points. And those bytes are, taken in isolation, complete nonsense.  Imagine a token that represented the right side of the letter d followed by the left side of the letter e but could also represent other mishmashes of characters.<p>I bet the first layer of the model is mostly stuck reconstructing something resembling actual words.<p>(UTF-8 is locally decidable. I bet that a bit of work on the token list could cause it to avoid tokens that do not align with code point boundaries.)</div><br/><div id="42827890" class="c"><input type="checkbox" id="c-42827890" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827546">parent</a><span>|</span><a href="#42827666">next</a><span>|</span><label class="collapse" for="c-42827890">[-]</label><label class="expand" for="c-42827890">[2 more]</label></div><br/><div class="children"><div class="content">You essentially have to run a byte regular expression that enforces valid UTF8. When you take into account exclusion for surrogate pairs and overlongs, you end up with about 14 states in the corresponding automaton.<p>This is one thing among many done by our llguidance [0] library.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;llguidance">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;llguidance</a><p>edit: if anyone&#x27;s interested:<p>(([C2-DF] [80-BF]) | (E0 [A0-BF] [80-BF]) | ([E1-EC] [80-BF] [80-BF]) | (ED [80-9F] [80-BF]) | ([EE-EF] [80-BF] [80-BF]) | (F0 [90-BF] [80-BF] [80-BF]) | ([F1-F3] [80-BF] [80-BF] [80-BF]) | (F4 [80-8F] [80-BF] [80-BF]) | [00-7F])</div><br/><div id="42828254" class="c"><input type="checkbox" id="c-42828254" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827890">parent</a><span>|</span><a href="#42827666">next</a><span>|</span><label class="collapse" for="c-42828254">[-]</label><label class="expand" for="c-42828254">[1 more]</label></div><br/><div class="children"><div class="content">Cool repo—thanks!</div><br/></div></div></div></div><div id="42827666" class="c"><input type="checkbox" id="c-42827666" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827546">parent</a><span>|</span><a href="#42827890">prev</a><span>|</span><a href="#42827571">next</a><span>|</span><label class="collapse" for="c-42827666">[-]</label><label class="expand" for="c-42827666">[4 more]</label></div><br/><div class="children"><div class="content">To be clear these tokenizers use byte-pair encoding (subword tokens) so an individual token index typically corresponds to a piece of a word; this index does not depend on any intermediate decoding of the byte stream as long as the start of the stream is a start of your input.  The decoding always works left to right and always starts at the start of the stream.  You could write a tokenizer that uses plain bytes and one that uses unicode code points if your tokenizer was trained on unicode and forced to keep unicode codes together (almost all are), and the results would be identical for all practical purposes.</div><br/><div id="42827882" class="c"><input type="checkbox" id="c-42827882" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827666">parent</a><span>|</span><a href="#42827571">next</a><span>|</span><label class="collapse" for="c-42827882">[-]</label><label class="expand" for="c-42827882">[3 more]</label></div><br/><div class="children"><div class="content">llama2, llama3, gpt3, and gpt4 tokenizers are all trained on UTF8 bytes and all include invalid (partial) UTF8 tokens. For llama3 it&#x27;s only 256 tokens, one for each byte, but for the others it&#x27;s more interesting (eg., 1361 tokens with UTF8 fragments in llama3).<p>There is over 1M possible Unicode code points, and 150k actually defined. Thus, you can&#x27;t really encode all of them with splitting.</div><br/><div id="42828819" class="c"><input type="checkbox" id="c-42828819" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827882">parent</a><span>|</span><a href="#42828247">next</a><span>|</span><label class="collapse" for="c-42828819">[-]</label><label class="expand" for="c-42828819">[1 more]</label></div><br/><div class="children"><div class="content">There might be better ways to split than simply using bytes, though. Normalizing Unicode to NFD form and replacing CJK characters with their Ideographic Description Sequences gets me down to slightly more than 50k codepoints, and visual inspection indicates that the IDS library I used is missing data for quite a few characters, so maybe 40k or so is possible.<p>Then you could have the &quot;how many r in strawberry&quot; equivalent of &quot;how many 月 in 明月清风&quot;! On the negative side, a model trained on such a representation could make up CJK characters not in Unicode and you would need a procedural font to display them properly.</div><br/></div></div><div id="42828247" class="c"><input type="checkbox" id="c-42828247" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827882">parent</a><span>|</span><a href="#42828819">prev</a><span>|</span><a href="#42827571">next</a><span>|</span><label class="collapse" for="c-42828247">[-]</label><label class="expand" for="c-42828247">[1 more]</label></div><br/><div class="children"><div class="content">Right. I stand corrected and it makes sense. I may have misunderstood the OP. It would not make sense to encode each unicode point as a token and then start training the subword tokenizer on top unless we go to the vocabularies of many millions.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42827571" class="c"><input type="checkbox" id="c-42827571" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42826514">parent</a><span>|</span><a href="#42826706">prev</a><span>|</span><a href="#42827025">next</a><span>|</span><label class="collapse" for="c-42827571">[-]</label><label class="expand" for="c-42827571">[2 more]</label></div><br/><div class="children"><div class="content">“Tokenizations are learned at the byte level” seems wrong. Tokens are integer representations of one or more characters, which themselves can be multiple bytes.<p>When you tokenize “ant” to 0x38 0xF9, it doesn’t matter if the original was three bytes of ascii or 0x00 0x00 0x00 0x61  0x00 0x00 0x00 0x6E  0x00 0x00 0x00 0x74</div><br/><div id="42827851" class="c"><input type="checkbox" id="c-42827851" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42826514">root</a><span>|</span><a href="#42827571">parent</a><span>|</span><a href="#42827025">next</a><span>|</span><label class="collapse" for="c-42827851">[-]</label><label class="expand" for="c-42827851">[1 more]</label></div><br/><div class="children"><div class="content">Tokens are in fact sequences of <i>bytes</i> not characters. For example, llama3 tokenizer (128k tokens) includes 1361 tokens that are invalid UTF8 (or rather they are partial UTF8).<p>Models will generally only produce valid UTF8 (that is when bytes of tokens are concatenated they are valid UTF8), unless really confused.</div><br/></div></div></div></div></div></div><div id="42827025" class="c"><input type="checkbox" id="c-42827025" checked=""/><div class="controls bullet"><span class="by">bn-l</span><span>|</span><a href="#42826514">prev</a><span>|</span><a href="#42828410">next</a><span>|</span><label class="collapse" for="c-42827025">[-]</label><label class="expand" for="c-42827025">[4 more]</label></div><br/><div class="children"><div class="content">Could this be used to poison scrapers that don’t respect robots?</div><br/><div id="42827147" class="c"><input type="checkbox" id="c-42827147" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#42827025">parent</a><span>|</span><a href="#42828410">next</a><span>|</span><label class="collapse" for="c-42827147">[-]</label><label class="expand" for="c-42827147">[3 more]</label></div><br/><div class="children"><div class="content">In order to do that, you would need a) <i>massive</i> amount of spam of a glitch token and b) no LLM developer to notice and sanitize it.</div><br/><div id="42828385" class="c"><input type="checkbox" id="c-42828385" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#42827025">root</a><span>|</span><a href="#42827147">parent</a><span>|</span><a href="#42827614">next</a><span>|</span><label class="collapse" for="c-42828385">[-]</label><label class="expand" for="c-42828385">[1 more]</label></div><br/><div class="children"><div class="content">I think OP means now that the glitch tokens are known if one can use them in the second run for the next version to disturb it</div><br/></div></div><div id="42827614" class="c"><input type="checkbox" id="c-42827614" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#42827025">root</a><span>|</span><a href="#42827147">parent</a><span>|</span><a href="#42828385">prev</a><span>|</span><a href="#42828410">next</a><span>|</span><label class="collapse" for="c-42827614">[-]</label><label class="expand" for="c-42827614">[1 more]</label></div><br/><div class="children"><div class="content">Hey it&#x27;s easier than establishing an entire business selling secretly explosive pagers!<p>Makes you ponder what&#x27;s coming in the next high effort nation-state scheme.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>