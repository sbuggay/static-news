<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691658067940" as="style"/><link rel="stylesheet" href="styles.css?v=1691658067940"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference">Making AMD GPUs competitive for LLM inference</a> <span class="domain">(<a href="https://blog.mlc.ai">blog.mlc.ai</a>)</span></div><div class="subtext"><span>djoldman</span> | <span>93 comments</span></div><br/><div><div id="37068983" class="c"><input type="checkbox" id="c-37068983" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37073235">next</a><span>|</span><label class="collapse" for="c-37068983">[-]</label><label class="expand" for="c-37068983">[33 more]</label></div><br/><div class="children"><div class="content">One of the authors here. Glad it’s on HackerNews!<p>There are two points I personally wanted to make through this project:<p>1) With a sufficiently optimized software stack, AMD GPUs can be sufficiently cost-efficient to use in LLM serving;
2) ML compilation (MLC) techniques, through its underlying TVM Unity software stack, are the best fit in terms of cross-hardware generalizable performance optimizations, quickly delivering time-to-market values, etc.<p>So far, to the best of our knowledge, MLC LLM delivers the best performance across NVIDIA and AMD GPUs in single-batch inference on quantized models, and batched&#x2F;distributed inference is on the horizon too.</div><br/><div id="37073574" class="c"><input type="checkbox" id="c-37073574" checked=""/><div class="controls bullet"><span class="by">postmeta</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37071909">next</a><span>|</span><label class="collapse" for="c-37073574">[-]</label><label class="expand" for="c-37073574">[1 more]</label></div><br/><div class="children"><div class="content">is this similar to the mosaicml amd MI250 vs nvidia A100 results but with consumer grade hardware?  
<a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;amd-mi250" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;amd-mi250</a><p>might be interesting to team up</div><br/></div></div><div id="37071909" class="c"><input type="checkbox" id="c-37071909" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37073574">prev</a><span>|</span><a href="#37070422">next</a><span>|</span><label class="collapse" for="c-37071909">[-]</label><label class="expand" for="c-37071909">[2 more]</label></div><br/><div class="children"><div class="content">Congrats Junru! I&#x27;m not on AMD but love seeing progress in this project. Excited for batched inference -- I didn&#x27;t think it&#x27;d be useful for me but I&#x27;ve realized batched inference is also useful for a single user &#x2F; edge device workload.<p>Btw - I got biased sampling working in ad-llama! Catching up to guidance slowly but surely :)</div><br/><div id="37072208" class="c"><input type="checkbox" id="c-37072208" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37071909">parent</a><span>|</span><a href="#37070422">next</a><span>|</span><label class="collapse" for="c-37072208">[-]</label><label class="expand" for="c-37072208">[1 more]</label></div><br/><div class="children"><div class="content">This is amazing to hear Steven! (Sorry I locked myself out of discord a couple of days ago...) I&#x27;m sure there&#x27;s bunch of features missing like biased sampling you mentioned, and more than happy to merge PRs if you&#x27;d love to :)</div><br/></div></div></div></div><div id="37070422" class="c"><input type="checkbox" id="c-37070422" checked=""/><div class="controls bullet"><span class="by">htirwklj4523432</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37071909">prev</a><span>|</span><a href="#37072595">next</a><span>|</span><label class="collapse" for="c-37070422">[-]</label><label class="expand" for="c-37070422">[2 more]</label></div><br/><div class="children"><div class="content">The numbers look amazing.<p>Can you comment on how difficult it was to achieve this, and what the relative advantages b&#x2F;w cards ? AFAIR, AMD cards were not not deemed competitive with Nvidia in DL space largely because of the amazing job Nvidia pulled off with CUDNN and its conv. kernels.<p>LLMs etc. OTOH doesn&#x27;t really depend on convolutions (atleast the pure transformer bits), and instead depends a lot more on plain old GEMM + low-bit float&#x2F;int compute.</div><br/><div id="37071799" class="c"><input type="checkbox" id="c-37071799" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37070422">parent</a><span>|</span><a href="#37072595">next</a><span>|</span><label class="collapse" for="c-37071799">[-]</label><label class="expand" for="c-37071799">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Can you comment on how difficult it was to achieve this, and what the relative advantages b&#x2F;w cards?<p>Thanks for asking! I personally believe TVM Unity is a proper software stack for ML compilation (MLC), and its existing optimizations (e.g. TensorCore offloading) can be transparently transferred to AMD&#x2F;Intel&#x2F;Apple&#x2F;mobile GPUs without too much engineering effort.<p>Of course my claim is limited to ML workloads. Not an expert outside the ML world, so I couldn&#x27;t say for general HPC.</div><br/></div></div></div></div><div id="37072595" class="c"><input type="checkbox" id="c-37072595" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37070422">prev</a><span>|</span><a href="#37072186">next</a><span>|</span><label class="collapse" for="c-37072595">[-]</label><label class="expand" for="c-37072595">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for this work. I will be staying on nvidia for now, but applaud any progress towards much needed credible competition in the consumer&#x2F;enthusiast AI hardware space.<p>One question: given your experience, when would you predict a near parity in software stack support between te different platforms, so that a choice of GPU becomes one mostly of price&#x2F;performance? It does not need to be like the AMD&#x2F;Intel in the CPU market where a consumer will have no doubts about software compatibility, but let&#x27;s say like the gaming gpu market where a game having problems on a gpu architecture is a newsworthy exception that is quickly corrected.</div><br/></div></div><div id="37072186" class="c"><input type="checkbox" id="c-37072186" checked=""/><div class="controls bullet"><span class="by">KingOfCoders</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37072595">prev</a><span>|</span><a href="#37069709">next</a><span>|</span><label class="collapse" for="c-37072186">[-]</label><label class="expand" for="c-37072186">[3 more]</label></div><br/><div class="children"><div class="content">Can I use two at the same time? Two 7900 XTX would be the price of 1 4090 but with much higher performance (260tok&#x2F;sec)</div><br/><div id="37072452" class="c"><input type="checkbox" id="c-37072452" checked=""/><div class="controls bullet"><span class="by">sullx</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37072186">parent</a><span>|</span><a href="#37069709">next</a><span>|</span><label class="collapse" for="c-37072452">[-]</label><label class="expand" for="c-37072452">[2 more]</label></div><br/><div class="children"><div class="content">This is coming! Myself and others at OctoML and in the TVM community are actively working on multi-gpu support in the compiler and runtime. Here are some of the merged and active PRs on the multi-GPU (multi-device) roadmap:<p>Support in TVM’s graph IR (Relax) - <a href="https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;tvm&#x2F;pull&#x2F;15447">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;tvm&#x2F;pull&#x2F;15447</a>
Support in TVM’s loop IR (TensorIR) - <a href="https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;tvm&#x2F;pull&#x2F;14862">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;tvm&#x2F;pull&#x2F;14862</a>
Distributed dialect of TVM’s graph IR for multi-node (GSPMD-type): <a href="https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;tvm&#x2F;pull&#x2F;15289">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;tvm&#x2F;pull&#x2F;15289</a><p>The first target will be LLM&#x27;s on multiple NVIDIA GPUs but as with all of MLC-LLM effort, the approach will generalize to other hardware including AMD&#x27;s wonderful hardware.</div><br/><div id="37073076" class="c"><input type="checkbox" id="c-37073076" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37072452">parent</a><span>|</span><a href="#37069709">next</a><span>|</span><label class="collapse" for="c-37073076">[-]</label><label class="expand" for="c-37073076">[1 more]</label></div><br/><div class="children"><div class="content">This exciting, but still it is very apparent more time is needed.</div><br/></div></div></div></div></div></div><div id="37069709" class="c"><input type="checkbox" id="c-37069709" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37072186">prev</a><span>|</span><a href="#37069383">next</a><span>|</span><label class="collapse" for="c-37069709">[-]</label><label class="expand" for="c-37069709">[4 more]</label></div><br/><div class="children"><div class="content">Have you tested Vulkan API on the 7900 XTX? Was it faster or slower than ROCm?</div><br/><div id="37071806" class="c"><input type="checkbox" id="c-37071806" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069709">parent</a><span>|</span><a href="#37069383">next</a><span>|</span><label class="collapse" for="c-37071806">[-]</label><label class="expand" for="c-37071806">[3 more]</label></div><br/><div class="children"><div class="content">Generally speaking I expect Vulkan to be slower than ROCm given it&#x27;s designed for generic gaming across GPU vendors, so the takeaway is, whenever ROCm is available and usable, we should use ROCm. And it&#x27;s the same for CUDA vs Vulkan.</div><br/><div id="37073526" class="c"><input type="checkbox" id="c-37073526" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37071806">parent</a><span>|</span><a href="#37072041">next</a><span>|</span><label class="collapse" for="c-37073526">[-]</label><label class="expand" for="c-37073526">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have any expectations, but there&#x27;re reasons for Vulkan to be faster.<p>It&#x27;s a mature technology used my millions of people every day.<p>Unlike GPGPU compute, for videogames performance directly affects usability.<p>For these reasons, the software on all levels of the stack might be more optimized.</div><br/></div></div><div id="37072041" class="c"><input type="checkbox" id="c-37072041" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37071806">parent</a><span>|</span><a href="#37073526">prev</a><span>|</span><a href="#37069383">next</a><span>|</span><label class="collapse" for="c-37072041">[-]</label><label class="expand" for="c-37072041">[1 more]</label></div><br/><div class="children"><div class="content">What slows it down? Shouldn&#x27;t Vulkan expose compute queues of the GPUs as well?</div><br/></div></div></div></div></div></div><div id="37069383" class="c"><input type="checkbox" id="c-37069383" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37069709">prev</a><span>|</span><a href="#37070465">next</a><span>|</span><label class="collapse" for="c-37069383">[-]</label><label class="expand" for="c-37069383">[7 more]</label></div><br/><div class="children"><div class="content">Did the ROCm 5.6 toolchain work for you out of the box? If not, what sort of hacking &#x2F; hand holding did it need?<p>I don&#x27;t know whether there&#x27;s a LLM inference benchmark in the CI suite, if not perhaps something like this should be included in it.</div><br/><div id="37069446" class="c"><input type="checkbox" id="c-37069446" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069383">parent</a><span>|</span><a href="#37069412">next</a><span>|</span><label class="collapse" for="c-37069446">[-]</label><label class="expand" for="c-37069446">[5 more]</label></div><br/><div class="children"><div class="content">ROCm has improved a lot over the past few months, and now ROCm 5.6 seems to work out of box by just following this tutorial: <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;deploy&#x2F;linux&#x2F;installer&#x2F;install.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;deploy&#x2F;linux&#x2F;installer&#x2F;i...</a>. TVM Unity, the underlying compiler MLC LLM uses, seems to work out of box too on ROCm 5.6 - from Bohan Hou who sets up the environment</div><br/><div id="37073375" class="c"><input type="checkbox" id="c-37073375" checked=""/><div class="controls bullet"><span class="by">fweimer</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069446">parent</a><span>|</span><a href="#37069502">next</a><span>|</span><label class="collapse" for="c-37073375">[-]</label><label class="expand" for="c-37073375">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;release&#x2F;gpu_os_support.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;release&#x2F;gpu_os_support.h...</a> and <a href="https:&#x2F;&#x2F;community.amd.com&#x2F;t5&#x2F;rocm&#x2F;new-rocm-5-6-release-brings-enhancements-and-optimizations-for&#x2F;ba-p&#x2F;614745&#x2F;jump-to&#x2F;first-unread-message" rel="nofollow noreferrer">https:&#x2F;&#x2F;community.amd.com&#x2F;t5&#x2F;rocm&#x2F;new-rocm-5-6-release-bring...</a> suggest that Linux support is really limited at this point. Is this information inaccurate?</div><br/></div></div><div id="37069502" class="c"><input type="checkbox" id="c-37069502" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069446">parent</a><span>|</span><a href="#37073375">prev</a><span>|</span><a href="#37071410">next</a><span>|</span><label class="collapse" for="c-37069502">[-]</label><label class="expand" for="c-37069502">[1 more]</label></div><br/><div class="children"><div class="content">Awesome. I&#x27;m going to paste that into the rocm dev channel. Actual positive feedback on HN, novel and delightful. Thank you for the blog post too!</div><br/></div></div><div id="37071410" class="c"><input type="checkbox" id="c-37071410" checked=""/><div class="controls bullet"><span class="by">kstenerud</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069446">parent</a><span>|</span><a href="#37069502">prev</a><span>|</span><a href="#37069412">next</a><span>|</span><label class="collapse" for="c-37071410">[-]</label><label class="expand" for="c-37071410">[2 more]</label></div><br/><div class="children"><div class="content">Are there any docker images containing this? I&#x27;d like to avoid getting into dependency hell with other software on my system, as happens all too often with new technologies.</div><br/><div id="37072104" class="c"><input type="checkbox" id="c-37072104" checked=""/><div class="controls bullet"><span class="by">scrps</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37071410">parent</a><span>|</span><a href="#37069412">next</a><span>|</span><label class="collapse" for="c-37072104">[-]</label><label class="expand" for="c-37072104">[1 more]</label></div><br/><div class="children"><div class="content">There are thankfully, quite a few, ive mostly used rocm&#x2F;rocm-terminal and rocm&#x2F;rocm-dev.<p><a href="https:&#x2F;&#x2F;hub.docker.com&#x2F;u&#x2F;rocm" rel="nofollow noreferrer">https:&#x2F;&#x2F;hub.docker.com&#x2F;u&#x2F;rocm</a></div><br/></div></div></div></div></div></div><div id="37069412" class="c"><input type="checkbox" id="c-37069412" checked=""/><div class="controls bullet"><span class="by">crowwork</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069383">parent</a><span>|</span><a href="#37069446">prev</a><span>|</span><a href="#37070465">next</a><span>|</span><label class="collapse" for="c-37069412">[-]</label><label class="expand" for="c-37069412">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it works out of box and the blog contains a prebuilt python package that you can try out</div><br/></div></div></div></div><div id="37070465" class="c"><input type="checkbox" id="c-37070465" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37069383">prev</a><span>|</span><a href="#37069187">next</a><span>|</span><label class="collapse" for="c-37070465">[-]</label><label class="expand" for="c-37070465">[4 more]</label></div><br/><div class="children"><div class="content">Thanks! Just curious why there is no &quot;team&quot; or &quot;about us&quot; page? It&#x27;s nice sharing credit, but it also is a little unsettling when blog posts do not name contributors.<p>Good work though. And you have an activity community on github, congratulations.</div><br/><div id="37071811" class="c"><input type="checkbox" id="c-37071811" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37070465">parent</a><span>|</span><a href="#37069187">next</a><span>|</span><label class="collapse" for="c-37071811">[-]</label><label class="expand" for="c-37071811">[3 more]</label></div><br/><div class="children"><div class="content">Well, I&#x27;m very much into true open source, and my belief is that any contributor is automatically part of the team :)</div><br/><div id="37072284" class="c"><input type="checkbox" id="c-37072284" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37071811">parent</a><span>|</span><a href="#37069187">next</a><span>|</span><label class="collapse" for="c-37072284">[-]</label><label class="expand" for="c-37072284">[2 more]</label></div><br/><div class="children"><div class="content">I know plenty of open-source projects who list and thank every individual contributor. The website could do that too!</div><br/><div id="37072720" class="c"><input type="checkbox" id="c-37072720" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37072284">parent</a><span>|</span><a href="#37069187">next</a><span>|</span><label class="collapse" for="c-37072720">[-]</label><label class="expand" for="c-37072720">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a great idea! We should dig around and see if there&#x27;s any plugin to use</div><br/></div></div></div></div></div></div></div></div><div id="37069187" class="c"><input type="checkbox" id="c-37069187" checked=""/><div class="controls bullet"><span class="by">tails4e</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37070465">prev</a><span>|</span><a href="#37069639">next</a><span>|</span><label class="collapse" for="c-37069187">[-]</label><label class="expand" for="c-37069187">[4 more]</label></div><br/><div class="children"><div class="content">When you say best performance on nvidia, do you mean against any other method of running this model an nvidia card?</div><br/><div id="37070758" class="c"><input type="checkbox" id="c-37070758" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069187">parent</a><span>|</span><a href="#37069409">next</a><span>|</span><label class="collapse" for="c-37070758">[-]</label><label class="expand" for="c-37070758">[2 more]</label></div><br/><div class="children"><div class="content">I can confirm this, mlc is shockingly fast on my RTX 2060.<p>The catch is:<p>- MLC&#x27;s quantization is somewhat different (though I havent run any perplexity tests yet)<p>- There is no CPU offloading (or splitting onto an IGP) like Llama.cpp yet (unless its new and I missed it).</div><br/><div id="37072174" class="c"><input type="checkbox" id="c-37072174" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37070758">parent</a><span>|</span><a href="#37069409">next</a><span>|</span><label class="collapse" for="c-37072174">[-]</label><label class="expand" for="c-37072174">[1 more]</label></div><br/><div class="children"><div class="content">True and there are some other issues to be addressed. Those two particular issue is on our roadmap.<p>Regarding quantization, we wanted to develop a code path that absorbs any quantization formats, for example, those from GGML or GPTQ, so that they could be all used. ML compilation (MLC) is agnostic to any quantization formats, but we just haven&#x27;t exposed such abstractions yet.<p>On CPU offloading, imagine if you are writing PyTorch, it should be as simple as a one-liner `some_tensor.cpu()` to bring something down to host memory, and `some_tensor.cuda()` to get it back to CUDA - seems a low-hanging fruit but it&#x27;s not implemented yet in MLC LLM :( Lots of stuff to do and we should make this happen soon.</div><br/></div></div></div></div><div id="37069409" class="c"><input type="checkbox" id="c-37069409" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069187">parent</a><span>|</span><a href="#37070758">prev</a><span>|</span><a href="#37069639">next</a><span>|</span><label class="collapse" for="c-37069409">[-]</label><label class="expand" for="c-37069409">[1 more]</label></div><br/><div class="children"><div class="content">yeah we tried out popular solutions like exllama and llama.cpp among others that support inference of 4bit quantized models</div><br/></div></div></div></div><div id="37069639" class="c"><input type="checkbox" id="c-37069639" checked=""/><div class="controls bullet"><span class="by">melony</span><span>|</span><a href="#37068983">parent</a><span>|</span><a href="#37069187">prev</a><span>|</span><a href="#37073235">next</a><span>|</span><label class="collapse" for="c-37069639">[-]</label><label class="expand" for="c-37069639">[4 more]</label></div><br/><div class="children"><div class="content">Does it work with WSL2?</div><br/><div id="37069825" class="c"><input type="checkbox" id="c-37069825" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069639">parent</a><span>|</span><a href="#37070390">next</a><span>|</span><label class="collapse" for="c-37069825">[-]</label><label class="expand" for="c-37069825">[1 more]</label></div><br/><div class="children"><div class="content">Really depends on how good ROCm support for WSL2 is. Our team don&#x27;t have a windows machine so could not verify ourselves, but if you got ROCm set up properly on WSL2, MLC LLM should work out of the box</div><br/></div></div><div id="37070390" class="c"><input type="checkbox" id="c-37070390" checked=""/><div class="controls bullet"><span class="by">crowwork</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069639">parent</a><span>|</span><a href="#37069825">prev</a><span>|</span><a href="#37071813">next</a><span>|</span><label class="collapse" for="c-37070390">[-]</label><label class="expand" for="c-37070390">[1 more]</label></div><br/><div class="children"><div class="content">You can also try out the vulkan backend, which we know should work for windows, although speed might be slower than rocm</div><br/></div></div><div id="37071813" class="c"><input type="checkbox" id="c-37071813" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#37068983">root</a><span>|</span><a href="#37069639">parent</a><span>|</span><a href="#37070390">prev</a><span>|</span><a href="#37073235">next</a><span>|</span><label class="collapse" for="c-37071813">[-]</label><label class="expand" for="c-37071813">[1 more]</label></div><br/><div class="children"><div class="content">FWIW I did get the CUDA backend running via WSL2</div><br/></div></div></div></div></div></div><div id="37073235" class="c"><input type="checkbox" id="c-37073235" checked=""/><div class="controls bullet"><span class="by">MezzoDelCammin</span><span>|</span><a href="#37068983">prev</a><span>|</span><a href="#37068901">next</a><span>|</span><label class="collapse" for="c-37073235">[-]</label><label class="expand" for="c-37073235">[1 more]</label></div><br/><div class="children"><div class="content">Nice writeup. A minor nitpick though - why is the price of the 3090Ti stated as almost 2k USD? I guess the card might be hard to get new and this might indeed be the price somewhere, but since it&#x27;s a previous generation, in my opinion it would make sense to use the price of a 2nd hand (probably under 1k USD, depending on place &#x2F; luck).<p>BTW - that&#x27;s exactly the dillema I&#x27;m pondering now when thinking about a new PC build to play with some machine learning &#x2F; LLMs. In my personal situation, 7900XTX and 3090Ti are about the same price &#x2F; TDP &#x2F; profile. Almost the only difference is then CUDA vs ROCm and the fact that one is new and might therefore give me some basic warranty.</div><br/></div></div><div id="37068901" class="c"><input type="checkbox" id="c-37068901" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#37073235">prev</a><span>|</span><a href="#37068558">next</a><span>|</span><label class="collapse" for="c-37068901">[-]</label><label class="expand" for="c-37068901">[25 more]</label></div><br/><div class="children"><div class="content">&gt; AMD GPUs using ROCm<p>Oh great. The AMD RX 580 was released in April 2018. AMD had already dropped ROCm support for it by 2021. They only supported the card for 3 years. 3 years. It&#x27;s so lame it&#x27;s bordering on fraudulent, even if not legally fraud. Keep this in mind when reading this news. The support won&#x27;t last long, especially if you don&#x27;t buy at launch. Then you&#x27;ll be stuck in the dependency hell that is trying to use old drivers&#x2F;stack.</div><br/><div id="37069767" class="c"><input type="checkbox" id="c-37069767" checked=""/><div class="controls bullet"><span class="by">greenknight</span><span>|</span><a href="#37068901">parent</a><span>|</span><a href="#37069014">next</a><span>|</span><label class="collapse" for="c-37069767">[-]</label><label class="expand" for="c-37069767">[3 more]</label></div><br/><div class="children"><div class="content">&gt; AMD RX 580 was released in April 2018<p>It was actually Apr 18, 2017 -- <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Radeon_500_series" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Radeon_500_series</a></div><br/><div id="37072054" class="c"><input type="checkbox" id="c-37072054" checked=""/><div class="controls bullet"><span class="by">elabajaba</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069767">parent</a><span>|</span><a href="#37069014">next</a><span>|</span><label class="collapse" for="c-37072054">[-]</label><label class="expand" for="c-37072054">[2 more]</label></div><br/><div class="children"><div class="content">And the 580 was just a rebranded 480, which was released in June 2016.</div><br/><div id="37073029" class="c"><input type="checkbox" id="c-37073029" checked=""/><div class="controls bullet"><span class="by">ktta</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37072054">parent</a><span>|</span><a href="#37069014">next</a><span>|</span><label class="collapse" for="c-37073029">[-]</label><label class="expand" for="c-37073029">[1 more]</label></div><br/><div class="children"><div class="content">This just makes the case for supporting it even further, because then they&#x27;d be supporting multiple years worth of hardware with just the effort for one uarch</div><br/></div></div></div></div></div></div><div id="37069014" class="c"><input type="checkbox" id="c-37069014" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068901">parent</a><span>|</span><a href="#37069767">prev</a><span>|</span><a href="#37068920">next</a><span>|</span><label class="collapse" for="c-37069014">[-]</label><label class="expand" for="c-37069014">[4 more]</label></div><br/><div class="children"><div class="content">tbh im not sure what amds plan is on ROCm support on consumer devices, but i dont really think amd is being fraudulent or something.<p>Both rocm and vulkan are supported in MLC LLM as mentioned in our blog post. we are aware that rocm is not sufficient to cover consumer hardwares, and in this case vulkan is a nice backup!</div><br/><div id="37073077" class="c"><input type="checkbox" id="c-37073077" checked=""/><div class="controls bullet"><span class="by">ttctciyf</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069014">parent</a><span>|</span><a href="#37069289">next</a><span>|</span><label class="collapse" for="c-37073077">[-]</label><label class="expand" for="c-37073077">[1 more]</label></div><br/><div class="children"><div class="content">If you click the &quot;Radeon&quot; tab here[1], dated 27 Jul, AMD claim ROCm support on a wide range of consumer cards, with HIP SDK support on RX 6800 and up, under Windows. The Linux situation seems less clear.<p>1: <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;release&#x2F;windows_support.html#windows-supported-gpus" rel="nofollow noreferrer">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;release&#x2F;windows_support....</a></div><br/></div></div><div id="37069289" class="c"><input type="checkbox" id="c-37069289" checked=""/><div class="controls bullet"><span class="by">zorgmonkey</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069014">parent</a><span>|</span><a href="#37073077">prev</a><span>|</span><a href="#37068920">next</a><span>|</span><label class="collapse" for="c-37069289">[-]</label><label class="expand" for="c-37069289">[2 more]</label></div><br/><div class="children"><div class="content">How does the performance with Vulkan compare to the ROCm performance on the same hardware?</div><br/><div id="37071816" class="c"><input type="checkbox" id="c-37071816" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069289">parent</a><span>|</span><a href="#37068920">next</a><span>|</span><label class="collapse" for="c-37071816">[-]</label><label class="expand" for="c-37071816">[1 more]</label></div><br/><div class="children"><div class="content">We haven&#x27;t done any comparison them yet, but generally we believe Vulkan as a more generic cross-vendor API should be slower than ROCm. Same for CUDA vs Vulkan.</div><br/></div></div></div></div></div></div><div id="37068920" class="c"><input type="checkbox" id="c-37068920" checked=""/><div class="controls bullet"><span class="by">crowwork</span><span>|</span><a href="#37068901">parent</a><span>|</span><a href="#37069014">prev</a><span>|</span><a href="#37069659">next</a><span>|</span><label class="collapse" for="c-37068920">[-]</label><label class="expand" for="c-37068920">[1 more]</label></div><br/><div class="children"><div class="content">There is also vulkan support which should be more universal(also included in the post), for example, the post also shows running LLM on a steamdeck APU.</div><br/></div></div><div id="37069659" class="c"><input type="checkbox" id="c-37069659" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37068901">parent</a><span>|</span><a href="#37068920">prev</a><span>|</span><a href="#37068558">next</a><span>|</span><label class="collapse" for="c-37069659">[-]</label><label class="expand" for="c-37069659">[16 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not going to find rx580&#x27;s with enough vram for AI. Typically 4-8gb.</div><br/><div id="37070293" class="c"><input type="checkbox" id="c-37070293" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069659">parent</a><span>|</span><a href="#37069768">next</a><span>|</span><label class="collapse" for="c-37070293">[-]</label><label class="expand" for="c-37070293">[5 more]</label></div><br/><div class="children"><div class="content">I am currently using my RX 580 8GB for running large language models on my home computer using llama.cpp opencl (clBLAST) offloading of layers. I can fit up to 13 billion parameter llamas (1 or 2) if they&#x27;re quantized at 4 bits. It&#x27;s not super fast but at least my AI IRC bots aren&#x27;t eating into my CPU time anymore.<p>But my attempts to get direct ROCm support were thwarted by AMD.</div><br/><div id="37070362" class="c"><input type="checkbox" id="c-37070362" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070293">parent</a><span>|</span><a href="#37069768">next</a><span>|</span><label class="collapse" for="c-37070362">[-]</label><label class="expand" for="c-37070362">[4 more]</label></div><br/><div class="children"><div class="content">Great for home use, zero commercial value. Can&#x27;t expect AMD to invest time&#x2F;money into ROCm for that.</div><br/><div id="37070416" class="c"><input type="checkbox" id="c-37070416" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070362">parent</a><span>|</span><a href="#37071979">next</a><span>|</span><label class="collapse" for="c-37070416">[-]</label><label class="expand" for="c-37070416">[2 more]</label></div><br/><div class="children"><div class="content">You can say the same thing about a 24GB consumer card. Going from being able to run 13B llamas to 33B doesn&#x27;t really help you in a commercial sense. This holds true, generally, for other LLM foundational models as well. To do commercial work you&#x27;re going to need more RAM than consumer cards have. You need at least two if you&#x27;re going to run the 70B and even then the 70B (and similar) aren&#x27;t useful commercially. Except in the gathering money from investors who don&#x27;t know better sense.</div><br/><div id="37071252" class="c"><input type="checkbox" id="c-37071252" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070416">parent</a><span>|</span><a href="#37071979">next</a><span>|</span><label class="collapse" for="c-37071252">[-]</label><label class="expand" for="c-37071252">[1 more]</label></div><br/><div class="children"><div class="content">No one is arguing any of that. You&#x27;re the one that brought up the 580 specifically.<p>By the way, still waiting for you to take me up on your &#x27;bet&#x27;.</div><br/></div></div></div></div><div id="37071979" class="c"><input type="checkbox" id="c-37071979" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070362">parent</a><span>|</span><a href="#37070416">prev</a><span>|</span><a href="#37069768">next</a><span>|</span><label class="collapse" for="c-37071979">[-]</label><label class="expand" for="c-37071979">[1 more]</label></div><br/><div class="children"><div class="content">Home use is how you get employees that push your products at work. The lack of focus on home use is AMD&#x27;s biggest ML weakness.</div><br/></div></div></div></div></div></div><div id="37069768" class="c"><input type="checkbox" id="c-37069768" checked=""/><div class="controls bullet"><span class="by">FirmwareBurner</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069659">parent</a><span>|</span><a href="#37070293">prev</a><span>|</span><a href="#37068558">next</a><span>|</span><label class="collapse" for="c-37069768">[-]</label><label class="expand" for="c-37069768">[10 more]</label></div><br/><div class="children"><div class="content">ROCm is not just for AI.</div><br/><div id="37069884" class="c"><input type="checkbox" id="c-37069884" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069768">parent</a><span>|</span><a href="#37068558">next</a><span>|</span><label class="collapse" for="c-37069884">[-]</label><label class="expand" for="c-37069884">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been waiting for someone to tell me what I can profitably do with the 120,000+ 470&#x2F;480&#x2F;580&#x27;s that I have sitting around doing nothing. It sounds like you have an idea?</div><br/><div id="37072653" class="c"><input type="checkbox" id="c-37072653" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069884">parent</a><span>|</span><a href="#37070371">next</a><span>|</span><label class="collapse" for="c-37072653">[-]</label><label class="expand" for="c-37072653">[1 more]</label></div><br/><div class="children"><div class="content">You could sell them or give them away to hobbyists, but that could eat into the lucrative “shoehorning old crypto mining into unrelated conversations” business</div><br/></div></div><div id="37070371" class="c"><input type="checkbox" id="c-37070371" checked=""/><div class="controls bullet"><span class="by">FirmwareBurner</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069884">parent</a><span>|</span><a href="#37072653">prev</a><span>|</span><a href="#37072360">next</a><span>|</span><label class="collapse" for="c-37070371">[-]</label><label class="expand" for="c-37070371">[5 more]</label></div><br/><div class="children"><div class="content">Crack passwords, also there was this craze a few years ago, where people were using their GPUS to crunch numbers and earn money, I forgot what it was called ... something with mines.</div><br/><div id="37071586" class="c"><input type="checkbox" id="c-37071586" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070371">parent</a><span>|</span><a href="#37070410">next</a><span>|</span><label class="collapse" for="c-37071586">[-]</label><label class="expand" for="c-37071586">[1 more]</label></div><br/><div class="children"><div class="content">Good luck trying to make enough money to pay for power let alone capex</div><br/></div></div><div id="37070410" class="c"><input type="checkbox" id="c-37070410" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070371">parent</a><span>|</span><a href="#37071586">prev</a><span>|</span><a href="#37072360">next</a><span>|</span><label class="collapse" for="c-37070410">[-]</label><label class="expand" for="c-37070410">[3 more]</label></div><br/><div class="children"><div class="content">Right... any legal ideas you have?</div><br/><div id="37070603" class="c"><input type="checkbox" id="c-37070603" checked=""/><div class="controls bullet"><span class="by">FirmwareBurner</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070410">parent</a><span>|</span><a href="#37072360">next</a><span>|</span><label class="collapse" for="c-37070603">[-]</label><label class="expand" for="c-37070603">[2 more]</label></div><br/><div class="children"><div class="content">Cracking passwords is legal if you obtained the hashes legally as part of your pentest conttract. So is shitcoin mining.<p>But you seem dead set that there are no uses for ROCm so I&#x27;ll leave you there.</div><br/><div id="37071240" class="c"><input type="checkbox" id="c-37071240" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37070603">parent</a><span>|</span><a href="#37072360">next</a><span>|</span><label class="collapse" for="c-37071240">[-]</label><label class="expand" for="c-37071240">[1 more]</label></div><br/><div class="children"><div class="content">If the best you can do is &#x27;password cracking&#x27; as your answer, you&#x27;re obviously not very well versed in things. Plus, you don&#x27;t need ROCm to crack passwords.</div><br/></div></div></div></div></div></div></div></div><div id="37072360" class="c"><input type="checkbox" id="c-37072360" checked=""/><div class="controls bullet"><span class="by">Arelius</span><span>|</span><a href="#37068901">root</a><span>|</span><a href="#37069884">parent</a><span>|</span><a href="#37070371">prev</a><span>|</span><a href="#37071133">next</a><span>|</span><label class="collapse" for="c-37072360">[-]</label><label class="expand" for="c-37072360">[1 more]</label></div><br/><div class="children"><div class="content">I mean, I&#x27;m using ROCm for VFX rendering. But regardless i&#x27;m not sure that cards as old as your 470&#x27;s can really be super competitive power usage wise to make them very profitable.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37068558" class="c"><input type="checkbox" id="c-37068558" checked=""/><div class="controls bullet"><span class="by">tails4e</span><span>|</span><a href="#37068901">prev</a><span>|</span><a href="#37072534">next</a><span>|</span><label class="collapse" for="c-37068558">[-]</label><label class="expand" for="c-37068558">[9 more]</label></div><br/><div class="children"><div class="content">Being 90% of a 4090 makes the 7900 xtx very attractive in a cost per compute perspective, as its about 65%, and power is significantly lower too</div><br/><div id="37073581" class="c"><input type="checkbox" id="c-37073581" checked=""/><div class="controls bullet"><span class="by">MezzoDelCammin</span><span>|</span><a href="#37068558">parent</a><span>|</span><a href="#37069837">next</a><span>|</span><label class="collapse" for="c-37073581">[-]</label><label class="expand" for="c-37073581">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not the 4090 that the 7900 XTX should be compared to. It&#x27;s the 3090Ti. They could be had for just about the same price when You consider 2nd hand cards.</div><br/></div></div><div id="37069837" class="c"><input type="checkbox" id="c-37069837" checked=""/><div class="controls bullet"><span class="by">tbruckner</span><span>|</span><a href="#37068558">parent</a><span>|</span><a href="#37073581">prev</a><span>|</span><a href="#37069016">next</a><span>|</span><label class="collapse" for="c-37069837">[-]</label><label class="expand" for="c-37069837">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it closer to 80%? Where is it 90% of a 4090?</div><br/><div id="37070175" class="c"><input type="checkbox" id="c-37070175" checked=""/><div class="controls bullet"><span class="by">tails4e</span><span>|</span><a href="#37068558">root</a><span>|</span><a href="#37069837">parent</a><span>|</span><a href="#37069888">next</a><span>|</span><label class="collapse" for="c-37070175">[-]</label><label class="expand" for="c-37070175">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, my mistake. Still cost and power per compute still favours AMD</div><br/></div></div><div id="37069888" class="c"><input type="checkbox" id="c-37069888" checked=""/><div class="controls bullet"><span class="by">meragrin_</span><span>|</span><a href="#37068558">root</a><span>|</span><a href="#37069837">parent</a><span>|</span><a href="#37070175">prev</a><span>|</span><a href="#37069016">next</a><span>|</span><label class="collapse" for="c-37069888">[-]</label><label class="expand" for="c-37069888">[1 more]</label></div><br/><div class="children"><div class="content">Article specifically says 80%.  My guess is mixing up the numbers between 4090 and 3090 TI.</div><br/></div></div></div></div><div id="37069016" class="c"><input type="checkbox" id="c-37069016" checked=""/><div class="controls bullet"><span class="by">nkingsy</span><span>|</span><a href="#37068558">parent</a><span>|</span><a href="#37069837">prev</a><span>|</span><a href="#37072534">next</a><span>|</span><label class="collapse" for="c-37069016">[-]</label><label class="expand" for="c-37069016">[4 more]</label></div><br/><div class="children"><div class="content">brutal slandering of AMD marketing tactics disguised as a benchmark summary:<p><a href="https:&#x2F;&#x2F;gpu.userbenchmark.com&#x2F;AMD-RX-7900-XT&#x2F;Rating&#x2F;4141" rel="nofollow noreferrer">https:&#x2F;&#x2F;gpu.userbenchmark.com&#x2F;AMD-RX-7900-XT&#x2F;Rating&#x2F;4141</a></div><br/><div id="37069262" class="c"><input type="checkbox" id="c-37069262" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37068558">root</a><span>|</span><a href="#37069016">parent</a><span>|</span><a href="#37069270">next</a><span>|</span><label class="collapse" for="c-37069262">[-]</label><label class="expand" for="c-37069262">[1 more]</label></div><br/><div class="children"><div class="content">It has been a while since I saw anyone send a Userbenchmark link. Userbenchmark is not a site with a good reputation, you can find many reasons online why this is, but I guess the site moderators just ignore it by pretending it&#x27;s &quot;Advanced Marketing&quot;, meanwhile even the CPU competitor of AMD has banned the site from their subreddit, one of my favorite explanation <a href="https:&#x2F;&#x2F;youtu.be&#x2F;RQSBj2LKkWg" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;RQSBj2LKkWg</a> but there are of course more recent ones.</div><br/></div></div><div id="37069270" class="c"><input type="checkbox" id="c-37069270" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#37068558">root</a><span>|</span><a href="#37069016">parent</a><span>|</span><a href="#37069262">prev</a><span>|</span><a href="#37069373">next</a><span>|</span><label class="collapse" for="c-37069270">[-]</label><label class="expand" for="c-37069270">[1 more]</label></div><br/><div class="children"><div class="content">Really? you are referring to userbenchmark&#x27;s assessment of AMD?<p>Look at literally any of their assessments for AMD products and they&#x27;ll be dragging them through the mud while pointing to worse performing intel products to flex how much better they are because they aren&#x27;t AMD.</div><br/></div></div><div id="37069373" class="c"><input type="checkbox" id="c-37069373" checked=""/><div class="controls bullet"><span class="by">zdw</span><span>|</span><a href="#37068558">root</a><span>|</span><a href="#37069016">parent</a><span>|</span><a href="#37069270">prev</a><span>|</span><a href="#37072534">next</a><span>|</span><label class="collapse" for="c-37069373">[-]</label><label class="expand" for="c-37069373">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a reason this site is banned from most hardware forums.</div><br/></div></div></div></div></div></div><div id="37072534" class="c"><input type="checkbox" id="c-37072534" checked=""/><div class="controls bullet"><span class="by">gok</span><span>|</span><a href="#37068558">prev</a><span>|</span><a href="#37069740">next</a><span>|</span><label class="collapse" for="c-37072534">[-]</label><label class="expand" for="c-37072534">[1 more]</label></div><br/><div class="children"><div class="content">Memory bandwidth for unbatched decoder-only autoregressive inference should be the only bottleneck. The listed numbers imply you&#x27;re only saturating about 500 GB&#x2F;sec, ignoring KV cache traffic. Implies there&#x27;s still quite a bit of inference performance left to squeeze out of the 960 GB&#x2F;s spec.</div><br/></div></div><div id="37069740" class="c"><input type="checkbox" id="c-37069740" checked=""/><div class="controls bullet"><span class="by">ineedasername</span><span>|</span><a href="#37072534">prev</a><span>|</span><a href="#37070407">next</a><span>|</span><label class="collapse" for="c-37069740">[-]</label><label class="expand" for="c-37069740">[5 more]</label></div><br/><div class="children"><div class="content">Somewhat as a general question, not just for AMD&#x2F;Nvidia: At what point does RAM stop being the bottleneck? By which I mean, given current chips, how much RAM could you theoretically bolt on to these cards before the limiting factor in performance is the GPU instead of RAM? And does that change when the task is training vs. deployment&#x2F;prompting?</div><br/><div id="37070099" class="c"><input type="checkbox" id="c-37070099" checked=""/><div class="controls bullet"><span class="by">coherentpony</span><span>|</span><a href="#37069740">parent</a><span>|</span><a href="#37070816">next</a><span>|</span><label class="collapse" for="c-37070099">[-]</label><label class="expand" for="c-37070099">[3 more]</label></div><br/><div class="children"><div class="content">What do you mean?  Are you talking about capacity?  Or bandwidth from RAM?<p>I&#x27;m in the HPC space, and pretty much everything I do on the GPU is bound by how quickly I can get data in and out of DRAM.<p>The point at which data motion to&#x2F;from DRAM is not the bottleneck is when you do enough work per byte of data moved.  How much work is that?  On today&#x27;s server GPUs it&#x27;s in the region of 50--100 double precision floating point operations per byte.  You can work out an exact number by taking the theoretical maximum floating point operations per unit time you can execute and divide by DRAM throughput (data moved per unit time).<p>O(50--100) double precision flops per byte is a  _lot_ of work.  We&#x27;re talking BLAS-3 type operations.  Anything level 2 or lower, or sparse operations, are typically bandwidth bound.</div><br/><div id="37070337" class="c"><input type="checkbox" id="c-37070337" checked=""/><div class="controls bullet"><span class="by">BobbyJo</span><span>|</span><a href="#37069740">root</a><span>|</span><a href="#37070099">parent</a><span>|</span><a href="#37070816">next</a><span>|</span><label class="collapse" for="c-37070337">[-]</label><label class="expand" for="c-37070337">[2 more]</label></div><br/><div class="children"><div class="content">The problem with a lot of machine learning algorithms is that you do hundreds or even thousands of operations per-value, but you do it in specialized orderings on incredibly large numbers of values, forcing you to swap partitions in and out of RAM. So, while you may do thousands of ops per-value, you may only be able to do tens of ops per-value per-write to RAM.<p>The more RAM you have on device, the fewer swaps you need to do (none at all if it&#x27;s big enough), and the less those operations get amortized, bringing you closer to theoretical max throughput.<p>Matrix multiples are such that going from fitting 75% of your values to fitting 100% of your values can mean an order of magnitude speedup.</div><br/><div id="37072131" class="c"><input type="checkbox" id="c-37072131" checked=""/><div class="controls bullet"><span class="by">coherentpony</span><span>|</span><a href="#37069740">root</a><span>|</span><a href="#37070337">parent</a><span>|</span><a href="#37070816">next</a><span>|</span><label class="collapse" for="c-37072131">[-]</label><label class="expand" for="c-37072131">[1 more]</label></div><br/><div class="children"><div class="content">Disclaimer: I have no idea how machine learning algorithms work.<p>I work with problems so huge they do not fit on a single device.  Multiple devices  each own a small piece of the global problem.  They solve a local problem and they must communicate over a network in order to solve the global problem.  This is almost universally true.<p>You would know more than I in this field, and I expect it really is better to swap a partition than it is to use a network.<p>There are certain methods in HPC applications that are almost universally avoided because of how terribly they scale to a large distributed memory system.  Matrix multiplies are one of them.  Outside of a handful of ab initio computational chemistry algorithms (which are incredibly important), basically the only reason someone does a large dense matrix-multiply on a supercomputer is usually because they&#x27;re running a benchmark and they&#x27;re not solving a real science problem.<p>Folks more knowledgeable than me here feel free to jump in.</div><br/></div></div></div></div></div></div><div id="37070816" class="c"><input type="checkbox" id="c-37070816" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37069740">parent</a><span>|</span><a href="#37070099">prev</a><span>|</span><a href="#37070407">next</a><span>|</span><label class="collapse" for="c-37070816">[-]</label><label class="expand" for="c-37070816">[1 more]</label></div><br/><div class="children"><div class="content">The ideal ram <i>capacity</i> is determined by the biggest available model you want to run. So... ~48GB for llama 70B? Maybe more with batching and very long contexts.<p>RAM <i>bandwidth</i> is basically always going to be a bottleneck. Microsft&#x27;s proposal to get around this is to just keep everything in SRAM and pipe chips together.</div><br/></div></div></div></div><div id="37070407" class="c"><input type="checkbox" id="c-37070407" checked=""/><div class="controls bullet"><span class="by">piskov</span><span>|</span><a href="#37069740">prev</a><span>|</span><a href="#37071607">next</a><span>|</span><label class="collapse" for="c-37070407">[-]</label><label class="expand" for="c-37070407">[10 more]</label></div><br/><div class="children"><div class="content">Is Geohot (George Hotz) part of this project or is it a parallel undertaking?</div><br/><div id="37070896" class="c"><input type="checkbox" id="c-37070896" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#37070407">parent</a><span>|</span><a href="#37073116">next</a><span>|</span><label class="collapse" for="c-37070896">[-]</label><label class="expand" for="c-37070896">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s parallel. Notably, Geohot&#x27;s focus has been on improving ROCm directly while this project focuses more on improving the downstream tooling.</div><br/><div id="37071646" class="c"><input type="checkbox" id="c-37071646" checked=""/><div class="controls bullet"><span class="by">pokeypokes</span><span>|</span><a href="#37070407">root</a><span>|</span><a href="#37070896">parent</a><span>|</span><a href="#37071470">next</a><span>|</span><label class="collapse" for="c-37071646">[-]</label><label class="expand" for="c-37071646">[1 more]</label></div><br/><div class="children"><div class="content">What contributions did he make to ROCm?</div><br/></div></div><div id="37071470" class="c"><input type="checkbox" id="c-37071470" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#37070407">root</a><span>|</span><a href="#37070896">parent</a><span>|</span><a href="#37071646">prev</a><span>|</span><a href="#37073116">next</a><span>|</span><label class="collapse" for="c-37071470">[-]</label><label class="expand" for="c-37071470">[3 more]</label></div><br/><div class="children"><div class="content">He is doing full stack, and is selling tinybox (a PC with 6 AMD GPUs in it).<p>As he went through the hard work of debugging the driver and getting AMD to care about it, I was expecting to see him in the acknowledgements.</div><br/><div id="37071643" class="c"><input type="checkbox" id="c-37071643" checked=""/><div class="controls bullet"><span class="by">pokeypokes</span><span>|</span><a href="#37070407">root</a><span>|</span><a href="#37071470">parent</a><span>|</span><a href="#37073116">next</a><span>|</span><label class="collapse" for="c-37071643">[-]</label><label class="expand" for="c-37071643">[2 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t he just find&#x2F;report a bug in the multi-gpu case? Then have a melt-down after an AMD engineer sent him a fix a couple of days later? I think you are overstating his contributions, in fact it&#x27;s not clear to me he made any.</div><br/><div id="37072795" class="c"><input type="checkbox" id="c-37072795" checked=""/><div class="controls bullet"><span class="by">kanwisher</span><span>|</span><a href="#37070407">root</a><span>|</span><a href="#37071643">parent</a><span>|</span><a href="#37073116">next</a><span>|</span><label class="collapse" for="c-37072795">[-]</label><label class="expand" for="c-37072795">[1 more]</label></div><br/><div class="children"><div class="content">He literally had a phone call with CEO of AMD, and got them to organize people to fix the problems. He has been whipping AMD into fixing their drivers</div><br/></div></div></div></div></div></div></div></div><div id="37073116" class="c"><input type="checkbox" id="c-37073116" checked=""/><div class="controls bullet"><span class="by">hruzgar</span><span>|</span><a href="#37070407">parent</a><span>|</span><a href="#37070896">prev</a><span>|</span><a href="#37071601">next</a><span>|</span><label class="collapse" for="c-37073116">[-]</label><label class="expand" for="c-37073116">[1 more]</label></div><br/><div class="children"><div class="content">He was the one who pushed AMD to improve their drivers. So yeah, without him this wouldn&#x27;t be possible.</div><br/></div></div><div id="37071601" class="c"><input type="checkbox" id="c-37071601" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37070407">parent</a><span>|</span><a href="#37073116">prev</a><span>|</span><a href="#37070751">next</a><span>|</span><label class="collapse" for="c-37071601">[-]</label><label class="expand" for="c-37071601">[2 more]</label></div><br/><div class="children"><div class="content">Off topic but feels like a good place
to ask? Can WebGPU give you decent performance on non-Cuda and help accomplish these kinds of aims? (Geohot I think is aiming to avoid a single chipmaker monopoly on AI which he sees as a bad thing &#x2F;paraphrase)</div><br/><div id="37072743" class="c"><input type="checkbox" id="c-37072743" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37070407">root</a><span>|</span><a href="#37071601">parent</a><span>|</span><a href="#37070751">next</a><span>|</span><label class="collapse" for="c-37072743">[-]</label><label class="expand" for="c-37072743">[1 more]</label></div><br/><div class="children"><div class="content">As of today performance in WebGPU isn&#x27;t as competitive yet, but there are really quite a lot of low-hanging fruits for WebGPU to pick up.</div><br/></div></div></div></div><div id="37070751" class="c"><input type="checkbox" id="c-37070751" checked=""/><div class="controls bullet"><span class="by">zorex</span><span>|</span><a href="#37070407">parent</a><span>|</span><a href="#37071601">prev</a><span>|</span><a href="#37071607">next</a><span>|</span><label class="collapse" for="c-37070751">[-]</label><label class="expand" for="c-37070751">[1 more]</label></div><br/><div class="children"><div class="content">no. different project <a href="https:&#x2F;&#x2F;tinygrad.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tinygrad.org&#x2F;</a></div><br/></div></div></div></div><div id="37071607" class="c"><input type="checkbox" id="c-37071607" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#37070407">prev</a><span>|</span><a href="#37073329">next</a><span>|</span><label class="collapse" for="c-37071607">[-]</label><label class="expand" for="c-37071607">[2 more]</label></div><br/><div class="children"><div class="content">Why is 4090 so slow in your tests? It regularly demolishes my 3090 in my own tests (2x speedup in most tasks).</div><br/><div id="37071835" class="c"><input type="checkbox" id="c-37071835" checked=""/><div class="controls bullet"><span class="by">junrushao1994</span><span>|</span><a href="#37071607">parent</a><span>|</span><a href="#37073329">next</a><span>|</span><label class="collapse" for="c-37071835">[-]</label><label class="expand" for="c-37071835">[1 more]</label></div><br/><div class="children"><div class="content">LLM decoding is dominated by memory bandwidth, and 3090Ti and 4090 happen to have the identical theoretical memory bandwidth</div><br/></div></div></div></div><div id="37073329" class="c"><input type="checkbox" id="c-37073329" checked=""/><div class="controls bullet"><span class="by">voytec</span><span>|</span><a href="#37071607">prev</a><span>|</span><a href="#37070908">next</a><span>|</span><label class="collapse" for="c-37073329">[-]</label><label class="expand" for="c-37073329">[1 more]</label></div><br/><div class="children"><div class="content">Do I understand correctly that it only applies to dedicated cards, not these GPUs bundled with Ryzens?</div><br/></div></div><div id="37070908" class="c"><input type="checkbox" id="c-37070908" checked=""/><div class="controls bullet"><span class="by">vladgur</span><span>|</span><a href="#37073329">prev</a><span>|</span><a href="#37070909">next</a><span>|</span><label class="collapse" for="c-37070908">[-]</label><label class="expand" for="c-37070908">[2 more]</label></div><br/><div class="children"><div class="content">What about using 3 8GB RX 6600 in place of 1 24GB RX 7900?</div><br/><div id="37073602" class="c"><input type="checkbox" id="c-37073602" checked=""/><div class="controls bullet"><span class="by">MezzoDelCammin</span><span>|</span><a href="#37070908">parent</a><span>|</span><a href="#37070909">next</a><span>|</span><label class="collapse" for="c-37073602">[-]</label><label class="expand" for="c-37073602">[1 more]</label></div><br/><div class="children"><div class="content">just out of curiosity - how would You go about that link of the 3 cards? Does AMD have anything like NVLink? (honest question, I really don&#x27;t know and suspect I might have missed a bit somewhere in the idea)</div><br/></div></div></div></div><div id="37070909" class="c"><input type="checkbox" id="c-37070909" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#37070908">prev</a><span>|</span><a href="#37069809">next</a><span>|</span><label class="collapse" for="c-37070909">[-]</label><label class="expand" for="c-37070909">[1 more]</label></div><br/><div class="children"><div class="content">Hm, this is interesting, is this enough to go long AMD?</div><br/></div></div><div id="37069809" class="c"><input type="checkbox" id="c-37069809" checked=""/><div class="controls bullet"><span class="by">71a54xd</span><span>|</span><a href="#37070909">prev</a><span>|</span><label class="collapse" for="c-37069809">[-]</label><label class="expand" for="c-37069809">[2 more]</label></div><br/><div class="children"><div class="content">I really wonder what kind of Koolaid they serve at AMD?  This feels very similar to AMD shilling &quot;multi core&quot; CPU&#x27;s back in the mid 2000&#x27;s when anyone seriously using multi-core GPUs didn&#x27;t care about cost and was just buying intel while AMD continued to fumble mediocre products.</div><br/><div id="37071085" class="c"><input type="checkbox" id="c-37071085" checked=""/><div class="controls bullet"><span class="by">unethical_ban</span><span>|</span><a href="#37069809">parent</a><span>|</span><label class="collapse" for="c-37071085">[-]</label><label class="expand" for="c-37071085">[1 more]</label></div><br/><div class="children"><div class="content">Talking s** about AMD CPUs in the mid-2000s is a gutsy move.</div><br/></div></div></div></div></div></div></div></div></div></body></html>