<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699347658365" as="style"/><link rel="stylesheet" href="styles.css?v=1699347658365"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/IST-DASLab/QUIK">QUIK is a method for quantizing LLM post-training weights to 4 bit precision</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>anigbrowl</span> | <span>18 comments</span></div><br/><div><div id="38174351" class="c"><input type="checkbox" id="c-38174351" checked=""/><div class="controls bullet"><span class="by">wg0</span><span>|</span><a href="#38170146">next</a><span>|</span><label class="collapse" for="c-38174351">[-]</label><label class="expand" for="c-38174351">[1 more]</label></div><br/><div class="children"><div class="content">Experts, if we bring the accuracy of operations to 4 bits, would it not impact the output quality?<p>Intuition says, yes. Would appreciate some practitioner&#x2F;theorist on the subject to say what is the impact of lower precision on accuracy&#x2F;output of a model.</div><br/></div></div><div id="38170146" class="c"><input type="checkbox" id="c-38170146" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#38174351">prev</a><span>|</span><a href="#38173968">next</a><span>|</span><label class="collapse" for="c-38170146">[-]</label><label class="expand" for="c-38170146">[11 more]</label></div><br/><div class="children"><div class="content">Title misses that it&#x27;s &quot;weights and activations&quot; as in they (from my skim) are doing all the math with mostly 4-bit values. Normally quantized weights are converted to 32-bits (edit: or more generally a type for which there are native machine instructions to multiply&#x2F;add, as mentioned below) as they are applied during a forward pass which saves memory but incurs extra processing. They are keeping everything in (mostly) 4-bits to make it faster.</div><br/><div id="38170294" class="c"><input type="checkbox" id="c-38170294" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#38170146">parent</a><span>|</span><a href="#38171257">next</a><span>|</span><label class="collapse" for="c-38170294">[-]</label><label class="expand" for="c-38170294">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty common to convert to bfloat16 nowadays since that&#x27;s much faster. e.g. bitsnbytes now defaults to bf16 compute for 4bit quant.</div><br/></div></div><div id="38171257" class="c"><input type="checkbox" id="c-38171257" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#38170146">parent</a><span>|</span><a href="#38170294">prev</a><span>|</span><a href="#38170194">next</a><span>|</span><label class="collapse" for="c-38171257">[-]</label><label class="expand" for="c-38171257">[3 more]</label></div><br/><div class="children"><div class="content">You only get 80 characters for the title so I had to rewrite it to fit.</div><br/><div id="38171359" class="c"><input type="checkbox" id="c-38171359" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#38170146">root</a><span>|</span><a href="#38171257">parent</a><span>|</span><a href="#38170194">next</a><span>|</span><label class="collapse" for="c-38171359">[-]</label><label class="expand" for="c-38171359">[2 more]</label></div><br/><div class="children"><div class="content">Might use activations rather than weights in the title then. Weights are commonly done, activations are not.</div><br/><div id="38171523" class="c"><input type="checkbox" id="c-38171523" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#38170146">root</a><span>|</span><a href="#38171359">parent</a><span>|</span><a href="#38170194">next</a><span>|</span><label class="collapse" for="c-38171523">[-]</label><label class="expand" for="c-38171523">[1 more]</label></div><br/><div class="children"><div class="content">Alas, the edit window has closed. But I&#x27;ll keep it in mind for any future submissions on that topic, thanks.</div><br/></div></div></div></div></div></div><div id="38170194" class="c"><input type="checkbox" id="c-38170194" checked=""/><div class="controls bullet"><span class="by">mattsan</span><span>|</span><a href="#38170146">parent</a><span>|</span><a href="#38171257">prev</a><span>|</span><a href="#38173968">next</a><span>|</span><label class="collapse" for="c-38170194">[-]</label><label class="expand" for="c-38170194">[6 more]</label></div><br/><div class="children"><div class="content">Huh, I thought this is what was usually meant by quantising</div><br/><div id="38170288" class="c"><input type="checkbox" id="c-38170288" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#38170146">root</a><span>|</span><a href="#38170194">parent</a><span>|</span><a href="#38170212">next</a><span>|</span><label class="collapse" for="c-38170288">[-]</label><label class="expand" for="c-38170288">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a nontrivial problem to do faster arithmetic operations with quantized values (which are not just rounded to fit in 4 bits but are usually block quantized with one or more scaling parameters stored at full precision) faster than to convert them to floats that the cpu&#x2F;GPU is already optimized to multiply&#x2F;add. That is what this project is proposing a solution to.</div><br/><div id="38170806" class="c"><input type="checkbox" id="c-38170806" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#38170146">root</a><span>|</span><a href="#38170288">parent</a><span>|</span><a href="#38170212">next</a><span>|</span><label class="collapse" for="c-38170806">[-]</label><label class="expand" for="c-38170806">[2 more]</label></div><br/><div class="children"><div class="content">If you have a 4 bit signal multiplied by a 4 bit weigh, you can just use a lookup table for any operation with any output type you want. A 256 entry table of 32bit floats fits in 1K for example, which will fit in cache with plenty left over. This is independent of the quantization scheme so long as it&#x27;s not dynamic.</div><br/><div id="38172130" class="c"><input type="checkbox" id="c-38172130" checked=""/><div class="controls bullet"><span class="by">ajtulloch</span><span>|</span><a href="#38170146">root</a><span>|</span><a href="#38170806">parent</a><span>|</span><a href="#38170212">next</a><span>|</span><label class="collapse" for="c-38172130">[-]</label><label class="expand" for="c-38172130">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s quite unfavorable on modern hardware. A Sapphire Rapids core can do 2 separate 32 half-precision FMAs (vfmadd132ph, [1]) per clock, which is 128 FLOPs&#x2F;cycle. It is not possible to achieve that kind of throughput with an 8-bit LUT and accumulation, even just a shuffle with vpshufb is too slow.<p>[1]: <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;docs&#x2F;intrinsics-guide&#x2F;index.html#text=_mm512_fmadd_ph&amp;ig_expand=3117,3117" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;docs&#x2F;intrinsics-guid...</a></div><br/></div></div></div></div></div></div><div id="38170212" class="c"><input type="checkbox" id="c-38170212" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38170146">root</a><span>|</span><a href="#38170194">parent</a><span>|</span><a href="#38170288">prev</a><span>|</span><a href="#38173968">next</a><span>|</span><label class="collapse" for="c-38170212">[-]</label><label class="expand" for="c-38170212">[2 more]</label></div><br/><div class="children"><div class="content">I believe the distinction they&#x27;re drawing is they&#x27;re also not &quot;cast&quot; to float32 during processing (which I also didn&#x27;t know)</div><br/><div id="38171180" class="c"><input type="checkbox" id="c-38171180" checked=""/><div class="controls bullet"><span class="by">mattsan</span><span>|</span><a href="#38170146">root</a><span>|</span><a href="#38170212">parent</a><span>|</span><a href="#38173968">next</a><span>|</span><label class="collapse" for="c-38171180">[-]</label><label class="expand" for="c-38171180">[1 more]</label></div><br/><div class="children"><div class="content">Yep I didn&#x27;t know other models cast during processing</div><br/></div></div></div></div></div></div></div></div><div id="38173968" class="c"><input type="checkbox" id="c-38173968" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#38170146">prev</a><span>|</span><a href="#38173051">next</a><span>|</span><label class="collapse" for="c-38173968">[-]</label><label class="expand" for="c-38173968">[1 more]</label></div><br/><div class="children"><div class="content">This is more of an off-topic, but is there research into not having to evaluate all LLM tokens for each output token (at perhaps some cost to output quality), thereby making it possible to run these models in a more compute and memory efficient manner?</div><br/></div></div><div id="38173051" class="c"><input type="checkbox" id="c-38173051" checked=""/><div class="controls bullet"><span class="by">grendelt</span><span>|</span><a href="#38173968">prev</a><span>|</span><a href="#38171818">next</a><span>|</span><label class="collapse" for="c-38173051">[-]</label><label class="expand" for="c-38173051">[1 more]</label></div><br/><div class="children"><div class="content">Oh, so this is QUIK and not QUIC.</div><br/></div></div><div id="38171818" class="c"><input type="checkbox" id="c-38171818" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#38173051">prev</a><span>|</span><a href="#38170199">next</a><span>|</span><label class="collapse" for="c-38171818">[-]</label><label class="expand" for="c-38171818">[2 more]</label></div><br/><div class="children"><div class="content">One thing that I&#x27;ve currently wondered about quantization... peft seems to require a GPU. Are there any current quantization approaches that make fine tuning more efficient purely on a CPU?</div><br/><div id="38172049" class="c"><input type="checkbox" id="c-38172049" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#38171818">parent</a><span>|</span><a href="#38170199">next</a><span>|</span><label class="collapse" for="c-38172049">[-]</label><label class="expand" for="c-38172049">[1 more]</label></div><br/><div class="children"><div class="content">OpenVINO: <a href="https:&#x2F;&#x2F;github.com&#x2F;openvinotoolkit&#x2F;openvino">https:&#x2F;&#x2F;github.com&#x2F;openvinotoolkit&#x2F;openvino</a></div><br/></div></div></div></div><div id="38170199" class="c"><input type="checkbox" id="c-38170199" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38171818">prev</a><span>|</span><label class="collapse" for="c-38170199">[-]</label><label class="expand" for="c-38170199">[1 more]</label></div><br/><div class="children"><div class="content">Astonishing work, I was muttering to myself last week about quantization and thought useful 4 bit might never happen. IIUC there&#x27;s a loss of 6-16% which is totally reasonable and means my 4.8 GB Mistral model looks more like 2.4 GB once this trickles through to MLC. That means on device GPT 3.25ish at 30 tkns&#x2F;sec...</div><br/></div></div></div></div></div></div></div></body></html>