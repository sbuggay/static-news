<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721898068683" as="style"/><link rel="stylesheet" href="styles.css?v=1721898068683"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2404.14394">A Multimodal Automated Interpretability Agent</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>el_duderino</span> | <span>8 comments</span></div><br/><div><div id="41061079" class="c"><input type="checkbox" id="c-41061079" checked=""/><div class="controls bullet"><span class="by">curious_cat_163</span><span>|</span><a href="#41058193">next</a><span>|</span><label class="collapse" for="c-41061079">[-]</label><label class="expand" for="c-41061079">[3 more]</label></div><br/><div class="children"><div class="content">&gt; We think MAIA augments, but does not replace, human over- sight of AI systems. MAIA still requires human supervision to catch mistakes such as confirmation bias and image generation&#x2F;editing failures. Absence of evidence (from MAIA) is not evidence of absence: though MAIA’s toolkit enables causal interventions on inputs in order to evaluate system behavior, MAIA’s explanations do not provide formal verification of system performance.<p>For folks who are more familiar with this branch of literature, given the above, why is this a fruitful line of inquiry? Isn&#x27;t this akin to stacking turtles on top of each other?</div><br/><div id="41066369" class="c"><input type="checkbox" id="c-41066369" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41061079">parent</a><span>|</span><a href="#41064288">next</a><span>|</span><label class="collapse" for="c-41066369">[-]</label><label class="expand" for="c-41066369">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s basically a known fact about LLMs, they need oversight. But if they make the task 100x easier, it&#x27;s still useful as a starting point. This kind of neural net analysis is difficult to do manually.<p>I am curious if they just start making inventories for all neurons in all layers, then they can compare models based on neuron types, or even train them to achieve the right mix of concepts.</div><br/></div></div><div id="41064288" class="c"><input type="checkbox" id="c-41064288" checked=""/><div class="controls bullet"><span class="by">yurimo</span><span>|</span><a href="#41061079">parent</a><span>|</span><a href="#41066369">prev</a><span>|</span><a href="#41058193">next</a><span>|</span><label class="collapse" for="c-41064288">[-]</label><label class="expand" for="c-41064288">[1 more]</label></div><br/><div class="children"><div class="content">I think what authors aimed for is perhaps a proof-of-concept work where they attempt to demonstrate that you can (to a degree) automate interpretability. Mech interpretability is challenging because it does not scale well at the moment, and there is a debate about whether localized structural discoveries on toy examples actually translate to patterns in large networks. My guess if you could build an automatic explainer system this would allow you to flag problems and find issues faster, basically as some sort of meta-heuristic for further investigation<p>Unfortunately, that title hypes it up, and as always, once you read the paper, the results are less impressive, but that is what the state of AI research is currently, speaking as a researcher myself.<p>In a similar vain: <a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;language-models-can-explain-neurons-in-language-models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;language-models-can-explain-neurons...</a></div><br/></div></div></div></div><div id="41058193" class="c"><input type="checkbox" id="c-41058193" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#41061079">prev</a><span>|</span><a href="#41064120">next</a><span>|</span><label class="collapse" for="c-41058193">[-]</label><label class="expand" for="c-41058193">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.14394" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.14394</a><p>Actual paper to save you from having to read the PR release.</div><br/><div id="41059306" class="c"><input type="checkbox" id="c-41059306" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#41058193">parent</a><span>|</span><a href="#41064120">next</a><span>|</span><label class="collapse" for="c-41059306">[-]</label><label class="expand" for="c-41059306">[1 more]</label></div><br/><div class="children"><div class="content">Ok, we&#x27;ll change the URL to that from <a href="https:&#x2F;&#x2F;news.mit.edu&#x2F;2024&#x2F;mit-researchers-advance-automated-interpretability-ai-models-maia-0723" rel="nofollow">https:&#x2F;&#x2F;news.mit.edu&#x2F;2024&#x2F;mit-researchers-advance-automated-...</a>. Users may still want to read the latter for a quick intro.</div><br/></div></div></div></div><div id="41064120" class="c"><input type="checkbox" id="c-41064120" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#41058193">prev</a><span>|</span><label class="collapse" for="c-41064120">[-]</label><label class="expand" for="c-41064120">[2 more]</label></div><br/><div class="children"><div class="content">We uncritically accept extraordinary claims on this. They might even be valid claims, but they are so rarely supported by evidence that is likewise extraordinary.<p>In my experience real, durable progress generally starts happening once we come back down to Earth and start iterating.<p>Are modern large models crucial to transportation? Maybe? Waymo is cool but it’s not yet an economic reality at scale, and I doubt there are 1.75T weight models running in cars. Are they crucial to finance? I’m quite sure that machine learning plays an important role in finance because I know people in finance who do it all day for serious firms, but I’m very skeptical that finance has been revolutionized in the last 18 months (unless you count the NVDA HODL).<p>Can we push back a little on the breathless hyperventilation? It was annoying a year ago, the AGI people were wrong, it’s offensive now, we got played for suckers.<p>“As artificial intelligence models become increasingly prevalent and are integrated into diverse sectors like health care, finance, education, transportation, and entertainment, understanding how they work under the hood is critical. Interpreting the mechanisms underlying AI models enables us to audit them for safety and biases, with the potential to deepen our understanding of the science behind intelligence itself.”</div><br/><div id="41064170" class="c"><input type="checkbox" id="c-41064170" checked=""/><div class="controls bullet"><span class="by">ainoobler</span><span>|</span><a href="#41064120">parent</a><span>|</span><label class="collapse" for="c-41064170">[-]</label><label class="expand" for="c-41064170">[1 more]</label></div><br/><div class="children"><div class="content">Eventually both the hype and its criticism will be automated with AI as well so that we can all go to the beach and relax.</div><br/></div></div></div></div></div></div></div></div></div></body></html>