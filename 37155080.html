<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692262859981" as="style"/><link rel="stylesheet" href="styles.css?v=1692262859981"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://huyenchip.com/2023/08/16/llm-research-open-challenges.html">Open Challenges in LLM Research</a> <span class="domain">(<a href="https://huyenchip.com">huyenchip.com</a>)</span></div><div class="subtext"><span>muggermuch</span> | <span>42 comments</span></div><br/><div><div id="37157193" class="c"><input type="checkbox" id="c-37157193" checked=""/><div class="controls bullet"><span class="by">errantspark</span><span>|</span><a href="#37158856">next</a><span>|</span><label class="collapse" for="c-37157193">[-]</label><label class="expand" for="c-37157193">[3 more]</label></div><br/><div class="children"><div class="content">Fun fact: I took the photo she used as a cover for one of her books, she asked me if she could use it and I said I&#x27;d like to be compensated and her response was something akin to &quot;oh I was just asking assuming you&#x27;d say yes, I&#x27;m going to do it anyway&quot;. Nobody&#x27;s perfect, maybe she regrets it, and it hasn&#x27;t really crossed my mind in years, but I guess it still sort of irks me to be reminded of it. Anyway if anyone needs a portrait for a book cover feel free to hit me up XD.</div><br/><div id="37157935" class="c"><input type="checkbox" id="c-37157935" checked=""/><div class="controls bullet"><span class="by">abatilo</span><span>|</span><a href="#37157193">parent</a><span>|</span><a href="#37158484">next</a><span>|</span><label class="collapse" for="c-37157935">[-]</label><label class="expand" for="c-37157935">[1 more]</label></div><br/><div class="children"><div class="content">Just another random anecdotal experience with Chip.<p>I was interviewing with Claypot.ai and when I met her for my first conversation, she was on a walking treadmill and very clearly was more interested in a Slack conversation she was having.<p>She moved me on to the next round which I irrefutably bombed and was respectfully told that I wouldn&#x27;t be moving on which was the right decision, but I&#x27;ll never forget watching her walking motion while looking at Slack on her second monitor almost the entire time we were talking.</div><br/></div></div><div id="37158484" class="c"><input type="checkbox" id="c-37158484" checked=""/><div class="controls bullet"><span class="by">wanderlust123</span><span>|</span><a href="#37157193">parent</a><span>|</span><a href="#37157935">prev</a><span>|</span><a href="#37158856">next</a><span>|</span><label class="collapse" for="c-37158484">[-]</label><label class="expand" for="c-37158484">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a pretty entitled and unpleasant person. At the bare minimum you should have had a say in whether you picture could hve been used.</div><br/></div></div></div></div><div id="37158856" class="c"><input type="checkbox" id="c-37158856" checked=""/><div class="controls bullet"><span class="by">blackkettle</span><span>|</span><a href="#37157193">prev</a><span>|</span><a href="#37155815">next</a><span>|</span><label class="collapse" for="c-37158856">[-]</label><label class="expand" for="c-37158856">[1 more]</label></div><br/><div class="children"><div class="content">Seems like it is basically a blog post review of Challenges and Applications of Large Language Models which was published to arXiv last month:<p>- <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10169" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.10169</a></div><br/></div></div><div id="37155815" class="c"><input type="checkbox" id="c-37155815" checked=""/><div class="controls bullet"><span class="by">thrwayaistartup</span><span>|</span><a href="#37158856">prev</a><span>|</span><a href="#37156345">next</a><span>|</span><label class="collapse" for="c-37155815">[-]</label><label class="expand" for="c-37155815">[14 more]</label></div><br/><div class="children"><div class="content">Looking back in 25 years, the &quot;Hallucination Problem&quot; will sound a lot like the &quot;Frame Problem&quot; of the 1970s.<p>Looking back, it&#x27;s a bit absurd to say that GOFAI would&#x27;ve got to AGI if only the Frame Problem could be solved. But the important point is <i>why</i> that sounds so absurd.<p>It doesn&#x27;t sound absurd because we found out that the frame problem can&#x27;t be solved; that&#x27;s beside the point.<p>It also doesn&#x27;t sound absurd because we found out that solving the frame problem isn&#x27;t the key to GOFAI-based AGI. That&#x27;s also beside the point.<p>It sounds absurd because the conjecture itself is... just funny. It&#x27;s almost goofy, looking back, how people thought about AGI.<p>Hallucination is the Frame Problem of the 2023 AI Summer. Looking back from the other side of the next Winter, the whole thing will seem a bit goofy.</div><br/><div id="37158427" class="c"><input type="checkbox" id="c-37158427" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#37155815">parent</a><span>|</span><a href="#37156205">next</a><span>|</span><label class="collapse" for="c-37158427">[-]</label><label class="expand" for="c-37158427">[1 more]</label></div><br/><div class="children"><div class="content">My feeling is that GOFAI had a real problem with representing uncertainty, and handling contradiction. So, we tried to approach it theoretically, with fuzzy logic and probability and so on. But the theoretical research on uncertainty didn&#x27;t reach any clear conclusion.<p>Meanwhile, the neural nets (and ML) researchers just trucked on, with more compute power, and pretty much ignored any theoretical issues with uncertainty. And surprisingly, with lots of amazing results.<p>But now they hit the same wall, we don&#x27;t actually understand how to do reasoning with uncertainty correctly. LLMs seem to solve this by &quot;just mimic reasoning that humans do&quot;. Except because we lack a good theory of reasoning, it can&#x27;t tell when mimicking is bad and when it&#x27;s good, unless there is a lot of specific examples. So in the most egregious cases, we get hallucinations but have no clue how to avoid them.</div><br/></div></div><div id="37156205" class="c"><input type="checkbox" id="c-37156205" checked=""/><div class="controls bullet"><span class="by">resonious</span><span>|</span><a href="#37155815">parent</a><span>|</span><a href="#37158427">prev</a><span>|</span><a href="#37156203">next</a><span>|</span><label class="collapse" for="c-37156205">[-]</label><label class="expand" for="c-37156205">[8 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know much about AI research but the idea of &quot;measuring&quot; hallucination definitely seems very loaded to me. Humans hallucinate too and I don&#x27;t think we can measure that. It almost feels like &quot;we need AGI in order to develop AGI&quot;.</div><br/><div id="37156483" class="c"><input type="checkbox" id="c-37156483" checked=""/><div class="controls bullet"><span class="by">melenaboija</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156205">parent</a><span>|</span><a href="#37156203">next</a><span>|</span><label class="collapse" for="c-37156483">[-]</label><label class="expand" for="c-37156483">[7 more]</label></div><br/><div class="children"><div class="content">Comparing human hallucinations with model “hallucinations” does not make sense to me.<p>Model hallucinations seems to me like a fancy way to call the model results that make no sense (ie blatant errors). Plus it makes the model more humanoid.</div><br/><div id="37157029" class="c"><input type="checkbox" id="c-37157029" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156483">parent</a><span>|</span><a href="#37156525">next</a><span>|</span><label class="collapse" for="c-37157029">[-]</label><label class="expand" for="c-37157029">[1 more]</label></div><br/><div class="children"><div class="content">Most hallucinations make sense. In fact, that is precisely the problem. They make so much sense it&#x27;s often difficult to distinguish. Most people refer to hallucinations as wrong and often confidently wrong details in a generated reply.<p>Humans are certainly better but we don&#x27;t have an absolute sense of what we do or don&#x27;t know either.</div><br/></div></div><div id="37156525" class="c"><input type="checkbox" id="c-37156525" checked=""/><div class="controls bullet"><span class="by">resonious</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156483">parent</a><span>|</span><a href="#37157029">prev</a><span>|</span><a href="#37156203">next</a><span>|</span><label class="collapse" for="c-37156525">[-]</label><label class="expand" for="c-37156525">[5 more]</label></div><br/><div class="children"><div class="content">Humans also very often produce results that don&#x27;t make sense.</div><br/><div id="37156558" class="c"><input type="checkbox" id="c-37156558" checked=""/><div class="controls bullet"><span class="by">fallous</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156525">parent</a><span>|</span><a href="#37156203">next</a><span>|</span><label class="collapse" for="c-37156558">[-]</label><label class="expand" for="c-37156558">[4 more]</label></div><br/><div class="children"><div class="content">Humans that produce output like LLMs are most likely to be diagnosed as schizophrenic, which I don&#x27;t believe is the goal.</div><br/><div id="37158379" class="c"><input type="checkbox" id="c-37158379" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156558">parent</a><span>|</span><a href="#37158531">next</a><span>|</span><label class="collapse" for="c-37158379">[-]</label><label class="expand" for="c-37158379">[1 more]</label></div><br/><div class="children"><div class="content">Confident human  bullshitters seem to thrive in business environments, in media and entertainment, in politics ... in fact in any profession where the production is just language instead of doing things. They might be more on the dark triangle spectrum, but I would not call them all &quot;schizophrenic&quot;.<p>The problem is we are so used to yielding to confidence we don&#x27;t apply the necessary checks even when we know it is projected by machine.</div><br/></div></div><div id="37158531" class="c"><input type="checkbox" id="c-37158531" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156558">parent</a><span>|</span><a href="#37158379">prev</a><span>|</span><a href="#37157131">next</a><span>|</span><label class="collapse" for="c-37158531">[-]</label><label class="expand" for="c-37158531">[1 more]</label></div><br/><div class="children"><div class="content">Untuned LLMs are most like people with Korsakoff&#x27;s syndrome. &quot;hallucination&quot; is a misleading term.</div><br/></div></div><div id="37157131" class="c"><input type="checkbox" id="c-37157131" checked=""/><div class="controls bullet"><span class="by">mlinhares</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156558">parent</a><span>|</span><a href="#37158531">prev</a><span>|</span><a href="#37156203">next</a><span>|</span><label class="collapse" for="c-37157131">[-]</label><label class="expand" for="c-37157131">[1 more]</label></div><br/><div class="children"><div class="content">I really haven’t seen much of that coming from my ChatGPT usage, it’s just someone lying with a lot of confidence, hardly a mental disorder.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37156203" class="c"><input type="checkbox" id="c-37156203" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#37155815">parent</a><span>|</span><a href="#37156205">prev</a><span>|</span><a href="#37156345">next</a><span>|</span><label class="collapse" for="c-37156203">[-]</label><label class="expand" for="c-37156203">[4 more]</label></div><br/><div class="children"><div class="content">Does anyone think we would have AGI if only we could solve the hallucination problem?</div><br/><div id="37156809" class="c"><input type="checkbox" id="c-37156809" checked=""/><div class="controls bullet"><span class="by">JimtheCoder</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156203">parent</a><span>|</span><a href="#37156519">next</a><span>|</span><label class="collapse" for="c-37156809">[-]</label><label class="expand" for="c-37156809">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t even have a generally accepted definition of AGI yet, so...no.</div><br/></div></div><div id="37156519" class="c"><input type="checkbox" id="c-37156519" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156203">parent</a><span>|</span><a href="#37156809">prev</a><span>|</span><a href="#37156345">next</a><span>|</span><label class="collapse" for="c-37156519">[-]</label><label class="expand" for="c-37156519">[2 more]</label></div><br/><div class="children"><div class="content">There’s people who thought we could just wire up ChatGPT to a bunch of API calls and have AGI by now. Or some similar version of bootstrapping an LLM.</div><br/><div id="37157964" class="c"><input type="checkbox" id="c-37157964" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#37155815">root</a><span>|</span><a href="#37156519">parent</a><span>|</span><a href="#37156345">next</a><span>|</span><label class="collapse" for="c-37157964">[-]</label><label class="expand" for="c-37157964">[1 more]</label></div><br/><div class="children"><div class="content">That could very well be the case.</div><br/></div></div></div></div></div></div></div></div><div id="37156345" class="c"><input type="checkbox" id="c-37156345" checked=""/><div class="controls bullet"><span class="by">ford</span><span>|</span><a href="#37155815">prev</a><span>|</span><a href="#37157739">next</a><span>|</span><label class="collapse" for="c-37156345">[-]</label><label class="expand" for="c-37156345">[7 more]</label></div><br/><div class="children"><div class="content">So far it&#x27;s been ~8 months since ChatGPT started the (popular) LLM craze. I&#x27;ve found raw GPT to be useful for a lot of things, but have yet to see my most frequently used apps integrate it in a useful way. Maybe I&#x27;m using the wrong apps...<p>It&#x27;ll be interesting to see what improvements (in a lab or at a company) need to happen before most people use purpose-built LLMs (or behind the scenes LLM prompts) in the apps they use every day. The answer might be &quot;no improvements&quot; and we&#x27;re just in the lag time before useful features can be built</div><br/><div id="37156571" class="c"><input type="checkbox" id="c-37156571" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#37156345">parent</a><span>|</span><a href="#37156455">next</a><span>|</span><label class="collapse" for="c-37156571">[-]</label><label class="expand" for="c-37156571">[5 more]</label></div><br/><div class="children"><div class="content">There are some unsolved practical problems like prompt injection, the difficulty of using them on your own data, etc.<p>But the biggest problem is that they take so much compute, which slows down both research and deployment. Only a handful of giant companies can train their own LLM, and it&#x27;s a major undertaking even for them. Academic researchers and everyday tinkerers can only run inference on pretrained models.</div><br/><div id="37157202" class="c"><input type="checkbox" id="c-37157202" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#37156345">root</a><span>|</span><a href="#37156571">parent</a><span>|</span><a href="#37156455">next</a><span>|</span><label class="collapse" for="c-37157202">[-]</label><label class="expand" for="c-37157202">[4 more]</label></div><br/><div class="children"><div class="content">Sounds like a great motivation for academic researchers to find a way to train LLMs with less compute. Or maybe invent something better than transformers. A brain trains on 20 Watts after all.</div><br/><div id="37157871" class="c"><input type="checkbox" id="c-37157871" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#37156345">root</a><span>|</span><a href="#37157202">parent</a><span>|</span><a href="#37156455">next</a><span>|</span><label class="collapse" for="c-37157871">[-]</label><label class="expand" for="c-37157871">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a hardware difference. Brains run at a very low clock speed and make up for it with massive parallelism. They also don&#x27;t suffer from the vonn neumann bottleneck - today&#x27;s computers spend most of their time and energy shuffling the network in and out of memory.<p>I believe that better hardware architectures will have more impact on AI than better neural network architectures.</div><br/><div id="37158070" class="c"><input type="checkbox" id="c-37158070" checked=""/><div class="controls bullet"><span class="by">pishpash</span><span>|</span><a href="#37156345">root</a><span>|</span><a href="#37157871">parent</a><span>|</span><a href="#37156455">next</a><span>|</span><label class="collapse" for="c-37158070">[-]</label><label class="expand" for="c-37158070">[2 more]</label></div><br/><div class="children"><div class="content">That would be FPGA&#x27;s.</div><br/><div id="37158679" class="c"><input type="checkbox" id="c-37158679" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#37156345">root</a><span>|</span><a href="#37158070">parent</a><span>|</span><a href="#37156455">next</a><span>|</span><label class="collapse" for="c-37158679">[-]</label><label class="expand" for="c-37158679">[1 more]</label></div><br/><div class="children"><div class="content">I doubt it. FPGAs are super inefficient in transistor count in exchange for being dynamically programmable. I suspect a better architecture will be taped out like any other chip.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37156455" class="c"><input type="checkbox" id="c-37156455" checked=""/><div class="controls bullet"><span class="by">netdur</span><span>|</span><a href="#37156345">parent</a><span>|</span><a href="#37156571">prev</a><span>|</span><a href="#37157739">next</a><span>|</span><label class="collapse" for="c-37156455">[-]</label><label class="expand" for="c-37156455">[1 more]</label></div><br/><div class="children"><div class="content">I have helped making behind sense cases, one was to classify emails and redirect them to intended sides, second was quality monitoring of call center.</div><br/></div></div></div></div><div id="37157739" class="c"><input type="checkbox" id="c-37157739" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37156345">prev</a><span>|</span><a href="#37158494">next</a><span>|</span><label class="collapse" for="c-37157739">[-]</label><label class="expand" for="c-37157739">[2 more]</label></div><br/><div class="children"><div class="content">Let me add a few:<p>- organic data exhaustion - we need to step up synthetic data and its validation<p>- imbalanced datasets - catalog, assess and fill in missing data<p>- backtracking - make LLMs better at combinatorial or search problems<p>- deduction - we need to augment the training set for revealing implicit knowledge, in other words to study the text before learning it<p>- defragmentation - information comes in small chunks, sits in separate siloes, and context size is short, we need to use retrieval to bring it together for analysis<p>tl;dr We need quantity, diversity and depth in our training sets</div><br/><div id="37158767" class="c"><input type="checkbox" id="c-37158767" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#37157739">parent</a><span>|</span><a href="#37158494">next</a><span>|</span><label class="collapse" for="c-37158767">[-]</label><label class="expand" for="c-37158767">[1 more]</label></div><br/><div class="children"><div class="content">And I’ll add some more:<p>- LLMs aren’t very good at large scale narrative construction. They get too distracted by low level details that they miss the high level details in long text. It feels like the same problem as stable diffusion giving people too many fingers.<p>- LLMs have 2 kinds of memory: current activations (context) and trained weights. This is like working memory and long term memory. How do we add short term memory? Like, if I read a function, I summarize it in my head and then remember the summary for as long as it’s relevant. (Maybe 20 minutes or something). How do we build a mechanism that can do this?<p>- How do we do gradient descent on the model architecture itself during training?<p>- Humans have lots more tricks to use when reading large, complex text - like re-reading relevant sections, making notes, thinking quietly, and so on. Can we introduce these thinking modalities into our systems? I bet they’d behave smarter if they could do this stuff.<p>- How do we combine multiple LLMs into a smarter overall system? Eg, does it make sense to build committees of “experts” (LLMs taking on different expert roles) to help in decision making? Can we get more intelligence out of chatgpt by using it in a different way in a larger system?</div><br/></div></div></div></div><div id="37158494" class="c"><input type="checkbox" id="c-37158494" checked=""/><div class="controls bullet"><span class="by">inciampati</span><span>|</span><a href="#37157739">prev</a><span>|</span><a href="#37156929">next</a><span>|</span><label class="collapse" for="c-37158494">[-]</label><label class="expand" for="c-37158494">[2 more]</label></div><br/><div class="children"><div class="content">One thing I&#x27;d like to see is more effort on developing citation systems for these models.<p>What I mean is that every part of the output of an LLM should be annotated with references to the content that is most important or relevant to it.<p>Who is leading this effort now?</div><br/><div id="37158528" class="c"><input type="checkbox" id="c-37158528" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37158494">parent</a><span>|</span><a href="#37156929">next</a><span>|</span><label class="collapse" for="c-37158528">[-]</label><label class="expand" for="c-37158528">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not possible to do this without a completely different and less efficient architecture. You can approximate it, but it won&#x27;t give you the correct answers, as there are no correct answers insofar as the model is learning to generalize rather than memorize things.<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;AnthropicAI&#x2F;status&#x2F;1688946685937090560" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;AnthropicAI&#x2F;status&#x2F;1688946685937090560</a></div><br/></div></div></div></div><div id="37156929" class="c"><input type="checkbox" id="c-37156929" checked=""/><div class="controls bullet"><span class="by">illusionist123</span><span>|</span><a href="#37158494">prev</a><span>|</span><a href="#37158690">next</a><span>|</span><label class="collapse" for="c-37156929">[-]</label><label class="expand" for="c-37156929">[7 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s not possible to get rid of hallucinations given the structure of LLMs. Getting rid of hallucinations requires knowing how to differentiate fact from fiction. An analogy from programming languages that people might understand is type systems. Well-typed programs are facts and ill-typed programs are fictions (relative to the given typing of the program). To eliminate hallucinations from LLMs would require something similar, i.e. a type system or grammar for what should be considered a fact. Another analogy is Prolog and logical resolution to determine consequences from a given database of facts. LLMs do not use logical resolution and they don&#x27;t have a database of facts to determine whether whatever is generated is actually factual (or logically follows from some set if facts) or not, LLMs are essentially Markov chains and I am certain it is impossible to have Markov chains without hallucinations.<p>So whoever is working on this problem, good luck because you have you have a lot of work to do to get Markov chains to only output facts and not just correlations of the training data.</div><br/><div id="37158948" class="c"><input type="checkbox" id="c-37158948" checked=""/><div class="controls bullet"><span class="by">blackkettle</span><span>|</span><a href="#37156929">parent</a><span>|</span><a href="#37156999">next</a><span>|</span><label class="collapse" for="c-37158948">[-]</label><label class="expand" for="c-37158948">[1 more]</label></div><br/><div class="children"><div class="content">While I&#x27;m definitely not going to argue that LLMs are inherently &#x27;thinking&#x27; like people do, one thing I do find pretty interesting is that all this talk about hallucinations and bias seems to often conveniently ignore the fact that people are often even more prone to these exact same problems - and as far as I know that&#x27;s also unlikely to be solved.<p>ChatGPT is often &#x27;confidently wrong&#x27; - I&#x27;m pretty sure I&#x27;ve been confidently wrong a few times too, and I&#x27;ve met a lot of other people in my life who&#x27;ve express that trait from time to time too, intentionally or otherwise.<p>I think there is an inherent trade off between &#x27;confidence&#x27;, &#x27;expression&#x27;, and of course &#x27;a-priori bias in the input&#x27;.  You can learn to be circumspect when you are unsure, and you can learn to better measure your level of expertise on a subject.<p>But you can&#x27;t escape that uncertainty entirely.  On the other hand, I&#x27;m not very convinced about efforts to train LLMs on things like mathematical reasoning.  These are situations where you really do have the tools to always produce an exact answer.  The goal in these types of problems should focus not on holistically learning how to both identify and solve them, but exclusively on how to identify and define them, and then subsequently pass them off to exact tools suitable for computing the solution.</div><br/></div></div><div id="37156999" class="c"><input type="checkbox" id="c-37156999" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37156929">parent</a><span>|</span><a href="#37158948">prev</a><span>|</span><a href="#37157607">next</a><span>|</span><label class="collapse" for="c-37156999">[-]</label><label class="expand" for="c-37156999">[3 more]</label></div><br/><div class="children"><div class="content">LLMs already know how to distinguish fact from fiction much better than random chance and the base non-RLHF GPT-4 model was excellently calibrated (its predicted confidence in an answer generally matches the probability of being correct). &quot;Eliminating&quot; it is not that important. Getting it to human levels is the goal. and boy do humans often &quot;hallucinate&quot;, i.e have a poor grasp of what they do or do not know and confidently spout nonsense.</div><br/><div id="37158006" class="c"><input type="checkbox" id="c-37158006" checked=""/><div class="controls bullet"><span class="by">illusionist123</span><span>|</span><a href="#37156929">root</a><span>|</span><a href="#37156999">parent</a><span>|</span><a href="#37157607">next</a><span>|</span><label class="collapse" for="c-37158006">[-]</label><label class="expand" for="c-37158006">[2 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t matter what humans do or do not do. Human performance as a benchmark is not a useful metric for what machines should or should not do.</div><br/><div id="37158341" class="c"><input type="checkbox" id="c-37158341" checked=""/><div class="controls bullet"><span class="by">camus_absurd</span><span>|</span><a href="#37156929">root</a><span>|</span><a href="#37158006">parent</a><span>|</span><a href="#37157607">next</a><span>|</span><label class="collapse" for="c-37158341">[-]</label><label class="expand" for="c-37158341">[1 more]</label></div><br/><div class="children"><div class="content">“Fake it till you make it”</div><br/></div></div></div></div></div></div><div id="37157607" class="c"><input type="checkbox" id="c-37157607" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37156929">parent</a><span>|</span><a href="#37156999">prev</a><span>|</span><a href="#37158690">next</a><span>|</span><label class="collapse" for="c-37157607">[-]</label><label class="expand" for="c-37157607">[2 more]</label></div><br/><div class="children"><div class="content">Technically, transformers condition on the entire past not just the last step, but RNNs are Markov Chains. RNNs have information bottleneck issues though.</div><br/><div id="37158772" class="c"><input type="checkbox" id="c-37158772" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#37156929">root</a><span>|</span><a href="#37157607">parent</a><span>|</span><a href="#37158690">next</a><span>|</span><label class="collapse" for="c-37158772">[-]</label><label class="expand" for="c-37158772">[1 more]</label></div><br/><div class="children"><div class="content">Transformers (for NLP) also perform steps on Markov chains. The difference is that with transformers (for NLP), which Markov chain it&#x27;s moving along changes every step.</div><br/></div></div></div></div></div></div><div id="37158690" class="c"><input type="checkbox" id="c-37158690" checked=""/><div class="controls bullet"><span class="by">_pdp_</span><span>|</span><a href="#37156929">prev</a><span>|</span><a href="#37157284">next</a><span>|</span><label class="collapse" for="c-37158690">[-]</label><label class="expand" for="c-37158690">[1 more]</label></div><br/><div class="children"><div class="content">Another challenge is also around how we think LLMs should be used vs understanding how LLMs can be used. It will take some time to figure this out.</div><br/></div></div><div id="37157284" class="c"><input type="checkbox" id="c-37157284" checked=""/><div class="controls bullet"><span class="by">techwizrd</span><span>|</span><a href="#37158690">prev</a><span>|</span><a href="#37157589">next</a><span>|</span><label class="collapse" for="c-37157284">[-]</label><label class="expand" for="c-37157284">[1 more]</label></div><br/><div class="children"><div class="content">I really like seeing articles or papers that describe the current advances and open challenges in a sub-field (such as [0]). They&#x27;re underappreciated, but good practice or reading for folks wanting to get in the field. They&#x27;re also worthwhile and humbling to look back at every few years: did we get the challenges right? How well did we understand the problem at the time?<p>0: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.04977" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.04977</a></div><br/></div></div><div id="37157589" class="c"><input type="checkbox" id="c-37157589" checked=""/><div class="controls bullet"><span class="by">karxxm</span><span>|</span><a href="#37157284">prev</a><span>|</span><a href="#37156585">next</a><span>|</span><label class="collapse" for="c-37157589">[-]</label><label class="expand" for="c-37157589">[1 more]</label></div><br/><div class="children"><div class="content">I have not seen many work on explainable AI regarding large language models. I remember many very nice visualizations and visual analysis tools trying to comprehend, what the network „is seeing“ (eg. in the realm of image classification) or doing</div><br/></div></div><div id="37156585" class="c"><input type="checkbox" id="c-37156585" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#37157589">prev</a><span>|</span><a href="#37156819">next</a><span>|</span><label class="collapse" for="c-37156585">[-]</label><label class="expand" for="c-37156585">[1 more]</label></div><br/><div class="children"><div class="content">Our APIs up to this point have been designed for computers. Json input, json output, and those are the nice ones.<p>I wonder if a deterministic but natural language API would be any better for LLMs to integrate with? Or do LLMs already speak Json well enough?</div><br/></div></div><div id="37156819" class="c"><input type="checkbox" id="c-37156819" checked=""/><div class="controls bullet"><span class="by">matanyal</span><span>|</span><a href="#37156585">prev</a><span>|</span><label class="collapse" for="c-37156819">[-]</label><label class="expand" for="c-37156819">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, no mention of Groq for number 6.</div><br/></div></div></div></div></div></div></div></body></html>