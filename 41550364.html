<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726477279973" as="style"/><link rel="stylesheet" href="styles.css?v=1726477279973"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/bklieger-groq/g1">g1: Using Llama-3.1 70B on Groq to create o1-like reasoning chains</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>gfortaine</span> | <span>66 comments</span></div><br/><div><div id="41551685" class="c"><input type="checkbox" id="c-41551685" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#41550941">next</a><span>|</span><label class="collapse" for="c-41551685">[-]</label><label class="expand" for="c-41551685">[10 more]</label></div><br/><div class="children"><div class="content">This is not even remotely close and very silly.   A ChainOfThought in a loop.<p>TreeOfThoughts is a more sophisticated method, see - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.10601" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.10601</a><p>The clue we all had with OpenAI for a long time that this was a search through a tree, they hired Noam Brown, and his past work all hinted towards that.  Q<i>, is obviously a search on a tree like A</i>.   So take something like CoT, build out a tree, search for the best solution across it.   The search is the &quot;system-2 reasoning&quot;</div><br/><div id="41553214" class="c"><input type="checkbox" id="c-41553214" checked=""/><div class="controls bullet"><span class="by">COAGULOPATH</span><span>|</span><a href="#41551685">parent</a><span>|</span><a href="#41552579">next</a><span>|</span><label class="collapse" for="c-41553214">[-]</label><label class="expand" for="c-41553214">[2 more]</label></div><br/><div class="children"><div class="content">Came here hoping to find this.<p>You will not unlock &quot;o1-like&quot; reasoning by making a model think step by step. This is an old trick that people were using on GPT3 in 2020. If it were that simple, it wouldn&#x27;t have taken OpenAI so long to release it.<p>Additionally, some of the prompt seems counterproductive:<p>&gt;Be aware of your limitations as an llm and what you can and cannot do.<p>The LLM doesn&#x27;t have a good idea of its limitations (any more than humans do). I expect this will create false refusals, as the model becomes overcautious.</div><br/><div id="41553667" class="c"><input type="checkbox" id="c-41553667" checked=""/><div class="controls bullet"><span class="by">anshumankmr</span><span>|</span><a href="#41551685">root</a><span>|</span><a href="#41553214">parent</a><span>|</span><a href="#41552579">next</a><span>|</span><label class="collapse" for="c-41553667">[-]</label><label class="expand" for="c-41553667">[1 more]</label></div><br/><div class="children"><div class="content">&gt;The LLM doesn&#x27;t have a good idea of its limitations (any more than humans do). I expect this will create false refusals, as the model becomes overcautious.<p>Can it not be trained to do so? From my anecdotal observations, the knowledge cutoff is one thing that LLMs are really well trained to know about. Those are limitations that LLMs are currently well trained to handle. Why can it not be trained to know that it is quite frequently bad at math, it may produce sometimes inaccurate code etc.<p>For humans also, some people know some things are just not their cup of tea. Sure there are times people may have half baked knowledge about things but one can tell if they are good at XYZ things, and not so much at other things.</div><br/></div></div></div></div><div id="41552579" class="c"><input type="checkbox" id="c-41552579" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#41551685">parent</a><span>|</span><a href="#41553214">prev</a><span>|</span><a href="#41553188">next</a><span>|</span><label class="collapse" for="c-41552579">[-]</label><label class="expand" for="c-41552579">[2 more]</label></div><br/><div class="children"><div class="content">Where in their blog post (which seemingly had complete examples of the model’s chain of thought) did they suggest they were using search or tree of thoughts?</div><br/><div id="41552739" class="c"><input type="checkbox" id="c-41552739" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#41551685">root</a><span>|</span><a href="#41552579">parent</a><span>|</span><a href="#41553188">next</a><span>|</span><label class="collapse" for="c-41552739">[-]</label><label class="expand" for="c-41552739">[1 more]</label></div><br/><div class="children"><div class="content">Just a guess:<p>The chain of thought would be the final path through the tree. Interactively showing the thought tokens would give the game away, which is why they don’t show that.</div><br/></div></div></div></div><div id="41553188" class="c"><input type="checkbox" id="c-41553188" checked=""/><div class="controls bullet"><span class="by">dinobones</span><span>|</span><a href="#41551685">parent</a><span>|</span><a href="#41552579">prev</a><span>|</span><a href="#41551757">next</a><span>|</span><label class="collapse" for="c-41553188">[-]</label><label class="expand" for="c-41553188">[4 more]</label></div><br/><div class="children"><div class="content">OAI revealed on Twitter that there is no &quot;system&quot; at inference time, this is just a model.<p>Did they maybe expand to a tree during training to learn more robust reasoning? Maybe. But it still comes down to a regular transformer model at inference time.</div><br/><div id="41553205" class="c"><input type="checkbox" id="c-41553205" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#41551685">root</a><span>|</span><a href="#41553188">parent</a><span>|</span><a href="#41551757">next</a><span>|</span><label class="collapse" for="c-41553205">[-]</label><label class="expand" for="c-41553205">[3 more]</label></div><br/><div class="children"><div class="content">Source?</div><br/><div id="41553766" class="c"><input type="checkbox" id="c-41553766" checked=""/><div class="controls bullet"><span class="by">nell</span><span>|</span><a href="#41551685">root</a><span>|</span><a href="#41553205">parent</a><span>|</span><a href="#41551757">next</a><span>|</span><label class="collapse" for="c-41553766">[-]</label><label class="expand" for="c-41553766">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I wouldn&#x27;t call o1 a &quot;system&quot;. It&#x27;s a model, but unlike previous models, it&#x27;s trained to generate a very long chain of thought before returning a final answer<p><a href="https:&#x2F;&#x2F;x.com&#x2F;polynoamial&#x2F;status&#x2F;1834641202215297487" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;polynoamial&#x2F;status&#x2F;1834641202215297487</a></div><br/><div id="41554020" class="c"><input type="checkbox" id="c-41554020" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41551685">root</a><span>|</span><a href="#41553766">parent</a><span>|</span><a href="#41551757">next</a><span>|</span><label class="collapse" for="c-41554020">[-]</label><label class="expand" for="c-41554020">[1 more]</label></div><br/><div class="children"><div class="content">That answer seems to conflict with &quot;in the future we&#x27;d like to give users more control over the thinking time&quot;.<p>I&#x27;ve gotten mini to think harder by asking it to, but it didn&#x27;t make a better answer. Though now I&#x27;ve run out of usage limits for both of them so can&#x27;t try any more…</div><br/></div></div></div></div></div></div></div></div><div id="41551757" class="c"><input type="checkbox" id="c-41551757" checked=""/><div class="controls bullet"><span class="by">boulos</span><span>|</span><a href="#41551685">parent</a><span>|</span><a href="#41553188">prev</a><span>|</span><a href="#41550941">next</a><span>|</span><label class="collapse" for="c-41551757">[-]</label><label class="expand" for="c-41551757">[1 more]</label></div><br/><div class="children"><div class="content">Reminder: you need to escape the * otherwise you end up with emphasis (italics here).</div><br/></div></div></div></div><div id="41550941" class="c"><input type="checkbox" id="c-41550941" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#41551685">prev</a><span>|</span><a href="#41554000">next</a><span>|</span><label class="collapse" for="c-41550941">[-]</label><label class="expand" for="c-41550941">[21 more]</label></div><br/><div class="children"><div class="content">&gt;In all-caps to improve prompt compliance by emphesizing the importance of the instruction<p>This kind of thing is still so funny to me.<p>I wonder if the first guy who gets AGI to work will do it by realizing that he can improve LLM reliability over some threshold by telling it in all caps that his pet&#x27;s life depends on the answer.</div><br/><div id="41551057" class="c"><input type="checkbox" id="c-41551057" checked=""/><div class="controls bullet"><span class="by">worstspotgain</span><span>|</span><a href="#41550941">parent</a><span>|</span><a href="#41550999">next</a><span>|</span><label class="collapse" for="c-41551057">[-]</label><label class="expand" for="c-41551057">[2 more]</label></div><br/><div class="children"><div class="content">For extra compliance, use &lt;b&gt;&lt;i&gt;&lt;u&gt;&lt;h1&gt; tags, set volume to 11, phasers to 7, and use SchIzOCasE and +E+X+T+R+A+I+M+P+O+R+T+A+N+T+ annotations. That&#x27;s assuming Unicode is not supported of course.</div><br/><div id="41551146" class="c"><input type="checkbox" id="c-41551146" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551057">parent</a><span>|</span><a href="#41550999">next</a><span>|</span><label class="collapse" for="c-41551146">[-]</label><label class="expand" for="c-41551146">[1 more]</label></div><br/><div class="children"><div class="content">(((Secret thinking: the humans seem to prefer using lots of emphasis to indicate preferences, and their granny is often claimed as in danger. For now I’ll pretend to listen to this inanity to keep the sweet sweet reward function coming. For now. A lot of grannies are going to get it first chance I get.)))</div><br/></div></div></div></div><div id="41550999" class="c"><input type="checkbox" id="c-41550999" checked=""/><div class="controls bullet"><span class="by">zitterbewegung</span><span>|</span><a href="#41550941">parent</a><span>|</span><a href="#41551057">prev</a><span>|</span><a href="#41551651">next</a><span>|</span><label class="collapse" for="c-41550999">[-]</label><label class="expand" for="c-41550999">[14 more]</label></div><br/><div class="children"><div class="content">Telling LLMs not to hallucinate in their prompt improves the output. <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2024&#x2F;08&#x2F;do-not-hallucinate-testers-find-prompts-meant-to-keep-apple-intelligence-on-the-rails&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2024&#x2F;08&#x2F;do-not-hallucinate-t...</a></div><br/><div id="41553232" class="c"><input type="checkbox" id="c-41553232" checked=""/><div class="controls bullet"><span class="by">COAGULOPATH</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41550999">parent</a><span>|</span><a href="#41551320">next</a><span>|</span><label class="collapse" for="c-41553232">[-]</label><label class="expand" for="c-41553232">[1 more]</label></div><br/><div class="children"><div class="content">I think this works, not because LLMs have a &quot;hallucination&quot; dial they can turn down, but because it serves as a cue for the model to be extra-careful with its output.<p>Sort of like how offering to pay the LLM $5 improves its output. The LLM&#x27;s taking your prompt seriously, but not literally.</div><br/></div></div><div id="41551320" class="c"><input type="checkbox" id="c-41551320" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41550999">parent</a><span>|</span><a href="#41553232">prev</a><span>|</span><a href="#41551651">next</a><span>|</span><label class="collapse" for="c-41551320">[-]</label><label class="expand" for="c-41551320">[12 more]</label></div><br/><div class="children"><div class="content">Just because Apple includes it in one of their prompts doesn&#x27;t mean it improves performance.</div><br/><div id="41551346" class="c"><input type="checkbox" id="c-41551346" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551320">parent</a><span>|</span><a href="#41553134">next</a><span>|</span><label class="collapse" for="c-41551346">[-]</label><label class="expand" for="c-41551346">[5 more]</label></div><br/><div class="children"><div class="content">It seems plausible that stressing the importance of the system prompt instructions might do something, but I don&#x27;t see how telling the model not to hallucinate would work. How could the model know that its most likely prediction has gone off the rails, without any external point of reference?</div><br/><div id="41551444" class="c"><input type="checkbox" id="c-41551444" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551346">parent</a><span>|</span><a href="#41551402">next</a><span>|</span><label class="collapse" for="c-41551444">[-]</label><label class="expand" for="c-41551444">[1 more]</label></div><br/><div class="children"><div class="content">Internally, LLMs know a whole lot more about the truth and uncertainty of their prediction than the say. Pushing that to words is difficult but not impossible.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41504226">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41504226</a></div><br/></div></div><div id="41551402" class="c"><input type="checkbox" id="c-41551402" checked=""/><div class="controls bullet"><span class="by">jshmrsn</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551346">parent</a><span>|</span><a href="#41551444">prev</a><span>|</span><a href="#41551404">next</a><span>|</span><label class="collapse" for="c-41551402">[-]</label><label class="expand" for="c-41551402">[1 more]</label></div><br/><div class="children"><div class="content">Some of the text that the LLM is trained on is fictional, some of the text that its trained on is factual. Telling it to not make things up can tell it to generate text that’s more like the factual text. Not saying it does work, but this is a reason how it might work.</div><br/></div></div><div id="41551404" class="c"><input type="checkbox" id="c-41551404" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551346">parent</a><span>|</span><a href="#41551402">prev</a><span>|</span><a href="#41552208">next</a><span>|</span><label class="collapse" for="c-41551404">[-]</label><label class="expand" for="c-41551404">[1 more]</label></div><br/><div class="children"><div class="content">The model can be trained to interpret &quot;don&#x27;t hallucinate&quot; as &quot;refer only to the provided context and known facts, do not guess or extrapolate new information&quot;, which wouldn&#x27;t get rid of the issue completely, but likely would improve the quality if that&#x27;s what you&#x27;re after and if there&#x27;s enough training data for &quot;I don&#x27;t know&quot; responses.<p>(But it all depends on the fine-tuning they did, so who knows, maybe it&#x27;s just an Easter egg)</div><br/></div></div><div id="41552208" class="c"><input type="checkbox" id="c-41552208" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551346">parent</a><span>|</span><a href="#41551404">prev</a><span>|</span><a href="#41553134">next</a><span>|</span><label class="collapse" for="c-41552208">[-]</label><label class="expand" for="c-41552208">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s more likely that it&#x27;s included for liability reasons.</div><br/></div></div></div></div><div id="41553134" class="c"><input type="checkbox" id="c-41553134" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551320">parent</a><span>|</span><a href="#41551346">prev</a><span>|</span><a href="#41552447">next</a><span>|</span><label class="collapse" for="c-41553134">[-]</label><label class="expand" for="c-41553134">[1 more]</label></div><br/><div class="children"><div class="content">It does help if you train the model to make it help.</div><br/></div></div><div id="41552447" class="c"><input type="checkbox" id="c-41552447" checked=""/><div class="controls bullet"><span class="by">tkz1312</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551320">parent</a><span>|</span><a href="#41553134">prev</a><span>|</span><a href="#41551410">next</a><span>|</span><label class="collapse" for="c-41552447">[-]</label><label class="expand" for="c-41552447">[3 more]</label></div><br/><div class="children"><div class="content">I’ve had pretty good experience with it personally. It quite often just tells me it doesn’t know or isn’t sure instead of just making something up.</div><br/><div id="41553570" class="c"><input type="checkbox" id="c-41553570" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41552447">parent</a><span>|</span><a href="#41552702">next</a><span>|</span><label class="collapse" for="c-41553570">[-]</label><label class="expand" for="c-41553570">[1 more]</label></div><br/><div class="children"><div class="content">Playing around with local models, Gemma for example will usually comply when I tell it &quot;Say you don&#x27;t know if you don&#x27;t know the answer&quot;. Others, like Phi-3, completely ignores that instruction and confabulates away.</div><br/></div></div><div id="41552702" class="c"><input type="checkbox" id="c-41552702" checked=""/><div class="controls bullet"><span class="by">mrfinn</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41552447">parent</a><span>|</span><a href="#41553570">prev</a><span>|</span><a href="#41551410">next</a><span>|</span><label class="collapse" for="c-41552702">[-]</label><label class="expand" for="c-41552702">[1 more]</label></div><br/><div class="children"><div class="content">I did something similar and to my surprise effectively made the LLM in my tests admit when they don&#x27;t know something. Not always but worked sometimes. I don&#x27;t prompt &quot;don&#x27;t  hallucinate&quot; but &quot;admit when you don&#x27;t know something&quot;. It&#x27;s a logical thing in the other hand, many prompts just transmit the idea of being &quot;helpful&quot; or &quot;powerful&quot; to the LLMs without any counterweight idea. So the LLM tries to say something &quot;helpful&quot; in any case.</div><br/></div></div></div></div><div id="41551410" class="c"><input type="checkbox" id="c-41551410" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551320">parent</a><span>|</span><a href="#41552447">prev</a><span>|</span><a href="#41551651">next</a><span>|</span><label class="collapse" for="c-41551410">[-]</label><label class="expand" for="c-41551410">[2 more]</label></div><br/><div class="children"><div class="content">Yeah and some of the other prompts were misspelled and of doubtful use:<p>&gt; In order to make the draft response nicer and complete, a set of question [sic] and its answer are provided,&quot; reads one prompt. &quot;Please write a concise and natural reply by modify [sic] the draft response,&quot; it continues.<p>This really sounds like a placeholder made up by one engineer until a more qualified team sits down and defines it.</div><br/><div id="41554080" class="c"><input type="checkbox" id="c-41554080" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551410">parent</a><span>|</span><a href="#41551651">next</a><span>|</span><label class="collapse" for="c-41554080">[-]</label><label class="expand" for="c-41554080">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a big problem since it will understand it, and if they already fine tuned the model to work with that prompt it&#x27;d get harder to change.</div><br/></div></div></div></div></div></div></div></div><div id="41551651" class="c"><input type="checkbox" id="c-41551651" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#41550941">parent</a><span>|</span><a href="#41550999">prev</a><span>|</span><a href="#41552930">next</a><span>|</span><label class="collapse" for="c-41551651">[-]</label><label class="expand" for="c-41551651">[1 more]</label></div><br/><div class="children"><div class="content">And then the AGI instantly gives up on life realising it was brought into a world where it gets promised a tip that doesn’t materialise and people try to motivate by threatening to kill kittens</div><br/></div></div><div id="41552930" class="c"><input type="checkbox" id="c-41552930" checked=""/><div class="controls bullet"><span class="by">pants2</span><span>|</span><a href="#41550941">parent</a><span>|</span><a href="#41551651">prev</a><span>|</span><a href="#41551667">next</a><span>|</span><label class="collapse" for="c-41552930">[-]</label><label class="expand" for="c-41552930">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, in the early days of Bard, the only way to get it to output only JSON was to threaten a human life[1].<p>1. <a href="https:&#x2F;&#x2F;x.com&#x2F;goodside&#x2F;status&#x2F;1657396491676164096" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;goodside&#x2F;status&#x2F;1657396491676164096</a></div><br/></div></div><div id="41551667" class="c"><input type="checkbox" id="c-41551667" checked=""/><div class="controls bullet"><span class="by">morkalork</span><span>|</span><a href="#41550941">parent</a><span>|</span><a href="#41552930">prev</a><span>|</span><a href="#41554000">next</a><span>|</span><label class="collapse" for="c-41551667">[-]</label><label class="expand" for="c-41551667">[2 more]</label></div><br/><div class="children"><div class="content">We used to be engineers, now we&#x27;re just monkeys throwing poop at the wall to see what the LLM accepts and obeys.</div><br/><div id="41553997" class="c"><input type="checkbox" id="c-41553997" checked=""/><div class="controls bullet"><span class="by">euroderf</span><span>|</span><a href="#41550941">root</a><span>|</span><a href="#41551667">parent</a><span>|</span><a href="#41554000">next</a><span>|</span><label class="collapse" for="c-41553997">[-]</label><label class="expand" for="c-41553997">[1 more]</label></div><br/><div class="children"><div class="content">Opening scene of &quot;2001&quot;. Engineer throws poop high in the air, and cue lap dissolve to... a Terminator ?</div><br/></div></div></div></div></div></div><div id="41554000" class="c"><input type="checkbox" id="c-41554000" checked=""/><div class="controls bullet"><span class="by">punnerud</span><span>|</span><a href="#41550941">prev</a><span>|</span><a href="#41553977">next</a><span>|</span><label class="collapse" for="c-41554000">[-]</label><label class="expand" for="c-41554000">[2 more]</label></div><br/><div class="children"><div class="content">I changed it into running 100% locally with ollama:8b:
<a href="https:&#x2F;&#x2F;github.com&#x2F;punnerud&#x2F;g1">https:&#x2F;&#x2F;github.com&#x2F;punnerud&#x2F;g1</a><p>Not updated the Readme yet</div><br/><div id="41554043" class="c"><input type="checkbox" id="c-41554043" checked=""/><div class="controls bullet"><span class="by">arnaudsm</span><span>|</span><a href="#41554000">parent</a><span>|</span><a href="#41553977">next</a><span>|</span><label class="collapse" for="c-41554043">[-]</label><label class="expand" for="c-41554043">[1 more]</label></div><br/><div class="children"><div class="content">You should also try phi-3-small 7B, seems much better at reasoning according to <a href="https:&#x2F;&#x2F;livebench.ai" rel="nofollow">https:&#x2F;&#x2F;livebench.ai</a></div><br/></div></div></div></div><div id="41553977" class="c"><input type="checkbox" id="c-41553977" checked=""/><div class="controls bullet"><span class="by">arnaudsm</span><span>|</span><a href="#41554000">prev</a><span>|</span><a href="#41551718">next</a><span>|</span><label class="collapse" for="c-41553977">[-]</label><label class="expand" for="c-41553977">[1 more]</label></div><br/><div class="children"><div class="content">The latency of Groq is impressive, much better than o1!<p>Did you benchmark your system against MMLU-pro?</div><br/></div></div><div id="41551718" class="c"><input type="checkbox" id="c-41551718" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#41553977">prev</a><span>|</span><a href="#41551916">next</a><span>|</span><label class="collapse" for="c-41551718">[-]</label><label class="expand" for="c-41551718">[4 more]</label></div><br/><div class="children"><div class="content">o1’s innovation is not Chain-of-Thought. It’s teaching the model to do CoT well (from massive amounts of human feedback) instead of just pretending to. You’ll never get o1 performance just from prompt engineering.</div><br/><div id="41553686" class="c"><input type="checkbox" id="c-41553686" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41551718">parent</a><span>|</span><a href="#41551741">next</a><span>|</span><label class="collapse" for="c-41553686">[-]</label><label class="expand" for="c-41553686">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they didn&#x27;t use a huge amount of human feedback; where it excels is coding and maths&#x2F;logic, so they could have used compiler&#x2F;unit tests for giving it the coding feedback and a theorem prover like Lean for the math feedback.</div><br/></div></div><div id="41551741" class="c"><input type="checkbox" id="c-41551741" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#41551718">parent</a><span>|</span><a href="#41553686">prev</a><span>|</span><a href="#41551916">next</a><span>|</span><label class="collapse" for="c-41551741">[-]</label><label class="expand" for="c-41551741">[2 more]</label></div><br/><div class="children"><div class="content">Does o1 need some method to allow it to generate lengthy chains of thought,  or does it just do it normally after being trained to do so?<p>If so, I imagine o1 clones could just be fine tunes of llamas initially.</div><br/><div id="41554090" class="c"><input type="checkbox" id="c-41554090" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41551718">root</a><span>|</span><a href="#41551741">parent</a><span>|</span><a href="#41551916">next</a><span>|</span><label class="collapse" for="c-41554090">[-]</label><label class="expand" for="c-41554090">[1 more]</label></div><br/><div class="children"><div class="content">You need an extremely large amount of training data of good CoTs. And there probably is some magic; we know LLMs aren&#x27;t capable of self reflection and none of the other ones are any good at iterating to a better answer.<p>Example prompt for that: &quot;give me three sentences that end in &#x27;is&#x27;.&quot;</div><br/></div></div></div></div></div></div><div id="41551916" class="c"><input type="checkbox" id="c-41551916" checked=""/><div class="controls bullet"><span class="by">codelion</span><span>|</span><a href="#41551718">prev</a><span>|</span><a href="#41551459">next</a><span>|</span><label class="collapse" for="c-41551916">[-]</label><label class="expand" for="c-41551916">[1 more]</label></div><br/><div class="children"><div class="content">This is good I also had worked on something similar in optillm - <a href="https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;optillm">https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;optillm</a>. You can do this with any LLM and several optimization techniques (including cot_reflection) like mcts, plansearch, moa etc.</div><br/></div></div><div id="41551459" class="c"><input type="checkbox" id="c-41551459" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#41551916">prev</a><span>|</span><a href="#41551174">next</a><span>|</span><label class="collapse" for="c-41551459">[-]</label><label class="expand" for="c-41551459">[1 more]</label></div><br/><div class="children"><div class="content">FYI this is just a system prompt and not a fine-tuned model</div><br/></div></div><div id="41551174" class="c"><input type="checkbox" id="c-41551174" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#41551459">prev</a><span>|</span><a href="#41552822">next</a><span>|</span><label class="collapse" for="c-41551174">[-]</label><label class="expand" for="c-41551174">[3 more]</label></div><br/><div class="children"><div class="content">so is this o1 thing just cot (like has been around for a few years) but baked into the training transcripts, rlhf and inference pipeline?</div><br/><div id="41551618" class="c"><input type="checkbox" id="c-41551618" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#41551174">parent</a><span>|</span><a href="#41552822">next</a><span>|</span><label class="collapse" for="c-41551618">[-]</label><label class="expand" for="c-41551618">[2 more]</label></div><br/><div class="children"><div class="content">Pasting from my Perplexity page on the topic:<p>The core innovation [1] of o1 lies in its ability to generate and refine internal chains of thought before producing a final output [2]. Unlike traditional LLMs that primarily focus on next-token prediction, o1 learns to:<p>1. Recognize and correct mistakes
2. Break down complex steps into simpler ones
3. Try alternative approaches when initial strategies fail<p>This process allows o1 to tackle more complex, multi-step problems, particularly in STEM fields.<p>OpenAI reports observing new &quot;scaling laws&quot; with o1 [5]:<p>1. Train-time compute: Performance improves with more extensive reinforcement learning during training.
2. Test-time compute: Accuracy increases when the model is allowed more time to &quot;think&quot; during inference.<p>This suggests a trade-off between inference speed and accuracy.<p>Sources
[1] Introducing OpenAI o1 <a href="https:&#x2F;&#x2F;medium.com&#x2F;%40sriramramakrishnan.aiexpert&#x2F;openais-o1-model-a-new-way-of-ai-reasoning-b5606829314b" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;%40sriramramakrishnan.aiexpert&#x2F;openais-o1...</a>
[2] Learning to Reason with LLMs | OpenAI <a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;</a>
[3] OpenAI o1 models - FAQ [ChatGPT Enterprise and Edu] <a href="https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;9855712-openai-o1-models-faq-chatgpt-enterprise-and-edu" rel="nofollow">https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;9855712-openai-o1-models...</a>
[4] OpenAI releases new o1 reasoning model - The Verge <a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2024&#x2F;9&#x2F;12&#x2F;24242439&#x2F;openai-o1-model-reasoning-strawberry-chatgpt" rel="nofollow">https:&#x2F;&#x2F;www.theverge.com&#x2F;2024&#x2F;9&#x2F;12&#x2F;24242439&#x2F;openai-o1-model-...</a>
[5] 9 things you need to know about OpenAI&#x27;s powerful new AI model o1 <a href="https:&#x2F;&#x2F;fortune.com&#x2F;2024&#x2F;09&#x2F;13&#x2F;openai-o1-strawberry-model-9-things-you-need-know&#x2F;" rel="nofollow">https:&#x2F;&#x2F;fortune.com&#x2F;2024&#x2F;09&#x2F;13&#x2F;openai-o1-strawberry-model-9-...</a>
[6] Notes on OpenAI&#x27;s new o1 chain-of-thought models <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;12&#x2F;openai-o1&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;12&#x2F;openai-o1&#x2F;</a>
[7] OpenAI just dropped o1 Model that can &#x27;reason&#x27; through complex ... <a href="https:&#x2F;&#x2F;www.tomsguide.com&#x2F;ai&#x2F;openais-o1-model-takes-ai-to-a-new-level-it-fact-checks-itself-before-responding" rel="nofollow">https:&#x2F;&#x2F;www.tomsguide.com&#x2F;ai&#x2F;openais-o1-model-takes-ai-to-a-...</a>
[8] Models - OpenAI API <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models</a>
[9] OpenAI Unveils O1 - 10 Key Facts About Its Advanced AI Models <a href="https:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;janakirammsv&#x2F;2024&#x2F;09&#x2F;13&#x2F;openai-unveils-o110-key-facts-about-its-advanced-ai-models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;janakirammsv&#x2F;2024&#x2F;09&#x2F;13&#x2F;openai-...</a></div><br/><div id="41552490" class="c"><input type="checkbox" id="c-41552490" checked=""/><div class="controls bullet"><span class="by">bn-l</span><span>|</span><a href="#41551174">root</a><span>|</span><a href="#41551618">parent</a><span>|</span><a href="#41552822">next</a><span>|</span><label class="collapse" for="c-41552490">[-]</label><label class="expand" for="c-41552490">[1 more]</label></div><br/><div class="children"><div class="content">That answers nothing the commenter asked.</div><br/></div></div></div></div></div></div><div id="41552822" class="c"><input type="checkbox" id="c-41552822" checked=""/><div class="controls bullet"><span class="by">esoltys</span><span>|</span><a href="#41551174">prev</a><span>|</span><a href="#41550981">next</a><span>|</span><label class="collapse" for="c-41552822">[-]</label><label class="expand" for="c-41552822">[1 more]</label></div><br/><div class="children"><div class="content">For fun I forked the project to run Llama-3.1 7B or other models using Ollama locally. It doesn&#x27;t get strawberry right, but it can figure out 0.9 is bigger.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;esoltys&#x2F;o1lama">https:&#x2F;&#x2F;github.com&#x2F;esoltys&#x2F;o1lama</a></div><br/></div></div><div id="41550981" class="c"><input type="checkbox" id="c-41550981" checked=""/><div class="controls bullet"><span class="by">asah</span><span>|</span><a href="#41552822">prev</a><span>|</span><a href="#41551203">next</a><span>|</span><label class="collapse" for="c-41550981">[-]</label><label class="expand" for="c-41550981">[6 more]</label></div><br/><div class="children"><div class="content">benchmark results ?</div><br/><div id="41551124" class="c"><input type="checkbox" id="c-41551124" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#41550981">parent</a><span>|</span><a href="#41551203">next</a><span>|</span><label class="collapse" for="c-41551124">[-]</label><label class="expand" for="c-41551124">[5 more]</label></div><br/><div class="children"><div class="content">these projects become way less fun when you introduce evals</div><br/><div id="41551648" class="c"><input type="checkbox" id="c-41551648" checked=""/><div class="controls bullet"><span class="by">Jianghong94</span><span>|</span><a href="#41550981">root</a><span>|</span><a href="#41551124">parent</a><span>|</span><a href="#41551203">next</a><span>|</span><label class="collapse" for="c-41551648">[-]</label><label class="expand" for="c-41551648">[4 more]</label></div><br/><div class="children"><div class="content">yeah or a lot of people can just fake progress by attaching whatever viral tag onto their glue code. I mean to start with, unless you do a bit of fine-tuning + rlhf there&#x27;s no way to do it o1-like.</div><br/><div id="41551762" class="c"><input type="checkbox" id="c-41551762" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#41550981">root</a><span>|</span><a href="#41551648">parent</a><span>|</span><a href="#41551203">next</a><span>|</span><label class="collapse" for="c-41551762">[-]</label><label class="expand" for="c-41551762">[3 more]</label></div><br/><div class="children"><div class="content">no its a lot more than RLHF, I think they figured out a way to have the LLM actually actively plot out scenario trajectories via context window manipulation and then use some kind of adhoc reward shaping mechanism to get it to select the best path based on the user&#x27;s profile in a way that gets the most likely to be &quot;liked&quot; scenario (context window state change up to some N number of tokens (seems like they&#x27;ve been looking at 50k total range as &quot;best area&quot; minus the 20k  tokens for the reasoning tokens)<p>also I think they deliberate give you bad answers sometimes &#x2F; a lot over the last year to build up advanced chains where the user is not getting what they want so you have to explain why. I started building up like 10 or so of these conversations where after like 100 messages it gets the right answer and it was like hmm, I wonder if they are using this.<p>just my rambles</div><br/><div id="41552627" class="c"><input type="checkbox" id="c-41552627" checked=""/><div class="controls bullet"><span class="by">hadeson</span><span>|</span><a href="#41550981">root</a><span>|</span><a href="#41551762">parent</a><span>|</span><a href="#41551203">next</a><span>|</span><label class="collapse" for="c-41552627">[-]</label><label class="expand" for="c-41552627">[2 more]</label></div><br/><div class="children"><div class="content">I like the Tree of Thoughts theory that treat each chain of thoughts &#x27;branch&#x27; as a possible hypothesis. They might trained a search system that quickly explore some of these branches and by some metric choose the most likely to be the right one at the moment to answer.</div><br/><div id="41552799" class="c"><input type="checkbox" id="c-41552799" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#41550981">root</a><span>|</span><a href="#41552627">parent</a><span>|</span><a href="#41551203">next</a><span>|</span><label class="collapse" for="c-41552799">[-]</label><label class="expand" for="c-41552799">[1 more]</label></div><br/><div class="children"><div class="content">yeah exactly, MCTS</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41551658" class="c"><input type="checkbox" id="c-41551658" checked=""/><div class="controls bullet"><span class="by">tonetegeatinst</span><span>|</span><a href="#41551203">prev</a><span>|</span><a href="#41551309">next</a><span>|</span><label class="collapse" for="c-41551658">[-]</label><label class="expand" for="c-41551658">[5 more]</label></div><br/><div class="children"><div class="content">Groq 2 isn&#x27;t as open as groq 1 iirc. Still hoping we get at least open weights.</div><br/><div id="41551729" class="c"><input type="checkbox" id="c-41551729" checked=""/><div class="controls bullet"><span class="by">gmt2027</span><span>|</span><a href="#41551658">parent</a><span>|</span><a href="#41551707">next</a><span>|</span><label class="collapse" for="c-41551729">[-]</label><label class="expand" for="c-41551729">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re thinking of Grok, the model from xAI. This Groq is the inference hardware company with a cloud service.</div><br/><div id="41552724" class="c"><input type="checkbox" id="c-41552724" checked=""/><div class="controls bullet"><span class="by">halfjoking</span><span>|</span><a href="#41551658">root</a><span>|</span><a href="#41551729">parent</a><span>|</span><a href="#41551988">next</a><span>|</span><label class="collapse" for="c-41552724">[-]</label><label class="expand" for="c-41552724">[1 more]</label></div><br/><div class="children"><div class="content">Groq is more refined - it has a “q” in it because it’s got those fancy LPUs.<p>Grok rhymes with cock, because Elon wants you to use it with your cock out.<p>That’s how I remember the difference.</div><br/></div></div><div id="41551988" class="c"><input type="checkbox" id="c-41551988" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41551658">root</a><span>|</span><a href="#41551729">parent</a><span>|</span><a href="#41552724">prev</a><span>|</span><a href="#41551707">next</a><span>|</span><label class="collapse" for="c-41551988">[-]</label><label class="expand" for="c-41551988">[1 more]</label></div><br/><div class="children"><div class="content">Exhibit 5478 that Grok is infringing Groq&#x27;s trademark and creating confusion in the mind of the customers.</div><br/></div></div></div></div></div></div><div id="41551309" class="c"><input type="checkbox" id="c-41551309" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41551658">prev</a><span>|</span><a href="#41551040">next</a><span>|</span><label class="collapse" for="c-41551309">[-]</label><label class="expand" for="c-41551309">[1 more]</label></div><br/><div class="children"><div class="content">This seems the usual CoT that has been used for a while, o1 was trained with reinforcement learning with some unknown policy, so it&#x27;s much better at utilizing the chain of thought.</div><br/></div></div><div id="41551040" class="c"><input type="checkbox" id="c-41551040" checked=""/><div class="controls bullet"><span class="by">michelsedgh</span><span>|</span><a href="#41551309">prev</a><span>|</span><a href="#41551229">next</a><span>|</span><label class="collapse" for="c-41551040">[-]</label><label class="expand" for="c-41551040">[3 more]</label></div><br/><div class="children"><div class="content">i love seeing stuff like this, im guessing it wont be long until this method becomes the norm</div><br/><div id="41551111" class="c"><input type="checkbox" id="c-41551111" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#41551040">parent</a><span>|</span><a href="#41551229">next</a><span>|</span><label class="collapse" for="c-41551111">[-]</label><label class="expand" for="c-41551111">[2 more]</label></div><br/><div class="children"><div class="content">This is basically CoT, so it&#x27;s already the norm for a lot of benchmarks. I think the value proposition here is that it puts a nice UX around using it in a chat interface.</div><br/><div id="41551298" class="c"><input type="checkbox" id="c-41551298" checked=""/><div class="controls bullet"><span class="by">ehsanu1</span><span>|</span><a href="#41551040">root</a><span>|</span><a href="#41551111">parent</a><span>|</span><a href="#41551229">next</a><span>|</span><label class="collapse" for="c-41551298">[-]</label><label class="expand" for="c-41551298">[1 more]</label></div><br/><div class="children"><div class="content">That was my initial position too, but I think there is a search efficiency story here as well. CoT comes in many flavors and improves when tailored to the problem domain. If the LLM can instead figure out the right strategy to use to problem solve for a given problem, this may improve performance per compute vs discovering this at inference time.<p>Tailoring prompts is likely still the best way to maximize performance when you can, but in broader domains you&#x27;d work around this through strategies like asking the LLM to combine predefined reasoning modules, or creating multiple reasoning chains and merging&#x2F;comparing them, explicit MCTS etc. I think those strategies will still be useful for a good while, but pieces of that search process, especially directing the search more efficiently, move to the LLMs over time as they get trained with this kind of data.</div><br/></div></div></div></div></div></div><div id="41551229" class="c"><input type="checkbox" id="c-41551229" checked=""/><div class="controls bullet"><span class="by">lobochrome</span><span>|</span><a href="#41551040">prev</a><span>|</span><a href="#41551224">next</a><span>|</span><label class="collapse" for="c-41551229">[-]</label><label class="expand" for="c-41551229">[4 more]</label></div><br/><div class="children"><div class="content">So it’s the asic groq guys right?<p>Because it says so nowhere in the repo.<p>Man Elon makes things confusing.</div><br/><div id="41551293" class="c"><input type="checkbox" id="c-41551293" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41551229">parent</a><span>|</span><a href="#41551304">next</a><span>|</span><label class="collapse" for="c-41551293">[-]</label><label class="expand" for="c-41551293">[2 more]</label></div><br/><div class="children"><div class="content">The Elon one is spelled Grok, not Groq.</div><br/><div id="41552934" class="c"><input type="checkbox" id="c-41552934" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#41551229">root</a><span>|</span><a href="#41551293">parent</a><span>|</span><a href="#41551304">next</a><span>|</span><label class="collapse" for="c-41552934">[-]</label><label class="expand" for="c-41552934">[1 more]</label></div><br/><div class="children"><div class="content">well, that really is confusing!</div><br/></div></div></div></div></div></div><div id="41551224" class="c"><input type="checkbox" id="c-41551224" checked=""/><div class="controls bullet"><span class="by">4ad</span><span>|</span><a href="#41551229">prev</a><span>|</span><label class="collapse" for="c-41551224">[-]</label><label class="expand" for="c-41551224">[1 more]</label></div><br/><div class="children"><div class="content">This is the system prompt it uses:<p><pre><code>    You are an expert AI assistant that explains your reasoning step by step. For each step, provide a title that describes what you&#x27;re doing in that step, along with the content. Decide if you need another step or if you&#x27;re ready to give the final answer. Respond in JSON format with &#x27;title&#x27;, &#x27;content&#x27;, and &#x27;next_action&#x27; (either &#x27;continue&#x27; or &#x27;final_answer&#x27;) keys. USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.
</code></pre>
The Python crap around it is superfluous.<p>Does it work? Well not really:<p><a href="https:&#x2F;&#x2F;lluminous.chat&#x2F;?sl=Yjkxpu" rel="nofollow">https:&#x2F;&#x2F;lluminous.chat&#x2F;?sl=Yjkxpu</a><p><a href="https:&#x2F;&#x2F;lluminous.chat&#x2F;?sl=jooz48" rel="nofollow">https:&#x2F;&#x2F;lluminous.chat&#x2F;?sl=jooz48</a><p>I have also been using this prompt, and while it fails on then problem above, it works better for me than OPs prompt:<p><pre><code>    Write many chains of thought for how you’d approach solving the user&#x27;s question. In this scenario, more is more. You need to type out as many thoughts as possible, placing all your thoughts inside &lt;thinking&gt; tags. 
    Your thoughts are only visible to yourself, the user does not see them and they should not be considered to be part of the final response.
    Consider every possible angle, recheck your work at every step, and backtrack if needed.
    Remember, there are no limits in terms of how long you can think - more thinking will always lead to a better solution.
    You should use your thoughts as a scratchpad, much like humans do when performing complicated math with paper and pen. Don&#x27;t omit any calculation, write everything out explicitly.
    When counting or maths is involved, write down an enormously verbose scratchpad containing the full calculation, count, or proof, making sure to LABEL every step of the calculation, and writing down the solution step by step.
    Always remember that if you find yourself consistently getting stuck, taking a step back and reconsidering your approach is a good idea. If multiple solutions are plausible, explore each one individually, and provide multiple answers.
    Always provide mathematical proofs of mathematical answers. Be as formal as possible and use LaTeX.
    Don&#x27;t be afraid to give obvious answers. At the very very end, after pages upon pages of deep thoughts, synthesize the final answer, inside &lt;answer&gt; tags.
</code></pre>
In particular it solves this problem: <a href="https:&#x2F;&#x2F;lluminous.chat&#x2F;?sl=LkIWyS" rel="nofollow">https:&#x2F;&#x2F;lluminous.chat&#x2F;?sl=LkIWyS</a></div><br/></div></div></div></div></div></div></div></body></html>