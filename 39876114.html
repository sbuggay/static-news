<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711875653635" as="style"/><link rel="stylesheet" href="styles.css?v=1711875653635"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://thegradient.pub/mamba-explained/">Mamba Explained</a>Â <span class="domain">(<a href="https://thegradient.pub">thegradient.pub</a>)</span></div><div class="subtext"><span>andreyk</span> | <span>41 comments</span></div><br/><div><div id="39877117" class="c"><input type="checkbox" id="c-39877117" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#39878061">next</a><span>|</span><label class="collapse" for="c-39877117">[-]</label><label class="expand" for="c-39877117">[21 more]</label></div><br/><div class="children"><div class="content">&gt; But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions.<p>Lately I&#x27;ve been wondering... is this a problem, or a strength?<p>It might be a fallacy to compare how LLMs &quot;think&quot; with how humans think. But humor me for a second. When you are speaking, each time you emit a word, you are not attending to every previous word in your sentence (like transformers), rather you have a state in your mind that represents the grammar and concepts, which is continuously updated as you speak (more similar to SSMs).<p>Similarly, when you read a book, every time you read a word, you are not attending to every previous word in the book. Your model of &quot;the book&quot; is rather a fuzzy&#x2F;approximate state that is updated with new information every time a new word appears. Right? (I&#x27;m sorry I know this is very handwavy and psuedoscientific but bear with me).<p>Ok, so if (big if) you feel like the above is true, then to match human-type language modelling, SSMs seem more human-like than transformers.<p>BUT... then aren&#x27;t transformers <i>strictly better</i> in terms of accuracy? Because a transformer never &quot;forgets&quot; information, as long as it is within the context window, because it revisits that information every time it emits a new token.<p>So let&#x27;s say we can remove the &quot;quadratic attention&quot; problem of transformers with SSMs. That&#x27;s a nice training&#x2F;inference performance boost. But... look at where we got with &quot;naive&quot; attention. GPT 4, Claude 3. It&#x27;s not like we&#x27;re hitting a wall with quadratic attention. It&#x27;s absurdly more expensive than SSMs, but GPUs certainly aren&#x27;t getting slower. If all AI work stops now, and only hardware improves, it wouldn&#x27;t be long until GPT4 could run on local hardware, right, provided Moore&#x27;s law?<p>&#x2F;end rant, not really sure what my point was, I&#x27;m not against SSMs (they&#x27;re cool) but rather I&#x27;m wondering if the SOTA will ever be SSM when attention is so damn good</div><br/><div id="39881203" class="c"><input type="checkbox" id="c-39881203" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39880443">next</a><span>|</span><label class="collapse" for="c-39881203">[-]</label><label class="expand" for="c-39881203">[3 more]</label></div><br/><div class="children"><div class="content">Yes transformers are obviously more capable than humans in my opinion. Claude can ingest dozens of pages in seconds and -- in a single shot -- write a summary bringing in relevant passages.<p>The innovation is not the speed, but the lack of recursion or iteration. Humans, even accomplished ones, have to reread sections and really &#x27;internalize&#x27; ideas before being able to summarize and very few humans can -- in a single attempt -- generate perfect speech. Most of us speak and unknowingly revise our own speech as we go along. Unlike transformers, that speak confidently, we start making a sentence and then decide halfway through its not going where we like. Then we start it over again, and by the powers of human attention, no one seems to really notice.<p>Transformers Are just insanely complicated and expensive to train.</div><br/><div id="39881719" class="c"><input type="checkbox" id="c-39881719" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39881203">parent</a><span>|</span><a href="#39881440">next</a><span>|</span><label class="collapse" for="c-39881719">[-]</label><label class="expand" for="c-39881719">[1 more]</label></div><br/><div class="children"><div class="content">I view transformers as like the language center of the brain. When we write or speak, especially when it&#x27;s critical to get things right, we have this ability to think &quot;that doesn&#x27;t make sense&quot; and start over. I view this recursion as more of a strength than weakness. You can get an LLM to generate an answer and when asked about the validity of the answer it would acknowledge that it got it wrong. This begs the question that if it had perfect recall and understanding why did it give the wrong answer in the first place?<p>I don&#x27;t know how the reasoning part comes to us but if we could implant that capability to a transformer model then it would end up pretty good.</div><br/></div></div><div id="39881440" class="c"><input type="checkbox" id="c-39881440" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39881203">parent</a><span>|</span><a href="#39881719">prev</a><span>|</span><a href="#39880443">next</a><span>|</span><label class="collapse" for="c-39881440">[-]</label><label class="expand" for="c-39881440">[1 more]</label></div><br/><div class="children"><div class="content">&gt; we start making a sentence and then decide halfway through its not going where we like<p>I&#x27;ll just add the observation that when we do this it&#x27;s largely based on feedback receive from the recipient (well, so long as you&#x27;re talking-with as opposed to talking-at) - we&#x27;re paying attention to how the audience is paying attention or not, any small facial tics that might betray skepticism or agreement and so on. I&#x27;m looking forward to interacting with an LLM that pairs an emotion-vector along with each token it has previously produced.<p>hume.ai goes a long way analyzing audio, just a matter of time before they&#x27;re ingesting realtime facial cues to also incorporate their audience&#x27;s reaction in their choice of what to say next</div><br/></div></div></div></div><div id="39880443" class="c"><input type="checkbox" id="c-39880443" checked=""/><div class="controls bullet"><span class="by">koayon</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39881203">prev</a><span>|</span><a href="#39880160">next</a><span>|</span><label class="collapse" for="c-39880443">[-]</label><label class="expand" for="c-39880443">[3 more]</label></div><br/><div class="children"><div class="content">This is a very fair point! If we had infinite compute then it&#x27;s undeniable that transformers (i.e. full attention) would be better (exactly as you characterise it)<p>But that&#x27;s the efficiency-effectiveness tradeoff that we have to make: given that compute is limited, would we prefer attention over shorter sequences or SSMs over longer sequences? The answer is probably &quot;well, it depends on your use case&quot; - I can definitely see reasons for both!<p>A fairly compelling thought for me is hybrid architectures (Jamba is a recent one). Here you can imagine having perfect recall over recent tokens and lossy recall over distant tokens. E.g. if the AI is generating a feature-length film, you &quot;could imagine having Attention look at the most recent frames for short-term fluidity and an SSM for long-term narrative consistency&quot; (quote from the OP)</div><br/><div id="39881659" class="c"><input type="checkbox" id="c-39881659" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39880443">parent</a><span>|</span><a href="#39880454">next</a><span>|</span><label class="collapse" for="c-39881659">[-]</label><label class="expand" for="c-39881659">[1 more]</label></div><br/><div class="children"><div class="content">If I remember it right, the llm big bird had something like this. For a particular word it would attend strongly with its closer neighbours but weakly to words far from it. Look for sparse attention. I think that&#x27;s the relevant terminology. Not sure if it matches exactly what you described</div><br/></div></div><div id="39880454" class="c"><input type="checkbox" id="c-39880454" checked=""/><div class="controls bullet"><span class="by">koayon</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39880443">parent</a><span>|</span><a href="#39881659">prev</a><span>|</span><a href="#39880160">next</a><span>|</span><label class="collapse" for="c-39880454">[-]</label><label class="expand" for="c-39880454">[1 more]</label></div><br/><div class="children"><div class="content">And given that the compute is O(n^2) with context window, it&#x27;s a very real tradeoff, at least in the short term</div><br/></div></div></div></div><div id="39880160" class="c"><input type="checkbox" id="c-39880160" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39880443">prev</a><span>|</span><a href="#39877211">next</a><span>|</span><label class="collapse" for="c-39880160">[-]</label><label class="expand" for="c-39880160">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; But Transformers have one core problem. In a transformer, every token can look back at every previous token when making predictions.<p>&gt; Lately I&#x27;ve been wondering... is this a problem, or a strength?<p>Exactly. There are lot of use cases where perfect recall is important. And earlier data may be more or less incompressible, such as if an LLM is working on a large table of data.<p>Maybe we&#x27;ll end up with different architectures being used for different applications. E.g. simple chat may be OK with an RNN type architecture.<p>I&#x27;ve also seen people combine Mamba and Transformer layers. Maybe that&#x27;s a good tradeoff for some other applications.</div><br/></div></div><div id="39877211" class="c"><input type="checkbox" id="c-39877211" checked=""/><div class="controls bullet"><span class="by">maccam912</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39880160">prev</a><span>|</span><a href="#39882120">next</a><span>|</span><label class="collapse" for="c-39877211">[-]</label><label class="expand" for="c-39877211">[1 more]</label></div><br/><div class="children"><div class="content">It depends on the task I imagine. Like writing a novel was mentioned, keeping important story lines in your memory for a long time will be necessary, or at least certainly more important than remembering what the characters were eating for lunch on page 10. But if you need to find that one loophole in a contact you probably will benefit from the perfect recall.</div><br/></div></div><div id="39882120" class="c"><input type="checkbox" id="c-39882120" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39877211">prev</a><span>|</span><a href="#39878574">next</a><span>|</span><label class="collapse" for="c-39882120">[-]</label><label class="expand" for="c-39882120">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s an SSM?<p>For the uninitiated (like me), apparently it stands for State Space Models.</div><br/></div></div><div id="39878574" class="c"><input type="checkbox" id="c-39878574" checked=""/><div class="controls bullet"><span class="by">y42</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39882120">prev</a><span>|</span><a href="#39880610">next</a><span>|</span><label class="collapse" for="c-39878574">[-]</label><label class="expand" for="c-39878574">[2 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Is this a problem or a strength?<p>I was wondering the same thing.  I understand, why the initial developers of this method declared it as a strength. Still I think it&#x27;s a problem, too:<p>If the Tranformer reads this sentence:<p>A equals B<p>It understands, that B comes after A and therefore A equals B. But how does it learn that after A comes B and therefore B equals A.<p>I am referring to the logical problems, that most (all?) modern language models suffer of.</div><br/><div id="39878949" class="c"><input type="checkbox" id="c-39878949" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39878574">parent</a><span>|</span><a href="#39880610">next</a><span>|</span><label class="collapse" for="c-39878949">[-]</label><label class="expand" for="c-39878949">[1 more]</label></div><br/><div class="children"><div class="content">I see many people get confused by this due to the widely spread (and false) &quot;stochastic parrot&quot; theme. But these models are much more than mere senzence-repeaters. In a way, the model is not learning that after A comes B. I mean, it could. With a lack of additional training data it probably would, too. But with enough data, this kind of sentence completion based purely on existing sentences no longer works because it would saturate parameters. So to retain and improve accuracy during training, it will have to come up with a compression that essentially forms a model of the real world. Or at least the world that the training corpus describes [1]. In that sense, it no longer &quot;knows&quot; that B comes after A (except for the input context), but it would have learned that there is a special relation between A and B. In can then also apply this kind of learned logic to new concepts that appear first in the context during inference. With all that happening internally, it only has to morph this state back into a natural language output. But with billions of parameters and countless layers, there is more than enough computational room for this to happen. In fact, recent models have shown that even small models can get pretty good at logic if you only get the training data right.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.13382</a></div><br/></div></div></div></div><div id="39880610" class="c"><input type="checkbox" id="c-39880610" checked=""/><div class="controls bullet"><span class="by">nlrk</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39878574">prev</a><span>|</span><a href="#39877883">next</a><span>|</span><label class="collapse" for="c-39880610">[-]</label><label class="expand" for="c-39880610">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; When you are speaking, each time you emit a word, you are not attending to every previous word in your sentence<p>I was exactly doing this until late in my youth. until I learnt people do it sequentially. But it is doable to create connections and pick the sensible case. Not the most relaxing thing.</div><br/></div></div><div id="39877883" class="c"><input type="checkbox" id="c-39877883" checked=""/><div class="controls bullet"><span class="by">aCoreyJ</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39880610">prev</a><span>|</span><a href="#39879053">next</a><span>|</span><label class="collapse" for="c-39877883">[-]</label><label class="expand" for="c-39877883">[4 more]</label></div><br/><div class="children"><div class="content">We&#x27;re running out of the ability to make transistors smaller and closer together so beyond some major breakthrough I wouldnt expect Moore&#x27;s law to continue nearly long enough to get to the point of running GPT4 on consumer hardware in the short term</div><br/><div id="39879059" class="c"><input type="checkbox" id="c-39879059" checked=""/><div class="controls bullet"><span class="by">timschmidt</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39877883">parent</a><span>|</span><a href="#39879053">next</a><span>|</span><label class="collapse" for="c-39879059">[-]</label><label class="expand" for="c-39879059">[3 more]</label></div><br/><div class="children"><div class="content">Ah, but we&#x27;ve just begun stacking transistors in the third dimension.</div><br/><div id="39879945" class="c"><input type="checkbox" id="c-39879945" checked=""/><div class="controls bullet"><span class="by">ctrw</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39879059">parent</a><span>|</span><a href="#39881444">next</a><span>|</span><label class="collapse" for="c-39879945">[-]</label><label class="expand" for="c-39879945">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t solve the problem, it just pushes is down the road a bit. The exponential growth is merely offset by a constant factor once. Unless we figure out how to push transistors in the 5th, 6th etc dimension with every new generation.</div><br/></div></div><div id="39881444" class="c"><input type="checkbox" id="c-39881444" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#39877117">root</a><span>|</span><a href="#39879059">parent</a><span>|</span><a href="#39879945">prev</a><span>|</span><a href="#39879053">next</a><span>|</span><label class="collapse" for="c-39881444">[-]</label><label class="expand" for="c-39881444">[1 more]</label></div><br/><div class="children"><div class="content">It was never a solution, Moore&#x27;s law has more than one dimension as well, not just density but heat dissipation. Can&#x27;t cool down a transistor that&#x27;s surrounded by transistors on all sides.</div><br/></div></div></div></div></div></div><div id="39879053" class="c"><input type="checkbox" id="c-39879053" checked=""/><div class="controls bullet"><span class="by">tippytippytango</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39877883">prev</a><span>|</span><a href="#39877430">next</a><span>|</span><label class="collapse" for="c-39879053">[-]</label><label class="expand" for="c-39879053">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a tradeoff to be managed depending on the application rather than a problem.</div><br/></div></div><div id="39877430" class="c"><input type="checkbox" id="c-39877430" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39879053">prev</a><span>|</span><a href="#39882097">next</a><span>|</span><label class="collapse" for="c-39877430">[-]</label><label class="expand" for="c-39877430">[1 more]</label></div><br/><div class="children"><div class="content">very good point and the sooner we can accept this difference (we access hyperdimensional entities we discover through language and math via fast and slow access and vocalize it through the alphabets we learned to read) the more &quot;intelligence&quot; we can unlock from AI.</div><br/></div></div><div id="39882097" class="c"><input type="checkbox" id="c-39882097" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39877430">prev</a><span>|</span><a href="#39878929">next</a><span>|</span><label class="collapse" for="c-39882097">[-]</label><label class="expand" for="c-39882097">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Lately I&#x27;ve been wondering... is this a problem, or a strength?<p>It&#x27;s a strength; fundamentally it&#x27;s impossible to achieve the same degree of accuracy with a sub-quadratic attention mechanism: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2209.04881" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2209.04881</a> (unless the Strong Exponential Time Hypothesis is false, which is very unlikely, like P=NP).</div><br/></div></div><div id="39878929" class="c"><input type="checkbox" id="c-39878929" checked=""/><div class="controls bullet"><span class="by">incrudible</span><span>|</span><a href="#39877117">parent</a><span>|</span><a href="#39882097">prev</a><span>|</span><a href="#39878061">next</a><span>|</span><label class="collapse" for="c-39878929">[-]</label><label class="expand" for="c-39878929">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s not like we&#x27;re hitting a wall with quadratic attention. It&#x27;s absurdly more expensive than SSMs, but GPUs certainly aren&#x27;t getting slower.<p>We are not hitting a wall, but a slope. Hardware improvements will not make up for it indefinitely. Software will have to make up for it, but the problem is that it costs millions of dollars to hit compile.</div><br/></div></div></div></div><div id="39878061" class="c"><input type="checkbox" id="c-39878061" checked=""/><div class="controls bullet"><span class="by">etbebl</span><span>|</span><a href="#39877117">prev</a><span>|</span><a href="#39877180">next</a><span>|</span><label class="collapse" for="c-39878061">[-]</label><label class="expand" for="c-39878061">[3 more]</label></div><br/><div class="children"><div class="content">Anyone else keep seeing articles about Mamba and thinking it&#x27;s about Python&#x2F;Conda? It&#x27;s annoying when the new cool thing picks the same name as something else you like that deserves attention.</div><br/><div id="39881861" class="c"><input type="checkbox" id="c-39881861" checked=""/><div class="controls bullet"><span class="by">tempaccount420</span><span>|</span><a href="#39878061">parent</a><span>|</span><a href="#39878394">next</a><span>|</span><label class="collapse" for="c-39881861">[-]</label><label class="expand" for="c-39881861">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like you need a language model to help you categorize Mamba articles into Python and non-Python articles?</div><br/></div></div><div id="39878394" class="c"><input type="checkbox" id="c-39878394" checked=""/><div class="controls bullet"><span class="by">ragebol</span><span>|</span><a href="#39878061">parent</a><span>|</span><a href="#39881861">prev</a><span>|</span><a href="#39877180">next</a><span>|</span><label class="collapse" for="c-39878394">[-]</label><label class="expand" for="c-39878394">[1 more]</label></div><br/><div class="children"><div class="content">&gt; attention<p>I see what you did there</div><br/></div></div></div></div><div id="39877180" class="c"><input type="checkbox" id="c-39877180" checked=""/><div class="controls bullet"><span class="by">password4321</span><span>|</span><a href="#39878061">prev</a><span>|</span><a href="#39877052">next</a><span>|</span><label class="collapse" for="c-39877180">[-]</label><label class="expand" for="c-39877180">[3 more]</label></div><br/><div class="children"><div class="content">Links to more about Mamba (selective state space models) on HN yesterday:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39853958#39855430">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39853958#39855430</a></div><br/><div id="39881799" class="c"><input type="checkbox" id="c-39881799" checked=""/><div class="controls bullet"><span class="by">fisian</span><span>|</span><a href="#39877180">parent</a><span>|</span><a href="#39877052">next</a><span>|</span><label class="collapse" for="c-39881799">[-]</label><label class="expand" for="c-39881799">[2 more]</label></div><br/><div class="children"><div class="content">This submission has the same content as the link here (submitted to HN about a month ago):<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39501982">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39501982</a>
<a href="https:&#x2F;&#x2F;www.kolaayonrinde.com&#x2F;blog&#x2F;2024&#x2F;02&#x2F;11&#x2F;mamba.html" rel="nofollow">https:&#x2F;&#x2F;www.kolaayonrinde.com&#x2F;blog&#x2F;2024&#x2F;02&#x2F;11&#x2F;mamba.html</a></div><br/><div id="39882270" class="c"><input type="checkbox" id="c-39882270" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#39877180">root</a><span>|</span><a href="#39881799">parent</a><span>|</span><a href="#39877052">next</a><span>|</span><label class="collapse" for="c-39882270">[-]</label><label class="expand" for="c-39882270">[1 more]</label></div><br/><div class="children"><div class="content">Yes and its the same author this time published on the Gradient (the link before was to the personal blog).  The Gradient by the way are amazing curators of AI news in general and have one of the better podcasts I am aware of interviewing developers in the trenches.<p>Adding: this resurgence in Mamba in general is also due to some actual sota progress with SSM like the new AI21 lab released this week [1] and likely to see others merging different architecture layers (this is a 52B MoE with 12B params active during inference blending both Mamba and transformers)<p>&gt;As the first production-grade model based on Mamba architecture, Jamba achieves an unprecedented 3X throughput and fits 140K context on a single GPU.<p>[1] <a href="https:&#x2F;&#x2F;www.ai21.com&#x2F;jamba" rel="nofollow">https:&#x2F;&#x2F;www.ai21.com&#x2F;jamba</a></div><br/></div></div></div></div></div></div><div id="39877052" class="c"><input type="checkbox" id="c-39877052" checked=""/><div class="controls bullet"><span class="by">xz18r</span><span>|</span><a href="#39877180">prev</a><span>|</span><a href="#39877735">next</a><span>|</span><label class="collapse" for="c-39877052">[-]</label><label class="expand" for="c-39877052">[2 more]</label></div><br/><div class="children"><div class="content">I just have to say it: that image shows gunpla, i.e. Mobile Suit Gundam, not Transformers!</div><br/><div id="39877087" class="c"><input type="checkbox" id="c-39877087" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#39877052">parent</a><span>|</span><a href="#39877735">next</a><span>|</span><label class="collapse" for="c-39877087">[-]</label><label class="expand" for="c-39877087">[1 more]</label></div><br/><div class="children"><div class="content">An official request has been made to ICANN to rescind the OP&#x27;s nerd card.</div><br/></div></div></div></div><div id="39877735" class="c"><input type="checkbox" id="c-39877735" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#39877052">prev</a><span>|</span><a href="#39878212">next</a><span>|</span><label class="collapse" for="c-39877735">[-]</label><label class="expand" for="c-39877735">[2 more]</label></div><br/><div class="children"><div class="content">This is the best explanation I have seen for Mamba.</div><br/><div id="39877772" class="c"><input type="checkbox" id="c-39877772" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39877735">parent</a><span>|</span><a href="#39878212">next</a><span>|</span><label class="collapse" for="c-39877772">[-]</label><label class="expand" for="c-39877772">[1 more]</label></div><br/><div class="children"><div class="content">TLDR: Friendship ended with transformers. Now Mamba is my best friend.</div><br/></div></div></div></div><div id="39878212" class="c"><input type="checkbox" id="c-39878212" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39877735">prev</a><span>|</span><a href="#39880910">next</a><span>|</span><label class="collapse" for="c-39878212">[-]</label><label class="expand" for="c-39878212">[2 more]</label></div><br/><div class="children"><div class="content">So in an effective Mamba query the question goes at the end, after input data? I thought that the question should go at the beginning, so it can decide which information in the data is relevant.</div><br/><div id="39880803" class="c"><input type="checkbox" id="c-39880803" checked=""/><div class="controls bullet"><span class="by">eropple</span><span>|</span><a href="#39878212">parent</a><span>|</span><a href="#39880910">next</a><span>|</span><label class="collapse" for="c-39880803">[-]</label><label class="expand" for="c-39880803">[1 more]</label></div><br/><div class="children"><div class="content">I could be wrong, as I haven&#x27;t used Mamba, but it seems to remain similar to transformers in that it doesn&#x27;t &quot;decide&quot; anything and streams tokens to follow the existing ones; attention isn&#x27;t a thing in the same way, but recency does still have impact. To that end, putting context after the question makes it more likely to follow the context, not the question.</div><br/></div></div></div></div><div id="39880910" class="c"><input type="checkbox" id="c-39880910" checked=""/><div class="controls bullet"><span class="by">jongjong</span><span>|</span><a href="#39878212">prev</a><span>|</span><a href="#39878054">next</a><span>|</span><label class="collapse" for="c-39880910">[-]</label><label class="expand" for="c-39880910">[5 more]</label></div><br/><div class="children"><div class="content">I find it difficult to understand certain math and science papers&#x2F;articles due to ambiguous use of language.<p>For example &quot;all previous tokens can be passed to the current token.&quot; That seems like a poorly constructed sentence. A token is not a function and it&#x27;s not an algorithm either... How can you pass tokens to a token? This type of ambiguous language in academic papers makes it hard to read... Maybe the phrase &#x27;every token has an association with every other previously encountered token&#x27; would be better? Or every token is used to compute the token vector for each token... I don&#x27;t know, all I can do is guess the meaning of the word &#x27;passed&#x27;. They want us to infer and fill in the gaps with our own assumptions. It assumes that we are primed to think in a certain highly constrained way...<p>For some reason a lot of academia around AI is littered with such imprecise language. They choose to use niche concepts and repurposed wording that their own small community invented rather using words and ideas that are more widely understood but which would convey the same information.<p>Rational people who aren&#x27;t directly involved in those fields who generally resist jumping to conclusions will struggle to understand what is meant because a lot of those words and ideas have different interpretations in their own fields.<p>I studied machine learning at university and wrote ANNs from scratch and trained them and even I find the language and concepts around LLMs too ambiguous. I&#x27;d rather just ask ChatGPT.<p>One thing that bothers me is that the community has moved away from relating concepts to neurons, interconnections, input layers, hidden layers and output layers. Instead, they jump straight into vectors and matrices... Pretending as though there is only one way to map those calculations to neurons and weights. But in fact, this abstraction has many possible interpretations. You could have fully connected layers or partially connected layers... Maybe you need a transformer only in front of the input layer or between every layer... So many possibilities.<p>The entire article means little if considered in isolation outside of the context of current configurations of various popular frameworks and tools.</div><br/><div id="39881204" class="c"><input type="checkbox" id="c-39881204" checked=""/><div class="controls bullet"><span class="by">derbOac</span><span>|</span><a href="#39880910">parent</a><span>|</span><a href="#39880951">next</a><span>|</span><label class="collapse" for="c-39881204">[-]</label><label class="expand" for="c-39881204">[1 more]</label></div><br/><div class="children"><div class="content">I agree although I&#x27;ve always interpreted it as a combination of difficulty explaining complex architecture, and also not really understanding why things work the way they do. A lot of modern AI sits in this kind of quasi-empirical realm just above (in an emergent properties sense) analytic math and statistics, and it seems like there&#x27;s not a very good integrative account or understanding of what&#x27;s going on, or a way of deriving what direction to go in. So you end up with poor explanations in part because the authors of the structures themselves don&#x27;t quite understand why things are working as they are.</div><br/></div></div><div id="39880951" class="c"><input type="checkbox" id="c-39880951" checked=""/><div class="controls bullet"><span class="by">king_magic</span><span>|</span><a href="#39880910">parent</a><span>|</span><a href="#39881204">prev</a><span>|</span><a href="#39878054">next</a><span>|</span><label class="collapse" for="c-39880951">[-]</label><label class="expand" for="c-39880951">[3 more]</label></div><br/><div class="children"><div class="content">that&#x27;s not what it says in the article. it actually says &quot;information from all previous tokens can be passed to the current token&quot;.<p>that statement is meaningfully different from &quot;all previous tokens can be passed to the current token&quot;. and both really makes sense if you understand attention mechanisms.</div><br/><div id="39881124" class="c"><input type="checkbox" id="c-39881124" checked=""/><div class="controls bullet"><span class="by">jongjong</span><span>|</span><a href="#39880910">root</a><span>|</span><a href="#39880951">parent</a><span>|</span><a href="#39878054">next</a><span>|</span><label class="collapse" for="c-39881124">[-]</label><label class="expand" for="c-39881124">[2 more]</label></div><br/><div class="children"><div class="content">Sorry for the misquote but it&#x27;s a distraction from my issue which was with the usage of the word &#x27;passed&#x27;.<p>Do you pass information from other tokens to a token in the sense that each token processes information from other tokens? A token isn&#x27;t a processing unit AFAIK, it&#x27;s just a word part. The processing is not the responsibility of the token itself. My understanding is that tokens may be associated with each other via an external structure but not passed to each other. Or maybe they meant a token vector? And the token vector contains information from related tokens? It&#x27;s unclear.<p>To me, &#x27;passed&#x27; means data passed to a function or algorithm for processing. It&#x27;s confusing unless a token is a function or algorithm.<p>My point is that this language only makes sense if you are already up to date in that field.</div><br/><div id="39881211" class="c"><input type="checkbox" id="c-39881211" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39880910">root</a><span>|</span><a href="#39881124">parent</a><span>|</span><a href="#39878054">next</a><span>|</span><label class="collapse" for="c-39881211">[-]</label><label class="expand" for="c-39881211">[1 more]</label></div><br/><div class="children"><div class="content">Well they gave the equations so follow closely where the token representations end up and how they&#x27;re acted upon.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>