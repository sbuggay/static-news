<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715850055887" as="style"/><link rel="stylesheet" href="styles.css?v=1715850055887"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/reworkd/tarsier">Show HN: Tarsier – Vision utilities for web interaction agents</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>KhoomeiK</span> | <span>56 comments</span></div><br/><div><div id="40369904" class="c"><input type="checkbox" id="c-40369904" checked=""/><div class="controls bullet"><span class="by">bckmn</span><span>|</span><a href="#40370833">next</a><span>|</span><label class="collapse" for="c-40369904">[-]</label><label class="expand" for="c-40369904">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of [Language as Intermediate Representation](<a href="https:&#x2F;&#x2F;chrisvoncsefalvay.com&#x2F;posts&#x2F;lair&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chrisvoncsefalvay.com&#x2F;posts&#x2F;lair&#x2F;</a>) - LLMs are optimized for language, so translate an image into language and they&#x27;ll do better at modeling it.</div><br/><div id="40370089" class="c"><input type="checkbox" id="c-40370089" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40369904">parent</a><span>|</span><a href="#40370833">next</a><span>|</span><label class="collapse" for="c-40370089">[-]</label><label class="expand" for="c-40370089">[1 more]</label></div><br/><div class="children"><div class="content">Cool connection, hadn&#x27;t seen this before but feels intuitively correct! I also formulate similar (but a bit more out-there) philosophical thoughts on word-meaning as being described by the topological structure of its corresponding images in embedding space, in Section 5.3 of my undergrad thesis [1].<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.16328" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.16328</a></div><br/></div></div></div></div><div id="40370833" class="c"><input type="checkbox" id="c-40370833" checked=""/><div class="controls bullet"><span class="by">abrichr</span><span>|</span><a href="#40369904">prev</a><span>|</span><a href="#40369721">next</a><span>|</span><label class="collapse" for="c-40370833">[-]</label><label class="expand" for="c-40370833">[3 more]</label></div><br/><div class="children"><div class="content">Congratulations on shipping!<p>In <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;blob&#x2F;main&#x2F;openadapt&#x2F;strategies&#x2F;visual.py">https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;blob&#x2F;main&#x2F;openadapt...</a> we use FastSAM to first segment the UI elements, then have the LLM describe each segment individually. This seems to work quite well; see <a href="https:&#x2F;&#x2F;twitter.com&#x2F;OpenAdaptAI&#x2F;status&#x2F;1789430587314336212" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;OpenAdaptAI&#x2F;status&#x2F;1789430587314336212</a> for a demo.<p>More coming soon!</div><br/><div id="40371651" class="c"><input type="checkbox" id="c-40371651" checked=""/><div class="controls bullet"><span class="by">jackienotchan</span><span>|</span><a href="#40370833">parent</a><span>|</span><a href="#40369721">next</a><span>|</span><label class="collapse" for="c-40371651">[-]</label><label class="expand" for="c-40371651">[2 more]</label></div><br/><div class="children"><div class="content">Looking at OpenAdapt, I&#x27;m wondering why they didn&#x27;t integrate Tarsier into AgentGPT, which is their flagship github repo but doesn&#x27;t seem to be under active development anymore.</div><br/><div id="40371832" class="c"><input type="checkbox" id="c-40371832" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40370833">root</a><span>|</span><a href="#40371651">parent</a><span>|</span><a href="#40369721">next</a><span>|</span><label class="collapse" for="c-40371832">[-]</label><label class="expand" for="c-40371832">[1 more]</label></div><br/><div class="children"><div class="content">We have a lot more powerful use-cases for Tarsier in web data extraction at the moment. Stay tuned for a broader launch soon!</div><br/></div></div></div></div></div></div><div id="40369721" class="c"><input type="checkbox" id="c-40369721" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#40370833">prev</a><span>|</span><a href="#40369614">next</a><span>|</span><label class="collapse" for="c-40369721">[-]</label><label class="expand" for="c-40369721">[7 more]</label></div><br/><div class="children"><div class="content">How do you make sure the tagging of elements is robust? With regular browser automation it&#x27;s quite hard to write selectors that will keep working after webpages get updated; often when writing E2E testing teams end up putting [data] attributes into the elements to aid with selection. Using a numerical identifier seems quite fragile.</div><br/><div id="40369843" class="c"><input type="checkbox" id="c-40369843" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40369721">parent</a><span>|</span><a href="#40369849">next</a><span>|</span><label class="collapse" for="c-40369843">[-]</label><label class="expand" for="c-40369843">[4 more]</label></div><br/><div class="children"><div class="content">Totally agreed—this is a design choice that basically comes from our agent architecture, and the codegen-based architecture that we think will likely proliferate for web agent tasks in the future. We provide Tarsier&#x27;s text&#x2F;screenshot to an LLM and have it write code with generically written selectors rather than the naive selectors that Tarsier assigns to each element.<p>It&#x27;s sort of like when you (as a human) write a web scraper and visually click on individual elements to look at the surrounding HTML structure &#x2F; their selectors, but then end up writing code with more general selectors—not copypasting the selectors of the elements you clicked.</div><br/><div id="40371007" class="c"><input type="checkbox" id="c-40371007" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#40369721">root</a><span>|</span><a href="#40369843">parent</a><span>|</span><a href="#40369849">next</a><span>|</span><label class="collapse" for="c-40371007">[-]</label><label class="expand" for="c-40371007">[3 more]</label></div><br/><div class="children"><div class="content">Ooh that&#x27;s a very neat approach, great idea! Chains of thought across abstraction layers. Definitely worth a blog post I reckon.<p>Good luck!</div><br/><div id="40371088" class="c"><input type="checkbox" id="c-40371088" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40369721">root</a><span>|</span><a href="#40371007">parent</a><span>|</span><a href="#40369849">next</a><span>|</span><label class="collapse" for="c-40371088">[-]</label><label class="expand" for="c-40371088">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! We might put out a paper about it with some Carnegie Mellon collaborators this summer.</div><br/><div id="40375814" class="c"><input type="checkbox" id="c-40375814" checked=""/><div class="controls bullet"><span class="by">bryanrasmussen</span><span>|</span><a href="#40369721">root</a><span>|</span><a href="#40371088">parent</a><span>|</span><a href="#40369849">next</a><span>|</span><label class="collapse" for="c-40375814">[-]</label><label class="expand" for="c-40375814">[1 more]</label></div><br/><div class="children"><div class="content">You might like to look at <a href="https:&#x2F;&#x2F;tsigalko18.github.io&#x2F;assets&#x2F;pdf&#x2F;2016-Leotta-JSEP.pdf" rel="nofollow">https:&#x2F;&#x2F;tsigalko18.github.io&#x2F;assets&#x2F;pdf&#x2F;2016-Leotta-JSEP.pdf</a><p>ROBULA+: An Algorithm for Generating Robust XPath Locators for Web Testing.</div><br/></div></div></div></div></div></div></div></div><div id="40369849" class="c"><input type="checkbox" id="c-40369849" checked=""/><div class="controls bullet"><span class="by">ghxst</span><span>|</span><a href="#40369721">parent</a><span>|</span><a href="#40369843">prev</a><span>|</span><a href="#40369614">next</a><span>|</span><label class="collapse" for="c-40369849">[-]</label><label class="expand" for="c-40369849">[2 more]</label></div><br/><div class="children"><div class="content">Great question, also situations where you have multiple CTAs with similar names&#x2F;contexts on a page is still something I see LLM based automation struggle with.</div><br/><div id="40370016" class="c"><input type="checkbox" id="c-40370016" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40369721">root</a><span>|</span><a href="#40369849">parent</a><span>|</span><a href="#40369614">next</a><span>|</span><label class="collapse" for="c-40370016">[-]</label><label class="expand" for="c-40370016">[1 more]</label></div><br/><div class="children"><div class="content">Hm, not sure I follow why those situations would be especially difficult? Regarding website changes, the nice thing about using LLMs is that we can simply provide the previous scraper as context and have it regenerate the scraper to &quot;self-heal&quot; when significant website changes are detected.</div><br/></div></div></div></div></div></div><div id="40369614" class="c"><input type="checkbox" id="c-40369614" checked=""/><div class="controls bullet"><span class="by">dbish</span><span>|</span><a href="#40369721">prev</a><span>|</span><a href="#40373310">next</a><span>|</span><label class="collapse" for="c-40369614">[-]</label><label class="expand" for="c-40369614">[8 more]</label></div><br/><div class="children"><div class="content">Very cool. We do something similar by combining OCR along with accessiblity data and other data (speech reco et. al.) for desktop based screensharing understanding, but evaluation compared to multi-modal LLMs has not been easy. How are you evaluating to come up with this number &quot;consistently beats multimodal GPT-4V&#x2F;4o + webpage screenshot by 10-20%,&quot;?<p>fwiw so far we&#x27;ve seen that Azure has the best OCR for screenshot type data across the proprietary and open source models, though we are far more focused on grabbing data from desktop based applications then web pages so ymmv</div><br/><div id="40369713" class="c"><input type="checkbox" id="c-40369713" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40369614">parent</a><span>|</span><a href="#40370183">next</a><span>|</span><label class="collapse" for="c-40369713">[-]</label><label class="expand" for="c-40369713">[4 more]</label></div><br/><div class="children"><div class="content">Yup, evals can definitely be tough. We basically have a suite of several hundred web data extraction evals in a tool we built called Bananalyzer [1]. It&#x27;s made it pretty straightforward for us to benchmark how accurately our agent generates code when it uses Tarsier-text (+ GPT-4) for perception v.s. Tarsier-screenshot (+ GPT-4V&#x2F;o).<p>Will have to look into supporting Azure OCR in Tarsier then—thanks for the tip!<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;reworkd&#x2F;bananalyzer">https:&#x2F;&#x2F;github.com&#x2F;reworkd&#x2F;bananalyzer</a></div><br/><div id="40371077" class="c"><input type="checkbox" id="c-40371077" checked=""/><div class="controls bullet"><span class="by">dbish</span><span>|</span><a href="#40369614">root</a><span>|</span><a href="#40369713">parent</a><span>|</span><a href="#40370968">next</a><span>|</span><label class="collapse" for="c-40371077">[-]</label><label class="expand" for="c-40371077">[1 more]</label></div><br/><div class="children"><div class="content">Awesome, will take a look at this. thank you</div><br/></div></div><div id="40370968" class="c"><input type="checkbox" id="c-40370968" checked=""/><div class="controls bullet"><span class="by">timabdulla</span><span>|</span><a href="#40369614">root</a><span>|</span><a href="#40369713">parent</a><span>|</span><a href="#40371077">prev</a><span>|</span><a href="#40370183">next</a><span>|</span><label class="collapse" for="c-40370968">[-]</label><label class="expand" for="c-40370968">[2 more]</label></div><br/><div class="children"><div class="content">Neat. Do you have the Bananalyzer eval results for Tarsier published somewhere?</div><br/><div id="40371189" class="c"><input type="checkbox" id="c-40371189" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40369614">root</a><span>|</span><a href="#40370968">parent</a><span>|</span><a href="#40370183">next</a><span>|</span><label class="collapse" for="c-40371189">[-]</label><label class="expand" for="c-40371189">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re hoping to release an evals paper about Bananalyzer this summer and compare Tarsier to a variety of other perception systems in it. The hard part with evaluating a perception&#x2F;context system though is that it&#x27;s very intertwined with the agent&#x27;s architecture, and that&#x27;s not something we&#x27;re comfortable fully open-sourcing yet. We&#x27;ll have to think of interesting ways to decouple the perception system and eval them with Bananalyzer.</div><br/></div></div></div></div></div></div><div id="40370183" class="c"><input type="checkbox" id="c-40370183" checked=""/><div class="controls bullet"><span class="by">SomaticPirate</span><span>|</span><a href="#40369614">parent</a><span>|</span><a href="#40369713">prev</a><span>|</span><a href="#40373310">next</a><span>|</span><label class="collapse" for="c-40370183">[-]</label><label class="expand" for="c-40370183">[3 more]</label></div><br/><div class="children"><div class="content">Surprised to hear Azure beats AWS Textract. I found it to be the best OCR offering but that was when I was doing documents.</div><br/><div id="40370931" class="c"><input type="checkbox" id="c-40370931" checked=""/><div class="controls bullet"><span class="by">navanchauhan</span><span>|</span><a href="#40369614">root</a><span>|</span><a href="#40370183">parent</a><span>|</span><a href="#40371069">next</a><span>|</span><label class="collapse" for="c-40370931">[-]</label><label class="expand" for="c-40370931">[1 more]</label></div><br/><div class="children"><div class="content">In my experience Azure is probably the best OCR offering right now. They are also the only ones to be able to recognize my terrible handwriting.</div><br/></div></div><div id="40371069" class="c"><input type="checkbox" id="c-40371069" checked=""/><div class="controls bullet"><span class="by">dbish</span><span>|</span><a href="#40369614">root</a><span>|</span><a href="#40370183">parent</a><span>|</span><a href="#40370931">prev</a><span>|</span><a href="#40373310">next</a><span>|</span><label class="collapse" for="c-40371069">[-]</label><label class="expand" for="c-40371069">[1 more]</label></div><br/><div class="children"><div class="content">Yes, Textract does not work as well for desktop screenshots from our testing</div><br/></div></div></div></div></div></div><div id="40373310" class="c"><input type="checkbox" id="c-40373310" checked=""/><div class="controls bullet"><span class="by">reidbarber</span><span>|</span><a href="#40369614">prev</a><span>|</span><a href="#40373831">next</a><span>|</span><label class="collapse" for="c-40373310">[-]</label><label class="expand" for="c-40373310">[1 more]</label></div><br/><div class="children"><div class="content">Neat! Been building something similar to the tagging feature in Typescript: <a href="https:&#x2F;&#x2F;github.com&#x2F;reidbarber&#x2F;webmarker">https:&#x2F;&#x2F;github.com&#x2F;reidbarber&#x2F;webmarker</a><p>The Python API on this is really nice though.</div><br/></div></div><div id="40373831" class="c"><input type="checkbox" id="c-40373831" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#40373310">prev</a><span>|</span><a href="#40371219">next</a><span>|</span><label class="collapse" for="c-40373831">[-]</label><label class="expand" for="c-40373831">[3 more]</label></div><br/><div class="children"><div class="content">A few questions:<p>Does this work in headless mode?<p>Are you getting a screenshot of the whole webpage including scrolling? Or just the visible part. The whole page, like singlepage.js would be great and is much more useful in many circumstances, although I&#x27;m not sure sure how to handle infinite scrolling. (If not, clean simple APIs for scrolling that don&#x27;t require fiddling and experimentation would be great.)<p>Instead of Google OCR (the only OCR), what about Apple&#x27;s native OCR? That would be amazing.</div><br/><div id="40374169" class="c"><input type="checkbox" id="c-40374169" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40373831">parent</a><span>|</span><a href="#40371219">next</a><span>|</span><label class="collapse" for="c-40374169">[-]</label><label class="expand" for="c-40374169">[2 more]</label></div><br/><div class="children"><div class="content">Yes it does work headless and we do grab a fullpage screenshot including scrolling (by resizing viewport to content height). We haven’t had to deal with infinite scrolling much but that’s an interesting feature we’d appreciate a PR for.<p>We haven’t tried Apple’s OCR but hopefully will integrate Azure OCR soon based on others’ advice.</div><br/><div id="40374610" class="c"><input type="checkbox" id="c-40374610" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#40373831">root</a><span>|</span><a href="#40374169">parent</a><span>|</span><a href="#40371219">next</a><span>|</span><label class="collapse" for="c-40374610">[-]</label><label class="expand" for="c-40374610">[1 more]</label></div><br/><div class="children"><div class="content">By Apple OCR, I mean instead of calling an external cloud API which requires tokens, etc. I simply mean vision which runs on OSX. It can be done in about 30 lines of Swift code.</div><br/></div></div></div></div></div></div><div id="40371219" class="c"><input type="checkbox" id="c-40371219" checked=""/><div class="controls bullet"><span class="by">savy91</span><span>|</span><a href="#40373831">prev</a><span>|</span><a href="#40369513">next</a><span>|</span><label class="collapse" for="c-40371219">[-]</label><label class="expand" for="c-40371219">[2 more]</label></div><br/><div class="children"><div class="content">Am I wrong thinking this could very well be the backbone of an alternative to the Rabbit AI? Where you basically end up having possibly infinite tools for your LLM assistant to use to reach a goal without having to build api integrations.</div><br/><div id="40371329" class="c"><input type="checkbox" id="c-40371329" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40371219">parent</a><span>|</span><a href="#40369513">next</a><span>|</span><label class="collapse" for="c-40371329">[-]</label><label class="expand" for="c-40371329">[1 more]</label></div><br/><div class="children"><div class="content">Yup, it could! There are a lot of players in the generalist personal web agent space but I personally think that use-case will be eaten by big players since fundamental foundation model improvements are required. That being said, Tarsier is a great place to start for building an open-source web agent for automating cool little tasks.<p>At Reworkd, we&#x27;re focused on web agents for data extraction at scale, which isn&#x27;t as hyped as the generalist agents but we find provides a lot of value and already works pretty well.</div><br/></div></div></div></div><div id="40369513" class="c"><input type="checkbox" id="c-40369513" checked=""/><div class="controls bullet"><span class="by">pk19238</span><span>|</span><a href="#40371219">prev</a><span>|</span><a href="#40370571">next</a><span>|</span><label class="collapse" for="c-40369513">[-]</label><label class="expand" for="c-40369513">[2 more]</label></div><br/><div class="children"><div class="content">this is such a creative solution. reminds me of how a team rendered wolfenstein into ASCII characters and fine tuned mistral to successfully play it.</div><br/><div id="40369569" class="c"><input type="checkbox" id="c-40369569" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40369513">parent</a><span>|</span><a href="#40370571">next</a><span>|</span><label class="collapse" for="c-40369569">[-]</label><label class="expand" for="c-40369569">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Yeah, it seems like a lot can be done with just text while we wait for multimodal models to catch up. The recent Platonic Representation Hypothesis [1] also suggests that different models, regardless of modality, build the same internal representations of the world.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.07987" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.07987</a></div><br/></div></div></div></div><div id="40370571" class="c"><input type="checkbox" id="c-40370571" checked=""/><div class="controls bullet"><span class="by">shekhar101</span><span>|</span><a href="#40369513">prev</a><span>|</span><a href="#40370829">next</a><span>|</span><label class="collapse" for="c-40370571">[-]</label><label class="expand" for="c-40370571">[7 more]</label></div><br/><div class="children"><div class="content">Tangential - I just want a decent (financial transaction) Table to text conversion that can retain the table structure well enough (e.g. merged cells) and have tried everything under the sun short of fine tuning my own model, including all the multimodal LLMs. None of them work very well without a lot of prompt engineering on case by case basis. Can this help? How can I set it up with a large number of pdfs that are sorted by type and extract tabular information? Any other suggestions?</div><br/><div id="40370868" class="c"><input type="checkbox" id="c-40370868" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#40370571">parent</a><span>|</span><a href="#40370651">next</a><span>|</span><label class="collapse" for="c-40370868">[-]</label><label class="expand" for="c-40370868">[1 more]</label></div><br/><div class="children"><div class="content">Or how about the opposite? Give me a CLI tool to pipe implicitly-tabular space-padded text into — a smart cut(1) — where I can say &quot;give me column 3&quot; and it understands how to analyze the document as a whole (or at least a running sample of a dozen lines or so), to model the correct column boundaries, to extract the contents of that column. (Which would also include trimming off any space-padding from the content. I want the data, not a fixed-width field containing it!)<p>For that matter, give me a CLI tool that takes in an <i>entire</i> such table, and lets me say &quot;give me rows 4-6 of column Foo&quot; — and it reads the table&#x27;s header (even through fancy box-drawing line-art) to determine which column is Foo, ignores any horizontal dividing lines, etc.<p>I&#x27;m not sure whether these tasks actually require full-on ML — probably just a pile of heuristics would work. Anything would be better than the low-level tools we have today.</div><br/></div></div><div id="40370651" class="c"><input type="checkbox" id="c-40370651" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40370571">parent</a><span>|</span><a href="#40370868">prev</a><span>|</span><a href="#40370795">next</a><span>|</span><label class="collapse" for="c-40370651">[-]</label><label class="expand" for="c-40370651">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an interesting problem—Tarsier probably isn&#x27;t the best solution here since it&#x27;s focused on webpage perception rather than any kind of OCR. But one could try adapting the `format_text` function in tarsier&#x2F;text_format.py to convert any set of OCR annotations to a whitespace-structured string. Curious to see if that works.</div><br/></div></div><div id="40370795" class="c"><input type="checkbox" id="c-40370795" checked=""/><div class="controls bullet"><span class="by">vikp</span><span>|</span><a href="#40370571">parent</a><span>|</span><a href="#40370651">prev</a><span>|</span><a href="#40370702">next</a><span>|</span><label class="collapse" for="c-40370795">[-]</label><label class="expand" for="c-40370795">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t specifically tuned for tables (more for general pdf to markdown), but it&#x27;s worked for some people with similar use-cases - <a href="https:&#x2F;&#x2F;github.com&#x2F;VikParuchuri&#x2F;marker">https:&#x2F;&#x2F;github.com&#x2F;VikParuchuri&#x2F;marker</a></div><br/></div></div><div id="40370702" class="c"><input type="checkbox" id="c-40370702" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#40370571">parent</a><span>|</span><a href="#40370795">prev</a><span>|</span><a href="#40371019">next</a><span>|</span><label class="collapse" for="c-40370702">[-]</label><label class="expand" for="c-40370702">[2 more]</label></div><br/><div class="children"><div class="content">Have you tried AWS textract for table extraction then LLM to format the data?</div><br/><div id="40371032" class="c"><input type="checkbox" id="c-40371032" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#40370571">root</a><span>|</span><a href="#40370702">parent</a><span>|</span><a href="#40371019">next</a><span>|</span><label class="collapse" for="c-40371032">[-]</label><label class="expand" for="c-40371032">[1 more]</label></div><br/><div class="children"><div class="content">Azure have a decent set of offerings for this too, they work quite well: <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-gb&#x2F;azure&#x2F;ai-services&#x2F;document-intelligence&#x2F;choose-model-feature?view=doc-intel-4.0.0" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-gb&#x2F;azure&#x2F;ai-services&#x2F;document...</a></div><br/></div></div></div></div><div id="40371019" class="c"><input type="checkbox" id="c-40371019" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#40370571">parent</a><span>|</span><a href="#40370702">prev</a><span>|</span><a href="#40370829">next</a><span>|</span><label class="collapse" for="c-40371019">[-]</label><label class="expand" for="c-40371019">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m having decent success with GPT4o on this. Have you given it a try? It probably varies from table structure to table structure.</div><br/></div></div></div></div><div id="40370829" class="c"><input type="checkbox" id="c-40370829" checked=""/><div class="controls bullet"><span class="by">shodai80</span><span>|</span><a href="#40370571">prev</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40370829">[-]</label><label class="expand" for="c-40370829">[8 more]</label></div><br/><div class="children"><div class="content">How do you know, for a specific webelement, what label it is associated with for a textbox or select?<p>For instance, I might want to tag as you did where elements are, but I still need an association with a label, quite often, to determine what the actual context of the textbox or select is.</div><br/><div id="40370919" class="c"><input type="checkbox" id="c-40370919" checked=""/><div class="controls bullet"><span class="by">awtkns</span><span>|</span><a href="#40370829">parent</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40370919">[-]</label><label class="expand" for="c-40370919">[7 more]</label></div><br/><div class="children"><div class="content">Tarsier provides a mapping of element number (eg: [23]) to xpath. So for any tagged item we&#x27;re able to map it back to the actual element in the DOM, allowing for easy interaction with the elements on the page.</div><br/><div id="40370929" class="c"><input type="checkbox" id="c-40370929" checked=""/><div class="controls bullet"><span class="by">shodai80</span><span>|</span><a href="#40370829">root</a><span>|</span><a href="#40370919">parent</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40370929">[-]</label><label class="expand" for="c-40370929">[6 more]</label></div><br/><div class="children"><div class="content">I understand that, I assume you are tagging the node and making a basic xpath to the node&#x2F;attribute with your tag id. Understood. But how relevant is tagging a node when I have no idea what the node is actually for?<p>EX: Given a simple login form, I may not know if the label is above or below the username textbox. A password box would be below it. I have a hard time understanding the relevance to tagging without context.<p>Tagging is basically irrelevant to any automated task if we do not know the context. I am not trying to diminish your great work, don&#x27;t get me wrong, but if you don&#x27;t have context I don&#x27;t see much relevance. Youre doing something that is easily scripted with xpath templates which I&#x27;ve done for over a decade.</div><br/><div id="40371042" class="c"><input type="checkbox" id="c-40371042" checked=""/><div class="controls bullet"><span class="by">awtkns</span><span>|</span><a href="#40370829">root</a><span>|</span><a href="#40370929">parent</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40371042">[-]</label><label class="expand" for="c-40371042">[5 more]</label></div><br/><div class="children"><div class="content">This is where a LLM comes it. In a typical pipeline would tag a page, transform it into a textual representation and then pass it to an llm which would be able to reason about which field(s) are the one you&#x27;re looking for much like a human.</div><br/><div id="40371264" class="c"><input type="checkbox" id="c-40371264" checked=""/><div class="controls bullet"><span class="by">shodai80</span><span>|</span><a href="#40370829">root</a><span>|</span><a href="#40371042">parent</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40371264">[-]</label><label class="expand" for="c-40371264">[4 more]</label></div><br/><div class="children"><div class="content">My point still stands. How do you augment data for an LLM when you know the context of a page? Do you go through every element and setup the data for an associated label? Do you use div scoping via offset parent through a script to generate associated div (good approach, bad in real-life conditions though)? Do you convert the DOM to JSON or some data structure? That means little because you still don&#x27;t have context, you&#x27;d have to do it by hand every time the layout changes...and you would have to be very specific, which is a separate problem for modeling as layouts are modified. What if the UI can be modified to have different layout types, such as label above, label to side, label below...where this can be dynamically set.<p>What I am pointing here is, even data modeling is mostly irrelevant unless you want to go through every page&#x2F;permutation of a page...all the while hoping the layout isn&#x27;t modified or back to training all over again...which is downtime, and at some point you&#x27;ll realize its just better to store user created xpath&#x27;s, as its quicker to update those than retrain.<p>How do you reason with an LLM without going through any of the above? Automation cannot consistently have downtime for retraining, it&#x27;s the antithesis for its purpose.<p>Let&#x27;s not even get into shadow dom issues.<p>I am keying on your third bullet point on Github:<p>&quot;How can you inform a text-only LLM about the page&#x27;s visual structure?&quot;<p>My questions suggest a gap in your awesome accomplishment.</div><br/><div id="40371469" class="c"><input type="checkbox" id="c-40371469" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40370829">root</a><span>|</span><a href="#40371264">parent</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40371469">[-]</label><label class="expand" for="c-40371469">[3 more]</label></div><br/><div class="children"><div class="content">We run OCR on the screenshot &amp; convert it to whitespace-structured text, that is passed to the LLM. The images below might make it clearer for you:<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;reworkd&#x2F;tarsier&#x2F;blob&#x2F;main&#x2F;.github&#x2F;assets&#x2F;tagged.png">https:&#x2F;&#x2F;github.com&#x2F;reworkd&#x2F;tarsier&#x2F;blob&#x2F;main&#x2F;.github&#x2F;assets&#x2F;...</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;reworkd&#x2F;tarsier&#x2F;blob&#x2F;main&#x2F;.github&#x2F;assets&#x2F;tagged_text.png">https:&#x2F;&#x2F;github.com&#x2F;reworkd&#x2F;tarsier&#x2F;blob&#x2F;main&#x2F;.github&#x2F;assets&#x2F;...</a></div><br/><div id="40371547" class="c"><input type="checkbox" id="c-40371547" checked=""/><div class="controls bullet"><span class="by">shodai80</span><span>|</span><a href="#40370829">root</a><span>|</span><a href="#40371469">parent</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40371547">[-]</label><label class="expand" for="c-40371547">[2 more]</label></div><br/><div class="children"><div class="content">Provided screenshots below do not show textboxes, selects, or other input nodes with labels. Show me text output with associated labels for inputs being correct and I will be shocked.</div><br/><div id="40371931" class="c"><input type="checkbox" id="c-40371931" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40370829">root</a><span>|</span><a href="#40371547">parent</a><span>|</span><a href="#40371223">next</a><span>|</span><label class="collapse" for="c-40371931">[-]</label><label class="expand" for="c-40371931">[1 more]</label></div><br/><div class="children"><div class="content">They do show textboxes with labels. From our readme:<p>&quot;Keep in mind that Tarsier tags different types of elements differently to help your LLM identify what actions are performable on each element. Specifically:<p>[#ID]: text-insertable fields (e.g. textarea, input with textual type)<p>[@ID]: hyperlinks (&lt;a&gt; tags)<p>[$ID]: other interactable elements (e.g. button, select)<p>[ID]: plain text (if you pass tag_text_elements=True)&quot;<p>Do you see the search boxes labeled [#4] and [#5] at the top? And before you say that the tag is on a different line from the placeholder text—yes, and our agent is smart enough to handle that minor idiosyncrasy. Are you shocked? :)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40371223" class="c"><input type="checkbox" id="c-40371223" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#40370829">prev</a><span>|</span><a href="#40373893">next</a><span>|</span><label class="collapse" for="c-40371223">[-]</label><label class="expand" for="c-40371223">[2 more]</label></div><br/><div class="children"><div class="content">How does the performance compare to VimGPT[0]?<p>I assume the screenshot-based approach is similar, whereas the text approach should be improved?<p>Very cool either way!<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;ishan0102&#x2F;vimGPT">https:&#x2F;&#x2F;github.com&#x2F;ishan0102&#x2F;vimGPT</a></div><br/><div id="40371276" class="c"><input type="checkbox" id="c-40371276" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40371223">parent</a><span>|</span><a href="#40373893">next</a><span>|</span><label class="collapse" for="c-40371276">[-]</label><label class="expand" for="c-40371276">[1 more]</label></div><br/><div class="children"><div class="content">VimGPT couples the perception to a specific LLM&#x2F;agent whereas Tarsier is solely a perception system that you can use for any uni&#x2F;multi-modal web agent. So it&#x27;s hard to compare, but you could say that VimGPT&#x27;s performance probably lies somewhere in the middle of Tarsier&#x27;s performance distribution (which varies as a function of your specific agent&#x2F;prompt system).</div><br/></div></div></div></div><div id="40373893" class="c"><input type="checkbox" id="c-40373893" checked=""/><div class="controls bullet"><span class="by">esha_manideep</span><span>|</span><a href="#40371223">prev</a><span>|</span><a href="#40374559">next</a><span>|</span><label class="collapse" for="c-40373893">[-]</label><label class="expand" for="c-40373893">[2 more]</label></div><br/><div class="children"><div class="content">Great work guys! How did you benchmark traiser&#x27;s 10-20% better? Would love to see exactly how each method scored</div><br/><div id="40374173" class="c"><input type="checkbox" id="c-40374173" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40373893">parent</a><span>|</span><a href="#40374559">next</a><span>|</span><label class="collapse" for="c-40374173">[-]</label><label class="expand" for="c-40374173">[1 more]</label></div><br/><div class="children"><div class="content">Great question! See this thread:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40369713">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40369713</a></div><br/></div></div></div></div><div id="40374559" class="c"><input type="checkbox" id="c-40374559" checked=""/><div class="controls bullet"><span class="by">jadbox</span><span>|</span><a href="#40373893">prev</a><span>|</span><a href="#40370869">next</a><span>|</span><label class="collapse" for="c-40374559">[-]</label><label class="expand" for="c-40374559">[1 more]</label></div><br/><div class="children"><div class="content">Anything like this for nodejs? (This is py)</div><br/></div></div><div id="40370869" class="c"><input type="checkbox" id="c-40370869" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#40374559">prev</a><span>|</span><a href="#40370957">next</a><span>|</span><label class="collapse" for="c-40370869">[-]</label><label class="expand" for="c-40370869">[2 more]</label></div><br/><div class="children"><div class="content">Since it is just a wrapper around hosted API if Google , can&#x27;t be ran as local fully opensource</div><br/><div id="40371113" class="c"><input type="checkbox" id="c-40371113" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40370869">parent</a><span>|</span><a href="#40370957">next</a><span>|</span><label class="collapse" for="c-40371113">[-]</label><label class="expand" for="c-40371113">[1 more]</label></div><br/><div class="children"><div class="content">More OCR providers are on the roadmap and we&#x27;d love for you to contribute any local OCR models you think could be useful! I wouldn&#x27;t call it a wrapper though :)</div><br/></div></div></div></div><div id="40370957" class="c"><input type="checkbox" id="c-40370957" checked=""/><div class="controls bullet"><span class="by">jackienotchan</span><span>|</span><a href="#40370869">prev</a><span>|</span><label class="collapse" for="c-40370957">[-]</label><label class="expand" for="c-40370957">[5 more]</label></div><br/><div class="children"><div class="content">Why was the Show HN text removed? Too much self promotion? You&#x27;re a YC company, so I&#x27;m surprised the mods would do that.<p><a href="https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=pastYear&amp;page=0&amp;prefix=true&amp;query=tarsier&amp;sort=byDate&amp;type=story" rel="nofollow">https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=pastYear&amp;page=0&amp;prefix=tru...</a><p>&gt; Hey HN! I built a tool that gives LLMs the ability to understand the visual structure of a webpage even if they don&#x27;t accept image input. We&#x27;ve found that unimodal GPT-4 + Tarsier&#x27;s textual webpage representation consistently beats multimodal GPT-4V&#x2F;4o + webpage screenshot by 10-20%, probably because multimodal LLMs still aren&#x27;t as performant as they&#x27;re hyped to be.
Over the course of experimenting with pruned HTML, accessibility trees, and other perception systems for web agents, we&#x27;ve iterated on Tarsier&#x27;s components to maximize downstream agent&#x2F;codegen performance.<p>Here&#x27;s the Tarsier pipeline in a nutshell:<p>1. tag interactable elements with IDs for the LLM to act upon &amp; grab a full-sized webpage screenshot<p>2. for text-only LLMs, run OCR on the screenshot &amp; convert it to whitespace-structured text (this is the coolest part imo)<p>3. map LLM intents back to actions on elements in the browser via an ID-to-XPath dict<p>Humans interact with the web through visually-rendered pages, and agents should too. We run Tarsier in production for thousands of web data extraction agents a day at Reworkd (<a href="https:&#x2F;&#x2F;reworkd.ai">https:&#x2F;&#x2F;reworkd.ai</a>).<p>By the way, we&#x27;re hiring backend&#x2F;infra engineers with experience in compute-intensive distributed systems!<p>reworkd.ai&#x2F;careers</div><br/><div id="40371116" class="c"><input type="checkbox" id="c-40371116" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40370957">parent</a><span>|</span><a href="#40370993">next</a><span>|</span><label class="collapse" for="c-40371116">[-]</label><label class="expand" for="c-40371116">[1 more]</label></div><br/><div class="children"><div class="content">Not sure what happened there! I&#x27;ve restored the text now.</div><br/></div></div><div id="40370993" class="c"><input type="checkbox" id="c-40370993" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40370957">parent</a><span>|</span><a href="#40371116">prev</a><span>|</span><label class="collapse" for="c-40370993">[-]</label><label class="expand" for="c-40370993">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for pointing this out! Yeah, it&#x27;s pretty strange. We thought including Show HN text was encouraged to engage with the community?</div><br/><div id="40371037" class="c"><input type="checkbox" id="c-40371037" checked=""/><div class="controls bullet"><span class="by">jackienotchan</span><span>|</span><a href="#40370957">root</a><span>|</span><a href="#40370993">parent</a><span>|</span><label class="collapse" for="c-40371037">[-]</label><label class="expand" for="c-40371037">[2 more]</label></div><br/><div class="children"><div class="content">Did you delete it or the mods?</div><br/><div id="40371075" class="c"><input type="checkbox" id="c-40371075" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40370957">root</a><span>|</span><a href="#40371037">parent</a><span>|</span><label class="collapse" for="c-40371075">[-]</label><label class="expand" for="c-40371075">[1 more]</label></div><br/><div class="children"><div class="content">Must have been the mods, I spent quite a bit of time on the content lol</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>