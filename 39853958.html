<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711702870159" as="style"/><link rel="stylesheet" href="styles.css?v=1711702870159"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.maginative.com/article/ai21-labs-unveils-jamba-the-first-production-grade-mamba-based-ai-model/">Jamba: Production-grade Mamba-based AI model</a> <span class="domain">(<a href="https://www.maginative.com">www.maginative.com</a>)</span></div><div class="subtext"><span>bubblehack3r</span> | <span>81 comments</span></div><br/><div><div id="39855430" class="c"><input type="checkbox" id="c-39855430" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#39856785">next</a><span>|</span><label class="collapse" for="c-39855430">[-]</label><label class="expand" for="c-39855430">[3 more]</label></div><br/><div class="children"><div class="content">There was a recent thread on explaining Mamba <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39501982">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39501982</a> (<a href="https:&#x2F;&#x2F;www.kolaayonrinde.com&#x2F;blog&#x2F;2024&#x2F;02&#x2F;11&#x2F;mamba.html" rel="nofollow">https:&#x2F;&#x2F;www.kolaayonrinde.com&#x2F;blog&#x2F;2024&#x2F;02&#x2F;11&#x2F;mamba.html</a>)<p>There was another one on the same thing, probably better <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39482428">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39482428</a> (<a href="https:&#x2F;&#x2F;jackcook.com&#x2F;2024&#x2F;02&#x2F;23&#x2F;mamba.html" rel="nofollow">https:&#x2F;&#x2F;jackcook.com&#x2F;2024&#x2F;02&#x2F;23&#x2F;mamba.html</a>)</div><br/><div id="39856065" class="c"><input type="checkbox" id="c-39856065" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#39855430">parent</a><span>|</span><a href="#39856785">next</a><span>|</span><label class="collapse" for="c-39856065">[-]</label><label class="expand" for="c-39856065">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! Macroexpanded:<p><i>Mamba Explained: The State Space Model Taking On Transformers</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39501982">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39501982</a> - Feb 2024 (93 comments)<p><i>Mamba: The Easy Way</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39482428">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39482428</a> - Feb 2024 (60 comments)<p><i>Is Mamba Capable of In-Context Learning?</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39286410">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39286410</a> - Feb 2024 (1 comment)<p><i>Vision Mamba: Efficient Visual Representation Learning with Bidirectional SSM</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39214939">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39214939</a> - Feb 2024 (16 comments)<p><i>MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38932350">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38932350</a> - Jan 2024 (39 comments)<p><i>Implementation of Mamba in one file of PyTorch</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38708730">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38708730</a> - Dec 2023 (109 comments)<p><i>Show HN: Fortran inference code for the Mamba state space language model</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38687342">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38687342</a> - Dec 2023 (1 comment)<p><i>Guide to the Mamba architecture that claims to be a replacement for Transformers</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38659238">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38659238</a> - Dec 2023 (2 comments)<p><i>Mamba outperforms transformers &quot;everywhere we tried&quot;</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38606590">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38606590</a> - Dec 2023 (25 comments)<p><i>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38522428">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38522428</a> - Dec 2023 (37 comments)<p><i>Mamba: New SSM arch with linear-time scaling that outperforms Transformers</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38520992">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38520992</a> - Dec 2023 (2 comments)</div><br/><div id="39860871" class="c"><input type="checkbox" id="c-39860871" checked=""/><div class="controls bullet"><span class="by">garyiskidding</span><span>|</span><a href="#39855430">root</a><span>|</span><a href="#39856065">parent</a><span>|</span><a href="#39856785">next</a><span>|</span><label class="collapse" for="c-39860871">[-]</label><label class="expand" for="c-39860871">[1 more]</label></div><br/><div class="children"><div class="content">thank you, these are very helpful.</div><br/></div></div></div></div></div></div><div id="39856785" class="c"><input type="checkbox" id="c-39856785" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#39855430">prev</a><span>|</span><a href="#39855435">next</a><span>|</span><label class="collapse" for="c-39856785">[-]</label><label class="expand" for="c-39856785">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone gotten this to work in linux using 1 or 2 4090s? I get stuck on &quot;Loading checkpoint shards:  71%&quot; and then it bails. But weirdly nvidia-smi shows plenty of VRAM available. My machine has 256gb of RAM so I don&#x27;t think that&#x27;s the problem either. Really excited to try this one.</div><br/></div></div><div id="39855435" class="c"><input type="checkbox" id="c-39855435" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39856785">prev</a><span>|</span><a href="#39855262">next</a><span>|</span><label class="collapse" for="c-39855435">[-]</label><label class="expand" for="c-39855435">[2 more]</label></div><br/><div class="children"><div class="content">To those curious about the tradeoffs between transformer and state space model layers, I highly recommend Sasha Rush&#x27;s video on it: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dKJEpOtVgXc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dKJEpOtVgXc</a></div><br/><div id="39860588" class="c"><input type="checkbox" id="c-39860588" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#39855435">parent</a><span>|</span><a href="#39855262">next</a><span>|</span><label class="collapse" for="c-39860588">[-]</label><label class="expand" for="c-39860588">[1 more]</label></div><br/><div class="children"><div class="content">They use less memory for inference but remember the details less well. For instance if you’re implementing code and want edits, it will forget various functions to be part of the script. Even transformers aren’t perfect at this and SSMs are even worse. For many use cases, that ability isn’t needed as much so the memory savings is a bigger lever.</div><br/></div></div></div></div><div id="39855262" class="c"><input type="checkbox" id="c-39855262" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#39855435">prev</a><span>|</span><a href="#39855349">next</a><span>|</span><label class="collapse" for="c-39855262">[-]</label><label class="expand" for="c-39855262">[15 more]</label></div><br/><div class="children"><div class="content">It&#x27;s great to see a full production level model using Mamba. But when it comes to long context window benchmarks, I&#x27;d love to see performance as well as throughput. I was under the impressions that Mamba has huge increases in throughput at the cost of modest losses in accuracy when using long contexts.</div><br/><div id="39856509" class="c"><input type="checkbox" id="c-39856509" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39855262">parent</a><span>|</span><a href="#39855406">next</a><span>|</span><label class="collapse" for="c-39856509">[-]</label><label class="expand" for="c-39856509">[3 more]</label></div><br/><div class="children"><div class="content">This one should have you covered :-) one out of every eight layers is a traditional Transformer layer, which should ensure precision, at least over short distances.</div><br/><div id="39859022" class="c"><input type="checkbox" id="c-39859022" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39856509">parent</a><span>|</span><a href="#39855406">next</a><span>|</span><label class="collapse" for="c-39859022">[-]</label><label class="expand" for="c-39859022">[2 more]</label></div><br/><div class="children"><div class="content">&gt; which should ensure precision, at least over short distances.<p>why? i dont follow. transformers should provide some attention over -all- distances, no? why does layering truncate this to &quot;short distances&quot;?</div><br/><div id="39859556" class="c"><input type="checkbox" id="c-39859556" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39859022">parent</a><span>|</span><a href="#39855406">next</a><span>|</span><label class="collapse" for="c-39859556">[-]</label><label class="expand" for="c-39859556">[1 more]</label></div><br/><div class="children"><div class="content">I mean &quot;short&quot; in comparison to the unlimited, but lossy recall that the Mamba blocks provide. Transformers are limited to the context length, while Mamba can carry along state. While it can remember things from a lot farther back, it is limited and must thus eventually drop things and&#x2F;or lose precision.</div><br/></div></div></div></div></div></div><div id="39855406" class="c"><input type="checkbox" id="c-39855406" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39855262">parent</a><span>|</span><a href="#39856509">prev</a><span>|</span><a href="#39855349">next</a><span>|</span><label class="collapse" for="c-39855406">[-]</label><label class="expand" for="c-39855406">[11 more]</label></div><br/><div class="children"><div class="content">I would too -- long context has been such a red herring across providers, Claude 3 is the first I&#x27;ve seen that seems to genuinely have some sort of qualitative leap in noticing things.<p>It is worth noting I&#x27;m fairly sure there&#x27;s no inherent theoratical decrease to accuracy in long contexts, the claimed theoratical change is an _increase_ in long-term accuracy in long contexts.</div><br/><div id="39856651" class="c"><input type="checkbox" id="c-39856651" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39855406">parent</a><span>|</span><a href="#39856525">next</a><span>|</span><label class="collapse" for="c-39856651">[-]</label><label class="expand" for="c-39856651">[3 more]</label></div><br/><div class="children"><div class="content">Every long context sucks right now. All the model providers benchmark on fact recall which is very limited. Actual ability to do anything complicated beyond 16k tokens is not present in any current model I have seen.</div><br/><div id="39859167" class="c"><input type="checkbox" id="c-39859167" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39856651">parent</a><span>|</span><a href="#39856525">next</a><span>|</span><label class="collapse" for="c-39859167">[-]</label><label class="expand" for="c-39859167">[2 more]</label></div><br/><div class="children"><div class="content">This is not current. GPT-4-Turbo (128k) has lossless recall to the first 64k input tokens and produces output indistinguishable from GPT-4 (32k), though both are limited to 4k output tokens.<p>Several downsides: Recall accuracy past the first 64k tokens suffers badly; Cost is astronomical; Response latency is too high for most interactive use-cases.<p>I would point out the astounding leap in input context in just one year. Should we assume effectively-infinite (RAG-free) context in the near-future?</div><br/><div id="39859404" class="c"><input type="checkbox" id="c-39859404" checked=""/><div class="controls bullet"><span class="by">anoncareer0212</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39859167">parent</a><span>|</span><a href="#39856525">next</a><span>|</span><label class="collapse" for="c-39859404">[-]</label><label class="expand" for="c-39859404">[1 more]</label></div><br/><div class="children"><div class="content">This is grossly untrue in a way that denotes surface-level familiarity on several fronts<p>You&#x27;re referring to the needle-in-a-haystack retrieval problem.<p>Which the person you&#x27;re replying to explicitly mentioned is the only benchmark providers are using, for good reason.<p>Consider the &quot;translate Moby Dick to comedic zoomer&quot; problem. This does not even come remotely close to working unless I do it in maximum chunks of 5,000 tokens.<p>Consider the API output limit of 4096 tokens, across all providers.<p>And no, you shouldn&#x27;t assume effectively infinite (RAG free) context in the near future. This time last year, Anthropic was demonstrating 120,000 token context. It released 200K a few weeks ago. And runtime cost scales with N^2.</div><br/></div></div></div></div></div></div><div id="39856525" class="c"><input type="checkbox" id="c-39856525" checked=""/><div class="controls bullet"><span class="by">binalpatel</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39855406">parent</a><span>|</span><a href="#39856651">prev</a><span>|</span><a href="#39856085">next</a><span>|</span><label class="collapse" for="c-39856525">[-]</label><label class="expand" for="c-39856525">[1 more]</label></div><br/><div class="children"><div class="content">Gemini 1.5 Pro is really good at long context in my experience.</div><br/></div></div><div id="39856085" class="c"><input type="checkbox" id="c-39856085" checked=""/><div class="controls bullet"><span class="by">Arthur_ODC</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39855406">parent</a><span>|</span><a href="#39856525">prev</a><span>|</span><a href="#39855349">next</a><span>|</span><label class="collapse" for="c-39856085">[-]</label><label class="expand" for="c-39856085">[6 more]</label></div><br/><div class="children"><div class="content">Long Context is great and all, but it sucks that all of these LLM&#x27;s have really poor output length. If I feed something an entire book and ask for a comprehensive summary then I&#x27;m expecting at least a full 3-page summary. I get that they try to force these things to be &quot;concise&quot; to save on compute, but good lord it&#x27;s so annoying.</div><br/><div id="39857749" class="c"><input type="checkbox" id="c-39857749" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39856085">parent</a><span>|</span><a href="#39856237">next</a><span>|</span><label class="collapse" for="c-39857749">[-]</label><label class="expand" for="c-39857749">[3 more]</label></div><br/><div class="children"><div class="content">Have you tried asking it for a specific concrete length, like a number of words? I was also frustrated with concise answers when asking for long ones, but I found that the outputs improved significantly if I asked for e.g. 4000 words specifically. Further than that, have it break it down into sections and write X words per section.</div><br/><div id="39857942" class="c"><input type="checkbox" id="c-39857942" checked=""/><div class="controls bullet"><span class="by">Arthur_ODC</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39857749">parent</a><span>|</span><a href="#39856237">next</a><span>|</span><label class="collapse" for="c-39857942">[-]</label><label class="expand" for="c-39857942">[2 more]</label></div><br/><div class="children"><div class="content">Yes, all the possible length extending custom instructions you can think of. I can get some reasonable length responses out of it, but I&#x27;ve never seen them go over 1 page worth, and multi-shot example prompts using multiple USER and GPT exchanges to define the format. Seems like GPT4 has a hard limit as to how much it will output when you click &quot;continue&quot;, and Claude Opus never goes over a page either. Another user pointed out using the API, which I have done in the past, but it&#x27;s been a long while, and I can&#x27;t really justify the cost of using the advanced models via API for my general use.</div><br/><div id="39858535" class="c"><input type="checkbox" id="c-39858535" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39857942">parent</a><span>|</span><a href="#39856237">next</a><span>|</span><label class="collapse" for="c-39858535">[-]</label><label class="expand" for="c-39858535">[1 more]</label></div><br/><div class="children"><div class="content">Everyone&#x27;s coalescing at a max of 4096 tokens&#x2F;12 &quot;pages&quot; via API (page is 250 words, which is 1 8.5&quot;x11&quot; double spaced)<p>To your point, doesn&#x27;t matter anyway, it&#x27;s nigh impossible to get over 2K of output with every trick and bit of guidance you can think of (I got desperate when 16K&#x2F;48 pages came out to &quot;make it work&quot;, even completely deforming tricks  like making it number each line and write a reminder on each line that it should write 1000 lines don&#x27;t work)</div><br/></div></div></div></div></div></div><div id="39856237" class="c"><input type="checkbox" id="c-39856237" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39856085">parent</a><span>|</span><a href="#39857749">prev</a><span>|</span><a href="#39855349">next</a><span>|</span><label class="collapse" for="c-39856237">[-]</label><label class="expand" for="c-39856237">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a chat gpt problem, if you hit the API it&#x27;s not nearly so hard to get good output.</div><br/><div id="39856598" class="c"><input type="checkbox" id="c-39856598" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39855262">root</a><span>|</span><a href="#39856237">parent</a><span>|</span><a href="#39855349">next</a><span>|</span><label class="collapse" for="c-39856598">[-]</label><label class="expand" for="c-39856598">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t say that, my latest big user story for making sure I&#x27;m handling huge inputs was &quot;translate Moby dick to zoomer&quot;. Cant give any service chunks larger than ~5K tokens, over API, without it failing.<p>(Miserably, like, I&#x27;d be fine if it gave a paragraph back. But at least on this &quot;map&quot; task, there&#x27;s a critical point where there&#x27;s so much input that the reward function ends up imitating the input more instead of chatting)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39855349" class="c"><input type="checkbox" id="c-39855349" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#39855262">prev</a><span>|</span><a href="#39855292">next</a><span>|</span><label class="collapse" for="c-39855349">[-]</label><label class="expand" for="c-39855349">[15 more]</label></div><br/><div class="children"><div class="content">&gt; Jamba boasts an extensive context window of 256K tokens, equivalent to around 210 pages of text, while fitting up to 140K tokens on a single 80GB GPU.<p>I realize this is a big improvement, but it’s striking how inefficient LLM’s are, that you need 80GB of GPU memory to analyze less than 1 megabyte of data. That’s a lot of bloat! Hopefully there’s a lot of room for algorithmic improvements.</div><br/><div id="39860568" class="c"><input type="checkbox" id="c-39860568" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39855349">parent</a><span>|</span><a href="#39861610">next</a><span>|</span><label class="collapse" for="c-39860568">[-]</label><label class="expand" for="c-39860568">[2 more]</label></div><br/><div class="children"><div class="content">The big (huge?) memory requirement is during training.  These LLMs work with high dimensional vectors and they calculate gradients with respect to high dimensional vectors and they do updates that require state of the optimizer.  If you have 3 particles in 3 dimensions and you need their forces that creates 3 new 3D vectors and once you update their position along the forces then they also carry momenta.  Now generalize these simple 3-body physics to the typical 60-layer creatures inside the LLM with vectors of several thousand dimensions, interactions&#x2F;weights that are scaling like the squares of these vectors, to a total parameter count that adds up to the 10s to 100s of billions of parameters, and then take derivatives and start to keep track of momenta.  It is a feat of modern engineering that some groups can train such models efficiently. I hope we will see more of the training stories becoming public in the near future.</div><br/><div id="39861600" class="c"><input type="checkbox" id="c-39861600" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39860568">parent</a><span>|</span><a href="#39861610">next</a><span>|</span><label class="collapse" for="c-39861600">[-]</label><label class="expand" for="c-39861600">[1 more]</label></div><br/><div class="children"><div class="content">This is wrong. You need big memory during inference too.<p>The difference there is you can use tricks like quantisation and offloading to CPU to reduce it somewhat at the cost of accuracy and&#x2F;or speed.</div><br/></div></div></div></div><div id="39861610" class="c"><input type="checkbox" id="c-39861610" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39855349">parent</a><span>|</span><a href="#39860568">prev</a><span>|</span><a href="#39857344">next</a><span>|</span><label class="collapse" for="c-39861610">[-]</label><label class="expand" for="c-39861610">[1 more]</label></div><br/><div class="children"><div class="content">That’s all the world’s knowledge compressed into 80GB. It’s not analysing 1MB data, it’s analysing all of that knowledge plus and additional 1MB.</div><br/></div></div><div id="39857344" class="c"><input type="checkbox" id="c-39857344" checked=""/><div class="controls bullet"><span class="by">nostrowski</span><span>|</span><a href="#39855349">parent</a><span>|</span><a href="#39861610">prev</a><span>|</span><a href="#39857845">next</a><span>|</span><label class="collapse" for="c-39857344">[-]</label><label class="expand" for="c-39857344">[2 more]</label></div><br/><div class="children"><div class="content">Two things I&#x27;m curious to know:<p>1. How many tokens can &#x27;traditional&#x27; models (e.g. Mistral&#x27;s 8x7B) fit on a single 80GB GPU?
2. How does quantization affect the single transformer layer in the stack? What are the performance&#x2F;accuracy trade-offs that happen when so little of the stack depends on this bottleneck?</div><br/><div id="39857449" class="c"><input type="checkbox" id="c-39857449" checked=""/><div class="controls bullet"><span class="by">patrakov</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39857344">parent</a><span>|</span><a href="#39857845">next</a><span>|</span><label class="collapse" for="c-39857449">[-]</label><label class="expand" for="c-39857449">[1 more]</label></div><br/><div class="children"><div class="content">Mixtral 8x7b runs well (i.e., produces the correct output faster than I can read it) on a modern AMD or Intel laptop without any use of a GPU - provided that you have enough RAM and CPU cores. 32 GB of RAM and 16 hyperthreads are enough with 4-bit quantization if you don&#x27;t ask too much in terms of context.<p>P.S. Dell Inspiron 7415 upgraded to 64 GB of RAM here.</div><br/></div></div></div></div><div id="39857845" class="c"><input type="checkbox" id="c-39857845" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39855349">parent</a><span>|</span><a href="#39857344">prev</a><span>|</span><a href="#39861496">next</a><span>|</span><label class="collapse" for="c-39857845">[-]</label><label class="expand" for="c-39857845">[1 more]</label></div><br/><div class="children"><div class="content">&gt; that you need 80GB of GPU memory to analyze less than 1 megabyte of data<p>80GB is compressed all human knowledge applied on that 1mb..</div><br/></div></div><div id="39861496" class="c"><input type="checkbox" id="c-39861496" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39855349">parent</a><span>|</span><a href="#39857845">prev</a><span>|</span><a href="#39855393">next</a><span>|</span><label class="collapse" for="c-39861496">[-]</label><label class="expand" for="c-39861496">[1 more]</label></div><br/><div class="children"><div class="content">Compared to the human brain they are shockingly efficient. It&#x27;s the hardware that isn&#x27;t, but that is just a matter of time.</div><br/></div></div><div id="39855393" class="c"><input type="checkbox" id="c-39855393" checked=""/><div class="controls bullet"><span class="by">electric_mayhem</span><span>|</span><a href="#39855349">parent</a><span>|</span><a href="#39861496">prev</a><span>|</span><a href="#39855292">next</a><span>|</span><label class="collapse" for="c-39855393">[-]</label><label class="expand" for="c-39855393">[7 more]</label></div><br/><div class="children"><div class="content">It’s literally simulating a neural network.<p>How much of your 5-sense experiential memories and decades of academic book learning are you bringing to understand my reply to your post?<p>How many gigabytes do you think that’s equivalent to?</div><br/><div id="39855790" class="c"><input type="checkbox" id="c-39855790" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39855393">parent</a><span>|</span><a href="#39858634">next</a><span>|</span><label class="collapse" for="c-39855790">[-]</label><label class="expand" for="c-39855790">[2 more]</label></div><br/><div class="children"><div class="content">Jamba seems to be distributed as 21 5-gigabyte files [1] so I guess that’s another way of looking at it.<p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;ai21labs&#x2F;Jamba-v0.1&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;ai21labs&#x2F;Jamba-v0.1&#x2F;tree&#x2F;main</a></div><br/><div id="39861514" class="c"><input type="checkbox" id="c-39861514" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39855790">parent</a><span>|</span><a href="#39858634">next</a><span>|</span><label class="collapse" for="c-39861514">[-]</label><label class="expand" for="c-39861514">[1 more]</label></div><br/><div class="children"><div class="content">So what? I have seen models distributed as 26x 10GB files.</div><br/></div></div></div></div><div id="39858634" class="c"><input type="checkbox" id="c-39858634" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39855393">parent</a><span>|</span><a href="#39855790">prev</a><span>|</span><a href="#39855714">next</a><span>|</span><label class="collapse" for="c-39858634">[-]</label><label class="expand" for="c-39858634">[3 more]</label></div><br/><div class="children"><div class="content">It’s kinda simulating our brains but not really. When I attempted to dig more into how neurons work I realised that it’s a massive chasm of difference. Very much worth doing if you haven’t (you might know far better then me, this is for people who don’t yet.)<p>In terms of results:
Our brains are working with 20w of power and can be trained to compete with LLM’s using a tiny fraction of the world’s data. They also have to keep you breathing and your blood pumping and manage all the dangers of catching a ball near traffic. Or skiing, or poetry, or sunsets. And they remember stuff five minutes later and don’t need a training run that takes months.<p>We have SO many opportunities to improve the AI architecture it’s ridiculous. This is a good thing.</div><br/><div id="39858908" class="c"><input type="checkbox" id="c-39858908" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39858634">parent</a><span>|</span><a href="#39861527">next</a><span>|</span><label class="collapse" for="c-39858908">[-]</label><label class="expand" for="c-39858908">[1 more]</label></div><br/><div class="children"><div class="content">To be fair most of the brain is more like a pretrained model — it isn&#x27;t being trained at any point after conception to keep your blood pumping or your lungs working, it does that out of the box roughly as soon as you sprout those organs (or the minute you&#x27;re born, in the case of lungs). The training process was billions of years of evolution. And, well, given fairly persistent cross-cultural cognitive biases, I expect the conscious thought parts are starting from a pretrained model, too, and all we&#x27;re doing in school is finetuning ;)</div><br/></div></div><div id="39861527" class="c"><input type="checkbox" id="c-39861527" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39858634">parent</a><span>|</span><a href="#39858908">prev</a><span>|</span><a href="#39855714">next</a><span>|</span><label class="collapse" for="c-39861527">[-]</label><label class="expand" for="c-39861527">[1 more]</label></div><br/><div class="children"><div class="content">People don&#x27;t understand that to simulate a single neuron, you need an entire neural network. So 70 billion parameters might at best be equivalent to a million neurons but that is assuming that your neural network architecture is akin to the connections between neurons. Considering the physical sparsity, you might need even more parameters to model the connections of a biological neural network. So less than a million neurons in practice.</div><br/></div></div></div></div><div id="39855714" class="c"><input type="checkbox" id="c-39855714" checked=""/><div class="controls bullet"><span class="by">_false</span><span>|</span><a href="#39855349">root</a><span>|</span><a href="#39855393">parent</a><span>|</span><a href="#39858634">prev</a><span>|</span><a href="#39855292">next</a><span>|</span><label class="collapse" for="c-39855714">[-]</label><label class="expand" for="c-39855714">[1 more]</label></div><br/><div class="children"><div class="content">I love both parent post perspectives on this.</div><br/></div></div></div></div></div></div><div id="39855292" class="c"><input type="checkbox" id="c-39855292" checked=""/><div class="controls bullet"><span class="by">gautamcgoel</span><span>|</span><a href="#39855349">prev</a><span>|</span><a href="#39857436">next</a><span>|</span><label class="collapse" for="c-39855292">[-]</label><label class="expand" for="c-39855292">[9 more]</label></div><br/><div class="children"><div class="content">Why include self-attention layers at all? In other words, why not just alternate SSM and MLP layers?</div><br/><div id="39855518" class="c"><input type="checkbox" id="c-39855518" checked=""/><div class="controls bullet"><span class="by">NLPaep</span><span>|</span><a href="#39855292">parent</a><span>|</span><a href="#39857436">next</a><span>|</span><label class="collapse" for="c-39855518">[-]</label><label class="expand" for="c-39855518">[8 more]</label></div><br/><div class="children"><div class="content">Mamba is bad with long context. It doesn&#x27;t remember phone numbers<p><a href="https:&#x2F;&#x2F;www.harvard.edu&#x2F;kempner-institute&#x2F;2024&#x2F;02&#x2F;05&#x2F;repeat-after-me-transformers-are-better-than-state-space-models-at-copying&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.harvard.edu&#x2F;kempner-institute&#x2F;2024&#x2F;02&#x2F;05&#x2F;repeat-...</a></div><br/><div id="39856985" class="c"><input type="checkbox" id="c-39856985" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39855292">root</a><span>|</span><a href="#39855518">parent</a><span>|</span><a href="#39857429">next</a><span>|</span><label class="collapse" for="c-39856985">[-]</label><label class="expand" for="c-39856985">[2 more]</label></div><br/><div class="children"><div class="content">Good! DNNs unlock <i>semantics</i> (parsing, transforming, producing). That&#x27;s the basis of general intelligence, not encyclopedic random string recall. Models shouldn&#x27;t burn ungodly quantities of compute emulating DDR5 with their working memory. We need machines that <i>think better</i>, not <i>memorize</i> well. We already have plenty of those.<p>Massive context windows, and their needle tests, are misguided. We won&#x27;t reach human-level AGI by basically inventing a natural language RDBMS. Our resources should primarily target better reasoning systems for our models, reinforcement learning, etc.<p>If we can build a GPT4-level problem solving system that coincidentally also can&#x27;t remember telephone numbers, I&#x27;ll consider it major progress.</div><br/><div id="39858667" class="c"><input type="checkbox" id="c-39858667" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#39855292">root</a><span>|</span><a href="#39856985">parent</a><span>|</span><a href="#39857429">next</a><span>|</span><label class="collapse" for="c-39858667">[-]</label><label class="expand" for="c-39858667">[1 more]</label></div><br/><div class="children"><div class="content">Memorization usually refers to training data. It&#x27;s often useful to have something that can utilize instructions losslessly, which is the distinction between these models.</div><br/></div></div></div></div><div id="39857429" class="c"><input type="checkbox" id="c-39857429" checked=""/><div class="controls bullet"><span class="by">Rodeoclash</span><span>|</span><a href="#39855292">root</a><span>|</span><a href="#39855518">parent</a><span>|</span><a href="#39856985">prev</a><span>|</span><a href="#39857430">next</a><span>|</span><label class="collapse" for="c-39857429">[-]</label><label class="expand" for="c-39857429">[4 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t remember phone numbers either but I can use a device suited to remembering them to look them up</div><br/><div id="39857576" class="c"><input type="checkbox" id="c-39857576" checked=""/><div class="controls bullet"><span class="by">orra</span><span>|</span><a href="#39855292">root</a><span>|</span><a href="#39857429">parent</a><span>|</span><a href="#39861558">next</a><span>|</span><label class="collapse" for="c-39857576">[-]</label><label class="expand" for="c-39857576">[2 more]</label></div><br/><div class="children"><div class="content">Hell, it looks like you forgot you already said that (-:</div><br/><div id="39858720" class="c"><input type="checkbox" id="c-39858720" checked=""/><div class="controls bullet"><span class="by">Rodeoclash</span><span>|</span><a href="#39855292">root</a><span>|</span><a href="#39857576">parent</a><span>|</span><a href="#39861558">next</a><span>|</span><label class="collapse" for="c-39858720">[-]</label><label class="expand" for="c-39858720">[1 more]</label></div><br/><div class="children"><div class="content">Haha, I blame the Harmonic app :&#x2F;</div><br/></div></div></div></div><div id="39861558" class="c"><input type="checkbox" id="c-39861558" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39855292">root</a><span>|</span><a href="#39857429">parent</a><span>|</span><a href="#39857576">prev</a><span>|</span><a href="#39857430">next</a><span>|</span><label class="collapse" for="c-39861558">[-]</label><label class="expand" for="c-39861558">[1 more]</label></div><br/><div class="children"><div class="content">What if your field of vision was infinite and you are looking at a unrolled telephone book?<p>Would you need a device to remember the phone number? You wouldn&#x27;t. You would need a method or algorithm to find the number, but there is no reason why that algorithm couldn&#x27;t be part of the attention mechanism. The attention mechanism is akin to reading the entire phone book for every word you are about to say. It would be unreasonable to expect you to not find the right phone number eventually.</div><br/></div></div></div></div><div id="39857430" class="c"><input type="checkbox" id="c-39857430" checked=""/><div class="controls bullet"><span class="by">Rodeoclash</span><span>|</span><a href="#39855292">root</a><span>|</span><a href="#39855518">parent</a><span>|</span><a href="#39857429">prev</a><span>|</span><a href="#39857436">next</a><span>|</span><label class="collapse" for="c-39857430">[-]</label><label class="expand" for="c-39857430">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t remember phone numbers either but I can use a device suited to remembering them to look them up.</div><br/></div></div></div></div></div></div><div id="39857436" class="c"><input type="checkbox" id="c-39857436" checked=""/><div class="controls bullet"><span class="by">unraveller</span><span>|</span><a href="#39855292">prev</a><span>|</span><a href="#39861541">next</a><span>|</span><label class="collapse" for="c-39857436">[-]</label><label class="expand" for="c-39857436">[1 more]</label></div><br/><div class="children"><div class="content">Jamba-v0.1-hybrid-MoE (16x6B?) is like giving a big NOS boost to a mixtral 8x7B tier LLM. If true 256k context, 3x longer, faster &amp; cheaper than anything else, it should mean an end to the One Model To Rule Them All mindset for now. The big boys will have to offer some version of it as separate but close side-kick integration to their hero offering.</div><br/></div></div><div id="39861541" class="c"><input type="checkbox" id="c-39861541" checked=""/><div class="controls bullet"><span class="by">zzzzzzzzzz10</span><span>|</span><a href="#39857436">prev</a><span>|</span><a href="#39860678">next</a><span>|</span><label class="collapse" for="c-39861541">[-]</label><label class="expand" for="c-39861541">[1 more]</label></div><br/><div class="children"><div class="content">Where can I download and use it?</div><br/></div></div><div id="39860678" class="c"><input type="checkbox" id="c-39860678" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#39861541">prev</a><span>|</span><a href="#39856729">next</a><span>|</span><label class="collapse" for="c-39860678">[-]</label><label class="expand" for="c-39860678">[1 more]</label></div><br/><div class="children"><div class="content">Does this mean that I can continue a chat without needing to send a full transcript? This feels like it could make inference a lot cheaper for multi-step dialogs.</div><br/></div></div><div id="39856729" class="c"><input type="checkbox" id="c-39856729" checked=""/><div class="controls bullet"><span class="by">ninjahatori</span><span>|</span><a href="#39860678">prev</a><span>|</span><a href="#39859550">next</a><span>|</span><label class="collapse" for="c-39856729">[-]</label><label class="expand" for="c-39856729">[1 more]</label></div><br/><div class="children"><div class="content">On a side note: working over longer contexts also reminds me of MemGPT(<a href="https:&#x2F;&#x2F;github.com&#x2F;cpacker&#x2F;MemGPT">https:&#x2F;&#x2F;github.com&#x2F;cpacker&#x2F;MemGPT</a>)
I think a similar concept can be applied to Mamba architecture models too.</div><br/></div></div><div id="39859550" class="c"><input type="checkbox" id="c-39859550" checked=""/><div class="controls bullet"><span class="by">zelphirkalt</span><span>|</span><a href="#39856729">prev</a><span>|</span><a href="#39859563">next</a><span>|</span><label class="collapse" for="c-39859550">[-]</label><label class="expand" for="c-39859550">[1 more]</label></div><br/><div class="children"><div class="content">Is there a Sparabo too?<p>It is always funny to see old names associated with totally different new things!</div><br/></div></div><div id="39859563" class="c"><input type="checkbox" id="c-39859563" checked=""/><div class="controls bullet"><span class="by">toddmorey</span><span>|</span><a href="#39859550">prev</a><span>|</span><a href="#39858710">next</a><span>|</span><label class="collapse" for="c-39859563">[-]</label><label class="expand" for="c-39859563">[1 more]</label></div><br/><div class="children"><div class="content">Released with open weights!</div><br/></div></div><div id="39858710" class="c"><input type="checkbox" id="c-39858710" checked=""/><div class="controls bullet"><span class="by">moneycantbuy</span><span>|</span><a href="#39859563">prev</a><span>|</span><a href="#39856221">next</a><span>|</span><label class="collapse" for="c-39858710">[-]</label><label class="expand" for="c-39858710">[1 more]</label></div><br/><div class="children"><div class="content">would a 192GB RAM mac studio or even a 7950x with 192GB RAM be practical for running this model for inference and possibly fine tuning? Especially if I don&#x27;t need very low latency e.g. 1 token per second is fine for inference. i also have two 3090s.</div><br/></div></div><div id="39856221" class="c"><input type="checkbox" id="c-39856221" checked=""/><div class="controls bullet"><span class="by">haddr</span><span>|</span><a href="#39858710">prev</a><span>|</span><a href="#39856306">next</a><span>|</span><label class="collapse" for="c-39856221">[-]</label><label class="expand" for="c-39856221">[2 more]</label></div><br/><div class="children"><div class="content">Will it be possible to run such model family in ollama?</div><br/><div id="39856247" class="c"><input type="checkbox" id="c-39856247" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#39856221">parent</a><span>|</span><a href="#39856306">next</a><span>|</span><label class="collapse" for="c-39856247">[-]</label><label class="expand" for="c-39856247">[1 more]</label></div><br/><div class="children"><div class="content">Mamba is supported in llama.cpp so should be (edit - apparently it&#x27;s not strictly the mamba architecture, it&#x27;s a mix of mamba and transformers, so it looks like it would have to be ported to llama.cpp)</div><br/></div></div></div></div><div id="39856306" class="c"><input type="checkbox" id="c-39856306" checked=""/><div class="controls bullet"><span class="by">google234123</span><span>|</span><a href="#39856221">prev</a><span>|</span><a href="#39855135">next</a><span>|</span><label class="collapse" for="c-39856306">[-]</label><label class="expand" for="c-39856306">[3 more]</label></div><br/><div class="children"><div class="content">I’m pretty sure computational chemists were combining NNs with Kalman Filters for a while now… I recall the issue it was slow due to the N^2 size of the covariance matrix</div><br/><div id="39856528" class="c"><input type="checkbox" id="c-39856528" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#39856306">parent</a><span>|</span><a href="#39855135">next</a><span>|</span><label class="collapse" for="c-39856528">[-]</label><label class="expand" for="c-39856528">[2 more]</label></div><br/><div class="children"><div class="content">Surprised they hadn&#x27;t found ways to advance their techniques with e.g. low-rank approximations, etc.</div><br/><div id="39859843" class="c"><input type="checkbox" id="c-39859843" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#39856306">root</a><span>|</span><a href="#39856528">parent</a><span>|</span><a href="#39855135">next</a><span>|</span><label class="collapse" for="c-39859843">[-]</label><label class="expand" for="c-39859843">[1 more]</label></div><br/><div class="children"><div class="content">That’s one strategy. Also flash attention.</div><br/></div></div></div></div></div></div><div id="39855135" class="c"><input type="checkbox" id="c-39855135" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#39856306">prev</a><span>|</span><a href="#39856816">next</a><span>|</span><label class="collapse" for="c-39855135">[-]</label><label class="expand" for="c-39855135">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad we&#x27;re seeing exploration into scaling post-transformer LLM architectures, but I&#x27;m disappointed that it <i>has</i> a context window. That was kind of the selling point of Mamba(and SSM models in general), right linear scaling because state+input=next_state+output?</div><br/><div id="39857954" class="c"><input type="checkbox" id="c-39857954" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39855135">parent</a><span>|</span><a href="#39857084">next</a><span>|</span><label class="collapse" for="c-39857954">[-]</label><label class="expand" for="c-39857954">[1 more]</label></div><br/><div class="children"><div class="content">256k is huge dude. that is like 1&#x2F;2 of the average non fiction novel<p>i think at least 200~300 pages of PDF<p>im not complaining here and it also fits in GPU</div><br/></div></div><div id="39857084" class="c"><input type="checkbox" id="c-39857084" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39855135">parent</a><span>|</span><a href="#39857954">prev</a><span>|</span><a href="#39855364">next</a><span>|</span><label class="collapse" for="c-39857084">[-]</label><label class="expand" for="c-39857084">[2 more]</label></div><br/><div class="children"><div class="content">state = context<p>The difference between SSMs and GPTs here is <i>how</i> that state&#x2F;context scales. Per usual in engineering, there are big trade offs!</div><br/><div id="39857212" class="c"><input type="checkbox" id="c-39857212" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#39855135">root</a><span>|</span><a href="#39857084">parent</a><span>|</span><a href="#39855364">next</a><span>|</span><label class="collapse" for="c-39857212">[-]</label><label class="expand" for="c-39857212">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not following. State is a multi-dimensional vector and context is a list of tokens. State is perturbed by A and Bx(t), while context is appended to by sampling the predicted token distribution.</div><br/></div></div></div></div><div id="39855364" class="c"><input type="checkbox" id="c-39855364" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39855135">parent</a><span>|</span><a href="#39857084">prev</a><span>|</span><a href="#39856816">next</a><span>|</span><label class="collapse" for="c-39855364">[-]</label><label class="expand" for="c-39855364">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure I follow fully, it is also the case for (handwaves) &quot;traditional&quot; LLMs that state + input = next state + output. Its just that output increases, so as output becomes input, eventually state + input &#x2F; next state + output is greater than the context size.<p>Re: linear scaling, that means the runtime cost is O(n) to context size, rather than traditional transformer O(n^2)</div><br/><div id="39856113" class="c"><input type="checkbox" id="c-39856113" checked=""/><div class="controls bullet"><span class="by">maccam912</span><span>|</span><a href="#39855135">root</a><span>|</span><a href="#39855364">parent</a><span>|</span><a href="#39856168">next</a><span>|</span><label class="collapse" for="c-39856113">[-]</label><label class="expand" for="c-39856113">[2 more]</label></div><br/><div class="children"><div class="content">I think kelseyfrog meant that the state for a mamba model is supposed to &quot;remember&quot; stuff even if it doesn&#x27;t have the actual tokens to reference any more. It might not be guaranteed to hang on to some information about tokens from a long time ago, but at least in theory it&#x27;s possible, whereas tokens from before a context window in a tradional llms may as well never have existed.</div><br/><div id="39856315" class="c"><input type="checkbox" id="c-39856315" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#39855135">root</a><span>|</span><a href="#39856113">parent</a><span>|</span><a href="#39856168">next</a><span>|</span><label class="collapse" for="c-39856315">[-]</label><label class="expand" for="c-39856315">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you said it better than I did :)</div><br/></div></div></div></div><div id="39856168" class="c"><input type="checkbox" id="c-39856168" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39855135">root</a><span>|</span><a href="#39855364">parent</a><span>|</span><a href="#39856113">prev</a><span>|</span><a href="#39856816">next</a><span>|</span><label class="collapse" for="c-39856168">[-]</label><label class="expand" for="c-39856168">[1 more]</label></div><br/><div class="children"><div class="content">That is valid for Mamba, this model (Jamba) is a mix of transformer and mamba layers, so it still has a quadratic memory cost, but divided by 8.</div><br/></div></div></div></div></div></div><div id="39856816" class="c"><input type="checkbox" id="c-39856816" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39855135">prev</a><span>|</span><a href="#39856462">next</a><span>|</span><label class="collapse" for="c-39856816">[-]</label><label class="expand" for="c-39856816">[4 more]</label></div><br/><div class="children"><div class="content">Please link to the original post:<p><a href="https:&#x2F;&#x2F;www.ai21.com&#x2F;blog&#x2F;announcing-jamba" rel="nofollow">https:&#x2F;&#x2F;www.ai21.com&#x2F;blog&#x2F;announcing-jamba</a><p>Jamba looks <i>fabulous</i>. Good performance for its size <i>and</i> much more efficient than the available open alternatives.<p>The key idea: One of out of every eight transformer blocks in Jamba applies dot-product attention with quadratic cost, but the other seven out of eight apply a Mamba layer with linear cost. And the entire model is a mixture of experts(MoE) so only ~12B parameters are used at once for inference.<p>Thank you to the folks at AI21 for making Jamba available!</div><br/><div id="39858412" class="c"><input type="checkbox" id="c-39858412" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39856816">parent</a><span>|</span><a href="#39858869">next</a><span>|</span><label class="collapse" for="c-39858412">[-]</label><label class="expand" for="c-39858412">[2 more]</label></div><br/><div class="children"><div class="content">i havent seen anyone mention this yet so i&#x27;ll be the first - what is the comparison vs StripedHyena? <a href="https:&#x2F;&#x2F;www.together.ai&#x2F;blog&#x2F;stripedhyena-7b" rel="nofollow">https:&#x2F;&#x2F;www.together.ai&#x2F;blog&#x2F;stripedhyena-7b</a></div><br/><div id="39858826" class="c"><input type="checkbox" id="c-39858826" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39856816">root</a><span>|</span><a href="#39858412">parent</a><span>|</span><a href="#39858869">next</a><span>|</span><label class="collapse" for="c-39858826">[-]</label><label class="expand" for="c-39858826">[1 more]</label></div><br/><div class="children"><div class="content">Mamba came out of the same research group, Hazy Research, led by Chris Ré. This new &quot;Jamba&quot; model incorporating Mamba and dot-product attention layers has ~8x more parameters than the largest open Striped Hyena, and appears to work much better.</div><br/></div></div></div></div></div></div><div id="39856462" class="c"><input type="checkbox" id="c-39856462" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39856816">prev</a><span>|</span><a href="#39855241">next</a><span>|</span><label class="collapse" for="c-39856462">[-]</label><label class="expand" for="c-39856462">[2 more]</label></div><br/><div class="children"><div class="content">@dang this is blogspam for the official post: <a href="https:&#x2F;&#x2F;www.ai21.com&#x2F;blog&#x2F;announcing-jamba" rel="nofollow">https:&#x2F;&#x2F;www.ai21.com&#x2F;blog&#x2F;announcing-jamba</a></div><br/></div></div><div id="39855241" class="c"><input type="checkbox" id="c-39855241" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#39856462">prev</a><span>|</span><a href="#39859138">next</a><span>|</span><label class="collapse" for="c-39855241">[-]</label><label class="expand" for="c-39855241">[5 more]</label></div><br/><div class="children"><div class="content">The license is a proper open-source one: Apache 2.0. Thanks, AI21 Labs.</div><br/><div id="39857161" class="c"><input type="checkbox" id="c-39857161" checked=""/><div class="controls bullet"><span class="by">popalchemist</span><span>|</span><a href="#39855241">parent</a><span>|</span><a href="#39857972">next</a><span>|</span><label class="collapse" for="c-39857161">[-]</label><label class="expand" for="c-39857161">[1 more]</label></div><br/><div class="children"><div class="content">In addition to the architectural and performance benefits, this is the big deal here, IMO.</div><br/></div></div><div id="39857972" class="c"><input type="checkbox" id="c-39857972" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39855241">parent</a><span>|</span><a href="#39857161">prev</a><span>|</span><a href="#39855385">next</a><span>|</span><label class="collapse" for="c-39857972">[-]</label><label class="expand" for="c-39857972">[2 more]</label></div><br/><div class="children"><div class="content">im so used to seeing AGPLv3<p>apache 2 is a more generous license</div><br/><div id="39858054" class="c"><input type="checkbox" id="c-39858054" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#39855241">root</a><span>|</span><a href="#39857972">parent</a><span>|</span><a href="#39855385">next</a><span>|</span><label class="collapse" for="c-39858054">[-]</label><label class="expand" for="c-39858054">[1 more]</label></div><br/><div class="children"><div class="content">AGPLv3 is a fine license too. But most of the models nowadays come with bullshit licenses, like Llama 2 with its &quot;acceptable use policy&quot; enforced by the license: <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;llama&#x2F;use-policy&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;llama&#x2F;use-policy&#x2F;</a></div><br/></div></div></div></div></div></div><div id="39857341" class="c"><input type="checkbox" id="c-39857341" checked=""/><div class="controls bullet"><span class="by">sleepingreset</span><span>|</span><a href="#39859138">prev</a><span>|</span><a href="#39855179">next</a><span>|</span><label class="collapse" for="c-39857341">[-]</label><label class="expand" for="c-39857341">[1 more]</label></div><br/><div class="children"><div class="content">god damn</div><br/></div></div><div id="39855179" class="c"><input type="checkbox" id="c-39855179" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39857341">prev</a><span>|</span><label class="collapse" for="c-39855179">[-]</label><label class="expand" for="c-39855179">[2 more]</label></div><br/><div class="children"><div class="content">compute still has cost?</div><br/><div id="39856567" class="c"><input type="checkbox" id="c-39856567" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39855179">parent</a><span>|</span><label class="collapse" for="c-39856567">[-]</label><label class="expand" for="c-39856567">[1 more]</label></div><br/><div class="children"><div class="content">In not sure I understood your question.<p>This model should have much lower computational cost since only one out of eight layers is a traditional transformer layer with masked self-attention. Additionally, half of the Mamba layers are MoEs.</div><br/></div></div></div></div></div></div></div></div></div></body></html>