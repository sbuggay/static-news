<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708506072825" as="style"/><link rel="stylesheet" href="styles.css?v=1708506072825"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="http://databasearchitects.blogspot.com/2024/02/ssds-have-become-ridiculously-fast.html">SSDs have become fast, except in the cloud</a> <span class="domain">(<a href="http://databasearchitects.blogspot.com">databasearchitects.blogspot.com</a>)</span></div><div class="subtext"><span>greghn</span> | <span>339 comments</span></div><br/><div><div id="39443994" class="c"><input type="checkbox" id="c-39443994" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39445795">next</a><span>|</span><label class="collapse" for="c-39443994">[-]</label><label class="expand" for="c-39443994">[206 more]</label></div><br/><div class="children"><div class="content">This was a huge technical problem I worked on at Google, and is sort of fundamental to a cloud. I believe this is actually a big deal that drives peoples&#x27; technology directions.<p>SSDs in the cloud are attached over a network, and fundamentally have to be. The problem is that this network is so large and slow that it can&#x27;t give you anywhere near the performance of a local SSD. This wasn&#x27;t a problem for hard drives, which was the backing technology when a lot of these network attached storage systems were invented, because they are fundamentally slow compared to networks, but it is a problem for SSD.</div><br/><div id="39444096" class="c"><input type="checkbox" id="c-39444096" checked=""/><div class="controls bullet"><span class="by">jsnell</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39451330">next</a><span>|</span><label class="collapse" for="c-39444096">[-]</label><label class="expand" for="c-39444096">[107 more]</label></div><br/><div class="children"><div class="content">According to the submitted article, the numbers are from AWS instance types where the SSD is &quot;physically attached&quot; to the host, not about SSD-backed NAS solutions.<p>Also, the article isn&#x27;t just about SSDs being no faster than a network. It&#x27;s about SSDs being two orders of magnitude slower than datacenter networks.</div><br/><div id="39444161" class="c"><input type="checkbox" id="c-39444161" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444096">parent</a><span>|</span><a href="#39448728">next</a><span>|</span><label class="collapse" for="c-39444161">[-]</label><label class="expand" for="c-39444161">[103 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because the &quot;local&quot; SSDs are not actually physically attached and there&#x27;s a network protocol in the way.</div><br/><div id="39444373" class="c"><input type="checkbox" id="c-39444373" checked=""/><div class="controls bullet"><span class="by">jsnell</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39446742">next</a><span>|</span><label class="collapse" for="c-39444373">[-]</label><label class="expand" for="c-39444373">[37 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re wrong about that. AWS calls this class of storage &quot;instance storage&quot; [0], and defines it as:<p>&gt; Many Amazon EC2 instances can also include storage from devices that are located inside the host computer, referred to as instance storage.<p>There might be some wiggle room in &quot;physically attached&quot;, but there&#x27;s none in &quot;storage devices located inside the host computer&quot;. It&#x27;s not some kind of AWS-only thing either. GCP has &quot;local SSD disks&quot;[1], which I&#x27;m going to claim are likewise local, not over the network block storage. (Though the language isn&#x27;t as explicit as for AWS.)<p>[0] <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;</a><p>[1] <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks#localssds" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks#localssds</a></div><br/><div id="39450882" class="c"><input type="checkbox" id="c-39450882" checked=""/><div class="controls bullet"><span class="by">reactordev</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444373">parent</a><span>|</span><a href="#39445545">next</a><span>|</span><label class="collapse" for="c-39450882">[-]</label><label class="expand" for="c-39450882">[1 more]</label></div><br/><div class="children"><div class="content">AWS is so large, every concept of hardware is virtualized over a software layer. “Instance storage” is no different. It’s just closer to the edge with your node. It’s not some box in a rack where some AWS tech slots in an SSD. AWS has a hardware layer, but you’ll never see it.</div><br/></div></div><div id="39445545" class="c"><input type="checkbox" id="c-39445545" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444373">parent</a><span>|</span><a href="#39450882">prev</a><span>|</span><a href="#39447509">next</a><span>|</span><label class="collapse" for="c-39445545">[-]</label><label class="expand" for="c-39445545">[11 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the abstraction they want you to work with, yes. That doesn&#x27;t mean it&#x27;s what is actually happening - at least not in the same way that you&#x27;re thinking.<p>As a hint for you, I said &quot;<i>a</i> network&quot;, not &quot;<i>the</i> network.&quot; You can also look at public presentations about how Nitro works.</div><br/><div id="39446809" class="c"><input type="checkbox" id="c-39446809" checked=""/><div class="controls bullet"><span class="by">jsnell</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445545">parent</a><span>|</span><a href="#39445944">next</a><span>|</span><label class="collapse" for="c-39446809">[-]</label><label class="expand" for="c-39446809">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve linked to public documentation that is pretty clearly in conflict with what you said. There&#x27;s no wiggle room in how AWS describes their service without it being false advertising. There&#x27;s no &quot;ah, but what if we define the entire building to be the host computer, then the networked SSDs really <i>are</i> inside the host computer&quot; sleight of hand to pull off here.<p>You&#x27;ve provided cryptic hints and a suggestion to watch some unnamed presentation.<p>At this point I really think the burden of proof is on you.</div><br/><div id="39449527" class="c"><input type="checkbox" id="c-39449527" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446809">parent</a><span>|</span><a href="#39451140">next</a><span>|</span><label class="collapse" for="c-39449527">[-]</label><label class="expand" for="c-39449527">[1 more]</label></div><br/><div class="children"><div class="content">You are correct, and the parent you’re replying to is confused. Nitro is for EBS, not the i3 local NVMe instances.<p>Those i3 instances lose your data whenever you stop and start them again (ie migrate to a different host machine), there’s absolutely no reason they would use network.<p>EBS itself uses a different network than the “normal” internet, if I were to guess it’s a converged Ethernet network optimized for iSCSI. Which is what Nitro optimizes for as well. But it’s not relevant for the local NVMe storage.</div><br/></div></div><div id="39451140" class="c"><input type="checkbox" id="c-39451140" checked=""/><div class="controls bullet"><span class="by">fcsp</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446809">parent</a><span>|</span><a href="#39449527">prev</a><span>|</span><a href="#39445944">next</a><span>|</span><label class="collapse" for="c-39451140">[-]</label><label class="expand" for="c-39451140">[1 more]</label></div><br/><div class="children"><div class="content">I see wiggle room in the statement you posted in that the SSD storage that is physically inside the machine hosting the instance might be mounted into the hypervised instance itself via some kind of network protocol still, adding overhead.</div><br/></div></div></div></div><div id="39445944" class="c"><input type="checkbox" id="c-39445944" checked=""/><div class="controls bullet"><span class="by">jng</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445545">parent</a><span>|</span><a href="#39446809">prev</a><span>|</span><a href="#39447443">next</a><span>|</span><label class="collapse" for="c-39445944">[-]</label><label class="expand" for="c-39445944">[1 more]</label></div><br/><div class="children"><div class="content">Nitro &quot;virtual NVME&quot; device are mostly (only?) for EBS -- remote network storage, transparently managed, using a separate network backbone, and presented to the host as a regular local NVME device. SSD drives in instances such as i4i, etc. are physically attached in a different way -- but physically, unlike EBS, they are ephemeral and the content becomes unavaiable as you stop the instance, and when you restart, you get a new &quot;blank slate&quot;. Their performance is 1 order of magnitude faster than standard-level EBS, and the cost structure is completely different (and many orders of magnitude more affordable than EBS volumes configured to have comparable I&#x2F;O performance).</div><br/></div></div><div id="39447443" class="c"><input type="checkbox" id="c-39447443" checked=""/><div class="controls bullet"><span class="by">jasonwatkinspdx</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445545">parent</a><span>|</span><a href="#39445944">prev</a><span>|</span><a href="#39447308">next</a><span>|</span><label class="collapse" for="c-39447443">[-]</label><label class="expand" for="c-39447443">[2 more]</label></div><br/><div class="children"><div class="content">Both the documentation and Amazon employees are in here telling you that you&#x27;re wrong. Can you resolve that contradiction or do you just want to act coy like you know some secret? The latter behavior is not productive.</div><br/><div id="39450406" class="c"><input type="checkbox" id="c-39450406" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447443">parent</a><span>|</span><a href="#39447308">next</a><span>|</span><label class="collapse" for="c-39450406">[-]</label><label class="expand" for="c-39450406">[1 more]</label></div><br/><div class="children"><div class="content">The parent thinks that AWS&#x27; i3 NVMe local instance storage is using a PCIe switch, which is not the case. EBS (and the AWS Nitro card) use a PCIe switch, and as such all EBS storage is exposed as e.g. &#x2F;dev&#x2F;nvmeXnY . But that&#x27;s not the same as the i3 instances are offering, so the parent is confused.</div><br/></div></div></div></div><div id="39447308" class="c"><input type="checkbox" id="c-39447308" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445545">parent</a><span>|</span><a href="#39447443">prev</a><span>|</span><a href="#39447509">next</a><span>|</span><label class="collapse" for="c-39447308">[-]</label><label class="expand" for="c-39447308">[4 more]</label></div><br/><div class="children"><div class="content">it sounds like you&#x27;re trying to say &quot;PCI switch&quot; without saying &quot;PCI switch&quot; (I worked at Google for over a decade, including hardware division).</div><br/><div id="39449574" class="c"><input type="checkbox" id="c-39449574" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447308">parent</a><span>|</span><a href="#39447509">next</a><span>|</span><label class="collapse" for="c-39449574">[-]</label><label class="expand" for="c-39449574">[3 more]</label></div><br/><div class="children"><div class="content">That is what I am trying to say without actually giving it out. PCIe switches are very much not transparent devices. Apparently AWS has not published anything about this, and doesn&#x27;t have Nitro moderating access to &quot;local&quot; SSD, though - that I did get confused with EBS.</div><br/><div id="39450226" class="c"><input type="checkbox" id="c-39450226" checked=""/><div class="controls bullet"><span class="by">pzb</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449574">parent</a><span>|</span><a href="#39450902">next</a><span>|</span><label class="collapse" for="c-39450226">[-]</label><label class="expand" for="c-39450226">[1 more]</label></div><br/><div class="children"><div class="content">AWS has stated that there is a &quot;Nitro Card for Instance Storage&quot;[0][1] which is a NVMe PCIe controller that implements transparent encryption[2].<p>I don&#x27;t have access to an EC2 instance to check, but you should be able to see the PCIe topology to determine how many physical cards are likely in i4i and im4gn and their PCIe connections.  i4i claims to have 8 x 3,750 AWS Nitro SSD, but it isn&#x27;t clear how many PCIe lanes are used.<p>Also, AWS claims &quot;Traditionally, SSDs maximize the peak read and write I&#x2F;O performance. AWS Nitro SSDs are architected to minimize latency and latency variability of I&#x2F;O intensive workloads [...] which continuously read and write from the SSDs in a sustained manner, for fast and more predictable performance. AWS Nitro SSDs deliver up to 60% lower storage I&#x2F;O latency and up to 75% reduced storage I&#x2F;O latency variability [...]&quot;<p>This could explain the findings in the article - they only meared peak r&#x2F;w, not predictability.<p>[0] <a href="https:&#x2F;&#x2F;perspectives.mvdirona.com&#x2F;2019&#x2F;02&#x2F;aws-nitro-system&#x2F;" rel="nofollow">https:&#x2F;&#x2F;perspectives.mvdirona.com&#x2F;2019&#x2F;02&#x2F;aws-nitro-system&#x2F;</a>
[1] <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;nitro&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;nitro&#x2F;</a>
[2] <a href="https:&#x2F;&#x2F;d1.awsstatic.com&#x2F;events&#x2F;reinvent&#x2F;2019&#x2F;REPEAT_2_Powering_next-gen_Amazon_EC2_Deep_dive_into_the_Nitro_system_CMP303-R2.pdf" rel="nofollow">https:&#x2F;&#x2F;d1.awsstatic.com&#x2F;events&#x2F;reinvent&#x2F;2019&#x2F;REPEAT_2_Power...</a></div><br/></div></div><div id="39450902" class="c"><input type="checkbox" id="c-39450902" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449574">parent</a><span>|</span><a href="#39450226">prev</a><span>|</span><a href="#39447509">next</a><span>|</span><label class="collapse" for="c-39450902">[-]</label><label class="expand" for="c-39450902">[1 more]</label></div><br/><div class="children"><div class="content">Why are you acting as if PCIe switches are some secret technology? It was extremely grating for me to read your comments.</div><br/></div></div></div></div></div></div></div></div><div id="39447509" class="c"><input type="checkbox" id="c-39447509" checked=""/><div class="controls bullet"><span class="by">wstuartcl</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444373">parent</a><span>|</span><a href="#39445545">prev</a><span>|</span><a href="#39444464">next</a><span>|</span><label class="collapse" for="c-39447509">[-]</label><label class="expand" for="c-39447509">[8 more]</label></div><br/><div class="children"><div class="content">the tests were for these local (metal direct connect ssds).  The issue is not network overhead -- its that just like everything else in cloud the performance of 10 years ago was used as the baseline that carries over today with upcharges to buy back the gains.<p>there is a reason why vcpu performance is still locked to the typical core from 10 years ago when every core on a machine today in those data scenters is 3-5x or more speed basis.  Its cause they can charge you for 5x the cores to get that gain.</div><br/><div id="39450455" class="c"><input type="checkbox" id="c-39450455" checked=""/><div class="controls bullet"><span class="by">flaminHotSpeedo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447509">parent</a><span>|</span><a href="#39448553">next</a><span>|</span><label class="collapse" for="c-39450455">[-]</label><label class="expand" for="c-39450455">[1 more]</label></div><br/><div class="children"><div class="content">&gt; there is a reason why vcpu performance is still locked to the typical core from 10 years ago<p>That is transparently nonsense.<p>You can disprove that claim in 5 minutes, and it makes literally zero sense for offerings that aren&#x27;t oversubscribed</div><br/></div></div><div id="39448553" class="c"><input type="checkbox" id="c-39448553" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447509">parent</a><span>|</span><a href="#39450455">prev</a><span>|</span><a href="#39444464">next</a><span>|</span><label class="collapse" for="c-39448553">[-]</label><label class="expand" for="c-39448553">[6 more]</label></div><br/><div class="children"><div class="content"><i>vcpu performance is still locked to the typical core from 10 years ago</i><p>No. In some cases I think AWS actually buys special processors that are clocked <i>higher</i> than the ones you can buy.</div><br/><div id="39450390" class="c"><input type="checkbox" id="c-39450390" checked=""/><div class="controls bullet"><span class="by">phanimahesh</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448553">parent</a><span>|</span><a href="#39449463">next</a><span>|</span><label class="collapse" for="c-39450390">[-]</label><label class="expand" for="c-39450390">[2 more]</label></div><br/><div class="children"><div class="content">The parent claims that though aws uses better hardware, they bill in vcpus whose benchmarks are from a few years ago, so that they can sell more vcpu units per performant physical cpu. This does not contradict your claim that aws buys better hardware.</div><br/><div id="39450513" class="c"><input type="checkbox" id="c-39450513" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39450390">parent</a><span>|</span><a href="#39449463">next</a><span>|</span><label class="collapse" for="c-39450513">[-]</label><label class="expand" for="c-39450513">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s so obviously wrong that I can&#x27;t really explain it. Maybe someone else can. To believe that requires a complete misunderstanding of IaaS.</div><br/></div></div></div></div><div id="39449463" class="c"><input type="checkbox" id="c-39449463" checked=""/><div class="controls bullet"><span class="by">gowld</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448553">parent</a><span>|</span><a href="#39450390">prev</a><span>|</span><a href="#39444464">next</a><span>|</span><label class="collapse" for="c-39449463">[-]</label><label class="expand" for="c-39449463">[3 more]</label></div><br/><div class="children"><div class="content">You are talking about real CPU not virtual cpu</div><br/><div id="39449912" class="c"><input type="checkbox" id="c-39449912" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449463">parent</a><span>|</span><a href="#39444464">next</a><span>|</span><label class="collapse" for="c-39449912">[-]</label><label class="expand" for="c-39449912">[2 more]</label></div><br/><div class="children"><div class="content">Generally each vCPU is a dedicated hardware thread, which has gotten significantly faster in the last 10 years. Only lambdas, micros, and nanos have shared vCPUs and those have probably also gotten faster although it&#x27;s not guaranteed.</div><br/><div id="39450872" class="c"><input type="checkbox" id="c-39450872" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449912">parent</a><span>|</span><a href="#39444464">next</a><span>|</span><label class="collapse" for="c-39450872">[-]</label><label class="expand" for="c-39450872">[1 more]</label></div><br/><div class="children"><div class="content">In fairness, there are a not insignificant number of workloads that do not benefit from hardware threads on CPUs [0], instead isolating processes along physical cores for optimal performance.<p>[0] Assertion not valid for barrel processors.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39444464" class="c"><input type="checkbox" id="c-39444464" checked=""/><div class="controls bullet"><span class="by">20after4</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444373">parent</a><span>|</span><a href="#39447509">prev</a><span>|</span><a href="#39449306">next</a><span>|</span><label class="collapse" for="c-39444464">[-]</label><label class="expand" for="c-39444464">[15 more]</label></div><br/><div class="children"><div class="content">If the SSD is installed in the host server, doesn&#x27;t that still allow for it to be shared among many instances running on said host?  I can imagine that a compute node has just a handful of SSDs and many hundreds of instances sharing the I&#x2F;O bandwidth.</div><br/><div id="39444584" class="c"><input type="checkbox" id="c-39444584" checked=""/><div class="controls bullet"><span class="by">discodave</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444464">parent</a><span>|</span><a href="#39444763">next</a><span>|</span><label class="collapse" for="c-39444584">[-]</label><label class="expand" for="c-39444584">[1 more]</label></div><br/><div class="children"><div class="content">If you have one of the metal instance types, then you get the whole host, e.g. i4i.metal:<p><a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;i4i&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;i4i&#x2F;</a></div><br/></div></div><div id="39444763" class="c"><input type="checkbox" id="c-39444763" checked=""/><div class="controls bullet"><span class="by">aeyes</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444464">parent</a><span>|</span><a href="#39444584">prev</a><span>|</span><a href="#39446130">next</a><span>|</span><label class="collapse" for="c-39444763">[-]</label><label class="expand" for="c-39444763">[1 more]</label></div><br/><div class="children"><div class="content">On AWS yes, the older instances which I am familiar with had 900GB drives and they sliced that up into volumes of 600, 450, 300, 150, 75GB depending on instance size.<p>But they also tell you how much IOPS you get: <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;WindowsGuide&#x2F;storage-optimized-instances.html#storage-instances-diskperf" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;WindowsGuide&#x2F;stora...</a></div><br/></div></div><div id="39446130" class="c"><input type="checkbox" id="c-39446130" checked=""/><div class="controls bullet"><span class="by">queuebert</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444464">parent</a><span>|</span><a href="#39444763">prev</a><span>|</span><a href="#39444820">next</a><span>|</span><label class="collapse" for="c-39446130">[-]</label><label class="expand" for="c-39446130">[10 more]</label></div><br/><div class="children"><div class="content">How <i>do</i> these machines manage the sharing of one local SSD across multiple VMs?  Is there some wrapper around the I&#x2F;O stack?  Does it appear as a network share? Geniuinely curious...</div><br/><div id="39446222" class="c"><input type="checkbox" id="c-39446222" checked=""/><div class="controls bullet"><span class="by">felixg3</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446130">parent</a><span>|</span><a href="#39447488">next</a><span>|</span><label class="collapse" for="c-39446222">[-]</label><label class="expand" for="c-39446222">[2 more]</label></div><br/><div class="children"><div class="content">Probably NVME namespaces [0]?<p>[0]: <a href="https:&#x2F;&#x2F;nvmexpress.org&#x2F;resource&#x2F;nvme-namespaces&#x2F;" rel="nofollow">https:&#x2F;&#x2F;nvmexpress.org&#x2F;resource&#x2F;nvme-namespaces&#x2F;</a></div><br/><div id="39448056" class="c"><input type="checkbox" id="c-39448056" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446222">parent</a><span>|</span><a href="#39447488">next</a><span>|</span><label class="collapse" for="c-39448056">[-]</label><label class="expand" for="c-39448056">[1 more]</label></div><br/><div class="children"><div class="content">Less fancy, quite often... at least on VPS providers [1]. They like to use reflinked files off the base images. This way they only store what differs.<p>1: Which is really a cloud without a certain degree of software defined networking&#x2F;compute&#x2F;storage&#x2F;whatever.</div><br/></div></div></div></div><div id="39447488" class="c"><input type="checkbox" id="c-39447488" checked=""/><div class="controls bullet"><span class="by">icedchai</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446130">parent</a><span>|</span><a href="#39446222">prev</a><span>|</span><a href="#39446276">next</a><span>|</span><label class="collapse" for="c-39447488">[-]</label><label class="expand" for="c-39447488">[2 more]</label></div><br/><div class="children"><div class="content">With Linux and KVM&#x2F;QEMU, you can map an entire physical disk, disk partition, or file to a block device in the VM. For my own VM hosts, I use LVM and map a logical volume to the VM. I assumed cloud providers did something conceptually similar, only much more sophisticated.</div><br/><div id="39448087" class="c"><input type="checkbox" id="c-39448087" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447488">parent</a><span>|</span><a href="#39446276">next</a><span>|</span><label class="collapse" for="c-39448087">[-]</label><label class="expand" for="c-39448087">[1 more]</label></div><br/><div class="children"><div class="content">Files with reflinks are a common choice, the main benefit being: only storing deltas. The base OS costs basically nothing<p>LVM&#x2F;block like you suggest is a good idea. You&#x27;d be surprised how much access time is trimmed by skipping another filesystem like you&#x27;d have with a raw image file</div><br/></div></div></div></div><div id="39446276" class="c"><input type="checkbox" id="c-39446276" checked=""/><div class="controls bullet"><span class="by">dan-robertson</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446130">parent</a><span>|</span><a href="#39447488">prev</a><span>|</span><a href="#39446886">next</a><span>|</span><label class="collapse" for="c-39446276">[-]</label><label class="expand" for="c-39446276">[1 more]</label></div><br/><div class="children"><div class="content">AWS have custom firmware for at least some of their SSDs, so could be that</div><br/></div></div><div id="39446886" class="c"><input type="checkbox" id="c-39446886" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446130">parent</a><span>|</span><a href="#39446276">prev</a><span>|</span><a href="#39444820">next</a><span>|</span><label class="collapse" for="c-39446886">[-]</label><label class="expand" for="c-39446886">[4 more]</label></div><br/><div class="children"><div class="content">In say VirtualBox you can create a file backed on the physical disk, and attach it to the VM so the VM sees it as a NVMe drive.<p>In my experience this is also orders of magnitude slower that true direct access, ie PCIe pass-through, as all access has to pass through the VM storage driver and so <i>could</i> explain what is happening.</div><br/><div id="39448073" class="c"><input type="checkbox" id="c-39448073" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446886">parent</a><span>|</span><a href="#39444820">next</a><span>|</span><label class="collapse" for="c-39448073">[-]</label><label class="expand" for="c-39448073">[3 more]</label></div><br/><div class="children"><div class="content">The storage driver may have more impact on VBox. You can get very impressive results with <i>&#x27;virtio&#x27;</i> on KVM</div><br/><div id="39448690" class="c"><input type="checkbox" id="c-39448690" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448073">parent</a><span>|</span><a href="#39444820">next</a><span>|</span><label class="collapse" for="c-39448690">[-]</label><label class="expand" for="c-39448690">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I&#x27;ve yet to try that. I know I get a similar lack of performance with Bhyve (FreeBSD) using VirtIO, so it&#x27;s not a given it&#x27;s fast.<p>I have no idea how AWS run their VMs, was just saying a slow storage driver could give such results.</div><br/><div id="39449157" class="c"><input type="checkbox" id="c-39449157" checked=""/><div class="controls bullet"><span class="by">bravetraveler</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448690">parent</a><span>|</span><a href="#39444820">next</a><span>|</span><label class="collapse" for="c-39449157">[-]</label><label class="expand" for="c-39449157">[1 more]</label></div><br/><div class="children"><div class="content">&gt; just saying a slow storage driver could give such results<p>Oh, absolutely - not to contest that! There&#x27;s a whole lot of academia on &#x27;para-virtualized&#x27; and so on in this light.<p>That&#x27;s interesting to hear about FreeBSD; basically all of my experience has been with Linux&#x2F;Windows.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39444820" class="c"><input type="checkbox" id="c-39444820" checked=""/><div class="controls bullet"><span class="by">ownagefool</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444464">parent</a><span>|</span><a href="#39446130">prev</a><span>|</span><a href="#39445938">next</a><span>|</span><label class="collapse" for="c-39444820">[-]</label><label class="expand" for="c-39444820">[1 more]</label></div><br/><div class="children"><div class="content">PCI bus, etc too</div><br/></div></div><div id="39445938" class="c"><input type="checkbox" id="c-39445938" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444464">parent</a><span>|</span><a href="#39444820">prev</a><span>|</span><a href="#39449306">next</a><span>|</span><label class="collapse" for="c-39445938">[-]</label><label class="expand" for="c-39445938">[1 more]</label></div><br/><div class="children"><div class="content">Instance storage is not networked. That&#x27;s why it&#x27;s there.</div><br/></div></div></div></div><div id="39449306" class="c"><input type="checkbox" id="c-39449306" checked=""/><div class="controls bullet"><span class="by">Hewitt821</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444373">parent</a><span>|</span><a href="#39444464">prev</a><span>|</span><a href="#39446742">next</a><span>|</span><label class="collapse" for="c-39449306">[-]</label><label class="expand" for="c-39449306">[1 more]</label></div><br/><div class="children"><div class="content">Local SSD is part of the machine, not network attached.</div><br/></div></div></div></div><div id="39446742" class="c"><input type="checkbox" id="c-39446742" checked=""/><div class="controls bullet"><span class="by">yolovoe</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39444373">prev</a><span>|</span><a href="#39444352">next</a><span>|</span><label class="collapse" for="c-39446742">[-]</label><label class="expand" for="c-39446742">[13 more]</label></div><br/><div class="children"><div class="content">You’re wrong. 
Instance local means SSD is physically attached to the droplet and is inside the server chassis, connected via PCIe.<p>Sourece: I work on nitro cards.</div><br/><div id="39447200" class="c"><input type="checkbox" id="c-39447200" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446742">parent</a><span>|</span><a href="#39444352">next</a><span>|</span><label class="collapse" for="c-39447200">[-]</label><label class="expand" for="c-39447200">[12 more]</label></div><br/><div class="children"><div class="content">&quot;Attached to the droplet&quot;?</div><br/><div id="39447821" class="c"><input type="checkbox" id="c-39447821" checked=""/><div class="controls bullet"><span class="by">sargun</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447200">parent</a><span>|</span><a href="#39447513">next</a><span>|</span><label class="collapse" for="c-39447821">[-]</label><label class="expand" for="c-39447821">[9 more]</label></div><br/><div class="children"><div class="content">Droplets are what EC2 calls their hosts. Confusing? I know.</div><br/><div id="39447828" class="c"><input type="checkbox" id="c-39447828" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447821">parent</a><span>|</span><a href="#39447513">next</a><span>|</span><label class="collapse" for="c-39447828">[-]</label><label class="expand" for="c-39447828">[8 more]</label></div><br/><div class="children"><div class="content">Yes! That is confusing! Tell them to stop it!</div><br/><div id="39448983" class="c"><input type="checkbox" id="c-39448983" checked=""/><div class="controls bullet"><span class="by">kiwijamo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447828">parent</a><span>|</span><a href="#39447513">next</a><span>|</span><label class="collapse" for="c-39448983">[-]</label><label class="expand" for="c-39448983">[7 more]</label></div><br/><div class="children"><div class="content">FYI it&#x27;s not a AWS term, it&#x27;s a DigitalOcean term.</div><br/><div id="39449067" class="c"><input type="checkbox" id="c-39449067" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448983">parent</a><span>|</span><a href="#39449367">next</a><span>|</span><label class="collapse" for="c-39449067">[-]</label><label class="expand" for="c-39449067">[5 more]</label></div><br/><div class="children"><div class="content">I could not be more confused. Does EC2 quietly call their hosting machines &quot;droplets&quot;? I knew &quot;droplets&quot; to be a DigitalOcean team, but DigitalOcean doesn&#x27;t have Nitro cards.</div><br/><div id="39449214" class="c"><input type="checkbox" id="c-39449214" checked=""/><div class="controls bullet"><span class="by">apitman</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449067">parent</a><span>|</span><a href="#39449914">next</a><span>|</span><label class="collapse" for="c-39449214">[-]</label><label class="expand" for="c-39449214">[3 more]</label></div><br/><div class="children"><div class="content">Now I&#x27;m wondering if that&#x27;s where DO got the name in the first place</div><br/><div id="39449620" class="c"><input type="checkbox" id="c-39449620" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449214">parent</a><span>|</span><a href="#39449914">next</a><span>|</span><label class="collapse" for="c-39449620">[-]</label><label class="expand" for="c-39449620">[2 more]</label></div><br/><div class="children"><div class="content">Surely &quot;droplet&quot; is a derivative of &quot;ocean?&quot;</div><br/><div id="39449858" class="c"><input type="checkbox" id="c-39449858" checked=""/><div class="controls bullet"><span class="by">arrakeenrevived</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449620">parent</a><span>|</span><a href="#39449914">next</a><span>|</span><label class="collapse" for="c-39449858">[-]</label><label class="expand" for="c-39449858">[1 more]</label></div><br/><div class="children"><div class="content">Clouds (like, the big fluffy things in the sky) are made up of many droplets of liquid. Using &quot;droplet&quot; to refer to the things that make up cloud computing is a pretty natural nickname for any cloud provider, not just DO. I do imagine that DO uses &quot;droplet&quot; as a public product branding because it works well with their &quot;Ocean&quot; brand, though.<p>...now I&#x27;m actually interested in knowing if &quot;droplet&quot; is derived from &quot;ocean&quot;, or if &quot;Digital Ocean&quot; was derived from having many droplets (which was derived from cloud). Maybe neither.</div><br/></div></div></div></div></div></div></div></div><div id="39449367" class="c"><input type="checkbox" id="c-39449367" checked=""/><div class="controls bullet"><span class="by">sargun</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448983">parent</a><span>|</span><a href="#39449067">prev</a><span>|</span><a href="#39447513">next</a><span>|</span><label class="collapse" for="c-39449367">[-]</label><label class="expand" for="c-39449367">[1 more]</label></div><br/><div class="children"><div class="content">I believe AWS was calling them droplets prior to digital ocean.</div><br/></div></div></div></div></div></div></div></div><div id="39447513" class="c"><input type="checkbox" id="c-39447513" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447200">parent</a><span>|</span><a href="#39447821">prev</a><span>|</span><a href="#39444352">next</a><span>|</span><label class="collapse" for="c-39447513">[-]</label><label class="expand" for="c-39447513">[2 more]</label></div><br/><div class="children"><div class="content">digitalocean squad</div><br/><div id="39449627" class="c"><input type="checkbox" id="c-39449627" checked=""/><div class="controls bullet"><span class="by">jbnorth</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447513">parent</a><span>|</span><a href="#39444352">next</a><span>|</span><label class="collapse" for="c-39449627">[-]</label><label class="expand" for="c-39449627">[1 more]</label></div><br/><div class="children"><div class="content">No, that’s AWS.</div><br/></div></div></div></div></div></div></div></div><div id="39444352" class="c"><input type="checkbox" id="c-39444352" checked=""/><div class="controls bullet"><span class="by">jrullman</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39446742">prev</a><span>|</span><a href="#39445175">next</a><span>|</span><label class="collapse" for="c-39444352">[-]</label><label class="expand" for="c-39444352">[1 more]</label></div><br/><div class="children"><div class="content">I can attest to the fact that on EC2, &quot;instance store&quot; volumes are actually physically attached.</div><br/></div></div><div id="39445175" class="c"><input type="checkbox" id="c-39445175" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39444352">prev</a><span>|</span><a href="#39444253">next</a><span>|</span><label class="collapse" for="c-39445175">[-]</label><label class="expand" for="c-39445175">[9 more]</label></div><br/><div class="children"><div class="content">I suspect you must be conflating several different storage products.  Are you saying <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd</a> devices talk to the host through a network (say, ethernet with some layer on top)?  Because the documentation very clearly says otherwise, &quot;This is because Local SSD disks are physically attached to the server that hosts your VM. For this same reason, Local SSD disks can only provide temporary storage.&quot; (at least, I&#x27;m presuming that by physically attached, they mean it&#x27;s connected to the PCI bus without a network in between).<p>I suspect you&#x27;re thinking of SSD-PD.  If &quot;local&quot; SSDs are not actually local and go through a network, I need to have a discussion with my GCS TAM about truth in advertising.</div><br/><div id="39449322" class="c"><input type="checkbox" id="c-39449322" checked=""/><div class="controls bullet"><span class="by">Hewitt821</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445175">parent</a><span>|</span><a href="#39445299">next</a><span>|</span><label class="collapse" for="c-39449322">[-]</label><label class="expand" for="c-39449322">[1 more]</label></div><br/><div class="children"><div class="content">Local SSD is part of the machine.</div><br/></div></div><div id="39445299" class="c"><input type="checkbox" id="c-39445299" checked=""/><div class="controls bullet"><span class="by">op00to</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445175">parent</a><span>|</span><a href="#39449322">prev</a><span>|</span><a href="#39446138">next</a><span>|</span><label class="collapse" for="c-39445299">[-]</label><label class="expand" for="c-39445299">[1 more]</label></div><br/><div class="children"><div class="content">&gt; physically attached<p>Believe it or not, superglue and a wifi module! &#x2F;s</div><br/></div></div><div id="39446138" class="c"><input type="checkbox" id="c-39446138" checked=""/><div class="controls bullet"><span class="by">mint2</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445175">parent</a><span>|</span><a href="#39445299">prev</a><span>|</span><a href="#39444253">next</a><span>|</span><label class="collapse" for="c-39446138">[-]</label><label class="expand" for="c-39446138">[6 more]</label></div><br/><div class="children"><div class="content">I don’t really agree with assuming the form of physical attachment and interaction unless it is spelled out.<p>If that’s what’s meant it will be stated in some fine print, if it’s not stated anywhere then there is no guarantee what the term means, except I would guess they may want people to infer things that may not necessarily be true.</div><br/><div id="39446940" class="c"><input type="checkbox" id="c-39446940" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446138">parent</a><span>|</span><a href="#39444253">next</a><span>|</span><label class="collapse" for="c-39446940">[-]</label><label class="expand" for="c-39446940">[5 more]</label></div><br/><div class="children"><div class="content">&quot;Physically attached&quot; has had a fairly well defined meaning and i don&#x27;t normally expect a cloud provider to play word salad to convince me a network drive is locally attached (like I said, if true, I would need to have a chat with my TAM about it).<p>Physically attached for servers, for the past 20+ years, has meant a direct electrical connection to a host bus (such as the PCI bus attached to the front-side bus).  I&#x27;d like to see some alternative examples that violate that convention.</div><br/><div id="39447819" class="c"><input type="checkbox" id="c-39447819" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446940">parent</a><span>|</span><a href="#39444253">next</a><span>|</span><label class="collapse" for="c-39447819">[-]</label><label class="expand" for="c-39447819">[4 more]</label></div><br/><div class="children"><div class="content">Ethernet cables are physical...</div><br/><div id="39450138" class="c"><input type="checkbox" id="c-39450138" checked=""/><div class="controls bullet"><span class="by">SteveNuts</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447819">parent</a><span>|</span><a href="#39447896">next</a><span>|</span><label class="collapse" for="c-39450138">[-]</label><label class="expand" for="c-39450138">[2 more]</label></div><br/><div class="children"><div class="content">If that’s the game we’re going to play then technically my driveway is on the same road as the White House.</div><br/><div id="39450275" class="c"><input type="checkbox" id="c-39450275" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39450138">parent</a><span>|</span><a href="#39447896">next</a><span>|</span><label class="collapse" for="c-39450275">[-]</label><label class="expand" for="c-39450275">[1 more]</label></div><br/><div class="children"><div class="content">exactly. it&#x27;s not about what&#x27;s good for the consumer, it&#x27;s about what they can do without losing a lawsuit for false advertising.</div><br/></div></div></div></div><div id="39447896" class="c"><input type="checkbox" id="c-39447896" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447819">parent</a><span>|</span><a href="#39450138">prev</a><span>|</span><a href="#39444253">next</a><span>|</span><label class="collapse" for="c-39447896">[-]</label><label class="expand" for="c-39447896">[1 more]</label></div><br/><div class="children"><div class="content">The NIC is attached to the host bus through the north bridge.  But other hosts on the same ethernetwork are not considered to be &quot;local&quot;.  We dont need to get crazy about teh semantics to know that when a cloud provider says an SSD is locally attached, that it&#x27;s closer than an ethernetwork away.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39444253" class="c"><input type="checkbox" id="c-39444253" checked=""/><div class="controls bullet"><span class="by">candiddevmike</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39445175">prev</a><span>|</span><a href="#39446024">next</a><span>|</span><label class="collapse" for="c-39444253">[-]</label><label class="expand" for="c-39444253">[12 more]</label></div><br/><div class="children"><div class="content">Depends on the cloud provider.  Local SSDs are physically attached to the host on GCP, but that makes them only useful for temporary storage.</div><br/><div id="39444754" class="c"><input type="checkbox" id="c-39444754" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444253">parent</a><span>|</span><a href="#39444326">next</a><span>|</span><label class="collapse" for="c-39444754">[-]</label><label class="expand" for="c-39444754">[2 more]</label></div><br/><div class="children"><div class="content">Which is a weird sort of limitation. For any sort of you-own-the-hardware arrangement, NVMe disks are fine for long term storage. (Obviously one should have backups, but that’s a separate issue. One should have a DR plan for data on EBS, too.)<p>You need to <i>migrate</i> that data if you replace an entire server, but this usually isn’t a very big deal.</div><br/><div id="39444869" class="c"><input type="checkbox" id="c-39444869" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444754">parent</a><span>|</span><a href="#39444326">next</a><span>|</span><label class="collapse" for="c-39444869">[-]</label><label class="expand" for="c-39444869">[1 more]</label></div><br/><div class="children"><div class="content">This is Hyrum’s law at play: AWS wants to make sure that the instance stores aren’t seen as persistent, and therefore enforce the failure mode for normal operations as well.<p>You should also see how they enforce similar things for their other products and APIs, for example, most of their services have encrypted pagination tokens.</div><br/></div></div></div></div><div id="39444326" class="c"><input type="checkbox" id="c-39444326" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444253">parent</a><span>|</span><a href="#39444754">prev</a><span>|</span><a href="#39445986">next</a><span>|</span><label class="collapse" for="c-39444326">[-]</label><label class="expand" for="c-39444326">[8 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re at G, you should read the internal docs on exactly how this happens and it will be interesting.</div><br/><div id="39450805" class="c"><input type="checkbox" id="c-39450805" checked=""/><div class="controls bullet"><span class="by">seedless-sensat</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444326">parent</a><span>|</span><a href="#39450240">next</a><span>|</span><label class="collapse" for="c-39450805">[-]</label><label class="expand" for="c-39450805">[1 more]</label></div><br/><div class="children"><div class="content">Why are you protecting Google&#x27;s internal architecture onto to AWS? Your Google mental model is not correct here</div><br/></div></div><div id="39450240" class="c"><input type="checkbox" id="c-39450240" checked=""/><div class="controls bullet"><span class="by">jsolson</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444326">parent</a><span>|</span><a href="#39450805">prev</a><span>|</span><a href="#39444529">next</a><span>|</span><label class="collapse" for="c-39450240">[-]</label><label class="expand" for="c-39450240">[1 more]</label></div><br/><div class="children"><div class="content">In most cases, they&#x27;re physically plugged into a PCIe CEM slot in the host.<p>There is no network in the way, you are either misinformed or thinking of a different product.</div><br/></div></div><div id="39444529" class="c"><input type="checkbox" id="c-39444529" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444326">parent</a><span>|</span><a href="#39450240">prev</a><span>|</span><a href="#39445986">next</a><span>|</span><label class="collapse" for="c-39444529">[-]</label><label class="expand" for="c-39444529">[5 more]</label></div><br/><div class="children"><div class="content">Why would I lose all data on these SSDs when I initiate a power off of the VM on console, then?<p>I believe local SSDs are definitely attached to the host. They are just not exposed via NVMe ZNS hence the performance hit.</div><br/><div id="39445006" class="c"><input type="checkbox" id="c-39445006" checked=""/><div class="controls bullet"><span class="by">res0nat0r</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444529">parent</a><span>|</span><a href="#39444859">next</a><span>|</span><label class="collapse" for="c-39445006">[-]</label><label class="expand" for="c-39445006">[2 more]</label></div><br/><div class="children"><div class="content">Your EC2 instance with instance-store storage when stopped can be launched on any other random host in the AZ when you power it back on. Since your rootdisk is an EBS volume attached across the network, so when you start your instance back up you&#x27;re going to be launched likely somewhere else with an empty slot, and empty local-storage. This is why there is always a disclaimer that this local storage is ephemeral and don&#x27;t count on it being around long-term.</div><br/><div id="39446333" class="c"><input type="checkbox" id="c-39446333" checked=""/><div class="controls bullet"><span class="by">mrcarrot</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445006">parent</a><span>|</span><a href="#39444859">next</a><span>|</span><label class="collapse" for="c-39446333">[-]</label><label class="expand" for="c-39446333">[1 more]</label></div><br/><div class="children"><div class="content">I think the parent was agreeing with you. If the “local” SSDs _weren’t_ actually local, then presumably they wouldn’t need to be ephemeral since they could be connected over the network to whichever host your instance was launched on.</div><br/></div></div></div></div><div id="39444859" class="c"><input type="checkbox" id="c-39444859" checked=""/><div class="controls bullet"><span class="by">manquer</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444529">parent</a><span>|</span><a href="#39445006">prev</a><span>|</span><a href="#39445986">next</a><span>|</span><label class="collapse" for="c-39444859">[-]</label><label class="expand" for="c-39444859">[2 more]</label></div><br/><div class="children"><div class="content">It is because on reboot you may not get the same physical server . They are not rebooting the physical server for you , just the VM<p>Same VM is not allocated for a variety of reasons , scheduled maintenance, proximity to other hosts on the vpc , balancing quiet and noisy neighbors so on.<p>It is not that the disk will always wiped , sometimes the data is still there on reboot just that there is no guarantee allowing them to freely move between hosts</div><br/><div id="39448758" class="c"><input type="checkbox" id="c-39448758" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444859">parent</a><span>|</span><a href="#39445986">next</a><span>|</span><label class="collapse" for="c-39448758">[-]</label><label class="expand" for="c-39448758">[1 more]</label></div><br/><div class="children"><div class="content">Data persists between reboots, but not shutdowns:<p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ec2-instance-reboot.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ec2-inst...</a><p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ec2-instance-lifecycle.html#lifecycle-differences" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ec2-inst...</a></div><br/></div></div></div></div></div></div></div></div><div id="39445986" class="c"><input type="checkbox" id="c-39445986" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444253">parent</a><span>|</span><a href="#39444326">prev</a><span>|</span><a href="#39446024">next</a><span>|</span><label class="collapse" for="c-39445986">[-]</label><label class="expand" for="c-39445986">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s what their purpose is in cloud applications: temporary high performance storage only.<p>If you want long term local storage you&#x27;ll have to reserve an instance host.</div><br/></div></div></div></div><div id="39446024" class="c"><input type="checkbox" id="c-39446024" checked=""/><div class="controls bullet"><span class="by">crotchfire</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39444253">prev</a><span>|</span><a href="#39446840">next</a><span>|</span><label class="collapse" for="c-39446024">[-]</label><label class="expand" for="c-39446024">[1 more]</label></div><br/><div class="children"><div class="content">This is incorrect.<p>Amazon offers both locally-attached storage devices as well as instance-attached storage devices.  The article is about the latter kind.</div><br/></div></div><div id="39446840" class="c"><input type="checkbox" id="c-39446840" checked=""/><div class="controls bullet"><span class="by">hathawsh</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39446024">prev</a><span>|</span><a href="#39444261">next</a><span>|</span><label class="collapse" for="c-39446840">[-]</label><label class="expand" for="c-39446840">[11 more]</label></div><br/><div class="children"><div class="content">That seems like a big opportunity for other cloud providers. They could provide SSDs that are actually physically attached and boast (rightfully) that their SSDs are a lot faster, drawing away business from older cloud providers.</div><br/><div id="39449642" class="c"><input type="checkbox" id="c-39449642" checked=""/><div class="controls bullet"><span class="by">jbnorth</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446840">parent</a><span>|</span><a href="#39447849">next</a><span>|</span><label class="collapse" for="c-39449642">[-]</label><label class="expand" for="c-39449642">[1 more]</label></div><br/><div class="children"><div class="content">This is already a thing. AWS instance store volumes are directly attached to the host. I’m pretty sure GCP and Azure also have an equivalent local storage option.</div><br/></div></div><div id="39447849" class="c"><input type="checkbox" id="c-39447849" checked=""/><div class="controls bullet"><span class="by">ddorian43</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446840">parent</a><span>|</span><a href="#39449642">prev</a><span>|</span><a href="#39447445">next</a><span>|</span><label class="collapse" for="c-39447849">[-]</label><label class="expand" for="c-39447849">[1 more]</label></div><br/><div class="children"><div class="content">Next thing the other clouds will offer is cheaper bandwidth pricing, right?</div><br/></div></div><div id="39447445" class="c"><input type="checkbox" id="c-39447445" checked=""/><div class="controls bullet"><span class="by">solardev</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446840">parent</a><span>|</span><a href="#39447849">prev</a><span>|</span><a href="#39444261">next</a><span>|</span><label class="collapse" for="c-39447445">[-]</label><label class="expand" for="c-39447445">[8 more]</label></div><br/><div class="children"><div class="content">For what kind of workloads would a slower SSD be a significant bottleneck?</div><br/><div id="39449776" class="c"><input type="checkbox" id="c-39449776" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447445">parent</a><span>|</span><a href="#39448439">next</a><span>|</span><label class="collapse" for="c-39449776">[-]</label><label class="expand" for="c-39449776">[4 more]</label></div><br/><div class="children"><div class="content">I run very large database-y workloads. Storage bandwidth is by far the throughput rate limiting factor. Cloud environments are highly constrained in this regard and there is a mismatch between the amount of CPU you are required to buy to get a given amount of bandwidth. I could saturate a much faster storage system with a fraction of the CPU but that isn’t an option. Note that latency is not a major concern here.<p>This has an enormous economic impact. I once did a TCO study with AWS to run data-intensive workload running on purpose-built infrastructure on their cloud. AWS would have been 3x more expensive per their own numbers, they didn’t even argue it. The main difference is that we had highly optimized our storage configuration to provide exceptional throughput for our workload on cheap hardware.<p>I currently run workloads in the cloud because it is convenient. At scale though, the cost difference to run it on your own hardware is compelling. The cloud companies also benefit from a learned helplessness when it comes to physical infrastructure. Ironically, it has never been easier to do a custom infrastructure build, which companies used to do all the time, but most people act like it is deep magic now.</div><br/><div id="39450050" class="c"><input type="checkbox" id="c-39450050" checked=""/><div class="controls bullet"><span class="by">solardev</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39449776">parent</a><span>|</span><a href="#39448439">next</a><span>|</span><label class="collapse" for="c-39450050">[-]</label><label class="expand" for="c-39450050">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for the details!<p>Does this mean you&#x27;re colocating your own server in a data center somewhere? Or do you have your own data center&#x2F;running it off a bare metal server with a business connection?<p>Just wondering if the TCO included the same levels of redundancy and bandwidth, etc.</div><br/><div id="39450828" class="c"><input type="checkbox" id="c-39450828" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39450050">parent</a><span>|</span><a href="#39448439">next</a><span>|</span><label class="collapse" for="c-39450828">[-]</label><label class="expand" for="c-39450828">[2 more]</label></div><br/><div class="children"><div class="content">We were colocated in large data centers right on the major IX with redundancy. All of this was accounted for in their TCO model. We had a better switch fabric than is typical for the cloud but that didn’t materially contribute to cost. We were using AWS for overflow capacity when we exceeded the capacity of our infrastructure at the time; they wanted us to move our primary workload there.<p>The difference in cost could be attributed mostly to the server hardware build, and to a lesser extent the better scalability with a better network. In this case, we ended up working with Quanta on servers that had everything we needed and nothing we didn’t, optimizing heavily for bandwidth&#x2F;$. We worked directly with storage manufacturers to find SKUs that stripped out features we didn’t need and optimized for cost per byte given our device write throughput and durability requirements. They all have hundreds of custom SKUs that they don’t publicly list, you just have to ask. A hidden factor is that the software was designed to take advantage of hardware that most enterprises would not deign to use for high-performance applications. There was a bit of supply chain management but we did this as a startup buying not that many units. The final core server configuration cost us just under $8k each delivered, and it outperformed every off-the-shelf server for twice the price and essentially wasn’t something you could purchase in the cloud (and still isn’t). These servers were brilliant, bulletproof, and exceptionally performant for our use case. You can model out the economics of this and the zero-crossing shows up at a lower burn rate than I think many people imagine.<p>We were extremely effective at using storage, and we did not attach it to expensive, overly-powered servers where the CPUs would have been sitting idle anyway. The sweet spot was low-clock high-core CPUs, which are typically at a low-mid price point but optimal performance-per-dollar if you can effectively scale software to the core count. Since the software architecture was thread-per-core, the core count was not a bottleneck. The economics have not shifted much over time.<p>AWS uses the same pricing model as everyone else in the server leasing game. Roughly speaking, you model your prices to recover your CapEx in 6 months of utilization. Ignoring overhead, doing it ourselves pulled that closer to 1.5-2 months for the same burn. This moves a lot of the cost structure to things like power, space, and bandwidth. We definitely were paying more for space and power than AWS (usually less for bandwidth) but not nearly enough to offset our huge CapEx advantage relative to workload.<p>All of this can be modeled out in Excel. No one does it anymore but I am from a time when it was common, so I have that skill in my back pocket. It isn’t nearly as much work as it sounds like, much of the details are formulaic. You do need to have good data on how your workload uses hardware resources to know what to build.</div><br/><div id="39451474" class="c"><input type="checkbox" id="c-39451474" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39450828">parent</a><span>|</span><a href="#39448439">next</a><span>|</span><label class="collapse" for="c-39451474">[-]</label><label class="expand" for="c-39451474">[1 more]</label></div><br/><div class="children"><div class="content">&gt; All of this can be modeled out in Excel. No one does it anymore but I am from a time when it was common, so I have that skill in my back pocket. It isn’t nearly as much work as it sounds like, much of the details are formulaic. You do need to have good data on how your workload uses hardware resources to know what to build.<p>And this is one of the big &quot;screts&quot; AWS success: Shifting a lot of resource allocation and power from people with budgeting responsibility to developers who have usually never seen the budget or accounts, don&#x27;t keep track, and at most retrospectively gets pulled in to explain line items in expenses, and obscuring it (to the point where I know people who&#x27;ve spent 6 figure amounts worth of dev time building analytics to figure out where their cloud spend goes... tooling has gotten better but is still awful)<p>I believe a whole lot of tech stacks would look <i>very</i> different if developers and architects were more directly involved in budgeting, and bonuses etc. were linked at least in part to financial outcomes affected by their technical choices.<p>A whole lot of claims to low cloud costs come from people who have never done actual comparisons and who seem to have a pathological fear of hardware, even when for most people you don&#x27;t need to ever touch a physical box yourself - you can get maybe 2&#x2F;3&#x27;s of the savings with managed hosting as well.<p>You don&#x27;t get the super-customized server builds, but you do get far more choice than with cloud providers, and you can often make up for the lack of fine-grained control by being able to rent&#x2F;lease them somewhere where the physical hosting is cheaper (e.g. at a previous employer what finally made us switch to Hetzner for most new capacity was that while we didn&#x27;t get exactly the hardware we wanted, we got &quot;close enough&quot; coupled with data centre space in their locations in Germany being far below data centre space in London - it didn&#x27;t make them much cheaper, but it did make them <i>sufficiently</i> cheaper to outweigh the hardware differences with a margin sufficient for us to deploy new stuff there but still keep some of our colo footprint)</div><br/></div></div></div></div></div></div></div></div><div id="39448439" class="c"><input type="checkbox" id="c-39448439" checked=""/><div class="controls bullet"><span class="by">lolc</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447445">parent</a><span>|</span><a href="#39449776">prev</a><span>|</span><a href="#39449079">next</a><span>|</span><label class="collapse" for="c-39448439">[-]</label><label class="expand" for="c-39448439">[2 more]</label></div><br/><div class="children"><div class="content">I tend some workloads that transform data grids of varying sizes. The grids are anon mmaps so that when mem runs out, they get paged out. This means processing stays mostly in-mem yet won&#x27;t abort when mem runs tight. The processes that get hit by paging slow to a crawl though. Getting faster SSD means they&#x27;re still crawling but crawling faster. Doubling SSD throughput would pretty much half the tail latency.</div><br/><div id="39448632" class="c"><input type="checkbox" id="c-39448632" checked=""/><div class="controls bullet"><span class="by">solardev</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448439">parent</a><span>|</span><a href="#39449079">next</a><span>|</span><label class="collapse" for="c-39448632">[-]</label><label class="expand" for="c-39448632">[1 more]</label></div><br/><div class="children"><div class="content">I see. Thanks for explaining!</div><br/></div></div></div></div><div id="39449079" class="c"><input type="checkbox" id="c-39449079" checked=""/><div class="controls bullet"><span class="by">ReflectedImage</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447445">parent</a><span>|</span><a href="#39448439">prev</a><span>|</span><a href="#39444261">next</a><span>|</span><label class="collapse" for="c-39449079">[-]</label><label class="expand" for="c-39449079">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much all work loads, work loads that are not affected would be the exception</div><br/></div></div></div></div></div></div><div id="39444261" class="c"><input type="checkbox" id="c-39444261" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39446840">prev</a><span>|</span><a href="#39444341">next</a><span>|</span><label class="collapse" for="c-39444261">[-]</label><label class="expand" for="c-39444261">[11 more]</label></div><br/><div class="children"><div class="content">They do this because they want SSDs to be in a physically separate part of the building for operational reasons, or what&#x27;s the point in giving you a &quot;local&quot; SSD that isn&#x27;t actually plugged into the real machine?</div><br/><div id="39446759" class="c"><input type="checkbox" id="c-39446759" checked=""/><div class="controls bullet"><span class="by">yolovoe</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444261">parent</a><span>|</span><a href="#39444961">next</a><span>|</span><label class="collapse" for="c-39446759">[-]</label><label class="expand" for="c-39446759">[1 more]</label></div><br/><div class="children"><div class="content">The comment you’re responding to is wrong. AWS offers many kinds of storage. Instance local storage is physically attached to the droplet. EBS isn’t but that’s a separate thing entirely.<p>I literally work in EC2 Nitro.</div><br/></div></div><div id="39444961" class="c"><input type="checkbox" id="c-39444961" checked=""/><div class="controls bullet"><span class="by">ianburrell</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444261">parent</a><span>|</span><a href="#39446759">prev</a><span>|</span><a href="#39444341">next</a><span>|</span><label class="collapse" for="c-39444961">[-]</label><label class="expand" for="c-39444961">[9 more]</label></div><br/><div class="children"><div class="content">The reason for having most instances use network storage is that it makes possible migrating instances to other hosts. If the host fails, the network storage can be pointed at the new host with a reboot. AWS sends out notices regularly when they are going to reboot or migrate instances.<p>Their probably should be more local instance storage types for using with instances that can be recreated without loss. But it is simple for them to have a single way of doing things.<p>At work, someone used fast NVMe instance storage for Clickhouse which is a database. It was a huge hassle to copy data when instances were going to be restarted because the data would be lost.</div><br/><div id="39445385" class="c"><input type="checkbox" id="c-39445385" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444961">parent</a><span>|</span><a href="#39447003">next</a><span>|</span><label class="collapse" for="c-39445385">[-]</label><label class="expand" for="c-39445385">[4 more]</label></div><br/><div class="children"><div class="content">Sure, I understand that, but this user is claiming that on GCP even local SSDs aren&#x27;t really local, which raises the question of why not.<p>I suspect the answer is something to do with their manufacturing processes&#x2F;rack designs. When I worked there (pre GCP) machines had only a tiny disk used for booting and they wanted to get rid of that. Storage was handled by &quot;diskful&quot; machines that had dedicated trays of HDDs connected to their motherboards. If your datacenters and manufacturing processes are optimized for building machines that are either compute or storage but not both, perhaps the more normal cloud model is hard to support and that pushes you towards trying to aggregate storage even for &quot;local&quot; SSD or something.</div><br/><div id="39445512" class="c"><input type="checkbox" id="c-39445512" checked=""/><div class="controls bullet"><span class="by">deadmutex</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445385">parent</a><span>|</span><a href="#39450483">next</a><span>|</span><label class="collapse" for="c-39445512">[-]</label><label class="expand" for="c-39445512">[2 more]</label></div><br/><div class="children"><div class="content">The GCE claim is unverified. OP seems to be referring to PD-SSD and not LocalSSD</div><br/><div id="39450257" class="c"><input type="checkbox" id="c-39450257" checked=""/><div class="controls bullet"><span class="by">rwiggins</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445512">parent</a><span>|</span><a href="#39450483">next</a><span>|</span><label class="collapse" for="c-39450257">[-]</label><label class="expand" for="c-39450257">[1 more]</label></div><br/><div class="children"><div class="content">GCE local SSDs absolutely are on the same host as the VM. The docs [0] are pretty clear on this, I think:<p>&gt; Local SSD disks are physically attached to the server that hosts your VM.<p>Disclosure: I work on GCE.<p>[0] <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd</a></div><br/></div></div></div></div><div id="39450483" class="c"><input type="checkbox" id="c-39450483" checked=""/><div class="controls bullet"><span class="by">flaminHotSpeedo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445385">parent</a><span>|</span><a href="#39445512">prev</a><span>|</span><a href="#39447003">next</a><span>|</span><label class="collapse" for="c-39450483">[-]</label><label class="expand" for="c-39450483">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re claiming so, but they&#x27;re wrong.</div><br/></div></div></div></div><div id="39447003" class="c"><input type="checkbox" id="c-39447003" checked=""/><div class="controls bullet"><span class="by">wiredfool</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444961">parent</a><span>|</span><a href="#39445385">prev</a><span>|</span><a href="#39446444">next</a><span>|</span><label class="collapse" for="c-39447003">[-]</label><label class="expand" for="c-39447003">[2 more]</label></div><br/><div class="children"><div class="content">Are you saying that a reboot wipes the ephemeral disks?  Or a stop the instance and start the instance from AWS console&#x2F;api?</div><br/><div id="39447073" class="c"><input type="checkbox" id="c-39447073" checked=""/><div class="controls bullet"><span class="by">ianburrell</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447003">parent</a><span>|</span><a href="#39446444">next</a><span>|</span><label class="collapse" for="c-39447073">[-]</label><label class="expand" for="c-39447073">[1 more]</label></div><br/><div class="children"><div class="content">Reboot keeps the instance storage volumes. Restarting wipes them. Starting frequently migrates to new host. And the &quot;restart&quot; notices AWS sends are likely cause the host has a problem and need to migrate it.</div><br/></div></div></div></div><div id="39446444" class="c"><input type="checkbox" id="c-39446444" checked=""/><div class="controls bullet"><span class="by">youngtaff</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444961">parent</a><span>|</span><a href="#39447003">prev</a><span>|</span><a href="#39444341">next</a><span>|</span><label class="collapse" for="c-39446444">[-]</label><label class="expand" for="c-39446444">[2 more]</label></div><br/><div class="children"><div class="content">&gt; At work, someone used fast NVMe instance storage for Clickhouse which is a database. It was a huge hassle to copy data when instances were going to be restarted because the data would be lost.<p>This post on how Discord RAIDed local NVMe volumes with slower remote volumes might be on interest <a href="https:&#x2F;&#x2F;discord.com&#x2F;blog&#x2F;how-discord-supercharges-network-disks-for-extreme-low-latency" rel="nofollow">https:&#x2F;&#x2F;discord.com&#x2F;blog&#x2F;how-discord-supercharges-network-di...</a></div><br/><div id="39447097" class="c"><input type="checkbox" id="c-39447097" checked=""/><div class="controls bullet"><span class="by">ianburrell</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446444">parent</a><span>|</span><a href="#39444341">next</a><span>|</span><label class="collapse" for="c-39447097">[-]</label><label class="expand" for="c-39447097">[1 more]</label></div><br/><div class="children"><div class="content">We moved to running Clickhouse on EKS with EBS volumes for storage. It can better survive instances going down. I didn&#x27;t work on it so don&#x27;t how much slower it is. Lowering the management burden was big priority.</div><br/></div></div></div></div></div></div></div></div><div id="39444341" class="c"><input type="checkbox" id="c-39444341" checked=""/><div class="controls bullet"><span class="by">colechristensen</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39444261">prev</a><span>|</span><a href="#39446893">next</a><span>|</span><label class="collapse" for="c-39444341">[-]</label><label class="expand" for="c-39444341">[2 more]</label></div><br/><div class="children"><div class="content">For AWS there are EBS volumes attached through a custom hardware NVMe interface and then there&#x27;s Instance Store which is actually local SSD storage.  These are different things.<p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;InstanceStorage.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;Instance...</a></div><br/><div id="39444710" class="c"><input type="checkbox" id="c-39444710" checked=""/><div class="controls bullet"><span class="by">kwillets</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444341">parent</a><span>|</span><a href="#39446893">next</a><span>|</span><label class="collapse" for="c-39444710">[-]</label><label class="expand" for="c-39444710">[1 more]</label></div><br/><div class="children"><div class="content">EBS is also slower than local NVMe mounts on i3&#x27;s.<p>Also, both features use Nitro SSD cards, according to AWS docs. The Nitro architecture is all locally attached -- instance storage to the instance, EBS to the EBS server.</div><br/></div></div></div></div><div id="39446893" class="c"><input type="checkbox" id="c-39446893" checked=""/><div class="controls bullet"><span class="by">Salgat</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39444341">prev</a><span>|</span><a href="#39444248">next</a><span>|</span><label class="collapse" for="c-39446893">[-]</label><label class="expand" for="c-39446893">[1 more]</label></div><br/><div class="children"><div class="content">At first you&#x27;d think maybe they can do a volume copy from a snapshot to a local drive on instance creation but even at 100gbps you&#x27;re looking at almost 3 minutes for a 2TB drive.</div><br/></div></div><div id="39444248" class="c"><input type="checkbox" id="c-39444248" checked=""/><div class="controls bullet"><span class="by">ddorian43</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39446893">prev</a><span>|</span><a href="#39444222">next</a><span>|</span><label class="collapse" for="c-39444248">[-]</label><label class="expand" for="c-39444248">[1 more]</label></div><br/><div class="children"><div class="content">Do you have a link to explain this? I dont think its true.</div><br/></div></div><div id="39444222" class="c"><input type="checkbox" id="c-39444222" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39444248">prev</a><span>|</span><a href="#39446163">next</a><span>|</span><label class="collapse" for="c-39444222">[-]</label><label class="expand" for="c-39444222">[1 more]</label></div><br/><div class="children"><div class="content">What makes you think that?</div><br/></div></div><div id="39446163" class="c"><input type="checkbox" id="c-39446163" checked=""/><div class="controls bullet"><span class="by">bfncieezo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39444222">prev</a><span>|</span><a href="#39446271">next</a><span>|</span><label class="collapse" for="c-39446163">[-]</label><label class="expand" for="c-39446163">[1 more]</label></div><br/><div class="children"><div class="content">instances can have block storage which is network attached, or local attached ssd&#x2F;nvme.  its 2 separate things.</div><br/></div></div><div id="39446271" class="c"><input type="checkbox" id="c-39446271" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444161">parent</a><span>|</span><a href="#39446163">prev</a><span>|</span><a href="#39448728">next</a><span>|</span><label class="collapse" for="c-39446271">[-]</label><label class="expand" for="c-39446271">[1 more]</label></div><br/><div class="children"><div class="content">Nope!  Well not as advertised. There are instances, usually more expensive ones, where there are supposed to be local NVME disks dedicated to the instance.  You&#x27;re totally right that providing good I&#x2F;O is a big problem!  And I have done studies myself showing just how bad Google Cloud is here, and have totally ditched Google Cloud for providing crappy compute service (and even worse customer service).</div><br/></div></div></div></div><div id="39448728" class="c"><input type="checkbox" id="c-39448728" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444096">parent</a><span>|</span><a href="#39444161">prev</a><span>|</span><a href="#39444353">next</a><span>|</span><label class="collapse" for="c-39448728">[-]</label><label class="expand" for="c-39448728">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve run CI&#x2F;CD pipelines on EC2 machines with local storage, typically running Raid-0, btrfs, noaccestime. I didn&#x27;t care if the filesystem got corrupt or whatever, I had a script that would rebuild it in under 30mins. In addition to the performance you&#x27;re not paying by IOPs.</div><br/></div></div><div id="39444353" class="c"><input type="checkbox" id="c-39444353" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444096">parent</a><span>|</span><a href="#39448728">prev</a><span>|</span><a href="#39451330">next</a><span>|</span><label class="collapse" for="c-39444353">[-]</label><label class="expand" for="c-39444353">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>It&#x27;s about SSDs being two orders of magnitude slower than datacenter networks.</i><p>Could that have to do with every operation requiring a round trip, rather than being able to queue up operations in a buffer to saturate throughput?<p>It seems plausible if the interface protocol was built for a device it assumed was physically local and so waited for confirmation after each operation before performing the next.<p>In this case it&#x27;s not so much the throughput rate that matters, but the latency -- which can also be heavily affected by buffering of other network traffic.</div><br/><div id="39444467" class="c"><input type="checkbox" id="c-39444467" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444353">parent</a><span>|</span><a href="#39451330">next</a><span>|</span><label class="collapse" for="c-39444467">[-]</label><label class="expand" for="c-39444467">[1 more]</label></div><br/><div class="children"><div class="content">Underlying protocol limitations wouldn&#x27;t be an issue - the cloud provider&#x27;s implementation can work around that. They&#x27;re unlikely to be sending sequential SCSI&#x2F;NVMe commands over the wire - instead, the hypervisor pretends to be the NVME device, but then converts to some internal protocol (that&#x27;s less chatty and can coalesce requests without waiting on individual ACKs) before sending that to the storage server.<p>The problem is that ultimately your application often requires the outcome of a given IO operation to decide which operation to perform next - let&#x27;s say when it comes to a database, it should first read the index (and wait for that to complete) before it knows the on-disk location of the actual row data which it needs to be able to issue the next IO operation.<p>In this case, there&#x27;s no other solution than to move that application closer to the data itself. Instead of the networked storage node being a dumb blob storage returning bytes, the networked &quot;storage&quot; node is your database itself, returning query results. I believe that&#x27;s what RDS Aurora does for example, every storage node can itself understand query predicates.</div><br/></div></div></div></div></div></div><div id="39451330" class="c"><input type="checkbox" id="c-39451330" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444096">prev</a><span>|</span><a href="#39444028">next</a><span>|</span><label class="collapse" for="c-39451330">[-]</label><label class="expand" for="c-39451330">[1 more]</label></div><br/><div class="children"><div class="content">It does not fundamentally have to be. That&#x27;s an architectural choice driven by cloud providers backing off from the &quot;instances can die on you&quot; choice that AWS <i>started with</i> and then realized customers struggled with, towards attempting to keep things running in the face of many types of hardware failures.<p>Which is ironic given that when building on-prem&#x2F;colo&#x27;ed setups you&#x27;ll replicate things to be prepared for unknown lengths of downtime while equipment is repaired or replaced, so this was largely cloud marketing coming bak to bite cloud providers&#x27; collective asses. Not wanting instances to die &quot;randomly&quot; for no good reason does not always mean wanting performance sacrifices for the sake of more resilient instances.<p>But AWS at least still offers plenty of instances with instance storage.<p>If I&#x27;m setting up my own database cluster, while I don&#x27;t want it running on cheap consumer-grade hardware without dual power supplies and RAID, I also don&#x27;t want to sacrifice SSD speed for something network-attached to survive a catastrophic failure when I&#x27;m going to have both backups, archived shipped logs <i>and</i> at least one replica anyway.<p>I&#x27;d happily pick network-attached storage for many things if it gets me increased resilience, but selling me a network-attached SSD, unless it replicates local SSD performance characteristics, is not competitive for applications where performance matters and I&#x27;m set up to easily handle system-wide failures anyway.</div><br/></div></div><div id="39444028" class="c"><input type="checkbox" id="c-39444028" checked=""/><div class="controls bullet"><span class="by">_Rabs_</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39451330">prev</a><span>|</span><a href="#39450172">next</a><span>|</span><label class="collapse" for="c-39444028">[-]</label><label class="expand" for="c-39444028">[21 more]</label></div><br/><div class="children"><div class="content">So much of this. 
The amount of times I&#x27;ve seen someone complain about slow DB performance when they&#x27;re trying to connect to it from a different VPC, and bottlenecking themselves to 100Mbits is stupidly high.<p>Literally depending on where things are in a data center... If you&#x27;re looking for closely coupled and on a 10G line on the same switch, going to the same server rack. I bet you performance will be so much more consistent.</div><br/><div id="39444090" class="c"><input type="checkbox" id="c-39444090" checked=""/><div class="controls bullet"><span class="by">bugbuddy</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444028">parent</a><span>|</span><a href="#39444438">next</a><span>|</span><label class="collapse" for="c-39444090">[-]</label><label class="expand" for="c-39444090">[9 more]</label></div><br/><div class="children"><div class="content">Aren’t 10G and 100G connections standard nowadays in data centers? Heck, I thought they were standard 10 years ago.</div><br/><div id="39444293" class="c"><input type="checkbox" id="c-39444293" checked=""/><div class="controls bullet"><span class="by">geerlingguy</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444090">parent</a><span>|</span><a href="#39446155">next</a><span>|</span><label class="collapse" for="c-39444293">[-]</label><label class="expand" for="c-39444293">[1 more]</label></div><br/><div class="children"><div class="content">Datacenters are up to 400 Gbps and beyond (many places are adopting 1+ Tbps on core switching).<p>However, individual servers may still operate at 10, 25, or 40 Gbps to save cost on the thousands of NICs in a row of racks. Alternatively, servers with multiple 100G connections split that bandwidth allocation up among dozens of VMs so each one gets 1 or 10G.</div><br/></div></div><div id="39446155" class="c"><input type="checkbox" id="c-39446155" checked=""/><div class="controls bullet"><span class="by">KaiserPro</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444090">parent</a><span>|</span><a href="#39444293">prev</a><span>|</span><a href="#39444315">next</a><span>|</span><label class="collapse" for="c-39446155">[-]</label><label class="expand" for="c-39446155">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but you have to think about contention. Whilst the Top of rack <i>might</i> have 2x400 gig links to the core, thats shared with the entire rack, and all the other machines trying to shout at the core switching infra.<p>Then stuff goes away, or route congested, etc, etc, etc.</div><br/></div></div><div id="39444315" class="c"><input type="checkbox" id="c-39444315" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444090">parent</a><span>|</span><a href="#39446155">prev</a><span>|</span><a href="#39444309">next</a><span>|</span><label class="collapse" for="c-39444315">[-]</label><label class="expand" for="c-39444315">[5 more]</label></div><br/><div class="children"><div class="content">Bandwidth delay product does not help serialized transactions. If you&#x27;re reaching out to disk for results, or if you have locking transactions on a table the achievable operations drops dramatically as latency between the host and the disk increases.</div><br/><div id="39444997" class="c"><input type="checkbox" id="c-39444997" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444315">parent</a><span>|</span><a href="#39444309">next</a><span>|</span><label class="collapse" for="c-39444997">[-]</label><label class="expand" for="c-39444997">[4 more]</label></div><br/><div class="children"><div class="content">The typical way to trade bandwidth away for latency would, I guess, be speculative requests. In the CPU world at least. I wonder if any cloud providers have some sort of framework built around speculative disk reads (or maybe it is a totally crazy trade to make in this context)?</div><br/><div id="39448491" class="c"><input type="checkbox" id="c-39448491" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444997">parent</a><span>|</span><a href="#39446671">next</a><span>|</span><label class="collapse" for="c-39448491">[-]</label><label class="expand" for="c-39448491">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d need the whole stack to understand your data format in order to make speculative requests useful. It wouldn&#x27;t surprise me if cloud providers indeed do speculative reads but there isn&#x27;t much they can do to understand your data format, so chances are they&#x27;re just reading a few extra blocks beyond where your OS read and are hoping that the next OS-initiated read will fall there so it can be serviced using this prefetched data. Because of full-disk-encryption, the storage stack may not be privy to the actual data so it couldn&#x27;t make smarter, data-aware decisions even if it wanted to, limiting it to primitive readahead or maybe statistics based on previously-seen patterns (if it sees that a request for block X is often followed by block Y, it may choose to prefetch that next time it sees block X accessed).<p>A problem in applications such as databases is when the outcome of an IO operation is required to initiate the next one - for example, you must first read an index to know the on-disk location of the actual row data. This is where the higher latency absolutely tanks performance.<p>A solution could be to make the storage drives smarter - have an NVME command that could say like &quot;search in between this range for this byte pattern&quot; and one that can say &quot;use the outcome of the previous command as a the start address and read N bytes from there&quot;. This could help speed up the aforementioned scenario (effectively your drive will do the index scan &amp; row retrieval for you), but would require cooperation between the application, the filesystem and the encryption system (typical, current FDE would break this).</div><br/></div></div><div id="39446671" class="c"><input type="checkbox" id="c-39446671" checked=""/><div class="controls bullet"><span class="by">treflop</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444997">parent</a><span>|</span><a href="#39448491">prev</a><span>|</span><a href="#39446077">next</a><span>|</span><label class="collapse" for="c-39446671">[-]</label><label class="expand" for="c-39446671">[1 more]</label></div><br/><div class="children"><div class="content">Often times it’s the app (or something high level) that would need speculative requests, which it may not be possible in the given domain.<p>I don’t think it’s possible in most domains.</div><br/></div></div><div id="39446077" class="c"><input type="checkbox" id="c-39446077" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444997">parent</a><span>|</span><a href="#39446671">prev</a><span>|</span><a href="#39444309">next</a><span>|</span><label class="collapse" for="c-39446077">[-]</label><label class="expand" for="c-39446077">[1 more]</label></div><br/><div class="children"><div class="content">I mean we already have readahead in the kernel.<p>This said the problem can get more complex than this really fast. Write barriers for example and dirty caches. Any application that forces writes and the writes are enforced by the kernel are going to suffer.<p>The same is true for SSD settings. There are a number of tweakable values on SSDs when it comes to write commit and cache usage which can affect performance. Desktop OS&#x27;s tend to play more fast and loose with these settings and servers defaults tend to be more conservative.</div><br/></div></div></div></div></div></div><div id="39444309" class="c"><input type="checkbox" id="c-39444309" checked=""/><div class="controls bullet"><span class="by">nixass</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444090">parent</a><span>|</span><a href="#39444315">prev</a><span>|</span><a href="#39444438">next</a><span>|</span><label class="collapse" for="c-39444309">[-]</label><label class="expand" for="c-39444309">[1 more]</label></div><br/><div class="children"><div class="content">400G is fairly normal thing in DCs nowadays</div><br/></div></div></div></div><div id="39444438" class="c"><input type="checkbox" id="c-39444438" checked=""/><div class="controls bullet"><span class="by">silverquiet</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444028">parent</a><span>|</span><a href="#39444090">prev</a><span>|</span><a href="#39450172">next</a><span>|</span><label class="collapse" for="c-39444438">[-]</label><label class="expand" for="c-39444438">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Literally depending on where things are in a data center<p>I thought cloud was supposed to abstract this away? That&#x27;s a bit of a sarcastic question from a long-time cloud skeptic, but... wasn&#x27;t it?</div><br/><div id="39444488" class="c"><input type="checkbox" id="c-39444488" checked=""/><div class="controls bullet"><span class="by">doubled112</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444438">parent</a><span>|</span><a href="#39446736">next</a><span>|</span><label class="collapse" for="c-39444488">[-]</label><label class="expand" for="c-39444488">[7 more]</label></div><br/><div class="children"><div class="content">Reality always beats the abstraction.  After all, it&#x27;s just somebody else&#x27;s computer in somebody else&#x27;s data center.</div><br/><div id="39444553" class="c"><input type="checkbox" id="c-39444553" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444488">parent</a><span>|</span><a href="#39446736">next</a><span>|</span><label class="collapse" for="c-39444553">[-]</label><label class="expand" for="c-39444553">[6 more]</label></div><br/><div class="children"><div class="content">Which can cause considerable &quot;amusement&quot; depending on the provider - one I won&#x27;t name directly but is much more centered on actual renting racks than their (now) cloud offering - if you had a virtual machine older than a year or so, deleting and restoring it would get you on a newer &quot;host&quot; and you&#x27;d be faster for the same cost.<p>Otherwise it&#x27;d stay on the same physical piece of hardware it was allocated to when new.</div><br/><div id="39444620" class="c"><input type="checkbox" id="c-39444620" checked=""/><div class="controls bullet"><span class="by">doubled112</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444553">parent</a><span>|</span><a href="#39446736">next</a><span>|</span><label class="collapse" for="c-39444620">[-]</label><label class="expand" for="c-39444620">[5 more]</label></div><br/><div class="children"><div class="content">Amusing is a good description.<p>&quot;Hardware degradation detected, please turn it off and back on again&quot;<p>I could do a migration with zero downtime in VMware for a decade but they can&#x27;t seamlessly move my VM to a machine that works in 2024?  Great, thanks.  Amusing.</div><br/><div id="39445263" class="c"><input type="checkbox" id="c-39445263" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444620">parent</a><span>|</span><a href="#39445751">next</a><span>|</span><label class="collapse" for="c-39445263">[-]</label><label class="expand" for="c-39445263">[1 more]</label></div><br/><div class="children"><div class="content">I have always been incredibly saddened that apparently the cloud providers usually have nothing as advanced as old VMware was.</div><br/></div></div><div id="39445751" class="c"><input type="checkbox" id="c-39445751" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444620">parent</a><span>|</span><a href="#39445263">prev</a><span>|</span><a href="#39446736">next</a><span>|</span><label class="collapse" for="c-39445751">[-]</label><label class="expand" for="c-39445751">[3 more]</label></div><br/><div class="children"><div class="content">Cloud providers have live migration now but I guess they don&#x27;t want to guarantee anything.</div><br/><div id="39447261" class="c"><input type="checkbox" id="c-39447261" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445751">parent</a><span>|</span><a href="#39446736">next</a><span>|</span><label class="collapse" for="c-39447261">[-]</label><label class="expand" for="c-39447261">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s better (and better still with other providers) but I naively thought that &quot;add more RAM&quot; or &quot;add more disk&quot; was something they would be able to do with a reboot at most.<p>Nope, some require a full backup and restore.</div><br/><div id="39447719" class="c"><input type="checkbox" id="c-39447719" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447261">parent</a><span>|</span><a href="#39446736">next</a><span>|</span><label class="collapse" for="c-39447719">[-]</label><label class="expand" for="c-39447719">[1 more]</label></div><br/><div class="children"><div class="content">Resizing VMs doesn&#x27;t really fit the &quot;cattle&quot; thinking of public cloud, although IMO that was kind of a premature optimization. This would be a perfect use case for live migration.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39446736" class="c"><input type="checkbox" id="c-39446736" checked=""/><div class="controls bullet"><span class="by">treflop</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444438">parent</a><span>|</span><a href="#39444488">prev</a><span>|</span><a href="#39445334">next</a><span>|</span><label class="collapse" for="c-39446736">[-]</label><label class="expand" for="c-39446736">[2 more]</label></div><br/><div class="children"><div class="content">Cloud makes provisioning more servers quicker because you are paying someone to basically have a bunch of servers ready to go right away with an API call instead of a phone call, maintained by a team that isn’t yours, with economies of scale working for the provider.<p>Cloud does not do anything else.<p>None of these latency&#x2F;speed problems are cloud-specific. If you have on-premise servers and you are storing your data on network-attached storage, you have the exact same problems (and also the same advantages).<p>Unfortunately the gap between local and network storage is wide. You win some, you lose some.</div><br/><div id="39447952" class="c"><input type="checkbox" id="c-39447952" checked=""/><div class="controls bullet"><span class="by">silverquiet</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446736">parent</a><span>|</span><a href="#39445334">next</a><span>|</span><label class="collapse" for="c-39447952">[-]</label><label class="expand" for="c-39447952">[1 more]</label></div><br/><div class="children"><div class="content">Oh, I&#x27;m not a complete neophyte (in what seems like a different life now, I worked for a big hosting provider actually), I was just surprised that there was a big penalty for cross-VPC traffic implied by the parent poster.</div><br/></div></div></div></div><div id="39445334" class="c"><input type="checkbox" id="c-39445334" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444438">parent</a><span>|</span><a href="#39446736">prev</a><span>|</span><a href="#39450172">next</a><span>|</span><label class="collapse" for="c-39445334">[-]</label><label class="expand" for="c-39445334">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more of a matter of adding additional abstraction layers. For example in most public clouds the best you can hope for is to place two things in the same availability zone to get the best performance. But when I worked at Google, internally they had more sophisticated colocation constraint than that: for example you can require two things to be on the same rack.</div><br/></div></div></div></div></div></div><div id="39450172" class="c"><input type="checkbox" id="c-39450172" checked=""/><div class="controls bullet"><span class="by">jsolson</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444028">prev</a><span>|</span><a href="#39444396">next</a><span>|</span><label class="collapse" for="c-39450172">[-]</label><label class="expand" for="c-39450172">[3 more]</label></div><br/><div class="children"><div class="content">This is untrue of Local SSD (<a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;local-ssd" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;local-ssd</a>) in Google Cloud. Local SSDs are PCIe peripherals, not network attached.<p>There are also multiple Persistent Disk (<a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;persistent-disk" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;persistent-disk</a>) offerings that are backed by SSDs over the network.<p>(I&#x27;m an engineer on GCE. I work directly on the physical hardware that backs our virtualization platform.)</div><br/><div id="39450847" class="c"><input type="checkbox" id="c-39450847" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39450172">parent</a><span>|</span><a href="#39444396">next</a><span>|</span><label class="collapse" for="c-39450847">[-]</label><label class="expand" for="c-39450847">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s notable that your second link has a screenshot for 24(!) NVMe SSDs totalling 9 terabytes, but the aggregate performance is 2.4M IOPS and 9.3 GB&#x2F;s for reads. In other words, just 100K&#x2F;400MB per individual SSD, which is very low these days.<p>For comparison, a single 1 TB consumer SSD can deliver comparable numbers (lower IOPS but higher throughput).<p>If I plugged 24 consumer SSDs into a box, I would expect over 30M IOPS and near the memory bus limit for throughput (&gt;50 GB&#x2F;s).</div><br/><div id="39451102" class="c"><input type="checkbox" id="c-39451102" checked=""/><div class="controls bullet"><span class="by">jsolson</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39450847">parent</a><span>|</span><a href="#39444396">next</a><span>|</span><label class="collapse" for="c-39451102">[-]</label><label class="expand" for="c-39451102">[1 more]</label></div><br/><div class="children"><div class="content">I should&#x27;ve aimed for more clarity in my original comment -- the first link is to locally attached storage. The second is network attached storage (what the GP was likely referring to, but not what is described in the article).<p>Persistent Disk is not backed by single devices (even for a single NVMe attachment), but by multiple redundant copies spread across power and network failure domains. Those volumes will survive the failure of the VM to which they are attached as well as the failure of any individual volume or host.</div><br/></div></div></div></div></div></div><div id="39444396" class="c"><input type="checkbox" id="c-39444396" checked=""/><div class="controls bullet"><span class="by">boulos</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39450172">prev</a><span>|</span><a href="#39444952">next</a><span>|</span><label class="collapse" for="c-39444396">[-]</label><label class="expand" for="c-39444396">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure which external or internal product you&#x27;re talking about, but there are no networks involved for Local SSD on GCE: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd</a><p>Are you referring to PD-SSD? Internal storage usage?</div><br/></div></div><div id="39444952" class="c"><input type="checkbox" id="c-39444952" checked=""/><div class="controls bullet"><span class="by">scottlamb</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444396">prev</a><span>|</span><a href="#39444024">next</a><span>|</span><label class="collapse" for="c-39444952">[-]</label><label class="expand" for="c-39444952">[14 more]</label></div><br/><div class="children"><div class="content">&gt; The problem is that this network is so large and slow that it can&#x27;t give you anywhere near the performance of a local SSD. This wasn&#x27;t a problem for hard drives, which was the backing technology when a lot of these network attached storage systems were invented, because they are fundamentally slow compared to networks, but it is a problem for SSD.<p>Certainly true that SSD bandwidth and latency improvements are hard to match, but I don&#x27;t understand why intra-datacenter network latency in particular is so bad. This ~2020-I-think version of the &quot;Latency Numbers Everyone Should Know&quot; says 0.5 ms round trip (and mentions &quot;10 Gbps network&quot; on another line). [1] It was the same thing in a 2012 version (that only mentions &quot;1 Gbps network&quot;). [2] Why no improvement? I think that 2020 version might have been a bit conservative on this line, and nice datacenters may even have multiple 100 Gbit&#x2F;sec NICs per machine in 2024, but still I think the round trip actually is strangely bad.<p>I&#x27;ve seen experimental networking stuff (e.g. RDMA) that claims significantly better latency, so I don&#x27;t think it&#x27;s a physical limitation of the networking gear but rather something at the machine&#x2F;OS interaction area. I would design large distributed systems significantly differently (be much more excited about extra tiers in my stack) if the standard RPC system offered say 10 µs typical round trip latency.<p>[1] <a href="https:&#x2F;&#x2F;static.googleusercontent.com&#x2F;media&#x2F;sre.google&#x2F;en&#x2F;&#x2F;static&#x2F;pdf&#x2F;rule-of-thumb-latency-numbers-letter.pdf" rel="nofollow">https:&#x2F;&#x2F;static.googleusercontent.com&#x2F;media&#x2F;sre.google&#x2F;en&#x2F;&#x2F;st...</a><p>[2] <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;jboner&#x2F;2841832" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;jboner&#x2F;2841832</a></div><br/><div id="39445409" class="c"><input type="checkbox" id="c-39445409" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444952">parent</a><span>|</span><a href="#39446206">next</a><span>|</span><label class="collapse" for="c-39445409">[-]</label><label class="expand" for="c-39445409">[1 more]</label></div><br/><div class="children"><div class="content">Modern data center networks don&#x27;t have full cross connectivity.  Instead they are built using graphs and hierarchies that provide less than the total bandwidth required for all pairs of hosts to be communicating.  This means, as workloads start to grow and large numbers of compute hosts demand data IO to&#x2F;from storage hosts, the network eventually gets congested, which typically exhibits as higher latencies and more dropped packets.  Batch jobs are often relegated to &quot;spare&quot; bandwidth while serving jobs often get dedicated bandwidth<p>At the same time, ethernetworks with layered network protocols on top typically have a fair amount of latency overhead, that makes it much slower than bus-based direct-host-attached storage.  I was definitely impressed at how quickly SSDs reached and then exceeded SATA bandwidth.  nvme has made a HUGE difference here.</div><br/></div></div><div id="39446206" class="c"><input type="checkbox" id="c-39446206" checked=""/><div class="controls bullet"><span class="by">KaiserPro</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444952">parent</a><span>|</span><a href="#39445409">prev</a><span>|</span><a href="#39445433">next</a><span>|</span><label class="collapse" for="c-39446206">[-]</label><label class="expand" for="c-39446206">[7 more]</label></div><br/><div class="children"><div class="content">Networks are not reliable, despite what you hear, so latency is used to mask re-tries and delays.<p>The other thing to note about big inter-DC links are heavily QoS&#x27;d and contented, because they are both expensive and a bollock to maintain.<p>Also, from what I recall, 40gig links are just parallel 10 gig links, so have no lower latency. I&#x27;m not sure if 100&#x2F;400 gigs are ten&#x2F;fourty lines of ten gigs in parallel or actually able to issue packets at 10&#x2F;40 times a ten gig link. I&#x27;ve been away from networking too long</div><br/><div id="39446971" class="c"><input type="checkbox" id="c-39446971" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446206">parent</a><span>|</span><a href="#39446231">next</a><span>|</span><label class="collapse" for="c-39446971">[-]</label><label class="expand" for="c-39446971">[1 more]</label></div><br/><div class="children"><div class="content"><i>40gig links are just parallel 10 gig links, so have no lower latency</i><p>That&#x27;s not correct. Higher link speeds do have lower serialization latency, although that&#x27;s a small fraction of overall network latency.</div><br/></div></div><div id="39446231" class="c"><input type="checkbox" id="c-39446231" checked=""/><div class="controls bullet"><span class="by">scottlamb</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446206">parent</a><span>|</span><a href="#39446971">prev</a><span>|</span><a href="#39445433">next</a><span>|</span><label class="collapse" for="c-39446231">[-]</label><label class="expand" for="c-39446231">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Networks are not reliable, despite what you hear, so latency is used to mask re-tries and delays.<p>Of course, but even the 50%ile case is strangely slow, and if that involves retries something is deeply wrong.</div><br/><div id="39446331" class="c"><input type="checkbox" id="c-39446331" checked=""/><div class="controls bullet"><span class="by">KaiserPro</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446231">parent</a><span>|</span><a href="#39445433">next</a><span>|</span><label class="collapse" for="c-39446331">[-]</label><label class="expand" for="c-39446331">[4 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, but TCP doesn&#x27;t like packets being dropped halfway through a stream. If you have a highly QoS&#x27;d link then you&#x27;ll see latency spikes.</div><br/><div id="39446582" class="c"><input type="checkbox" id="c-39446582" checked=""/><div class="controls bullet"><span class="by">scottlamb</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446331">parent</a><span>|</span><a href="#39445433">next</a><span>|</span><label class="collapse" for="c-39446582">[-]</label><label class="expand" for="c-39446582">[3 more]</label></div><br/><div class="children"><div class="content">Again, I&#x27;m not talking about spikes (though better tail latency is always desirable) but poor latency in the 50%ile case. And for high-QoS applications, not batch stuff. The snap paper linked elsewhere in the thread shows 10 µs latencies; they&#x27;ve put in some optimization to achieve that, but I don&#x27;t really understand why we don&#x27;t expect close to that with standard kernel networking and TCP.</div><br/><div id="39448620" class="c"><input type="checkbox" id="c-39448620" checked=""/><div class="controls bullet"><span class="by">vitus</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446582">parent</a><span>|</span><a href="#39445433">next</a><span>|</span><label class="collapse" for="c-39448620">[-]</label><label class="expand" for="c-39448620">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The snap paper linked elsewhere in the thread shows 10 µs latencies; they&#x27;ve put in some optimization to achieve that, but I don&#x27;t really understand why we don&#x27;t expect close to that with standard kernel networking and TCP.<p>You can get similar results by looking at comparisons between DPDK and kernel networking. Most of the usual gap comes from not needing to context-switch for kernel interrupt handling, zero-copy abstractions, and busy polling (wherein you trade CPU for lower latency instead of sleeping between iterations if there&#x27;s no work to be done).<p><a href="https:&#x2F;&#x2F;talawah.io&#x2F;blog&#x2F;linux-kernel-vs-dpdk-http-performance-showdown&#x2F;" rel="nofollow">https:&#x2F;&#x2F;talawah.io&#x2F;blog&#x2F;linux-kernel-vs-dpdk-http-performanc...</a> goes into some amount of detail comparing request throughput of an unoptimized kernel networking stack, optimized kernel networking stack, and DPDK. I&#x27;m not aware of any benchmarks (public or private) comparing Snap vs DPDK vs Linux, so that&#x27;s probably as close as you&#x27;ll get.</div><br/><div id="39450493" class="c"><input type="checkbox" id="c-39450493" checked=""/><div class="controls bullet"><span class="by">CoolCold</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448620">parent</a><span>|</span><a href="#39445433">next</a><span>|</span><label class="collapse" for="c-39450493">[-]</label><label class="expand" for="c-39450493">[1 more]</label></div><br/><div class="children"><div class="content">Great reading, thanks for the link on vanilla vs dpdk</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39445433" class="c"><input type="checkbox" id="c-39445433" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444952">parent</a><span>|</span><a href="#39446206">prev</a><span>|</span><a href="#39444024">next</a><span>|</span><label class="collapse" for="c-39445433">[-]</label><label class="expand" for="c-39445433">[5 more]</label></div><br/><div class="children"><div class="content">That document is probably deliberately on the pessimistic side to encourage your code to be portable across all kinds of &quot;data centers&quot; (however that is defined). When I previously worked at Google, the standard RPC system definitely offered 50 microseconds of round trip latency at the median (I measured it myself in a real application), and their advanced user-space implementation called Snap could offer about 10 microseconds of round trip latency. The latter figure comes from page 9 of <a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;gweb-research2023-media&#x2F;pubtools&#x2F;pdf&#x2F;36f0f9b41e969a00d75da7693571e988996c9f4c.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;gweb-research2023-media&#x2F;pubto...</a><p>&gt;  nice datacenters may even have multiple 100 Gbit&#x2F;sec NICs per machine in 2024,<p>Google exceeded 100Gbps per machine long before 2024. IIRC it had been 400Gbps for a while.</div><br/><div id="39445578" class="c"><input type="checkbox" id="c-39445578" checked=""/><div class="controls bullet"><span class="by">scottlamb</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445433">parent</a><span>|</span><a href="#39445672">next</a><span>|</span><label class="collapse" for="c-39445578">[-]</label><label class="expand" for="c-39445578">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. I worked at Google until January 2021. I see 2019 dates on that PDF, but I wasn&#x27;t aware of snap when I left. There was some alternate RPC approach (Pony Express, maybe? I get the names mixed up) that claimed 10 µs or so but was advertised as experimental (iirc had some bad failure modes at the time in practice) and was simply unavailable in many of the datacenters I needed to deploy in. Maybe they&#x27;re two names for the same thing. [edit: oh, yes, starting to actually read the paper now, and: &quot;Through Snap, we created a new communication stack called Pony Express that implements a custom reliable transport and communications API.&quot;]<p>Actual latency with standard Stubby-over-TCP and warmed channels...it&#x27;s been a while, so I don&#x27;t remember the number I observed, but I remember it wasn&#x27;t <i>that</i> much better than 0.5 ms. It was still bad enough that I didn&#x27;t want to add a tier that would have helped with isolation in a particularly high-reliability system.</div><br/><div id="39446740" class="c"><input type="checkbox" id="c-39446740" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445578">parent</a><span>|</span><a href="#39445672">next</a><span>|</span><label class="collapse" for="c-39446740">[-]</label><label class="expand" for="c-39446740">[2 more]</label></div><br/><div class="children"><div class="content">Snap was the external name for the internal project known as User Space Packet Service (abbreviated USPS) so naturally they renamed it prior to publication. I deployed an app using Pony Express in 2023 and it was available in the majority of cells worldwide. Pony Express supported more than just RPC though. The alternate RPC approach that you spoke of was called Void. It had been experimental for a long time and indeed it wasn&#x27;t well known even inside Google.<p>&gt; but I remember it wasn&#x27;t that much better than 0.5 ms.<p>If you and I still worked at Google I&#x27;d just give you an automon dashboard link showing latency an order of magnitude better than that to prove myself…</div><br/><div id="39447211" class="c"><input type="checkbox" id="c-39447211" checked=""/><div class="controls bullet"><span class="by">scottlamb</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446740">parent</a><span>|</span><a href="#39445672">next</a><span>|</span><label class="collapse" for="c-39447211">[-]</label><label class="expand" for="c-39447211">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, thanks!<p>&gt; If you and I still worked at Google I&#x27;d just give you an automon dashboard link showing latency an order of magnitude better than that to prove myself…<p>I believe you, and I think in principle we should all be getting the 50 µs latency you&#x27;re describing within a datacenter with no special effort.<p>...but it doesn&#x27;t match what I observed, and I&#x27;m not sure why. Maybe difference of a couple years. Maybe I was checking somewhere with older equipment, or some important config difference in our tests. And obviously my memory&#x27;s a bit fuzzy by now but I know I didn&#x27;t like the result I got.</div><br/></div></div></div></div></div></div><div id="39445672" class="c"><input type="checkbox" id="c-39445672" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445433">parent</a><span>|</span><a href="#39445578">prev</a><span>|</span><a href="#39444024">next</a><span>|</span><label class="collapse" for="c-39445672">[-]</label><label class="expand" for="c-39445672">[1 more]</label></div><br/><div class="children"><div class="content">with such speed and CXL gaining traction (think ram and GPUs over network) why network SSD is still issue?
you could have like one storage server per rack that would serve storage only for that particular rack<p>you could easily have like 40GB&#x2F;s with some over provisioning &#x2F; bucketing</div><br/></div></div></div></div></div></div><div id="39444024" class="c"><input type="checkbox" id="c-39444024" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444952">prev</a><span>|</span><a href="#39444099">next</a><span>|</span><label class="collapse" for="c-39444024">[-]</label><label class="expand" for="c-39444024">[24 more]</label></div><br/><div class="children"><div class="content">Why do they fundamentally need to be network attached storage instead of local to the VM?</div><br/><div id="39444197" class="c"><input type="checkbox" id="c-39444197" checked=""/><div class="controls bullet"><span class="by">drewda</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444024">parent</a><span>|</span><a href="#39444042">next</a><span>|</span><label class="collapse" for="c-39444197">[-]</label><label class="expand" for="c-39444197">[1 more]</label></div><br/><div class="children"><div class="content">The major clouds do offer VMs with fast local storage, such as SSDs connected by NVMe connections directly to the VM host machine:<p>- <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;disks&#x2F;local-ssd</a><p>- <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;virtual-machines&#x2F;enable-nvme-interface" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;virtual-machines&#x2F;ena...</a><p>- <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ssd-instance-store.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AWSEC2&#x2F;latest&#x2F;UserGuide&#x2F;ssd-inst...</a><p>They sell these VMs at a higher cost because it requires more expensive components and is limited to host machines with certain configurations. In our experience, it&#x27;s also harder to request quota increases to get more of these VMs -- some of the public clouds have a limited supply of these specific types of configurations in some regions&#x2F;zones.<p>As others have noted, instance storage isn&#x27;t as dependable. But it can be the most performant way to do IO-intense processing or to power one node of a distributed database.</div><br/></div></div><div id="39444042" class="c"><input type="checkbox" id="c-39444042" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444024">parent</a><span>|</span><a href="#39444197">prev</a><span>|</span><a href="#39444132">next</a><span>|</span><label class="collapse" for="c-39444042">[-]</label><label class="expand" for="c-39444042">[11 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t. Some cloud providers (i.e. Hetzner) let you rent VMs with locally attached NVMe, which is dramatically faster than network-attached even factoring in the VM tax.<p>Of course then you have a single point of failure, in the PCIe fabric of the machine you&#x27;re running on if not the NVMe itself. But if you have good backups, which you should, then the juice really isn&#x27;t worth the squeeze for NAS storage.</div><br/><div id="39444103" class="c"><input type="checkbox" id="c-39444103" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444042">parent</a><span>|</span><a href="#39444132">next</a><span>|</span><label class="collapse" for="c-39444103">[-]</label><label class="expand" for="c-39444103">[10 more]</label></div><br/><div class="children"><div class="content">A network adds more points of failure.  It does not reduce them.</div><br/><div id="39444177" class="c"><input type="checkbox" id="c-39444177" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444103">parent</a><span>|</span><a href="#39444445">next</a><span>|</span><label class="collapse" for="c-39444177">[-]</label><label class="expand" for="c-39444177">[7 more]</label></div><br/><div class="children"><div class="content">A network attached, replicated storage hedges against data loss but increases latency; however most customers usually prefer higher latency to data loss. As an example, see the highly upvoted fly.io thread[1] with customers complaining about the same thing.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36808296">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36808296</a></div><br/><div id="39444649" class="c"><input type="checkbox" id="c-39444649" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444177">parent</a><span>|</span><a href="#39444445">next</a><span>|</span><label class="collapse" for="c-39444649">[-]</label><label class="expand" for="c-39444649">[6 more]</label></div><br/><div class="children"><div class="content">Locally-attached, replicated storage also hedges against data loss.</div><br/><div id="39444814" class="c"><input type="checkbox" id="c-39444814" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444649">parent</a><span>|</span><a href="#39444445">next</a><span>|</span><label class="collapse" for="c-39444814">[-]</label><label class="expand" for="c-39444814">[5 more]</label></div><br/><div class="children"><div class="content">RAID rebuild times make it an unviable option and customers typically expect problematic VMs to be live-migrated to other hosts with the disks still having their intended data.<p>The self hosted version of this is GlusterFS and Ceph, which have the same dynamics as EBS and its equivalents in other cloud providers.</div><br/><div id="39445429" class="c"><input type="checkbox" id="c-39445429" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444814">parent</a><span>|</span><a href="#39444445">next</a><span>|</span><label class="collapse" for="c-39445429">[-]</label><label class="expand" for="c-39445429">[4 more]</label></div><br/><div class="children"><div class="content">With NVMe SSDs? What makes RAID unviable in that environment?</div><br/><div id="39446043" class="c"><input type="checkbox" id="c-39446043" checked=""/><div class="controls bullet"><span class="by">dijit</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445429">parent</a><span>|</span><a href="#39444445">next</a><span>|</span><label class="collapse" for="c-39446043">[-]</label><label class="expand" for="c-39446043">[3 more]</label></div><br/><div class="children"><div class="content">This depends, like all things.<p>When you say RAID, what level? Software-raid or hardware raid? What controller?<p>Let&#x27;s take best-case:<p>RAID10, small enough (but many) NVMe drives <i>and</i> an LVM&#x2F;Software RAID like ZFS, which is data aware so only rebuilds actual data: rebuilds will degrade performance enough potentially that your application can become unavailable if your IOPS are 70%+ of maximum.<p>That&#x27;s an ideal scenario, if you use hardware raid which is not data-aware then your rebuild times depend entirely on the size of the drive being rebuilt <i>and</i> it can punish IOPs even more during the rebuild. But it will affect your CPU less.<p>There&#x27;s no panacea. Most people opt for higher latency distributed storage where the RAID is spread across an <i>enormous</i> amount of drives, which makes rebuilds much less painful.</div><br/><div id="39450519" class="c"><input type="checkbox" id="c-39450519" checked=""/><div class="controls bullet"><span class="by">timc3</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446043">parent</a><span>|</span><a href="#39444445">next</a><span>|</span><label class="collapse" for="c-39450519">[-]</label><label class="expand" for="c-39450519">[2 more]</label></div><br/><div class="children"><div class="content">What I used to do was swap machines over from the one with failing disks to a live spare (slave in the old frowned upon terminology), do the maintenance  and then replicate from the now live spare back if I had confidence it was all good.<p>Yes it’s costly having the hardware to do that as it mostly meant multiple machines  as I always wanted to be able to rebuild one whilst having at least two machines online.</div><br/><div id="39450939" class="c"><input type="checkbox" id="c-39450939" checked=""/><div class="controls bullet"><span class="by">dijit</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39450519">parent</a><span>|</span><a href="#39444445">next</a><span>|</span><label class="collapse" for="c-39450939">[-]</label><label class="expand" for="c-39450939">[1 more]</label></div><br/><div class="children"><div class="content">If you are doing this with your own hardware it is still less costly than cloud even if it mostly sits idle.<p>Cloud is approx 5x sticker cost for compute if its sustained.<p>Your discounts may vary, rue the day those discounts are taken away because we are all sufficiently locked in.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39444445" class="c"><input type="checkbox" id="c-39444445" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444103">parent</a><span>|</span><a href="#39444177">prev</a><span>|</span><a href="#39444132">next</a><span>|</span><label class="collapse" for="c-39444445">[-]</label><label class="expand" for="c-39444445">[2 more]</label></div><br/><div class="children"><div class="content">A network adds more <i>points</i> of failures but also <i>reduces user-facing failures</i> overall when properly architected.<p>If one CPU attached to storage dies, another can take over and reattach -- or vice-versa. If one network link dies, it can be rerouted around.</div><br/><div id="39444568" class="c"><input type="checkbox" id="c-39444568" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444445">parent</a><span>|</span><a href="#39444132">next</a><span>|</span><label class="collapse" for="c-39444568">[-]</label><label class="expand" for="c-39444568">[1 more]</label></div><br/><div class="children"><div class="content">Using a SAN (which is what networked storage is, after all) also lets you get various &quot;tricks&quot; such as snapshots, instant migration, etc for &quot;free&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="39444132" class="c"><input type="checkbox" id="c-39444132" checked=""/><div class="controls bullet"><span class="by">SteveNuts</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444024">parent</a><span>|</span><a href="#39444042">prev</a><span>|</span><a href="#39444065">next</a><span>|</span><label class="collapse" for="c-39444132">[-]</label><label class="expand" for="c-39444132">[6 more]</label></div><br/><div class="children"><div class="content">Because even if you can squeeze 100TB or more of SSD&#x2F;NVMe in a server, and there are 10 tenants using the machine, you&#x27;re limited to 10TB as a hard ceiling.<p>What happens when one tenant needs 200TB attached to a server?<p>Cloud providers are starting to offer local SSD&#x2F;NVMe, but you&#x27;re renting the entire machine, and you&#x27;re still limited to exactly what&#x27;s installed in that server.</div><br/><div id="39444774" class="c"><input type="checkbox" id="c-39444774" checked=""/><div class="controls bullet"><span class="by">vel0city</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444132">parent</a><span>|</span><a href="#39444256">next</a><span>|</span><label class="collapse" for="c-39444774">[-]</label><label class="expand" for="c-39444774">[1 more]</label></div><br/><div class="children"><div class="content">Given AWS and GCP offer multiple sizes for the same processor version with local SSDs, I don&#x27;t think you have to rent the entire machine.<p>Search for i3en API names and you&#x27;ll see:<p>i3en.large, 2x CPU, 1250GB SSD<p>i3en.xlarge, 4x CPU, 2500GB SSD<p>i3en.2xlarge, 8x CPU, 2x2500GB SSD<p>i3en.3xlarge, 12x CPU, 7500GB SSD<p>i3en.6xlarge, 24x CPU, 2x7500GB SSD<p>i3en.12xlarge, 48x CPU, 4x7500GB SSD<p>i3en.24xlarge, 96x CPU, 8x7500GB SSD<p>i3en.metal, 96x CPU, 8x7500GB SSD<p>So they&#x27;ve got servers with 96 CPUs and 8x7500GB SSDs. You can get a slice of one, or you can get the whole one. All of these are the ratio of 625GB of local SSD per CPU core.<p><a href="https:&#x2F;&#x2F;instances.vantage.sh&#x2F;" rel="nofollow">https:&#x2F;&#x2F;instances.vantage.sh&#x2F;</a><p>On GCP you can get a 2-core N2 instance type and attach multiple local SSDs. I doubt they have many physical 2-core Xeons in their datacenters.</div><br/></div></div><div id="39444256" class="c"><input type="checkbox" id="c-39444256" checked=""/><div class="controls bullet"><span class="by">jalk</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444132">parent</a><span>|</span><a href="#39444774">prev</a><span>|</span><a href="#39446160">next</a><span>|</span><label class="collapse" for="c-39444256">[-]</label><label class="expand" for="c-39444256">[3 more]</label></div><br/><div class="children"><div class="content">How is that different from how cores, mem and network bandwidth is allotted to tenants?</div><br/><div id="39444566" class="c"><input type="checkbox" id="c-39444566" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444256">parent</a><span>|</span><a href="#39444337">next</a><span>|</span><label class="collapse" for="c-39444566">[-]</label><label class="expand" for="c-39444566">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t. You could ask for network-attached CPUs or RAM. You&#x27;d be the only one, though, so in practice only network-attached storage makes sense business-wise. It also makes sense if you need to provision larger-than-usual amounts like tens of TB - these are usually hard to come by in a single server, but quite mundane for storage appliances.</div><br/></div></div><div id="39444337" class="c"><input type="checkbox" id="c-39444337" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444256">parent</a><span>|</span><a href="#39444566">prev</a><span>|</span><a href="#39446160">next</a><span>|</span><label class="collapse" for="c-39444337">[-]</label><label class="expand" for="c-39444337">[1 more]</label></div><br/><div class="children"><div class="content">Because a fair number of customers spin up another image when cores&#x2F;mem&#x2F;bandwidth run low. Dedicated storage breaks that paradigm.<p>Also, adding, if I am on an 8 core machine and need 16, network storage can be detached from host A and connected to host B. In dedicated storage it must be fully copied over first.</div><br/></div></div></div></div><div id="39446160" class="c"><input type="checkbox" id="c-39446160" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444132">parent</a><span>|</span><a href="#39444256">prev</a><span>|</span><a href="#39444065">next</a><span>|</span><label class="collapse" for="c-39446160">[-]</label><label class="expand" for="c-39446160">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What happens when one tenant needs 200TB attached to a server?<p>Link to this mythical hosting service that expects far less than 200TB of data per client but just pulls a sad face and takes the extra cost on board when a client demands it. :D</div><br/></div></div></div></div><div id="39444065" class="c"><input type="checkbox" id="c-39444065" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444024">parent</a><span>|</span><a href="#39444132">prev</a><span>|</span><a href="#39444055">next</a><span>|</span><label class="collapse" for="c-39444065">[-]</label><label class="expand" for="c-39444065">[2 more]</label></div><br/><div class="children"><div class="content">Reliability. SSDs break and screw up a lot more frequently and more quickly than CPUs. Amazon has published a lot on the architecture of EBS, and they go through a good analysis of this. If you have a broken disk and you locally attach, you have a broken machine.<p>RAID helps you locally, but fundamentally relies on locality and low latency (and maybe custom hardware) to minimize the time window where you get true data corruption on a bad disk. That is insufficient for cloud storage.</div><br/><div id="39450096" class="c"><input type="checkbox" id="c-39450096" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444065">parent</a><span>|</span><a href="#39444055">next</a><span>|</span><label class="collapse" for="c-39450096">[-]</label><label class="expand" for="c-39450096">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but there&#x27;s plenty of software that&#x27;s written to use distributed unreliable storage similar to how cloud providers write their own software (e.g. Kafka). I can understand if many applications are just need something like EBS that&#x27;s durable but looks like a normal disk, but not so sure it&#x27;s a fundamentally required abstraction.</div><br/></div></div></div></div><div id="39444055" class="c"><input type="checkbox" id="c-39444055" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444024">parent</a><span>|</span><a href="#39444065">prev</a><span>|</span><a href="#39444099">next</a><span>|</span><label class="collapse" for="c-39444055">[-]</label><label class="expand" for="c-39444055">[3 more]</label></div><br/><div class="children"><div class="content">Redundancy, local storage is a single point of failure.<p>You can use local SSD’s as slow RAM, but anything on it can go away at any moment.</div><br/><div id="39444944" class="c"><input type="checkbox" id="c-39444944" checked=""/><div class="controls bullet"><span class="by">cduzz</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444055">parent</a><span>|</span><a href="#39444099">next</a><span>|</span><label class="collapse" for="c-39444944">[-]</label><label class="expand" for="c-39444944">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen SANs get nuked by operator error or by environmental issues (overheated DC == SAN shuts itself down).<p>Distributed clusters of things can work just fine on ephemeral local storage (aka <i>local storage</i>).  A kafka cluster or an opensearch cluster will be fine using instance local storage, for instance.<p>As with everything else.... &quot;it depends&quot;</div><br/><div id="39445679" class="c"><input type="checkbox" id="c-39445679" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444944">parent</a><span>|</span><a href="#39444099">next</a><span>|</span><label class="collapse" for="c-39445679">[-]</label><label class="expand" for="c-39445679">[1 more]</label></div><br/><div class="children"><div class="content">Sure distributed clusters get back to network&#x2F;workload limitations.</div><br/></div></div></div></div></div></div></div></div><div id="39444099" class="c"><input type="checkbox" id="c-39444099" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444024">prev</a><span>|</span><a href="#39444062">next</a><span>|</span><label class="collapse" for="c-39444099">[-]</label><label class="expand" for="c-39444099">[1 more]</label></div><br/><div class="children"><div class="content">No, the i3&#x2F;i4 VMs discussed in the blog have local SSDs. The network isn&#x27;t the reason local SSDs are slow.</div><br/></div></div><div id="39444062" class="c"><input type="checkbox" id="c-39444062" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444099">prev</a><span>|</span><a href="#39449994">next</a><span>|</span><label class="collapse" for="c-39444062">[-]</label><label class="expand" for="c-39444062">[2 more]</label></div><br/><div class="children"><div class="content">&gt; SSDs in the cloud are attached over a network, and fundamentally have to be<p>Not on AWS. Instance stores (what the article is about) are physical local disks.</div><br/><div id="39450230" class="c"><input type="checkbox" id="c-39450230" checked=""/><div class="controls bullet"><span class="by">jsolson</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444062">parent</a><span>|</span><a href="#39449994">next</a><span>|</span><label class="collapse" for="c-39450230">[-]</label><label class="expand" for="c-39450230">[1 more]</label></div><br/><div class="children"><div class="content">Same for Google (the GP is incorrect about how GCE Local SSD works).</div><br/></div></div></div></div><div id="39449994" class="c"><input type="checkbox" id="c-39449994" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444062">prev</a><span>|</span><a href="#39444085">next</a><span>|</span><label class="collapse" for="c-39449994">[-]</label><label class="expand" for="c-39449994">[1 more]</label></div><br/><div class="children"><div class="content">This is an interesting benchmark that compares disk read latency across different clouds across various disk configurations, including SSD&#x27;s and EBS: <a href="https:&#x2F;&#x2F;github.com&#x2F;scylladb&#x2F;diskplorer">https:&#x2F;&#x2F;github.com&#x2F;scylladb&#x2F;diskplorer</a><p>Google&#x27;s disk perform quite poorly.<p>And how Discord worked around it: <a href="https:&#x2F;&#x2F;discord.com&#x2F;blog&#x2F;how-discord-supercharges-network-disks-for-extreme-low-latency" rel="nofollow">https:&#x2F;&#x2F;discord.com&#x2F;blog&#x2F;how-discord-supercharges-network-di...</a></div><br/></div></div><div id="39444085" class="c"><input type="checkbox" id="c-39444085" checked=""/><div class="controls bullet"><span class="by">mkoubaa</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39449994">prev</a><span>|</span><a href="#39444655">next</a><span>|</span><label class="collapse" for="c-39444085">[-]</label><label class="expand" for="c-39444085">[7 more]</label></div><br/><div class="children"><div class="content">Dumb question. Why does the network have to be slow? If the SSDs are two feet away from the motherboard and there&#x27;s an optical connection to it, shouldn&#x27;t it be fast? Are data centers putting SSDs super far away from motherboards?</div><br/><div id="39444510" class="c"><input type="checkbox" id="c-39444510" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444085">parent</a><span>|</span><a href="#39444146">next</a><span>|</span><label class="collapse" for="c-39444510">[-]</label><label class="expand" for="c-39444510">[1 more]</label></div><br/><div class="children"><div class="content">It’s not the network being slow, but dividing the available network bandwidth amongst all users, while also distributing the written data to multiple nodes reliably so that one tenant doesn’t hog resources is quite challenging. The pricing structure is meant to control resource usage; a discussion of the exact prices and how much profit AWS or any other cloud provider makes is a separate discussion.</div><br/></div></div><div id="39444146" class="c"><input type="checkbox" id="c-39444146" checked=""/><div class="controls bullet"><span class="by">formercoder</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444085">parent</a><span>|</span><a href="#39444510">prev</a><span>|</span><a href="#39444142">next</a><span>|</span><label class="collapse" for="c-39444146">[-]</label><label class="expand" for="c-39444146">[1 more]</label></div><br/><div class="children"><div class="content">What happens when your vm is live migrated 1000 feet away or to a different zone?</div><br/></div></div><div id="39444142" class="c"><input type="checkbox" id="c-39444142" checked=""/><div class="controls bullet"><span class="by">bugbuddy</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444085">parent</a><span>|</span><a href="#39444146">prev</a><span>|</span><a href="#39444655">next</a><span>|</span><label class="collapse" for="c-39444142">[-]</label><label class="expand" for="c-39444142">[4 more]</label></div><br/><div class="children"><div class="content">&gt; One theory is that EC2 intentionally caps the write speed at 1 GB&#x2F;s to avoid frequent device failure, given the total number of writes per SSD is limited.<p>This is the theory that I would bet on because it lines up with their bottom line.</div><br/><div id="39444409" class="c"><input type="checkbox" id="c-39444409" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444142">parent</a><span>|</span><a href="#39444655">next</a><span>|</span><label class="collapse" for="c-39444409">[-]</label><label class="expand" for="c-39444409">[3 more]</label></div><br/><div class="children"><div class="content">But the sentence right after undermines it.<p>&gt; However, this does not explain why the read bandwidth is stuck at 2 GB&#x2F;s.<p>Faster read speeds would give them a more enticing product without wearing drives out.</div><br/><div id="39445381" class="c"><input type="checkbox" id="c-39445381" checked=""/><div class="controls bullet"><span class="by">bugbuddy</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444409">parent</a><span>|</span><a href="#39444655">next</a><span>|</span><label class="collapse" for="c-39445381">[-]</label><label class="expand" for="c-39445381">[2 more]</label></div><br/><div class="children"><div class="content">They may be limiting the read artificially to increase your resource utilization else where. If you have disk bottleneck then you would be more likely to use more instances. It is still about the bottom line.</div><br/><div id="39445990" class="c"><input type="checkbox" id="c-39445990" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39445381">parent</a><span>|</span><a href="#39444655">next</a><span>|</span><label class="collapse" for="c-39445990">[-]</label><label class="expand" for="c-39445990">[1 more]</label></div><br/><div class="children"><div class="content">That could be.  But it&#x27;s a completely different reason.  If you summarize everything as &quot;bottom line&quot;, you lose all the valuable information.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39444655" class="c"><input type="checkbox" id="c-39444655" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444085">prev</a><span>|</span><a href="#39444429">next</a><span>|</span><label class="collapse" for="c-39444655">[-]</label><label class="expand" for="c-39444655">[2 more]</label></div><br/><div class="children"><div class="content">&gt;The problem is that this network is so large and slow that it can&#x27;t give you anywhere near the performance of a local SSD.<p>*As implemented in the public cloud providers.<p>You can absolutely get better than local disk speeds from SAN devices and we&#x27;ve been doing it for decades.  To do it on-prem with flash devices will require NVMe over FC or Ethernet and an appropriate storage array.  Modern all-flash array performance is measured in millions of IOPS.<p>Will there be a slight uptick in latency?  Sure, but it&#x27;s well worth it for the data services and capacity of an external array for nearly every workload.</div><br/><div id="39448671" class="c"><input type="checkbox" id="c-39448671" checked=""/><div class="controls bullet"><span class="by">folmar</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444655">parent</a><span>|</span><a href="#39444429">next</a><span>|</span><label class="collapse" for="c-39448671">[-]</label><label class="expand" for="c-39448671">[1 more]</label></div><br/><div class="children"><div class="content">A quick comparison of marketing slides lands you with 0.5 MOPS for FC-NVMe for Qlogic 2770 and 0.7 MOPS for PCIe Micron 9100 PRO, so the better speeds are not quite there, although spending quite a lot on server gear lands you near the performance of a workstation-grade drive from 2017.<p>Which is still not bad, when I was shopping around in 2018 no money could buy performance comparable to a locally-attached NVMe in a more professional&#x2F;datacenter-ready form.<p>[1] <a href="https:&#x2F;&#x2F;media-www.micron.com&#x2F;-&#x2F;media&#x2F;client&#x2F;global&#x2F;documents&#x2F;products&#x2F;technical-marketing-brief&#x2F;brief_business_case_pcie_ssd.pdf?la=en&amp;rev=ce226f742a3741209f98d5f0bb152f2c" rel="nofollow">https:&#x2F;&#x2F;media-www.micron.com&#x2F;-&#x2F;media&#x2F;client&#x2F;global&#x2F;documents...</a><p>[2] <a href="https:&#x2F;&#x2F;www.marvell.com&#x2F;content&#x2F;dam&#x2F;marvell&#x2F;en&#x2F;public-collateral&#x2F;fibre-channel&#x2F;marvell-fibre-channel-nvme-over-fabrics-white-paper.pdf" rel="nofollow">https:&#x2F;&#x2F;www.marvell.com&#x2F;content&#x2F;dam&#x2F;marvell&#x2F;en&#x2F;public-collat...</a></div><br/></div></div></div></div><div id="39444429" class="c"><input type="checkbox" id="c-39444429" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444655">prev</a><span>|</span><a href="#39447467">next</a><span>|</span><label class="collapse" for="c-39444429">[-]</label><label class="expand" for="c-39444429">[3 more]</label></div><br/><div class="children"><div class="content">Makes me wonder if we&#x27;re on the crux of a shift back to client-based software.  Historically changes in the relative cost of computing components have driven most of the shifts in the computing industry.  Cheap teletypes &amp; peripherals fueled the shift from batch-processing mainframes to timesharing minicomputers.  Cheap CPUs &amp; RAM fueled the shift from minicomputers to microcomputers.  Cheap and fast networking fueled the shift from desktop software to the cloud.  Will cheap SSDs &amp; TPU&#x2F;GPUs fuel a shift back toward thicker clients?<p>There are a bunch of supporting social trends toward this as well.  Renewed emphasis on privacy.  Big Tech canceling beloved products, bricking devices, and generally enshittifying everything - a lot of people want locally-controlled software that isn&#x27;t going to get worse at the next update.  Ever-rising prices which make people want to lock in a price for the device and not deal with increasing rents for computing power.</div><br/><div id="39446613" class="c"><input type="checkbox" id="c-39446613" checked=""/><div class="controls bullet"><span class="by">davkan</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444429">parent</a><span>|</span><a href="#39447467">next</a><span>|</span><label class="collapse" for="c-39446613">[-]</label><label class="expand" for="c-39446613">[2 more]</label></div><br/><div class="children"><div class="content">I think a major limiting factor here for many applications is that mobile users are a huge portion of the user base. In that space storage, and more importantly battery life, are still at a premium. Granted the storage cost just seems to be gouging from my layman’s point of view, so industry needs might force a shift upwards.</div><br/><div id="39448324" class="c"><input type="checkbox" id="c-39448324" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39446613">parent</a><span>|</span><a href="#39447467">next</a><span>|</span><label class="collapse" for="c-39448324">[-]</label><label class="expand" for="c-39448324">[1 more]</label></div><br/><div class="children"><div class="content">Mobile devices are the desktop computers of the 2010s though. They are mostly used with very thick clients.</div><br/></div></div></div></div></div></div><div id="39447467" class="c"><input type="checkbox" id="c-39447467" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444429">prev</a><span>|</span><a href="#39444046">next</a><span>|</span><label class="collapse" for="c-39447467">[-]</label><label class="expand" for="c-39447467">[1 more]</label></div><br/><div class="children"><div class="content">They don’t have to be.  Architecturally there are many benefits to storage area networks but I have built plenty of systems which are self-contained, download a dataset to a cloud instance with a direct attached SSD, load it into a database and provide a different way.</div><br/></div></div><div id="39444046" class="c"><input type="checkbox" id="c-39444046" checked=""/><div class="controls bullet"><span class="by">ejb999</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39447467">prev</a><span>|</span><a href="#39446248">next</a><span>|</span><label class="collapse" for="c-39444046">[-]</label><label class="expand" for="c-39444046">[11 more]</label></div><br/><div class="children"><div class="content">How much faster would the network need to get, in order to meet (or at least approach) the speed of a local SSD? are we talking about needing to 2x or 3x the speed, or by factors of hundreds or thousands?</div><br/><div id="39444218" class="c"><input type="checkbox" id="c-39444218" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444046">parent</a><span>|</span><a href="#39444119">next</a><span>|</span><label class="collapse" for="c-39444218">[-]</label><label class="expand" for="c-39444218">[5 more]</label></div><br/><div class="children"><div class="content">The problem isn&#x27;t necessarily speed, it&#x27;s random access latency. What makes SSDs fast and &quot;magical&quot; is their low random-access latency compared to a spinning disk. The sequential-access read speed is merely a bonus.<p>Networked storage negates that significantly, absolutely killing performance for certain applications. You could have a 100Gbps network and it still won&#x27;t match a direct-attached SSD in terms of latency (it can only match it in terms of sequential access throughput).<p>For many applications such as databases, random access is crucial, thus why nowadays&#x27; mid-range consumer hardware often outperforms hosted databases such as RDS unless they&#x27;re so overprovisioned on RAM that the dataset is effectively always in there.</div><br/><div id="39444606" class="c"><input type="checkbox" id="c-39444606" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444218">parent</a><span>|</span><a href="#39447996">next</a><span>|</span><label class="collapse" for="c-39444606">[-]</label><label class="expand" for="c-39444606">[1 more]</label></div><br/><div class="children"><div class="content">100Gbps direct <i>shouldn&#x27;t be</i> too bad, but it might be difficult to get anyone to sell it to you for exclusive usage in a vm...</div><br/></div></div><div id="39447996" class="c"><input type="checkbox" id="c-39447996" checked=""/><div class="controls bullet"><span class="by">Ericson2314</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444218">parent</a><span>|</span><a href="#39444606">prev</a><span>|</span><a href="#39444119">next</a><span>|</span><label class="collapse" for="c-39447996">[-]</label><label class="expand" for="c-39447996">[3 more]</label></div><br/><div class="children"><div class="content">Um... why the hell does the network care whether I am doing random or sequential access? Your left that part out of your argument.</div><br/><div id="39448284" class="c"><input type="checkbox" id="c-39448284" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39447996">parent</a><span>|</span><a href="#39444119">next</a><span>|</span><label class="collapse" for="c-39448284">[-]</label><label class="expand" for="c-39448284">[2 more]</label></div><br/><div class="children"><div class="content">Ah sorry, my bad. You are correct that you can fire off many random access operations in parallel and get good throughput that way.<p>The problem is that this is not possible when the next IO request depends on the result of a previous one, like in a database where you must first read the index to know the location of the row data itself.</div><br/><div id="39448578" class="c"><input type="checkbox" id="c-39448578" checked=""/><div class="controls bullet"><span class="by">Ericson2314</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39448284">parent</a><span>|</span><a href="#39444119">next</a><span>|</span><label class="collapse" for="c-39448578">[-]</label><label class="expand" for="c-39448578">[1 more]</label></div><br/><div class="children"><div class="content">OK thanks yes that makes sense. Pipelining problems are real.</div><br/></div></div></div></div></div></div></div></div><div id="39444119" class="c"><input type="checkbox" id="c-39444119" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444046">parent</a><span>|</span><a href="#39444218">prev</a><span>|</span><a href="#39444137">next</a><span>|</span><label class="collapse" for="c-39444119">[-]</label><label class="expand" for="c-39444119">[2 more]</label></div><br/><div class="children"><div class="content">The Samsung 990 in my desktop provides ~3.5 GB&#x2F;s streaming reads, ~2 GB&#x2F;s 4k random-access reads, all at a latency measured at around 20-30 microseconds. My exact numbers might be a little off, but that&#x27;s the ballpark you&#x27;re looking at, and a 990 is a relatively cheap device.<p>10GbE is about the best you can hope for from a local network these days, but that&#x27;s 1&#x2F;5th the bandwidth and many times the latency. 100GbE would work, except the latency would still mean any read dependencies would be far slower than local storage, and I&#x27;m not sure there&#x27;s much to be done about that; at these speeds the physical distance matters.<p>In practice I&#x27;m having to architecture the entire system around the SSD just to not bottleneck it. So far ext4 is the only filesystem that even gets close to the SSD&#x27;s limits, which is a bit of a pity.</div><br/><div id="39451386" class="c"><input type="checkbox" id="c-39451386" checked=""/><div class="controls bullet"><span class="by">ants_a</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444119">parent</a><span>|</span><a href="#39444137">next</a><span>|</span><label class="collapse" for="c-39451386">[-]</label><label class="expand" for="c-39451386">[1 more]</label></div><br/><div class="children"><div class="content">Networking doesn&#x27;t have to have high latency. You can buy network hardware that is able to provide sub-microsecond latency. Physical distance still matters, but 10% of typical NVMe latency gets you through a kilometer of fiber.</div><br/></div></div></div></div><div id="39444137" class="c"><input type="checkbox" id="c-39444137" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444046">parent</a><span>|</span><a href="#39444119">prev</a><span>|</span><a href="#39444115">next</a><span>|</span><label class="collapse" for="c-39444137">[-]</label><label class="expand" for="c-39444137">[1 more]</label></div><br/><div class="children"><div class="content">Around 4x-10x depending on how many SSDs you want. A single SSD is around the speed of a 100 Gbps Ethernet link.</div><br/></div></div><div id="39444150" class="c"><input type="checkbox" id="c-39444150" checked=""/><div class="controls bullet"><span class="by">selectodude</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444046">parent</a><span>|</span><a href="#39444115">prev</a><span>|</span><a href="#39446248">next</a><span>|</span><label class="collapse" for="c-39444150">[-]</label><label class="expand" for="c-39444150">[1 more]</label></div><br/><div class="children"><div class="content">SATA3 is 6 Gbit, so each VM on a machine multiplied by 6 Gbit. For NVMe, probably closer to 4-5x that. You’d need some serious interconnects to get a server rack access to un-bottlenecked SSD storage.</div><br/></div></div></div></div><div id="39446248" class="c"><input type="checkbox" id="c-39446248" checked=""/><div class="controls bullet"><span class="by">dan-robertson</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444046">prev</a><span>|</span><a href="#39444009">next</a><span>|</span><label class="collapse" for="c-39446248">[-]</label><label class="expand" for="c-39446248">[1 more]</label></div><br/><div class="children"><div class="content">I can see network attached SSDs having poor latency, but shouldn’t the networking numbers quoted in the article allow for higher throughput than observed?</div><br/></div></div><div id="39444009" class="c"><input type="checkbox" id="c-39444009" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39446248">prev</a><span>|</span><a href="#39444374">next</a><span>|</span><label class="collapse" for="c-39444009">[-]</label><label class="expand" for="c-39444009">[1 more]</label></div><br/><div class="children"><div class="content">Yeah this was my impression.<p>I am but an end user, but I noticed that disk IO for a certain app was glacial compared to a local test deployment, and I chalked it up to networking&#x2F;VM overhead</div><br/></div></div><div id="39444374" class="c"><input type="checkbox" id="c-39444374" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444009">prev</a><span>|</span><a href="#39444120">next</a><span>|</span><label class="collapse" for="c-39444374">[-]</label><label class="expand" for="c-39444374">[1 more]</label></div><br/><div class="children"><div class="content">Even assuming that &quot;local&quot; storage is a lie, hasn&#x27;t the network gotten a lot faster?  The author is only asking for a 5x increase at the end of the post.</div><br/></div></div><div id="39444120" class="c"><input type="checkbox" id="c-39444120" checked=""/><div class="controls bullet"><span class="by">fngjdflmdflg</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444374">prev</a><span>|</span><a href="#39444138">next</a><span>|</span><label class="collapse" for="c-39444120">[-]</label><label class="expand" for="c-39444120">[1 more]</label></div><br/><div class="children"><div class="content">So do cloud vendors simply not use fast SSDs? If so I would expect the SSD manufacturers themselves to work on this problem. Perhaps they already are.</div><br/></div></div><div id="39444138" class="c"><input type="checkbox" id="c-39444138" checked=""/><div class="controls bullet"><span class="by">adrr</span><span>|</span><a href="#39443994">parent</a><span>|</span><a href="#39444120">prev</a><span>|</span><a href="#39445795">next</a><span>|</span><label class="collapse" for="c-39444138">[-]</label><label class="expand" for="c-39444138">[2 more]</label></div><br/><div class="children"><div class="content">If the local drives are network drives(eg: SAN) then why are they ephemeral?</div><br/><div id="39444632" class="c"><input type="checkbox" id="c-39444632" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39443994">root</a><span>|</span><a href="#39444138">parent</a><span>|</span><a href="#39445795">next</a><span>|</span><label class="collapse" for="c-39444632">[-]</label><label class="expand" for="c-39444632">[1 more]</label></div><br/><div class="children"><div class="content">live vm migrations, perhaps</div><br/></div></div></div></div></div></div><div id="39445795" class="c"><input type="checkbox" id="c-39445795" checked=""/><div class="controls bullet"><span class="by">kwillets</span><span>|</span><a href="#39443994">prev</a><span>|</span><a href="#39444187">next</a><span>|</span><label class="collapse" for="c-39445795">[-]</label><label class="expand" for="c-39445795">[6 more]</label></div><br/><div class="children"><div class="content">AWS docs and blogs describe the Nitro SSD architecture, which is locally attached with custom firmware.<p>&gt; The Nitro Cards are physically connected to the system main board and its processors via PCIe, but are otherwise logically isolated from the system main board that runs customer workloads.<p><a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;whitepapers&#x2F;latest&#x2F;security-design-of-aws-nitro-system&#x2F;the-components-of-the-nitro-system.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;whitepapers&#x2F;latest&#x2F;security-desi...</a><p>&gt; In order to make the [SSD] devices last as long as possible, the firmware is responsible for a process known as wear leveling.... There’s some housekeeping (a form of garbage collection) involved in this process, and garden-variety SSDs can slow down (creating latency spikes) at unpredictable times when dealing with a barrage of writes. We also took advantage of our database expertise and built a very sophisticated, power-fail-safe journal-based database into the SSD firmware.<p><a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;aws-nitro-ssd-high-performance-storage-for-your-i-o-intensive-applications&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;aws-nitro-ssd-high-performa...</a><p>This firmware layer seems like a good candidate for the slowdown.</div><br/><div id="39446592" class="c"><input type="checkbox" id="c-39446592" checked=""/><div class="controls bullet"><span class="by">dan-robertson</span><span>|</span><a href="#39445795">parent</a><span>|</span><a href="#39447438">next</a><span>|</span><label class="collapse" for="c-39446592">[-]</label><label class="expand" for="c-39446592">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, I’m curious how they would respond to the claims in the article. In [1], they talk about aiming for low latency, for consistent performance (apparently other SSDs could stall at inopportune times), and support on-disk encryption. Latency is often in direct conflict with throughput (eg batching usually trades one for the other), and also matters a lot for plenty of filesystem or database tasks (indeed the OP links to a paper showing that popular databases, even column stores, struggle to use the full disk throughput, though I didn’t read why). Encryption is probably not the reason – dedicated hardware on modern chips can do AES at 50GB&#x2F;s, though maybe it is if it increases latency? So maybe there’s something else to it like sharing between many vms on one host<p>[1] <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=Cxie0FgLogg" rel="nofollow">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=Cxie0FgLogg</a></div><br/><div id="39446782" class="c"><input type="checkbox" id="c-39446782" checked=""/><div class="controls bullet"><span class="by">kwillets</span><span>|</span><a href="#39445795">root</a><span>|</span><a href="#39446592">parent</a><span>|</span><a href="#39447438">next</a><span>|</span><label class="collapse" for="c-39446782">[-]</label><label class="expand" for="c-39446782">[2 more]</label></div><br/><div class="children"><div class="content">The Nitro chipset claims 100 GB&#x2F;s encryption, so that doesn&#x27;t seem to be the reason.</div><br/></div></div></div></div><div id="39447438" class="c"><input type="checkbox" id="c-39447438" checked=""/><div class="controls bullet"><span class="by">someguydave</span><span>|</span><a href="#39445795">parent</a><span>|</span><a href="#39446592">prev</a><span>|</span><a href="#39447865">next</a><span>|</span><label class="collapse" for="c-39447438">[-]</label><label class="expand" for="c-39447438">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I wonder how Nitro balances the latency &amp; bandwidth demands of multiple VMs while also minimizing memory cache misses on the CPU (I am assuming it uses DMA to talk to the main CPU cores)</div><br/></div></div><div id="39447865" class="c"><input type="checkbox" id="c-39447865" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#39445795">parent</a><span>|</span><a href="#39447438">prev</a><span>|</span><a href="#39444187">next</a><span>|</span><label class="collapse" for="c-39447865">[-]</label><label class="expand" for="c-39447865">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve noticed the firmware programming positions have been more common in their job listings lately.</div><br/></div></div></div></div><div id="39444187" class="c"><input type="checkbox" id="c-39444187" checked=""/><div class="controls bullet"><span class="by">c0l0</span><span>|</span><a href="#39445795">prev</a><span>|</span><a href="#39444259">next</a><span>|</span><label class="collapse" for="c-39444187">[-]</label><label class="expand" for="c-39444187">[11 more]</label></div><br/><div class="children"><div class="content">Seeing the really just puny &quot;provisioned IOPS&quot; numbers on hugely expensive cloud instances made me chuckle (first in disbelief, then in horror) when I joined a &quot;cloud-first&quot; enterprise shop in 2020 (having come from a company that hosted their own hardware at a colo).<p>It&#x27;s no wonder that many people nowadays, esp. those who are so young that they&#x27;ve never experienced anything but cloud instances, seem to have little idea of how much performance you can actually pack in just one or two RUs today. Ultra-fast (I&#x27;m not parroting some marketing speak here - I just take a look at IOPS numbers, and compare them to those from highest-end storage some 10-12 years ago) NVMe storage is a <i>big</i> part of that astonishing magic.</div><br/><div id="39448208" class="c"><input type="checkbox" id="c-39448208" checked=""/><div class="controls bullet"><span class="by">Aurornis</span><span>|</span><a href="#39444187">parent</a><span>|</span><a href="#39449930">next</a><span>|</span><label class="collapse" for="c-39448208">[-]</label><label class="expand" for="c-39448208">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s no wonder that many people nowadays, esp. those who are so young that they&#x27;ve never experienced anything but cloud instances, seem to have little idea of how much performance you can actually pack in just one or two RUs today.<p>On the contrary, young people often show up having learned on their super fast Apple SSD or a top of the line gaming machine with NVMe SSD.<p>Many know what hardware can do. There’s no need to dunk on young people.<p>Anyway, the cloud performance realities are well know to anyone who works in cloud performance. It’s part of the game and it’s learned by anyone scaling a system. It doesn’t really matter what you could do if you build a couple RUs yourself and hauled them down to the data center, because beyond simple single-purpose applications with flexible uptime requirements, that’s not a realistic option.</div><br/><div id="39449212" class="c"><input type="checkbox" id="c-39449212" checked=""/><div class="controls bullet"><span class="by">EB66</span><span>|</span><a href="#39444187">root</a><span>|</span><a href="#39448208">parent</a><span>|</span><a href="#39448623">next</a><span>|</span><label class="collapse" for="c-39449212">[-]</label><label class="expand" for="c-39449212">[1 more]</label></div><br/><div class="children"><div class="content">&gt; because beyond simple single-purpose applications with flexible uptime requirements, that’s not a realistic option.<p>I frequently hear this point expressed in cloud vs colo debates. The notion that you can&#x27;t achieve high availability with simple colo deploys is just nonsense.<p>Two colo deploys in two geographically distinct datacenters, two active physical servers with identical builds (RAIDed drives, dual NICs, A+B power) in both datacenters, a third server racked up just sitting as a cold spare, pick your favorite container orchestration scheme, rig up your database replication, script the database failover activation process, add HAProxy (or use whatever built-in scheme your orchestration system offers), sprinkle in a cloud service for DNS load balancing&#x2F;failover (Cloudflare or AWS Route 53), automate and store backups off-site and you&#x27;re done.<p>Yes it&#x27;s a lot of work, but so is configuring a similar level of redundancy and high availability in AWS. I&#x27;ve done it both ways and I prefer the bare metal colo approach. With colo you get vastly more bang for your buck and when things go wrong, you have a greater ability to get hands on, understand exactly what&#x27;s going on and fix it immediately.</div><br/></div></div><div id="39448623" class="c"><input type="checkbox" id="c-39448623" checked=""/><div class="controls bullet"><span class="by">zten</span><span>|</span><a href="#39444187">root</a><span>|</span><a href="#39448208">parent</a><span>|</span><a href="#39449212">prev</a><span>|</span><a href="#39449930">next</a><span>|</span><label class="collapse" for="c-39448623">[-]</label><label class="expand" for="c-39448623">[2 more]</label></div><br/><div class="children"><div class="content">&gt; On the contrary, young people often show up having learned on their super fast Apple SSD or a top of the line gaming machine with NVMe SSD.<p>Yes, this is often a big surprise. You can test out some disk-heavy app locally on your laptop and observe decent performance, and then have your day completely ruined when you provision a slice of an NVMe SSD instance type (like, i4i.2xlarge) and discover you&#x27;re only paying for SATA SSD performance.</div><br/><div id="39450654" class="c"><input type="checkbox" id="c-39450654" checked=""/><div class="controls bullet"><span class="by">seabrookmx</span><span>|</span><a href="#39444187">root</a><span>|</span><a href="#39448623">parent</a><span>|</span><a href="#39449930">next</a><span>|</span><label class="collapse" for="c-39450654">[-]</label><label class="expand" for="c-39450654">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t stop at SSD&#x27;s.<p>Spin up an E2 VM in Google Cloud and there&#x27;s a good chance you&#x27;ll get a nearly 9 year Broadwell architecture chip running your workload!</div><br/></div></div></div></div></div></div><div id="39449930" class="c"><input type="checkbox" id="c-39449930" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#39444187">parent</a><span>|</span><a href="#39448208">prev</a><span>|</span><a href="#39448367">next</a><span>|</span><label class="collapse" for="c-39449930">[-]</label><label class="expand" for="c-39449930">[1 more]</label></div><br/><div class="children"><div class="content">Some of us are making a good living offboarding workloads from cloud onto bare metal with on-node NVMe storage.</div><br/></div></div><div id="39448367" class="c"><input type="checkbox" id="c-39448367" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#39444187">parent</a><span>|</span><a href="#39449930">prev</a><span>|</span><a href="#39444259">next</a><span>|</span><label class="collapse" for="c-39448367">[-]</label><label class="expand" for="c-39448367">[5 more]</label></div><br/><div class="children"><div class="content">NVMe has been ridiculously great. I&#x27;m excited to see what happens to prices as E1 form factor ramps up! Much physically bigger drives allows for consolidation of parts, a higher ratio of flash chips to everything else, which seems promising. It&#x27;s more a value line, but Intel&#x27;s P5315 is 15TB at a quite low $0.9&#x2F;GB.<p>It might not help much with oops though. Amazing that we have PCIe 5.0 16GB&#x2F;s and already are so near theoretical max (some lost to overhead), even on consumer cards.<p>Going enterprise for the drive-writes-per-day (DWPD) is 100% worth it for most folks, but I am morbidly curious how different the performance profile would be running enterprise vs non these days. But reciprocally the high DWPD drives (Kioxia CD8P-V for example is DWPD of 3) seems to often come with somewhat more mild sustained 4k write oops, making me think maybe there&#x27;s a speed vs reliability tradeoff that could be taken advantage of from consumer drives in some cases; not sure who wants tons of iops but doesn&#x27;t actually intend to hit their Total Drive Writes, but it save you some iops&#x2F;$ if so. That said, I&#x27;m shocked to see the enterprise premium is a lot less absurd than it used to be! (If you can find stock.)</div><br/><div id="39449159" class="c"><input type="checkbox" id="c-39449159" checked=""/><div class="controls bullet"><span class="by">bcaxis</span><span>|</span><a href="#39444187">root</a><span>|</span><a href="#39448367">parent</a><span>|</span><a href="#39444259">next</a><span>|</span><label class="collapse" for="c-39449159">[-]</label><label class="expand" for="c-39449159">[4 more]</label></div><br/><div class="children"><div class="content">The main problem with consumer drives is the missing power loss protection (plp).  M.2 drives just don&#x27;t have space for the caps like an enterprise 2.5 u.2&#x2F;u.3 drive will have.<p>This matters when the DB calls a sync and it&#x27;s expecting the data to be written safely to disk before it returns.<p>A consumer drive basically stops everything until it can report success and your IOPS falls to like 1&#x2F;100th of what the drive is capable of if it&#x27;s happening alot.<p>An enterprise drive with plp will just report success knowing it has the power to finish the pending writes.  Full speed ahead.<p>You can &quot;lie&quot; to the process at the VPS level by enabling unsafe write back cache.  You can do it at the OS level by launching the DB with &quot;eatmydata&quot;.  You will get the full performance of your SSD.<p>In the event of power loss you may well end up in an unrecoverable corrupted condition with these enabled.<p>I believe that if you buy all consumer parts - an enterprise drive is the best place to up spend your money profitably on an enterprise bit.</div><br/><div id="39449317" class="c"><input type="checkbox" id="c-39449317" checked=""/><div class="controls bullet"><span class="by">tumult</span><span>|</span><a href="#39444187">root</a><span>|</span><a href="#39449159">parent</a><span>|</span><a href="#39444259">next</a><span>|</span><label class="collapse" for="c-39449317">[-]</label><label class="expand" for="c-39449317">[3 more]</label></div><br/><div class="children"><div class="content">My experience lately is that consumer drives will also lie and use a cache, but then drop your data on the floor if the power is lost or there’s a kernel panic &#x2F; BSOD. (Samsung and others.)</div><br/><div id="39449891" class="c"><input type="checkbox" id="c-39449891" checked=""/><div class="controls bullet"><span class="by">bcaxis</span><span>|</span><a href="#39444187">root</a><span>|</span><a href="#39449317">parent</a><span>|</span><a href="#39444259">next</a><span>|</span><label class="collapse" for="c-39449891">[-]</label><label class="expand" for="c-39449891">[2 more]</label></div><br/><div class="children"><div class="content">Rumors of that.  I&#x27;ve never actually seen it myself.</div><br/><div id="39450500" class="c"><input type="checkbox" id="c-39450500" checked=""/><div class="controls bullet"><span class="by">hypercube33</span><span>|</span><a href="#39444187">root</a><span>|</span><a href="#39449891">parent</a><span>|</span><a href="#39444259">next</a><span>|</span><label class="collapse" for="c-39450500">[-]</label><label class="expand" for="c-39450500">[1 more]</label></div><br/><div class="children"><div class="content">Only thing I ever have seen is some cheap Samsung drives slow to a crawl when their buffer fills or those super old Intel ssds that power loss to 8mb due to some firmware bug.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39444259" class="c"><input type="checkbox" id="c-39444259" checked=""/><div class="controls bullet"><span class="by">Twirrim</span><span>|</span><a href="#39444187">prev</a><span>|</span><a href="#39451243">next</a><span>|</span><label class="collapse" for="c-39444259">[-]</label><label class="expand" for="c-39444259">[5 more]</label></div><br/><div class="children"><div class="content">Disclaimer: I work for OCI, opinion my own etc.<p>We offer faster NVMe drives in instances.  Our E4 Dense shapes ship with SAMSUNG MZWLJ7T6HALA-00AU3, which supports Sequential Reads of 7000 MB&#x2F;s, and Sequential Write 3800 MB&#x2F;s.<p>From a general perspective, I would say the _likely_ answer to why AWS doesn&#x27;t have faster NVMes at the moment is likely to be lack of specific demand.  That&#x27;s a guess, but that&#x27;s generally how things go.  If there&#x27;s not enough specific demand being fed in through TAMs and the like for faster disks, upgrades are likely to be more of an after-thought, or reflecting supply chain.<p>I know there&#x27;s a tendency when you engineer things, to just work around, or work with the constraints, and grumble amongst your team, but it&#x27;s incredibly invaluable if you can make sure your account manager knows what shortcomings you&#x27;ve had to work around.</div><br/><div id="39448704" class="c"><input type="checkbox" id="c-39448704" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#39444259">parent</a><span>|</span><a href="#39448674">next</a><span>|</span><label class="collapse" for="c-39448704">[-]</label><label class="expand" for="c-39448704">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I work for OCI<p>Ah: Oracle cloud infra<p><a href="https:&#x2F;&#x2F;blogs.oracle.com&#x2F;cloud-infrastructure&#x2F;post&#x2F;announcing-e4-denseio-instances-with-2x-performance-for-database-and-analytics-workloads" rel="nofollow">https:&#x2F;&#x2F;blogs.oracle.com&#x2F;cloud-infrastructure&#x2F;post&#x2F;announcin...</a><p>I keep forgetting Oracle is in the cloud business too.<p>&quot;Make it rain&quot;, I guess :)</div><br/><div id="39449216" class="c"><input type="checkbox" id="c-39449216" checked=""/><div class="controls bullet"><span class="by">Twirrim</span><span>|</span><a href="#39444259">root</a><span>|</span><a href="#39448704">parent</a><span>|</span><a href="#39448674">next</a><span>|</span><label class="collapse" for="c-39449216">[-]</label><label class="expand" for="c-39449216">[1 more]</label></div><br/><div class="children"><div class="content">Doh, sorry.  I&#x27;m usually good about not using that acronym!</div><br/></div></div></div></div><div id="39448674" class="c"><input type="checkbox" id="c-39448674" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#39444259">parent</a><span>|</span><a href="#39448704">prev</a><span>|</span><a href="#39451243">next</a><span>|</span><label class="collapse" for="c-39448674">[-]</label><label class="expand" for="c-39448674">[2 more]</label></div><br/><div class="children"><div class="content">I very much expect AWS SSD &#x2F; NVMe upgrades to be well thought-out ahead of time, and optimized for both upfront cost and for longevity &#x2F; durability. Speed may be a third consideration.</div><br/><div id="39450516" class="c"><input type="checkbox" id="c-39450516" checked=""/><div class="controls bullet"><span class="by">flaminHotSpeedo</span><span>|</span><a href="#39444259">root</a><span>|</span><a href="#39448674">parent</a><span>|</span><a href="#39451243">next</a><span>|</span><label class="collapse" for="c-39450516">[-]</label><label class="expand" for="c-39450516">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, hardware and forecasting for cloud providers is basically the definition of deliberate.<p>If anything I&#x27;d guess it&#x27;s a procurement issue, parity between regions is a big thing and it&#x27;s hard to supply dozens of regions around the world with the latest hardware hotness</div><br/></div></div></div></div></div></div><div id="39451243" class="c"><input type="checkbox" id="c-39451243" checked=""/><div class="controls bullet"><span class="by">vinay_ys</span><span>|</span><a href="#39444259">prev</a><span>|</span><a href="#39444011">next</a><span>|</span><label class="collapse" for="c-39451243">[-]</label><label class="expand" for="c-39451243">[1 more]</label></div><br/><div class="children"><div class="content">Well, as hardware becomes more and more powerful, what&#x27;s possible in a small footprint becomes bigger and bigger. And distributed software for disaggregated storage is becoming more accessible. You put these two together, running on-prem footprints at the scale of 50-100M$ capex makes a lot of sense. In my personal experience, at this scale, (if your cloud bill for compute+storage+local-network is $50M+&#x2F;year), you can get 2-4x more on-prem private-cloud capacity for the same money. Of course, this only makes sense if you have an in-house software engineering team already and the marginal cost of adding another 50-100 engineers to build and operate this is strategically valuable to your business.<p>In big data AI space, this is exactly what&#x27;s happening with the top 20th to 100th companies in the world right now.</div><br/></div></div><div id="39444011" class="c"><input type="checkbox" id="c-39444011" checked=""/><div class="controls bullet"><span class="by">siliconc0w</span><span>|</span><a href="#39451243">prev</a><span>|</span><a href="#39444037">next</a><span>|</span><label class="collapse" for="c-39444011">[-]</label><label class="expand" for="c-39444011">[34 more]</label></div><br/><div class="children"><div class="content">Core count plus modern nvme actually make a great case for moving away from the cloud- before it was, &quot;your data probably fits into memory&quot;.  These are so fast that they&#x27;re close enough to memory so it&#x27;s &quot;your data surely fits on disk&quot;.  This reduces the complexity of a lot of workloads so you can just buy a beefy server and do pretty insane caching&#x2F;calculation&#x2F;serving with just a single box or two for redundancy.</div><br/><div id="39444175" class="c"><input type="checkbox" id="c-39444175" checked=""/><div class="controls bullet"><span class="by">malfist</span><span>|</span><a href="#39444011">parent</a><span>|</span><a href="#39444040">next</a><span>|</span><label class="collapse" for="c-39444175">[-]</label><label class="expand" for="c-39444175">[27 more]</label></div><br/><div class="children"><div class="content">I keep hearing that, but that&#x27;s simply not true. SSDs are fast, but they&#x27;re several orders of magnitude slower than RAM, which is orders of magnitude slower than CPU Cache.<p>Samsung 990 Pro 2TB has a latency of 40 μs<p>DDR4-2133 with a CAS 15 has a latency of 14 nano seconds.<p>DDR4 latency is 0.035% of one of the fastest SSDs, or to put it another way, DDR4 is 2,857x faster than an SSD.<p>L1 cache is typically accessible in 4 clock cycles, in 4.8 ghz cpu like the i7-10700, L1 cache latency is sub 1ns.</div><br/><div id="39444384" class="c"><input type="checkbox" id="c-39444384" checked=""/><div class="controls bullet"><span class="by">LeifCarrotson</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444175">parent</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39444384">[-]</label><label class="expand" for="c-39444384">[20 more]</label></div><br/><div class="children"><div class="content">I wonder how many people have built failed businesses that never had enough customer data to exceed the DDR4 in the average developer laptop, and never had so many simultaneous queries it couldn&#x27;t be handled by a single core running SQLite, but built the software architecture on a distributed cloud system just in case it eventually scaled to hundreds of terabytes and billions of simultaneous queries.</div><br/><div id="39445790" class="c"><input type="checkbox" id="c-39445790" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444384">parent</a><span>|</span><a href="#39445536">next</a><span>|</span><label class="collapse" for="c-39445790">[-]</label><label class="expand" for="c-39445790">[2 more]</label></div><br/><div class="children"><div class="content">In may day job I often see systems that have the opposite. Especially for database queries, developers tested on local machine with 100s of records and everything was quick and snappy and on production with mere millions of records I often see queries taking minutes up to a hour just because some developer didn&#x27;t see need for creating indexes or created query in a way there is no way to even create any index that would work</div><br/><div id="39446670" class="c"><input type="checkbox" id="c-39446670" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39445790">parent</a><span>|</span><a href="#39445536">next</a><span>|</span><label class="collapse" for="c-39446670">[-]</label><label class="expand" for="c-39446670">[1 more]</label></div><br/><div class="children"><div class="content">That’s true, but has little to do with distributed cloud architecture vs. single local instance.</div><br/></div></div></div></div><div id="39445536" class="c"><input type="checkbox" id="c-39445536" checked=""/><div class="controls bullet"><span class="by">icedchai</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444384">parent</a><span>|</span><a href="#39445790">prev</a><span>|</span><a href="#39444883">next</a><span>|</span><label class="collapse" for="c-39445536">[-]</label><label class="expand" for="c-39445536">[1 more]</label></div><br/><div class="children"><div class="content">Many. I regularly see systems built for &quot;big data&quot;, built for scale using &quot;serverless&quot; and some proprietary cloud database (like DynamoDB), storing a few hundred megabytes total. 20 years ago we would&#x27;ve built this on PHP and MySQL and called it a day.</div><br/></div></div><div id="39444883" class="c"><input type="checkbox" id="c-39444883" checked=""/><div class="controls bullet"><span class="by">malfist</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444384">parent</a><span>|</span><a href="#39445536">prev</a><span>|</span><a href="#39444867">next</a><span>|</span><label class="collapse" for="c-39444883">[-]</label><label class="expand" for="c-39444883">[8 more]</label></div><br/><div class="children"><div class="content">I totally hear you about that. I work for FAANG, and I&#x27;m working on a service that has to be capable of sending 1.6m text messages in less than 10 minutes.<p>The amount of complexity the architecture has because of those constraints is insane.<p>When I worked at my previous job, management kept asking for that scale of designs for less than 1&#x2F;1000 of the throughput and I was constantly pushing back. There&#x27;s real costs to building for more scale than you need. It&#x27;s not as simple as just tweaking a few things.<p>To me there&#x27;s a couple of big breakpoints in scale:<p>* When you can run on a single server<p>* When you need to run on a single server, but with HA redundancies<p>* When you have to scale beyond a single server<p>* When you have to adapt your scale to deal with the limits of a distributed system, i.e. designing for DyanmoDB&#x27;s partition limits.<p>Each step in that chain add irrevocable complexity, adds to OE, adds to cost to run and cost to build. Be sure you have to take those steps before you decide too.</div><br/><div id="39446459" class="c"><input type="checkbox" id="c-39446459" checked=""/><div class="controls bullet"><span class="by">kuschku</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444883">parent</a><span>|</span><a href="#39446823">next</a><span>|</span><label class="collapse" for="c-39446459">[-]</label><label class="expand" for="c-39446459">[2 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m misunderstanding something, but that&#x27;s about 2700 a second. Or about 3Mbps.<p>Even a very unoptimized application running on a dev laptop can serve 1Gbps nowadays without issues.<p>So what are the constraints that demand a complex architecture?</div><br/><div id="39448463" class="c"><input type="checkbox" id="c-39448463" checked=""/><div class="controls bullet"><span class="by">rdoherty</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39446459">parent</a><span>|</span><a href="#39446823">next</a><span>|</span><label class="collapse" for="c-39448463">[-]</label><label class="expand" for="c-39448463">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not the OP but a few things:<p>* Reading&#x2F;fetching the data - usernames, phone number, message, etc.<p>* Generating the content for each message - it might be custom per person<p>* This is using a 3rd party API that might take anywhere from 100ms to 2s to respond, and you need to leave a connection open.<p>* Retries on errors, rescheduling, backoffs<p>* At least once or at most once sends? Each has tradeoffs<p>* Stopping&#x2F;starting that many messages at any time<p>* Rate limits on some services you might be using alongside your service (network gateway, database, etc)<p>* Recordkeeping - did the message send? When?</div><br/></div></div></div></div><div id="39446823" class="c"><input type="checkbox" id="c-39446823" checked=""/><div class="controls bullet"><span class="by">goguy</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444883">parent</a><span>|</span><a href="#39446459">prev</a><span>|</span><a href="#39446187">next</a><span>|</span><label class="collapse" for="c-39446823">[-]</label><label class="expand" for="c-39446823">[2 more]</label></div><br/><div class="children"><div class="content">That really doesn&#x27;t require that much complexity.<p>I used to send something like 250k a minute complete with delivery report processing from a single machine running a bunch of other services like 10 years ago.</div><br/><div id="39448654" class="c"><input type="checkbox" id="c-39448654" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39446823">parent</a><span>|</span><a href="#39446187">next</a><span>|</span><label class="collapse" for="c-39448654">[-]</label><label class="expand" for="c-39448654">[1 more]</label></div><br/><div class="children"><div class="content">Nice.<p>But average latency is not the whole picture; tail latency is. For good tail latency and handling of spikes, you have to have a sizable &quot;untapped&quot; reserve of performance.</div><br/></div></div></div></div><div id="39446187" class="c"><input type="checkbox" id="c-39446187" checked=""/><div class="controls bullet"><span class="by">disqard</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444883">parent</a><span>|</span><a href="#39446823">prev</a><span>|</span><a href="#39444867">next</a><span>|</span><label class="collapse" for="c-39446187">[-]</label><label class="expand" for="c-39446187">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m trying to guess what &quot;OE&quot; stands for... over engineering?  operating expenditure? I&#x27;d love to know what you meant :)</div><br/><div id="39446438" class="c"><input type="checkbox" id="c-39446438" checked=""/><div class="controls bullet"><span class="by">madisp</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39446187">parent</a><span>|</span><a href="#39447039">next</a><span>|</span><label class="collapse" for="c-39446438">[-]</label><label class="expand" for="c-39446438">[1 more]</label></div><br/><div class="children"><div class="content">probably operating expenses</div><br/></div></div><div id="39447039" class="c"><input type="checkbox" id="c-39447039" checked=""/><div class="controls bullet"><span class="by">malfist</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39446187">parent</a><span>|</span><a href="#39446438">prev</a><span>|</span><a href="#39444867">next</a><span>|</span><label class="collapse" for="c-39447039">[-]</label><label class="expand" for="c-39447039">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, thought it was a common term. Operational Excellence. All the effort and time it takes to keep a service online, on call included</div><br/></div></div></div></div></div></div><div id="39444867" class="c"><input type="checkbox" id="c-39444867" checked=""/><div class="controls bullet"><span class="by">Repulsion9513</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444384">parent</a><span>|</span><a href="#39444883">prev</a><span>|</span><a href="#39448007">next</a><span>|</span><label class="collapse" for="c-39444867">[-]</label><label class="expand" for="c-39444867">[1 more]</label></div><br/><div class="children"><div class="content">A <i>LOT</i>... especially here.</div><br/></div></div><div id="39448007" class="c"><input type="checkbox" id="c-39448007" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444384">parent</a><span>|</span><a href="#39444867">prev</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39448007">[-]</label><label class="expand" for="c-39448007">[7 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not considered serious if you don&#x27;t. Kinda stupid.</div><br/><div id="39448616" class="c"><input type="checkbox" id="c-39448616" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39448007">parent</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39448616">[-]</label><label class="expand" for="c-39448616">[6 more]</label></div><br/><div class="children"><div class="content">In the startup world, this is correct.<p>The success that VCs are after is when your customer base doubles every month. Better yet, every week. Having a reasonably scalable infra at the start ensures that a success won&#x27;t kill you.<p>Of course, the chances of a runaway success like this are slim, so 99% or more startups overbuild, given their resulting customer base. But it&#x27;s like 99% or more pilots who put on a parachute don&#x27;t end up using it; the whole point is the small minority who do, and you never know.<p>For a stable, predictable, medium-scale business it may make total sense to have a few dedicated physical boxes and run their whole operation from them comfortably, for a fraction of cloud costs. But <i>starting</i> with it is more expensive than starting with a cloud, because you immediately need an SRE, or two.</div><br/><div id="39449301" class="c"><input type="checkbox" id="c-39449301" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39448616">parent</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39449301">[-]</label><label class="expand" for="c-39449301">[5 more]</label></div><br/><div class="children"><div class="content">You aren&#x27;t going to get there. The risks and complexity of a startup are high to begin with. Adding artificial roadblocks because of aspirational fantasies is going to hold you back.<p>Look at the big successes such as youtube, twitter, facebook, airbnb, lyft, google, yahoo - exactly zero of them did this preventatively. Even altavista and babelfish, done by DEC and running on Alphas, which they had plenty of, had to be redone multiple times due to growth. Heck, look at the first 5 years of Amazon. AWS was initially ideated in a contract job for Target.<p>Address the immediate and real needs and business cases, not pie in the sky aspirations of global dominance - wait until it becomes a need and then do it.<p>The chances of getting there are only reasonable if you move instead of plan, otherwise you&#x27;ll miss the window and product opportunity.<p>I know it ruffles your engineering feathers - that&#x27;s one of the reasons most attempts at building these things fails. The best ways feel wrong, are counterintuitive and are incidentally often executed by young college kids who don&#x27;t know any better. It&#x27;s why successful tech founders tend to be inexperienced; it can actually be advantageous if they make the right &quot;mistakes&quot;.<p>Forget about any supposedly inevitable disaster until it&#x27;s actually affecting your numbers. I know it&#x27;s hard but the most controllable difference between success and failure in the startup space is in the behavioral patterns of the stakeholders.</div><br/><div id="39449864" class="c"><input type="checkbox" id="c-39449864" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39449301">parent</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39449864">[-]</label><label class="expand" for="c-39449864">[4 more]</label></div><br/><div class="children"><div class="content">Do you remember the companies that did not scale? friendster did well until it failed to scale, and Facebook took over.<p>So the converse argument might be: don&#x27;t bungle it up because you failed to plan. Provision for at least 10x growth with every (re-)implementation.<p><a href="https:&#x2F;&#x2F;highscalability.com&#x2F;friendster-lost-lead-because-of-a-failure-to-scale&#x2F;" rel="nofollow">https:&#x2F;&#x2F;highscalability.com&#x2F;friendster-lost-lead-because-of-...</a></div><br/><div id="39450384" class="c"><input type="checkbox" id="c-39450384" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39449864">parent</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39450384">[-]</label><label class="expand" for="c-39450384">[3 more]</label></div><br/><div class="children"><div class="content">Hold on... You think Facebook took over from Friendster because of scaling problems?!<p>MySpace was the one that took the lead over Friendster and it withered after it got acquired for $500 million by news corp because that was the liquidity event. That&#x27;s when Facebook gained ground. Your timeline is wrong.<p>The MySpace switch was because of themes and other features the users found more appealing. Twitter had similar crashes with its fail whale for a long time and they survived it fine. The teen exodus of Friendster wasn&#x27;t because of TTLB waterfall graphs.<p>Also MySpace did everything on cheap Microsoft IIS 6 servers in ASP 2.0 after switching from Coldfusion in Macromedia HomeSite, they weren&#x27;t genuises. It was a knockoff created by amateurs with a couple new twists. (A modern clone has 2.5 mil users: see <a href="https:&#x2F;&#x2F;spacehey.com&#x2F;browse" rel="nofollow">https:&#x2F;&#x2F;spacehey.com&#x2F;browse</a> still mostly teenagers)<p>Besides, when the final Friendster holdout of the Asian market had exponential decline in 2008, the scaling problems of 5 years ago had long been fixed. Faster load times did not make up for a product consumers no longer found compelling.<p>Also Facebook initially was running literally out of Mark&#x27;s dorm room. In 2007, after they had won the war, their code got leaked because they were deploying the .svn directory in their deploy strategy. Their code was widely mocked. So there we are again.<p>I don&#x27;t care if you can find someone who agrees with you on the Friendster scaling thing, almost every collapsed startup has someone that says &quot;we were just too successful and couldn&#x27;t keep up&quot; because thinking you were just too awesome is the gentler on the ego than realizing a bunch of scrappy hackers just gave people more of what they wanted and either you didn&#x27;t realize it or you thought your lack of adaption was a virtue.</div><br/><div id="39451064" class="c"><input type="checkbox" id="c-39451064" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39450384">parent</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39451064">[-]</label><label class="expand" for="c-39451064">[2 more]</label></div><br/><div class="children"><div class="content">How sure are you that they switched because of themes? Did you see user research? I left because of its poor performance, and MySpace was no substitute for friendster; it targeted an artsy demographic. But Facebook was.</div><br/><div id="39451267" class="c"><input type="checkbox" id="c-39451267" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39451064">parent</a><span>|</span><a href="#39447096">next</a><span>|</span><label class="collapse" for="c-39451267">[-]</label><label class="expand" for="c-39451267">[1 more]</label></div><br/><div class="children"><div class="content">Yes. I worked in social networks 15 years ago. It was a heavy research topic for me.<p>You&#x27;re a highly technical user. Non-technical people are weird - part of the MySpace exodus was the belief that it spread &quot;computer viruses&quot;, really<p>There was more to the switches but I&#x27;d have to dredge it up probably through archive sites these days. The reasons the surveys supported I considered ridiculous but it doesn&#x27;t matter it&#x27;s better to understand consumer behavior - we can&#x27;t easily change it.<p>Especially these days. It was not possible for me to be a teenager with high speed wi-fi when I was one 30 years ago. I&#x27;ve got near zero understanding of the modern consumer youth market or what they think. Against all my expectations I&#x27;ve become an old person.<p>Anyways, the freeform HTML was a major driver - it was geocities with less effort, which had also exited through a liquidity event and currently has a clone these days <a href="https:&#x2F;&#x2F;neocities.org&#x2F;browse" rel="nofollow">https:&#x2F;&#x2F;neocities.org&#x2F;browse</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39447096" class="c"><input type="checkbox" id="c-39447096" checked=""/><div class="controls bullet"><span class="by">BackBlast</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444175">parent</a><span>|</span><a href="#39444384">prev</a><span>|</span><a href="#39448236">next</a><span>|</span><label class="collapse" for="c-39447096">[-]</label><label class="expand" for="c-39447096">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re missing the purpose of the cache.  At least for this argument it&#x27;s mostly for network responses.<p>HDD was 10ms, which was noticeable for cached network request that needs to go back out on the wire.  This was also bottle necked by IOPS, after 100-150 IOPS you were done.  You could do a bit better with raid, but not the 2-3 orders of magnitude you really needed to be an effective cache.  So it just couldn&#x27;t work as a serious cache, the next step up was RAM. This is the operational environment which redis and such memory caches evolved.<p>40 us latency is fine for caching.  Even the high load 500-600us latency is fine for the network request cache purpose.  You can buy individual drives with &gt; 1 million read IOPS.  Plenty for a good cache.  HDD couldn&#x27;t fit the bill for the above reasons.  RAM is faster, no question, but the lower latency of the RAM over the SSD isn&#x27;t really helping performance here as the network latency is dominating.<p>Rails conference 2023 has a talk that mentions this.  They moved from a memory based cache system to an SSD based cache system.  The Redis RAM based system latency was 0.8ms and the SSD based system was 1.2ms for some known system.  Which is fine.  It saves you a couple of orders of magnitude on cost and you can do much much larger and more aggressive caching with the extra space.<p>Often times these RAM caching servers are a network hop away anyway, or at least a loopback TCP request.  Making the question of comparing SSD latency to RAM totally irrelevant.</div><br/></div></div><div id="39448236" class="c"><input type="checkbox" id="c-39448236" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444175">parent</a><span>|</span><a href="#39447096">prev</a><span>|</span><a href="#39444275">next</a><span>|</span><label class="collapse" for="c-39448236">[-]</label><label class="expand" for="c-39448236">[1 more]</label></div><br/><div class="children"><div class="content">RAM is not as fast in practice as the specs claim, because there is a lot of overhead in accessing it. I did some latency benchmarking on my M2 Max MBP when I got it last year. As long as the working set fits in L1 cache, read latency is ~2 ns. Then it starts increasing slowly, reaching ~10 ns at 10 MiB. Then there is a rapid rise to ~100 ns at 100 MiB, followed by slow growth until ~10 GiB. Then the latency starts increasing rapidly again, reaching ~330 ns at 64 GiB.</div><br/></div></div><div id="39444275" class="c"><input type="checkbox" id="c-39444275" checked=""/><div class="controls bullet"><span class="by">avg_dev</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444175">parent</a><span>|</span><a href="#39448236">prev</a><span>|</span><a href="#39444040">next</a><span>|</span><label class="collapse" for="c-39444275">[-]</label><label class="expand" for="c-39444275">[4 more]</label></div><br/><div class="children"><div class="content">pretty cool comparisons. quite some differences there.<p>tangent, I remember reading some post called something like &quot;Latency numbers every programmer should know&quot; and being slightly ashamed when I could not internalize it.</div><br/><div id="39445089" class="c"><input type="checkbox" id="c-39445089" checked=""/><div class="controls bullet"><span class="by">nzgrover</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444275">parent</a><span>|</span><a href="#39445548">next</a><span>|</span><label class="collapse" for="c-39445089">[-]</label><label class="expand" for="c-39445089">[1 more]</label></div><br/><div class="children"><div class="content">You might enjoy Grace Hopper&#x27;s leacture which includes this snippet: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;ZR0ujwlvbkQ?si=vjEQHIGmffjqfHBN&amp;t=2706" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;ZR0ujwlvbkQ?si=vjEQHIGmffjqfHBN&amp;t=2706</a></div><br/></div></div><div id="39445548" class="c"><input type="checkbox" id="c-39445548" checked=""/><div class="controls bullet"><span class="by">darzu</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444275">parent</a><span>|</span><a href="#39445089">prev</a><span>|</span><a href="#39444800">next</a><span>|</span><label class="collapse" for="c-39445548">[-]</label><label class="expand" for="c-39445548">[1 more]</label></div><br/><div class="children"><div class="content">probably this one: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;hellerbarde&#x2F;2843375" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;hellerbarde&#x2F;2843375</a></div><br/></div></div><div id="39444800" class="c"><input type="checkbox" id="c-39444800" checked=""/><div class="controls bullet"><span class="by">malfist</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444275">parent</a><span>|</span><a href="#39445548">prev</a><span>|</span><a href="#39444040">next</a><span>|</span><label class="collapse" for="c-39444800">[-]</label><label class="expand" for="c-39444800">[1 more]</label></div><br/><div class="children"><div class="content">Oh don&#x27;t feel bad. I had to look up every one of those numbers</div><br/></div></div></div></div></div></div><div id="39444040" class="c"><input type="checkbox" id="c-39444040" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#39444011">parent</a><span>|</span><a href="#39444175">prev</a><span>|</span><a href="#39444225">next</a><span>|</span><label class="collapse" for="c-39444040">[-]</label><label class="expand" for="c-39444040">[3 more]</label></div><br/><div class="children"><div class="content">The reasons to switch away from cloud keep piling up.<p>We&#x27;re doing some amount of on-prem, and I&#x27;m eager to do more.</div><br/><div id="39446225" class="c"><input type="checkbox" id="c-39446225" checked=""/><div class="controls bullet"><span class="by">aarmenaa</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444040">parent</a><span>|</span><a href="#39444225">next</a><span>|</span><label class="collapse" for="c-39446225">[-]</label><label class="expand" for="c-39446225">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve previously worked for a place that ran most of their production network &quot;on-prem&quot;.  They had a few thousand physical machines spread across 6 or so colocation sites on three continents.  I enjoyed that job immensely; I&#x27;d jump at the chance to build something like it from the ground up.  I&#x27;m not sure if that actually makes sense for very many businesses though.</div><br/><div id="39450400" class="c"><input type="checkbox" id="c-39450400" checked=""/><div class="controls bullet"><span class="by">kuchenbecker</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39446225">parent</a><span>|</span><a href="#39444225">next</a><span>|</span><label class="collapse" for="c-39450400">[-]</label><label class="expand" for="c-39450400">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m getting that opportunity, however I expect it will be the last as most have migrated to the cloud and smaller companies are appealing to me; smallest company (of 5) I&#x27;ve worked for had 4.4k employees and large companies have the resources to roll their own.<p>Unless there is an onprem movement I expect cloud to be the future as maintaining the tech stack onprem is difficult and we need to nake decisions down to the hardware we order.</div><br/></div></div></div></div></div></div><div id="39444225" class="c"><input type="checkbox" id="c-39444225" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39444011">parent</a><span>|</span><a href="#39444040">prev</a><span>|</span><a href="#39444037">next</a><span>|</span><label class="collapse" for="c-39444225">[-]</label><label class="expand" for="c-39444225">[3 more]</label></div><br/><div class="children"><div class="content">&quot;I will simply have another box for redundancy&quot; is already a system so complex that having it in or out of the cloud won&#x27;t make a difference.</div><br/><div id="39444624" class="c"><input type="checkbox" id="c-39444624" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444225">parent</a><span>|</span><a href="#39444037">next</a><span>|</span><label class="collapse" for="c-39444624">[-]</label><label class="expand" for="c-39444624">[2 more]</label></div><br/><div class="children"><div class="content">It really depends on business requirements. Real-time redundancy is hard. Taking backups at 15-min intervals and having the standby box merely pull down the last backup when starting up is much easier, and this may actually be fine for a lot of applications.<p>Unfortunately very few actually think about failure modes, set realistic targets, and actually <i>test</i> the process. Everyone <i>thinks</i> they need 100% uptime and consistency, few actually achieve it in practice (many think they do, but when shit hits the fan it uncovers an edge-case they haven&#x27;t thought of), but it turns out that in most cases it doesn&#x27;t matter and they could&#x27;ve saved themselves a lot of trouble and complexity.</div><br/><div id="39446365" class="c"><input type="checkbox" id="c-39446365" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39444011">root</a><span>|</span><a href="#39444624">parent</a><span>|</span><a href="#39444037">next</a><span>|</span><label class="collapse" for="c-39446365">[-]</label><label class="expand" for="c-39446365">[1 more]</label></div><br/><div class="children"><div class="content">So much this.<p>I&#x27;d github can afford the amount of downtime they do, it&#x27;s likely that your business can afford 15 minutes of downtime every once in a while due to a failing server.<p>Also, the less servers you have overall, the least common a failure will be.<p>Backups and cold failover server are mandatory, but anything past that should be weighted on a rational cost&#x2F;benefit analysis, and for most people the cost&#x2F;benefit ratio just isn&#x27;t enough to justify infrastructure complexity.</div><br/></div></div></div></div></div></div></div></div><div id="39444037" class="c"><input type="checkbox" id="c-39444037" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39444011">prev</a><span>|</span><a href="#39447602">next</a><span>|</span><label class="collapse" for="c-39444037">[-]</label><label class="expand" for="c-39444037">[15 more]</label></div><br/><div class="children"><div class="content">&gt; Since then, several NVMe instance types, including i4i and im4gn, have been launched. Surprisingly, however, the performance has not increased; seven years after the i3 launch, we are still stuck with 2 GB&#x2F;s per SSD.<p>AWS marketing claims otherwise:<p><pre><code>    Up to 800K random write IOPS
    Up to 1 million random read IOPS
    Up to 5600 MB&#x2F;second of sequential writes
    Up to 8000 MB&#x2F;second of sequential reads

</code></pre>
<a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-storage-optimized-amazon-ec2-i4g-instances-graviton-processors-and-aws-nitro-ssds&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-storage-optimized-amazo...</a></div><br/><div id="39444172" class="c"><input type="checkbox" id="c-39444172" checked=""/><div class="controls bullet"><span class="by">sprachspiel</span><span>|</span><a href="#39444037">parent</a><span>|</span><a href="#39447602">next</a><span>|</span><label class="collapse" for="c-39444172">[-]</label><label class="expand" for="c-39444172">[14 more]</label></div><br/><div class="children"><div class="content">This is for 8 SSDs and a single modern PCIe 5.0 has better specs than this.</div><br/><div id="39444404" class="c"><input type="checkbox" id="c-39444404" checked=""/><div class="controls bullet"><span class="by">nik_0_0</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444172">parent</a><span>|</span><a href="#39444346">next</a><span>|</span><label class="collapse" for="c-39444404">[-]</label><label class="expand" for="c-39444404">[12 more]</label></div><br/><div class="children"><div class="content">Is it? The line preceding the bullet list on that page seems to state otherwise:<p>“”<p><pre><code>  Each storage volume can deliver the following performance (all measured using 4 KiB blocks):

  * Up to 8000 MB&#x2F;second of sequential reads
</code></pre>
“”</div><br/><div id="39444564" class="c"><input type="checkbox" id="c-39444564" checked=""/><div class="controls bullet"><span class="by">sprachspiel</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444404">parent</a><span>|</span><a href="#39444346">next</a><span>|</span><label class="collapse" for="c-39444564">[-]</label><label class="expand" for="c-39444564">[11 more]</label></div><br/><div class="children"><div class="content">Just tested a i4i.32xlarge:<p><pre><code>  $ lsblk
  NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
  loop0          7:0    0  24.9M  1 loop &#x2F;snap&#x2F;amazon-ssm-agent&#x2F;7628
  loop1          7:1    0  55.7M  1 loop &#x2F;snap&#x2F;core18&#x2F;2812
  loop2          7:2    0  63.5M  1 loop &#x2F;snap&#x2F;core20&#x2F;2015
  loop3          7:3    0 111.9M  1 loop &#x2F;snap&#x2F;lxd&#x2F;24322
  loop4          7:4    0  40.9M  1 loop &#x2F;snap&#x2F;snapd&#x2F;20290
  nvme0n1      259:0    0     8G  0 disk 
  ├─nvme0n1p1  259:1    0   7.9G  0 part &#x2F;
  ├─nvme0n1p14 259:2    0     4M  0 part 
  └─nvme0n1p15 259:3    0   106M  0 part &#x2F;boot&#x2F;efi
  nvme2n1      259:4    0   3.4T  0 disk 
  nvme4n1      259:5    0   3.4T  0 disk 
  nvme1n1      259:6    0   3.4T  0 disk 
  nvme5n1      259:7    0   3.4T  0 disk 
  nvme7n1      259:8    0   3.4T  0 disk 
  nvme6n1      259:9    0   3.4T  0 disk 
  nvme3n1      259:10   0   3.4T  0 disk 
  nvme8n1      259:11   0   3.4T  0 disk
</code></pre>
Since nvme0n1 is the EBS boot volume, we have 8 SSDs. And here&#x27;s the read bandwidth for one of them:<p><pre><code>  $ sudo fio --name=bla --filename=&#x2F;dev&#x2F;nvme2n1 --rw=read --iodepth=128 --ioengine=libaio --direct=1 --blocksize=16m
  bla: (g=0): rw=read, bs=(R) 16.0MiB-16.0MiB, (W) 16.0MiB-16.0MiB, (T) 16.0MiB-16.0MiB, ioengine=libaio, iodepth=128
  fio-3.28
  Starting 1 process
  ^Cbs: 1 (f=1): [R(1)][0.5%][r=2704MiB&#x2F;s][r=169 IOPS][eta 20m:17s]
</code></pre>
So we should have a total bandwidth of 2.7*8=21 GB&#x2F;s. Not that great for 2024.</div><br/><div id="39444735" class="c"><input type="checkbox" id="c-39444735" checked=""/><div class="controls bullet"><span class="by">Aachen</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444564">parent</a><span>|</span><a href="#39444657">next</a><span>|</span><label class="collapse" for="c-39444735">[-]</label><label class="expand" for="c-39444735">[1 more]</label></div><br/><div class="children"><div class="content">So if I&#x27;m reading it right, the quote from the original article that started this thread was ballpark correct?<p>&gt; we are still stuck with 2 GB&#x2F;s per SSD<p>Versus the ~2.7 GiB&#x2F;s your benchmark shows (bit hard to know where to look on mobile with all that line-wrapped output, and when not familiar with the fio tool; not your fault but that&#x27;s why I&#x27;m double checking my conclusion)</div><br/></div></div><div id="39444657" class="c"><input type="checkbox" id="c-39444657" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444564">parent</a><span>|</span><a href="#39444735">prev</a><span>|</span><a href="#39445456">next</a><span>|</span><label class="collapse" for="c-39444657">[-]</label><label class="expand" for="c-39444657">[1 more]</label></div><br/><div class="children"><div class="content">If you still have this machine, I wonder if you can get this bandwidth in parallel across all SSDs? There could be some hypervisor-level or host-level bottleneck that means while any SSD in isolation will give you the observed bandwidth, you can&#x27;t actually reach that if you try to access them all in parallel?</div><br/></div></div><div id="39445456" class="c"><input type="checkbox" id="c-39445456" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444564">parent</a><span>|</span><a href="#39444657">prev</a><span>|</span><a href="#39445321">next</a><span>|</span><label class="collapse" for="c-39445456">[-]</label><label class="expand" for="c-39445456">[1 more]</label></div><br/><div class="children"><div class="content">Can you addjust --blocksize to correspond to the block size on the device?
And with&#x2F;without --direct=1</div><br/></div></div><div id="39445321" class="c"><input type="checkbox" id="c-39445321" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444564">parent</a><span>|</span><a href="#39445456">prev</a><span>|</span><a href="#39444982">next</a><span>|</span><label class="collapse" for="c-39445321">[-]</label><label class="expand" for="c-39445321">[5 more]</label></div><br/><div class="children"><div class="content">I wonder if there is some tuning that needs to be done here, it seems suprising that the advertised rate would be this much off otherwise.</div><br/><div id="39445596" class="c"><input type="checkbox" id="c-39445596" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39445321">parent</a><span>|</span><a href="#39444982">next</a><span>|</span><label class="collapse" for="c-39445596">[-]</label><label class="expand" for="c-39445596">[4 more]</label></div><br/><div class="children"><div class="content">I would start with the LBA format, which is likely to be suboptimal for compatibility.</div><br/><div id="39447787" class="c"><input type="checkbox" id="c-39447787" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39445596">parent</a><span>|</span><a href="#39444982">next</a><span>|</span><label class="collapse" for="c-39447787">[-]</label><label class="expand" for="c-39447787">[3 more]</label></div><br/><div class="children"><div class="content">somehow I4g drives don&#x27;t like to get formatted<p><pre><code>    # nvme format &#x2F;dev&#x2F;nvme1 -n1 -f
    NVMe status: INVALID_OPCODE: The associated command opcode field is not valid(0x2001)
    # nvme id-ctrl &#x2F;dev&#x2F;nvme1 | grep oacs
    oacs      : 0
</code></pre>
but the LBA format indeed is sus:<p><pre><code>    LBA Format  0 : Metadata Size: 0   bytes - Data Size: 512 bytes - Relative Performance: 0 Best (in use)</code></pre></div><br/><div id="39448000" class="c"><input type="checkbox" id="c-39448000" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39447787">parent</a><span>|</span><a href="#39444982">next</a><span>|</span><label class="collapse" for="c-39448000">[-]</label><label class="expand" for="c-39448000">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a shame. The recent &quot;datacenter nvme&quot; standards involving fb, goog, et al mandate 4K LBA support.</div><br/><div id="39448552" class="c"><input type="checkbox" id="c-39448552" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39448000">parent</a><span>|</span><a href="#39444982">next</a><span>|</span><label class="collapse" for="c-39448552">[-]</label><label class="expand" for="c-39448552">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;d be great if you&#x27;d manage to throw together quick blogpost about i4g io perf, there obviously something funny going on and I imagine you guys could figure it out much easier than anybody else, especially if you are already having some figures in the marketing.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39444982" class="c"><input type="checkbox" id="c-39444982" checked=""/><div class="controls bullet"><span class="by">dangoodmanUT</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444564">parent</a><span>|</span><a href="#39445321">prev</a><span>|</span><a href="#39444346">next</a><span>|</span><label class="collapse" for="c-39444982">[-]</label><label class="expand" for="c-39444982">[2 more]</label></div><br/><div class="children"><div class="content">that&#x27;s 16m blocks, not 4k</div><br/><div id="39445601" class="c"><input type="checkbox" id="c-39445601" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444982">parent</a><span>|</span><a href="#39444346">next</a><span>|</span><label class="collapse" for="c-39445601">[-]</label><label class="expand" for="c-39445601">[1 more]</label></div><br/><div class="children"><div class="content">Last I checked, Linux splits up massive IO requests like that before sending them to the disk. But there&#x27;s no benefit to splitting a sequential IO request all the way down to 4kB.</div><br/></div></div></div></div></div></div></div></div><div id="39444346" class="c"><input type="checkbox" id="c-39444346" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39444037">root</a><span>|</span><a href="#39444172">parent</a><span>|</span><a href="#39444404">prev</a><span>|</span><a href="#39447602">next</a><span>|</span><label class="collapse" for="c-39444346">[-]</label><label class="expand" for="c-39444346">[1 more]</label></div><br/><div class="children"><div class="content">Those claims are per device. There isn&#x27;t even an instance in that family with 8 devices.</div><br/></div></div></div></div></div></div><div id="39447602" class="c"><input type="checkbox" id="c-39447602" checked=""/><div class="controls bullet"><span class="by">StillBored</span><span>|</span><a href="#39444037">prev</a><span>|</span><a href="#39450577">next</a><span>|</span><label class="collapse" for="c-39447602">[-]</label><label class="expand" for="c-39447602">[2 more]</label></div><br/><div class="children"><div class="content">Its worse than the article mentions. Because bandwidth isn&#x27;t the problem its IOPS that are the problem.<p>Last time (about a year ago) I ran a couple random IO benchmarks against a storage optimized instances and the random IOPs behavior is closer to a large spinning RAID array than SSDs if the disk size is over some threshold.<p>IIRC, What it looks like is that there is a fast local SSD cache with a couple hundred GB of storage and then the rest is backed by remote spinning media.<p>Its one of the many reasons I have a hard time taking cloud optimization seriously, the lack of direct tiering controls means that database&#x2F;etc style workloads are not going to optimize well and that will end up costing a lot of $$$$$.<p>So, maybe it was the instance types&#x2F;configuration I was using, but &lt;shrug&gt; it was just something I was testing in passing.</div><br/><div id="39448132" class="c"><input type="checkbox" id="c-39448132" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39447602">parent</a><span>|</span><a href="#39450577">next</a><span>|</span><label class="collapse" for="c-39448132">[-]</label><label class="expand" for="c-39448132">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  # fio --name=read_iops_test   --filename=&#x2F;dev&#x2F;nvme1n1 --filesize=1500G   --time_based --ramp_time=1s --runtime=15s   --ioengine=io_uring --fixedbufs --direct=1 --verify=0 --randrep
  eat=0   --bs=4K --iodepth=256 --rw=randread   --iodepth_batch_submit=256  --iodepth_batch_complete_max=256
  read_iops_test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=256
  fio-3.32
  Starting 1 process
  Jobs: 1 (f=1): [r(1)][100.0%][r=2082MiB&#x2F;s][r=533k IOPS][eta 00m:00s]
  read_iops_test: (groupid=0, jobs=1): err= 0: pid=34235: Tue Feb 20 22:57:00 2024
    read: IOPS=534k, BW=2086MiB&#x2F;s (2187MB&#x2F;s)(30.6GiB&#x2F;15001msec)
      slat (nsec): min=713, max=255840, avg=31174.74, stdev=16248.45
      clat (nsec): min=1419, max=1175.6k, avg=443782.26, stdev=277389.66
      lat (usec): min=133, max=1240, avg=474.96, stdev=274.50
      clat percentiles (usec):
      |  1.00th=[  169],  5.00th=[  198], 10.00th=[  217], 20.00th=[  243],
      | 30.00th=[  265], 40.00th=[  285], 50.00th=[  306], 60.00th=[  334],
      | 70.00th=[  396], 80.00th=[  865], 90.00th=[  922], 95.00th=[  947],
      | 99.00th=[  996], 99.50th=[ 1012], 99.90th=[ 1045], 99.95th=[ 1057],
      | 99.99th=[ 1074]
    bw (  MiB&#x2F;s): min= 2080, max= 2092, per=100.00%, avg=2086.72, stdev= 2.35, samples=30
    iops        : min=532548, max=535738, avg=534199.13, stdev=601.82, samples=30
    lat (usec)   : 2=0.01%, 100=0.01%, 250=23.06%, 500=50.90%, 750=0.28%
    lat (usec)   : 1000=24.90%
    lat (msec)   : 2=0.87%
    cpu          : usr=14.17%, sys=67.83%, ctx=156851, majf=0, minf=37
    IO depths    : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=100.0%
      submit    : 0=0.0%, 4=7.8%, 8=11.3%, 16=39.7%, 32=30.6%, 64=10.5%, &gt;=64=0.1%
      complete  : 0=0.0%, 4=5.3%, 8=9.5%, 16=40.3%, 32=32.4%, 64=12.4%, &gt;=64=0.1%
      issued rwts: total=8010661,0,0,0 short=0,0,0,0 dropped=0,0,0,0
      latency   : target=0, window=0, percentile=100.00%, depth=256

  Run status group 0 (all jobs):
    READ: bw=2086MiB&#x2F;s (2187MB&#x2F;s), 2086MiB&#x2F;s-2086MiB&#x2F;s (2187MB&#x2F;s-2187MB&#x2F;s), io=30.6GiB (32.8GB), run=15001-15001msec

  Disk stats (read&#x2F;write):
    nvme1n1: ios=8542481&#x2F;0, merge=0&#x2F;0, ticks=3822266&#x2F;0, in_queue=3822266, util=99.37%

</code></pre>
tldr: random 4k reads pretty much saturate the available 2GB&#x2F;s bandwidth (this is on m6id)</div><br/></div></div></div></div><div id="39450577" class="c"><input type="checkbox" id="c-39450577" checked=""/><div class="controls bullet"><span class="by">HippoBaro</span><span>|</span><a href="#39447602">prev</a><span>|</span><a href="#39443872">next</a><span>|</span><label class="collapse" for="c-39450577">[-]</label><label class="expand" for="c-39450577">[1 more]</label></div><br/><div class="children"><div class="content">As a database engineer who worked extensively with both i3 and i4 instances, I want to add that although i4 has lower IOPS, the latencies distribution of IO ops is an order of magnitude better.<p>IOPS indeed matters a lot, but so does latency! For our use case, it was much easier to saturate those disks than the old i3s, and we attribute it to the better latencies, making IO scheduling a lot more accurate.</div><br/></div></div><div id="39443872" class="c"><input type="checkbox" id="c-39443872" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39450577">prev</a><span>|</span><a href="#39449867">next</a><span>|</span><label class="collapse" for="c-39443872">[-]</label><label class="expand" for="c-39443872">[8 more]</label></div><br/><div class="children"><div class="content">I think the obvious answer is there&#x27;s not much demand, and keeping it &quot;low&quot; allows trickery and funny business with the virtualization layer (think: SAN, etc) that you can&#x27;t do with &quot;raw hardware speed&quot;.</div><br/><div id="39444796" class="c"><input type="checkbox" id="c-39444796" checked=""/><div class="controls bullet"><span class="by">Aachen</span><span>|</span><a href="#39443872">parent</a><span>|</span><a href="#39444053">next</a><span>|</span><label class="collapse" for="c-39444796">[-]</label><label class="expand" for="c-39444796">[1 more]</label></div><br/><div class="children"><div class="content">I ended up buying a SATA SSD for 50 euros to stick in an old laptop that I was already using as server and, my god, it is so much faster than the thing I was trying to run on digitalocean. The DO VPS barely beat the old 5400 rpm spinning rust that was in the laptop originally (the reason why I was trying to rent a fast, advertised-with-SSD, server). Doing this i&#x2F;o task effectively in the cloud, at least with DO, seems to require putting it in RAM which was a bit expensive for the few hundred gigabytes of data I wanted to process into an indexed format<p>So there is demand, but I&#x27;m certainly not interested in paying many multiples of 50 euros over an expected lifespan of a few years, so it may not make economic sense for them to offer it to users like me at least. On the other hand, for the couple hours this should have taken (rather than the days it initially did), I&#x27;d certainly have been willing to pay that cloud premium and that&#x27;s why I tried to get me one of these allegedly SSD-backed VPSes... but now that I have a fast system permanently, I don&#x27;t think that was a wise decision of past me</div><br/></div></div><div id="39444053" class="c"><input type="checkbox" id="c-39444053" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39443872">parent</a><span>|</span><a href="#39444796">prev</a><span>|</span><a href="#39443971">next</a><span>|</span><label class="collapse" for="c-39444053">[-]</label><label class="expand" for="c-39444053">[1 more]</label></div><br/><div class="children"><div class="content">There is no trickery with AWS instance stores, they are honest to god local disks.</div><br/></div></div><div id="39443971" class="c"><input type="checkbox" id="c-39443971" checked=""/><div class="controls bullet"><span class="by">_Rabs_</span><span>|</span><a href="#39443872">parent</a><span>|</span><a href="#39444053">prev</a><span>|</span><a href="#39443975">next</a><span>|</span><label class="collapse" for="c-39443971">[-]</label><label class="expand" for="c-39443971">[4 more]</label></div><br/><div class="children"><div class="content">Sure, but it does make me wonder what kind of speeds we are paying for if we can&#x27;t even get raw hardware speeds...<p>Sounds like one more excuse for AWS to obfuscate any meaning in their billing structure and take control of the narrative.<p>How much are they getting away with by virtualization. (Think how banks use your money for loans and stuff)<p>You actually don&#x27;t get to really see the internals other than IOPS which doesn&#x27;t help when it&#x27;s gatekept already.</div><br/><div id="39444504" class="c"><input type="checkbox" id="c-39444504" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39443872">root</a><span>|</span><a href="#39443971">parent</a><span>|</span><a href="#39443975">next</a><span>|</span><label class="collapse" for="c-39444504">[-]</label><label class="expand" for="c-39444504">[3 more]</label></div><br/><div class="children"><div class="content">The biggest &quot;scam&quot; if you can call it that is reducing all factors of CPU performance to &quot;cores&quot;.</div><br/><div id="39449042" class="c"><input type="checkbox" id="c-39449042" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39443872">root</a><span>|</span><a href="#39444504">parent</a><span>|</span><a href="#39445254">next</a><span>|</span><label class="collapse" for="c-39449042">[-]</label><label class="expand" for="c-39449042">[1 more]</label></div><br/><div class="children"><div class="content">AWS is pretty transparent about what sort of cores you are exactly getting, and has different types available for different use-cases; typical example would something like r7iz that is aimed for peak single-threaded perf <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;r7iz&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;r7iz&#x2F;</a></div><br/></div></div><div id="39445254" class="c"><input type="checkbox" id="c-39445254" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39443872">root</a><span>|</span><a href="#39444504">parent</a><span>|</span><a href="#39449042">prev</a><span>|</span><a href="#39443975">next</a><span>|</span><label class="collapse" for="c-39445254">[-]</label><label class="expand" for="c-39445254">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d argue the even bigger scam is charging for egress data transfer rather than just for the size of the pipe.</div><br/></div></div></div></div></div></div><div id="39443975" class="c"><input type="checkbox" id="c-39443975" checked=""/><div class="controls bullet"><span class="by">s1gnp0st</span><span>|</span><a href="#39443872">parent</a><span>|</span><a href="#39443971">prev</a><span>|</span><a href="#39449867">next</a><span>|</span><label class="collapse" for="c-39443975">[-]</label><label class="expand" for="c-39443975">[1 more]</label></div><br/><div class="children"><div class="content">True but the funny business buys a lot of fault tolerance, and predictable performance if not maximum performance.</div><br/></div></div></div></div><div id="39449867" class="c"><input type="checkbox" id="c-39449867" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#39443872">prev</a><span>|</span><a href="#39450976">next</a><span>|</span><label class="collapse" for="c-39449867">[-]</label><label class="expand" for="c-39449867">[3 more]</label></div><br/><div class="children"><div class="content">This got me thinking about the recent Rails improvement on moving cache from RAM to SSD. The test they did from what I remembered was RAM at 0.8ms and SSD was ~1.5ms. Moving to SSD you could afford to cache 10x more data and still be cheaper. Now I wonder if the test results would be the same on cloud. Assuming they tested it locally.</div><br/><div id="39449897" class="c"><input type="checkbox" id="c-39449897" checked=""/><div class="controls bullet"><span class="by">samtheprogram</span><span>|</span><a href="#39449867">parent</a><span>|</span><a href="#39449934">next</a><span>|</span><label class="collapse" for="c-39449897">[-]</label><label class="expand" for="c-39449897">[1 more]</label></div><br/><div class="children"><div class="content">They (Basecamp) recently moved off of cloud, so I’m wondering if they even care for their particular use case.<p>I’m sure this is configurable in general though?</div><br/></div></div><div id="39449934" class="c"><input type="checkbox" id="c-39449934" checked=""/><div class="controls bullet"><span class="by">aditya</span><span>|</span><a href="#39449867">parent</a><span>|</span><a href="#39449897">prev</a><span>|</span><a href="#39450976">next</a><span>|</span><label class="collapse" for="c-39449934">[-]</label><label class="expand" for="c-39449934">[1 more]</label></div><br/><div class="children"><div class="content">got a link to the rails improvement?<p>nvm, found it: <a href="https:&#x2F;&#x2F;dev.37signals.com&#x2F;solid-cache&#x2F;" rel="nofollow">https:&#x2F;&#x2F;dev.37signals.com&#x2F;solid-cache&#x2F;</a></div><br/></div></div></div></div><div id="39450976" class="c"><input type="checkbox" id="c-39450976" checked=""/><div class="controls bullet"><span class="by">jnsaff2</span><span>|</span><a href="#39449867">prev</a><span>|</span><a href="#39444088">next</a><span>|</span><label class="collapse" for="c-39450976">[-]</label><label class="expand" for="c-39450976">[1 more]</label></div><br/><div class="children"><div class="content">The article ends with the question about why the bandwidth is capped.<p>I think the answer could be mclock scheduler.<p><a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;event&#x2F;osdi10&#x2F;tech&#x2F;full_papers&#x2F;Gulati.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;event&#x2F;osdi10&#x2F;tech&#x2F;full_papers&#x2F;...</a></div><br/></div></div><div id="39444088" class="c"><input type="checkbox" id="c-39444088" checked=""/><div class="controls bullet"><span class="by">cogman10</span><span>|</span><a href="#39450976">prev</a><span>|</span><a href="#39444360">next</a><span>|</span><label class="collapse" for="c-39444088">[-]</label><label class="expand" for="c-39444088">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a 4th option. Cost.<p>The fastest SSDs tend to also be MLC which tend to have much lower write life vs other technologies.  This isn&#x27;t unusual, increasing data density generally also makes it easier to increase performance.  However, it&#x27;s at the cost that the writes are typically done for a block&#x2F;cell in memory rather than for single bits.  So if one cell goes bad, they all fail.<p>But even if that&#x27;s not the problem, there is a problem of upgrading the fleet in a cost effective mechanism.  When you start introducing new tech into the stack, replacing that tech now requires your datacenters to have 2 different types of hardware on hand AND for the techs swapping drives to have a way to identify and replace that stuff when it goes bad.</div><br/></div></div><div id="39444360" class="c"><input type="checkbox" id="c-39444360" checked=""/><div class="controls bullet"><span class="by">fabioyy</span><span>|</span><a href="#39444088">prev</a><span>|</span><a href="#39444670">next</a><span>|</span><label class="collapse" for="c-39444360">[-]</label><label class="expand" for="c-39444360">[7 more]</label></div><br/><div class="children"><div class="content">it is not worth to use cloud if you need a lot of iops&#x2F;bandwidth<p>heck, its not worth for anything besides scalability<p>dedicated servers are wayyyy cheaper</div><br/><div id="39444778" class="c"><input type="checkbox" id="c-39444778" checked=""/><div class="controls bullet"><span class="by">kstrauser</span><span>|</span><a href="#39444360">parent</a><span>|</span><a href="#39444670">next</a><span>|</span><label class="collapse" for="c-39444778">[-]</label><label class="expand" for="c-39444778">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not certain that&#x27;s true if you look at TCO. Yes, you can probably buy a server for less than the yearly rent on the equivalent EC2 instance. But then you&#x27;ve got to put that server somewhere, with reliable power and probably redundant Internet connections. You have to pay someone&#x27;s salary to set it up and load it to the point that a user can SSH in and configure it. You have to maintain an inventory of spares, and pay someone to swap it out if it breaks. You have to pay to put its backups somewhere.<p>Yeah, you <i>can</i> skip a lot of that if your goal is to get <i>a</i> server online as cheaply as possible, reliability be damned. As soon as you start caring about keeping it in a business-ready state, costs start to skyrocket.<p>I&#x27;ve worn the sysadmin hat. If AWS burned down, I&#x27;d be ready and willing to recreate the important parts locally so that my company could stay in business. But wow, would they ever be in for some sticker shock.</div><br/><div id="39445279" class="c"><input type="checkbox" id="c-39445279" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#39444360">root</a><span>|</span><a href="#39444778">parent</a><span>|</span><a href="#39446313">next</a><span>|</span><label class="collapse" for="c-39445279">[-]</label><label class="expand" for="c-39445279">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But then you&#x27;ve got to put that server somewhere, with reliable power and probably redundant Internet connections. You have to pay someone&#x27;s salary to set it up and load it to the point that a user can SSH in and configure it. You have to maintain an inventory of spares, and pay someone to swap it out if it breaks.<p>There&#x27;s a middle-ground between cloud and colocation. There are plenty of providers such as OVH, Hetzner, Equinix, etc which will do all of the above for you.</div><br/></div></div><div id="39446313" class="c"><input type="checkbox" id="c-39446313" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#39444360">root</a><span>|</span><a href="#39444778">parent</a><span>|</span><a href="#39445279">prev</a><span>|</span><a href="#39446784">next</a><span>|</span><label class="collapse" for="c-39446313">[-]</label><label class="expand" for="c-39446313">[1 more]</label></div><br/><div class="children"><div class="content">At least in the workstation segment cloud doesn&#x27;t compete. We use Threadrippers + A6000 GPUs at work. Getting the equivalent datacenter-type GPUs and EPYC processors is more expensive, even after accounting for IT and utilization.</div><br/></div></div><div id="39446784" class="c"><input type="checkbox" id="c-39446784" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39444360">root</a><span>|</span><a href="#39444778">parent</a><span>|</span><a href="#39446313">prev</a><span>|</span><a href="#39447014">next</a><span>|</span><label class="collapse" for="c-39446784">[-]</label><label class="expand" for="c-39446784">[1 more]</label></div><br/><div class="children"><div class="content">Where I live, a number of SMEs are doing this. It’s really not that costly, unless you are a tiny startup I guess.</div><br/></div></div><div id="39447014" class="c"><input type="checkbox" id="c-39447014" checked=""/><div class="controls bullet"><span class="by">justsomehnguy</span><span>|</span><a href="#39444360">root</a><span>|</span><a href="#39444778">parent</a><span>|</span><a href="#39446784">prev</a><span>|</span><a href="#39447444">next</a><span>|</span><label class="collapse" for="c-39447014">[-]</label><label class="expand" for="c-39447014">[1 more]</label></div><br/><div class="children"><div class="content">&gt; as cheaply as possible, <i>reliability be damned</i>. As soon as you start caring about keeping it in a business-ready state, costs start to skyrocket.<p>The demand for five-nines is greatly exaggerated.</div><br/></div></div><div id="39447444" class="c"><input type="checkbox" id="c-39447444" checked=""/><div class="controls bullet"><span class="by">BackBlast</span><span>|</span><a href="#39444360">root</a><span>|</span><a href="#39444778">parent</a><span>|</span><a href="#39447014">prev</a><span>|</span><a href="#39444670">next</a><span>|</span><label class="collapse" for="c-39447444">[-]</label><label class="expand" for="c-39447444">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not certain that&#x27;s true if you look at TCO.<p>Sigh.  This old trope from ancient history in internet time.<p>&gt; Yes, you can probably buy a server for less than the yearly rent on the equivalent EC2 instance.<p>Or a monthly bill...  I can oft times buy a higher performing server for the cost of a rental for a single month.<p>&gt; But then you&#x27;ve got to put that server somewhere, with reliable power and probably redundant Internet connections<p>Power:<p>The power problem is a lot lower with modern systems because they can use a lot less of it per unit of compute&#x2F;memory&#x2F;disk performance.  Idle power has improved a lot too.  You don&#x27;t need 700 watts of server power anymore for a 2 socket 8 core monster that is outclassed by a modern $400 mini-pc that maxes out at 45 watts.<p>You can buy server rack batteries now in a modern chemistry that&#x27;ll go 20 years with zero maintenance.  4U sized 5kwh cost 1000-1500.  EVs have pushed battery cost down a LOT.  How much do you really need?  Do you even need a generator if your battery just carries the day?  Even if your power reliability totally sucks?<p>Network:<p>Never been easier to buy network transfer.  Fiber is available in many places, even cable speeds are well beyond the past, and there&#x27;s starlink if you want to be fully resistant to local power issues.  Sure, get two vendors for redundancy.  Then you can hit cloud-style uptimes out of your closet.<p>Overlay networks like tailscale make the networking issues within the reach of almost anyone.<p>&gt; Yeah, you can skip a lot of that if your goal is to get a server online as cheaply as possible, reliability be damned<p>Google cut it&#x27;s teeth with cheap consumer class white box computers when &quot;best practice&quot; of the day was to buy expensive server class hardware.  It&#x27;s a tried and true method of bootstrapping.<p>&gt; You have to maintain an inventory of spares, and pay someone to swap it out if it breaks. You have to pay to put its backups somewhere.<p>Have you seen the size of M.2 sticks?  Memory sticks?  They aren&#x27;t very big...  I happened to like opening up systems and actually touching the hardware I use.<p>But yeah, if you just can&#x27;t make it work or be bothered in the modern era of computing.  Then stick with the cloud and the 10-100x premium they charge for their services.<p>&gt; I&#x27;ve worn the sysadmin hat. If AWS burned down, I&#x27;d be ready and willing to recreate the important parts locally so that my company could stay in business. But wow, would they ever be in for some sticker shock.<p>Nice.  But I don&#x27;t think it cost as much as you think.  If you run apps on the stuff you rent and then compare it to your own hardware, it&#x27;s night and day.</div><br/></div></div></div></div></div></div><div id="39444670" class="c"><input type="checkbox" id="c-39444670" checked=""/><div class="controls bullet"><span class="by">rbranson</span><span>|</span><a href="#39444360">prev</a><span>|</span><a href="#39448649">next</a><span>|</span><label class="collapse" for="c-39444670">[-]</label><label class="expand" for="c-39444670">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s awfully similar to a lane of PCIe 4.0, if we&#x27;re talking about instance storage. It reads like they&#x27;ve chosen to map each physical device to a single PCIe lane. Surely the AWS Nitro hardware platform has longer cycle times than PCIe. Note that once the instance type has multiple block devices exposed (i.e. im4gn.8xlarge or higher), striping across these will reach higher throughput (2 devices yields 4G&#x2F;s, 4 yields 8G&#x2F;s).</div><br/></div></div><div id="39448649" class="c"><input type="checkbox" id="c-39448649" checked=""/><div class="controls bullet"><span class="by">spintin</span><span>|</span><a href="#39444670">prev</a><span>|</span><a href="#39444821">next</a><span>|</span><label class="collapse" for="c-39448649">[-]</label><label class="expand" for="c-39448649">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m going with hybrid:<p>- 2011 X-25E 64GB (2W write and almost nothing read&#x2F;idle) at 100.000 writes per bit for OS<p>- 2021 PM897 3.7TB (2.3 Watt (read) ¦ 3 Watt (write) ¦ 1.4 Watt (idle) down from the PM983 (8.7 Watt (read) ¦ 10.6 Watt (write) ¦ 4 Watt (idle)) for DB.<p>This way I can get the most robust solution, with largest DB at lowest power. They are both in a 8-core Atom Mini-ITX board at 25W TDP.</div><br/></div></div><div id="39444821" class="c"><input type="checkbox" id="c-39444821" checked=""/><div class="controls bullet"><span class="by">0cf8612b2e1e</span><span>|</span><a href="#39448649">prev</a><span>|</span><a href="#39446520">next</a><span>|</span><label class="collapse" for="c-39444821">[-]</label><label class="expand" for="c-39444821">[2 more]</label></div><br/><div class="children"><div class="content">Serious question, for a consumer does it make any sense to compare SSD benchmarks? I assume the best and worst models give a user an identical experience in 99% of cases, and it is only prosumer activities (video? sustained writes?) which would differentiate them.</div><br/><div id="39445908" class="c"><input type="checkbox" id="c-39445908" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39444821">parent</a><span>|</span><a href="#39446520">next</a><span>|</span><label class="collapse" for="c-39445908">[-]</label><label class="expand" for="c-39445908">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, that&#x27;s pretty much the case. Cheap SSDs provide good enough performance for desktop use.</div><br/></div></div></div></div><div id="39446520" class="c"><input type="checkbox" id="c-39446520" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39444821">prev</a><span>|</span><a href="#39446219">next</a><span>|</span><label class="collapse" for="c-39446520">[-]</label><label class="expand" for="c-39446520">[5 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot of talk about cloud network and disk performance in this thread. I recently benchmarked both Azure and AWS and found that:<p>- Azure network latency is about 85 microseconds.<p>- AWS network latency is about 55 microseconds.<p>- Both can do better, but only in special circumstances such as RDMA NICs in HPC clusters.<p>- Cross-VPC or cross-VNET is basically identical. Some people were saying it&#x27;s terribly slow, but I didn&#x27;t see that in my tests.<p>- Cross-zone is 300-1200 microseconds due to the inescapable speed of light delay.<p>- VM-to-VM bandwidth is over 10 Gbps (&gt;1 GB&#x2F;s) for both clouds, even for the <i>smallest</i> two vCPU VMs!<p>- Azure Premium SSD v1 latency varies between about 800 to 3,000 microseconds, which is many times worse than the network latency.<p>- Azure Premium SSD v2 latency is about 400 to 2,000 microseconds, which isn&#x27;t that much better, because:<p>- Local SSD <i>caches</i> in Azure are so much faster than remote disk that we found that Premium SSD v1 is almost always faster than Premium SSD v2 because the latter doesn&#x27;t support caching.<p>- Again in Azure, the local SSD &quot;cache&quot; and also the local &quot;temp disks&quot; both have latency as low as 40 microseconds, on par with a modern laptop NVMe drive. We found that switching to the latest-gen VM SKU and turning on the &quot;read caching&quot; for the data disks was the magic &quot;go-fast&quot; button for databases... without the risk of losing out data.<p>We investigated the various local-SSD VM SKUs in both clouds such as the Lasv3 series, and as the article mentioned, the performance delta didn&#x27;t blow my skirt up, but the data loss risk made these not worth the hassle.</div><br/><div id="39446865" class="c"><input type="checkbox" id="c-39446865" checked=""/><div class="controls bullet"><span class="by">computerdork</span><span>|</span><a href="#39446520">parent</a><span>|</span><a href="#39446219">next</a><span>|</span><label class="collapse" for="c-39446865">[-]</label><label class="expand" for="c-39446865">[4 more]</label></div><br/><div class="children"><div class="content">Interesting. And would you happen to have the numbers on the performance of the local SSD? Is it&#x27;s read and write throughput up to the level of modern SSD&#x27;s?</div><br/><div id="39447325" class="c"><input type="checkbox" id="c-39447325" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39446520">root</a><span>|</span><a href="#39446865">parent</a><span>|</span><a href="#39446219">next</a><span>|</span><label class="collapse" for="c-39447325">[-]</label><label class="expand" for="c-39447325">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty much like how the article said. The cloud local SSDs are notably slower than what you&#x27;d get in an ordinary laptop, let alone a high-end server.<p>I&#x27;m not an insider and don&#x27;t have any exclusive knowledge, but from reading a lot about the topic my impression is that the issue in both clouds is the virtualization overheads.<p>That is, having the networking or storage go through <i>any</i> hypervisor software layer is what kills the performance. I&#x27;ve seen similar numbers with on-prem VMware, Xen, and Nutanix setups as well.<p>Both clouds appear to be working on next-generation VM SKUs where the hypervisor network and storage functions are offloaded into 100% hardware, either into FPGAs or custom ASICs.<p>&quot;Azure Boost&quot; is Microsoft&#x27;s marketing name for this, and it basically amounts to both local and remote disks going through an NVMe controller directly mapped into the memory space of the VM. That is, the VM OS kernel talks <i>directly</i> to the hardware, bypassing the hypervisor completely. This is shown in their documentation diagrams: <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;azure-boost&#x2F;overview" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;azure-boost&#x2F;overview</a><p>They&#x27;re claiming up to 3.8M IOPS for a single VM, which is 3-10x what you&#x27;d get out of a single NVMe SSD stick, so... not too shabby at all!<p>Similarly, Microsoft Azure Network Adapter (MANA) is the equivalent for the NIC, which will similarly connect the VM OS directly into the network, bypassing the hypervisor software.<p>I&#x27;m not an AWS expert, but from what I&#x27;ve seen they&#x27;ve been working on similar tech (Nitro) for years.</div><br/><div id="39448240" class="c"><input type="checkbox" id="c-39448240" checked=""/><div class="controls bullet"><span class="by">computerdork</span><span>|</span><a href="#39446520">root</a><span>|</span><a href="#39447325">parent</a><span>|</span><a href="#39447827">prev</a><span>|</span><a href="#39446219">next</a><span>|</span><label class="collapse" for="c-39448240">[-]</label><label class="expand" for="c-39448240">[1 more]</label></div><br/><div class="children"><div class="content">Makes a lot of sense! Yeah, seems like for the OP&#x27;s performance-issue, you pretty much have the reason why it&#x27;s happening (VM overhead) and solutions for it (bypassing the software layer using custom hardware like Azure Boost).<p>Thanks for the info!</div><br/></div></div></div></div></div></div></div></div><div id="39446219" class="c"><input type="checkbox" id="c-39446219" checked=""/><div class="controls bullet"><span class="by">dan-robertson</span><span>|</span><a href="#39446520">prev</a><span>|</span><a href="#39445033">next</a><span>|</span><label class="collapse" for="c-39446219">[-]</label><label class="expand" for="c-39446219">[1 more]</label></div><br/><div class="children"><div class="content">Is read&#x2F;write throughput the only difference? Eg I don’t know how latency compares, or indeed failure rates or whether fsync lies or not and writes won’t always survive power failures.</div><br/></div></div><div id="39445033" class="c"><input type="checkbox" id="c-39445033" checked=""/><div class="controls bullet"><span class="by">eisa01</span><span>|</span><a href="#39446219">prev</a><span>|</span><a href="#39448004">next</a><span>|</span><label class="collapse" for="c-39445033">[-]</label><label class="expand" for="c-39445033">[3 more]</label></div><br/><div class="children"><div class="content">Would this be a consequence of the cloud providers not being on the latest technology CPU-wise?<p>At least I have the impression they are lagging, eg., still offering things like:
z1d: Skylake (2017) <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;z1d&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;z1d&#x2F;</a>
x2i: Cascade Lake (2019) and Ice lake (2021) <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;x2i&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;x2i&#x2F;</a><p>I have not been able to find instances powered by the 4th (Q1 2023) or 5th generation (Q4 2023) Xeons?<p>We solve large capacity expansion power market models that need as fast single-threaded performance as possible coupled with lots of RAM (32:1 ratio or higher ideal). One model may take 256-512 GB RAM, but not being able to use more than 4 threads effectively (interior point algorithms have very diminishing returns past this point)<p>Our dispatch models do not have the same RAM requirement, but you still wish to have the fastest single-threaded processors available (and then parallelize)</div><br/><div id="39448097" class="c"><input type="checkbox" id="c-39448097" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39445033">parent</a><span>|</span><a href="#39445455">next</a><span>|</span><label class="collapse" for="c-39448097">[-]</label><label class="expand" for="c-39448097">[1 more]</label></div><br/><div class="children"><div class="content">AWS was offering Sapphire Rapids instances before those CPUs became even publicly available<p><a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;about-aws&#x2F;whats-new&#x2F;2022&#x2F;11&#x2F;introducing-amazon-ec2-r7iz-instances&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;about-aws&#x2F;whats-new&#x2F;2022&#x2F;11&#x2F;introduci...</a></div><br/></div></div><div id="39445455" class="c"><input type="checkbox" id="c-39445455" checked=""/><div class="controls bullet"><span class="by">deadmutex</span><span>|</span><a href="#39445033">parent</a><span>|</span><a href="#39448097">prev</a><span>|</span><a href="#39448004">next</a><span>|</span><label class="collapse" for="c-39445455">[-]</label><label class="expand" for="c-39445455">[1 more]</label></div><br/><div class="children"><div class="content">You can find Intel Sapphire Rapids powered VM instances on GCE</div><br/></div></div></div></div><div id="39448004" class="c"><input type="checkbox" id="c-39448004" checked=""/><div class="controls bullet"><span class="by">Ericson2314</span><span>|</span><a href="#39445033">prev</a><span>|</span><a href="#39444207">next</a><span>|</span><label class="collapse" for="c-39448004">[-]</label><label class="expand" for="c-39448004">[1 more]</label></div><br/><div class="children"><div class="content">The cloud really is a scam for those afraid of hardware</div><br/></div></div><div id="39444207" class="c"><input type="checkbox" id="c-39444207" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39448004">prev</a><span>|</span><a href="#39443860">next</a><span>|</span><label class="collapse" for="c-39444207">[-]</label><label class="expand" for="c-39444207">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone disk benchmarks for M7gd (or C&#x2F;R equivalents) instance stores? While probably not at the level of I4g, would still be interesting comparison.</div><br/></div></div><div id="39443860" class="c"><input type="checkbox" id="c-39443860" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#39444207">prev</a><span>|</span><a href="#39445052">next</a><span>|</span><label class="collapse" for="c-39443860">[-]</label><label class="expand" for="c-39443860">[20 more]</label></div><br/><div class="children"><div class="content">What’s a good small cloud competitor to AWS? For teams that just need two AZs to get HA and your standard stuff like VMs, k8s, etc.</div><br/><div id="39444133" class="c"><input type="checkbox" id="c-39444133" checked=""/><div class="controls bullet"><span class="by">catherinecodes</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39444067">next</a><span>|</span><label class="collapse" for="c-39444133">[-]</label><label class="expand" for="c-39444133">[4 more]</label></div><br/><div class="children"><div class="content">Hetzner and Entrywan are pure-play cloud companies with good prices and support.  Hetzner is based in Germany and Entrywan in the US.</div><br/><div id="39447159" class="c"><input type="checkbox" id="c-39447159" checked=""/><div class="controls bullet"><span class="by">madars</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39444133">parent</a><span>|</span><a href="#39445397">next</a><span>|</span><label class="collapse" for="c-39447159">[-]</label><label class="expand" for="c-39447159">[1 more]</label></div><br/><div class="children"><div class="content">Hetzner has a reputation for locking accounts for &quot;identity verification&quot; (Google &quot;hetzner kyc&quot; or &quot;hetzner identity verification&quot;). Might be worthwhile to go through a reseller just to avoid downtime like that.</div><br/></div></div><div id="39445397" class="c"><input type="checkbox" id="c-39445397" checked=""/><div class="controls bullet"><span class="by">infamia</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39444133">parent</a><span>|</span><a href="#39447159">prev</a><span>|</span><a href="#39444067">next</a><span>|</span><label class="collapse" for="c-39445397">[-]</label><label class="expand" for="c-39445397">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for mentioning Entrywan, they look great from what I can tell on their site. Have you used their services? If so, I&#x27;m curious about your experiences with them.</div><br/><div id="39445695" class="c"><input type="checkbox" id="c-39445695" checked=""/><div class="controls bullet"><span class="by">catherinecodes</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39445397">parent</a><span>|</span><a href="#39444067">next</a><span>|</span><label class="collapse" for="c-39445695">[-]</label><label class="expand" for="c-39445695">[1 more]</label></div><br/><div class="children"><div class="content">My irc bouncer and two kubernetes clusters are running there.  So far the service has been good.</div><br/></div></div></div></div></div></div><div id="39444067" class="c"><input type="checkbox" id="c-39444067" checked=""/><div class="controls bullet"><span class="by">dimgl</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39444133">prev</a><span>|</span><a href="#39443905">next</a><span>|</span><label class="collapse" for="c-39444067">[-]</label><label class="expand" for="c-39444067">[1 more]</label></div><br/><div class="children"><div class="content">I am _super_ impressed with Supabase. I can see how a lot of people may not like it given how reliant it is on Postgres, but I find it to be absolutely genius.<p>It basically allows me to forego having to make a server for the CRUD operations so I can focus on the actual business implications. My REST API is automatically managed for me (mostly with lightweight views and functions) and all of my other core logic is either spread out through edge functions or in a separate data store (Redis) where I perform the more CPU intensive operations related to my business.<p>There&#x27;s some rough edges around their documentation and DX but I&#x27;m really loving it so far.</div><br/></div></div><div id="39443905" class="c"><input type="checkbox" id="c-39443905" checked=""/><div class="controls bullet"><span class="by">pritambarhate</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39444067">prev</a><span>|</span><a href="#39444164">next</a><span>|</span><label class="collapse" for="c-39443905">[-]</label><label class="expand" for="c-39443905">[1 more]</label></div><br/><div class="children"><div class="content">Digital Ocean. But they don’t have concept of multi az as far as I know. But they have multiple data centers in same region. But I am not aware if there is any private networking between DCs in the same region.</div><br/></div></div><div id="39444164" class="c"><input type="checkbox" id="c-39444164" checked=""/><div class="controls bullet"><span class="by">infecto</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39443905">prev</a><span>|</span><a href="#39443904">next</a><span>|</span><label class="collapse" for="c-39444164">[-]</label><label class="expand" for="c-39444164">[1 more]</label></div><br/><div class="children"><div class="content">AWS is pretty great and I think reasonably cheap, I would include any of the other large cloud players. The amount saved is just not reasonable enough for me, I would rather work with something that I have used over and over with.<p>Along with that, ChatGPT has knocked down most of the remaining barriers I have had when permissions get confusing in one of the cloud services.</div><br/></div></div><div id="39443904" class="c"><input type="checkbox" id="c-39443904" checked=""/><div class="controls bullet"><span class="by">ThrowawayTestr</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39444164">prev</a><span>|</span><a href="#39443924">next</a><span>|</span><label class="collapse" for="c-39443904">[-]</label><label class="expand" for="c-39443904">[8 more]</label></div><br/><div class="children"><div class="content">Buy a server</div><br/><div id="39443993" class="c"><input type="checkbox" id="c-39443993" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39443904">parent</a><span>|</span><a href="#39443924">next</a><span>|</span><label class="collapse" for="c-39443993">[-]</label><label class="expand" for="c-39443993">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t like this answer.<p>When I look at cloud, I get to think &quot;finally! No more hardware to manage. No OS to manage&quot;. It&#x27;s the best thing about the cloud, provided your workload is amenable to PaaS. It&#x27;s great because I don&#x27;t have to manage Windows or IIS. Microsoft does that part for me and significantly cheaper than it would be to employ me to do that work.</div><br/><div id="39444718" class="c"><input type="checkbox" id="c-39444718" checked=""/><div class="controls bullet"><span class="by">tjoff</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39443993">parent</a><span>|</span><a href="#39445684">next</a><span>|</span><label class="collapse" for="c-39444718">[-]</label><label class="expand" for="c-39444718">[1 more]</label></div><br/><div class="children"><div class="content">And now you have to manage the cloud instead. Which turns out to be more hassle and with no overlap with the actual problem you are trying to solve.<p>So not only do you spend time on the wrong thing you don&#x27;t even know how it works. And the providers goals are not aligned either as all they care about is locking you in.<p>How is that better?</div><br/></div></div><div id="39445684" class="c"><input type="checkbox" id="c-39445684" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39443993">parent</a><span>|</span><a href="#39444718">prev</a><span>|</span><a href="#39444176">next</a><span>|</span><label class="collapse" for="c-39445684">[-]</label><label class="expand" for="c-39445684">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>No more hardware to manage. No OS to manage</i><p>We must be using different clouds.<p>For some of the much higher-level services … maybe some semblance of that statement holds. But for VMs? Definitely not &quot;no OS to manage&quot; … the OS is usually on the customer. There might be OS-level agents from your cloud of choice that make certain operations easier … but I&#x27;m still on the hook for updates.<p>Even &quot;No machine&quot; is a stretch, though I&#x27;ve found this is much more dependent on cloud. AWS typically notices failures before I do, and by the time I notice something is up, the VM has been migrated to a new host and I&#x27;m none the wiser sans the reboot that cost. But other clouds I&#x27;ve been less lucky with: we&#x27;ve caught host failures well before the cloud provider, to an extent where I&#x27;ve wished there was a &quot;vote of no confidence&quot; API call I could make to say &quot;give me new HW, and I personally think this HW is suss&quot;.<p>Even on higher level services like RDS, or S3, I&#x27;ve noticed failures prior to AWS … or even to the extent that I don&#x27;t know that AWS would have noticed those failures unless we had opened the support ticket. (E.g., in the S3 case, even though we clearly reported the problem, and the problem was occurring on basically every request, we <i>still</i> had to provide example request IDs before they&#x27;d believe us. The service was basically in an outage as far as we could tell … though I think AWS ended up claiming it was &quot;just us&quot;.)<p>That said, S3 in particular is still an excellent service, and I&#x27;d happily use it again. But cloud == 0 time on my part. It depends heavily on the cloud, and less heavily on the service how <i>much</i> time, and sometimes, it is still worthwhile.</div><br/><div id="39450065" class="c"><input type="checkbox" id="c-39450065" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39445684">parent</a><span>|</span><a href="#39444176">next</a><span>|</span><label class="collapse" for="c-39450065">[-]</label><label class="expand" for="c-39450065">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m baffled after you quoted &#x27;no OS to manage&#x27; why you&#x27;d start discussing virtual machines.</div><br/></div></div></div></div><div id="39444176" class="c"><input type="checkbox" id="c-39444176" checked=""/><div class="controls bullet"><span class="by">cynicalsecurity</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39443993">parent</a><span>|</span><a href="#39445684">prev</a><span>|</span><a href="#39444126">next</a><span>|</span><label class="collapse" for="c-39444176">[-]</label><label class="expand" for="c-39444176">[2 more]</label></div><br/><div class="children"><div class="content">A cloud is just someone else&#x27;s computer.<p>When you rent a bare metal server, you don&#x27;t manage your hardware either. The failed parts are replaced for you. Unless you can&#x27;t  figure out what hardware configuration you need - which would be a really big red flag for your level of expertise.</div><br/><div id="39448506" class="c"><input type="checkbox" id="c-39448506" checked=""/><div class="controls bullet"><span class="by">kikimora</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39444176">parent</a><span>|</span><a href="#39444126">next</a><span>|</span><label class="collapse" for="c-39448506">[-]</label><label class="expand" for="c-39448506">[1 more]</label></div><br/><div class="children"><div class="content">A cloud is network and tools, less hardware. For example see this [1] Reddit thread discussing network in Hetzner. Any other bare-metal would have same challenges. Once you solve network security you have to deal with server access. People hired and fired, hardcoded SSH keys is a bad idea. Once you solve access you likely have AD, LDAP and SSO of some sort. Then backups, and automated test suite + periodical test recoveries. Then database and backups. Then secrets, does all members of your team know production db password? And so on and on.<p>Maybe TCO still favors bare-metal but you have to spend a lot of time on configuration.<p>[1] <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;hetzner&#x2F;comments&#x2F;rjuzcs&#x2F;securing_network_in_hetzner&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;hetzner&#x2F;comments&#x2F;rjuzcs&#x2F;securing_ne...</a></div><br/></div></div></div></div><div id="39444126" class="c"><input type="checkbox" id="c-39444126" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#39443860">root</a><span>|</span><a href="#39443993">parent</a><span>|</span><a href="#39444176">prev</a><span>|</span><a href="#39443924">next</a><span>|</span><label class="collapse" for="c-39444126">[-]</label><label class="expand" for="c-39444126">[1 more]</label></div><br/><div class="children"><div class="content">There is tooling to provide the same PaaS interface though, so your cost doing those things amounts to running OS updates.</div><br/></div></div></div></div></div></div><div id="39443924" class="c"><input type="checkbox" id="c-39443924" checked=""/><div class="controls bullet"><span class="by">prisenco</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39443904">prev</a><span>|</span><a href="#39443898">next</a><span>|</span><label class="collapse" for="c-39443924">[-]</label><label class="expand" for="c-39443924">[1 more]</label></div><br/><div class="children"><div class="content">Really needs more information for a good answer. How many req&#x2F;s are you handling? Do you have dedicated devops? Write heavy, read heavy or pipeline&#x2F;etl heavy? Is autoscaling a must or would you rather failure over a bank breaking cloud bill?</div><br/></div></div><div id="39443898" class="c"><input type="checkbox" id="c-39443898" checked=""/><div class="controls bullet"><span class="by">leroman</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39443924">prev</a><span>|</span><a href="#39445619">next</a><span>|</span><label class="collapse" for="c-39443898">[-]</label><label class="expand" for="c-39443898">[1 more]</label></div><br/><div class="children"><div class="content">DigitalOcean has been great, have managed k8s and generally good bang for your buck</div><br/></div></div><div id="39445619" class="c"><input type="checkbox" id="c-39445619" checked=""/><div class="controls bullet"><span class="by">mciancia</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39443898">prev</a><span>|</span><a href="#39448768">next</a><span>|</span><label class="collapse" for="c-39445619">[-]</label><label class="expand" for="c-39445619">[1 more]</label></div><br/><div class="children"><div class="content">I like using scaleway for personal projects, but they are available only in Europe</div><br/></div></div><div id="39448768" class="c"><input type="checkbox" id="c-39448768" checked=""/><div class="controls bullet"><span class="by">folmar</span><span>|</span><a href="#39443860">parent</a><span>|</span><a href="#39445619">prev</a><span>|</span><a href="#39445052">next</a><span>|</span><label class="collapse" for="c-39448768">[-]</label><label class="expand" for="c-39448768">[1 more]</label></div><br/><div class="children"><div class="content">Hetzner, OVH, Scaleway</div><br/></div></div></div></div><div id="39444485" class="c"><input type="checkbox" id="c-39444485" checked=""/><div class="controls bullet"><span class="by">pkstn</span><span>|</span><a href="#39445052">prev</a><span>|</span><label class="collapse" for="c-39444485">[-]</label><label class="expand" for="c-39444485">[1 more]</label></div><br/><div class="children"><div class="content">UpCloud has super fast MaxIOPS: <a href="https:&#x2F;&#x2F;upcloud.com&#x2F;products&#x2F;block-storage" rel="nofollow">https:&#x2F;&#x2F;upcloud.com&#x2F;products&#x2F;block-storage</a><p>Here&#x27;s referral link with free credits: <a href="https:&#x2F;&#x2F;upcloud.com&#x2F;signup&#x2F;?promo=J3JYWZ" rel="nofollow">https:&#x2F;&#x2F;upcloud.com&#x2F;signup&#x2F;?promo=J3JYWZ</a></div><br/></div></div></div></div></div></div></div></body></html>