<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714554069589" as="style"/><link rel="stylesheet" href="styles.css?v=1714554069589"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://zju3dv.github.io/LoG_webpage/">Level of Gaussians: Real-Time View Synthesis for Millions of Square Meters</a> <span class="domain">(<a href="https://zju3dv.github.io">zju3dv.github.io</a>)</span></div><div class="subtext"><span>corysama</span> | <span>22 comments</span></div><br/><div><div id="40219255" class="c"><input type="checkbox" id="c-40219255" checked=""/><div class="controls bullet"><span class="by">mrwyz</span><span>|</span><a href="#40215875">next</a><span>|</span><label class="collapse" for="c-40219255">[-]</label><label class="expand" for="c-40219255">[2 more]</label></div><br/><div class="children"><div class="content">Cool, but not touching this; no license and requires Inria&#x27;s proprietary rasterizer.<p>People should stop basing all of this new research on proprietary software, when we have open source implementations [1][2].<p>[1] gsplat: <a href="https:&#x2F;&#x2F;github.com&#x2F;nerfstudio-project&#x2F;gsplat">https:&#x2F;&#x2F;github.com&#x2F;nerfstudio-project&#x2F;gsplat</a>
[2] opensplat: <a href="https:&#x2F;&#x2F;github.com&#x2F;pierotofy&#x2F;opensplat">https:&#x2F;&#x2F;github.com&#x2F;pierotofy&#x2F;opensplat</a></div><br/><div id="40220483" class="c"><input type="checkbox" id="c-40220483" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#40219255">parent</a><span>|</span><a href="#40215875">next</a><span>|</span><label class="collapse" for="c-40220483">[-]</label><label class="expand" for="c-40220483">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised anything in 3D Gaussian splatting uses a rasterizer. I thought those were only used for polygonal data.</div><br/></div></div></div></div><div id="40215875" class="c"><input type="checkbox" id="c-40215875" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#40219255">prev</a><span>|</span><a href="#40216312">next</a><span>|</span><label class="collapse" for="c-40215875">[-]</label><label class="expand" for="c-40215875">[2 more]</label></div><br/><div class="children"><div class="content">Crazy good results but without the paper (which the link at the time just goes back to the site) it&#x27;s a bit difficult to check <i>how</i> good. What data is required, how long are training runs&#x2F;how many steps?</div><br/><div id="40217922" class="c"><input type="checkbox" id="c-40217922" checked=""/><div class="controls bullet"><span class="by">logtempo</span><span>|</span><a href="#40215875">parent</a><span>|</span><a href="#40216312">next</a><span>|</span><label class="collapse" for="c-40217922">[-]</label><label class="expand" for="c-40217922">[1 more]</label></div><br/><div class="children"><div class="content">Using 200 photos taken with a conventional camera, at a refresh rate of 105 frames per second - the quality of video game images - the result gives the illusion of walking through the video. Better still, if you zoom in, you can see finer details, such as the spokes of a bicycle wheel, in excellent detail.<p>It use neural network techniques, but it&#x27;s not strictly using NN.<p>it do the same result as Nerf from google in 30min, nvidia result in 7minute. It can achieve more than 100fps if you let it train longer.<p><a href="https:&#x2F;&#x2F;www.inria.fr&#x2F;fr&#x2F;3d-gaussian-splatting-vision-ordinateur-informatique-graphique" rel="nofollow">https:&#x2F;&#x2F;www.inria.fr&#x2F;fr&#x2F;3d-gaussian-splatting-vision-ordinat...</a></div><br/></div></div></div></div><div id="40216312" class="c"><input type="checkbox" id="c-40216312" checked=""/><div class="controls bullet"><span class="by">speps</span><span>|</span><a href="#40215875">prev</a><span>|</span><a href="#40217166">next</a><span>|</span><label class="collapse" for="c-40216312">[-]</label><label class="expand" for="c-40216312">[2 more]</label></div><br/><div class="children"><div class="content">Actual title is: Real-Time View Synthesis for Large Scenes with Millions of Square Meters<p>Which makes more sense than: Real-Time View Synthesis for Square Meters</div><br/><div id="40216633" class="c"><input type="checkbox" id="c-40216633" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#40216312">parent</a><span>|</span><a href="#40217166">next</a><span>|</span><label class="collapse" for="c-40216633">[-]</label><label class="expand" for="c-40216633">[1 more]</label></div><br/><div class="children"><div class="content">Title edited. Thanks. I couldn&#x27;t fit the whole title. But, didn&#x27;t think I cut out &quot;Millions of&quot;...</div><br/></div></div></div></div><div id="40217166" class="c"><input type="checkbox" id="c-40217166" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#40216312">prev</a><span>|</span><a href="#40219022">next</a><span>|</span><label class="collapse" for="c-40217166">[-]</label><label class="expand" for="c-40217166">[6 more]</label></div><br/><div class="children"><div class="content">Please Google, implement this in google maps (especially on mobile).<p>It&#x27;s been <i>over a decade</i> and we&#x27;re still stuck with 2D maps and boxy untextured buildings.</div><br/><div id="40218438" class="c"><input type="checkbox" id="c-40218438" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#40217166">parent</a><span>|</span><a href="#40219703">next</a><span>|</span><label class="collapse" for="c-40218438">[-]</label><label class="expand" for="c-40218438">[1 more]</label></div><br/><div class="children"><div class="content">The reason there isn&#x27;t much investment here is that it&#x27;s expensive to update the image data and the result isn&#x27;t very useful.<p>You barely ever need to look at 3D photogrammetry buildings for anything and there aren&#x27;t many questions it answers outside of curiosity.<p>I do wonder if they could integrate street view images into it better.</div><br/></div></div><div id="40219703" class="c"><input type="checkbox" id="c-40219703" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#40217166">parent</a><span>|</span><a href="#40218438">prev</a><span>|</span><a href="#40218372">next</a><span>|</span><label class="collapse" for="c-40219703">[-]</label><label class="expand" for="c-40219703">[1 more]</label></div><br/><div class="children"><div class="content">Google Maps has 3D (in some areas). Click on Layers -&gt; More -&gt; Globe view.<p>Looks like this: <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;wcCJmbd.png" rel="nofollow">https:&#x2F;&#x2F;i.imgur.com&#x2F;wcCJmbd.png</a></div><br/></div></div><div id="40218372" class="c"><input type="checkbox" id="c-40218372" checked=""/><div class="controls bullet"><span class="by">leodriesch</span><span>|</span><a href="#40217166">parent</a><span>|</span><a href="#40219703">prev</a><span>|</span><a href="#40218050">next</a><span>|</span><label class="collapse" for="c-40218372">[-]</label><label class="expand" for="c-40218372">[2 more]</label></div><br/><div class="children"><div class="content">I am really impressed by the Apple Maps implementation. I think it also uses textured polygons, but does so in a very good looking way and at 120 fps on an iPhone, showing even a whole city in textured 3d.</div><br/><div id="40220376" class="c"><input type="checkbox" id="c-40220376" checked=""/><div class="controls bullet"><span class="by">martinkallstrom</span><span>|</span><a href="#40217166">root</a><span>|</span><a href="#40218372">parent</a><span>|</span><a href="#40218050">next</a><span>|</span><label class="collapse" for="c-40220376">[-]</label><label class="expand" for="c-40220376">[1 more]</label></div><br/><div class="children"><div class="content">Apple bought a Swedish startup called C3 and their became 3D part of Apple Maps. That startup was a spin-off from Saab Aerospace, who had developed a vision system for terrain-following missiles. Saab ran a project with the municipal innovation agency in Linköping and the result was that they decided this tech should be possible to find civilian use cases for. C3 decided to fly small Cessnas in grids across a few major cities and also Hoover Dam, and built a ton of code on top of the already extremely solid foundation from Saab. The timing was impeccable (now many years ago) and they managed to get Microsoft, Apple and Samsung into a bidding war which drove up the price. But it was worth it for Apple to have solid 3D in Apple Maps and the tech has stood the test of time.</div><br/></div></div></div></div><div id="40218050" class="c"><input type="checkbox" id="c-40218050" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#40217166">parent</a><span>|</span><a href="#40218372">prev</a><span>|</span><a href="#40219022">next</a><span>|</span><label class="collapse" for="c-40218050">[-]</label><label class="expand" for="c-40218050">[1 more]</label></div><br/><div class="children"><div class="content">Google uses texture mapped polygons instead of 3D Gaussians, so this wouldn&#x27;t work for Google Maps. But there actually is a collection of libraries which does the same thing for polygonal data: <a href="https:&#x2F;&#x2F;vcg.isti.cnr.it&#x2F;nexus&#x2F;" rel="nofollow">https:&#x2F;&#x2F;vcg.isti.cnr.it&#x2F;nexus&#x2F;</a><p>One of the guys working on this is Federico Ponchio. His 2008 PhD thesis, which provided the core insight for Unreal Engine&#x27;s Nanite, is referenced at bottom.</div><br/></div></div></div></div><div id="40219022" class="c"><input type="checkbox" id="c-40219022" checked=""/><div class="controls bullet"><span class="by">angusturner</span><span>|</span><a href="#40217166">prev</a><span>|</span><a href="#40217154">next</a><span>|</span><label class="collapse" for="c-40219022">[-]</label><label class="expand" for="c-40219022">[5 more]</label></div><br/><div class="children"><div class="content">Can anyone familiar with 3d graphics speculate what would be required to implement this into a game engine?<p>I&#x27;m guessing that adding physics, collision-detection etc. on top of this is non-trivial compared to using a mesh?<p>But I feel like for stuff like tree foliage (where maybe you don&#x27;t care about collisions?), this would be really awesome, given the limitations of polygons. + also just any like background scenery, stuff out of the player&#x27;s reach.</div><br/><div id="40219137" class="c"><input type="checkbox" id="c-40219137" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40219022">parent</a><span>|</span><a href="#40219132">next</a><span>|</span><label class="collapse" for="c-40219137">[-]</label><label class="expand" for="c-40219137">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easy to render these in a game engine. I&#x27;m sure physics and collision detection are possible. The big, huge, gigantic issue is actually lighting.<p>These scenes come with real world lighting baked in. This is great because it looks amazing, it&#x27;s 100% correct, far better than the lighting computed by any game engine or even movie-quality offline ray tracer. This is a big part of why they look so good! But it&#x27;s also a curse. Games need to be interactive. When things move, lighting changes. Even something as simple as opening a door can have a profound effect on lighting. Anything that moves changes the lighting on itself <i>and</i> everything around it. Let alone moving actual lights around, changing the time of day, etc.<p>There&#x27;s absolutely no way to change the baked-in lighting in one of these captures in a high quality way. I&#x27;ve seen several papers that attempt it and the results all suck. It&#x27;s not the fault of the researchers, it&#x27;s a very hard problem. There are two main issues:<p>One, in order to perfectly re-light a scene you first have to de-light it, that is, compute the lighting-independent BRDF of every surface. The capture itself doesn&#x27;t even contain enough information to do this in an unambiguous way. You can&#x27;t know for sure how a surface would react under different lighting conditions than were present in the pictures that made up the original scan. Maybe in theory you can guess well enough in most cases and extrapolate, and AI can likely help a lot here, but in practice we are far away from good quality so far.<p>Two, given the BRDF of all surfaces and a set of new lights, you have to apply the new lighting to the scene. Real-time solutions for lighting are <i>very</i> approximate and won&#x27;t be anywhere near the quality of the lighting in the original scan. So you&#x27;ll lose some of that photorealistic quality when you do this, even if your BRDFs are perfect (they won&#x27;t be). It will end up looking like regular game graphics instead of the picture-perfect scans you want. If you try to blend the new lighting with the original lighting, the boundaries will probably be obvious. You&#x27;re competing with perfection! Even offline rendering would struggle to match the quality of the baked-in lighting in these captures.<p>To me the ultimate solution needs to involve AI. Analytically relighting everything perfectly is infeasible, but AI can likely do approximate lighting that looks more plausible in most cases, especially when trying to match captured natural lighting. I&#x27;m not sure exactly how it will work, but AI is already being used in rendering and its use will only increase.</div><br/><div id="40219596" class="c"><input type="checkbox" id="c-40219596" checked=""/><div class="controls bullet"><span class="by">rallyforthesun</span><span>|</span><a href="#40219022">root</a><span>|</span><a href="#40219137">parent</a><span>|</span><a href="#40219132">next</a><span>|</span><label class="collapse" for="c-40219596">[-]</label><label class="expand" for="c-40219596">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for pointing out the challenges with gaussian splattings. Are there any AI based relighting methods out there?
Some prompt based editing like nerf2nerf or Language-embedded NerFs maybe?</div><br/></div></div></div></div><div id="40219132" class="c"><input type="checkbox" id="c-40219132" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#40219022">parent</a><span>|</span><a href="#40219137">prev</a><span>|</span><a href="#40219442">next</a><span>|</span><label class="collapse" for="c-40219132">[-]</label><label class="expand" for="c-40219132">[1 more]</label></div><br/><div class="children"><div class="content">I worked in game engines for a long time. The main hurdle is just that it’s new. There’s a many-decade legacy pipeline of tools and techniques built around triangles.  Splats are something new.<p>The good news is that splats are really simple once they’ve been generated. Maybe simpler than triangles depending on how you look at it. It’s just a matter of doing the work to set up new tools and pipelines.</div><br/></div></div><div id="40219442" class="c"><input type="checkbox" id="c-40219442" checked=""/><div class="controls bullet"><span class="by">gct</span><span>|</span><a href="#40219022">parent</a><span>|</span><a href="#40219132">prev</a><span>|</span><a href="#40217154">next</a><span>|</span><label class="collapse" for="c-40219442">[-]</label><label class="expand" for="c-40219442">[1 more]</label></div><br/><div class="children"><div class="content">Given they&#x27;re indexing into a tree, animation will be a pain.</div><br/></div></div></div></div><div id="40217154" class="c"><input type="checkbox" id="c-40217154" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#40219022">prev</a><span>|</span><a href="#40215781">next</a><span>|</span><label class="collapse" for="c-40217154">[-]</label><label class="expand" for="c-40217154">[1 more]</label></div><br/><div class="children"><div class="content">I hope the next-gen google earth looks something like this.</div><br/></div></div><div id="40215781" class="c"><input type="checkbox" id="c-40215781" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#40217154">prev</a><span>|</span><a href="#40217921">next</a><span>|</span><label class="collapse" for="c-40215781">[-]</label><label class="expand" for="c-40215781">[1 more]</label></div><br/><div class="children"><div class="content">I got excited because the code was just released. But, apparently the paper is still not available? Sorry...</div><br/></div></div><div id="40217921" class="c"><input type="checkbox" id="c-40217921" checked=""/><div class="controls bullet"><span class="by">lend000</span><span>|</span><a href="#40215781">prev</a><span>|</span><a href="#40219020">next</a><span>|</span><label class="collapse" for="c-40217921">[-]</label><label class="expand" for="c-40217921">[1 more]</label></div><br/><div class="children"><div class="content">Looks even better than Microsoft flight simulator. Awesome!</div><br/></div></div><div id="40219020" class="c"><input type="checkbox" id="c-40219020" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40217921">prev</a><span>|</span><label class="collapse" for="c-40219020">[-]</label><label class="expand" for="c-40219020">[1 more]</label></div><br/><div class="children"><div class="content">So this  is just Level-of-Detail (LoD) implemented for Gaussian splats? Impressive results, but I would have figured this is an obvious next-step...<p>Also, is it bad that the first thing I thought of was that commanders in the Ukraine war could use this? E.g.: stitch together the video streams from thousands of drones to build up an up-to-date view of the battlefield?</div><br/></div></div></div></div></div></div></div></body></html>