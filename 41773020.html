<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728723674136" as="style"/><link rel="stylesheet" href="styles.css?v=1728723674136"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/anordin95/run-llama-locally">Run Llama locally with only PyTorch on CPU</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>anordin95</span> | <span>31 comments</span></div><br/><div><div id="41810695" class="c"><input type="checkbox" id="c-41810695" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41812018">next</a><span>|</span><label class="collapse" for="c-41810695">[-]</label><label class="expand" for="c-41810695">[23 more]</label></div><br/><div class="children"><div class="content">If your goal is<p>&gt; I want to peel back the layers of the onion and other gluey-mess to gain insight into these models.<p>Then this is great.<p>If your goal is<p>&gt; Run and explore Llama models locally with minimal dependencies on CPU<p>then I recommend <a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile</a> which ships as a single file with no dependencies and runs on CPU with great performance. Like, such great performance that I&#x27;ve mostly given up on GPU for LLMs. It was a game changer.</div><br/><div id="41810831" class="c"><input type="checkbox" id="c-41810831" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#41810695">parent</a><span>|</span><a href="#41810936">next</a><span>|</span><label class="collapse" for="c-41810831">[-]</label><label class="expand" for="c-41810831">[7 more]</label></div><br/><div class="children"><div class="content">Ollama (also wrapping llama.cpp) has GPU support, unless you&#x27;re really in love with the idea of bundling weights into the inference executable probably a better choice for most people.</div><br/><div id="41810988" class="c"><input type="checkbox" id="c-41810988" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41810831">parent</a><span>|</span><a href="#41810869">next</a><span>|</span><label class="collapse" for="c-41810988">[-]</label><label class="expand" for="c-41810988">[5 more]</label></div><br/><div class="children"><div class="content">Ollama is great if you&#x27;re really in love with the idea of having your multi gigabyte models (likely the majority of your disk space) stored in obfuscated UUID filenames. Ollama also still hasn&#x27;t addressed the license violations I reported to them back in March. <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;3185">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;3185</a></div><br/><div id="41811238" class="c"><input type="checkbox" id="c-41811238" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41810988">parent</a><span>|</span><a href="#41812661">next</a><span>|</span><label class="collapse" for="c-41811238">[-]</label><label class="expand" for="c-41811238">[1 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t aware of the license issue, wow. Not a good look especially considering how simple that is to resolve.<p>The model storage doesn&#x27;t bother me but I also use Docker so I&#x27;m used to having a lot of tool-managed data to deal with. YMMV.<p>Edit: Removed question about GPU support.</div><br/></div></div><div id="41812661" class="c"><input type="checkbox" id="c-41812661" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41810988">parent</a><span>|</span><a href="#41811238">prev</a><span>|</span><a href="#41817092">next</a><span>|</span><label class="collapse" for="c-41812661">[-]</label><label class="expand" for="c-41812661">[2 more]</label></div><br/><div class="children"><div class="content">I think this is also a problem in a lot of tools, that is never talked about.<p>Even myself I’ve not thought about this so deeply, even though I am also very concerned about honoring other people’s work and that licenses are followed.<p>I have some command line tools for example that I’ve written in Rust that depend on various libraries. But because I distribute my software in source form mostly, I haven’t really paid attention to how a command-line tool which is distributed as a compiled binary would make sure to include attribution and copies of the licenses of its dependencies.<p>And so the main place where I’ve given more thought to those concerns is for example in full-blown GUI apps. There they usually have an about menu that will include info about their dependencies. And the other part where I’ve thought about it is in commercial electronics making use of open source software in their firmware. In those physical products they usually include either some printed documents alongside the product where attributions and license texts are sometimes found, and sometimes if the product has a display, or a display output, they have a menu you can find somewhere with that sort of info.<p>I know that for example Debian is very good at being thorough with details about licenses, but I’ve never looked at what they do with command line tools that compile third-party code into them. Like does Debian package maintainers then for example dig up copies of the licenses from the source and dependencies and put them somewhere in &#x2F;usr&#x2F;share&#x2F; as plain text files? Or do the .deb files themselves contain license text copies you can view but which are not installed onto the system? Or they work with software authors to add a flag that will show licenses? Or something else?</div><br/><div id="41815170" class="c"><input type="checkbox" id="c-41815170" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41812661">parent</a><span>|</span><a href="#41817092">next</a><span>|</span><label class="collapse" for="c-41815170">[-]</label><label class="expand" for="c-41815170">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s really something that should be abstracted by the linker. Codebases like zlib for example will just put a `const char notice[] = &quot;Copyright Adler et al&quot;;` in one of their files so the license issue with zlib is solved simply by using zlib. However modern linkers have gotten so good that -fdata-sections -Wl,--gc-sections will strip that away and probably LTO too. In Cosmopolitan Libc I used to use the GNU assembler `.ident` directive in an asm() tag at the tops of .c files to automate license compliance. But some changes to ld.bfd ended up breaking that. Now I have to use these custom defines like <a href="https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;cosmopolitan&#x2F;blob&#x2F;706cb6631021bbe7b125dca62247be41fcee5744&#x2F;libc&#x2F;tinymath&#x2F;acos.c#L30">https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;cosmopolitan&#x2F;blob&#x2F;706cb6631021bbe7b1...</a> and <a href="https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;cosmopolitan&#x2F;blob&#x2F;706cb6631021bbe7b125dca62247be41fcee5744&#x2F;libc&#x2F;intrin&#x2F;fdlibm.c#L1">https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;cosmopolitan&#x2F;blob&#x2F;706cb6631021bbe7b1...</a> and <a href="https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;cosmopolitan&#x2F;blob&#x2F;706cb6631021bbe7b125dca62247be41fcee5744&#x2F;libc&#x2F;integral&#x2F;c.inc#L650">https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;cosmopolitan&#x2F;blob&#x2F;706cb6631021bbe7b1...</a> to get the job done. It really should be a language feature so that library authors can make it as simple as possible for users to comply with their license. I just don&#x27;t think I&#x27;ve ever seen anyone think about it this way except for maybe Google&#x27;s JavaScript minifiers, which is where I got the idea.</div><br/></div></div></div></div><div id="41817092" class="c"><input type="checkbox" id="c-41817092" checked=""/><div class="controls bullet"><span class="by">gertop</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41810988">parent</a><span>|</span><a href="#41812661">prev</a><span>|</span><a href="#41810869">next</a><span>|</span><label class="collapse" for="c-41817092">[-]</label><label class="expand" for="c-41817092">[1 more]</label></div><br/><div class="children"><div class="content">Llamafile is great if you don&#x27;t want to run any meaningful models because it&#x27;s limited to 4GB.</div><br/></div></div></div></div><div id="41810869" class="c"><input type="checkbox" id="c-41810869" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41810831">parent</a><span>|</span><a href="#41810988">prev</a><span>|</span><a href="#41810936">next</a><span>|</span><label class="collapse" for="c-41810869">[-]</label><label class="expand" for="c-41810869">[1 more]</label></div><br/><div class="children"><div class="content">When I said<p>&gt; such great performance that I&#x27;ve mostly given up on GPU for LLMs<p>I mean I used to run ollama on GPU, but llamafile was approximately the same performance on just CPU so I switched. Now that might just be because my GPU is weak by current standards, but that is in fact the comparison I was making.<p>Edit: Though to be clear, ollama would easily be my second pick; it also has minimal dependencies and is super easy to run locally.</div><br/></div></div></div></div><div id="41810936" class="c"><input type="checkbox" id="c-41810936" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#41810695">parent</a><span>|</span><a href="#41810831">prev</a><span>|</span><a href="#41816664">next</a><span>|</span><label class="collapse" for="c-41810936">[-]</label><label class="expand" for="c-41810936">[2 more]</label></div><br/><div class="children"><div class="content">A great place to start is with the LLaMA 3.2 q6 llamafile I posted a few days ago. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Mozilla&#x2F;Llama-3.2-3B-Instruct-llamafile" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Mozilla&#x2F;Llama-3.2-3B-Instruct-llamafi...</a> We have a new CLI chatbot interface that&#x27;s really fun to use. Syntax highlighting and all. You can also use GPU by passing the -ngl 999 flag.</div><br/><div id="41811094" class="c"><input type="checkbox" id="c-41811094" checked=""/><div class="controls bullet"><span class="by">cromka</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41810936">parent</a><span>|</span><a href="#41816664">next</a><span>|</span><label class="collapse" for="c-41811094">[-]</label><label class="expand" for="c-41811094">[1 more]</label></div><br/><div class="children"><div class="content">„On <i>Windows</i>, only the graphics card driver needs to be installed if you own an NVIDIA GPU. On <i>Windows</i>, if you have an AMD GPU, you should install the ROCm SDK v6.1 and then pass the flags --recompile --gpu amd the first time you run your llamafile.”<p>Looks like there’s a typo, Windows is mentioned twice.</div><br/></div></div></div></div><div id="41816664" class="c"><input type="checkbox" id="c-41816664" checked=""/><div class="controls bullet"><span class="by">anordin95</span><span>|</span><a href="#41810695">parent</a><span>|</span><a href="#41810936">prev</a><span>|</span><a href="#41812025">next</a><span>|</span><label class="collapse" for="c-41816664">[-]</label><label class="expand" for="c-41816664">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the suggestion. I&#x27;ve added a link to llamafile in the repo&#x27;s README. Though, my focus was on exploring the model itself.</div><br/></div></div><div id="41812025" class="c"><input type="checkbox" id="c-41812025" checked=""/><div class="controls bullet"><span class="by">rmbyrro</span><span>|</span><a href="#41810695">parent</a><span>|</span><a href="#41816664">prev</a><span>|</span><a href="#41813917">next</a><span>|</span><label class="collapse" for="c-41812025">[-]</label><label class="expand" for="c-41812025">[4 more]</label></div><br/><div class="children"><div class="content">Do you have a ballpark idea of how much RAM would be necessary to run llama 3.1 8b and 70b on 8-quant?</div><br/><div id="41813799" class="c"><input type="checkbox" id="c-41813799" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41812025">parent</a><span>|</span><a href="#41813917">next</a><span>|</span><label class="collapse" for="c-41813799">[-]</label><label class="expand" for="c-41813799">[3 more]</label></div><br/><div class="children"><div class="content">Roughly, at Q8 the model sizes translate to GB, so ~3 and ~70GB.</div><br/><div id="41816787" class="c"><input type="checkbox" id="c-41816787" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41813799">parent</a><span>|</span><a href="#41813917">next</a><span>|</span><label class="collapse" for="c-41816787">[-]</label><label class="expand" for="c-41816787">[2 more]</label></div><br/><div class="children"><div class="content">You mean 8, not 3?</div><br/><div id="41817406" class="c"><input type="checkbox" id="c-41817406" checked=""/><div class="controls bullet"><span class="by">karolist</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41816787">parent</a><span>|</span><a href="#41813917">next</a><span>|</span><label class="collapse" for="c-41817406">[-]</label><label class="expand" for="c-41817406">[1 more]</label></div><br/><div class="children"><div class="content">Yes, apologies, can&#x27;t edit now</div><br/></div></div></div></div></div></div></div></div><div id="41813917" class="c"><input type="checkbox" id="c-41813917" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#41810695">parent</a><span>|</span><a href="#41812025">prev</a><span>|</span><a href="#41811612">next</a><span>|</span><label class="collapse" for="c-41813917">[-]</label><label class="expand" for="c-41813917">[3 more]</label></div><br/><div class="children"><div class="content">How great is the performance? Tokens&#x2F;s?</div><br/><div id="41816094" class="c"><input type="checkbox" id="c-41816094" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41813917">parent</a><span>|</span><a href="#41811612">next</a><span>|</span><label class="collapse" for="c-41816094">[-]</label><label class="expand" for="c-41816094">[2 more]</label></div><br/><div class="children"><div class="content">Random sample query (&quot;What shape should a kumquat be?&quot;) against a 7B model quantised to 4b running on an i7-9750H (so a good CPU, but also a good <i>laptop</i> CPU from 2019) gives:<p><pre><code>  148 tokens predicted, 159 ms per token, 6.27 tokens per second</code></pre></div><br/><div id="41816145" class="c"><input type="checkbox" id="c-41816145" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41816094">parent</a><span>|</span><a href="#41811612">next</a><span>|</span><label class="collapse" for="c-41816145">[-]</label><label class="expand" for="c-41816145">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, that helps.</div><br/></div></div></div></div></div></div><div id="41811612" class="c"><input type="checkbox" id="c-41811612" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#41810695">parent</a><span>|</span><a href="#41813917">prev</a><span>|</span><a href="#41812156">next</a><span>|</span><label class="collapse" for="c-41811612">[-]</label><label class="expand" for="c-41811612">[3 more]</label></div><br/><div class="children"><div class="content">Can it use GPU if available, say on Apple silicon Macs</div><br/><div id="41811717" class="c"><input type="checkbox" id="c-41811717" checked=""/><div class="controls bullet"><span class="by">unkeen</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41811612">parent</a><span>|</span><a href="#41812156">next</a><span>|</span><label class="collapse" for="c-41811717">[-]</label><label class="expand" for="c-41811717">[2 more]</label></div><br/><div class="children"><div class="content">&gt; GPU on MacOS ARM64 is supported by compiling a small module using the Xcode Command Line Tools, which need to be installed. This is a one time cost that happens the first time you run your llamafile.</div><br/><div id="41814284" class="c"><input type="checkbox" id="c-41814284" checked=""/><div class="controls bullet"><span class="by">xyc</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41811717">parent</a><span>|</span><a href="#41812156">next</a><span>|</span><label class="collapse" for="c-41814284">[-]</label><label class="expand" for="c-41814284">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if it&#x27;s possible for llamafile to distribute without the need for Xcode Command Line Tools, but perhaps it&#x27;s necessary for the single cross-platform binary.<p>Loved llamafile and used it to build the first version of <a href="https:&#x2F;&#x2F;recurse.chat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;recurse.chat&#x2F;</a>, but live compilation using XCode Command Line Tool is a no-go for Mac App Store builds (runs in Mac App Sandbox). llama.cpp doesn&#x27;t need compiling on user&#x27;s machine fwiw.</div><br/></div></div></div></div></div></div><div id="41812156" class="c"><input type="checkbox" id="c-41812156" checked=""/><div class="controls bullet"><span class="by">AlfredBarnes</span><span>|</span><a href="#41810695">parent</a><span>|</span><a href="#41811612">prev</a><span>|</span><a href="#41812018">next</a><span>|</span><label class="collapse" for="c-41812156">[-]</label><label class="expand" for="c-41812156">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for posting this!</div><br/><div id="41816591" class="c"><input type="checkbox" id="c-41816591" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41810695">root</a><span>|</span><a href="#41812156">parent</a><span>|</span><a href="#41812018">next</a><span>|</span><label class="collapse" for="c-41816591">[-]</label><label class="expand" for="c-41816591">[1 more]</label></div><br/><div class="children"><div class="content">Very happy to have helped:)</div><br/></div></div></div></div></div></div><div id="41812018" class="c"><input type="checkbox" id="c-41812018" checked=""/><div class="controls bullet"><span class="by">Ship_Star_1010</span><span>|</span><a href="#41810695">prev</a><span>|</span><a href="#41813608">next</a><span>|</span><label class="collapse" for="c-41812018">[-]</label><label class="expand" for="c-41812018">[2 more]</label></div><br/><div class="children"><div class="content">PyTorch has a native llm solution 
It supports all the LLama models. It supports CPU, MPS and CUDA
<a href="https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;torchchat">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;torchchat</a>
Getting 4.5 tokens a second using 3.1 8B full precision using CPU only on my M1</div><br/><div id="41812227" class="c"><input type="checkbox" id="c-41812227" checked=""/><div class="controls bullet"><span class="by">ajaksalad</span><span>|</span><a href="#41812018">parent</a><span>|</span><a href="#41813608">next</a><span>|</span><label class="collapse" for="c-41812227">[-]</label><label class="expand" for="c-41812227">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I was a bit surprised Meta didn&#x27;t publish an example way to simply invoke one of these LLM&#x27;s with only torch (or some minimal set of dependencies)<p>Seems like torchchat is exactly what the author was looking for.<p>&gt; And the 8B model typically gets killed by the OS for using too much memory.<p>Torchchat also provides some quantization options so you can reduce the model size to fit into memory.</div><br/></div></div></div></div><div id="41813608" class="c"><input type="checkbox" id="c-41813608" checked=""/><div class="controls bullet"><span class="by">I_am_tiberius</span><span>|</span><a href="#41812018">prev</a><span>|</span><a href="#41812231">next</a><span>|</span><label class="collapse" for="c-41813608">[-]</label><label class="expand" for="c-41813608">[2 more]</label></div><br/><div class="children"><div class="content">Does anyone know what&#x27;s the easiest way to finetune a model locally is today?</div><br/><div id="41816772" class="c"><input type="checkbox" id="c-41816772" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41813608">parent</a><span>|</span><a href="#41812231">next</a><span>|</span><label class="collapse" for="c-41816772">[-]</label><label class="expand" for="c-41816772">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;axolotl-ai-cloud&#x2F;axolotl">https:&#x2F;&#x2F;github.com&#x2F;axolotl-ai-cloud&#x2F;axolotl</a></div><br/></div></div></div></div><div id="41812231" class="c"><input type="checkbox" id="c-41812231" checked=""/><div class="controls bullet"><span class="by">tcdent</span><span>|</span><a href="#41813608">prev</a><span>|</span><a href="#41811051">next</a><span>|</span><label class="collapse" for="c-41812231">[-]</label><label class="expand" for="c-41812231">[1 more]</label></div><br/><div class="children"><div class="content">&gt; from llama_models.llama3.reference_impl.model import Transformer<p>This just imports the Llama reference implementation and patches the device FYI.<p>There are more robust implementations out there.</div><br/></div></div><div id="41811051" class="c"><input type="checkbox" id="c-41811051" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41812231">prev</a><span>|</span><a href="#41773021">next</a><span>|</span><label class="collapse" for="c-41811051">[-]</label><label class="expand" for="c-41811051">[1 more]</label></div><br/><div class="children"><div class="content">With the same mindset, but without even PyTorch as dependency there&#x27;s a straightforward CPU implementation of llama&#x2F;gemma in Rust: <a href="https:&#x2F;&#x2F;github.com&#x2F;samuel-vitorino&#x2F;lm.rs&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;samuel-vitorino&#x2F;lm.rs&#x2F;</a><p>It&#x27;s impressive to realize how little code is needed to run these models at all.</div><br/></div></div><div id="41773021" class="c"><input type="checkbox" id="c-41773021" checked=""/><div class="controls bullet"><span class="by">anordin95</span><span>|</span><a href="#41811051">prev</a><span>|</span><label class="collapse" for="c-41773021">[-]</label><label class="expand" for="c-41773021">[1 more]</label></div><br/><div class="children"><div class="content">Peel back the layers of the onion and other gluey-mess to gain insight into these models.</div><br/></div></div></div></div></div></div></div></body></html>