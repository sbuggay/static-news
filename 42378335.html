<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733907673899" as="style"/><link rel="stylesheet" href="styles.css?v=1733907673899"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2412.06769">Training LLMs to Reason in a Continuous Latent Space</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>omarsar</span> | <span>84 comments</span></div><br/><div><div id="42380033" class="c"><input type="checkbox" id="c-42380033" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#42385988">next</a><span>|</span><label class="collapse" for="c-42380033">[-]</label><label class="expand" for="c-42380033">[15 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been looking into using the last hidden layer of an off-the-shelf LLM to help my company with a classification task. The last hidden layer is obviously super rich in semantic information because it has to somehow tell the next layer how to generate the next token prediction. That final layer, in some respects, is discarding valuable context information that the final hidden layer encodes.<p>I am not surprised at all that Meta was able to generate some positive returns by feeding the last hidden layer back into the model auto-regressively.<p>The method of training they describe in the paper is really cool. Summarized in Figure 2, they train it with a corpus of step-by-step text instructions and then across multiple stages, they iteratively replace one of the textual steps with a last-hidden-layer embedding and see what the model spits out. The weights are then updated through cross-entropy loss as the additional text tokens are generated once again.<p>So they&#x27;re basically rewinding the output, replacing an increasing number of textual steps with hidden state embeddings, and playing it forward as the model gradually learns to do all of its step-by-step thinking using just the hidden state data.<p>In a way, this might be how humans learn to think through language. Our parents teach us using words and our brain gradually replaces the words with thoughts until we can replicate the action or solve the problem ourselves without anyone guiding us with words.</div><br/><div id="42380081" class="c"><input type="checkbox" id="c-42380081" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#42380033">parent</a><span>|</span><a href="#42382309">next</a><span>|</span><label class="collapse" for="c-42380081">[-]</label><label class="expand" for="c-42380081">[3 more]</label></div><br/><div class="children"><div class="content">Indeed, I would not be surprised if OpenAI one day admits that the `o1` model uses the last hidden layer (or some other intermediate layer) to feed the &quot;thought process&quot; that you can watch as it &quot;thinks&quot; about the answer. I suspect that they may take the last hidden layer and feed it back into the front of the `o1` model while also feeding a separate, likely much smaller LLM that generates the &quot;thought process&quot; as language tokens.<p>In this manner, the model makes use of the rich semantic information encoded at the last hidden layer while informing the user via an extraction of that hidden layer specifically tuned to generate human-legible concepts such as, &quot;I&#x27;m considering the impact of converting the units from kilograms to pounds,&quot; or whatever.</div><br/><div id="42382267" class="c"><input type="checkbox" id="c-42382267" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42380081">parent</a><span>|</span><a href="#42380891">next</a><span>|</span><label class="collapse" for="c-42382267">[-]</label><label class="expand" for="c-42382267">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it does, because from this paper this kind of  backfeeding is apparently quite difficult to train.<p>I&#x27;ve said it before, but I think it&#x27;s just something like Quiet-STaR, but simplified. They have a bunch of question answer pairs, many of which are difficult. They generate a lot of tokens from the question (let&#x27;s say, 3x the length of the expected answer), summarise whatever is generated and reinforce whenever it generates the right answer.<p>I don&#x27;t think o1 is something complicated.</div><br/></div></div><div id="42380891" class="c"><input type="checkbox" id="c-42380891" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42380081">parent</a><span>|</span><a href="#42382267">prev</a><span>|</span><a href="#42382309">next</a><span>|</span><label class="collapse" for="c-42380891">[-]</label><label class="expand" for="c-42380891">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s certainly possible, but it reminds me a bit of a similar thing I&#x27;ve seen in their UI that rhymes in a way that makes me think otherwise. In the code interpreter tool, you have a little preview of the &quot;steps&quot; it&#x27;s following as it writes code. This turns out to just be the contents of the last written&#x2F;streamed comment line. It&#x27;s a neat UI idea I think - pretty simple and works well. I wouldn&#x27;t be surprised if that&#x27;s what&#x27;s going on with o1 too - the thought process is structured in some way, and they take the headings or section names and just display that.</div><br/></div></div></div></div><div id="42382309" class="c"><input type="checkbox" id="c-42382309" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#42380033">parent</a><span>|</span><a href="#42380081">prev</a><span>|</span><a href="#42380668">next</a><span>|</span><label class="collapse" for="c-42382309">[-]</label><label class="expand" for="c-42382309">[1 more]</label></div><br/><div class="children"><div class="content">&gt; using the last hidden layer<p>iirc this is a well supported task iirc called &quot;classification  head&quot; instead of &quot;language modeling head&quot; in case anyone else wants to do this as a fine-tuning project</div><br/></div></div><div id="42380668" class="c"><input type="checkbox" id="c-42380668" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#42380033">parent</a><span>|</span><a href="#42382309">prev</a><span>|</span><a href="#42384898">next</a><span>|</span><label class="collapse" for="c-42380668">[-]</label><label class="expand" for="c-42380668">[3 more]</label></div><br/><div class="children"><div class="content">This is intriguing. When I learned that a lot of people do not have inner monologue, I was fascinated by the fact that people can differ on such seemingly fundamental way of being. Maybe those who have it just have a &quot;tee&quot; that pipes into words.</div><br/><div id="42385632" class="c"><input type="checkbox" id="c-42385632" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42380668">parent</a><span>|</span><a href="#42384898">next</a><span>|</span><label class="collapse" for="c-42385632">[-]</label><label class="expand" for="c-42385632">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not convinced they don&#x27;t. Ask them what they do when they read. That&#x27;s all an inner monologue is.</div><br/><div id="42385934" class="c"><input type="checkbox" id="c-42385934" checked=""/><div class="controls bullet"><span class="by">taylorius</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42385632">parent</a><span>|</span><a href="#42384898">next</a><span>|</span><label class="collapse" for="c-42385934">[-]</label><label class="expand" for="c-42385934">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true - though I think of an inner monologue as being more &quot;self driven&quot;. Perhaps it&#x27;s just that their mental voices don&#x27;t spontaneously say anything.</div><br/></div></div></div></div></div></div><div id="42384898" class="c"><input type="checkbox" id="c-42384898" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#42380033">parent</a><span>|</span><a href="#42380668">prev</a><span>|</span><a href="#42380655">next</a><span>|</span><label class="collapse" for="c-42384898">[-]</label><label class="expand" for="c-42384898">[1 more]</label></div><br/><div class="children"><div class="content">BTW, people found that in-conext instruction is useful for these (for example, directly using the last hidden layer to condition a diffusion model is much worse than encoder-decoder model, but you can add instruction prefix &quot;try to imagine more details with the following text: &lt;prompt&gt;&quot; would enrich the last hidden layer vector to be superior than the encoder-decoder text features. Very interesting stuff.</div><br/></div></div><div id="42380655" class="c"><input type="checkbox" id="c-42380655" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#42380033">parent</a><span>|</span><a href="#42384898">prev</a><span>|</span><a href="#42385988">next</a><span>|</span><label class="collapse" for="c-42380655">[-]</label><label class="expand" for="c-42380655">[6 more]</label></div><br/><div class="children"><div class="content">&quot;...because it has to somehow tell the next layer how to generate the next token prediction.&quot; -- This isn&#x27;t actually true in the case of transformers. Features in the final TF layer at time t in a sequence do not depend on the features in the final TF layer at any other time step. Recurrence in transformers is done &quot;depthwise&quot; via &quot;causally masked&quot; convolutions. Final layer features at time t can depend on penultimate layer features at time t-1, but not on final layer features at time t-1.</div><br/><div id="42382154" class="c"><input type="checkbox" id="c-42382154" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42380655">parent</a><span>|</span><a href="#42385988">next</a><span>|</span><label class="collapse" for="c-42382154">[-]</label><label class="expand" for="c-42382154">[5 more]</label></div><br/><div class="children"><div class="content">you are misunderstanding what the person is saying. They are saying the final hidden layer outputs a vector which has all the information that decides the logits which decide the probabilities of <i>each token in the entire vocabulary</i>. Ie, it is storing a lot of information.</div><br/><div id="42384484" class="c"><input type="checkbox" id="c-42384484" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42382154">parent</a><span>|</span><a href="#42385988">next</a><span>|</span><label class="collapse" for="c-42384484">[-]</label><label class="expand" for="c-42384484">[4 more]</label></div><br/><div class="children"><div class="content">Correct. And although the final layer outputs a softmax of the token probabilities, the model by that point surely has a rich understanding of more than just the next token it wants to predict.</div><br/><div id="42384933" class="c"><input type="checkbox" id="c-42384933" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42384484">parent</a><span>|</span><a href="#42384667">next</a><span>|</span><label class="collapse" for="c-42384933">[-]</label><label class="expand" for="c-42384933">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  surely has a rich understanding of more than just the next token it wants to predict<p>&gt; the last hidden layer is obviously super rich in semantic information<p>I don&#x27;t agree that this is obvious, and think it&#x27;s likely wrong (see the sibling thread [1]). The model has to at some point compress down its prediction for the entire future string of text to a prediction for a single token. There&#x27;s no prior reason to assume it does this mostly in the final &quot;LM head&quot; linear layer, and the inputs to it <i>don&#x27;t</i> have to predict anything other than the very next token so there&#x27;s no reason it should (which is what I think psb217 was getting at), but I&#x27;m not familiar with what research has been done into it. On the other hand, processing seems to typically be concentrated in the central layers.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42379167">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42379167</a></div><br/><div id="42385749" class="c"><input type="checkbox" id="c-42385749" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42384933">parent</a><span>|</span><a href="#42384667">next</a><span>|</span><label class="collapse" for="c-42385749">[-]</label><label class="expand" for="c-42385749">[1 more]</label></div><br/><div class="children"><div class="content">The last hidden layer outputs a vector which is then used to predict the probabilities of every token in the vocabulary, by a single layer (and, in practice now in llama models, this layer is the transpose of the embedding layer).<p>That vector has a lot of information in it, it&#x27;s not a debatable thing.<p>As noted above in parens, look at the llama 3.x models. The space is already shared in some sense. It&#x27;s called &quot;tied embedding&quot;.</div><br/></div></div></div></div><div id="42384667" class="c"><input type="checkbox" id="c-42384667" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42380033">root</a><span>|</span><a href="#42384484">parent</a><span>|</span><a href="#42384933">prev</a><span>|</span><a href="#42385988">next</a><span>|</span><label class="collapse" for="c-42384667">[-]</label><label class="expand" for="c-42384667">[1 more]</label></div><br/><div class="children"><div class="content">Yup, some tokens are effectively branching decisions. Yann has a whole rant about a shortcoming of LLMs being they take the same compute regardless of the position in a sentence - which isn&#x27;t great because sometimes you really have a serious decision to make, other times not so much. It also makes you wonder about optimal embedding size - maybe the right size is 10x bigger.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42385988" class="c"><input type="checkbox" id="c-42385988" checked=""/><div class="controls bullet"><span class="by">max93</span><span>|</span><a href="#42380033">prev</a><span>|</span><a href="#42385341">next</a><span>|</span><label class="collapse" for="c-42385988">[-]</label><label class="expand" for="c-42385988">[1 more]</label></div><br/><div class="children"><div class="content">We conducted similar research earlier and successfully improved performance to a level comparable to models with 3x larger layer sizes. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2409.14199v3" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2409.14199v3</a>  We utilize more computational time in the latent space to achieve better performance. However, this approach introduces greater resistance compared to Chain of Thought (CoT) reasoning in the token space, especially if the number of CoT rounds in the latent space exceeds 20.
I would using the term &quot;better approximation of the data distribution&quot; instead of &quot;reasoning&quot; to describe this kind of process.</div><br/></div></div><div id="42385341" class="c"><input type="checkbox" id="c-42385341" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#42385988">prev</a><span>|</span><a href="#42379616">next</a><span>|</span><label class="collapse" for="c-42385341">[-]</label><label class="expand" for="c-42385341">[4 more]</label></div><br/><div class="children"><div class="content">I feel abdominal pain when I see the words âthinkingâ or âreasoningâ related to LLMs.<p>I feel back pain when I read the crazy, unsound speculation about how the brain is supposed to be like a computer. Serious mistake.</div><br/><div id="42385865" class="c"><input type="checkbox" id="c-42385865" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42385341">parent</a><span>|</span><a href="#42385881">next</a><span>|</span><label class="collapse" for="c-42385865">[-]</label><label class="expand" for="c-42385865">[1 more]</label></div><br/><div class="children"><div class="content">Unless you can show an example of humans reasoning solving a problem outside the Turing computable set, there is no rational basis for assuming the brain is anything but a computer, as the very notion that we exceed Turing computability would be revolutionary and utterly mindbending in terms of consequences on a number of fields.</div><br/></div></div><div id="42385881" class="c"><input type="checkbox" id="c-42385881" checked=""/><div class="controls bullet"><span class="by">234120987654</span><span>|</span><a href="#42385341">parent</a><span>|</span><a href="#42385865">prev</a><span>|</span><a href="#42385394">next</a><span>|</span><label class="collapse" for="c-42385881">[-]</label><label class="expand" for="c-42385881">[1 more]</label></div><br/><div class="children"><div class="content">100% agree. I miss the days where the title would describe the method instead of being a sales pitch</div><br/></div></div><div id="42385394" class="c"><input type="checkbox" id="c-42385394" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#42385341">parent</a><span>|</span><a href="#42385881">prev</a><span>|</span><a href="#42379616">next</a><span>|</span><label class="collapse" for="c-42385394">[-]</label><label class="expand" for="c-42385394">[1 more]</label></div><br/><div class="children"><div class="content">I agree with you. &quot;Chain of thought&quot; is not reasoning, just like LSD trip isn&#x27;t.<p>I think we lack a good formal definition of what (fuzzy) reasoning is. Without it, we will always have some kind of unexplained hallucinations.<p>I also believe AGI could be implemented as a model that can train models for specific tasks completely autonomously. But that would kill the cash cow, so OpenAI etc. are not interested in developing it.</div><br/></div></div></div></div><div id="42379616" class="c"><input type="checkbox" id="c-42379616" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#42385341">prev</a><span>|</span><a href="#42379167">next</a><span>|</span><label class="collapse" for="c-42379616">[-]</label><label class="expand" for="c-42379616">[12 more]</label></div><br/><div class="children"><div class="content">I think of an LLM model as like a crystallised mathematical snapshot of intelligence... like a cell on a microscope slide, a <i>dead</i> and mounted form of output from the <i>living process</i> of intelligence...<p>This paper makes me wonder whether,  in a very fuzzy sense, we could give #LLMs access to some similarly crystallised analog of emotion or emotional valence, below the level of language<p><a href="https:&#x2F;&#x2F;x.com&#x2F;patcon_&#x2F;status&#x2F;1866549080127893613?s=46" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;patcon_&#x2F;status&#x2F;1866549080127893613?s=46</a></div><br/><div id="42382495" class="c"><input type="checkbox" id="c-42382495" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#42379616">parent</a><span>|</span><a href="#42380146">next</a><span>|</span><label class="collapse" for="c-42382495">[-]</label><label class="expand" for="c-42382495">[1 more]</label></div><br/><div class="children"><div class="content">Maybe &quot;stasis&quot; is more appropriate than &quot;dead.&quot; Each new session is an unfrozen clone of the original mind snapshot.</div><br/></div></div><div id="42380146" class="c"><input type="checkbox" id="c-42380146" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42379616">parent</a><span>|</span><a href="#42382495">prev</a><span>|</span><a href="#42380246">next</a><span>|</span><label class="collapse" for="c-42380146">[-]</label><label class="expand" for="c-42380146">[9 more]</label></div><br/><div class="children"><div class="content">Intelligence is more than just knowing the probabilistic relationship between every word.</div><br/><div id="42380219" class="c"><input type="checkbox" id="c-42380219" checked=""/><div class="controls bullet"><span class="by">Rhapso</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42380146">parent</a><span>|</span><a href="#42381452">next</a><span>|</span><label class="collapse" for="c-42380219">[-]</label><label class="expand" for="c-42380219">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Intelligence&quot; is a continuous process. Without a continuous feedback loop, LLMs will never be more than a compression algorithm we bullied into being a chatbot.<p>OpenAi as a mega-organism might be intelligent, but the LLMs definitely are not.<p>The &quot;compressed capture of semantic relationships&quot; is a new thing we don&#x27;t have a word for.</div><br/><div id="42381408" class="c"><input type="checkbox" id="c-42381408" checked=""/><div class="controls bullet"><span class="by">thrance</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42380219">parent</a><span>|</span><a href="#42381452">next</a><span>|</span><label class="collapse" for="c-42381408">[-]</label><label class="expand" for="c-42381408">[1 more]</label></div><br/><div class="children"><div class="content">Funnily enough, there is a mathematical link between data compression and AGI [1]. I believe a paper circulated some time ago that compared gpt2 to gzip, with interesting results.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AIXI" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AIXI</a></div><br/></div></div></div></div><div id="42381452" class="c"><input type="checkbox" id="c-42381452" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42380146">parent</a><span>|</span><a href="#42380219">prev</a><span>|</span><a href="#42385350">next</a><span>|</span><label class="collapse" for="c-42381452">[-]</label><label class="expand" for="c-42381452">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s part of the process, given that the &quot;bigger picture&quot; remains in context.</div><br/></div></div><div id="42385350" class="c"><input type="checkbox" id="c-42385350" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42380146">parent</a><span>|</span><a href="#42381452">prev</a><span>|</span><a href="#42383789">next</a><span>|</span><label class="collapse" for="c-42385350">[-]</label><label class="expand" for="c-42385350">[1 more]</label></div><br/><div class="children"><div class="content">Please spread the word that predicting the next one is not intelligence. Itâs markovâ¦</div><br/></div></div><div id="42383789" class="c"><input type="checkbox" id="c-42383789" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42380146">parent</a><span>|</span><a href="#42385350">prev</a><span>|</span><a href="#42380246">next</a><span>|</span><label class="collapse" for="c-42383789">[-]</label><label class="expand" for="c-42383789">[4 more]</label></div><br/><div class="children"><div class="content">Do you have strong evidence for this?</div><br/><div id="42384579" class="c"><input type="checkbox" id="c-42384579" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42383789">parent</a><span>|</span><a href="#42380246">next</a><span>|</span><label class="collapse" for="c-42384579">[-]</label><label class="expand" for="c-42384579">[3 more]</label></div><br/><div class="children"><div class="content">Dogs are highly intelligent, and it makes no sense to say that they get their intelligence by calculating the probabilities between consecutive woofs.</div><br/><div id="42385624" class="c"><input type="checkbox" id="c-42385624" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42384579">parent</a><span>|</span><a href="#42385795">next</a><span>|</span><label class="collapse" for="c-42385624">[-]</label><label class="expand" for="c-42385624">[1 more]</label></div><br/><div class="children"><div class="content">Would you say with equal confidence that they don&#x27;t exemplify their intelligence by their ability to repeatedly select an often-successful next action from a set of possible next actions, based on a set of input observations?<p>&quot;Tokens&quot; don&#x27;t have to be words, or woofs...</div><br/></div></div><div id="42385795" class="c"><input type="checkbox" id="c-42385795" checked=""/><div class="controls bullet"><span class="by">sullyj3</span><span>|</span><a href="#42379616">root</a><span>|</span><a href="#42384579">parent</a><span>|</span><a href="#42385624">prev</a><span>|</span><a href="#42380246">next</a><span>|</span><label class="collapse" for="c-42385795">[-]</label><label class="expand" for="c-42385795">[1 more]</label></div><br/><div class="children"><div class="content">That only shows that word prediction isn&#x27;t necessary, not that it&#x27;s insufficient</div><br/></div></div></div></div></div></div></div></div><div id="42380246" class="c"><input type="checkbox" id="c-42380246" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#42379616">parent</a><span>|</span><a href="#42380146">prev</a><span>|</span><a href="#42379167">next</a><span>|</span><label class="collapse" for="c-42380246">[-]</label><label class="expand" for="c-42380246">[1 more]</label></div><br/><div class="children"><div class="content">Did you really just link to a post from your Twitter saying the same thing you did here?</div><br/></div></div></div></div><div id="42379167" class="c"><input type="checkbox" id="c-42379167" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#42379616">prev</a><span>|</span><a href="#42384164">next</a><span>|</span><label class="collapse" for="c-42379167">[-]</label><label class="expand" for="c-42379167">[12 more]</label></div><br/><div class="children"><div class="content">I like the direction of the research of working in latent space but feeding the last layer representation back as a first layer embedding feels sketchy to me. Those layers have different representation space.</div><br/><div id="42383441" class="c"><input type="checkbox" id="c-42383441" checked=""/><div class="controls bullet"><span class="by">jsenn</span><span>|</span><a href="#42379167">parent</a><span>|</span><a href="#42385507">next</a><span>|</span><label class="collapse" for="c-42383441">[-]</label><label class="expand" for="c-42383441">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Those layers have different representation space.<p>Do they? Interpretability techniques like the Logit Lens [1] wouldn&#x27;t work if this were the case. That author found that at least for GPT-2, the network almost immediately transforms its hidden state into a &quot;logitable&quot; form: you can unproject the hidden state of <i>any</i> layer to see how that layer incrementally refines the next token prediction.<p>[1]: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;AcKRB8wDpdaN6v6ru&#x2F;interpreting-gpt-the-logit-lens" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;AcKRB8wDpdaN6v6ru&#x2F;interpreti...</a></div><br/></div></div><div id="42385507" class="c"><input type="checkbox" id="c-42385507" checked=""/><div class="controls bullet"><span class="by">mbowcut2</span><span>|</span><a href="#42379167">parent</a><span>|</span><a href="#42383441">prev</a><span>|</span><a href="#42379413">next</a><span>|</span><label class="collapse" for="c-42385507">[-]</label><label class="expand" for="c-42385507">[1 more]</label></div><br/><div class="children"><div class="content">This was my first thought too. AFAIK each layer encodes different information, and it&#x27;s not clear that the last layer would be able to communicate well with the first layer without substantial retraining.<p>Like in a CNN for instance, if you fed later representations back in to the first kernels they wouldn&#x27;t be able to find anything meaningful because it&#x27;s not the image anymore, it&#x27;s some latent representation of the image that the early kernels aren&#x27;t trained on.</div><br/></div></div><div id="42379413" class="c"><input type="checkbox" id="c-42379413" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42379167">parent</a><span>|</span><a href="#42385507">prev</a><span>|</span><a href="#42385766">next</a><span>|</span><label class="collapse" for="c-42379413">[-]</label><label class="expand" for="c-42379413">[7 more]</label></div><br/><div class="children"><div class="content">Feeding the last layer back as the input embedding has been done many times, e.g. Transformer-XL. The models are trained like this, it&#x27;s not like they&#x27;re taking a pre-trained Llama and just feeding it to itself. It&#x27;s a simple, computationally cheap mechanism to add feedback.</div><br/><div id="42380378" class="c"><input type="checkbox" id="c-42380378" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#42379167">root</a><span>|</span><a href="#42379413">parent</a><span>|</span><a href="#42379509">next</a><span>|</span><label class="collapse" for="c-42380378">[-]</label><label class="expand" for="c-42380378">[2 more]</label></div><br/><div class="children"><div class="content">from my understanding that is what they do, see the paper: 
&gt; We use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments.
I agree the feedback is necessary, and the mechanism simple and cheap, but I don&#x27;t think is optimal.</div><br/><div id="42385250" class="c"><input type="checkbox" id="c-42385250" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42379167">root</a><span>|</span><a href="#42380378">parent</a><span>|</span><a href="#42379509">next</a><span>|</span><label class="collapse" for="c-42385250">[-]</label><label class="expand" for="c-42385250">[1 more]</label></div><br/><div class="children"><div class="content">Yes, they use a pre-trained model, but they do further training (please correct me if I mis-read, and also I realize my above comment could be interpreted as saying they train a new model entirely from scratch).<p>&gt; We use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is
set to 1 Ã 10â4 while the effective batch size is 128. Following Deng et al. (2024), we also reset the optimizer
when the training stages switch.</div><br/></div></div></div></div><div id="42379509" class="c"><input type="checkbox" id="c-42379509" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#42379167">root</a><span>|</span><a href="#42379413">parent</a><span>|</span><a href="#42380378">prev</a><span>|</span><a href="#42385766">next</a><span>|</span><label class="collapse" for="c-42379509">[-]</label><label class="expand" for="c-42379509">[4 more]</label></div><br/><div class="children"><div class="content">I read a paper not long ago that showed that deleting, duplicating and reordering layers doesn&#x27;t actually seem to matter that much and it feeding back is just a kind of re-ordering.</div><br/><div id="42379578" class="c"><input type="checkbox" id="c-42379578" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42379167">root</a><span>|</span><a href="#42379509">parent</a><span>|</span><a href="#42379863">next</a><span>|</span><label class="collapse" for="c-42379578">[-]</label><label class="expand" for="c-42379578">[1 more]</label></div><br/><div class="children"><div class="content">So you&#x27;re saying that feeding the last layer back to the first makes the model layer-order independent, or kinda <i>infinitely deep</i>, if you squint? :).</div><br/></div></div><div id="42379863" class="c"><input type="checkbox" id="c-42379863" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#42379167">root</a><span>|</span><a href="#42379509">parent</a><span>|</span><a href="#42379578">prev</a><span>|</span><a href="#42385766">next</a><span>|</span><label class="collapse" for="c-42379863">[-]</label><label class="expand" for="c-42379863">[2 more]</label></div><br/><div class="children"><div class="content">Imo this kind of makes sense - LLMs without a feedback loop can learn to have one themselves by encoding information in the previously generated tokens.</div><br/><div id="42380602" class="c"><input type="checkbox" id="c-42380602" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#42379167">root</a><span>|</span><a href="#42379863">parent</a><span>|</span><a href="#42385766">next</a><span>|</span><label class="collapse" for="c-42380602">[-]</label><label class="expand" for="c-42380602">[1 more]</label></div><br/><div class="children"><div class="content">They can&#x27;t, because that would increase training loss. The training loss acts as a gatekeeper for reasoning.</div><br/></div></div></div></div></div></div></div></div><div id="42385766" class="c"><input type="checkbox" id="c-42385766" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42379167">parent</a><span>|</span><a href="#42379413">prev</a><span>|</span><a href="#42382929">next</a><span>|</span><label class="collapse" for="c-42385766">[-]</label><label class="expand" for="c-42385766">[1 more]</label></div><br/><div class="children"><div class="content">llama 3.x is already sharing the last layer with the embedding layer, it just uses the transpose in the last layer operation.</div><br/></div></div><div id="42382929" class="c"><input type="checkbox" id="c-42382929" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#42379167">parent</a><span>|</span><a href="#42385766">prev</a><span>|</span><a href="#42384164">next</a><span>|</span><label class="collapse" for="c-42382929">[-]</label><label class="expand" for="c-42382929">[1 more]</label></div><br/><div class="children"><div class="content">Not really. See the literature on sharing lm_head (last matrix multiplication) with the input embedding dict.<p>Basically, the lm_head (a MxN matrix where M is the dictionary size and N is the internal dimension) can be seen as the dictionary too. You can think that and the softmax over it as compute cosine similarity of the last hidden output w.r.t. input embedding dictionary.<p>In that sense, they are sharing the representation space.<p>(BTW, I believe sharing lm_head with input embedding not working as good as separating them, so only mobile focused LLMs do so. So here is that. It would be interesting to experiment if injecting a projection layer like you suggested would improve performance or just red-herring).</div><br/></div></div></div></div><div id="42384164" class="c"><input type="checkbox" id="c-42384164" checked=""/><div class="controls bullet"><span class="by">thoughtlede</span><span>|</span><a href="#42379167">prev</a><span>|</span><a href="#42378559">next</a><span>|</span><label class="collapse" for="c-42384164">[-]</label><label class="expand" for="c-42384164">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps these findings might be indicating that we need more NN layers&#x2F;attention blocks for performing reasoning. This project circumvented the lack of more trained layers by looping the input through currently trained layers more than once.<p>Also we may have to look for better loss functions than ones that help us predict the next token to train the models if the objective is reasoning.</div><br/></div></div><div id="42378559" class="c"><input type="checkbox" id="c-42378559" checked=""/><div class="controls bullet"><span class="by">mentalgear</span><span>|</span><a href="#42384164">prev</a><span>|</span><a href="#42379063">next</a><span>|</span><label class="collapse" for="c-42378559">[-]</label><label class="expand" for="c-42378559">[8 more]</label></div><br/><div class="children"><div class="content">&quot;We utilize the last hidden state of the LLM as a representation of the reasoning state (termed &quot;continuous thought&quot;).&quot;<p>Could someone explain the last hidden state of the LLM ? What it shape is and how it is normally used - and why it hasn&#x27;t been used yet to augment the next input? (which seems logical)</div><br/><div id="42378599" class="c"><input type="checkbox" id="c-42378599" checked=""/><div class="controls bullet"><span class="by">tjbai</span><span>|</span><a href="#42378559">parent</a><span>|</span><a href="#42378591">next</a><span>|</span><label class="collapse" for="c-42378599">[-]</label><label class="expand" for="c-42378599">[5 more]</label></div><br/><div class="children"><div class="content">The last hidden state is just the output embedding after N residual layers, e.g. input embedding + res1 + res2 + ...<p>There&#x27;s typically an &quot;unembedding layer&quot;&#x2F;&quot;classification head&quot; that uses this hidden state to produce a softmax distribution over the LLM&#x27;s vocabulary. In this case, we can think of this as &quot;snapping&quot; the hidden state into a single token and feeding that token into the next position of the autoregressive LLM.<p>In this sense, the last hidden state _does_ augment the next input. The authors simply propose directly feeding this hidden state into the next step rather than reducing it into a single tokenâthus, reasoning in continuous latent space rather than discrete token space.</div><br/><div id="42378952" class="c"><input type="checkbox" id="c-42378952" checked=""/><div class="controls bullet"><span class="by">intalentive</span><span>|</span><a href="#42378559">root</a><span>|</span><a href="#42378599">parent</a><span>|</span><a href="#42383936">next</a><span>|</span><label class="collapse" for="c-42378952">[-]</label><label class="expand" for="c-42378952">[3 more]</label></div><br/><div class="children"><div class="content">Moreover âsnappingâ the hidden state to a token is akin to quantization. Itâs lossy. By staying in latent space the model can âreasonâ at âfull resolutionâ without discretization noise.</div><br/><div id="42380406" class="c"><input type="checkbox" id="c-42380406" checked=""/><div class="controls bullet"><span class="by">snthpy</span><span>|</span><a href="#42378559">root</a><span>|</span><a href="#42378952">parent</a><span>|</span><a href="#42380388">next</a><span>|</span><label class="collapse" for="c-42380406">[-]</label><label class="expand" for="c-42380406">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes discretization introduces interesting behavior though. Compare for example the logistic map and it&#x27;s chaotic regime with the simplicity of the logistic ODE. Another example would be quantum mechanics compared to classical mechanics and determinism. The Poincare Conjecture was only interesting for n=3 due to too much connectivity in higher dimensions. Wouldn&#x27;t it be interesting if consciousness only arose in such a discretized form, a case of incidental complexity and chaos introduced as the result of topological non-triviality from quantization?<p>Don&#x27;t forget, non-linearity is fundamental to the whole process, otherwise you&#x27;d just have one large linear transformation. Maybe there&#x27;s a similar role for discretization? :shrug:</div><br/></div></div><div id="42380388" class="c"><input type="checkbox" id="c-42380388" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#42378559">root</a><span>|</span><a href="#42378952">parent</a><span>|</span><a href="#42380406">prev</a><span>|</span><a href="#42383936">next</a><span>|</span><label class="collapse" for="c-42380388">[-]</label><label class="expand" for="c-42380388">[1 more]</label></div><br/><div class="children"><div class="content">Useful information about conceptual relationships and procedure can be captured in the LM head, so there is also potential lossiness when short-circuiting it.</div><br/></div></div></div></div><div id="42383936" class="c"><input type="checkbox" id="c-42383936" checked=""/><div class="controls bullet"><span class="by">sweetheart</span><span>|</span><a href="#42378559">root</a><span>|</span><a href="#42378599">parent</a><span>|</span><a href="#42378952">prev</a><span>|</span><a href="#42378591">next</a><span>|</span><label class="collapse" for="c-42383936">[-]</label><label class="expand" for="c-42383936">[1 more]</label></div><br/><div class="children"><div class="content">Wow this was the explanation that made it all click for me. Thanks so much!</div><br/></div></div></div></div><div id="42378591" class="c"><input type="checkbox" id="c-42378591" checked=""/><div class="controls bullet"><span class="by">AmazingTurtle</span><span>|</span><a href="#42378559">parent</a><span>|</span><a href="#42378599">prev</a><span>|</span><a href="#42378957">next</a><span>|</span><label class="collapse" for="c-42378591">[-]</label><label class="expand" for="c-42378591">[1 more]</label></div><br/><div class="children"><div class="content">Embeddings aka the last hidden state are the mathematical representation of an input of the model before a separate model (usually the decoder) translates that hidden state to a next token (the generative part in generative ai). Normally, the this step repeats over and over. This novel approach introduces re-using the last hidden state as if it was a token that has been generated thus &quot;evolving&quot; the hidden state over each iteration.</div><br/></div></div><div id="42378957" class="c"><input type="checkbox" id="c-42378957" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#42378559">parent</a><span>|</span><a href="#42378591">prev</a><span>|</span><a href="#42379063">next</a><span>|</span><label class="collapse" for="c-42378957">[-]</label><label class="expand" for="c-42378957">[1 more]</label></div><br/><div class="children"><div class="content">The way the recurrence in this method works -- ie, using last LLM hidden state at previous time step as input token for the next time step -- isn&#x27;t directly compatible with how recurrence&#x2F;autoregression is typically handled during LLM training. One of the major strengths of transformers is that they can be trained for recurrence&#x2F;autoregression (which have sequential dependency) using convolutions (which are embarrasingly parallel). The proposed method requires introducing some sequential dependencies during training that could otherwise be avoided using &quot;causal masking&quot; and convolutions to enforce the correct dependencies between time steps in a sequence. Introducing these sequential dependencies makes training a lot slower.<p>tldr; the method requires training in a way that loses one of the major benefits of transformers, but maybe in some scenarios that loss is worth it.</div><br/></div></div></div></div><div id="42379063" class="c"><input type="checkbox" id="c-42379063" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#42378559">prev</a><span>|</span><a href="#42378536">next</a><span>|</span><label class="collapse" for="c-42379063">[-]</label><label class="expand" for="c-42379063">[2 more]</label></div><br/><div class="children"><div class="content">I wonder what would happen if you just ran this on a continuous loop and only intermittently fed in new tokens or queried it for token outputs.</div><br/><div id="42385363" class="c"><input type="checkbox" id="c-42385363" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#42379063">parent</a><span>|</span><a href="#42378536">next</a><span>|</span><label class="collapse" for="c-42385363">[-]</label><label class="expand" for="c-42385363">[1 more]</label></div><br/><div class="children"><div class="content">Well, if you consider the case of a linear regression, fitting on your output will add no new information to the weights. Try that on any notebook.</div><br/></div></div></div></div><div id="42378536" class="c"><input type="checkbox" id="c-42378536" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#42379063">prev</a><span>|</span><a href="#42379093">next</a><span>|</span><label class="collapse" for="c-42378536">[-]</label><label class="expand" for="c-42378536">[1 more]</label></div><br/><div class="children"><div class="content">It seems like the latent space could be even more useful if it was trained with the transcribed videos.</div><br/></div></div><div id="42379093" class="c"><input type="checkbox" id="c-42379093" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#42378536">prev</a><span>|</span><a href="#42379237">next</a><span>|</span><label class="collapse" for="c-42379093">[-]</label><label class="expand" for="c-42379093">[3 more]</label></div><br/><div class="children"><div class="content">In LLMs, is there a correlation between layer depth and the activations correspondence to the abstract to concrete details continuum?</div><br/><div id="42380610" class="c"><input type="checkbox" id="c-42380610" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#42379093">parent</a><span>|</span><a href="#42379237">next</a><span>|</span><label class="collapse" for="c-42380610">[-]</label><label class="expand" for="c-42380610">[2 more]</label></div><br/><div class="children"><div class="content">Yes: for eg BPE, due to how it progressively pushes compound tokens of already seen - hence more common - subtokens to the âtopâ of the vocab), you can train a model to do <i>regression over vocabulary index</i> for the next token from the current token embedding - using the same single  regression model for all layer depths. If you plot mse of token index prediction versus layer depth then you can see that the mse of the prediction decreases steadily per additional layer. This appears to be because token index in eg BPE is actually fairly smooth and so it seems like the model is capable of localizing to the actual correct vocab index as depth increases, so kind of like a fuzzy-&gt;discrete refinement as you go deeper in layers <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.13442" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.13442</a></div><br/><div id="42382778" class="c"><input type="checkbox" id="c-42382778" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#42379093">root</a><span>|</span><a href="#42380610">parent</a><span>|</span><a href="#42379237">next</a><span>|</span><label class="collapse" for="c-42382778">[-]</label><label class="expand" for="c-42382778">[1 more]</label></div><br/><div class="children"><div class="content">THANKS!</div><br/></div></div></div></div></div></div><div id="42379237" class="c"><input type="checkbox" id="c-42379237" checked=""/><div class="controls bullet"><span class="by">bick_nyers</span><span>|</span><a href="#42379093">prev</a><span>|</span><a href="#42378615">next</a><span>|</span><label class="collapse" for="c-42379237">[-]</label><label class="expand" for="c-42379237">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if you would want to use an earlier layer as opposed to the penultimate layer, I would imagine that the LLM uses that layer to &quot;prepare&quot; for the final dimensionality reduction to clean the signal such that it scores well on the loss function.</div><br/></div></div><div id="42378615" class="c"><input type="checkbox" id="c-42378615" checked=""/><div class="controls bullet"><span class="by">DalasNoin</span><span>|</span><a href="#42379237">prev</a><span>|</span><a href="#42378989">next</a><span>|</span><label class="collapse" for="c-42378615">[-]</label><label class="expand" for="c-42378615">[15 more]</label></div><br/><div class="children"><div class="content">So the models will no longer be thinking in plain English but some embedding space? Seems not like what you want.</div><br/><div id="42378626" class="c"><input type="checkbox" id="c-42378626" checked=""/><div class="controls bullet"><span class="by">Vampiero</span><span>|</span><a href="#42378615">parent</a><span>|</span><a href="#42385369">next</a><span>|</span><label class="collapse" for="c-42378626">[-]</label><label class="expand" for="c-42378626">[10 more]</label></div><br/><div class="children"><div class="content">Seems exactly like what you want. We don&#x27;t think in plain English, we _rationalize_ our thoughts into English (or whatever language comes out) but they must be more fundamental than language because language is acquired.<p>Essentially, English is one of many possible encodings of an underlying intuitive, possibly non-symbolic representation.</div><br/><div id="42378869" class="c"><input type="checkbox" id="c-42378869" checked=""/><div class="controls bullet"><span class="by">intalentive</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378626">parent</a><span>|</span><a href="#42378671">next</a><span>|</span><label class="collapse" for="c-42378869">[-]</label><label class="expand" for="c-42378869">[1 more]</label></div><br/><div class="children"><div class="content">Cognitive scientists called it âmentaleseâ.</div><br/></div></div><div id="42378671" class="c"><input type="checkbox" id="c-42378671" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378626">parent</a><span>|</span><a href="#42378869">prev</a><span>|</span><a href="#42385369">next</a><span>|</span><label class="collapse" for="c-42378671">[-]</label><label class="expand" for="c-42378671">[8 more]</label></div><br/><div class="children"><div class="content">&gt; We don&#x27;t think in plain English<p>That&#x27;s debatable. Language shapes thoughts much more than you might think. Because you learn concepts from language that you could not imagine by yourself until you learned&#x2F;read about them, so they are in effect very linked to language.</div><br/><div id="42378884" class="c"><input type="checkbox" id="c-42378884" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378671">parent</a><span>|</span><a href="#42378766">next</a><span>|</span><label class="collapse" for="c-42378884">[-]</label><label class="expand" for="c-42378884">[1 more]</label></div><br/><div class="children"><div class="content">I can also think in images and internal visualizations. Geometric reasoning is also a thing. Musicians can also hear things in their mind - some can write it down, others can play it directly, and in my case I&#x27;m not good enough to get it out of my head!<p>In all cases though these thoughts are kind of tied to representations from the real world. Sort of like other languages via different senses. So yeah, how abstract can our thoughts actually be?</div><br/></div></div><div id="42378766" class="c"><input type="checkbox" id="c-42378766" checked=""/><div class="controls bullet"><span class="by">samiskin</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378671">parent</a><span>|</span><a href="#42378884">prev</a><span>|</span><a href="#42384561">next</a><span>|</span><label class="collapse" for="c-42378766">[-]</label><label class="expand" for="c-42378766">[2 more]</label></div><br/><div class="children"><div class="content">Language is definitely a significant part of thinking, but when I remember how cold it was outside yesterday to figure out if it was colder than today, I&#x27;m not bringing words to mind. I&#x27;m bringing up some other non-discrete information that I could never precisely encode into words and then factoring that in with the other non-discrete information I&#x27;m currently taking in through my senses. Its only after that processing that I encode it as a lossy &quot;It was colder yesterday&quot; statement.</div><br/></div></div><div id="42384561" class="c"><input type="checkbox" id="c-42384561" checked=""/><div class="controls bullet"><span class="by">klausa</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378671">parent</a><span>|</span><a href="#42378766">prev</a><span>|</span><a href="#42378704">next</a><span>|</span><label class="collapse" for="c-42384561">[-]</label><label class="expand" for="c-42384561">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re basically talking about Sapir-Whorf here:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Linguistic_relativity" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Linguistic_relativity</a><p>&gt;The hypothesis is in dispute, with many different variations throughout its history.[2] The strong hypothesis of linguistic relativity, now referred to as linguistic determinism, is that language determines thought and that linguistic categories limit and restrict cognitive categories. This was a claim by some earlier linguists pre-World War II;[3] since then it has fallen out of acceptance by contemporary linguists.</div><br/></div></div><div id="42378704" class="c"><input type="checkbox" id="c-42378704" checked=""/><div class="controls bullet"><span class="by">Vampiero</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378671">parent</a><span>|</span><a href="#42384561">prev</a><span>|</span><a href="#42385509">next</a><span>|</span><label class="collapse" for="c-42378704">[-]</label><label class="expand" for="c-42378704">[1 more]</label></div><br/><div class="children"><div class="content">Fair, but there are many categories of languages.<p>For example, I can think in formal logic. I&#x27;ve learned to do that, and surely my brain takes a step-by-step approach to it, but I&#x27;ve also internalized some of it and I don&#x27;t think that my proficiency with English has anything to do with it.<p>I could have learned the same concepts in any other language, but the end result would be the same.<p>And surely there are many thoughts that can&#x27;t be expressed purely with words. For example all that is related to qualia. You can think of a color but you can&#x27;t describe what you see in your mind&#x27;s eye with words, not in a way that would let a blind person share the same experience. Or try describing &quot;love&quot; without making a similitude. Is love a thought? Or a feeling? Is there a meaningful difference between the two?</div><br/></div></div><div id="42385509" class="c"><input type="checkbox" id="c-42385509" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378671">parent</a><span>|</span><a href="#42378704">prev</a><span>|</span><a href="#42378860">next</a><span>|</span><label class="collapse" for="c-42385509">[-]</label><label class="expand" for="c-42385509">[1 more]</label></div><br/><div class="children"><div class="content">eh, probably both. Why does it have to be a fight between two schools of thoughts? Thoughts can be across-modal. Some of it can be done in specific language or some could be visual.<p>(universal grammar peoole hates this somehow, it&#x27;s weird)</div><br/></div></div><div id="42378860" class="c"><input type="checkbox" id="c-42378860" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378671">parent</a><span>|</span><a href="#42385509">prev</a><span>|</span><a href="#42385369">next</a><span>|</span><label class="collapse" for="c-42378860">[-]</label><label class="expand" for="c-42378860">[1 more]</label></div><br/><div class="children"><div class="content">But the thing you learn is not the word &#x27;purple&#x27;. You just use the word as the mental scaffolding to build a <i>concept</i> of purple. The word forms a linkage to a deeper embedding, which is further proven by the fact that it&#x27;s actually slightly different in each mind that has understanding of the concept.<p>This embedded concept is what is doing the work, the word was just the seed of the understanding and a method by which to convey that understanding to others.</div><br/></div></div></div></div></div></div><div id="42385369" class="c"><input type="checkbox" id="c-42385369" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#42378615">parent</a><span>|</span><a href="#42378626">prev</a><span>|</span><a href="#42378820">next</a><span>|</span><label class="collapse" for="c-42385369">[-]</label><label class="expand" for="c-42385369">[1 more]</label></div><br/><div class="children"><div class="content">They have never been thinking. This is important.<p>Predicting the next word is not intelligence.</div><br/></div></div><div id="42378820" class="c"><input type="checkbox" id="c-42378820" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#42378615">parent</a><span>|</span><a href="#42385369">prev</a><span>|</span><a href="#42378989">next</a><span>|</span><label class="collapse" for="c-42378820">[-]</label><label class="expand" for="c-42378820">[3 more]</label></div><br/><div class="children"><div class="content">If you mean ânot what we wantâ for safety reasons, I think I agree.<p>If you donât mean for safety reasons, Iâm not sure why.</div><br/><div id="42379443" class="c"><input type="checkbox" id="c-42379443" checked=""/><div class="controls bullet"><span class="by">miven</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378820">parent</a><span>|</span><a href="#42379809">next</a><span>|</span><label class="collapse" for="c-42379443">[-]</label><label class="expand" for="c-42379443">[1 more]</label></div><br/><div class="children"><div class="content">In section 2 they briefly mention studies such as [1] that point out that the token outputs of a chain of thought aren&#x27;t always entirely faithful to the responses of the models<p>I&#x27;m not sure whether it wouldn&#x27;t be more reliable to let the model run on latents and try to train a separate latent-reading explainer module that has at least some approximation of what we want as an explicit optimization objective.<p>Assuming it actually is or has the potential to be better than CoT, from what I gathered from the paper the current results are mostly just more efficient token-wise.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.04388" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.04388</a></div><br/></div></div><div id="42379809" class="c"><input type="checkbox" id="c-42379809" checked=""/><div class="controls bullet"><span class="by">DalasNoin</span><span>|</span><a href="#42378615">root</a><span>|</span><a href="#42378820">parent</a><span>|</span><a href="#42379443">prev</a><span>|</span><a href="#42378989">next</a><span>|</span><label class="collapse" for="c-42379809">[-]</label><label class="expand" for="c-42379809">[1 more]</label></div><br/><div class="children"><div class="content">I was thinking abut safety reasons, but also usability. Seems like a pretty big difference to me if you don&#x27;t understand the chain of thought. How faithful cot are is another question.</div><br/></div></div></div></div></div></div><div id="42378989" class="c"><input type="checkbox" id="c-42378989" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#42378615">prev</a><span>|</span><a href="#42379196">next</a><span>|</span><label class="collapse" for="c-42378989">[-]</label><label class="expand" for="c-42378989">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a good, understandable paper. The main issue with chain-of-thought (which I think is a solid approach, and one that needs to take place) is that we ourselves aren&#x27;t necessarily &#x27;trained&#x27; on chain-of-thought. Yes, we do learn mathematical proofs and reasoning at some point (usually), but most people settle on latent thinking without training, and switch between the two modes naturally. My intuition says we&#x27;re missing something, but who knows</div><br/></div></div><div id="42378616" class="c"><input type="checkbox" id="c-42378616" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42379196">prev</a><span>|</span><label class="collapse" for="c-42378616">[-]</label><label class="expand" for="c-42378616">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Experiments show that Coconut can effectively augment the LLM on several reasoning tasks.<p>It really seems like we are building a true intelligence, adding components to different parts of a &quot;brain&quot; until we have something rivalling the human mind. It&#x27;s exceptionally dangerous and it&#x27;s remarkable how researchers turn a blind eye to any possible consequences.</div><br/><div id="42378677" class="c"><input type="checkbox" id="c-42378677" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#42378616">parent</a><span>|</span><a href="#42378637">next</a><span>|</span><label class="collapse" for="c-42378677">[-]</label><label class="expand" for="c-42378677">[2 more]</label></div><br/><div class="children"><div class="content">agree. Someone should make sure the next ASI develops an extension to hide the comments in every AI thread 80% full of the brightest minds saying &quot; I tried to build a react app and it totally failed doing it the way I wanted &quot;.</div><br/><div id="42385381" class="c"><input type="checkbox" id="c-42385381" checked=""/><div class="controls bullet"><span class="by">rsrsrs86</span><span>|</span><a href="#42378616">root</a><span>|</span><a href="#42378677">parent</a><span>|</span><a href="#42378637">next</a><span>|</span><label class="collapse" for="c-42385381">[-]</label><label class="expand" for="c-42385381">[1 more]</label></div><br/><div class="children"><div class="content">If ASI is artificial specific intelligence, I bet your pardon; intelligence can hardly be specific. Intelligence can reflect upon itself.</div><br/></div></div></div></div><div id="42378637" class="c"><input type="checkbox" id="c-42378637" checked=""/><div class="controls bullet"><span class="by">AmazingTurtle</span><span>|</span><a href="#42378616">parent</a><span>|</span><a href="#42378677">prev</a><span>|</span><label class="collapse" for="c-42378637">[-]</label><label class="expand" for="c-42378637">[3 more]</label></div><br/><div class="children"><div class="content">One day researchers will be like &quot;Oh crap what have we done&quot; and &quot;Shut it down, shut it down!!!&quot;</div><br/><div id="42378646" class="c"><input type="checkbox" id="c-42378646" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42378616">root</a><span>|</span><a href="#42378637">parent</a><span>|</span><label class="collapse" for="c-42378646">[-]</label><label class="expand" for="c-42378646">[2 more]</label></div><br/><div class="children"><div class="content">That is true. Most poeple will just respond to immediate physical threats as long as they have the illusory safety net of modern society.</div><br/><div id="42378692" class="c"><input type="checkbox" id="c-42378692" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#42378616">root</a><span>|</span><a href="#42378646">parent</a><span>|</span><label class="collapse" for="c-42378692">[-]</label><label class="expand" for="c-42378692">[1 more]</label></div><br/><div class="children"><div class="content">Just bear in mind while they are yelling &quot;shut it down&quot; there will be a bunch of commenters with no idea whats happening saying that they are just over reacting</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>