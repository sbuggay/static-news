<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685523667010" as="style"/><link rel="stylesheet" href="styles.css?v=1685523667010"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://developer.nvidia.com/blog/announcing-nvidia-dgx-gh200-first-100-terabyte-gpu-memory-system/">Nvidia DGX GH200: 100 Terabyte GPU Memory System</a> <span class="domain">(<a href="https://developer.nvidia.com">developer.nvidia.com</a>)</span></div><div class="subtext"><span>MacsHeadroom</span> | <span>185 comments</span></div><br/><div><div id="36136056" class="c"><input type="checkbox" id="c-36136056" checked=""/><div class="controls bullet"><span class="by">ch33zer</span><span>|</span><a href="#36133992">next</a><span>|</span><label class="collapse" for="c-36136056">[-]</label><label class="expand" for="c-36136056">[1 more]</label></div><br/><div class="children"><div class="content">Have people experimented with distributed training of parts of the model to avoid needing these absolutely massive GPUs? Anyone have pointers to large scale distributed training done recently?</div><br/></div></div><div id="36133992" class="c"><input type="checkbox" id="c-36133992" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#36136056">prev</a><span>|</span><a href="#36134504">next</a><span>|</span><label class="collapse" for="c-36133992">[-]</label><label class="expand" for="c-36133992">[24 more]</label></div><br/><div class="children"><div class="content">I really have to wonder if anyone can compete with this kind of systems integration capability. A core having 900GBps connectivity to the cluster memory at such relative low power is epic beyond words. 800Gbps ethernet across PCIe is uncompetitive in extreme.<p>How the rest of the industry can respond is such a mystery. And will it be lone competitors, or will a new PC era be able to start, with an ecosystem of capabilities?</div><br/><div id="36134316" class="c"><input type="checkbox" id="c-36134316" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#36133992">parent</a><span>|</span><a href="#36135046">next</a><span>|</span><label class="collapse" for="c-36134316">[-]</label><label class="expand" for="c-36134316">[19 more]</label></div><br/><div class="children"><div class="content">This seems to be competing directly with Google&#x27;s TPU pods. Looks like TPU v4 has a 300 GB&#x2F;s interconnect, and 32 GB HBM per chip * 4096 chips = 131 TB (which is all HBM, so higher bandwidth than the LPDDR in Nvidia&#x27;s system). So yeah, Nvidia&#x27;s interconnect seems better. However, TPU v4 was deployed in 2020 (!) and Nvidia&#x27;s thing won&#x27;t be ready until next year. I&#x27;ve gotta imagine that TPU v5 has already been deployed internally for a while now, but hasn&#x27;t been disclosed yet. Who knows, TPU v6 might even be deployed before this Nvidia thing.</div><br/><div id="36134444" class="c"><input type="checkbox" id="c-36134444" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134316">parent</a><span>|</span><a href="#36135291">next</a><span>|</span><label class="collapse" for="c-36134444">[-]</label><label class="expand" for="c-36134444">[13 more]</label></div><br/><div class="children"><div class="content">If TPUv4 pods were so powerful, why  are most new models trained with NVIDIA cards rather than TPU?</div><br/><div id="36134591" class="c"><input type="checkbox" id="c-36134591" checked=""/><div class="controls bullet"><span class="by">ioedward</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134444">parent</a><span>|</span><a href="#36134581">next</a><span>|</span><label class="collapse" for="c-36134591">[-]</label><label class="expand" for="c-36134591">[6 more]</label></div><br/><div class="children"><div class="content">TPUs are mostly hoarded by Google Research (including Deepmind) and Ads. Very few are being used by external people.</div><br/><div id="36135145" class="c"><input type="checkbox" id="c-36135145" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134591">parent</a><span>|</span><a href="#36134581">next</a><span>|</span><label class="collapse" for="c-36135145">[-]</label><label class="expand" for="c-36135145">[5 more]</label></div><br/><div class="children"><div class="content">The greatest artificial minds of our generation are thinking about how to make us click on ads?</div><br/><div id="36135265" class="c"><input type="checkbox" id="c-36135265" checked=""/><div class="controls bullet"><span class="by">philjohn</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36135145">parent</a><span>|</span><a href="#36135816">next</a><span>|</span><label class="collapse" for="c-36135265">[-]</label><label class="expand" for="c-36135265">[1 more]</label></div><br/><div class="children"><div class="content">It was ads that made the money to develop the artificial minds in the first place.</div><br/></div></div><div id="36135816" class="c"><input type="checkbox" id="c-36135816" checked=""/><div class="controls bullet"><span class="by">wiz21c</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36135145">parent</a><span>|</span><a href="#36135265">prev</a><span>|</span><a href="#36135244">next</a><span>|</span><label class="collapse" for="c-36135816">[-]</label><label class="expand" for="c-36135816">[1 more]</label></div><br/><div class="children"><div class="content">Damn right but I don&#x27;t understand why. That is, why is ads business generating so much profits that it allows to build such ridiculously powerfull devices ? Is it because it&#x27;s genuinely full of money or is it because Google is so central that it makes tons of money out of lots an dlots and lots of small adverts ?</div><br/></div></div><div id="36135244" class="c"><input type="checkbox" id="c-36135244" checked=""/><div class="controls bullet"><span class="by">ktta</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36135145">parent</a><span>|</span><a href="#36135816">prev</a><span>|</span><a href="#36135644">next</a><span>|</span><label class="collapse" for="c-36135244">[-]</label><label class="expand" for="c-36135244">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been that way for over a decade now. Welcome</div><br/></div></div><div id="36135644" class="c"><input type="checkbox" id="c-36135644" checked=""/><div class="controls bullet"><span class="by">f6v</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36135145">parent</a><span>|</span><a href="#36135244">prev</a><span>|</span><a href="#36134581">next</a><span>|</span><label class="collapse" for="c-36135644">[-]</label><label class="expand" for="c-36135644">[1 more]</label></div><br/><div class="children"><div class="content">How else would I know that “Elon Musk created a TeslaX platform that allows everyone to get rich”? Or was it Pavel Durov… Seriously, I can’t even report these on YouTube.</div><br/></div></div></div></div></div></div><div id="36134581" class="c"><input type="checkbox" id="c-36134581" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134444">parent</a><span>|</span><a href="#36134591">prev</a><span>|</span><a href="#36134511">next</a><span>|</span><label class="collapse" for="c-36134581">[-]</label><label class="expand" for="c-36134581">[1 more]</label></div><br/><div class="children"><div class="content">To add to the other answers, TPUv4 was not released to cloud customers until last year. And I bet availability is not as good as GPUs, even in Google Cloud (obviously TPUs are not available at all in other clouds).</div><br/></div></div><div id="36134511" class="c"><input type="checkbox" id="c-36134511" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134444">parent</a><span>|</span><a href="#36134581">prev</a><span>|</span><a href="#36134509">next</a><span>|</span><label class="collapse" for="c-36134511">[-]</label><label class="expand" for="c-36134511">[1 more]</label></div><br/><div class="children"><div class="content">These are just my guesses but:<p>Software for TPU is still in its early stages. CUDA is well established. You can test on a gaming GPU that you can find (locally!) in many markets. XLA is meant to solve this, but first impressions matter and my first impression was that it has not yet &quot;solved&quot; this issue.<p>TPU is only available via Google Cloud - as far as I know they don&#x27;t have NVIDIA&#x27;s widespread distribution to various HPC&#x2F;supercomputer systems. This also has implications on scaling up more than a few pods, as they will need to be colocated with speedy interconnect (which is provided by the various existing HPC systems that use NVIDIA&#x27;s chips).<p>Finally, I think many people are discovering that the supposed benefits of TPU are marginal at best in the face of the types of natural scaling issues that both GPU&#x27;s and TPU&#x27;s suffer from when scaling out to e.g. hundreds of pods.<p>I&#x27;m certain that someone with more experience than I could give a better answer though - and again, all speculation. I refuse to use TPU because Google Cloud&#x27;s system for getting access to said TPU&#x27;s was horrible for me when I tried it. I believe John Carmack has a nice tweet thread specifying the same issues I ran into.<p>In general, Google has a habit of developing tech for other Googlers first, and as such winds up ignoring a lot of real-world scenarios faced by researchers&#x2F;practitioners. NVIDIA on the other hand has been working directly with a ton of institutions and businesses ever since the inception of CUDA.<p>That their TPU&#x27;s have seen any adoption at all is mostly due to their research program which granted very cheap access to TPU&#x27;s to tons of people.</div><br/></div></div><div id="36134509" class="c"><input type="checkbox" id="c-36134509" checked=""/><div class="controls bullet"><span class="by">boyka</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134444">parent</a><span>|</span><a href="#36134511">prev</a><span>|</span><a href="#36135291">next</a><span>|</span><label class="collapse" for="c-36134509">[-]</label><label class="expand" for="c-36134509">[4 more]</label></div><br/><div class="children"><div class="content">Availability only on GCP and in particular cost.</div><br/><div id="36134626" class="c"><input type="checkbox" id="c-36134626" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134509">parent</a><span>|</span><a href="#36135291">next</a><span>|</span><label class="collapse" for="c-36134626">[-]</label><label class="expand" for="c-36134626">[3 more]</label></div><br/><div class="children"><div class="content">Google has advertised that they have better perf&#x2F;$ than GPUs, is this wrong or do you just mean absolute cost (so not available in small enough slices)?<p>edit: actually now i can&#x27;t find the claim, maybe i misremember what the papers said.</div><br/><div id="36134718" class="c"><input type="checkbox" id="c-36134718" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134626">parent</a><span>|</span><a href="#36135168">next</a><span>|</span><label class="collapse" for="c-36134718">[-]</label><label class="expand" for="c-36134718">[1 more]</label></div><br/><div class="children"><div class="content">Perf&#x2F;$ where $ is what it cost _them_ , not $ they&#x27;re ready to sell to others as a product. Cloud margins in the high two-digit percents are typical, and I&#x27;d imagine even higher for very specialized products in high-demand from deep-pocketed customers.</div><br/></div></div><div id="36135168" class="c"><input type="checkbox" id="c-36135168" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134626">parent</a><span>|</span><a href="#36134718">prev</a><span>|</span><a href="#36135291">next</a><span>|</span><label class="collapse" for="c-36135168">[-]</label><label class="expand" for="c-36135168">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.01433" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.01433</a> does claim &quot;1.2x-1.7x faster and uses 1.3x-1.9x less power than the NVIDIA A100&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="36135291" class="c"><input type="checkbox" id="c-36135291" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134316">parent</a><span>|</span><a href="#36134444">prev</a><span>|</span><a href="#36134660">next</a><span>|</span><label class="collapse" for="c-36135291">[-]</label><label class="expand" for="c-36135291">[2 more]</label></div><br/><div class="children"><div class="content">What evidence is there that Google would be able to out compete nvidia on AI hardware?</div><br/><div id="36135517" class="c"><input type="checkbox" id="c-36135517" checked=""/><div class="controls bullet"><span class="by">madaxe_again</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36135291">parent</a><span>|</span><a href="#36134660">next</a><span>|</span><label class="collapse" for="c-36135517">[-]</label><label class="expand" for="c-36135517">[1 more]</label></div><br/><div class="children"><div class="content">None. Heck, I can’t even search my gmail effectively any more, so if they can’t maintain a core product, I doubt they can build a new one of any quality. alphabet are now just a big, bloated catch-up corporation running on inertia and past glory.<p>I don’t think they will exist in 10 years.</div><br/></div></div></div></div><div id="36134660" class="c"><input type="checkbox" id="c-36134660" checked=""/><div class="controls bullet"><span class="by">ijidak</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134316">parent</a><span>|</span><a href="#36135291">prev</a><span>|</span><a href="#36135046">next</a><span>|</span><label class="collapse" for="c-36134660">[-]</label><label class="expand" for="c-36134660">[3 more]</label></div><br/><div class="children"><div class="content">Is HBM mostly Samsung?</div><br/><div id="36134857" class="c"><input type="checkbox" id="c-36134857" checked=""/><div class="controls bullet"><span class="by">mahkeiro</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134660">parent</a><span>|</span><a href="#36134690">next</a><span>|</span><label class="collapse" for="c-36134857">[-]</label><label class="expand" for="c-36134857">[1 more]</label></div><br/><div class="children"><div class="content">Market share for last year were 50% SK, 40% Samsung and 10% Micron, but as there is currently a huge demand things may change depending on capacity.</div><br/></div></div><div id="36134690" class="c"><input type="checkbox" id="c-36134690" checked=""/><div class="controls bullet"><span class="by">atty</span><span>|</span><a href="#36133992">root</a><span>|</span><a href="#36134660">parent</a><span>|</span><a href="#36134857">prev</a><span>|</span><a href="#36135046">next</a><span>|</span><label class="collapse" for="c-36134690">[-]</label><label class="expand" for="c-36134690">[1 more]</label></div><br/><div class="children"><div class="content">I thought SK Hynix was the big producer of HBM? But that could be out of date.</div><br/></div></div></div></div></div></div><div id="36135046" class="c"><input type="checkbox" id="c-36135046" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36133992">parent</a><span>|</span><a href="#36134316">prev</a><span>|</span><a href="#36135341">next</a><span>|</span><label class="collapse" for="c-36135046">[-]</label><label class="expand" for="c-36135046">[1 more]</label></div><br/><div class="children"><div class="content">Cerebras supposedly can: <a href="https:&#x2F;&#x2F;www.servethehome.com&#x2F;cerebras-wafer-scale-engine-2-wse-2-at-hot-chips-33&#x2F;hc33-cerebras-wse-2-swarmx-interconnect&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.servethehome.com&#x2F;cerebras-wafer-scale-engine-2-w...</a><p>In hindsight the 40GB of SRAM feels kind of quaint, but nevertheless their <i>very</i> fat nodes let them get away with more than Nvidia could with A100s, as you can see in the slides.<p>CS2 is a little old now. I bet an update is just around the corner.</div><br/></div></div><div id="36135341" class="c"><input type="checkbox" id="c-36135341" checked=""/><div class="controls bullet"><span class="by">epolanski</span><span>|</span><a href="#36133992">parent</a><span>|</span><a href="#36135046">prev</a><span>|</span><a href="#36135702">next</a><span>|</span><label class="collapse" for="c-36135341">[-]</label><label class="expand" for="c-36135341">[1 more]</label></div><br/><div class="children"><div class="content">As always in economics it is about volumes and margins.<p>If the competitors (mainly AMD, Intel and to some extent ARM) will keep seeing growing volumes and insane margins they will be attracted to bring to invest and take part of that market.<p>Till now gaming GPU market did not bring to AMD the necessary margins to really push them to bring a better competition to Nvidia. Even 10&#x2F;15 years ago when ATI was way ahead of Nvidia technologically for 2&#x2F;3 years (the HD 4000 and HD 5000 generations vs the Nvidia flops of the 9000, 200 and 400 series) Nvidia was posting billions of profits and ATI posted a whole...19 millions of profits across 3 years.<p>But today&#x27;s GPU market thanks to it&#x27;s non-gaming sales is much bigger to ignore (which is why Intel entered it as well) and those players will likely react.<p>You don&#x27;t need to have the best premier product, you need to have your products good and priced well enough that they will be chosen over the competitor&#x27;s.</div><br/></div></div><div id="36135702" class="c"><input type="checkbox" id="c-36135702" checked=""/><div class="controls bullet"><span class="by">ironbound</span><span>|</span><a href="#36133992">parent</a><span>|</span><a href="#36135341">prev</a><span>|</span><a href="#36135317">next</a><span>|</span><label class="collapse" for="c-36135702">[-]</label><label class="expand" for="c-36135702">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like you havent seen Wafer-scale integration computing, Tesla has one and comercial companies like cerebras will sell you a cabnet without the miles of fiber networking.<p><a href="https:&#x2F;&#x2F;www.cerebras.net&#x2F;andromeda&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cerebras.net&#x2F;andromeda&#x2F;</a></div><br/></div></div><div id="36135317" class="c"><input type="checkbox" id="c-36135317" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#36133992">parent</a><span>|</span><a href="#36135702">prev</a><span>|</span><a href="#36134504">next</a><span>|</span><label class="collapse" for="c-36135317">[-]</label><label class="expand" for="c-36135317">[1 more]</label></div><br/><div class="children"><div class="content">Ultimately how fast their transistors can switch and at what power is determined by TSMC, which everyone else can use too. Same for density of interconnect.</div><br/></div></div></div></div><div id="36134504" class="c"><input type="checkbox" id="c-36134504" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#36133992">prev</a><span>|</span><a href="#36135366">next</a><span>|</span><label class="collapse" for="c-36134504">[-]</label><label class="expand" for="c-36134504">[25 more]</label></div><br/><div class="children"><div class="content">There were rumors floating around that GPT-4 was going to be a 100 trillion parameter model. Those rumors seemed ridiculous in hindsight, but this announcement makes me rethink how ridiculous it really was. 100 Terabytes of GPU memory is exactly what you need to train that class of model.<p>However, I’m not even sure enough text data exists in the world to saturate 100T parameters. Maybe if you generated massive quantities of text with GPT-4 and used that dataset as your pre-training data.   Training on the entirety of the internet then becomes just another fine tuning step. The bulk of the training could be on some 400TB dataset of generated text.</div><br/><div id="36135025" class="c"><input type="checkbox" id="c-36135025" checked=""/><div class="controls bullet"><span class="by">EvgeniyZh</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134788">next</a><span>|</span><label class="collapse" for="c-36135025">[-]</label><label class="expand" for="c-36135025">[1 more]</label></div><br/><div class="children"><div class="content">Rule of thumb is that you need ~20 tokens per parameter. The average token size is ~4 characters, probably more for larger models where you want larger dictionary, but for simplicity I&#x27;ll say it&#x27;s 5 bytes to make numbers round. So you need 100 bytes of text data per parameter, or 10 PB for 100T model. Now, recent research says that you can reuse the same data like 4 times before it becomes hindering performance but it doesn&#x27;t help much in our case.<p>But in this case what is really ridiculous is the compute requirement. The required compute for optimal model growth roughly quadratically (both your model and your data grow linearly). So for 100T model you need 1e30 FLOPs. This machine gives you 1e18 FLOPs per second. It will take 30k years to train this model on one of these (or 30k of these to train it in a year, but then utilization will start kicking in).</div><br/></div></div><div id="36134788" class="c"><input type="checkbox" id="c-36134788" checked=""/><div class="controls bullet"><span class="by">arugulum</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36135025">prev</a><span>|</span><a href="#36135001">next</a><span>|</span><label class="collapse" for="c-36134788">[-]</label><label class="expand" for="c-36134788">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Those rumors seemed ridiculous in hindsight<p>No, those rumors seemed ridiculous even then. Many AI influencers were posting some of the most absurd material, often makes basic mistakes (like confusing training tokens with parameters), but anyone in the field could have easily told you that 100T parameters sounded ridiculous.<p>On that note, &quot;100 Terabytes of GPU memory is exactly what you need to train that class of model.&quot; is also likely false. That&#x27;s how much you&#x27;d need to fit such a model into memory at 1 byte per param. Not train it.</div><br/></div></div><div id="36135001" class="c"><input type="checkbox" id="c-36135001" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134788">prev</a><span>|</span><a href="#36134629">next</a><span>|</span><label class="collapse" for="c-36135001">[-]</label><label class="expand" for="c-36135001">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;perf_train_gpu_one#anatomy-of-models-memory" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;perf_train_gpu_one#...</a><p>You can&#x27;t train a 100T model with &quot;only&quot; 100TB of VRAM, you need for each parameters 4 bytes + 4 bytes (gradient) + 8 bytes (AdamW optimizer) + forward activations that depends on the batch size, sequence length etc, maybe more if you use mixed precision and also you need to distribute the weights.</div><br/></div></div><div id="36134629" class="c"><input type="checkbox" id="c-36134629" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36135001">prev</a><span>|</span><a href="#36134536">next</a><span>|</span><label class="collapse" for="c-36134629">[-]</label><label class="expand" for="c-36134629">[7 more]</label></div><br/><div class="children"><div class="content">The general rule of thumb that I&#x27;m familiar with is that you need about 80 bytes of VRAM per parameter when you are doing training. Inference is different and a lot more efficient, and LoRA is also different and more efficient, but training a base model requires a LOT of memory.<p>A machine like this would top out below 2 trillion parameters using the training algorithms that I&#x27;m familiar with.</div><br/><div id="36135227" class="c"><input type="checkbox" id="c-36135227" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134629">parent</a><span>|</span><a href="#36134825">next</a><span>|</span><label class="collapse" for="c-36135227">[-]</label><label class="expand" for="c-36135227">[1 more]</label></div><br/><div class="children"><div class="content">I suppose it would be 12 bytes? 4 bytes for base model, 4 bytes for optimizer momentum and 4 bytes for optimizer second moment EWA.</div><br/></div></div><div id="36134825" class="c"><input type="checkbox" id="c-36134825" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134629">parent</a><span>|</span><a href="#36135227">prev</a><span>|</span><a href="#36134536">next</a><span>|</span><label class="collapse" for="c-36134825">[-]</label><label class="expand" for="c-36134825">[5 more]</label></div><br/><div class="children"><div class="content">Why 80? It&#x27;s matrix operations on 4 byte numbers for single precision.</div><br/><div id="36134879" class="c"><input type="checkbox" id="c-36134879" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134825">parent</a><span>|</span><a href="#36134536">next</a><span>|</span><label class="collapse" for="c-36134879">[-]</label><label class="expand" for="c-36134879">[4 more]</label></div><br/><div class="children"><div class="content">Because you need a lot more information to perform back-propagation.</div><br/><div id="36134932" class="c"><input type="checkbox" id="c-36134932" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134879">parent</a><span>|</span><a href="#36134536">next</a><span>|</span><label class="collapse" for="c-36134932">[-]</label><label class="expand" for="c-36134932">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not &quot;a lot more&quot; information, it&#x27;s holding derivative (single number) per parameter, right?</div><br/><div id="36136079" class="c"><input type="checkbox" id="c-36136079" checked=""/><div class="controls bullet"><span class="by">ioedward</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134932">parent</a><span>|</span><a href="#36135198">next</a><span>|</span><label class="collapse" for="c-36136079">[-]</label><label class="expand" for="c-36136079">[1 more]</label></div><br/><div class="children"><div class="content">You also need the optimizer (e.g. Adam)&#x27;s state, which is usually double the parameter&#x27;s size. So if using fp16, one parameter takes up 6 bytes in memory.</div><br/></div></div><div id="36135198" class="c"><input type="checkbox" id="c-36135198" checked=""/><div class="controls bullet"><span class="by">gmueckl</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134932">parent</a><span>|</span><a href="#36136079">prev</a><span>|</span><a href="#36134536">next</a><span>|</span><label class="collapse" for="c-36135198">[-]</label><label class="expand" for="c-36135198">[1 more]</label></div><br/><div class="children"><div class="content">Not the GP, but I believe that they are talking about the size of the training data set in relation to the model size.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36134536" class="c"><input type="checkbox" id="c-36134536" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134629">prev</a><span>|</span><a href="#36134642">next</a><span>|</span><label class="collapse" for="c-36134536">[-]</label><label class="expand" for="c-36134536">[5 more]</label></div><br/><div class="children"><div class="content">For the numerically challenged like me: 100TB is 100 trillion bytes, giving you 1 byte per parameter at 100T params.<p>LLaMA can apparently run quantized to 4 bits per param (not sure if worth it though), which would allow you to run a 200TB model on one of these cards if I&#x27;m understanding right.</div><br/><div id="36134632" class="c"><input type="checkbox" id="c-36134632" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134536">parent</a><span>|</span><a href="#36135608">next</a><span>|</span><label class="collapse" for="c-36134632">[-]</label><label class="expand" for="c-36134632">[1 more]</label></div><br/><div class="children"><div class="content">I think people talking about a 100T GPT didn&#x27;t mean a dense transformer but some sort of extreme Mixture-of-Experts which is much more amenable to low-resource setups and complicates this discussion.<p>In any case, it&#x27;s almost certainly not bigger than 1T, even if it&#x27;s not a dense transformer (PaLM-2 is and makes do with 340B, but it isn&#x27;t exactly on par).</div><br/></div></div><div id="36135608" class="c"><input type="checkbox" id="c-36135608" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134536">parent</a><span>|</span><a href="#36134632">prev</a><span>|</span><a href="#36135237">next</a><span>|</span><label class="collapse" for="c-36135608">[-]</label><label class="expand" for="c-36135608">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLaMA can apparently run quantized to 4 bits per param (<i>not sure if worth it though</i>)<p>From the GPTQ paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.17323" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.17323</a>:<p>&quot;... <i>with negligible accuracy degradation relative to the uncompressed baseline</i>&quot;</div><br/></div></div><div id="36135237" class="c"><input type="checkbox" id="c-36135237" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134536">parent</a><span>|</span><a href="#36135608">prev</a><span>|</span><a href="#36134589">next</a><span>|</span><label class="collapse" for="c-36135237">[-]</label><label class="expand" for="c-36135237">[1 more]</label></div><br/><div class="children"><div class="content">You can’t quantize it for training due to numerical instability. For inference you don’t usually use such a big cluster.</div><br/></div></div><div id="36134589" class="c"><input type="checkbox" id="c-36134589" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134536">parent</a><span>|</span><a href="#36135237">prev</a><span>|</span><a href="#36134642">next</a><span>|</span><label class="collapse" for="c-36134589">[-]</label><label class="expand" for="c-36134589">[1 more]</label></div><br/><div class="children"><div class="content">That would work for inference, but for efficient training you’d also want you training set to fit in memory.</div><br/></div></div></div></div><div id="36134642" class="c"><input type="checkbox" id="c-36134642" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134536">prev</a><span>|</span><a href="#36134746">next</a><span>|</span><label class="collapse" for="c-36134642">[-]</label><label class="expand" for="c-36134642">[2 more]</label></div><br/><div class="children"><div class="content">&gt; However, I’m not even sure enough text data exists in the world<p>I hope these models move significantly beyond text at some point. For backend programmers it&#x27;s ok, but for the rest of the technical world (circuits, mechanical engineering, front end, sound, etc), it&#x27;s fairly limited.</div><br/><div id="36135335" class="c"><input type="checkbox" id="c-36135335" checked=""/><div class="controls bullet"><span class="by">liamwire</span><span>|</span><a href="#36134504">root</a><span>|</span><a href="#36134642">parent</a><span>|</span><a href="#36134746">next</a><span>|</span><label class="collapse" for="c-36135335">[-]</label><label class="expand" for="c-36135335">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that this is already the case, see PaLM-E as one such example of a multimodal model.</div><br/></div></div></div></div><div id="36134746" class="c"><input type="checkbox" id="c-36134746" checked=""/><div class="controls bullet"><span class="by">in3d</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134642">prev</a><span>|</span><a href="#36134990">next</a><span>|</span><label class="collapse" for="c-36134746">[-]</label><label class="expand" for="c-36134746">[1 more]</label></div><br/><div class="children"><div class="content">These 100T rumors were ridiculous from the start, not just in hindsight.</div><br/></div></div><div id="36134990" class="c"><input type="checkbox" id="c-36134990" checked=""/><div class="controls bullet"><span class="by">martinko</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134746">prev</a><span>|</span><a href="#36134877">next</a><span>|</span><label class="collapse" for="c-36134990">[-]</label><label class="expand" for="c-36134990">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Maybe if you generated massive quantities of text with GPT-4 and used that dataset as your pre-training data<p>Hello spurious regression</div><br/></div></div><div id="36134877" class="c"><input type="checkbox" id="c-36134877" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134990">prev</a><span>|</span><a href="#36134878">next</a><span>|</span><label class="collapse" for="c-36134877">[-]</label><label class="expand" for="c-36134877">[1 more]</label></div><br/><div class="children"><div class="content">There may not be enough text content on the internet, but there’s plenty of audio and video content, and there has already been some research about connecting that as an input to an LLM. So far we’ve seen that the more diverse the training data the more versatile the model, so I suspect multi-modal input training is inevitably where LLM’s are going.</div><br/></div></div><div id="36134878" class="c"><input type="checkbox" id="c-36134878" checked=""/><div class="controls bullet"><span class="by">ivalm</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134877">prev</a><span>|</span><a href="#36134766">next</a><span>|</span><label class="collapse" for="c-36134878">[-]</label><label class="expand" for="c-36134878">[1 more]</label></div><br/><div class="children"><div class="content">You don’t really need to fit fully in memory. Memory requirement to train is<p>~6DP * precision<p>Where D is number of tokens*mini batch size and P is number of parameters.<p>So if you want to fit fully into memory with a mini batch of 1, context window 32k, and 16 bit precision, that’s
144e12&#x2F;6&#x2F;32e3&#x2F;2 = 375M param.<p>If you apply one token at a time then<p>144e12&#x2F;6&#x2F;2 = 12 T param<p>Ofc, in reality you have model parallelism as well…</div><br/></div></div><div id="36134766" class="c"><input type="checkbox" id="c-36134766" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134878">prev</a><span>|</span><a href="#36134743">next</a><span>|</span><label class="collapse" for="c-36134766">[-]</label><label class="expand" for="c-36134766">[1 more]</label></div><br/><div class="children"><div class="content">As far as I can tell, the &quot;100 trillion&quot; number comes from an interview with the CEO of Cerebras when he was doing press for the WSE-2 release in 2021: <a href="https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;cerebras-chip-cluster-neural-networks-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;cerebras-chip-cluster-neural-net...</a></div><br/></div></div><div id="36134743" class="c"><input type="checkbox" id="c-36134743" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134766">prev</a><span>|</span><a href="#36134848">next</a><span>|</span><label class="collapse" for="c-36134743">[-]</label><label class="expand" for="c-36134743">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s record every conversation on Android to collect training data! Anyone can do the math?</div><br/></div></div><div id="36134848" class="c"><input type="checkbox" id="c-36134848" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36134504">parent</a><span>|</span><a href="#36134743">prev</a><span>|</span><a href="#36135366">next</a><span>|</span><label class="collapse" for="c-36134848">[-]</label><label class="expand" for="c-36134848">[1 more]</label></div><br/><div class="children"><div class="content">There is much more out there than text. Audio, visual, touch, smell. Text isn&#x27;t something humans directly train on, but representations of text from our senses.<p>GPT-4 was trained on image data. Besides gaining understanding of image content it also showed improved language abilities over a GPT-4 trained with only text. Facebook is working on a smaller model with text, image, video, audio, lidar depth, infrared heat, and 6-axis motion data. If a GPT-4 was trained with data like that, what capabilities would it have? Rumor says we will know in a few months.</div><br/></div></div></div></div><div id="36135366" class="c"><input type="checkbox" id="c-36135366" checked=""/><div class="controls bullet"><span class="by">unwind</span><span>|</span><a href="#36134504">prev</a><span>|</span><a href="#36135433">next</a><span>|</span><label class="collapse" for="c-36135366">[-]</label><label class="expand" for="c-36135366">[6 more]</label></div><br/><div class="children"><div class="content">This is awe-inspiring and almost scary, it&#x27;s pretty much beyond my understanding how much data these systems are meant to process.<p>What is also beyond me is how someone at Nvidia thinks that the label sequence &quot;1.00E+2; 1.00E+3; 1.00E+4; 1.00E+5; 1.00E+6&quot; for the vertical axis in &quot;Figure 1&quot; is more readable than &quot;100; 1,000; 10,000; 100,000; 1,000,000&quot; would have been. The latter is 5 chars less (total), even. Or, if exponential notation is important for the Big Serious Computing People, then perhaps they could have dropped the &quot;.00&quot; part from each value? Or, if I&#x27;m allowed to dream, gone with actual exponential notation?</div><br/><div id="36135449" class="c"><input type="checkbox" id="c-36135449" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#36135366">parent</a><span>|</span><a href="#36135909">next</a><span>|</span><label class="collapse" for="c-36135449">[-]</label><label class="expand" for="c-36135449">[2 more]</label></div><br/><div class="children"><div class="content">The exponent number is the number of zeros, its way more readable and faster to interpret than counting zeros.</div><br/><div id="36135906" class="c"><input type="checkbox" id="c-36135906" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#36135366">root</a><span>|</span><a href="#36135449">parent</a><span>|</span><a href="#36135909">next</a><span>|</span><label class="collapse" for="c-36135906">[-]</label><label class="expand" for="c-36135906">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easier to think in (possibly relative) orders of magnitude than with absolute numbers, instinctively it&#x27;s what we do when we read large numbers.</div><br/></div></div></div></div><div id="36135909" class="c"><input type="checkbox" id="c-36135909" checked=""/><div class="controls bullet"><span class="by">timthelion</span><span>|</span><a href="#36135366">parent</a><span>|</span><a href="#36135449">prev</a><span>|</span><a href="#36135612">next</a><span>|</span><label class="collapse" for="c-36135909">[-]</label><label class="expand" for="c-36135909">[1 more]</label></div><br/><div class="children"><div class="content">There is a semantic difference: 1,000,000 == 1,000,000 where-as 1.00E+6 &gt;= 1,000,000 &lt; 1,010,000. The decimal places after the 1 in 1.00E+6 specify the precision of the measurement.</div><br/></div></div><div id="36135612" class="c"><input type="checkbox" id="c-36135612" checked=""/><div class="controls bullet"><span class="by">tuetuopay</span><span>|</span><a href="#36135366">parent</a><span>|</span><a href="#36135909">prev</a><span>|</span><a href="#36135408">next</a><span>|</span><label class="collapse" for="c-36135612">[-]</label><label class="expand" for="c-36135612">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the scientific notation, and makes the graph to be a log scale. It allows you to see they gained more than two orders of magnitude in a single generation.</div><br/></div></div><div id="36135408" class="c"><input type="checkbox" id="c-36135408" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#36135366">parent</a><span>|</span><a href="#36135612">prev</a><span>|</span><a href="#36135433">next</a><span>|</span><label class="collapse" for="c-36135408">[-]</label><label class="expand" for="c-36135408">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what&#x27;s the problem with the exponential notation? It shows scale in order of magnitudes.</div><br/></div></div></div></div><div id="36135433" class="c"><input type="checkbox" id="c-36135433" checked=""/><div class="controls bullet"><span class="by">fock</span><span>|</span><a href="#36135366">prev</a><span>|</span><a href="#36135622">next</a><span>|</span><label class="collapse" for="c-36135433">[-]</label><label class="expand" for="c-36135433">[1 more]</label></div><br/><div class="children"><div class="content">So how exactly (in the technical sense) is this more energy efficient than both PCIe and Infiniband (which seems to be a claim somewhere too, together with the added bandwidth)?<p>EDIT: so the whitepaper is surprisingly good for that (somehow all the articles are very weird...):  <a href="https:&#x2F;&#x2F;resources.nvidia.com&#x2F;en-us-grace-cpu&#x2F;nvidia-grace-hopper" rel="nofollow">https:&#x2F;&#x2F;resources.nvidia.com&#x2F;en-us-grace-cpu&#x2F;nvidia-grace-ho...</a> - essentially they connect the GPUs with NVlink instead of PCIe (so, vertical integrators heaven) and then NVLink forms a separate interconnect for GPUs. So this is cool and essentially what Fujitsu, Google, ... have done for some time. A fun thing is, that they like to add up their nvlink-duplex bandwith and don&#x27;t do for PCIe... (which then suddenly would have the same bandwith as the GPU-side).<p>Still very cool to see the mainframe come back alive ...<p>(it&#x27;s a bit sad they bought Mellanox - monopolies are sad...)</div><br/></div></div><div id="36135622" class="c"><input type="checkbox" id="c-36135622" checked=""/><div class="controls bullet"><span class="by">xipix</span><span>|</span><a href="#36135433">prev</a><span>|</span><a href="#36134381">next</a><span>|</span><label class="collapse" for="c-36135622">[-]</label><label class="expand" for="c-36135622">[1 more]</label></div><br/><div class="children"><div class="content">A large system, so much higher chance of something breaking.<p>What happens if it loses a node or a link? Or some memory becomes unreliable? This thing needs some sophisticated fault tolerance.</div><br/></div></div><div id="36134381" class="c"><input type="checkbox" id="c-36134381" checked=""/><div class="controls bullet"><span class="by">mupuff1234</span><span>|</span><a href="#36135622">prev</a><span>|</span><a href="#36133537">next</a><span>|</span><label class="collapse" for="c-36134381">[-]</label><label class="expand" for="c-36134381">[13 more]</label></div><br/><div class="children"><div class="content">Did supercomputers ever produce something meaningful or did advancement usually come out of more &quot;scrappier&quot; setups?<p>I remember hearing a lot about rankings of supercomputers, but less so about what they actually achieved.</div><br/><div id="36134580" class="c"><input type="checkbox" id="c-36134580" checked=""/><div class="controls bullet"><span class="by">gwoolhurme</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36135289">next</a><span>|</span><label class="collapse" for="c-36134580">[-]</label><label class="expand" for="c-36134580">[1 more]</label></div><br/><div class="children"><div class="content">Yeah they do all the time. I remember in my parallel computing course where we got to use our 800 core test PC back in grad school where people were running simulations of different weather patterns and climate change. Earthquake simulations and what not. A lot of that can be done taking advantage of all of those cores. Academia specifically heavily uses these to get closer to the &quot;physics&quot; with clear discrete limitations</div><br/></div></div><div id="36135289" class="c"><input type="checkbox" id="c-36135289" checked=""/><div class="controls bullet"><span class="by">lannisterstark</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36134580">prev</a><span>|</span><a href="#36134389">next</a><span>|</span><label class="collapse" for="c-36135289">[-]</label><label class="expand" for="c-36135289">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Did supercomputers ever produce something meaningful<p>supercomputers do all the hard work in research universities all the time. Hell, astrophysics and research involving telescopes and observatories use em all the time.</div><br/></div></div><div id="36134389" class="c"><input type="checkbox" id="c-36134389" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36135289">prev</a><span>|</span><a href="#36134750">next</a><span>|</span><label class="collapse" for="c-36134389">[-]</label><label class="expand" for="c-36134389">[1 more]</label></div><br/><div class="children"><div class="content">Supercomputers exist in meaningful part to compensate for our lack of ability to do nuclear tests. This is why the national labs run them.</div><br/></div></div><div id="36134750" class="c"><input type="checkbox" id="c-36134750" checked=""/><div class="controls bullet"><span class="by">zacmps</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36134389">prev</a><span>|</span><a href="#36134784">next</a><span>|</span><label class="collapse" for="c-36134750">[-]</label><label class="expand" for="c-36134750">[1 more]</label></div><br/><div class="children"><div class="content">&gt; supercomputers ever produce something meaningful<p>Absolutely, they contribute to research all of the time.<p>Some of them have pages where they list research outputs that they enabled (though this is of course limited to those authors tell them about!).</div><br/></div></div><div id="36134784" class="c"><input type="checkbox" id="c-36134784" checked=""/><div class="controls bullet"><span class="by">MooMooMilkParty</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36134750">prev</a><span>|</span><a href="#36135136">next</a><span>|</span><label class="collapse" for="c-36134784">[-]</label><label class="expand" for="c-36134784">[1 more]</label></div><br/><div class="children"><div class="content">Yes, absolutely. Most climate models run on supercomputers, same with molecular dynamics, large scale fluid dynamics, energy systems simulations and of course a whole lot of weapons research.</div><br/></div></div><div id="36135136" class="c"><input type="checkbox" id="c-36135136" checked=""/><div class="controls bullet"><span class="by">fhe</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36134784">prev</a><span>|</span><a href="#36135053">next</a><span>|</span><label class="collapse" for="c-36135136">[-]</label><label class="expand" for="c-36135136">[1 more]</label></div><br/><div class="children"><div class="content">Ever better weather forecast for one. I can remember that, about two decades ago, weather forecast was still rather wobbly, and could only see a couple days into the future. Now 10-day forecast is routine, and surprisingly good. Much of that improvement came about as a result of more powerful supercomputers.</div><br/></div></div><div id="36135053" class="c"><input type="checkbox" id="c-36135053" checked=""/><div class="controls bullet"><span class="by">carabiner</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36135136">prev</a><span>|</span><a href="#36135391">next</a><span>|</span><label class="collapse" for="c-36135053">[-]</label><label class="expand" for="c-36135053">[1 more]</label></div><br/><div class="children"><div class="content">Weather forecasts are vastly more accurate because of supercomputers. And they&#x27;re improving all the time.</div><br/></div></div><div id="36135391" class="c"><input type="checkbox" id="c-36135391" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36135053">prev</a><span>|</span><a href="#36134420">next</a><span>|</span><label class="collapse" for="c-36135391">[-]</label><label class="expand" for="c-36135391">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve never had architectures that scale so effectively, unlocking new cognitive capabilities by just increasing parameters&#x2F;exaflops&#x2F;datasets without writing a lot more code or changing the architecture. Ilya Sutskever mentioned this in some interview, that transformers are the first with that property but probably won&#x27;t be the last or best.</div><br/></div></div><div id="36134420" class="c"><input type="checkbox" id="c-36134420" checked=""/><div class="controls bullet"><span class="by">anaganisk</span><span>|</span><a href="#36134381">parent</a><span>|</span><a href="#36135391">prev</a><span>|</span><a href="#36134751">next</a><span>|</span><label class="collapse" for="c-36134420">[-]</label><label class="expand" for="c-36134420">[3 more]</label></div><br/><div class="children"><div class="content">Google might&#x27;ve been built on a laptop, but it can&#x27;t scale on a laptop.
Same applies to coding an algorithm on a scrappy setup,  and then scaling it to sequence DNA or simulate a phenomenon.</div><br/><div id="36134443" class="c"><input type="checkbox" id="c-36134443" checked=""/><div class="controls bullet"><span class="by">kortilla</span><span>|</span><a href="#36134381">root</a><span>|</span><a href="#36134420">parent</a><span>|</span><a href="#36134751">next</a><span>|</span><label class="collapse" for="c-36134443">[-]</label><label class="expand" for="c-36134443">[2 more]</label></div><br/><div class="children"><div class="content">Not sure what this means, because google effectively scaled on laptops (generic x86).</div><br/><div id="36134678" class="c"><input type="checkbox" id="c-36134678" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#36134381">root</a><span>|</span><a href="#36134443">parent</a><span>|</span><a href="#36134751">next</a><span>|</span><label class="collapse" for="c-36134678">[-]</label><label class="expand" for="c-36134678">[1 more]</label></div><br/><div class="children"><div class="content">In my head the way I differentiate &quot;supercomputers&quot; (national labs) and &quot;warehouse-scale computers&quot; (google&#x2F;amazon&#x2F;azure) is:<p>1. workload
for national labs this is mostly sparse fp64 in my understanding, for warehouse-scale computing is lots of integer work, highly branchy, lots of pointer chasing, stuff like that.<p>2. latency&#x2F;reliability vs throughput
warehouse-scale computing jobs often run at awful utilization, in the 5-20% range depending on how you measure, in order to respond to shocks of various kinds and provide nice abstractions for developers. fundamentally these systems are used live by humans and human time is very valuable so making sure it stays up always and returns quickly is paramount. In my understanding supercomputing workloads are much more throughput-oriented, where you need to do an enormous amount of computation to get some answer but it doesn&#x27;t much matter whether the answer comes in one week or two weeks.<p>3. interconnect
warehouse-scale computing workloads are mostly fairly separable and the place where different requests become intertangled is in the database. In the supercomputing world, in my understanding, there are often significant interconnect needs all the time, so extremely high performance networking is emphasized.</div><br/></div></div></div></div></div></div></div></div><div id="36133537" class="c"><input type="checkbox" id="c-36133537" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36134381">prev</a><span>|</span><a href="#36133892">next</a><span>|</span><label class="collapse" for="c-36133537">[-]</label><label class="expand" for="c-36133537">[16 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t find how much it will cost or how much power it will use. I mean it will be a lot and maybe only Google and Microsoft and Facebook can afford it but I still want to know.</div><br/><div id="36133665" class="c"><input type="checkbox" id="c-36133665" checked=""/><div class="controls bullet"><span class="by">Bedon292</span><span>|</span><a href="#36133537">parent</a><span>|</span><a href="#36135919">next</a><span>|</span><label class="collapse" for="c-36133665">[-]</label><label class="expand" for="c-36133665">[8 more]</label></div><br/><div class="children"><div class="content">The DGX A100 was $200k at launch. I found a DGX H100 in the mid $300k area. And those are 8 GPU systems. So you need 32 of those, and each one will definitely cost more plus networking. Super low estimate would be $500k each for $16M total. But considering its moving from 98GB to 480GB RAM per GPU. Might be more like $1.5M per 8, round it to say $50M.<p>And at 1&#x2F;8th the power per GB, you have 700 Watts &#x2F; 96GB &#x2F; 8 * 480GB come to around 450 Watts per. And 115kw for the 256.</div><br/><div id="36133865" class="c"><input type="checkbox" id="c-36133865" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36133665">parent</a><span>|</span><a href="#36134406">next</a><span>|</span><label class="collapse" for="c-36133865">[-]</label><label class="expand" for="c-36133865">[3 more]</label></div><br/><div class="children"><div class="content">What does this mean for the AI race? For example what if a newish company (newer than Google&#x2F;Facebook&#x2F;Microsoft&#x2F;etc.) like Anthropic, Scale, Perplexity, or Stability is able to scrape together $5B USD funding and spend their hardware budget on these things. Say that can buy $1B of them and spend the rest on hackers and operating expenses (idk if that&#x27;s realistic). So maybe they could purchase and operate like 20 of them. Say that they spend six months doing experimental things and then the next six months training their Tsar Model. If they follow the Chinchilla scaling laws and normal architectures, how good will these models be?</div><br/><div id="36134031" class="c"><input type="checkbox" id="c-36134031" checked=""/><div class="controls bullet"><span class="by">bushbaba</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36133865">parent</a><span>|</span><a href="#36134886">next</a><span>|</span><label class="collapse" for="c-36134031">[-]</label><label class="expand" for="c-36134031">[1 more]</label></div><br/><div class="children"><div class="content">For AI race means it opens the door to a competitor. Could be AMD, Google, or Amazon. All which have offerings in this space.<p>However while the hardware isn’t cheap, it’s still likely not a blocker. Costs do  inhibitor more experimental research.</div><br/></div></div><div id="36134886" class="c"><input type="checkbox" id="c-36134886" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36133865">parent</a><span>|</span><a href="#36134031">prev</a><span>|</span><a href="#36134406">next</a><span>|</span><label class="collapse" for="c-36134886">[-]</label><label class="expand" for="c-36134886">[1 more]</label></div><br/><div class="children"><div class="content">Startup which will rent this compute when needed will likely have more advantage on AI front.<p>Selling shovels is good business, but doesn&#x27;t compete directly in AI area.</div><br/></div></div></div></div><div id="36134406" class="c"><input type="checkbox" id="c-36134406" checked=""/><div class="controls bullet"><span class="by">nvy</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36133665">parent</a><span>|</span><a href="#36133865">prev</a><span>|</span><a href="#36134172">next</a><span>|</span><label class="collapse" for="c-36134406">[-]</label><label class="expand" for="c-36134406">[1 more]</label></div><br/><div class="children"><div class="content">With that much memory I could probably run Crysis 4 and an Electron app side by side!</div><br/></div></div><div id="36134172" class="c"><input type="checkbox" id="c-36134172" checked=""/><div class="controls bullet"><span class="by">BonoboIO</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36133665">parent</a><span>|</span><a href="#36134406">prev</a><span>|</span><a href="#36135919">next</a><span>|</span><label class="collapse" for="c-36134172">[-]</label><label class="expand" for="c-36134172">[3 more]</label></div><br/><div class="children"><div class="content">I have no expertise in GPU System used for AI Learning, but would It be possible to buy a bunch of consumer cards and get the same performance?
Or is this not possible because consumer cards go to 40 ish GB RAM and Models would not fit or „swapping“ like crazy and be slow.</div><br/><div id="36135856" class="c"><input type="checkbox" id="c-36135856" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36134172">parent</a><span>|</span><a href="#36134385">next</a><span>|</span><label class="collapse" for="c-36135856">[-]</label><label class="expand" for="c-36135856">[1 more]</label></div><br/><div class="children"><div class="content">Not the same.  Not all problems can be efficiently divided among NUMA nodes with low bandwidth interconnects.</div><br/></div></div><div id="36134385" class="c"><input type="checkbox" id="c-36134385" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36134172">parent</a><span>|</span><a href="#36135856">prev</a><span>|</span><a href="#36135919">next</a><span>|</span><label class="collapse" for="c-36134385">[-]</label><label class="expand" for="c-36134385">[1 more]</label></div><br/><div class="children"><div class="content"><i>would It be possible</i><p>No</div><br/></div></div></div></div></div></div><div id="36135919" class="c"><input type="checkbox" id="c-36135919" checked=""/><div class="controls bullet"><span class="by">einarfd</span><span>|</span><a href="#36133537">parent</a><span>|</span><a href="#36133665">prev</a><span>|</span><a href="#36134168">next</a><span>|</span><label class="collapse" for="c-36135919">[-]</label><label class="expand" for="c-36135919">[1 more]</label></div><br/><div class="children"><div class="content">It’s in interest of Nvidia, to try to make sure they do not end up in a situation where they have a small group of very big customer that buy a large slice of their production. For Nvidia a market of the same size, with many small to medium customer is a lot better as those customers will have a lot less power to force Nvidia to do something that isn&#x27;t in it&#x27;s interest or it does not want.
I expect to see moves from Nvidia to help smaller players, open source or semi open models to not be crushed by the big players. Not because they are nice, but because it is in their best interest.</div><br/></div></div><div id="36134168" class="c"><input type="checkbox" id="c-36134168" checked=""/><div class="controls bullet"><span class="by">bigmattystyles</span><span>|</span><a href="#36133537">parent</a><span>|</span><a href="#36135919">prev</a><span>|</span><a href="#36133892">next</a><span>|</span><label class="collapse" for="c-36134168">[-]</label><label class="expand" for="c-36134168">[6 more]</label></div><br/><div class="children"><div class="content">I wonder if you can even run this on a regular 20A circuit - I&#x27;m thinking no - 20 * 120 = 2400 Watts - I assume that will not be enough...</div><br/><div id="36134853" class="c"><input type="checkbox" id="c-36134853" checked=""/><div class="controls bullet"><span class="by">mattlondon</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36134168">parent</a><span>|</span><a href="#36134465">next</a><span>|</span><label class="collapse" for="c-36134853">[-]</label><label class="expand" for="c-36134853">[2 more]</label></div><br/><div class="children"><div class="content">My hob in my kitchen is 7.3kw? Normal 32a * 240 circuit allows up to 7.68kw, and the 6mm^2 cable is rated to something like 45amps<p>This seems fairly common e.g. <a href="https:&#x2F;&#x2F;www.currys.co.uk&#x2F;products&#x2F;aeg-ikb64401fb-59-cm-electric-induction-hob-black-10188242.html" rel="nofollow">https:&#x2F;&#x2F;www.currys.co.uk&#x2F;products&#x2F;aeg-ikb64401fb-59-cm-elect...</a><p>I am sure data centers have larger circuit breakers and chunkier cables than my kitchen appliances!</div><br/><div id="36135744" class="c"><input type="checkbox" id="c-36135744" checked=""/><div class="controls bullet"><span class="by">throwaway2037</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36134853">parent</a><span>|</span><a href="#36134465">next</a><span>|</span><label class="collapse" for="c-36135744">[-]</label><label class="expand" for="c-36135744">[1 more]</label></div><br/><div class="children"><div class="content">Woah, I looked at the specs:<p><pre><code>    Front left: 2.3 kW &#x2F; 3.7 kW
</code></pre>
Cripes.  You can boil water extremely fast on that IH setup!  I&#x27;m living with 1.5 kW, and it is painful...</div><br/></div></div></div></div><div id="36134465" class="c"><input type="checkbox" id="c-36134465" checked=""/><div class="controls bullet"><span class="by">jeffnappi</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36134168">parent</a><span>|</span><a href="#36134853">prev</a><span>|</span><a href="#36134256">next</a><span>|</span><label class="collapse" for="c-36134465">[-]</label><label class="expand" for="c-36134465">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s an example of an 8x H100 machine - look at the tech specs: <a href="https:&#x2F;&#x2F;lambdalabs.com&#x2F;deep-learning&#x2F;servers&#x2F;hyperplane" rel="nofollow">https:&#x2F;&#x2F;lambdalabs.com&#x2F;deep-learning&#x2F;servers&#x2F;hyperplane</a><p>6x 3000W PSUs in a 3x3 redundant config. So 9000 watts total. So at least 240v x 50A. x2 for redundancy.</div><br/></div></div><div id="36134256" class="c"><input type="checkbox" id="c-36134256" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#36133537">root</a><span>|</span><a href="#36134168">parent</a><span>|</span><a href="#36134465">prev</a><span>|</span><a href="#36134242">next</a><span>|</span><label class="collapse" for="c-36134256">[-]</label><label class="expand" for="c-36134256">[1 more]</label></div><br/><div class="children"><div class="content">For a single Grace+Hopper node? I&#x27;d bet it fits in that budget, the Grace Hopper datasheet says the combo has a CPU + GPU + memory TDP of 450W - 1000W programmable, and that leaves more than half of the room for the rest of the node&#x27;s power budget. For the DGX GH200? It&#x27;s 18,432 CPU cores with 256 GPUs across 16 full racks of servers :p.</div><br/></div></div></div></div></div></div><div id="36133892" class="c"><input type="checkbox" id="c-36133892" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#36133537">prev</a><span>|</span><a href="#36134558">next</a><span>|</span><label class="collapse" for="c-36133892">[-]</label><label class="expand" for="c-36133892">[31 more]</label></div><br/><div class="children"><div class="content">Wow, 480 GB per GPU! What happened to end of Moore&#x27;s law?<p>I hope this improvement translates to consumer GPUs as 24GB is a big limitation.</div><br/><div id="36133966" class="c"><input type="checkbox" id="c-36133966" checked=""/><div class="controls bullet"><span class="by">bushbaba</span><span>|</span><a href="#36133892">parent</a><span>|</span><a href="#36135971">next</a><span>|</span><label class="collapse" for="c-36133966">[-]</label><label class="expand" for="c-36133966">[16 more]</label></div><br/><div class="children"><div class="content">Moores law stated transistor density per square inch roughly doubled every 2 years.<p>I see no end in sight for that specific law. As we can always go vertically if needed. It’s also held through so far in 2020[1].<p>The folks who conflated moores law to also mean doubling of compute processing capabilities of a CPU double every 2 years were wrong.<p>[1]<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moore%27s_law" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moore%27s_law</a></div><br/><div id="36134205" class="c"><input type="checkbox" id="c-36134205" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133966">parent</a><span>|</span><a href="#36134062">next</a><span>|</span><label class="collapse" for="c-36134205">[-]</label><label class="expand" for="c-36134205">[2 more]</label></div><br/><div class="children"><div class="content">Per your link, Moore&#x27;s law also doesn&#x27;t state anything about density. Density is just one of the ways &quot;The complexity for minimum component costs has increased at a rate of roughly a factor of two per year.&quot;, i.e. Moore&#x27;s law only ever stated transistor <i>count per a given price</i> roughly doubled every 2 years.</div><br/><div id="36134340" class="c"><input type="checkbox" id="c-36134340" checked=""/><div class="controls bullet"><span class="by">reaperman</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134205">parent</a><span>|</span><a href="#36134062">next</a><span>|</span><label class="collapse" for="c-36134340">[-]</label><label class="expand" for="c-36134340">[1 more]</label></div><br/><div class="children"><div class="content">Moore&#x27;s original article on the topic in 1965[0], and the same with additional context interview form 2005[1].<p>&gt;  &quot;The original Moore’s Law came out of an article I published in 1965...I had
no idea this was going to be an accurate prediction, but amazingly enough instead of ten doubling, we got 9 over the 10 years, but still followed pretty well along the curve. And one of my friends, Dr. Carver Mead, a Professor at Cal Tech, dubbed this Moore’s Law. So the original one was doubling every year in complexity now in 1975, I had to go back and revisit this... and I noticed we were losing one of the key factors that let us make this remarkable rate of progress... and it was one that was contributing about half of the advances were making. So then I changed it to looking forward, we’d only be doubling every couple of years, and that was really the two predictions I made. Now the one that gets quoted is doubling every 18 months...I think it was Dave House, who used to work here at Intel, did that, he decided that the complexity was doubling every two years and the transistors were getting faster, that computer performance was going to double every 18 months... but that’s what got on Intel’s Website... and everything else. I never said 18 months that’s the way it often gets quoted.&quot;<p>Anyways,  See slide 13 here[2] (2021). &quot;Pop-culture&quot; Moore&#x27;s law stated that the number of transistors per area will double every n months. That&#x27;s still happening. Besides, neither Moore&#x27;s law nor Dennard scaling are even the most critical scaling law to be concerned about...<p>...that&#x27;s probably Koomey&#x27;s law[3][5], which looks well on track to hold for the rest of our careers. But eventually as computing approaches the Landauer limit[4] it must asymptotically level off as well. Probably starting around year 2050. Then we&#x27;ll need to actually start &quot;doing more with less&quot; and minimizing the number of computations done for specific tasks. That will begin a very very productive time for custom silicon that is very task-specialized and low-level algorithmic optimization.<p>[2] Shows that Moore&#x27;s law (green line) is expected to start leveling off soon, but it has not yet slowed down. It also shows Koomey&#x27;s law (orange line) holding indefinitely. Fun fact, if Koomey&#x27;s law holds, we&#x27;ll have exaflop power in &lt;20W in about 20 years. Which should be enough for people to create ChatGPT-4 in their pocket.<p>0: <a href="https:&#x2F;&#x2F;www.rfcafe.com&#x2F;references&#x2F;electronics-mag&#x2F;gordon-moores-law-electronics-mag-april-19-1965.htm" rel="nofollow">https:&#x2F;&#x2F;www.rfcafe.com&#x2F;references&#x2F;electronics-mag&#x2F;gordon-moo...</a><p>1: <a href="https:&#x2F;&#x2F;cdn3.weka-fachmedien.de&#x2F;media_uploads&#x2F;documents&#x2F;1429521922-13-gordonmoore1965article.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn3.weka-fachmedien.de&#x2F;media_uploads&#x2F;documents&#x2F;1429...</a><p>2: (Slide 13) <a href="https:&#x2F;&#x2F;www.sec.gov&#x2F;Archives&#x2F;edgar&#x2F;data&#x2F;937966&#x2F;000119312521287742&#x2F;d133133dex993.htm" rel="nofollow">https:&#x2F;&#x2F;www.sec.gov&#x2F;Archives&#x2F;edgar&#x2F;data&#x2F;937966&#x2F;0001193125212...</a><p>3: &quot;The constant rate of doubling of the number of computations per joule of energy dissipated&quot; <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Koomey%27s_law" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Koomey%27s_law</a><p>4: &quot;The thermodynamic limit for the minimum amount of energy theoretically necessary to perform an irreversible single-bit operation.&quot; <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Landauer%27s_principle" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Landauer%27s_principle</a><p>5: <a href="https:&#x2F;&#x2F;www.koomey.com&#x2F;post&#x2F;14466436072" rel="nofollow">https:&#x2F;&#x2F;www.koomey.com&#x2F;post&#x2F;14466436072</a><p>6: <a href="https:&#x2F;&#x2F;www.koomey.com&#x2F;post&#x2F;153838038643" rel="nofollow">https:&#x2F;&#x2F;www.koomey.com&#x2F;post&#x2F;153838038643</a></div><br/></div></div></div></div><div id="36134062" class="c"><input type="checkbox" id="c-36134062" checked=""/><div class="controls bullet"><span class="by">bigmattystyles</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133966">parent</a><span>|</span><a href="#36134205">prev</a><span>|</span><a href="#36134065">next</a><span>|</span><label class="collapse" for="c-36134062">[-]</label><label class="expand" for="c-36134062">[3 more]</label></div><br/><div class="children"><div class="content">To be fair, if you stack them, density is not going up - only if ignore the number of stacks and take one of their areas for the total number of stacked transistors would it then go up. 
Plus, stacking is great, but with heat issues, isn&#x27;t the industry going to many dielets with a massive interconnect?</div><br/><div id="36134350" class="c"><input type="checkbox" id="c-36134350" checked=""/><div class="controls bullet"><span class="by">reaperman</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134062">parent</a><span>|</span><a href="#36134723">next</a><span>|</span><label class="collapse" for="c-36134350">[-]</label><label class="expand" for="c-36134350">[1 more]</label></div><br/><div class="children"><div class="content">Heat issues are very valid. But the &quot;per square inch&quot; density is relative to a square inch of fab wafer. So if it can be done on one wafer, it counts. If it&#x27;s stacking discrete chipsets, not so much.</div><br/></div></div><div id="36134723" class="c"><input type="checkbox" id="c-36134723" checked=""/><div class="controls bullet"><span class="by">grogenaut</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134062">parent</a><span>|</span><a href="#36134350">prev</a><span>|</span><a href="#36134065">next</a><span>|</span><label class="collapse" for="c-36134723">[-]</label><label class="expand" for="c-36134723">[1 more]</label></div><br/><div class="children"><div class="content">If measured in square inch the 3rd and more importantly the 4th dimension are not accounted for and are basically free.<p>Another way to say it is to count the famous founder brown: ymmoore wasn&#x27;t thinking 4th dimensionally.<p>For shame really</div><br/></div></div></div></div><div id="36134065" class="c"><input type="checkbox" id="c-36134065" checked=""/><div class="controls bullet"><span class="by">rudedogg</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133966">parent</a><span>|</span><a href="#36134062">prev</a><span>|</span><a href="#36134056">next</a><span>|</span><label class="collapse" for="c-36134065">[-]</label><label class="expand" for="c-36134065">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I see no end in sight for that specific law. As we can always go vertically if needed.<p>Not a hardware person but heat dissipation becomes more of a problem when you go vertical IIRC.</div><br/></div></div><div id="36134056" class="c"><input type="checkbox" id="c-36134056" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133966">parent</a><span>|</span><a href="#36134065">prev</a><span>|</span><a href="#36134058">next</a><span>|</span><label class="collapse" for="c-36134056">[-]</label><label class="expand" for="c-36134056">[2 more]</label></div><br/><div class="children"><div class="content">Smaller transistor means lower energy consumption, going vertical won’t solve this.<p>This ceiling will be hit much earlier than what process&#x2F;technique allowed.</div><br/><div id="36134067" class="c"><input type="checkbox" id="c-36134067" checked=""/><div class="controls bullet"><span class="by">bigmattystyles</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134056">parent</a><span>|</span><a href="#36134058">next</a><span>|</span><label class="collapse" for="c-36134067">[-]</label><label class="expand" for="c-36134067">[1 more]</label></div><br/><div class="children"><div class="content">smaller energy per transistor, but if you&#x27;re packing more in the package, the package&#x27;s consumption will grow up. Also, I think leakage current (and the heat that comes with it) goes up the smaller the feature size.</div><br/></div></div></div></div><div id="36134058" class="c"><input type="checkbox" id="c-36134058" checked=""/><div class="controls bullet"><span class="by">valianteffort</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133966">parent</a><span>|</span><a href="#36134056">prev</a><span>|</span><a href="#36134166">next</a><span>|</span><label class="collapse" for="c-36134058">[-]</label><label class="expand" for="c-36134058">[6 more]</label></div><br/><div class="children"><div class="content">Not to be pedantic but wouldn&#x27;t stacking transistors have no effect on density per square inch? Since it would only increase density per cubic inch.</div><br/><div id="36134376" class="c"><input type="checkbox" id="c-36134376" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134058">parent</a><span>|</span><a href="#36134345">next</a><span>|</span><label class="collapse" for="c-36134376">[-]</label><label class="expand" for="c-36134376">[1 more]</label></div><br/><div class="children"><div class="content">We include multi-story buildings when we calculate population density, why not include multi-story chips?</div><br/></div></div><div id="36134345" class="c"><input type="checkbox" id="c-36134345" checked=""/><div class="controls bullet"><span class="by">reaperman</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134058">parent</a><span>|</span><a href="#36134376">prev</a><span>|</span><a href="#36134071">next</a><span>|</span><label class="collapse" for="c-36134345">[-]</label><label class="expand" for="c-36134345">[1 more]</label></div><br/><div class="children"><div class="content">Stacking transistors increases density per square inch if it can be done on a single wafer of silicon, because its &quot;per square inch of fab wafer silicon&quot;</div><br/></div></div><div id="36134071" class="c"><input type="checkbox" id="c-36134071" checked=""/><div class="controls bullet"><span class="by">bushbaba</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134058">parent</a><span>|</span><a href="#36134345">prev</a><span>|</span><a href="#36134166">next</a><span>|</span><label class="collapse" for="c-36134071">[-]</label><label class="expand" for="c-36134071">[3 more]</label></div><br/><div class="children"><div class="content">I view it as if you cut 1 square inch of a motherboard. That the every 2 years you’d expect to see roughly double the number of transistors in that cut out piece.<p>Scaling vertically would “technically” still meet the above.</div><br/><div id="36134160" class="c"><input type="checkbox" id="c-36134160" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134071">parent</a><span>|</span><a href="#36134478">next</a><span>|</span><label class="collapse" for="c-36134160">[-]</label><label class="expand" for="c-36134160">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think that’s what they mean by per square inch. They mean in a plane, not a volume. If you add a third dimension the law stays the same, because a volume is two planes and the density law applies to each independently.  That’s why node sizes are a single value not a two dimensional value. A 3nm node is 3nm feature sizes, regardless of dimensionality.</div><br/></div></div><div id="36134478" class="c"><input type="checkbox" id="c-36134478" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134071">parent</a><span>|</span><a href="#36134160">prev</a><span>|</span><a href="#36134166">next</a><span>|</span><label class="collapse" for="c-36134478">[-]</label><label class="expand" for="c-36134478">[1 more]</label></div><br/><div class="children"><div class="content">Even a 1000ft thick motherboard?</div><br/></div></div></div></div></div></div><div id="36134166" class="c"><input type="checkbox" id="c-36134166" checked=""/><div class="controls bullet"><span class="by">lmpdev</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133966">parent</a><span>|</span><a href="#36134058">prev</a><span>|</span><a href="#36135971">next</a><span>|</span><label class="collapse" for="c-36134166">[-]</label><label class="expand" for="c-36134166">[1 more]</label></div><br/><div class="children"><div class="content">&gt; As we can always go vertically if needed.<p>I don&#x27;t think you understand how difficult non-planar transistors are to engineer at scale</div><br/></div></div></div></div><div id="36135971" class="c"><input type="checkbox" id="c-36135971" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36133892">parent</a><span>|</span><a href="#36133966">prev</a><span>|</span><a href="#36134370">next</a><span>|</span><label class="collapse" for="c-36135971">[-]</label><label class="expand" for="c-36135971">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a 24 GB limitation for consumer GPUs, because AMD and Intel aren&#x27;t competitive.</div><br/></div></div><div id="36134370" class="c"><input type="checkbox" id="c-36134370" checked=""/><div class="controls bullet"><span class="by">SomaticPirate</span><span>|</span><a href="#36133892">parent</a><span>|</span><a href="#36135971">prev</a><span>|</span><a href="#36134099">next</a><span>|</span><label class="collapse" for="c-36134370">[-]</label><label class="expand" for="c-36134370">[2 more]</label></div><br/><div class="children"><div class="content">A bit self-serving but GPU scaling supposedly follows Huang’s Law (from Jensen Huang of Nvidia) which claims GPUs more than double (~1.7x) every 2 years<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Huang%27s_law#" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Huang%27s_law#</a></div><br/><div id="36134561" class="c"><input type="checkbox" id="c-36134561" checked=""/><div class="controls bullet"><span class="by">aix1</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134370">parent</a><span>|</span><a href="#36134099">next</a><span>|</span><label class="collapse" for="c-36134561">[-]</label><label class="expand" for="c-36134561">[1 more]</label></div><br/><div class="children"><div class="content">&gt; more than double (~1.7x) every 2 years<p>To clarify in case anyone else finds this confusing.  The linked article suggests a 1.7x <i>annual</i> increase, which compounds to 2.89x every two years.</div><br/></div></div></div></div><div id="36134099" class="c"><input type="checkbox" id="c-36134099" checked=""/><div class="controls bullet"><span class="by">benaadams</span><span>|</span><a href="#36133892">parent</a><span>|</span><a href="#36134370">prev</a><span>|</span><a href="#36133978">next</a><span>|</span><label class="collapse" for="c-36134099">[-]</label><label class="expand" for="c-36134099">[1 more]</label></div><br/><div class="children"><div class="content">End of Dennard scaling was the performance breakdown. Meant chip frequencies couldn&#x27;t be cranked higher and higher as temperature dissipation became more and more of an issue <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dennard_scaling" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dennard_scaling</a></div><br/></div></div><div id="36133978" class="c"><input type="checkbox" id="c-36133978" checked=""/><div class="controls bullet"><span class="by">packetlost</span><span>|</span><a href="#36133892">parent</a><span>|</span><a href="#36134099">prev</a><span>|</span><a href="#36135577">next</a><span>|</span><label class="collapse" for="c-36133978">[-]</label><label class="expand" for="c-36133978">[7 more]</label></div><br/><div class="children"><div class="content">big limitations for what? AI models? It&#x27;s certainly not for gaming and we&#x27;re not quite to the point of consumers running huge AI models on their desktops. The HN crowd is, as always, not representative of the broader consumer market.</div><br/><div id="36134190" class="c"><input type="checkbox" id="c-36134190" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133978">parent</a><span>|</span><a href="#36134070">next</a><span>|</span><label class="collapse" for="c-36134190">[-]</label><label class="expand" for="c-36134190">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Deep_learning_super_sampling" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Deep_learning_super_sampling</a></div><br/><div id="36135488" class="c"><input type="checkbox" id="c-36135488" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134190">parent</a><span>|</span><a href="#36134070">next</a><span>|</span><label class="collapse" for="c-36135488">[-]</label><label class="expand" for="c-36135488">[1 more]</label></div><br/><div class="children"><div class="content">DLSS needs less memory than rendering in native resolution, not more.</div><br/></div></div></div></div><div id="36134070" class="c"><input type="checkbox" id="c-36134070" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36133978">parent</a><span>|</span><a href="#36134190">prev</a><span>|</span><a href="#36135577">next</a><span>|</span><label class="collapse" for="c-36134070">[-]</label><label class="expand" for="c-36134070">[4 more]</label></div><br/><div class="children"><div class="content">Aren’t we?<p>We’re seeing ChatGPT plug-ins for games, to provide intelligent conversation — and we’ve seen DNNs in StarCraft and similar.<p>To me, the “next gen” of gaming is intelligent NPCs, combining those features to create realistic behavior. That will require that consumer GPUs get closer to supercomputer GPUs:<p>More tensor cores and higher memory.</div><br/><div id="36134318" class="c"><input type="checkbox" id="c-36134318" checked=""/><div class="controls bullet"><span class="by">bigmattystyles</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134070">parent</a><span>|</span><a href="#36135577">next</a><span>|</span><label class="collapse" for="c-36134318">[-]</label><label class="expand" for="c-36134318">[3 more]</label></div><br/><div class="children"><div class="content">Other than the holographic projection, it feels like we&#x27;re in reach of the holodeck - you ask for a scene with a character or general backstory, and you go in. Fun times. Now on that energy to matter and holographic projection part...</div><br/><div id="36134764" class="c"><input type="checkbox" id="c-36134764" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134318">parent</a><span>|</span><a href="#36135577">next</a><span>|</span><label class="collapse" for="c-36134764">[-]</label><label class="expand" for="c-36134764">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a holodeck but Google has an interactive display now that feels like an open window. It doesn&#x27;t even register in my mind as a display, it feels like looking through a literal portal to another location in physical space.</div><br/><div id="36135704" class="c"><input type="checkbox" id="c-36135704" checked=""/><div class="controls bullet"><span class="by">throwaway2037</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134764">parent</a><span>|</span><a href="#36135577">next</a><span>|</span><label class="collapse" for="c-36135704">[-]</label><label class="expand" for="c-36135704">[1 more]</label></div><br/><div class="children"><div class="content">I tried to Google for more information, but I didn&#x27;t find anything.  Can you share a link?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36134500" class="c"><input type="checkbox" id="c-36134500" checked=""/><div class="controls bullet"><span class="by">spullara</span><span>|</span><a href="#36133892">parent</a><span>|</span><a href="#36135577">prev</a><span>|</span><a href="#36134558">next</a><span>|</span><label class="collapse" for="c-36134500">[-]</label><label class="expand" for="c-36134500">[2 more]</label></div><br/><div class="children"><div class="content">80 GB per GPU and the consumer GPU is purposefully lower memory to induce demand for server grade.</div><br/><div id="36134647" class="c"><input type="checkbox" id="c-36134647" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#36133892">root</a><span>|</span><a href="#36134500">parent</a><span>|</span><a href="#36134558">next</a><span>|</span><label class="collapse" for="c-36134647">[-]</label><label class="expand" for="c-36134647">[1 more]</label></div><br/><div class="children"><div class="content">On top of contracts strictly penalizing utilization of consumer GPUs in data centers, at that! Even with the memory, bandwidth etc. handicaps, servers with 4090&#x2F;3090s would have been competitive for many ML tasks.</div><br/></div></div></div></div></div></div><div id="36134558" class="c"><input type="checkbox" id="c-36134558" checked=""/><div class="controls bullet"><span class="by">thwoiu4o2i34234</span><span>|</span><a href="#36133892">prev</a><span>|</span><a href="#36135827">next</a><span>|</span><label class="collapse" for="c-36134558">[-]</label><label class="expand" for="c-36134558">[2 more]</label></div><br/><div class="children"><div class="content">The trouble with all this is that people are forced to buy these insanely expensive systems merely for the benefit of fitting all that stuff in the vram even if they don&#x27;t end up using the compute cores on these machines (which, let&#x27;s be honest, aren&#x27;t really all that better than the gaming gpus).<p>The needs are more in line with consumer server hardware with user-choice on cpu&#x2F;ram etc. Sounds to me like there&#x27;s a market for disruption. Pity that the deep-learning community is under the choke-hold of nvidia&#x27;s software.</div><br/><div id="36135830" class="c"><input type="checkbox" id="c-36135830" checked=""/><div class="controls bullet"><span class="by">tzhenghao</span><span>|</span><a href="#36134558">parent</a><span>|</span><a href="#36135827">next</a><span>|</span><label class="collapse" for="c-36135830">[-]</label><label class="expand" for="c-36135830">[1 more]</label></div><br/><div class="children"><div class="content">Eh, time will tell. Nvidia is killing it because they have &quot;semi decent&quot; software toolchains like CUDA when just about hardware player botched software. That said, a lot of interesting development on the XLA and PyTorch 2.0 sides that lower straight down to LLVM, bypassing Nvidia&#x27;s CUDA moat today.</div><br/></div></div></div></div><div id="36135827" class="c"><input type="checkbox" id="c-36135827" checked=""/><div class="controls bullet"><span class="by">prmoustache</span><span>|</span><a href="#36134558">prev</a><span>|</span><a href="#36133514">next</a><span>|</span><label class="collapse" for="c-36135827">[-]</label><label class="expand" for="c-36135827">[3 more]</label></div><br/><div class="children"><div class="content">Why do we still call those processors GPU when they are not meant to process graphics?</div><br/><div id="36135848" class="c"><input type="checkbox" id="c-36135848" checked=""/><div class="controls bullet"><span class="by">tzhenghao</span><span>|</span><a href="#36135827">parent</a><span>|</span><a href="#36135873">next</a><span>|</span><label class="collapse" for="c-36135848">[-]</label><label class="expand" for="c-36135848">[1 more]</label></div><br/><div class="children"><div class="content">Good point. &quot;AI accelerators&quot; is a thing now that competes directly with &quot;GPUs used for AI as its sole purpose&quot;.</div><br/></div></div><div id="36135873" class="c"><input type="checkbox" id="c-36135873" checked=""/><div class="controls bullet"><span class="by">pixelpoet</span><span>|</span><a href="#36135827">parent</a><span>|</span><a href="#36135848">prev</a><span>|</span><a href="#36133514">next</a><span>|</span><label class="collapse" for="c-36135873">[-]</label><label class="expand" for="c-36135873">[1 more]</label></div><br/><div class="children"><div class="content">CPUs are also arguably not the centre of processing in such systems.</div><br/></div></div></div></div><div id="36133514" class="c"><input type="checkbox" id="c-36133514" checked=""/><div class="controls bullet"><span class="by">iaw</span><span>|</span><a href="#36135827">prev</a><span>|</span><a href="#36134297">next</a><span>|</span><label class="collapse" for="c-36133514">[-]</label><label class="expand" for="c-36133514">[24 more]</label></div><br/><div class="children"><div class="content">How far are we from fully modeling the human brain?  I mean besides an easy way to identify all the neuronal connections...<p>This makes me feel like we&#x27;re close to that one terrifying short-story.</div><br/><div id="36133787" class="c"><input type="checkbox" id="c-36133787" checked=""/><div class="controls bullet"><span class="by">tbenst</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36133769">next</a><span>|</span><label class="collapse" for="c-36133787">[-]</label><label class="expand" for="c-36133787">[4 more]</label></div><br/><div class="children"><div class="content">We are massively far away from modeling the human brain. First of all, no one can agree what level is necessary to model the brain, and that varies tremendously by scientific question. Personally, my lower limit would be something like the computational package Neuron which models voltages across axon compartments and distribution of ion channels, My upper limit confidence bound is we don’t care about anything subatomic.<p>At the upper bound: In molecular dynamics, which is used extensively in modern day neuroscience to understand the function of ion channels and GPCRs, a single H100 can model 70ns&#x2F;day of compute for 1M atoms. There are 8.64e+13 nanoseconds per day. There are ~10^26 atoms in a human brain. Therefore, an upper limit back of envelope is you need fewer than 10e+26 atoms &#x2F; 10e+9 atoms * 8.64e+13 ns &#x2F; 70 ns = 1.23e+29 H100 GPUs.<p>Calculating the lower bound is more difficult, but let’s start by saying you can get away with a fp16 for each synapse. Storing the weights of that model for 100 trillion synapses is 200 Terabytes, and if you figure weight size * 4 or so to do anything useful then this is in spitting distance. Note that this example lower bound is massively less complex than the Neuron model I suggested, as the entire field of neuromodulators, homeostatic mechanisms, glia, and more are thrown out, which are all important for modeling how the brain works under certain computational regimes.</div><br/><div id="36134127" class="c"><input type="checkbox" id="c-36134127" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36133787">parent</a><span>|</span><a href="#36134060">next</a><span>|</span><label class="collapse" for="c-36134127">[-]</label><label class="expand" for="c-36134127">[1 more]</label></div><br/><div class="children"><div class="content">For the lower bound there is a dark horse factor that has spooked Geoffrey Hinton. He thinks that biological brains aren&#x27;t able to do backpropagation effectively through multiple layers, and so differentiable programming frameworks are much more powerful than what the brain has, at an algorithmic level. In other words, he thinks that computers are able to learn more effectively than any neuron-based biological brain. Of course right now there are caveats. The brain appears to have more &#x27;statistical efficiency&#x27; meaning it appears to learn more from less data, and the brain is obviously more energy-efficient. There is also the possibility that Geoffrey Hinton is just wrong.</div><br/></div></div><div id="36134060" class="c"><input type="checkbox" id="c-36134060" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36133787">parent</a><span>|</span><a href="#36134127">prev</a><span>|</span><a href="#36134334">next</a><span>|</span><label class="collapse" for="c-36134060">[-]</label><label class="expand" for="c-36134060">[1 more]</label></div><br/><div class="children"><div class="content">That is a spectacular response!<p>My bio knowledge is very basic, so forgive naiviety in these two questions.<p>First, I&#x27;m not asking you to go through the math on the spot, but I&#x27;m guessing that lower-bound capability is well understood in &#x27;the field&#x27;, but is it documented against various species?  Perhaps mapping against current &#x2F; projected GPU&#x2F;compute systems capabilities?   (I know there&#x27;s a project to model a worm&#x27;s brain, IIRC down to molecular level.  But I&#x27;m picturing a &#x27;we are 3 years away from being able to emulate a basset hound, 4 years for a border collie&#x27; - that kind of roadmap.)<p>Second, you said upper bound is to ignore sub-atomic. I thought we had proton and electron gradients, at least in metabolism. I believe proton there is a synonym for Hydrogen (atom), but electron would imply some potential need to emulate at sub-atomic?  Have I misunderstood the bounding &#x2F; chemistry involved?</div><br/></div></div><div id="36134334" class="c"><input type="checkbox" id="c-36134334" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36133787">parent</a><span>|</span><a href="#36134060">prev</a><span>|</span><a href="#36133769">next</a><span>|</span><label class="collapse" for="c-36134334">[-]</label><label class="expand" for="c-36134334">[1 more]</label></div><br/><div class="children"><div class="content">We will never simulate the entire brain atom-by-atom, we won&#x27;t need to, the same way we never simulate atom-by-atom and we don&#x27;t even place structural atoms by hand when we build a bridge, a rocket, or a tree house, we can be way more intelligent than that [1]. In the limit, the entire thing could be even more <i>simple</i> than we currently can imagine [2]. But yes, before we start leveraging equations, we must find the principle of gravitation for collective intelligence first [3].<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hodgkin%E2%80%93Huxley_model" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hodgkin%E2%80%93Huxley_model</a><p>[2] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reaction%E2%80%93diffusion_system" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reaction%E2%80%93diffusion_sys...</a><p>[3] Michael Levin | Cell Intelligence in Physiological and Morphological Spaces, <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=jLiHLDrOTW8">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=jLiHLDrOTW8</a></div><br/></div></div></div></div><div id="36133769" class="c"><input type="checkbox" id="c-36133769" checked=""/><div class="controls bullet"><span class="by">TheAlchemist</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36133787">prev</a><span>|</span><a href="#36134011">next</a><span>|</span><label class="collapse" for="c-36133769">[-]</label><label class="expand" for="c-36133769">[1 more]</label></div><br/><div class="children"><div class="content">You would need to define what you mean by &#x27;modeling the human brain&#x27;. If it means AGI or anything similar, then we&#x27;re very far.<p>To paraphrase an analogy I&#x27;ve heard somewhere (in similar context) - We&#x27;re building better and better ladders, maybe even lifts with this last push in ML field. But the brain is on the moon - even the best lifts won&#x27;t get us there.</div><br/></div></div><div id="36134011" class="c"><input type="checkbox" id="c-36134011" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36133769">prev</a><span>|</span><a href="#36133998">next</a><span>|</span><label class="collapse" for="c-36134011">[-]</label><label class="expand" for="c-36134011">[2 more]</label></div><br/><div class="children"><div class="content">When AI becomes good enough we will maybe we will stop thinking about trying to imitate human brains. If we viewed our brain&#x27;s decision making power objectively we can find several flaws, for example our heuristics to make quick decisions for mundane things is also our greatest weakness(short sightedness). We are poor at incorporating data to make good decisions and constantly bias due to some external stimuli.<p>Why would you want to make anything close to the brain? What real scientific or engineering or humanitarian uses does doing that even have? AI is already and going forward should strive to be a groundup of redesign of intelligence.</div><br/><div id="36134296" class="c"><input type="checkbox" id="c-36134296" checked=""/><div class="controls bullet"><span class="by">notamy</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134011">parent</a><span>|</span><a href="#36133998">next</a><span>|</span><label class="collapse" for="c-36134296">[-]</label><label class="expand" for="c-36134296">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why would you want to make anything close to the brain? What real scientific or engineering or humanitarian uses does doing that even have?<p>To have models of the human brain that we can poke at and change and tinker with and etc., so that we can get better ideas of how therapy techniques, medications, ... will impact the actual real people that might benefit from them.</div><br/></div></div></div></div><div id="36133998" class="c"><input type="checkbox" id="c-36133998" checked=""/><div class="controls bullet"><span class="by">predictabl3</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36134011">prev</a><span>|</span><a href="#36134275">next</a><span>|</span><label class="collapse" for="c-36133998">[-]</label><label class="expand" for="c-36133998">[3 more]</label></div><br/><div class="children"><div class="content">Sorry if I&#x27;m missing an obvious reference, but what short story do you mean?<p>(edit: thanks for both replies already, and any others that might fit; I understand now the reference was likely to Asimov)</div><br/><div id="36134026" class="c"><input type="checkbox" id="c-36134026" checked=""/><div class="controls bullet"><span class="by">iaw</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36133998">parent</a><span>|</span><a href="#36134019">next</a><span>|</span><label class="collapse" for="c-36134026">[-]</label><label class="expand" for="c-36134026">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;qntm.org&#x2F;mmacevedo" rel="nofollow">https:&#x2F;&#x2F;qntm.org&#x2F;mmacevedo</a></div><br/></div></div><div id="36134019" class="c"><input type="checkbox" id="c-36134019" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36133998">parent</a><span>|</span><a href="#36134026">prev</a><span>|</span><a href="#36134275">next</a><span>|</span><label class="collapse" for="c-36134019">[-]</label><label class="expand" for="c-36134019">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;users.ece.cmu.edu&#x2F;~gamvrosi&#x2F;thelastq.html" rel="nofollow">https:&#x2F;&#x2F;users.ece.cmu.edu&#x2F;~gamvrosi&#x2F;thelastq.html</a></div><br/></div></div></div></div><div id="36134275" class="c"><input type="checkbox" id="c-36134275" checked=""/><div class="controls bullet"><span class="by">dsign</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36133998">prev</a><span>|</span><a href="#36133814">next</a><span>|</span><label class="collapse" for="c-36134275">[-]</label><label class="expand" for="c-36134275">[1 more]</label></div><br/><div class="children"><div class="content">The question that interests me is how far are we from modelling a human cell, neuron or not? Because that&#x27;s how we cure cancer.</div><br/></div></div><div id="36133814" class="c"><input type="checkbox" id="c-36133814" checked=""/><div class="controls bullet"><span class="by">photochemsyn</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36134275">prev</a><span>|</span><a href="#36134075">next</a><span>|</span><label class="collapse" for="c-36133814">[-]</label><label class="expand" for="c-36133814">[1 more]</label></div><br/><div class="children"><div class="content">I think a long ways away, if I understand this article about the difficulties involved in accurately modeling even a single biological neuron:<p><a href="https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;how-computationally-complex-is-a-single-neuron-20210902&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;how-computationally-complex-i...</a><p>&gt; &quot;If each biological neuron is like a five-layer artificial neural network, then perhaps an image classification network with 50 layers is equivalent to 10 real neurons in a biological network.&quot;<p>The complexity explodes quickly because each biological neuron&#x27;s behavior is modulated by a large number of biochemical neurotransmitters, on top of all the dendritic connections (up to 15,000 each, apparently).</div><br/></div></div><div id="36134075" class="c"><input type="checkbox" id="c-36134075" checked=""/><div class="controls bullet"><span class="by">leriksen</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36133814">prev</a><span>|</span><a href="#36133666">next</a><span>|</span><label class="collapse" for="c-36134075">[-]</label><label class="expand" for="c-36134075">[9 more]</label></div><br/><div class="children"><div class="content">The brain is analog and chemical, AI will be digital and silicon. We have no idea how the map from one to the other.</div><br/><div id="36134516" class="c"><input type="checkbox" id="c-36134516" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134075">parent</a><span>|</span><a href="#36134342">next</a><span>|</span><label class="collapse" for="c-36134516">[-]</label><label class="expand" for="c-36134516">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The brain is analog and chemical, AI will be digital and silicon.<p>Says who?<p>Sure, <i>if</i> you assume that “AGI is just scaling up GPT”, it will be digital and silicon. But that’s a <i>big</i> assumption.<p>For all we know, AGI will only ever, if it exists, be analog and chemical.<p>&gt;  We have no idea how the map from one to the other.<p>Plus, even if we had an easy one-to-one mapping function between them, we don’t understand the source well enough to do the mapping.</div><br/><div id="36136084" class="c"><input type="checkbox" id="c-36136084" checked=""/><div class="controls bullet"><span class="by">mejutoco</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134516">parent</a><span>|</span><a href="#36134342">next</a><span>|</span><label class="collapse" for="c-36136084">[-]</label><label class="expand" for="c-36136084">[1 more]</label></div><br/><div class="children"><div class="content">It does not need to be, but today the computers we use are overwhelmingly based on silicon. Also OP mentioned AI, not AGI.</div><br/></div></div></div></div><div id="36134342" class="c"><input type="checkbox" id="c-36134342" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134075">parent</a><span>|</span><a href="#36134516">prev</a><span>|</span><a href="#36134520">next</a><span>|</span><label class="collapse" for="c-36134342">[-]</label><label class="expand" for="c-36134342">[3 more]</label></div><br/><div class="children"><div class="content">Is the Quantum computer hypothesis dead?</div><br/><div id="36134476" class="c"><input type="checkbox" id="c-36134476" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134342">parent</a><span>|</span><a href="#36134520">next</a><span>|</span><label class="collapse" for="c-36134476">[-]</label><label class="expand" for="c-36134476">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how quantum computers are relevant? We can&#x27;t build them, and there certainly isn&#x27;t any interesting quantum computation in the brain.</div><br/><div id="36135098" class="c"><input type="checkbox" id="c-36135098" checked=""/><div class="controls bullet"><span class="by">svnt</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134476">parent</a><span>|</span><a href="#36134520">next</a><span>|</span><label class="collapse" for="c-36135098">[-]</label><label class="expand" for="c-36135098">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean we can’t build them?<p>To your second point, we have little to no ability to understand yet what quantum effects may or may not be active in brain&#x2F;consciousness function. We certainly can’t exclude the possibility.</div><br/></div></div></div></div></div></div><div id="36134520" class="c"><input type="checkbox" id="c-36134520" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134075">parent</a><span>|</span><a href="#36134342">prev</a><span>|</span><a href="#36133666">next</a><span>|</span><label class="collapse" for="c-36134520">[-]</label><label class="expand" for="c-36134520">[3 more]</label></div><br/><div class="children"><div class="content">An analog-to-digital and digital-to-analog converter is less than a cup of coffee in some places [1].<p>[1] <a href="https:&#x2F;&#x2F;protosupplies.com&#x2F;product&#x2F;pcf8591-a-d-and-d-a-converter-module" rel="nofollow">https:&#x2F;&#x2F;protosupplies.com&#x2F;product&#x2F;pcf8591-a-d-and-d-a-conver...</a></div><br/><div id="36134605" class="c"><input type="checkbox" id="c-36134605" checked=""/><div class="controls bullet"><span class="by">gwoolhurme</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134520">parent</a><span>|</span><a href="#36133666">next</a><span>|</span><label class="collapse" for="c-36134605">[-]</label><label class="expand" for="c-36134605">[2 more]</label></div><br/><div class="children"><div class="content">Oh god so my background is CE&#x2F;ECE stuff and you managed to trigger me. I don&#x27;t want to be rude... just bluntly saying you triggered me. Doing something really small for A&#x2F;D D&#x2F;A with 8bit and not worrying much about resolution and data loss is one thing. For something massive scale the problem is a lot less trivial and a lot more mathematical.</div><br/><div id="36135601" class="c"><input type="checkbox" id="c-36135601" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#36133514">root</a><span>|</span><a href="#36134605">parent</a><span>|</span><a href="#36133666">next</a><span>|</span><label class="collapse" for="c-36135601">[-]</label><label class="expand" for="c-36135601">[1 more]</label></div><br/><div class="children"><div class="content">Haha, sorry, was more of a tongue-in-cheek reply to &quot;We have no idea how [to] map from [analog] to [digital]&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="36133666" class="c"><input type="checkbox" id="c-36133666" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36134075">prev</a><span>|</span><a href="#36134517">next</a><span>|</span><label class="collapse" for="c-36133666">[-]</label><label class="expand" for="c-36133666">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t modelling the human brain mean we&#x27;d be using less power? We&#x27;re using brute force to try get similar results to what the brain does.</div><br/></div></div><div id="36134517" class="c"><input type="checkbox" id="c-36134517" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#36133514">parent</a><span>|</span><a href="#36133666">prev</a><span>|</span><a href="#36134297">next</a><span>|</span><label class="collapse" for="c-36134517">[-]</label><label class="expand" for="c-36134517">[1 more]</label></div><br/><div class="children"><div class="content">For many applications there is no need to fully model the human brain. An approximation of a particular aspect would be sufficient in most cases. We didn’t build aeroplanes by fully modeling a bird, we just need aerodynamics.</div><br/></div></div></div></div><div id="36134297" class="c"><input type="checkbox" id="c-36134297" checked=""/><div class="controls bullet"><span class="by">kqr2</span><span>|</span><a href="#36133514">prev</a><span>|</span><a href="#36133774">next</a><span>|</span><label class="collapse" for="c-36134297">[-]</label><label class="expand" for="c-36134297">[2 more]</label></div><br/><div class="children"><div class="content">How does this compare to Cerebras? <a href="https:&#x2F;&#x2F;www.cerebras.net&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cerebras.net&#x2F;</a></div><br/><div id="36134505" class="c"><input type="checkbox" id="c-36134505" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#36134297">parent</a><span>|</span><a href="#36133774">next</a><span>|</span><label class="collapse" for="c-36134505">[-]</label><label class="expand" for="c-36134505">[1 more]</label></div><br/><div class="children"><div class="content">I think this has a lot more memory than Cerebras. Their site doesn&#x27;t say how much memory they can attach to each Cerebras chip and I&#x27;ve gotta imagine that&#x27;s because it doesn&#x27;t look good vs their competitors.</div><br/></div></div></div></div><div id="36133774" class="c"><input type="checkbox" id="c-36133774" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#36134297">prev</a><span>|</span><a href="#36133672">next</a><span>|</span><label class="collapse" for="c-36133774">[-]</label><label class="expand" for="c-36133774">[11 more]</label></div><br/><div class="children"><div class="content">That’s very interesting - why such an increase in memory capacity? I hope they can translate this to cheaper cards too</div><br/><div id="36133871" class="c"><input type="checkbox" id="c-36133871" checked=""/><div class="controls bullet"><span class="by">jrk</span><span>|</span><a href="#36133774">parent</a><span>|</span><a href="#36133806">next</a><span>|</span><label class="collapse" for="c-36133871">[-]</label><label class="expand" for="c-36133871">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not an increase in capacity of per-GPU &quot;GPU memory&quot; (the HBM directly connected to the H100 here is up to 96GB, where the previous generation was 80GB), but rather reflects the product of two things:<p>1. Each node here is a more tightly-coupled CPU+GPU two-chip pairing, and the <i>CPU</i> side has a significantly larger pool of 480GB of LPDDR (&quot;regular&quot; RAM). So each GPU is part of a node that includes up to 480+96GB of total memory.<p>2. There are way more nodes: 256, up from 8.</div><br/><div id="36135748" class="c"><input type="checkbox" id="c-36135748" checked=""/><div class="controls bullet"><span class="by">senttoschool</span><span>|</span><a href="#36133774">root</a><span>|</span><a href="#36133871">parent</a><span>|</span><a href="#36133991">next</a><span>|</span><label class="collapse" for="c-36135748">[-]</label><label class="expand" for="c-36135748">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>480+96GB of total memory</i><p>Is this memory unified like Apple Silicon? Meaning, can a model be deployed onto 574GB of total memory? Can the GPU read memory directly from the 480GB pool? Same question for CPU being able to directly access the 96GB.</div><br/></div></div><div id="36133991" class="c"><input type="checkbox" id="c-36133991" checked=""/><div class="controls bullet"><span class="by">tacticus</span><span>|</span><a href="#36133774">root</a><span>|</span><a href="#36133871">parent</a><span>|</span><a href="#36135748">prev</a><span>|</span><a href="#36133806">next</a><span>|</span><label class="collapse" for="c-36133991">[-]</label><label class="expand" for="c-36133991">[6 more]</label></div><br/><div class="children"><div class="content">`Nvidia discovers DMA`</div><br/><div id="36134225" class="c"><input type="checkbox" id="c-36134225" checked=""/><div class="controls bullet"><span class="by">llm_nerd</span><span>|</span><a href="#36133774">root</a><span>|</span><a href="#36133991">parent</a><span>|</span><a href="#36134431">next</a><span>|</span><label class="collapse" for="c-36134225">[-]</label><label class="expand" for="c-36134225">[3 more]</label></div><br/><div class="children"><div class="content">This device has a fully switched fabric allowing comms between any of the 256 &quot;superchip&quot; clusters at 900GB&#x2F;s. That is dramatically faster than a direct host to GPU 32-lane PCI-E connection (which is crazy), and obviously dwarfs any existing machine to machine connectivity. The actual <i>usability</i> of shared memory across the array is improved significantly.<p>I mean...nvidia has obviously been using DMA for decades. This isn&#x27;t just DMA.</div><br/><div id="36134473" class="c"><input type="checkbox" id="c-36134473" checked=""/><div class="controls bullet"><span class="by">zeusk</span><span>|</span><a href="#36133774">root</a><span>|</span><a href="#36134225">parent</a><span>|</span><a href="#36134973">next</a><span>|</span><label class="collapse" for="c-36134473">[-]</label><label class="expand" for="c-36134473">[1 more]</label></div><br/><div class="children"><div class="content">Parent discovers the difference between DMA and RDMA</div><br/></div></div><div id="36134973" class="c"><input type="checkbox" id="c-36134973" checked=""/><div class="controls bullet"><span class="by">tacticus</span><span>|</span><a href="#36133774">root</a><span>|</span><a href="#36134225">parent</a><span>|</span><a href="#36134473">prev</a><span>|</span><a href="#36134431">next</a><span>|</span><label class="collapse" for="c-36134973">[-]</label><label class="expand" for="c-36134973">[1 more]</label></div><br/><div class="children"><div class="content">No i mean the fact that Nvidia is now claiming that the memory the CPU has access to can be counted as memory for the GPU. the fabric is neat. the &quot;We have 500 GB of ram per gpu&quot; claim is questionable.</div><br/></div></div></div></div><div id="36134431" class="c"><input type="checkbox" id="c-36134431" checked=""/><div class="controls bullet"><span class="by">jabl</span><span>|</span><a href="#36133774">root</a><span>|</span><a href="#36133991">parent</a><span>|</span><a href="#36134225">prev</a><span>|</span><a href="#36134260">next</a><span>|</span><label class="collapse" for="c-36134431">[-]</label><label class="expand" for="c-36134431">[1 more]</label></div><br/><div class="children"><div class="content">Nvlink provides cache coherent load-store access, so the point is actually that it&#x27;s not DMA.</div><br/></div></div><div id="36134260" class="c"><input type="checkbox" id="c-36134260" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36133774">root</a><span>|</span><a href="#36133991">parent</a><span>|</span><a href="#36134431">prev</a><span>|</span><a href="#36133806">next</a><span>|</span><label class="collapse" for="c-36134260">[-]</label><label class="expand" for="c-36134260">[1 more]</label></div><br/><div class="children"><div class="content">They do make PCI hardware, don&#x27;t they?</div><br/></div></div></div></div></div></div><div id="36133806" class="c"><input type="checkbox" id="c-36133806" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#36133774">parent</a><span>|</span><a href="#36133871">prev</a><span>|</span><a href="#36134006">next</a><span>|</span><label class="collapse" for="c-36133806">[-]</label><label class="expand" for="c-36133806">[1 more]</label></div><br/><div class="children"><div class="content">It is surely driven by the gargantuan increase in demand for training and running massive models.</div><br/></div></div></div></div><div id="36133343" class="c"><input type="checkbox" id="c-36133343" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36133475">prev</a><span>|</span><a href="#36133465">next</a><span>|</span><label class="collapse" for="c-36133343">[-]</label><label class="expand" for="c-36133343">[1 more]</label></div><br/><div class="children"><div class="content">imagine a beowulf cluster of them</div><br/></div></div><div id="36133465" class="c"><input type="checkbox" id="c-36133465" checked=""/><div class="controls bullet"><span class="by">sheeshkebab</span><span>|</span><a href="#36133343">prev</a><span>|</span><a href="#36133496">next</a><span>|</span><label class="collapse" for="c-36133465">[-]</label><label class="expand" for="c-36133465">[9 more]</label></div><br/><div class="children"><div class="content">But can it play Doom?</div><br/><div id="36134096" class="c"><input type="checkbox" id="c-36134096" checked=""/><div class="controls bullet"><span class="by">speed_spread</span><span>|</span><a href="#36133465">parent</a><span>|</span><a href="#36134108">next</a><span>|</span><label class="collapse" for="c-36134096">[-]</label><label class="expand" for="c-36134096">[1 more]</label></div><br/><div class="children"><div class="content">Hell, with a developer emulation force of 13 MegaCarmacks it can rewrite 50,000 Doom per second!<p>(But that number drops to only 10&#x2F;s if the rewrites are in Rust)</div><br/></div></div><div id="36134108" class="c"><input type="checkbox" id="c-36134108" checked=""/><div class="controls bullet"><span class="by">RandomBK</span><span>|</span><a href="#36133465">parent</a><span>|</span><a href="#36134096">prev</a><span>|</span><a href="#36134584">next</a><span>|</span><label class="collapse" for="c-36134108">[-]</label><label class="expand" for="c-36134108">[2 more]</label></div><br/><div class="children"><div class="content">Yes, you will be able to train an AI model that is capable of beating Doom using this machine.</div><br/><div id="36134179" class="c"><input type="checkbox" id="c-36134179" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#36133465">root</a><span>|</span><a href="#36134108">parent</a><span>|</span><a href="#36134584">next</a><span>|</span><label class="collapse" for="c-36134179">[-]</label><label class="expand" for="c-36134179">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re more likely to train a model capable of <i>writing</i> Doom using this machine.</div><br/></div></div></div></div><div id="36134584" class="c"><input type="checkbox" id="c-36134584" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#36133465">parent</a><span>|</span><a href="#36134108">prev</a><span>|</span><a href="#36133499">next</a><span>|</span><label class="collapse" for="c-36134584">[-]</label><label class="expand" for="c-36134584">[1 more]</label></div><br/><div class="children"><div class="content">If it can infer 35 fps, then it can be Doom.</div><br/></div></div><div id="36133499" class="c"><input type="checkbox" id="c-36133499" checked=""/><div class="controls bullet"><span class="by">zakki</span><span>|</span><a href="#36133465">parent</a><span>|</span><a href="#36134584">prev</a><span>|</span><a href="#36133496">next</a><span>|</span><label class="collapse" for="c-36133499">[-]</label><label class="expand" for="c-36133499">[4 more]</label></div><br/><div class="children"><div class="content">You mean millions of Doom?</div><br/><div id="36133703" class="c"><input type="checkbox" id="c-36133703" checked=""/><div class="controls bullet"><span class="by">Xeoncross</span><span>|</span><a href="#36133465">root</a><span>|</span><a href="#36133499">parent</a><span>|</span><a href="#36135515">next</a><span>|</span><label class="collapse" for="c-36133703">[-]</label><label class="expand" for="c-36133703">[1 more]</label></div><br/><div class="children"><div class="content">At this point, I&#x27;m convinced that someone is here which could make that cluster play trillions of Doom. I mean, just how many pregnancy tests worth of compute is this?</div><br/></div></div><div id="36135515" class="c"><input type="checkbox" id="c-36135515" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#36133465">root</a><span>|</span><a href="#36133499">parent</a><span>|</span><a href="#36133703">prev</a><span>|</span><a href="#36133770">next</a><span>|</span><label class="collapse" for="c-36135515">[-]</label><label class="expand" for="c-36135515">[1 more]</label></div><br/><div class="children"><div class="content">Megadoom</div><br/></div></div><div id="36133770" class="c"><input type="checkbox" id="c-36133770" checked=""/><div class="controls bullet"><span class="by">robotnikman</span><span>|</span><a href="#36133465">root</a><span>|</span><a href="#36133499">parent</a><span>|</span><a href="#36135515">prev</a><span>|</span><a href="#36133496">next</a><span>|</span><label class="collapse" for="c-36133770">[-]</label><label class="expand" for="c-36133770">[1 more]</label></div><br/><div class="children"><div class="content">With each doom instance being played by an AI?</div><br/></div></div></div></div></div></div><div id="36133496" class="c"><input type="checkbox" id="c-36133496" checked=""/><div class="controls bullet"><span class="by">belltaco</span><span>|</span><a href="#36133465">prev</a><span>|</span><a href="#36134008">next</a><span>|</span><label class="collapse" for="c-36133496">[-]</label><label class="expand" for="c-36133496">[3 more]</label></div><br/><div class="children"><div class="content">Chrome will still find a way to eat all that up and lag.</div><br/><div id="36134068" class="c"><input type="checkbox" id="c-36134068" checked=""/><div class="controls bullet"><span class="by">parker_mountain</span><span>|</span><a href="#36133496">parent</a><span>|</span><a href="#36133929">next</a><span>|</span><label class="collapse" for="c-36134068">[-]</label><label class="expand" for="c-36134068">[1 more]</label></div><br/><div class="children"><div class="content">Epic meme sir, here&#x27;s your updoot</div><br/></div></div><div id="36133929" class="c"><input type="checkbox" id="c-36133929" checked=""/><div class="controls bullet"><span class="by">boredemployee</span><span>|</span><a href="#36133496">parent</a><span>|</span><a href="#36134068">prev</a><span>|</span><a href="#36134008">next</a><span>|</span><label class="collapse" for="c-36133929">[-]</label><label class="expand" for="c-36133929">[1 more]</label></div><br/><div class="children"><div class="content">Best comment of the day!</div><br/></div></div></div></div><div id="36134008" class="c"><input type="checkbox" id="c-36134008" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#36133496">prev</a><span>|</span><a href="#36133829">next</a><span>|</span><label class="collapse" for="c-36134008">[-]</label><label class="expand" for="c-36134008">[8 more]</label></div><br/><div class="children"><div class="content">Celebrating new ways to further burn up the planet rather than discovering more efficient and better ways for training, inference and fine-tuning AI systems without needing to scale up more GPUs, TPUs, data centers and water for the same purpose.<p>The end result of this announcement is another expensive system only available to the same incumbent of tech giants with tens of billions at their disposal.</div><br/><div id="36136006" class="c"><input type="checkbox" id="c-36136006" checked=""/><div class="controls bullet"><span class="by">metaphor</span><span>|</span><a href="#36134008">parent</a><span>|</span><a href="#36134170">next</a><span>|</span><label class="collapse" for="c-36136006">[-]</label><label class="expand" for="c-36136006">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>The end result of this announcement is another expensive system only available to the same incumbent of tech giants with tens of billions at their disposal.</i><p>Certainly wasn&#x27;t the case when a public research university partnership seeded by a generous donation from Nvidia co-founder&#x2F;UF alumnus Chris Malachowsky was formally announced[1] shortly after DGX A100 launch[2] several years ago, never mind the handful of other academic early adopters mentioned in the press release.<p>Of course, we tend to conveniently forget such exogenous details.<p>[1] <a href="https:&#x2F;&#x2F;news.ufl.edu&#x2F;2020&#x2F;07&#x2F;nvidia-partnership&#x2F;" rel="nofollow">https:&#x2F;&#x2F;news.ufl.edu&#x2F;2020&#x2F;07&#x2F;nvidia-partnership&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;nvidianews.nvidia.com&#x2F;news&#x2F;nvidias-new-ampere-data-center-gpu-in-full-production" rel="nofollow">https:&#x2F;&#x2F;nvidianews.nvidia.com&#x2F;news&#x2F;nvidias-new-ampere-data-c...</a></div><br/></div></div><div id="36134170" class="c"><input type="checkbox" id="c-36134170" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#36134008">parent</a><span>|</span><a href="#36136006">prev</a><span>|</span><a href="#36133829">next</a><span>|</span><label class="collapse" for="c-36134170">[-]</label><label class="expand" for="c-36134170">[6 more]</label></div><br/><div class="children"><div class="content">We found a way: nuclear fission reactors. So that problem is solved.</div><br/><div id="36134447" class="c"><input type="checkbox" id="c-36134447" checked=""/><div class="controls bullet"><span class="by">anaganisk</span><span>|</span><a href="#36134008">root</a><span>|</span><a href="#36134170">parent</a><span>|</span><a href="#36134676">next</a><span>|</span><label class="collapse" for="c-36134447">[-]</label><label class="expand" for="c-36134447">[3 more]</label></div><br/><div class="children"><div class="content">Yet, the climate is still changing. Inflation is rising.
The world definitely has advanced but never became a better place.</div><br/><div id="36134551" class="c"><input type="checkbox" id="c-36134551" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#36134008">root</a><span>|</span><a href="#36134447">parent</a><span>|</span><a href="#36134676">next</a><span>|</span><label class="collapse" for="c-36134551">[-]</label><label class="expand" for="c-36134551">[2 more]</label></div><br/><div class="children"><div class="content">The world has never been better. This is an incredible time to be alive.</div><br/><div id="36134936" class="c"><input type="checkbox" id="c-36134936" checked=""/><div class="controls bullet"><span class="by">gwoolhurme</span><span>|</span><a href="#36134008">root</a><span>|</span><a href="#36134551">parent</a><span>|</span><a href="#36134676">next</a><span>|</span><label class="collapse" for="c-36134936">[-]</label><label class="expand" for="c-36134936">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t feel that way personally... I have Nth level anxiety, maybe even N+1 level anxiety about losing my job to AI. Maybe in the future whoever comes next can enjoy things, but this literally keeps me up at night. With talks of extinction, job loss, etc. I feel like I wish I wasn&#x27;t alive at this time.</div><br/></div></div></div></div></div></div><div id="36134676" class="c"><input type="checkbox" id="c-36134676" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#36134008">root</a><span>|</span><a href="#36134170">parent</a><span>|</span><a href="#36134447">prev</a><span>|</span><a href="#36133829">next</a><span>|</span><label class="collapse" for="c-36134676">[-]</label><label class="expand" for="c-36134676">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We found a way: nuclear fission reactors.<p>Nope. I&#x27;m talking about <i>efficient methods in training, inferencing and fine-tuning these AI models</i> that doesn&#x27;t require lots of data centers, TPUs, GPUs, etc. You&#x27;re talking about something else.<p>Petrol and diesel cars are already burning the planet, but the main difference is, that there are efficient alternatives available today like electric cars to use instead.<p>AI (Deep learning) however, does <i>not</i> have any viable and efficient methods in training, fine-turning these AI models, at all [0] [1] and wastes a tremendous amount of resources, all to keep up with scalability.<p>So that problem is still NOT solved after a decade of using GPUs, the wastage is getting worse.<p>[0] <a href="https:&#x2F;&#x2F;gizmodo.com&#x2F;chatgpt-ai-water-185000-gallons-training-nuclear-1850324249" rel="nofollow">https:&#x2F;&#x2F;gizmodo.com&#x2F;chatgpt-ai-water-185000-gallons-training...</a><p>[1] <a href="https:&#x2F;&#x2F;www.independent.co.uk&#x2F;tech&#x2F;chatgpt-data-centre-water-consumption-b2318972.html" rel="nofollow">https:&#x2F;&#x2F;www.independent.co.uk&#x2F;tech&#x2F;chatgpt-data-centre-water...</a></div><br/><div id="36135566" class="c"><input type="checkbox" id="c-36135566" checked=""/><div class="controls bullet"><span class="by">detrites</span><span>|</span><a href="#36134008">root</a><span>|</span><a href="#36134676">parent</a><span>|</span><a href="#36133829">next</a><span>|</span><label class="collapse" for="c-36135566">[-]</label><label class="expand" for="c-36135566">[1 more]</label></div><br/><div class="children"><div class="content">&gt; efficient methods in training, inferencing and fine-tuning these AI models that doesn&#x27;t require lots of data centers, TPUs, GPUs, etc.<p>Exactly the types of problems future AI models could solve.<p>Dire climate alarms are based on the predictions made using models. As modelling advances as a field, both predictions, and solutions become more and more voluminous and accurate, along with revealing mistakes and failures of prior models.<p>Anyone concerned with climate should rally behind this kind of general progress. Further, it simply is progressing, and fields that don&#x27;t embrace it, will be left behind. We&#x27;re in the midst of an unprecedented revolution which touches all.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>