<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718010073828" as="style"/><link rel="stylesheet" href="styles.css?v=1718010073828"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/labmlai/inspectus">Show HN: We&#x27;ve open-sourced our LLM attention visualization library</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>lakshith-403</span> | <span>15 comments</span></div><br/><div><div id="40630178" class="c"><input type="checkbox" id="c-40630178" checked=""/><div class="controls bullet"><span class="by">xcodevn</span><span>|</span><a href="#40630665">next</a><span>|</span><label class="collapse" for="c-40630178">[-]</label><label class="expand" for="c-40630178">[4 more]</label></div><br/><div class="children"><div class="content">On a related note: recently, I released a visualization of all MLP  neurons inside the llama3 8B model. Here is an example &quot;derivative&quot; neuron which is triggered when talking about the derivative concept.<p><a href="https:&#x2F;&#x2F;neuralblog.github.io&#x2F;llama3-neurons&#x2F;neuron_viewer.html#24,2362" rel="nofollow">https:&#x2F;&#x2F;neuralblog.github.io&#x2F;llama3-neurons&#x2F;neuron_viewer.ht...</a></div><br/><div id="40630677" class="c"><input type="checkbox" id="c-40630677" checked=""/><div class="controls bullet"><span class="by">skulk</span><span>|</span><a href="#40630178">parent</a><span>|</span><a href="#40630465">next</a><span>|</span><label class="collapse" for="c-40630677">[-]</label><label class="expand" for="c-40630677">[1 more]</label></div><br/><div class="children"><div class="content">This is insanely fun to just flip through. I found a &quot;sex&quot; neuron. <a href="https:&#x2F;&#x2F;neuralblog.github.io&#x2F;llama3-neurons&#x2F;neuron_viewer.html#21,911" rel="nofollow">https:&#x2F;&#x2F;neuralblog.github.io&#x2F;llama3-neurons&#x2F;neuron_viewer.ht...</a></div><br/></div></div><div id="40630465" class="c"><input type="checkbox" id="c-40630465" checked=""/><div class="controls bullet"><span class="by">vpj</span><span>|</span><a href="#40630178">parent</a><span>|</span><a href="#40630677">prev</a><span>|</span><a href="#40630665">next</a><span>|</span><label class="collapse" for="c-40630465">[-]</label><label class="expand" for="c-40630465">[2 more]</label></div><br/><div class="children"><div class="content">Pretty cool. The tokens are highlighted based on the activation?</div><br/><div id="40630564" class="c"><input type="checkbox" id="c-40630564" checked=""/><div class="controls bullet"><span class="by">xcodevn</span><span>|</span><a href="#40630178">root</a><span>|</span><a href="#40630465">parent</a><span>|</span><a href="#40630665">next</a><span>|</span><label class="collapse" for="c-40630564">[-]</label><label class="expand" for="c-40630564">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you&#x27;re correct. The tokens are highlighted based on the neuron activation value, which is scaled to a range of 0 to 10.</div><br/></div></div></div></div></div></div><div id="40625255" class="c"><input type="checkbox" id="c-40625255" checked=""/><div class="controls bullet"><span class="by">ravjo</span><span>|</span><a href="#40630665">prev</a><span>|</span><a href="#40625839">next</a><span>|</span><label class="collapse" for="c-40625255">[-]</label><label class="expand" for="c-40625255">[3 more]</label></div><br/><div class="children"><div class="content">Sounds great. Non-engineer, but curious. Is there a walkthrough blog post or video that can help someone appreciate&#x2F;understand this easily?</div><br/><div id="40626487" class="c"><input type="checkbox" id="c-40626487" checked=""/><div class="controls bullet"><span class="by">swifthesitation</span><span>|</span><a href="#40625255">parent</a><span>|</span><a href="#40625839">next</a><span>|</span><label class="collapse" for="c-40626487">[-]</label><label class="expand" for="c-40626487">[2 more]</label></div><br/><div class="children"><div class="content">Attention in transformers, visually explained | Chapter 6, Deep Learning - 3Blue1Brown: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=eMlx5fFNoYc&amp;t=" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=eMlx5fFNoYc&amp;t=</a></div><br/><div id="40626569" class="c"><input type="checkbox" id="c-40626569" checked=""/><div class="controls bullet"><span class="by">lakshith-403</span><span>|</span><a href="#40625255">root</a><span>|</span><a href="#40626487">parent</a><span>|</span><a href="#40625839">next</a><span>|</span><label class="collapse" for="c-40626569">[-]</label><label class="expand" for="c-40626569">[1 more]</label></div><br/><div class="children"><div class="content">Thank you</div><br/></div></div></div></div></div></div><div id="40625839" class="c"><input type="checkbox" id="c-40625839" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#40625255">prev</a><span>|</span><a href="#40625250">next</a><span>|</span><label class="collapse" for="c-40625839">[-]</label><label class="expand" for="c-40625839">[3 more]</label></div><br/><div class="children"><div class="content">This seems to be what Anthropic and OpenAI did in their research<p>Golden Gate Claude - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40459543">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40459543</a> - (60 comments, 16 days ago)<p>Extracting Concepts from GPT-4 - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40599749">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40599749</a> (144 comments, 2 days ago)</div><br/><div id="40629090" class="c"><input type="checkbox" id="c-40629090" checked=""/><div class="controls bullet"><span class="by">dimatura</span><span>|</span><a href="#40625839">parent</a><span>|</span><a href="#40626567">next</a><span>|</span><label class="collapse" for="c-40629090">[-]</label><label class="expand" for="c-40629090">[1 more]</label></div><br/><div class="children"><div class="content">That OpenAI work is more elaborate. It trains an additional network in such a way that it encodes what GPT is doing in terms of activations, but in a more interpretable way (hopefully). Here, as far as I can tell, it&#x27;s visualizing the activation of the attention layers directly.</div><br/></div></div><div id="40626567" class="c"><input type="checkbox" id="c-40626567" checked=""/><div class="controls bullet"><span class="by">lakshith-403</span><span>|</span><a href="#40625839">parent</a><span>|</span><a href="#40629090">prev</a><span>|</span><a href="#40625250">next</a><span>|</span><label class="collapse" for="c-40626567">[-]</label><label class="expand" for="c-40626567">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. I think OpenAI here uses sparse autoencoders to map out sparse activation patterns in networks. Comparing them to how a real person reasons about a situations.<p>Inspectus, on the other hand is a general tool to visualize how transformer models pay attention to different parts of the data they process.</div><br/></div></div></div></div><div id="40625250" class="c"><input type="checkbox" id="c-40625250" checked=""/><div class="controls bullet"><span class="by">benf76</span><span>|</span><a href="#40625839">prev</a><span>|</span><a href="#40628448">next</a><span>|</span><label class="collapse" for="c-40625250">[-]</label><label class="expand" for="c-40625250">[2 more]</label></div><br/><div class="children"><div class="content">This looks cool but can you explain how to make it useful?</div><br/><div id="40626382" class="c"><input type="checkbox" id="c-40626382" checked=""/><div class="controls bullet"><span class="by">lakshith-403</span><span>|</span><a href="#40625250">parent</a><span>|</span><a href="#40628448">next</a><span>|</span><label class="collapse" for="c-40626382">[-]</label><label class="expand" for="c-40626382">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a primary user. Just cleaned up the existing codebase to make it open source. But you could use this to visualise attentions and debug the model.<p>For an example if you&#x27;re working on a Q&amp;A model, you can check which tokens in the prompt contributed to the output. It&#x27;s possible to detect issues like output not paying attention to any important part of the prompt.</div><br/></div></div></div></div><div id="40628448" class="c"><input type="checkbox" id="c-40628448" checked=""/><div class="controls bullet"><span class="by">JackYoustra</span><span>|</span><a href="#40625250">prev</a><span>|</span><label class="collapse" for="c-40628448">[-]</label><label class="expand" for="c-40628448">[1 more]</label></div><br/><div class="children"><div class="content">Hey! This is pretty neat, it reminds me of the graphs made by transformer_lens. Cool to see all of these visualization libraries popping up!</div><br/></div></div></div></div></div></div></div></body></html>