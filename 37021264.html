<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691398881124" as="style"/><link rel="stylesheet" href="styles.css?v=1691398881124"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.everydayscientist.com/replace-peer-review-with-peer-replication/">Replace peer review with “peer replication” (2021)</a> <span class="domain">(<a href="https://blog.everydayscientist.com">blog.everydayscientist.com</a>)</span></div><div class="subtext"><span>dongping</span> | <span>216 comments</span></div><br/><div><div id="37022504" class="c"><input type="checkbox" id="c-37022504" checked=""/><div class="controls bullet"><span class="by">fabian2k</span><span>|</span><a href="#37023183">next</a><span>|</span><label class="collapse" for="c-37022504">[-]</label><label class="expand" for="c-37022504">[121 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how this could ever work, and non-scientists seem to often dramatically underestimate the amount of work it would be to replicate every published paper.<p>This of course depends a lot on the specific field, but it can easily be months of effort to replicate a paper. You save some time compared to the original as you don&#x27;t have to repeat the dead ends and you might receive some samples and can skip parts of the preparation that way. But properly replicating a paper will still be a lot of effort, especially when there are any issues and it doesn&#x27;t work on the first try. Then you have to troubleshoot your experiments and make sure that no mistakes were made. That can add a lot of time to the process.<p>This is also all work that doesn&#x27;t benefit the scientists replicating the paper. It only costs them money and time.<p>If someone cares enough about the work to build on it, they will replicate it anyway. And in that case they have a good incentive to spend the effort. If that works this will indirectly support the original paper even if the following papers don&#x27;t specifically replicate the original results. Though this part is much more problematic if the following experiments fail, then this will likely remain entirely unpublished. But the solution here unfortunately isn&#x27;t as simple as just publishing negative results, it take far more work to create a solid negative result than just trying the experiments and abandoning them if they&#x27;re not promising.</div><br/><div id="37023255" class="c"><input type="checkbox" id="c-37023255" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37025220">next</a><span>|</span><label class="collapse" for="c-37023255">[-]</label><label class="expand" for="c-37023255">[22 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t see how this could ever work, and non-scientists seem to often dramatically underestimate the amount of work it would be to replicate every published paper.<p>They also tend to over-estimate the effect of peer review (often equating peer review with validity).<p>&gt; If someone cares enough about the work to build on it, they will replicate it anyway. And in that case they have a good incentive to spend the effort. If that works this will indirectly support the original paper even if the following papers don&#x27;t specifically replicate the original results. Though this part is much more problematic if the following experiments fail, then this will likely remain entirely unpublished.<p>It can also remain unpublished if other things did not work out, even if the results could be replicated. A half-fictional example: a team is working on a revolutionary new material to solve complicated engineering problems. They found a material that was synthesised by someone in the 1980s, published once and never reproduced, which they think could have the specific property they are after. So they synthesise it, and it turns out that the material exists, with the expected structure but not with the property they hoped. They aren’t going to write it up and publish it; they’re just going to scrap it and move on to the next candidate. Different teams might be doing the same thing at the same time, and nobody coming after them will have a clue.</div><br/><div id="37025092" class="c"><input type="checkbox" id="c-37025092" checked=""/><div class="controls bullet"><span class="by">techdragon</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023255">parent</a><span>|</span><a href="#37025986">next</a><span>|</span><label class="collapse" for="c-37025092">[-]</label><label class="expand" for="c-37025092">[2 more]</label></div><br/><div class="children"><div class="content">This waste of effort by way of duplicating unpublished negative results is a big factor in why replicated results deserve to be rated more highly than results that have not been replicated regardless of the prestige of the researchers or the institutions involved… if no one can prove your work work was correct… how much can anyone trust your work…<p>I have gone down the rabbit hole of engineering research before and 90% of the time I’ve managed to find an anecdote or subsequent research footnotes or actual subsequent research publications, that substantially invalidated the lofty claims of the engineers in the 70s or 80s (which is amazing still despite this, a genuine treasure trove of research unused and sometimes useful aerospace engineering research and development) and unfortunately outside the few proper publications, a lot of the invalidations are not properly reverse cited research material and I could have spent a week cross referencing before I spot the link and realise the unnamed work they are saying they are proving wrong is actually some footnotes containing the only published data (before their new paper) on some old work that has a bad scan copy on the NASA NTRS server under some obscure title and no related keywords to the topic the research is notionally about…<p>Academic research can genuinely suck sometimes… particularly when you want to actually apply it.</div><br/><div id="37030613" class="c"><input type="checkbox" id="c-37030613" checked=""/><div class="controls bullet"><span class="by">aeternum</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025092">parent</a><span>|</span><a href="#37025986">next</a><span>|</span><label class="collapse" for="c-37030613">[-]</label><label class="expand" for="c-37030613">[1 more]</label></div><br/><div class="children"><div class="content">Publishing positive&#x2F;noteworth results only does seem like an embarrassingly obvious major flaw in academia and the greater scientific community.<p>A research assistant would quickly be thrown out if he&#x2F;she refused to record negative experimental results, yet we somehow decide that is fine when operating as a collective.</div><br/></div></div></div></div><div id="37025986" class="c"><input type="checkbox" id="c-37025986" checked=""/><div class="controls bullet"><span class="by">vibrio</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023255">parent</a><span>|</span><a href="#37025092">prev</a><span>|</span><a href="#37025220">next</a><span>|</span><label class="collapse" for="c-37025986">[-]</label><label class="expand" for="c-37025986">[19 more]</label></div><br/><div class="children"><div class="content">“They also tend to over-estimate the effect of peer review (often equating peer review with validity).“<p>In my experience, scientists ate comfortably cynical about peer review- even those that serve as reviewers and editors- except maybe junior scientists that haven’t gotten burned yet.</div><br/><div id="37026425" class="c"><input type="checkbox" id="c-37026425" checked=""/><div class="controls bullet"><span class="by">jakear</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025986">parent</a><span>|</span><a href="#37026369">next</a><span>|</span><label class="collapse" for="c-37026425">[-]</label><label class="expand" for="c-37026425">[14 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the general public that equates &quot;peer reviewed&quot; with &quot;definitely correct, does not need to be questioned&quot;.</div><br/><div id="37028481" class="c"><input type="checkbox" id="c-37028481" checked=""/><div class="controls bullet"><span class="by">kibibu</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026425">parent</a><span>|</span><a href="#37028432">next</a><span>|</span><label class="collapse" for="c-37028481">[-]</label><label class="expand" for="c-37028481">[12 more]</label></div><br/><div class="children"><div class="content">There is genuine merit in peer review though. A lot of the marketing &quot;papers&quot; from OpenAI and Google would benefit from going through a peer review process instead of scholarwashing and uploading straight to Arxiv.<p>One simple example, (from memory) the Bard paper doesn&#x27;t include results for experiments in which GPT-4 outperforms it. As a result, people come away from these works with an inflated understanding of their capabilities.
This wouldn&#x27;t pass peer review.</div><br/><div id="37030236" class="c"><input type="checkbox" id="c-37030236" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028481">parent</a><span>|</span><a href="#37030525">next</a><span>|</span><label class="collapse" for="c-37030236">[-]</label><label class="expand" for="c-37030236">[1 more]</label></div><br/><div class="children"><div class="content">You can often see a p-hacked study from a mile away because they measure large numbers of unnecessary variables. They then pick those 4 variables that yield a probably false signal and publish on it. One would think these would never pass peer review, but they&#x27;re regularly peer reviewed, published, and then, shockingly enough, fail to replicate. Hypothesizing after the results are known falls in the same bucket here. This is why pre-publishing kicked off, yet it&#x27;s also hardly a savior.<p>The point I make is that peer review can not be guaranteed to &#x27;fix&#x27; science in any way we might like. The Sokal Affair [1] has now been replicated repeatedly, including in peer reviewed journals. The most recent one even got quite cheeky and published it under the names &quot;Sage Owens, Kal Avers-Lynde III&quot; - Sokal III. [2] It always preys on the same weakness - bias confirmation.<p>[1] - <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sokal_affair" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sokal_affair</a><p>[2] - <a href="https:&#x2F;&#x2F;www.nationalreview.com&#x2F;news&#x2F;academic-journal-publishes-apparent-hoax-paper-alleging-right-wing-donor-influence-in-universities&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nationalreview.com&#x2F;news&#x2F;academic-journal-publish...</a></div><br/></div></div><div id="37030525" class="c"><input type="checkbox" id="c-37030525" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028481">parent</a><span>|</span><a href="#37030236">prev</a><span>|</span><a href="#37029047">next</a><span>|</span><label class="collapse" for="c-37030525">[-]</label><label class="expand" for="c-37030525">[1 more]</label></div><br/><div class="children"><div class="content">What is honestly a lot more compelling than peer review is multiple supporting pieces of evidence from different sources. One result might be spurious, but if you can find a couple independent studies showing as much its probably a real phenomenon.</div><br/></div></div><div id="37029047" class="c"><input type="checkbox" id="c-37029047" checked=""/><div class="controls bullet"><span class="by">jakear</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028481">parent</a><span>|</span><a href="#37030525">prev</a><span>|</span><a href="#37028432">next</a><span>|</span><label class="collapse" for="c-37029047">[-]</label><label class="expand" for="c-37029047">[9 more]</label></div><br/><div class="children"><div class="content">Industry doesn&#x27;t need peer review. The proof of the pudding is in the sales.</div><br/><div id="37029982" class="c"><input type="checkbox" id="c-37029982" checked=""/><div class="controls bullet"><span class="by">kibibu</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37029047">parent</a><span>|</span><a href="#37029352">next</a><span>|</span><label class="collapse" for="c-37029982">[-]</label><label class="expand" for="c-37029982">[2 more]</label></div><br/><div class="children"><div class="content">Then why does it dress its marketing up in academic paper format?</div><br/><div id="37029999" class="c"><input type="checkbox" id="c-37029999" checked=""/><div class="controls bullet"><span class="by">jakear</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37029982">parent</a><span>|</span><a href="#37029352">next</a><span>|</span><label class="collapse" for="c-37029999">[-]</label><label class="expand" for="c-37029999">[1 more]</label></div><br/><div class="children"><div class="content">In want of sales.</div><br/></div></div></div></div><div id="37029352" class="c"><input type="checkbox" id="c-37029352" checked=""/><div class="controls bullet"><span class="by">ModernMech</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37029047">parent</a><span>|</span><a href="#37029982">prev</a><span>|</span><a href="#37028432">next</a><span>|</span><label class="collapse" for="c-37029352">[-]</label><label class="expand" for="c-37029352">[6 more]</label></div><br/><div class="children"><div class="content">This is the final stage of capitalism, where market success is conflated with scientific rigor.</div><br/><div id="37029508" class="c"><input type="checkbox" id="c-37029508" checked=""/><div class="controls bullet"><span class="by">jakear</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37029352">parent</a><span>|</span><a href="#37028432">next</a><span>|</span><label class="collapse" for="c-37029508">[-]</label><label class="expand" for="c-37029508">[5 more]</label></div><br/><div class="children"><div class="content">Engineering, not scientific.<p>High Sales = A large number of people can attest to the engineered good or service being of high enough quality that they will exchange hard earned money for the ability to use it.<p>Peer Review = Some folks who derive self-worth from citations ask you to add a citation to their work and you do it because you&#x27;ll probably need to ask them for the same some time later.</div><br/><div id="37030769" class="c"><input type="checkbox" id="c-37030769" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37029508">parent</a><span>|</span><a href="#37030176">next</a><span>|</span><label class="collapse" for="c-37030769">[-]</label><label class="expand" for="c-37030769">[2 more]</label></div><br/><div class="children"><div class="content">A quick trip around audiophile companies should quickly disabuse you of the notion that high sales implies any kind of worth to a product. There are several companies making lots of money on selling gold HDMI, ethernet etc cables &quot;to improve sound quality&quot;, and plenty of rubes buying them and &quot;hearing the difference&quot;.</div><br/><div id="37030826" class="c"><input type="checkbox" id="c-37030826" checked=""/><div class="controls bullet"><span class="by">memefrog</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37030769">parent</a><span>|</span><a href="#37030176">next</a><span>|</span><label class="collapse" for="c-37030826">[-]</label><label class="expand" for="c-37030826">[1 more]</label></div><br/><div class="children"><div class="content">Real Hi-Fi companies like Sennheiser and Bose make far more money than people selling gold HDMI cables.  And they&#x27;ve been making much more money for decades and will continue to make money for decades.<p>Grifters don&#x27;t win in the long-term.</div><br/></div></div></div></div><div id="37030176" class="c"><input type="checkbox" id="c-37030176" checked=""/><div class="controls bullet"><span class="by">depereo</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37029508">parent</a><span>|</span><a href="#37030769">prev</a><span>|</span><a href="#37031421">next</a><span>|</span><label class="collapse" for="c-37030176">[-]</label><label class="expand" for="c-37030176">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t really account for fraud or abuse. Plenty of profitable companies whose product is a scam, or has other factors that make it successful.<p>Market success isn&#x27;t the same thing as validity.</div><br/></div></div><div id="37031421" class="c"><input type="checkbox" id="c-37031421" checked=""/><div class="controls bullet"><span class="by">optionalsquid</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37029508">parent</a><span>|</span><a href="#37030176">prev</a><span>|</span><a href="#37028432">next</a><span>|</span><label class="collapse" for="c-37031421">[-]</label><label class="expand" for="c-37031421">[1 more]</label></div><br/><div class="children"><div class="content">You frequently see this kind of reasoning in medical quackery:<p>&gt; Butt-candling[1] must work, just look all these happy customers!<p>But history is replete with ineffective or downright harmful treatments being popular long after the evidence showed them to be ineffective or harmful. Homeopathy is a prime example of this, seeing as those concoctions contain either no active ingredients or (in cases of low dilutions still labeled &quot;homeopathic&quot;) contain ingredients picked based on notions of sympathetic magic (&quot;like cures like&quot;).<p>[1] A hopefully fictional example.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37028432" class="c"><input type="checkbox" id="c-37028432" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026425">parent</a><span>|</span><a href="#37028481">prev</a><span>|</span><a href="#37026369">next</a><span>|</span><label class="collapse" for="c-37028432">[-]</label><label class="expand" for="c-37028432">[1 more]</label></div><br/><div class="children"><div class="content">Except for the ones that conclude it is all a scam.</div><br/></div></div></div></div><div id="37026369" class="c"><input type="checkbox" id="c-37026369" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025986">parent</a><span>|</span><a href="#37026425">prev</a><span>|</span><a href="#37027843">next</a><span>|</span><label class="collapse" for="c-37026369">[-]</label><label class="expand" for="c-37026369">[3 more]</label></div><br/><div class="children"><div class="content">Yes, because we know how the metaphorical sausage is made: with unpaid reviewers who have many other, more interesting things to do and often an axe to grind. That is, if they don’t delegate the review to one of their post-docs.</div><br/><div id="37026892" class="c"><input type="checkbox" id="c-37026892" checked=""/><div class="controls bullet"><span class="by">aftoprokrustes</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026369">parent</a><span>|</span><a href="#37027843">next</a><span>|</span><label class="collapse" for="c-37026892">[-]</label><label class="expand" for="c-37026892">[2 more]</label></div><br/><div class="children"><div class="content">Post doc? In what kind of utopian field did you work? In my former institute virtually all papers were written by PhD candidates, and reviewed by PhD candidates. With the expected effect on quality (due to lack of experience and impostor-syndrome-induced &quot;how can I propose to reject? They are likely better than me&quot;). But the Prof-to-postdoc-to-PhD-ratio was particularly bad (1-2-15).</div><br/><div id="37027427" class="c"><input type="checkbox" id="c-37027427" checked=""/><div class="controls bullet"><span class="by">kelipso</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026892">parent</a><span>|</span><a href="#37027843">next</a><span>|</span><label class="collapse" for="c-37027427">[-]</label><label class="expand" for="c-37027427">[1 more]</label></div><br/><div class="children"><div class="content">I was reviewing papers starting second semester of grad school with my advisor just signing off on it, so not even PhD candidates, and it was the same for my lab mates too.<p>Initially we spent probably a few hours on a paper for peer review because we were relatively unfamiliar with the field but eventually I spent maybe a couple of hours doing the review. Wouldn&#x27;t say peer review is a joke but it&#x27;s definitely overrated by the public.</div><br/></div></div></div></div></div></div><div id="37027843" class="c"><input type="checkbox" id="c-37027843" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025986">parent</a><span>|</span><a href="#37026369">prev</a><span>|</span><a href="#37025220">next</a><span>|</span><label class="collapse" for="c-37027843">[-]</label><label class="expand" for="c-37027843">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know how scientists handle peer review but aren&#x27;t they fighting with peer review to get their papers published and apply for PhD and tenure and grants etc with these publications?</div><br/></div></div></div></div></div></div><div id="37025220" class="c"><input type="checkbox" id="c-37025220" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37023255">prev</a><span>|</span><a href="#37025937">next</a><span>|</span><label class="collapse" for="c-37025220">[-]</label><label class="expand" for="c-37025220">[21 more]</label></div><br/><div class="children"><div class="content">&gt;I don&#x27;t see how this could ever work, and non-scientists seem to often dramatically underestimate the amount of work it would be to replicate every published paper.<p>I think it would be fine to half the productivity of these fields, if it means that you can reasonably expect papers to be accurate.</div><br/><div id="37025682" class="c"><input type="checkbox" id="c-37025682" checked=""/><div class="controls bullet"><span class="by">dmarchand90</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025220">parent</a><span>|</span><a href="#37025449">next</a><span>|</span><label class="collapse" for="c-37025682">[-]</label><label class="expand" for="c-37025682">[3 more]</label></div><br/><div class="children"><div class="content">I believe that, contrary to popular belief, the implementation of this system would lead to a substantial increase in productivity in the long run. Here&#x27;s why:<p>Currently, a significant proportion of research results in various fields cannot be reproduced. This essentially means that a lot of work turns out to be flawed, leading to wasted efforts (you can refer to the &#x27;reproducibility crisis&#x27; for more context). Moreover, future research often builds upon this erroneous information, wasting even more resources. As a result, academic journals get cluttered with substandard work, making them increasingly difficult to monitor and comprehend. Additionally, the overall quality of written communication deteriorates as emphasis shifts from the accurate transfer and reproduction of knowledge to the inflated portrayal of novelty.<p>Now consider a scenario where 50% of all research is dedicated to reproduction. Although this may seem to decelerate progress in the short term, it ensures a more consistent and reliable advancement in the long term. The quality of writing would likely improve to facilitate replication. Furthermore, research methodology would be disseminated more quickly, enhancing overall research effectiveness.</div><br/><div id="37026263" class="c"><input type="checkbox" id="c-37026263" checked=""/><div class="controls bullet"><span class="by">matthewdgreen</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025682">parent</a><span>|</span><a href="#37025449">next</a><span>|</span><label class="collapse" for="c-37026263">[-]</label><label class="expand" for="c-37026263">[2 more]</label></div><br/><div class="children"><div class="content">In the current system scientists allocate reproduction efforts to results that they intend to build on. So if you’ve claimed a breakthrough technique for levitating widgets — and I think this widget technique can be used to build spacecraft (or if I think your technique is wrong) — then I will allocate precious time and resources to reproducing your work. By contrast if I don’t think your work is significant and worth following up on, then I allocate my efforts somewhere else. The advantage is that more apparently-significant results (“might cure cancer”) tend to get a bigger slice of very limited resources, while dead-end or useless results (“might slightly reduce flatulence in cats”) don’t. This distributed entrepreneurial approach isn’t perfect, but it works better than central planning. By contrast you could adopt a Soviet-like approach where cat farts and cancer both share replication resources, but this seems like it would be bad for everyone (except the cats.)</div><br/><div id="37028960" class="c"><input type="checkbox" id="c-37028960" checked=""/><div class="controls bullet"><span class="by">GravelRocks</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026263">parent</a><span>|</span><a href="#37025449">next</a><span>|</span><label class="collapse" for="c-37028960">[-]</label><label class="expand" for="c-37028960">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Cat fart&quot; research might also be incredibly expensive to replicate compared to the &quot;might cure cancer&quot; research. In that case it would effectively get a bigger slice of resources because we&#x27;re treating all research the same!</div><br/></div></div></div></div></div></div><div id="37025449" class="c"><input type="checkbox" id="c-37025449" checked=""/><div class="controls bullet"><span class="by">advisedwang</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025220">parent</a><span>|</span><a href="#37025682">prev</a><span>|</span><a href="#37028269">next</a><span>|</span><label class="collapse" for="c-37025449">[-]</label><label class="expand" for="c-37025449">[12 more]</label></div><br/><div class="children"><div class="content">It would be more than just half productivity. Not only do you have to do the work twice, but you add the delay of someone else replicating before something can be published and built upon by others. If you are developer, imagine how much your productivity would drop going from a 3 minute build to a 1 day build.</div><br/><div id="37028286" class="c"><input type="checkbox" id="c-37028286" checked=""/><div class="controls bullet"><span class="by">wilde</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025449">parent</a><span>|</span><a href="#37025640">next</a><span>|</span><label class="collapse" for="c-37028286">[-]</label><label class="expand" for="c-37028286">[1 more]</label></div><br/><div class="children"><div class="content">You can easily look up non peer reviewed papers on arxiv. Why would this be different with replication?</div><br/></div></div><div id="37025640" class="c"><input type="checkbox" id="c-37025640" checked=""/><div class="controls bullet"><span class="by">orangepurple</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025449">parent</a><span>|</span><a href="#37028286">prev</a><span>|</span><a href="#37028269">next</a><span>|</span><label class="collapse" for="c-37025640">[-]</label><label class="expand" for="c-37025640">[10 more]</label></div><br/><div class="children"><div class="content">Terrible analogy. It might take months to come up with an idea but another should be able to follow your method and implement it much more quickly than it took you to come up with the concept and implement it.</div><br/><div id="37026544" class="c"><input type="checkbox" id="c-37026544" checked=""/><div class="controls bullet"><span class="by">cycomanic</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025640">parent</a><span>|</span><a href="#37026447">next</a><span>|</span><label class="collapse" for="c-37026544">[-]</label><label class="expand" for="c-37026544">[4 more]</label></div><br/><div class="children"><div class="content">I think you don&#x27;t understand how much work is involved in just building the techniques and expertise to pull some experiments off (let&#x27;s not even talk about the equipment).<p>Even if someone meticulously documents their process, it could still take months to replicate the results.<p>I&#x27;m familiar with lithography&#x2F;nanofabrication and I know that it is typically the case that a process developed in one clean-room can not be directly applied to a different clean room and instead one has to develop a new process based on what the other results.<p>Even in the same lab it can often happen that if you come back to a process after a longer time, that things don&#x27;t work out anymore and quite a bit of troubleshooting ensues (maybe a supplier for some chemical changed and even though it should be the same formula it behaves slightly different).</div><br/><div id="37027162" class="c"><input type="checkbox" id="c-37027162" checked=""/><div class="controls bullet"><span class="by">RoyalHenOil</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026544">parent</a><span>|</span><a href="#37029882">next</a><span>|</span><label class="collapse" for="c-37027162">[-]</label><label class="expand" for="c-37027162">[2 more]</label></div><br/><div class="children"><div class="content">Months. Haha.<p>I previously worked in agricultural research (in the private sector), and we spent YEARS trying to replicate some published research from overseas. And that was research that had previously been successfully replicated, and we even flew in the original scientists and borrowed a number of their PhD students for several months, year after year, to help us try to make it work.<p>We never did get it to fully replicate in our country. We ended up having to make some pretty extreme changes to the research to get similar (albeit less reliable) results here.<p>We never did figure out why it worked in one part of the world but not another, since we controlled for every other factor we could think of (including literally importing the original team&#x27;s lab supplies at great expense, just in case there was some trace contaminant on locally sourced materials).</div><br/><div id="37030578" class="c"><input type="checkbox" id="c-37030578" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027162">parent</a><span>|</span><a href="#37029882">next</a><span>|</span><label class="collapse" for="c-37030578">[-]</label><label class="expand" for="c-37030578">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We never did figure out why it worked in one part of the world but not another<p>Doesn&#x27;t that indicate further research is needed? It sounds fascinating to me. (I know it isn&#x27;t interesting for the people who couldn&#x27;t get it working.) It also might indicate that the original research was incomplete in the sense that it might be a fluke due to specific conditions in the original country which isn&#x27;t universal.</div><br/></div></div></div></div><div id="37029882" class="c"><input type="checkbox" id="c-37029882" checked=""/><div class="controls bullet"><span class="by">faeriechangling</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026544">parent</a><span>|</span><a href="#37027162">prev</a><span>|</span><a href="#37026447">next</a><span>|</span><label class="collapse" for="c-37029882">[-]</label><label class="expand" for="c-37029882">[1 more]</label></div><br/><div class="children"><div class="content">You are making an argument for replication not against it by stressing how meticulously documenting your process is an “if”, strewing how things like a supplier for a chemical can change and render reproduction impossible, and even if an observation can only be replicated in one clean room that means you effectively only have as long as that clean room remains opens to replicate it.<p>You are almost stressing all the ways we are producing garbage rendered non-reproducible with deficient documentation of processes, changes in supply, and changes in the environment.  All three can be minimized through peer replication.</div><br/></div></div></div></div><div id="37026447" class="c"><input type="checkbox" id="c-37026447" checked=""/><div class="controls bullet"><span class="by">evandrofisico</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025640">parent</a><span>|</span><a href="#37026544">prev</a><span>|</span><a href="#37025814">next</a><span>|</span><label class="collapse" for="c-37026447">[-]</label><label class="expand" for="c-37026447">[4 more]</label></div><br/><div class="children"><div class="content">Usually coming up with a idea is the <i>easy</i> part. For example, in my PhD project, i started with an idea from my advisor that he had in the early 2000.<p>Implementing the code for the simulation and analysis of the data? four months, at most. Running the simulation? almost three years until I had data with good enough resolution for publishing.</div><br/><div id="37027253" class="c"><input type="checkbox" id="c-37027253" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026447">parent</a><span>|</span><a href="#37025814">next</a><span>|</span><label class="collapse" for="c-37027253">[-]</label><label class="expand" for="c-37027253">[3 more]</label></div><br/><div class="children"><div class="content">It’s also very easy to come up with bad ideas — I did plenty of that and I still do, albeit less than I used to. Finding an idea that is novel, interesting, and tractable given your time, skills, resources, and knowledge of the literature is hard, and maybe the most important skill you develop as a researcher.<p>For a reductive example, the idea to solve P vs NP is a great one, but I’m not going to do that any time soon!</div><br/><div id="37028448" class="c"><input type="checkbox" id="c-37028448" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027253">parent</a><span>|</span><a href="#37025814">next</a><span>|</span><label class="collapse" for="c-37028448">[-]</label><label class="expand" for="c-37028448">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say it is a bad idea because you&#x27;re not going to succeed at it.</div><br/><div id="37030105" class="c"><input type="checkbox" id="c-37030105" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028448">parent</a><span>|</span><a href="#37025814">next</a><span>|</span><label class="collapse" for="c-37030105">[-]</label><label class="expand" for="c-37030105">[1 more]</label></div><br/><div class="children"><div class="content">Sure because that’s an easy example and kind of my point. When you’re starting your PhD, it can be hard to determine what questions you can and cannot answer given your skills and surrounding literature. That comes from experience and trying (and failing) a lot.</div><br/></div></div></div></div></div></div></div></div><div id="37025814" class="c"><input type="checkbox" id="c-37025814" checked=""/><div class="controls bullet"><span class="by">magimas</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025640">parent</a><span>|</span><a href="#37026447">prev</a><span>|</span><a href="#37028269">next</a><span>|</span><label class="collapse" for="c-37025814">[-]</label><label class="expand" for="c-37025814">[1 more]</label></div><br/><div class="children"><div class="content">horrible take. Taking the LK99 situation as an example: simply copying and adapting a well described growth recipee to your own setup and lab conditions may take weeks.
And how would you address situations where measurement setups only exist once on the earth? How would you do peer replication of LHC measurements? Wait for 50 years till the next super-collider is built and someone else can finally verify the results?
On a smaller scale: If you need measurements at a synchrotron radiation source to replicate a measurement, is someone supposed to give up his precious measurement time to replicate a paper he isn&#x27;t interested in? And is the original author of a paper that&#x27;s in the queue for peer replication supposed to wait for a year or two till the reviewer gets a beamtime on an appropriate measurement station?
Even smaller:
I did my PhD in a lab with a specific setup that only a single other group in the world had an equivalent to. You simply would not be able to replicate these results.<p>Peer replication is completely unfeasible in experimental fields of science.
The current process of peer review is alright, people just need to learn that single papers standing by themselves don&#x27;t mean too much. The &quot;peer replication&quot; happens over time anyway when others use the same tools, samples, techniques on related problems and find results in agreement with earlier papers.</div><br/></div></div></div></div></div></div><div id="37028269" class="c"><input type="checkbox" id="c-37028269" checked=""/><div class="controls bullet"><span class="by">mapt</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025220">parent</a><span>|</span><a href="#37025449">prev</a><span>|</span><a href="#37025974">next</a><span>|</span><label class="collapse" for="c-37028269">[-]</label><label class="expand" for="c-37028269">[2 more]</label></div><br/><div class="children"><div class="content">We could easily 10x the funding and 5x the manpower we throw at STEM research if we actually cared what they produced.<p>NSF grants distribute 8.5 billion dollars a year, which is less than Major League Baseball (and its Congressionally granted monopoly) makes.  The US Congress has directed 75 billion dollars in aid to Ukraine to date.</div><br/><div id="37028748" class="c"><input type="checkbox" id="c-37028748" checked=""/><div class="controls bullet"><span class="by">dkqmduems</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028269">parent</a><span>|</span><a href="#37025974">next</a><span>|</span><label class="collapse" for="c-37028748">[-]</label><label class="expand" for="c-37028748">[1 more]</label></div><br/><div class="children"><div class="content">That would be reasonable, which isn&#x27;t the point.</div><br/></div></div></div></div><div id="37025974" class="c"><input type="checkbox" id="c-37025974" checked=""/><div class="controls bullet"><span class="by">harimau777</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025220">parent</a><span>|</span><a href="#37028269">prev</a><span>|</span><a href="#37025262">next</a><span>|</span><label class="collapse" for="c-37025974">[-]</label><label class="expand" for="c-37025974">[1 more]</label></div><br/><div class="children"><div class="content">The issue that I see is: even if halving productivity is acceptable to the field as a whole; how do you incentivize a given scientist to put in the effort?<p>This seems particularly problematic because it is already notoriously hard to get tenure and academia is already notoriously unrewarding to researchers who don&#x27;t have tenure.</div><br/></div></div><div id="37025262" class="c"><input type="checkbox" id="c-37025262" checked=""/><div class="controls bullet"><span class="by">hoosieree</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025220">parent</a><span>|</span><a href="#37025974">prev</a><span>|</span><a href="#37025528">next</a><span>|</span><label class="collapse" for="c-37025262">[-]</label><label class="expand" for="c-37025262">[1 more]</label></div><br/><div class="children"><div class="content">Half is wildly optimistic.</div><br/></div></div><div id="37025528" class="c"><input type="checkbox" id="c-37025528" checked=""/><div class="controls bullet"><span class="by">ImPostingOnHN</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025220">parent</a><span>|</span><a href="#37025262">prev</a><span>|</span><a href="#37025937">next</a><span>|</span><label class="collapse" for="c-37025528">[-]</label><label class="expand" for="c-37025528">[1 more]</label></div><br/><div class="children"><div class="content">half would only be possible if, for every single paper published by a given team, there exists a second team just as talented as the original team, skilled in that specific package of techniques, just waiting to replicate that paper</div><br/></div></div></div></div><div id="37025937" class="c"><input type="checkbox" id="c-37025937" checked=""/><div class="controls bullet"><span class="by">sqrt_1</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37025220">prev</a><span>|</span><a href="#37022981">next</a><span>|</span><label class="collapse" for="c-37025937">[-]</label><label class="expand" for="c-37025937">[1 more]</label></div><br/><div class="children"><div class="content">FYI there is a at least one science journal that only publishes reproduced research:<p>Organic Syntheses
&quot;A unique feature of the review process is that all of the data and experiments reported in an article must be successfully repeated in the laboratory of a member of the editorial board as a check for reproducibility prior to publication&quot;<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Organic_Syntheses" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Organic_Syntheses</a></div><br/></div></div><div id="37022981" class="c"><input type="checkbox" id="c-37022981" checked=""/><div class="controls bullet"><span class="by">ebiester</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37025937">prev</a><span>|</span><a href="#37023365">next</a><span>|</span><label class="collapse" for="c-37022981">[-]</label><label class="expand" for="c-37022981">[12 more]</label></div><br/><div class="children"><div class="content">It&#x27;s simple but not easy: You create another path to tenure which is based on replication, or on equal terms as a part of a tenure package. (For example, x fewer papers but x number of replications, and you are expected to have x replications in your specialty.) You also create a grant funding section for replication which is then passed on to these independent systems. (You would have to have some sort of randomization handled as well.) Replication has to be considered at the same value as original research.<p>And maybe smaller faculties at R2s pivot to replication hubs. And maybe this is easier for some sections of biology, chemistry and psychology than it is for particle physics. We could start where cost of replication is relatively low and work out the details.<p>It&#x27;s completely doable in some cases. (It may never be doable in some areas either.)</div><br/><div id="37025648" class="c"><input type="checkbox" id="c-37025648" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37022981">parent</a><span>|</span><a href="#37026599">next</a><span>|</span><label class="collapse" for="c-37025648">[-]</label><label class="expand" for="c-37025648">[2 more]</label></div><br/><div class="children"><div class="content">Your proposal has a whole slew of issues.<p>First, people that want to be professors normally do so because they want to steer their research agenda, not repeat what other people are doing without contribution. Second, who works in their lab? Most of the people doing the leg work in a lab are PhD students, and, to graduate, they need to do something novel to write up in their dissertation. Thus, they can’t just replicate three experiments and get a doctorate. Third, you underestimate how specialized lab groups are — both in terms of the incredibly expensive equipment it is equipped with and the expertise within the lab. Even folks in the same subfield (or even in the same research group!) often don’t have much in common when it comes to interests, experience, and practical skills.<p>For every lab doing new work, you’d basically need a clone of that lab to replicate their work.</div><br/><div id="37027806" class="c"><input type="checkbox" id="c-37027806" checked=""/><div class="controls bullet"><span class="by">majormajor</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025648">parent</a><span>|</span><a href="#37026599">next</a><span>|</span><label class="collapse" for="c-37027806">[-]</label><label class="expand" for="c-37027806">[1 more]</label></div><br/><div class="children"><div class="content">&gt; First, people that want to be professors normally do so because they want to steer their research agenda, not repeat what other people are doing without contribution.<p>If we&#x27;re talking about weird incentives and academia you hit on one of the worst ones right here, I think, since nothing there is very closely connected to helping students learn.<p>I know that&#x27;s a dead horse, but it&#x27;s VERY easy to find reasons that we shouldn&#x27;t be too closely attached to the status quo.<p>&gt; For every lab doing new work, you’d basically need a clone of that lab to replicate their work.<p>Hell, that&#x27;s how startup funding works, or market economies in general. Top-down, non-redundant systems are way more fragile than distributed ecosystems. If you don&#x27;t have the competition and the complete disconnection, you so much more easily fall into political games of &quot;how do we get this published even if it ain&#x27;t great&quot; vs &quot;how do we find shit that will survive the competition&quot;</div><br/></div></div></div></div><div id="37026599" class="c"><input type="checkbox" id="c-37026599" checked=""/><div class="controls bullet"><span class="by">rapjr9</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37022981">parent</a><span>|</span><a href="#37025648">prev</a><span>|</span><a href="#37026043">next</a><span>|</span><label class="collapse" for="c-37026599">[-]</label><label class="expand" for="c-37026599">[1 more]</label></div><br/><div class="children"><div class="content">Another approach I&#x27;ve seen actually used in Computer Science and Physics is to make replication a part of teaching to undergrads and masters candidates.  The students learn how to do the science, and they get a paper out of replicating the work (which may or may not support the original results), and the field benefits from the replication.</div><br/></div></div><div id="37026043" class="c"><input type="checkbox" id="c-37026043" checked=""/><div class="controls bullet"><span class="by">harimau777</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37022981">parent</a><span>|</span><a href="#37026599">prev</a><span>|</span><a href="#37023063">next</a><span>|</span><label class="collapse" for="c-37026043">[-]</label><label class="expand" for="c-37026043">[1 more]</label></div><br/><div class="children"><div class="content">I think that there&#x27;s also a lot of psychological&#x2F;cultural&#x2F;political issues that work also need to be worked out:<p>If someone wins the Nobel Prize, do the people who replicated their work also win it? When the history books are written do the replicators get equal billing to the people who made the discovery?<p>When selecting candidates for prestigious positions, are they really going to consider a replicator equal to an original researcher?</div><br/></div></div><div id="37023063" class="c"><input type="checkbox" id="c-37023063" checked=""/><div class="controls bullet"><span class="by">Eddy_Viscosity2</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37022981">parent</a><span>|</span><a href="#37026043">prev</a><span>|</span><a href="#37023252">next</a><span>|</span><label class="collapse" for="c-37023063">[-]</label><label class="expand" for="c-37023063">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not easy because it isn&#x27;t simple. How do get all of the universities to change their incentives to back this?</div><br/><div id="37023475" class="c"><input type="checkbox" id="c-37023475" checked=""/><div class="controls bullet"><span class="by">ebiester</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023063">parent</a><span>|</span><a href="#37023252">next</a><span>|</span><label class="collapse" for="c-37023475">[-]</label><label class="expand" for="c-37023475">[2 more]</label></div><br/><div class="children"><div class="content">We agree - the &quot;simple not easy&quot; turn of phrase is speaking to that point. It is easy once implemented, but it isn&#x27;t easy to transition. (I am academia-adjacent by marriage but closer to the humanities, so I understand the amount of work it would take to perform the transition.)</div><br/><div id="37023626" class="c"><input type="checkbox" id="c-37023626" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023475">parent</a><span>|</span><a href="#37023252">next</a><span>|</span><label class="collapse" for="c-37023626">[-]</label><label class="expand" for="c-37023626">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t just not easy, it would probably be extremely political to change the structure of the NSF, National Labs, all universities and colleges, etc., so dramatically.</div><br/></div></div></div></div></div></div><div id="37023252" class="c"><input type="checkbox" id="c-37023252" checked=""/><div class="controls bullet"><span class="by">SkyMarshal</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37022981">parent</a><span>|</span><a href="#37023063">prev</a><span>|</span><a href="#37023365">next</a><span>|</span><label class="collapse" for="c-37023252">[-]</label><label class="expand" for="c-37023252">[4 more]</label></div><br/><div class="children"><div class="content"><i>&gt; x fewer papers but x number of replications, and you are expected to have x replications in your specialty.</i><p>Could it be simplified it even further to say x number of papers, but they only count if they’re replicated by others in the field?</div><br/><div id="37023419" class="c"><input type="checkbox" id="c-37023419" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023252">parent</a><span>|</span><a href="#37023365">next</a><span>|</span><label class="collapse" for="c-37023419">[-]</label><label class="expand" for="c-37023419">[3 more]</label></div><br/><div class="children"><div class="content">No, the idea is that the same researcher should produce <i>k</i> papers and <i>n</i> replications, instead of just <i>k + n</i> published papers.<p>I&#x27;d argue that since replication is somehow faster than original research, the requirement would count a replication somewhat lower than an original paper (say, at 0.75).</div><br/><div id="37023583" class="c"><input type="checkbox" id="c-37023583" checked=""/><div class="controls bullet"><span class="by">ebiester</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023419">parent</a><span>|</span><a href="#37023365">next</a><span>|</span><label class="collapse" for="c-37023583">[-]</label><label class="expand" for="c-37023583">[2 more]</label></div><br/><div class="children"><div class="content">That is my idea... If we opened it up, there&#x27;s probably more interesting iterations, such as requiring pre-registration for all papers, having papers with pre-registration count as some portion of a full paper even if they fail so long as the pre-registration passed scrutiny, having non-replicated papers count as some portion of a fully replicated paper, and having replication as a separate category such that there is a minimum k, a minimum n, and a minimum k+n.<p>The non-easy part of this is once we start making changes to the criteria for tenure, this opens up people trying to stuff all the solutions for all of the problems that everyone knows already. (See Above.) Would some one try to stuff code-available for CS conference papers, for example? What does it mean for a poster session? At what point are papers released for pre-print? What does it mean for the tenure clock or the Ph.D clock? Does it mean that pre-tenure can&#x27;t depend on studies that take time to replicate? What do we do with longitudinal studies?<p>I think you&#x27;re looking at a 50 year transition where you would have to start simple and iterate.</div><br/><div id="37026166" class="c"><input type="checkbox" id="c-37026166" checked=""/><div class="controls bullet"><span class="by">harimau777</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023583">parent</a><span>|</span><a href="#37023365">next</a><span>|</span><label class="collapse" for="c-37026166">[-]</label><label class="expand" for="c-37026166">[1 more]</label></div><br/><div class="children"><div class="content">Is tenure really as mechanical as &quot;publish this many papers and you get it&quot;? My impression was that it took into account things like impact factor and was much more subjective. If that were the case, then wouldn&#x27;t you run into problems with whoever decides tenure paying lip service to counting replication or failed pre-registered papers but in practice being biased in favor of original research?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37023365" class="c"><input type="checkbox" id="c-37023365" checked=""/><div class="controls bullet"><span class="by">justinpombrio</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37022981">prev</a><span>|</span><a href="#37025206">next</a><span>|</span><label class="collapse" for="c-37023365">[-]</label><label class="expand" for="c-37023365">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If someone cares enough about the work to build on it, they will replicate it anyway.<p>Well, the trouble is that hasn&#x27;t been the case in practice. A lot of the replication crisis was attempting for the first time to replicate a <i>foundational</i> paper that dozens of other papers took as true and built on top of, and then seeing said foundational paper fail to replicate. The incentives point toward doing new research instead of replication, and that needs to change.</div><br/><div id="37025492" class="c"><input type="checkbox" id="c-37025492" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023365">parent</a><span>|</span><a href="#37025206">next</a><span>|</span><label class="collapse" for="c-37025492">[-]</label><label class="expand" for="c-37025492">[2 more]</label></div><br/><div class="children"><div class="content">It is the case in my field (ML): if I care enough about a published result I try to replicate it.</div><br/><div id="37027495" class="c"><input type="checkbox" id="c-37027495" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025492">parent</a><span>|</span><a href="#37025206">next</a><span>|</span><label class="collapse" for="c-37027495">[-]</label><label class="expand" for="c-37027495">[1 more]</label></div><br/><div class="children"><div class="content">This is something very sensible in ML since, you likely want to use that algorithm for something else (or to extend &#x2F; modify it), so you need to get it working in your pipeline and verify it works by comparing with the published result.<p>In something like psychology that is likely harder, since the experiment you want to do might be related to but differ significantly from the prior work. I am no psychologist, but I’d like to think that they don’t take one study as ground truth for that reason but try to understand causal mechanisms with multiple studies as data points. If the hypothesis is correct, it will likely present in multiple ways.</div><br/></div></div></div></div></div></div><div id="37025206" class="c"><input type="checkbox" id="c-37025206" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37023365">prev</a><span>|</span><a href="#37026912">next</a><span>|</span><label class="collapse" for="c-37025206">[-]</label><label class="expand" for="c-37025206">[18 more]</label></div><br/><div class="children"><div class="content">&gt; If someone cares enough about the work to build on it, they will replicate it anyway.<p>Does it really deserve to be called <i>work</i> if it doesn&#x27;t include the a full, working set of instructions that if followed to a T allow it to be replicated? To me that&#x27;s more like pollution, making it someone else&#x27;s problem. I certainly don&#x27;t see how &quot;we did this, just trust us&quot; can even be considered science, and that&#x27;s not because I don&#x27;t understand the scientific method, that&#x27;s because I don&#x27;t make a living with it, and have no incentive to not rock the boat.</div><br/><div id="37025620" class="c"><input type="checkbox" id="c-37025620" checked=""/><div class="controls bullet"><span class="by">MrJohz</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025206">parent</a><span>|</span><a href="#37025329">next</a><span>|</span><label class="collapse" for="c-37025620">[-]</label><label class="expand" for="c-37025620">[8 more]</label></div><br/><div class="children"><div class="content">I work with code, which is about as reproducible as it is possible to get - the artifacts I produce are literally just instructions on how to reproduce the work I&#x27;ve done again, and again, and again. And still people come to me with some bug that they&#x27;ve experienced on their machine, that I cannot reproduce on my machine, despite the two environments being as identical as I can possibly make them.<p>I agree that reproduction in scientific work is important, but it is also apparently impossible in the best possible circumstances. When dealing with physical materials, inexact measurements, margins of error, etc, I think we have to accept that there is no set of instructions that, if followed to a T, will ever ensure perfect replication.</div><br/><div id="37025692" class="c"><input type="checkbox" id="c-37025692" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025620">parent</a><span>|</span><a href="#37025329">next</a><span>|</span><label class="collapse" for="c-37025692">[-]</label><label class="expand" for="c-37025692">[7 more]</label></div><br/><div class="children"><div class="content">&gt; And still people come to me with some bug that they&#x27;ve experienced on their machine, that I cannot reproduce on my machine<p>But this is the other way around. Have you ever written a program that doesn&#x27;t run <i>anywhere</i> except a single machine of yours? Would you release it and advertise it and encourage other people to use it as dependency in their software?<p>If it only runs on one machine of yours, you don&#x27;t even know if your code is doing something, or something else in the machine&#x2F;OS. Or in terms of science, whether the research says something about the world, or just about the research setup.</div><br/><div id="37025949" class="c"><input type="checkbox" id="c-37025949" checked=""/><div class="controls bullet"><span class="by">MrJohz</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025692">parent</a><span>|</span><a href="#37025329">next</a><span>|</span><label class="collapse" for="c-37025949">[-]</label><label class="expand" for="c-37025949">[6 more]</label></div><br/><div class="children"><div class="content">I think you misunderstand the point of scientific publication here (at least in theory, perhaps less so in practice). The purpose of a paper is typically to say &quot;I have achieved these results in this environment (as far as I can tell)&quot;, and encourages reproduction. But the original result is useful in its own right - it tells us that there may be something worth exploring. Yes, it may just be a measurement error (I remember the magic faster than light neutrinos), but if it is exciting enough, and lots of eyes end up looking, then flaws are typically found fairly quickly.<p>And yes, there are often overly excited press releases that accompany it - the &quot;advertise it and encourage others to us it as a dependency&quot; part of it analogy - but this is typically just noise in the context of scientific research. If that is your main problem with scientific publishing, you may want to be more critical of science journalism instead.<p>Fwiw, yes of course I&#x27;ve written code that only runs on my machine. I imagine everyone has, typically accidentally. You do it, you realise your mistake, you learn something from it. Which is exactly what we expect from scientific papers that can&#x27;t be reproduced.</div><br/><div id="37027041" class="c"><input type="checkbox" id="c-37027041" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025949">parent</a><span>|</span><a href="#37025329">next</a><span>|</span><label class="collapse" for="c-37027041">[-]</label><label class="expand" for="c-37027041">[5 more]</label></div><br/><div class="children"><div class="content">&gt; But the original result is useful in its own right - it tells us that there may be something worth exploring.<p>I disagree. It shows that when someone writes something in a text editor and publishes it, others can read the words they wrote. That&#x27;s all it shows, by itself. Just like someone writing something on the web only tells us that a textarea accepts just about any input.<p>And even if it did show more than that, when someone &quot;explores&quot; it, is the result is more of that, something that might be true, might not be, but &quot;is worth exploring&quot;? Then at what point does falsifiability enter into it? Why not right away? To me it&#x27;s just another variation of making it someone else&#x27;s problem, kicking the can down the road.<p>&gt; if it is exciting enough, and lots of eyes end up looking, then flaws are typically found fairly quickly.<p>If that was true, there wouldn&#x27;t even be a replication issue, much less a replication crisis. It&#x27;s like saying open source means a lot of people look at the code, if it&#x27;s important enough. Time and time again that&#x27;s proven wrong, e.g. <a href="https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;open-source-software-security-vulnerabilities-exist-for-over-four-years-before-detection-study&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;open-source-software-security-...</a><p>&gt;  yes of course I&#x27;ve written code that only runs on my machine. I imagine everyone has<p>I wouldn&#x27;t even know how to go about doing that. Can you post something that only runs on one of your machines, and you don&#x27;t know why? Note I didn&#x27;t say your machine, I said <i>one</i> machine of yours. Would you publish something that runs on one machine of yours but not a single other one, other than to ask &quot;can anyone tell me why this only runs on this machine&quot;? I doubt it.</div><br/><div id="37027425" class="c"><input type="checkbox" id="c-37027425" checked=""/><div class="controls bullet"><span class="by">MrJohz</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027041">parent</a><span>|</span><a href="#37027393">next</a><span>|</span><label class="collapse" for="c-37027425">[-]</label><label class="expand" for="c-37027425">[2 more]</label></div><br/><div class="children"><div class="content">I think you may be seeing the purpose of these papers differently to me, which may be the cause of this confusion.<p>The way you&#x27;re describing a scientific publication is as if it were the end result of the scientific act. To use the software analogy, you&#x27;re describing publication like a software release: all tests have been performed, all CI workflows have passed, QA have checked everything, and the result is about to be shipped to customers.<p>But talking to researchers, they see publishing more like making a new branch in a repository. There is no expectation that the code in that branch already be perfect (hence why it might only run on one machine, or not even run at all, because sometimes even something that doesn&#x27;t work is still worth committing and exploring later).<p>And just like in software, where you might eventually merge those branches and create a release out of it, in the scientific world you have metastudies or other forms of analysis and literature reviews that attempt to glean a consensus out of what has been published so far. And typically in the scientific world, this is what happens. However, in journalism, this isn&#x27;t usually what happens, and one person&#x27;s experimental, &quot;I&#x27;ve only tested this on my machine&quot; research is often treated as equivalent to another person&#x27;s &quot;release branch&quot; paper evaluating the state of a field and identifying which findings are likely to represent real, universal truths.<p>Which isn&#x27;t to say that journalists are the only ones at fault here - universities that evaluate researchers primarily on getting papers into journals, and prestige systems that make it hard to go against conventional wisdom in the field both cause similar problems by conflating different levels of research or adding competing incentives to researchers&#x27; work. But I don&#x27;t think that invalidates the basic idea of published research: to present a found result (or non-really), provide as much information as possible about how to replicate the result again, and then let other people use that information to inform their work. It just requires us to be mindful of how we let that research inform us.</div><br/><div id="37027801" class="c"><input type="checkbox" id="c-37027801" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027425">parent</a><span>|</span><a href="#37027393">next</a><span>|</span><label class="collapse" for="c-37027801">[-]</label><label class="expand" for="c-37027801">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But talking to researchers, they see publishing more like making a new branch in a repository.<p>Well some do, others don&#x27;t. Like the one who wrote the article this is a discussion of.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Replication_crisis" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Replication_crisis</a><p>&gt; Replication is one of the central issues in any empirical science. To confirm results or hypotheses by a repetition procedure is at the basis of any scientific conception. A replication experiment to demonstrate that the same findings can be obtained in any other place by any other researcher is conceived as an operationalization of objectivity. It is the proof that the experiment reflects knowledge that can be separated from the specific circumstances (such as time, place, or persons) under which it was gained.<p>Or, in short, &quot;one is none&quot;. One <i>might</i> turn into more than one, it might not. Until it does, it&#x27;s not real.<p>more snippets from the above WP article:<p>&gt; This experiment was part of a series of three studies that had been widely cited throughout the years, was regularly taught in university courses<p>&gt; what the community found particularly upsetting was that many of the flawed procedures and statistical tools used in Bem’s studies were part of common research practice in psychology.<p>&gt; alarmingly low replication rates (11-20%) of landmark findings in preclinical oncological research<p>&gt; A 2019 study in Scientific Data estimated with 95% confidence that of 1,989 articles on water resources and management published in 2017, study results might be reproduced for only 0.6% to 6.8%, even if each of these articles were to provide sufficient information that allowed for replication<p>I&#x27;m not saying it couldn&#x27;t be fine to just publish things because they &quot;could be interesting&quot;. But the overall situation seems like quite the dumpster fire to me. As does software, FWIW.</div><br/></div></div></div></div><div id="37027393" class="c"><input type="checkbox" id="c-37027393" checked=""/><div class="controls bullet"><span class="by">varjag</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027041">parent</a><span>|</span><a href="#37027425">prev</a><span>|</span><a href="#37025329">next</a><span>|</span><label class="collapse" for="c-37027393">[-]</label><label class="expand" for="c-37027393">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Note I didn&#x27;t say your machine, I said one machine of yours.<p>This thread discusses <i>peer</i> replication, this is not even an analogy.</div><br/><div id="37027613" class="c"><input type="checkbox" id="c-37027613" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027393">parent</a><span>|</span><a href="#37025329">next</a><span>|</span><label class="collapse" for="c-37027613">[-]</label><label class="expand" for="c-37027613">[1 more]</label></div><br/><div class="children"><div class="content">If you can&#x27;t <i>even</i> replicate it yourself, what makes you think peers could? We are talking about something not being replicated, not even by the original author. The most extreme version would be something that you could only get to run once on the same machine, and never on any other machine.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37025329" class="c"><input type="checkbox" id="c-37025329" checked=""/><div class="controls bullet"><span class="by">davidktr</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025206">parent</a><span>|</span><a href="#37025620">prev</a><span>|</span><a href="#37026912">next</a><span>|</span><label class="collapse" for="c-37025329">[-]</label><label class="expand" for="c-37025329">[9 more]</label></div><br/><div class="children"><div class="content">You just described the majority of scientific papers. A &quot;working set of instructions&quot; is not really feasible in most cases. You can&#x27;t include every piece of hard- and software required to replicate your own setup.</div><br/><div id="37025559" class="c"><input type="checkbox" id="c-37025559" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025329">parent</a><span>|</span><a href="#37025537">next</a><span>|</span><label class="collapse" for="c-37025559">[-]</label><label class="expand" for="c-37025559">[6 more]</label></div><br/><div class="children"><div class="content">Then don&#x27;t call it science, since it doesn&#x27;t contribute anything to the body of human knowledge.<p>I think it&#x27;s fascinating that we can at the same time hold things like &quot;one is none&quot; to be true, or that you should write tests first, but with science we already got so used to a lack of discipline that we just declare it fine.<p>It&#x27;s not hard to not climb a tower you can&#x27;t get down from. It&#x27;s the default, actually. You start with something small where you can describe everything that goes into replicating it. Then you replicate it yourself, based on your own instructions. Before that, you don&#x27;t bother anyone else with it. Once that is done, and others can replicate as well, it &quot;actually exists&quot;.<p>And if that means the majority of stuff has to be thrown out, I&#x27;d suggest doing that sooner rather than later, instead of just accumulating scientific debt.</div><br/><div id="37026861" class="c"><input type="checkbox" id="c-37026861" checked=""/><div class="controls bullet"><span class="by">cycomanic</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025559">parent</a><span>|</span><a href="#37026334">next</a><span>|</span><label class="collapse" for="c-37026861">[-]</label><label class="expand" for="c-37026861">[2 more]</label></div><br/><div class="children"><div class="content">This is a very simplistic view. Why do believe QC departments exist? Even in an industrial setting,  companies make the same thing at the same place on the same equipment after sometimes years of process optimisation of well understood technology. This is essentially a best case scenario and still results fail to reproduce. How are scientists who work at the cutting edge of technology with much smaller budgets supposed to give instructions that can be easily reproduced on first go? Moreover how are they supposed to easily reproduce other results?<p>That is not to say that scientist should not document the process to their best ability so it can be reproduced in principle. I&#x27;m just arguing that it is impossible to easily reproduce other people&#x27;s results. Again when chemical&#x2F;manufacturing companies open another location they often spend months to years to make the process work in the new factory.</div><br/><div id="37027119" class="c"><input type="checkbox" id="c-37027119" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026861">parent</a><span>|</span><a href="#37026334">next</a><span>|</span><label class="collapse" for="c-37027119">[-]</label><label class="expand" for="c-37027119">[1 more]</label></div><br/><div class="children"><div class="content">&gt; companies make the same thing at the same place on the same equipment after sometimes years of process optimisation of well understood technology. This is essentially a best case scenario and still results fail to reproduce.<p>We&#x27;re not talking about 1 of 10 reproduction attempts failing, we&#x27;re talking about 100%. And no, companies don&#x27;t time and time again try to reproduce something that has never been reproduced and fail, to then try again, endlessly. That&#x27;s just not a thing.<p>&gt; it is impossible to easily reproduce other people&#x27;s results<p>We&#x27;re also not talking about &quot;easily&quot; reproducing something, but <i>at all</i>. And in principle doesn&#x27;t cut it, it needs to be reproduced in practice.</div><br/></div></div></div></div><div id="37026334" class="c"><input type="checkbox" id="c-37026334" checked=""/><div class="controls bullet"><span class="by">davidktr</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025559">parent</a><span>|</span><a href="#37026861">prev</a><span>|</span><a href="#37025537">next</a><span>|</span><label class="collapse" for="c-37026334">[-]</label><label class="expand" for="c-37026334">[3 more]</label></div><br/><div class="children"><div class="content">Imagine two scientists, Bob and Alice. Bob has spent the last 5 years examining a theory thoroughly. Now he can explain down to the last detail why the theory does not hold water, and why generations of researchers have been wrong about the issue. Unfortunately, he cannot offer an alternative, and nobody else can follow his long winded arguments anyway.<p>Meanwhile, Alice has spent the last 5 years making the best possible use of the flawed theory, and published a lot of original research. Sure, many of her publications are rubbish, but a few contain interesting results. Contrary to Bob, Alice can show actual results and has publications.<p>Who do you believe will remain in academia? And, according to public perception, will seem more like an actual scientist?</div><br/><div id="37027364" class="c"><input type="checkbox" id="c-37027364" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026334">parent</a><span>|</span><a href="#37026881">next</a><span>|</span><label class="collapse" for="c-37027364">[-]</label><label class="expand" for="c-37027364">[1 more]</label></div><br/><div class="children"><div class="content">Then Bob has failed.<p>Academic science isn’t just the doing science part but the articulation and presentation of your work to the broader community. If Bob knows this space so well, he should be able to clearly communicate the issue and, ideally, present an easily understandable counter example to the existing theory.<p>Technical folks undervalue presentation when writing articles and presenting at conferences. The burden of proof is on the presenter, and, unless there’s some incredible demonstration at the end, most researchers won’t have the time or attention to slog through your mess of a paper to decipher it. There’s only so much time in the day and too many papers to read.<p>In my experience, the best researchers are also the best presenters. I’ve been to great talks out of my domain that I left feeling like I understood the importance of their work despite not understanding the details. I’ve also seen many talks in my field that I thought were awful because the presentation was convoluted or they didn’t motivate the importance of their problem &#x2F; why their work addressed it</div><br/></div></div><div id="37026881" class="c"><input type="checkbox" id="c-37026881" checked=""/><div class="controls bullet"><span class="by">johnnyworker</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026334">parent</a><span>|</span><a href="#37027364">prev</a><span>|</span><a href="#37025537">next</a><span>|</span><label class="collapse" for="c-37026881">[-]</label><label class="expand" for="c-37026881">[1 more]</label></div><br/><div class="children"><div class="content">I disagree that Bob doesn&#x27;t produce actual results, or that something that is mostly rubbish, but partly &quot;interesting&quot; is an actual result. We know the current incentives are all sorts of broken, across the board. Goodhart&#x27;s law and all that. To me the question isn&#x27;t who remains in academia given the current broken model, but who would remain in academia in one that isn&#x27;t as broken.<p>To put a point on it, if public distrust of science becomes big enough, it all can go away before you can say &quot;cultural revolution&quot; or &quot;fascist strongman&quot;. Then there&#x27;d be no more academia, and its shell would be inhabited by party members, so to speak. I&#x27;d gladly sacrifice the ability of Alice and others like her to live off producing &quot;mostly rubbish&quot; to at least have a <i>chance</i> to save science itself.</div><br/></div></div></div></div></div></div><div id="37025537" class="c"><input type="checkbox" id="c-37025537" checked=""/><div class="controls bullet"><span class="by">lliamander</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025329">parent</a><span>|</span><a href="#37025559">prev</a><span>|</span><a href="#37025444">next</a><span>|</span><label class="collapse" for="c-37025537">[-]</label><label class="expand" for="c-37025537">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a problem worth solving.</div><br/></div></div><div id="37025444" class="c"><input type="checkbox" id="c-37025444" checked=""/><div class="controls bullet"><span class="by">johngladtj</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025329">parent</a><span>|</span><a href="#37025537">prev</a><span>|</span><a href="#37026912">next</a><span>|</span><label class="collapse" for="c-37025444">[-]</label><label class="expand" for="c-37025444">[1 more]</label></div><br/><div class="children"><div class="content">You should.</div><br/></div></div></div></div></div></div><div id="37026912" class="c"><input type="checkbox" id="c-37026912" checked=""/><div class="controls bullet"><span class="by">RugnirViking</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37025206">prev</a><span>|</span><a href="#37023808">next</a><span>|</span><label class="collapse" for="c-37026912">[-]</label><label class="expand" for="c-37026912">[8 more]</label></div><br/><div class="children"><div class="content">lets be brutally honest with ourselves.<p>99% of all papers mean nothing. They add nothing to the collective knowledge of humanity. In my field of robotics there are SOOO many papers that are basically taking three or four established algorithms&#x2F;machine learning models, and applying them to off-the-shelf hardware. The kind of thing any person educated in the field could almost guess the results exactly. Hundreds of such iterations for any reasonably popular problems space (prosthetics, drones for wildfires, museum guide robot) etc every month. Far more than could possibly be useful to anyone.<p>There should probably be some sort of separate process for things that actually claim to make important discoveries. I don&#x27;t know what or how that should work. In all honesty maybe there should just be less papers, however that could be achieved.</div><br/><div id="37027045" class="c"><input type="checkbox" id="c-37027045" checked=""/><div class="controls bullet"><span class="by">indymike</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026912">parent</a><span>|</span><a href="#37027315">next</a><span>|</span><label class="collapse" for="c-37027045">[-]</label><label class="expand" for="c-37027045">[3 more]</label></div><br/><div class="children"><div class="content">&gt; 99% of all papers mean nothing. They add nothing to the collective knowledge of humanity.<p>A lot of papers are done as a part of the process of getting a degree or keeping or getting job. The value is mostly the candidate showing they have the acumen to produce a paper of such quality that meets the publisher and peer review requirements. In some cases, it is to show a future employer some level of accomplishment or renown. The knowledge for humanity is mostly the authors ability to get published.</div><br/><div id="37027229" class="c"><input type="checkbox" id="c-37027229" checked=""/><div class="controls bullet"><span class="by">RugnirViking</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027045">parent</a><span>|</span><a href="#37027315">next</a><span>|</span><label class="collapse" for="c-37027229">[-]</label><label class="expand" for="c-37027229">[2 more]</label></div><br/><div class="children"><div class="content">well yes. But these should go somewhere else than the papers that may actually contain significant results. The problem we have here is that there is an enormous quantity of such useless papers mixed in with the ones actually trying to do science.<p>I understand that part of the reason for that is that people need to appear as though they are part of the &quot;actually trying&quot; crowd to get the desired job effects. But it is nonetheless a problem, and a large one very worth at least trying to solve.</div><br/><div id="37027855" class="c"><input type="checkbox" id="c-37027855" checked=""/><div class="controls bullet"><span class="by">indymike</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027229">parent</a><span>|</span><a href="#37027315">next</a><span>|</span><label class="collapse" for="c-37027855">[-]</label><label class="expand" for="c-37027855">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I understand that part of the reason for that is that people need to appear as though they are part of the &quot;actually trying&quot; crowd to get the desired job effects<p>There&#x27;s another less obvious problem: we don&#x27;t always know what is groundbreaking and what is not until something is published and is accepted at large as being a really big deal.</div><br/></div></div></div></div></div></div><div id="37027315" class="c"><input type="checkbox" id="c-37027315" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026912">parent</a><span>|</span><a href="#37027045">prev</a><span>|</span><a href="#37028107">next</a><span>|</span><label class="collapse" for="c-37027315">[-]</label><label class="expand" for="c-37027315">[1 more]</label></div><br/><div class="children"><div class="content">99% of science is a waste of time, not just the papers. We just don&#x27;t know which 1% will turn out not to be. The point is that this is making progress. As such, these 99% definitely <i>are</i> adding to the collective knowledge. Maybe they add very little and maybe it&#x27;s not worth the effort but it&#x27;s not nothing. I think one of the effects of AI progress will be allowing to extract much more of the little value such publications have (the 99% of papers might not be worth reading but are good enough for feeding the AI).</div><br/></div></div><div id="37028107" class="c"><input type="checkbox" id="c-37028107" checked=""/><div class="controls bullet"><span class="by">LBTables</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026912">parent</a><span>|</span><a href="#37027315">prev</a><span>|</span><a href="#37029113">next</a><span>|</span><label class="collapse" for="c-37028107">[-]</label><label class="expand" for="c-37028107">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In my field of robotics there are SOOO many papers that are basically taking three or four established algorithms&#x2F;machine learning models, and applying them to off-the-shelf hardware.<p>This is a direct result of the aggressive &quot;publish or perish&quot; system.  I worked as an aide in an autonomous vehicles lab for a year and a half during my undergrad, and while the actual work we were doing was really cool cutting edge stuff, it was absolutely maddening the amount of time we wasted blatantly pulling bullshit nothing papers exactly like you describe out of our asses to satisfy the constant chewing out we got that &quot;your lab has only published X papers this month&quot;.</div><br/></div></div><div id="37029113" class="c"><input type="checkbox" id="c-37029113" checked=""/><div class="controls bullet"><span class="by">jimkoen</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026912">parent</a><span>|</span><a href="#37028107">prev</a><span>|</span><a href="#37028467">next</a><span>|</span><label class="collapse" for="c-37029113">[-]</label><label class="expand" for="c-37029113">[1 more]</label></div><br/><div class="children"><div class="content">Thank you. Not even saying this to shit on academia, but modern scientific publishing follows the same governing rules as publishing a YouTube video (in principle).<p>&gt; There should probably be some sort of separate process for things that actually claim to make important discoveries.<p>This used to be Springer Nature and the likes, but they&#x27;ve had so many retractions in the past years + they broke their integrity in the Schoen scandal, allowing lenience in the review process to secure a prestigious publication in their journal.<p>In reality, I mean you&#x27;re probably my academic senior: How does true advancement get publicized these days? You post a YouTube video somewhere. See LK99. No peer review, no fancy stuff, a YouTube video was enough to get Argonne National lab on the case.</div><br/></div></div><div id="37028467" class="c"><input type="checkbox" id="c-37028467" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026912">parent</a><span>|</span><a href="#37029113">prev</a><span>|</span><a href="#37023808">next</a><span>|</span><label class="collapse" for="c-37028467">[-]</label><label class="expand" for="c-37028467">[1 more]</label></div><br/><div class="children"><div class="content">99% is bombastic.  What I <i>would</i> say is that the median scientific paper is wrong and back that up with a very long list of things that could make a paper &quot;wrong&quot; or &quot;not even wrong&quot;.  In the case of physics,  everything about string theory may be one day considered &quot;wrong&quot;.  In the case of medicine all the studies where N &lt; 1&#x2F;10 the number it would take to draw a reliable conclusion are wrong.</div><br/></div></div></div></div><div id="37023808" class="c"><input type="checkbox" id="c-37023808" checked=""/><div class="controls bullet"><span class="by">jofer</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37026912">prev</a><span>|</span><a href="#37028055">next</a><span>|</span><label class="collapse" for="c-37023808">[-]</label><label class="expand" for="c-37023808">[1 more]</label></div><br/><div class="children"><div class="content">Also, don&#x27;t forget that a lot of replication would fundamentally involve going and collecting additional samples &#x2F; observations &#x2F; etc in the field area, which is often expensive, time consuming, and logistically difficult.<p>It&#x27;s not just &quot;can we replicate the analysis on sample X&quot;, but also &quot;can we collect a sample similar to X and do we observe similar things in the vicinity&quot; in many cases.  That alone may require multiple seasons of rather expensive fieldwork.<p>Then you have tens to hundreds of thousands of dollars in instrument time to pay to run various analysis which are needed in parallel with the field observations.<p>It&#x27;s rarely the simple data analysis that&#x27;s flawed and far more frequently subtle issues with everything else.<p>In most cases, rather than try to replicate, it&#x27;s best to test something slightly different to build confidence in a given hypothesis about what&#x27;s going on overall. That merits a separate paper and also serves a similar purpose.<p>E.g. don&#x27;t test &quot;can we observe the same thing at the same place?&quot;, and instead test &quot;can we observe something similar&#x2F;analogous at a different place &#x2F; under different conditions?&quot;.  That&#x27;s the basis of a lot of replication work in geosciences. It&#x27;s not considered replication, as it&#x27;s a completely independent body of work, but it serves a similar purpose (and unlike replication studies, it&#x27;s actually publishable).</div><br/></div></div><div id="37028055" class="c"><input type="checkbox" id="c-37028055" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37023808">prev</a><span>|</span><a href="#37027372">next</a><span>|</span><label class="collapse" for="c-37028055">[-]</label><label class="expand" for="c-37028055">[2 more]</label></div><br/><div class="children"><div class="content">&gt;<i>I don&#x27;t see how this could ever work, and non-scientists seem to often dramatically underestimate the amount of work it would be to replicate every published paper.</i><p>Then perhaps those papers shouldn&#x27;t be published? Or held in any higher esteem than a blog post by the same authors?</div><br/><div id="37028506" class="c"><input type="checkbox" id="c-37028506" checked=""/><div class="controls bullet"><span class="by">gus_massa</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028055">parent</a><span>|</span><a href="#37027372">next</a><span>|</span><label class="collapse" for="c-37028506">[-]</label><label class="expand" for="c-37028506">[1 more]</label></div><br/><div class="children"><div class="content">An arXiv preprint is like a blog post.<p>A paper in a peer review journal is like posting a request for reproduction in a heavily moderated mailing list.<p>A paper in a predatory journal is like the &quot;You are the best ___&quot; price that you get if you pay to go to the &quot;congress&quot; invitation in spam.<p>Neither of them guaranty that the result is true. The publication in some peer review journals give a minimal guaranty that the paper is not horribly bad, but I&#x27;ve seen too much crap there too.<p>I know a few journals and author in my area that are serious and I can guess the result will hold, but I find very difficult to evaluate journals and authors in other areas.</div><br/></div></div></div></div><div id="37027372" class="c"><input type="checkbox" id="c-37027372" checked=""/><div class="controls bullet"><span class="by">majormajor</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37028055">prev</a><span>|</span><a href="#37029844">next</a><span>|</span><label class="collapse" for="c-37027372">[-]</label><label class="expand" for="c-37027372">[1 more]</label></div><br/><div class="children"><div class="content">I think the current system is just measuring entirely the wrong thing. Yes, fewer papers would be published. But today&#x27;s goal is &quot;publish papers&quot; not &quot;learn and disseminate truly useful and novel things&quot;, and while this doesn&#x27;t solve it entirely, it pushes incentives further away from &quot;publish whatever pure crap you can get away with.&quot; You get what you measure -&gt; sometimes you need to change what&#x2F;how you measure.<p>&gt; If someone cares enough about the work to build on it, they will replicate it anyway.<p>That&#x27;s duplicative at the &quot;oh maybe this will be useful to me&quot; stage, with N different people trying to replicate. And with replication not a first-class part of the system, the effort of replication (e_R) is high. For appealing things, N is probably &gt; 2. So N X e_R total effort.<p>If you move the burden at the &quot;replicate to publish&quot; stage, you can fix the number of replicas needed so N=2 (or whatever) <i>and</i> you incentive the orginal researchers to make e_R lower (which will improve the quality of their research <i>even before the submit-for-publication stage</i>).<p>I&#x27;ve been in the system, I spent a year or two chasing the tail of rewrites, submissions, etc, for something that was detectable as low-effect-size in the first place but I was told would still be publishable. I found out as part of that that it would only sometimes yield a good p-value! And everything in the system incentivized me to hide that for as long as possible, instead of incentivizing me to look for something else or make it easy for others to replicate and judge for themselves.<p>Hell, do something like &quot;give undergrads the opportunity to earn Master&#x27;s on top of their BSes, say, by replicating (or blowing holes in) other people&#x27;s submissions.&quot; I would&#x27;ve eaten up an opportunity like that to go <i>really really deep* in some specialized area in exchange for a masters degree in a less-structured way than &quot;just take a bunch more courses.&quot;</i></div><br/></div></div><div id="37029844" class="c"><input type="checkbox" id="c-37029844" checked=""/><div class="controls bullet"><span class="by">faeriechangling</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37027372">prev</a><span>|</span><a href="#37026939">next</a><span>|</span><label class="collapse" for="c-37029844">[-]</label><label class="expand" for="c-37029844">[1 more]</label></div><br/><div class="children"><div class="content">In some fields research can’t be replicated later.  Much of all autism research will NEVER be replicated because the population of those considered autistic is not stable over time.<p>Other research proves impossible to replicate because whatever experiment was not described in enough detail to actually replicate it, which should be grounds to immediately dismiss the research before publishing, but which can’t truly be caught if you don’t actually try to reproduce.<p>Finally these practical concerns don’t even touch on the biggest benefit of reproduction as standard which is that almost nobody wants to reproduce research as they are not rewarded for doing so.  This would give somebody, namely those who want to publish something, a strong impetus to get that reproduction done which wouldn’t otherwise exist.</div><br/></div></div><div id="37026939" class="c"><input type="checkbox" id="c-37026939" checked=""/><div class="controls bullet"><span class="by">throwaway4aday</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37029844">prev</a><span>|</span><a href="#37022948">next</a><span>|</span><label class="collapse" for="c-37026939">[-]</label><label class="expand" for="c-37026939">[5 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the value in publishing something that is never replicated? If no one ever reproduces the experiment and gets the same results then you don&#x27;t know if any interpretations based on that experiment are valid. It would also mean that whatever practical applications could have come from the experiment are never realized. It makes the entire pursuit seem completely useless.</div><br/><div id="37026964" class="c"><input type="checkbox" id="c-37026964" checked=""/><div class="controls bullet"><span class="by">wizofaus</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026939">parent</a><span>|</span><a href="#37027442">next</a><span>|</span><label class="collapse" for="c-37026964">[-]</label><label class="expand" for="c-37026964">[3 more]</label></div><br/><div class="children"><div class="content">&gt; What&#x27;s the value in publishing something that is never replicated?<p>Because it presents an experimental result to other scientists that they may consider worth trying to replicate?</div><br/><div id="37027073" class="c"><input type="checkbox" id="c-37027073" checked=""/><div class="controls bullet"><span class="by">dongping</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026964">parent</a><span>|</span><a href="#37027442">next</a><span>|</span><label class="collapse" for="c-37027073">[-]</label><label class="expand" for="c-37027073">[2 more]</label></div><br/><div class="children"><div class="content">Then those unconfirmed results are better put on arxiv, instead of being used to evaluate the performance of scientists. Tenure and grant committees should only consider replicated work.</div><br/><div id="37027486" class="c"><input type="checkbox" id="c-37027486" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37027073">parent</a><span>|</span><a href="#37027442">next</a><span>|</span><label class="collapse" for="c-37027486">[-]</label><label class="expand" for="c-37027486">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t agree. A published article should not be taken for Gods Truth no matter if it&#x27;s replicated or peer reviewed.<p>Lots of &quot;replicated&quot; &quot;peer-reviewed&quot; research have been found to be wrong. That&#x27;s fine, it&#x27;s part of the process of discovery.<p>A paper should be taken for what it is: a piece of scientific work, a part of a puzzle.</div><br/></div></div></div></div></div></div><div id="37027442" class="c"><input type="checkbox" id="c-37027442" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37026939">parent</a><span>|</span><a href="#37026964">prev</a><span>|</span><a href="#37022948">next</a><span>|</span><label class="collapse" for="c-37027442">[-]</label><label class="expand" for="c-37027442">[1 more]</label></div><br/><div class="children"><div class="content">It still has value if we assume the experiment was done by competent honest people who are unlikely to try to fool us on purpose and unlikely do have made errors.<p>It would be even better if it was replicated of course.<p>Depending on what certainty you need you might have to wait for the result of one or several replications, but that is application dependent.</div><br/></div></div></div></div><div id="37022948" class="c"><input type="checkbox" id="c-37022948" checked=""/><div class="controls bullet"><span class="by">mattkrause</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37026939">prev</a><span>|</span><a href="#37023312">next</a><span>|</span><label class="collapse" for="c-37022948">[-]</label><label class="expand" for="c-37022948">[3 more]</label></div><br/><div class="children"><div class="content">Longer, even!<p>Some experiments that study biological development or trained animals can take a year or more of fairly intense effort to <i>start</i> generating data.</div><br/><div id="37025695" class="c"><input type="checkbox" id="c-37025695" checked=""/><div class="controls bullet"><span class="by">Maxion</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37022948">parent</a><span>|</span><a href="#37025717">next</a><span>|</span><label class="collapse" for="c-37025695">[-]</label><label class="expand" for="c-37025695">[1 more]</label></div><br/><div class="children"><div class="content">A year? some data sets take decades to build up before significant papers can be published on their data. Replication of the dataset is just not feasible.<p>This whole thread just shows how little the average HNer knows about the academic sciences.</div><br/></div></div><div id="37025717" class="c"><input type="checkbox" id="c-37025717" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37022948">parent</a><span>|</span><a href="#37025695">prev</a><span>|</span><a href="#37023312">next</a><span>|</span><label class="collapse" for="c-37025717">[-]</label><label class="expand" for="c-37025717">[1 more]</label></div><br/><div class="children"><div class="content">I know people that had to take a 6+ month trip to Antarctica for part of their work and others that had to share time on a piece of experimental equipment with a whole department — they got a few weeks per year to run their experiment and had to milk that for all it’s worth. Even if they had funding, that machine required large amounts of space and staff to keep it running and they aren’t off the shelf products — only a few exist at large research centers.</div><br/></div></div></div></div><div id="37023312" class="c"><input type="checkbox" id="c-37023312" checked=""/><div class="controls bullet"><span class="by">kshahkshah</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37022948">prev</a><span>|</span><a href="#37025323">next</a><span>|</span><label class="collapse" for="c-37023312">[-]</label><label class="expand" for="c-37023312">[1 more]</label></div><br/><div class="children"><div class="content">When I looked into this, more than 15 years ago, I thought the difficult portion wasn&#x27;t sharing the recipe, but the ingredients, if you will - granted I was in a molecular biology lab. Effectively the Material Transfer Agreements between Universities all trying to protect their IP made working with each other unbelievably inefficient.<p>You&#x27;d have no idea if you were going down a well trodden path which would yield no success because you have no idea it was well trod. No one publishes negative results, etc.</div><br/></div></div><div id="37025323" class="c"><input type="checkbox" id="c-37025323" checked=""/><div class="controls bullet"><span class="by">boxed</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37023312">prev</a><span>|</span><a href="#37023009">next</a><span>|</span><label class="collapse" for="c-37025323">[-]</label><label class="expand" for="c-37025323">[10 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t see how this could ever work, and non-scientists seem to often dramatically underestimate the amount of work it would be to replicate every published paper.<p>I don&#x27;t see how the current system works really either. Fraud is rampant, and replication crisis is the most common state of most fields.<p>Basically the current system is failing at finding out what is true. Which is the entire point. That&#x27;s pretty damn bad.</div><br/><div id="37025360" class="c"><input type="checkbox" id="c-37025360" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025323">parent</a><span>|</span><a href="#37023009">next</a><span>|</span><label class="collapse" for="c-37025360">[-]</label><label class="expand" for="c-37025360">[9 more]</label></div><br/><div class="children"><div class="content">Fraud seems rampant because you hear about cases of fraud, but not about the tens of thousands of research labs plugging away day after day.</div><br/><div id="37025469" class="c"><input type="checkbox" id="c-37025469" checked=""/><div class="controls bullet"><span class="by">lliamander</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025360">parent</a><span>|</span><a href="#37026954">next</a><span>|</span><label class="collapse" for="c-37025469">[-]</label><label class="expand" for="c-37025469">[7 more]</label></div><br/><div class="children"><div class="content">I agree that most labs are probably not out to defraud people. But without replication I don&#x27;t think it&#x27;s reasonable to have much confidence in what is published.</div><br/><div id="37026066" class="c"><input type="checkbox" id="c-37026066" checked=""/><div class="controls bullet"><span class="by">magimas</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025469">parent</a><span>|</span><a href="#37025649">next</a><span>|</span><label class="collapse" for="c-37026066">[-]</label><label class="expand" for="c-37026066">[1 more]</label></div><br/><div class="children"><div class="content">replication happens over time.
For example, when I did my PhD I wanted to grow TaS2 monolayers on a graphene layer on an Iridium crystal. So I took published growth recipees of related materials, adapted them to our setup and then finetuned the recipee for TaS2. This way I basically &quot;peer replicated&quot; the growth of the original paper. I then took those samples to a measurement device and modified the sample in-situ by evaporating Li atoms on top (which was the actual paper but I needed a sample to modify first). I published the paper with the growth recipee and the modification procedure and other colleagues then took those instructions to grow their own samples for their own studies (I think it was MoS2 on Graphene on Cobalt that they grew).<p>This way papers are peer replicated in an emerging manner because the knowledge is passed from one group to another and they use parts of that knowledge to then apply it to their own research. You have to see this from a more holistic picture. Individual papers don&#x27;t mean too much, it&#x27;s their overlap that generates scientific consesus.<p>In contrast, requiring some random reviewer to instead replicate my full paper would be an impossible task. He&#x2F;she would not have the required equipment (because there&#x27;s only 2 lab setups in the whole world with the necessary equipment), he&#x2F;she would probably not have the required knowledge (because mine and his research only partially overlap - e.g. we&#x27;re researching the same materials but I use angle-resolved photoemission experiments and he&#x27;s doing electronic transport) and he&#x2F;she would need to spend weeks first adapting the growth recipee to the point where his sample quality is the same as mine.</div><br/></div></div><div id="37025649" class="c"><input type="checkbox" id="c-37025649" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025469">parent</a><span>|</span><a href="#37026066">prev</a><span>|</span><a href="#37025703">next</a><span>|</span><label class="collapse" for="c-37025649">[-]</label><label class="expand" for="c-37025649">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what publication is about. Publication is a conversation with other researchers; it is part of the process of reaching the truth, not its endpoint.</div><br/><div id="37025719" class="c"><input type="checkbox" id="c-37025719" checked=""/><div class="controls bullet"><span class="by">cpach</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025649">parent</a><span>|</span><a href="#37027037">next</a><span>|</span><label class="collapse" for="c-37025719">[-]</label><label class="expand" for="c-37025719">[1 more]</label></div><br/><div class="children"><div class="content">People in general (at least on da Internetz) seem to focus way to much on single studies, and way too little on meta-studies.<p>AFAICT meta-studies is the level where we as a society really can try to say something intelligent about how stuff works. If an important question is not included in a meta-study, we (i.e. universities and research labs) probably need to do more research on that topic before we really can say that much about it.</div><br/></div></div><div id="37027037" class="c"><input type="checkbox" id="c-37027037" checked=""/><div class="controls bullet"><span class="by">lliamander</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025649">parent</a><span>|</span><a href="#37025719">prev</a><span>|</span><a href="#37025703">next</a><span>|</span><label class="collapse" for="c-37027037">[-]</label><label class="expand" for="c-37027037">[1 more]</label></div><br/><div class="children"><div class="content">Sure, and scientists need a place to have such conversations.<p>But publication is not a closed system. The &quot;published, peer-reviewed paper&quot; is frequently an artifact used to decide practical policy matters in many institutions both public and private.  To the extent that Science (as an institution in its own right) wants to influence policy, that influence needs to be grounded in reproducible results.<p>Also, I would not be surprised if stronger emphasis on reproducibility improved the quality of conversation among scientists.</div><br/></div></div></div></div><div id="37025703" class="c"><input type="checkbox" id="c-37025703" checked=""/><div class="controls bullet"><span class="by">vladms</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025469">parent</a><span>|</span><a href="#37025649">prev</a><span>|</span><a href="#37026954">next</a><span>|</span><label class="collapse" for="c-37025703">[-]</label><label class="expand" for="c-37025703">[2 more]</label></div><br/><div class="children"><div class="content">Maybe replication should (and probably does) happen when the published thing is relevant to some entity and also interesting.<p>I never seen papers as &quot;truth&quot;, but more as &quot;possibilities&quot;. After many other &quot;proofs&quot; (products, papers, demos, etc.) you can assign some concepts&#x2F;ideas the label &quot;truth&quot; but one&#x2F;two papers from the same group is definitely not enough.</div><br/><div id="37027687" class="c"><input type="checkbox" id="c-37027687" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025703">parent</a><span>|</span><a href="#37026954">next</a><span>|</span><label class="collapse" for="c-37027687">[-]</label><label class="expand" for="c-37027687">[1 more]</label></div><br/><div class="children"><div class="content">Yeah passing peer review doesn’t mean that the article is perfect and to be taken as truth now (and remember, to err is human; any coder on here has had some long standing bug that went mostly unnoticed in their code base). It means it passed the journal’s standards for novelty, interest, and rigor based on the described methods as a retained by the editor &#x2F; area chair and peer reviewers that are selected for being knowledgeable on the topic.<p>Implicit in this process is that the authors are acting in good faith. To treat the authors as hostile is both demoralizing for the reviewers (who wants to be that cynical about their field) and would require extensive verification of each statement well beyond what is required to return the review in a timely manner.<p>Unless your paper has mathematical theory (and mistakes do slip through), a publication should not be taken as proof of something on its own, but a data point. Over time and with enough data points, a field builds evidence to turn a hypothesis into a scientific theory.</div><br/></div></div></div></div></div></div><div id="37026954" class="c"><input type="checkbox" id="c-37026954" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025360">parent</a><span>|</span><a href="#37025469">prev</a><span>|</span><a href="#37023009">next</a><span>|</span><label class="collapse" for="c-37026954">[-]</label><label class="expand" for="c-37026954">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately there&#x27;s a lot of evidence that fraud really is very prevalent and we don&#x27;t hear about it anywhere near enough. It depends a lot on the field though.<p>One piece of evidence comes from software like GRIM and SPRITE. GRIM was run over psychology papers and found around 50% had impossible means in them (that could not be arrived at by any combination of allowed inputs) [1]. The authors generally did not cooperate to help uncover the sources of the problems.<p>Yet another comes from estimates by editors of well known journals. For example Richard Horton at the Lancet is no stranger to fraud, having published and promoted the Surgisphere paper. He estimates that maybe 50% of medical papers are making untrue claims, which is interesting in that this intuition matches the number obtained in a different field by a more rigorous method. The former editor of the New England Journal of Medicine stated that it was &quot;no longer possible to believe much of the medical research that is published&quot;.<p>50%+ is a number that crops up frequently in medicine. The famous Ioannidis paper, &quot;Why most published research findings are false&quot; (2005) has been cited over 12,000 times.<p>Marc Andreessen has said in an interview that he talked to the head of a very large government grant agency, and asked him whether it could really be true that half of all biomedical research claims were fake? The guy laughed and said no it&#x27;s not true, it&#x27;s more like 90%. [2]<p>Elizabeth Bik uncovers a lot of fraud. Her work is behind the recent resignation of the head of Stanford University for example. Years ago she said, <i>&quot;Science has a huge problem: 100s (1000s?) of science papers with obvious photoshops that have been reported, but that are all swept under the proverbial rug, with no action or only an author-friendly correction … There are dozens of examples where journals rather accept a clean (better photoshopped?) figure redo than asking the authors for a thorough explanation.&quot;</i>  In reality there seem to be far more than mere thousands, as there are companies that specialize in professionally producing fake scientific papers, and whole markets where they are bought and sold.<p>So you have people who are running the scientific system saying, on the record, that they think science is overrun with fake results. And there is some quantitive data to support this. And it seems to happen quite often now that presidents of entire universities are being caught having engaged in or having signed off on rule breaking behavior, like image manipulation or plagiarism, implying that this behavior is at least rewarded or possibly just very common.<p>There are also whole fields in which the underlying premises are known to be false so arguably that&#x27;s also pretty deceptive (e.g. &quot;bot studies&quot;). If you include those then it&#x27;s quite likely indeed that most published research is simply untrue.<p>[1] <a href="https:&#x2F;&#x2F;peerj.com&#x2F;preprints&#x2F;2064v1&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;peerj.com&#x2F;preprints&#x2F;2064v1&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;www.richardhanania.com&#x2F;p&#x2F;flying-x-wings-into-the-death-star" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.richardhanania.com&#x2F;p&#x2F;flying-x-wings-into-the-dea...</a></div><br/></div></div></div></div></div></div><div id="37023009" class="c"><input type="checkbox" id="c-37023009" checked=""/><div class="controls bullet"><span class="by">dongping</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37025323">prev</a><span>|</span><a href="#37024570">next</a><span>|</span><label class="collapse" for="c-37023009">[-]</label><label class="expand" for="c-37023009">[3 more]</label></div><br/><div class="children"><div class="content">While it is a lot of work, I tend to think that one can then always publish preprints if they can&#x27;t wait for the replication. I don&#x27;t understand why a published paper should count as an achievement (against tenure or funding) at all before the work is replicated. The current model just creates perverse incentives to encourage lying, P-hacking, and cherry-picking. This would at least work for fields like machine learning.<p>This is, of course, a naive proposal without too much thought into it. But I was wondering what I would have missed here.</div><br/><div id="37023887" class="c"><input type="checkbox" id="c-37023887" checked=""/><div class="controls bullet"><span class="by">i_no_can_eat</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023009">parent</a><span>|</span><a href="#37024570">next</a><span>|</span><label class="collapse" for="c-37023887">[-]</label><label class="expand" for="c-37023887">[2 more]</label></div><br/><div class="children"><div class="content">and in this proposal, who will be tasked with replicating the work?</div><br/><div id="37025499" class="c"><input type="checkbox" id="c-37025499" checked=""/><div class="controls bullet"><span class="by">dongping</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023887">parent</a><span>|</span><a href="#37024570">next</a><span>|</span><label class="collapse" for="c-37025499">[-]</label><label class="expand" for="c-37025499">[1 more]</label></div><br/><div class="children"><div class="content">In some fields, replication is already the prerequisite to benchmark the SoTA. So the incentives boil down to publishing them along with negative results. Or as some have suggested, make it mandatory for PHD candidates to replicate.<p>Though, it seems that it is possible to game the system, by creating positive&#x2F;negative replication intentionally, to collude with&#x2F;harm the author.</div><br/></div></div></div></div></div></div><div id="37024570" class="c"><input type="checkbox" id="c-37024570" checked=""/><div class="controls bullet"><span class="by">DoctorOetker</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37023009">prev</a><span>|</span><a href="#37030464">next</a><span>|</span><label class="collapse" for="c-37024570">[-]</label><label class="expand" for="c-37024570">[1 more]</label></div><br/><div class="children"><div class="content">&gt; [...] non-scientists seem to often dramatically underestimate the amount of work it would be to replicate every published paper<p>Either &quot;peer reviewed&quot; articles describe progress of promising results, or they don&#x27;t. If they don&#x27;t the research is effectively ignored (at least until someone finds it promising). So let&#x27;s consider specifically output that described promising results.<p>After &quot;peer review&quot; any apparently promising results prompt other groups to build on them by utilizing it as a step or building block.<p>It can take many failed attempts by independent groups before anyone dares publish the absence of the proclaimed observations, since they may try it over multiple times thinking they must have botched it somewhere.<p>On paper it sounds more expensive to require independent replication, but only because the costs of replication attempts are hidden until its typically rather late.<p>Is it really more expensive if the replication attempts are in some sense mandatory?<p>Or is it perhaps more expensive to pretend science has found a one-shot &quot;peer reviewed&quot; method, resulting in uncoordinated independent reproduction attempts that may go unannounced before, or even after failed replications?<p>The pseudo-final word, end of line?<p>What about the &quot;in some sense mandatory&quot; replication? Perhaps roll provable dice for each article, and in-domain sortition to randomly assign replicators. So every scientist would be spending a certain fraction of their time replicating the research of others. The types of acceptable excuses to derelict these duties should be scrutinized and controlled. But some excuses should be very valid, for example <i>conscientious objection</i>. If you are tasked to reproduce some of Dr. Mengele&#x27;s works, you can cop out on condition that you thoroughly motivate your ethical concerns and objections. This could also bring a lot of healthy criticism to a lot of practices, which is otherwise just ignored an glossed over for fear of future career opportunities.</div><br/></div></div><div id="37030464" class="c"><input type="checkbox" id="c-37030464" checked=""/><div class="controls bullet"><span class="by">chmod600</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37024570">prev</a><span>|</span><a href="#37028021">next</a><span>|</span><label class="collapse" for="c-37030464">[-]</label><label class="expand" for="c-37030464">[1 more]</label></div><br/><div class="children"><div class="content">Please excuse my ignorance, but I&#x27;m not convinced.<p>What are we supposed to do in a hundred years when the scientists of today are dead and we have a bunch of results with important implications that aren&#x27;t documented well enough to replicate?</div><br/></div></div><div id="37028021" class="c"><input type="checkbox" id="c-37028021" checked=""/><div class="controls bullet"><span class="by">iamthemonster</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37030464">prev</a><span>|</span><a href="#37023624">next</a><span>|</span><label class="collapse" for="c-37028021">[-]</label><label class="expand" for="c-37028021">[1 more]</label></div><br/><div class="children"><div class="content">My Master&#x27;s thesis was basically taking a purely theoretical paper and &quot;replicating&quot; it, by which I mean taking the formulae and just writing the software to run them. It sounds trivial to an outsider but even that was I guess 300 hours of work.<p>In general I think undergraduate projects are a great space to attempt to replicate findings, but it heavily depends on the field. Fundamental physics experiments can be expensive and require equipment that&#x27;s outside the reach of undergrads. But one thing I love about engineering as an academic field, by comparison, is that anything you research tends to be more achievable for others to replicate because as your end goal you are aiming for something that&#x27;s practical in the field.</div><br/></div></div><div id="37023624" class="c"><input type="checkbox" id="c-37023624" checked=""/><div class="controls bullet"><span class="by">brightball</span><span>|</span><a href="#37022504">parent</a><span>|</span><a href="#37028021">prev</a><span>|</span><a href="#37023183">next</a><span>|</span><label class="collapse" for="c-37023624">[-]</label><label class="expand" for="c-37023624">[5 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t see how this could ever work, and non-scientists seem to often dramatically underestimate the amount of work it would be to replicate every published paper.<p>The alternative is a bunch of stuff being published which people belief as &quot;science&quot; that doesn&#x27;t hold up under scrutiny, which undermines the reliability of science itself. The current approach simply gives people reason to be skeptical.</div><br/><div id="37025600" class="c"><input type="checkbox" id="c-37025600" checked=""/><div class="controls bullet"><span class="by">ImPostingOnHN</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37023624">parent</a><span>|</span><a href="#37023183">next</a><span>|</span><label class="collapse" for="c-37025600">[-]</label><label class="expand" for="c-37025600">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not convinced this proposed alternative is better than the status quo. It&#x27;s simply not feasible, no matter how many benefits one might imagine.<p>the concern about skepticism is not irrelevant, but many of these skeptics also are skeptical of the earth being round, or older than a few thousand years, or not created by an omnipotent skylord, and I&#x27;m not sure it&#x27;s actually a significant concern given the current number and expertise of those who are skeptical<p>so, we can hear their arguments for their skepticism, but that doesn&#x27;t mean the arguments are valid to warrant the skepticism exhibited. And in the end, that&#x27;s what matters: skepticism warranted by valid arguments, not just any Cletus McCletus&#x27;s skepticism of heliocentrism, as if his opinion is equal to that of an astrophysicist (it isn&#x27;t). And you know what? It isn&#x27;t necessary to convince a ditch digger that the earth goes around the sun, if they feel like arguing about it.</div><br/><div id="37028500" class="c"><input type="checkbox" id="c-37028500" checked=""/><div class="controls bullet"><span class="by">brightball</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37025600">parent</a><span>|</span><a href="#37023183">next</a><span>|</span><label class="collapse" for="c-37028500">[-]</label><label class="expand" for="c-37028500">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  skeptical of the earth being round<p>I’m skeptical that these people truly exist outside of the internet wanting it to be true.</div><br/><div id="37028795" class="c"><input type="checkbox" id="c-37028795" checked=""/><div class="controls bullet"><span class="by">ImPostingOnHN</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028500">parent</a><span>|</span><a href="#37023183">next</a><span>|</span><label class="collapse" for="c-37028795">[-]</label><label class="expand" for="c-37028795">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; I’m skeptical that these people truly exist outside of the internet wanting it to be true</i><p>I&#x27;m not, just like I&#x27;m not skeptical that climate change denialists truly exist outside of the internet<p>there&#x27;s simply no valid argument to warrant skepticism of either, given the ease of locating evidence that both do</div><br/><div id="37030978" class="c"><input type="checkbox" id="c-37030978" checked=""/><div class="controls bullet"><span class="by">brightball</span><span>|</span><a href="#37022504">root</a><span>|</span><a href="#37028795">parent</a><span>|</span><a href="#37023183">next</a><span>|</span><label class="collapse" for="c-37030978">[-]</label><label class="expand" for="c-37030978">[1 more]</label></div><br/><div class="children"><div class="content">Sure there is. Ease of proof.<p>I can walk outside and prove gravity.<p>I can find a million pictures of the earth taken from space, look at a globe and view trade routes that circumnavigate it. I can also look to the sky and see the sun and moon are clearly circular which makes a pretty good case for a pattern.<p>Climate change or the age of the earth are based on a whole lot more interconnected bits of science that even if you studied your entire life you could not truly say that you understand. You’re putting your trust in layers of science that add up to a certain conclusion (which is good). When people are given good reasons to believe that science and peer reviews aren’t always legitimate it undermines that process of trust building on trust.<p>Modern aviation is layer upon layer of science building on each other, but I can easily watch a plane takeoff to validate all of those processes.<p>That’s it in a nutshell. If you can easily replicate it, it’s easy to trust. If you can’t, it’s not…especially when it’s used to drive politics.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37023183" class="c"><input type="checkbox" id="c-37023183" checked=""/><div class="controls bullet"><span class="by">matthewdgreen</span><span>|</span><a href="#37022504">prev</a><span>|</span><a href="#37021951">next</a><span>|</span><label class="collapse" for="c-37023183">[-]</label><label class="expand" for="c-37023183">[19 more]</label></div><br/><div class="children"><div class="content">The purpose of science publications is to share new results with other scientists, so others can build on or verify the correctness of the work. There has always been an element of “receiving credit” to this, but the communication aspect is what actually matters <i>from the perspective of maximizing scientific progress.</i><p>In the distant past, publication was an informal process that mostly involved mailing around letters, or for a major result, self-publishing a book. Eventually publishers began to devise formal journals for this purpose, and some of those journals began to receive more submissions than it was feasible to publish or verify just by reputation. Some of the more popular journals hit upon the idea of applying basic editorial standards to reject badly-written papers and obvious spam. Since the journal editors weren’t experts in all fields of science, they asked for volunteers to help with this process. That’s what peer review is.<p>Eventually bureaucrats (inside and largely outside of the scientific community) demanded a technique for measuring the productivity of a scientist, so they could allocate budgets or promotions. They hit on the idea of using publications in a few prestigious journals as a metric, which turned a useful process (sharing results with other scientists) into [from an outsider perspective] a process of receiving “academic points”, where the publication of a result appears to be the end-goal and not just an intermediate point in the validation of a result.<p>Still other outsiders, who misunderstand the entire process, are upset that intermediate results are sometimes incorrect. This confuses them, and they’re angry that the process sometimes assigns “points” to people who they perceive as undeserving. So instead of simply accepting that <i>sharing results widely to maximize the chance of verification</i> is the whole point of the publication process, or coming up with a better set of promotion metrics, they want to gum up the essential sharing process to make it much less efficient and reduce the fan-out degree and rate of publication. This whole mess seems like it could be handled a lot more intelligently.</div><br/><div id="37025157" class="c"><input type="checkbox" id="c-37025157" checked=""/><div class="controls bullet"><span class="by">sebastos</span><span>|</span><a href="#37023183">parent</a><span>|</span><a href="#37025375">next</a><span>|</span><label class="collapse" for="c-37025157">[-]</label><label class="expand" for="c-37025157">[2 more]</label></div><br/><div class="children"><div class="content">Very well put. This is the clearest way of looking at it in my view.<p>I’ll pile on to say that you also have the variable of how the non-scientist public gleans information from the academics. Academia used to be a more insular cadre of people seeking knowledge for its own sake, so this was less relevant. What’s new here is that our society has fixated on the idea that matters of state and administration should be significantly guided by the results and opinions of academia. Our enthusiasm for science-guided policy is a triple whammy, because
1. Knowing that the results of your study have the potential to affect policy creates incentives that may change how the underlying science is performed
2. Knowing that results of academia have outside influence may change WHICH science is performed, and draw in less-than-impartial actors to perform it  
3. The outsized potential impact invites the uninformed public to peer into the world of academia and draw half-baked conclusions from results that are still preliminary or unreplicated. Relatively narrow or specious studies can gain a lot of undue traction if their conclusions appear, to the untrained eye, to provide a good bat to hit your opponent with.</div><br/><div id="37025818" class="c"><input type="checkbox" id="c-37025818" checked=""/><div class="controls bullet"><span class="by">Maxion</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37025157">parent</a><span>|</span><a href="#37025375">next</a><span>|</span><label class="collapse" for="c-37025818">[-]</label><label class="expand" for="c-37025818">[1 more]</label></div><br/><div class="children"><div class="content">A significant problem we face today is the way research, especially in academia, gets spotlighted in the media. They often hyper-focus on single studies, which can give a skewed representation of scientific progress.<p>The reality is that science isn&#x27;t about isolated findings; it&#x27;s a cumulative effort. One paper might suggest a conclusion, but it&#x27;s the collective weight of multiple studies that provides a more rounded understanding. Media&#x27;s tendency to cherry-pick results often distorts this nuanced process.<p>It&#x27;s also worth noting the trend of prioritizing certain studies, like large RCTs or systematic reviews, while overlooking smaller ones, especially pilot studies. Pilot studies are foundational—they often act as the preliminary research needed before larger studies can even be considered or funded. By sidelining or dismissing these smaller, exploratory studies, we risk undermining the very foundation that bigger, more definitive research efforts are built on. If we consistently ignore or undervalue pilot studies, the bigger and often more impactful studies may never even see the light of day.</div><br/></div></div></div></div><div id="37025375" class="c"><input type="checkbox" id="c-37025375" checked=""/><div class="controls bullet"><span class="by">casualscience</span><span>|</span><a href="#37023183">parent</a><span>|</span><a href="#37025157">prev</a><span>|</span><a href="#37023562">next</a><span>|</span><label class="collapse" for="c-37025375">[-]</label><label class="expand" for="c-37025375">[4 more]</label></div><br/><div class="children"><div class="content">Most of this is very legit, but this<p>&gt; 
Still other outsiders, who misunderstand the entire process, are upset that intermediate results are sometimes incorrect. This confuses them, and they’re angry that the process sometimes assigns “points” to people who they perceive as undeserving. So instead of simply accepting that sharing results widely to maximize the chance of verification is the whole point of the publication process, or coming up with a better set of promotion metrics, they want to gum up the essential sharing process to make it much less efficient and reduce the fan-out degree and rate of publication.<p>Does not represent my experience in the academy at all. There is a ton of gamesmanship in publishing. That is ultimately the yardstick academics are measured against, whether we like it or not. No one misunderstands that IMO, the issue is that it&#x27;s a poor incentive. I think creating a new class of publication, one that requires replication, could be workable in some fields (e.g. optics&#x2F;photonics), but probably is totally impossible in others (e.g. experimental particle physics).<p>For purely intellectual fields like mathematics, theoretical physics, philosophy, you probably don&#x27;t need this at all. Then there are &#x27;in the middle fields&#x27; like machine learning which in theory would be easy to replicate, but also would be prohibitively expensive for, e.g. baseline training of LLMs.</div><br/><div id="37025832" class="c"><input type="checkbox" id="c-37025832" checked=""/><div class="controls bullet"><span class="by">Maxion</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37025375">parent</a><span>|</span><a href="#37025493">next</a><span>|</span><label class="collapse" for="c-37025832">[-]</label><label class="expand" for="c-37025832">[2 more]</label></div><br/><div class="children"><div class="content">And on the extreme end you have the multi-decade longitudinal studies in epidemiology &#x2F; biomedicine that would be more-or-less impossible to replicate.</div><br/><div id="37027849" class="c"><input type="checkbox" id="c-37027849" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37025832">parent</a><span>|</span><a href="#37025493">next</a><span>|</span><label class="collapse" for="c-37027849">[-]</label><label class="expand" for="c-37027849">[1 more]</label></div><br/><div class="children"><div class="content">I remember reading that some epidemiologists saw the wealth of new data from CoVID as a silver lining because of how few events there are at that scale. Apparently it’s not uncommon to still use the Spanish Flu data which is spotty at best because it might be the only thing available at the scale you’re interested in</div><br/></div></div></div></div></div></div><div id="37023562" class="c"><input type="checkbox" id="c-37023562" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#37023183">parent</a><span>|</span><a href="#37025375">prev</a><span>|</span><a href="#37023447">next</a><span>|</span><label class="collapse" for="c-37023562">[-]</label><label class="expand" for="c-37023562">[2 more]</label></div><br/><div class="children"><div class="content">For sharing results widely, there&#x27;s arxiv. The problem is that the fanout is now overwhelming.<p>The public perception of a publication in a prestigious journal as the established truth does not help, too.</div><br/><div id="37025181" class="c"><input type="checkbox" id="c-37025181" checked=""/><div class="controls bullet"><span class="by">isaacremuant</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37023562">parent</a><span>|</span><a href="#37023447">next</a><span>|</span><label class="collapse" for="c-37025181">[-]</label><label class="expand" for="c-37025181">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The public perception of a publication in a prestigious journal as the established truth does not help, too.<p>it&#x27;s not so much the public perception but what govs&#x2F;media&#x2F;tech and other institutions have pushed down so that the public doesn&#x27;t question whatever resulting policy they&#x27;re trying to put forth.<p>&quot;Trust the science&quot; means &quot;Thou shalt not question us, simply obey&quot;.<p>Anyone with eyes who has worked in institutions knows that bureocracy, careerism and corruption are intrinsic to them.</div><br/></div></div></div></div><div id="37023447" class="c"><input type="checkbox" id="c-37023447" checked=""/><div class="controls bullet"><span class="by">dmbche</span><span>|</span><a href="#37023183">parent</a><span>|</span><a href="#37023562">prev</a><span>|</span><a href="#37021951">next</a><span>|</span><label class="collapse" for="c-37023447">[-]</label><label class="expand" for="c-37023447">[10 more]</label></div><br/><div class="children"><div class="content">Your analysis seems to portray all scientists as pure hearted. May I remind you of the latest Stanford scandal where the president of Stanford was found to have manipulated data?<p>Today, publications do not serve the same purpose as they did before the internet. It is trivial today to write a convincing paper without research and getting that published(www.theatlantic.com&#x2F;ideas&#x2F;archive&#x2F;2018&#x2F;10&#x2F;new-sokal-hoax&#x2F;572212&#x2F;&amp;sa=U&amp;ved=2ahUKEwjnp5mRtsiAAxVwF1kFHesBDC8QFnoECAkQAg&amp;usg=AOvVaw0t_Bo31BrT5D9zHBdmNAqi).</div><br/><div id="37023622" class="c"><input type="checkbox" id="c-37023622" checked=""/><div class="controls bullet"><span class="by">matthewdgreen</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37023447">parent</a><span>|</span><a href="#37021951">next</a><span>|</span><label class="collapse" for="c-37023622">[-]</label><label class="expand" for="c-37023622">[9 more]</label></div><br/><div class="children"><div class="content">No subset of humanity is “pure hearted.” Fraud and malice will exist in everything people do. Fortunately these fraudulent incidents seem relatively rare, when one compares the number of reported incidents to the number of publications and scientists. But this doesn’t change anything. The benefit of scientific publication is <i>to make it easier to detect and verify incorrect results</i>, which is exactly what happened in this case.<p>I understand that it’s frustrating it didn’t happen instantly. And I also understand that it’s deeply frustrating that some undeserving person accumulated status points with non-scientists based on fraud, and that let them take a high-status position outside of their field. (I think maybe you should assign some blame to the Stanford Trustees for this, but that’s up to you.) None of this means we’d be better off making publication more difficult: it means the metrics are bad.<p>PS When a TFA raises something like “the replication crisis” and then entangles it with accusations of deliberate fraud (high profile but exceedingly rare) it’s like trying to have a serious conversation about automobile accidents, but spending half the conversation on a handful of rare incidents of intentional vehicular homicide. You’re not going to get useful solutions out of this conversation, because it’s (perhaps deliberately) misunderstanding the impact and causes of the problem.</div><br/><div id="37024961" class="c"><input type="checkbox" id="c-37024961" checked=""/><div class="controls bullet"><span class="by">dmbche</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37023622">parent</a><span>|</span><a href="#37027118">next</a><span>|</span><label class="collapse" for="c-37024961">[-]</label><label class="expand" for="c-37024961">[2 more]</label></div><br/><div class="children"><div class="content">For your analogy on car accidents - a notable difference between both is that in the case of car accidents, we are able to get numbers on when, how and why they happen and then make conclusions from that.<p>In this case, we are not even aware of most events of fraud&#x2F;&quot;bad papers&quot;&#x2F;manipulation - the &quot;crisis&quot; is that we are losing faith in the science we are doing - results that were cornerstones of entire fields are found to be nonreproducible, making all the work built on top of it pointless.(psychology, cancer, economics, etc - I&#x27;m being very broad)<p>At this point, we don&#x27;t know how deep the rot goes. We are at the point of recognizing that it&#x27;s real, and looking for solutions. For car accidents, we&#x27;re past that - we&#x27;re just arguing about what are the best solutions. For the replication crisis, we&#x27;re trying to find a way forward.<p>Like that scene in The Thing, where they test the blood? We&#x27;re at the point where we don&#x27;t know who to trust.<p>Ps: what&#x27;s a tfa?</div><br/></div></div><div id="37027118" class="c"><input type="checkbox" id="c-37027118" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37023622">parent</a><span>|</span><a href="#37024961">prev</a><span>|</span><a href="#37021951">next</a><span>|</span><label class="collapse" for="c-37027118">[-]</label><label class="expand" for="c-37027118">[6 more]</label></div><br/><div class="children"><div class="content">Fraud isn&#x27;t exceedingly rare :( It only seems that way because academia doesn&#x27;t pay anyone to find it, reacts to volunteer reports by ignoring it, and the media generally isn&#x27;t interested.<p>Fraud is so frequent and easy to find that there are volunteers who in their spare time manage to routinely uncover not just individual instances of fraud but entire companies whose sole purpose is to generate and sell fake papers on an industrial scale.<p><a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-023-01780-w" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-023-01780-w</a><p>Fraud is so easy and common that there are a steady stream of journals which publish entire editions consisting of nothing but AI generated articles!<p><a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-021-03035-y" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-021-03035-y</a><p>Despite being written as a joke over a decade ago, you can page through an endless stream of papers that were generated by SciGen - a Perl script - and yet they are getting published:<p><a href="https:&#x2F;&#x2F;pubpeer.com&#x2F;search?q=scigen" rel="nofollow noreferrer">https:&#x2F;&#x2F;pubpeer.com&#x2F;search?q=scigen</a><p>The problem is so prevalent that some people created the Problematic Paper Screener, a tool that automatically locates articles that contain text indicative of auto-generation.<p><a href="https:&#x2F;&#x2F;dbrech.irit.fr&#x2F;pls&#x2F;apex&#x2F;f?p=9999:1" rel="nofollow noreferrer">https:&#x2F;&#x2F;dbrech.irit.fr&#x2F;pls&#x2F;apex&#x2F;f?p=9999:1</a>::::::<p>This is all pre-ChatGPT, and is just the researchers who can&#x27;t be bothered writing a paper at all. The more serious problem is all the human written fraudulent papers with bad data and bad methodologies that are never detected, or only detected by randos with  blogs or Twitter accounts that you never hear around.</div><br/><div id="37028003" class="c"><input type="checkbox" id="c-37028003" checked=""/><div class="controls bullet"><span class="by">matthewdgreen</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37027118">parent</a><span>|</span><a href="#37030622">next</a><span>|</span><label class="collapse" for="c-37028003">[-]</label><label class="expand" for="c-37028003">[2 more]</label></div><br/><div class="children"><div class="content">The wonderful thing about the western world is that most countries value freedom of the press. The dark side of this is that you can spin up your own “scientific journal” and charge people to publish in it, game the rankings like any common SEO scam, and nobody will stop you because (especially here in the US) you’re exercising your first amendment rights. Then people can fill it with nonsense and even script-generated fake papers. People outside the scientific community can also scam more “legitimate” for-profit journals in various ways, resulting in more silly publications that the actual scientific community has to filter out. It’s very annoying.<p>None of this has any more bearing on fraud by professional scientists than, say, the existence of some garbage-filled Wikimedia server or a badly-edited Wikipedia page means that the Wikipedia editors themselves are fraudsters.</div><br/><div id="37031888" class="c"><input type="checkbox" id="c-37031888" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37028003">parent</a><span>|</span><a href="#37030622">next</a><span>|</span><label class="collapse" for="c-37031888">[-]</label><label class="expand" for="c-37031888">[1 more]</label></div><br/><div class="children"><div class="content">With respect, I think you should research the topic more deeply before assuming that this is some sort of fringe problem that doesn&#x27;t exist in the &quot;actual&quot; scientific community. The second link I provided is by Nature News and states specifically that the problem affects &quot;prestigious journals&quot; (their words).<p>Auto-generated papers have been published in journals from the IEEE, Elsevier, Springer Nature and other well known publishing houses. These papers have supposedly passed peer review in western journals that have been around for decades, and have been signed off by professional academics. Invariably no satisfactory explanation for how this happens is provided, with &quot;we got hacked&quot; being a remarkably common claim. Quite how you publish an entire magazine full of fraudulent articles due to one person getting hacked is unclear; actual newspapers and magazines don&#x27;t ever have this problem.<p>Here&#x27;s an example. The Springer Nature journal &quot;Personal and Ubiquitous Computing&quot; was established in 1997 and has its own Wikipedia page:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Personal_and_Ubiquitous_Computing" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Personal_and_Ubiquitous_Comput...</a><p>The Editor-In-Chief is a British academic, who also has his own Wikipedia page. So these aren&#x27;t fly-by-night no-brand nobodies. Yet this journal somehow managed to publish dozens of obviously auto-generated papers, like this one:<p><a href="https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1007%2Fs00779-021-01621-5&#x2F;MediaObjects&#x2F;779_2021_1621_MOESM1_ESM.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1007%2Fs007...</a><p>&quot;The conversion of traditional arts and crafts to modern art design under the background of 5G mobile communication network&quot;<p>or<p><a href="https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1007%2Fs00779-021-01611-7&#x2F;MediaObjects&#x2F;779_2021_1611_MOESM1_ESM.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1007%2Fs007...</a><p>&quot;The application of twin network target tracking and support tensor machine in the evaluation of orienteering teaching&quot;<p>The papers are just template paragraphs from totally unrelated topics spliced together. Nobody noticed this had happened until months after publication, strongly implying that this journal has no readers at all (this is a common theme in all these stories, they never seem to notice themselves). The editor agreed the papers were nonsense (his words), but blamed peer reviewers. Yet this journal claims to have a large editorial board with over 40 people on it, mostly from universities in the Europe, USA and China.<p>What&#x27;s amazing is that this exact same &quot;attack&quot; had happened before. The previous year Springer Nature had to retract over 400 papers which were auto-generated in the exact same way. They learned nothing and appear to treat the problem as a similar level of severity to filtering email spam.<p>And in the last six months alone we&#x27;ve seen major fraud scandals impacting Stanford (the President no less), Harvard and Yale. These are supposedly elite universities and researchers. Francesca Gino was earning over $1M a year. Yet their fraud is being uncovered by motivated volunteers, not any kind of systematic well funded science police.<p>So all the signs here point towards fraud being incredibly easy to get away with. Whole journals have literally no readers at all, academia relies on Scooby-Doo levels of policing, and supposedly prestigious brands are constantly having fraud uncovered by random tweeters, undergrads doing journalism as a hobby etc.</div><br/></div></div></div></div><div id="37030622" class="c"><input type="checkbox" id="c-37030622" checked=""/><div class="controls bullet"><span class="by">snowwrestler</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37027118">parent</a><span>|</span><a href="#37028003">prev</a><span>|</span><a href="#37027675">next</a><span>|</span><label class="collapse" for="c-37030622">[-]</label><label class="expand" for="c-37030622">[2 more]</label></div><br/><div class="children"><div class="content">I hope you appreciate the futility of trying to prove that fraud is ruining science publication by linking to a bunch of publications that capably detect and point out all the fraud.</div><br/><div id="37031911" class="c"><input type="checkbox" id="c-37031911" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37030622">parent</a><span>|</span><a href="#37027675">next</a><span>|</span><label class="collapse" for="c-37031911">[-]</label><label class="expand" for="c-37031911">[1 more]</label></div><br/><div class="children"><div class="content">What makes you think they detect and point out all the fraud? These sites are hobby sites run by people who just run some very basic text filtering software. If fraud was being reliably detected paper mills wouldn&#x27;t have businesses, yet these companies seem to be quite common and exist in multiple countries.<p>And recall that I said all this work pre-dates ChatGPT. Using LLMs to generate scientific papers works great, and you won&#x27;t be able to find them using regexs.<p>The journals themselves admit there are serious fraud problems and that they don&#x27;t know what to do about it. So it&#x27;s very concerning. The world needs a trustworthy scientific literature.</div><br/></div></div></div></div><div id="37027675" class="c"><input type="checkbox" id="c-37027675" checked=""/><div class="controls bullet"><span class="by">dmbche</span><span>|</span><a href="#37023183">root</a><span>|</span><a href="#37027118">parent</a><span>|</span><a href="#37030622">prev</a><span>|</span><a href="#37021951">next</a><span>|</span><label class="collapse" for="c-37027675">[-]</label><label class="expand" for="c-37027675">[1 more]</label></div><br/><div class="children"><div class="content">Thanks you - just discovered Scigen, these links are incredible</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37021951" class="c"><input type="checkbox" id="c-37021951" checked=""/><div class="controls bullet"><span class="by">miga</span><span>|</span><a href="#37023183">prev</a><span>|</span><a href="#37024937">next</a><span>|</span><label class="collapse" for="c-37021951">[-]</label><label class="expand" for="c-37021951">[20 more]</label></div><br/><div class="children"><div class="content">Peer review does not serve to assure replication, but assure readability and comprehensibility of the paper.<p>Given that some experiments cost billions to conduct, it is impossible to implement &quot;Peer Replication&quot; for all papers.<p>What could be done is to add metadata about papers that were replicated.</div><br/><div id="37022118" class="c"><input type="checkbox" id="c-37022118" checked=""/><div class="controls bullet"><span class="by">NalNezumi</span><span>|</span><a href="#37021951">parent</a><span>|</span><a href="#37022768">next</a><span>|</span><label class="collapse" for="c-37022118">[-]</label><label class="expand" for="c-37022118">[7 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t readability and comprehensibility the job of the editor&#x2F;journal to check. (after all they&#x27;re actually paid) maybe not for conference, but peer review is more for checking if the methodology, scope, claim, direction, conclusion and relevances is sound&amp;trustable.<p>At least that&#x27;s my understanding</div><br/><div id="37022809" class="c"><input type="checkbox" id="c-37022809" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022118">parent</a><span>|</span><a href="#37030661">next</a><span>|</span><label class="collapse" for="c-37022809">[-]</label><label class="expand" for="c-37022809">[1 more]</label></div><br/><div class="children"><div class="content">The editor is often not the right person to decide based on technical details. Most often, articles they receive anre outside their field of expertise and they don’t really have a way of deciding if a section is comprehensible or not. It’s very difficult for an outsider to know what bit of jargon is redundant and what bit is actually important to make sense of the results. So this bit of readability check falls to the referees.<p>In theory editors (or rather copyeditors, the editors themselves have to handle too many papers to do this sort of thing) should help with things like style, grammar, and spelling. In practice, quality varies but it is often subpar.</div><br/></div></div><div id="37030661" class="c"><input type="checkbox" id="c-37030661" checked=""/><div class="controls bullet"><span class="by">snowwrestler</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022118">parent</a><span>|</span><a href="#37022809">prev</a><span>|</span><a href="#37030614">next</a><span>|</span><label class="collapse" for="c-37030661">[-]</label><label class="expand" for="c-37030661">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Isn’t readability and comprehensibility the job of the editor&#x2F;journal to check<p>Yes, who do you think ask the reviewers to perform their reviews?<p>&gt; peer review is more for checking if the methodology, scope, claim, direction, conclusion and relevances is sound&amp;trustable.<p>No, the parent comment has it right. The only thing being reviewed is the paper, and the point is to make sure it communicates clearly, not that it’s “sound and trustable.”</div><br/></div></div><div id="37030614" class="c"><input type="checkbox" id="c-37030614" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022118">parent</a><span>|</span><a href="#37030661">prev</a><span>|</span><a href="#37022829">next</a><span>|</span><label class="collapse" for="c-37030614">[-]</label><label class="expand" for="c-37030614">[1 more]</label></div><br/><div class="children"><div class="content">The editor is basically deferring to people with expertise who can put the paper into context better than they could. The editor might be an expert in the field, but they can’ speak for every aspect of it like someone working day to day in that specific aspect of the field could. Sometimes the authors themselves even recommend potentially relevant reviewers for the editor to contact for peer reviewing.</div><br/></div></div><div id="37022829" class="c"><input type="checkbox" id="c-37022829" checked=""/><div class="controls bullet"><span class="by">kkylin</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022118">parent</a><span>|</span><a href="#37030614">prev</a><span>|</span><a href="#37029646">next</a><span>|</span><label class="collapse" for="c-37022829">[-]</label><label class="expand" for="c-37022829">[1 more]</label></div><br/><div class="children"><div class="content">Highly dependent on journal &#x2F; field.  In mine (mathematics) most associate editors work for free, same as reviwers.  The reviewer do all the things you say, and in addition try to ensure readability &amp; novelty.  Most journals do have professional copy editing, but that&#x27;s separate from the content review.<p>I don&#x27;t know how refereed conference proceedings work (we don&#x27;t really use these).  The only journals I know of that have professional editors (i.e., editors who are not active researchers themselves) are Nature and affiiliated journals, but someone more knowledgeble should correct me here.</div><br/></div></div><div id="37029646" class="c"><input type="checkbox" id="c-37029646" checked=""/><div class="controls bullet"><span class="by">jxramos</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022118">parent</a><span>|</span><a href="#37022829">prev</a><span>|</span><a href="#37025060">next</a><span>|</span><label class="collapse" for="c-37029646">[-]</label><label class="expand" for="c-37029646">[1 more]</label></div><br/><div class="children"><div class="content">Yes a metadata relationship link would be outstanding. Reproduced in some paper xyz, or by some institution, named individuals, etc. some kind of structured information would be very useful.</div><br/></div></div><div id="37025060" class="c"><input type="checkbox" id="c-37025060" checked=""/><div class="controls bullet"><span class="by">hedora</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022118">parent</a><span>|</span><a href="#37029646">prev</a><span>|</span><a href="#37022768">next</a><span>|</span><label class="collapse" for="c-37025060">[-]</label><label class="expand" for="c-37025060">[1 more]</label></div><br/><div class="children"><div class="content">In CS, the editor &#x2F; journal don’t do those things.  Instead, the reviewers do.  (Sometimes reviewers “shepherd” papers to help fix readability after acceptance).<p>Also, most work goes to conferences; journals typically publish longer versions of published works.</div><br/></div></div></div></div><div id="37022768" class="c"><input type="checkbox" id="c-37022768" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#37021951">parent</a><span>|</span><a href="#37022118">prev</a><span>|</span><a href="#37023046">next</a><span>|</span><label class="collapse" for="c-37022768">[-]</label><label class="expand" for="c-37022768">[5 more]</label></div><br/><div class="children"><div class="content">Barriers to publication should be lower for replication studies, I think that’s the main problem.<p>If someone wants to spend some time replicating something that’s only been described in a paper or two, that is valuable work for the community and should be encouraged. If the person is a PhD student using that as an opportunity to hone their skills, it’s even better. It’s not glamorous, it’s not something entirely new, but it is <i>useful</i> and <i>important</i>. And this work needs to go to normal journals, otherwise there’s just be journals dedicated to replication and their impact factor will be terrible and nobody will care.</div><br/><div id="37029940" class="c"><input type="checkbox" id="c-37029940" checked=""/><div class="controls bullet"><span class="by">jxramos</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022768">parent</a><span>|</span><a href="#37023164">next</a><span>|</span><label class="collapse" for="c-37029940">[-]</label><label class="expand" for="c-37029940">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if undergrads could be harnessed to enter into this kind of work, maybe under the supervision of doctoral students and a well meaning and interested PI.</div><br/></div></div><div id="37023164" class="c"><input type="checkbox" id="c-37023164" checked=""/><div class="controls bullet"><span class="by">s1artibartfast</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022768">parent</a><span>|</span><a href="#37029940">prev</a><span>|</span><a href="#37023046">next</a><span>|</span><label class="collapse" for="c-37023164">[-]</label><label class="expand" for="c-37023164">[3 more]</label></div><br/><div class="children"><div class="content">They&#x27;re basically no barriers to publication. There are a number of normal journals that publish everything submitted if it appears to be honest research.</div><br/><div id="37024076" class="c"><input type="checkbox" id="c-37024076" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37023164">parent</a><span>|</span><a href="#37023046">next</a><span>|</span><label class="collapse" for="c-37024076">[-]</label><label class="expand" for="c-37024076">[2 more]</label></div><br/><div class="children"><div class="content">Not nice journals, though. At least not in my experience but that’s probably very field-dependent. It’s not uncommon to get a summary rejection letter for lack of novelty and that is one aspect they stress when they ask us to review articles.</div><br/><div id="37025079" class="c"><input type="checkbox" id="c-37025079" checked=""/><div class="controls bullet"><span class="by">s1artibartfast</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37024076">parent</a><span>|</span><a href="#37023046">next</a><span>|</span><label class="collapse" for="c-37025079">[-]</label><label class="expand" for="c-37025079">[1 more]</label></div><br/><div class="children"><div class="content">But novelty IS what makes those journals nice and prestigious in the first place. It is the basis of their reputation.<p>It&#x27;s basically a catch 22. We want replication in prestigious journals, but any Journal with replications becomes less novel and prestigious.<p>It all comes down to what people value about journals. If people valued replication more than novelty, replication journals would be the prestigious ones.<p>It all comes back to the fact that doing novel science is considered more prestigious than replication. Institutions can play all kinds of games to try to make it harder for readers to tell novelty apart from replication, but people will just find new ways to signal and determine the difference.<p>Let&#x27;s say we pass a law that prestigious journals must published 50% replications. The Prestige from publishing in that journal will just shift to publishing in that journal with something like first demonstration in the title or publishing in that journal Plus having a high citation or impact value.<p>It is really difficult to come up with the system or institution level solution when novelty is still what individuals value.<p>As long as companies and universities value innovation, figure out ways to determine which scientists are innovative, and value them more</div><br/></div></div></div></div></div></div></div></div><div id="37023046" class="c"><input type="checkbox" id="c-37023046" checked=""/><div class="controls bullet"><span class="by">ebiester</span><span>|</span><a href="#37021951">parent</a><span>|</span><a href="#37022768">prev</a><span>|</span><a href="#37024566">next</a><span>|</span><label class="collapse" for="c-37023046">[-]</label><label class="expand" for="c-37023046">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s not make perfect be the enemy of good. We may never be able to replicate every field, but we could start many fields today. It means changing our values to make replication as a valid path to tenure and promotion and a required element of Ph.D studies.</div><br/></div></div><div id="37024566" class="c"><input type="checkbox" id="c-37024566" checked=""/><div class="controls bullet"><span class="by">strangattractor</span><span>|</span><a href="#37021951">parent</a><span>|</span><a href="#37023046">prev</a><span>|</span><a href="#37023649">next</a><span>|</span><label class="collapse" for="c-37024566">[-]</label><label class="expand" for="c-37024566">[1 more]</label></div><br/><div class="children"><div class="content">Maybe add people as special authors&#x2F;contributors to the original work.<p>There always seems to be a contingent of people that think that anything less than %100 solution is inadequate so nothing is done. Peer review has proven itself inadequate and people hang on to it tooth and nail. Some disciplines should require replication on everything - I won&#x27;t name Psychology or Social Sciences in general but the failure to replicate rate for some is unacceptable.</div><br/></div></div><div id="37023649" class="c"><input type="checkbox" id="c-37023649" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#37021951">parent</a><span>|</span><a href="#37024566">prev</a><span>|</span><a href="#37022341">next</a><span>|</span><label class="collapse" for="c-37023649">[-]</label><label class="expand" for="c-37023649">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Peer review does not serve to assure replication, but assure readability and comprehensibility of the paper.<p>I have had a paper rejected twice in a row over the last year. Both times the comments include something like &quot;paper was very well-wriiten; well-written enough that an undergrad could read it&quot;.<p>Peer review ensures the gates are kept.</div><br/></div></div><div id="37022341" class="c"><input type="checkbox" id="c-37022341" checked=""/><div class="controls bullet"><span class="by">julienreszka</span><span>|</span><a href="#37021951">parent</a><span>|</span><a href="#37023649">prev</a><span>|</span><a href="#37024937">next</a><span>|</span><label class="collapse" for="c-37022341">[-]</label><label class="expand" for="c-37022341">[4 more]</label></div><br/><div class="children"><div class="content">&gt;Experiments that cost billions to conduct<p>If you can&#x27;t replicate them it&#x27;s like they didn&#x27;t happen anyways</div><br/><div id="37022880" class="c"><input type="checkbox" id="c-37022880" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022341">parent</a><span>|</span><a href="#37025962">next</a><span>|</span><label class="collapse" for="c-37022880">[-]</label><label class="expand" for="c-37022880">[1 more]</label></div><br/><div class="children"><div class="content">It’s a bit more subtle than that. Not all papers are equal and I’d trust an article from a large team where error and uncertainty analysis has been done properly (think the Higgs boson paper) over a handful of dodgy experiments that are barely documented properly.<p>But yeah, in the grand scheme of things if it hasn’t been replicated, then it hasn’t been proven, but some works are credible on their own.</div><br/></div></div><div id="37025962" class="c"><input type="checkbox" id="c-37025962" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022341">parent</a><span>|</span><a href="#37022880">prev</a><span>|</span><a href="#37022839">next</a><span>|</span><label class="collapse" for="c-37025962">[-]</label><label class="expand" for="c-37025962">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, if I can’t run the LHC at home, none of the work there happened</div><br/></div></div><div id="37022839" class="c"><input type="checkbox" id="c-37022839" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#37021951">root</a><span>|</span><a href="#37022341">parent</a><span>|</span><a href="#37025962">prev</a><span>|</span><a href="#37024937">next</a><span>|</span><label class="collapse" for="c-37022839">[-]</label><label class="expand" for="c-37022839">[1 more]</label></div><br/><div class="children"><div class="content">So no experiments have happened because I don&#x27;t have a lab, and CERN is just an elaborate ruse?</div><br/></div></div></div></div></div></div><div id="37024937" class="c"><input type="checkbox" id="c-37024937" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#37021951">prev</a><span>|</span><a href="#37022806">next</a><span>|</span><label class="collapse" for="c-37024937">[-]</label><label class="expand" for="c-37024937">[3 more]</label></div><br/><div class="children"><div class="content">For a while Reddit had the mantra “pics or it didn’t happen”.<p>At least in CS&#x2F;ML there needs to be a “code or it didn’t happen”. Why? Papers are ambiguous. Even if they have mathematical formulas, not all components are defined.<p>Peer replication in these fields is an easy low hanging fruit that could set an example for other fields of science.</div><br/><div id="37030492" class="c"><input type="checkbox" id="c-37030492" checked=""/><div class="controls bullet"><span class="by">bradley13</span><span>|</span><a href="#37024937">parent</a><span>|</span><a href="#37025896">next</a><span>|</span><label class="collapse" for="c-37030492">[-]</label><label class="expand" for="c-37030492">[1 more]</label></div><br/><div class="children"><div class="content">CS and ML are my field, although I&#x27;m no longer active in research. I always made a code archive available. Want to replicate? Download and run.<p>This should be standard now, in the age of GitHub, GitLab, et al. If a paper discusses an implementation, but doesn&#x27;t provide code, it is probably BS.</div><br/></div></div><div id="37025896" class="c"><input type="checkbox" id="c-37025896" checked=""/><div class="controls bullet"><span class="by">simlan</span><span>|</span><a href="#37024937">parent</a><span>|</span><a href="#37030492">prev</a><span>|</span><a href="#37022806">next</a><span>|</span><label class="collapse" for="c-37025896">[-]</label><label class="expand" for="c-37025896">[1 more]</label></div><br/><div class="children"><div class="content">That is too simplistic. You underestimate the depth of academia. Sure the latest break through Alzheimers study or related research would benefit from a replication. Which is done out of commercial interest anyway.<p>But your run of the mill niche topic will not have the dollars behind it to replicate everyones research.just because CS&#x2F;AI research is very convenient to replicate does not mean this can be extended to all research being done.<p>That is exactly why peer review exists to weed out the implausible and low effort&#x2F;relevance work. It is not fraud proof because it was not designed to be.</div><br/></div></div></div></div><div id="37022806" class="c"><input type="checkbox" id="c-37022806" checked=""/><div class="controls bullet"><span class="by">infogulch</span><span>|</span><a href="#37024937">prev</a><span>|</span><a href="#37022291">next</a><span>|</span><label class="collapse" for="c-37022806">[-]</label><label class="expand" for="c-37022806">[7 more]</label></div><br/><div class="children"><div class="content">I like the idea of splitting &quot;peer review&quot; into two, and then having a citation threshold standard where a field agrees that a paper should be replicated after a certain number of citations. And journals should have a dedicated section for attempted replications.<p>1. Rebrand peer review as a &quot;readability review&quot; which is what reviewers tend to focus on today.<p>2. A &quot;replicability statement&quot;, a separately published document where reviewers push authors to go into detail about the methodology and strategy used to perform the experiments, including specifics that someone outside of their specialty may not know. Credit NalNezumi ITT</div><br/><div id="37024204" class="c"><input type="checkbox" id="c-37024204" checked=""/><div class="controls bullet"><span class="by">analog31</span><span>|</span><a href="#37022806">parent</a><span>|</span><a href="#37022291">next</a><span>|</span><label class="collapse" for="c-37024204">[-]</label><label class="expand" for="c-37024204">[6 more]</label></div><br/><div class="children"><div class="content">Every experimental paper I&#x27;ve ever read has contained an &quot;Experimental&quot; section, where they provide the details on how they did it. Those sections tend to be general enough, albeit concise.<p>In some fields, aside from specialized knowledge, good experimental work requires what we call &quot;hands.&quot; For instance, handling air sensitive compounds, or anything in a condensed or crystalline state. In my thesis experiment, some of the equipment was hand made, by me.<p>Sometimes specialized facilities are needed. My doctoral thesis project used roughly 1&#x2F;2 million dollars of gear, and some of the equipment that I used was obsolete and unavailable by the time I finished.</div><br/><div id="37024995" class="c"><input type="checkbox" id="c-37024995" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#37022806">root</a><span>|</span><a href="#37024204">parent</a><span>|</span><a href="#37024543">next</a><span>|</span><label class="collapse" for="c-37024995">[-]</label><label class="expand" for="c-37024995">[3 more]</label></div><br/><div class="children"><div class="content">“Concise” isn’t good enough. If other scientists are trying to read through the tea leaves at what you’re trying to say you did, that defeats the entire point of a paper. The purpose of science is to create knowledge <i>that other people can use</i> and if people can’t replicate your work that’s not science.</div><br/><div id="37030681" class="c"><input type="checkbox" id="c-37030681" checked=""/><div class="controls bullet"><span class="by">snowwrestler</span><span>|</span><a href="#37022806">root</a><span>|</span><a href="#37024995">parent</a><span>|</span><a href="#37026271">next</a><span>|</span><label class="collapse" for="c-37030681">[-]</label><label class="expand" for="c-37030681">[1 more]</label></div><br/><div class="children"><div class="content">Yes, maybe there should be a process where peer scientists review a paper before it’s published, to make sure it is written clearly enough for other scientists to understand it.</div><br/></div></div><div id="37026271" class="c"><input type="checkbox" id="c-37026271" checked=""/><div class="controls bullet"><span class="by">analog31</span><span>|</span><a href="#37022806">root</a><span>|</span><a href="#37024995">parent</a><span>|</span><a href="#37030681">prev</a><span>|</span><a href="#37024543">next</a><span>|</span><label class="collapse" for="c-37026271">[-]</label><label class="expand" for="c-37026271">[1 more]</label></div><br/><div class="children"><div class="content">I think the point is you don&#x27;t have to give a complete BOM that includes where you got the power cables. Each scientist has to decide what amount of information needs to be conveyed. Of course this can be abused, or done sloppily, like anything else.<p>A place where you can spread out more is in dissertations. Mine contained an entire chapter on the experiment, another on the analysis, and appendices full of source code, schematics, etc. I happily sent out copies, at my expense. My setup was replicated roughly 3 times.</div><br/></div></div></div></div><div id="37024543" class="c"><input type="checkbox" id="c-37024543" checked=""/><div class="controls bullet"><span class="by">ahmadmijot</span><span>|</span><a href="#37022806">root</a><span>|</span><a href="#37024204">parent</a><span>|</span><a href="#37024995">prev</a><span>|</span><a href="#37022291">next</a><span>|</span><label class="collapse" for="c-37024543">[-]</label><label class="expand" for="c-37024543">[2 more]</label></div><br/><div class="children"><div class="content">&gt; My doctoral thesis project used roughly 1&#x2F;2 million dollars of gear,<p>Wow I envy you. My doctoral thesis project spent like... USD2.5k directly for gears (half of it just to buy lego bricks to build our own instrument exactly because we can&#x27;t afford to buy commercial one lol)</div><br/><div id="37026147" class="c"><input type="checkbox" id="c-37026147" checked=""/><div class="controls bullet"><span class="by">xioxox</span><span>|</span><a href="#37022806">root</a><span>|</span><a href="#37024543">parent</a><span>|</span><a href="#37022291">next</a><span>|</span><label class="collapse" for="c-37026147">[-]</label><label class="expand" for="c-37026147">[1 more]</label></div><br/><div class="children"><div class="content">I used a 3 billion dollar space telescope. I don&#x27;t think NASA are going to launch another to replicate some of my results.</div><br/></div></div></div></div></div></div></div></div><div id="37022291" class="c"><input type="checkbox" id="c-37022291" checked=""/><div class="controls bullet"><span class="by">NalNezumi</span><span>|</span><a href="#37022806">prev</a><span>|</span><a href="#37024102">next</a><span>|</span><label class="collapse" for="c-37022291">[-]</label><label class="expand" for="c-37022291">[3 more]</label></div><br/><div class="children"><div class="content">Imo, A more realistic thing to do is &quot;replicability review&quot; and&#x2F;or requirement to submit &quot;methodology map&quot; to each paper.<p>The former would be a back and forth between a reviewer that inquire and ask questions (based on the paper) with the goal to <i>reproduce the result</i>, but don&#x27;t have to actually reproduce it. This is usually good to find out missing details in the paper that the writer just took for granted everyone in the field knows (I&#x27;ve met Bio PHD that have wasted Months of their life tracking up experimental details not mentioned in a paper)<p>The latter would be the result of the former. Instead of having pages long &quot;appendix&quot; section in the main paper, you produce another document with meticulous details of the experiment&#x2F;methodology with every stone turned together with an peer reviewer. Stamp it with the peer reviewes name so they can&#x27;t get away with hand wavy review.<p>I&#x27;ve read too many papers where important information to reproduce the result is omitted. (for ML&#x2F;RL) If the code is included I&#x27;ve countless of times found implementation details that is not mentioned in the paper. In matter of fact, there&#x27;s even results suggesting that those details are the make or break of certain algorithms. [1] I&#x27;ve also seen breaking details only mentioned in code comments...<p>Another atrocious thing I&#x27;ve witnessed is a paper claiming they evaluated their method on a benchmark and if you check the benchmark, the task they evaluated on doesn&#x27;t exit! They forked the benchmark and made their own task without being clear about it! [2]<p>Shit like this make me lose faith in certain science directions. And I&#x27;ve seen a couple of junior researcher giving it all up because they concluded it&#x27;s all just house of cards.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.12729" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.12729</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.02465" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.02465</a><p>Edit: also if you think that&#x27;s too tedious&#x2F;costly, reminder that publishers rake in record profits so the resources are already there
<a href="https:&#x2F;&#x2F;youtu.be&#x2F;ukAkG6c_N4M" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;ukAkG6c_N4M</a></div><br/><div id="37022978" class="c"><input type="checkbox" id="c-37022978" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#37022291">parent</a><span>|</span><a href="#37024102">next</a><span>|</span><label class="collapse" for="c-37022978">[-]</label><label class="expand" for="c-37022978">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve met Bio PHD that have wasted Months of their life tracking up experimental details not mentioned in a paper<p>Same. Now, when I review manuscripts, I pay much more attention to whether there is enough information to replicate the experiment or simulation. We can put out a paper with wrong interpretations and that’s fine because other people will realise that when doing their own work. We cannot let papers get published if their results cannot be replicated.<p>&gt; The latter would be the result of the former. Instead of having pages long &quot;appendix&quot; section in the main paper, you produce another document with meticulous details of the experiment&#x2F;methodology with every stone turned together with an peer reviewer. Stamp it with the peer reviewes name so they can&#x27;t get away with hand wavy review<p>Things that take too much space to go in the experimental section should go to a electronic supplementary information document. But then it would be nice if the ESI were appended to the article when we download a PDF because tracking them is a pain in the backside. Some fields are better than others about this, for example in materials characterisation studies it’s very common to have ESI with a whole bunch of data and details.<p>Large dataset should go to a repository or a dataset journal, that way the method is still peer reviewed and the dataset has a doi and is much easier to re-use. It’s also a nice way of doubling a student’s papers count by the end of their PhD.<p>&gt; Another atrocious thing I&#x27;ve witnessed is a paper claiming they evaluated their method on a benchmark and if you check the benchmark, the task they evaluated on doesn&#x27;t exit! They forked the benchmark and made their own task without being clear about it! [2]<p>That’s just evil!</div><br/><div id="37025866" class="c"><input type="checkbox" id="c-37025866" checked=""/><div class="controls bullet"><span class="by">Maxion</span><span>|</span><a href="#37022291">root</a><span>|</span><a href="#37022978">parent</a><span>|</span><a href="#37024102">next</a><span>|</span><label class="collapse" for="c-37025866">[-]</label><label class="expand" for="c-37025866">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Large dataset should go to a repository or a dataset journal, that way the method is still peer reviewed and the dataset has a doi and is much easier to re-use.<p>This may be possible in some sciences, but not in epidemiology or biomed. Often the study is based on tissue samples owned by some entity, with permission granted only to some certain entity.<p>Datasets in epidemiology are often full of PII, and cannot be shared publicly for many reasons.</div><br/></div></div></div></div></div></div><div id="37024102" class="c"><input type="checkbox" id="c-37024102" checked=""/><div class="controls bullet"><span class="by">waynecochran</span><span>|</span><a href="#37022291">prev</a><span>|</span><a href="#37024614">next</a><span>|</span><label class="collapse" for="c-37024102">[-]</label><label class="expand" for="c-37024102">[4 more]</label></div><br/><div class="children"><div class="content">I spent a lot of my graduate years in CS implementing the details of papers only to learn that, time and time again, the paper failed to mention all the short comings and fail cases of the techniques. There are great exceptions to this.<p>Due to the pressure of &quot;publish or die&quot; there is very little honesty in research. Fortunately there are some who are transparent with their work. But for the most part, science is drowning in a sea of research that lacks transparency and replication short falls.</div><br/><div id="37025084" class="c"><input type="checkbox" id="c-37025084" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#37024102">parent</a><span>|</span><a href="#37024695">next</a><span>|</span><label class="collapse" for="c-37025084">[-]</label><label class="expand" for="c-37025084">[2 more]</label></div><br/><div class="children"><div class="content">I had a very similar experience in my masters. Really made me think, what exactly are the peers “reviewing” if they don’t even know whether the technique works in the first place.</div><br/><div id="37025978" class="c"><input type="checkbox" id="c-37025978" checked=""/><div class="controls bullet"><span class="by">waynecochran</span><span>|</span><a href="#37024102">root</a><span>|</span><a href="#37025084">parent</a><span>|</span><a href="#37024695">next</a><span>|</span><label class="collapse" for="c-37025978">[-]</label><label class="expand" for="c-37025978">[1 more]</label></div><br/><div class="children"><div class="content">I have reviewed many papers and there is never the time to recreate the work and test. That is why I love the &quot;papers w code&quot; site. I think every published CS paper should require a git repo with all their code and experimental data.</div><br/></div></div></div></div><div id="37024695" class="c"><input type="checkbox" id="c-37024695" checked=""/><div class="controls bullet"><span class="by">cptskippy</span><span>|</span><a href="#37024102">parent</a><span>|</span><a href="#37025084">prev</a><span>|</span><a href="#37024614">next</a><span>|</span><label class="collapse" for="c-37024695">[-]</label><label class="expand" for="c-37024695">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ll quickly discover when you enter the workforce that the reasons we have CI&#x2F;CD, Docker, and virtualization are because of a similar problem. The dread &quot;it works on my machine&quot; response.<p>CI&#x2F;CD forces people to codify exactly how to build and deploy something in order for it to get into a production environment.  Docker and VMs are ways around this by giving people a &quot;my machine&quot; that can be copied and shared easily.</div><br/></div></div></div></div><div id="37024614" class="c"><input type="checkbox" id="c-37024614" checked=""/><div class="controls bullet"><span class="by">titzer</span><span>|</span><a href="#37024102">prev</a><span>|</span><a href="#37025707">next</a><span>|</span><label class="collapse" for="c-37024614">[-]</label><label class="expand" for="c-37024614">[5 more]</label></div><br/><div class="children"><div class="content">In the PL field, conferences have started to allow authors to submit packaged artifacts (typically, source code, input data, training data, etc) that are evaluated separately, typically post-review. The artifacts are evaluated by a separate committee, usually graduate students. As usual, everything is volunteer. Even with explicit instructions, it is hard enough to even get the same <i>code</i> to run in a different environment and give the same results. Would &quot;replication&quot; of a software technique require another team to reimplement something from scratch? That seems unworkable.<p>I can&#x27;t even <i>imagine</i> how hard it would be to write instructions for another lab to successfully replicate an experiment at the forefront of physics or chemistry, or biology. Not just the specialized equipment, but we&#x27;re talking about the frontiers of Science with people doing cutting-edge research.<p>I get the impression that suggestions like these are written by non-scientists who do not have experience with the peer review process of <i>any</i> discipline. Things just don&#x27;t work like that.</div><br/><div id="37027326" class="c"><input type="checkbox" id="c-37027326" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#37024614">parent</a><span>|</span><a href="#37025898">next</a><span>|</span><label class="collapse" for="c-37027326">[-]</label><label class="expand" for="c-37027326">[1 more]</label></div><br/><div class="children"><div class="content">Is PL theory actually science? Although we call it computer science, I don&#x27;t personally think CS is actually a science in the sense of studying nature to understand it. Computers are artificial constructs. CS is a lot closer to engineering than science. Indeed it&#x27;s kind of nonsensical to talk about replicating an experiment in programming language theory.<p>For the &quot;hard&quot; sciences, replication often isn&#x27;t so difficult it seems. LK-99 being an interesting study in this, where people are apparently successfully replicating an experiment described in a rushed paper that is widely agreed to lack sufficient details. It&#x27;s cutting edge science but replication still isn&#x27;t a problem. Most science isn&#x27;t the LHC.<p>The real problems with replication are found in the softer fields. There it&#x27;s not just an issue of randomness or difficulty of doing the experiments. If that&#x27;s all there was to it, no problem. In these fields it&#x27;s common to find papers or entire fields where none of the work is replicable even in principle. As in, the people doing it don&#x27;t think other people being able to replicate their work is even important at all, and they may go out of their way to <i>stop</i> people being able to replicate their work (most frequently by gathering data in non-replicable ways and then withholding it deliberately, but sometimes it&#x27;s just due to the design of the study). The most obvious inference when you see this is that maybe they don&#x27;t want replication attempts because they know their claims probably aren&#x27;t true.<p>So even if peer reviewers or journals were just checking really basic things like, is this claim even replicable in principle, that would be a good start. You would still be left with a lot of papers that replicate fine but their conclusions are still wrong because their methodology is illogical, or papers that replicate because their findings are obvious. But there&#x27;s so much low hanging fruit.</div><br/></div></div><div id="37025898" class="c"><input type="checkbox" id="c-37025898" checked=""/><div class="controls bullet"><span class="by">Maxion</span><span>|</span><a href="#37024614">parent</a><span>|</span><a href="#37027326">prev</a><span>|</span><a href="#37027936">next</a><span>|</span><label class="collapse" for="c-37025898">[-]</label><label class="expand" for="c-37025898">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I get the impression that suggestions like these are written by non-scientists who do not have experience with the peer review process of any discipline. Things just don&#x27;t work like that.<p>Not to mention that the cutting edge in many sciences are perhaps two-three research groups of 5-30 individuals each in varying research institutions around the world.</div><br/></div></div><div id="37027936" class="c"><input type="checkbox" id="c-37027936" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37024614">parent</a><span>|</span><a href="#37025898">prev</a><span>|</span><a href="#37025707">next</a><span>|</span><label class="collapse" for="c-37027936">[-]</label><label class="expand" for="c-37027936">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Even with explicit instructions, it is hard enough to even get the same code to run in a different environment and give the same results.<p>Is it really that hard for researchers to standardize around providing Dockerfiles? Environment replication is a solved problem.</div><br/><div id="37029733" class="c"><input type="checkbox" id="c-37029733" checked=""/><div class="controls bullet"><span class="by">titzer</span><span>|</span><a href="#37024614">root</a><span>|</span><a href="#37027936">parent</a><span>|</span><a href="#37025707">next</a><span>|</span><label class="collapse" for="c-37029733">[-]</label><label class="expand" for="c-37029733">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Environment replication is a solved problem.<p>Unfortunately, no. Dockerfiles aren&#x27;t as portable as you think, and not architecture-independent. VMs are better, but even then, performance isn&#x27;t portable either.<p>The last artifact I produced included builds of 3 web browsers from source--it was over 10GB. One doesn&#x27;t just &quot;build Chrome in a dockerfile&quot;.</div><br/></div></div></div></div></div></div><div id="37025707" class="c"><input type="checkbox" id="c-37025707" checked=""/><div class="controls bullet"><span class="by">hedora</span><span>|</span><a href="#37024614">prev</a><span>|</span><a href="#37026087">next</a><span>|</span><label class="collapse" for="c-37025707">[-]</label><label class="expand" for="c-37025707">[2 more]</label></div><br/><div class="children"><div class="content">The website dies if I try to figure out who the author (“sam”) is, but it sounds like they are used to some awful backwater of academia.<p>They have this idea that a single editor screens papers to decide if they are uninteresting or fundamentally flawed, then they want a bunch of professors to do grunt work litigating the correctness of the experiments.<p>In modern (post industrial revolution) branches of science, the work of determining what is worthy of publication is distributed amongst a program committee, which is comprised of reviewers.   The editor &#x2F; conference organizers pick the program committee.  There are typically dozens of program committee members, and authors and reviewers both disclose conflicts.  Also, papers are anonymized, so the people that see the author list are not involved in accept&#x2F;reject decisions.<p>This mostly eliminates the problem where work is suppressed for political reasons, etc.<p>It is increasingly common for paper PDFs to be annotated with badges showing the level of reproducibility of the work, and papers can win awards for being highly reproducible.  The people that check reproducibility simply execute directions from a separate reproducibility submission that is produced after the paper is accepted.<p>I argue the above approach is about 100 years ahead of what the blog post is suggesting.<p>Ideally, we would tie federal funding to double blind review and venues with program committees, and papers selected by editors would not count toward tenure at universities that receive public funding.</div><br/><div id="37026318" class="c"><input type="checkbox" id="c-37026318" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#37025707">parent</a><span>|</span><a href="#37026087">next</a><span>|</span><label class="collapse" for="c-37026318">[-]</label><label class="expand" for="c-37026318">[1 more]</label></div><br/><div class="children"><div class="content">The computer science practice you describe is the exception, not the norm. It causes a lot of trouble when evaluating the merits of researchers, because most people in the academia are not familiar with it. In many places, conference papers don&#x27;t even count as real publications, putting CS researchers at a disadvantage.<p>From my point of view, the biggest issue is accepting&#x2F;rejecting papers based on first impressions. Because there is often only one round of reviews, you can&#x27;t ask the authors for clarifications, and they can&#x27;t try to fix the issues you have identified. Conferences tend to follow fashionable topics, and they are often narrower in scope than what they claim to be, because it&#x27;s easier to evaluate papers on topics the program committee is familiar with.<p>The work done by the program committee was not even supposed to be proper peer review but only the first filter. Old conference papers often call themselves extended abstracts, and they don&#x27;t contain all the details you would expect in the full paper. For example, a theoretical paper may omit key proofs. Once the program committee has determined that the results look interesting and plausible and the authors have presented them in a conference, the authors are supposed to write the full paper and submit it to a journal for peer review. Of course, this doesn&#x27;t always happen, for a number of reasons.</div><br/></div></div></div></div><div id="37026087" class="c"><input type="checkbox" id="c-37026087" checked=""/><div class="controls bullet"><span class="by">fastneutron</span><span>|</span><a href="#37025707">prev</a><span>|</span><a href="#37022182">next</a><span>|</span><label class="collapse" for="c-37026087">[-]</label><label class="expand" for="c-37026087">[1 more]</label></div><br/><div class="children"><div class="content">As much as I agree with the sentiment, we have to admit it isn&#x27;t always practical. There&#x27;s only one LIGO, LHC or JWST, for example. Similarly, not every lab has the resources or know-how to host multi-TB datasets for the general public to pick through, even if they wanted to. I sure didn&#x27;t when I was a grad student.<p>That said, it infuriates me to no end when I read a Phys. Rev. paper that consists of a computational study of a particular physical system, and the only replicability information provided is the governing equation and a vague description of the numerical technique. No discretized example, no algorithm, and sure as hell no code repository. I&#x27;m sure other fields have this too. The only motivation I see for this behavior is the desire for a monopoly on the research topic on the part of authors, or embarrassment by poor code quality (real or perceived).</div><br/></div></div><div id="37022182" class="c"><input type="checkbox" id="c-37022182" checked=""/><div class="controls bullet"><span class="by">leedrake5</span><span>|</span><a href="#37026087">prev</a><span>|</span><a href="#37030408">next</a><span>|</span><label class="collapse" for="c-37022182">[-]</label><label class="expand" for="c-37022182">[2 more]</label></div><br/><div class="children"><div class="content">Peer Review is the right solution to the wrong problem: <a href="https:&#x2F;&#x2F;open.substack.com&#x2F;pub&#x2F;experimentalhistory&#x2F;p&#x2F;science-is-a-strong-link-problem" rel="nofollow noreferrer">https:&#x2F;&#x2F;open.substack.com&#x2F;pub&#x2F;experimentalhistory&#x2F;p&#x2F;science-...</a><p>On replication, it is a worthwhile goal but the career incentives need to be there. I think replicating studies should be a part of the curriculum in most programs - a step toward getting a PhD in lieu of one of the papers.</div><br/><div id="37023823" class="c"><input type="checkbox" id="c-37023823" checked=""/><div class="controls bullet"><span class="by">vinnyvichy</span><span>|</span><a href="#37022182">parent</a><span>|</span><a href="#37030408">next</a><span>|</span><label class="collapse" for="c-37023823">[-]</label><label class="expand" for="c-37023823">[1 more]</label></div><br/><div class="children"><div class="content">Fear of the frontier.. that&#x27;s why instead of people getting excited to look for new rtsp superconductor candidates, we get a lot of talk downplaying the only known one. Strong link vs weak link reminds me of how some cultures frown on stimulants while other cultures frown on relaxants.</div><br/></div></div></div></div><div id="37030408" class="c"><input type="checkbox" id="c-37030408" checked=""/><div class="controls bullet"><span class="by">okaleniuk</span><span>|</span><a href="#37022182">prev</a><span>|</span><a href="#37031730">next</a><span>|</span><label class="collapse" for="c-37030408">[-]</label><label class="expand" for="c-37030408">[2 more]</label></div><br/><div class="children"><div class="content">Back when I was active in academia, our publishers were reluctant to print source code or even repository links (that was largely before GitHub) but they could still share a paper source on demand. If you reference someone else&#x27;s paper and want to quote some formula, it is easier and less error prone to copy rather than retype.<p>At that point I thought about making a TeX interpreter so one could easily &quot;run a paper&quot; on their own data to see if the papers claims hold. As it turned out, people often write the same formula in multiple ways and to make a TeX interpreter you&#x27;d have to specify a &quot;runnable&quot; subset and convince anyone to use that subset instead of what they got used to. So the idea stalled.<p>In a few years, publishing a GitHub link along the paper became the norm, and the problem disappeared. At least in applied geometry, people do replicate each other results all the time.</div><br/><div id="37030587" class="c"><input type="checkbox" id="c-37030587" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#37030408">parent</a><span>|</span><a href="#37031730">next</a><span>|</span><label class="collapse" for="c-37030587">[-]</label><label class="expand" for="c-37030587">[1 more]</label></div><br/><div class="children"><div class="content">“Running a paper” is honestly challenging these days because of the resource requirements of a lot of scientific code, or the size of certain datasets. One group might have access to a beefy cluster and there’s no pressure for very performant code when they can parallelize the work across a few dozen xeons or have access to tbs of memory. Another group might be running their code on a laptop. Maybe if your data is much larger than the authors data, their code doesn’t even work since it was designed for much smaller datasets.<p>Tools like nextflow or snakemake help with respect to having a one liner to generate all data in a paper potentially, handle dependencies, list resource expectations, use your own profile to handle your environment specific job scheduling commands and parameters. However, this still doesn’t do anything for whether you have access to the resources needed.</div><br/></div></div></div></div><div id="37031730" class="c"><input type="checkbox" id="c-37031730" checked=""/><div class="controls bullet"><span class="by">wcerfgba</span><span>|</span><a href="#37030408">prev</a><span>|</span><a href="#37029329">next</a><span>|</span><label class="collapse" for="c-37031730">[-]</label><label class="expand" for="c-37031730">[1 more]</label></div><br/><div class="children"><div class="content">What do we recommend for qualitative research, where replicability is not a quality criterion?</div><br/></div></div><div id="37029329" class="c"><input type="checkbox" id="c-37029329" checked=""/><div class="controls bullet"><span class="by">jxramos</span><span>|</span><a href="#37031730">prev</a><span>|</span><a href="#37026177">next</a><span>|</span><label class="collapse" for="c-37029329">[-]</label><label class="expand" for="c-37029329">[1 more]</label></div><br/><div class="children"><div class="content">You know what I would love to see is metadata attributes surrounding a paper such as [retracted], [reproduced], [rejected], etc. We already have the preprint thing down. Some of these would be implied by being published, ie not a preprint. Maybe even a quick symbol for what method of proof was relied upon—-video evidence, randomized control trial, observational study, Sample count of n&gt;1000 (predefined inequality brackets), etc. I think having this quick digest of information would help an individual wade through a lot of studies quickly.</div><br/></div></div><div id="37026177" class="c"><input type="checkbox" id="c-37026177" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37029329">prev</a><span>|</span><a href="#37028015">next</a><span>|</span><label class="collapse" for="c-37026177">[-]</label><label class="expand" for="c-37026177">[2 more]</label></div><br/><div class="children"><div class="content">Reproducibility would become a much higher priority if electronic versions of papers are required (by their distributors, archives, institutions, ...) to have reproduction sections, which the authors are encouraged to update over time.<p>UPDATABLE COVER PAGE:<p>Title
Authors<p>Abstract<p><pre><code>   Blah, blah, ...
</code></pre>
State of reproduction:<p><pre><code>    Not reproduced.
    Successful reproductions: ...citations...
    Reproduction attempts: ...citations...
    Countering reproductions: ...citations...
</code></pre>
UPDATABLE REPRODUCTION SECTION ATTACHED AT END<p>Reproduction resources:<p><pre><code>   Data, algorithms, processes, materials, ...
</code></pre>
Reproduction challenges:<p><pre><code>   Cost, time, one-off events, ...
</code></pre>
Making this stuff more visible would help reproducers validated the value of reproduction to their home and funding institutions.<p>Having a standard section for this, with an initial state of &quot;Not reproduced&quot; provides more incentive for original workers to provide better reproduction info.<p>For algorithm and math work the reproduction could be served best with downloadable executable bundle.</div><br/><div id="37029785" class="c"><input type="checkbox" id="c-37029785" checked=""/><div class="controls bullet"><span class="by">andromaton</span><span>|</span><a href="#37026177">parent</a><span>|</span><a href="#37028015">next</a><span>|</span><label class="collapse" for="c-37029785">[-]</label><label class="expand" for="c-37029785">[1 more]</label></div><br/><div class="children"><div class="content">I think Tim Berners-Lee would approve.</div><br/></div></div></div></div><div id="37028015" class="c"><input type="checkbox" id="c-37028015" checked=""/><div class="controls bullet"><span class="by">jonnycomputer</span><span>|</span><a href="#37026177">prev</a><span>|</span><a href="#37021928">next</a><span>|</span><label class="collapse" for="c-37028015">[-]</label><label class="expand" for="c-37028015">[6 more]</label></div><br/><div class="children"><div class="content">If they are, in fact, implying that another lab should produce a matching data-set to try to replicate results, well, I&#x27;m sorry, but that won&#x27;t work, at least in a whole lot of fields. Data collection can be very expensive, and take a lot of time. It certainly is in my field.<p>If on, the other hand, they just want the raw data, and let others go to town on it in their own way, that&#x27;s fine, probably. Results that don&#x27;t depend on very particular details of the processing pipeline are probably more robust anyway.</div><br/><div id="37028039" class="c"><input type="checkbox" id="c-37028039" checked=""/><div class="controls bullet"><span class="by">peteradio</span><span>|</span><a href="#37028015">parent</a><span>|</span><a href="#37021928">next</a><span>|</span><label class="collapse" for="c-37028039">[-]</label><label class="expand" for="c-37028039">[5 more]</label></div><br/><div class="children"><div class="content">What field is it too expensive or difficult to reproduce the data?</div><br/><div id="37029881" class="c"><input type="checkbox" id="c-37029881" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#37028015">root</a><span>|</span><a href="#37028039">parent</a><span>|</span><a href="#37029123">next</a><span>|</span><label class="collapse" for="c-37029881">[-]</label><label class="expand" for="c-37029881">[1 more]</label></div><br/><div class="children"><div class="content">On the top of my head: Say you want to research biological data from the Mariana trench.<p>Or even worse: Some passing by comet, or planet&#x2F;moon&#x2F;whatever in the solar system. And just to make things EVEN worse, you need to analyze the data in some destructive way.<p>Certainly very plausible scenarios, but also some which could prohibitively expensive to do multiple times.</div><br/></div></div><div id="37029123" class="c"><input type="checkbox" id="c-37029123" checked=""/><div class="controls bullet"><span class="by">jonnycomputer</span><span>|</span><a href="#37028015">root</a><span>|</span><a href="#37028039">parent</a><span>|</span><a href="#37029881">prev</a><span>|</span><a href="#37028167">next</a><span>|</span><label class="collapse" for="c-37029123">[-]</label><label class="expand" for="c-37029123">[1 more]</label></div><br/><div class="children"><div class="content">Lots?<p>Human subjects research, for one e.g. very often they involve clinical populations that are very hard to recruit. You can spend tens of thousands in advertising, and multiples more in labor, to get a hundred participants in, over the course of an entire year of effort, and that&#x27;s not even counting the money spent on a clinician doing a diagnosis. And then, when you do, you may, say, pay $1000 for the MRI per subject, plus the $100 bucks you pay directly to the participant themselves.</div><br/></div></div><div id="37028167" class="c"><input type="checkbox" id="c-37028167" checked=""/><div class="controls bullet"><span class="by">542458</span><span>|</span><a href="#37028015">root</a><span>|</span><a href="#37028039">parent</a><span>|</span><a href="#37029123">prev</a><span>|</span><a href="#37029159">next</a><span>|</span><label class="collapse" for="c-37028167">[-]</label><label class="expand" for="c-37028167">[1 more]</label></div><br/><div class="children"><div class="content">As reviewers are paid nothing and get no substantial credit for their work, I’m going to say “Every field”. Why would you do a significant chunk of a paper’s work for no reward? Replication studies are typically a bad deal even when you get to pick a notable study to reproduce and you get a paper to your name out of it - replicating what will probably be an obscure paper for no credit is not something most academics (let alone commercial research labs) would entertain.</div><br/></div></div></div></div></div></div><div id="37021928" class="c"><input type="checkbox" id="c-37021928" checked=""/><div class="controls bullet"><span class="by">nomilk</span><span>|</span><a href="#37028015">prev</a><span>|</span><a href="#37024567">next</a><span>|</span><label class="collapse" for="c-37021928">[-]</label><label class="expand" for="c-37021928">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230130143126&#x2F;https:&#x2F;&#x2F;blog.everydayscientist.com&#x2F;replace-peer-review-with-peer-replication&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230130143126&#x2F;https:&#x2F;&#x2F;blog.ever...</a></div><br/><div id="37022369" class="c"><input type="checkbox" id="c-37022369" checked=""/><div class="controls bullet"><span class="by">the_arun</span><span>|</span><a href="#37021928">parent</a><span>|</span><a href="#37024567">next</a><span>|</span><label class="collapse" for="c-37022369">[-]</label><label class="expand" for="c-37022369">[1 more]</label></div><br/><div class="children"><div class="content">Thank you. Currently the original article is throttled.<p>Seems like article is not about software code.</div><br/></div></div></div></div><div id="37024567" class="c"><input type="checkbox" id="c-37024567" checked=""/><div class="controls bullet"><span class="by">jimmar</span><span>|</span><a href="#37021928">prev</a><span>|</span><a href="#37027246">next</a><span>|</span><label class="collapse" for="c-37024567">[-]</label><label class="expand" for="c-37024567">[2 more]</label></div><br/><div class="children"><div class="content">How do you replicate a literature review? Theoretical physics? A neuro case? Research that relies upon natural experiments? There are many types of research. Not all of them lend themselves to replication, but they can still contribute to our body of knowledge. Peer review is helpful in each of these instances.<p>Science is a process. Peer review isn&#x27;t perfect. Replication is important. But it doesn&#x27;t seem like the author understands what it would take to simply replace peer review with replication.</div><br/><div id="37025122" class="c"><input type="checkbox" id="c-37025122" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#37024567">parent</a><span>|</span><a href="#37027246">next</a><span>|</span><label class="collapse" for="c-37025122">[-]</label><label class="expand" for="c-37025122">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think the existence of papers that are difficult to replicate undermines the value of replicating those that are easier.</div><br/></div></div></div></div><div id="37027246" class="c"><input type="checkbox" id="c-37027246" checked=""/><div class="controls bullet"><span class="by">cycomanic</span><span>|</span><a href="#37024567">prev</a><span>|</span><a href="#37025303">next</a><span>|</span><label class="collapse" for="c-37027246">[-]</label><label class="expand" for="c-37027246">[1 more]</label></div><br/><div class="children"><div class="content">While I agree with the general sentiment of the paper and creating incentives for more replication is definitely a good idea, I do think the approach is flawed in several ways.<p>The main point is that the paper seriously underestimates the difficulty and time it requires to replicate experiments in many experimental fields. Who will decide which work needs to be replicated? Should capable labs somehow become bogged down with just doing replication work? Even if they don&#x27;t find the results not interesting?<p>In reality if labs find results interesting enough to replicate they will try to do so. The current LK-99 hurrah is a perfect example of that, but it happens on a much smaller scale all the time. Researchers do replicate and build on other work all the time, they just use that replication to create new results (and acknowledge the previous work) instead of publishing a &quot;we replicated paper&quot;.<p>Where things usually fail is in publication of &quot;failed replication&quot; studies, and those are tricky. It is not always clear if the original research was flawed or the people trying to reproduce made an error (again just have a look at what&#x27;s happening with LK-99 at the moment). Moreover, it can be politically difficult to try to publish a &quot;fail to reproduce&quot; result if you are small unknown lab, if the original result came from a big known group. Most people will believe that you are the one who made the error (and unfortunately big egos might get in the way, and the small lab will have a hard time).<p>More generally, in my opinion the lack of replication of results is just one symptom of a bigger problem in science today. We (as in society) have essentially turned the scientific environment increasingly competitive, under the guise of &quot;value for tax payer money&quot;. Academic scientists now have to constantly compete for grant funding, publish to keep the funding going. It&#x27;s incredibly competitive to even get in ... At the same time they are supposed to constantly provide big headlines for university press releases, communicate their results to the general public and investigate (and patent) the potential for commercial exploitation. No wonder we see less cooperation.</div><br/></div></div><div id="37025303" class="c"><input type="checkbox" id="c-37025303" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#37027246">prev</a><span>|</span><a href="#37027262">next</a><span>|</span><label class="collapse" for="c-37025303">[-]</label><label class="expand" for="c-37025303">[1 more]</label></div><br/><div class="children"><div class="content">Both review and replication has their place. The mistake is treating researchers and the scientific community as a machine: &quot;pull here, fill these forms, comment this research, have a gold star&quot;<p>Let people review what they want, where they want, how they want. Let people replicate when they find interesting and motivating to work on.</div><br/></div></div><div id="37027262" class="c"><input type="checkbox" id="c-37027262" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#37025303">prev</a><span>|</span><a href="#37025910">next</a><span>|</span><label class="collapse" for="c-37027262">[-]</label><label class="expand" for="c-37027262">[1 more]</label></div><br/><div class="children"><div class="content">How about we create a Nobel prize for replication.  One impressive replication or refutation from last decade (that holds up) gets the prize split up to three ways among the most important authors.</div><br/></div></div><div id="37025910" class="c"><input type="checkbox" id="c-37025910" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#37027262">prev</a><span>|</span><a href="#37027432">next</a><span>|</span><label class="collapse" for="c-37025910">[-]</label><label class="expand" for="c-37025910">[1 more]</label></div><br/><div class="children"><div class="content">Seems to have been hugged to death.<p>But - a quick counterexample - as far as replication goes: What if the experiments were run on custom made or exceedingly expensive equipment? How are the replicators supposed to access that equipment? Even in fields which are &quot;easy&quot; to replicate - like machine learning - we are seeing barriers of entry due to expensive computing power. Or data collection. Or both.<p>But then you move over to physics, and suddenly you&#x27;re also dealing with these one-off custom setups, doing experiments which could be close to impossible to replicate (say you want to conduct experiments on some physical event that only occurs every xxxx years or whatever)</div><br/></div></div><div id="37027432" class="c"><input type="checkbox" id="c-37027432" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#37025910">prev</a><span>|</span><a href="#37029837">next</a><span>|</span><label class="collapse" for="c-37027432">[-]</label><label class="expand" for="c-37027432">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s get people to publish their data and code first, shall we? That&#x27;s sooo much easier than demanding whole studies to be replicated... and people still don&#x27;t do it!</div><br/></div></div><div id="37029837" class="c"><input type="checkbox" id="c-37029837" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#37027432">prev</a><span>|</span><a href="#37027610">next</a><span>|</span><label class="collapse" for="c-37029837">[-]</label><label class="expand" for="c-37029837">[1 more]</label></div><br/><div class="children"><div class="content">Why would you bother replicating someone else’s work (thereby validating it), when you could use that time and resources to do something novel?</div><br/></div></div><div id="37027610" class="c"><input type="checkbox" id="c-37027610" checked=""/><div class="controls bullet"><span class="by">SonOfLilit</span><span>|</span><a href="#37029837">prev</a><span>|</span><a href="#37029692">next</a><span>|</span><label class="collapse" for="c-37027610">[-]</label><label class="expand" for="c-37027610">[1 more]</label></div><br/><div class="children"><div class="content">My first thought was &quot;this would never work, there is so much science being published and not enough resources to replicate it all&quot;.<p>Then I remembered that my main issue with modern academia is that everyone is incentivized to publish a huge amount of research that nobody cares about, and how I wish we would put much more work into each of much fewer research directions.</div><br/></div></div><div id="37029692" class="c"><input type="checkbox" id="c-37029692" checked=""/><div class="controls bullet"><span class="by">tonmoy</span><span>|</span><a href="#37027610">prev</a><span>|</span><a href="#37027693">next</a><span>|</span><label class="collapse" for="c-37029692">[-]</label><label class="expand" for="c-37029692">[1 more]</label></div><br/><div class="children"><div class="content">We just need a second LHC with double the number of particle physicists in the world to replicate observation of the Higgs Boson, no big deal</div><br/></div></div><div id="37027693" class="c"><input type="checkbox" id="c-37027693" checked=""/><div class="controls bullet"><span class="by">ayakang31415</span><span>|</span><a href="#37029692">prev</a><span>|</span><a href="#37028208">next</a><span>|</span><label class="collapse" for="c-37027693">[-]</label><label class="expand" for="c-37027693">[1 more]</label></div><br/><div class="children"><div class="content">One of the Nobel prizes in Physics was the discovery of Higgs Boson at LHC. It cost billions of dollars just to build the facility, and required hundreds of physicists working on it to just conduct the experiment. You can&#x27;t replicate this. Although I fully agree that replication must come first when it is reasonably doable.</div><br/></div></div><div id="37028208" class="c"><input type="checkbox" id="c-37028208" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#37027693">prev</a><span>|</span><a href="#37022437">next</a><span>|</span><label class="collapse" for="c-37028208">[-]</label><label class="expand" for="c-37028208">[1 more]</label></div><br/><div class="children"><div class="content">Why not just develop a standard &quot;replication instructions&quot; format that papers would need to adhere to? All methods, source code, ingredients, processes, etc are documented in a standard way.  This could help tease out a lot of bullshit just by reading this section.</div><br/></div></div><div id="37022437" class="c"><input type="checkbox" id="c-37022437" checked=""/><div class="controls bullet"><span class="by">jhart99</span><span>|</span><a href="#37028208">prev</a><span>|</span><a href="#37023123">next</a><span>|</span><label class="collapse" for="c-37022437">[-]</label><label class="expand" for="c-37022437">[1 more]</label></div><br/><div class="children"><div class="content">Replication in many fields comes with substantial costs. We are unlikely to see this strategy employed on many&#x2F;most papers.  I agree with other commenters that materials and methodology should be provided in sufficient detail so that others could replicate if desired.</div><br/></div></div><div id="37023123" class="c"><input type="checkbox" id="c-37023123" checked=""/><div class="controls bullet"><span class="by">gordian-not</span><span>|</span><a href="#37022437">prev</a><span>|</span><label class="collapse" for="c-37023123">[-]</label><label class="expand" for="c-37023123">[1 more]</label></div><br/><div class="children"><div class="content">The incentive should be to clear the way for tenure track<p>The junior faculty will clear the rotten apples at the top by finding flaws in their research and then will win the tenure that was lost in return<p>This will create a nice political atmosphere and improve science</div><br/></div></div></div></div></div></div></div></body></html>