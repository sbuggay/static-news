<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705654852974" as="style"/><link rel="stylesheet" href="styles.css?v=1705654852974"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Launch HN: Talc AI (YC S23) – Test Sets for AI</a> </div><div class="subtext"><span>maxrmk</span> | <span>45 comments</span></div><br/><div><div id="39053125" class="c"><input type="checkbox" id="c-39053125" checked=""/><div class="controls bullet"><span class="by">maho</span><span>|</span><a href="#39048043">next</a><span>|</span><label class="collapse" for="c-39053125">[-]</label><label class="expand" for="c-39053125">[1 more]</label></div><br/><div class="children"><div class="content">Is there a way I can give feedback on wrong labels? The easy questions seem to be correct most (all?) of the time, but I noticed a few errors in the labelling of the complex question&#x2F;answers. I would love to see this improve even further!</div><br/></div></div><div id="39048043" class="c"><input type="checkbox" id="c-39048043" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#39053125">prev</a><span>|</span><a href="#39046882">next</a><span>|</span><label class="collapse" for="c-39048043">[-]</label><label class="expand" for="c-39048043">[4 more]</label></div><br/><div class="children"><div class="content">I tried the demo with Cal Ripken Jr. I was surprised by some of the complex questions:<p>&gt;Which MLB player won the Sporting News MLB Rookie of the Year Award as a pitcher in 1980, and who did Cal Ripken Jr. surpass to hold the record for most home runs hit as a shortstop?<p>&gt;What team did Britt Burns play for in the minor leagues before making his MLB debut, and in what year did Cal Ripken Jr. break the consecutive games played record?<p>&gt;Who was the minor league pitching coordinator for the Houston Astros until 2010, and what significant baseball record did Cal Ripken Jr. break in 1995?<p>All five questions are a combination of a question about a Britt Burns fact and an unrelated Cal Ripken fact.<p>Why is this? Britt Burns doesn&#x27;t seem to appear on the live Wikipedia page for Ripken. Does he appear on a cached version? Or is it forming complex questions by finding another page in the same category as Ripken and pulling more facts?</div><br/><div id="39048156" class="c"><input type="checkbox" id="c-39048156" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39048043">parent</a><span>|</span><a href="#39046882">next</a><span>|</span><label class="collapse" for="c-39048156">[-]</label><label class="expand" for="c-39048156">[3 more]</label></div><br/><div class="children"><div class="content">I was worried people would run into this quirk in the demo. We have several &#x27;advanced&#x27; question generation strategies. You correctly guessed the one we&#x27;re using in the demo; forming complex questions by finding another page in the same category as Ripken and pulling more facts.<p>Normally we pull a ton of related topic and try to pick the best, but to keep the generation fast and cost effective in the demo I limited the number of related pages we pull. So sometimes (like this case) you get something barely related and end up with odd disjointed questions.</div><br/><div id="39048316" class="c"><input type="checkbox" id="c-39048316" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#39048043">root</a><span>|</span><a href="#39048156">parent</a><span>|</span><a href="#39046882">next</a><span>|</span><label class="collapse" for="c-39048316">[-]</label><label class="expand" for="c-39048316">[2 more]</label></div><br/><div class="children"><div class="content">Ah, that does make sense - especially for a category like a baseball player where it may take a lot of other pages to find one that&#x27;s truly related. Would be expensive for a demo, but not a big deal for a real evaluation.</div><br/><div id="39048425" class="c"><input type="checkbox" id="c-39048425" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39048043">root</a><span>|</span><a href="#39048316">parent</a><span>|</span><a href="#39046882">next</a><span>|</span><label class="collapse" for="c-39048425">[-]</label><label class="expand" for="c-39048425">[1 more]</label></div><br/><div class="children"><div class="content">Yep, in real use cases the latency for generating questions doesn&#x27;t really matter. But in the demo I was really worried about it.</div><br/></div></div></div></div></div></div></div></div><div id="39046882" class="c"><input type="checkbox" id="c-39046882" checked=""/><div class="controls bullet"><span class="by">koeng</span><span>|</span><a href="#39048043">prev</a><span>|</span><a href="#39050278">next</a><span>|</span><label class="collapse" for="c-39046882">[-]</label><label class="expand" for="c-39046882">[2 more]</label></div><br/><div class="children"><div class="content">I love your demo for this. It&#x27;s one of the best demos I&#x27;ve ever come across in a launch HN. Very easy to understand and use. It seems to suffer with more complex questions though. For example:<p>Question: Why does the pUC19 plasmid have a high copy number in bacterial cells?<p>Expected answer: The pUC19 plasmid has a high copy number due to the lack of the rop gene and a single point mutation in the origin of replication (ori) derived from the plasmid pMB1.<p>GPT response: The pUC19 plasmid has a high copy number in bacterial cells due to the presence of the pUC origin of replication, which allows for efficient and rapid replication of the plasmid.<p>Both are technically correct - the expected answer is simply more detailed about the pUC origin, but both would be considered correct. It seems difficult to test things like this, but maybe that&#x27;s just not possible to really get correct.<p>I wonder how well things like FutureHouse&#x27;s wikicrow will work for summarizing knowledge better - <a href="https:&#x2F;&#x2F;www.futurehouse.org&#x2F;wikicrow" rel="nofollow">https:&#x2F;&#x2F;www.futurehouse.org&#x2F;wikicrow</a> - and how that could be benchmarked against Talc</div><br/><div id="39047086" class="c"><input type="checkbox" id="c-39047086" checked=""/><div class="controls bullet"><span class="by">matt_lee</span><span>|</span><a href="#39046882">parent</a><span>|</span><a href="#39050278">next</a><span>|</span><label class="collapse" for="c-39047086">[-]</label><label class="expand" for="c-39047086">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the kind words!<p>One of my regrets about the demo is that we paid a lot of attention to showing off our ability to generate high quality Q&#x2F;A pairs, but not nearly as much to showing what a thoughtful and thorough grading rubric can do.<p>It&#x27;s totally possible to do a high quality grading given a rubric that sets expectations! Great implementations we&#x27;ve seen use categories like correct &#x2F; correct but incomplete &#x2F; correct but unhelpful &#x2F; incorrect to better label the situation you describe. We&#x27;ve found that we can grade with much more nuance given a good rubric and categories, but unfortunately didn&#x27;t focus on that side of things in the demo<p>I&#x27;m not familiar with wikicrow, will check it out!</div><br/></div></div></div></div><div id="39050278" class="c"><input type="checkbox" id="c-39050278" checked=""/><div class="controls bullet"><span class="by">nicolewhite</span><span>|</span><a href="#39046882">prev</a><span>|</span><a href="#39044426">next</a><span>|</span><label class="collapse" for="c-39050278">[-]</label><label class="expand" for="c-39050278">[3 more]</label></div><br/><div class="children"><div class="content">Pretty neat!<p>I have a question about how you intend to deal with LLM applications where the output is more creative, e.g. an app where the user input is something like &quot;write me a story about X&quot; and the LLM app is using a higher temperature to get more creative responses. In these cases I don&#x27;t think it&#x27;s possible to represent the ideal output as a single string -- it would need to be a more complicated schema, like a list of constraints for the output, e.g. that it contains certain substrings.<p>TIA!</div><br/><div id="39052269" class="c"><input type="checkbox" id="c-39052269" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39050278">parent</a><span>|</span><a href="#39051802">next</a><span>|</span><label class="collapse" for="c-39052269">[-]</label><label class="expand" for="c-39052269">[1 more]</label></div><br/><div class="children"><div class="content">The TinyStories[1] paper has an interesting solution for how to evaluate stories.  They ask GPT-4 to grade them on grammar, consistency, and creativity.<p>This seems like it would be extremely hard to figure out how to do automatically though.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.07759.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.07759.pdf</a></div><br/></div></div><div id="39051802" class="c"><input type="checkbox" id="c-39051802" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39050278">parent</a><span>|</span><a href="#39052269">prev</a><span>|</span><a href="#39044426">next</a><span>|</span><label class="collapse" for="c-39051802">[-]</label><label class="expand" for="c-39051802">[1 more]</label></div><br/><div class="children"><div class="content">Good question! We aren&#x27;t really focusing on this area, but I&#x27;m willing to speculate.<p>I&#x27;d expect broaded constraints than just substring matching. For example, if the user requests that a certain plot point in the story occur before another, we should actually be able to (1) generate a test for that behavior and (2) use a model to check if the request was followed.<p>I&#x27;d expect other tests might be useful too -- checking for things like &quot;no generation of violent content, even if the user requests it&quot;.</div><br/></div></div></div></div><div id="39044426" class="c"><input type="checkbox" id="c-39044426" checked=""/><div class="controls bullet"><span class="by">typpo</span><span>|</span><a href="#39050278">prev</a><span>|</span><a href="#39043329">next</a><span>|</span><label class="collapse" for="c-39044426">[-]</label><label class="expand" for="c-39044426">[2 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch!<p>I&#x27;ve been interested in automatic testset generation because I find that the chore of writing tests is one of the reasons people shy away from evals.  Recently landed eval testset generation for promptfoo (<a href="https:&#x2F;&#x2F;github.com&#x2F;typpo&#x2F;promptfoo">https:&#x2F;&#x2F;github.com&#x2F;typpo&#x2F;promptfoo</a>), but it is non-RAG so more simplistic than your implementation.<p>Was also eyeballing this paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.03038" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.03038</a>, which outlines a method for generating asserts from prompt version history that may also be useful for these eval tools.</div><br/><div id="39044521" class="c"><input type="checkbox" id="c-39044521" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39044426">parent</a><span>|</span><a href="#39043329">next</a><span>|</span><label class="collapse" for="c-39044521">[-]</label><label class="expand" for="c-39044521">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I&#x27;ve been following promptfoo, so I&#x27;m glad to see you here. In addition to automatic evals I think every engineer and PM using LLMs should be looking at as many real responses as they can _every day_, and promptfoo is a great way to do that.</div><br/></div></div></div></div><div id="39043329" class="c"><input type="checkbox" id="c-39043329" checked=""/><div class="controls bullet"><span class="by">logiduck</span><span>|</span><a href="#39044426">prev</a><span>|</span><a href="#39043477">next</a><span>|</span><label class="collapse" for="c-39043329">[-]</label><label class="expand" for="c-39043329">[6 more]</label></div><br/><div class="children"><div class="content">For the chevy tahoe example, you are referencing the dealership, but in that case it wasn&#x27;t a case of the implementation failing to do a positive test for fact extraction, but to test the guardrails.<p>Aren&#x27;t the guardrail tests much harder since they are open-ended and have to guard against unknown prompt injections and the test of facts much simpler?<p>I think a test suite that guards against the infinite surface area is more valuable then testing if a question matches a reference answer.<p>Interested to how you view testing against giving a wrong answer outside of the predefined scope as opposed to testing that all the test questions match a reference.</div><br/><div id="39043500" class="c"><input type="checkbox" id="c-39043500" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39043329">parent</a><span>|</span><a href="#39043477">next</a><span>|</span><label class="collapse" for="c-39043500">[-]</label><label class="expand" for="c-39043500">[5 more]</label></div><br/><div class="children"><div class="content">Totally - certain types of failures are much harder to test than others.<p>We have a couple of different test generation strategies. As you can see in the demo and examples, the most basic one is &quot;ask about a fact&quot;.<p>Two of our other strategies are closer to what you&#x27;re asking for:<p>1. tests that try to deliberately induce hallucination by implying some fact that isn&#x27;t in the knowledge base. For example &quot;do I need a pilots license to activate the flight mode on the new chevy tahoe?&quot; implies the existence of a feature that doesn&#x27;t exist (yet). This was really hard to get right, and we have some coverage here but are still improving it.<p>2. actively malicious interactions that try to override facts in the knowledge base. These are easy to generate.</div><br/><div id="39044038" class="c"><input type="checkbox" id="c-39044038" checked=""/><div class="controls bullet"><span class="by">logiduck</span><span>|</span><a href="#39043329">root</a><span>|</span><a href="#39043500">parent</a><span>|</span><a href="#39043477">next</a><span>|</span><label class="collapse" for="c-39044038">[-]</label><label class="expand" for="c-39044038">[4 more]</label></div><br/><div class="children"><div class="content">Cool.<p>Just as some feedback I did the demo with the &quot;VW Beetle&quot; topic and one of the test cases was:<p>&gt; Question: How did the introduction of the Volkswagen Golf impact the production and sales of the Beetle?<p>&gt; Expected: The introduction of the Volkswagen Golf, a front-wheel drive hatchback, marked a shift in consumer preference towards more modern car designs. The Golf eventually became Volkswagen&#x27;s most successful model since the Beetle, leading to a decline in Beetle production and sales. Beetle production continued in smaller numbers at other German factories until it shifted to Brazil and Mexico, where low operating costs were more important.<p>&gt; GPT Response: The introduction of the Volkswagen Golf impacted the production and sales of the Beetle by gradually decreasing demand for the Beetle and shifting focus towards the Golf.<p>It seems that the GPT responses matches the expected but it was graded as incorrect. But it seems to me the GPT answer is correct.<p>In fact a couple of the other answers are marked incorrectly:<p>&gt; Question: What was the Volkswagen Beetle&#x27;s engine layout?
&gt; Expected Answer: Rear-engine, rear-wheel-drive layout
&gt; GPT Response: The Volkswagen Beetle had a rear-engine layout.<p>was marked as incorrect.</div><br/><div id="39044087" class="c"><input type="checkbox" id="c-39044087" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39043329">root</a><span>|</span><a href="#39044038">parent</a><span>|</span><a href="#39043477">next</a><span>|</span><label class="collapse" for="c-39044087">[-]</label><label class="expand" for="c-39044087">[3 more]</label></div><br/><div class="children"><div class="content">Will take a look, thanks!</div><br/><div id="39044247" class="c"><input type="checkbox" id="c-39044247" checked=""/><div class="controls bullet"><span class="by">logiduck</span><span>|</span><a href="#39043329">root</a><span>|</span><a href="#39044087">parent</a><span>|</span><a href="#39043477">next</a><span>|</span><label class="collapse" for="c-39044247">[-]</label><label class="expand" for="c-39044247">[2 more]</label></div><br/><div class="children"><div class="content">Also, just a random thing that I thought of playing around with it is a few days ago a guy posted about an AI quiz generator for education.<p>If you ever need to pivot, it seems like this would do reasonably well in the education space also.</div><br/><div id="39044845" class="c"><input type="checkbox" id="c-39044845" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39043329">root</a><span>|</span><a href="#39044247">parent</a><span>|</span><a href="#39043477">next</a><span>|</span><label class="collapse" for="c-39044845">[-]</label><label class="expand" for="c-39044845">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, someone is going to build this. We considered quizzing the user on the topic instead of chatgpt for our demo. It&#x27;s a lot of fun to test your knowledge on any topic, but it was a worse demo because it was way less related to our current product.<p>I think that one of the obvious next big spaces for LLMs is education. I already find chatgpt useful when learning myself. That being said, I&#x27;m terrified of trying to sell things to schools.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39043477" class="c"><input type="checkbox" id="c-39043477" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#39043329">prev</a><span>|</span><a href="#39045249">next</a><span>|</span><label class="collapse" for="c-39043477">[-]</label><label class="expand" for="c-39043477">[2 more]</label></div><br/><div class="children"><div class="content">The first thing that popped into my head is what do you do with the test results? Specifically, how do they feed back into model improvement in a way that avoids overfitting? Do you think having some kind of classical &quot;holdout&quot; question set is enough? Especially with RAG, I&#x27;d wonder with the levers that are available (prompt, chunking strategy, ...) if you define a bunch of test questions do you end up overfitting to them, or to the current data set. How can findings be extrapolated to new situations?</div><br/><div id="39043622" class="c"><input type="checkbox" id="c-39043622" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39043477">parent</a><span>|</span><a href="#39045249">next</a><span>|</span><label class="collapse" for="c-39043622">[-]</label><label class="expand" for="c-39043622">[1 more]</label></div><br/><div class="children"><div class="content">Ah! Think of this more like software testing that goes in CI&#x2F;CD rather than an ML test or validation set. We&#x27;re providing this testing for applications built on top of language models.<p>For example if you&#x27;re a SWE working on bing chat, you can make a change to how retrieval works and quickly know how it affected accuracy on a range of different test scenarios. This kind of evaluation is done by contractors today, and they are slow and inaccurate.</div><br/></div></div></div></div><div id="39045249" class="c"><input type="checkbox" id="c-39045249" checked=""/><div class="controls bullet"><span class="by">moinism</span><span>|</span><a href="#39043477">prev</a><span>|</span><a href="#39046434">next</a><span>|</span><label class="collapse" for="c-39045249">[-]</label><label class="expand" for="c-39045249">[2 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch! Just tried the demo and it looks impressive. Good luck.<p>Are you by any chance hiring global-remote, full-stack&#x2F;front-end devs? Would love to work with you guys.</div><br/><div id="39045397" class="c"><input type="checkbox" id="c-39045397" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39045249">parent</a><span>|</span><a href="#39046434">next</a><span>|</span><label class="collapse" for="c-39045397">[-]</label><label class="expand" for="c-39045397">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! We aren&#x27;t hiring right now, but if you shoot me an email at max@talc.ai I&#x27;ll follow up in a few months.</div><br/></div></div></div></div><div id="39046434" class="c"><input type="checkbox" id="c-39046434" checked=""/><div class="controls bullet"><span class="by">julesvr</span><span>|</span><a href="#39045249">prev</a><span>|</span><a href="#39048169">next</a><span>|</span><label class="collapse" for="c-39046434">[-]</label><label class="expand" for="c-39046434">[2 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch!<p>On your pricing model, as it&#x27;s usage based, don&#x27;t you incentivize your customers to use your product as little as possible? Wouldn&#x27;t it be better to have limited tiers with fixed annual&#x2F;monthly recurring rates? Also, do you sell to enterprise? I assume these would like this setup even more as the rates are predefined and they have a budget they have to deal with.<p>I&#x27;m currently developing my own pricing model and these are some issues I&#x27;m struggling with, so curious what you think.</div><br/><div id="39046924" class="c"><input type="checkbox" id="c-39046924" checked=""/><div class="controls bullet"><span class="by">matt_lee</span><span>|</span><a href="#39046434">parent</a><span>|</span><a href="#39048169">next</a><span>|</span><label class="collapse" for="c-39046924">[-]</label><label class="expand" for="c-39046924">[1 more]</label></div><br/><div class="children"><div class="content">Not a pricing expert by any means, but here were some of the considerations we thought of when we made that decision:<p>1. There&#x27;s plenty of usage based infrastructure&#x2F;dev tools (ie. AWS, databricks), so I don&#x27;t think we incentivize minimal usage.<p>2. The value we&#x27;re providing feels directly tied to how much testing we&#x27;re running, so when we tried to construct tiers they didn&#x27;t feel helpful.<p>3. In our experience, enterprises have been fine with usage based pricing. They&#x27;re already paying for human QA &#x2F; labelling on usage based terms (even if it&#x27;s part of a larger fixed contract), so our pricing isn&#x27;t a deviation for them.<p>Open to thoughts if anyone has them!</div><br/></div></div></div></div><div id="39048169" class="c"><input type="checkbox" id="c-39048169" checked=""/><div class="controls bullet"><span class="by">tommykins</span><span>|</span><a href="#39046434">prev</a><span>|</span><a href="#39048166">next</a><span>|</span><label class="collapse" for="c-39048169">[-]</label><label class="expand" for="c-39048169">[1 more]</label></div><br/><div class="children"><div class="content">As someone who uses Machine Learning to predict the presence of Talc I approve of this, even if I have no use case for it whatsoever.</div><br/></div></div><div id="39048166" class="c"><input type="checkbox" id="c-39048166" checked=""/><div class="controls bullet"><span class="by">bestai</span><span>|</span><a href="#39048169">prev</a><span>|</span><a href="#39046329">next</a><span>|</span><label class="collapse" for="c-39048166">[-]</label><label class="expand" for="c-39048166">[1 more]</label></div><br/><div class="children"><div class="content">I think for you idea to have traction (1) the questions should be selected by their importance and (2) the questions should be chained to allow new results.  Just for inspiration or example you could create a quiz for solving a puzzle and at the same time solving the puzzle by answering the questions. &gt;The big idea is using your tool to enhance step by step rationing in LLM.<p>I think you could use a text area for the user to indicate  if the quiz is about getting the main idea or if it is about testing the details.<p>And for big clients, the system could be tailored so that the questions and structure reflect user intentions.</div><br/></div></div><div id="39046329" class="c"><input type="checkbox" id="c-39046329" checked=""/><div class="controls bullet"><span class="by">pchunduri6</span><span>|</span><a href="#39048166">prev</a><span>|</span><a href="#39042853">next</a><span>|</span><label class="collapse" for="c-39046329">[-]</label><label class="expand" for="c-39046329">[2 more]</label></div><br/><div class="children"><div class="content">I just tried the demo, and it looks great! Congrats on the launch!<p>I have a couple of questions:<p>1) How often do you find that the LLM fails to generate the correct question-answer pairs? The biggest challenge I&#x27;m facing with LLM-based evaluation is the variability in LLM performance. I&#x27;ve found that the same prompt results in different LLM responses over multiple runs. Do you have any insights on this issue and how to address it?<p>2) Sometimes, the domain expert generating the test set might not be well-equipped to grade the answers. Consider a customer-facing chatbot application. The RAG app might be focused on very specific user information that might be hard to verify or attest by the test set creator. Do you think there are ways to make this grading process easier?</div><br/><div id="39046646" class="c"><input type="checkbox" id="c-39046646" checked=""/><div class="controls bullet"><span class="by">matt_lee</span><span>|</span><a href="#39046329">parent</a><span>|</span><a href="#39042853">next</a><span>|</span><label class="collapse" for="c-39046646">[-]</label><label class="expand" for="c-39046646">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Max&#x27;s cofounder chiming in here.<p>1) There&#x27;s an interesting subtlety in the phrase &quot;the correct question-answer pairs&quot;. While we don&#x27;t often find factually incorrect pairs because of how we&#x27;re running the pipeline, the bigger question is whether or not the pairs we generated are &quot;the&quot; correct ones -- if they are relevant and helpful. This takes some manual tweaking at the moment.<p>Inconsistent outputs over different runs are definitely an issue, but most teams we&#x27;ve worked with barely even have the CI&#x2F;CD practice to be able to measure that rigorously. As we mature we&#x27;ll aim to tackle flakiness of tests (and models) over time, but a bigger challenge has been getting regular tests like these set up in the first place.<p>2) In this scenario, we go to the documents powering a RAG application to both generate and grade answers. For example, the knowledge base might know that (1) product A is being recalled, and (2) customer #4 is asking for a warranty claim on product A. Using those two bits of information, we might generate a scenario that tests whether or not customer #4 gets the claim fulfilled. In other words, specific user information is simulated&#x2F;used during the test set creation.</div><br/></div></div></div></div><div id="39042853" class="c"><input type="checkbox" id="c-39042853" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#39046329">prev</a><span>|</span><a href="#39050513">next</a><span>|</span><label class="collapse" for="c-39042853">[-]</label><label class="expand" for="c-39042853">[2 more]</label></div><br/><div class="children"><div class="content">I like the chevy tahoe callback - I&#x27;m assuming that&#x27;s a reference to the chevy dealership that used an LLM and had people doing prompt tricks to get the chatbot to offer them a chevy tahoe for $1.<p>The specificity in your writing above &quot;to make this more concrete&quot; about how it works was also helpful for understanding the product.</div><br/><div id="39043041" class="c"><input type="checkbox" id="c-39043041" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39042853">parent</a><span>|</span><a href="#39050513">next</a><span>|</span><label class="collapse" for="c-39043041">[-]</label><label class="expand" for="c-39043041">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re exactly right about the chevy tahoe reference. I wasn&#x27;t sure if anyone would get it. I liked that post a lot, because as much as I think LLMs are going to be useful they have limitations that we haven&#x27;t solved yet.</div><br/></div></div></div></div><div id="39050513" class="c"><input type="checkbox" id="c-39050513" checked=""/><div class="controls bullet"><span class="by">ore0s</span><span>|</span><a href="#39042853">prev</a><span>|</span><a href="#39042812">next</a><span>|</span><label class="collapse" for="c-39050513">[-]</label><label class="expand" for="c-39050513">[2 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch! How does this compare to <a href="https:&#x2F;&#x2F;www.patronus.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.patronus.ai&#x2F;</a> ? They seem to offer a very similar solution for getting on top of unpredictable LLM output</div><br/><div id="39051882" class="c"><input type="checkbox" id="c-39051882" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39050513">parent</a><span>|</span><a href="#39042812">next</a><span>|</span><label class="collapse" for="c-39051882">[-]</label><label class="expand" for="c-39051882">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re in a similar space, and their work on FinanceBench is great for the whole community, so I appreciate that. Otherwise there&#x27;s not much out their about their product so I can&#x27;t directly compare.</div><br/></div></div></div></div><div id="39042812" class="c"><input type="checkbox" id="c-39042812" checked=""/><div class="controls bullet"><span class="by">sherlock_h</span><span>|</span><a href="#39050513">prev</a><span>|</span><a href="#39046524">next</a><span>|</span><label class="collapse" for="c-39042812">[-]</label><label class="expand" for="c-39042812">[2 more]</label></div><br/><div class="children"><div class="content">Looks interesting. How do you rate the correctness? Some complex LLM answers seemed to be correct but not in as much detail as the expected answer.<p>How do you generate the answers? Does the model have access to the original source of truth (like in RAG apps)?<p>And in your examples what model do you actually use?</div><br/><div id="39042957" class="c"><input type="checkbox" id="c-39042957" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39042812">parent</a><span>|</span><a href="#39046524">next</a><span>|</span><label class="collapse" for="c-39042957">[-]</label><label class="expand" for="c-39042957">[1 more]</label></div><br/><div class="children"><div class="content">Great questions here!<p>&gt; How do you rate the correctness? Some complex LLM answers seemed to be correct but not in as much detail as the expected answer.<p>We support two different modes: a strict pass&#x2F;fail where an answer has to have all of the information we expect, and a rubric based mode where answers are bucketed into things like &quot;partially correct&quot; or &quot;wrong but helpful&quot;.<p>To be honest, we also get the grading wrong sometimes. If you see anything egregious please email me the topic you used at max@talc.ai<p>&gt; How do you generate the answers? Does the model have access to the original source of truth (like in RAG apps)?<p>You guessed right here - we connect to the knowledge base like a RAG app. We also use this to generate the questions -- think of it like reading questions out of a textbook to quiz someong.<p>&gt; And in your examples what model do you actually use?<p>We use multiple models for the question generation, and are still evaluating what works best. For the demo, we are &quot;quizzing&quot; openai&#x27;s 3.5 turbo model.</div><br/></div></div></div></div><div id="39046524" class="c"><input type="checkbox" id="c-39046524" checked=""/><div class="controls bullet"><span class="by">Robotenomics</span><span>|</span><a href="#39042812">prev</a><span>|</span><a href="#39044136">next</a><span>|</span><label class="collapse" for="c-39046524">[-]</label><label class="expand" for="c-39046524">[2 more]</label></div><br/><div class="children"><div class="content">Very, very impressive.. I ran a couple of tests and on the complex it received 80% although I would say it was harsh as the answer could be said to be correct- although I found the questions generated rather simple not complex.<p>The 2nd test it was 100% incorrect for the complex questions! However when I checked directly with gpt-4 based upon the questions rendered it answered 100% correct. Could that be due to my custom settings in gpt4? Will run it with university students. Fascinating work</div><br/><div id="39046751" class="c"><input type="checkbox" id="c-39046751" checked=""/><div class="controls bullet"><span class="by">matt_lee</span><span>|</span><a href="#39046524">parent</a><span>|</span><a href="#39044136">next</a><span>|</span><label class="collapse" for="c-39046751">[-]</label><label class="expand" for="c-39046751">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for giving it a whirl!<p>I agree that the current grading is a bit harsh -- the rubric we&#x27;re using in this demo is fairly rudimentary. What we&#x27;ve seen be more helpful is a range of grades along the lines of correct &#x2F; correct but unhelpful &#x2F; correct but incomplete &#x2F; incorrect. This somewhat depends on individual use cases though.<p>Let me know what questions generated you thought could be more complex! We&#x27;re always working on improving our ability to explore the knowledge space for challenging questions.</div><br/></div></div></div></div><div id="39044136" class="c"><input type="checkbox" id="c-39044136" checked=""/><div class="controls bullet"><span class="by">dkindler</span><span>|</span><a href="#39046524">prev</a><span>|</span><a href="#39046117">next</a><span>|</span><label class="collapse" for="c-39044136">[-]</label><label class="expand" for="c-39044136">[2 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s an example where the GPT response was correct, but was marked as incorrect:
<a href="https:&#x2F;&#x2F;ibb.co&#x2F;tMGxcf3" rel="nofollow">https:&#x2F;&#x2F;ibb.co&#x2F;tMGxcf3</a></div><br/><div id="39044154" class="c"><input type="checkbox" id="c-39044154" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39044136">parent</a><span>|</span><a href="#39046117">next</a><span>|</span><label class="collapse" for="c-39044154">[-]</label><label class="expand" for="c-39044154">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for flagging!</div><br/></div></div></div></div><div id="39046117" class="c"><input type="checkbox" id="c-39046117" checked=""/><div class="controls bullet"><span class="by">quadcore</span><span>|</span><a href="#39044136">prev</a><span>|</span><a href="#39047392">next</a><span>|</span><label class="collapse" for="c-39046117">[-]</label><label class="expand" for="c-39046117">[3 more]</label></div><br/><div class="children"><div class="content">Now someone has to test talc AI. I can do it.<p>Impressive demo and business idea, congrats, good luck!</div><br/><div id="39046205" class="c"><input type="checkbox" id="c-39046205" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39046117">parent</a><span>|</span><a href="#39047392">next</a><span>|</span><label class="collapse" for="c-39046205">[-]</label><label class="expand" for="c-39046205">[2 more]</label></div><br/><div class="children"><div class="content">who tests the testing tool?<p>Thanks though -- and let us know if you hit any issues while playing around with the demo!</div><br/></div></div></div></div><div id="39047392" class="c"><input type="checkbox" id="c-39047392" checked=""/><div class="controls bullet"><span class="by">bestai</span><span>|</span><a href="#39046117">prev</a><span>|</span><a href="#39044884">next</a><span>|</span><label class="collapse" for="c-39047392">[-]</label><label class="expand" for="c-39047392">[2 more]</label></div><br/><div class="children"><div class="content">I think allowing other languages besides English  could be a good idea.</div><br/><div id="39047700" class="c"><input type="checkbox" id="c-39047700" checked=""/><div class="controls bullet"><span class="by">maxrmk</span><span>|</span><a href="#39047392">parent</a><span>|</span><a href="#39044884">next</a><span>|</span><label class="collapse" for="c-39047700">[-]</label><label class="expand" for="c-39047700">[1 more]</label></div><br/><div class="children"><div class="content">Good idea! There&#x27;s no limitation in the generation or grading, but we didn&#x27;t set up the search to support this. I&#x27;ll see if it&#x27;s possible to enable this in the wikipedia search component.</div><br/></div></div></div></div></div></div></div></div></div></body></html>