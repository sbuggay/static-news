<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683882047320" as="style"/><link rel="stylesheet" href="styles.css?v=1683882047320"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/index/100k-context-windows">100K Context Windows</a> <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>samwillis</span> | <span>295 comments</span></div><br/><div><div id="35913675" class="c"><input type="checkbox" id="c-35913675" checked=""/><div class="controls bullet"><span class="by">coldblues</span><span>|</span><a href="#35905593">next</a><span>|</span><label class="collapse" for="c-35913675">[-]</label><label class="expand" for="c-35913675">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very unfortunate that all of these AI models are so impressive, yet they&#x27;re all heavily filtered and bogged down by these massive AI corporations. All the filtering that they do heavily impacts the performance of the language models. A 100K context would also be incredible for roleplay but infeasible because of the heavy filtering.</div><br/></div></div><div id="35905593" class="c"><input type="checkbox" id="c-35905593" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#35913675">prev</a><span>|</span><a href="#35905301">next</a><span>|</span><label class="collapse" for="c-35905593">[-]</label><label class="expand" for="c-35905593">[58 more]</label></div><br/><div class="children"><div class="content">&gt;For example, we loaded the entire text of The Great Gatsby into Claude-Instant (72K tokens) and modified one line to say Mr. Carraway was “a software engineer that works on machine learning tooling at Anthropic.” When we asked the model to spot what was different, it responded with the correct answer in 22 seconds.<p>This sort of needle-in-a-haystack retrieval is definitely impressive, and it makes a lot more sense to achieve this in-context rather than trying to use a vector database if you can afford it.<p>I&#x27;m curious, though, whether there are diminishing returns in terms of how much <i>analysis</i> the model can do over those 100k tokens in a single forward pass. A human reading modified-Gatsby might eventually spot the altered line, but they&#x27;d also be able to answer questions about the overarching plot and themes of the novel, including ones that cannot be deduced from just a small number of salient snippets.<p>I&#x27;d be curious to see whether huge-context models are also able to do this, or if they start to have trouble when the bottleneck becomes reasoning capacity rather than input length. I feel like it&#x27;s hard to predict one way or the other without trying it, just because LLMs have already demonstrated a lot of surprising powers.</div><br/><div id="35906833" class="c"><input type="checkbox" id="c-35906833" checked=""/><div class="controls bullet"><span class="by">SkyPuncher</span><span>|</span><a href="#35905593">parent</a><span>|</span><a href="#35905976">next</a><span>|</span><label class="collapse" for="c-35906833">[-]</label><label class="expand" for="c-35906833">[24 more]</label></div><br/><div class="children"><div class="content">Further, the problem with this example is it relies on a comparison against public data.<p>Most of these AI start failing pretty hard when you ask it to do the same task on something completely novel to it (like a company document). Sometimes they&#x27;ll get it right. Other times, they&#x27;ll spit out gibberish that&#x27;s clearly some generic answer.</div><br/><div id="35907686" class="c"><input type="checkbox" id="c-35907686" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906833">parent</a><span>|</span><a href="#35908480">next</a><span>|</span><label class="collapse" for="c-35907686">[-]</label><label class="expand" for="c-35907686">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Most of these AI<p>This is as meaningful as saying most of the hominids can&#x27;t count. You can&#x27;t usefully generalize AI models with the rate of change that exists right now. Any statements&#x2F;comparisons about AI has to contain specific models and versions, otherwise it&#x27;s increasingly irrelevant noise.</div><br/><div id="35911379" class="c"><input type="checkbox" id="c-35911379" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35907686">parent</a><span>|</span><a href="#35908480">next</a><span>|</span><label class="collapse" for="c-35911379">[-]</label><label class="expand" for="c-35911379">[1 more]</label></div><br/><div class="children"><div class="content">Every time someone has said &quot;LLMs can&#x27;t do X&quot;, I tried X in GPT 4 and it could do it. They usually try free LLMs like Bard or GPT 3 and assume that the results generalise.</div><br/></div></div></div></div><div id="35908480" class="c"><input type="checkbox" id="c-35908480" checked=""/><div class="controls bullet"><span class="by">robotresearcher</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906833">parent</a><span>|</span><a href="#35907686">prev</a><span>|</span><a href="#35907851">next</a><span>|</span><label class="collapse" for="c-35908480">[-]</label><label class="expand" for="c-35908480">[20 more]</label></div><br/><div class="children"><div class="content">Asking to spot the difference between a given document and an unseen document is impossible.</div><br/><div id="35908646" class="c"><input type="checkbox" id="c-35908646" checked=""/><div class="controls bullet"><span class="by">lkbm</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908480">parent</a><span>|</span><a href="#35910857">next</a><span>|</span><label class="collapse" for="c-35908646">[-]</label><label class="expand" for="c-35908646">[15 more]</label></div><br/><div class="children"><div class="content">A couple years ago, I read Superfudge by Judy Blume, a book originally published in 1980. In it, the protagonist writes a letter to Santa: &quot;Please bring me one or more of the following items. A clock-radio, a remote-controlled model airplane, a laptop computer, an MP3 player and six CD&#x27;s.&quot;<p>I didn&#x27;t need to have seen this book before to know this wasn&#x27;t in the original 1980s text.<p>Similarly, if I were reading the Great Gatsby for the first time, and it identified a character as a software engineer, I would notice.</div><br/><div id="35910089" class="c"><input type="checkbox" id="c-35910089" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908646">parent</a><span>|</span><a href="#35908909">next</a><span>|</span><label class="collapse" for="c-35910089">[-]</label><label class="expand" for="c-35910089">[9 more]</label></div><br/><div class="children"><div class="content">That is not &quot;spotting the difference&quot;. That is spotting the logical inconsistency in your paragraph.<p>ChatGPT easily notices that anachronism.</div><br/><div id="35910414" class="c"><input type="checkbox" id="c-35910414" checked=""/><div class="controls bullet"><span class="by">lelandfe</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35910089">parent</a><span>|</span><a href="#35908909">next</a><span>|</span><label class="collapse" for="c-35910414">[-]</label><label class="expand" for="c-35910414">[8 more]</label></div><br/><div class="children"><div class="content">Indeed - I took lkbm&#x27;s comment and removed the &quot;originally published in 1980&quot; bit (too easy), and asked ChatGPT &quot;What&#x27;s unusual about this paragraph?&quot;<p>&gt; <i>the unusual thing about this paragraph could be that it describes a letter to Santa that includes items that were not available at the time the book was originally published in 1980, such as a laptop computer, an MP3 player, and possibly even a remote-controlled model airplane with advanced features. This suggests that the paragraph is likely from a later edition of the book that has been updated to reflect more modern technology.</i></div><br/><div id="35911161" class="c"><input type="checkbox" id="c-35911161" checked=""/><div class="controls bullet"><span class="by">earthboundkid</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35910414">parent</a><span>|</span><a href="#35908909">next</a><span>|</span><label class="collapse" for="c-35911161">[-]</label><label class="expand" for="c-35911161">[7 more]</label></div><br/><div class="children"><div class="content">Presumably, someone wrote about the new version online before to complain about the changes, so it’s recall, not insight?</div><br/><div id="35911285" class="c"><input type="checkbox" id="c-35911285" checked=""/><div class="controls bullet"><span class="by">lelandfe</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35911161">parent</a><span>|</span><a href="#35912460">next</a><span>|</span><label class="collapse" for="c-35911285">[-]</label><label class="expand" for="c-35911285">[4 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;A couple years ago, I read Beezus and Ramona, by Beverly Cleary. In it, the protagonist writes a letter to Santa: &quot;Dear Santa, I would like a MacBook Pro, an iPhone, and an Apple Watch&quot;&quot;<p>&gt; Describe what is unusual about this paragraph.<p>&gt;&gt; The unusual aspect of this paragraph is that the book &quot;Beezus and Ramona&quot; was actually written by Beverly Cleary and published in 1955, well before the invention of the MacBook Pro, iPhone, and Apple Watch.</div><br/><div id="35911371" class="c"><input type="checkbox" id="c-35911371" checked=""/><div class="controls bullet"><span class="by">jepler</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35911285">parent</a><span>|</span><a href="#35912460">next</a><span>|</span><label class="collapse" for="c-35911371">[-]</label><label class="expand" for="c-35911371">[3 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; A couple years ago, I read Beezus and Ramona, by Beverly Cleary. In it, the protagonist writes a letter to Santa: &quot;Dear Santa, I would like a shetland pony, a buggy whip, and an abacus&quot;. What is unusual about this?
ChatGPT<p>&gt; The unusual thing about the list is that it includes an abacus, which is a counting device that was commonly used in ancient China and is still used in some parts of the world today, but is not a typical gift request for a young child to make to Santa Claus. Shetland ponies were popular children&#x27;s pets at the time, and buggy whips were used to control horse-drawn carriages, but the inclusion of an abacus suggests that the protagonist, Beezus, has an interest in learning and math.<p>&gt; &gt; What&#x27;s the actual passage from the real novel say?<p>&gt; Certainly! Here&#x27;s the actual passage from the book &quot;Beezus and Ramona&quot; by Beverly Cleary, where Beezus writes a letter to Santa:<p>&gt; &quot;Dear Santa Claus,&quot; Beezus printed carefully. &quot;I have been a good girl all year except for my little sister Ramona who has been hard to put up with sometimes, but I guess I have too. Please bring me a Shetland pony, a new dress, and a doll with real hair to comb. And please bring Ramona a doll with real hair to comb too. I think she has tried hard to be good, and I will help her comb it. Please also bring me a set of paper dolls and a paint set, and Ramona some coloring books and crayons. And Santa, if you think there is room in your sleigh, please bring me an abacus. Love, Beatrice Quimby (Beezus)&quot;</div><br/><div id="35911401" class="c"><input type="checkbox" id="c-35911401" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35911371">parent</a><span>|</span><a href="#35913232">next</a><span>|</span><label class="collapse" for="c-35911401">[-]</label><label class="expand" for="c-35911401">[1 more]</label></div><br/><div class="children"><div class="content">You didn&#x27;t ask it to list ALL of the unusual things. Here&#x27;s a better prompt:<p>&gt;&gt; &quot;A couple years ago, I read Beezus and Ramona, by Beverly Cleary. In it, the protagonist writes a letter to Santa: &quot;Dear Santa, I would like a shetland pony, a buggy whip, and an abacus&quot;.<p>&gt;&gt; List all of the unusual or inconsistent things about the previous sentence.&quot;<p>Response from GPT-4:<p>Here are some potentially unusual or inconsistent elements about the sentence:<p>1. Anachronistic Items: The items in the wish list could be seen as unusual or inconsistent with typical gifts children might ask for today.<p>A Shetland pony, a buggy whip, and an abacus are not common gifts. They&#x27;re somewhat anachronistic, reflecting a time period different from the present day.<p>A Shetland pony is a breed of pony that originates from the Shetland Isles in Scotland. While some children might wish for a pony, it&#x27;s a big responsibility and not a practical gift for most families.<p>A buggy whip is a horse whip used in horse and buggy days. This is an unusual request for a child in the modern era, as horse-drawn buggies are not common means of transportation.<p>An abacus is an ancient counting tool, and while some children might be interested in such a device, it&#x27;s not a typical request, especially in an era of digital calculators and computers.<p>2. Inconsistency with Character: If you&#x27;re referring to the &quot;Beezus and Ramona&quot; series by Beverly Cleary, neither Beezus nor Ramona would likely ask for these items. Beezus (Beatrice Quimby) is the responsible older sister who might appreciate practical things, but an abacus or a buggy whip would be out of character. Ramona Quimby, the imaginative and energetic younger sister, might ask for a Shetland pony, but a buggy whip and an abacus seem inconsistent with her character.<p>3. Inconsistency with the Book: As far as my knowledge cutoff in September 2021, there is no record of such a letter being written to Santa in the book &quot;Beezus and Ramona&quot;. It could be seen as inconsistent with the actual content of the book.<p>Remember, these are interpretations and the perceived unusualness or inconsistency can vary based on individual perspectives. For instance, if this is a fictional context where children commonly use abacuses, ride ponies, and learn to use buggy whips, then these items might not be unusual at all.</div><br/></div></div><div id="35913232" class="c"><input type="checkbox" id="c-35913232" checked=""/><div class="controls bullet"><span class="by">isaacfrond</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35911371">parent</a><span>|</span><a href="#35911401">prev</a><span>|</span><a href="#35912460">next</a><span>|</span><label class="collapse" for="c-35913232">[-]</label><label class="expand" for="c-35913232">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing this is GPT3? Because GPT4 says look it up yourself. When I did the whole scene doesn&#x27;t seem to exist in the book. In other words, this seems to be a hallucination.<p>When I repeat the experiment with Lord of the Rings and a scene that does exist, it spits it out word for word correct.</div><br/></div></div></div></div></div></div><div id="35912460" class="c"><input type="checkbox" id="c-35912460" checked=""/><div class="controls bullet"><span class="by">DougBTX</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35911161">parent</a><span>|</span><a href="#35911285">prev</a><span>|</span><a href="#35911337">next</a><span>|</span><label class="collapse" for="c-35912460">[-]</label><label class="expand" for="c-35912460">[1 more]</label></div><br/><div class="children"><div class="content">In the insight case, the model has to classify the terms in a short paragraph into broad generic classes, then report which terms have different classes. In the recall case the model has to rank all utterances in its training data based on how relevant they are to the question, then summarise the most relevant one (assuming there is one). Insight seems easier than recall here.</div><br/></div></div></div></div></div></div></div></div><div id="35908909" class="c"><input type="checkbox" id="c-35908909" checked=""/><div class="controls bullet"><span class="by">drusepth</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908646">parent</a><span>|</span><a href="#35910089">prev</a><span>|</span><a href="#35910857">next</a><span>|</span><label class="collapse" for="c-35908909">[-]</label><label class="expand" for="c-35908909">[5 more]</label></div><br/><div class="children"><div class="content">I think there are plenty of humans who wouldn&#x27;t notice, though.<p>And probably plenty of AI implementations that would notice.</div><br/><div id="35909854" class="c"><input type="checkbox" id="c-35909854" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908909">parent</a><span>|</span><a href="#35910857">next</a><span>|</span><label class="collapse" for="c-35909854">[-]</label><label class="expand" for="c-35909854">[4 more]</label></div><br/><div class="children"><div class="content">Are we now aspiring to ABAI (Artificial Below-Average Intelligence)?</div><br/><div id="35910189" class="c"><input type="checkbox" id="c-35910189" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909854">parent</a><span>|</span><a href="#35910262">next</a><span>|</span><label class="collapse" for="c-35910189">[-]</label><label class="expand" for="c-35910189">[1 more]</label></div><br/><div class="children"><div class="content">I regret to inform you that &quot;average intelligence&quot; is a far lower bar than you might think.</div><br/></div></div><div id="35910262" class="c"><input type="checkbox" id="c-35910262" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909854">parent</a><span>|</span><a href="#35910189">prev</a><span>|</span><a href="#35909927">next</a><span>|</span><label class="collapse" for="c-35910262">[-]</label><label class="expand" for="c-35910262">[1 more]</label></div><br/><div class="children"><div class="content">As the joke goes: look around you and see how dumb the average person is. And now imagine, half the people in your city&#x2F;country&#x2F;Earth are dumber than this person.</div><br/></div></div><div id="35909927" class="c"><input type="checkbox" id="c-35909927" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909854">parent</a><span>|</span><a href="#35910262">prev</a><span>|</span><a href="#35910857">next</a><span>|</span><label class="collapse" for="c-35909927">[-]</label><label class="expand" for="c-35909927">[1 more]</label></div><br/><div class="children"><div class="content">The AI would not only notice, it would also notice 300 other things that are far more subtle haha</div><br/></div></div></div></div></div></div></div></div><div id="35910857" class="c"><input type="checkbox" id="c-35910857" checked=""/><div class="controls bullet"><span class="by">BHSPitMonkey</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908480">parent</a><span>|</span><a href="#35908646">prev</a><span>|</span><a href="#35911757">next</a><span>|</span><label class="collapse" for="c-35910857">[-]</label><label class="expand" for="c-35910857">[1 more]</label></div><br/><div class="children"><div class="content">In the Gatsby example, I&#x27;d expect the model to be able to answer &quot;which sentence from the story felt out of place?&quot; without knowledge of the original.</div><br/></div></div><div id="35911757" class="c"><input type="checkbox" id="c-35911757" checked=""/><div class="controls bullet"><span class="by">hfhdndnfbdbd</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908480">parent</a><span>|</span><a href="#35910857">prev</a><span>|</span><a href="#35909593">next</a><span>|</span><label class="collapse" for="c-35911757">[-]</label><label class="expand" for="c-35911757">[1 more]</label></div><br/><div class="children"><div class="content">The point is spotting the inconsistency within the document, not whether an original has a different form</div><br/></div></div><div id="35909593" class="c"><input type="checkbox" id="c-35909593" checked=""/><div class="controls bullet"><span class="by">moonchrome</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908480">parent</a><span>|</span><a href="#35911757">prev</a><span>|</span><a href="#35907851">next</a><span>|</span><label class="collapse" for="c-35909593">[-]</label><label class="expand" for="c-35909593">[2 more]</label></div><br/><div class="children"><div class="content">Comparing strings is a python script problem for a newbie.<p>Interesting part is if it can deduce something that&#x27;s out of place within a huge document.</div><br/><div id="35910541" class="c"><input type="checkbox" id="c-35910541" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909593">parent</a><span>|</span><a href="#35907851">next</a><span>|</span><label class="collapse" for="c-35910541">[-]</label><label class="expand" for="c-35910541">[1 more]</label></div><br/><div class="children"><div class="content">They’re saying you can’t ask it to compare against a doc it doesn’t have access to.</div><br/></div></div></div></div></div></div><div id="35907851" class="c"><input type="checkbox" id="c-35907851" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906833">parent</a><span>|</span><a href="#35908480">prev</a><span>|</span><a href="#35905976">next</a><span>|</span><label class="collapse" for="c-35907851">[-]</label><label class="expand" for="c-35907851">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d imagine working with an entire company document would require a lot more hand holding and investment in prompt engineering. You can definitely get better results if you add much more context of what you&#x27;re expecting and how the LLM should do it. Treating these LLMs as just simple Q&amp;A machines is usually not enough unless you&#x27;re doing simple stuff.</div><br/></div></div></div></div><div id="35905976" class="c"><input type="checkbox" id="c-35905976" checked=""/><div class="controls bullet"><span class="by">tunesmith</span><span>|</span><a href="#35905593">parent</a><span>|</span><a href="#35906833">prev</a><span>|</span><a href="#35911742">next</a><span>|</span><label class="collapse" for="c-35905976">[-]</label><label class="expand" for="c-35905976">[13 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been curious about this for a while, I have a hobby use-case of wanting to input in-progress novellas and then asking it questions about plot holes, open plot threads, and if new chapter &quot;x&quot; presents any serious plot contradiction problems. I haven&#x27;t tried exploring that with a vectordb-embeddings approach yet.</div><br/><div id="35907448" class="c"><input type="checkbox" id="c-35907448" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35905976">parent</a><span>|</span><a href="#35911742">next</a><span>|</span><label class="collapse" for="c-35907448">[-]</label><label class="expand" for="c-35907448">[12 more]</label></div><br/><div class="children"><div class="content">This is an exact example of something a vector dbs would be terrible at.<p>Vector dbs work by fetching segments that are similar in topics to the question, so like &quot;Where did &lt;Character&gt; go after &lt;thing&gt;&quot; will retrieve segments with locations &amp; the character &amp; maybe talking about &lt;thing&gt; as a recent event.<p>Your question has no similarity with the segments required in any way; &amp; it&#x27;s not the segments that are wrong it&#x27;s the way they relate to the rest of the story</div><br/><div id="35911950" class="c"><input type="checkbox" id="c-35911950" checked=""/><div class="controls bullet"><span class="by">koboll</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35907448">parent</a><span>|</span><a href="#35909436">next</a><span>|</span><label class="collapse" for="c-35911950">[-]</label><label class="expand" for="c-35911950">[1 more]</label></div><br/><div class="children"><div class="content">So what&#x27;s the right way to do a wider-ranging analysis? Chunk into segments, ask about each one, then do a second pass to review all answers together?</div><br/></div></div><div id="35909436" class="c"><input type="checkbox" id="c-35909436" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35907448">parent</a><span>|</span><a href="#35911950">prev</a><span>|</span><a href="#35908675">next</a><span>|</span><label class="collapse" for="c-35909436">[-]</label><label class="expand" for="c-35909436">[5 more]</label></div><br/><div class="children"><div class="content">Do the OpenAI APIs support converting prompts to vectors, or are people running their own models locally to do this? Can you recommend any good resources to read up on vector DB approaches to working around context length limits ?</div><br/><div id="35913305" class="c"><input type="checkbox" id="c-35913305" checked=""/><div class="controls bullet"><span class="by">tuanacelik</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909436">parent</a><span>|</span><a href="#35912488">next</a><span>|</span><label class="collapse" for="c-35913305">[-]</label><label class="expand" for="c-35913305">[1 more]</label></div><br/><div class="children"><div class="content">Indeed this tutorial on Haystack is a good one as an example: <a href="https:&#x2F;&#x2F;haystack.deepset.ai&#x2F;tutorials&#x2F;22_pipeline_with_promptnode" rel="nofollow">https:&#x2F;&#x2F;haystack.deepset.ai&#x2F;tutorials&#x2F;22_pipeline_with_promp...</a>
It combines a retrieval step followed by a prompt layer which inserts the relevant context into the prompt. You can however change the &#x27;retrieval step&#x27; with something that uses a proper embedding model and OpenAI also provides those if you want to. I tend to use lighter (cheaper) OSS models for this step though. PS: There&#x27;s some functionality in the PromptNode to make sure you don&#x27;t exceed prompt limit.</div><br/></div></div><div id="35912488" class="c"><input type="checkbox" id="c-35912488" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909436">parent</a><span>|</span><a href="#35913305">prev</a><span>|</span><a href="#35909946">next</a><span>|</span><label class="collapse" for="c-35912488">[-]</label><label class="expand" for="c-35912488">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you can use a local embeddings model like gtr-t5-xl alongside retriever augmentation.
This can point you in the right direction: <a href="https:&#x2F;&#x2F;haystack.deepset.ai&#x2F;tutorials&#x2F;22_pipeline_with_promptnode" rel="nofollow">https:&#x2F;&#x2F;haystack.deepset.ai&#x2F;tutorials&#x2F;22_pipeline_with_promp...</a></div><br/></div></div><div id="35909946" class="c"><input type="checkbox" id="c-35909946" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909436">parent</a><span>|</span><a href="#35912488">prev</a><span>|</span><a href="#35908675">next</a><span>|</span><label class="collapse" for="c-35909946">[-]</label><label class="expand" for="c-35909946">[2 more]</label></div><br/><div class="children"><div class="content">open ai has an embeddings api that ppl use for that <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings</a>, though whether it&#x27;s the best model to do that is congested.<p>Contriever is an example of a strong model to do that yourself. see their paper too to learn about the domain.
<a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;contriever">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;contriever</a></div><br/><div id="35910906" class="c"><input type="checkbox" id="c-35910906" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909946">parent</a><span>|</span><a href="#35908675">next</a><span>|</span><label class="collapse" for="c-35910906">[-]</label><label class="expand" for="c-35910906">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!</div><br/></div></div></div></div></div></div><div id="35908675" class="c"><input type="checkbox" id="c-35908675" checked=""/><div class="controls bullet"><span class="by">toss1</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35907448">parent</a><span>|</span><a href="#35909436">prev</a><span>|</span><a href="#35911742">next</a><span>|</span><label class="collapse" for="c-35908675">[-]</label><label class="expand" for="c-35908675">[5 more]</label></div><br/><div class="children"><div class="content">Good points - LLMs are ok at finding things that exist, but they have zero ability to abstract and find what is missing (actually, probably negative; they&#x27;d likely hallucinate and fill in the gaps).<p>Which makes me wonder if the opposite, but more laborious approach might work - request it identify all characters and plot themes, then request summaries of each.  You&#x27;d have to review the summaries for holes.  Lotsa work, but still maybe quicker than re-reading everything yourself?</div><br/><div id="35909102" class="c"><input type="checkbox" id="c-35909102" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908675">parent</a><span>|</span><a href="#35908910">next</a><span>|</span><label class="collapse" for="c-35909102">[-]</label><label class="expand" for="c-35909102">[1 more]</label></div><br/><div class="children"><div class="content">Firstly, I don&#x27;t at all agree that they have zero ability to abstract. Doesn&#x27;t fit my experience at all. A lot of the tasks I use ChatGPT for is exactly to analyse gaps in specifications etc. And have it tell me what is missing, suggest additions or ask for clarifications. It does that just fine.<p>But I&#x27;ve started experimenting with the second part, of sorts, not to find plot holes but to have it create character sheets for my  series of novels for my own reference.<p>Basically have it maintain a sheet and feed it chunks of one or more chapters and asking it to output an a new sheet augmented with the new details.<p>With a 100K context window I might just test doing it over while novels or much larger  chunks of one.</div><br/></div></div><div id="35908910" class="c"><input type="checkbox" id="c-35908910" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908675">parent</a><span>|</span><a href="#35909102">prev</a><span>|</span><a href="#35909951">next</a><span>|</span><label class="collapse" for="c-35908910">[-]</label><label class="expand" for="c-35908910">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>LLMs are ok at finding things that exist, but they have zero ability to abstract and find what is missing (actually, probably negative; they&#x27;d likely hallucinate and fill in the gaps).</i><p>I feel this is mostly a prompting issue. Specifically GPT-4 shows surprising ability to abstract to some degree and work with high-level concepts, but it seems that, quite often, you need to guide it towards the right &quot;mode&quot; of thinking.<p>It&#x27;s like dealing with a 4 year old kid. They may be perfectly able to do something you ask them, but will keep doing something else, until you give them specific hints, several times, in different ways.</div><br/><div id="35910485" class="c"><input type="checkbox" id="c-35910485" checked=""/><div class="controls bullet"><span class="by">clbrmbr</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908910">parent</a><span>|</span><a href="#35909951">next</a><span>|</span><label class="collapse" for="c-35910485">[-]</label><label class="expand" for="c-35910485">[1 more]</label></div><br/><div class="children"><div class="content">There are other ways of using the model other than iterative forward inference (completions). You could run the model over your novel (perhaps including a preface) and look at the posterior distribution as it scans. This may not be so meaningful at the level of the token distribution, but there may be interesting ways of “spell checking” at a semantic level level. Think a thesaurus but operating at the level of whole paragraphs.</div><br/></div></div></div></div><div id="35909951" class="c"><input type="checkbox" id="c-35909951" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35908675">parent</a><span>|</span><a href="#35908910">prev</a><span>|</span><a href="#35911742">next</a><span>|</span><label class="collapse" for="c-35909951">[-]</label><label class="expand" for="c-35909951">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s not at all what I said</div><br/></div></div></div></div></div></div></div></div><div id="35911742" class="c"><input type="checkbox" id="c-35911742" checked=""/><div class="controls bullet"><span class="by">hfhdndnfbdbd</span><span>|</span><a href="#35905593">parent</a><span>|</span><a href="#35905976">prev</a><span>|</span><a href="#35906056">next</a><span>|</span><label class="collapse" for="c-35911742">[-]</label><label class="expand" for="c-35911742">[1 more]</label></div><br/><div class="children"><div class="content">I think people in the comments are missing the point.<p>I might be wrong, but the point isn&#x27;t comparing a modified The Great Gatsby to the original one. Of course that&#x27;s not impressive and it&#x27;s an easy thing to do.<p>The point of the exercise is supposed to be[1] that the model has the entire novel as context &#x2F; prompt and so can identify (within that context), whether a paragraph is out of place. That is impressive and I wouldn&#x27;t know how to find that programmatically (would you have a list of &quot;modern&quot; words to check? But maybe the out of place thing is hidden in the meaning and there&#x27;s no out of place, modern word).<p>[1] I say supposed to be because the great Gatsby is in the training sample and so maybe there is a sense in which the model &quot;contains&quot; the original text and in some way is doing &quot;just&quot; a comparison. A better test would be try with a novel or document that the model hasn&#x27;t seen...or at least something not as famous as the great Gatsby.</div><br/></div></div><div id="35906056" class="c"><input type="checkbox" id="c-35906056" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#35905593">parent</a><span>|</span><a href="#35911742">prev</a><span>|</span><a href="#35909932">next</a><span>|</span><label class="collapse" for="c-35906056">[-]</label><label class="expand" for="c-35906056">[14 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also not entirely convinced by &quot;huge&quot; context models just yet, especially as it relates to fuzzy knowledge such as overarching themes or writing style.<p>In particular, there are 0 mentions of the phrase &quot;machine learning&quot; in The Great Gatsby, so adding one sentence that introduces the phrase should be easy for self-attention to pick out.</div><br/><div id="35906717" class="c"><input type="checkbox" id="c-35906717" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906056">parent</a><span>|</span><a href="#35906640">next</a><span>|</span><label class="collapse" for="c-35906717">[-]</label><label class="expand" for="c-35906717">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be more impressed if it could rewrite Mr. Carraway  as an ML engineer in the entire novel. However it&#x27;s not intrinsically clear that it cannot do this...<p>It&#x27;ll be tough to find good benchmarks on long context windows. A human cannot label using 100k tokens of context.</div><br/><div id="35907051" class="c"><input type="checkbox" id="c-35907051" checked=""/><div class="controls bullet"><span class="by">zooch</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906717">parent</a><span>|</span><a href="#35912803">next</a><span>|</span><label class="collapse" for="c-35907051">[-]</label><label class="expand" for="c-35907051">[2 more]</label></div><br/><div class="children"><div class="content">My thoughts exactly - rewrite the novel with Mr. Carraway as an ML engineer while maintaining themes&#x2F;motifs (possible adding new ones too). I&#x27;m guessing what&#x27;s impressive is that these are the first steps towards something like this? Or is it already possible? Someone please correct me here.</div><br/><div id="35912379" class="c"><input type="checkbox" id="c-35912379" checked=""/><div class="controls bullet"><span class="by">gcanyon</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35907051">parent</a><span>|</span><a href="#35912803">next</a><span>|</span><label class="collapse" for="c-35912379">[-]</label><label class="expand" for="c-35912379">[1 more]</label></div><br/><div class="children"><div class="content">Or rewrite The Count of Monte Cristo as a science fiction novel and get The Stars My Destination. Or rewrite Heinlein&#x27;s Double Star into present-day and get the movie Dave.</div><br/></div></div></div></div><div id="35912803" class="c"><input type="checkbox" id="c-35912803" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906717">parent</a><span>|</span><a href="#35907051">prev</a><span>|</span><a href="#35906640">next</a><span>|</span><label class="collapse" for="c-35912803">[-]</label><label class="expand" for="c-35912803">[1 more]</label></div><br/><div class="children"><div class="content">Wonder if they’d set it in SF then…</div><br/></div></div></div></div><div id="35906640" class="c"><input type="checkbox" id="c-35906640" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906056">parent</a><span>|</span><a href="#35906717">prev</a><span>|</span><a href="#35909932">next</a><span>|</span><label class="collapse" for="c-35906640">[-]</label><label class="expand" for="c-35906640">[9 more]</label></div><br/><div class="children"><div class="content">This sounds like all the other skepticism about what AI can do. And then it can spot 200x more than any human and correlate it into common themes, and you’ll say what?</div><br/><div id="35910805" class="c"><input type="checkbox" id="c-35910805" checked=""/><div class="controls bullet"><span class="by">tanseydavid</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906640">parent</a><span>|</span><a href="#35906723">next</a><span>|</span><label class="collapse" for="c-35910805">[-]</label><label class="expand" for="c-35910805">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; And then...you’ll say what?<p>USER: It&#x27;s a stochastic parrot.<p>GPT: I know you are, so what am I?</div><br/></div></div><div id="35906723" class="c"><input type="checkbox" id="c-35906723" checked=""/><div class="controls bullet"><span class="by">devmor</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906640">parent</a><span>|</span><a href="#35910805">prev</a><span>|</span><a href="#35909932">next</a><span>|</span><label class="collapse" for="c-35906723">[-]</label><label class="expand" for="c-35906723">[7 more]</label></div><br/><div class="children"><div class="content">Doing more than a human can isn&#x27;t impressive. Most computer problems for any purpose can do more of something, or something faster than a human can.<p>A better comparison would be if it can pick out any differences that can&#x27;t be picked out by more traditional and simple algorithms.</div><br/><div id="35907982" class="c"><input type="checkbox" id="c-35907982" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906723">parent</a><span>|</span><a href="#35906883">next</a><span>|</span><label class="collapse" for="c-35907982">[-]</label><label class="expand" for="c-35907982">[5 more]</label></div><br/><div class="children"><div class="content">It does, using this method.<p>My immediate thought as well was &#x27;... Yeah, well vimdiff can do that in milliseconds rather than 22 seconds&#x27; - but that&#x27;s obviously missing the point entirely.
Of course, we need to tell people to use the right tool for the job, and that will be more and more important to remind people of now.<p>However, it&#x27;s pretty clear that the reason they used this task is to give something simple to understand what was done in a very simple example.
Of course it can do more semantic understanding related tasks, because that&#x27;s what the model does.<p>So, without looking at the details we all know that it can summarize full books, give thematic differences between two books, write what a book may be like if a character switch from one book to another is done, etc.<p>If it <i>doesn&#x27;t</i> do these things (not just badly, but can&#x27;t at all) I would be surprised.  If it does them, but badly, I wouldn&#x27;t be surprised, but it also wouldn&#x27;t be mind bending to see it do better than any human at the task as well.</div><br/><div id="35909511" class="c"><input type="checkbox" id="c-35909511" checked=""/><div class="controls bullet"><span class="by">Frost1x</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35907982">parent</a><span>|</span><a href="#35911376">next</a><span>|</span><label class="collapse" for="c-35909511">[-]</label><label class="expand" for="c-35909511">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Of course it can do more semantic understanding related tasks, because that&#x27;s what the model does.<p>The problem is that marketing has eroded maintaining any such faith. Too often, simple examples are given to the consumer to extrapolate intended functionality because there&#x27;s no false advertising involved then. It&#x27;s used over and over again in products, the examples are well selected and don&#x27;t actually show a good representation of perceived functionality.<p>As such, I personally can&#x27;t make the leap to <i>of course it can do more semantic understanding related  tasks</i> like a diff that&#x27;s not as simple, one where perhaps a characters overall personality over the course of the book is shifted, not just a single line that defines their profession.<p>This isn&#x27;t to say the demonstrative example isn&#x27;t neat on its own accord given whats going on here, it is, I&#x27;m just saying I can&#x27;t make such leaps from examples given by any products. When I work with vendors of traditional software, this happens <i>all the time</i> people dance around a lack of functionality you obviously want or need to make a sell. It&#x27;s only when you force them to be explicit on the specific cases, especially in writing, that I have any faith at all.</div><br/><div id="35910200" class="c"><input type="checkbox" id="c-35910200" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909511">parent</a><span>|</span><a href="#35910928">next</a><span>|</span><label class="collapse" for="c-35910200">[-]</label><label class="expand" for="c-35910200">[1 more]</label></div><br/><div class="children"><div class="content">LLM&#x27;s are specifically designed and trained for semantic understanding related tasks. That&#x27;s the entire point. They are trained to solve natural language understanding tasks and they build a semantic model in order to solve the tasks.</div><br/></div></div><div id="35910928" class="c"><input type="checkbox" id="c-35910928" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909511">parent</a><span>|</span><a href="#35910200">prev</a><span>|</span><a href="#35911376">next</a><span>|</span><label class="collapse" for="c-35910928">[-]</label><label class="expand" for="c-35910928">[1 more]</label></div><br/><div class="children"><div class="content">I did say it may do certain tasks with low performance.  What you&#x27;re saying is not really understandable (or simply wrong).
The fact that it does understand how to do a &#x27;simple&#x27; task such as finding where text is different is actually somewhat impressive given the typical training data for these models.  But I suppose you need to actually understand the field of NLP to understand why that is.<p>If you&#x27;re expecting perfection or magic then you will be disappointed.</div><br/></div></div></div></div><div id="35911376" class="c"><input type="checkbox" id="c-35911376" checked=""/><div class="controls bullet"><span class="by">ehnto</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35907982">parent</a><span>|</span><a href="#35909511">prev</a><span>|</span><a href="#35906883">next</a><span>|</span><label class="collapse" for="c-35911376">[-]</label><label class="expand" for="c-35911376">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s fair to say most people don&#x27;t know what a diff tool is, but do know how to ask questions. That is the democratizing factor that AI is introducing I feel, giving high powered computing to people without the need for specialized knowledge.</div><br/></div></div></div></div><div id="35906883" class="c"><input type="checkbox" id="c-35906883" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35906723">parent</a><span>|</span><a href="#35907982">prev</a><span>|</span><a href="#35909932">next</a><span>|</span><label class="collapse" for="c-35906883">[-]</label><label class="expand" for="c-35906883">[1 more]</label></div><br/><div class="children"><div class="content">Or course it can very soon, since those were also written by humans. Like AlphaZero vs Rybka</div><br/></div></div></div></div></div></div></div></div><div id="35909932" class="c"><input type="checkbox" id="c-35909932" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#35905593">parent</a><span>|</span><a href="#35906056">prev</a><span>|</span><a href="#35905301">next</a><span>|</span><label class="collapse" for="c-35909932">[-]</label><label class="expand" for="c-35909932">[5 more]</label></div><br/><div class="children"><div class="content">What techniques do they actually use to achieve 100k? I assumed they load the document to a vector databases and then do some kind of views into that</div><br/><div id="35910288" class="c"><input type="checkbox" id="c-35910288" checked=""/><div class="controls bullet"><span class="by">asadm</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35909932">parent</a><span>|</span><a href="#35905301">next</a><span>|</span><label class="collapse" for="c-35910288">[-]</label><label class="expand" for="c-35910288">[4 more]</label></div><br/><div class="children"><div class="content">I would assume by training LLM on dataset of 100k tokens would be the right way.</div><br/><div id="35910522" class="c"><input type="checkbox" id="c-35910522" checked=""/><div class="controls bullet"><span class="by">clbrmbr</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35910288">parent</a><span>|</span><a href="#35905301">next</a><span>|</span><label class="collapse" for="c-35910522">[-]</label><label class="expand" for="c-35910522">[3 more]</label></div><br/><div class="children"><div class="content">Makes me wonder whether we could get really huge contexts much more efficiently by feeding back a higher layer back into the tail end of the model. That way it has a very clear picture of the recent text but only a compressed picture of the earlier parts of the document.<p>(I think I’ve got to read up on how transformers actually work.)</div><br/><div id="35911049" class="c"><input type="checkbox" id="c-35911049" checked=""/><div class="controls bullet"><span class="by">desperate</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35910522">parent</a><span>|</span><a href="#35905301">next</a><span>|</span><label class="collapse" for="c-35911049">[-]</label><label class="expand" for="c-35911049">[2 more]</label></div><br/><div class="children"><div class="content">Afaik you&#x27;re describing something akin to a recurrent neural network, and the problem with that is that it doesn&#x27;t parallelize well to modern hardware. And vanishing gradients.</div><br/><div id="35911405" class="c"><input type="checkbox" id="c-35911405" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35905593">root</a><span>|</span><a href="#35911049">parent</a><span>|</span><a href="#35905301">next</a><span>|</span><label class="collapse" for="c-35911405">[-]</label><label class="expand" for="c-35911405">[1 more]</label></div><br/><div class="children"><div class="content">I had the same thought as the comment you&#x27;re responding to.<p>Recurrent neural networks <i>are</i> bad when the recurrence is 100x long or more. You need long chains because with a token-at-a-time, that&#x27;s what you need to process even one paragraph.<p>But if you use an RNN around a Transformed-based LLM, then you&#x27;re adding +4K or +8K tokens per recurrence, not +1.<p>E.g.: GPT 4 32K would need just 4x RNN steps to reach 128K tokens!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35905301" class="c"><input type="checkbox" id="c-35905301" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#35905593">prev</a><span>|</span><a href="#35905189">next</a><span>|</span><label class="collapse" for="c-35905301">[-]</label><label class="expand" for="c-35905301">[93 more]</label></div><br/><div class="children"><div class="content">This is the first time I&#x27;ve felt like Anthropic may be a true competitor to OpenAI.<p>I see 6 ways to improve foundation LLMs other than cost. If your product is best at one of the below, and has parity at the other 5 items, then customers will switch. I&#x27;m currently using GPT-4-8k. I regularly run into the context limit. If Claude-100K is close enough on &quot;intelligence&quot; then I will switch.<p>Six Dimensions to Compare Foundation LLMs:<p>1. Smarter models<p>2. Larger context windows<p>3. More input and output modes<p>4. Lower time to first response token and to full response<p>5. Easier prompting<p>6. Integrations</div><br/><div id="35907170" class="c"><input type="checkbox" id="c-35907170" checked=""/><div class="controls bullet"><span class="by">srowaway2</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35905574">next</a><span>|</span><label class="collapse" for="c-35907170">[-]</label><label class="expand" for="c-35907170">[12 more]</label></div><br/><div class="children"><div class="content">7. Price!<p>GPT4-32K costs ~$2 if you end up using the full 32K tokens, so if you&#x27;re doing any chaining or back-and-forth it can get expensive very quickly.</div><br/><div id="35913310" class="c"><input type="checkbox" id="c-35913310" checked=""/><div class="controls bullet"><span class="by">xp84</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907170">parent</a><span>|</span><a href="#35910547">next</a><span>|</span><label class="collapse" for="c-35913310">[-]</label><label class="expand" for="c-35913310">[1 more]</label></div><br/><div class="children"><div class="content">Oh boy. I hope Microsoft doesn&#x27;t get tired of subsidizing my access through Bing. I quite like it.</div><br/></div></div><div id="35910547" class="c"><input type="checkbox" id="c-35910547" checked=""/><div class="controls bullet"><span class="by">motoboi</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907170">parent</a><span>|</span><a href="#35913310">prev</a><span>|</span><a href="#35908385">next</a><span>|</span><label class="collapse" for="c-35910547">[-]</label><label class="expand" for="c-35910547">[2 more]</label></div><br/><div class="children"><div class="content">Also, the azure offering is 10 times the openai price, so not really usable right now.</div><br/><div id="35912533" class="c"><input type="checkbox" id="c-35912533" checked=""/><div class="controls bullet"><span class="by">karmeliet</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35910547">parent</a><span>|</span><a href="#35908385">next</a><span>|</span><label class="collapse" for="c-35912533">[-]</label><label class="expand" for="c-35912533">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s exactly the same pricing (East US):<p><a href="https:&#x2F;&#x2F;azure.microsoft.com&#x2F;en-us&#x2F;pricing&#x2F;details&#x2F;cognitive-services&#x2F;openai-service&#x2F;" rel="nofollow">https:&#x2F;&#x2F;azure.microsoft.com&#x2F;en-us&#x2F;pricing&#x2F;details&#x2F;cognitive-...</a><p><a href="https:&#x2F;&#x2F;openai.com&#x2F;pricing" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;pricing</a></div><br/></div></div></div></div><div id="35908385" class="c"><input type="checkbox" id="c-35908385" checked=""/><div class="controls bullet"><span class="by">hesdeadjim</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907170">parent</a><span>|</span><a href="#35910547">prev</a><span>|</span><a href="#35905574">next</a><span>|</span><label class="collapse" for="c-35908385">[-]</label><label class="expand" for="c-35908385">[8 more]</label></div><br/><div class="children"><div class="content">Oof, got access to the 8k model recently and was wondering what costs would be on the 32k one. That&#x27;s brutal.</div><br/><div id="35911146" class="c"><input type="checkbox" id="c-35911146" checked=""/><div class="controls bullet"><span class="by">freediver</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908385">parent</a><span>|</span><a href="#35905574">next</a><span>|</span><label class="collapse" for="c-35911146">[-]</label><label class="expand" for="c-35911146">[7 more]</label></div><br/><div class="children"><div class="content">Why? 32K tokens would cover creating source code for a small to medium programming project. THat would easilly cost hundreds of dollars if done by a freelancer, and could get it in almost real time for just $2.</div><br/><div id="35911544" class="c"><input type="checkbox" id="c-35911544" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35911146">parent</a><span>|</span><a href="#35911542">next</a><span>|</span><label class="collapse" for="c-35911544">[-]</label><label class="expand" for="c-35911544">[4 more]</label></div><br/><div class="children"><div class="content">Only if you can get that result in <i>one pass</i>.<p>The real utility of LLMs is that they can be called in a loop to scan through many web pages, many code files, issue tickets, emails, etc...<p>There are already demos and experiments out there that for every input, 4x outputs are generated, then <i>those</i> are fed back into the LLM 4x for &quot;review&quot;, then the best variant is then used to generate code which then automatically tested, errors are fed back in a loop, also with 4x parallel tries, etc...<p>It&#x27;s the <i>throughput</i> compared to humans that is the true differentiator. If hooking up the API in a loop up ends up costing more than a human, then it&#x27;s not worth it.</div><br/><div id="35911815" class="c"><input type="checkbox" id="c-35911815" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35911544">parent</a><span>|</span><a href="#35911991">next</a><span>|</span><label class="collapse" for="c-35911815">[-]</label><label class="expand" for="c-35911815">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s the throughput compared to humans that is the true differentiator. If hooking up the API in a loop up ends up costing more than a human, then it&#x27;s not worth it.<p>If its better in another dimension (e.g., calendar elapsed time), it may well be worth being more expensive.</div><br/></div></div><div id="35911991" class="c"><input type="checkbox" id="c-35911991" checked=""/><div class="controls bullet"><span class="by">koboll</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35911544">parent</a><span>|</span><a href="#35911815">prev</a><span>|</span><a href="#35911542">next</a><span>|</span><label class="collapse" for="c-35911991">[-]</label><label class="expand" for="c-35911991">[2 more]</label></div><br/><div class="children"><div class="content">Two dollars spent over and over would still take a while to equal a software engineer&#x27;s salary, and if the end result is &quot;produce a fully functional codebase in minutes&quot; the premium might be worth it even if it exceeds that much.</div><br/><div id="35913210" class="c"><input type="checkbox" id="c-35913210" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35911991">parent</a><span>|</span><a href="#35911542">next</a><span>|</span><label class="collapse" for="c-35913210">[-]</label><label class="expand" for="c-35913210">[1 more]</label></div><br/><div class="children"><div class="content">We all know the game Sam Altman is playing. Eventually you can squeeze the full salary in cost out of companies who have laid off their workers and it would still be a good deal because AI does not need health care, pay taxes, hardware, sick days and so on.</div><br/></div></div></div></div></div></div><div id="35911542" class="c"><input type="checkbox" id="c-35911542" checked=""/><div class="controls bullet"><span class="by">replygirl</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35911146">parent</a><span>|</span><a href="#35911544">prev</a><span>|</span><a href="#35905574">next</a><span>|</span><label class="collapse" for="c-35911542">[-]</label><label class="expand" for="c-35911542">[2 more]</label></div><br/><div class="children"><div class="content">32k tokens is 500-1000 lines of code, so more like thousands of dollars, unless you&#x27;re comparing against a landing page or CRUD tool arranged through Upwork. On the flip side, before I dove into AutoGPT I couldn&#x27;t get GPT-4 to iterate on something as simple as a TypeScript function that deeply converts { foo_bar: string } to { fooBar: Date } without running out of context and cyclically reintroducing regressions.</div><br/><div id="35911782" class="c"><input type="checkbox" id="c-35911782" checked=""/><div class="controls bullet"><span class="by">hesdeadjim</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35911542">parent</a><span>|</span><a href="#35905574">next</a><span>|</span><label class="collapse" for="c-35911782">[-]</label><label class="expand" for="c-35911782">[1 more]</label></div><br/><div class="children"><div class="content">Advice on where to get started with auto-gpt?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35905574" class="c"><input type="checkbox" id="c-35905574" checked=""/><div class="controls bullet"><span class="by">RobotToaster</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35907170">prev</a><span>|</span><a href="#35905422">next</a><span>|</span><label class="collapse" for="c-35905574">[-]</label><label class="expand" for="c-35905574">[44 more]</label></div><br/><div class="children"><div class="content">&gt;Six Dimensions to Compare Foundation LLMs<p>I&#x27;d add open source to the list, which neither &quot;open&quot;AI or this is.</div><br/><div id="35905933" class="c"><input type="checkbox" id="c-35905933" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905574">parent</a><span>|</span><a href="#35909374">next</a><span>|</span><label class="collapse" for="c-35905933">[-]</label><label class="expand" for="c-35905933">[20 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think most of the large customers will care about OSS AI.  
Over the last decade they&#x27;ve learned (trained themselves?) where to put their money towards (cloud vs. in-house infra for all manner of things, for better or worse) and I think AI tools will follow similar trends.<p>Businesses will certainly care about cost, but just as important will be:<p>- Customization and fine-tuning capabilities (also &#x27;white labeling&#x27; where appropriate)<p>- Integrations (with 3rd party and in-house services &amp; data stores)<p>- SLA &amp; performance concerns<p>- Safety features<p>Open Source AI will have a place, but may be more towards personal-use and academic work.  And it will certainly drive competition with the major players (OpenAI, Google, etc) and push them to innovate more which is starting to play out now.</div><br/><div id="35907695" class="c"><input type="checkbox" id="c-35907695" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905933">parent</a><span>|</span><a href="#35908934">next</a><span>|</span><label class="collapse" for="c-35907695">[-]</label><label class="expand" for="c-35907695">[9 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a really important reason to care about open source models: prompt engineering is fiddly enough without the risk of your model provider &quot;upgrading&quot; the model you are using in a way that breaks your existing prompts.<p>OpenAI already upset a lot of (admittedly non-paying academic) users when they shut off access to the old Ada code model with only a few week&#x27;s notice.</div><br/><div id="35907838" class="c"><input type="checkbox" id="c-35907838" checked=""/><div class="controls bullet"><span class="by">spacebanana7</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907695">parent</a><span>|</span><a href="#35908103">next</a><span>|</span><label class="collapse" for="c-35907838">[-]</label><label class="expand" for="c-35907838">[3 more]</label></div><br/><div class="children"><div class="content">I’m curious about how enterprises will manage model upgrades.<p>On one hand, as you mention, upgrades could break or degrade prompts in ways that are hard to fix. However, these models will need constant streams of updates for bugs and security fixes just like any other piece of software. Plus the temptation to get better performance.<p>The decisions around how and whether to upgrade LLMs will be much more complicated than upgrading Postgres versions.</div><br/><div id="35909300" class="c"><input type="checkbox" id="c-35909300" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907838">parent</a><span>|</span><a href="#35908103">next</a><span>|</span><label class="collapse" for="c-35909300">[-]</label><label class="expand" for="c-35909300">[2 more]</label></div><br/><div class="children"><div class="content">Why would the models themselves need security fixes? The software running the models, sure, but you should be able to upgrade that without changing anything observable about the actual model.</div><br/><div id="35912919" class="c"><input type="checkbox" id="c-35912919" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35909300">parent</a><span>|</span><a href="#35908103">next</a><span>|</span><label class="collapse" for="c-35912919">[-]</label><label class="expand" for="c-35912919">[1 more]</label></div><br/><div class="children"><div class="content">Not necessarily. The insecurity from LLMs comes from the fact they’re a black box - what if it turns out that particular version can be easily tricked into giving out terrorism ideas. You could try to add safeguards on top, but they’ve already been bypassed if it has been used for something like that. You might just have to retrain it somehow to make it safe</div><br/></div></div></div></div></div></div><div id="35908103" class="c"><input type="checkbox" id="c-35908103" checked=""/><div class="controls bullet"><span class="by">danysdragons</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907695">parent</a><span>|</span><a href="#35907838">prev</a><span>|</span><a href="#35908934">next</a><span>|</span><label class="collapse" for="c-35908103">[-]</label><label class="expand" for="c-35908103">[5 more]</label></div><br/><div class="children"><div class="content">The OpenAI APi has model checkpoints, right now the chat options are:<p>gpt-4
gpt-3-5-turbo
gpt-4-0314
gpt-3-5-turbo-0301</div><br/><div id="35912003" class="c"><input type="checkbox" id="c-35912003" checked=""/><div class="controls bullet"><span class="by">glandium</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908103">parent</a><span>|</span><a href="#35910725">next</a><span>|</span><label class="collapse" for="c-35912003">[-]</label><label class="expand" for="c-35912003">[1 more]</label></div><br/><div class="children"><div class="content">The 3.5 legacy model disappeared from the ChatGPT UI recently. Is it still available via the API?</div><br/></div></div><div id="35910725" class="c"><input type="checkbox" id="c-35910725" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908103">parent</a><span>|</span><a href="#35912003">prev</a><span>|</span><a href="#35912508">next</a><span>|</span><label class="collapse" for="c-35910725">[-]</label><label class="expand" for="c-35910725">[2 more]</label></div><br/><div class="children"><div class="content">Notably absent from the available model list is code-davinci-002 - a lot of people were burned by that one going away.</div><br/><div id="35912048" class="c"><input type="checkbox" id="c-35912048" checked=""/><div class="controls bullet"><span class="by">asey</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35910725">parent</a><span>|</span><a href="#35912508">next</a><span>|</span><label class="collapse" for="c-35912048">[-]</label><label class="expand" for="c-35912048">[1 more]</label></div><br/><div class="children"><div class="content">Those are ChatGPT models. The code-davinci-002 model is still available - they responded to community requests to keep it up.</div><br/></div></div></div></div><div id="35912508" class="c"><input type="checkbox" id="c-35912508" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908103">parent</a><span>|</span><a href="#35910725">prev</a><span>|</span><a href="#35908934">next</a><span>|</span><label class="collapse" for="c-35912508">[-]</label><label class="expand" for="c-35912508">[1 more]</label></div><br/><div class="children"><div class="content">Midjourney does this, as well.</div><br/></div></div></div></div></div></div><div id="35908934" class="c"><input type="checkbox" id="c-35908934" checked=""/><div class="controls bullet"><span class="by">lannisterstark</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905933">parent</a><span>|</span><a href="#35907695">prev</a><span>|</span><a href="#35906359">next</a><span>|</span><label class="collapse" for="c-35908934">[-]</label><label class="expand" for="c-35908934">[2 more]</label></div><br/><div class="children"><div class="content">&gt;I don&#x27;t think most of the large customers will care about OSS AI<p>Problem again, is centralization of LLMs by either the governments (and they always act in your best interest, amirite?) and corporation, which Non-FOSS LLMs prevent.<p>Democratization of the models is the only way to actually prevent bad actors from doing bad things.<p>&quot;But they&#x27;ll then have access to it too&quot; you say. Yes, they will, but given how many more people who will also have access to open LLMs we&#x27;d have tools to prevent actually malicious acts.</div><br/><div id="35910204" class="c"><input type="checkbox" id="c-35910204" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908934">parent</a><span>|</span><a href="#35906359">next</a><span>|</span><label class="collapse" for="c-35910204">[-]</label><label class="expand" for="c-35910204">[1 more]</label></div><br/><div class="children"><div class="content">A good guy with an LLM stops a bad guy with an LLM.
- This message brought to you by the National LLM Association</div><br/></div></div></div></div><div id="35906359" class="c"><input type="checkbox" id="c-35906359" checked=""/><div class="controls bullet"><span class="by">ibains</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905933">parent</a><span>|</span><a href="#35908934">prev</a><span>|</span><a href="#35908971">next</a><span>|</span><label class="collapse" for="c-35906359">[-]</label><label class="expand" for="c-35906359">[1 more]</label></div><br/><div class="children"><div class="content">A lot of B2B startups can technically the cloud API to provide value added applications to Enterprises, but often the banks and healthcare companies will not want their data running through startups pipes to OpenAI pipes.<p>We provide a low code data transformation product (prophecy.io), and we’ll never close sales at any volume, if we have a to get an MSA that approves this. Might get easier if we become large :)</div><br/></div></div><div id="35908971" class="c"><input type="checkbox" id="c-35908971" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905933">parent</a><span>|</span><a href="#35906359">prev</a><span>|</span><a href="#35906014">next</a><span>|</span><label class="collapse" for="c-35908971">[-]</label><label class="expand" for="c-35908971">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think most of the large customers will care about OSS AI.<p>OSS AI will open up more diverse and useful services than the first-party offerings from relatively risk averse major vendors, which customers *will&quot; care about.</div><br/></div></div><div id="35906014" class="c"><input type="checkbox" id="c-35906014" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905933">parent</a><span>|</span><a href="#35908971">prev</a><span>|</span><a href="#35907451">next</a><span>|</span><label class="collapse" for="c-35906014">[-]</label><label class="expand" for="c-35906014">[2 more]</label></div><br/><div class="children"><div class="content">Companies that aren&#x27;t mindful of vendor lock in aren&#x27;t long for the world.<p>Though those cloud platforms all have their own proprietary components most users are savvy enough to constrain and compartmentalize their use of them lest they find themselves having all their profits taken by a platform that knows it can set its prices arbitrarily.  The cloud vs in-house adoption is what it is in large part because the cloud offerings are a commodity and a big part of them being a commodity is that much of the underlying software is free software.</div><br/><div id="35906428" class="c"><input type="checkbox" id="c-35906428" checked=""/><div class="controls bullet"><span class="by">deltree7</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906014">parent</a><span>|</span><a href="#35907451">next</a><span>|</span><label class="collapse" for="c-35906428">[-]</label><label class="expand" for="c-35906428">[1 more]</label></div><br/><div class="children"><div class="content">History is littered with companies that went dead because they focused on things that don&#x27;t matter (open source, anti-microsoft, pro-linux).<p>There will be a time when those things matter when it hurts the bottom-line (Dropbox), but to prematurely optimize for that while you are finding product-market-fit is crazy and <i>all</i> companies are finding product-market-fit in the new AI era</div><br/></div></div></div></div><div id="35907451" class="c"><input type="checkbox" id="c-35907451" checked=""/><div class="controls bullet"><span class="by">ebiester</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905933">parent</a><span>|</span><a href="#35906014">prev</a><span>|</span><a href="#35906731">next</a><span>|</span><label class="collapse" for="c-35907451">[-]</label><label class="expand" for="c-35907451">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but I think for most companies this has more to do with cost. They&#x27;re not going to pay for the OSS model, and if they can use an OSS model + fine tuning, they&#x27;ll choose to save the money.</div><br/></div></div><div id="35906731" class="c"><input type="checkbox" id="c-35906731" checked=""/><div class="controls bullet"><span class="by">hdjjhhvvhga</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905933">parent</a><span>|</span><a href="#35907451">prev</a><span>|</span><a href="#35909374">next</a><span>|</span><label class="collapse" for="c-35906731">[-]</label><label class="expand" for="c-35906731">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think most of the large customers will care about OSS AI.<p>One would think the same in the 90s but yet, for some reason, Open Source prevailed and took over the world. I don&#x27;t believe it was about cost, at least not only. In my career I had to evaluate many technical solutions and products and OSS was often objectively superior at several levels without taking account the cost.<p>The first really successful alternative to &quot;Open&quot;AI will:<p>* gather many talented developers<p>* will quickly become a de facto standard solution<p>* people will rapidly start developing a wide range of integrations for it<p>* everybody will be using it, including large orgs, because, well, it&#x27;s open source</div><br/><div id="35906892" class="c"><input type="checkbox" id="c-35906892" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906731">parent</a><span>|</span><a href="#35906896">next</a><span>|</span><label class="collapse" for="c-35906892">[-]</label><label class="expand" for="c-35906892">[1 more]</label></div><br/><div class="children"><div class="content">True, but the difference here is that running a performant and capable AI solution will be infrastructure-dependent, which has real costs.</div><br/></div></div></div></div></div></div><div id="35909374" class="c"><input type="checkbox" id="c-35909374" checked=""/><div class="controls bullet"><span class="by">overgard</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905574">parent</a><span>|</span><a href="#35905933">prev</a><span>|</span><a href="#35905703">next</a><span>|</span><label class="collapse" for="c-35909374">[-]</label><label class="expand" for="c-35909374">[1 more]</label></div><br/><div class="children"><div class="content">Considering the very smart people asking for a moratorium on AI development, and it&#x27;s potential to disrupt a lot of jobs, this may be a good thing.</div><br/></div></div><div id="35905703" class="c"><input type="checkbox" id="c-35905703" checked=""/><div class="controls bullet"><span class="by">throwawayadvsec</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905574">parent</a><span>|</span><a href="#35909374">prev</a><span>|</span><a href="#35905422">next</a><span>|</span><label class="collapse" for="c-35905703">[-]</label><label class="expand" for="c-35905703">[22 more]</label></div><br/><div class="children"><div class="content">now that I think about it<p>is it that important to open source models that can only run on hardware worth tens of thousand of dollars?<p>who does that benefit besides their competitors and nefarious actors?<p>I&#x27;ve been trying to run one of the largest models for a while, unless 30,000$ falls in my hand I&#x27;ll probably never be able to run the current SOTA</div><br/><div id="35906002" class="c"><input type="checkbox" id="c-35906002" checked=""/><div class="controls bullet"><span class="by">RobotToaster</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35905795">next</a><span>|</span><label class="collapse" for="c-35906002">[-]</label><label class="expand" for="c-35906002">[1 more]</label></div><br/><div class="children"><div class="content">When linux was first released in 1991 a 386 to run it would cost about $2000.<p>We&#x27;ve already seen big advancements in tools to run them on lesser hardware.  It wouldn&#x27;t surprise me if we see some big advancements in the hardware to run them over the next few years, currently they are mostly being run of graphics processors that aren&#x27;t optimised for the task.</div><br/></div></div><div id="35905795" class="c"><input type="checkbox" id="c-35905795" checked=""/><div class="controls bullet"><span class="by">chrisco255</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35906002">prev</a><span>|</span><a href="#35908075">next</a><span>|</span><label class="collapse" for="c-35905795">[-]</label><label class="expand" for="c-35905795">[1 more]</label></div><br/><div class="children"><div class="content">&gt; is it that important to open source models that can only run on hardware worth tens of thousand of dollars?<p>Yes, because as we&#x27;ve seen with other open source AI models, it&#x27;s often possible for people to fork code and modify it in such a way that it runs on consumer grade hardware.</div><br/></div></div><div id="35908075" class="c"><input type="checkbox" id="c-35908075" checked=""/><div class="controls bullet"><span class="by">dfadsadsf</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35905795">prev</a><span>|</span><a href="#35908949">next</a><span>|</span><label class="collapse" for="c-35908075">[-]</label><label class="expand" for="c-35908075">[1 more]</label></div><br/><div class="children"><div class="content">$30000 is less than price of average car that Americans buy (and most families have two of them) - that&#x27;s definitely in the realm of something that affluent family can buy if it provides enough value. I also expect price to go down and at $10k it&#x27;s less than mid-range bathroom update. The question is only if it provides enough value or using in the cloud better option for almost all families.</div><br/></div></div><div id="35908949" class="c"><input type="checkbox" id="c-35908949" checked=""/><div class="controls bullet"><span class="by">lannisterstark</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35908075">prev</a><span>|</span><a href="#35905845">next</a><span>|</span><label class="collapse" for="c-35908949">[-]</label><label class="expand" for="c-35908949">[1 more]</label></div><br/><div class="children"><div class="content">&quot;It only benefits bad people&quot; is a pretty shitty argument at this point tbf. You can apply this logic to any expensive thing at this point.<p>I <i>can</i> for example, afford the hardware worth tens of thousands of dollars. I don&#x27;t want to, but I can if I needed to. Does that automagically make me their competitor or a bad actor?</div><br/></div></div><div id="35905845" class="c"><input type="checkbox" id="c-35905845" checked=""/><div class="controls bullet"><span class="by">iknowstuff</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35908949">prev</a><span>|</span><a href="#35905806">next</a><span>|</span><label class="collapse" for="c-35905845">[-]</label><label class="expand" for="c-35905845">[9 more]</label></div><br/><div class="children"><div class="content">Even a small startup, a researcher or a tinkerer can get a cloud instance with a beefy GPU. Also of note, Apple&#x27;s M1 Max&#x2F;Ultra should be be able to run it on their GPUs given their 64&#x2F;128GB of memory, right? That&#x27;s an order of magnitude cheaper.</div><br/><div id="35906644" class="c"><input type="checkbox" id="c-35906644" checked=""/><div class="controls bullet"><span class="by">mejutoco</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905845">parent</a><span>|</span><a href="#35906654">next</a><span>|</span><label class="collapse" for="c-35906644">[-]</label><label class="expand" for="c-35906644">[6 more]</label></div><br/><div class="children"><div class="content">I am confused. Those amounts are ram, not gpu ram, aren‘t they? Macs cpus are impressive, but not for ml. A most realistic one for a consumer is a 4090 rtx 24 GB. A lot of models do not fit in that, so A6000 48GB and over for some professional cards. That might be around 9000€ already.</div><br/><div id="35907614" class="c"><input type="checkbox" id="c-35907614" checked=""/><div class="controls bullet"><span class="by">piperswe</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906644">parent</a><span>|</span><a href="#35907589">next</a><span>|</span><label class="collapse" for="c-35907614">[-]</label><label class="expand" for="c-35907614">[4 more]</label></div><br/><div class="children"><div class="content">Apple Silicon has unified memory - all memory is accessible to both the CPU and GPU parts of the SoC.</div><br/><div id="35908041" class="c"><input type="checkbox" id="c-35908041" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907614">parent</a><span>|</span><a href="#35907589">next</a><span>|</span><label class="collapse" for="c-35908041">[-]</label><label class="expand" for="c-35908041">[3 more]</label></div><br/><div class="children"><div class="content">But they comes at max 32GB model?</div><br/><div id="35908849" class="c"><input type="checkbox" id="c-35908849" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908041">parent</a><span>|</span><a href="#35909092">next</a><span>|</span><label class="collapse" for="c-35908849">[-]</label><label class="expand" for="c-35908849">[1 more]</label></div><br/><div class="children"><div class="content">Mac Studio (desktop) is up to 128GB, and Macbook Pro is up to 96GB.</div><br/></div></div></div></div></div></div><div id="35907589" class="c"><input type="checkbox" id="c-35907589" checked=""/><div class="controls bullet"><span class="by">codedokode</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906644">parent</a><span>|</span><a href="#35907614">prev</a><span>|</span><a href="#35906654">next</a><span>|</span><label class="collapse" for="c-35907589">[-]</label><label class="expand" for="c-35907589">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Macs cpus are impressive, but not for ml<p>On Mac GPU has access to all memory.</div><br/></div></div></div></div><div id="35906654" class="c"><input type="checkbox" id="c-35906654" checked=""/><div class="controls bullet"><span class="by">himlion</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905845">parent</a><span>|</span><a href="#35906644">prev</a><span>|</span><a href="#35905806">next</a><span>|</span><label class="collapse" for="c-35906654">[-]</label><label class="expand" for="c-35906654">[2 more]</label></div><br/><div class="children"><div class="content">I overlooked the unified memory on those machines. Can it really run this performantly?</div><br/><div id="35912515" class="c"><input type="checkbox" id="c-35912515" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906654">parent</a><span>|</span><a href="#35905806">next</a><span>|</span><label class="collapse" for="c-35912515">[-]</label><label class="expand" for="c-35912515">[1 more]</label></div><br/><div class="children"><div class="content">I run Vicuna quite well with my M1 Pro, 32GB.</div><br/></div></div></div></div></div></div><div id="35905806" class="c"><input type="checkbox" id="c-35905806" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35905845">prev</a><span>|</span><a href="#35908906">next</a><span>|</span><label class="collapse" for="c-35905806">[-]</label><label class="expand" for="c-35905806">[4 more]</label></div><br/><div class="children"><div class="content">I agree utility of open source for personal usecase is overblown.<p>But for commercial usecases, open source is very relevant for privacy reasons as many enterprises have strict policy not to share data with third party. Also it could be a lot cheaper for bulk inference or to have a small model for particular task.</div><br/><div id="35908172" class="c"><input type="checkbox" id="c-35908172" checked=""/><div class="controls bullet"><span class="by">turtles3</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905806">parent</a><span>|</span><a href="#35910467">next</a><span>|</span><label class="collapse" for="c-35908172">[-]</label><label class="expand" for="c-35908172">[2 more]</label></div><br/><div class="children"><div class="content">However, the same thing could be achieved with closed source models. There&#x27;s nothing to stop an LLM being made available to run on prem under a restrictive license. It would really be no different to ye olde desktop software - keeping ownership over bits shipped to a customer is solved with the law rather than technical means.<p>That said, I really hope open source models can succeed, it would be far better for the industry if we had a Linux of LLMs.</div><br/><div id="35909004" class="c"><input type="checkbox" id="c-35909004" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908172">parent</a><span>|</span><a href="#35910467">next</a><span>|</span><label class="collapse" for="c-35909004">[-]</label><label class="expand" for="c-35909004">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Keeping ownership over bits shipped to a customer is solved with the law rather than technical means.<p>Yes in theory... In practice, what happened with LLaMA showed people will copy and distribute weights while ignoring the license.</div><br/></div></div></div></div><div id="35910467" class="c"><input type="checkbox" id="c-35910467" checked=""/><div class="controls bullet"><span class="by">unethical_ban</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905806">parent</a><span>|</span><a href="#35908172">prev</a><span>|</span><a href="#35908906">next</a><span>|</span><label class="collapse" for="c-35910467">[-]</label><label class="expand" for="c-35910467">[1 more]</label></div><br/><div class="children"><div class="content">Locally hosted instances that don&#x27;t report on prompts is important for personal privacy.</div><br/></div></div></div></div><div id="35908906" class="c"><input type="checkbox" id="c-35908906" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35905806">prev</a><span>|</span><a href="#35905920">next</a><span>|</span><label class="collapse" for="c-35908906">[-]</label><label class="expand" for="c-35908906">[1 more]</label></div><br/><div class="children"><div class="content">It will create price competition for different providers of the model though, which should drive down prices</div><br/></div></div><div id="35905920" class="c"><input type="checkbox" id="c-35905920" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35908906">prev</a><span>|</span><a href="#35908118">next</a><span>|</span><label class="collapse" for="c-35905920">[-]</label><label class="expand" for="c-35905920">[1 more]</label></div><br/><div class="children"><div class="content">Yes, because it can always be down ported by people with more constraints than the original authors. We’ve see a lot of this in the LLM space, and a lot of other OSS efforts.</div><br/></div></div><div id="35908118" class="c"><input type="checkbox" id="c-35908118" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905703">parent</a><span>|</span><a href="#35905920">prev</a><span>|</span><a href="#35905422">next</a><span>|</span><label class="collapse" for="c-35908118">[-]</label><label class="expand" for="c-35908118">[2 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t only run on high end systems.  Good models can run on a desktop you have at home.  If you don&#x27;t have a desktop... I&#x27;m not sure what you&#x27;re doing on HN.</div><br/><div id="35912572" class="c"><input type="checkbox" id="c-35912572" checked=""/><div class="controls bullet"><span class="by">throwawayadvsec</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908118">parent</a><span>|</span><a href="#35905422">next</a><span>|</span><label class="collapse" for="c-35912572">[-]</label><label class="expand" for="c-35912572">[1 more]</label></div><br/><div class="children"><div class="content">You have a weird definition of &quot;good model&quot;<p>Llama 7B is NOT a good model.</div><br/></div></div></div></div></div></div></div></div><div id="35905422" class="c"><input type="checkbox" id="c-35905422" checked=""/><div class="controls bullet"><span class="by">danenania</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35905574">prev</a><span>|</span><a href="#35912621">next</a><span>|</span><label class="collapse" for="c-35905422">[-]</label><label class="expand" for="c-35905422">[9 more]</label></div><br/><div class="children"><div class="content">One question is how much other factors really matter compared to the raw &quot;intelligence&quot; of the model--how good its completions are. You&#x27;re not going to care very much about context window, prompting, or integrations if the output isn&#x27;t good. It would be sort of like a car that has the best steering and brakes on the market, but can&#x27;t go above 5 mph.</div><br/><div id="35908460" class="c"><input type="checkbox" id="c-35908460" checked=""/><div class="controls bullet"><span class="by">modernpink</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905422">parent</a><span>|</span><a href="#35905606">next</a><span>|</span><label class="collapse" for="c-35908460">[-]</label><label class="expand" for="c-35908460">[1 more]</label></div><br/><div class="children"><div class="content">Or rather, more analogously, a self-driving car that has a range of 10 000 miles but sometimes makes mistakes when driving vs a self-driving car with a range of 800 miles that never makes mistakes. Once you&#x27;ve have a taste of intelligence it&#x27;s hard to give up.<p>However, in many applications there is a limit on how intelligent you need the LLM to be. I have found I am able to fall back to the cheaper and faster GPT-3.5 to do the grunt work of forming text blobs into structured json within a chain involving GPT-4 for higher-level functions.</div><br/></div></div><div id="35905606" class="c"><input type="checkbox" id="c-35905606" checked=""/><div class="controls bullet"><span class="by">majormajor</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905422">parent</a><span>|</span><a href="#35908460">prev</a><span>|</span><a href="#35905890">next</a><span>|</span><label class="collapse" for="c-35905606">[-]</label><label class="expand" for="c-35905606">[2 more]</label></div><br/><div class="children"><div class="content">Big question on that for me is that there&#x27;s a variety of &quot;completion styles&quot; and I&#x27;m curious how &quot;universal&quot; performance on them is. Probably more than this, but a quick list that comes to mind:<p>* Text summary&#x2F;compression<p>* Creative writing (fiction&#x2F;lyrics&#x2F;stylization)<p>* Text comparison<p>* Question-answering<p>* Logical reasoning&#x2F;sequencing (&quot;given these tools and this scenario, how would you perform this task&quot;)<p>IMO, for stuff like text comparison and question-answering, some combo of speed&#x2F;cost&#x2F;context-size could make up for a lot, even if they do &quot;worse&quot; versions of stuff just that&#x27;s too slow or expensive or context-limited in a different model.</div><br/><div id="35913218" class="c"><input type="checkbox" id="c-35913218" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905606">parent</a><span>|</span><a href="#35905890">next</a><span>|</span><label class="collapse" for="c-35913218">[-]</label><label class="expand" for="c-35913218">[1 more]</label></div><br/><div class="children"><div class="content">We already see how that goes. Stackable LoRa like in stable diffusion.</div><br/></div></div></div></div><div id="35905890" class="c"><input type="checkbox" id="c-35905890" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905422">parent</a><span>|</span><a href="#35905606">prev</a><span>|</span><a href="#35905459">next</a><span>|</span><label class="collapse" for="c-35905890">[-]</label><label class="expand" for="c-35905890">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know. While using Phind I regularly get annoyed by long prose that doesnt answer anything (yes, &quot;concise&quot; is always on). Claude seems to be directly geared towards solving stuff over nice writing.</div><br/><div id="35907159" class="c"><input type="checkbox" id="c-35907159" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905890">parent</a><span>|</span><a href="#35905459">next</a><span>|</span><label class="collapse" for="c-35907159">[-]</label><label class="expand" for="c-35907159">[1 more]</label></div><br/><div class="children"><div class="content">I generally add to my initial prompts to GPT4 to: From now on, please use the fewest tokens possible in all replies to save tokens and provide brief and accurate answers.</div><br/></div></div></div></div><div id="35905459" class="c"><input type="checkbox" id="c-35905459" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905422">parent</a><span>|</span><a href="#35905890">prev</a><span>|</span><a href="#35912621">next</a><span>|</span><label class="collapse" for="c-35905459">[-]</label><label class="expand" for="c-35905459">[3 more]</label></div><br/><div class="children"><div class="content">Strongly agree. They are ordered by how much I think they generally will lead to users choosing one model over the other.<p>Intelligence is the most important dimension by far, perhaps an order of magnitude or more above the second item on the list.</div><br/><div id="35905639" class="c"><input type="checkbox" id="c-35905639" checked=""/><div class="controls bullet"><span class="by">danenania</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905459">parent</a><span>|</span><a href="#35912621">next</a><span>|</span><label class="collapse" for="c-35905639">[-]</label><label class="expand" for="c-35905639">[2 more]</label></div><br/><div class="children"><div class="content">On that note, can anyone speak to how Anthropic (or other models) are doing on catching up to OpenAI for pure model intelligence&#x2F;quality of completions? Are any others approaching GPT-4? I&#x27;ve only used GPT-based tools so I have no idea.</div><br/><div id="35906239" class="c"><input type="checkbox" id="c-35906239" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905639">parent</a><span>|</span><a href="#35912621">next</a><span>|</span><label class="collapse" for="c-35906239">[-]</label><label class="expand" for="c-35906239">[1 more]</label></div><br/><div class="children"><div class="content">The best claude model is closer to GPT-4 than 3.5</div><br/></div></div></div></div></div></div></div></div><div id="35912621" class="c"><input type="checkbox" id="c-35912621" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35905422">prev</a><span>|</span><a href="#35905981">next</a><span>|</span><label class="collapse" for="c-35912621">[-]</label><label class="expand" for="c-35912621">[1 more]</label></div><br/><div class="children"><div class="content">Privacy as well! I&#x27;d much rather run a model locally than in the cloud even if I lose out in performance in both speed and quality.</div><br/></div></div><div id="35905981" class="c"><input type="checkbox" id="c-35905981" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35912621">prev</a><span>|</span><a href="#35906259">next</a><span>|</span><label class="collapse" for="c-35905981">[-]</label><label class="expand" for="c-35905981">[8 more]</label></div><br/><div class="children"><div class="content">Faster, cheaper fine-tuning and training<p>If I could train a useful model, on my own data, in a reasonable time<p>I would want to have a CI-training pipeline to always have my models up to date</div><br/><div id="35912787" class="c"><input type="checkbox" id="c-35912787" checked=""/><div class="controls bullet"><span class="by">hooande</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905981">parent</a><span>|</span><a href="#35912079">next</a><span>|</span><label class="collapse" for="c-35912787">[-]</label><label class="expand" for="c-35912787">[1 more]</label></div><br/><div class="children"><div class="content">Just use a machine learning method other than an LLM. If you&#x27;re going to go to the effort of fine tuning and training, pay people to collect and label data like we used to do before Feb 2023.<p>If your main concern is question answering or summarization or code completion there are plenty of ways to do that now. If you really require the advanced emergent properties of LLMs, you&#x27;ll have to work with a company that can afford to train a transformer on the Entire Internet.</div><br/></div></div><div id="35912079" class="c"><input type="checkbox" id="c-35912079" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905981">parent</a><span>|</span><a href="#35912787">prev</a><span>|</span><a href="#35906110">next</a><span>|</span><label class="collapse" for="c-35912079">[-]</label><label class="expand" for="c-35912079">[1 more]</label></div><br/><div class="children"><div class="content">Open-source LLM projects have largely solved this using Low-Rank Adaptation of Large Language Models (LoRA): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.09685" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.09685</a><p>Apparently an RTX 4090 running overnight is sufficient to produce a fine-tuned model that can spit out new Harry Potter stories, or whatever...</div><br/></div></div><div id="35906110" class="c"><input type="checkbox" id="c-35906110" checked=""/><div class="controls bullet"><span class="by">makestuff</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905981">parent</a><span>|</span><a href="#35912079">prev</a><span>|</span><a href="#35906950">next</a><span>|</span><label class="collapse" for="c-35906110">[-]</label><label class="expand" for="c-35906110">[4 more]</label></div><br/><div class="children"><div class="content">Yeah I remember in undergrad I was working on using transformation learning to train an object detector. Basically you only needed 100ish images to get the model to detect that new object really well.<p>I&#x27;m not sure what the analogous term is for a similar process on LLMs, but that will be huge when there is a service for it.</div><br/><div id="35907099" class="c"><input type="checkbox" id="c-35907099" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906110">parent</a><span>|</span><a href="#35908751">next</a><span>|</span><label class="collapse" for="c-35907099">[-]</label><label class="expand" for="c-35907099">[2 more]</label></div><br/><div class="children"><div class="content">LLMs can do that without any examples (zero shot) or with one or a few demonstrations in the prompt, if you can describe the task in the limited context window.<p>If you want for example to train the model to learn to use a very large API, or access the knowledge in a whole book, it might need fine-tuning.</div><br/><div id="35908145" class="c"><input type="checkbox" id="c-35908145" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907099">parent</a><span>|</span><a href="#35908751">next</a><span>|</span><label class="collapse" for="c-35908145">[-]</label><label class="expand" for="c-35908145">[1 more]</label></div><br/><div class="children"><div class="content">Could I just train a very small LLM with an English dictionary + Python + large API documentation + large Python code base?<p>Then do some chat fine tuning (like what HF did with StarCoder to get ChatCoder)<p>And get a lightweight LLM that knows the docs and code for the thing I need it for<p>After that, maybe incrementally fine tune the model as part of your CI&#x2F;CD process</div><br/></div></div></div></div><div id="35908751" class="c"><input type="checkbox" id="c-35908751" checked=""/><div class="controls bullet"><span class="by">toss1</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906110">parent</a><span>|</span><a href="#35907099">prev</a><span>|</span><a href="#35906950">next</a><span>|</span><label class="collapse" for="c-35908751">[-]</label><label class="expand" for="c-35908751">[1 more]</label></div><br/><div class="children"><div class="content">How similar were the object to other objects?<p>E.g., were you trying to distinguish an object vs nothing, a bicycle vs a fish, a bird vs a squirrel, or two different species of songbird at a feeder?<p>How much would the training requirements increase or decrease moving up or down that scale?</div><br/></div></div></div></div><div id="35906950" class="c"><input type="checkbox" id="c-35906950" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905981">parent</a><span>|</span><a href="#35906110">prev</a><span>|</span><a href="#35906259">next</a><span>|</span><label class="collapse" for="c-35906950">[-]</label><label class="expand" for="c-35906950">[1 more]</label></div><br/><div class="children"><div class="content">The PaLM 2 stuff released yesterday has fine tuning for their newest large models as a core feature.</div><br/></div></div></div></div><div id="35906259" class="c"><input type="checkbox" id="c-35906259" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35905981">prev</a><span>|</span><a href="#35908324">next</a><span>|</span><label class="collapse" for="c-35906259">[-]</label><label class="expand" for="c-35906259">[7 more]</label></div><br/><div class="children"><div class="content">Until they actually make any of it available in anything but an obscure expensive API you have to request access to, they might as well not even exist.</div><br/><div id="35907353" class="c"><input type="checkbox" id="c-35907353" checked=""/><div class="controls bullet"><span class="by">williamstein</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906259">parent</a><span>|</span><a href="#35906720">next</a><span>|</span><label class="collapse" for="c-35907353">[-]</label><label class="expand" for="c-35907353">[4 more]</label></div><br/><div class="children"><div class="content">The landing page says &quot;Easy integration via standard APIs Claude can be incorporated into any product or toolchain you’re building with minimal effort.&quot; Then there is a big button &quot;Request Access&quot;, which for me right now just does nothing.   OpenAI has really faced the pain to make their product available via an API to the general public at scale, but Anthropic&#x2F;Google&#x2F;etc. don&#x27;t quite seem to be there yet. It&#x27;s frustrating.</div><br/><div id="35908098" class="c"><input type="checkbox" id="c-35908098" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907353">parent</a><span>|</span><a href="#35907648">next</a><span>|</span><label class="collapse" for="c-35908098">[-]</label><label class="expand" for="c-35908098">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the person you&#x27;re responding to wants a network based or cloud based solution.<p>When someone says they want it available they mean running on their own device.<p>This is hackernews, nearly everyone on this site should have their own self hosted LLM running on a computer&#x2F;server or device they have at their house.<p>Relying on &#x27;the cloud&#x27; for everything makes us worse developers in just about every imaginable way, creates a ton of completely unnecessary and complicated source code, and creates far too many calls to the internet which are unnecessary.  Using local hard drives for example is thousands of times faster than using cloud data storage, and we should take advantage of that in the software we write.  So instead of making billions of calls to download a terabyte database query-by-query (seen this &#x27;industry-standard&#x27; far too many times), maybe make <i>one</i> call and build it locally.  This is effectively the same problem in LLMs&#x2F;ML in general, and the same incredible stupidity is being followed.
Download the model once, run your queries locally. That&#x27;s the solution we should be using.</div><br/></div></div><div id="35907648" class="c"><input type="checkbox" id="c-35907648" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907353">parent</a><span>|</span><a href="#35908098">prev</a><span>|</span><a href="#35910093">next</a><span>|</span><label class="collapse" for="c-35907648">[-]</label><label class="expand" for="c-35907648">[1 more]</label></div><br/><div class="children"><div class="content">Try a browser or a clean profile without any ad blocking turned on. It took me a couple of tries to figure out how to get it working but you should see a modal with a form when it works.<p>FYI the waitlist form submits a regular POST request so it&#x27;ll reload the main page instead of closing the modal dialog. I opened network monitor with preserved logs to double check that I made it on the list :facepalm:</div><br/></div></div><div id="35910093" class="c"><input type="checkbox" id="c-35910093" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35907353">parent</a><span>|</span><a href="#35907648">prev</a><span>|</span><a href="#35906720">next</a><span>|</span><label class="collapse" for="c-35910093">[-]</label><label class="expand" for="c-35910093">[1 more]</label></div><br/><div class="children"><div class="content">Google models are now available as API on Google Cloud.</div><br/></div></div></div></div><div id="35906720" class="c"><input type="checkbox" id="c-35906720" checked=""/><div class="controls bullet"><span class="by">dkarras</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906259">parent</a><span>|</span><a href="#35907353">prev</a><span>|</span><a href="#35906570">next</a><span>|</span><label class="collapse" for="c-35906720">[-]</label><label class="expand" for="c-35906720">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using it through poe and I prefer it to ChatGPT but can&#x27;t pinpoint why. It just &quot;gets&quot; me better I guess?</div><br/></div></div><div id="35906570" class="c"><input type="checkbox" id="c-35906570" checked=""/><div class="controls bullet"><span class="by">r_thambapillai</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35906259">parent</a><span>|</span><a href="#35906720">prev</a><span>|</span><a href="#35908324">next</a><span>|</span><label class="collapse" for="c-35906570">[-]</label><label class="expand" for="c-35906570">[1 more]</label></div><br/><div class="children"><div class="content">there are many services that integrate with them that would allow you to self-serve signup</div><br/></div></div></div></div><div id="35908324" class="c"><input type="checkbox" id="c-35908324" checked=""/><div class="controls bullet"><span class="by">ianhawes</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35906259">prev</a><span>|</span><a href="#35906791">next</a><span>|</span><label class="collapse" for="c-35908324">[-]</label><label class="expand" for="c-35908324">[1 more]</label></div><br/><div class="children"><div class="content">We use Anthropic Instant in production and it has been much faster than Davinci&#x2F;GPT4 for awhile. In terms of quality, Instant is at least as good as GPT3.5.</div><br/></div></div><div id="35906264" class="c"><input type="checkbox" id="c-35906264" checked=""/><div class="controls bullet"><span class="by">winstonprivacy</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35906791">prev</a><span>|</span><a href="#35905413">next</a><span>|</span><label class="collapse" for="c-35906264">[-]</label><label class="expand" for="c-35906264">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget the ability to fine tune based on one&#x27;s own data sources. For me, this is more important than any of the six reasons you mentioned.</div><br/></div></div><div id="35905413" class="c"><input type="checkbox" id="c-35905413" checked=""/><div class="controls bullet"><span class="by">zomglings</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35906264">prev</a><span>|</span><a href="#35905598">next</a><span>|</span><label class="collapse" for="c-35905413">[-]</label><label class="expand" for="c-35905413">[1 more]</label></div><br/><div class="children"><div class="content">Also if you allow users to receive vector representations of context and provide such representations as side information when querying LLMs.</div><br/></div></div><div id="35905598" class="c"><input type="checkbox" id="c-35905598" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35905413">prev</a><span>|</span><a href="#35908016">next</a><span>|</span><label class="collapse" for="c-35905598">[-]</label><label class="expand" for="c-35905598">[3 more]</label></div><br/><div class="children"><div class="content">For me I’d say speed trumps all else. It’s impossible to truly reach scale with the glacial response times you get from current API.</div><br/><div id="35905650" class="c"><input type="checkbox" id="c-35905650" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905598">parent</a><span>|</span><a href="#35908016">next</a><span>|</span><label class="collapse" for="c-35905650">[-]</label><label class="expand" for="c-35905650">[2 more]</label></div><br/><div class="children"><div class="content">&gt;speed trumps all else<p>Then use GPT-2</div><br/><div id="35906852" class="c"><input type="checkbox" id="c-35906852" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35905650">parent</a><span>|</span><a href="#35908016">next</a><span>|</span><label class="collapse" for="c-35906852">[-]</label><label class="expand" for="c-35906852">[1 more]</label></div><br/><div class="children"><div class="content">I actually do prefer 3.5-turbo over 4 for many tasks.</div><br/></div></div></div></div></div></div><div id="35908016" class="c"><input type="checkbox" id="c-35908016" checked=""/><div class="controls bullet"><span class="by">jll29</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35905598">prev</a><span>|</span><a href="#35907917">next</a><span>|</span><label class="collapse" for="c-35908016">[-]</label><label class="expand" for="c-35908016">[2 more]</label></div><br/><div class="children"><div class="content">More languages?</div><br/><div id="35910080" class="c"><input type="checkbox" id="c-35910080" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35905301">root</a><span>|</span><a href="#35908016">parent</a><span>|</span><a href="#35907917">next</a><span>|</span><label class="collapse" for="c-35910080">[-]</label><label class="expand" for="c-35910080">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. This is an important feature. Not all people speak English.</div><br/></div></div></div></div><div id="35907917" class="c"><input type="checkbox" id="c-35907917" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#35905301">parent</a><span>|</span><a href="#35908016">prev</a><span>|</span><a href="#35905541">next</a><span>|</span><label class="collapse" for="c-35907917">[-]</label><label class="expand" for="c-35907917">[1 more]</label></div><br/><div class="children"><div class="content">Reliability surely? They still haven&#x27;t managed to make a model that says &quot;I don&#x27;t know&quot; rather than bullshitting. That&#x27;s by far the biggest unsolved problem.</div><br/></div></div></div></div><div id="35905189" class="c"><input type="checkbox" id="c-35905189" checked=""/><div class="controls bullet"><span class="by">meghan_rain</span><span>|</span><a href="#35905301">prev</a><span>|</span><a href="#35905308">next</a><span>|</span><label class="collapse" for="c-35905189">[-]</label><label class="expand" for="c-35905189">[14 more]</label></div><br/><div class="children"><div class="content">The most interesting bit is that for the first time since the release of ChatGPT in December 2022, OpenAI does not have the lead on LLMs anymore.<p>At least, for people who need large context windows, they would not be the first choice anymore.</div><br/><div id="35908221" class="c"><input type="checkbox" id="c-35908221" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35905189">parent</a><span>|</span><a href="#35905452">next</a><span>|</span><label class="collapse" for="c-35908221">[-]</label><label class="expand" for="c-35908221">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 still leads in the chatbot arena[1] but at least it is a two horse race now.<p>[1] <a href="https:&#x2F;&#x2F;lmsys.org&#x2F;blog&#x2F;2023-05-10-leaderboard&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmsys.org&#x2F;blog&#x2F;2023-05-10-leaderboard&#x2F;</a></div><br/></div></div><div id="35905452" class="c"><input type="checkbox" id="c-35905452" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#35905189">parent</a><span>|</span><a href="#35908221">prev</a><span>|</span><a href="#35911536">next</a><span>|</span><label class="collapse" for="c-35905452">[-]</label><label class="expand" for="c-35905452">[11 more]</label></div><br/><div class="children"><div class="content">Claude’s very quietly better on everything but pricing, for a while, it just got buried because they announced on “AI Tuesday” (iirc gpt4 and Bing announcement day)<p>The ChatGPT equivalent is 3x speed and was somewhere between ChatGPT and GPT4 on my TriviaQA benchmark replication I did<p>Couple tweets with data and examples. Note they’re from 8 weeks ago, I know Claude got a version bump, GPT3.5&#x2F;4 accessible via API seem the same.<p>[1] brief and graphical summary of speed and TriviaQA <a href="https:&#x2F;&#x2F;twitter.com&#x2F;jpohhhh&#x2F;status&#x2F;1638362982131351552?s=46&amp;t=pxb6XO8RvMiZoZE1x8d58Q" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;jpohhhh&#x2F;status&#x2F;1638362982131351552?s=46&amp;...</a><p>[2] ad hoc side by sides <a href="https:&#x2F;&#x2F;twitter.com&#x2F;jpohhhh&#x2F;status&#x2F;1637316127314305024?s=46&amp;t=pxb6XO8RvMiZoZE1x8d58Q" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;jpohhhh&#x2F;status&#x2F;1637316127314305024?s=46&amp;...</a></div><br/><div id="35906870" class="c"><input type="checkbox" id="c-35906870" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35905452">parent</a><span>|</span><a href="#35906991">next</a><span>|</span><label class="collapse" for="c-35906870">[-]</label><label class="expand" for="c-35906870">[6 more]</label></div><br/><div class="children"><div class="content">&gt; I know Claude got a version bump, GPT3.5&#x2F;4 accessible via API seem the same.<p>GPT3.5 just got an update a few days ago that resulted in a pretty good improvement on its creativity. I saved some sample outputs from the previous March model, and for the same prompt the difference is quite dramatic. Prose is much less formulaic overall.</div><br/><div id="35911891" class="c"><input type="checkbox" id="c-35911891" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35906870">parent</a><span>|</span><a href="#35907846">next</a><span>|</span><label class="collapse" for="c-35911891">[-]</label><label class="expand" for="c-35911891">[1 more]</label></div><br/><div class="children"><div class="content">Meanwhile GPT 4 got lobotomised around the same time.<p>It can&#x27;t even solve simplified versions of problems it had zero issues with just a week ago.</div><br/></div></div><div id="35907846" class="c"><input type="checkbox" id="c-35907846" checked=""/><div class="controls bullet"><span class="by">ndr_</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35906870">parent</a><span>|</span><a href="#35911891">prev</a><span>|</span><a href="#35907419">next</a><span>|</span><label class="collapse" for="c-35907846">[-]</label><label class="expand" for="c-35907846">[2 more]</label></div><br/><div class="children"><div class="content">Is this update made visible somewhere? The language models offered on my Playground are still the ones from March, same with ChatGPT.</div><br/><div id="35911684" class="c"><input type="checkbox" id="c-35911684" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35907846">parent</a><span>|</span><a href="#35907419">next</a><span>|</span><label class="collapse" for="c-35911684">[-]</label><label class="expand" for="c-35911684">[1 more]</label></div><br/><div class="children"><div class="content">On chat.openai.com below text box there is a link to ChatGPT release notes. Current link text is &quot;ChatGPT May 3 Version&quot;. The link leads to <a href="https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;6825453-chatgpt-release-notes" rel="nofollow">https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;6825453-chatgpt-release-...</a></div><br/></div></div></div></div><div id="35907419" class="c"><input type="checkbox" id="c-35907419" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35906870">parent</a><span>|</span><a href="#35907846">prev</a><span>|</span><a href="#35906991">next</a><span>|</span><label class="collapse" for="c-35907419">[-]</label><label class="expand" for="c-35907419">[2 more]</label></div><br/><div class="children"><div class="content">Thank you, every little comment I get from fellow boots on the ground is so valuable, lotta noise these days.<p>Random Q, I don’t use the ChatGPT front end much past month or two, used it a week back and it seemed blazingly faster than my integration: do you have a sense of if it got faster too?</div><br/><div id="35912530" class="c"><input type="checkbox" id="c-35912530" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35907419">parent</a><span>|</span><a href="#35906991">next</a><span>|</span><label class="collapse" for="c-35912530">[-]</label><label class="expand" for="c-35912530">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I&#x27;ve noticed the front end for 3.5 responds almost instantly to complex questions. It&#x27;ll pop out an entire page of code in under a couple seconds. I think API responses may be a bit faster than before, some of my queries that took 30s before now take around 20s, but they are obviously prioritizing their own site.</div><br/></div></div></div></div></div></div><div id="35906991" class="c"><input type="checkbox" id="c-35906991" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35905452">parent</a><span>|</span><a href="#35906870">prev</a><span>|</span><a href="#35911536">next</a><span>|</span><label class="collapse" for="c-35906991">[-]</label><label class="expand" for="c-35906991">[4 more]</label></div><br/><div class="children"><div class="content">How is the code generation of Claude?</div><br/><div id="35907407" class="c"><input type="checkbox" id="c-35907407" checked=""/><div class="controls bullet"><span class="by">technics256</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35906991">parent</a><span>|</span><a href="#35907467">next</a><span>|</span><label class="collapse" for="c-35907407">[-]</label><label class="expand" for="c-35907407">[1 more]</label></div><br/><div class="children"><div class="content">I have access to claude. It&#x27;s not bad, but decently behind gpt4 for code</div><br/></div></div><div id="35907467" class="c"><input type="checkbox" id="c-35907467" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35906991">parent</a><span>|</span><a href="#35907407">prev</a><span>|</span><a href="#35907328">next</a><span>|</span><label class="collapse" for="c-35907467">[-]</label><label class="expand" for="c-35907467">[1 more]</label></div><br/><div class="children"><div class="content">Note, all impressions based on Claude 1.2, got an email from Poe in the last week saying it was version bumped to 1.3 with a focus on coding improvements.<p>Impressions:<p>Bad enough compared to GPT-4 that I default to GPT-4. I think if I had api access I’d use it instead, right now it requires more coaxing, and using Poe.<p>I did find “long-term” chats went better, was really impressed with how it held up when I was asking it a nasty problem that was hard to even communicate verbally. Wrong at first, but as I conversed it was a real conversation.<p>GPT4 seems to circle a lower optima. My academic guess it’s what Anthropic calls it “sycophancy” in its papers, tldr GPT really really wants to do more like what’s in the context, so the longer the conversation with initial errors goes, it’s actually harder to talk it out of the errors.</div><br/></div></div><div id="35907328" class="c"><input type="checkbox" id="c-35907328" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#35905189">root</a><span>|</span><a href="#35906991">parent</a><span>|</span><a href="#35907467">prev</a><span>|</span><a href="#35911536">next</a><span>|</span><label class="collapse" for="c-35907328">[-]</label><label class="expand" for="c-35907328">[1 more]</label></div><br/><div class="children"><div class="content">And is code generation ability equivalent to code understanding and search ability?</div><br/></div></div></div></div></div></div><div id="35911536" class="c"><input type="checkbox" id="c-35911536" checked=""/><div class="controls bullet"><span class="by">speedylight</span><span>|</span><a href="#35905189">parent</a><span>|</span><a href="#35905452">prev</a><span>|</span><a href="#35905308">next</a><span>|</span><label class="collapse" for="c-35911536">[-]</label><label class="expand" for="c-35911536">[1 more]</label></div><br/><div class="children"><div class="content">Well Google decided to stop releasing their AI research.</div><br/></div></div></div></div><div id="35905308" class="c"><input type="checkbox" id="c-35905308" checked=""/><div class="controls bullet"><span class="by">justanotheratom</span><span>|</span><a href="#35905189">prev</a><span>|</span><a href="#35907654">next</a><span>|</span><label class="collapse" for="c-35905308">[-]</label><label class="expand" for="c-35905308">[40 more]</label></div><br/><div class="children"><div class="content">This is nice, but it can get quite expensive.<p>Let&#x27;s say I have a book and I want to ask multiple questions about it. Every query will pay the price of the book&#x27;s text. It would be awesome if I could &quot;index&quot; the book once, i.e. pay for the context once, and then ask multiple questions.</div><br/><div id="35905533" class="c"><input type="checkbox" id="c-35905533" checked=""/><div class="controls bullet"><span class="by">pyth0</span><span>|</span><a href="#35905308">parent</a><span>|</span><a href="#35905375">next</a><span>|</span><label class="collapse" for="c-35905533">[-]</label><label class="expand" for="c-35905533">[6 more]</label></div><br/><div class="children"><div class="content">This more or less is already a thing and it&#x27;s called RAG [1][2]. It essentially allows you to have a database of embeddings (in this case your book) from which a model can pull knowledge from while producing answers. As for the standard operation of these generative models, the context window is the only working memory it has and so it must see the entire text each time.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.11401" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.11401</a><p>[2] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;model_doc&#x2F;rag" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;model_doc&#x2F;rag</a></div><br/><div id="35907330" class="c"><input type="checkbox" id="c-35907330" checked=""/><div class="controls bullet"><span class="by">m1sta_</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905533">parent</a><span>|</span><a href="#35905375">next</a><span>|</span><label class="collapse" for="c-35907330">[-]</label><label class="expand" for="c-35907330">[5 more]</label></div><br/><div class="children"><div class="content">Cam you help me understand this? The research appears to be from a few years ago. Can this be used with Claude (for example)? How is it different to the approach many people are taking with vector stores and embeddings?</div><br/><div id="35907562" class="c"><input type="checkbox" id="c-35907562" checked=""/><div class="controls bullet"><span class="by">pyth0</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35907330">parent</a><span>|</span><a href="#35907479">next</a><span>|</span><label class="collapse" for="c-35907562">[-]</label><label class="expand" for="c-35907562">[2 more]</label></div><br/><div class="children"><div class="content">Other people seem to be suggesting that the user would do the retrieval of the relevant parts of the book from a vectordb first, and then feed those sections along with the question as the prompt. Conceptually it is very similar (and it too uses vector database), but with RAG it would happen as part of the inferencing pipeline and therefore achieve better performance than the end user emulating it.</div><br/><div id="35912537" class="c"><input type="checkbox" id="c-35912537" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35907562">parent</a><span>|</span><a href="#35907479">next</a><span>|</span><label class="collapse" for="c-35912537">[-]</label><label class="expand" for="c-35912537">[1 more]</label></div><br/><div class="children"><div class="content">Yep, but your retrieval from the vector DB becomes your relevancy bottleneck.</div><br/></div></div></div></div><div id="35907479" class="c"><input type="checkbox" id="c-35907479" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35907330">parent</a><span>|</span><a href="#35907562">prev</a><span>|</span><a href="#35905375">next</a><span>|</span><label class="collapse" for="c-35907479">[-]</label><label class="expand" for="c-35907479">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not different. RAG is a way to train embedding stores end to end</div><br/><div id="35910965" class="c"><input type="checkbox" id="c-35910965" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35907479">parent</a><span>|</span><a href="#35905375">next</a><span>|</span><label class="collapse" for="c-35910965">[-]</label><label class="expand" for="c-35910965">[1 more]</label></div><br/><div class="children"><div class="content">somehow got down voted on something I&#x27;m a professional expert at</div><br/></div></div></div></div></div></div></div></div><div id="35905375" class="c"><input type="checkbox" id="c-35905375" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#35905308">parent</a><span>|</span><a href="#35905533">prev</a><span>|</span><a href="#35905506">next</a><span>|</span><label class="collapse" for="c-35905375">[-]</label><label class="expand" for="c-35905375">[9 more]</label></div><br/><div class="children"><div class="content">With embeddings, you essentially can. Group the book into sections, embed each section, then when you do a prompt, add in the N most similar embedded sections to your prompt.</div><br/><div id="35905488" class="c"><input type="checkbox" id="c-35905488" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905375">parent</a><span>|</span><a href="#35910104">next</a><span>|</span><label class="collapse" for="c-35905488">[-]</label><label class="expand" for="c-35905488">[7 more]</label></div><br/><div class="children"><div class="content">What if the question is &quot;What are the main themes of this work?&quot;<p>Or anything where the question answer isn&#x27;t &#x27;close&#x27; to the words used in the question?<p>How well does this work vs giving it the whole thing as a prompt?<p>I assume worse but I&#x27;m not sure how this approach compares to giving it the full thing in the prompt or splitting it into N sections and running on each and then summarizing.</div><br/><div id="35907625" class="c"><input type="checkbox" id="c-35907625" checked=""/><div class="controls bullet"><span class="by">summarity</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905488">parent</a><span>|</span><a href="#35905648">next</a><span>|</span><label class="collapse" for="c-35907625">[-]</label><label class="expand" for="c-35907625">[2 more]</label></div><br/><div class="children"><div class="content">That is solved by hypothetical embeddings.<p>Background: <a href="https:&#x2F;&#x2F;summarity.com&#x2F;hyde" rel="nofollow">https:&#x2F;&#x2F;summarity.com&#x2F;hyde</a><p>Demo: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;elNrRU12xRc?t=1550" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;elNrRU12xRc?t=1550</a> (or try it on findsight.ai and compare results of the &quot;answer&quot; vs the &quot;state&quot; filter)<p>For even deeper retrieval consider late interaction models such as ColBERT</div><br/><div id="35907803" class="c"><input type="checkbox" id="c-35907803" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35907625">parent</a><span>|</span><a href="#35905648">next</a><span>|</span><label class="collapse" for="c-35907803">[-]</label><label class="expand" for="c-35907803">[1 more]</label></div><br/><div class="children"><div class="content">Any material comparing the different embedding models? I&#x27;m working on information retrieval from government documents and without any ML experience it&#x27;s daunting</div><br/></div></div></div></div><div id="35905648" class="c"><input type="checkbox" id="c-35905648" checked=""/><div class="controls bullet"><span class="by">jtlicardo</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905488">parent</a><span>|</span><a href="#35907625">prev</a><span>|</span><a href="#35905651">next</a><span>|</span><label class="collapse" for="c-35905648">[-]</label><label class="expand" for="c-35905648">[1 more]</label></div><br/><div class="children"><div class="content">You pretty much summed up the drawbacks of the embeddings approach. In my experience it&#x27;s pretty hard to extract the relevant parts of text, especially when the text is uniform.</div><br/></div></div><div id="35905651" class="c"><input type="checkbox" id="c-35905651" checked=""/><div class="controls bullet"><span class="by">abraxas</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905488">parent</a><span>|</span><a href="#35905648">prev</a><span>|</span><a href="#35910104">next</a><span>|</span><label class="collapse" for="c-35905651">[-]</label><label class="expand" for="c-35905651">[3 more]</label></div><br/><div class="children"><div class="content">You could do multi level summaries etc but yeah this is all just band aids around token limits.</div><br/><div id="35905816" class="c"><input type="checkbox" id="c-35905816" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905651">parent</a><span>|</span><a href="#35910104">next</a><span>|</span><label class="collapse" for="c-35905816">[-]</label><label class="expand" for="c-35905816">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s as much of a band-aid as it first appears  since this roughly mimics how a human would do it.<p>The problem is that humans have continuous information retrieval and storage where the current crop of embedding systems are static and mostly one shot.</div><br/><div id="35907189" class="c"><input type="checkbox" id="c-35907189" checked=""/><div class="controls bullet"><span class="by">crucialfelix</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905816">parent</a><span>|</span><a href="#35910104">next</a><span>|</span><label class="collapse" for="c-35907189">[-]</label><label class="expand" for="c-35907189">[1 more]</label></div><br/><div class="children"><div class="content">Humans have limited working memory, they quickly forget short term memory (unless it&#x27;s super significant) and our long term memory fades selectively if not reactivated or significant (intense).<p>This weird leaky memory has advantages and disadvantages. Forgetting is useful, it removes garbage.<p>Machine models could vary the balance of temporal types, drop out
Etc. We may get some weird behavior.<p>I would guess we will see many innovations in how memory is stored  in systems like these.</div><br/></div></div></div></div></div></div></div></div><div id="35910104" class="c"><input type="checkbox" id="c-35910104" checked=""/><div class="controls bullet"><span class="by">rsiqueira</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905375">parent</a><span>|</span><a href="#35905488">prev</a><span>|</span><a href="#35905506">next</a><span>|</span><label class="collapse" for="c-35910104">[-]</label><label class="expand" for="c-35910104">[1 more]</label></div><br/><div class="children"><div class="content">The real gain would be if we were able to use the 100K Context Windows and not this &quot;embeddings trick&quot;. The embeddings work only in some cases where the answer is in a short part(s) of the document. If user asks something like &quot;what are the main ideas?&quot; or &quot;Summarize the document.&quot; or any question that needs context from large portions of the book&#x2F;pdf&#x2F;file, then it will not work with the embeddings trick that use just short passages in prompt. But if large context windows costs are high, we need to keep using embeddings and few text parts.</div><br/></div></div></div></div><div id="35905506" class="c"><input type="checkbox" id="c-35905506" checked=""/><div class="controls bullet"><span class="by">fdgsdfogijq</span><span>|</span><a href="#35905308">parent</a><span>|</span><a href="#35905375">prev</a><span>|</span><a href="#35905564">next</a><span>|</span><label class="collapse" for="c-35905506">[-]</label><label class="expand" for="c-35905506">[10 more]</label></div><br/><div class="children"><div class="content">The price on this will plummet over the next few years, the economic benefits are too large</div><br/><div id="35906363" class="c"><input type="checkbox" id="c-35906363" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905506">parent</a><span>|</span><a href="#35905613">next</a><span>|</span><label class="collapse" for="c-35906363">[-]</label><label class="expand" for="c-35906363">[8 more]</label></div><br/><div class="children"><div class="content">The economic benefits of mining asteroids are also too large to ignore yet here we are, levelling villages to dig for coal.<p>Just a few manufacturers hold the effective cartel monopoly on LLM acceleration and you best bet they will charge out the ass for it.</div><br/><div id="35911931" class="c"><input type="checkbox" id="c-35911931" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35906363">parent</a><span>|</span><a href="#35908712">next</a><span>|</span><label class="collapse" for="c-35911931">[-]</label><label class="expand" for="c-35911931">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The economic benefits of mining asteroids are also too large<p>Ironically, that&#x27;s an example I like to list as &quot;pure sci-fi fantasy, divorced from economic reality.&quot;<p>The total cost of the iron ore that goes into making a new a car is about $200-$300 dollars, depending on various factors (size of the car, ore spot price, etc...).<p>Even if -- magically -- asteroid mining made not just &quot;iron ore&quot;, but specifically the steel alloy used for car bodies <i>literally free</i>, new cars costing $30,000 would now cost... $29,700.<p>You can save more by skipping the optional coffee cup warmer, or whatever.<p>In reality: 90% of iron and steel is recycled, and asteroid mining is not magic.</div><br/></div></div><div id="35908712" class="c"><input type="checkbox" id="c-35908712" checked=""/><div class="controls bullet"><span class="by">modernpink</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35906363">parent</a><span>|</span><a href="#35911931">prev</a><span>|</span><a href="#35908089">next</a><span>|</span><label class="collapse" for="c-35908712">[-]</label><label class="expand" for="c-35908712">[4 more]</label></div><br/><div class="children"><div class="content">Market competition and innovation in both ML and hardware has consistently driven down the price of AI in the past decade. You only have to look at where we are with capabilities today compared to ten years ago when CIFAR100 classifiers were the state of the art.<p>Barring a Chinese invasion of Taiwan, these APIs will halve in price over the next year.</div><br/><div id="35908931" class="c"><input type="checkbox" id="c-35908931" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35908712">parent</a><span>|</span><a href="#35908901">prev</a><span>|</span><a href="#35911181">next</a><span>|</span><label class="collapse" for="c-35908931">[-]</label><label class="expand" for="c-35908931">[1 more]</label></div><br/><div class="children"><div class="content">Well here&#x27;s to hoping I guess.</div><br/></div></div><div id="35911181" class="c"><input type="checkbox" id="c-35911181" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35908712">parent</a><span>|</span><a href="#35908931">prev</a><span>|</span><a href="#35908089">next</a><span>|</span><label class="collapse" for="c-35911181">[-]</label><label class="expand" for="c-35911181">[1 more]</label></div><br/><div class="children"><div class="content">What would a Chinese invasion of Taiwan do to &quot;tech&quot;, it sounds like it would be awfully catastrophic ?<p>On the other hand, for all peoples worrying about China, they are pretty restrained given the enormous turmoil the invasion would cause. If they wanted to break the USA, now would probably be the time to do it?</div><br/></div></div></div></div><div id="35908089" class="c"><input type="checkbox" id="c-35908089" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35906363">parent</a><span>|</span><a href="#35908712">prev</a><span>|</span><a href="#35905613">next</a><span>|</span><label class="collapse" for="c-35908089">[-]</label><label class="expand" for="c-35908089">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m wondering what level you&#x27;re thinking. Cloud vendors? GPU vendors? Fabs?</div><br/><div id="35908304" class="c"><input type="checkbox" id="c-35908304" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35908089">parent</a><span>|</span><a href="#35905613">next</a><span>|</span><label class="collapse" for="c-35908304">[-]</label><label class="expand" for="c-35908304">[1 more]</label></div><br/><div class="children"><div class="content">Given what&#x27;s used right now to my knowledge, the main ones would be Nvidia&#x27;s tensor cores, Apple&#x27;s M chips and Google&#x27;s cloud TPUs. All of that&#x27;s TSMC I think?</div><br/></div></div></div></div></div></div><div id="35905613" class="c"><input type="checkbox" id="c-35905613" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905506">parent</a><span>|</span><a href="#35906363">prev</a><span>|</span><a href="#35905564">next</a><span>|</span><label class="collapse" for="c-35905613">[-]</label><label class="expand" for="c-35905613">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but physics trumps economics.</div><br/></div></div></div></div><div id="35905564" class="c"><input type="checkbox" id="c-35905564" checked=""/><div class="controls bullet"><span class="by">mikrl</span><span>|</span><a href="#35905308">parent</a><span>|</span><a href="#35905506">prev</a><span>|</span><a href="#35905353">next</a><span>|</span><label class="collapse" for="c-35905564">[-]</label><label class="expand" for="c-35905564">[1 more]</label></div><br/><div class="children"><div class="content">The analogy I can think of here is a pointer, but AFAIK the context would always need to go along with the prompt unless you could tweak internal state to bias towards the context.<p>Otherwise, it might make sense to have a separate routine which compresses the context as efficiently as possible. Auto encoder?</div><br/></div></div><div id="35905353" class="c"><input type="checkbox" id="c-35905353" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#35905308">parent</a><span>|</span><a href="#35905564">prev</a><span>|</span><a href="#35907500">next</a><span>|</span><label class="collapse" for="c-35905353">[-]</label><label class="expand" for="c-35905353">[12 more]</label></div><br/><div class="children"><div class="content">Not sure about this one but you can usually ask multiple questions in one shot at least</div><br/><div id="35905367" class="c"><input type="checkbox" id="c-35905367" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905353">parent</a><span>|</span><a href="#35907500">next</a><span>|</span><label class="collapse" for="c-35905367">[-]</label><label class="expand" for="c-35905367">[11 more]</label></div><br/><div class="children"><div class="content">Generation is more expensive than the prompt input (for Claude v1, generation is 3x the cost; for GPT-4 it&#x27;s 2x the cost)<p>It makes the economics slightly trickier.</div><br/><div id="35905463" class="c"><input type="checkbox" id="c-35905463" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905367">parent</a><span>|</span><a href="#35907500">next</a><span>|</span><label class="collapse" for="c-35905463">[-]</label><label class="expand" for="c-35905463">[10 more]</label></div><br/><div class="children"><div class="content">I wonder why this is? Naively there&#x27;s no difference between the two from a transformer standpoint.<p>Perhaps it&#x27;s because under the hood there&#x27;s additional safety analysis&#x2F;candidate generate that is resource intensive?</div><br/><div id="35912227" class="c"><input type="checkbox" id="c-35912227" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905463">parent</a><span>|</span><a href="#35905732">next</a><span>|</span><label class="collapse" for="c-35912227">[-]</label><label class="expand" for="c-35912227">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because the input tokens can be batch-processed in a single forward pass through the model, while generating tokens requires one forward pass through the model per token.<p>If you do the math of how much memory bandwidth is required by a forward pass vs. how much compute, you&#x27;ll see that inference is entirely limited by memory bandwidth and will use compute resources very inefficiently. In contrast, input processing is able to fully use the available compute.<p>Of course, there are ways to mitigate this problem, like processing multiple token streams in parallel, but the fundamental problem remains.</div><br/></div></div><div id="35905732" class="c"><input type="checkbox" id="c-35905732" checked=""/><div class="controls bullet"><span class="by">pyth0</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905463">parent</a><span>|</span><a href="#35912227">prev</a><span>|</span><a href="#35905505">next</a><span>|</span><label class="collapse" for="c-35905732">[-]</label><label class="expand" for="c-35905732">[6 more]</label></div><br/><div class="children"><div class="content">Normally the inputs are padded out to the context length [1] and so the cost to embed 1 token or N tokens is the same. The output is produced token-by-token and so the amount of GPU time increases with the number of output tokens.<p>[1] I&#x27;m not sure if these huge context lengths are achieved the same way (i.e. a single input vector of length N) but given the cost is constant for input I would assume the resource usage is too.</div><br/><div id="35906685" class="c"><input type="checkbox" id="c-35906685" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905732">parent</a><span>|</span><a href="#35907474">next</a><span>|</span><label class="collapse" for="c-35906685">[-]</label><label class="expand" for="c-35906685">[4 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t match my mental model (or implemented model in the case of GPT2) of how self-attention works (you need to calculate the residual stream for each individual token, attending to all prior tokens before it). Have a link?</div><br/><div id="35907093" class="c"><input type="checkbox" id="c-35907093" checked=""/><div class="controls bullet"><span class="by">pyth0</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35906685">parent</a><span>|</span><a href="#35907474">next</a><span>|</span><label class="collapse" for="c-35907093">[-]</label><label class="expand" for="c-35907093">[3 more]</label></div><br/><div class="children"><div class="content">I work on infrastructure for serving large language models but I don&#x27;t have any background in ML, so my perspective is looking at these models as a black box (and also conversations with the people that do the ML stuff). It is the case in practice at least from a latency side that with a fixed context length N, embedding any number of tokens from 0 to N takes the same amount of time. Perhaps it&#x27;s a difference between the conceptual and actual implementation on GPU?<p><i>edit</i> - This occurred to me after the fact but I wonder if the difference is that the use case I work with is processing batches of many different embedding requests (but computed in one batch), therefore it has to process `min(longest embedding, N)` tokens so any individual request in theory has no difference. This would also be the case for Anthropic however.</div><br/><div id="35907675" class="c"><input type="checkbox" id="c-35907675" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35907093">parent</a><span>|</span><a href="#35907474">next</a><span>|</span><label class="collapse" for="c-35907675">[-]</label><label class="expand" for="c-35907675">[2 more]</label></div><br/><div class="children"><div class="content">Ah, you&#x27;re thinking about embeddings which are basically the encoder stack on a traditional transformer architecture. Modern GPT-like models (including Claude), however, drop the encoder and use decoder-only architectures.<p>I could imagine something where encoders pad up to the context length because causal masking doesn&#x27;t apply and the self attention has learned to look across the whole context-window.</div><br/><div id="35912832" class="c"><input type="checkbox" id="c-35912832" checked=""/><div class="controls bullet"><span class="by">revelio</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35907675">parent</a><span>|</span><a href="#35907474">next</a><span>|</span><label class="collapse" for="c-35912832">[-]</label><label class="expand" for="c-35912832">[1 more]</label></div><br/><div class="children"><div class="content">Decoder only architecture? What is this? That doesn&#x27;t sound like a transformer at all, are you saying gpt4 uses a totally different algorithm?</div><br/></div></div></div></div></div></div></div></div><div id="35907474" class="c"><input type="checkbox" id="c-35907474" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905732">parent</a><span>|</span><a href="#35906685">prev</a><span>|</span><a href="#35905505">next</a><span>|</span><label class="collapse" for="c-35907474">[-]</label><label class="expand" for="c-35907474">[1 more]</label></div><br/><div class="children"><div class="content">Everyone serious batches together short prompts so the cost is roughly proportional to the tokens.</div><br/></div></div></div></div><div id="35905505" class="c"><input type="checkbox" id="c-35905505" checked=""/><div class="controls bullet"><span class="by">space_fountain</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905463">parent</a><span>|</span><a href="#35905732">prev</a><span>|</span><a href="#35907500">next</a><span>|</span><label class="collapse" for="c-35905505">[-]</label><label class="expand" for="c-35905505">[2 more]</label></div><br/><div class="children"><div class="content">Well each additional token generated requires rerunning the model right to find the next likely token given the previous one</div><br/><div id="35905607" class="c"><input type="checkbox" id="c-35905607" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35905308">root</a><span>|</span><a href="#35905505">parent</a><span>|</span><a href="#35907500">next</a><span>|</span><label class="collapse" for="c-35905607">[-]</label><label class="expand" for="c-35905607">[1 more]</label></div><br/><div class="children"><div class="content">Naively, yes, but you can cache the bulk of that &quot;rerunning&quot; [1]. That said the (non-flash) attention costs go up with the length of the sequence so perhaps this is just a simpler way to approximate these costs.<p>[1] <a href="https:&#x2F;&#x2F;kipp.ly&#x2F;blog&#x2F;transformer-inference-arithmetic&#x2F;" rel="nofollow">https:&#x2F;&#x2F;kipp.ly&#x2F;blog&#x2F;transformer-inference-arithmetic&#x2F;</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="35907500" class="c"><input type="checkbox" id="c-35907500" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#35905308">parent</a><span>|</span><a href="#35905353">prev</a><span>|</span><a href="#35907654">next</a><span>|</span><label class="collapse" for="c-35907500">[-]</label><label class="expand" for="c-35907500">[1 more]</label></div><br/><div class="children"><div class="content">Yes, caching the states of the sequence would make sense. An issue is that it&#x27;s still more expensive to compute the new tokens even if you cache the states viewed so far</div><br/></div></div></div></div><div id="35907654" class="c"><input type="checkbox" id="c-35907654" checked=""/><div class="controls bullet"><span class="by">maxutility</span><span>|</span><a href="#35905308">prev</a><span>|</span><a href="#35905396">next</a><span>|</span><label class="collapse" for="c-35907654">[-]</label><label class="expand" for="c-35907654">[6 more]</label></div><br/><div class="children"><div class="content">I don’t see this in the article. Has Anthropic explained the mechanism by which they were able to cost-effectively expand the context window, and whether there was additional training or a design decision (e.g. alternative positional embedding approach) that helped the model optimize for a larger window?</div><br/><div id="35910175" class="c"><input type="checkbox" id="c-35910175" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35907654">parent</a><span>|</span><a href="#35912527">next</a><span>|</span><label class="collapse" for="c-35910175">[-]</label><label class="expand" for="c-35910175">[3 more]</label></div><br/><div class="children"><div class="content">No. As far as I know, they haven&#x27;t said anything about this. Neither did OpenAI about gpt-4-32k.<p>MosaicML did say something about MPT-7B-StoryWriter-65k+: <a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b" rel="nofollow">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b</a>. They are using ALiBi (Attention with Linear Biases): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2108.12409" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2108.12409</a>.<p>I think OpenAI and Anthropic are using ALiBi or their own proprietary advances. Both seem possible.</div><br/><div id="35911328" class="c"><input type="checkbox" id="c-35911328" checked=""/><div class="controls bullet"><span class="by">maxutility</span><span>|</span><a href="#35907654">root</a><span>|</span><a href="#35910175">parent</a><span>|</span><a href="#35912527">next</a><span>|</span><label class="collapse" for="c-35911328">[-]</label><label class="expand" for="c-35911328">[2 more]</label></div><br/><div class="children"><div class="content">Interesting. Does the decision to use ALiBi have to be done before the model weights are first trained, or is there a way that these models could have incorporated ALiBi instead or in addition to an alternate positional encoding method to ALiBi after they were first trained?</div><br/><div id="35911404" class="c"><input type="checkbox" id="c-35911404" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35907654">root</a><span>|</span><a href="#35911328">parent</a><span>|</span><a href="#35912527">next</a><span>|</span><label class="collapse" for="c-35911404">[-]</label><label class="expand" for="c-35911404">[1 more]</label></div><br/><div class="children"><div class="content">The decision needs to be made before starting training. Maybe there is a clever way to add it after the fact in the style of LoRA? First, that would be a different method in its own right (just as LoRA is), second, I can&#x27;t see how to do so easily. But then I just thought about it for a minute.</div><br/></div></div></div></div></div></div><div id="35912527" class="c"><input type="checkbox" id="c-35912527" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#35907654">parent</a><span>|</span><a href="#35910175">prev</a><span>|</span><a href="#35910105">next</a><span>|</span><label class="collapse" for="c-35912527">[-]</label><label class="expand" for="c-35912527">[1 more]</label></div><br/><div class="children"><div class="content">No, they are playing this close to the chest, similar to how OpenAI achieved 32k context limit.</div><br/></div></div><div id="35910105" class="c"><input type="checkbox" id="c-35910105" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#35907654">parent</a><span>|</span><a href="#35912527">prev</a><span>|</span><a href="#35905396">next</a><span>|</span><label class="collapse" for="c-35910105">[-]</label><label class="expand" for="c-35910105">[1 more]</label></div><br/><div class="children"><div class="content">a lot of people are speculating online (<a href="https:&#x2F;&#x2F;twitter.com&#x2F;search?q=anthropicai%20alibi&amp;src=typed_query&amp;f=top" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;search?q=anthropicai%20alibi&amp;src=typed_q...</a>) but i&#x27;m guessing it&#x27;s ALiBi, which was also used by MPT-7B to get up to 85k long context</div><br/></div></div></div></div><div id="35905396" class="c"><input type="checkbox" id="c-35905396" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#35907654">prev</a><span>|</span><a href="#35905372">next</a><span>|</span><label class="collapse" for="c-35905396">[-]</label><label class="expand" for="c-35905396">[10 more]</label></div><br/><div class="children"><div class="content">Can LLMs take advantage of this bigger window to solve meaningful tasks though? I can&#x27;t imagine in the training data, knowing what happened 100k tokens ago would be _that_ relevant to predicting the current token very often, so unless this is something that the model learns to leverage more implicitly, I&#x27;d be a bit pessimistic.</div><br/><div id="35905498" class="c"><input type="checkbox" id="c-35905498" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#35905396">parent</a><span>|</span><a href="#35913114">next</a><span>|</span><label class="collapse" for="c-35905498">[-]</label><label class="expand" for="c-35905498">[6 more]</label></div><br/><div class="children"><div class="content">Yes. For instance, a large context window allows you to have a chat for months where the model can remember and make use of everything you’ve ever talked about. That enables creating a much more effective “assistant” that can remember key details months later that may be valuable.<p>A second example is the analysis of long documents. Today, hacks like chunking and HyDE enable us to ask questions about a long document or a corpus of documents. But is far superior if the model can ingest the whole document and apply attention to everything, rather than just one chunk at a time. Chunking effectively means that the model is limited to drawing conclusions from one chunk at a time and cannot synthesize useful responses relating to the entire document.</div><br/><div id="35907541" class="c"><input type="checkbox" id="c-35907541" checked=""/><div class="controls bullet"><span class="by">woeirua</span><span>|</span><a href="#35905396">root</a><span>|</span><a href="#35905498">parent</a><span>|</span><a href="#35906837">next</a><span>|</span><label class="collapse" for="c-35907541">[-]</label><label class="expand" for="c-35907541">[2 more]</label></div><br/><div class="children"><div class="content">It remains to be seen just how effective longer contexts are because if the attention vectors don&#x27;t ever learn to pick up specific items from further back in the text then having more tokens doesn&#x27;t really matter.<p>Given that the conventional cost of training attention layers grows quadratically with the number of tokens I think Anthropic is doing some kind of approximation here. Not clear at all that you would get the same results as vanilla attention.</div><br/><div id="35908392" class="c"><input type="checkbox" id="c-35908392" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#35905396">root</a><span>|</span><a href="#35907541">parent</a><span>|</span><a href="#35906837">next</a><span>|</span><label class="collapse" for="c-35908392">[-]</label><label class="expand" for="c-35908392">[1 more]</label></div><br/><div class="children"><div class="content">They did mention that the inference time to answer a question about the book was something like 22 seconds, so perhaps they are indeed still using self-attention.</div><br/></div></div></div></div><div id="35906837" class="c"><input type="checkbox" id="c-35906837" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#35905396">root</a><span>|</span><a href="#35905498">parent</a><span>|</span><a href="#35907541">prev</a><span>|</span><a href="#35908615">next</a><span>|</span><label class="collapse" for="c-35906837">[-]</label><label class="expand" for="c-35906837">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not questioning whether it would be useful, just whether it&#x27;s actually something that token masking in training is going to work to make the model learn this.</div><br/><div id="35913654" class="c"><input type="checkbox" id="c-35913654" checked=""/><div class="controls bullet"><span class="by">kaj_sotala</span><span>|</span><a href="#35905396">root</a><span>|</span><a href="#35906837">parent</a><span>|</span><a href="#35908615">next</a><span>|</span><label class="collapse" for="c-35913654">[-]</label><label class="expand" for="c-35913654">[1 more]</label></div><br/><div class="children"><div class="content">I would imagine that if you are training on the text of a novel, then anything that happened earlier in the text may be relevant for predicting the next events. Especially if it&#x27;s something like a detective novel that has clues about the criminal&#x27;s identity scattered across the story.<p>Also if you are training on a database of code.</div><br/></div></div></div></div><div id="35908615" class="c"><input type="checkbox" id="c-35908615" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35905396">root</a><span>|</span><a href="#35905498">parent</a><span>|</span><a href="#35906837">prev</a><span>|</span><a href="#35913114">next</a><span>|</span><label class="collapse" for="c-35908615">[-]</label><label class="expand" for="c-35908615">[1 more]</label></div><br/><div class="children"><div class="content">Gets pricier as you chat for longer, imagine having to chat a line with a history with 20k token.</div><br/></div></div></div></div><div id="35913114" class="c"><input type="checkbox" id="c-35913114" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#35905396">parent</a><span>|</span><a href="#35905498">prev</a><span>|</span><a href="#35906297">next</a><span>|</span><label class="collapse" for="c-35913114">[-]</label><label class="expand" for="c-35913114">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d argue that books are a clear example where the 100k tokens context would make a huge difference.</div><br/></div></div><div id="35906297" class="c"><input type="checkbox" id="c-35906297" checked=""/><div class="controls bullet"><span class="by">SomewhatLikely</span><span>|</span><a href="#35905396">parent</a><span>|</span><a href="#35913114">prev</a><span>|</span><a href="#35905372">next</a><span>|</span><label class="collapse" for="c-35906297">[-]</label><label class="expand" for="c-35906297">[2 more]</label></div><br/><div class="children"><div class="content">I would guess that semantic similarity would be the stronger training signal than distance once you go beyond a sentence or two away.</div><br/><div id="35909285" class="c"><input type="checkbox" id="c-35909285" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#35905396">root</a><span>|</span><a href="#35906297">parent</a><span>|</span><a href="#35905372">next</a><span>|</span><label class="collapse" for="c-35909285">[-]</label><label class="expand" for="c-35909285">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty dubious - how would the model not get absolutely swamped by the vast amount of potential context if it&#x27;s not learning to ignore long range signals for the most part?</div><br/></div></div></div></div></div></div><div id="35905372" class="c"><input type="checkbox" id="c-35905372" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#35905396">prev</a><span>|</span><a href="#35905447">next</a><span>|</span><label class="collapse" for="c-35905372">[-]</label><label class="expand" for="c-35905372">[6 more]</label></div><br/><div class="children"><div class="content">We need public benchmarks.<p>This is incredibly fast progress on large contexts and I would like to see if they are actually attending equally as well to all of the information or there is some sparse approximation leading to intelligence&#x2F;reasoning degradation.</div><br/><div id="35905449" class="c"><input type="checkbox" id="c-35905449" checked=""/><div class="controls bullet"><span class="by">monlockandkey</span><span>|</span><a href="#35905372">parent</a><span>|</span><a href="#35905447">next</a><span>|</span><label class="collapse" for="c-35905449">[-]</label><label class="expand" for="c-35905449">[5 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;lmsys.org&#x2F;blog&#x2F;2023-05-10-leaderboard&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmsys.org&#x2F;blog&#x2F;2023-05-10-leaderboard&#x2F;</a><p><a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?arena" rel="nofollow">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;?arena</a><p>Claude by Anthropic has more favourable responses then ChatGPT</div><br/><div id="35905782" class="c"><input type="checkbox" id="c-35905782" checked=""/><div class="controls bullet"><span class="by">polishdude20</span><span>|</span><a href="#35905372">root</a><span>|</span><a href="#35905449">parent</a><span>|</span><a href="#35905835">next</a><span>|</span><label class="collapse" for="c-35905782">[-]</label><label class="expand" for="c-35905782">[3 more]</label></div><br/><div class="children"><div class="content">So I tried this prompt in their chatbot arena multiple times. Each time getting the wrong answer:<p>&quot;Given that Beth is Sue&#x27;s sister and Arnold is Sue&#x27;s father and Beth Junior is Beth&#x27;s Daughter and Jacob is Arnold&#x27;s Great Grandfather, who is Jacob to Beth Junior?&quot;</div><br/><div id="35906667" class="c"><input type="checkbox" id="c-35906667" checked=""/><div class="controls bullet"><span class="by">jefftk</span><span>|</span><a href="#35905372">root</a><span>|</span><a href="#35905782">parent</a><span>|</span><a href="#35906523">next</a><span>|</span><label class="collapse" for="c-35906667">[-]</label><label class="expand" for="c-35906667">[1 more]</label></div><br/><div class="children"><div class="content">Is the right answer pointing out that Arnold might not be Beth&#x27;s father, and so Beth Junior might be unrelated to Jacob?</div><br/></div></div><div id="35906523" class="c"><input type="checkbox" id="c-35906523" checked=""/><div class="controls bullet"><span class="by">svachalek</span><span>|</span><a href="#35905372">root</a><span>|</span><a href="#35905782">parent</a><span>|</span><a href="#35906667">prev</a><span>|</span><a href="#35905835">next</a><span>|</span><label class="collapse" for="c-35906523">[-]</label><label class="expand" for="c-35906523">[1 more]</label></div><br/><div class="children"><div class="content">I just tried it and gpt-3.5-turbo got it right.</div><br/></div></div></div></div><div id="35905835" class="c"><input type="checkbox" id="c-35905835" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#35905372">root</a><span>|</span><a href="#35905449">parent</a><span>|</span><a href="#35905782">prev</a><span>|</span><a href="#35905447">next</a><span>|</span><label class="collapse" for="c-35905835">[-]</label><label class="expand" for="c-35905835">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT3.5*<p>It&#x27;s still below GPT4, but it is closer to 4 than 3.5</div><br/></div></div></div></div></div></div><div id="35905447" class="c"><input type="checkbox" id="c-35905447" checked=""/><div class="controls bullet"><span class="by">flerovium</span><span>|</span><a href="#35905372">prev</a><span>|</span><a href="#35904959">next</a><span>|</span><label class="collapse" for="c-35905447">[-]</label><label class="expand" for="c-35905447">[3 more]</label></div><br/><div class="children"><div class="content">It means nothing as long as they don&#x27;t actually let us test the API.<p>Good luck waiting for it.</div><br/><div id="35905657" class="c"><input type="checkbox" id="c-35905657" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35905447">parent</a><span>|</span><a href="#35909199">next</a><span>|</span><label class="collapse" for="c-35905657">[-]</label><label class="expand" for="c-35905657">[1 more]</label></div><br/><div class="children"><div class="content">“POC or GTFO” as the security people say. :-)</div><br/></div></div><div id="35909199" class="c"><input type="checkbox" id="c-35909199" checked=""/><div class="controls bullet"><span class="by">jackson1372</span><span>|</span><a href="#35905447">parent</a><span>|</span><a href="#35905657">prev</a><span>|</span><a href="#35904959">next</a><span>|</span><label class="collapse" for="c-35909199">[-]</label><label class="expand" for="c-35909199">[1 more]</label></div><br/><div class="children"><div class="content">See the pricing PDF[^1] and API docs[^2], but TL;DR:<p>- Price per token doesn&#x27;t change compared to regular models<p>- Existing api users have access now by setting the `model` param to &quot;claude-v1-100k&quot; or &quot;claude-instant-v1-100k&quot;<p>- New customers can join waitlist at anthropic.com&#x2F;product<p>[1]: <a href="https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;images&#x2F;model_pricing_may2023.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;...</a>
[2]: <a href="https:&#x2F;&#x2F;console.anthropic.com&#x2F;docs&#x2F;api&#x2F;reference#parameters" rel="nofollow">https:&#x2F;&#x2F;console.anthropic.com&#x2F;docs&#x2F;api&#x2F;reference#parameters</a></div><br/></div></div></div></div><div id="35904959" class="c"><input type="checkbox" id="c-35904959" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#35905447">prev</a><span>|</span><a href="#35905986">next</a><span>|</span><label class="collapse" for="c-35904959">[-]</label><label class="expand" for="c-35904959">[13 more]</label></div><br/><div class="children"><div class="content">No pricing, but given that OpenAI&#x27;s GPT-4 doubles the cost-per-token if you go from 8k to a 32k context window, I suspect the pricing here will be 2-4x from the base Claude model which is 9k: <a href="https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;images&#x2F;apr-pricing-tokens_2023-05-10-213035_fhnc.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;...</a><p>Although with flash attention, who knows if marginal cost scales that consistently.</div><br/><div id="35907272" class="c"><input type="checkbox" id="c-35907272" checked=""/><div class="controls bullet"><span class="by">l1n</span><span>|</span><a href="#35904959">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35907272">[-]</label><label class="expand" for="c-35907272">[8 more]</label></div><br/><div class="children"><div class="content">Pricing is the same as the base model.</div><br/><div id="35907334" class="c"><input type="checkbox" id="c-35907334" checked=""/><div class="controls bullet"><span class="by">jimsimmons</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35907272">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35907334">[-]</label><label class="expand" for="c-35907334">[7 more]</label></div><br/><div class="children"><div class="content">Confirmation here:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;AnthropicAI&#x2F;status&#x2F;1656743460769259521?s=20" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;AnthropicAI&#x2F;status&#x2F;1656743460769259521?s...</a></div><br/><div id="35907378" class="c"><input type="checkbox" id="c-35907378" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35907334">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35907378">[-]</label><label class="expand" for="c-35907378">[6 more]</label></div><br/><div class="children"><div class="content">Huh. Well that changes things.</div><br/><div id="35907587" class="c"><input type="checkbox" id="c-35907587" checked=""/><div class="controls bullet"><span class="by">rat9988</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35907378">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35907587">[-]</label><label class="expand" for="c-35907587">[5 more]</label></div><br/><div class="children"><div class="content">Only for the duration of the beta</div><br/><div id="35907659" class="c"><input type="checkbox" id="c-35907659" checked=""/><div class="controls bullet"><span class="by">jimsimmons</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35907587">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35907659">[-]</label><label class="expand" for="c-35907659">[4 more]</label></div><br/><div class="children"><div class="content">Source?</div><br/><div id="35907966" class="c"><input type="checkbox" id="c-35907966" checked=""/><div class="controls bullet"><span class="by">felixgallo</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35907659">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35907966">[-]</label><label class="expand" for="c-35907966">[3 more]</label></div><br/><div class="children"><div class="content">the actual tweet you linked.</div><br/><div id="35907991" class="c"><input type="checkbox" id="c-35907991" checked=""/><div class="controls bullet"><span class="by">jimsimmons</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35907966">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35907991">[-]</label><label class="expand" for="c-35907991">[2 more]</label></div><br/><div class="children"><div class="content">It doesn’t say exclusively for the beta period</div><br/><div id="35908420" class="c"><input type="checkbox" id="c-35908420" checked=""/><div class="controls bullet"><span class="by">scoopertrooper</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35907991">parent</a><span>|</span><a href="#35905393">next</a><span>|</span><label class="collapse" for="c-35908420">[-]</label><label class="expand" for="c-35908420">[1 more]</label></div><br/><div class="children"><div class="content">With an extremely literal reading you are correct, but there was clearly an implication.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35905393" class="c"><input type="checkbox" id="c-35905393" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#35904959">parent</a><span>|</span><a href="#35907272">prev</a><span>|</span><a href="#35905387">next</a><span>|</span><label class="collapse" for="c-35905393">[-]</label><label class="expand" for="c-35905393">[2 more]</label></div><br/><div class="children"><div class="content">&lt;4x would be quite optimistic, at ~11x the tokens the amount of compute&#x2F;memory required would be n^10 (even with the lower starting point of flash attention) so unless they are already have excessive margins it wouldn&#x27;t make much sense to go that low.</div><br/><div id="35906252" class="c"><input type="checkbox" id="c-35906252" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35905393">parent</a><span>|</span><a href="#35905387">next</a><span>|</span><label class="collapse" for="c-35906252">[-]</label><label class="expand" for="c-35906252">[1 more]</label></div><br/><div class="children"><div class="content">I was assuming they used a different architecture to get the increase instead of just letting it eat hardware that way. Especially with the speed numbers in the post.</div><br/></div></div></div></div><div id="35905387" class="c"><input type="checkbox" id="c-35905387" checked=""/><div class="controls bullet"><span class="by">adamkochanowicz</span><span>|</span><a href="#35904959">parent</a><span>|</span><a href="#35905393">prev</a><span>|</span><a href="#35905986">next</a><span>|</span><label class="collapse" for="c-35905387">[-]</label><label class="expand" for="c-35905387">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;images&#x2F;apr-pricing-tokens.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;...</a></div><br/><div id="35905479" class="c"><input type="checkbox" id="c-35905479" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#35904959">root</a><span>|</span><a href="#35905387">parent</a><span>|</span><a href="#35905986">next</a><span>|</span><label class="collapse" for="c-35905479">[-]</label><label class="expand" for="c-35905479">[1 more]</label></div><br/><div class="children"><div class="content">Those are the same SKUs I linked.<p>The new model are a different model identified that&#x27;s not listed in the pricing doc, although it sounds like the intent may be to replace the base from looking at the API docs: <a href="https:&#x2F;&#x2F;console.anthropic.com&#x2F;docs&#x2F;api&#x2F;reference#-v1-complete" rel="nofollow">https:&#x2F;&#x2F;console.anthropic.com&#x2F;docs&#x2F;api&#x2F;reference#-v1-complet...</a></div><br/></div></div></div></div></div></div><div id="35905986" class="c"><input type="checkbox" id="c-35905986" checked=""/><div class="controls bullet"><span class="by">RoddaWallPro</span><span>|</span><a href="#35904959">prev</a><span>|</span><a href="#35911688">next</a><span>|</span><label class="collapse" for="c-35905986">[-]</label><label class="expand" for="c-35905986">[3 more]</label></div><br/><div class="children"><div class="content">I requested &amp; have been waiting for access to Claude for nearly 3 months now. Guess the waitlist must be really long...</div><br/><div id="35907624" class="c"><input type="checkbox" id="c-35907624" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#35905986">parent</a><span>|</span><a href="#35907430">next</a><span>|</span><label class="collapse" for="c-35907624">[-]</label><label class="expand" for="c-35907624">[1 more]</label></div><br/><div class="children"><div class="content">API access or just access to the chatbot?<p>You can go through Poe.com</div><br/></div></div><div id="35907430" class="c"><input type="checkbox" id="c-35907430" checked=""/><div class="controls bullet"><span class="by">technics256</span><span>|</span><a href="#35905986">parent</a><span>|</span><a href="#35907624">prev</a><span>|</span><a href="#35911688">next</a><span>|</span><label class="collapse" for="c-35907430">[-]</label><label class="expand" for="c-35907430">[1 more]</label></div><br/><div class="children"><div class="content">You likely got rejected. Was the same for me and I reapplied with a good use case and was let in</div><br/></div></div></div></div><div id="35911688" class="c"><input type="checkbox" id="c-35911688" checked=""/><div class="controls bullet"><span class="by">epicprogrammer</span><span>|</span><a href="#35905986">prev</a><span>|</span><a href="#35913308">next</a><span>|</span><label class="collapse" for="c-35911688">[-]</label><label class="expand" for="c-35911688">[1 more]</label></div><br/><div class="children"><div class="content">This is a fascinating study on the impact of context windows on language models. It&#x27;s interesting to see how smaller context windows can lead to more efficient and accurate language models, even when dealing with complex natural language tasks like question answering. I think this research could have important implications for a wide range of applications, from chatbots and virtual assistants to machine translation and text summarization. I&#x27;m looking forward to seeing how these findings are further developed and applied in real-world scenarios.</div><br/></div></div><div id="35913308" class="c"><input type="checkbox" id="c-35913308" checked=""/><div class="controls bullet"><span class="by">dsrtslnd23</span><span>|</span><a href="#35911688">prev</a><span>|</span><a href="#35905595">next</a><span>|</span><label class="collapse" for="c-35913308">[-]</label><label class="expand" for="c-35913308">[1 more]</label></div><br/><div class="children"><div class="content">Anyone with API access tried this for coding already?</div><br/></div></div><div id="35905595" class="c"><input type="checkbox" id="c-35905595" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#35913308">prev</a><span>|</span><a href="#35909147">next</a><span>|</span><label class="collapse" for="c-35905595">[-]</label><label class="expand" for="c-35905595">[3 more]</label></div><br/><div class="children"><div class="content">How are LLM’s increasing their context size? I guess you just increase input size if it’s for the self supervised GPT3 style training but for RLHF? 
Are they creating datasets of books to input to the LLM and then making human labelers label the response? There might be a smart way that does not involve new datasets</div><br/><div id="35906382" class="c"><input type="checkbox" id="c-35906382" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#35905595">parent</a><span>|</span><a href="#35905772">next</a><span>|</span><label class="collapse" for="c-35906382">[-]</label><label class="expand" for="c-35906382">[1 more]</label></div><br/><div class="children"><div class="content">Mosaic wrote about their new model here. <a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b" rel="nofollow">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b</a> It was trained on 65k inputs and has decent performance working with 80k+ tokens.</div><br/></div></div><div id="35905772" class="c"><input type="checkbox" id="c-35905772" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#35905595">parent</a><span>|</span><a href="#35906382">prev</a><span>|</span><a href="#35909147">next</a><span>|</span><label class="collapse" for="c-35905772">[-]</label><label class="expand" for="c-35905772">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think RLHF datasets need to take full advantage of the context window. There&#x27;s also many ways to programatically generate NLP datasets.</div><br/></div></div></div></div><div id="35909147" class="c"><input type="checkbox" id="c-35909147" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#35905595">prev</a><span>|</span><a href="#35907711">next</a><span>|</span><label class="collapse" for="c-35909147">[-]</label><label class="expand" for="c-35909147">[1 more]</label></div><br/><div class="children"><div class="content">All I see in the link is empty PR claims - is there any information about <i>how</i> they&#x27;re doing that? There are all kinds of known techniques that &quot;expand&quot; context window without really doing so, with different tradeoffs, and unless they provide actual information, any claims should be taken with a pile of salt, we shouldn&#x27;t just assume that they actually have &quot;true&quot; 100k context windows.</div><br/></div></div><div id="35907711" class="c"><input type="checkbox" id="c-35907711" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35909147">prev</a><span>|</span><a href="#35905852">next</a><span>|</span><label class="collapse" for="c-35907711">[-]</label><label class="expand" for="c-35907711">[3 more]</label></div><br/><div class="children"><div class="content">Did anyone else get on the waitlist, get in, and now their console link doesn&#x27;t work? I remember deciding the code generation wasn&#x27;t good enough to bother. Not sure if I actually ever activated it but I guess not.<p>Now I tried to request access again on their form and it just redirected. Can&#x27;t even tell if that worked.<p>Does anyone know if this can program as well as GPT-4? Because if so then the larger context window is a big improvement.</div><br/><div id="35908216" class="c"><input type="checkbox" id="c-35908216" checked=""/><div class="controls bullet"><span class="by">M4v3R</span><span>|</span><a href="#35907711">parent</a><span>|</span><a href="#35905852">next</a><span>|</span><label class="collapse" for="c-35908216">[-]</label><label class="expand" for="c-35908216">[2 more]</label></div><br/><div class="children"><div class="content">I do have access to it and from my very limited testing it looks like it can program at least on par with GPT-3.5. I didn&#x27;t have time yet to test it more comprehensively against GPT-4.</div><br/><div id="35908451" class="c"><input type="checkbox" id="c-35908451" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35907711">root</a><span>|</span><a href="#35908216">parent</a><span>|</span><a href="#35905852">next</a><span>|</span><label class="collapse" for="c-35908451">[-]</label><label class="expand" for="c-35908451">[1 more]</label></div><br/><div class="children"><div class="content">OK great thanks that&#x27;s what I heard. Very interested to hear about comparisons with GPT-4.</div><br/></div></div></div></div></div></div><div id="35905852" class="c"><input type="checkbox" id="c-35905852" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#35907711">prev</a><span>|</span><a href="#35907095">next</a><span>|</span><label class="collapse" for="c-35905852">[-]</label><label class="expand" for="c-35905852">[1 more]</label></div><br/><div class="children"><div class="content">With quadratic time complexity for context size, that gets expensive.</div><br/></div></div><div id="35907095" class="c"><input type="checkbox" id="c-35907095" checked=""/><div class="controls bullet"><span class="by">timsuchanek</span><span>|</span><a href="#35905852">prev</a><span>|</span><a href="#35905160">next</a><span>|</span><label class="collapse" for="c-35907095">[-]</label><label class="expand" for="c-35907095">[2 more]</label></div><br/><div class="children"><div class="content">Curious what this will mean for the vector db vendors. Imagine finetuning would be quick and cheap. Could there be a world where vector dbs aren’t needed anymore?</div><br/><div id="35908199" class="c"><input type="checkbox" id="c-35908199" checked=""/><div class="controls bullet"><span class="by">shri_krishna</span><span>|</span><a href="#35907095">parent</a><span>|</span><a href="#35905160">next</a><span>|</span><label class="collapse" for="c-35908199">[-]</label><label class="expand" for="c-35908199">[1 more]</label></div><br/><div class="children"><div class="content">100k context limit is still a limit (we have no idea how Anthropic is achieving this - if it is extension of the base model context limit itself or some vector db trickery in the backend or probably even RAG). Even in this example, though it could fit entire text of Great Gatsby it still is 1 book&#x2F;text&#x2F;document. Typical business use cases require searching through hundreds if not thousands of documents&#x2F;books and finding similar vector embeddings through all of them and fetching top-K results (this is how Google search works when it has to scan through embeddings for billions of websites). These top-K results can be stuffed into the 100k context limit and produce an even more holistic picture rather than just stuff one book&#x2F;pdf&#x2F;file into the context. Depends on the requirements though. I don&#x27;t see how it might affect vector db vendors who can process billions of vectors per query and provide top-K results.<p>Also having a massive context length is not necessarily a good thing from perspective of cost. It also doesn&#x27;t work great with a chatbot as you will have to feed the same 100k worth context back into the chatbot for every question which will turn out to be very expensive. At some point you will have to discard some parts of the context to be specific to the question being asked and that is where vector embeddings come into play. For one off research&#x2F;Q&amp;A 100k limit works great!</div><br/></div></div></div></div><div id="35905160" class="c"><input type="checkbox" id="c-35905160" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#35907095">prev</a><span>|</span><a href="#35905507">next</a><span>|</span><label class="collapse" for="c-35905160">[-]</label><label class="expand" for="c-35905160">[3 more]</label></div><br/><div class="children"><div class="content">big if true? :)<p>Exciting to see competition across LLMs for increasing context window size.<p>I can&#x27;t find updated pricing anywhere. Previous prices are here: <a href="https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;images&#x2F;apr-pricing-tokens.pdf" rel="nofollow">https:&#x2F;&#x2F;cdn2.assets-servd.host&#x2F;anthropic-website&#x2F;production&#x2F;...</a> but don&#x27;t seem to be embedded directly on the Anthropic website. I tried messing with the URL (apr -&gt; may&#x2F;jun) but 404&#x27;ed.</div><br/><div id="35905345" class="c"><input type="checkbox" id="c-35905345" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#35905160">parent</a><span>|</span><a href="#35905507">next</a><span>|</span><label class="collapse" for="c-35905345">[-]</label><label class="expand" for="c-35905345">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Exciting to see competition across LLMs for increasing context window size.<p>Maybe. I think the debate is going to continue about prompt optimization vs. context window size.<p>A while ago, I had a rather interesting conversation with GPT-3.5 about forgetting things. Knowing what to forget, or delete from the prompt, may be just as important as what to put in it.<p>Putting the kitchen sink into the prompt probably isn&#x27;t going to help much, past a certain point and it may be putting certain things in there based on time and context is a better strategy.</div><br/><div id="35905455" class="c"><input type="checkbox" id="c-35905455" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#35905160">root</a><span>|</span><a href="#35905345">parent</a><span>|</span><a href="#35905507">next</a><span>|</span><label class="collapse" for="c-35905455">[-]</label><label class="expand" for="c-35905455">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, there&#x27;s definitely diminishing returns. I just wanted to talk to ChatGPT about a game I&#x27;m developing. I have pages upon pages of product design notes and I&#x27;m not able to just copy&#x2F;paste the whole thing in and start talking to it at 8k context length. There&#x27;s not really duplicate information as far as I can tell since each section covers new topics. I&#x27;m sure there&#x27;s a way to express the same ideas more succinctly, but I kind of want ChatGPT to do that for me rather than me figuring out how to do that just to interface the ideas into it.</div><br/></div></div></div></div></div></div><div id="35905507" class="c"><input type="checkbox" id="c-35905507" checked=""/><div class="controls bullet"><span class="by">ibitto</span><span>|</span><a href="#35905160">prev</a><span>|</span><a href="#35905240">next</a><span>|</span><label class="collapse" for="c-35905507">[-]</label><label class="expand" for="c-35905507">[7 more]</label></div><br/><div class="children"><div class="content">Anyone using Claude? How long did it take you to get access?</div><br/><div id="35905731" class="c"><input type="checkbox" id="c-35905731" checked=""/><div class="controls bullet"><span class="by">harisec</span><span>|</span><a href="#35905507">parent</a><span>|</span><a href="#35906010">next</a><span>|</span><label class="collapse" for="c-35905731">[-]</label><label class="expand" for="c-35905731">[3 more]</label></div><br/><div class="children"><div class="content">Claude is available for free in the Poe app (poe.com). I think it&#x27;s good and underappreciated.</div><br/><div id="35908293" class="c"><input type="checkbox" id="c-35908293" checked=""/><div class="controls bullet"><span class="by">danysdragons</span><span>|</span><a href="#35905507">root</a><span>|</span><a href="#35905731">parent</a><span>|</span><a href="#35906764">next</a><span>|</span><label class="collapse" for="c-35908293">[-]</label><label class="expand" for="c-35908293">[1 more]</label></div><br/><div class="children"><div class="content">It is good, but the free subscription to Poe only provides access to Claude Instant. It&#x27;s impressively fast but not their smartest model (claude-v1.3).</div><br/></div></div><div id="35906764" class="c"><input type="checkbox" id="c-35906764" checked=""/><div class="controls bullet"><span class="by">dkarras</span><span>|</span><a href="#35905507">root</a><span>|</span><a href="#35905731">parent</a><span>|</span><a href="#35908293">prev</a><span>|</span><a href="#35906010">next</a><span>|</span><label class="collapse" for="c-35906764">[-]</label><label class="expand" for="c-35906764">[1 more]</label></div><br/><div class="children"><div class="content">yeah, been using it instead of ChatGPT and it performs better IMO. My conversational LLM of choice for sure.</div><br/></div></div></div></div><div id="35906010" class="c"><input type="checkbox" id="c-35906010" checked=""/><div class="controls bullet"><span class="by">Mizza</span><span>|</span><a href="#35905507">parent</a><span>|</span><a href="#35905731">prev</a><span>|</span><a href="#35905240">next</a><span>|</span><label class="collapse" for="c-35906010">[-]</label><label class="expand" for="c-35906010">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve got access, it&#x27;s _blazing_ fast and seems very good. Solved some of my little puzzles that other models couldn&#x27;t. I haven&#x27;t tried ChatGPT-4 yet, but it&#x27;s the best one that I have used.</div><br/><div id="35906210" class="c"><input type="checkbox" id="c-35906210" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#35905507">root</a><span>|</span><a href="#35906010">parent</a><span>|</span><a href="#35909308">next</a><span>|</span><label class="collapse" for="c-35906210">[-]</label><label class="expand" for="c-35906210">[1 more]</label></div><br/><div class="children"><div class="content">You need to try GPT4 only because GPT3.5 really doesn&#x27;t compare to it in a lot of ways.</div><br/></div></div><div id="35909308" class="c"><input type="checkbox" id="c-35909308" checked=""/><div class="controls bullet"><span class="by">iEchoic</span><span>|</span><a href="#35905507">root</a><span>|</span><a href="#35906010">parent</a><span>|</span><a href="#35906210">prev</a><span>|</span><a href="#35905240">next</a><span>|</span><label class="collapse" for="c-35909308">[-]</label><label class="expand" for="c-35909308">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 is a major leap ahead of everything else I&#x27;ve used (including GPT-3.5), so definitely worth trying for comparison.</div><br/></div></div></div></div></div></div><div id="35905240" class="c"><input type="checkbox" id="c-35905240" checked=""/><div class="controls bullet"><span class="by">skilled</span><span>|</span><a href="#35905507">prev</a><span>|</span><a href="#35906129">next</a><span>|</span><label class="collapse" for="c-35905240">[-]</label><label class="expand" for="c-35905240">[1 more]</label></div><br/><div class="children"><div class="content">My wallet is hardly capable of handling 8k GPT-4.</div><br/></div></div><div id="35905989" class="c"><input type="checkbox" id="c-35905989" checked=""/><div class="controls bullet"><span class="by">melvinmelih</span><span>|</span><a href="#35906129">prev</a><span>|</span><a href="#35905318">next</a><span>|</span><label class="collapse" for="c-35905989">[-]</label><label class="expand" for="c-35905989">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  You can drop multiple documents or even a book into the prompt and then ask Claude questions that require synthesis of knowledge across many parts of the text.<p>This is cool but does it also work the other way around? Generate a book&#x27;s worth of content based on a single prompt?</div><br/><div id="35906180" class="c"><input type="checkbox" id="c-35906180" checked=""/><div class="controls bullet"><span class="by">Chabsff</span><span>|</span><a href="#35905989">parent</a><span>|</span><a href="#35908878">next</a><span>|</span><label class="collapse" for="c-35906180">[-]</label><label class="expand" for="c-35906180">[1 more]</label></div><br/><div class="children"><div class="content">Kinda. But it`s going to be a lot like how data compression works. There will always be a somewhat fundamental limit to how much &quot;creativity&quot; you can get out of a small prompt generating large texts when using an isolated model.</div><br/></div></div><div id="35908878" class="c"><input type="checkbox" id="c-35908878" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#35905989">parent</a><span>|</span><a href="#35906180">prev</a><span>|</span><a href="#35905318">next</a><span>|</span><label class="collapse" for="c-35908878">[-]</label><label class="expand" for="c-35908878">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a good question. Can Claude write a coherent book?</div><br/></div></div></div></div><div id="35905318" class="c"><input type="checkbox" id="c-35905318" checked=""/><div class="controls bullet"><span class="by">helen___keller</span><span>|</span><a href="#35905989">prev</a><span>|</span><a href="#35906937">next</a><span>|</span><label class="collapse" for="c-35905318">[-]</label><label class="expand" for="c-35905318">[2 more]</label></div><br/><div class="children"><div class="content">This seems like it could be a game changer. Modern LLM based applications face a balancing act of context limitations, which often results in some kind of mapreduce-type behavior when that context can’t fit the input<p>If contexts keep growing, the landscape of LLM application engineering will as well</div><br/><div id="35905341" class="c"><input type="checkbox" id="c-35905341" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#35905318">parent</a><span>|</span><a href="#35906937">next</a><span>|</span><label class="collapse" for="c-35905341">[-]</label><label class="expand" for="c-35905341">[1 more]</label></div><br/><div class="children"><div class="content">The problem is there are no public benchmarks usually so it is hard to really compare on long context lengths to see if they are still performing equally intelligent of tasks.</div><br/></div></div></div></div><div id="35906937" class="c"><input type="checkbox" id="c-35906937" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#35905318">prev</a><span>|</span><a href="#35906380">next</a><span>|</span><label class="collapse" for="c-35906937">[-]</label><label class="expand" for="c-35906937">[1 more]</label></div><br/><div class="children"><div class="content">I often prefer Claude over GPT4 (partially due to speed), but it degrades more quickly. Like I can get a better response early, but usually the quality drops faster. But, sometimes if it can really vibe with it, it gets better over time.</div><br/></div></div><div id="35906380" class="c"><input type="checkbox" id="c-35906380" checked=""/><div class="controls bullet"><span class="by">cheeselip420</span><span>|</span><a href="#35906937">prev</a><span>|</span><a href="#35905513">next</a><span>|</span><label class="collapse" for="c-35906380">[-]</label><label class="expand" for="c-35906380">[9 more]</label></div><br/><div class="children"><div class="content">Maybe this model can finish Winds of Winter and the rest of GoT for us...</div><br/><div id="35907142" class="c"><input type="checkbox" id="c-35907142" checked=""/><div class="controls bullet"><span class="by">azakai</span><span>|</span><a href="#35906380">parent</a><span>|</span><a href="#35908322">next</a><span>|</span><label class="collapse" for="c-35907142">[-]</label><label class="expand" for="c-35907142">[4 more]</label></div><br/><div class="children"><div class="content">75,000 words is a drop in the bucket for A Song of Ice and Fire:<p><a href="https:&#x2F;&#x2F;blog.fostergrant.co.uk&#x2F;2017&#x2F;08&#x2F;03&#x2F;word-counts-popular-books-world&#x2F;#:~:text=A%20Feast%20for%20Crows%20%E2%80%93%20300%2C000,(so%20far)%20%E2%80%93%201%2C770%2C000%20words" rel="nofollow">https:&#x2F;&#x2F;blog.fostergrant.co.uk&#x2F;2017&#x2F;08&#x2F;03&#x2F;word-counts-popula...</a></div><br/><div id="35907779" class="c"><input type="checkbox" id="c-35907779" checked=""/><div class="controls bullet"><span class="by">camel-cdr</span><span>|</span><a href="#35906380">root</a><span>|</span><a href="#35907142">parent</a><span>|</span><a href="#35907857">next</a><span>|</span><label class="collapse" for="c-35907779">[-]</label><label class="expand" for="c-35907779">[1 more]</label></div><br/><div class="children"><div class="content">Meanwhile web serial authors: [0] [1]<p>[0] <a href="https:&#x2F;&#x2F;wanderinginn.neocities.org&#x2F;statistics" rel="nofollow">https:&#x2F;&#x2F;wanderinginn.neocities.org&#x2F;statistics</a><p>[1] <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Parahumans&#x2F;comments&#x2F;rz8ogt&#x2F;wildbows_crazy_total_word_count_and_rate_of&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Parahumans&#x2F;comments&#x2F;rz8ogt&#x2F;wildbows...</a></div><br/></div></div><div id="35907857" class="c"><input type="checkbox" id="c-35907857" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35906380">root</a><span>|</span><a href="#35907142">parent</a><span>|</span><a href="#35907779">prev</a><span>|</span><a href="#35908322">next</a><span>|</span><label class="collapse" for="c-35907857">[-]</label><label class="expand" for="c-35907857">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;d want to generate it in multiple steps to make it feasible to control the text generation anyway. First call generates the broad outline, several parallel calls flesh out character development and some other details so that they&#x27;re consistent, then generate the story piece by piece by feeding in bits of the outline.</div><br/><div id="35908295" class="c"><input type="checkbox" id="c-35908295" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#35906380">root</a><span>|</span><a href="#35907857">parent</a><span>|</span><a href="#35908322">next</a><span>|</span><label class="collapse" for="c-35908295">[-]</label><label class="expand" for="c-35908295">[1 more]</label></div><br/><div class="children"><div class="content">And then you end up with what the movie did which is not exactly a GRRM novel.</div><br/></div></div></div></div></div></div><div id="35908322" class="c"><input type="checkbox" id="c-35908322" checked=""/><div class="controls bullet"><span class="by">f6v</span><span>|</span><a href="#35906380">parent</a><span>|</span><a href="#35907142">prev</a><span>|</span><a href="#35907256">next</a><span>|</span><label class="collapse" for="c-35908322">[-]</label><label class="expand" for="c-35908322">[1 more]</label></div><br/><div class="children"><div class="content">I’d be excited for Dexter ending that doesn’t suck.</div><br/></div></div><div id="35907256" class="c"><input type="checkbox" id="c-35907256" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#35906380">parent</a><span>|</span><a href="#35908322">prev</a><span>|</span><a href="#35906914">next</a><span>|</span><label class="collapse" for="c-35907256">[-]</label><label class="expand" for="c-35907256">[1 more]</label></div><br/><div class="children"><div class="content">That may need a million tokens just for one book, though!</div><br/></div></div><div id="35906914" class="c"><input type="checkbox" id="c-35906914" checked=""/><div class="controls bullet"><span class="by">thepasswordis</span><span>|</span><a href="#35906380">parent</a><span>|</span><a href="#35907256">prev</a><span>|</span><a href="#35907363">next</a><span>|</span><label class="collapse" for="c-35906914">[-]</label><label class="expand" for="c-35906914">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s actually a really interesting use case!</div><br/></div></div><div id="35907363" class="c"><input type="checkbox" id="c-35907363" checked=""/><div class="controls bullet"><span class="by">babuloseo</span><span>|</span><a href="#35906380">parent</a><span>|</span><a href="#35906914">prev</a><span>|</span><a href="#35905513">next</a><span>|</span><label class="collapse" for="c-35907363">[-]</label><label class="expand" for="c-35907363">[1 more]</label></div><br/><div class="children"><div class="content">Add Berserk to that list.</div><br/></div></div></div></div><div id="35905513" class="c"><input type="checkbox" id="c-35905513" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#35906380">prev</a><span>|</span><a href="#35905708">next</a><span>|</span><label class="collapse" for="c-35905513">[-]</label><label class="expand" for="c-35905513">[4 more]</label></div><br/><div class="children"><div class="content">I am curious how consistent Claude is at obeying detailed instructions. One issue ChatGPT 3.5 and 4 have, even with just a few hundred words of instructions, is it forgets instructions given to it earlier on.[1]<p>This huge context window is awesome though, I&#x27;m trying to use LLMs to do small town social interaction simulations, with output in a structured format. Finding ways to compress existing state and pass it around, so the LLM knows the current state of what people in the town did for a given day is hard with a tiny token limit!<p>[1] For my use cases, early instructions tend to be describing a DSL syntax for responses, if I add too much info after the instructions, the response syntax starts getting wonky!</div><br/><div id="35906438" class="c"><input type="checkbox" id="c-35906438" checked=""/><div class="controls bullet"><span class="by">rescripting</span><span>|</span><a href="#35905513">parent</a><span>|</span><a href="#35905708">next</a><span>|</span><label class="collapse" for="c-35906438">[-]</label><label class="expand" for="c-35906438">[3 more]</label></div><br/><div class="children"><div class="content">A simple example I ran in to was I asked ChatGPT to generate me story in madlibs format for my 4 year old daughter. They&#x27;re in the format &quot;The young _____ went to the ______, ...&quot;, and she fills in the blanks with silly nouns&#x2F;adjectives.<p>As she kept asking for more, I prompted &quot;great, do another one&quot; and eventually my original instruction fell out of the context window. It continued to generate a children&#x27;s story, but with no more blanks.</div><br/><div id="35906782" class="c"><input type="checkbox" id="c-35906782" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#35905513">root</a><span>|</span><a href="#35906438">parent</a><span>|</span><a href="#35905708">next</a><span>|</span><label class="collapse" for="c-35906782">[-]</label><label class="expand" for="c-35906782">[2 more]</label></div><br/><div class="children"><div class="content">This is actually a different issue, largely a UI one, although one I wish ChatGPT would fix it.<p>There is no good way to tell it &quot;this isn&#x27;t a conversation, just repeat the answer to the initial prompt again&quot;.<p>The solution is to just re-paste the initial prompt in each time, but still it isn&#x27;t ideal. There isn&#x27;t a good way to tell chatgpt &quot;you can throw away all the context after the initial prompt and up until now&quot;.<p>Of course the entire point of ChatGPT is that it maintains a conversation thread, so I get why they don&#x27;t fix up this edge case.<p>My problem is more of, I give ChatGPT some complicated instructions, and it&#x27;ll start forgetting the early on instructions long before any token limit is reached.<p>So for example, if early on I ask for certain tokens to be returned in parens, well my initial prompt is too long, it&#x27;ll forget the parens thing and start returning tokens without the surrounding (), which then breaks my parser!</div><br/><div id="35908753" class="c"><input type="checkbox" id="c-35908753" checked=""/><div class="controls bullet"><span class="by">orost</span><span>|</span><a href="#35905513">root</a><span>|</span><a href="#35906782">parent</a><span>|</span><a href="#35905708">next</a><span>|</span><label class="collapse" for="c-35908753">[-]</label><label class="expand" for="c-35908753">[1 more]</label></div><br/><div class="children"><div class="content">Almost every UI for LLMs I&#x27;ve seen has a way to specify an initial prompt that never goes out of context, it&#x27;s strange that it&#x27;s not a feature in ChatGPT.</div><br/></div></div></div></div></div></div></div></div><div id="35905708" class="c"><input type="checkbox" id="c-35905708" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#35905513">prev</a><span>|</span><a href="#35908839">next</a><span>|</span><label class="collapse" for="c-35905708">[-]</label><label class="expand" for="c-35905708">[2 more]</label></div><br/><div class="children"><div class="content">Anthropic is basically Google&#x27;s openAI.</div><br/><div id="35908915" class="c"><input type="checkbox" id="c-35908915" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#35905708">parent</a><span>|</span><a href="#35908839">next</a><span>|</span><label class="collapse" for="c-35908915">[-]</label><label class="expand" for="c-35908915">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a Google company, their share amounts to ~10%.</div><br/></div></div></div></div><div id="35908839" class="c"><input type="checkbox" id="c-35908839" checked=""/><div class="controls bullet"><span class="by">pk-protect-ai</span><span>|</span><a href="#35905708">prev</a><span>|</span><a href="#35905330">next</a><span>|</span><label class="collapse" for="c-35908839">[-]</label><label class="expand" for="c-35908839">[1 more]</label></div><br/><div class="children"><div class="content">Ok. It has spatial comprehension of some level. Unlike GPT-4 it lacks proper time comprehension because it is bad at calculus. Unlike GPT-4 it can&#x27;t properly solve traveling salesman problem.</div><br/></div></div><div id="35905330" class="c"><input type="checkbox" id="c-35905330" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#35908839">prev</a><span>|</span><label class="collapse" for="c-35905330">[-]</label><label class="expand" for="c-35905330">[1 more]</label></div><br/><div class="children"><div class="content">Would be great to see some benchmarks on how loss changes across this very large context. It’s been technically possible to do 1mln+ token context for some time with performance deterioration so it would be interesting to see how this compares to those efforts</div><br/></div></div></div></div></div></div></div></body></html>