<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716022854490" as="style"/><link rel="stylesheet" href="styles.css?v=1716022854490"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.06067">HMT: Hierarchical Memory Transformer for Long Context Language Processing</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>jasondavies</span> | <span>6 comments</span></div><br/><div><div id="40390201" class="c"><input type="checkbox" id="c-40390201" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><label class="collapse" for="c-40390201">[-]</label><label class="expand" for="c-40390201">[5 more]</label></div><br/><div class="children"><div class="content">Code: <a href="https:&#x2F;&#x2F;github.com&#x2F;OswaldHe&#x2F;HMT-pytorch">https:&#x2F;&#x2F;github.com&#x2F;OswaldHe&#x2F;HMT-pytorch</a><p>This looks really interesting. I&#x27;ve added the paper to my reading list and look forward to playing with the code. I&#x27;m curious to see what kinds of improvements we can get by agumenting Transformers and other generative sequence models with this and other mechanisms implementing hierarchical memory.[a]<p>Shouldn&#x27;t the authors cite the work by Jeff Hawkins et al at Numenta? Hawkins has been proposing AI models with hierarchical temporal memory for a long time.[b] I can&#x27;t help but wonder if there is a way, somehow, to incorporate his work and ideas in Transformers and other generative sequence models.<p>We sure live in interesting times!<p>---<p>[a] In the past, I&#x27;ve experimented with mechanisms that add memory to Transformers, but never with <i>hierarchy</i>.<p>[b] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hierarchical_temporal_memory" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hierarchical_temporal_memory</a></div><br/><div id="40392771" class="c"><input type="checkbox" id="c-40392771" checked=""/><div class="controls bullet"><span class="by">anthonyskipper</span><span>|</span><a href="#40390201">parent</a><span>|</span><a href="#40395369">next</a><span>|</span><label class="collapse" for="c-40392771">[-]</label><label class="expand" for="c-40392771">[2 more]</label></div><br/><div class="children"><div class="content">I thought Hawkins&#x27;s book &quot;On Intelligence&quot; was amazing.  It&#x27;s a bit wild how close things have followed to the direction he laid out.</div><br/><div id="40396435" class="c"><input type="checkbox" id="c-40396435" checked=""/><div class="controls bullet"><span class="by">rawrawrawrr</span><span>|</span><a href="#40390201">root</a><span>|</span><a href="#40392771">parent</a><span>|</span><a href="#40395369">next</a><span>|</span><label class="collapse" for="c-40396435">[-]</label><label class="expand" for="c-40396435">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s kinda hilarious, because I think the book was exactly wrong in its predictions. This can be evidenced by the continuous failures of his AI company, Numenta.</div><br/></div></div></div></div><div id="40395369" class="c"><input type="checkbox" id="c-40395369" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40390201">parent</a><span>|</span><a href="#40392771">prev</a><span>|</span><label class="collapse" for="c-40395369">[-]</label><label class="expand" for="c-40395369">[2 more]</label></div><br/><div class="children"><div class="content">Does it relate to applications in time series? How does the hierarchy play a role in Transformers?</div><br/><div id="40395649" class="c"><input type="checkbox" id="c-40395649" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40390201">root</a><span>|</span><a href="#40395369">parent</a><span>|</span><label class="collapse" for="c-40395649">[-]</label><label class="expand" for="c-40395649">[1 more]</label></div><br/><div class="children"><div class="content">Not really. But questions about the work should be asked to the authors. I&#x27;ve only skimmed the paper.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>