<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704445253794" as="style"/><link rel="stylesheet" href="styles.css?v=1704445253794"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/facebookresearch/audio2photoreal">Audio2Photoreal</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>wildpeaks</span> | <span>49 comments</span></div><br/><div><div id="38874426" class="c"><input type="checkbox" id="c-38874426" checked=""/><div class="controls bullet"><span class="by">1shooner</span><span>|</span><a href="#38873725">next</a><span>|</span><label class="collapse" for="c-38874426">[-]</label><label class="expand" for="c-38874426">[1 more]</label></div><br/><div class="children"><div class="content">I appreciate this is research, but I wonder: are these gestures actually semantically distinct information, which the model is better at extrapolating from audio than the listener? Or are they just redundant visual cues that perhaps relieve some cognitive load when communicating with someone?<p>I&#x27;m apprehensive about accepting nonverbal communication that a model has appended to a human source.</div><br/></div></div><div id="38873725" class="c"><input type="checkbox" id="c-38873725" checked=""/><div class="controls bullet"><span class="by">holmesworcester</span><span>|</span><a href="#38874426">prev</a><span>|</span><a href="#38871953">next</a><span>|</span><label class="collapse" for="c-38873725">[-]</label><label class="expand" for="c-38873725">[4 more]</label></div><br/><div class="children"><div class="content">The sample conversations here are hilarious, especially compared to the typical academic or corporate AI paper.</div><br/><div id="38876023" class="c"><input type="checkbox" id="c-38876023" checked=""/><div class="controls bullet"><span class="by">55555</span><span>|</span><a href="#38873725">parent</a><span>|</span><a href="#38874839">next</a><span>|</span><label class="collapse" for="c-38876023">[-]</label><label class="expand" for="c-38876023">[1 more]</label></div><br/><div class="children"><div class="content">I was also shocked. I think Meta is making this for their metaverse project, and their internal Facebook data shows that most people most of the time talk like complete idiots, barely able to mash a thought out of their mouth. I’ve never heard more inarticulate training data.</div><br/></div></div><div id="38874839" class="c"><input type="checkbox" id="c-38874839" checked=""/><div class="controls bullet"><span class="by">white_eskimo</span><span>|</span><a href="#38873725">parent</a><span>|</span><a href="#38876023">prev</a><span>|</span><a href="#38873988">next</a><span>|</span><label class="collapse" for="c-38874839">[-]</label><label class="expand" for="c-38874839">[1 more]</label></div><br/><div class="children"><div class="content">Who came up with these conversations? Hard to believe this was part of the training data. And where does the training data come from? To what extent is it applicable to academic or corporate settings?</div><br/></div></div><div id="38873988" class="c"><input type="checkbox" id="c-38873988" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#38873725">parent</a><span>|</span><a href="#38874839">prev</a><span>|</span><a href="#38871953">next</a><span>|</span><label class="collapse" for="c-38873988">[-]</label><label class="expand" for="c-38873988">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re hilarious but the stuttering feels a little over the top IMHO</div><br/></div></div></div></div><div id="38871953" class="c"><input type="checkbox" id="c-38871953" checked=""/><div class="controls bullet"><span class="by">ArekDymalski</span><span>|</span><a href="#38873725">prev</a><span>|</span><a href="#38876525">next</a><span>|</span><label class="collapse" for="c-38871953">[-]</label><label class="expand" for="c-38871953">[1 more]</label></div><br/><div class="children"><div class="content">Impressive. Even at current state it would make RPGs like Fallout or Skyrim sooo much more alive ...</div><br/></div></div><div id="38876525" class="c"><input type="checkbox" id="c-38876525" checked=""/><div class="controls bullet"><span class="by">philsnow</span><span>|</span><a href="#38871953">prev</a><span>|</span><a href="#38876111">next</a><span>|</span><label class="collapse" for="c-38876525">[-]</label><label class="expand" for="c-38876525">[1 more]</label></div><br/><div class="children"><div class="content">Really want to see this on the broccoli man bit about wanting to serve 5TB</div><br/></div></div><div id="38876111" class="c"><input type="checkbox" id="c-38876111" checked=""/><div class="controls bullet"><span class="by">19h</span><span>|</span><a href="#38876525">prev</a><span>|</span><a href="#38873454">next</a><span>|</span><label class="collapse" for="c-38876111">[-]</label><label class="expand" for="c-38876111">[1 more]</label></div><br/><div class="children"><div class="content">I expected something this: <a href="https:&#x2F;&#x2F;speech2face.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;speech2face.github.io&#x2F;</a> (arbitrary voices) .. this model seems to have been trained for each and every specific speaker?</div><br/></div></div><div id="38873454" class="c"><input type="checkbox" id="c-38873454" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#38876111">prev</a><span>|</span><a href="#38873926">next</a><span>|</span><label class="collapse" for="c-38873454">[-]</label><label class="expand" for="c-38873454">[5 more]</label></div><br/><div class="children"><div class="content">Goddamn that’s cool.<p>End-state for Winamp vizualizers: synthesize an entire living world from the audio alone.</div><br/><div id="38873717" class="c"><input type="checkbox" id="c-38873717" checked=""/><div class="controls bullet"><span class="by">bagful</span><span>|</span><a href="#38873454">parent</a><span>|</span><a href="#38873926">next</a><span>|</span><label class="collapse" for="c-38873717">[-]</label><label class="expand" for="c-38873717">[4 more]</label></div><br/><div class="children"><div class="content">I dare you to make it look better than the pictures that form in my mind’s eye</div><br/><div id="38873868" class="c"><input type="checkbox" id="c-38873868" checked=""/><div class="controls bullet"><span class="by">error9348</span><span>|</span><a href="#38873454">root</a><span>|</span><a href="#38873717">parent</a><span>|</span><a href="#38873926">next</a><span>|</span><label class="collapse" for="c-38873868">[-]</label><label class="expand" for="c-38873868">[3 more]</label></div><br/><div class="children"><div class="content">Easy to beat my mind&#x27;s eye<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Aphantasia" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Aphantasia</a></div><br/><div id="38874377" class="c"><input type="checkbox" id="c-38874377" checked=""/><div class="controls bullet"><span class="by">ghhdvki</span><span>|</span><a href="#38873454">root</a><span>|</span><a href="#38873868">parent</a><span>|</span><a href="#38873926">next</a><span>|</span><label class="collapse" for="c-38874377">[-]</label><label class="expand" for="c-38874377">[2 more]</label></div><br/><div class="children"><div class="content">Huh. Didn&#x27;t know this was a thing, but I guess I&#x27;m aphantasic.<p>Thanks for letting me know.</div><br/><div id="38876180" class="c"><input type="checkbox" id="c-38876180" checked=""/><div class="controls bullet"><span class="by">GauntletWizard</span><span>|</span><a href="#38873454">root</a><span>|</span><a href="#38874377">parent</a><span>|</span><a href="#38873926">next</a><span>|</span><label class="collapse" for="c-38876180">[-]</label><label class="expand" for="c-38876180">[1 more]</label></div><br/><div class="children"><div class="content">When it was brought up a bit ago, I thought I was aphantasic. It turned out I&#x27;m just poorly trained at it - I don&#x27;t tend to rely on memory for physical details, and I&#x27;m aggressively poor at visual arts. With a little thought put into it, though, I realized I can &quot;See&quot; in my &quot;Minds Eye&quot;, and with some active training I&#x27;m still not good at it - It&#x27;s not a tool I reach for naturally - but I can &quot;Imagine&quot; some images if I try at it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38873926" class="c"><input type="checkbox" id="c-38873926" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38873454">prev</a><span>|</span><a href="#38873653">next</a><span>|</span><label class="collapse" for="c-38873926">[-]</label><label class="expand" for="c-38873926">[3 more]</label></div><br/><div class="children"><div class="content">This is one of the coolest things I&#x27;ve seen that I also cannot understand... why? Aren&#x27;t you going to need to tune it on yourself? Because otherwise you&#x27;re going to adopt the gesticulation of others (who it was trained on). Maybe for videogames? Or like NPCs in VR environments? But then doesn&#x27;t that become robotic and then we get back to feeling uncanny valley after we normalized? I mean the network __has__ to do significant amounts of memorization unless conceivably the microphone can pick up a signal that actually corresponds to the 3d spatial movements (could be possible, but this doesn&#x27;t seem that). Maybe that&#x27;s what they&#x27;re working towards and this is an iteration towards that?<p>It&#x27;s technologically impressive, but I&#x27;m failing to see the use. Can someone else enlighten me? I&#x27;m sure there&#x27;s something I&#x27;m failing to see.</div><br/><div id="38876033" class="c"><input type="checkbox" id="c-38876033" checked=""/><div class="controls bullet"><span class="by">55555</span><span>|</span><a href="#38873926">parent</a><span>|</span><a href="#38874050">next</a><span>|</span><label class="collapse" for="c-38876033">[-]</label><label class="expand" for="c-38876033">[1 more]</label></div><br/><div class="children"><div class="content">Google “Horizon Worlds”</div><br/></div></div></div></div><div id="38873653" class="c"><input type="checkbox" id="c-38873653" checked=""/><div class="controls bullet"><span class="by">iamleppert</span><span>|</span><a href="#38873926">prev</a><span>|</span><a href="#38872447">next</a><span>|</span><label class="collapse" for="c-38873653">[-]</label><label class="expand" for="c-38873653">[1 more]</label></div><br/><div class="children"><div class="content">This reminds me of the old Titanic CD-ROM adventure game avatars.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;0pXBXIrB478?si=iQ5YtDPBSaq0ynsv" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;0pXBXIrB478?si=iQ5YtDPBSaq0ynsv</a><p>I honestly prefer the Titanic avatars though.</div><br/></div></div><div id="38872447" class="c"><input type="checkbox" id="c-38872447" checked=""/><div class="controls bullet"><span class="by">leshokunin</span><span>|</span><a href="#38873653">prev</a><span>|</span><a href="#38874261">next</a><span>|</span><label class="collapse" for="c-38872447">[-]</label><label class="expand" for="c-38872447">[4 more]</label></div><br/><div class="children"><div class="content">Pretty cool. It&#x27;s going to take a while to make it into a usable product though. Having conversations with people flailing their hands algorithmically is going to feel weird until it gets more natural. Right now it feels like those &quot;blink every n&quot; scripts.</div><br/><div id="38873458" class="c"><input type="checkbox" id="c-38873458" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#38872447">parent</a><span>|</span><a href="#38874261">next</a><span>|</span><label class="collapse" for="c-38873458">[-]</label><label class="expand" for="c-38873458">[3 more]</label></div><br/><div class="children"><div class="content">Every video game NPC is basically following such an algorithm.</div><br/><div id="38873621" class="c"><input type="checkbox" id="c-38873621" checked=""/><div class="controls bullet"><span class="by">leshokunin</span><span>|</span><a href="#38872447">root</a><span>|</span><a href="#38873458">parent</a><span>|</span><a href="#38874700">next</a><span>|</span><label class="collapse" for="c-38873621">[-]</label><label class="expand" for="c-38873621">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s incorrect to generalize like this. NPCs maybe use broad procedural rules, full mocap or really anything in between.<p>Yes, this could apply to them. No, it doesn&#x27;t mean it always does.<p>This also doesn&#x27;t really add to the point that it wouldn&#x27;t be suited to human conversation. You rarely use NPCs to have human like chats, especially for several minutes. The few games that do would be Mass Effect or LA Noire, both of which use mocap to avoid the effect I&#x27;m referring to.</div><br/></div></div><div id="38874700" class="c"><input type="checkbox" id="c-38874700" checked=""/><div class="controls bullet"><span class="by">alanbernstein</span><span>|</span><a href="#38872447">root</a><span>|</span><a href="#38873458">parent</a><span>|</span><a href="#38873621">prev</a><span>|</span><a href="#38874261">next</a><span>|</span><label class="collapse" for="c-38874700">[-]</label><label class="expand" for="c-38874700">[1 more]</label></div><br/><div class="children"><div class="content">See this classic deep-learning based example: <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=pAsw16y3U-c" rel="nofollow">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=pAsw16y3U-c</a></div><br/></div></div></div></div></div></div><div id="38874261" class="c"><input type="checkbox" id="c-38874261" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#38872447">prev</a><span>|</span><a href="#38872148">next</a><span>|</span><label class="collapse" for="c-38874261">[-]</label><label class="expand" for="c-38874261">[4 more]</label></div><br/><div class="children"><div class="content">Why do so many of these news demos require old versions of CUDA? It’s quite annoying having to juggle the installs and disk usage of CUDA 11.6,11.7,11.8,12.1,12.2,12.3</div><br/><div id="38875507" class="c"><input type="checkbox" id="c-38875507" checked=""/><div class="controls bullet"><span class="by">fxtentacle</span><span>|</span><a href="#38874261">parent</a><span>|</span><a href="#38874383">next</a><span>|</span><label class="collapse" for="c-38875507">[-]</label><label class="expand" for="c-38875507">[1 more]</label></div><br/><div class="children"><div class="content">Because new CUDA versions often come with new bugs, so as a researcher with a publication deadline, you&#x27;re incentivised to never upgrade. I still vividly remember how messy it was to downgrade a large TensorFlow project to a previous version which still supported an older CUDA because the current CUDA cuBLAS couldn&#x27;t do matrix multiplication without silent memory corruption. (I believe it was 9.x to 10.0 that broke it) And we lacked the connections to directly talk to any engineer at NVIDIA, which meant that nobody even confirmed the bugs existence after we submitted a GitHub issue. Eventually, we found a way to trigger a crash on Google Colab using the bug and then someone from Google filed it again with NVIDIA and then things finally got fixed. About 6 months after the research paper was finished.</div><br/></div></div><div id="38874383" class="c"><input type="checkbox" id="c-38874383" checked=""/><div class="controls bullet"><span class="by">gwervc</span><span>|</span><a href="#38874261">parent</a><span>|</span><a href="#38875507">prev</a><span>|</span><a href="#38872148">next</a><span>|</span><label class="collapse" for="c-38874383">[-]</label><label class="expand" for="c-38874383">[2 more]</label></div><br/><div class="children"><div class="content">Because it takes time to write a paper and publish it. And researchers usually don&#x27;t care about code so the version used when starting the project is never upgraded.</div><br/><div id="38876810" class="c"><input type="checkbox" id="c-38876810" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#38874261">root</a><span>|</span><a href="#38874383">parent</a><span>|</span><a href="#38872148">next</a><span>|</span><label class="collapse" for="c-38876810">[-]</label><label class="expand" for="c-38876810">[1 more]</label></div><br/><div class="children"><div class="content">Never upgrading the dependencies is not specific to research.<p>For example the npm installations of react show that old versions are still downloaded a lot in the last 7 days: <a href="https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;react?activeTab=versions" rel="nofollow">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;react?activeTab=versions</a><p>jQuery on a random CDN with stats also shows that many websites are likely never going to update it: <a href="https:&#x2F;&#x2F;www.jsdelivr.com&#x2F;package&#x2F;npm&#x2F;jquery?tab=stats" rel="nofollow">https:&#x2F;&#x2F;www.jsdelivr.com&#x2F;package&#x2F;npm&#x2F;jquery?tab=stats</a><p>You also sees that with system administrators: here is curl on debian from people who shares about it: <a href="https:&#x2F;&#x2F;qa.debian.org&#x2F;popcon-graph.php?packages=curl&amp;show_installed=on&amp;show_old=on&amp;show_recent=on&amp;want_legend=on&amp;want_ticks=on&amp;from_date=&amp;to_date=&amp;hlght_date=&amp;date_fmt=%25Y-%25m&amp;beenhere=1" rel="nofollow">https:&#x2F;&#x2F;qa.debian.org&#x2F;popcon-graph.php?packages=curl&amp;show_in...</a></div><br/></div></div></div></div></div></div><div id="38872148" class="c"><input type="checkbox" id="c-38872148" checked=""/><div class="controls bullet"><span class="by">aantix</span><span>|</span><a href="#38874261">prev</a><span>|</span><a href="#38871851">next</a><span>|</span><label class="collapse" for="c-38872148">[-]</label><label class="expand" for="c-38872148">[12 more]</label></div><br/><div class="children"><div class="content">Why would we want an avatar vs a real video stream of the actual person?</div><br/><div id="38872208" class="c"><input type="checkbox" id="c-38872208" checked=""/><div class="controls bullet"><span class="by">kuschku</span><span>|</span><a href="#38872148">parent</a><span>|</span><a href="#38873419">next</a><span>|</span><label class="collapse" for="c-38872208">[-]</label><label class="expand" for="c-38872208">[5 more]</label></div><br/><div class="children"><div class="content">Being able to have an avatar that fits your voice without having to actually look like that has many applications.<p>Whether you&#x27;re trans or you just want to join a video call early in the morning without dressing up, the applications are endless.<p>In many situations we demand that people dress or present a certain way, just out of bullshit social expectations. This is one way to eat your cake and have it too.</div><br/><div id="38873976" class="c"><input type="checkbox" id="c-38873976" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38872148">root</a><span>|</span><a href="#38872208">parent</a><span>|</span><a href="#38872905">next</a><span>|</span><label class="collapse" for="c-38873976">[-]</label><label class="expand" for="c-38873976">[1 more]</label></div><br/><div class="children"><div class="content">I get that this is to drive the avatar, but I&#x27;m curious as to why. There&#x27;s stronger signals with video, which I&#x27;m certain taking in even a very low resolution image would have stronger signals to convey movement than audio does (either the network is memorizing a lot (which is fine, but limited), or this is an iteration towards a 3D high sensitivity audio driven for precise sound? Something else?). I mean the quest has cameras in it, so why not use those? Computation? They aren&#x27;t big models (largest is 1.42G, smallest 0.58GB)</div><br/></div></div><div id="38872905" class="c"><input type="checkbox" id="c-38872905" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#38872148">root</a><span>|</span><a href="#38872208">parent</a><span>|</span><a href="#38873976">prev</a><span>|</span><a href="#38873419">next</a><span>|</span><label class="collapse" for="c-38872905">[-]</label><label class="expand" for="c-38872905">[3 more]</label></div><br/><div class="children"><div class="content">For those use cases you should be able to get much more accurate results using a base video stream. This more fits use cases where you&#x27;re lacking a video stream but not necessarily because you just don&#x27;t want to turn it on.</div><br/><div id="38873465" class="c"><input type="checkbox" id="c-38873465" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#38872148">root</a><span>|</span><a href="#38872905">parent</a><span>|</span><a href="#38873419">next</a><span>|</span><label class="collapse" for="c-38873465">[-]</label><label class="expand" for="c-38873465">[2 more]</label></div><br/><div class="children"><div class="content">A video stream isn’t volumetric.<p>This is for the metaverse.</div><br/><div id="38873753" class="c"><input type="checkbox" id="c-38873753" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#38872148">root</a><span>|</span><a href="#38873465">parent</a><span>|</span><a href="#38873419">next</a><span>|</span><label class="collapse" for="c-38873753">[-]</label><label class="expand" for="c-38873753">[1 more]</label></div><br/><div class="children"><div class="content">I think metaverse is their primary target in making it as well, as I mentioned in a sibling comment, but modeling an avatar then rendering it is probably the easiest computational way of generating a video, even if the video itself isn&#x27;t volumetric. See: traditional 3D avatars already in meeting apps which use the video feed.<p>I&#x27;m not sure if there would be any other potential use cases beyond these two. Or rather, I&#x27;m not able to think of them at so far.</div><br/></div></div></div></div></div></div></div></div><div id="38873419" class="c"><input type="checkbox" id="c-38873419" checked=""/><div class="controls bullet"><span class="by">plaguuuuuu</span><span>|</span><a href="#38872148">parent</a><span>|</span><a href="#38872208">prev</a><span>|</span><a href="#38872278">next</a><span>|</span><label class="collapse" for="c-38873419">[-]</label><label class="expand" for="c-38873419">[1 more]</label></div><br/><div class="children"><div class="content">Either games or its just interesting research that mostly ties in with what FB is doing. Cause there are problems like, e.g. imagine the bandwidth requirement of streaming 3D copies of like 20 people in a room<p>it&#x27;s simply not possible within the near future, even today zoom&#x2F;teams video conferencing is somehow highly compressed and shit quality with just low res 2D video.</div><br/></div></div><div id="38872278" class="c"><input type="checkbox" id="c-38872278" checked=""/><div class="controls bullet"><span class="by">bigfishrunning</span><span>|</span><a href="#38872148">parent</a><span>|</span><a href="#38873419">prev</a><span>|</span><a href="#38873994">next</a><span>|</span><label class="collapse" for="c-38872278">[-]</label><label class="expand" for="c-38872278">[1 more]</label></div><br/><div class="children"><div class="content">You could generate the avatar clientside and save a ton of bandwidth vs a compressed video stream...</div><br/></div></div><div id="38873994" class="c"><input type="checkbox" id="c-38873994" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#38872148">parent</a><span>|</span><a href="#38872278">prev</a><span>|</span><a href="#38872954">next</a><span>|</span><label class="collapse" for="c-38873994">[-]</label><label class="expand" for="c-38873994">[1 more]</label></div><br/><div class="children"><div class="content">Because my avatar can more easily travel to a made-up photorealistic 3D render of an exoplanet</div><br/></div></div><div id="38872954" class="c"><input type="checkbox" id="c-38872954" checked=""/><div class="controls bullet"><span class="by">RobCodeSlayer</span><span>|</span><a href="#38872148">parent</a><span>|</span><a href="#38873994">prev</a><span>|</span><a href="#38872893">next</a><span>|</span><label class="collapse" for="c-38872954">[-]</label><label class="expand" for="c-38872954">[1 more]</label></div><br/><div class="children"><div class="content">I’m imaging video game applications where the avatars are controlled by both online users and LLMs</div><br/></div></div><div id="38872893" class="c"><input type="checkbox" id="c-38872893" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#38872148">parent</a><span>|</span><a href="#38872954">prev</a><span>|</span><a href="#38872334">next</a><span>|</span><label class="collapse" for="c-38872893">[-]</label><label class="expand" for="c-38872893">[1 more]</label></div><br/><div class="children"><div class="content">Given it&#x27;s by meta I&#x27;m guessing it&#x27;s related to their metaverse goals.</div><br/></div></div><div id="38872334" class="c"><input type="checkbox" id="c-38872334" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38872148">parent</a><span>|</span><a href="#38872893">prev</a><span>|</span><a href="#38871851">next</a><span>|</span><label class="collapse" for="c-38872334">[-]</label><label class="expand" for="c-38872334">[1 more]</label></div><br/><div class="children"><div class="content">Old recordings of people without pictures, for one!</div><br/></div></div></div></div><div id="38871851" class="c"><input type="checkbox" id="c-38871851" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38872148">prev</a><span>|</span><a href="#38873219">next</a><span>|</span><label class="collapse" for="c-38871851">[-]</label><label class="expand" for="c-38871851">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s amazing. It&#x27;s a non-commercial license though.<p>How feasible is it to imitate what this model and codebase is doing to use it in a commercial capacity?<p>Did they release the dataset?<p>It would also be nice if Facebook would consider making an API to give Heygen and Diarupt some competition, if they aren&#x27;t going to allow commercial use.<p>Although there will probably be a bunch of people who become millionaires using this for their porn gf bot service who just don&#x27;t care about license restrictions.</div><br/></div></div><div id="38873219" class="c"><input type="checkbox" id="c-38873219" checked=""/><div class="controls bullet"><span class="by">CrzyLngPwd</span><span>|</span><a href="#38871851">prev</a><span>|</span><a href="#38873425">next</a><span>|</span><label class="collapse" for="c-38873219">[-]</label><label class="expand" for="c-38873219">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s really impressive.<p>I wonder where it is headed.</div><br/></div></div><div id="38873425" class="c"><input type="checkbox" id="c-38873425" checked=""/><div class="controls bullet"><span class="by">aaroninsf</span><span>|</span><a href="#38873219">prev</a><span>|</span><a href="#38871923">next</a><span>|</span><label class="collapse" for="c-38873425">[-]</label><label class="expand" for="c-38873425">[1 more]</label></div><br/><div class="children"><div class="content">Below the right wing, the world famous Uncanny Valley of Menlo Park, one of the seven blunders of the natural world.</div><br/></div></div><div id="38871923" class="c"><input type="checkbox" id="c-38871923" checked=""/><div class="controls bullet"><span class="by">pseudosavant</span><span>|</span><a href="#38873425">prev</a><span>|</span><label class="collapse" for="c-38871923">[-]</label><label class="expand" for="c-38871923">[8 more]</label></div><br/><div class="children"><div class="content">Like the rest of Facebook&#x27;s AI research... I find this underwhelming. Not even good enough to trigger uncanny valley issues.</div><br/><div id="38872921" class="c"><input type="checkbox" id="c-38872921" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#38871923">parent</a><span>|</span><a href="#38871983">next</a><span>|</span><label class="collapse" for="c-38872921">[-]</label><label class="expand" for="c-38872921">[1 more]</label></div><br/><div class="children"><div class="content">This is amazing if used in games. Game designer can easily create realistic body movement just using audio.</div><br/></div></div><div id="38871983" class="c"><input type="checkbox" id="c-38871983" checked=""/><div class="controls bullet"><span class="by">dtauzell</span><span>|</span><a href="#38871923">parent</a><span>|</span><a href="#38872921">prev</a><span>|</span><a href="#38872376">next</a><span>|</span><label class="collapse" for="c-38871983">[-]</label><label class="expand" for="c-38871983">[3 more]</label></div><br/><div class="children"><div class="content">Are there some similar models that are currently better?</div><br/><div id="38872679" class="c"><input type="checkbox" id="c-38872679" checked=""/><div class="controls bullet"><span class="by">pseudosavant</span><span>|</span><a href="#38871923">root</a><span>|</span><a href="#38871983">parent</a><span>|</span><a href="#38872376">next</a><span>|</span><label class="collapse" for="c-38872679">[-]</label><label class="expand" for="c-38872679">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know, but I can&#x27;t imagine having this as a feature in any app (Zoom, etc) and leaving it on. That is how most of FB&#x27;s AI research seems. Not good enough to make into a real product or feature.</div><br/><div id="38873135" class="c"><input type="checkbox" id="c-38873135" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#38871923">root</a><span>|</span><a href="#38872679">parent</a><span>|</span><a href="#38872376">next</a><span>|</span><label class="collapse" for="c-38873135">[-]</label><label class="expand" for="c-38873135">[1 more]</label></div><br/><div class="children"><div class="content">The nature of this type of research is that there are long term goals which are currently unachievable with no clear concept for how to approach them, so researchers need to start putting small pieces together and working out how to make it all work smoothly as a single concept. It looks like someone had a neural network for mouth movement. Someone had one for body movement, etc. Composing multiple systems in to one teaches us how we can approach more complex problems and how to better tie things together than just inserting the output of one in to the input of another.<p>Long term this type of work helps solve big problems even if the intermediate steps don’t produce exciting results.<p>As an example, early image generators were pretty uninteresting but today they are widely utilized and generally considered impressive. The thing that researchers in the field know that the public doesn’t is that there’s 100 boring steps before the exciting release, and some of the boring steps are very exciting on a technical level. Those intermediate achievements represent 99% of what machine learning research actually is and others in the field appreciate those works.</div><br/></div></div></div></div></div></div><div id="38872376" class="c"><input type="checkbox" id="c-38872376" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#38871923">parent</a><span>|</span><a href="#38871983">prev</a><span>|</span><label class="collapse" for="c-38872376">[-]</label><label class="expand" for="c-38872376">[3 more]</label></div><br/><div class="children"><div class="content">Also CC-NC. They want free feedback, but won&#x27;t let you use it to make anything yourself.</div><br/><div id="38874310" class="c"><input type="checkbox" id="c-38874310" checked=""/><div class="controls bullet"><span class="by">pk-protect-ai</span><span>|</span><a href="#38871923">root</a><span>|</span><a href="#38872376">parent</a><span>|</span><label class="collapse" for="c-38874310">[-]</label><label class="expand" for="c-38874310">[2 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s suppose the code will be re-implemented within huggingface transformers library and then you pour bunch of money into new dataset and the training and license it under MIT or create a separate product and sell the result of its work. Will this violate the CC-NC?</div><br/><div id="38874479" class="c"><input type="checkbox" id="c-38874479" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#38871923">root</a><span>|</span><a href="#38874310">parent</a><span>|</span><label class="collapse" for="c-38874479">[-]</label><label class="expand" for="c-38874479">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;ve reimplemented the code and recreated your own dataset, I don&#x27;t see how that would violate the licence.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>