<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="apple-mobile-web-app-capable" content="yes"/><link rel="preload" href="styles.css?v=1682488128755" as="style"/><link rel="stylesheet" href="styles.css?v=1682488128755"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2301.10743">Tighter bounds on the expressivity of transformer encoders</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>bmc7505</span> | <span>15 comments</span></div><br/><div><div id="35708361" class="c"><input type="checkbox" id="c-35708361" checked=""/><div class="controls bullet"><span class="by">meltyness</span><span>|</span><a href="#35708301">next</a><span>|</span><label class="collapse" for="c-35708361">[-]</label><label class="expand" for="c-35708361">[1 more]</label></div><br/><div class="children"><div class="content">Does this mean that transformers are most akin to rule-based expert systems with opaque (currently) statistically-devised rules?</div><br/></div></div><div id="35704063" class="c"><input type="checkbox" id="c-35704063" checked=""/><div class="controls bullet"><span class="by">sharemywin</span><span>|</span><a href="#35708301">prev</a><span>|</span><a href="#35707924">next</a><span>|</span><label class="collapse" for="c-35704063">[-]</label><label class="expand" for="c-35704063">[7 more]</label></div><br/><div class="children"><div class="content">We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders.<p>not sure what a fixed precision transformer is?</div><br/><div id="35704224" class="c"><input type="checkbox" id="c-35704224" checked=""/><div class="controls bullet"><span class="by">bmc7505</span><span>|</span><a href="#35704063">parent</a><span>|</span><a href="#35704209">next</a><span>|</span><label class="collapse" for="c-35704224">[-]</label><label class="expand" for="c-35704224">[4 more]</label></div><br/><div class="children"><div class="content">See Merrill &amp; Sabharwal (2023) [1]:<p><pre><code>  A transformer N is a specific kind of a computation graph family over vectors of floating-point numbers with p precision. We work with fixed-precision transformers, i.e., we take p to be constant with respect to the input sequence length n. In practice, transformers typically use p = 32, and, several newer, larger transformer language models use p = 16 (Brown et al., 2020; Zhang et al., 2022), even while allowing larger context lengths n than their predecessors.
</code></pre>
[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2210.02671.pdf#page=7" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2210.02671.pdf#page=7</a></div><br/><div id="35708157" class="c"><input type="checkbox" id="c-35708157" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#35704063">root</a><span>|</span><a href="#35704224">parent</a><span>|</span><a href="#35704209">next</a><span>|</span><label class="collapse" for="c-35708157">[-]</label><label class="expand" for="c-35708157">[3 more]</label></div><br/><div class="children"><div class="content">Please exclude my ignorance, but what would happen if network weights were modelled with arbitrary-precision numerics, such as a rational type, instead of IEEE floats?<p>(Other than drastically slowing things down, of course)</div><br/><div id="35708489" class="c"><input type="checkbox" id="c-35708489" checked=""/><div class="controls bullet"><span class="by">bmc7505</span><span>|</span><a href="#35704063">root</a><span>|</span><a href="#35708157">parent</a><span>|</span><a href="#35708522">next</a><span>|</span><label class="collapse" for="c-35708489">[-]</label><label class="expand" for="c-35708489">[1 more]</label></div><br/><div class="children"><div class="content">With arbitrary-precision, you can essentially replace the entire neural network with a single trainable parameter. [1] Likewise, in model checking, one often uses bounded-length bitvectors, because even a PDA with two stacks is Turing-equivalent. The poor separability between theoretical and physically-realizable models of computation is one of the weaknesses of the Chomsky hierarchy and classical approximation theory, and why more descriptive theories to characterize function complexity, e.g., circuit complexity, algebraic complexity theory and&#x2F;or logical expressivity, are often needed.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1904.12320.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1904.12320.pdf</a></div><br/></div></div><div id="35708522" class="c"><input type="checkbox" id="c-35708522" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#35704063">root</a><span>|</span><a href="#35708157">parent</a><span>|</span><a href="#35708489">prev</a><span>|</span><a href="#35704209">next</a><span>|</span><label class="collapse" for="c-35708522">[-]</label><label class="expand" for="c-35708522">[1 more]</label></div><br/><div class="children"><div class="content">I guess wrt ieee floats, it depends on how often these cases happen and their consequences in the model:<p>- rounding errors in arithmetic between small and large numbers, large and large numbers, or small and small numbers<p>- unrepresentable numbers (eg 0.1)<p>- non-associativity of arithmetic making order of evaluation relevant<p>- residual connections can introduce, quite easily, arithmetic errors (heard this from research I can’t find atm): x + f(x) happens a lot in transformers, and the terms might be prone to cancellation or inadvertent rounding<p>- layernorm blocks introducing some error<p>- gradients might get added with error if too small or too big<p>On the bright side, some people have suggested that ieee float problems are usually not so terrible that you can see them instead as introducing slightly beneficial regularization</div><br/></div></div></div></div></div></div><div id="35704209" class="c"><input type="checkbox" id="c-35704209" checked=""/><div class="controls bullet"><span class="by">eutectic</span><span>|</span><a href="#35704063">parent</a><span>|</span><a href="#35704224">prev</a><span>|</span><a href="#35707924">next</a><span>|</span><label class="collapse" for="c-35704209">[-]</label><label class="expand" for="c-35704209">[2 more]</label></div><br/><div class="children"><div class="content">I think it means the analysis accounts for rounding error.</div><br/><div id="35705806" class="c"><input type="checkbox" id="c-35705806" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35704063">root</a><span>|</span><a href="#35704209">parent</a><span>|</span><a href="#35707924">next</a><span>|</span><label class="collapse" for="c-35705806">[-]</label><label class="expand" for="c-35705806">[1 more]</label></div><br/><div class="children"><div class="content">Yes it&#x27;s this. They have identified an expressivity level that they are calling FOC[+;MOD] which is intuitively supposed to capture the expressivity that transformers and transformers-with-rounding have in common. More formally they are trying to track down exactly how this level compares to that of transformers, transformers-with-rounding, simplified stateless counter machines, and uniform TC(0), and they have done it with partial success and they are reporting their results.</div><br/></div></div></div></div></div></div><div id="35707924" class="c"><input type="checkbox" id="c-35707924" checked=""/><div class="controls bullet"><span class="by">transformi</span><span>|</span><a href="#35704063">prev</a><span>|</span><label class="collapse" for="c-35707924">[-]</label><label class="expand" for="c-35707924">[5 more]</label></div><br/><div class="children"><div class="content">is it possible to solve NP-hard problems with transformers&#x2F;LLM?</div><br/><div id="35709507" class="c"><input type="checkbox" id="c-35709507" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#35707924">parent</a><span>|</span><a href="#35708221">next</a><span>|</span><label class="collapse" for="c-35709507">[-]</label><label class="expand" for="c-35709507">[1 more]</label></div><br/><div class="children"><div class="content">You should check out the work referenced in the abstract, the most recent here: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;lambdaviking&#x2F;status&#x2F;1630581475425828864" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;lambdaviking&#x2F;status&#x2F;1630581475425828864</a><p>There are limitations for what a Transformer can compute if we do not allow for Chain-of-Thought type of output.
By allowing the model to &quot;show its work&quot; allows it to effectively use the output as an &quot;infinite tape&quot;. I&#x27;m simplifying but that&#x27;s the basic gist of it.<p>I&#x27;ll shamelessly plug my blog post from a week ago for a simpler take on the matter: <a href="https:&#x2F;&#x2F;blog.wtf.sg&#x2F;posts&#x2F;2023-02-03-the-new-xor-problem&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.wtf.sg&#x2F;posts&#x2F;2023-02-03-the-new-xor-problem&#x2F;</a></div><br/></div></div><div id="35708791" class="c"><input type="checkbox" id="c-35708791" checked=""/><div class="controls bullet"><span class="by">cardboardbach</span><span>|</span><a href="#35707924">parent</a><span>|</span><a href="#35708221">prev</a><span>|</span><a href="#35708296">next</a><span>|</span><label class="collapse" for="c-35708791">[-]</label><label class="expand" for="c-35708791">[1 more]</label></div><br/><div class="children"><div class="content">Of course.</div><br/></div></div><div id="35708296" class="c"><input type="checkbox" id="c-35708296" checked=""/><div class="controls bullet"><span class="by">ironbound</span><span>|</span><a href="#35707924">parent</a><span>|</span><a href="#35708791">prev</a><span>|</span><label class="collapse" for="c-35708296">[-]</label><label class="expand" for="c-35708296">[1 more]</label></div><br/><div class="children"><div class="content">you&#x27;ll be limited right away by the context lenght of 4096 tokens</div><br/></div></div></div></div></div></div></div></div></div></body></html>