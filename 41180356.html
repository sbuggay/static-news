<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1723366868474" as="style"/><link rel="stylesheet" href="styles.css?v=1723366868474"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://newsletter.posthog.com/p/ab-testing-mistakes-i-learned-the">A/B testing mistakes I learned the hard way</a> <span class="domain">(<a href="https://newsletter.posthog.com">newsletter.posthog.com</a>)</span></div><div class="subtext"><span>Lior539</span> | <span>18 comments</span></div><br/><div><div id="41213271" class="c"><input type="checkbox" id="c-41213271" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#41213119">next</a><span>|</span><label class="collapse" for="c-41213271">[-]</label><label class="expand" for="c-41213271">[6 more]</label></div><br/><div class="children"><div class="content">I feel like I&#x27;ve too often seen in products new (anti)features that are way too easy to accidentally click, and whenever I do accidentally click I just imagine it&#x27;s increasing some statistics counter that&#x27;s ultimately showing the product managers super high engagement, clearly meaning the users must love it to be using it all the time</div><br/><div id="41213997" class="c"><input type="checkbox" id="c-41213997" checked=""/><div class="controls bullet"><span class="by">Jemaclus</span><span>|</span><a href="#41213271">parent</a><span>|</span><a href="#41213818">next</a><span>|</span><label class="collapse" for="c-41213997">[-]</label><label class="expand" for="c-41213997">[2 more]</label></div><br/><div class="children"><div class="content">To your point, my company is doing some A&#x2F;B tests and I insisted that we not just measure conversion (&quot;it works&quot;) and additionally measure some metrics that would indicate that it works _well_. For example, if you have a carousel of products, and someone buys something from the carousel, then it &quot;works,&quot; but it would work _better_ if the item they bought was the first thing on the carousel rather than the last. That indicates that we showed relevant products first, which is better than showing the relevant product last!<p>Sure, conversion would go up if it was in the first slot versus the last, but it takes effort (however little) to scroll through a carousel, so ensuring that we can measure the quality of the result and not just the quantity is really important.<p>This is one way I&#x27;ve tried to avoid the problem you describe. It&#x27;s not enough that people can engage with the feature, but they need to engage with it meaningfully and in such a way that would encourage repeat behavior.<p>Another example of what you describe is that on our site if you search for &quot;neon blue guitar&quot;, don&#x27;t interact with the search results whatsoever, go back to the home page, click on a product on a carousel, and purchase _that_ product, it counts as a &quot;successful search event,&quot; even though the search technically failed because they didn&#x27;t interact with it in any meaningful way. To your point: PM is happy; user is not.<p>TL;DR it&#x27;s really important to think through tests and how you measure success!</div><br/><div id="41214443" class="c"><input type="checkbox" id="c-41214443" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#41213271">root</a><span>|</span><a href="#41213997">parent</a><span>|</span><a href="#41213818">next</a><span>|</span><label class="collapse" for="c-41214443">[-]</label><label class="expand" for="c-41214443">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but it would work _better_ if the item they bought was the first thing on the carousel rather than the last<p>Depends, does that increase overall sales? Or is it ‘better’ to make the customer ‘walk past’ the other items to get to the thing they want (the way supermarkets make you walk up the back of the shop to get to the milk), and maybe buy something else too?</div><br/></div></div></div></div><div id="41213818" class="c"><input type="checkbox" id="c-41213818" checked=""/><div class="controls bullet"><span class="by">texuf</span><span>|</span><a href="#41213271">parent</a><span>|</span><a href="#41213997">prev</a><span>|</span><a href="#41213970">next</a><span>|</span><label class="collapse" for="c-41213818">[-]</label><label class="expand" for="c-41213818">[1 more]</label></div><br/><div class="children"><div class="content">Then in the code there’s a bug that over&#x2F;under reports those clicks (because ui is not procedural code that lends itself to straightforward metrics) and i think this could explain Spotify’s product decisions.</div><br/></div></div><div id="41213970" class="c"><input type="checkbox" id="c-41213970" checked=""/><div class="controls bullet"><span class="by">ffhhj</span><span>|</span><a href="#41213271">parent</a><span>|</span><a href="#41213818">prev</a><span>|</span><a href="#41214336">next</a><span>|</span><label class="collapse" for="c-41213970">[-]</label><label class="expand" for="c-41213970">[1 more]</label></div><br/><div class="children"><div class="content">Several years ago Google implemented this A&#x2F;B feature in which you had to choose one image or another. Do you remember that one? Ofcourse I always chose the wrong one ;) It didn&#x27;t last long.</div><br/></div></div></div></div><div id="41213119" class="c"><input type="checkbox" id="c-41213119" checked=""/><div class="controls bullet"><span class="by">clarle</span><span>|</span><a href="#41213271">prev</a><span>|</span><a href="#41213091">next</a><span>|</span><label class="collapse" for="c-41213119">[-]</label><label class="expand" for="c-41213119">[2 more]</label></div><br/><div class="children"><div class="content">#2 is a slippery slope if you don&#x27;t do it properly.<p>You might look end up looking at lots of different slices of your data, and you might come to the conclusion, &quot;Oh, it looks like France is statistically significant negative on our new signup flow changes&quot;.<p>It&#x27;s important to make sure you have a hypothesis for the given slice before you start the experiment and not just hunt for outliers after the fact, or otherwise you&#x27;re just p-hacking [1].<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;p-hacking" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;p-hacking</a></div><br/></div></div><div id="41213091" class="c"><input type="checkbox" id="c-41213091" checked=""/><div class="controls bullet"><span class="by">sokoloff</span><span>|</span><a href="#41213119">prev</a><span>|</span><a href="#41213215">next</a><span>|</span><label class="collapse" for="c-41213091">[-]</label><label class="expand" for="c-41213091">[4 more]</label></div><br/><div class="children"><div class="content">I recall getting into a heated debate with an analyst at my company over the topic of &quot;peeking&quot; (he was right; I was wrong, but it took me several days to finally understand what he was saying.)<p>The temptation to &quot;peek&quot; and keep on peeking until the test confesses to the thing you want it to say is very high.</div><br/><div id="41213557" class="c"><input type="checkbox" id="c-41213557" checked=""/><div class="controls bullet"><span class="by">jakevoytko</span><span>|</span><a href="#41213091">parent</a><span>|</span><a href="#41213215">next</a><span>|</span><label class="collapse" for="c-41213557">[-]</label><label class="expand" for="c-41213557">[3 more]</label></div><br/><div class="children"><div class="content">This is the most &quot;damned if you do, damned if you don&#x27;t&quot; part of testing. I&#x27;ve found so many coding errors that weren&#x27;t obvious until you looked at the day 2 or day 3 test results. &quot;Hm, that&#x27;s weird. Why is $thing happening in this test? It shouldn&#x27;t even touch that component.&quot;<p>If you peek, you really have to commit to running the test for the full duration no matter what.</div><br/><div id="41213683" class="c"><input type="checkbox" id="c-41213683" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41213091">root</a><span>|</span><a href="#41213557">parent</a><span>|</span><a href="#41213215">next</a><span>|</span><label class="collapse" for="c-41213683">[-]</label><label class="expand" for="c-41213683">[2 more]</label></div><br/><div class="children"><div class="content">No you don&#x27;t. If your protocol involves peeking (and early stopping), you need different thresholds to declare statistical significance. But you can do that. You just need to know whether you&#x27;re peeking or not, which everybody does.</div><br/></div></div></div></div></div></div><div id="41213215" class="c"><input type="checkbox" id="c-41213215" checked=""/><div class="controls bullet"><span class="by">iamcreasy</span><span>|</span><a href="#41213091">prev</a><span>|</span><a href="#41213726">next</a><span>|</span><label class="collapse" for="c-41213215">[-]</label><label class="expand" for="c-41213215">[2 more]</label></div><br/><div class="children"><div class="content">The article says &#x27;Changing the color of the &quot;Proceed to checkout&quot; button will increase purchases.&#x27; is a bad hypothesis because it is underspecified.<p>But what else is there to measure other than checkout button click count(and follow up purchases) to measure the effect of button color change?<p>Or perhaps this is not a robust example to illustrates undespeficaition?</div><br/><div id="41214046" class="c"><input type="checkbox" id="c-41214046" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#41213215">parent</a><span>|</span><a href="#41213726">next</a><span>|</span><label class="collapse" for="c-41214046">[-]</label><label class="expand" for="c-41214046">[1 more]</label></div><br/><div class="children"><div class="content">The number of people who start checkout and the number of people who check out are different, and I think that was what they meant</div><br/></div></div></div></div><div id="41213726" class="c"><input type="checkbox" id="c-41213726" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#41213215">prev</a><span>|</span><a href="#41213713">next</a><span>|</span><label class="collapse" for="c-41213726">[-]</label><label class="expand" for="c-41213726">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not Simpson&#x27;s paradox!<p>&gt; In fact, while the new flow worked great on mobile, conversion was lower on desktop – an insight we missed when we combined these metrics.<p>&gt; This phenomenon is known as Simpson&#x27;s paradox – i.e. when experiments show one outcome when analyzed at an aggregated level, but a different one when analyzed by subgroups.<p>There&#x27;s nothing strange about finding out that some groups benefit and others lose out when diving up you data. You&#x27;re looking at an average and some parts are positive and others are negative. Where&#x27;s the paradox there?<p>Simpson&#x27;s paradox is when more button presses lead to more purchases. But then you look at desktop vs mobile and you find out that for both desktop and mobile more clicks doesn&#x27;t mean more purchases (or worse, more clicks means fewer purchases).<p>That&#x27;s why it&#x27;s a paradox. The association between two variables exists at the aggregate level but doesn&#x27;t exist or is backwards when you split up the population. It&#x27;s not a statement about the average performance of something.<p>I would add a 7th A&#x2F;B testing mistake to that list and it&#x27;s not learning about basic probability, statical tests, power, etc. Flying by the seat of your pants when statistics are involved always ends badly.</div><br/><div id="41214517" class="c"><input type="checkbox" id="c-41214517" checked=""/><div class="controls bullet"><span class="by">iamacyborg</span><span>|</span><a href="#41213726">parent</a><span>|</span><a href="#41213713">next</a><span>|</span><label class="collapse" for="c-41214517">[-]</label><label class="expand" for="c-41214517">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I would add a 7th A&#x2F;B testing mistake to that list and it&#x27;s not learning about basic probability, statical tests, power, etc. Flying by the seat of your pants when statistics are involved always ends badly.<p>This is where most tests fail, in my experience.<p>Everyone wants to run A&#x2F;B tests because that’s what the big co’s are doing and they want to look like the sort of person BigCo might hire, but they’re making silly mistakes because stats is hard and not taught well at school.</div><br/></div></div></div></div></div></div></div></div></div></body></html>