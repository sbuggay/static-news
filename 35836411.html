<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683363668492" as="style"/><link rel="stylesheet" href="styles.css?v=1683363668492"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.together.xyz/blog/redpajama-models-v1">Releasing 3B and 7B RedPajama</a> <span class="domain">(<a href="https://www.together.xyz">www.together.xyz</a>)</span></div><div class="subtext"><span>antimatter15</span> | <span>83 comments</span></div><br/><div><div id="35836921" class="c"><input type="checkbox" id="c-35836921" checked=""/><div class="controls bullet"><span class="by">sphars</span><span>|</span><a href="#35836606">next</a><span>|</span><label class="collapse" for="c-35836921">[-]</label><label class="expand" for="c-35836921">[5 more]</label></div><br/><div class="children"><div class="content">Slightly off-topic, but as the parent of a toddler, I got a bit of a chuckle out of the name. It&#x27;s based off the children&#x27;s book series of &quot;Llama Llama Red Pajama&quot;</div><br/><div id="35837344" class="c"><input type="checkbox" id="c-35837344" checked=""/><div class="controls bullet"><span class="by">elkos</span><span>|</span><a href="#35836921">parent</a><span>|</span><a href="#35837712">next</a><span>|</span><label class="collapse" for="c-35837344">[-]</label><label class="expand" for="c-35837344">[1 more]</label></div><br/><div class="children"><div class="content">Thanks.<p>As non-native English speaker (while though a parent of a toddler too) I wasn&#x27;t familiar with the book series.</div><br/></div></div><div id="35837712" class="c"><input type="checkbox" id="c-35837712" checked=""/><div class="controls bullet"><span class="by">dllthomas</span><span>|</span><a href="#35836921">parent</a><span>|</span><a href="#35837344">prev</a><span>|</span><a href="#35836952">next</a><span>|</span><label class="collapse" for="c-35837712">[-]</label><label class="expand" for="c-35837712">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m holding out for the MadAtMama model.</div><br/></div></div><div id="35836952" class="c"><input type="checkbox" id="c-35836952" checked=""/><div class="controls bullet"><span class="by">macawfish</span><span>|</span><a href="#35836921">parent</a><span>|</span><a href="#35837712">prev</a><span>|</span><a href="#35837883">next</a><span>|</span><label class="collapse" for="c-35836952">[-]</label><label class="expand" for="c-35836952">[1 more]</label></div><br/><div class="children"><div class="content">Not off topic at all</div><br/></div></div><div id="35837883" class="c"><input type="checkbox" id="c-35837883" checked=""/><div class="controls bullet"><span class="by">innagadadavida</span><span>|</span><a href="#35836921">parent</a><span>|</span><a href="#35836952">prev</a><span>|</span><a href="#35836606">next</a><span>|</span><label class="collapse" for="c-35837883">[-]</label><label class="expand" for="c-35837883">[1 more]</label></div><br/><div class="children"><div class="content">Founder ex-apple Siri search. Had a baby a couple of years ago. Not too surprising to me :)</div><br/></div></div></div></div><div id="35836606" class="c"><input type="checkbox" id="c-35836606" checked=""/><div class="controls bullet"><span class="by">rawrmaan</span><span>|</span><a href="#35836921">prev</a><span>|</span><a href="#35836595">next</a><span>|</span><label class="collapse" for="c-35836606">[-]</label><label class="expand" for="c-35836606">[26 more]</label></div><br/><div class="children"><div class="content">There was a lot of detail and data in here, but it&#x27;s not very useful to me because all of the comparisons are to things I have no experience with.<p>There&#x27;s really only one thing I care about: How does this compare to GPT-4?<p>I have no use for models that aren&#x27;t at that level. Even though this almost definitely isn&#x27;t at that level, it&#x27;s hard to know how close or far it is from the data presented.</div><br/><div id="35838735" class="c"><input type="checkbox" id="c-35838735" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#35836606">parent</a><span>|</span><a href="#35836621">next</a><span>|</span><label class="collapse" for="c-35838735">[-]</label><label class="expand" for="c-35838735">[6 more]</label></div><br/><div class="children"><div class="content">None of the 3B and 7B models are at ChatGPT’s level, let alone GPT-4. The 13B models start doing really interesting things, but you don’t get near ChatGPT results until you move up to the best 30B and 65B models, which require beefier hardware. Nothing out there right now approximates GPT-4.<p>The big story here for me is that the difference in training set is what makes the difference in quality. There is no secret sauce, the open source architectures do well, provided you give them a large and diverse enough training set. That would mean it is just a matter of pooling resources to train really capable open source models. That makes what RedPajama is doing, compiling the best open dataset, very important for the future of high quality open source LLM’s.<p>If you want to play around with this yourself you can install oobabooga and figure out what model fits your hardware from the locallama reddit wiki. The llama.cpp 7B and 13B models can be run on CPU if you have enough RAM. I’ve had lots of fun talking to 7B and 13B alpaca and vicuna models running locally.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;models&#x2F;</a></div><br/><div id="35839196" class="c"><input type="checkbox" id="c-35839196" checked=""/><div class="controls bullet"><span class="by">nullsense</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35838735">parent</a><span>|</span><a href="#35839161">next</a><span>|</span><label class="collapse" for="c-35839196">[-]</label><label class="expand" for="c-35839196">[2 more]</label></div><br/><div class="children"><div class="content">LLaVA 13B is a great multimodal model that has first class support in oobabooga too.<p>It&#x27;s really fun to enable both the whisper extension and the TTS extension and have two-way voice chats with your computer while being able to send it pictures as well. Truly mind bending.<p>Quantized 30B models run at acceptable speeds on decent hardware and are pretty capable. It&#x27;s my understanding that the open source community is iterating extremely fast on small model sizes getting the most out of them by pushing the data quality higher and higher, and then they plan to scale up to at least 30B parameter models.<p>I really can&#x27;t wait to see the results of that process. In the end you&#x27;re going to have a 30B model that&#x27;s totally uncensored and is a mix of Wizard + Vicuna. It&#x27;s going to be a veryyyy capable model.</div><br/><div id="35839405" class="c"><input type="checkbox" id="c-35839405" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35839196">parent</a><span>|</span><a href="#35839161">next</a><span>|</span><label class="collapse" for="c-35839405">[-]</label><label class="expand" for="c-35839405">[1 more]</label></div><br/><div class="children"><div class="content">I usually even prefer GPT-3.5, as it&#x27;s faster and much cheaper. GPT-4 is great for the hardcore logical reasoning, but when I want something that knows to turn my lights on and turn the TV to a channel, it&#x27;s overkill.</div><br/></div></div></div></div><div id="35839161" class="c"><input type="checkbox" id="c-35839161" checked=""/><div class="controls bullet"><span class="by">Semaphor</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35838735">parent</a><span>|</span><a href="#35839196">prev</a><span>|</span><a href="#35839365">next</a><span>|</span><label class="collapse" for="c-35839161">[-]</label><label class="expand" for="c-35839161">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The llama.cpp 7B and 13B models can be run on CPU if you have enough RAM.<p>Bigger ones as well, you just have to wait longer. Nothing for real time usage, but if you can wait 10-20 minutes, you can use them on CPU.</div><br/><div id="35839462" class="c"><input type="checkbox" id="c-35839462" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35839161">parent</a><span>|</span><a href="#35839365">next</a><span>|</span><label class="collapse" for="c-35839462">[-]</label><label class="expand" for="c-35839462">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not even that bad. Core i7-12700K with DDR5 gives me ~1 word per second on llama-30b - that is fast enough for real-time chat, with some patience. And things are even better on M1&#x2F;M2 Macs.</div><br/></div></div></div></div><div id="35839365" class="c"><input type="checkbox" id="c-35839365" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35838735">parent</a><span>|</span><a href="#35839161">prev</a><span>|</span><a href="#35836621">next</a><span>|</span><label class="collapse" for="c-35839365">[-]</label><label class="expand" for="c-35839365">[1 more]</label></div><br/><div class="children"><div class="content">Do these red pajama models work with llama.cpp?</div><br/></div></div></div></div><div id="35836621" class="c"><input type="checkbox" id="c-35836621" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35836606">parent</a><span>|</span><a href="#35838735">prev</a><span>|</span><a href="#35836780">next</a><span>|</span><label class="collapse" for="c-35836621">[-]</label><label class="expand" for="c-35836621">[2 more]</label></div><br/><div class="children"><div class="content">The bit I liked best was the response examples. Look at those. Clearly not as good as GPT-4 but good enough I feel that for say a scenario where you care about privacy or data provenance this would be a contender.<p>For example a therapist, a search bot for you diary, a company intranet help bot. Anything where the prompt contains something you don’t want to send to a third party.</div><br/><div id="35836720" class="c"><input type="checkbox" id="c-35836720" checked=""/><div class="controls bullet"><span class="by">rawrmaan</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35836621">parent</a><span>|</span><a href="#35836780">next</a><span>|</span><label class="collapse" for="c-35836720">[-]</label><label class="expand" for="c-35836720">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a great point, I definitely overlooked these. They look pretty good, too, and I agree with your use cases.<p>Thanks!</div><br/></div></div></div></div><div id="35836780" class="c"><input type="checkbox" id="c-35836780" checked=""/><div class="controls bullet"><span class="by">blihp</span><span>|</span><a href="#35836606">parent</a><span>|</span><a href="#35836621">prev</a><span>|</span><a href="#35836861">next</a><span>|</span><label class="collapse" for="c-35836780">[-]</label><label class="expand" for="c-35836780">[3 more]</label></div><br/><div class="children"><div class="content">Then you probably don&#x27;t care about this (yet)<p>Assume a truly competitive model in the Open Source world is still a ways off.  These teams and their infrastructure are still in their early days while OpenAI is more at the fine-tuning and polishing stage.  The fact that these open teams are able to have something in the same universe in terms of functionality this fast is pretty amazing... but it will take time before there&#x27;s an artifact that will be a strong competitor.</div><br/><div id="35839224" class="c"><input type="checkbox" id="c-35839224" checked=""/><div class="controls bullet"><span class="by">nullsense</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35836780">parent</a><span>|</span><a href="#35836850">next</a><span>|</span><label class="collapse" for="c-35839224">[-]</label><label class="expand" for="c-35839224">[1 more]</label></div><br/><div class="children"><div class="content">The pace of the progress the open source models are making is pretty astonishing. The smaller model sizes are cheap to train so there is a lot of iteration by many different teams. People are also combining proven approaches together. Then they&#x27;re going to nail it and scale it. Will be very interesting to see where we are in 3 months time.</div><br/></div></div></div></div><div id="35836861" class="c"><input type="checkbox" id="c-35836861" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#35836606">parent</a><span>|</span><a href="#35836780">prev</a><span>|</span><a href="#35838445">next</a><span>|</span><label class="collapse" for="c-35836861">[-]</label><label class="expand" for="c-35836861">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a nice chart in the leaked Google memos that compares some of the open models against ChatGPT and Bard so you can get a sense where these models land by comparing them to these.<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;jelleprins&#x2F;status&#x2F;1654197282311491592" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;jelleprins&#x2F;status&#x2F;1654197282311491592</a></div><br/></div></div><div id="35838445" class="c"><input type="checkbox" id="c-35838445" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#35836606">parent</a><span>|</span><a href="#35836861">prev</a><span>|</span><a href="#35836649">next</a><span>|</span><label class="collapse" for="c-35838445">[-]</label><label class="expand" for="c-35838445">[4 more]</label></div><br/><div class="children"><div class="content">&gt; How does this compare to GPT-4?<p>I&#x27;ll give you the answer for every open source model over the next 2 years: It&#x27;s far worse</div><br/><div id="35838871" class="c"><input type="checkbox" id="c-35838871" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35838445">parent</a><span>|</span><a href="#35836649">next</a><span>|</span><label class="collapse" for="c-35838871">[-]</label><label class="expand" for="c-35838871">[3 more]</label></div><br/><div class="children"><div class="content">If you&#x27;d said that about OpenAI&#x27;s DALL-E 2 you&#x27;d have been wrong.<p>I suspect Open Source LLMs will outpace the release version of GPT-4 before the end of this year.<p>It&#x27;s less likely they will outpace whatever version of GPT-4 is shipped later this year, but still very much possible.</div><br/><div id="35839494" class="c"><input type="checkbox" id="c-35839494" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35838871">parent</a><span>|</span><a href="#35839030">next</a><span>|</span><label class="collapse" for="c-35839494">[-]</label><label class="expand" for="c-35839494">[1 more]</label></div><br/><div class="children"><div class="content">Open source LLMs might do that, but I very much doubt that those models will be small enough to run even on high-end consumer hardware (like say RTX 3090 or 4090).</div><br/></div></div><div id="35839030" class="c"><input type="checkbox" id="c-35839030" checked=""/><div class="controls bullet"><span class="by">Sugimot0</span><span>|</span><a href="#35836606">root</a><span>|</span><a href="#35838871">parent</a><span>|</span><a href="#35839494">prev</a><span>|</span><a href="#35836649">next</a><span>|</span><label class="collapse" for="c-35839030">[-]</label><label class="expand" for="c-35839030">[1 more]</label></div><br/><div class="children"><div class="content">Relevant post: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35813322" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35813322</a></div><br/></div></div></div></div></div></div></div></div><div id="35836595" class="c"><input type="checkbox" id="c-35836595" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#35836606">prev</a><span>|</span><a href="#35836707">next</a><span>|</span><label class="collapse" for="c-35836595">[-]</label><label class="expand" for="c-35836595">[18 more]</label></div><br/><div class="children"><div class="content">This is beyond exciting. Welcome to the new reality!<p>On one hand, the resources required to run these models continues falling dramatically, thanks to the techniques discovered by researchers: GPTQ quantizing down to 4, 3, 2, even 1 bits! model pruning! hybrid vram offloading! better, more efficient architectures! 1-click finetuning on consumer hardware!  Of course, the free lunches won&#x27;t last forever, and this will level off, but it&#x27;s still incredible.<p>And on the other side of the coin, the power of <i>all</i> computing devices continues its ever-upward exponential growth.<p>So you have a continuous <i>lowering</i> of requirements, combined with a continuous <i>increase</i> in available power... surely these two trends will collide, and I can only imagine what this stuff will be like at that intersection.</div><br/><div id="35836637" class="c"><input type="checkbox" id="c-35836637" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35836595">parent</a><span>|</span><a href="#35836770">next</a><span>|</span><label class="collapse" for="c-35836637">[-]</label><label class="expand" for="c-35836637">[7 more]</label></div><br/><div class="children"><div class="content">I would love to see an article on why quantising to low bits works. Seems counterintuitive to me. For example do that with a CD and it will sound awful. It took smarts to come up with mp3
format rather than just reduce number of bits.</div><br/><div id="35836951" class="c"><input type="checkbox" id="c-35836951" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35836595">root</a><span>|</span><a href="#35836637">parent</a><span>|</span><a href="#35836770">next</a><span>|</span><label class="collapse" for="c-35836951">[-]</label><label class="expand" for="c-35836951">[6 more]</label></div><br/><div class="children"><div class="content">I think a lot of it is that they are intentionally not measuring the &quot;degradation&quot; in quality experienced. I&#x27;ve noticed that 8 bit quantization of a model like dolly is significantly worse than the 32bit version of it. Seen similar results with using quantization with stable diffusion - the images really are worse, just so little at half percision that it&#x27;s worth the trade-off.</div><br/><div id="35837852" class="c"><input type="checkbox" id="c-35837852" checked=""/><div class="controls bullet"><span class="by">readyplayeremma</span><span>|</span><a href="#35836595">root</a><span>|</span><a href="#35836951">parent</a><span>|</span><a href="#35837844">next</a><span>|</span><label class="collapse" for="c-35837852">[-]</label><label class="expand" for="c-35837852">[1 more]</label></div><br/><div class="children"><div class="content">What size model are you quantizing and comparing? The interesting thing about quantization, is how the larger the number of parameters, the less of a difference it makes to quantize the weights, even to an extreme degree when working with the largest parameter models. For small models is can be a disaster though.</div><br/></div></div><div id="35837844" class="c"><input type="checkbox" id="c-35837844" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35836595">root</a><span>|</span><a href="#35836951">parent</a><span>|</span><a href="#35837852">prev</a><span>|</span><a href="#35837071">next</a><span>|</span><label class="collapse" for="c-35837844">[-]</label><label class="expand" for="c-35837844">[1 more]</label></div><br/><div class="children"><div class="content">This seems to explain the process: <a href="https:&#x2F;&#x2F;medium.com&#x2F;towards-data-science&#x2F;how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;towards-data-science&#x2F;how-to-accelerate-an...</a></div><br/></div></div><div id="35837071" class="c"><input type="checkbox" id="c-35837071" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35836595">root</a><span>|</span><a href="#35836951">parent</a><span>|</span><a href="#35837844">prev</a><span>|</span><a href="#35836770">next</a><span>|</span><label class="collapse" for="c-35837071">[-]</label><label class="expand" for="c-35837071">[3 more]</label></div><br/><div class="children"><div class="content">Thanks. I am more surprised it works at all.<p>So do they use the weights that are say 32 bit floats and just round them to the nearest something putting them in a range 0-255? I guess I can see how it could work if weights are all close to zero, so -1 to 1 is mapped to 0-255.<p>But I would have though the model relied on the higher accuracy during training. So losing that would screw it up.</div><br/><div id="35838970" class="c"><input type="checkbox" id="c-35838970" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35836595">root</a><span>|</span><a href="#35837071">parent</a><span>|</span><a href="#35839058">next</a><span>|</span><label class="collapse" for="c-35838970">[-]</label><label class="expand" for="c-35838970">[1 more]</label></div><br/><div class="children"><div class="content">That commenter is just wrong. We have empirical tests of quality loss due to quantization and even down to 4bits the loss is so negligible no human would ever be able to detect it. The loss only even registers on the benchmarks after generating tens of thousands of full context generations.<p>&gt;So do they use the weights that are say 32 bit floats and just round them to the nearest<p>That&#x27;s how they used to do it, and still how 8bit quantization works. That&#x27;s called &quot;Round to Nearest&quot; or RTN quantization. That&#x27;s not how it works anymore though.<p>The current algorithms (GPTQ, RTPQ, etc.) are more complex, including things like lining up the weights in order of least to greatest, placing them in bins (typically 32 or 128 weights per bin), and then computing an offset for each bin which is added to the RTN value. In some cases bins are identical and redundant and can be re-used without saving the same identical bin twice. These are just a few of the space saving measures which go into effective low-bit quantization without sacrificing quality.<p>It&#x27;s very similar to state of the art video codecs or image compression algorithms. A raw photograph taken by my digital camera is 60MB, but a PNG of the same photo is 30x smaller at 2MB without a single artifact. It should be no surprise that we can reduce models by 4x, 8x, or even more without sacrificing quality.</div><br/></div></div><div id="35839058" class="c"><input type="checkbox" id="c-35839058" checked=""/><div class="controls bullet"><span class="by">alpaca128</span><span>|</span><a href="#35836595">root</a><span>|</span><a href="#35837071">parent</a><span>|</span><a href="#35838970">prev</a><span>|</span><a href="#35836770">next</a><span>|</span><label class="collapse" for="c-35839058">[-]</label><label class="expand" for="c-35839058">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But I would have though the model relied on the higher accuracy during training. So losing that would screw it up.<p>Yes, <i>during training</i>, where you need to make tiny adjustments to weights. But as far as I understand it inference can still work well because of the sheer number of weights. Give a black-and-white image a high resolution and you can represent any shade of gray if you zoom out a bit.</div><br/></div></div></div></div></div></div></div></div><div id="35836770" class="c"><input type="checkbox" id="c-35836770" checked=""/><div class="controls bullet"><span class="by">wcunning</span><span>|</span><a href="#35836595">parent</a><span>|</span><a href="#35836637">prev</a><span>|</span><a href="#35836614">next</a><span>|</span><label class="collapse" for="c-35836770">[-]</label><label class="expand" for="c-35836770">[2 more]</label></div><br/><div class="children"><div class="content">Do you have reading links on the consumer hardware fine tuning? I can’t find much from that description…</div><br/><div id="35837885" class="c"><input type="checkbox" id="c-35837885" checked=""/><div class="controls bullet"><span class="by">Oranguru</span><span>|</span><a href="#35836595">root</a><span>|</span><a href="#35836770">parent</a><span>|</span><a href="#35836614">next</a><span>|</span><label class="collapse" for="c-35837885">[-]</label><label class="expand" for="c-35837885">[1 more]</label></div><br/><div class="children"><div class="content">Take a look at: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;trl-peft" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;trl-peft</a></div><br/></div></div></div></div></div></div><div id="35836707" class="c"><input type="checkbox" id="c-35836707" checked=""/><div class="controls bullet"><span class="by">knaik94</span><span>|</span><a href="#35836595">prev</a><span>|</span><a href="#35839318">next</a><span>|</span><label class="collapse" for="c-35836707">[-]</label><label class="expand" for="c-35836707">[1 more]</label></div><br/><div class="children"><div class="content">I have been really impressed with the uncensored WizardLM I was playing with. Having a truely open uncensored model to work with is a really important research tool. Censorship of the training data and results in such a heavy handed way is not really possible without lowering the quality of all output.<p>As the resouces required to train and fine tune these models becomes consumer handware friendly, I think we&#x27;ll see a shift towards a bunch of smaller models. Open models like these also mean the results of securty and capability research is publicly available. Models like this one and the Replit code model will become the new base all open source models are based on. I am really looking forward to the gptj 4bit, cuda optimized 7b models, the others I have tested run fast on 2070max q and 16gb ram, I was getting ~7tokens&#x2F;second. Lora can work directly with 4bit quantized models. While ggml, cpu models are very strong, I don&#x27;t believe we&#x27;re move away from gpu accelarated training and fine tuning anytime soon.</div><br/></div></div><div id="35839318" class="c"><input type="checkbox" id="c-35839318" checked=""/><div class="controls bullet"><span class="by">ibitto</span><span>|</span><a href="#35836707">prev</a><span>|</span><a href="#35837026">next</a><span>|</span><label class="collapse" for="c-35839318">[-]</label><label class="expand" for="c-35839318">[1 more]</label></div><br/><div class="children"><div class="content">I am really interested in knowing what people are using these smaller models for. I have seen a lot of projects on top of GPT-3.5 &#x2F; GPT-4, but I have yet to see any using these smaller models.</div><br/></div></div><div id="35837026" class="c"><input type="checkbox" id="c-35837026" checked=""/><div class="controls bullet"><span class="by">practice9</span><span>|</span><a href="#35839318">prev</a><span>|</span><a href="#35836560">next</a><span>|</span><label class="collapse" for="c-35837026">[-]</label><label class="expand" for="c-35837026">[3 more]</label></div><br/><div class="children"><div class="content">Models replicating LLaMA are cool, but they are all missing proper multilingual support, which GPT-3.5 is quite good at.</div><br/><div id="35837420" class="c"><input type="checkbox" id="c-35837420" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#35837026">parent</a><span>|</span><a href="#35836560">next</a><span>|</span><label class="collapse" for="c-35837420">[-]</label><label class="expand" for="c-35837420">[2 more]</label></div><br/><div class="children"><div class="content">IMHO multilingual support would just pollute precious available estate in those models. Why not use it in english and use another one for translation?</div><br/><div id="35837874" class="c"><input type="checkbox" id="c-35837874" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#35837026">root</a><span>|</span><a href="#35837420">parent</a><span>|</span><a href="#35836560">next</a><span>|</span><label class="collapse" for="c-35837874">[-]</label><label class="expand" for="c-35837874">[1 more]</label></div><br/><div class="children"><div class="content">That would work if all information is available in English as the primary language. That&#x27;s not the case though. You may be missing out on interesting information if you&#x27;re skipping other languages.</div><br/></div></div></div></div></div></div><div id="35836560" class="c"><input type="checkbox" id="c-35836560" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35837026">prev</a><span>|</span><a href="#35836826">next</a><span>|</span><label class="collapse" for="c-35836560">[-]</label><label class="expand" for="c-35836560">[1 more]</label></div><br/><div class="children"><div class="content">With this one and mosaicml we now got so many of these consumer-gpu-sized models!</div><br/></div></div><div id="35836826" class="c"><input type="checkbox" id="c-35836826" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35836560">prev</a><span>|</span><a href="#35837638">next</a><span>|</span><label class="collapse" for="c-35836826">[-]</label><label class="expand" for="c-35836826">[1 more]</label></div><br/><div class="children"><div class="content">So I tried RedPajama-INCITE-Instruct-7B-v0.1 and the AutoModelForCausalLM.from_pretrained(...) call takes two minutes every time. My GPU is big enough. I don&#x27;t know why it&#x27;s so slow. I feel like it&#x27;s somehow precomputing stuff that can be used across queries, and I had hoped that this stuff would have already been precomputed on the disk and I could just load it up.</div><br/></div></div><div id="35837638" class="c"><input type="checkbox" id="c-35837638" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#35836826">prev</a><span>|</span><a href="#35837022">next</a><span>|</span><label class="collapse" for="c-35837638">[-]</label><label class="expand" for="c-35837638">[1 more]</label></div><br/><div class="children"><div class="content">idea: linked parameters &#x2F; models tree<p>build a model that can change the number of parameters in the vicinity of some meaning, effectively increasing the local resolution around that meaning<p>so parameter space becomes linked-parameter space, between models<p>links could be pruned based on activation frequency<p>another way of seeing the concept is a tree of models&#x2F;llms<p>and one additional model&#x2F;llm that all it does is manage the tree (ie. build it as it goes, use it to infer, prune it, etc)<p>Or is it too dumb what I’m saying?</div><br/></div></div><div id="35837022" class="c"><input type="checkbox" id="c-35837022" checked=""/><div class="controls bullet"><span class="by">mirker</span><span>|</span><a href="#35837638">prev</a><span>|</span><a href="#35837371">next</a><span>|</span><label class="collapse" for="c-35837022">[-]</label><label class="expand" for="c-35837022">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone have experience using these open source models in production?</div><br/></div></div><div id="35837371" class="c"><input type="checkbox" id="c-35837371" checked=""/><div class="controls bullet"><span class="by">born-jre</span><span>|</span><a href="#35837022">prev</a><span>|</span><a href="#35836588">next</a><span>|</span><label class="collapse" for="c-35837371">[-]</label><label class="expand" for="c-35837371">[1 more]</label></div><br/><div class="children"><div class="content">i also wonder how powerful will 3b model will be ? can it act as a prompt router   where it can make API call to ChatGPT or other specified model for actual processing. its probably possible to do this with langchain but i have not tried it yet.</div><br/></div></div><div id="35836588" class="c"><input type="checkbox" id="c-35836588" checked=""/><div class="controls bullet"><span class="by">acapybara</span><span>|</span><a href="#35837371">prev</a><span>|</span><label class="collapse" for="c-35836588">[-]</label><label class="expand" for="c-35836588">[23 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been following the RedPajama project closely and I must say, it&#x27;s quite an impressive undertaking. The fact that it&#x27;s all open-source, and the collaboration between various institutions, is nothing short of amazing. This shows the power of the open-source community in action, with a bunch of smart people coming together to build something truly remarkable.<p>The 3B model, being super fast and accessible, is a game changer for a lot of us who may not have the latest hardware. I mean, running on an RTX 2070 that was released 5 years ago? That&#x27;s pretty cool.<p>As for the 7B model, it&#x27;s great to see that it&#x27;s already outperforming the Pythia 7B. The bigger dataset definitely seems to be making a difference here. I&#x27;m eager to see how far this project goes, and what kinda improvements we can expect in the coming weeks with the new RedPajama dataset they&#x27;re working on.<p>One thing I found interesting is the mention of differences between the LLaMA 7B and their replication. I&#x27;d love to learn more about those differences, as it could shed light on what&#x27;s working well and what could be improved further.</div><br/><div id="35836652" class="c"><input type="checkbox" id="c-35836652" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#35836588">parent</a><span>|</span><a href="#35836739">next</a><span>|</span><label class="collapse" for="c-35836652">[-]</label><label class="expand" for="c-35836652">[18 more]</label></div><br/><div class="children"><div class="content">Sorry, excuse my ignorance, but why is having access to a 3B model a gamechanger?<p>I played with a pirated 7B model a while back. My computer runs a 1080 TI - so it used to be good but now it&#x27;s pretty old. The model ran with a reasonable number of tokens&#x2F;sec, but the quality was just trash compared to what I&#x27;d grown used to with ChatGPT. It was a novelty I interacted with for just a single evening.<p>I truly don&#x27;t understand the use case for a 3B model with our current technologies.<p>What are you going to use it for?</div><br/><div id="35836709" class="c"><input type="checkbox" id="c-35836709" checked=""/><div class="controls bullet"><span class="by">examplary_cable</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836652">parent</a><span>|</span><a href="#35836736">next</a><span>|</span><label class="collapse" for="c-35836709">[-]</label><label class="expand" for="c-35836709">[7 more]</label></div><br/><div class="children"><div class="content">You can ultra fine tune those models ... look at vicune 13B, if you know how to prompt it well, you can get it to work as &quot;&quot;&quot;&quot;well&quot;&quot;&quot;&quot; as ChatGPT. Running on local hardware .... I just got vicune 13b on gradio[1] to act as japanese kanji personal trainer, and I&#x27;ve only used a simple prompt: &quot;I want you to act as a Japanese Kanji quiz machine. Each time I ask you for the next question, you are to provide one random Japanese kanji from JLPT N5 kanji list and ask for its meaning. You will generate four options, one correct, three wrong. The options will be labeled from A to D. I will reply to you with one letter, corresponding to one of these labels. You will evaluate my each answer based on your last question and tell me if I chose the right option. If I chose the right label, you will congratulate me. Otherwise you will tell me the right answer. Then you will ask me the next question. Avoid simple kanjis, let&#x27;s go.&quot;<p>[1] <a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;</a></div><br/><div id="35836958" class="c"><input type="checkbox" id="c-35836958" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836709">parent</a><span>|</span><a href="#35836885">next</a><span>|</span><label class="collapse" for="c-35836958">[-]</label><label class="expand" for="c-35836958">[1 more]</label></div><br/><div class="children"><div class="content">Sure, a 13B model can be fine-tuned to be pretty decent, which is quite remarkable compared to GPT3&#x27;s 175B paramters. But a 3B model has 1&#x2F;4th as many parameters as Vicune-13B, or about twice as many as GPT2. Can you really fine-tune that to do anything useful that wouldn&#x27;t be better handled by a more specialized open-source model?</div><br/></div></div><div id="35836885" class="c"><input type="checkbox" id="c-35836885" checked=""/><div class="controls bullet"><span class="by">ym555</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836709">parent</a><span>|</span><a href="#35836958">prev</a><span>|</span><a href="#35836830">next</a><span>|</span><label class="collapse" for="c-35836885">[-]</label><label class="expand" for="c-35836885">[2 more]</label></div><br/><div class="children"><div class="content">While I recognize that this only one example of what you can do, you can just ask chatgpt to program you a traditional program that does something like this and not have to run a (pretty big&#x2F;power-intensive&#x2F;slow on most hardware) 3B&#x2F;7B parameter model for simple tasks like these.<p>Yeah it wouldn&#x27;t be as flexible as a LLM (for example synonyms won&#x27;t work), but I doubt that for this particular task it&#x27;ll be that big of problem, and you can ask it to tweak the program in various ways (for example introducing crude spaced-repetition) making it arguably better than the AI solution which takes sometime to prompt engineer and will never be &quot;perfect&quot;.<p>I don&#x27;t really know how much better fine-tuning makes these models, so I can&#x27;t think of anything that they can actually be used for where they aren&#x27;t worse than traditional programs, maybe as an AI in games? for example making them role-play as a historical figure in Civilization 6.</div><br/><div id="35836924" class="c"><input type="checkbox" id="c-35836924" checked=""/><div class="controls bullet"><span class="by">examplary_cable</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836885">parent</a><span>|</span><a href="#35836830">next</a><span>|</span><label class="collapse" for="c-35836924">[-]</label><label class="expand" for="c-35836924">[1 more]</label></div><br/><div class="children"><div class="content">My example here was silly and I admit. But the point was that this simple task cab become more &quot;nuanced&quot;(Aside from ChatRWVK-raven, no other model quite &quot;works&quot; like Vicuna or &quot;tuned LLama&quot;), it can, given the correct prompt act as someone in a fictional work which might help you learn the language better by increase conversational time(most important metric, I&#x27;m talking comprehensible input here) by the virtue of being more enjoyable.<p>Overall I like the progress: LLama releases -&gt; LLama fine turned on larger models gets similar performance to ChatGPT on lower parameters(more efficient) -&gt; People can replicate LLama&#x27;s model without anything special, effectively making LLMs a &quot;Commodity&quot; -&gt; You are Here.</div><br/></div></div></div></div><div id="35836830" class="c"><input type="checkbox" id="c-35836830" checked=""/><div class="controls bullet"><span class="by">cced</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836709">parent</a><span>|</span><a href="#35836885">prev</a><span>|</span><a href="#35836736">next</a><span>|</span><label class="collapse" for="c-35836830">[-]</label><label class="expand" for="c-35836830">[3 more]</label></div><br/><div class="children"><div class="content">How can someone get into using these models? How does ‘tuning’ work? How might I go about using these models for doing things like say summarizing news articles or video transcriptions? When someone tunes a model for a task, what exactly are they doing and how does this ‘change’ the model?</div><br/><div id="35836901" class="c"><input type="checkbox" id="c-35836901" checked=""/><div class="controls bullet"><span class="by">examplary_cable</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836830">parent</a><span>|</span><a href="#35836998">next</a><span>|</span><label class="collapse" for="c-35836901">[-]</label><label class="expand" for="c-35836901">[1 more]</label></div><br/><div class="children"><div class="content">(I&#x27;m not an expert)<p>&gt; How can someone get into using these models<p>You can use gradio(online) or download(git will not download, it&#x27;s too big, do it manually) the weights at <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;lmsys&#x2F;vicuna-13b-delta-v1.1&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;lmsys&#x2F;vicuna-13b-delta-v1.1&#x2F;tree&#x2F;main</a> and then load the model in pytourch and try inference(text generation). But you&#x27;ll need either a lot of RAM(16GB,32GB+) or VRAM(Card).<p>&gt; How might I go about using these models for doing things like say summarizing news articles or video transcriptions
Again, you might try online or setup a python&#x2F;bash&#x2F;powershell script to load the model for you so you can use it. If you can pay I would recommend runpod for the shared GPUs.<p>&gt; When someone tunes a model for a task, what exactly are they doing and how does this ‘change’ the model?
From my view ... not much ... &quot;fine-tuning&quot; means training(tuning) on a specific dataset(fine, as in fine-grained). As I believe(I&#x27;m not sure) they just run more epochs on the model with the new data you have provided it until they reach a good loss(the model works), that&#x27;s why quality data is important.<p>You might try <a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a> they have a pretty easy setup config. Again, you&#x27;ll need a lot of RAM and a good CPU for inference on CPU or a GPU.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;lmsys&#x2F;vicuna-13b-delta-v1.1&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;lmsys&#x2F;vicuna-13b-delta-v1.1&#x2F;tree&#x2F;main</a></div><br/></div></div><div id="35836998" class="c"><input type="checkbox" id="c-35836998" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836830">parent</a><span>|</span><a href="#35836901">prev</a><span>|</span><a href="#35836736">next</a><span>|</span><label class="collapse" for="c-35836998">[-]</label><label class="expand" for="c-35836998">[1 more]</label></div><br/><div class="children"><div class="content">A newer but much better system actually reduces the model size while reducing the functionality of the system - similar to training a NN for a very specific task (as was typical several years ago), but now it can happen with <i>far</i> less data.
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.02301.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.02301.pdf</a>
This paper is quite fantastic, and will likely shape up to be a quite important glue task for LLM models to generate.</div><br/></div></div></div></div></div></div><div id="35836736" class="c"><input type="checkbox" id="c-35836736" checked=""/><div class="controls bullet"><span class="by">ttt3ts</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836652">parent</a><span>|</span><a href="#35836709">prev</a><span>|</span><a href="#35836730">next</a><span>|</span><label class="collapse" for="c-35836736">[-]</label><label class="expand" for="c-35836736">[1 more]</label></div><br/><div class="children"><div class="content">Finetuning which can easily be done on consumer hardware and can give these models a lot more power for specific applications.<p>Also, ChatGPT just can&#x27;t do a lot of things because of their &quot;rules&quot;. I was doing question answering about products on Amazon with ChatGPT and refused to answer any questions about underwear, certain books&#x2F;videos, etc</div><br/></div></div><div id="35836730" class="c"><input type="checkbox" id="c-35836730" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836652">parent</a><span>|</span><a href="#35836736">prev</a><span>|</span><a href="#35837336">next</a><span>|</span><label class="collapse" for="c-35836730">[-]</label><label class="expand" for="c-35836730">[1 more]</label></div><br/><div class="children"><div class="content">Depends on what you want it for. Chatting isn&#x27;t the only application. For text summarization a model like Vicuna-13b has similar performance to ChatGPT 3.5. Fine-tuned models like the one in this thread might perform way better than the initial ones that leaked from Meta. The important thing is that there&#x27;s constant progress in this area from the Open Source community and we&#x27;re about to see amazing things in the future.</div><br/></div></div><div id="35837336" class="c"><input type="checkbox" id="c-35837336" checked=""/><div class="controls bullet"><span class="by">barbariangrunge</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836652">parent</a><span>|</span><a href="#35836730">prev</a><span>|</span><a href="#35836692">next</a><span>|</span><label class="collapse" for="c-35837336">[-]</label><label class="expand" for="c-35837336">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m in the market for a laptop. If I was crazy and wanted to run or train models like these, what kind of resources would I need?<p>Would the way the m2 MacBooks share memory be an advantage, or would the lack of cuda support be a killer? Can you do anything with 16GB, or do you need 128gb or something like that? How large are the datasets?<p>I&#x27;ve only used scikit-learn and pandas so far, I&#x27;m not very familiar with neural networks yet</div><br/><div id="35837378" class="c"><input type="checkbox" id="c-35837378" checked=""/><div class="controls bullet"><span class="by">zamnos</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35837336">parent</a><span>|</span><a href="#35836692">next</a><span>|</span><label class="collapse" for="c-35837378">[-]</label><label class="expand" for="c-35837378">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not crazy to want to train or run models like these, it&#x27;s actually quite popular right now! :) The question for you to answer is how handy with scikit-learn and pandas are you, and how much do you want to be on the bleeding edge of things? Most stuff is coming out for CUDA first, since that&#x27;s what the industrial grade GPUS (A100s) use, so with Apple Arm you either have to wait for someone to port it, or port it yourself.<p>On the other hand, getting &gt; 8 GiB VRAM on a laptop GPU is rare; you&#x27;re definitely not getting 128 GiB VRAM, so Apple Arm, with 32 or 64 GiB or RAM (get 128 if you can afford it) is going to get you more gigabytes of usable RAM for training&#x2F;inference.</div><br/><div id="35837427" class="c"><input type="checkbox" id="c-35837427" checked=""/><div class="controls bullet"><span class="by">barbariangrunge</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35837378">parent</a><span>|</span><a href="#35836692">next</a><span>|</span><label class="collapse" for="c-35837427">[-]</label><label class="expand" for="c-35837427">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. It seems to me that it&#x27;s really hard to get more than 10-14 GB of VRAM without using some sort of hyper expensive cluster. What would it cost if you wanted to do it with Nvidia? Being able to share ordinary ram with the GPU in a Mac could maybe be a unique value proposition</div><br/></div></div></div></div></div></div><div id="35836692" class="c"><input type="checkbox" id="c-35836692" checked=""/><div class="controls bullet"><span class="by">youssefabdelm</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836652">parent</a><span>|</span><a href="#35837336">prev</a><span>|</span><a href="#35836691">next</a><span>|</span><label class="collapse" for="c-35836692">[-]</label><label class="expand" for="c-35836692">[1 more]</label></div><br/><div class="children"><div class="content">Completely agree. Perhaps they were planning to fine-tune it for something though.</div><br/></div></div><div id="35836691" class="c"><input type="checkbox" id="c-35836691" checked=""/><div class="controls bullet"><span class="by">acapybara</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836652">parent</a><span>|</span><a href="#35836692">prev</a><span>|</span><a href="#35836739">next</a><span>|</span><label class="collapse" for="c-35836691">[-]</label><label class="expand" for="c-35836691">[4 more]</label></div><br/><div class="children"><div class="content">Hey SeanAnderson, good question! While parameter count is certainly an important factor in model performance, it&#x27;s not the only one. The RedPajama project is taking a more nuanced approach to understanding what makes a model perform well, and their focus on smaller models like the 3B is a big part of that.<p>Sure, you may have played with a 7B model in the past, but that doesn&#x27;t mean there&#x27;s no use case for a smaller model like the 3B. In fact, having a performant, smaller model is a game changer for a lot of applications that don&#x27;t require the massive scale of the larger models. Plus, smaller models are generally faster and more accessible, which is always a plus.</div><br/><div id="35836824" class="c"><input type="checkbox" id="c-35836824" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836691">parent</a><span>|</span><a href="#35837363">next</a><span>|</span><label class="collapse" for="c-35836824">[-]</label><label class="expand" for="c-35836824">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In fact, having a performant, smaller model is a game changer for a lot of applications that don&#x27;t require the massive scale of the larger models.<p>So we <i>are</i> all in agreement here that a 3B model is <i>fundamentally inferior</i> to a larger model?<p>Not that it doesn’t have uses; not that there’s no value in research in small models.<p>Just, honestly, that these smaller models don’t have the capabilities of the larger models.<p>It’d be good to be a direct acknowledgment of that, because it seems like you’re going out of your way to promote the “it’s fine to have a small model”; and it is, roughly speaking. Parameter count isn’t everything. Small models are accessible, you can easily fine tune them. They are interesting.<p>…but, they are not as good, as far as I’m aware, in terms of output, in terms of general purpose function, as larger models.</div><br/></div></div><div id="35837363" class="c"><input type="checkbox" id="c-35837363" checked=""/><div class="controls bullet"><span class="by">robertlagrant</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836691">parent</a><span>|</span><a href="#35836824">prev</a><span>|</span><a href="#35836797">next</a><span>|</span><label class="collapse" for="c-35837363">[-]</label><label class="expand" for="c-35837363">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Hey SeanAnderson, good question! While parameter count is certainly an important factor in model performance, it&#x27;s not the only one. The RedPajama project is taking a more nuanced approach to understanding what makes a model perform well, and their focus on smaller models like the 3B is a big part of that.<p>Sure, you may have played with a 7B model in the past, but that doesn&#x27;t mean there&#x27;s no use case for a smaller model like the 3B. In fact, having a performant, smaller model is a game changer for a lot of applications that don&#x27;t require the massive scale of the larger models. Plus, smaller models are generally faster and more accessible, which is always a plus.<p>It&#x27;s hard to pick out the actual answer: what is the application that this is good at? What has their &quot;more nuanced&quot; approach to understanding performance increased this model&#x27;s performance at doing?</div><br/></div></div><div id="35836797" class="c"><input type="checkbox" id="c-35836797" checked=""/><div class="controls bullet"><span class="by">hhh</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836691">parent</a><span>|</span><a href="#35837363">prev</a><span>|</span><a href="#35836739">next</a><span>|</span><label class="collapse" for="c-35836797">[-]</label><label class="expand" for="c-35836797">[1 more]</label></div><br/><div class="children"><div class="content">is this comment generated by an LLM?</div><br/></div></div></div></div></div></div><div id="35836739" class="c"><input type="checkbox" id="c-35836739" checked=""/><div class="controls bullet"><span class="by">Sunhold</span><span>|</span><a href="#35836588">parent</a><span>|</span><a href="#35836652">prev</a><span>|</span><label class="collapse" for="c-35836739">[-]</label><label class="expand" for="c-35836739">[4 more]</label></div><br/><div class="children"><div class="content">Took me a bit to realize this comment was written by an LLM.</div><br/><div id="35836879" class="c"><input type="checkbox" id="c-35836879" checked=""/><div class="controls bullet"><span class="by">awegio</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836739">parent</a><span>|</span><label class="collapse" for="c-35836879">[-]</label><label class="expand" for="c-35836879">[3 more]</label></div><br/><div class="children"><div class="content">How did you realize it here? This user has multiple comments in this thread but this one actually sounds more normal than the others.<p>I find it very uncanny to see comments like this that sound like ChatGPT but are surprisingly relevant to the discussion.</div><br/><div id="35838643" class="c"><input type="checkbox" id="c-35838643" checked=""/><div class="controls bullet"><span class="by">fphhotchips</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836879">parent</a><span>|</span><a href="#35837610">next</a><span>|</span><label class="collapse" for="c-35838643">[-]</label><label class="expand" for="c-35838643">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t realise it was written by an LLM but it did come off as weird to me because it borrows phrases (most obviously the bit about a &quot;2070 released 5 years ago&quot;) from the press release itself.</div><br/></div></div><div id="35837610" class="c"><input type="checkbox" id="c-35837610" checked=""/><div class="controls bullet"><span class="by">zvolsky</span><span>|</span><a href="#35836588">root</a><span>|</span><a href="#35836879">parent</a><span>|</span><a href="#35838643">prev</a><span>|</span><label class="collapse" for="c-35837610">[-]</label><label class="expand" for="c-35837610">[1 more]</label></div><br/><div class="children"><div class="content">It is the vacuous word fluff. My best guess is that it is a genuine human comment rephrased using a language model.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>