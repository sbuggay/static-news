<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696064459061" as="style"/><link rel="stylesheet" href="styles.css?v=1696064459061"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://techcrunch.com/2023/09/27/cloudflare-launches-new-ai-tools-to-help-customers-deploy-and-run-models/">Cloudflare launches new AI tools to help customers deploy and run models</a> <span class="domain">(<a href="https://techcrunch.com">techcrunch.com</a>)</span></div><div class="subtext"><span>malavwarke</span> | <span>6 comments</span></div><br/><div><div id="37713393" class="c"><input type="checkbox" id="c-37713393" checked=""/><div class="controls bullet"><span class="by">cateye</span><span>|</span><a href="#37713372">next</a><span>|</span><label class="collapse" for="c-37713393">[-]</label><label class="expand" for="c-37713393">[4 more]</label></div><br/><div class="children"><div class="content">The expected pricing is very strange:<p><i>Regular Twitch Neurons (RTN) - running wherever there&#x27;s capacity at $0.01 &#x2F; 1k neurons<p>Fast Twitch Neurons (FTN) - running at nearest user location at $0.125 &#x2F; 1k neurons<p>Neurons are a way to measure AI output that always scales down to zero. To give you a sense of what you can accomplish with a thousand neurons, you can: generate 130 LLM responses, 830 image classifications, or 1,250 embeddings.</i><p>Who came up with this? This is ridiculous. I understand the underlying issues but would still prefer a metric like seconds of utilization multiplied by the size of worker.<p>Besides this, the expected pricing doesn&#x27;t talk about the expected pricing but just the pricing model. Have the feeling that this is not going to be competitive to platforms like Vast.ai</div><br/><div id="37713429" class="c"><input type="checkbox" id="c-37713429" checked=""/><div class="controls bullet"><span class="by">jokethrowaway</span><span>|</span><a href="#37713393">parent</a><span>|</span><a href="#37713372">next</a><span>|</span><label class="collapse" for="c-37713429">[-]</label><label class="expand" for="c-37713429">[3 more]</label></div><br/><div class="children"><div class="content">Depending on how many tokens a typical response is using, pricing will vary wildly but a rough estimate put the fast one as more expensive than chatgpt3.5 and the cheap one as way cheaper.<p>Quality will likely be heaps worse than chatgpt3.5, given it&#x27;s llama 7b<p>It&#x27;s 0.96$ per 100 fast chat responses
It&#x27;s 0.0076$ per 100 slow chat responses<p>Chatgpt 3.5 with 50 tokens input, 50 tokens output will give you 0.02$ per 100 fast responses
If the llm responses are 500 tokens in and 500 tokens out then you get 0.2$ per 100 fast responses<p>I presume people will flock to the cheap version for when they can&#x27;t afford the price and quality of chatgpt3.5.</div><br/><div id="37713506" class="c"><input type="checkbox" id="c-37713506" checked=""/><div class="controls bullet"><span class="by">akmittal</span><span>|</span><a href="#37713393">root</a><span>|</span><a href="#37713429">parent</a><span>|</span><a href="#37713372">next</a><span>|</span><label class="collapse" for="c-37713506">[-]</label><label class="expand" for="c-37713506">[2 more]</label></div><br/><div class="children"><div class="content">So running fast is &gt;100x expensive? That&#x27;s too much of a difference</div><br/><div id="37713637" class="c"><input type="checkbox" id="c-37713637" checked=""/><div class="controls bullet"><span class="by">037</span><span>|</span><a href="#37713393">root</a><span>|</span><a href="#37713506">parent</a><span>|</span><a href="#37713372">next</a><span>|</span><label class="collapse" for="c-37713637">[-]</label><label class="expand" for="c-37713637">[1 more]</label></div><br/><div class="children"><div class="content">On the other hand, if it reflects their costs, I&#x27;m very happy to have an option that is 100x cheaper, rather than a more strategic one that raises the lower price by 10x.</div><br/></div></div></div></div></div></div></div></div><div id="37713372" class="c"><input type="checkbox" id="c-37713372" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37713393">prev</a><span>|</span><label class="collapse" for="c-37713372">[-]</label><label class="expand" for="c-37713372">[1 more]</label></div><br/><div class="children"><div class="content">There isn&#x27;t much about pricing, but this fragment suggests it will be economical mostly for light use cases.<p>&gt;“Currently, customers are paying for a lot of idle compute in the form of virtual machines and GPUs that go unused,”<p>I&#x27;m definitely looking forward to having a lot more competition in the &quot;pay as you go LLM AI&quot; space. Especially services that use models one can download and run on your own hardware once a good use case has been developed.</div><br/></div></div></div></div></div></div></div></body></html>