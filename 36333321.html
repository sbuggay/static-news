<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686819663727" as="style"/><link rel="stylesheet" href="styles.css?v=1686819663727"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://llm-utils.org/Nvidia+H100+and+A100+GPUs+-+comparing+available+capacity+at+GPU+cloud+providers">Nvidia H100 and A100 GPUs – comparing available capacity at GPU cloud providers</a> <span class="domain">(<a href="https://llm-utils.org">llm-utils.org</a>)</span></div><div class="subtext"><span>tikkun</span> | <span>89 comments</span></div><br/><div><div id="36337963" class="c"><input type="checkbox" id="c-36337963" checked=""/><div class="controls bullet"><span class="by">dna_polymerase</span><span>|</span><a href="#36334142">next</a><span>|</span><label class="collapse" for="c-36337963">[-]</label><label class="expand" for="c-36337963">[1 more]</label></div><br/><div class="children"><div class="content">Who are the companies renting out GPUs at scale? There is obviously GAFAM (which have their clouds basically), OpenAI and other startups with big contracts for said cloud services, but aside from them, which companies actually rent out dozens of GPUs to train their networks? And what do they bring to the table that makes them think they can archive better than OpenAI and GAFAM?<p>Serious question. I think most companies without the top talent would do better using an API product.</div><br/></div></div><div id="36334142" class="c"><input type="checkbox" id="c-36334142" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#36337963">prev</a><span>|</span><a href="#36334214">next</a><span>|</span><label class="collapse" for="c-36334142">[-]</label><label class="expand" for="c-36334142">[5 more]</label></div><br/><div class="children"><div class="content">&gt; If you want access to H100s, you&#x27;ll have to talk to a sales rep. None that we looked at offered instant access.<p>Unless something has changed in the last few days, I was literally just using an H100 on LambdaLabs without having to speak to a sales rep…</div><br/><div id="36334592" class="c"><input type="checkbox" id="c-36334592" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#36334142">parent</a><span>|</span><a href="#36334473">next</a><span>|</span><label class="collapse" for="c-36334592">[-]</label><label class="expand" for="c-36334592">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, I was wrong - thanks!</div><br/></div></div><div id="36334473" class="c"><input type="checkbox" id="c-36334473" checked=""/><div class="controls bullet"><span class="by">kaliqt</span><span>|</span><a href="#36334142">parent</a><span>|</span><a href="#36334592">prev</a><span>|</span><a href="#36335241">next</a><span>|</span><label class="collapse" for="c-36334473">[-]</label><label class="expand" for="c-36334473">[1 more]</label></div><br/><div class="children"><div class="content">Same, since day one it&#x27;s been available for our org.</div><br/></div></div><div id="36335241" class="c"><input type="checkbox" id="c-36335241" checked=""/><div class="controls bullet"><span class="by">gavi</span><span>|</span><a href="#36334142">parent</a><span>|</span><a href="#36334473">prev</a><span>|</span><a href="#36334560">next</a><span>|</span><label class="collapse" for="c-36335241">[-]</label><label class="expand" for="c-36335241">[1 more]</label></div><br/><div class="children"><div class="content">seems like corrected on the site now</div><br/></div></div></div></div><div id="36334214" class="c"><input type="checkbox" id="c-36334214" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#36334142">prev</a><span>|</span><a href="#36334669">next</a><span>|</span><label class="collapse" for="c-36334214">[-]</label><label class="expand" for="c-36334214">[15 more]</label></div><br/><div class="children"><div class="content">Nice compiled list of stats.  I&#x27;m not sure what they mean by H100s requiring pre-approval on LambdaLabs?  Maybe I was grandfathered in since I had an account prior to their rollout, but I never had to do anything special to rent H100s there.<p>Somewhat related and hopefully helpful: My experience so far using the H100 PCIes to train ViTs:<p>Pros: A little over 2x performance compared to A100 40GB.  80GB by default.  fp8 support, which I haven&#x27;t played with but supposedly is another 2x performance win for LLMs.  For datacenters and local workstations it&#x27;s twice as power efficient as the A100s.  Supposedly they have better multi-GPU and multi-node bandwidth, but my workload doesn&#x27;t stress that and I can only rent 1xH100s at the moment.<p>Cons: I had trouble using them with anything but the most recent nVidia docker containers.  The somewhat official PyTorch containers didn&#x27;t work, nor did brewing my own.  Luckily the nVidia containers have worked fine so far.  I just don&#x27;t like that they use nightly PyTorch.  In addition to that, because of their increased performance, they really start to push the limits on feeding data fast enough to them with existing system configurations.  I was CPU-limited on the LambdaLabs 1xH100 machines because of this.<p>Overall the pricing has worked out equal for my use-case, but I&#x27;m sure fp8 would make them more affordable.  If fp8 were available out-of-the-box on PyTorch I&#x27;d play with it, but it&#x27;s only available right now from some nVidia codebase specific to LLMs.<p>Even at equal pricing, having twice the power per GPU and per node is a big win.  That increases experiment iteration across the board.<p>Side note:  If I recall correctly, nVidia is heavily differentiating the H100 products by their interface this go around, which I find quite odd.  The SXM version of the cards are supposed to be something like twice as beefy as the PCIE version?  Not sure why they&#x27;re doing that; gonna make comparing rentable instances all the more difficult if you overlook that little detail.  A100 had a little bit of this, but the difference was never much in practice except between specifically the A100 40 GB PCIe and the 80 GB SXM, which was something like 10% faster.<p>Anyone else having fun with the new toy our overlords have allowed us to play with?</div><br/><div id="36336122" class="c"><input type="checkbox" id="c-36336122" checked=""/><div class="controls bullet"><span class="by">ydau</span><span>|</span><a href="#36334214">parent</a><span>|</span><a href="#36334813">next</a><span>|</span><label class="collapse" for="c-36336122">[-]</label><label class="expand" for="c-36336122">[3 more]</label></div><br/><div class="children"><div class="content">Hey! FYI, we are hoping to roll out a fix for the PyTorch issue tomorrow (I’m one of the founders of Lambda).<p>Also, that’s good feedback on the CPUs bottlenecking. I’ll let our HPC hardware team know about this.<p>We are also looking into GPU direct storage to help resolve this:<p><a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;gpudirect-storage&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;gpudirect-storage&#x2F;</a></div><br/><div id="36336648" class="c"><input type="checkbox" id="c-36336648" checked=""/><div class="controls bullet"><span class="by">civilitty</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36336122">parent</a><span>|</span><a href="#36334813">next</a><span>|</span><label class="collapse" for="c-36336648">[-]</label><label class="expand" for="c-36336648">[2 more]</label></div><br/><div class="children"><div class="content">Can you guys expand the number of regions with storage? I don’t know what I’m doing wrong but the only storage available (Texas) never overlaps with available compute.</div><br/><div id="36336799" class="c"><input type="checkbox" id="c-36336799" checked=""/><div class="controls bullet"><span class="by">ydau</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36336648">parent</a><span>|</span><a href="#36334813">next</a><span>|</span><label class="collapse" for="c-36336799">[-]</label><label class="expand" for="c-36336799">[1 more]</label></div><br/><div class="children"><div class="content">Yes! It’s coming to all regions except Utah in a couple weeks. Utah will be a bit longer (couple months)</div><br/></div></div></div></div></div></div><div id="36334813" class="c"><input type="checkbox" id="c-36334813" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#36334214">parent</a><span>|</span><a href="#36336122">prev</a><span>|</span><a href="#36334361">next</a><span>|</span><label class="collapse" for="c-36334813">[-]</label><label class="expand" for="c-36334813">[1 more]</label></div><br/><div class="children"><div class="content">&gt;heavily differentiating the H100 products by their interface this go around, which I find quite odd.<p>Power draw. The 4090 pulls 450 watts, and is destroying connectors. H100 SXM uses 700 watts.<p>If the PCI consortium wants NVIDIA to use their standard for top end products, they need to make a better connector.</div><br/></div></div><div id="36334361" class="c"><input type="checkbox" id="c-36334361" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#36334214">parent</a><span>|</span><a href="#36334813">prev</a><span>|</span><a href="#36334435">next</a><span>|</span><label class="collapse" for="c-36334361">[-]</label><label class="expand" for="c-36334361">[1 more]</label></div><br/><div class="children"><div class="content">Many cloud providers limit what a brand new account can do.<p>Some automatically drop those limits at a certain age, others require you talk to them.</div><br/></div></div><div id="36334435" class="c"><input type="checkbox" id="c-36334435" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#36334214">parent</a><span>|</span><a href="#36334361">prev</a><span>|</span><a href="#36334338">next</a><span>|</span><label class="collapse" for="c-36334435">[-]</label><label class="expand" for="c-36334435">[7 more]</label></div><br/><div class="children"><div class="content">If you don&#x27;t mind answering, what&#x27;s your use case. Often I feel you either need to do something large scale like a 100 A100s, or you&#x27;re better off with 8 3090s to run many experiments in parallel</div><br/><div id="36335441" class="c"><input type="checkbox" id="c-36335441" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36334435">parent</a><span>|</span><a href="#36334523">next</a><span>|</span><label class="collapse" for="c-36335441">[-]</label><label class="expand" for="c-36335441">[5 more]</label></div><br/><div class="children"><div class="content">My biggest project right now is training a multi-label ViT-L&#x2F;16 model for a few hundred million samples.  Mostly a big experiment, so not something I want to invest serious money into.<p>I have a 2x3090 rig as my local machine, which has been useful for early experimentation, but I&#x27;m at the stage now where my runs are at 200 million samples which would take ages on that rig.  8xA100 can do it in tens of hours, which allows me to iterate faster.<p>An 8x 3090 or 4090 machine locally would be great, but is a huge hassle to build.  Last I looked into it there really wasn&#x27;t a lot of knowledge available online on how to even do it.  I did find an EPYC server motherboard and such that I could theoretically use, but couldn&#x27;t find a great source for a &gt;3,200 Watt server power supply.  Everything in that domain is geared towards either B2LargeCorp, or B2ServerBuilders2SmallBusinesses.<p>I could of course buy an 8xA100 rig no problem for some ungodly amount of money, but as noted above that&#x27;s not appropriate for this project.<p>In both cases, I now have to figure out what to do with 3,200kW of heat output in my office which I&#x27;m trying to avoid turning into sauna.  Or co-lo it for more money out of pocket.<p>So renting off the cloud has worked well enough at this scale.</div><br/><div id="36336756" class="c"><input type="checkbox" id="c-36336756" checked=""/><div class="controls bullet"><span class="by">esquire_900</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36335441">parent</a><span>|</span><a href="#36336619">next</a><span>|</span><label class="collapse" for="c-36336756">[-]</label><label class="expand" for="c-36336756">[2 more]</label></div><br/><div class="children"><div class="content">You might be interested in <a href="http:&#x2F;&#x2F;nonint.com&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;nonint.com&#x2F;</a>. He made a number of quite detailed blogposts on his 2 gpu machines (both 8x 3090&#x27;s), including racks, power delivery etc</div><br/><div id="36336883" class="c"><input type="checkbox" id="c-36336883" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36336756">parent</a><span>|</span><a href="#36336619">next</a><span>|</span><label class="collapse" for="c-36336883">[-]</label><label class="expand" for="c-36336883">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for that. Google utterly failed to bring...any relevant results when I was searching for what others were doing for ML rigs.<p>I&#x27;ll stick with 2x 3090s for now, but maybe I&#x27;ll go for an 8x monster in a few years when the hardware shakes out a bit if there is a good reason to do so.</div><br/></div></div></div></div><div id="36336619" class="c"><input type="checkbox" id="c-36336619" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36335441">parent</a><span>|</span><a href="#36336756">prev</a><span>|</span><a href="#36337584">next</a><span>|</span><label class="collapse" for="c-36336619">[-]</label><label class="expand" for="c-36336619">[1 more]</label></div><br/><div class="children"><div class="content"><i>&quot;Mostly a big experiment, so not something I want to invest serious money into.&quot;</i><p>So how much did it cost?</div><br/></div></div><div id="36337584" class="c"><input type="checkbox" id="c-36337584" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36335441">parent</a><span>|</span><a href="#36336619">prev</a><span>|</span><a href="#36334523">next</a><span>|</span><label class="collapse" for="c-36337584">[-]</label><label class="expand" for="c-36337584">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s ViT?</div><br/></div></div></div></div><div id="36334523" class="c"><input type="checkbox" id="c-36334523" checked=""/><div class="controls bullet"><span class="by">bluedino</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36334435">parent</a><span>|</span><a href="#36335441">prev</a><span>|</span><a href="#36334338">next</a><span>|</span><label class="collapse" for="c-36334523">[-]</label><label class="expand" for="c-36334523">[1 more]</label></div><br/><div class="children"><div class="content">We have a couple GPU nodes in our cluster, each have 4 80GB cards (or V100 nodess are only 32GB).<p>We have users that have a workstation with a 3090, but the larger amount of memory is what they are after.</div><br/></div></div></div></div><div id="36334338" class="c"><input type="checkbox" id="c-36334338" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#36334214">parent</a><span>|</span><a href="#36334435">prev</a><span>|</span><a href="#36334669">next</a><span>|</span><label class="collapse" for="c-36334338">[-]</label><label class="expand" for="c-36334338">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; The SXM version of the cards are supposed to be something like twice as beefy as the PCIE version? Not sure why they&#x27;re doing that</i><p>I can&#x27;t help but feel it&#x27;s another cloud&#x2F;enterprise cash grab. The SXM baseboards are way more expensive than server motherboards and NVIDIA makes them.</div><br/><div id="36334530" class="c"><input type="checkbox" id="c-36334530" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#36334214">root</a><span>|</span><a href="#36334338">parent</a><span>|</span><a href="#36334669">next</a><span>|</span><label class="collapse" for="c-36334530">[-]</label><label class="expand" for="c-36334530">[1 more]</label></div><br/><div class="children"><div class="content">It’s really not (well, no more than PCIE) - SXM has nvlink integrated, and more power delivery built in. Thus, they can crank the max wattage way higher, though it definitely gets into the diminishing return zone quickly past 300W.<p>SXM baseboards are made by more than Nvidia, Dell, HP, and Supermicro all have their own designs.<p>(Disclaimer - I work for MS Azure but have no internal knowledge about the costs&#x2F;designs&#x2F;capex whatever of these systems)</div><br/></div></div></div></div></div></div><div id="36334669" class="c"><input type="checkbox" id="c-36334669" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#36334214">prev</a><span>|</span><a href="#36335927">next</a><span>|</span><label class="collapse" for="c-36334669">[-]</label><label class="expand" for="c-36334669">[10 more]</label></div><br/><div class="children"><div class="content">The outcome of Nvidia&#x27;s monopoly hold on AI&#x2F;GPU computing is that consumer level devices that might otherwise be perfectly effective for this sort of stuff are prevented by Nvidia from being used for such purposes.<p>If there was real competition -like two or more other suppliers on par in terms of capability - then artificially constraining devices just to plump up prices would not be a thing.</div><br/><div id="36334836" class="c"><input type="checkbox" id="c-36334836" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36334669">parent</a><span>|</span><a href="#36335927">next</a><span>|</span><label class="collapse" for="c-36334836">[-]</label><label class="expand" for="c-36334836">[9 more]</label></div><br/><div class="children"><div class="content">&gt; consumer level devices that might otherwise be perfectly effective for this sort of stuff are prevented by Nvidia from being used for such purposes.<p>Citation?<p>To the contrary, millions of consumer-level Nvidia customers have access to datacenter-grade HPC APIs because of their vertical integration. Nvidia&#x27;s &quot;monopoly hold&quot; on GPGPU compute exists because the other competitors (eg. AMD and Apple) completely abandoned OpenCL. When the time came to build a successor, neither company ante&#x27;d up. So now we&#x27;re here.<p>CUDA is not a monopoly. If Apple or Microsoft wanted, they could start translating CUDA calls into native instructions for their own hardware. They don&#x27;t though, because it would be an investment that doesn&#x27;t make sense for their customers, costs tens of millions of dollars, and wouldn&#x27;t meaningfully hurt Nvidia unless it was Open Source.</div><br/><div id="36335664" class="c"><input type="checkbox" id="c-36335664" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#36334669">root</a><span>|</span><a href="#36334836">parent</a><span>|</span><a href="#36334878">next</a><span>|</span><label class="collapse" for="c-36335664">[-]</label><label class="expand" for="c-36335664">[3 more]</label></div><br/><div class="children"><div class="content">While OpenCL was simply not equivalent to CUDA, I think you&#x27;re correct that those other enterprises (Apple, AMD and similar) that could challenge Nvidia on the high-end GPU front simply choose not to. The thing is, the reason is if there was competition in this market, prices would sink much closer to costs and no one would be making bank whereas a large enterprise would want a higher return.<p>Also, a consumer-grade GPU can used for neural net training at the researcher level but large corporate use requires H100&#x2F;A100 and that is what&#x27;s getting traction.</div><br/><div id="36335781" class="c"><input type="checkbox" id="c-36335781" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36334669">root</a><span>|</span><a href="#36335664">parent</a><span>|</span><a href="#36334878">next</a><span>|</span><label class="collapse" for="c-36335781">[-]</label><label class="expand" for="c-36335781">[2 more]</label></div><br/><div class="children"><div class="content">&gt; prices would sink much closer to costs and no one would be making bank<p>For Apple and AMD, that&#x27;s not really a problem. Both of them drive considerable (40%+) margins on their products and can afford to drive things closer to the wire.<p>I also think more competition here would be good (and I do love lower prices) but Nvidia charges more here because they <i>know</i> they can. It&#x27;s value-based marketing that works, because their software APIs aren&#x27;t vaporware.<p>&gt; large corporate use requires H100&#x2F;A100 and that is what&#x27;s getting traction.<p>I guess... you really need a strict definition of &quot;requires&quot; for that to hold true. For every non-&quot;competing with ChatGPT&quot; application, you could probably train and deploy with consumer-grade cards. You&#x27;re technically right here though, and it invites the conversation around what <i>actually</i> constitutes abusive market positioning. Nvidia&#x27;s actions here really aren&#x27;t much different than AMD and Intel separating their datacenter and PC product lines. It&#x27;s a risky move from a &quot;keeping both users happy&quot; standpoint, but hardly anticompetitive.</div><br/><div id="36336783" class="c"><input type="checkbox" id="c-36336783" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#36334669">root</a><span>|</span><a href="#36335781">parent</a><span>|</span><a href="#36334878">next</a><span>|</span><label class="collapse" for="c-36336783">[-]</label><label class="expand" for="c-36336783">[1 more]</label></div><br/><div class="children"><div class="content"><i>Both of them drive considerable (40%+) margins on their products and can afford to drive things closer to the wire.</i><p>They could that - but the reason they command these margins is exactly because they don&#x27;t do that. I think do something like for fairly some investment but producing products that would compete with Nvidia would require a significant percentage amount of capital for any company - those dealing with tens of billions of dollar chunks expect above commodity revenues.</div><br/></div></div></div></div></div></div><div id="36334878" class="c"><input type="checkbox" id="c-36334878" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#36334669">root</a><span>|</span><a href="#36334836">parent</a><span>|</span><a href="#36335664">prev</a><span>|</span><a href="#36335927">next</a><span>|</span><label class="collapse" for="c-36334878">[-]</label><label class="expand" for="c-36334878">[5 more]</label></div><br/><div class="children"><div class="content">Consumer GPUs are very good for single GPU inference(or training small models), but Nvidia deliberately made GPU networking slower. eg 3090 has support for NVlink, but it was removed in 4090.</div><br/><div id="36334904" class="c"><input type="checkbox" id="c-36334904" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36334669">root</a><span>|</span><a href="#36334878">parent</a><span>|</span><a href="#36335207">prev</a><span>|</span><a href="#36335927">next</a><span>|</span><label class="collapse" for="c-36334904">[-]</label><label class="expand" for="c-36334904">[3 more]</label></div><br/><div class="children"><div class="content">SLI was never well-supported in the first place. It&#x27;s a shame it&#x27;s gone, but compared to multi-GPU tiling solutions I don&#x27;t think its much better, at least for AI.<p>Nvidia is certainly hostile to Open Source and not the kindest hardware vendor to boot, but that alone does not suffice a monopoly.</div><br/><div id="36335334" class="c"><input type="checkbox" id="c-36335334" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#36334669">root</a><span>|</span><a href="#36334904">parent</a><span>|</span><a href="#36335927">next</a><span>|</span><label class="collapse" for="c-36335334">[-]</label><label class="expand" for="c-36335334">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but that alone does not suffice a monopoly<p>You&#x27;re right.<p>CUDA is what does.</div><br/><div id="36335349" class="c"><input type="checkbox" id="c-36335349" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36334669">root</a><span>|</span><a href="#36335334">parent</a><span>|</span><a href="#36335927">next</a><span>|</span><label class="collapse" for="c-36335349">[-]</label><label class="expand" for="c-36335349">[1 more]</label></div><br/><div class="children"><div class="content">How? CUDA is a software API, any sufficiently motivated competitor could even legally re-impliment it for their hardware if they wanted to.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36335927" class="c"><input type="checkbox" id="c-36335927" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#36334669">prev</a><span>|</span><a href="#36334010">next</a><span>|</span><label class="collapse" for="c-36335927">[-]</label><label class="expand" for="c-36335927">[6 more]</label></div><br/><div class="children"><div class="content">It’s hard to have a serious conversation about black swan events because you’re a fool until they happen. Anyway, if China invades Taiwan, which appears increasingly likely, the availability of GPUs will plummet. If this occurs in about 2 years, it will hit at peak AI (IMO) before supply chains have adapted to new extreme levels of demand. In fact peak demand may be a catalyst for invasion. Hoarding GPUs at this point may seem laughable, but having capacity during and immediately after an invasion will be a competitive advantage of note.</div><br/><div id="36336153" class="c"><input type="checkbox" id="c-36336153" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#36335927">parent</a><span>|</span><a href="#36335984">next</a><span>|</span><label class="collapse" for="c-36336153">[-]</label><label class="expand" for="c-36336153">[3 more]</label></div><br/><div class="children"><div class="content">&gt;&gt;  if China invades Taiwan, which appears increasingly likely, the availability of GPUs will plummet<p>If China invades Taiwan, GPUs will be the least of our concerns.<p>&gt;&gt; which appears increasingly likely<p>It&#x27;s not likely. China gains from the sabre rattling and politicking.  They&#x27;ve seen what happens when a bully country attacks a smaller country.<p>The leaders of the CCP have everything they could possibly want in this world.  They won&#x27;t risk it all for an uncertain outcome attacking Taiwan.</div><br/><div id="36336772" class="c"><input type="checkbox" id="c-36336772" checked=""/><div class="controls bullet"><span class="by">koboll</span><span>|</span><a href="#36335927">root</a><span>|</span><a href="#36336153">parent</a><span>|</span><a href="#36337640">next</a><span>|</span><label class="collapse" for="c-36336772">[-]</label><label class="expand" for="c-36336772">[1 more]</label></div><br/><div class="children"><div class="content">so did Wilhelm II</div><br/></div></div><div id="36337640" class="c"><input type="checkbox" id="c-36337640" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#36335927">root</a><span>|</span><a href="#36336153">parent</a><span>|</span><a href="#36336772">prev</a><span>|</span><a href="#36335984">next</a><span>|</span><label class="collapse" for="c-36337640">[-]</label><label class="expand" for="c-36337640">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The leaders of the CCP have everything they could possibly want in this world. They won&#x27;t risk it all for an uncertain outcome attacking Taiwan.<p>They don&#x27;t have Taiwan. Historically, they think Taiwan is not a distinct country, but a splinter off of China that needs to be brought back into the fold again.<p>Just the same bullshit Putin thinks about Ukraine (and most probably also about Belarus). The only thing saving Taiwan from a Chinese invasion is just how bloody Putin&#x27;s nose got in Ukraine - and that&#x27;s <i>without</i> the defense pact Taiwan has with the US.</div><br/></div></div></div></div><div id="36335984" class="c"><input type="checkbox" id="c-36335984" checked=""/><div class="controls bullet"><span class="by">piyh</span><span>|</span><a href="#36335927">parent</a><span>|</span><a href="#36336153">prev</a><span>|</span><a href="#36334010">next</a><span>|</span><label class="collapse" for="c-36335984">[-]</label><label class="expand" for="c-36335984">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only thought about this from the perspective of TMSC fab getting hit with cruise missiles, but I&#x27;ve never gone down the long term impacts of the trade freeze that would happen with China.  Decades of globalization having to be torn down and reconstructed in a very, very short time period.</div><br/><div id="36336902" class="c"><input type="checkbox" id="c-36336902" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#36335927">root</a><span>|</span><a href="#36335984">parent</a><span>|</span><a href="#36334010">next</a><span>|</span><label class="collapse" for="c-36336902">[-]</label><label class="expand" for="c-36336902">[1 more]</label></div><br/><div class="children"><div class="content">You can be certain the US would not leave TSMC as anything but a pile of rubble if the CCP took Taiwan.</div><br/></div></div></div></div></div></div><div id="36334010" class="c"><input type="checkbox" id="c-36334010" checked=""/><div class="controls bullet"><span class="by">jpgvm</span><span>|</span><a href="#36335927">prev</a><span>|</span><a href="#36334092">next</a><span>|</span><label class="collapse" for="c-36334010">[-]</label><label class="expand" for="c-36334010">[8 more]</label></div><br/><div class="children"><div class="content">The only think I am happy about all this AI hype is Infiniband is getting some love again. A lot of people using RoCE on Connect-X HBAs but still a lot of folk doing native IB. If HPC becomes more commonplace maybe we get better subnet managers, IB routing, i.e all the stuff we were promised ~10+ years ago that never had a chance to materialise because HPC became so niche and the machines had different availability etc requirements than OLTP systems that didn&#x27;t demand that stuff getting built out. Especially the subnet managers as most HPC cluster just compute a static torus or clos-tree topology.<p>There was a time I was running QDR Infiniband (40G) at home while everyone else was still dreaming of 10G at home because the adapters and switches were so expensive.</div><br/><div id="36334069" class="c"><input type="checkbox" id="c-36334069" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36334010">parent</a><span>|</span><a href="#36334474">next</a><span>|</span><label class="collapse" for="c-36334069">[-]</label><label class="expand" for="c-36334069">[2 more]</label></div><br/><div class="children"><div class="content">QDR infiniband at home was such a great hack, and I was doing the same thing with my small network a little while ago.  As long as you run connections point-to-point (with no switch) and keep the computers near each other (in twinax range), it&#x27;s incredibly cheap and blazing fast.</div><br/><div id="36334104" class="c"><input type="checkbox" id="c-36334104" checked=""/><div class="controls bullet"><span class="by">jpgvm</span><span>|</span><a href="#36334010">root</a><span>|</span><a href="#36334069">parent</a><span>|</span><a href="#36334474">next</a><span>|</span><label class="collapse" for="c-36334104">[-]</label><label class="expand" for="c-36334104">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I ran it for years with no issues. I modified a Mellanox switch to use quieter fans and eventually switched to fibre SFPs so I could make longer cable runs (also twinax is bulky&#x2F;heavy). Was absolutely epic to use my ZFS NAS at essentially native speeds from my desktop and media PC. I was using the IB SRP target to export ZVOLs so with fast all-flash storage you really couldn&#x27;t tell it wasn&#x27;t running on a (giant) local SSD.</div><br/></div></div></div></div><div id="36334474" class="c"><input type="checkbox" id="c-36334474" checked=""/><div class="controls bullet"><span class="by">throw0101b</span><span>|</span><a href="#36334010">parent</a><span>|</span><a href="#36334069">prev</a><span>|</span><a href="#36334280">next</a><span>|</span><label class="collapse" for="c-36334474">[-]</label><label class="expand" for="c-36334474">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>The only think I am happy about all this AI hype is Infiniband is getting some love again.</i><p>Any particular reason? I know latency is lower, which is good for MPI and such, as is RDMA.<p>But for general use, why should folks care about IB instead of Ethernet?</div><br/><div id="36335194" class="c"><input type="checkbox" id="c-36335194" checked=""/><div class="controls bullet"><span class="by">jpgvm</span><span>|</span><a href="#36334010">root</a><span>|</span><a href="#36334474">parent</a><span>|</span><a href="#36334280">next</a><span>|</span><label class="collapse" for="c-36335194">[-]</label><label class="expand" for="c-36335194">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not for general use but if you build large infrastructure stuff IB can be a very interesting transport choice. Naturally you will only do this if you want to either use RDMA directly (via IB Verbs) or use protocols like IB SCSI RDMA Protocol which do so under the hood.<p>Things like &quot;Datacentre Ethernet&quot; and RDMA over Converged Ethernet (RoCE) are basically Infiniband anyway (you will find that such features are only available on Connect-X HBAs from Mellanox which can generally run in 100GE or IB mode).<p>The main reason these things matter is they are circuit switched and generally combined with network topologies that ensure full bisectional bandwidth (or something that is essentially good enough for the communication pattern the machine is designed for). This means the network becomes &quot;lossless&quot; once it&#x27;s properly verified as online and circuits are setup. For storage this is essentially unmatched, it&#x27;s equivalent to Fiber Channel but way faster and converged so you can also run your workload over the same links rather than having separate storage and network links.</div><br/></div></div></div></div><div id="36334280" class="c"><input type="checkbox" id="c-36334280" checked=""/><div class="controls bullet"><span class="by">floatinglotus</span><span>|</span><a href="#36334010">parent</a><span>|</span><a href="#36334474">prev</a><span>|</span><a href="#36334092">next</a><span>|</span><label class="collapse" for="c-36334280">[-]</label><label class="expand" for="c-36334280">[3 more]</label></div><br/><div class="children"><div class="content">IB is still so expensive.  The new Sprwctrum Ethernet switches are a better choice IMO.</div><br/><div id="36335210" class="c"><input type="checkbox" id="c-36335210" checked=""/><div class="controls bullet"><span class="by">jpgvm</span><span>|</span><a href="#36334010">root</a><span>|</span><a href="#36334280">parent</a><span>|</span><a href="#36335252">next</a><span>|</span><label class="collapse" for="c-36335210">[-]</label><label class="expand" for="c-36335210">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t kept up with the pricing for the last decade, back when I was working with it was a fraction of the cost of comparable Ethernet. Maybe things have changed drastically though.</div><br/></div></div><div id="36335252" class="c"><input type="checkbox" id="c-36335252" checked=""/><div class="controls bullet"><span class="by">throw0101b</span><span>|</span><a href="#36334010">root</a><span>|</span><a href="#36334280">parent</a><span>|</span><a href="#36335210">prev</a><span>|</span><a href="#36334092">next</a><span>|</span><label class="collapse" for="c-36335252">[-]</label><label class="expand" for="c-36335252">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>IB is still so expensive.</i><p>Also hard to get from vendors: wait lists for &gt;100G switches is a long time. No one has them in stock: probably because LLM is what all the cool kids are doing, so all inventory&#x2F;production has been spoken for.</div><br/></div></div></div></div></div></div><div id="36334092" class="c"><input type="checkbox" id="c-36334092" checked=""/><div class="controls bullet"><span class="by">garciasn</span><span>|</span><a href="#36334010">prev</a><span>|</span><a href="#36336181">next</a><span>|</span><label class="collapse" for="c-36334092">[-]</label><label class="expand" for="c-36334092">[3 more]</label></div><br/><div class="children"><div class="content">They’re saying there is a 16 GPU min for GCP. I don’t see that at all.<p><a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;gpus-pricing" rel="nofollow noreferrer">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;gpus-pricing</a><p>Am I misunderstanding the GCP pricing or how this analysis was done?</div><br/><div id="36334598" class="c"><input type="checkbox" id="c-36334598" checked=""/><div class="controls bullet"><span class="by">loosescrews</span><span>|</span><a href="#36334092">parent</a><span>|</span><a href="#36336181">next</a><span>|</span><label class="collapse" for="c-36334598">[-]</label><label class="expand" for="c-36334598">[2 more]</label></div><br/><div class="children"><div class="content">They also said that they couldn&#x27;t find pricing when it is listed on the page you linked to.</div><br/><div id="36335662" class="c"><input type="checkbox" id="c-36335662" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#36334092">root</a><span>|</span><a href="#36334598">parent</a><span>|</span><a href="#36336181">next</a><span>|</span><label class="collapse" for="c-36335662">[-]</label><label class="expand" for="c-36335662">[1 more]</label></div><br/><div class="children"><div class="content">The (sad) reality of GPU capacity in public clouds at the moment is your basically need a sales call and a reservation to get any capacity.<p>So the on demand price is not really relevant, maybe thats why they excluded it.</div><br/></div></div></div></div></div></div><div id="36336181" class="c"><input type="checkbox" id="c-36336181" checked=""/><div class="controls bullet"><span class="by">gm</span><span>|</span><a href="#36334092">prev</a><span>|</span><a href="#36334414">next</a><span>|</span><label class="collapse" for="c-36336181">[-]</label><label class="expand" for="c-36336181">[1 more]</label></div><br/><div class="children"><div class="content">Your price quotes for AWS support are wrong: Business support is 10% of your usage, or $100 USD whichever is higher. Enterprise support is 10% of your usage or $15k USD, whichever is higher. It&#x27;s actually more complicated if your spend is high. Better point people to the info page here: <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;premiumsupport&#x2F;plans&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;aws.amazon.com&#x2F;premiumsupport&#x2F;plans&#x2F;</a></div><br/></div></div><div id="36334414" class="c"><input type="checkbox" id="c-36334414" checked=""/><div class="controls bullet"><span class="by">hughw</span><span>|</span><a href="#36336181">prev</a><span>|</span><a href="#36334430">next</a><span>|</span><label class="collapse" for="c-36334414">[-]</label><label class="expand" for="c-36334414">[1 more]</label></div><br/><div class="children"><div class="content">What a great resource. I&#x27;ve been hoping someday for AWS to expose A100s 1 GPU at a time, but you can only rent the 8-bangers. A couple of vendors on this list will rent me one at a time.</div><br/></div></div><div id="36334430" class="c"><input type="checkbox" id="c-36334430" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#36334414">prev</a><span>|</span><a href="#36336220">next</a><span>|</span><label class="collapse" for="c-36334430">[-]</label><label class="expand" for="c-36334430">[6 more]</label></div><br/><div class="children"><div class="content">Crazy to see stuff like the H100 at $2.40 per GPU hour. It feels like not that long ago that K80s would cost more than that. Moore&#x27;s law still seems alive in dollars per FLOP.</div><br/><div id="36334582" class="c"><input type="checkbox" id="c-36334582" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#36334430">parent</a><span>|</span><a href="#36336220">next</a><span>|</span><label class="collapse" for="c-36334582">[-]</label><label class="expand" for="c-36334582">[5 more]</label></div><br/><div class="children"><div class="content">$21,000 per year ouch.<p>That&#x27;ll buy you about 13 RTX 4090&#x27;s outright.</div><br/><div id="36337561" class="c"><input type="checkbox" id="c-36337561" checked=""/><div class="controls bullet"><span class="by">raxxorraxor</span><span>|</span><a href="#36334430">root</a><span>|</span><a href="#36334582">parent</a><span>|</span><a href="#36335059">next</a><span>|</span><label class="collapse" for="c-36337561">[-]</label><label class="expand" for="c-36337561">[1 more]</label></div><br/><div class="children"><div class="content">Or 2-3 A100. The H100 are allegedly more budget friendly, but I din&#x27;t find that true on the &quot;black market&quot;. And I think it draws a bit less power, but I have absolutely no experience about the software suites NVidia offers.<p>I think the price is quite high to be honest, although probably realistic. Wouldn&#x27;t expect competitors to be able to offer something better and data centers need to calculate for maybe newer models with even better performance.</div><br/></div></div><div id="36335059" class="c"><input type="checkbox" id="c-36335059" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#36334430">root</a><span>|</span><a href="#36334582">parent</a><span>|</span><a href="#36337561">prev</a><span>|</span><a href="#36335386">next</a><span>|</span><label class="collapse" for="c-36335059">[-]</label><label class="expand" for="c-36335059">[1 more]</label></div><br/><div class="children"><div class="content">Why aren’t you comparing to the price of buying an H100?</div><br/></div></div><div id="36335386" class="c"><input type="checkbox" id="c-36335386" checked=""/><div class="controls bullet"><span class="by">selectodude</span><span>|</span><a href="#36334430">root</a><span>|</span><a href="#36334582">parent</a><span>|</span><a href="#36335059">prev</a><span>|</span><a href="#36335215">next</a><span>|</span><label class="collapse" for="c-36335386">[-]</label><label class="expand" for="c-36335386">[1 more]</label></div><br/><div class="children"><div class="content">An H100 is about $30,000.</div><br/></div></div></div></div></div></div><div id="36336220" class="c"><input type="checkbox" id="c-36336220" checked=""/><div class="controls bullet"><span class="by">sytelus</span><span>|</span><a href="#36334430">prev</a><span>|</span><a href="#36334171">next</a><span>|</span><label class="collapse" for="c-36336220">[-]</label><label class="expand" for="c-36336220">[1 more]</label></div><br/><div class="children"><div class="content">Looks like there is no dirth of H100s anymore. So, is GPU scarcity finally gone?</div><br/></div></div><div id="36334171" class="c"><input type="checkbox" id="c-36334171" checked=""/><div class="controls bullet"><span class="by">nickstinemates</span><span>|</span><a href="#36336220">prev</a><span>|</span><a href="#36334006">next</a><span>|</span><label class="collapse" for="c-36334171">[-]</label><label class="expand" for="c-36334171">[11 more]</label></div><br/><div class="children"><div class="content">It&#x27;s surprisingly simple to run all of this at home.<p>I just took a bunch of GPUs out of storage and racked up a machine yesterday. From 0 to Linux&#x2F;Docker install to Stable Diffusion &#x2F; llama in about 2 hours.</div><br/><div id="36334373" class="c"><input type="checkbox" id="c-36334373" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#36334171">parent</a><span>|</span><a href="#36334195">next</a><span>|</span><label class="collapse" for="c-36334373">[-]</label><label class="expand" for="c-36334373">[5 more]</label></div><br/><div class="children"><div class="content">Many old consumer gaming GPUs will run an implementation of Stable Diffusion.  But this page seems to be about getting use of H100 and A100, such as one might want for running or training decent-sized LLMs.</div><br/><div id="36335365" class="c"><input type="checkbox" id="c-36335365" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#36334171">root</a><span>|</span><a href="#36334373">parent</a><span>|</span><a href="#36334195">next</a><span>|</span><label class="collapse" for="c-36335365">[-]</label><label class="expand" for="c-36335365">[4 more]</label></div><br/><div class="children"><div class="content">Training anything resembling a current LLM from scratch is so far beyond the ability&#x2F;capital of anyone using a site like this.<p>Facebook&#x2F;Meta used 8,000 A100s to train LLaMA (for example).<p>If you’re doing anything with an LLM it’s fine-tuning and there’s a new approach nearly weekly to do it better, faster, cheaper, and easier on 24GB cards which can be had for less than $1000.<p>Same for inference.</div><br/><div id="36336326" class="c"><input type="checkbox" id="c-36336326" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#36334171">root</a><span>|</span><a href="#36335365">parent</a><span>|</span><a href="#36336084">next</a><span>|</span><label class="collapse" for="c-36336326">[-]</label><label class="expand" for="c-36336326">[1 more]</label></div><br/><div class="children"><div class="content">Is the earlier point that people should see what they can do with &quot;common household ingredients&quot;, before they assume they need to pay cloud providers for bigger&#x2F;more iron?<p>I agree, and I have a 3090 for that purpose, and once wrote a tutorial for others wanting to do ML stuff on a GPU at home rather than rent from a cloud provider.<p>But a consumer GPU (or eBay older Tesla card) can&#x27;t do everything that a rental pool of H100 and A100 can do, and I and other readers here will sometimes want to do those other things.<p>I didn&#x27;t want to confuse people that all they&#x27;d need was to buy up a bunch of random retired Ethereum miner GPUs, no matter what they wanted to do with ML.</div><br/></div></div><div id="36336084" class="c"><input type="checkbox" id="c-36336084" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#36334171">root</a><span>|</span><a href="#36335365">parent</a><span>|</span><a href="#36336326">prev</a><span>|</span><a href="#36336009">next</a><span>|</span><label class="collapse" for="c-36336084">[-]</label><label class="expand" for="c-36336084">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps we need a new term... &quot;Armchair LLMer&quot;?</div><br/></div></div><div id="36336009" class="c"><input type="checkbox" id="c-36336009" checked=""/><div class="controls bullet"><span class="by">nickstinemates</span><span>|</span><a href="#36334171">root</a><span>|</span><a href="#36335365">parent</a><span>|</span><a href="#36336084">prev</a><span>|</span><a href="#36334195">next</a><span>|</span><label class="collapse" for="c-36336009">[-]</label><label class="expand" for="c-36336009">[1 more]</label></div><br/><div class="children"><div class="content">We just don&#x27;t understand.</div><br/></div></div></div></div></div></div><div id="36334195" class="c"><input type="checkbox" id="c-36334195" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#36334171">parent</a><span>|</span><a href="#36334373">prev</a><span>|</span><a href="#36334585">next</a><span>|</span><label class="collapse" for="c-36334195">[-]</label><label class="expand" for="c-36334195">[4 more]</label></div><br/><div class="children"><div class="content">Most people don’t have multiple GPUs in storage, a server chassis, or a place to rack up said chassis.</div><br/><div id="36334294" class="c"><input type="checkbox" id="c-36334294" checked=""/><div class="controls bullet"><span class="by">nickstinemates</span><span>|</span><a href="#36334171">root</a><span>|</span><a href="#36334195">parent</a><span>|</span><a href="#36334585">next</a><span>|</span><label class="collapse" for="c-36334294">[-]</label><label class="expand" for="c-36334294">[3 more]</label></div><br/><div class="children"><div class="content">A lot of people have PC&#x27;s with modern video cards in them, especially this crowd. It&#x27;s really no different.</div><br/><div id="36334442" class="c"><input type="checkbox" id="c-36334442" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#36334171">root</a><span>|</span><a href="#36334294">parent</a><span>|</span><a href="#36334585">next</a><span>|</span><label class="collapse" for="c-36334442">[-]</label><label class="expand" for="c-36334442">[2 more]</label></div><br/><div class="children"><div class="content">An H100 or A100 have very different capabilities from the cards you&#x27;re talking about.</div><br/><div id="36334499" class="c"><input type="checkbox" id="c-36334499" checked=""/><div class="controls bullet"><span class="by">nickstinemates</span><span>|</span><a href="#36334171">root</a><span>|</span><a href="#36334442">parent</a><span>|</span><a href="#36334585">next</a><span>|</span><label class="collapse" for="c-36334499">[-]</label><label class="expand" for="c-36334499">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, I wasn&#x27;t aware.</div><br/></div></div></div></div></div></div></div></div><div id="36334585" class="c"><input type="checkbox" id="c-36334585" checked=""/><div class="controls bullet"><span class="by">Ologn</span><span>|</span><a href="#36334171">parent</a><span>|</span><a href="#36334195">prev</a><span>|</span><a href="#36334006">next</a><span>|</span><label class="collapse" for="c-36334585">[-]</label><label class="expand" for="c-36334585">[1 more]</label></div><br/><div class="children"><div class="content">I have an RTX 3060 on my desktop, and my friend has an RTX 4090 on his desktop turned server.<p>We are both running Stable Diffusion with Dreambooth etc., Llama (well, he is running Llama, I am running 4-bit Llama), Deep Floyd and so on.  Both machines are running Ubuntu, don&#x27;t know what distros are good for servers.<p>Incidentally, NVDA closed at 429.97 today, an all time high (it was 108.13 last year, after they were banned from selling A100s&#x2F;H100s to China - it has almost quadrupled in price in a year).</div><br/></div></div></div></div><div id="36334006" class="c"><input type="checkbox" id="c-36334006" checked=""/><div class="controls bullet"><span class="by">nkingsy</span><span>|</span><a href="#36334171">prev</a><span>|</span><a href="#36334153">next</a><span>|</span><label class="collapse" for="c-36334006">[-]</label><label class="expand" for="c-36334006">[10 more]</label></div><br/><div class="children"><div class="content">People talk about cards not being worth the electricity vs cloud. Seems like an a100 pulls 300w, costs $1.50&#x2F;hr ish to rent, and costs $12,000 to buy, meaning it pays for itself with 1 year of constant use.</div><br/><div id="36334303" class="c"><input type="checkbox" id="c-36334303" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#36334006">parent</a><span>|</span><a href="#36334156">next</a><span>|</span><label class="collapse" for="c-36334303">[-]</label><label class="expand" for="c-36334303">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a business opportunity. Of course, the risk is that by the time you make your money back, the cards you&#x27;ve been renting to customers are obsolete and you need to recapitalize...</div><br/></div></div><div id="36334156" class="c"><input type="checkbox" id="c-36334156" checked=""/><div class="controls bullet"><span class="by">JoeOfTexas</span><span>|</span><a href="#36334006">parent</a><span>|</span><a href="#36334303">prev</a><span>|</span><a href="#36334301">next</a><span>|</span><label class="collapse" for="c-36334156">[-]</label><label class="expand" for="c-36334156">[1 more]</label></div><br/><div class="children"><div class="content">Labor and infrastructure add much more costs.  But I&#x27;m sure with the current hype they are all making bank.</div><br/></div></div><div id="36334301" class="c"><input type="checkbox" id="c-36334301" checked=""/><div class="controls bullet"><span class="by">floatinglotus</span><span>|</span><a href="#36334006">parent</a><span>|</span><a href="#36334156">prev</a><span>|</span><a href="#36334153">next</a><span>|</span><label class="collapse" for="c-36334301">[-]</label><label class="expand" for="c-36334301">[7 more]</label></div><br/><div class="children"><div class="content">Most of the new providers are putting them in liquid cooling to cut down on electric.</div><br/><div id="36334886" class="c"><input type="checkbox" id="c-36334886" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#36334006">root</a><span>|</span><a href="#36334301">parent</a><span>|</span><a href="#36334153">next</a><span>|</span><label class="collapse" for="c-36334886">[-]</label><label class="expand" for="c-36334886">[6 more]</label></div><br/><div class="children"><div class="content">Why would liquid cooling cut down on electricity cost? The only difference is fan power vs pump power -- is it really much different?</div><br/><div id="36335671" class="c"><input type="checkbox" id="c-36335671" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#36334006">root</a><span>|</span><a href="#36334886">parent</a><span>|</span><a href="#36334985">next</a><span>|</span><label class="collapse" for="c-36335671">[-]</label><label class="expand" for="c-36335671">[4 more]</label></div><br/><div class="children"><div class="content">It is much easier to transfer X amount of heat with a liquid than with air. Liquid travels in pipes and has enormous mass heat capacity (Per unit of volume, water has about 3200 times the specific heat capacity of dry air (at 77°F)). Air needs wide straight ducts and fans every 10 meters to keep air flowing.</div><br/><div id="36335830" class="c"><input type="checkbox" id="c-36335830" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#36334006">root</a><span>|</span><a href="#36335671">parent</a><span>|</span><a href="#36334985">next</a><span>|</span><label class="collapse" for="c-36335830">[-]</label><label class="expand" for="c-36335830">[3 more]</label></div><br/><div class="children"><div class="content">Sure, but removing that heat to the outside still takes the same amount of energy. Unless they just pour the water down the drain it has to be transferred somewhere, no?</div><br/><div id="36337153" class="c"><input type="checkbox" id="c-36337153" checked=""/><div class="controls bullet"><span class="by">burmanm</span><span>|</span><a href="#36334006">root</a><span>|</span><a href="#36335830">parent</a><span>|</span><a href="#36334985">next</a><span>|</span><label class="collapse" for="c-36337153">[-]</label><label class="expand" for="c-36337153">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t that depend on the sink? If they can cool down to the water, then running a water-water-heatexchanger would require less power (due to better heat transfer) than simply cooling it to the air.<p>Same I guess if they could use evaporation to do some of the cooling.</div><br/><div id="36337562" class="c"><input type="checkbox" id="c-36337562" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#36334006">root</a><span>|</span><a href="#36337153">parent</a><span>|</span><a href="#36334985">next</a><span>|</span><label class="collapse" for="c-36337562">[-]</label><label class="expand" for="c-36337562">[1 more]</label></div><br/><div class="children"><div class="content">Just seems like a really complicated setup to use unless you were going to be saving enough power to make all the complexity worth it. Remember you have to put a water block on each card and design a pump and piping system that will run through all of them.</div><br/></div></div></div></div></div></div></div></div><div id="36334985" class="c"><input type="checkbox" id="c-36334985" checked=""/><div class="controls bullet"><span class="by">frankreyes</span><span>|</span><a href="#36334006">root</a><span>|</span><a href="#36334886">parent</a><span>|</span><a href="#36335671">prev</a><span>|</span><a href="#36334153">next</a><span>|</span><label class="collapse" for="c-36334985">[-]</label><label class="expand" for="c-36334985">[1 more]</label></div><br/><div class="children"><div class="content">If I had to guess the difference is made in building cooling, not per device.</div><br/></div></div></div></div></div></div></div></div><div id="36334153" class="c"><input type="checkbox" id="c-36334153" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#36334006">prev</a><span>|</span><a href="#36335608">next</a><span>|</span><label class="collapse" for="c-36334153">[-]</label><label class="expand" for="c-36334153">[2 more]</label></div><br/><div class="children"><div class="content">This page might like to be a table.</div><br/><div id="36334591" class="c"><input type="checkbox" id="c-36334591" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#36334153">parent</a><span>|</span><a href="#36335608">next</a><span>|</span><label class="collapse" for="c-36334591">[-]</label><label class="expand" for="c-36334591">[1 more]</label></div><br/><div class="children"><div class="content">Yes. What headers would you suggest? I&#x27;d like a table format that&#x27;ll still look good on mobile without needing to horizontally scroll.</div><br/></div></div></div></div><div id="36335608" class="c"><input type="checkbox" id="c-36335608" checked=""/><div class="controls bullet"><span class="by">babypuncher</span><span>|</span><a href="#36334153">prev</a><span>|</span><a href="#36334097">next</a><span>|</span><label class="collapse" for="c-36335608">[-]</label><label class="expand" for="c-36335608">[4 more]</label></div><br/><div class="children"><div class="content">Am I a bad person for wishing the AI bubble would burst already so Nvidia would go back to allocating their capacity towards affordable consumer GPUs?<p>ChatGPT is cool, but not cool enough for me to be OK with the ridiculous prices Nvidia can get away with on GPUs nowadays.<p>Consumer GPU sales are at the lowest point they&#x27;ve been in decades, but Nvidia doesn&#x27;t care. Their profits are higher than ever because everyone and their dog is paying out the nose to build up their AI nonsense.</div><br/><div id="36336669" class="c"><input type="checkbox" id="c-36336669" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#36335608">parent</a><span>|</span><a href="#36335810">next</a><span>|</span><label class="collapse" for="c-36336669">[-]</label><label class="expand" for="c-36336669">[1 more]</label></div><br/><div class="children"><div class="content">Strong demand for faster and more capable GPU&#x27;s is the best thing that can happen for GPU prices in the long run.<p>It&#x27;ll incentive GPU development, and the more new&#x2F;more-capable GPU models come out, the cheaper older&#x2F;less-capable models will get.<p>We&#x27;ve seen this cycle again and again with all sorts of computer hardware, from hard drives to monitors, CPU&#x27;s, to memory, to graphics cards themselves.  With some notable exceptions, the power and capacity of most hardware consumers can afford today dwarfs what was affordable a decade or two ago.<p>The same will happen to GPUs if demand remains high.  So bring it on!</div><br/></div></div><div id="36335810" class="c"><input type="checkbox" id="c-36335810" checked=""/><div class="controls bullet"><span class="by">verall</span><span>|</span><a href="#36335608">parent</a><span>|</span><a href="#36336669">prev</a><span>|</span><a href="#36334097">next</a><span>|</span><label class="collapse" for="c-36335810">[-]</label><label class="expand" for="c-36335810">[2 more]</label></div><br/><div class="children"><div class="content">Used 1080 ti&#x27;s are going for less then $200 right now and can do solid 1080p gaming and light 1440p gaming, if you don&#x27;t care about raytracing and DLSS and etc.<p>If you do care about those things, then you can pay for them by buying a 3000 or 4000 series card. They&#x27;re a bit more expensive.</div><br/><div id="36335905" class="c"><input type="checkbox" id="c-36335905" checked=""/><div class="controls bullet"><span class="by">babypuncher</span><span>|</span><a href="#36335608">root</a><span>|</span><a href="#36335810">parent</a><span>|</span><a href="#36334097">next</a><span>|</span><label class="collapse" for="c-36335905">[-]</label><label class="expand" for="c-36335905">[1 more]</label></div><br/><div class="children"><div class="content">I have a 3080, I paid MSRP ($699) for it in 2020 because I got lucky. It will last me a while, but I&#x27;m dreading seeing the 5080 next year and having to pay an MSRP that is twice that for the same class of product.<p>I don&#x27;t think Pascal cards will be relevant for AAA games much longer now that AAA games are starting to skip last-gen consoles. Ubisoft showed of their Avatar and Star Wars games this week, and both are shipping with ray-traced global illumination.</div><br/></div></div></div></div></div></div><div id="36334097" class="c"><input type="checkbox" id="c-36334097" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#36335608">prev</a><span>|</span><a href="#36335796">next</a><span>|</span><label class="collapse" for="c-36334097">[-]</label><label class="expand" for="c-36334097">[2 more]</label></div><br/><div class="children"><div class="content">When capacity or cost constrained (or both) consider running ML on “lesser” GPUs like A40, A10s and even consumer grade 3090&#x2F;4090s.  With compiler optimizations these uncover surprising price-performance ratios.</div><br/><div id="36334181" class="c"><input type="checkbox" id="c-36334181" checked=""/><div class="controls bullet"><span class="by">nickstinemates</span><span>|</span><a href="#36334097">parent</a><span>|</span><a href="#36335796">next</a><span>|</span><label class="collapse" for="c-36334181">[-]</label><label class="expand" for="c-36334181">[1 more]</label></div><br/><div class="children"><div class="content">Yeah a home cluster of a few 3090&#x27;s performs remarkably well.</div><br/></div></div></div></div><div id="36335796" class="c"><input type="checkbox" id="c-36335796" checked=""/><div class="controls bullet"><span class="by">technick</span><span>|</span><a href="#36334097">prev</a><span>|</span><a href="#36336735">next</a><span>|</span><label class="collapse" for="c-36335796">[-]</label><label class="expand" for="c-36335796">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad my cloud provider isn&#x27;t listed, spot instances are already becoming hard to come by.</div><br/></div></div><div id="36336735" class="c"><input type="checkbox" id="c-36336735" checked=""/><div class="controls bullet"><span class="by">villgax</span><span>|</span><a href="#36335796">prev</a><span>|</span><label class="collapse" for="c-36336735">[-]</label><label class="expand" for="c-36336735">[1 more]</label></div><br/><div class="children"><div class="content">Oblivius has A100s at $2.41&#x2F;hr(80GB VRAM) too &amp; no talking to sales&#x2F;pre-approval crap</div><br/></div></div></div></div></div></div></div></body></html>