<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705309267003" as="style"/><link rel="stylesheet" href="styles.css?v=1705309267003"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://boilingsteam.com/an-in-depth-look-at-the-steam-deck-apu/.">Inside the Steam Deck&#x27;s APU</a> <span class="domain">(<a href="https://boilingsteam.com">boilingsteam.com</a>)</span></div><div class="subtext"><span>ekianjo</span> | <span>149 comments</span></div><br/><div><div id="38997218" class="c"><input type="checkbox" id="c-38997218" checked=""/><div class="controls bullet"><span class="by">rjzzleep</span><span>|</span><a href="#38995764">next</a><span>|</span><label class="collapse" for="c-38997218">[-]</label><label class="expand" for="c-38997218">[17 more]</label></div><br/><div class="children"><div class="content">The steamdeck works so well, because Valve spent a LOT of effort fixing AMD, wayland and pipewire issues for their handheld. Some of that then trickles down to other machine other parts don&#x27;t(like sleep, and audio). For example, they have a proper filter chain for their speakers and microphone.<p>Since recently buying an AMD based laptop I&#x27;ve come to realize how much better Intel&#x27;s software support, both in Linux AND in windows is. And that patterns moves all across AMD product lines.<p>For example, they pressured all vendors to drop S3, dropped it from Phoenix and went all in with Microsoft&#x27;s s2idle without any clear way to support it. As a result you have multiple vendors with half working idle implementations, overheating and other bugs.<p>Intel has also vastly surpassed AMD is their ML stack, even though their GPU&#x27;s are less powerful.</div><br/><div id="38997860" class="c"><input type="checkbox" id="c-38997860" checked=""/><div class="controls bullet"><span class="by">vladvasiliu</span><span>|</span><a href="#38997218">parent</a><span>|</span><a href="#38997856">next</a><span>|</span><label class="collapse" for="c-38997860">[-]</label><label class="expand" for="c-38997860">[11 more]</label></div><br/><div class="children"><div class="content">&gt; For example, they pressured all vendors to drop S3, dropped it from Phoenix and went all in with Microsoft&#x27;s s2idle without any clear way to support it. As a result you have multiple vendors with half working idle implementations, overheating and other bugs.<p>Are there still Intel parts with working S3? Linux seems to think that my 11th gen intel laptop supports it, but it doesn&#x27;t work (hangs on going to sleep). I haven&#x27;t figured how to coax Windows into using that instead of s2idle. This particular model doesn&#x27;t offer a BIOS toggle for that, as I hear it was the case on some thinkpads.<p>I&#x27;m also not convinced the s2idle situation is that much better on the Intel side. I sometimes use Windows on my work laptop and let it hang around suspended when I&#x27;m done for the day. Yesterday evening (Sunday), after two days of doing nothing, it figured it would be as good a time as any to turn into a jet engine. I also sometimes find it is pretty warm coming out of my backpack after a 45-60 minute commute with a long portion of walking, even when it&#x27;s close to freezing outside (we&#x27;ve had a few weeks of 0-2º days where I live). This doesn&#x27;t seem like an isolated thing: see all the people complaining about other manufacturers&#x27; laptops not going to sleep properly while being carried around closed in bags.<p>---<p>edit: found a way to enable S3 on windows. It goes to sleep but doesn&#x27;t wake up. It actually seems to mess up the PC so much that after a forced reboot the fan goes crazy for a few minutes before showing the UEFI logo.</div><br/><div id="38998473" class="c"><input type="checkbox" id="c-38998473" checked=""/><div class="controls bullet"><span class="by">mnahkies</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38997860">parent</a><span>|</span><a href="#38998244">next</a><span>|</span><label class="collapse" for="c-38998473">[-]</label><label class="expand" for="c-38998473">[1 more]</label></div><br/><div class="children"><div class="content">I have a similar issue with my dell xps - if I leave it suspended with being plugged in for a few days the battery drains, but what is more frustrating is plugging in to charge from that state seems to boot to a bios screen and turn it into a space heater.<p>It&#x27;s caught me out a few times where I haven&#x27;t noticed and frankly seems like a safety hazard as it&#x27;ll get very hot.<p>(If anyone has tips to prevent this that would be great, running fedora)</div><br/></div></div><div id="38998244" class="c"><input type="checkbox" id="c-38998244" checked=""/><div class="controls bullet"><span class="by">egeozcan</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38997860">parent</a><span>|</span><a href="#38998473">prev</a><span>|</span><a href="#38998521">next</a><span>|</span><label class="collapse" for="c-38998244">[-]</label><label class="expand" for="c-38998244">[7 more]</label></div><br/><div class="children"><div class="content">The sleeping is so broken on all the laptops these days that I&#x27;m seriously considering buying an old laptop because all the nice things you get with a modern powerful laptop aren&#x27;t worth it if the battery is randomly empty and you can&#x27;t turn it off.<p>It&#x27;s astonishingly inexplicably broken and the industry completely ignores it.<p>I have 0 idea why.</div><br/><div id="38998321" class="c"><input type="checkbox" id="c-38998321" checked=""/><div class="controls bullet"><span class="by">leni536</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38998244">parent</a><span>|</span><a href="#38998281">next</a><span>|</span><label class="collapse" for="c-38998321">[-]</label><label class="expand" for="c-38998321">[4 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t decide if I&#x27;m incredibly lucky or what, but I never had broken sleep in the last decade on Linux.</div><br/><div id="38998629" class="c"><input type="checkbox" id="c-38998629" checked=""/><div class="controls bullet"><span class="by">izacus</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38998321">parent</a><span>|</span><a href="#38998585">next</a><span>|</span><label class="collapse" for="c-38998629">[-]</label><label class="expand" for="c-38998629">[1 more]</label></div><br/><div class="children"><div class="content">I see you didn&#x27;t have one of the modern laptops that only support s0idle (aka. Modern Standby) and have completely removed S3.</div><br/></div></div><div id="38998585" class="c"><input type="checkbox" id="c-38998585" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38998321">parent</a><span>|</span><a href="#38998629">prev</a><span>|</span><a href="#38998458">next</a><span>|</span><label class="collapse" for="c-38998585">[-]</label><label class="expand" for="c-38998585">[1 more]</label></div><br/><div class="children"><div class="content">It is <i>very</i> hardware dependent. So you have to get ~lucky with your machine and then you&#x27;re set, basically. (Though it&#x27;s a weighted distribution; ex. ThinkPads have better than average odds)</div><br/></div></div><div id="38998458" class="c"><input type="checkbox" id="c-38998458" checked=""/><div class="controls bullet"><span class="by">vladvasiliu</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38998321">parent</a><span>|</span><a href="#38998585">prev</a><span>|</span><a href="#38998281">next</a><span>|</span><label class="collapse" for="c-38998458">[-]</label><label class="expand" for="c-38998458">[1 more]</label></div><br/><div class="children"><div class="content">For me, that has technically been the case on Linux, too, including on the laptop I was referring to. On Windows, it happens that the screen will be garbled on wake, but I don&#x27;t use Windows that much to care.<p>The only issue is that s2idle drains the battery like crazy compared to S3. But I guess &quot;it&#x27;s not a bug &#x2F; works as intended&quot;.</div><br/></div></div></div></div><div id="38998281" class="c"><input type="checkbox" id="c-38998281" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38998244">parent</a><span>|</span><a href="#38998321">prev</a><span>|</span><a href="#38998521">next</a><span>|</span><label class="collapse" for="c-38998281">[-]</label><label class="expand" for="c-38998281">[2 more]</label></div><br/><div class="children"><div class="content">You can just hibernate though.  With fast SSD&#x2F;NVMe storage it&#x27;s nearly as quick as S3 standby was.  (The one pitfall is that hibernation might require you to disable Secure Boot if you&#x27;re on a recent version of Linux, due to lockdown-mode shenanigans.)</div><br/><div id="38998477" class="c"><input type="checkbox" id="c-38998477" checked=""/><div class="controls bullet"><span class="by">vladvasiliu</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38998281">parent</a><span>|</span><a href="#38998521">next</a><span>|</span><label class="collapse" for="c-38998477">[-]</label><label class="expand" for="c-38998477">[1 more]</label></div><br/><div class="children"><div class="content">It depends on other things. For me, waking from S3 has always been nearly instant. Booting from hibernation is nowhere near that. Just the freaking UEFI takes ages to initialize.</div><br/></div></div></div></div></div></div><div id="38998521" class="c"><input type="checkbox" id="c-38998521" checked=""/><div class="controls bullet"><span class="by">gU9x3u8XmQNG</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38997860">parent</a><span>|</span><a href="#38998244">prev</a><span>|</span><a href="#38998489">next</a><span>|</span><label class="collapse" for="c-38998521">[-]</label><label class="expand" for="c-38998521">[1 more]</label></div><br/><div class="children"><div class="content">You may find the S3 sleep issues are, to my brief experience and understanding, with TPM security - not so much an “amd” or “intel” fault.</div><br/></div></div><div id="38998489" class="c"><input type="checkbox" id="c-38998489" checked=""/><div class="controls bullet"><span class="by">formerly_proven</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38997860">parent</a><span>|</span><a href="#38998521">prev</a><span>|</span><a href="#38997856">next</a><span>|</span><label class="collapse" for="c-38998489">[-]</label><label class="expand" for="c-38998489">[1 more]</label></div><br/><div class="children"><div class="content">Alder&#x2F;Raptor Lake (-P&#x2F;-U) definitely supports it, firmware willing (e.g. LG).</div><br/></div></div></div></div><div id="38997856" class="c"><input type="checkbox" id="c-38997856" checked=""/><div class="controls bullet"><span class="by">saidinesh5</span><span>|</span><a href="#38997218">parent</a><span>|</span><a href="#38997860">prev</a><span>|</span><a href="#38998031">next</a><span>|</span><label class="collapse" for="c-38997856">[-]</label><label class="expand" for="c-38997856">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s quite interesting to know. Which laptop&#x2F;AMD cpu did you purchase?<p>I may have to buy a new laptop in the near future. Linux is my default OS. I&#x27;ve been happy with my existing Ryzen 7 3000 series laptop for 5 years or so. I want to know if Intel is doing better these days.</div><br/><div id="38998311" class="c"><input type="checkbox" id="c-38998311" checked=""/><div class="controls bullet"><span class="by">opengears</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38997856">parent</a><span>|</span><a href="#38998020">next</a><span>|</span><label class="collapse" for="c-38998311">[-]</label><label class="expand" for="c-38998311">[1 more]</label></div><br/><div class="children"><div class="content">Take a look at the &#x27;Ubuntu Certified&#x27; Laptops. Most Dell, HP and Lenovo should be supported. Here is a full list <a href="https:&#x2F;&#x2F;ubuntu.com&#x2F;certified&#x2F;laptops?q=%C2%B4&amp;limit=228&amp;category=Laptop&amp;release=22.04+LTS" rel="nofollow">https:&#x2F;&#x2F;ubuntu.com&#x2F;certified&#x2F;laptops?q=%C2%B4&amp;limit=228&amp;cate...</a></div><br/></div></div><div id="38998020" class="c"><input type="checkbox" id="c-38998020" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38997856">parent</a><span>|</span><a href="#38998311">prev</a><span>|</span><a href="#38998031">next</a><span>|</span><label class="collapse" for="c-38998020">[-]</label><label class="expand" for="c-38998020">[2 more]</label></div><br/><div class="children"><div class="content">Framework is very well supported on Linux, and have both Intel and AMD builds. Highly recommended.</div><br/><div id="38998309" class="c"><input type="checkbox" id="c-38998309" checked=""/><div class="controls bullet"><span class="by">pkaye</span><span>|</span><a href="#38997218">root</a><span>|</span><a href="#38998020">parent</a><span>|</span><a href="#38998031">next</a><span>|</span><label class="collapse" for="c-38998309">[-]</label><label class="expand" for="c-38998309">[1 more]</label></div><br/><div class="children"><div class="content">How is Framework power management in Linux? Does it need lot of tweaking in mainstream distributions&#x2F;</div><br/></div></div></div></div></div></div><div id="38998031" class="c"><input type="checkbox" id="c-38998031" checked=""/><div class="controls bullet"><span class="by">nicman23</span><span>|</span><a href="#38997218">parent</a><span>|</span><a href="#38997856">prev</a><span>|</span><a href="#38995764">next</a><span>|</span><label class="collapse" for="c-38998031">[-]</label><label class="expand" for="c-38998031">[1 more]</label></div><br/><div class="children"><div class="content">seems to me you bought a bad laptop. i bought a cheap generic lenovo and the only thing that is not working is the fingerprint reader, which is probably for the best.<p>even the touchscreen &#x2F; stylus works great and that is not my words but a family member that gifted my laptop (due to irrelevant to tech, life reasons)</div><br/></div></div></div></div><div id="38995764" class="c"><input type="checkbox" id="c-38995764" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#38997218">prev</a><span>|</span><a href="#38995136">next</a><span>|</span><label class="collapse" for="c-38995764">[-]</label><label class="expand" for="c-38995764">[9 more]</label></div><br/><div class="children"><div class="content">Sounds like Magic Leap &quot;overspent&quot; on a custom chip design and fab, like they overspent on everything else, and ended up with a fraction of the sales they expected.<p>And then Valve &quot;underspent&quot; (although perhaps more of an MVP) by taking an existing good-enough chip, perhaps able to get very good yield out of fab (at a time when fabs were costing a fortune), and found yet another corner to cut on a device that is overall pretty low build quality, to hit an aggressive price point. I love my Steam Deck, and I loved the low price, but the build quality is &quot;good enough&quot; at best.<p>Valve have reversed course with the new generation, having a custom chip now that they&#x27;ve proved out the demand, and I expect the device will improve across the board as they take advantage of higher volumes, and perhaps an ability to price a little higher too.<p>I wouldn&#x27;t be surprised if Magic Leap also reverse course (or already have done? are they still around?) by moving to much cheaper off the shelf or pre-existing hardware.</div><br/><div id="38998319" class="c"><input type="checkbox" id="c-38998319" checked=""/><div class="controls bullet"><span class="by">BiteCode_dev</span><span>|</span><a href="#38995764">parent</a><span>|</span><a href="#38997340">next</a><span>|</span><label class="collapse" for="c-38998319">[-]</label><label class="expand" for="c-38998319">[1 more]</label></div><br/><div class="children"><div class="content">How amazing it is to live in a world that has reached such level of quality that the deck is labelled as &quot;good enough&quot;.<p>We are really swiming in astoninishing objects, even the simplest glass or a ball pen is a marvel.<p>So much it&#x27;s now a baseline.</div><br/></div></div><div id="38997340" class="c"><input type="checkbox" id="c-38997340" checked=""/><div class="controls bullet"><span class="by">Narishma</span><span>|</span><a href="#38995764">parent</a><span>|</span><a href="#38998319">prev</a><span>|</span><a href="#38995136">next</a><span>|</span><label class="collapse" for="c-38997340">[-]</label><label class="expand" for="c-38997340">[7 more]</label></div><br/><div class="children"><div class="content">In other words, Valve pulled a Raspberry Pi.</div><br/><div id="38997680" class="c"><input type="checkbox" id="c-38997680" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#38995764">root</a><span>|</span><a href="#38997340">parent</a><span>|</span><a href="#38995136">next</a><span>|</span><label class="collapse" for="c-38997680">[-]</label><label class="expand" for="c-38997680">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know much about the Raspberry Pi situation. Are they using existing designs? Are they doing things like fabbing extra cores to increase yields? I assume they&#x27;re on an older process anyway. I don&#x27;t necessarily feel Raspberry Pis are low quality – they&#x27;re low spec, but I believe the ones I&#x27;ve had in the past seemed good, although I guess there&#x27;s less stuff to be low quality (vs Steam Decks having a large amount of not-great plastic, control surfaces that fail in very noticeable ways, etc).</div><br/><div id="38997910" class="c"><input type="checkbox" id="c-38997910" checked=""/><div class="controls bullet"><span class="by">xyx0826</span><span>|</span><a href="#38995764">root</a><span>|</span><a href="#38997680">parent</a><span>|</span><a href="#38997960">next</a><span>|</span><label class="collapse" for="c-38997910">[-]</label><label class="expand" for="c-38997910">[1 more]</label></div><br/><div class="children"><div class="content">Raspberry Pi’s original SoC is designed by Broadcom for the set-top box market. Subsequent SoCs up until Pi 4 are of the same architecture design but with the Arm cores swapped out and upgraded.<p><a href="https:&#x2F;&#x2F;raspberrypi.stackexchange.com&#x2F;a&#x2F;563" rel="nofollow">https:&#x2F;&#x2F;raspberrypi.stackexchange.com&#x2F;a&#x2F;563</a></div><br/></div></div><div id="38997960" class="c"><input type="checkbox" id="c-38997960" checked=""/><div class="controls bullet"><span class="by">drewzero1</span><span>|</span><a href="#38995764">root</a><span>|</span><a href="#38997680">parent</a><span>|</span><a href="#38997910">prev</a><span>|</span><a href="#38997892">next</a><span>|</span><label class="collapse" for="c-38997960">[-]</label><label class="expand" for="c-38997960">[1 more]</label></div><br/><div class="children"><div class="content">I believe the story was (at one point at least) that RPi used a Broadcom SoC that had already been developed for some other application, keeping the cost down by using a chip that was already in volume production. I don&#x27;t believe this is still the case but then again I haven&#x27;t been following RPi development very closely.<p>I don&#x27;t think it indicates any use of inferior materials or build quality per se, just that they designed around existing components to avoid the high initial cost of a bespoke SoC.</div><br/></div></div><div id="38997892" class="c"><input type="checkbox" id="c-38997892" checked=""/><div class="controls bullet"><span class="by">Narishma</span><span>|</span><a href="#38995764">root</a><span>|</span><a href="#38997680">parent</a><span>|</span><a href="#38997960">prev</a><span>|</span><a href="#38997905">next</a><span>|</span><label class="collapse" for="c-38997892">[-]</label><label class="expand" for="c-38997892">[1 more]</label></div><br/><div class="children"><div class="content">Like Valve, they initially repurposed an old SoC that was designed for something else, either set-top boxes or phones, don&#x27;t remember which. After their success they started doing custom work on the follow-up SoCs.</div><br/></div></div><div id="38997905" class="c"><input type="checkbox" id="c-38997905" checked=""/><div class="controls bullet"><span class="by">bonzini</span><span>|</span><a href="#38995764">root</a><span>|</span><a href="#38997680">parent</a><span>|</span><a href="#38997892">prev</a><span>|</span><a href="#38997755">next</a><span>|</span><label class="collapse" for="c-38997905">[-]</label><label class="expand" for="c-38997905">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the lead of the Raspberry Pi foundation used to work at Broadcom and used a pretty old design that was meant for set top boxes because Broadcom sold it for little money.</div><br/></div></div><div id="38997755" class="c"><input type="checkbox" id="c-38997755" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38995764">root</a><span>|</span><a href="#38997680">parent</a><span>|</span><a href="#38997905">prev</a><span>|</span><a href="#38995136">next</a><span>|</span><label class="collapse" for="c-38997755">[-]</label><label class="expand" for="c-38997755">[1 more]</label></div><br/><div class="children"><div class="content">Note these chips weren&#x27;t fabbed with extra cores to increase yields</div><br/></div></div></div></div></div></div></div></div><div id="38995136" class="c"><input type="checkbox" id="c-38995136" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38995764">prev</a><span>|</span><a href="#38997863">next</a><span>|</span><label class="collapse" for="c-38995136">[-]</label><label class="expand" for="c-38995136">[14 more]</label></div><br/><div class="children"><div class="content">Funny that AMD would design a bespoke chip for Magic Leap that sold a tiny number of units, and Valve would ride on their coattails for a game console that undoubtedly sold way more. I wonder how much Magic Leap paid AMD for this design? Guess that explains where some of their billions went.</div><br/><div id="38995476" class="c"><input type="checkbox" id="c-38995476" checked=""/><div class="controls bullet"><span class="by">blamazon</span><span>|</span><a href="#38995136">parent</a><span>|</span><a href="#38995203">next</a><span>|</span><label class="collapse" for="c-38995476">[-]</label><label class="expand" for="c-38995476">[3 more]</label></div><br/><div class="children"><div class="content">If I remember right, I think Valve has implied that the steam deck arose out of prototypes for a wireless VR headset, so it could be possible there was some early collaboration to that effect, and Valve ended up taking a different path once they realized what the steam deck could be?</div><br/><div id="38995895" class="c"><input type="checkbox" id="c-38995895" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38995476">parent</a><span>|</span><a href="#38995203">next</a><span>|</span><label class="collapse" for="c-38995895">[-]</label><label class="expand" for="c-38995895">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  the steam deck arose out of prototypes for a wireless VR headset<p>Do you have any source for this? I don&#x27;t recall seeing this kind of info so far.</div><br/><div id="38996540" class="c"><input type="checkbox" id="c-38996540" checked=""/><div class="controls bullet"><span class="by">wlesieutre</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38995895">parent</a><span>|</span><a href="#38995203">next</a><span>|</span><label class="collapse" for="c-38996540">[-]</label><label class="expand" for="c-38996540">[1 more]</label></div><br/><div class="children"><div class="content">Not quite what they said, but this article talks about how lessons learned with the Steam Deck will inform future VR devices<p><a href="https:&#x2F;&#x2F;www.roadtovr.com&#x2F;valve-working-on-vr-steam-deckard-oled&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.roadtovr.com&#x2F;valve-working-on-vr-steam-deckard-o...</a></div><br/></div></div></div></div></div></div><div id="38995203" class="c"><input type="checkbox" id="c-38995203" checked=""/><div class="controls bullet"><span class="by">gjsman-1000</span><span>|</span><a href="#38995136">parent</a><span>|</span><a href="#38995476">prev</a><span>|</span><a href="#38996514">next</a><span>|</span><label class="collapse" for="c-38995203">[-]</label><label class="expand" for="c-38995203">[9 more]</label></div><br/><div class="children"><div class="content">One possible explanation is that Magic Leap may have significantly overestimated how many chips they needed, and wondered if someone might buy excess inventory… or at least, might buy out their contract.<p>Consider the first Magic Leap headset. They thought it would do over 100,000 units. It only did 6,000. <a href="https:&#x2F;&#x2F;www.engadget.com&#x2F;2019-12-06-magic-leap-6000-headsets-sold-report.html" rel="nofollow">https:&#x2F;&#x2F;www.engadget.com&#x2F;2019-12-06-magic-leap-6000-headsets...</a><p>I don’t know how the Leap 2 is doing. But in the above example, let’s say AMD wanted a minimum order of 100,000. If you were Magic Leap, you’re running to find a buyer - any buyer. You’re also going to cut the price for your batch as far as necessary to cover for possible increased production costs in the future. Even if you recover just half of your costs, it’s better than being forced to buy chips nobody wants that will quickly go stale.</div><br/><div id="38995517" class="c"><input type="checkbox" id="c-38995517" checked=""/><div class="controls bullet"><span class="by">exmadscientist</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38995203">parent</a><span>|</span><a href="#38996514">next</a><span>|</span><label class="collapse" for="c-38995517">[-]</label><label class="expand" for="c-38995517">[8 more]</label></div><br/><div class="children"><div class="content">Thinking about modern chips in terms of units and inventory isn&#x27;t the best way to do it. Cutting-edge logic isn&#x27;t quite priced like software, but it&#x27;s close enough: the first unit costs $10m in R&amp;D and NRE (Non-Recurring Expenses, like mask sets and test tooling) and then the second unit costs $1 or so.<p>My guess is that Magic Leap, a company infamous for spending profligately, paid for the NRE on this design, and then didn&#x27;t buy very many of them or pay for exclusivity on it (or let any exclusivity go). AMD is notorious for shopping around designs they&#x27;ve already got ready to ship (probably related to sibling comment&#x27;s observations about the console graphics market), and Valve runs lean despite not having to, and so this story adds up quite nicely. There may or may not have been some inventory lying around, but the NRE is usually the real story on these things. (It&#x27;s also possible that the custom Magic Leap sections of this chip have terrible yield -- actually that wouldn&#x27;t surprise me at all -- and so these were great candidates for die harvest. Which of course is also the sort of thing AMD loves to sell.)</div><br/><div id="38996447" class="c"><input type="checkbox" id="c-38996447" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38995517">parent</a><span>|</span><a href="#38996998">next</a><span>|</span><label class="collapse" for="c-38996447">[-]</label><label class="expand" for="c-38996447">[3 more]</label></div><br/><div class="children"><div class="content">It seems smart to go by the only successful handheld strategy - Nintendo always shops around for the cheapest chip that can get the job done. Even if it’s many years old as long as it’s cheap , highly available and can run what they need</div><br/><div id="38997160" class="c"><input type="checkbox" id="c-38997160" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38996447">parent</a><span>|</span><a href="#38996998">next</a><span>|</span><label class="collapse" for="c-38997160">[-]</label><label class="expand" for="c-38997160">[2 more]</label></div><br/><div class="children"><div class="content">The difference is Nintendo builds their own killer apps exclusively for the hardware that they choose, so they can guarantee a good experience regardless of the performance level. Steam Deck doesn&#x27;t have any native games. It&#x27;s all games written for other platforms first and if the chip&#x27;s performance doesn&#x27;t meet a minimum bar then it just won&#x27;t work.</div><br/><div id="38997252" class="c"><input type="checkbox" id="c-38997252" checked=""/><div class="controls bullet"><span class="by">dagmx</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38997160">parent</a><span>|</span><a href="#38996998">next</a><span>|</span><label class="collapse" for="c-38997252">[-]</label><label class="expand" for="c-38997252">[1 more]</label></div><br/><div class="children"><div class="content">Nit: it does have the valve on-boarding game for it, but that doesn’t really change your point.<p>Also Valve do some steamdeck optimizations for every game via their shared shader cache. Which enables it to run a lot smoother than competing portable x86 systems.</div><br/></div></div></div></div></div></div><div id="38996998" class="c"><input type="checkbox" id="c-38996998" checked=""/><div class="controls bullet"><span class="by">acdha</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38995517">parent</a><span>|</span><a href="#38996447">prev</a><span>|</span><a href="#38995538">next</a><span>|</span><label class="collapse" for="c-38996998">[-]</label><label class="expand" for="c-38996998">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any idea what the lead time on a project like this would be? Given the timing for a bad 2019 leading into the pandemic and widespread predictions of severe economic shrinkage, I’m wondering whether what we’re seeing is that they had a design in-flight but someone panicked that they were going to be far short of their minimum commitments a second time. In that case, both Magic Leap and AMD would’ve been pretty motivated to have someone commit to things like fab capacity and Valve’s earlier work at least makes the possibility of shipping in a couple years seem potentially possible.</div><br/></div></div><div id="38995538" class="c"><input type="checkbox" id="c-38995538" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38995517">parent</a><span>|</span><a href="#38996998">prev</a><span>|</span><a href="#38995581">next</a><span>|</span><label class="collapse" for="c-38995538">[-]</label><label class="expand" for="c-38995538">[2 more]</label></div><br/><div class="children"><div class="content">Interesting, why would one part of a chip have significantly worse yield than another?</div><br/><div id="38995603" class="c"><input type="checkbox" id="c-38995603" checked=""/><div class="controls bullet"><span class="by">exmadscientist</span><span>|</span><a href="#38995136">root</a><span>|</span><a href="#38995538">parent</a><span>|</span><a href="#38995581">next</a><span>|</span><label class="collapse" for="c-38995603">[-]</label><label class="expand" for="c-38995603">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just where the tightest structures (physically, electrically, or timing-wise) are found. The AMD cores and uncore are, I&#x27;m guessing, fairly well tuned to the process by skilled people. The bolt-on stuff often isn&#x27;t so well done, particularly if an end-client project manager or beancounter is driving the schedule. So it&#x27;s almost certain to happen if the added stuff has very complex paths, and you wouldn&#x27;t be going to the effort of adding silicon if you didn&#x27;t need it to do something interesting....<p>So, no proof, but it wouldn&#x27;t be any kind of surprise if this were part of the problem.</div><br/></div></div></div></div></div></div></div></div><div id="38996514" class="c"><input type="checkbox" id="c-38996514" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#38995136">parent</a><span>|</span><a href="#38995203">prev</a><span>|</span><a href="#38997863">next</a><span>|</span><label class="collapse" for="c-38996514">[-]</label><label class="expand" for="c-38996514">[1 more]</label></div><br/><div class="children"><div class="content">It seems to similar to PS3 vs Xbox360 PowerPC story</div><br/></div></div></div></div><div id="38997863" class="c"><input type="checkbox" id="c-38997863" checked=""/><div class="controls bullet"><span class="by">phonon</span><span>|</span><a href="#38995136">prev</a><span>|</span><a href="#38996630">next</a><span>|</span><label class="collapse" for="c-38997863">[-]</label><label class="expand" for="c-38997863">[1 more]</label></div><br/><div class="children"><div class="content">Interesting to note that the estimated cost for the APU in the Magic Leap 2 is $136.53.[0]<p>[0] <a href="https:&#x2F;&#x2F;electronics360.globalspec.com&#x2F;article&#x2F;20179&#x2F;techinsights-teardown-magic-leap-2-ar-headset" rel="nofollow">https:&#x2F;&#x2F;electronics360.globalspec.com&#x2F;article&#x2F;20179&#x2F;techinsi...</a></div><br/></div></div><div id="38996630" class="c"><input type="checkbox" id="c-38996630" checked=""/><div class="controls bullet"><span class="by">ApolIllo</span><span>|</span><a href="#38997863">prev</a><span>|</span><a href="#38995029">next</a><span>|</span><label class="collapse" for="c-38996630">[-]</label><label class="expand" for="c-38996630">[2 more]</label></div><br/><div class="children"><div class="content">Great article but i wish they mentioned that the APU wasn&#x27;t in the AR glasses but in the magic leap&#x27;s  Compute Pack</div><br/><div id="38996702" class="c"><input type="checkbox" id="c-38996702" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38996630">parent</a><span>|</span><a href="#38995029">next</a><span>|</span><label class="collapse" for="c-38996702">[-]</label><label class="expand" for="c-38996702">[1 more]</label></div><br/><div class="children"><div class="content">Will do!</div><br/></div></div></div></div><div id="38995029" class="c"><input type="checkbox" id="c-38995029" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38996630">prev</a><span>|</span><a href="#38995945">next</a><span>|</span><label class="collapse" for="c-38995029">[-]</label><label class="expand" for="c-38995029">[11 more]</label></div><br/><div class="children"><div class="content">So what exactly were the Magic Leap cores? They don&#x27;t look like the GPU or CPU cores...</div><br/><div id="38995147" class="c"><input type="checkbox" id="c-38995147" checked=""/><div class="controls bullet"><span class="by">jsnell</span><span>|</span><a href="#38995029">parent</a><span>|</span><a href="#38995145">next</a><span>|</span><label class="collapse" for="c-38995147">[-]</label><label class="expand" for="c-38995147">[2 more]</label></div><br/><div class="children"><div class="content">The original source[0] speculates they&#x27;re computer vision DSPs, specifically Tensilica Vision DSPs from Cadence.<p>[0] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ERm1StY-4uY" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ERm1StY-4uY</a></div><br/><div id="38995542" class="c"><input type="checkbox" id="c-38995542" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#38995029">root</a><span>|</span><a href="#38995147">parent</a><span>|</span><a href="#38995145">next</a><span>|</span><label class="collapse" for="c-38995542">[-]</label><label class="expand" for="c-38995542">[1 more]</label></div><br/><div class="children"><div class="content">It wouldn&#x27;t be the first time AMD has bundled some Cadence DSP cores into their designs, they used them for TrueAudio back in the day as well.</div><br/></div></div></div></div><div id="38995145" class="c"><input type="checkbox" id="c-38995145" checked=""/><div class="controls bullet"><span class="by">spuz</span><span>|</span><a href="#38995029">parent</a><span>|</span><a href="#38995147">prev</a><span>|</span><a href="#38995663">next</a><span>|</span><label class="collapse" for="c-38995145">[-]</label><label class="expand" for="c-38995145">[1 more]</label></div><br/><div class="children"><div class="content">The specs of the Magic Leap 2 say &quot;14 core computer vision processing engine (CVIP)&quot; which matches the 14 cores shown in the photos.</div><br/></div></div><div id="38995663" class="c"><input type="checkbox" id="c-38995663" checked=""/><div class="controls bullet"><span class="by">Red_Leaves_Flyy</span><span>|</span><a href="#38995029">parent</a><span>|</span><a href="#38995145">prev</a><span>|</span><a href="#38995945">next</a><span>|</span><label class="collapse" for="c-38995663">[-]</label><label class="expand" for="c-38995663">[7 more]</label></div><br/><div class="children"><div class="content">And a follow on question… could they be turned on and used?</div><br/><div id="38995755" class="c"><input type="checkbox" id="c-38995755" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#38995029">root</a><span>|</span><a href="#38995663">parent</a><span>|</span><a href="#38995945">next</a><span>|</span><label class="collapse" for="c-38995755">[-]</label><label class="expand" for="c-38995755">[6 more]</label></div><br/><div class="children"><div class="content">Probably not, these modular bits of silicon are usually tied to eFuses that are blown at the factory to irrevocably disable them in SKUs that aren&#x27;t supposed to have them. Many years ago it was sometimes possible to re-enable disabled cores by poking the right registers, but manufacturers learned their lesson and now they make sure that silicon stays dead.<p>Besides, even if you could enable these DSP cores you&#x27;d be hard pressed to do anything useful with them, I don&#x27;t believe there&#x27;s any public documentation or tooling whatsoever for Cadence DSPs.</div><br/><div id="38997964" class="c"><input type="checkbox" id="c-38997964" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#38995029">root</a><span>|</span><a href="#38995755">parent</a><span>|</span><a href="#38995945">next</a><span>|</span><label class="collapse" for="c-38997964">[-]</label><label class="expand" for="c-38997964">[5 more]</label></div><br/><div class="children"><div class="content">How do eFuses work to ensure it&#x27;s physically irrevocable?</div><br/><div id="38997972" class="c"><input type="checkbox" id="c-38997972" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#38995029">root</a><span>|</span><a href="#38997964">parent</a><span>|</span><a href="#38995945">next</a><span>|</span><label class="collapse" for="c-38997972">[-]</label><label class="expand" for="c-38997972">[4 more]</label></div><br/><div class="children"><div class="content">eg if the fuse blows the line that powers up that part of the chip, then it simply can&#x27;t power up after the fuse has been blown and thus is unusable.<p>Going in and fixing that blown trace inside of an IC is beyond the capabilities of almost everyone because decapping a chip renders it inoperable.</div><br/><div id="38998002" class="c"><input type="checkbox" id="c-38998002" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#38995029">root</a><span>|</span><a href="#38997972">parent</a><span>|</span><a href="#38995945">next</a><span>|</span><label class="collapse" for="c-38998002">[-]</label><label class="expand" for="c-38998002">[3 more]</label></div><br/><div class="children"><div class="content">I think decapping doesn&#x27;t necessarily render the chip inoperable, there&#x27;s some research around attacking tamper resistant hardware and probing the decapped chips while powered on. But you have to be more careful about it of course. Also do all techniques require decapping? Seems in principle you could navigate by x-ray and fix traces disconnected by electromigration efuses using ion beam&#x2F;implantation, possibly through the packaging.<p>edit: here&#x27;s someone talking about running decapped chips. <a href="https:&#x2F;&#x2F;electronics.stackexchange.com&#x2F;a&#x2F;400899" rel="nofollow">https:&#x2F;&#x2F;electronics.stackexchange.com&#x2F;a&#x2F;400899</a></div><br/><div id="38998412" class="c"><input type="checkbox" id="c-38998412" checked=""/><div class="controls bullet"><span class="by">ZiiS</span><span>|</span><a href="#38995029">root</a><span>|</span><a href="#38998002">parent</a><span>|</span><a href="#38998171">next</a><span>|</span><label class="collapse" for="c-38998412">[-]</label><label class="expand" for="c-38998412">[1 more]</label></div><br/><div class="children"><div class="content">If you have a ion beam you can probably afford to just buy working and supported cores.</div><br/></div></div><div id="38998171" class="c"><input type="checkbox" id="c-38998171" checked=""/><div class="controls bullet"><span class="by">rpmisms</span><span>|</span><a href="#38995029">root</a><span>|</span><a href="#38998002">parent</a><span>|</span><a href="#38998412">prev</a><span>|</span><a href="#38995945">next</a><span>|</span><label class="collapse" for="c-38998171">[-]</label><label class="expand" for="c-38998171">[1 more]</label></div><br/><div class="children"><div class="content">I love when chip people pipe up on HN, because it reminds me that we absolutely do magic with silicon and most of us don&#x27;t think twice about it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38995945" class="c"><input type="checkbox" id="c-38995945" checked=""/><div class="controls bullet"><span class="by">rubatuga</span><span>|</span><a href="#38995029">prev</a><span>|</span><a href="#38995921">next</a><span>|</span><label class="collapse" for="c-38995945">[-]</label><label class="expand" for="c-38995945">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone know what the die space in the top right is used for? Somebody mentioned secure enclave as another component but I doubt it needs a lot of die space.</div><br/></div></div><div id="38995921" class="c"><input type="checkbox" id="c-38995921" checked=""/><div class="controls bullet"><span class="by">calamari4065</span><span>|</span><a href="#38995945">prev</a><span>|</span><a href="#38995443">next</a><span>|</span><label class="collapse" for="c-38995921">[-]</label><label class="expand" for="c-38995921">[2 more]</label></div><br/><div class="children"><div class="content">Neat. I wonder if these cores are exposed to thr system in any way and if there&#x27;s anything interesting you can do with them?</div><br/><div id="38995928" class="c"><input type="checkbox" id="c-38995928" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38995921">parent</a><span>|</span><a href="#38995443">next</a><span>|</span><label class="collapse" for="c-38995928">[-]</label><label class="expand" for="c-38995928">[1 more]</label></div><br/><div class="children"><div class="content">No, they are not, and probably disconnected at the chip level</div><br/></div></div></div></div><div id="38995443" class="c"><input type="checkbox" id="c-38995443" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38995921">prev</a><span>|</span><a href="#38995463">next</a><span>|</span><label class="collapse" for="c-38995443">[-]</label><label class="expand" for="c-38995443">[4 more]</label></div><br/><div class="children"><div class="content">How do they take those CPU pictures? Are those &quot;delidded&quot;? False color xrays?</div><br/><div id="38995565" class="c"><input type="checkbox" id="c-38995565" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#38995443">parent</a><span>|</span><a href="#38995528">next</a><span>|</span><label class="collapse" for="c-38995565">[-]</label><label class="expand" for="c-38995565">[1 more]</label></div><br/><div class="children"><div class="content">AFAIK it&#x27;s the other side of the die from a &quot;delidded&quot; view.
So, they desoldered the die, then etched&#x2F;ground at it until the surface coating was gone, and just the semiconductor structure remains. Due to the sub-micrometer regular structures, you get interference patterns like these. I.e., the color is &quot;structural&quot;.</div><br/></div></div><div id="38995528" class="c"><input type="checkbox" id="c-38995528" checked=""/><div class="controls bullet"><span class="by">exmadscientist</span><span>|</span><a href="#38995443">parent</a><span>|</span><a href="#38995565">prev</a><span>|</span><a href="#38995463">next</a><span>|</span><label class="collapse" for="c-38995528">[-]</label><label class="expand" for="c-38995528">[2 more]</label></div><br/><div class="children"><div class="content">Very carefully delidded and then with a nice metallurgical microscope. And then usually with focus stacking.</div><br/><div id="38998525" class="c"><input type="checkbox" id="c-38998525" checked=""/><div class="controls bullet"><span class="by">formerly_proven</span><span>|</span><a href="#38995443">root</a><span>|</span><a href="#38995528">parent</a><span>|</span><a href="#38995463">next</a><span>|</span><label class="collapse" for="c-38998525">[-]</label><label class="expand" for="c-38998525">[1 more]</label></div><br/><div class="children"><div class="content">Fritzchen&#x27;s Fritz uses a normal mirrorless camera and macro lens most of the time for the pictures you see everywhere (he posts them under Creative Commons to Flickr, there&#x27;s an automated pipeline from there to Wikimedia Commons, which is why virtually every Wikipedia article on a CPU or GPU features at least one of his works).<p>Only the clean head-on die-shots (he also does these) need a metallurgical microscope - those are hard to find and very expensive.</div><br/></div></div></div></div></div></div><div id="38995463" class="c"><input type="checkbox" id="c-38995463" checked=""/><div class="controls bullet"><span class="by">pdpi</span><span>|</span><a href="#38995443">prev</a><span>|</span><label class="collapse" for="c-38995463">[-]</label><label class="expand" for="c-38995463">[87 more]</label></div><br/><div class="children"><div class="content">AMD has almost completely taken over the console market. The PS2 was a MIPS system and the PS3&#x2F;Xbox360 were PowerPC, but, for the last ten years, Sony and Microsoft have been all AMD. Intel has been out of the game since the original XBox, and nvidia only has the switch to its name. The Steam Deck-style handhelds (like the ROG Ally and the Lenovo Legion Go) are AMD systems.<p>It’s kind of interesting how they have this hold on gaming outside of conventional PCs, but can’t seem to compete with nvidia on just that one market.</div><br/><div id="38995699" class="c"><input type="checkbox" id="c-38995699" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38995676">next</a><span>|</span><label class="collapse" for="c-38995699">[-]</label><label class="expand" for="c-38995699">[32 more]</label></div><br/><div class="children"><div class="content">A console wants a competitive GPU and a competitive CPU. Nvidia has the first, Intel has the second, AMD has both. The first one is more important, hence the Switch, and AMD&#x27;s ability to do it in the pre-Ryzen era. (The original Intel-CPU Xbox had an Nvidia GPU.) Console vendors are high volume institutional buyers with aggressive price targets, so being able to get both from the same place is a big advantage.<p>For PCs, discrete GPUs are getting bought separately so that doesn&#x27;t apply. AMD does alright there but they were historically the underdog and highly budget-constrained, so without some kind of separate advantage they were struggling.<p>Now they&#x27;re making a lot of money from Ryzen&#x2F;Epyc <i>and</i> GPUs, and reinvesting most of it, so it&#x27;s plausible they&#x27;ll be more competitive going forward as the fruits of those investments come to bear.</div><br/><div id="38996242" class="c"><input type="checkbox" id="c-38996242" checked=""/><div class="controls bullet"><span class="by">cassianoleal</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995699">parent</a><span>|</span><a href="#38995967">next</a><span>|</span><label class="collapse" for="c-38996242">[-]</label><label class="expand" for="c-38996242">[11 more]</label></div><br/><div class="children"><div class="content">For gaming, AMD GPUs are generally better bang for buck than Nvidia - notably, they tend to be about the same bang for less buck, at least on the high end and some of the middle tier. The notable exception is ray tracing but that&#x27;s still pretty niche.<p>If AMD gets their act together and get the AI tooling for their GPUs to be as accessible as Nvidia&#x27;s they have a good chance to become the winners there as you can get more VRAM bang for, again, less buck.<p>In a market where they have similar performance and everything minus ray tracing for somewhere between 50% and 70% of the competitor&#x27;s prices it will be pretty easy to choose AMD GPUs.<p>They already have the best CPUs for gaming and really are positioning themselves to have the best GPUs overall as well.</div><br/><div id="38996705" class="c"><input type="checkbox" id="c-38996705" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996242">parent</a><span>|</span><a href="#38996396">next</a><span>|</span><label class="collapse" for="c-38996705">[-]</label><label class="expand" for="c-38996705">[5 more]</label></div><br/><div class="children"><div class="content">I also think something important here is AMD&#x27;s strategy with APU has been small to large. Something that really stood out to me over the last few years is that NVidia was capturing the AI market with big and powerful GPU while AMD&#x27;s efforts were all going into APU research at the low end. My belief is that they were preparing for a mobile-heavy future where small, capable all-purpose chips would have a big edge.<p>They might even be right. One of the potential advantages of the APU approach is if they GPU can be absorbed into the CPU with shared memory, a lot of the memory management of CUDA would be obsoleted and it becomes not that interesting any more. AMD are competent, they just have sucky crash-prone GPU drivers.</div><br/><div id="38996956" class="c"><input type="checkbox" id="c-38996956" checked=""/><div class="controls bullet"><span class="by">cassianoleal</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996705">parent</a><span>|</span><a href="#38996396">next</a><span>|</span><label class="collapse" for="c-38996956">[-]</label><label class="expand" for="c-38996956">[4 more]</label></div><br/><div class="children"><div class="content">&gt; AMD are competent, they just have sucky crash-prone GPU drivers.<p>I have an AMD GPU on my desktop PC and I also have a Steam Deck which uses an AMD APU. Never had a driver crash on me on either system.</div><br/><div id="38998661" class="c"><input type="checkbox" id="c-38998661" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996956">parent</a><span>|</span><a href="#38997145">next</a><span>|</span><label class="collapse" for="c-38998661">[-]</label><label class="expand" for="c-38998661">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re probably using it for graphics though; the graphics drivers are great. I refuse to buy a Nvidia card just because I don&#x27;t want to put up with closed source drivers.<p>The issue is when using ROCm. Or more accurately when preparing to crash the system by attempting to use ROCm. Although in fairness as the other commenter notes it is probably a VRAM issue so I&#x27;ve been starting to suspect maybe the real culprit might be X [0]. But it presumably doesn&#x27;t happen with CUDA and it is a major blocker to using their platform for casual things like multiplying matricies.<p>But if CPU and GPU share a memory space or it happens automatically behind the scenes, then the problem neatly disappears. I&#x27;d imagine that was what AMD was aiming for and why they tolerated the low quality of the experience to start with in ROCm.<p>[0] I really don&#x27;t know, there is a lot going on and I&#x27;m not sure what tools I&#x27;m supposed to be using to debug that sort of locked system. Might be the drivers, might be X responding really badly to some sort of OOM. I lean towards it being a driver bug.</div><br/></div></div><div id="38997145" class="c"><input type="checkbox" id="c-38997145" checked=""/><div class="controls bullet"><span class="by">skirmish</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996956">parent</a><span>|</span><a href="#38998661">prev</a><span>|</span><a href="#38998572">next</a><span>|</span><label class="collapse" for="c-38997145">[-]</label><label class="expand" for="c-38997145">[1 more]</label></div><br/><div class="children"><div class="content">When I run a LLM (llama.cpp ROCm) or stable diffusion models (Automatic1111 ROCm) on my 7900XTX under Linux, and it runs out of VRAM, it messes up the driver or hardware so badly that without a reboot all subsequent runs fail.</div><br/></div></div><div id="38998572" class="c"><input type="checkbox" id="c-38998572" checked=""/><div class="controls bullet"><span class="by">cmplxconjugate</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996956">parent</a><span>|</span><a href="#38997145">prev</a><span>|</span><a href="#38996396">next</a><span>|</span><label class="collapse" for="c-38998572">[-]</label><label class="expand" for="c-38998572">[1 more]</label></div><br/><div class="children"><div class="content">I have had a RX580, RX590, 6600XT, and 7900XT using Linux with minimal issues. My partner has had a RX590, 7700XT on Windows and she&#x27;s had so many issues it&#x27;s infuriating.</div><br/></div></div></div></div></div></div><div id="38996396" class="c"><input type="checkbox" id="c-38996396" checked=""/><div class="controls bullet"><span class="by">wincy</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996242">parent</a><span>|</span><a href="#38996705">prev</a><span>|</span><a href="#38997209">next</a><span>|</span><label class="collapse" for="c-38996396">[-]</label><label class="expand" for="c-38996396">[3 more]</label></div><br/><div class="children"><div class="content">If you’re building a midlife crisis gaming PC the 4090 is the only good choice right now. 4K gaming with all the bells and whistles turned on is a reality with that GPU.</div><br/><div id="38996997" class="c"><input type="checkbox" id="c-38996997" checked=""/><div class="controls bullet"><span class="by">IggleSniggle</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996396">parent</a><span>|</span><a href="#38997934">next</a><span>|</span><label class="collapse" for="c-38996997">[-]</label><label class="expand" for="c-38996997">[1 more]</label></div><br/><div class="children"><div class="content">Yup, just did this. Still way cheaper than a convertible or whatever</div><br/></div></div><div id="38997934" class="c"><input type="checkbox" id="c-38997934" checked=""/><div class="controls bullet"><span class="by">trevyn</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996396">parent</a><span>|</span><a href="#38996997">prev</a><span>|</span><a href="#38997209">next</a><span>|</span><label class="collapse" for="c-38997934">[-]</label><label class="expand" for="c-38997934">[1 more]</label></div><br/><div class="children"><div class="content">PSA: GeForce Now Ultimate is a great way to check this out. You get a 4080 equivalent that can stream 4K 120Hz. If you have good Internet in the continental US or most of Europe, it’s surprisingly lag-free.</div><br/></div></div></div></div><div id="38997209" class="c"><input type="checkbox" id="c-38997209" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996242">parent</a><span>|</span><a href="#38996396">prev</a><span>|</span><a href="#38996311">next</a><span>|</span><label class="collapse" for="c-38997209">[-]</label><label class="expand" for="c-38997209">[1 more]</label></div><br/><div class="children"><div class="content">AMD tend to have better paper specs but worse drivers&#x2F;software; it&#x27;s been like that for decades. At least they&#x27;re now acknowledging that the state of their software is the problem, but I haven&#x27;t seen anything to give me confidence that they&#x27;re actually going to fix it this time.</div><br/></div></div><div id="38996311" class="c"><input type="checkbox" id="c-38996311" checked=""/><div class="controls bullet"><span class="by">causality0</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996242">parent</a><span>|</span><a href="#38997209">prev</a><span>|</span><a href="#38995967">next</a><span>|</span><label class="collapse" for="c-38996311">[-]</label><label class="expand" for="c-38996311">[1 more]</label></div><br/><div class="children"><div class="content">Specifically modern &quot;normal&quot; gaming. Once you get outside that comfort zone AMD has problems again. My 7900XTX has noticeably worse performance than the 1070 I replaced when it comes to Yuzu and Xenia.</div><br/></div></div></div></div><div id="38995967" class="c"><input type="checkbox" id="c-38995967" checked=""/><div class="controls bullet"><span class="by">p_l</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995699">parent</a><span>|</span><a href="#38996242">prev</a><span>|</span><a href="#38998522">next</a><span>|</span><label class="collapse" for="c-38995967">[-]</label><label class="expand" for="c-38995967">[5 more]</label></div><br/><div class="children"><div class="content">The original Xbox was supposed to use AMD CPU as well, and parts of that remained in the architecture (for example it uses Hyper Transport bus), backroom deals with intel led to last minute replacement of CPU for an intel P6 variant.</div><br/><div id="38996260" class="c"><input type="checkbox" id="c-38996260" checked=""/><div class="controls bullet"><span class="by">WhiteDawn</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995967">parent</a><span>|</span><a href="#38996644">next</a><span>|</span><label class="collapse" for="c-38996260">[-]</label><label class="expand" for="c-38996260">[3 more]</label></div><br/><div class="children"><div class="content">So last minute that the AMD engineers found out Microsoft went with Intel at the Xbox launch party [1]<p>[1] <a href="https:&#x2F;&#x2F;kotaku.com&#x2F;report-xboxs-last-second-intel-switcheroo-left-amd-eng-1847851074" rel="nofollow">https:&#x2F;&#x2F;kotaku.com&#x2F;report-xboxs-last-second-intel-switcheroo...</a></div><br/><div id="38997020" class="c"><input type="checkbox" id="c-38997020" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996260">parent</a><span>|</span><a href="#38996782">next</a><span>|</span><label class="collapse" for="c-38997020">[-]</label><label class="expand" for="c-38997020">[1 more]</label></div><br/><div class="children"><div class="content">Also, the security system assumes that the system has a general protection fault when the program counter wraps over the 4GB boundary.  That only happens on AMD, not Intel.</div><br/></div></div><div id="38996782" class="c"><input type="checkbox" id="c-38996782" checked=""/><div class="controls bullet"><span class="by">whatever1</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996260">parent</a><span>|</span><a href="#38997020">prev</a><span>|</span><a href="#38996644">next</a><span>|</span><label class="collapse" for="c-38996782">[-]</label><label class="expand" for="c-38996782">[1 more]</label></div><br/><div class="children"><div class="content">Wow! Good find!</div><br/></div></div></div></div><div id="38996644" class="c"><input type="checkbox" id="c-38996644" checked=""/><div class="controls bullet"><span class="by">bri3d</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995967">parent</a><span>|</span><a href="#38996260">prev</a><span>|</span><a href="#38998522">next</a><span>|</span><label class="collapse" for="c-38996644">[-]</label><label class="expand" for="c-38996644">[1 more]</label></div><br/><div class="children"><div class="content">This also led to a major vulnerability caused by different double-fault behavior between AMD and Intel known as “the visor bug”: <a href="https:&#x2F;&#x2F;xboxdevwiki.net&#x2F;17_Mistakes_Microsoft_Made_in_the_Xbox_Security_System#The_Visor_Backdoor" rel="nofollow">https:&#x2F;&#x2F;xboxdevwiki.net&#x2F;17_Mistakes_Microsoft_Made_in_the_Xb...</a></div><br/></div></div></div></div><div id="38998522" class="c"><input type="checkbox" id="c-38998522" checked=""/><div class="controls bullet"><span class="by">ffgjgf1</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995699">parent</a><span>|</span><a href="#38995967">prev</a><span>|</span><a href="#38996462">next</a><span>|</span><label class="collapse" for="c-38998522">[-]</label><label class="expand" for="c-38998522">[1 more]</label></div><br/><div class="children"><div class="content">Also I guess AMD is fine with having much lower margins compared to Nvidia which makes them very competitive in this marker?</div><br/></div></div><div id="38996462" class="c"><input type="checkbox" id="c-38996462" checked=""/><div class="controls bullet"><span class="by">coretx</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995699">parent</a><span>|</span><a href="#38998522">prev</a><span>|</span><a href="#38996228">next</a><span>|</span><label class="collapse" for="c-38996462">[-]</label><label class="expand" for="c-38996462">[5 more]</label></div><br/><div class="children"><div class="content">The playstation 3 was the last &quot;console&quot;. Everything that came after was general purpose computing crap.</div><br/><div id="38998475" class="c"><input type="checkbox" id="c-38998475" checked=""/><div class="controls bullet"><span class="by">erik</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996462">parent</a><span>|</span><a href="#38996793">next</a><span>|</span><label class="collapse" for="c-38998475">[-]</label><label class="expand" for="c-38998475">[1 more]</label></div><br/><div class="children"><div class="content">The current and previous generations of xbox and playstation do use x86 CPUs with integrated AMD GPUs.  But they aren&#x27;t just PCs in a small box.  Using a unified pool of GDDR memory* is a substantial architectural difference.<p>*except for the xbox one &amp; one s, which had its own weird setup with unified DDR3 and a programmer controlled ESRAM cache.</div><br/></div></div><div id="38996793" class="c"><input type="checkbox" id="c-38996793" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996462">parent</a><span>|</span><a href="#38998475">prev</a><span>|</span><a href="#38996840">next</a><span>|</span><label class="collapse" for="c-38996793">[-]</label><label class="expand" for="c-38996793">[2 more]</label></div><br/><div class="children"><div class="content">The PS3 could do general purpose computing. Its built in operating system let you play games, watch movies, play music, browse the web, etc. At the beginning of the console&#x27;s life you could even install Linux on it.<p>The hardware of consoles has been general purpose for decades.</div><br/><div id="38998180" class="c"><input type="checkbox" id="c-38998180" checked=""/><div class="controls bullet"><span class="by">rpmisms</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996793">parent</a><span>|</span><a href="#38996840">next</a><span>|</span><label class="collapse" for="c-38998180">[-]</label><label class="expand" for="c-38998180">[1 more]</label></div><br/><div class="children"><div class="content">The Pentagon even built a supercomputer using PS3s as compute cores.</div><br/></div></div></div></div><div id="38996840" class="c"><input type="checkbox" id="c-38996840" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996462">parent</a><span>|</span><a href="#38996793">prev</a><span>|</span><a href="#38996228">next</a><span>|</span><label class="collapse" for="c-38996840">[-]</label><label class="expand" for="c-38996840">[1 more]</label></div><br/><div class="children"><div class="content">Gaming <i>is</i> general purpose computing, or at least a part of it.</div><br/></div></div></div></div><div id="38996228" class="c"><input type="checkbox" id="c-38996228" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995699">parent</a><span>|</span><a href="#38996462">prev</a><span>|</span><a href="#38997201">next</a><span>|</span><label class="collapse" for="c-38996228">[-]</label><label class="expand" for="c-38996228">[7 more]</label></div><br/><div class="children"><div class="content">Also, AMD is the obvious choice for Valve over something like Nvidia due to AMD  having a decent upstream Linux support for both CPU and GPU features. It&#x27;s something Nvidia never cared about.</div><br/><div id="38997219" class="c"><input type="checkbox" id="c-38997219" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996228">parent</a><span>|</span><a href="#38997201">next</a><span>|</span><label class="collapse" for="c-38997219">[-]</label><label class="expand" for="c-38997219">[6 more]</label></div><br/><div class="children"><div class="content">NVidia has had more reliable and, at least until recently, more featureful drivers on Linux and FreeBSD for decades. They&#x27;re just not open-source, which doesn&#x27;t seem like it would matter for something like the Steam Deck.</div><br/><div id="38998374" class="c"><input type="checkbox" id="c-38998374" checked=""/><div class="controls bullet"><span class="by">tonyhart7</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997219">parent</a><span>|</span><a href="#38997295">next</a><span>|</span><label class="collapse" for="c-38998374">[-]</label><label class="expand" for="c-38998374">[1 more]</label></div><br/><div class="children"><div class="content">I mean Nvidia goes All out in AI&#x2F;ML Spaces, they even rebranded their company lol<p>I doubt they care to work with valve to release steam deck as much as high end compute market</div><br/></div></div><div id="38997295" class="c"><input type="checkbox" id="c-38997295" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997219">parent</a><span>|</span><a href="#38998374">prev</a><span>|</span><a href="#38997201">next</a><span>|</span><label class="collapse" for="c-38997295">[-]</label><label class="expand" for="c-38997295">[4 more]</label></div><br/><div class="children"><div class="content">Reliablie is very moot when they simply support only what they care about and don&#x27;t support the rest for those decades. Not being upstreamed and not using standard kernel interfaces makes it only worse.<p>So it&#x27;s not something Valve wanted to deal with. They commented on benefits of working with upstream GPU drivers, so it clearly matters.</div><br/><div id="38997445" class="c"><input type="checkbox" id="c-38997445" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997295">parent</a><span>|</span><a href="#38997201">next</a><span>|</span><label class="collapse" for="c-38997445">[-]</label><label class="expand" for="c-38997445">[3 more]</label></div><br/><div class="children"><div class="content">NVidia <i>are</i> the upstream for their drivers, and for a big client like Valve they would likely be willing to support the interfaces they need (and&#x2F;or work with them to get them using the interfaces NVidia like). Being less coupled to the Linux kernel could go either way - yes they don&#x27;t tend to support the most cutting-edge Linux features, but by the same token it&#x27;s easier to get newer NVidia drivers running on old versions of Linux than it is with AMD.<p>(Does Valve keep the Steam Deck on a rolling&#x2F;current Linux kernel? I&#x27;m honestly surprised if they do, because that seems like a lot of work and compatibility risk for minimal benefit)</div><br/><div id="38997537" class="c"><input type="checkbox" id="c-38997537" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997445">parent</a><span>|</span><a href="#38997556">next</a><span>|</span><label class="collapse" for="c-38997537">[-]</label><label class="expand" for="c-38997537">[1 more]</label></div><br/><div class="children"><div class="content">The current SteamOS (what Decks run) is based on Arch but is not rolling. The original for Steam Boxes was based on Debian.<p>Reasoning for the switch: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SteamOS#Development" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SteamOS#Development</a><p>&gt; The decision to move from Debian to Arch Linux was based on the different update schedule for these distributions. Debian, geared for server configurations, has its core software update in one large release, with intermediate patches for known bugs and security fixes, while Arch uses a rolling update approach for all parts. Valve found that using Arch&#x27;s rolling updates as a base would be better suited for the Steam Deck, allowing them to address issues and fixes much faster than Debian would allow. SteamOS itself is not rolling release.</div><br/></div></div><div id="38997556" class="c"><input type="checkbox" id="c-38997556" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997445">parent</a><span>|</span><a href="#38997537">prev</a><span>|</span><a href="#38997201">next</a><span>|</span><label class="collapse" for="c-38997556">[-]</label><label class="expand" for="c-38997556">[1 more]</label></div><br/><div class="children"><div class="content">Upstream is the kernel itself and standard kernel interfaces. Nvidia doing their own (non upstream) thing <i>is</i> the main problem here. They didn&#x27;t work with libdrm for years.<p>Being a client or even a partner doesn&#x27;t guarantee good cooperation with Nvidia (Evga has a lot to comment on that). As long as Nvidia is not a good citizen in working with upstream Linux kernel, it&#x27;s just not worth investing effort in using them for someone like Valve.<p>Stuff like HDR support or anything the like are major examples why it all matters.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38997201" class="c"><input type="checkbox" id="c-38997201" checked=""/><div class="controls bullet"><span class="by">yarg</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995699">parent</a><span>|</span><a href="#38996228">prev</a><span>|</span><a href="#38995676">next</a><span>|</span><label class="collapse" for="c-38997201">[-]</label><label class="expand" for="c-38997201">[2 more]</label></div><br/><div class="children"><div class="content">The CPU really only needs to be adequate - Nvidia can pull that off well enough for Nintendo at least.<p>Intel is in a far worse position - because they cannot do midrange graphics in an acceptable power&#x2F;thermal range.</div><br/><div id="38998560" class="c"><input type="checkbox" id="c-38998560" checked=""/><div class="controls bullet"><span class="by">ffgjgf1</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997201">parent</a><span>|</span><a href="#38995676">next</a><span>|</span><label class="collapse" for="c-38998560">[-]</label><label class="expand" for="c-38998560">[1 more]</label></div><br/><div class="children"><div class="content">Aren’t the midrange GPUs considered to be pretty decent price&#x2F;performance wise? IIRC they significantly improved their drivers over the last few years</div><br/></div></div></div></div></div></div><div id="38995676" class="c"><input type="checkbox" id="c-38995676" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38995699">prev</a><span>|</span><a href="#38995903">next</a><span>|</span><label class="collapse" for="c-38995676">[-]</label><label class="expand" for="c-38995676">[21 more]</label></div><br/><div class="children"><div class="content">Last years Xbox leak revealed that Microsoft was at least considering switching to ARM for the next Xbox generation, though regardless of the CPU architecture they chose they indicated they were sticking with AMD for the GPU. It will be interesting to see how that shakes out, going to ARM would complicate backwards compatibility so they would need a very good reason to move.</div><br/><div id="38995713" class="c"><input type="checkbox" id="c-38995713" checked=""/><div class="controls bullet"><span class="by">hypercube33</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995676">parent</a><span>|</span><a href="#38996105">next</a><span>|</span><label class="collapse" for="c-38995713">[-]</label><label class="expand" for="c-38995713">[10 more]</label></div><br/><div class="children"><div class="content">AMD holds an arm license and uses them as part of the software tpu in the pro series processors as I understand. It&#x27;s not unreasonable to assume they could make some ARM x86 hybrid CPU (like apple did for Rosetta) or a mixed arch chip we&#x27;ve never seen before that can run emulators native. Who knows.</div><br/><div id="38998690" class="c"><input type="checkbox" id="c-38998690" checked=""/><div class="controls bullet"><span class="by">mkopec</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995713">parent</a><span>|</span><a href="#38995761">next</a><span>|</span><label class="collapse" for="c-38998690">[-]</label><label class="expand" for="c-38998690">[1 more]</label></div><br/><div class="children"><div class="content">All Zen 1 CPUs and newer have the PSP &#x2F; ASP security processor which is ARM based and runs before the x86 cores are released from reset. This applies to all Zen models, not just the PRO versions.<p>The fTPM does indeed run on the PSP, so on the ARM cores, among many other things.</div><br/></div></div><div id="38995761" class="c"><input type="checkbox" id="c-38995761" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995713">parent</a><span>|</span><a href="#38998690">prev</a><span>|</span><a href="#38996299">next</a><span>|</span><label class="collapse" for="c-38995761">[-]</label><label class="expand" for="c-38995761">[3 more]</label></div><br/><div class="children"><div class="content">But what would be the point of that? If you&#x27;re already buying it from AMD and the previous generation was x86 so that&#x27;s what gives backwards compatibility, just do another x86 one. The reason to switch to ARM is to get it from someone other than Intel or AMD.</div><br/><div id="38997327" class="c"><input type="checkbox" id="c-38997327" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995761">parent</a><span>|</span><a href="#38996299">next</a><span>|</span><label class="collapse" for="c-38997327">[-]</label><label class="expand" for="c-38997327">[2 more]</label></div><br/><div class="children"><div class="content">The reason to switch to ARM is to get better performance, especially per-watt. If the supplier that&#x27;s making your graphics card can deliver that, then why risk onboarding someone new?</div><br/><div id="38997553" class="c"><input type="checkbox" id="c-38997553" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997327">parent</a><span>|</span><a href="#38996299">next</a><span>|</span><label class="collapse" for="c-38997553">[-]</label><label class="expand" for="c-38997553">[1 more]</label></div><br/><div class="children"><div class="content">In addition to better perf per watt, it’d allow them to shrink the console, potentially to something as small as a Mac Mini or NUC. Less need for cooling means a smaller console.<p>This could help them make inroads to people who might not have considered a home console before due to their relatively large size. Wouldn’t be surprised if the Switch did well with this market and now MS wants a slice of that pie.</div><br/></div></div></div></div></div></div><div id="38996299" class="c"><input type="checkbox" id="c-38996299" checked=""/><div class="controls bullet"><span class="by">bradfa</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995713">parent</a><span>|</span><a href="#38995761">prev</a><span>|</span><a href="#38995881">next</a><span>|</span><label class="collapse" for="c-38996299">[-]</label><label class="expand" for="c-38996299">[1 more]</label></div><br/><div class="children"><div class="content">AMD has previously made a variety of ARM CPUs, such as the recent Opteron “Seattle” parts.<p><a href="https:&#x2F;&#x2F;en.wikichip.org&#x2F;wiki&#x2F;amd&#x2F;cores&#x2F;seattle" rel="nofollow">https:&#x2F;&#x2F;en.wikichip.org&#x2F;wiki&#x2F;amd&#x2F;cores&#x2F;seattle</a></div><br/></div></div><div id="38995881" class="c"><input type="checkbox" id="c-38995881" checked=""/><div class="controls bullet"><span class="by">mratsim</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995713">parent</a><span>|</span><a href="#38996299">prev</a><span>|</span><a href="#38997053">next</a><span>|</span><label class="collapse" for="c-38995881">[-]</label><label class="expand" for="c-38995881">[3 more]</label></div><br/><div class="children"><div class="content">The stuff on the PRO is a Xilinx FPGA, or they have both an ARM Processor and an FPGA?</div><br/><div id="38995993" class="c"><input type="checkbox" id="c-38995993" checked=""/><div class="controls bullet"><span class="by">p_l</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995881">parent</a><span>|</span><a href="#38995915">next</a><span>|</span><label class="collapse" for="c-38995993">[-]</label><label class="expand" for="c-38995993">[1 more]</label></div><br/><div class="children"><div class="content">The NPUs are effectively hard IP blocks on xilinx FPGA fabric (i.e. there are close to no LUTs there - the chip takes in Xilinx bitstream but the only available resources are hard IP blocks)<p>There&#x27;s also Xtensa cores (audio coprocessor) and ARM cores included (PSP, Pluton, and I think there&#x27;s extra ARM coprocessor for handling some sensor stuff optionally)</div><br/></div></div></div></div></div></div><div id="38996105" class="c"><input type="checkbox" id="c-38996105" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995676">parent</a><span>|</span><a href="#38995713">prev</a><span>|</span><a href="#38996080">next</a><span>|</span><label class="collapse" for="c-38996105">[-]</label><label class="expand" for="c-38996105">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It will be interesting to see how that shakes out, going to ARM would complicate backwards compatibility so they would need a very good reason to move.<p>Sticking with AMD for GPUs makes sense no matter the CPU architecture. AMD is competitive on x86, and at the moment Samsung is working out the kinks to integrate Radeon GPUs with their ARM CPUs... so once Samsung has proven the concept works (and, of course, paid for the effort), maybe large consoles will make the switch.<p>It shouldn&#x27;t be much of a problem for game studios in the end, most games run on one of the household-name engines anyway and they have supported ARM for many years for mobile games.</div><br/></div></div><div id="38996080" class="c"><input type="checkbox" id="c-38996080" checked=""/><div class="controls bullet"><span class="by">dataangel</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995676">parent</a><span>|</span><a href="#38996105">prev</a><span>|</span><a href="#38995903">next</a><span>|</span><label class="collapse" for="c-38996080">[-]</label><label class="expand" for="c-38996080">[9 more]</label></div><br/><div class="children"><div class="content">Apple recently demonstrated ARM is pretty capable of emulating x86</div><br/><div id="38996109" class="c"><input type="checkbox" id="c-38996109" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996080">parent</a><span>|</span><a href="#38997400">next</a><span>|</span><label class="collapse" for="c-38996109">[-]</label><label class="expand" for="c-38996109">[6 more]</label></div><br/><div class="children"><div class="content">They did, but to get it running as well as they did they had to deviate from generic ARM by adding support for the x86 memory model <i>in hardware.</i> Apple was in a position to do that since they design their own ARM cores anyway, but the off-the-shelf ARM reference designs that most other players are relying on don&#x27;t have that capability.</div><br/><div id="38996330" class="c"><input type="checkbox" id="c-38996330" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996109">parent</a><span>|</span><a href="#38997400">next</a><span>|</span><label class="collapse" for="c-38996330">[-]</label><label class="expand" for="c-38996330">[5 more]</label></div><br/><div class="children"><div class="content">You can do it with standard ARM instructions these days.</div><br/><div id="38998284" class="c"><input type="checkbox" id="c-38998284" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996330">parent</a><span>|</span><a href="#38997089">next</a><span>|</span><label class="collapse" for="c-38998284">[-]</label><label class="expand" for="c-38998284">[2 more]</label></div><br/><div class="children"><div class="content">&gt; these days<p>Which instructions?</div><br/><div id="38998518" class="c"><input type="checkbox" id="c-38998518" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38998284">parent</a><span>|</span><a href="#38997089">next</a><span>|</span><label class="collapse" for="c-38998518">[-]</label><label class="expand" for="c-38998518">[1 more]</label></div><br/><div class="children"><div class="content">Those that are part of FEAT_LRCPC.</div><br/></div></div></div></div><div id="38997089" class="c"><input type="checkbox" id="c-38997089" checked=""/><div class="controls bullet"><span class="by">_kbh_</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996330">parent</a><span>|</span><a href="#38998284">prev</a><span>|</span><a href="#38997400">next</a><span>|</span><label class="collapse" for="c-38997089">[-]</label><label class="expand" for="c-38997089">[2 more]</label></div><br/><div class="children"><div class="content">You could always do it with instructions itd just be slow no?.</div><br/><div id="38998500" class="c"><input type="checkbox" id="c-38998500" checked=""/><div class="controls bullet"><span class="by">erik</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997089">parent</a><span>|</span><a href="#38997400">next</a><span>|</span><label class="collapse" for="c-38998500">[-]</label><label class="expand" for="c-38998500">[1 more]</label></div><br/><div class="children"><div class="content">I vouched for this comment, but it looks like you&#x27;re shadow banned.  You might want to email support.</div><br/></div></div></div></div></div></div></div></div><div id="38997400" class="c"><input type="checkbox" id="c-38997400" checked=""/><div class="controls bullet"><span class="by">Narishma</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996080">parent</a><span>|</span><a href="#38996109">prev</a><span>|</span><a href="#38995903">next</a><span>|</span><label class="collapse" for="c-38997400">[-]</label><label class="expand" for="c-38997400">[2 more]</label></div><br/><div class="children"><div class="content">They haven&#x27;t demonstrated it at console-level prices.</div><br/><div id="38998717" class="c"><input type="checkbox" id="c-38998717" checked=""/><div class="controls bullet"><span class="by">klausa</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997400">parent</a><span>|</span><a href="#38995903">next</a><span>|</span><label class="collapse" for="c-38998717">[-]</label><label class="expand" for="c-38998717">[1 more]</label></div><br/><div class="children"><div class="content">If you wanna play semantics, and argue that the demonstration requires a M-Something chip, then they have in a $599 iPad Air with M1. Or I guess a Mac Mini with M2 for the same price.<p>If you want to be more lenient, the $129 Apple TV has A15 which is ~same design, but with less cores.</div><br/></div></div></div></div></div></div></div></div><div id="38995903" class="c"><input type="checkbox" id="c-38995903" checked=""/><div class="controls bullet"><span class="by">chillfox</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38995676">prev</a><span>|</span><a href="#38995633">next</a><span>|</span><label class="collapse" for="c-38995903">[-]</label><label class="expand" for="c-38995903">[10 more]</label></div><br/><div class="children"><div class="content">Supposedly a big reason is that Nvidia is difficult to work with.</div><br/><div id="38996267" class="c"><input type="checkbox" id="c-38996267" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995903">parent</a><span>|</span><a href="#38995633">next</a><span>|</span><label class="collapse" for="c-38996267">[-]</label><label class="expand" for="c-38996267">[9 more]</label></div><br/><div class="children"><div class="content">Is there any info to substantiate that?</div><br/><div id="38996946" class="c"><input type="checkbox" id="c-38996946" checked=""/><div class="controls bullet"><span class="by">SpecialistK</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996267">parent</a><span>|</span><a href="#38996657">next</a><span>|</span><label class="collapse" for="c-38996946">[-]</label><label class="expand" for="c-38996946">[2 more]</label></div><br/><div class="children"><div class="content">There are a few examples &#x2F; anectodes:<p>The first is MS&#x27; trouble with the original Xbox (<a href="https:&#x2F;&#x2F;www.gamesindustry.biz&#x2F;ati-to-provide-chips-for-future-xbox-products" rel="nofollow">https:&#x2F;&#x2F;www.gamesindustry.biz&#x2F;ati-to-provide-chips-for-futur...</a> - not a great example (20+ year old articles are hard to find) but mentions the issues MS had with Nvidia)<p>Then there&#x27;s Apple&#x27;s drama, which involved warranty claims for laptop parts that led to them being AMD only until the Arm move (<a href="https:&#x2F;&#x2F;blog.greggant.com&#x2F;posts&#x2F;2021&#x2F;10&#x2F;13&#x2F;apple-vs-nvidia-what-happened.html" rel="nofollow">https:&#x2F;&#x2F;blog.greggant.com&#x2F;posts&#x2F;2021&#x2F;10&#x2F;13&#x2F;apple-vs-nvidia-w...</a>)<p>Sony only went with Nvidia for the PS3, but that may be more about AMD&#x27;s APU offerings than Nvidia&#x27;s shortcomings.<p>Whether these are signs of a trend or just public anecdotes is in the eye of the beholder or kept away in boardrooms.</div><br/><div id="38997579" class="c"><input type="checkbox" id="c-38997579" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996946">parent</a><span>|</span><a href="#38996657">next</a><span>|</span><label class="collapse" for="c-38997579">[-]</label><label class="expand" for="c-38997579">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Then there&#x27;s Apple&#x27;s drama, which involved warranty claims for laptop parts that led to them being AMD only until the Arm move (<a href="https:&#x2F;&#x2F;blog.greggant.com&#x2F;posts&#x2F;2021&#x2F;10&#x2F;13&#x2F;apple-vs-nvidia-w" rel="nofollow">https:&#x2F;&#x2F;blog.greggant.com&#x2F;posts&#x2F;2021&#x2F;10&#x2F;13&#x2F;apple-vs-nvidia-w</a>...)<p>It’s second-hand info so take it with a grain of salt, but I read somewhere that there was a lot of friction between Apple and Nvidia because Apple likes to tweak and tailor drivers per model of Mac and generally not be wholly dependent on third parties for driver changes, but that requires driver source access which Nvidia didn’t like (even though they agreed to it for quite some time — drivers for a range of Nvidia cards shipped with OS X for many years and those were all Apple-tweaked).</div><br/></div></div></div></div><div id="38996657" class="c"><input type="checkbox" id="c-38996657" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996267">parent</a><span>|</span><a href="#38996946">prev</a><span>|</span><a href="#38996314">next</a><span>|</span><label class="collapse" for="c-38996657">[-]</label><label class="expand" for="c-38996657">[4 more]</label></div><br/><div class="children"><div class="content">Well EVGA withdrew from making Nvidia GPUs despite being one of the best board partners due to how unreasonable Nvidia was.</div><br/><div id="38996692" class="c"><input type="checkbox" id="c-38996692" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996657">parent</a><span>|</span><a href="#38996314">next</a><span>|</span><label class="collapse" for="c-38996692">[-]</label><label class="expand" for="c-38996692">[3 more]</label></div><br/><div class="children"><div class="content">The question is... why EVGA only?</div><br/><div id="38996830" class="c"><input type="checkbox" id="c-38996830" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996692">parent</a><span>|</span><a href="#38996314">next</a><span>|</span><label class="collapse" for="c-38996830">[-]</label><label class="expand" for="c-38996830">[2 more]</label></div><br/><div class="children"><div class="content">...and why did EVGA withdraw from the GPU market altogether rather than pivoting to making AMD&#x2F;Intel cards, if Nvidia was truly the problem?</div><br/><div id="38997605" class="c"><input type="checkbox" id="c-38997605" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996830">parent</a><span>|</span><a href="#38996314">next</a><span>|</span><label class="collapse" for="c-38997605">[-]</label><label class="expand" for="c-38997605">[1 more]</label></div><br/><div class="children"><div class="content">I think EVGA’s withdrawal had to do with how impossible it was to compete with Nvidia’s first-party cards with the terms Nvidia was setting for AIBs. Other card makers like Asus have several other flagship product lines to be able to sustain the hit while EVGA’s other products consisted of lower-profit accessories.<p>They may have seen AMD selling their own first-party cards and anticipated AMD eventually following Nvidia’s footsteps. As for Intel, at that point they were probably seen as too much of a gamble to invest in (and probably still are, to a lesser extent).</div><br/></div></div></div></div></div></div></div></div><div id="38996314" class="c"><input type="checkbox" id="c-38996314" checked=""/><div class="controls bullet"><span class="by">B1FF_PSUVM</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996267">parent</a><span>|</span><a href="#38996657">prev</a><span>|</span><a href="#38995633">next</a><span>|</span><label class="collapse" for="c-38996314">[-]</label><label class="expand" for="c-38996314">[2 more]</label></div><br/><div class="children"><div class="content">Linus Torvalds said a couple of words about it ... <a href="https:&#x2F;&#x2F;www.google.com&#x2F;search?q=linus+nvidia" rel="nofollow">https:&#x2F;&#x2F;www.google.com&#x2F;search?q=linus+nvidia</a></div><br/><div id="38996632" class="c"><input type="checkbox" id="c-38996632" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996314">parent</a><span>|</span><a href="#38995633">next</a><span>|</span><label class="collapse" for="c-38996632">[-]</label><label class="expand" for="c-38996632">[1 more]</label></div><br/><div class="children"><div class="content">The friction between Linux wanting all drivers to be open source and Nvidia not wanting to open source their drivers isn&#x27;t really relevant to any other platform besides Linux. Console manufacturers have no reason to care that Nvidia&#x27;s drivers aren&#x27;t open source, they can get documentation and&#x2F;or source code under NDA if they choose to partner with Nvidia. Secrecy comes with the territory.</div><br/></div></div></div></div></div></div></div></div><div id="38995633" class="c"><input type="checkbox" id="c-38995633" checked=""/><div class="controls bullet"><span class="by">zitterbewegung</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38995903">prev</a><span>|</span><a href="#38996190">next</a><span>|</span><label class="collapse" for="c-38995633">[-]</label><label class="expand" for="c-38995633">[3 more]</label></div><br/><div class="children"><div class="content">From my understanding AMD holds the console market because its basically lowest price wins so the margins are business that Intel and NVIDIA doesn&#x27;t really want.</div><br/><div id="38995715" class="c"><input type="checkbox" id="c-38995715" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995633">parent</a><span>|</span><a href="#38996190">next</a><span>|</span><label class="collapse" for="c-38995715">[-]</label><label class="expand" for="c-38995715">[2 more]</label></div><br/><div class="children"><div class="content">They also more or less won the PS4&#x2F;XB1 generation by default as the only supplier capable of combining a half decent CPU and GPU into a single chip - Intel had good CPUs but terrible GPUs and Nvidia had good GPUs but terrible CPUs. That tide is shifting now with Intel&#x27;s renewed push into GPUs, and Nvidia getting access to high performance ARM reference designs.</div><br/><div id="38996412" class="c"><input type="checkbox" id="c-38996412" checked=""/><div class="controls bullet"><span class="by">PlutoIsAPlanet</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995715">parent</a><span>|</span><a href="#38996190">next</a><span>|</span><label class="collapse" for="c-38996412">[-]</label><label class="expand" for="c-38996412">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also interesting that at one point, Nvidia&#x27;s chips were going to support both x86 and ARM (see Project Denver), but x86 support was scrapped due to Intel&#x27;s patents.<p><a href="https:&#x2F;&#x2F;semiaccurate.com&#x2F;2011&#x2F;08&#x2F;05&#x2F;what-is-project-denver-based-on&#x2F;" rel="nofollow">https:&#x2F;&#x2F;semiaccurate.com&#x2F;2011&#x2F;08&#x2F;05&#x2F;what-is-project-denver-b...</a><p>Would of been interesting to have had another large x86 player</div><br/></div></div></div></div></div></div><div id="38996190" class="c"><input type="checkbox" id="c-38996190" checked=""/><div class="controls bullet"><span class="by">skhr0680</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38995633">prev</a><span>|</span><a href="#38996467">next</a><span>|</span><label class="collapse" for="c-38996190">[-]</label><label class="expand" for="c-38996190">[2 more]</label></div><br/><div class="children"><div class="content">ATi made the GameCube’s GPU if that counts</div><br/><div id="38997221" class="c"><input type="checkbox" id="c-38997221" checked=""/><div class="controls bullet"><span class="by">octotoad</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996190">parent</a><span>|</span><a href="#38996467">next</a><span>|</span><label class="collapse" for="c-38997221">[-]</label><label class="expand" for="c-38997221">[1 more]</label></div><br/><div class="children"><div class="content">Close. They bought the company that made it.</div><br/></div></div></div></div><div id="38996467" class="c"><input type="checkbox" id="c-38996467" checked=""/><div class="controls bullet"><span class="by">avery17</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38996190">prev</a><span>|</span><a href="#38995878">next</a><span>|</span><label class="collapse" for="c-38996467">[-]</label><label class="expand" for="c-38996467">[1 more]</label></div><br/><div class="children"><div class="content">Msi is making an intel based handheld called the claw. Its pretty sweet.</div><br/></div></div><div id="38995878" class="c"><input type="checkbox" id="c-38995878" checked=""/><div class="controls bullet"><span class="by">Fire-Dragon-DoL</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38996467">prev</a><span>|</span><a href="#38996081">next</a><span>|</span><label class="collapse" for="c-38995878">[-]</label><label class="expand" for="c-38995878">[6 more]</label></div><br/><div class="children"><div class="content">There is a new one steam-deck-like that&#x27;s intel, but I don&#x27;t remember the brand</div><br/><div id="38995884" class="c"><input type="checkbox" id="c-38995884" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995878">parent</a><span>|</span><a href="#38996081">next</a><span>|</span><label class="collapse" for="c-38995884">[-]</label><label class="expand" for="c-38995884">[5 more]</label></div><br/><div class="children"><div class="content">MSI. But expect very bad power consumption</div><br/><div id="38995987" class="c"><input type="checkbox" id="c-38995987" checked=""/><div class="controls bullet"><span class="by">Fire-Dragon-DoL</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995884">parent</a><span>|</span><a href="#38996473">next</a><span>|</span><label class="collapse" for="c-38995987">[-]</label><label class="expand" for="c-38995987">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, but I would be interested in the performance. I use my steam deck always connected to a power outlet (I have multiple connected around the home), so power consumption has never been a problem, the most I use it in handheld mode is on the subway for a total of about 1 hour&#x2F;1 hour and a half (this is the total time spent for a roundtrip)</div><br/></div></div><div id="38996473" class="c"><input type="checkbox" id="c-38996473" checked=""/><div class="controls bullet"><span class="by">avery17</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38995884">parent</a><span>|</span><a href="#38995987">prev</a><span>|</span><a href="#38996081">next</a><span>|</span><label class="collapse" for="c-38996473">[-]</label><label class="expand" for="c-38996473">[3 more]</label></div><br/><div class="children"><div class="content">They all have very bad power consumption lol</div><br/><div id="38996697" class="c"><input type="checkbox" id="c-38996697" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996473">parent</a><span>|</span><a href="#38996081">next</a><span>|</span><label class="collapse" for="c-38996697">[-]</label><label class="expand" for="c-38996697">[2 more]</label></div><br/><div class="children"><div class="content">Apart from the Steam Deck, that is.</div><br/><div id="38997626" class="c"><input type="checkbox" id="c-38997626" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996697">parent</a><span>|</span><a href="#38996081">next</a><span>|</span><label class="collapse" for="c-38997626">[-]</label><label class="expand" for="c-38997626">[1 more]</label></div><br/><div class="children"><div class="content">The OLED Deck in particular is great. On less demanding games or streaming with Moonlight where I can crank down the TDP it’s not hard to squeeze 10+ hours of playtime out of its 50Wh battery.<p>Linux probably helps here, with its greatly reduced background activity compared to Windows.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38996081" class="c"><input type="checkbox" id="c-38996081" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38995878">prev</a><span>|</span><a href="#38996301">next</a><span>|</span><label class="collapse" for="c-38996081">[-]</label><label class="expand" for="c-38996081">[7 more]</label></div><br/><div class="children"><div class="content">&gt; It’s kind of interesting how they have this hold on gaming outside of conventional PCs, but can’t seem to compete with nvidia on just that one market.<p>As for PC gaming, word of mouth plus a <i>huge</i> chunk of money for advertising deals. AMD, or back then rather ATI, drivers were always known for being more rough around the edges and unstable, but the cards were cheaper. Basically you get what you pay for. On the CPU side, it&#x27;s just the same but without the driver stuff... until AMD turned the tide with Zen and managed to kick Intel&#x27;s arse so hard they haven&#x27;t recovered until today.<p>The console market is a different beast, here the the show isn&#x27;t run by kids who have had NVIDIA sponsorships in games since they were first playing Unreal Tournament 2004 but by professional beancounters who only look at the price. For them, the answer is clear:<p>- generally, the studios prefer something that has some sort of market adoption because good luck finding someone skilled enough to work on PowerPC or weird-ass custom GPU architecture. So the console makers want something that their studios can get started on quickly without wasting too much time porting their exclusive titles.<p>- on the CPU side there&#x27;s only two major architectures left that fulfill this requirement, ARM and x86, and the only one pulling actual console-worthy high performance out of ARM is Apple who doesn&#x27;t license their stuff to anyone. That means x86, and in there there&#x27;s again just two players in town, and Intel can&#x27;t compete on price, performance or efficiency =&gt; off to AMD they go.<p>- on the GPU side it&#x27;s the same, it&#x27;s either AMD and NVIDIA, and NVIDIA won&#x27;t go and use their fab time to churn out low-margin GPUs for consoles when they can use that fab time to make high-margin gamer GPUs and especially all the ludicrous-margin stuff for first coin miners and now AI hyper(scaler)s =&gt; off to AMD they go.<p>The exception of course is the Nintendo Switch. For Nintendo, it was obvious that it must be an ARM CPU core - x86 and performance under battery constraints Just Is Not A Thing and all the other mobile CPU archs have long since died out. Where I have zero idea is why they went for NVIDIA Tegra that was originally aimed at automotive and settop boxes instead of Qualcomm, Samsung or Mediatek but I guess that the former two demanded inacceptable terms (Qualcomm), didn&#x27;t want to sell low-margin SoCs when they could run their own high-performance SoCs for their Galaxy lineup (Samsung) or were too sketchy (Mediatek), so they went for Nvidia who could actually use a large deal to showcase &quot;we can also do mobile&quot; for a world that was dominated by the three before-mentioned giants.</div><br/><div id="38997370" class="c"><input type="checkbox" id="c-38997370" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996081">parent</a><span>|</span><a href="#38996269">next</a><span>|</span><label class="collapse" for="c-38997370">[-]</label><label class="expand" for="c-38997370">[1 more]</label></div><br/><div class="children"><div class="content">It also helps that console makers like Sony and MS are less concerned with driver quality since they&#x27;re exposing a much more barebones OS to purpose built games and game engines, with fewer bells and whistles and much more control by Sony&#x2F;MS over the software on the console.</div><br/></div></div><div id="38996269" class="c"><input type="checkbox" id="c-38996269" checked=""/><div class="controls bullet"><span class="by">gjsman-1000</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996081">parent</a><span>|</span><a href="#38997370">prev</a><span>|</span><a href="#38996205">next</a><span>|</span><label class="collapse" for="c-38996269">[-]</label><label class="expand" for="c-38996269">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Where I have zero idea is why they went for NVIDIA Tegra that was originally aimed at automotive and settop boxes instead of Qualcomm, Samsung or Mediatek<p>I think a big part of this comes down to two things:<p>1. If you’re Nintendo, the Tegra X1 was the fastest mobile graphics chip available. Mali and Adreno weren’t anywhere close at the time. The alternative would’ve required shipping a Switch less powerful than the already-derided Wii U, which just was not an option.<p>2. Nintendo uses their own, in-house operating system that fits into under 400MB and runs on a microkernel. Naturally, you want good GPU drivers. NVIDIA’s philosophy is to basically make a binary blob with a shim for each OS. Not great, but it demonstrably shows the drivers can be ported with fairly little effort - Windows, Mac, Linux, Android, whatever. Qualcomm and MediaTek’s strategy is to make a bastard fork of the Linux kernel and, maybe, upstream some stuff later, with a particular interest in Android compatibility. I think it goes without saying, that the implementation which isn’t tied to a specific kernel, is a more desirable starting point.</div><br/></div></div><div id="38996205" class="c"><input type="checkbox" id="c-38996205" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996081">parent</a><span>|</span><a href="#38996269">prev</a><span>|</span><a href="#38996301">next</a><span>|</span><label class="collapse" for="c-38996205">[-]</label><label class="expand" for="c-38996205">[4 more]</label></div><br/><div class="children"><div class="content">&gt;until AMD turned the tide with Zen and managed to kick Intel&#x27;s arse so hard they haven&#x27;t recovered until today.<p>Let&#x27;s not revise history. Zen was better than Bulldozer, but it still took until Zen2+ or Zen 3 (I don&#x27;t recall exactly) until they reached parity with Intel Core i.<p>Before that, the vocal crowd was buying AMD because they are the underdog (and still are, to a point) and were cheaper (no longer the case).</div><br/><div id="38998173" class="c"><input type="checkbox" id="c-38998173" checked=""/><div class="controls bullet"><span class="by">MindSpunk</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996205">parent</a><span>|</span><a href="#38996907">next</a><span>|</span><label class="collapse" for="c-38998173">[-]</label><label class="expand" for="c-38998173">[2 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re going to not revise history it&#x27;s probably important to not leave out the actual meat of what made AMD&#x27;s Zen processors so compelling: core count.<p>Zen 1 launched offering double the core count of any of Intel&#x27;s competing products at the same price. Intel was ahead on single core performance for a long time but in any well multi threaded benchmark or app Intel was getting absolutely demolished, with AMD offering twice the performance Intel was at any pricepoint. Intel failed to compete in multithreaded apps for 4 product generations, giving AMD enough time to close the single threaded performance gap too.<p>Now they are both pretty close performance wise, but AMD is well ahead from a power efficiency standing compared to Intel&#x27;s competing CPUs.</div><br/><div id="38998547" class="c"><input type="checkbox" id="c-38998547" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38998173">parent</a><span>|</span><a href="#38996907">next</a><span>|</span><label class="collapse" for="c-38998547">[-]</label><label class="expand" for="c-38998547">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Intel failed to compete in multithreaded apps for 4 product generations<p>Sure, and they got away with it because multi-thread workloads <i>aren&#x27;t relevant</i> for the vast majority of the population. They still aren&#x27;t today.<p>Most consumer computing workloads, including gaming by far, are dependent on single-thread performance. The vast majority of people do not spend their computing hours encoding video, compiling source code, or simulating proteins. They play games, surf Facebook, watch Youtube, read Mysterious Twitter X, and chat or call friends and family on LINE&#x2F;Discord&#x2F;Skype&#x2F;et al.<p>In case you are detached from reality, I ask you to realize most peoples&#x27; computing needs today can be satisfactorily satisfied with an Intel N100. That&#x27;s a two generations old 4 core, 4 thread CPU among the lowest tiers of consumer CPUs availabe.<p>Hell, I personally can satisfy all my daily computing needs with an Intel i7-2700K Sandy Bridge CPU without feeling hindered. I surmise most people will be satisfied with far less.<p>Another way to put it is: For all the core counts AMD Ryzen (and now Intel) brought, most people can&#x27;t actually make full use of them even today. That&#x27;s another reason why AMD Ryzen took so long to become a practical competitor to Intel Core i instead of a meme spread by the vocal minority.</div><br/></div></div></div></div><div id="38996907" class="c"><input type="checkbox" id="c-38996907" checked=""/><div class="controls bullet"><span class="by">kcb</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996205">parent</a><span>|</span><a href="#38998173">prev</a><span>|</span><a href="#38996301">next</a><span>|</span><label class="collapse" for="c-38996907">[-]</label><label class="expand" for="c-38996907">[1 more]</label></div><br/><div class="children"><div class="content">The big factor is before Zen it was dual-core low-end, quad-core high-end for Intel consumer chips on both the desktop and laptop. Advancement in core count and multi-threaded performance clearly was a direct result of Zen.</div><br/></div></div></div></div></div></div><div id="38996301" class="c"><input type="checkbox" id="c-38996301" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#38995463">parent</a><span>|</span><a href="#38996081">prev</a><span>|</span><a href="#38997016">next</a><span>|</span><label class="collapse" for="c-38996301">[-]</label><label class="expand" for="c-38996301">[3 more]</label></div><br/><div class="children"><div class="content">As already mentioned by others, AMD won the console market because back then the only thing they could compete on was price.<p>Which coincidentally, the most important thing to Sony and Microsoft with regards to consoles (&quot;make tons and sell tons&quot; products) is cost of materials. Even getting that cost 1 cent cheaper still means a $1 difference over 100 units, $10 over 1,000 units, $100 over 10,000 units, and onwards. Remember, we&#x27;re talking many millions of basically identical units sold.<p>AMD couldn&#x27;t compete in performance nor efficiency, but they could absolutely compete in price, while both Intel and Nvidia couldn&#x27;t due either to their business strategy or the logistics for Sony&#x2F;Microsoft of procuring more materials from different suppliers.<p>So long as AMD can continue to undercut Intel, Nvidia, and any other contenders they will continue to dominate consoles.</div><br/><div id="38997698" class="c"><input type="checkbox" id="c-38997698" checked=""/><div class="controls bullet"><span class="by">choudharism</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38996301">parent</a><span>|</span><a href="#38997016">next</a><span>|</span><label class="collapse" for="c-38997698">[-]</label><label class="expand" for="c-38997698">[2 more]</label></div><br/><div class="children"><div class="content">I generally agree with this reasoning, but your example could use some scaling down to convince a reader.<p>1 cent cheaper would net Sony a total of 500K USD for all PS5 units sold till date. So about a hundred PS5 units at retail as pure profit. A company of the size of Sony for a product of the scale of PS5 would absolutely forego that profit if the alternative offered any tangible benefits at all.</div><br/><div id="38998617" class="c"><input type="checkbox" id="c-38998617" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#38995463">root</a><span>|</span><a href="#38997698">parent</a><span>|</span><a href="#38997016">next</a><span>|</span><label class="collapse" for="c-38998617">[-]</label><label class="expand" for="c-38998617">[1 more]</label></div><br/><div class="children"><div class="content">&gt;if the alternative offered any tangible benefits at all.<p>That&#x27;s the thing though: A new generation console only needs to be better than it&#x27;s predecessor. It doesn&#x27;t <i>have</i> to have groundbreaking technologies or innovations, let alone be a pioneer paving the way forward for other computing hardware products.<p>So cost of materials remains the chief concern.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>