<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1738486854908" as="style"/><link rel="stylesheet" href="styles.css?v=1738486854908"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://rlhfbook.com/">RLHF Book</a>Â <span class="domain">(<a href="https://rlhfbook.com">rlhfbook.com</a>)</span></div><div class="subtext"><span>jxmorris12</span> | <span>19 comments</span></div><br/><div><div id="42905447" class="c"><input type="checkbox" id="c-42905447" checked=""/><div class="controls bullet"><span class="by">natolambert</span><span>|</span><a href="#42904259">next</a><span>|</span><label class="collapse" for="c-42905447">[-]</label><label class="expand" for="c-42905447">[2 more]</label></div><br/><div class="children"><div class="content">Author here! Just wanted to say that this is indeed in a good place to share, some very useful stuff, but is also very work in progress. I&#x27;m may 60% or so to my first draft. Said progress is coming every day and I happily welcome fixes or suggestions on GitHub.</div><br/><div id="42906718" class="c"><input type="checkbox" id="c-42906718" checked=""/><div class="controls bullet"><span class="by">pknerd</span><span>|</span><a href="#42905447">parent</a><span>|</span><a href="#42904259">next</a><span>|</span><label class="collapse" for="c-42906718">[-]</label><label class="expand" for="c-42906718">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. Is there a PDF version? I kind of feel difficulty switching links.</div><br/></div></div></div></div><div id="42904259" class="c"><input type="checkbox" id="c-42904259" checked=""/><div class="controls bullet"><span class="by">alexhutcheson</span><span>|</span><a href="#42905447">prev</a><span>|</span><a href="#42904160">next</a><span>|</span><label class="collapse" for="c-42904259">[-]</label><label class="expand" for="c-42904259">[8 more]</label></div><br/><div class="children"><div class="content">Glad to see the author making a serious effort to fill the gap in public documentation of RLHF theory and practice. The current state of the art seems to be primarily documented in arXiv papers, but each paper is more like a &quot;diff&quot; than a &quot;snapshot&quot; - you need to patch together the knowledge from many previous papers to understand the current state. It&#x27;s extremely valuable to &quot;snapshot&quot; the current state of the art in a way that is easy to reference.<p>My friendly feedback on this work-in-progress: I believe it could benefit from more introductory material to establish motivations and set expectations for what is achievable with RLHF. In particular, I think it would be useful to situate RLHF in comparison with supervised fine-tuning (SFT), which readers are likely familiar with.<p>Stuff I&#x27;d cover (from the background of an RLHF user but non-specialist):<p>Advantages of RLHF over SFT:<p>- Tunes on the full generation (which is what you ultimately care about), not just token-by-token.<p>- Can tune on problems where there are many acceptable answers (or ways to word the answer), and you don&#x27;t want to push the model into one specific series of tokens.<p>- Can incorporate negative feedback (e.g. don&#x27;t generate this).<p>Disadvantages of RLHF over SFT:<p>- Regularization (KL or otherwise) puts an upper bound on how much impact RLHF can have on the model. Because of this, RLHF is almost never enough to get you &quot;all the way there&quot; by itself.<p>- Very sensitive to reward model quality, which can be hard to evaluate.<p>- Much more resource and time intensive.<p>Non-obvious practical considerations:<p>- How to evaluate quality? If you have a good measurement of quality, it&#x27;s tempting to just incorporate it in your reward model. But you want to make sure you&#x27;re able to measure &quot;is this actually good for my final use-case&quot;, not just &quot;does this score well on my reward model?&quot;.<p>- How prompt engineering interacts with fine-tuning (both SFT and RLHF). Often some iteration on the system prompt will make fine-tuning converge faster, and with higher quality. Conversely, attempting to tune on examples that don&#x27;t include a task-specific prompt (surprisingly common) will often yield subpar results. This is a &quot;boring&quot; implementation detail that I don&#x27;t normally see included in papers.<p>Excited to see where this goes, and thanks to the author for willingness to share a work in progress!</div><br/><div id="42905703" class="c"><input type="checkbox" id="c-42905703" checked=""/><div class="controls bullet"><span class="by">gr3ml1n</span><span>|</span><a href="#42904259">parent</a><span>|</span><a href="#42904365">next</a><span>|</span><label class="collapse" for="c-42905703">[-]</label><label class="expand" for="c-42905703">[1 more]</label></div><br/><div class="children"><div class="content">SFT can be used to give negative feedback&#x2F;examples.  That&#x27;s one of the lesser-known benefits&#x2F;tricks of system messages.  E.g:<p><pre><code>  System: You are a helpful chatbot.
  User: What is 1+1?
  Assistant: 2.
</code></pre>
And<p><pre><code>  System: You are terrible at math.
  User: What is 1+1?
  Assistant: 0.</code></pre></div><br/></div></div><div id="42904365" class="c"><input type="checkbox" id="c-42904365" checked=""/><div class="controls bullet"><span class="by">kadushka</span><span>|</span><a href="#42904259">parent</a><span>|</span><a href="#42905703">prev</a><span>|</span><a href="#42904160">next</a><span>|</span><label class="collapse" for="c-42904365">[-]</label><label class="expand" for="c-42904365">[6 more]</label></div><br/><div class="children"><div class="content">Has r1 made RLHF obsolete?</div><br/><div id="42904497" class="c"><input type="checkbox" id="c-42904497" checked=""/><div class="controls bullet"><span class="by">alexhutcheson</span><span>|</span><a href="#42904259">root</a><span>|</span><a href="#42904365">parent</a><span>|</span><a href="#42905436">next</a><span>|</span><label class="collapse" for="c-42904497">[-]</label><label class="expand" for="c-42904497">[3 more]</label></div><br/><div class="children"><div class="content">DeepSeek-R1 had an RLHF step in their post-training pipeline (section 2.3.4 of their technical report[1]).<p>In addition, the &quot;reasoning-oriented reinforcement learning&quot; step (section 2.3.2) used an approach that is almost identical to RLHF in theory and implementation. The main difference is that they used a rule-based reward system, rather than a model trained on human preference data.<p>If you want to train a model like DeepSeek-R1, you&#x27;ll need to know the fundamentals of reinforcement learning on language models, including RLHF.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2501.12948" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2501.12948</a></div><br/><div id="42905552" class="c"><input type="checkbox" id="c-42905552" checked=""/><div class="controls bullet"><span class="by">bryan0</span><span>|</span><a href="#42904259">root</a><span>|</span><a href="#42904497">parent</a><span>|</span><a href="#42905436">next</a><span>|</span><label class="collapse" for="c-42905552">[-]</label><label class="expand" for="c-42905552">[2 more]</label></div><br/><div class="children"><div class="content">Yes but these were steps were not used in R1-zero where its reasoning capabilities were trained.</div><br/></div></div></div></div><div id="42905436" class="c"><input type="checkbox" id="c-42905436" checked=""/><div class="controls bullet"><span class="by">natolambert</span><span>|</span><a href="#42904259">root</a><span>|</span><a href="#42904365">parent</a><span>|</span><a href="#42904497">prev</a><span>|</span><a href="#42905686">next</a><span>|</span><label class="collapse" for="c-42905436">[-]</label><label class="expand" for="c-42905436">[1 more]</label></div><br/><div class="children"><div class="content">As the other commenter said, R1 required very standard RLHF techniques too.
But a fun way to think about it is that reasoning models are going to be bigger and uplift the RLHF boat.<p>But we need a few years to establish basics before I can write a cumulative RL for LLMs book ;)</div><br/></div></div><div id="42905686" class="c"><input type="checkbox" id="c-42905686" checked=""/><div class="controls bullet"><span class="by">gr3ml1n</span><span>|</span><a href="#42904259">root</a><span>|</span><a href="#42904365">parent</a><span>|</span><a href="#42905436">prev</a><span>|</span><a href="#42904160">next</a><span>|</span><label class="collapse" for="c-42905686">[-]</label><label class="expand" for="c-42905686">[1 more]</label></div><br/><div class="children"><div class="content">This feels like a category mistake.  Why would R1 make RLHF obsolete?</div><br/></div></div></div></div></div></div><div id="42904160" class="c"><input type="checkbox" id="c-42904160" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42904259">prev</a><span>|</span><a href="#42904584">next</a><span>|</span><label class="collapse" for="c-42904160">[-]</label><label class="expand" for="c-42904160">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Reinforcement learning from human feedback (RLHF)<p>In case anyone else didnât know the definition.<p>Knowing the definition it sounds kind of like âlearn what we tell you mattersâ in a sense.<p>Not unlike how the world seems to work today. High hopes for the futureâ¦</div><br/></div></div><div id="42904584" class="c"><input type="checkbox" id="c-42904584" checked=""/><div class="controls bullet"><span class="by">nejsjsjsbsb</span><span>|</span><a href="#42904160">prev</a><span>|</span><a href="#42904886">next</a><span>|</span><label class="collapse" for="c-42904584">[-]</label><label class="expand" for="c-42904584">[1 more]</label></div><br/><div class="children"><div class="content">This is good also <a href="https:&#x2F;&#x2F;huyenchip.com&#x2F;2023&#x2F;05&#x2F;02&#x2F;rlhf.html" rel="nofollow">https:&#x2F;&#x2F;huyenchip.com&#x2F;2023&#x2F;05&#x2F;02&#x2F;rlhf.html</a></div><br/></div></div><div id="42904886" class="c"><input type="checkbox" id="c-42904886" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#42904584">prev</a><span>|</span><label class="collapse" for="c-42904886">[-]</label><label class="expand" for="c-42904886">[6 more]</label></div><br/><div class="children"><div class="content">Whats the difference between RLHF and distillation?</div><br/><div id="42905186" class="c"><input type="checkbox" id="c-42905186" checked=""/><div class="controls bullet"><span class="by">tintor</span><span>|</span><a href="#42904886">parent</a><span>|</span><label class="collapse" for="c-42905186">[-]</label><label class="expand" for="c-42905186">[5 more]</label></div><br/><div class="children"><div class="content">They are different processes.<p>- RLHF: Turns pre-trained model (which just performs autocomplete of text) into a model that you can speak with, ie. answer user questions and refuse providing harmful answers.<p>- Distillation: Transfer skills &#x2F; knowledge &#x2F; behavior from one model (and architecture) to a smaller model (and possibly different architecture), by training second model on output log probs of first model.</div><br/><div id="42905660" class="c"><input type="checkbox" id="c-42905660" checked=""/><div class="controls bullet"><span class="by">gr3ml1n</span><span>|</span><a href="#42904886">root</a><span>|</span><a href="#42905186">parent</a><span>|</span><a href="#42905337">next</a><span>|</span><label class="collapse" for="c-42905660">[-]</label><label class="expand" for="c-42905660">[1 more]</label></div><br/><div class="children"><div class="content">Your description of distillation is largely correct, but not RLHF.<p>The process of taking a base model that is capable of continuing (&#x27;autocomplete&#x27;) some text input and teaching it to respond to questions in a Q&amp;A chatbot-style format is called instruction tuning.  It&#x27;s pretty much always done via supervised fine-tuning.  Otherwise known as: show it a bunch of examples of chat transcripts.<p>RLHF is more granular and generally one of the last steps in a training pipeline.  With RLHF you train a new model, the <i>reward model</i>.<p>You make that model by having the LLM output a bunch of responses, and then having humans rank the output.  E.g.:<p><pre><code>  Q: What&#x27;s the Capital of France? A: Paris
</code></pre>
Might be scored as `1` by a human, while:<p><pre><code>  Q: What&#x27;s the Capital of France? A: Fuck if I know
</code></pre>
Would be scored as `0`.<p>You feed those rankings into the reward model.  Then, you have the LLM generate a ton of responses, and have the reward model score it.<p>If the reward model says it&#x27;s good, the LLM&#x27;s output is <i>reinforced</i>, i.e.: it&#x27;s told <i>&#x27;that was good, more like that&#x27;</i>.<p>If the output scores low, you do the opposite.<p>Because the reward model is trained based on human preferences, and the reward model is used to reinforce the LLMs output based on those preferences, the whole process is called <i>reinforcement learning from human feedback</i>.</div><br/></div></div><div id="42905337" class="c"><input type="checkbox" id="c-42905337" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#42904886">root</a><span>|</span><a href="#42905186">parent</a><span>|</span><a href="#42905660">prev</a><span>|</span><label class="collapse" for="c-42905337">[-]</label><label class="expand" for="c-42905337">[3 more]</label></div><br/><div class="children"><div class="content">So RLHF is the secret sauce behind modern LLMs?</div><br/><div id="42905553" class="c"><input type="checkbox" id="c-42905553" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42904886">root</a><span>|</span><a href="#42905337">parent</a><span>|</span><label class="collapse" for="c-42905553">[-]</label><label class="expand" for="c-42905553">[2 more]</label></div><br/><div class="children"><div class="content">No, this isn&#x27;t quite right. LLMs are trained in stages:<p>1. Pre-training. In this stage, the model is trained on a gigantic corpus of web documents, books, papers, etc., and the objective is to predict the next token of each training sample correctly.<p>2. Supervised fine-tuning. In this stage, the model is shown examples of chat transcripts that are formatted with a chat template. The examples show a user asking a question and an assistant providing an answer. The training objective is the same as in #1: to predict the next token in the training example correctly.<p>3. Reinforcement learning. Prior to R1, this has mainly taken the form of training a reward model on top of the LLM to steer the model toward arriving at whole sequences that are preferred by human feedback (although AI feedback is a similar reward that is often used instead). There are different ways to do this reward model. When OpenAI first published the technique (probably their last bit of interesting open research?), they were using PPO. There are now a variety of ways to do the reward model, including methods like Direct Preference Optimization that don&#x27;t use a separate reward model at all and are easier to do.<p>Stage 1 teaches the model to understand language and imparts world knowledge. Stage 2 teaches the model to act like an assistant. This is where the &quot;magic&quot; is. Stage 3 makes the model do a better job of being an assistant. The traditional analogy is that Stage 1 is the cake; Stage 2 is the frosting; and Stage 3 is the cherry on top.<p>R1-Zero departs from this &quot;recipe&quot; in that the reasoning magic comes from the reinforcement learning (stage 3). What DeepSeek showed is that, given a reward to produce a correct response, the model will learn to output chain-of-thought material on its own. It will, essentially, develop a chain-of-thought language that helps it accomplish the end goal. This is the most interesting part of the paper, IMO, and it&#x27;s a result that&#x27;s already been replicated on smaller base models.</div><br/><div id="42905722" class="c"><input type="checkbox" id="c-42905722" checked=""/><div class="controls bullet"><span class="by">marcosfelt</span><span>|</span><a href="#42904886">root</a><span>|</span><a href="#42905553">parent</a><span>|</span><label class="collapse" for="c-42905722">[-]</label><label class="expand" for="c-42905722">[1 more]</label></div><br/><div class="children"><div class="content">This is a great summary!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>