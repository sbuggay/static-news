<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729242071367" as="style"/><link rel="stylesheet" href="styles.css?v=1729242071367"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://unsloth.ai/blog/gradient">Bugs in LLM Training – Gradient Accumulation Fix</a> <span class="domain">(<a href="https://unsloth.ai">unsloth.ai</a>)</span></div><div class="subtext"><span>apsec112</span> | <span>6 comments</span></div><br/><div><div id="41876580" class="c"><input type="checkbox" id="c-41876580" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41877387">next</a><span>|</span><label class="collapse" for="c-41876580">[-]</label><label class="expand" for="c-41876580">[1 more]</label></div><br/><div class="children"><div class="content">Same issue described on HF: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;gradient_accumulation" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;gradient_accumulation</a><p>It also highlights the main disadvantage of Transformers codebase using the copy-paste method for models, where this fix needs to be applied to every single model separately.</div><br/></div></div><div id="41877387" class="c"><input type="checkbox" id="c-41877387" checked=""/><div class="controls bullet"><span class="by">xcodevn</span><span>|</span><a href="#41876580">prev</a><span>|</span><a href="#41876537">next</a><span>|</span><label class="collapse" for="c-41877387">[-]</label><label class="expand" for="c-41877387">[3 more]</label></div><br/><div class="children"><div class="content">Look from a different point of view: this is a feature, not a bug. With this, every example has equal weight, while with the <i>fix</i>, every token has equal weight.</div><br/><div id="41877547" class="c"><input type="checkbox" id="c-41877547" checked=""/><div class="controls bullet"><span class="by">oergiR</span><span>|</span><a href="#41877387">parent</a><span>|</span><a href="#41877406">next</a><span>|</span><label class="collapse" for="c-41877547">[-]</label><label class="expand" for="c-41877547">[1 more]</label></div><br/><div class="children"><div class="content">That makes it sound like it’s a choice, which it isn’t really. The way to look at it is from a probabilistic perspective: with the fix, you maximise the probability of the data. Without the fix, you fairly arbitrarily raise some probabilities to a power greater than one, and some to a power less than one.</div><br/></div></div><div id="41877406" class="c"><input type="checkbox" id="c-41877406" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#41877387">parent</a><span>|</span><a href="#41877547">prev</a><span>|</span><a href="#41876537">next</a><span>|</span><label class="collapse" for="c-41877406">[-]</label><label class="expand" for="c-41877406">[1 more]</label></div><br/><div class="children"><div class="content">Yes you&#x27;re correct, but in normal full batch training without gradient accumulation, all tokens are weighted equally. Standard grad accum does not, and so the &quot;fix&quot; makes grad accum and full batch training finally mathematically equivalent</div><br/></div></div></div></div><div id="41876537" class="c"><input type="checkbox" id="c-41876537" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#41877387">prev</a><span>|</span><label class="collapse" for="c-41876537">[-]</label><label class="expand" for="c-41876537">[1 more]</label></div><br/><div class="children"><div class="content">Oh hey! :)
TLDR naively gradient accumulation was over-weighting short sequence lengths in LLM finetuning and training runs, and under-weighting long sequence lengths.<p>For eg a text with sequence lengths of [1, 100] would be scaled by 1&#x2F;(100+1) in full batch training, but grad accum of 2 would weight [1] as 1&#x2F;1 * 1&#x2F;2 = 1&#x2F;2, whilst [100] as 1&#x2F;100 * 1&#x2F;2 = 1&#x2F;200. (1&#x2F;2 since grad accum needs to divide by the # of grad accum steps)</div><br/></div></div></div></div></div></div></div></body></html>