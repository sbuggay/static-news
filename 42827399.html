<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737882065672" as="style"/><link rel="stylesheet" href="styles.css?v=1737882065672"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://hkust-nlp.notion.site/simplerl-reason">Emerging Reasoning with Reinforcement Learning</a> <span class="domain">(<a href="https://hkust-nlp.notion.site">hkust-nlp.notion.site</a>)</span></div><div class="subtext"><span>pella</span> | <span>111 comments</span></div><br/><div><div id="42828509" class="c"><input type="checkbox" id="c-42828509" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42828473">next</a><span>|</span><label class="collapse" for="c-42828509">[-]</label><label class="expand" for="c-42828509">[10 more]</label></div><br/><div class="children"><div class="content">The real thing that surprises me (as a layman trying to get up to speed on this stuff) is that there&#x27;s no &quot;trick&quot; to it. It really just does seem to be a textbook application of RL to LLMs.<p>Going from a base LLM to human instruction-tuned (SFT) ones is definitely an ingenious leap where it&#x27;s not obvious that you&#x27;d get anything meaningful. But when we quickly saw afterwards that prompting for chain of thought improved performance, why wasn&#x27;t this the immediate next step that everyone took. It seems like even after the release of o1 the trick wasn&#x27;t apparent to everyone, and if it wasn&#x27;t for DeepSeek people still might not have realized it.</div><br/><div id="42828846" class="c"><input type="checkbox" id="c-42828846" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42828509">parent</a><span>|</span><a href="#42828631">next</a><span>|</span><label class="collapse" for="c-42828846">[-]</label><label class="expand" for="c-42828846">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  why wasn&#x27;t this the immediate next step that everyone took.<p>It was actually tested by various labs. Just probably not at this scale. The first model that featured RL prominently was DeepSeek-math-7b-RL, published last year in april. It was at the time the best model for math, and remained so until the qwen2.5-math series, that probably had way more data put into them.<p>There&#x27;s a thing about RL that makes it tricky - the models tend to behave very stubbornly. That is, if they see something that resembles their training method (i.e. math problems), they&#x27;ll solve the problem, and they&#x27;ll be <i>good</i> at it. But if you want something close to that but not quite solving it (i.e. analyse this math problem and write hints, or here are 5 problems extract the common methods used for solving, etc.) you&#x27;ll see that they perform very poorly, often times just going straight into &quot;to solve this problem we...&quot;.<p>This is even mentioned in the R1 paper. Poor adherence to prompts, especially ssytem prompts. So that is still challenging.</div><br/></div></div><div id="42828631" class="c"><input type="checkbox" id="c-42828631" checked=""/><div class="controls bullet"><span class="by">attentionmech</span><span>|</span><a href="#42828509">parent</a><span>|</span><a href="#42828846">prev</a><span>|</span><a href="#42828567">next</a><span>|</span><label class="collapse" for="c-42828631">[-]</label><label class="expand" for="c-42828631">[1 more]</label></div><br/><div class="children"><div class="content">the tulu team saw it. but, yes nobody like scaled it to the extent deepseek did. I am surprised that the faang labs which have the best of the best didn&#x27;t see this.</div><br/></div></div><div id="42828567" class="c"><input type="checkbox" id="c-42828567" checked=""/><div class="controls bullet"><span class="by">qnleigh</span><span>|</span><a href="#42828509">parent</a><span>|</span><a href="#42828631">prev</a><span>|</span><a href="#42828708">next</a><span>|</span><label class="collapse" for="c-42828567">[-]</label><label class="expand" for="c-42828567">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve wondered this too, I really hope someone with more knowledge can comment. My impression is that people worked on this kind of thing for years before they started seeing a &#x27;signal&#x27; i.e. that they actually got RL working to improve performance. But why is that happening now? What were the tricks that made it work?</div><br/><div id="42828589" class="c"><input type="checkbox" id="c-42828589" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42828509">root</a><span>|</span><a href="#42828567">parent</a><span>|</span><a href="#42828708">next</a><span>|</span><label class="collapse" for="c-42828589">[-]</label><label class="expand" for="c-42828589">[3 more]</label></div><br/><div class="children"><div class="content">DeepSeek only recently invented GRPO, it&#x27;s possible that was the final missing piece needed to make it viable.</div><br/><div id="42828688" class="c"><input type="checkbox" id="c-42828688" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42828509">root</a><span>|</span><a href="#42828589">parent</a><span>|</span><a href="#42828734">next</a><span>|</span><label class="collapse" for="c-42828688">[-]</label><label class="expand" for="c-42828688">[1 more]</label></div><br/><div class="children"><div class="content">The group in this article used straight and simple PPO, so I guess GRPO isn&#x27;t required.<p>My hypothesis is that everyone was just so stunned by oai&#x27;s result so most just decided to blindly chase it and do what oai did (i.e. scaling up). And it&#x27;s only after o1 people started seriously trying other ideas.</div><br/></div></div><div id="42828734" class="c"><input type="checkbox" id="c-42828734" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42828509">root</a><span>|</span><a href="#42828589">parent</a><span>|</span><a href="#42828688">prev</a><span>|</span><a href="#42828708">next</a><span>|</span><label class="collapse" for="c-42828734">[-]</label><label class="expand" for="c-42828734">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have any intuition here and am in no way qualified, but my read of the paper was that GRPO was mainly an optimization to reduce cost &amp; GPUs when training (by skipping the need to keep another copy of the LLM in memory as the value network), but otherwise any RL algorithm should have worked? I mean it seems R1 uses outcome rewards only and GRPO doesn&#x27;t do anything special to alleviate reward sparsity, so it feels like it shouldn&#x27;t affect viability too much.<p>Also on the note of RL optimizers, if anyone here is familiar with this space can they comment on how the recently introduced PRIME [1] compares to PPO directly? Their description is confusing since the &quot;implicit PRM&quot; they introduce which is trained alongside the policy network seems no different from the value network of PPO.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;PRIME-RL&#x2F;PRIME">https:&#x2F;&#x2F;github.com&#x2F;PRIME-RL&#x2F;PRIME</a></div><br/></div></div></div></div></div></div><div id="42828708" class="c"><input type="checkbox" id="c-42828708" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#42828509">parent</a><span>|</span><a href="#42828567">prev</a><span>|</span><a href="#42828583">next</a><span>|</span><label class="collapse" for="c-42828708">[-]</label><label class="expand" for="c-42828708">[1 more]</label></div><br/><div class="children"><div class="content">I think a lot of it had to do with DeepSeek need to use as fewer resources as possible why did it do this how can it do it in fewer steps using fewer resources. Where as most of the FAANG were looking at throwing more data and processing power at it.</div><br/></div></div><div id="42828583" class="c"><input type="checkbox" id="c-42828583" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42828509">parent</a><span>|</span><a href="#42828708">prev</a><span>|</span><a href="#42828473">next</a><span>|</span><label class="collapse" for="c-42828583">[-]</label><label class="expand" for="c-42828583">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if OpenAI did the same thing, or they instead took the approach of manually building an expensive, human-designed supervised learning dataset for reasoning. If the latter, they must be really kicking themselves now.</div><br/><div id="42828701" class="c"><input type="checkbox" id="c-42828701" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#42828509">root</a><span>|</span><a href="#42828583">parent</a><span>|</span><a href="#42828473">next</a><span>|</span><label class="collapse" for="c-42828701">[-]</label><label class="expand" for="c-42828701">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d bet $5 that o1 was also built with either RL or search, or a combination of the two. That was what I initially thought when they announced o1-preview, after I saw the sample reasoning traces.<p>But alas I am just an ML enthusiast, not a member of some lab with access to GPUs.</div><br/></div></div></div></div></div></div><div id="42828473" class="c"><input type="checkbox" id="c-42828473" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#42828509">prev</a><span>|</span><a href="#42827751">next</a><span>|</span><label class="collapse" for="c-42828473">[-]</label><label class="expand" for="c-42828473">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone have a good recent overview with paper links or review article for RL methods? 
A lot happening in that space</div><br/></div></div><div id="42827751" class="c"><input type="checkbox" id="c-42827751" checked=""/><div class="controls bullet"><span class="by">MIA_Alive</span><span>|</span><a href="#42828473">prev</a><span>|</span><a href="#42828535">next</a><span>|</span><label class="collapse" for="c-42827751">[-]</label><label class="expand" for="c-42827751">[1 more]</label></div><br/><div class="children"><div class="content">LOL, my RL professor is gonna be happy. After the field got overlooked for soooo long</div><br/></div></div><div id="42828535" class="c"><input type="checkbox" id="c-42828535" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#42827751">prev</a><span>|</span><a href="#42828106">next</a><span>|</span><label class="collapse" for="c-42828535">[-]</label><label class="expand" for="c-42828535">[5 more]</label></div><br/><div class="children"><div class="content">Anyone who puts emerging or emergent in their headlines should be required to come back in 2 years time and do penance for their optimism.</div><br/><div id="42828685" class="c"><input type="checkbox" id="c-42828685" checked=""/><div class="controls bullet"><span class="by">dsco</span><span>|</span><a href="#42828535">parent</a><span>|</span><a href="#42828613">next</a><span>|</span><label class="collapse" for="c-42828685">[-]</label><label class="expand" for="c-42828685">[1 more]</label></div><br/><div class="children"><div class="content">How is it not emerging, if the phenomena hasn’t been hard-wired in and is unexpected?</div><br/></div></div><div id="42828613" class="c"><input type="checkbox" id="c-42828613" checked=""/><div class="controls bullet"><span class="by">qnleigh</span><span>|</span><a href="#42828535">parent</a><span>|</span><a href="#42828685">prev</a><span>|</span><a href="#42828106">next</a><span>|</span><label class="collapse" for="c-42828613">[-]</label><label class="expand" for="c-42828613">[3 more]</label></div><br/><div class="children"><div class="content">Do you remember when people discovered that LLMs had emergent coding ability? That turned out to be a pretty big deal...</div><br/><div id="42828670" class="c"><input type="checkbox" id="c-42828670" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#42828535">root</a><span>|</span><a href="#42828613">parent</a><span>|</span><a href="#42828106">next</a><span>|</span><label class="collapse" for="c-42828670">[-]</label><label class="expand" for="c-42828670">[2 more]</label></div><br/><div class="children"><div class="content">...for who? people willing to commit code with a lot of hidden bugs? Or management in a hurry to lay people off? Don&#x27;t underestimate how quickly people will run straight into a wall if you tell them it&#x27;s a door.</div><br/><div id="42828693" class="c"><input type="checkbox" id="c-42828693" checked=""/><div class="controls bullet"><span class="by">jdhendrickson</span><span>|</span><a href="#42828535">root</a><span>|</span><a href="#42828670">parent</a><span>|</span><a href="#42828106">next</a><span>|</span><label class="collapse" for="c-42828693">[-]</label><label class="expand" for="c-42828693">[1 more]</label></div><br/><div class="children"><div class="content">Eloquently said.</div><br/></div></div></div></div></div></div></div></div><div id="42828106" class="c"><input type="checkbox" id="c-42828106" checked=""/><div class="controls bullet"><span class="by">almaight</span><span>|</span><a href="#42828535">prev</a><span>|</span><a href="#42827629">next</a><span>|</span><label class="collapse" for="c-42828106">[-]</label><label class="expand" for="c-42828106">[8 more]</label></div><br/><div class="children"><div class="content">This is American history written in R1, it is very logical:
Whenas the nations of Europa did contend upon the waves—Spain plundered gold in Mexica, Albion planted cotton in Virginia—thirteen colonies did kindle rebellion. General Washington raised the standard of liberty at Philadelphia; Franklin parleyed with Gaul’s envoys in Paris. When the cannons fell silent at Yorktown, a new republic arose in the wilderness, not by Heaven’s mandate, but by French muskets’ aid.<p>Yet the fledgling realm, hedged by western forests and eastern seas, waxed mighty. Jefferson purchased Louisiana’s plains; Monroe’s doctrine shackled southern realms. Gold-seekers pierced mountains, iron roads spanned the continent, while tribes wept blood upon the prairie. Then roared foundries by Great Lakes, bondsmen toiled in cotton fields, steel glowed in Pittsburgh’s fires, and black gold gushed from Texan soil—a molten surge none might stay.<p>Wilson trod Europe’s stage as nascent hegemon. Roosevelt’s New Deal healed wounds; Marshall’s gold revived ruined cities. The atom split at Alamogordo; greenbacks reigned at Bretton Woods. Armadas patrolled seven seas, spies wove webs across hemispheres. Through four decades’ contest with the Red Bear, Star Wars drained the Soviet coffers. Silicon’s chips commanded the world’s pulse, Hollywood’s myths shaped mankind’s dreams, Wall Street’s ledgers ruled nations’ fates—a fleeting &quot;End of History&quot; illusion.<p>But the colossus falters. Towers fell, and endless wars began; subprime cracks devoured fortunes. Pestilence slew multitudes while ballots bred discord. Red and Blue rend the Union’s fabric, gunfire echoes where laws grow faint. The Melting Pot now boils with strife, the Beacon dims to a prison’s glare. With dollar-cloth and patent-chains, with dreadnoughts’ threat, it binds the world—nations seethe yet dare not speak.<p>Three hundred million souls, guarded by two oceans, armed with nuclear flame, crowned with finance’s scepter—how came such dominion to waver? They fortified might but neglected virtue, wielded force but forgot mercy. As Mencius warned: &quot;He who rides tigers cannot dismount.&quot; Rome split asunder, Britannia’s sun set; behold now Old Glory’s tremulous flutter. Thus say the sages: A realm endures by benevolence, not arms; peace flows from harmony, not hegemony—this truth outlives all empires.</div><br/><div id="42828169" class="c"><input type="checkbox" id="c-42828169" checked=""/><div class="controls bullet"><span class="by">suraci</span><span>|</span><a href="#42828106">parent</a><span>|</span><a href="#42828154">next</a><span>|</span><label class="collapse" for="c-42828169">[-]</label><label class="expand" for="c-42828169">[2 more]</label></div><br/><div class="children"><div class="content">Just pointing out a factual error: &quot;He who rides tigers cannot dismount&quot; is not said by Mencius, but comes from Fang Xuanling of the Tang Dynasty.<p>However, it is still highly literate (both in English and Chinese), which I believe is one of its advantages</div><br/><div id="42828684" class="c"><input type="checkbox" id="c-42828684" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#42828106">root</a><span>|</span><a href="#42828169">parent</a><span>|</span><a href="#42828154">next</a><span>|</span><label class="collapse" for="c-42828684">[-]</label><label class="expand" for="c-42828684">[1 more]</label></div><br/><div class="children"><div class="content">Also, &quot;Star Wars&quot; appeared in the 80s, never took off, and certainly wasn&#x27;t a drain for &quot;four decades&quot; on the Soviet Union&#x27;s coffers.</div><br/></div></div></div></div><div id="42828154" class="c"><input type="checkbox" id="c-42828154" checked=""/><div class="controls bullet"><span class="by">alsaaro</span><span>|</span><a href="#42828106">parent</a><span>|</span><a href="#42828169">prev</a><span>|</span><a href="#42828251">next</a><span>|</span><label class="collapse" for="c-42828154">[-]</label><label class="expand" for="c-42828154">[4 more]</label></div><br/><div class="children"><div class="content">You mean deep seek r1 generated this?<p>With what prompt?</div><br/><div id="42828528" class="c"><input type="checkbox" id="c-42828528" checked=""/><div class="controls bullet"><span class="by">almaight</span><span>|</span><a href="#42828106">root</a><span>|</span><a href="#42828154">parent</a><span>|</span><a href="#42828251">next</a><span>|</span><label class="collapse" for="c-42828528">[-]</label><label class="expand" for="c-42828528">[3 more]</label></div><br/><div class="children"><div class="content">Write an epic narrative of American history, employing archaic English vocabulary and grandiose structure, rich with metaphors and allusions. Weave together elements of Eastern and Western traditions, transforming modern historical events into the solemn language of ancient inscriptions. Through this classical epic reconstruction, deconstruct contemporary state power, unveiling its complexities with the weight and dignity of antiquity. Each pivotal moment in history should be distilled into profound symbols, acquiring new metaphorical dimensions within the lexicon of the past. The result should be a transcendent dialogue of civilizations, bridging temporal and cultural divides, illuminating the echoes of history in a timeless and universal context.</div><br/><div id="42828538" class="c"><input type="checkbox" id="c-42828538" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#42828106">root</a><span>|</span><a href="#42828528">parent</a><span>|</span><a href="#42828698">next</a><span>|</span><label class="collapse" for="c-42828538">[-]</label><label class="expand" for="c-42828538">[1 more]</label></div><br/><div class="children"><div class="content">And with what prompt was this prompt written? Its prompts all the way down?</div><br/></div></div><div id="42828698" class="c"><input type="checkbox" id="c-42828698" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#42828106">root</a><span>|</span><a href="#42828528">parent</a><span>|</span><a href="#42828538">prev</a><span>|</span><a href="#42828251">next</a><span>|</span><label class="collapse" for="c-42828698">[-]</label><label class="expand" for="c-42828698">[1 more]</label></div><br/><div class="children"><div class="content">So in response it wrote a heroically tall mountain of bullshit, laced with falsehoods of the sort that even some neanderthal natcon would be unable to dream up, then served it as an abysmally long, overdubbed narration to the next Top Gun movie set in the same future as Idiocracy.</div><br/></div></div></div></div></div></div></div></div><div id="42827629" class="c"><input type="checkbox" id="c-42827629" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42828106">prev</a><span>|</span><a href="#42828048">next</a><span>|</span><label class="collapse" for="c-42827629">[-]</label><label class="expand" for="c-42827629">[13 more]</label></div><br/><div class="children"><div class="content">Can someone summarize the upshot for people here?</div><br/><div id="42828035" class="c"><input type="checkbox" id="c-42828035" checked=""/><div class="controls bullet"><span class="by">teej</span><span>|</span><a href="#42827629">parent</a><span>|</span><a href="#42827765">next</a><span>|</span><label class="collapse" for="c-42828035">[-]</label><label class="expand" for="c-42828035">[8 more]</label></div><br/><div class="children"><div class="content">I’ll give a “wtf does this mean” view.<p>We have observed that LLMs can perform better on hard tasks like math if we teach it to “think about” the problem first. The technique is called “chain-of-thought”. The language model is taught to emit a series of sentences that break a problem down before answering it. OpenAI’s o1 works this way, and performs well on benchmarks because of it.<p>To train a model to do this, you need to show it many examples of correct chains of thought. These are expensive to produce and it’s expensive to train models on them.<p>DeepSeek discovered something surprising. It turns out, you don’t need to explicitly train a model to produce a chain of thought. Instead, under the right conditions, models will learn this behavior emergently. They found a way for a language model to learn chain of thought very cheaply, and then released that model as open source.<p>Thought chains turn out to be extremely useful. And now that they’re cheap and easy to produce, we are learning all the different ways they can be put to use.<p>Some of the open questions right now are:<p>- Can we teach small models to learn chain-of-thought? (yes) How cheaply? On which tasks?<p>- Can we generate thought chains and just copy&#x2F;paste them into the prompts of other models? (yes) Which domains does this work for? How well does it generalize?<p>That’s what this post is going after.</div><br/><div id="42828746" class="c"><input type="checkbox" id="c-42828746" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42828035">parent</a><span>|</span><a href="#42828087">next</a><span>|</span><label class="collapse" for="c-42828746">[-]</label><label class="expand" for="c-42828746">[1 more]</label></div><br/><div class="children"><div class="content">But how exactly does it emerge, what did they do to make that happen vs previous trainings</div><br/></div></div><div id="42828087" class="c"><input type="checkbox" id="c-42828087" checked=""/><div class="controls bullet"><span class="by">pillefitz</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42828035">parent</a><span>|</span><a href="#42828746">prev</a><span>|</span><a href="#42827765">next</a><span>|</span><label class="collapse" for="c-42828087">[-]</label><label class="expand" for="c-42828087">[6 more]</label></div><br/><div class="children"><div class="content">Can you explain the RL part?</div><br/><div id="42828171" class="c"><input type="checkbox" id="c-42828171" checked=""/><div class="controls bullet"><span class="by">teej</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42828087">parent</a><span>|</span><a href="#42827765">next</a><span>|</span><label class="collapse" for="c-42828171">[-]</label><label class="expand" for="c-42828171">[5 more]</label></div><br/><div class="children"><div class="content">The way you taught chain-of-thought before was with supervised fine tuning (SFT). During training, you have to rate every sentence of reasoning the model writes, many times, to nudge it to reason correctly.<p>But this approach to teach chain-of-thought doesn’t do that. In this post, they take a small model (7B) that already knows math. Then they give it a relatively small number of problems to solve (8k). They use a simple reinforcement learning loop where the only goal is to get the problem right. They don’t care how the model got the right answer, just that it’s correct.<p>This is part of the “recipe” that DeepSeek used to create R1.<p>After many iterations, just like DeepSeek, they found that the model has an “aha” moment. It starts emitting chains-of-thought where it wasn’t before. And then it starts getting the math answers right.<p>This is the gist of it. I don’t fully understand the recipe involved.<p>Can you teach small models to “think” just using RL? How small can they be? What tasks does this work for? Is just RL best for this? RL+SFT? Everyone’s trying to figure it out.</div><br/><div id="42828655" class="c"><input type="checkbox" id="c-42828655" checked=""/><div class="controls bullet"><span class="by">attentionmech</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42828171">parent</a><span>|</span><a href="#42828718">next</a><span>|</span><label class="collapse" for="c-42828655">[-]</label><label class="expand" for="c-42828655">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s nice explanation. Is there any insights so far in the field about why chain of thought improves the capability of a model? Does it like provide model with more working memory or something in the context itself?</div><br/><div id="42828715" class="c"><input type="checkbox" id="c-42828715" checked=""/><div class="controls bullet"><span class="by">jobhadel</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42828655">parent</a><span>|</span><a href="#42828718">next</a><span>|</span><label class="collapse" for="c-42828715">[-]</label><label class="expand" for="c-42828715">[1 more]</label></div><br/><div class="children"><div class="content">Chain of thought breaks down a problem into smaller chunks which is easier to solve for a model than trying to find solution directly for larger problem</div><br/></div></div></div></div><div id="42828718" class="c"><input type="checkbox" id="c-42828718" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42828171">parent</a><span>|</span><a href="#42828655">prev</a><span>|</span><a href="#42828731">next</a><span>|</span><label class="collapse" for="c-42828718">[-]</label><label class="expand" for="c-42828718">[1 more]</label></div><br/><div class="children"><div class="content">whum... doesn&#x27;t not caring how it got the answer right create the same exact problem as fine tuning?</div><br/></div></div><div id="42828731" class="c"><input type="checkbox" id="c-42828731" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42828171">parent</a><span>|</span><a href="#42828718">prev</a><span>|</span><a href="#42827765">next</a><span>|</span><label class="collapse" for="c-42828731">[-]</label><label class="expand" for="c-42828731">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Then they give it a relatively small number of problems to solve (8k). They use a simple reinforcement learning loop where the only goal is to get the problem right. They don’t care how the model got the right answer, just that it’s correct.<p>I guess it only works if you select problems that are within reach of the model in the first place (but not too easy), so that there can actually be a positive feedback loop, right?</div><br/></div></div></div></div></div></div></div></div><div id="42827765" class="c"><input type="checkbox" id="c-42827765" checked=""/><div class="controls bullet"><span class="by">randomifcpfan</span><span>|</span><a href="#42827629">parent</a><span>|</span><a href="#42828035">prev</a><span>|</span><a href="#42828048">next</a><span>|</span><label class="collapse" for="c-42827765">[-]</label><label class="expand" for="c-42827765">[4 more]</label></div><br/><div class="children"><div class="content">The DeepSeek R1 paper explains how they trained their model in enough detail that people can replicate the process. Many people around the world are doing so, using various sizes of models and training data. Expect to see many posts like this over the next three months. The attempts that use small models will get done first. The larger models take much longer.<p>Small r1 style models are pretty limited, so this is interesting primarily from an “I reproduced the results” point of view, not a “here is a new model that’s useful” pov.</div><br/><div id="42827900" class="c"><input type="checkbox" id="c-42827900" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42827765">parent</a><span>|</span><a href="#42828048">next</a><span>|</span><label class="collapse" for="c-42827900">[-]</label><label class="expand" for="c-42827900">[3 more]</label></div><br/><div class="children"><div class="content">From the Deepseek R1 paper:<p><pre><code>  For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.
</code></pre>
The impression I got from the paper, although I don&#x27;t think it was explicitly stated, is that they think distillation will work better than training the smaller models using RL (as OP did).</div><br/><div id="42827996" class="c"><input type="checkbox" id="c-42827996" checked=""/><div class="controls bullet"><span class="by">nielsole</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42827900">parent</a><span>|</span><a href="#42828048">next</a><span>|</span><label class="collapse" for="c-42827996">[-]</label><label class="expand" for="c-42827996">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We demonstrate that the reasoning patterns of larger models can be distilled into smaller
models, resulting in better performance compared to the reasoning patterns discovered
through RL on small models<p>I found this statement from the paper to be at odds with what you cited, but I guess they mean SFT+RL would be better than either just SFT and RL</div><br/><div id="42828109" class="c"><input type="checkbox" id="c-42828109" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#42827629">root</a><span>|</span><a href="#42827996">parent</a><span>|</span><a href="#42828048">next</a><span>|</span><label class="collapse" for="c-42828109">[-]</label><label class="expand" for="c-42828109">[1 more]</label></div><br/><div class="children"><div class="content">I think they&#x27;re saying that some reasoning patterns which large models can learn using only RL (i.e. without the patterns existing in the training data), can&#x27;t be learned by smaller models in the same way. They have to be &#x27;taught&#x27; through examples provided during SFT.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42828048" class="c"><input type="checkbox" id="c-42828048" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42827629">prev</a><span>|</span><a href="#42828217">next</a><span>|</span><label class="collapse" for="c-42828048">[-]</label><label class="expand" for="c-42828048">[46 more]</label></div><br/><div class="children"><div class="content">There was a whole bunch of people who claimed LLMs can&#x27;t reason at all and that everything is a regurgitation. I wonder what they have to say about this. Like, what exactly is going on here with chain of thought reasoning from their expert perspective?</div><br/><div id="42828227" class="c"><input type="checkbox" id="c-42828227" checked=""/><div class="controls bullet"><span class="by">teej</span><span>|</span><a href="#42828048">parent</a><span>|</span><a href="#42828199">next</a><span>|</span><label class="collapse" for="c-42828227">[-]</label><label class="expand" for="c-42828227">[9 more]</label></div><br/><div class="children"><div class="content">My mental model for chain-of-thought is not “reasoning”. It’s more of an iterative search through the latent space of the model.</div><br/><div id="42828462" class="c"><input type="checkbox" id="c-42828462" checked=""/><div class="controls bullet"><span class="by">qnleigh</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828227">parent</a><span>|</span><a href="#42828357">next</a><span>|</span><label class="collapse" for="c-42828462">[-]</label><label class="expand" for="c-42828462">[1 more]</label></div><br/><div class="children"><div class="content">Can you elaborate? I think this is a really interesting question. It comes up over and over again, but it often feels like the two sides of the debate talk past each other. What does the mental model of &#x27;iterative search through latent space&#x27; convey that &#x27;reasoning&#x27; doesn&#x27;t? Human reasoning also often searches through a space of potential solution methods and similar problems, and keeps applying them until making progress.<p>I appreciate that there might be danger in using words like &#x27;thinking&#x27; and &#x27;reasoning&#x27; in that they cause us to anthropomorphize LLMs, but if we are careful not to do so then this is a separate issue.</div><br/></div></div><div id="42828357" class="c"><input type="checkbox" id="c-42828357" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828227">parent</a><span>|</span><a href="#42828462">prev</a><span>|</span><a href="#42828199">next</a><span>|</span><label class="collapse" for="c-42828357">[-]</label><label class="expand" for="c-42828357">[7 more]</label></div><br/><div class="children"><div class="content">And human reasoning is somehow more magical? Really struggling to understand the distinction between searching through a latent space and “reasoning”.</div><br/><div id="42828788" class="c"><input type="checkbox" id="c-42828788" checked=""/><div class="controls bullet"><span class="by">numba888</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828357">parent</a><span>|</span><a href="#42828689">next</a><span>|</span><label class="collapse" for="c-42828788">[-]</label><label class="expand" for="c-42828788">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it has at least two distinct parts: consciousness and sub. First is visible to &#x27;us&#x27;, it&#x27;s inner monologue or vision, or other senses. But we don&#x27;t &#x27;know&#x27; what happens in subconsciousness. The answer just pops up from nowhere. &#x27;Reasoning&#x27; LLMs for now have the first part. The &#x27;sub&#x27; part is questionable, depends how you look at latent space.</div><br/></div></div><div id="42828689" class="c"><input type="checkbox" id="c-42828689" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828357">parent</a><span>|</span><a href="#42828788">prev</a><span>|</span><a href="#42828420">next</a><span>|</span><label class="collapse" for="c-42828689">[-]</label><label class="expand" for="c-42828689">[1 more]</label></div><br/><div class="children"><div class="content">It would be startling if humans could reason this way with two kilos of meat consuming 40w. Even more surprising given that we get answers in ~1s with a cross brain comms time of about 0.25s<p>To me it&#x27;s clear that human reasoning is different from a massive search of a latent space. I can say that when I am thinking I maybe try half a dozen ideas or scenarios, but very rarely more. I can&#x27;t say where those ideas come from or how I make them up though. Maybe we can&#x27;t frame what it is and how it works with human languages though, which might make it seem magical in some way.<p>Or maybe there&#x27;s a good framing that I don&#x27;t know - would love to learn!</div><br/></div></div><div id="42828420" class="c"><input type="checkbox" id="c-42828420" checked=""/><div class="controls bullet"><span class="by">Exoristos</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828357">parent</a><span>|</span><a href="#42828689">prev</a><span>|</span><a href="#42828508">next</a><span>|</span><label class="collapse" for="c-42828420">[-]</label><label class="expand" for="c-42828420">[2 more]</label></div><br/><div class="children"><div class="content">As you&#x27;re demonstrating, it depends on the human.</div><br/><div id="42828816" class="c"><input type="checkbox" id="c-42828816" checked=""/><div class="controls bullet"><span class="by">error_logic</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828420">parent</a><span>|</span><a href="#42828508">next</a><span>|</span><label class="collapse" for="c-42828816">[-]</label><label class="expand" for="c-42828816">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, some of us are more willing to believe ourselves somehow special and block out our own irrationalities and missteps.<p>There is a deep need in some people to ignore their own flaws so they can present themselves as more competent. More reasonably, people try to have a thought ready before starting to talk, or will stop and think again if they notice a contradiction in what they&#x27;re saying.<p>Other people will keep right on going and double down if they&#x27;ve made a mistake, refusing to acknowledge the opportunity for learning and blocking out inconvenient contradiction.<p>Interestingly this is itself quite relevant to the AI alignment problem. Competence requires being able to accurately reflect [on] the patterns, whereas goal-seeking and morality require ignoring some options as costly&#x2F;dangerous distractions--but not to the point of incompetence. Balancing the two is tricky, and neither people nor governments have solved this universally, let alone an AI.</div><br/></div></div></div></div><div id="42828508" class="c"><input type="checkbox" id="c-42828508" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828357">parent</a><span>|</span><a href="#42828420">prev</a><span>|</span><a href="#42828199">next</a><span>|</span><label class="collapse" for="c-42828508">[-]</label><label class="expand" for="c-42828508">[2 more]</label></div><br/><div class="children"><div class="content">no, humans do the same thing - even the &quot;intuition&quot; of things to try are probably the results of searches, but we don&#x27;t have conscious access to them. certainly it&#x27;s the simplest explanation that fits the observations.</div><br/><div id="42828691" class="c"><input type="checkbox" id="c-42828691" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828508">parent</a><span>|</span><a href="#42828199">next</a><span>|</span><label class="collapse" for="c-42828691">[-]</label><label class="expand" for="c-42828691">[1 more]</label></div><br/><div class="children"><div class="content">Given the observations of the workings of brains though (slow, low energy) we&#x27;re back to some magic to make this happen?</div><br/></div></div></div></div></div></div></div></div><div id="42828199" class="c"><input type="checkbox" id="c-42828199" checked=""/><div class="controls bullet"><span class="by">calibas</span><span>|</span><a href="#42828048">parent</a><span>|</span><a href="#42828227">prev</a><span>|</span><a href="#42828771">next</a><span>|</span><label class="collapse" for="c-42828199">[-]</label><label class="expand" for="c-42828199">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get this whole debate, surely what&#x27;s meant by &quot;reason&quot; can be strictly defined and measured? Then we can conclusively say whether or not it&#x27;s happening with LLMs.<p>It seems to me like the debate is largely just semantics about how to define &quot;reason&quot;.</div><br/><div id="42828225" class="c"><input type="checkbox" id="c-42828225" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828199">parent</a><span>|</span><a href="#42828516">next</a><span>|</span><label class="collapse" for="c-42828225">[-]</label><label class="expand" for="c-42828225">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s semantics. But there&#x27;s a general motivation behind it that&#x27;s less technical. Basically if it can reason, it implies human level intelligence. That&#x27;s the line separating man from machine. Once you cross that line there&#x27;s a whole bunch of economic, cultural and existential changes that happen to society that are permanent. We can&#x27;t go back.<p>This is what people are debating about. Many many people don&#x27;t want to believe we crossed the line with LLMs. It brings about a sort of existential dread. Especially to programmers who&#x27;s pride is entirely dependent upon their intelligence and ability to program.</div><br/><div id="42828727" class="c"><input type="checkbox" id="c-42828727" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828225">parent</a><span>|</span><a href="#42828504">next</a><span>|</span><label class="collapse" for="c-42828727">[-]</label><label class="expand" for="c-42828727">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve had &quot;reasoning&quot; machines for a long time - I learned chess playing against a computer in the 1980&#x27;s.<p>But we don&#x27;t have reasoning that can be applied generally in the open world yet. Or at least I haven&#x27;t seen it.<p>In terms of society it should be easy to track if this is true or not. Healthcare and elder care settings will be a very early canary of this because there is huge pressure for improvement and change in these. General reasoning machines will make a very significant, clear and early impact here. I have seen note taking apps for nurses - but not much else so far.</div><br/></div></div><div id="42828504" class="c"><input type="checkbox" id="c-42828504" checked=""/><div class="controls bullet"><span class="by">eastbound</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828225">parent</a><span>|</span><a href="#42828727">prev</a><span>|</span><a href="#42828516">next</a><span>|</span><label class="collapse" for="c-42828504">[-]</label><label class="expand" for="c-42828504">[1 more]</label></div><br/><div class="children"><div class="content">It’s not about being afraid, it’s that the auto-reconfiguration of neurons seems advanced to decompile it at this time, and it surprising that LLM, which are just a probabilistic model of guessing the next word, could be capable of actual thought.<p>The day it happens, we’ll believe it. There are only 100bn neurons in a brain, after all, and many more than this in modern machines, so it is theoretically possible. Just LLMs seemed too simple for that.</div><br/></div></div></div></div><div id="42828516" class="c"><input type="checkbox" id="c-42828516" checked=""/><div class="controls bullet"><span class="by">qnleigh</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828199">parent</a><span>|</span><a href="#42828225">prev</a><span>|</span><a href="#42828771">next</a><span>|</span><label class="collapse" for="c-42828516">[-]</label><label class="expand" for="c-42828516">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s really hard to pin down what reasoning is and measure it precisely. How on earth would you do this?</div><br/></div></div></div></div><div id="42828771" class="c"><input type="checkbox" id="c-42828771" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42828048">parent</a><span>|</span><a href="#42828199">prev</a><span>|</span><a href="#42828075">next</a><span>|</span><label class="collapse" for="c-42828771">[-]</label><label class="expand" for="c-42828771">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There was a whole bunch of people who claimed LLMs can&#x27;t reason at all and that everything is a regurgitation. I wonder what they have to say about this.<p>I don&#x27;t see that as a refutation of the former actually: model trained to be stochastic parrots with next-token prediction as only learning target were indeed stochastic parrots and now we&#x27;ve moved to a completely different technology that features reinforcement learning in its training so it will go farther and farther from stochastic parrots and more and more towards “intelligence”.<p>If anything, the fact that the entire industry has now moved to RL instead of just cramming through trillions of tokens to make progress is a pretty strong acknowledgement that the “stochastic parrots” crowd was right.</div><br/></div></div><div id="42828075" class="c"><input type="checkbox" id="c-42828075" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42828048">parent</a><span>|</span><a href="#42828771">prev</a><span>|</span><a href="#42828217">next</a><span>|</span><label class="collapse" for="c-42828075">[-]</label><label class="expand" for="c-42828075">[30 more]</label></div><br/><div class="children"><div class="content">You can still regurgitate a chain of thought response…<p>It’s all still tokens…</div><br/><div id="42828113" class="c"><input type="checkbox" id="c-42828113" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828075">parent</a><span>|</span><a href="#42828089">next</a><span>|</span><label class="collapse" for="c-42828113">[-]</label><label class="expand" for="c-42828113">[20 more]</label></div><br/><div class="children"><div class="content"><i>You can still regurgitate a chain of thought response…</i><p>You people are <i>so</i> close to getting it.  So close to understanding that you&#x27;re the ones doing the regurgitating.</div><br/><div id="42828153" class="c"><input type="checkbox" id="c-42828153" checked=""/><div class="controls bullet"><span class="by">fmbb</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828113">parent</a><span>|</span><a href="#42828156">next</a><span>|</span><label class="collapse" for="c-42828153">[-]</label><label class="expand" for="c-42828153">[16 more]</label></div><br/><div class="children"><div class="content">Why do you believe that is how humans reason?</div><br/><div id="42828215" class="c"><input type="checkbox" id="c-42828215" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828153">parent</a><span>|</span><a href="#42828156">next</a><span>|</span><label class="collapse" for="c-42828215">[-]</label><label class="expand" for="c-42828215">[15 more]</label></div><br/><div class="children"><div class="content">Neuroscience and evidence be damned, [the brain is a computer] is being transformed into [the brain is an LLM]. Happens with every new technology.<p>Edit: What is happening with people these days? They seem to be reducing people and their minds to machines more easily than ever.</div><br/><div id="42828266" class="c"><input type="checkbox" id="c-42828266" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828215">parent</a><span>|</span><a href="#42828221">next</a><span>|</span><label class="collapse" for="c-42828266">[-]</label><label class="expand" for="c-42828266">[7 more]</label></div><br/><div class="children"><div class="content">The brain is a neural net. And so is the LLM.</div><br/><div id="42828335" class="c"><input type="checkbox" id="c-42828335" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828266">parent</a><span>|</span><a href="#42828374">next</a><span>|</span><label class="collapse" for="c-42828335">[-]</label><label class="expand" for="c-42828335">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The brain is a neural net<p>No it isn&#x27;t, neural networks aren&#x27;t a network of neurons, they are just named after neurons but they are nothing alike. Neurons grow new connections etc and do a whole slew of different things that neural networks doesn&#x27;t do.<p>The ability to grow new connections seems like an integral part to intelligence that neural networks can&#x27;t replicate, at least not without changing them to something very different than they are today.</div><br/></div></div><div id="42828374" class="c"><input type="checkbox" id="c-42828374" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828266">parent</a><span>|</span><a href="#42828335">prev</a><span>|</span><a href="#42828366">next</a><span>|</span><label class="collapse" for="c-42828374">[-]</label><label class="expand" for="c-42828374">[4 more]</label></div><br/><div class="children"><div class="content">Only in name, given by people in machine learning. Really it’s more accurate to say they’re both networks, except one is a million times more complicated in its design.</div><br/><div id="42828570" class="c"><input type="checkbox" id="c-42828570" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828374">parent</a><span>|</span><a href="#42828366">next</a><span>|</span><label class="collapse" for="c-42828570">[-]</label><label class="expand" for="c-42828570">[3 more]</label></div><br/><div class="children"><div class="content">These days it&#x27;s looking like closer to a factor a thousand than a million.<p>Appearances can be deceptive and intelligence in brains may be more complex than they currently looks, but appearances are also the only thing most of us can judge it by at this point — while there&#x27;s more stuff going on in living cells, nothing I&#x27;ve seen says the stuff needed to keep the cells themselves alive is directly contributing to the intelligence of the wet network in my skull.<p>It&#x27;s quite surprising that such a simple mind as an LLM is so very capable. What is all the rest of our human brain doing, when the LLMs demonstrate that a mere cubic centimetre of brain tissue could be reorganised to speak 50 languages fluently, let alone all the other things?</div><br/><div id="42828636" class="c"><input type="checkbox" id="c-42828636" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828570">parent</a><span>|</span><a href="#42828366">next</a><span>|</span><label class="collapse" for="c-42828636">[-]</label><label class="expand" for="c-42828636">[2 more]</label></div><br/><div class="children"><div class="content">This post is full of speculation. For starters, human minds do more than just language. There are some arguments (Moravec&#x27;s paradox) that posit that language is easy, while keeping the rest of the body working is the real hard problem. I&#x27;ve yet to see an humanoid robot controlled by an LLM that can tackle difficult physical problems like riding a bike or skiing in for the first time real-time, just as humans, that need to coordinate at least 5 senses (some say up to 20 but I digress) plus emotions and a train of thought to do so.<p>Plus, sure, LLMs are simpler (that only works if you don&#x27;t count the full complexity of its substrate: a lot of GPUs, interconnects, Data Centers, etc), yet they consume an insane amount of energy and matter when compared to humans. It is surprising that LLMs are so capable, but for 2000kcal a day humans are still more impressive to me.<p>People see a model dominating language and start thinking the only thing humans do is high-level language and reasoning. That may be why the brilliant minds of this decade talk about replacing white-collars, but no plate-cleaning bot in sight.<p>Metacommentary: a lot of LLM-human equality apologists seem to have little knowledge of the AI field and its history, as these themes are not new at all.</div><br/><div id="42828803" class="c"><input type="checkbox" id="c-42828803" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828636">parent</a><span>|</span><a href="#42828366">next</a><span>|</span><label class="collapse" for="c-42828803">[-]</label><label class="expand" for="c-42828803">[1 more]</label></div><br/><div class="children"><div class="content">&gt; For starters, human minds do more than just language.<p>These days, so do the AI (even though these ones are still called LLMs, which I think is now a bad name even though I continue to use the term without always being mindful of this).<p>&gt; I&#x27;ve yet to see an humanoid robot controlled by an LLM that can tackle difficult physical problems like riding a bike or skiing in for the first time real-time, just as humans, that need to coordinate at least 5 senses (some say up to 20 but I digress) plus emotions and a train of thought to do so.<p>1) I&#x27;d agree that AI are <i>slow</i> learners (as measured by number of examples rather than wall clock). For me, this is more significant than the difference in wattage.<p>2) 13 years ago, before the Transformer model: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mT3vfSQePcs" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mT3vfSQePcs</a><p>3) We don&#x27;t use 5 senses to ride a bike; I can believe touch, vision, proprioception, and balance are all involved, but that&#x27;s only 4. Why would hearing, smell, or taste be involved?<p>For emotion, my weakly-held belief is that this is what motivates us and tells us what the concept of &quot;good&quot; even is — i.e. it doesn&#x27;t give us reasoning beyond being a motivation to learn to reason.<p>&gt; It is surprising that LLMs are so capable, but for 2000kcal a day humans are still more impressive to me.<p>Our biological energy efficiency sure is nice, though progress with silicon is continuing even if the rate of improvement is decreasing: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Koomey%27s_law" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Koomey%27s_law</a><p>That said, I don&#x27;t consider the substrate efficiency to be an indication of if the thing running on it is or isn&#x27;t &quot;reasoning&quot; — if the same silicon was running a quantum chemistry simulation of the human brain it would be an even bigger power hog and yet I would say it &quot;must be&quot; reasoning, and conversely we use phrases such &quot;turn your brain off and enjoy&quot; to describe categories of film where the plot holes become pot holes if you stop to think about them (the novel I&#x27;m trying to write is due to me being nerd-sniped in this way by everything wrong with Independence Day).<p>&gt; That may be why the brilliant minds of this decade talk about replacing white-collars, but no plate-cleaning bot in sight.<p>I mean, I&#x27;ve got a dishwasher already… ;)<p>But more seriously, even though I&#x27;m getting cynical about press releases that turn out to be smoke and mirrors, (humanoid) robots doing housework is &quot;in sight&quot; in at least the same kind of way as self driving cars (which has been a decade of &quot;next year honest&quot; mirages, so I&#x27;m not giving a timeline): <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Sq1QZB5baNw" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Sq1QZB5baNw</a><p>&gt; Metacommentary: a lot of LLM-human equality apologists seem to have little knowledge of the AI field and its history, as these themes are not new at all.<p>I&#x27;ve been feeling the same, but on both sides, pro and con. The Turing paper laid out all the same talking points I&#x27;ve been seeing over the last few years, and those talking points weren&#x27;t new when he wrote them down.</div><br/></div></div></div></div></div></div></div></div><div id="42828366" class="c"><input type="checkbox" id="c-42828366" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828266">parent</a><span>|</span><a href="#42828374">prev</a><span>|</span><a href="#42828221">next</a><span>|</span><label class="collapse" for="c-42828366">[-]</label><label class="expand" for="c-42828366">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t that simple, the substrate is totally different. The brain is not a GPU, not by any stretch, even if it &quot;runs something analogue to a LLM&quot; inside.</div><br/></div></div></div></div><div id="42828221" class="c"><input type="checkbox" id="c-42828221" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828215">parent</a><span>|</span><a href="#42828266">prev</a><span>|</span><a href="#42828156">next</a><span>|</span><label class="collapse" for="c-42828221">[-]</label><label class="expand" for="c-42828221">[7 more]</label></div><br/><div class="children"><div class="content">You want to talk evidence, let&#x27;s talk evidence.  What does the brain do that these models don&#x27;t (or, more generally, can&#x27;t)?<p>Be specific.<p>And no, it doesn&#x27;t &quot;happen with every new technology.&quot;  Nothing even remotely like this has been seen before the present decade.</div><br/><div id="42828802" class="c"><input type="checkbox" id="c-42828802" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828221">parent</a><span>|</span><a href="#42828345">next</a><span>|</span><label class="collapse" for="c-42828802">[-]</label><label class="expand" for="c-42828802">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m no neuroscientist, but I hung out with a few, so here are some bits I picked up from them. In biological brians:<p>- neurons have signal surpression (-ive activation) as well as propagation (+activation)<p>- neurons appear to process and propagation due to unknown internal mechanisms.<p>- there are complex sub structures within biological neural nets where the architecture is radically different from in other sections of the network, strongly in contrast to the homogenous structure of ANN&#x27;s<p>- many different types of neurons with different properties and behaviors in terms of network formation and network activity are present in BNN&#x27;s in contrast to ANN&#x27;s<p>- BNN&#x27;s learn during processing. ANN&#x27;s are static after training.</div><br/></div></div><div id="42828345" class="c"><input type="checkbox" id="c-42828345" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828221">parent</a><span>|</span><a href="#42828802">prev</a><span>|</span><a href="#42828429">next</a><span>|</span><label class="collapse" for="c-42828345">[-]</label><label class="expand" for="c-42828345">[3 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t win this argument through logic less mere rhetoric. You need scientific evidence. Hint: There isn&#x27;t.<p>Edit: Ok, let me be less harsh and play with your argument. Can an LLM have an acid&#x2F;psilocybin&#x2F;ketamine trip, distort its view of self and reality, then rewire some of its internal connections and come a little different based on the experience? I guess not, there are more examples and they all show that as far as we know LLMs are not minds&#x2F;brains and viceversa even if they seem similar in a chat interface. (If you don&#x27;t empathise with that example  remove the drugs and change them for a near-death experience).<p>I strongly argue humans are not (just) LLMs, but I think we&#x27;re far from getting evidence*. I think we will get to AGI before we know how the brain works and there is no dicotomy in that, if anything the tech is showing us that we can get at least some intelligence in other substrates.<p>*PS: fwiw ausence of evidence does not support any sides. I shouldn&#x27;t remark that, but here we are.</div><br/><div id="42828634" class="c"><input type="checkbox" id="c-42828634" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828345">parent</a><span>|</span><a href="#42828429">next</a><span>|</span><label class="collapse" for="c-42828634">[-]</label><label class="expand" for="c-42828634">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You need scientific evidence<p>This, while true, is putting the cart before the horse.<p>One needs to define what the question is before it is possible to seek evidence.<p>I see many things different between Transformer models and brains, but I do not know which, if any, matters.<p>&gt; Can an LLM have an acid&#x2F;psilocybin&#x2F;ketamine trip, distort its view of self and reality, then rewire some of its internal connections and come a little different based on the experience?<p>Specifically those chemicals? No, not even during training when the weights aren&#x27;t frozen, as nobody bothered to simulate the relevant receptors*.<p>Can LLMs experience <i>anything</i>, in the way we mean it? Nobody even knows. <i>We don&#x27;t know what the question means well enough to more than merely guess what to look for</i>.<p>Do the inputs to an LLM, a broader idea of &quot;experience&quot; without needing to solve the question of which definition of consciousness everyone should be using, rewire some of its internal connections? All the time.<p>* caveat: it doesn&#x27;t, at first glance, seem <i>totally impossible</i> that the structure of these models is sufficiently complicated for them to create the simulation of those things in the weights themselves in order to better model the behaviour of humans generating text from experiencing those things. But I also don&#x27;t have any positive reason to expect this any more than I would expect an actor portraying a near-death-experience to have any idea what that&#x27;s like on the inside.</div><br/><div id="42828738" class="c"><input type="checkbox" id="c-42828738" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828634">parent</a><span>|</span><a href="#42828429">next</a><span>|</span><label class="collapse" for="c-42828738">[-]</label><label class="expand" for="c-42828738">[1 more]</label></div><br/><div class="children"><div class="content">The question&#x2F;argument was well defined by parent-grandparent posts: people reason as LLMs, including a suggestion that people discussing here are not being self-conscious of said similarity (i.e. people parroting&#x2F;regurgitating). You can check it above.<p>I won&#x27;t continue with these discussions as I feel people are being just obtuse about this and it is becoming more emotional than rational.</div><br/></div></div></div></div></div></div><div id="42828429" class="c"><input type="checkbox" id="c-42828429" checked=""/><div class="controls bullet"><span class="by">mkleczek</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828221">parent</a><span>|</span><a href="#42828345">prev</a><span>|</span><a href="#42828282">next</a><span>|</span><label class="collapse" for="c-42828429">[-]</label><label class="expand" for="c-42828429">[1 more]</label></div><br/><div class="children"><div class="content">The brain can invent formal language that enables unambiguously specifying an algorithm that when provided with a huge amount of input data can simulate the output of the brain itself.<p>Can these models do that?</div><br/></div></div><div id="42828282" class="c"><input type="checkbox" id="c-42828282" checked=""/><div class="controls bullet"><span class="by">mjmas</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828221">parent</a><span>|</span><a href="#42828429">prev</a><span>|</span><a href="#42828156">next</a><span>|</span><label class="collapse" for="c-42828282">[-]</label><label class="expand" for="c-42828282">[1 more]</label></div><br/><div class="children"><div class="content">Have experiences and feelings. Have personal knowledge of something.<p>Be able to express language with a training&#x2F;knowledge&#x2F;etc corpus of far smaller size.</div><br/></div></div></div></div></div></div></div></div><div id="42828156" class="c"><input type="checkbox" id="c-42828156" checked=""/><div class="controls bullet"><span class="by">hooverd</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828113">parent</a><span>|</span><a href="#42828153">prev</a><span>|</span><a href="#42828126">next</a><span>|</span><label class="collapse" for="c-42828156">[-]</label><label class="expand" for="c-42828156">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s with AI boosters and not viewing other people as human?</div><br/></div></div><div id="42828126" class="c"><input type="checkbox" id="c-42828126" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828113">parent</a><span>|</span><a href="#42828156">prev</a><span>|</span><a href="#42828089">next</a><span>|</span><label class="collapse" for="c-42828126">[-]</label><label class="expand" for="c-42828126">[2 more]</label></div><br/><div class="children"><div class="content">That sword cuts both ways.</div><br/><div id="42828197" class="c"><input type="checkbox" id="c-42828197" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828126">parent</a><span>|</span><a href="#42828089">next</a><span>|</span><label class="collapse" for="c-42828197">[-]</label><label class="expand" for="c-42828197">[1 more]</label></div><br/><div class="children"><div class="content">I mean, you talk about a predictable, deterministic next-token generator...</div><br/></div></div></div></div></div></div><div id="42828089" class="c"><input type="checkbox" id="c-42828089" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828075">parent</a><span>|</span><a href="#42828113">prev</a><span>|</span><a href="#42828217">next</a><span>|</span><label class="collapse" for="c-42828089">[-]</label><label class="expand" for="c-42828089">[9 more]</label></div><br/><div class="children"><div class="content">Oh stop. The neural network is piecing together the tokens in a way that indicates reasoning. Clearly. I don&#x27;t really need to say this, we all know it now and so do you. Your statement here is just weak.<p>It&#x27;s really embarrassing the stubborn stance people were taking that LLMs weren&#x27;t intelligent and wouldn&#x27;t make any progress towards agi. I sometimes wonder how people live with themselves when they realize their wrong outlook on things is just as canned and biased as the hallucinations of LLMs themselves.</div><br/><div id="42828104" class="c"><input type="checkbox" id="c-42828104" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828089">parent</a><span>|</span><a href="#42828170">next</a><span>|</span><label class="collapse" for="c-42828104">[-]</label><label class="expand" for="c-42828104">[3 more]</label></div><br/><div class="children"><div class="content">Personal attacks really make your argument stronger.</div><br/><div id="42828182" class="c"><input type="checkbox" id="c-42828182" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828104">parent</a><span>|</span><a href="#42828170">next</a><span>|</span><label class="collapse" for="c-42828182">[-]</label><label class="expand" for="c-42828182">[2 more]</label></div><br/><div class="children"><div class="content">I said your statement was weak I didn&#x27;t say YOU were weak. Don&#x27;t take it personally. It wasn&#x27;t meant to be that way. If you take offense then I apologize.<p>That being said, my argument was a statement about a general fact that is very true. The sentiment not too long ago was these things are just regurgitators with no similarity to human intelligence.<p>I think it&#x27;s clear now that all the previous claims were just completely baseless.</div><br/><div id="42828443" class="c"><input type="checkbox" id="c-42828443" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828182">parent</a><span>|</span><a href="#42828170">next</a><span>|</span><label class="collapse" for="c-42828443">[-]</label><label class="expand" for="c-42828443">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That being said, my argument was a statement about a general fact that is very true.<p>Is this a general fact that is very true? It sounds like you are judging rather than stating a fact.<p>&gt; It&#x27;s really embarrassing the stubborn stance people were taking that LLMs weren&#x27;t intelligent and wouldn&#x27;t make any progress towards agi. I sometimes wonder how people live with themselves when they realize their wrong outlook on things is just as canned and biased as the hallucinations of LLMs themselves.</div><br/></div></div></div></div></div></div><div id="42828170" class="c"><input type="checkbox" id="c-42828170" checked=""/><div class="controls bullet"><span class="by">cess11</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828089">parent</a><span>|</span><a href="#42828104">prev</a><span>|</span><a href="#42828217">next</a><span>|</span><label class="collapse" for="c-42828170">[-]</label><label class="expand" for="c-42828170">[5 more]</label></div><br/><div class="children"><div class="content">Reasoning is a human, social and embodied activity. TFA is about machines that output text reminiscent of the results of reasoning, but it&#x27;s obviously fake since the machine is neither human, social or embodied.<p>It&#x27;s an attempt at fixing perceived problems with the query planner in an irreversibly compressed database.</div><br/><div id="42828181" class="c"><input type="checkbox" id="c-42828181" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828170">parent</a><span>|</span><a href="#42828217">next</a><span>|</span><label class="collapse" for="c-42828181">[-]</label><label class="expand" for="c-42828181">[4 more]</label></div><br/><div class="children"><div class="content">That’s not what reason is. There’s nothing inherently human about reason.</div><br/><div id="42828449" class="c"><input type="checkbox" id="c-42828449" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828181">parent</a><span>|</span><a href="#42828252">next</a><span>|</span><label class="collapse" for="c-42828449">[-]</label><label class="expand" for="c-42828449">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m afraid it&#x27;s not so clear.  There are different perspectives on this.<p>For one, apart from humans, some animals, and now LLMs, there are but few entities that are able to apply reasoning.  It may well be that reason is something that exists universally, but empirically this sounds a bit unlikely.</div><br/></div></div><div id="42828252" class="c"><input type="checkbox" id="c-42828252" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828181">parent</a><span>|</span><a href="#42828449">prev</a><span>|</span><a href="#42828217">next</a><span>|</span><label class="collapse" for="c-42828252">[-]</label><label class="expand" for="c-42828252">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a sort of existential dread once man has created a machine that can perform on par with man himself. We don&#x27;t want to face the truth so we move the goal posts and definitions.<p>To reason is now a human behavior. We moved the goal posts so we don&#x27;t have to face the truth that AI crossed a barrier with the creation of LLMs. It doesn&#x27;t really matter, there&#x27;s no turning back now.</div><br/><div id="42828297" class="c"><input type="checkbox" id="c-42828297" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42828048">root</a><span>|</span><a href="#42828252">parent</a><span>|</span><a href="#42828217">next</a><span>|</span><label class="collapse" for="c-42828297">[-]</label><label class="expand" for="c-42828297">[1 more]</label></div><br/><div class="children"><div class="content">I think the perspective is diametrically opposite to what you’re suggesting. It’s saying things that human do are not singular or sacrosanct. It’s a full acceptance that humanity will be surpassed.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42828217" class="c"><input type="checkbox" id="c-42828217" checked=""/><div class="controls bullet"><span class="by">android521</span><span>|</span><a href="#42828048">prev</a><span>|</span><a href="#42827673">next</a><span>|</span><label class="collapse" for="c-42828217">[-]</label><label class="expand" for="c-42828217">[6 more]</label></div><br/><div class="children"><div class="content">[deleted due to controversy]</div><br/><div id="42828281" class="c"><input type="checkbox" id="c-42828281" checked=""/><div class="controls bullet"><span class="by">Stevvo</span><span>|</span><a href="#42828217">parent</a><span>|</span><a href="#42828323">next</a><span>|</span><label class="collapse" for="c-42828281">[-]</label><label class="expand" for="c-42828281">[2 more]</label></div><br/><div class="children"><div class="content">No. OpenAI never developed this method of reasoning through RL. If they had, they would have announced it.</div><br/></div></div><div id="42828323" class="c"><input type="checkbox" id="c-42828323" checked=""/><div class="controls bullet"><span class="by">govideo</span><span>|</span><a href="#42828217">parent</a><span>|</span><a href="#42828281">prev</a><span>|</span><a href="#42828285">next</a><span>|</span><label class="collapse" for="c-42828323">[-]</label><label class="expand" for="c-42828323">[1 more]</label></div><br/><div class="children"><div class="content">Did you read the paper? What do you think based on the methodology and details in the paper?<p>btw, I think this is a net major benefit for the US startup ecosystem -- from new model developers to applications.<p>Edit: Stevvo - Thanks for your info.</div><br/></div></div><div id="42828285" class="c"><input type="checkbox" id="c-42828285" checked=""/><div class="controls bullet"><span class="by">rapsey</span><span>|</span><a href="#42828217">parent</a><span>|</span><a href="#42828323">prev</a><span>|</span><a href="#42827673">next</a><span>|</span><label class="collapse" for="c-42828285">[-]</label><label class="expand" for="c-42828285">[2 more]</label></div><br/><div class="children"><div class="content">Constraints drive creativity. The US imposed constraints on China and they got creative.</div><br/></div></div></div></div><div id="42827673" class="c"><input type="checkbox" id="c-42827673" checked=""/><div class="controls bullet"><span class="by">cye131</span><span>|</span><a href="#42828217">prev</a><span>|</span><a href="#42827641">next</a><span>|</span><label class="collapse" for="c-42827673">[-]</label><label class="expand" for="c-42827673">[1 more]</label></div><br/><div class="children"><div class="content">Is it accurate to compare 8k example RL with 8k example SFT? RL with the same amount of examples would take massively more compute than the SFT version (though depending on how many rollouts they do per example).<p>RL is more data-efficient but that may not be relevant now that we can just use Deepseek-R1&#x27;s responses as the training data.</div><br/></div></div><div id="42827641" class="c"><input type="checkbox" id="c-42827641" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#42827673">prev</a><span>|</span><a href="#42828053">next</a><span>|</span><label class="collapse" for="c-42827641">[-]</label><label class="expand" for="c-42827641">[1 more]</label></div><br/><div class="children"><div class="content">This results is with disabled code execution, is this the line to reenable?  <a href="https:&#x2F;&#x2F;github.com&#x2F;hkust-nlp&#x2F;simpleRL-reason&#x2F;blob&#x2F;e37e8ef166357d913ca0a123fef2d88afe6ed88a&#x2F;eval&#x2F;math_eval.py#L327">https:&#x2F;&#x2F;github.com&#x2F;hkust-nlp&#x2F;simpleRL-reason&#x2F;blob&#x2F;e37e8ef166...</a></div><br/></div></div><div id="42828053" class="c"><input type="checkbox" id="c-42828053" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#42827641">prev</a><span>|</span><a href="#42827914">next</a><span>|</span><label class="collapse" for="c-42828053">[-]</label><label class="expand" for="c-42828053">[2 more]</label></div><br/><div class="children"><div class="content">see also <a href="https:&#x2F;&#x2F;trite-song-d6a.notion.site&#x2F;Deepseek-R1-for-Everyone-1860af77bef3806c9db5e5c2a256577d" rel="nofollow">https:&#x2F;&#x2F;trite-song-d6a.notion.site&#x2F;Deepseek-R1-for-Everyone-...</a><p>for some reason a lot of people are choosing to blog on notion</div><br/><div id="42828142" class="c"><input type="checkbox" id="c-42828142" checked=""/><div class="controls bullet"><span class="by">brandonasuncion</span><span>|</span><a href="#42828053">parent</a><span>|</span><a href="#42827914">next</a><span>|</span><label class="collapse" for="c-42828142">[-]</label><label class="expand" for="c-42828142">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, I&#x27;m welcoming this move to Notion. It&#x27;s much less cluttered than Medium.</div><br/></div></div></div></div><div id="42827914" class="c"><input type="checkbox" id="c-42827914" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#42828053">prev</a><span>|</span><a href="#42828451">next</a><span>|</span><label class="collapse" for="c-42827914">[-]</label><label class="expand" for="c-42827914">[13 more]</label></div><br/><div class="children"><div class="content">The doors on intelligence are getting blown wide open, what a time to be alive</div><br/><div id="42828073" class="c"><input type="checkbox" id="c-42828073" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42827914">parent</a><span>|</span><a href="#42828451">next</a><span>|</span><label class="collapse" for="c-42828073">[-]</label><label class="expand" for="c-42828073">[12 more]</label></div><br/><div class="children"><div class="content">Just a year ago everyone was saying LLMs aren&#x27;t intelligent and everything is regurgitation. A lot of people on HN &quot;knew&quot; this and defended this perspective vehemently. It&#x27;s quite embarrassing how wrong they are.<p>That being said, I don&#x27;t think it&#x27;s quite blown that wide open yet. But for sure the trendlines are pointing at AGI within our lifetimes.</div><br/><div id="42828111" class="c"><input type="checkbox" id="c-42828111" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828073">parent</a><span>|</span><a href="#42828237">next</a><span>|</span><label class="collapse" for="c-42828111">[-]</label><label class="expand" for="c-42828111">[4 more]</label></div><br/><div class="children"><div class="content">This idea was championed by the Stochastic Parrots paper. They assumed LLMs are just pattern learning, which doesn&#x27;t make sense.<p>- the simple-to-hard RL method recently discovered is one argument against it, the models can reason their way out on harder and harder problems<p>- zero shot translation shows the models really develop an interlingua, a semantic representation, otherwise they wouldn&#x27;t be able to translate between unseen pairs of languages<p>- the human in the room is also important. LLMs are like pianos, we play prompts on the keyboard to them, and they play back language to us. The quality and originality of the output aren&#x27;t solely inherent to the model, but are co-created in the dialogue with the prompter. It&#x27;s not just about the instrument, but also the &#x27;musician&#x27; playing it.</div><br/><div id="42828347" class="c"><input type="checkbox" id="c-42828347" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828111">parent</a><span>|</span><a href="#42828209">next</a><span>|</span><label class="collapse" for="c-42828347">[-]</label><label class="expand" for="c-42828347">[2 more]</label></div><br/><div class="children"><div class="content">&gt; - the human in the room is also important. LLMs are like pianos, we play prompts on the keyboard to them, and they play back language to us. The quality and originality of the output aren&#x27;t solely inherent to the model, but are co-created in the dialogue with the prompter. It&#x27;s not just about the instrument, but also the &#x27;musician&#x27; playing it.<p>So you agree they aren&#x27;t reasoning? Otherwise why would you need a human to do skillful prompting? Why can&#x27;t the AI just solve it on its own?</div><br/><div id="42828699" class="c"><input type="checkbox" id="c-42828699" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828347">parent</a><span>|</span><a href="#42828209">next</a><span>|</span><label class="collapse" for="c-42828699">[-]</label><label class="expand" for="c-42828699">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So you agree they aren&#x27;t reasoning? Otherwise why would you need a human to do skillful prompting? Why can&#x27;t the AI just solve it on its own?<p>That&#x27;s the argument kings used to justify looking down on peasants.<p>AI, as it is currently, has no motivation of its own; it is just as &quot;happy&quot; to solve a business problem as it is to write Chinese poetry about Swiss software engineers taking their dogs on a ride up the Adliswil-Felsenegg cable car.<p>Good communication skills are still important to get what you want, otherwise we merely descent into a particular SMBC comic: <a href="https:&#x2F;&#x2F;www.smbc-comics.com&#x2F;index.php?id=3576" rel="nofollow">https:&#x2F;&#x2F;www.smbc-comics.com&#x2F;index.php?id=3576</a><p>(This isn&#x27;t to say the AI we have now are &quot;people&quot;, nobody knows how to even test for that yet; I hope we figure this question out some time soon, preferably before we need to know the answer…)</div><br/></div></div></div></div><div id="42828209" class="c"><input type="checkbox" id="c-42828209" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828111">parent</a><span>|</span><a href="#42828347">prev</a><span>|</span><a href="#42828237">next</a><span>|</span><label class="collapse" for="c-42828209">[-]</label><label class="expand" for="c-42828209">[1 more]</label></div><br/><div class="children"><div class="content">&gt;- the human in the room is also important. LLMs are like pianos, we play prompts on the keyboard to them, and they play back language to us. The quality and originality of the output aren&#x27;t solely inherent to the model, but are co-created in the dialogue with the prompter. It&#x27;s not just about the instrument, but also the &#x27;musician&#x27; playing it.<p>I mean isn&#x27;t that what humans are as well? You can get two LLMs talking to each other and suddenly the musician in the room doesn&#x27;t matter.</div><br/></div></div></div></div><div id="42828237" class="c"><input type="checkbox" id="c-42828237" checked=""/><div class="controls bullet"><span class="by">suraci</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828073">parent</a><span>|</span><a href="#42828111">prev</a><span>|</span><a href="#42828097">next</a><span>|</span><label class="collapse" for="c-42828237">[-]</label><label class="expand" for="c-42828237">[3 more]</label></div><br/><div class="children"><div class="content">I used to look down on ChatGPT and other LLMs because they&#x27;re crazy expensive to train and need special tuning like SFT&#x2F;RLHF. I thought their &quot;intelligence&quot; was limited and economically impractical — good for assisting but not replacing humans. At best, they might take over some customer service, translator or teaching jobs.<p>But now, with R1 out there, I&#x27;m getting a little nervous, kinda like when Go players saw AlphaGo. Sure, AlphaGo didn&#x27;t replace humans, but Go is a game, not work. 
Companies won&#x27;t hire AI to play games, but they&#x27;ll absolutely use bots to code software and build cars.</div><br/><div id="42828302" class="c"><input type="checkbox" id="c-42828302" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828237">parent</a><span>|</span><a href="#42828097">next</a><span>|</span><label class="collapse" for="c-42828302">[-]</label><label class="expand" for="c-42828302">[2 more]</label></div><br/><div class="children"><div class="content">Clearly AI at it&#x27;s current state can&#x27;t replace humans. Everyone on the face of the earth knows this.<p>What people are generally hyped about is the trendline into the future. Look at the progress of all of AI up till now. LLMs are a precursor to an AGI that exists in the future.</div><br/><div id="42828341" class="c"><input type="checkbox" id="c-42828341" checked=""/><div class="controls bullet"><span class="by">suraci</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828302">parent</a><span>|</span><a href="#42828097">next</a><span>|</span><label class="collapse" for="c-42828341">[-]</label><label class="expand" for="c-42828341">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not saying how advanced R1 is—in fact, it doesn&#x27;t outperform O1 by much. What really surprised me is how it was built with pure RL without SFT, which I thought was impossible before. And it makes me wonder if human thoughts are synthesizable<p>I hope this is just a misunderstanding stemming from my incomplete knowledge</div><br/></div></div></div></div></div></div><div id="42828097" class="c"><input type="checkbox" id="c-42828097" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828073">parent</a><span>|</span><a href="#42828237">prev</a><span>|</span><a href="#42828118">next</a><span>|</span><label class="collapse" for="c-42828097">[-]</label><label class="expand" for="c-42828097">[1 more]</label></div><br/><div class="children"><div class="content"><i>Just a year ago everyone was saying LLMs aren&#x27;t intelligent and everything is regurgitation. A lot of people on HN &quot;knew&quot; this and defended this perspective vehemently. It&#x27;s quite embarrassing how wrong they are.</i><p>Plenty of people who should know better are still saying that  on HN now.  They&#x27;re not just wrong, they evidently don&#x27;t <i>mind</i> being wrong.  So there&#x27;s not much point arguing with them.<p>You show these people a talking dog, and all they&#x27;ll do is correct its grammar.</div><br/></div></div><div id="42828118" class="c"><input type="checkbox" id="c-42828118" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828073">parent</a><span>|</span><a href="#42828097">prev</a><span>|</span><a href="#42828451">next</a><span>|</span><label class="collapse" for="c-42828118">[-]</label><label class="expand" for="c-42828118">[3 more]</label></div><br/><div class="children"><div class="content">A year ago I was a believer, and I am more of a believer now. This isnt stopping, I think we have a generalized method to learn from data. It&#x27;s just a matter of getting the data into it.</div><br/><div id="42828188" class="c"><input type="checkbox" id="c-42828188" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42827914">root</a><span>|</span><a href="#42828118">parent</a><span>|</span><a href="#42828451">next</a><span>|</span><label class="collapse" for="c-42828188">[-]</label><label class="expand" for="c-42828188">[2 more]</label></div><br/><div class="children"><div class="content">If you can strap visual, audio and touch sensors onto them and they can genuinely start improving themselves based on the that raw input, then we have a general intelligence.</div><br/></div></div></div></div></div></div></div></div><div id="42828451" class="c"><input type="checkbox" id="c-42828451" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42827914">prev</a><span>|</span><a href="#42827541">next</a><span>|</span><label class="collapse" for="c-42828451">[-]</label><label class="expand" for="c-42828451">[2 more]</label></div><br/><div class="children"><div class="content">What this means is that OpenAI can serve even cheaper models in a month using this technique for their updated models</div><br/><div id="42828582" class="c"><input type="checkbox" id="c-42828582" checked=""/><div class="controls bullet"><span class="by">bwfan123</span><span>|</span><a href="#42828451">parent</a><span>|</span><a href="#42827541">next</a><span>|</span><label class="collapse" for="c-42828582">[-]</label><label class="expand" for="c-42828582">[1 more]</label></div><br/><div class="children"><div class="content">but it also means that reasoning models are a commodity, and there goes the moat for openAI, anthropic and others. they cannot charge a premium.</div><br/></div></div></div></div></div></div></div></div></div></body></html>