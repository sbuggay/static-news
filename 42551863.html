<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1735635663378" as="style"/><link rel="stylesheet" href="styles.css?v=1735635663378"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.jerpint.io/blog/advent-of-code-llms/">Performance of LLMs on Advent of Code 2024</a> <span class="domain">(<a href="https://www.jerpint.io">www.jerpint.io</a>)</span></div><div class="subtext"><span>jerpint</span> | <span>50 comments</span></div><br/><div><div id="42555898" class="c"><input type="checkbox" id="c-42555898" checked=""/><div class="controls bullet"><span class="by">101008</span><span>|</span><a href="#42554711">next</a><span>|</span><label class="collapse" for="c-42555898">[-]</label><label class="expand" for="c-42555898">[5 more]</label></div><br/><div class="children"><div class="content">This kind of mirror my experience with LLMs. If I ask them non-original problems (make this API, write this test, update this function (that must be written 100s of time by develoeprs around the world, etc), it works very well. Some minors changes here and there but it saves time.<p>When I ask them to code things that they never heard of (I am working on a online sport game), it fails catastrophically. The LLM should know the sport, and what I ask is pretty clear for anyone who understand the game (I tested against actual people and it was obvious what to expect), but the LLM failed miserably. Even worse when I ask them to write some designs in CSS for the game. It seems if you take them outside the 3-columsn layout or bootstrap or the overused landing page, LLMs fails miserably.<p>It works very well for the known cases, but as soon as you want them to do something original, they just can&#x27;t.</div><br/><div id="42557185" class="c"><input type="checkbox" id="c-42557185" checked=""/><div class="controls bullet"><span class="by">steve_adams_86</span><span>|</span><a href="#42555898">parent</a><span>|</span><a href="#42557082">next</a><span>|</span><label class="collapse" for="c-42557185">[-]</label><label class="expand" for="c-42557185">[2 more]</label></div><br/><div class="children"><div class="content">This has made me wonder just how few developers have been doing even remotely interesting things. I encounter a lot of situations where the LLMs catastrophically fail, but I don’t think it’s for lack of trying to guide it through to solutions properly or sufficiently. I give as much or as little context as I can think to, give partial solutions or zero solutions, try different LLMs with the same prompts, etc.<p>Maybe we were mostly actually doing the exact same stuff and these things are trained on a whole lot of the same. Like with react components, these things are amazing at pumping out the most basic varieties of them.</div><br/><div id="42557319" class="c"><input type="checkbox" id="c-42557319" checked=""/><div class="controls bullet"><span class="by">uludag</span><span>|</span><a href="#42555898">root</a><span>|</span><a href="#42557185">parent</a><span>|</span><a href="#42557082">next</a><span>|</span><label class="collapse" for="c-42557319">[-]</label><label class="expand" for="c-42557319">[1 more]</label></div><br/><div class="children"><div class="content">My thoughts exactly.  I honestly think that the majority of SWE work is completely unoriginal.<p>But then there are those idiosyncrasies that every company has, a finicky build system, strange hacks to get certain things to work, extensive webs of tribal knowledge, that will be the demise for any LLM SWE application besides being a tool for skilled professionals.</div><br/></div></div></div></div><div id="42557082" class="c"><input type="checkbox" id="c-42557082" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#42555898">parent</a><span>|</span><a href="#42557185">prev</a><span>|</span><a href="#42556595">next</a><span>|</span><label class="collapse" for="c-42557082">[-]</label><label class="expand" for="c-42557082">[1 more]</label></div><br/><div class="children"><div class="content">I ask llms things like I would spec it out for my team, which is to say, when I write a task, I would not include things about the sport, I would explain the logic etc one would need to write. No one needs to know what it is about in abstract as I see the same issue with humans; they get confused when using real world things to see the connection with what they need to do. I mean I don&#x27;t flesh it out to the extend I might as well write the code, but enough that you don&#x27;t   need to know anything about the underlying subject.<p>We are doing a large EU healthcare project and things differ per country obviously; if I would assume people to have a modicum of world knowledge or interest in it and look it up, nothing would get done. It is easier to deliver excel sheets with the proper wording and formulas and leave out what it is even for.<p>Works fine with LLMs.<p>Disclaimer; the people in my company know everything about the subject matter; the people (and LLMs) that implement it are usually not ours: we just manage them and in my experience, programmers at big companies are borderline worthless, hence, in the past 35 years, I have taken to write tasks as concrete as I can; we found that otherwise we either get garbage commits or tons of meetings with questions and then garbage commits. And now that comes in handy as this works much better on LLMs too.</div><br/></div></div><div id="42556595" class="c"><input type="checkbox" id="c-42556595" checked=""/><div class="controls bullet"><span class="by">jppope</span><span>|</span><a href="#42555898">parent</a><span>|</span><a href="#42557082">prev</a><span>|</span><a href="#42554711">next</a><span>|</span><label class="collapse" for="c-42556595">[-]</label><label class="expand" for="c-42556595">[1 more]</label></div><br/><div class="children"><div class="content">completely agree with this. The problem I&#x27;m starting to run into is that I do a lot of things that have been done before so I get really efficient at doing that stuff with LLMs... and now I&#x27;m slow to solve the stuff I used to be really fast at.</div><br/></div></div></div></div><div id="42554711" class="c"><input type="checkbox" id="c-42554711" checked=""/><div class="controls bullet"><span class="by">upghost</span><span>|</span><a href="#42555898">prev</a><span>|</span><a href="#42555160">next</a><span>|</span><label class="collapse" for="c-42554711">[-]</label><label class="expand" for="c-42554711">[5 more]</label></div><br/><div class="children"><div class="content">After looking at the charts I was like &quot;Whoa, damn, that Jerpint model seems amazing.  Where do I get that??&quot;  I spent some time trying to find it on Huggingface before I realized...</div><br/><div id="42555461" class="c"><input type="checkbox" id="c-42555461" checked=""/><div class="controls bullet"><span class="by">foldl2022</span><span>|</span><a href="#42554711">parent</a><span>|</span><a href="#42554955">next</a><span>|</span><label class="collapse" for="c-42555461">[-]</label><label class="expand" for="c-42555461">[1 more]</label></div><br/><div class="children"><div class="content">Just open-wights it. We need this. :)</div><br/></div></div><div id="42554955" class="c"><input type="checkbox" id="c-42554955" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#42554711">parent</a><span>|</span><a href="#42555461">prev</a><span>|</span><a href="#42554919">next</a><span>|</span><label class="collapse" for="c-42554955">[-]</label><label class="expand" for="c-42554955">[2 more]</label></div><br/><div class="children"><div class="content">Me too.<p>The author could make a model on huggingface routing requests to him.  Latency might vary.</div><br/><div id="42555785" class="c"><input type="checkbox" id="c-42555785" checked=""/><div class="controls bullet"><span class="by">tbagman</span><span>|</span><a href="#42554711">root</a><span>|</span><a href="#42554955">parent</a><span>|</span><a href="#42554919">next</a><span>|</span><label class="collapse" for="c-42555785">[-]</label><label class="expand" for="c-42555785">[1 more]</label></div><br/><div class="children"><div class="content">The good news is that training jerprint was probably cheaper than training the latest got models...</div><br/></div></div></div></div><div id="42554919" class="c"><input type="checkbox" id="c-42554919" checked=""/><div class="controls bullet"><span class="by">senordevnyc</span><span>|</span><a href="#42554711">parent</a><span>|</span><a href="#42554955">prev</a><span>|</span><a href="#42555160">next</a><span>|</span><label class="collapse" for="c-42554919">[-]</label><label class="expand" for="c-42554919">[1 more]</label></div><br/><div class="children"><div class="content">lol, I did the same thing</div><br/></div></div></div></div><div id="42555160" class="c"><input type="checkbox" id="c-42555160" checked=""/><div class="controls bullet"><span class="by">bryan0</span><span>|</span><a href="#42554711">prev</a><span>|</span><a href="#42556131">next</a><span>|</span><label class="collapse" for="c-42555160">[-]</label><label class="expand" for="c-42555160">[3 more]</label></div><br/><div class="children"><div class="content">Since you did not give the models a chance to test their code and correct any mistakes, I think a more accurate comparison would be if you compared them against you submitting answers without testing (or even running!) your code first</div><br/><div id="42557227" class="c"><input type="checkbox" id="c-42557227" checked=""/><div class="controls bullet"><span class="by">angarg12</span><span>|</span><a href="#42555160">parent</a><span>|</span><a href="#42557397">next</a><span>|</span><label class="collapse" for="c-42557227">[-]</label><label class="expand" for="c-42557227">[1 more]</label></div><br/><div class="children"><div class="content">People keep evaluating LLMs on essentially zero-shotting a perfect solution to a coding problem.<p>Once we use tools to easily iterate on code (e.g. generate, compile, test, use outcome to refine prompt) we will turbocharge LLMs coding abilities.</div><br/></div></div><div id="42557397" class="c"><input type="checkbox" id="c-42557397" checked=""/><div class="controls bullet"><span class="by">rhubarbtree</span><span>|</span><a href="#42555160">parent</a><span>|</span><a href="#42557227">prev</a><span>|</span><a href="#42556131">next</a><span>|</span><label class="collapse" for="c-42557397">[-]</label><label class="expand" for="c-42557397">[1 more]</label></div><br/><div class="children"><div class="content">This smacks of &quot;moving the goalposts&quot; just as much as the other side is accused of when unimpressed by advances.<p>It&#x27;s a reasonable test.</div><br/></div></div></div></div><div id="42556131" class="c"><input type="checkbox" id="c-42556131" checked=""/><div class="controls bullet"><span class="by">zaptheimpaler</span><span>|</span><a href="#42555160">prev</a><span>|</span><a href="#42554681">next</a><span>|</span><label class="collapse" for="c-42556131">[-]</label><label class="expand" for="c-42556131">[3 more]</label></div><br/><div class="children"><div class="content">I’m adjacent to some people who do AoC competitively and it’s clear many of the top 10 and maybe 1&#x2F;2 of the top 100 this year were heavily LLM assisted or wholly done by LLMs in a loop. They won first place on many days. It was disappointing to the community that people cheated and went against the community’s wishes but it’s clear LLMs can do much better than described here</div><br/><div id="42557411" class="c"><input type="checkbox" id="c-42557411" checked=""/><div class="controls bullet"><span class="by">rhubarbtree</span><span>|</span><a href="#42556131">parent</a><span>|</span><a href="#42556401">next</a><span>|</span><label class="collapse" for="c-42557411">[-]</label><label class="expand" for="c-42557411">[1 more]</label></div><br/><div class="children"><div class="content">No, that means it&#x27;s clear that LLM-assisted coding can do better than described here. Which implies humans are adding a lot.</div><br/></div></div><div id="42556401" class="c"><input type="checkbox" id="c-42556401" checked=""/><div class="controls bullet"><span class="by">davidclark</span><span>|</span><a href="#42556131">parent</a><span>|</span><a href="#42557411">prev</a><span>|</span><a href="#42554681">next</a><span>|</span><label class="collapse" for="c-42556401">[-]</label><label class="expand" for="c-42556401">[1 more]</label></div><br/><div class="children"><div class="content">I’d like the same article topic but from the person who did Day 1 pt1 in 4s and pt2 in 5s (9s total time).</div><br/></div></div></div></div><div id="42554681" class="c"><input type="checkbox" id="c-42554681" checked=""/><div class="controls bullet"><span class="by">unclad5968</span><span>|</span><a href="#42556131">prev</a><span>|</span><a href="#42557415">next</a><span>|</span><label class="collapse" for="c-42554681">[-]</label><label class="expand" for="c-42554681">[3 more]</label></div><br/><div class="children"><div class="content">Half the time I try to use gemini questions about the c++ std library, it fabricates non-existent types and functions. I&#x27;m honestly impressed it was able to solve any of the AoC problems.</div><br/><div id="42555186" class="c"><input type="checkbox" id="c-42555186" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#42554681">parent</a><span>|</span><a href="#42557388">next</a><span>|</span><label class="collapse" for="c-42555186">[-]</label><label class="expand" for="c-42555186">[1 more]</label></div><br/><div class="children"><div class="content">I’ve had a side job as an external examiner for CS students for almost a decade now. While LLMs are generally terrible at programming (in my experience) they excel at passing finals. If I were to guess it’s likely a combination of the relatively simple (or perhaps isolated is a better word) tasks coupled with how many times similar problems have been solved before in the available training data. Somewhat ironically the easiest way to spot students who “cheat” is when the code is great. Being an external examiner, meaning that I have a full time job in software development I personally find it sort of silly when students aren’t allowed to use a calculator. I guess changing the way you teach and test is a massive undertaking though, so right now we just pretend LLMs aren’t being used by basically every students. Luckily I’m not part of the “spot the cheater” process, so I can just judge them based on how well they can explain their code.<p>Anyway, I’m not at all surprised that they can handle AoC. If anything I would worry that AoC will still be a fun project to author when many people solve it with AI. It sure won’t be fun to curate any form of leaderboard.</div><br/></div></div><div id="42557388" class="c"><input type="checkbox" id="c-42557388" checked=""/><div class="controls bullet"><span class="by">uludag</span><span>|</span><a href="#42554681">parent</a><span>|</span><a href="#42555186">prev</a><span>|</span><a href="#42557415">next</a><span>|</span><label class="collapse" for="c-42557388">[-]</label><label class="expand" for="c-42557388">[1 more]</label></div><br/><div class="children"><div class="content">leetoced&#x2F;AoC-like problems are probably the easiest class of software related tasks LLMs can do. Using the correct library, the correct way, at the correct version, especially if the library has some level of churn and isn&#x27;t extremely common, can be a harder task for LLMs than the hardest AoC problems.</div><br/></div></div></div></div><div id="42557415" class="c"><input type="checkbox" id="c-42557415" checked=""/><div class="controls bullet"><span class="by">demirbey05</span><span>|</span><a href="#42554681">prev</a><span>|</span><a href="#42557296">next</a><span>|</span><label class="collapse" for="c-42557415">[-]</label><label class="expand" for="c-42557415">[1 more]</label></div><br/><div class="children"><div class="content">o1 is not included, I think each benchmark should include o1 and reasoning models.  o-series is really changed the game.</div><br/></div></div><div id="42557296" class="c"><input type="checkbox" id="c-42557296" checked=""/><div class="controls bullet"><span class="by">cheevly</span><span>|</span><a href="#42557415">prev</a><span>|</span><a href="#42556663">next</a><span>|</span><label class="collapse" for="c-42557296">[-]</label><label class="expand" for="c-42557296">[2 more]</label></div><br/><div class="children"><div class="content">Genuinely terrible prompt. Not only in structure, but also contains grammatical errors. I&#x27;m confident you could at least double their score if you improve your prompting significantly.</div><br/><div id="42557402" class="c"><input type="checkbox" id="c-42557402" checked=""/><div class="controls bullet"><span class="by">rhubarbtree</span><span>|</span><a href="#42557296">parent</a><span>|</span><a href="#42556663">next</a><span>|</span><label class="collapse" for="c-42557402">[-]</label><label class="expand" for="c-42557402">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t very constructive criticism. You could improve it through a number of levels:<p>1. You could have pointed out the grammatical errors and explained why they matter.<p>2. You could have pointed out the structural errors, and explained what structure should have been used.<p>3. You could have written a new prompt.<p>4. You could have re-run the experiment with the new prompt.<p>Otherwise what you say is just unsubstantiated criticism.</div><br/></div></div></div></div><div id="42556663" class="c"><input type="checkbox" id="c-42556663" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#42557296">prev</a><span>|</span><a href="#42554387">next</a><span>|</span><label class="collapse" for="c-42556663">[-]</label><label class="expand" for="c-42556663">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a bit of an AI skeptic, and i think i had the opposite reaction of the author. Even though this is far from welcoming our AI overlords, I am surprised that they are this good.</div><br/></div></div><div id="42554387" class="c"><input type="checkbox" id="c-42554387" checked=""/><div class="controls bullet"><span class="by">yunwal</span><span>|</span><a href="#42556663">prev</a><span>|</span><a href="#42554084">next</a><span>|</span><label class="collapse" for="c-42554387">[-]</label><label class="expand" for="c-42554387">[6 more]</label></div><br/><div class="children"><div class="content">With no prompt engineering this seems like a weird comparison. I wouldn’t expect anyone to be able to one-shot most of the AOC problems. A fair fight would at least use something like cursor’s agent on YOLO mode that can review a command’s output, add logs, etc</div><br/><div id="42557421" class="c"><input type="checkbox" id="c-42557421" checked=""/><div class="controls bullet"><span class="by">rhubarbtree</span><span>|</span><a href="#42554387">parent</a><span>|</span><a href="#42555007">next</a><span>|</span><label class="collapse" for="c-42557421">[-]</label><label class="expand" for="c-42557421">[1 more]</label></div><br/><div class="children"><div class="content">Seems reasonable to me. More people use ChatGPT than cursor, so use it like ChatGPT.<p>For most coding problems, people won&#x27;t magically know if the code is &quot;correct&quot; or not, so any one-shot answer that is wrong could be a real hindrance.<p>I don&#x27;t have time to prompt engineer for every bit of code. I need tools that accelerate my work, not tools that absorb my time.</div><br/></div></div><div id="42555007" class="c"><input type="checkbox" id="c-42555007" checked=""/><div class="controls bullet"><span class="by">fumeux_fume</span><span>|</span><a href="#42554387">parent</a><span>|</span><a href="#42557421">prev</a><span>|</span><a href="#42555392">next</a><span>|</span><label class="collapse" for="c-42555007">[-]</label><label class="expand" for="c-42555007">[1 more]</label></div><br/><div class="children"><div class="content">I certainly don&#x27;t think it&#x27;s weird to measure one-shot performance on the AOC. Sure, more could be done. More can always be done, but this is still useful and interesting.</div><br/></div></div><div id="42555392" class="c"><input type="checkbox" id="c-42555392" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#42554387">parent</a><span>|</span><a href="#42555007">prev</a><span>|</span><a href="#42554815">next</a><span>|</span><label class="collapse" for="c-42555392">[-]</label><label class="expand" for="c-42555392">[1 more]</label></div><br/><div class="children"><div class="content">I did zero shot 27 solutions successfully with local model code-qwen2.5-32b.   I think adding sonnet or latest gemini will probably get me to 40.</div><br/></div></div><div id="42554815" class="c"><input type="checkbox" id="c-42554815" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42554387">parent</a><span>|</span><a href="#42555392">prev</a><span>|</span><a href="#42555181">next</a><span>|</span><label class="collapse" for="c-42554815">[-]</label><label class="expand" for="c-42554815">[1 more]</label></div><br/><div class="children"><div class="content">If you immediately know the candlelight is fire, then the meal was cooked a long time ago.<p>And so it is with success of LLMs in one-shot challenges and any job that depends on such challenges: cooked a long time ago.</div><br/></div></div></div></div><div id="42554084" class="c"><input type="checkbox" id="c-42554084" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#42554387">prev</a><span>|</span><a href="#42556111">next</a><span>|</span><label class="collapse" for="c-42554084">[-]</label><label class="expand" for="c-42554084">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be interested to know how o1 compares. On may days after I completed the AoC puzzles I was putting them question into o1 and it seemed to do really well.</div><br/><div id="42554411" class="c"><input type="checkbox" id="c-42554411" checked=""/><div class="controls bullet"><span class="by">qsort</span><span>|</span><a href="#42554084">parent</a><span>|</span><a href="#42556111">next</a><span>|</span><label class="collapse" for="c-42554411">[-]</label><label class="expand" for="c-42554411">[2 more]</label></div><br/><div class="children"><div class="content">According to this thread: <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;adventofcode&#x2F;comments&#x2F;1hnk1c5&#x2F;results_of_a_multiyear_llm_experiment&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;adventofcode&#x2F;comments&#x2F;1hnk1c5&#x2F;resul...</a><p>o1 got 20 out of 25 (or 19 out of 24, depending on how you want to count). Unclear experimental setup (it&#x27;s not obvious how much it was prompted), but it seems to check out with leaderboard times, where problems solvable with LLMs had clear times flat out impossible for humans.<p>An agent-type setup using Claude got 14 out of 25 (or, again, 13&#x2F;24)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;JasonSteving99&#x2F;agent-of-code&#x2F;tree&#x2F;main">https:&#x2F;&#x2F;github.com&#x2F;JasonSteving99&#x2F;agent-of-code&#x2F;tree&#x2F;main</a></div><br/><div id="42555623" class="c"><input type="checkbox" id="c-42555623" checked=""/><div class="controls bullet"><span class="by">joseneca</span><span>|</span><a href="#42554084">root</a><span>|</span><a href="#42554411">parent</a><span>|</span><a href="#42556111">next</a><span>|</span><label class="collapse" for="c-42555623">[-]</label><label class="expand" for="c-42555623">[1 more]</label></div><br/><div class="children"><div class="content">I have to wonder why o1 didn&#x27;t work. That post is unfortunately light on details that seem pretty important.</div><br/></div></div></div></div></div></div><div id="42556111" class="c"><input type="checkbox" id="c-42556111" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#42554084">prev</a><span>|</span><a href="#42554285">next</a><span>|</span><label class="collapse" for="c-42556111">[-]</label><label class="expand" for="c-42556111">[2 more]</label></div><br/><div class="children"><div class="content">Wanted to try with o1 and o1-mini but looks like there&#x27;s no code available, although I guess I could just ask 3.5 Sonnet&#x2F;o1 to make the evaluation suite ;)</div><br/><div id="42556469" class="c"><input type="checkbox" id="c-42556469" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#42556111">parent</a><span>|</span><a href="#42554285">next</a><span>|</span><label class="collapse" for="c-42556469">[-]</label><label class="expand" for="c-42556469">[1 more]</label></div><br/><div class="children"><div class="content">Author here: all the code to reproduce this is actually all on the huggingface space here [1]<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;jerpint&#x2F;advent24-llm&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;jerpint&#x2F;advent24-llm&#x2F;tree&#x2F;main</a></div><br/></div></div></div></div><div id="42554285" class="c"><input type="checkbox" id="c-42554285" checked=""/><div class="controls bullet"><span class="by">grumple</span><span>|</span><a href="#42556111">prev</a><span>|</span><a href="#42554731">next</a><span>|</span><label class="collapse" for="c-42554285">[-]</label><label class="expand" for="c-42554285">[1 more]</label></div><br/><div class="children"><div class="content">I’m both surprised and not surprised. I’m surprised because these sort of problems with very clear prompts and fairly clear algorithmic requirements are exactly what I’d expect LLMs to perform best at.<p>But I’m not surprised because I’ve seen them fail on many problems even with lots of prompt engineering and test cases.</div><br/></div></div><div id="42554731" class="c"><input type="checkbox" id="c-42554731" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42554285">prev</a><span>|</span><a href="#42555512">next</a><span>|</span><label class="collapse" for="c-42554731">[-]</label><label class="expand" for="c-42554731">[2 more]</label></div><br/><div class="children"><div class="content">At first I was like &quot;What is this jerpint model that&#x27;s beating the competition so soundly?&quot; then it hit me, lol.<p>Anyhow this is like night and day compared to last year, and it&#x27;s impressive that Sonnet is now apparently 50% as good as a professional human at this sort of thing.</div><br/><div id="42557204" class="c"><input type="checkbox" id="c-42557204" checked=""/><div class="controls bullet"><span class="by">zkry</span><span>|</span><a href="#42554731">parent</a><span>|</span><a href="#42555512">next</a><span>|</span><label class="collapse" for="c-42557204">[-]</label><label class="expand" for="c-42557204">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think comparing star counts would be a good measure though, as with AOC 90% of the effort and difficulty goes into the harder problems towards the end and it was the beginning, easy problems where the bulk of the sonnet&#x27;s stars came from.</div><br/></div></div></div></div><div id="42555512" class="c"><input type="checkbox" id="c-42555512" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42554731">prev</a><span>|</span><a href="#42554185">next</a><span>|</span><label class="collapse" for="c-42555512">[-]</label><label class="expand" for="c-42555512">[1 more]</label></div><br/><div class="children"><div class="content">I think a major mistake was giving parts 1 and 2 all at once. I had great results having it solve 1, then 2. I think I got 4o to one shot parts 1 then 2 up to about day 12. It started to struggle a bit after that and I got bored with it at day 18. It did way better than I expected, I don&#x27;t understand why the author is disappointed. This shit is magic.</div><br/></div></div><div id="42554185" class="c"><input type="checkbox" id="c-42554185" checked=""/><div class="controls bullet"><span class="by">johnea</span><span>|</span><a href="#42555512">prev</a><span>|</span><a href="#42554996">next</a><span>|</span><label class="collapse" for="c-42554185">[-]</label><label class="expand" for="c-42554185">[2 more]</label></div><br/><div class="children"><div class="content">LLMs are writing code for the coming of the lil&#x27; baby jesus?</div><br/><div id="42554424" class="c"><input type="checkbox" id="c-42554424" checked=""/><div class="controls bullet"><span class="by">valbaca</span><span>|</span><a href="#42554185">parent</a><span>|</span><a href="#42554996">next</a><span>|</span><label class="collapse" for="c-42554424">[-]</label><label class="expand" for="c-42554424">[1 more]</label></div><br/><div class="children"><div class="content">adventofcode.com</div><br/></div></div></div></div><div id="42554996" class="c"><input type="checkbox" id="c-42554996" checked=""/><div class="controls bullet"><span class="by">BugsJustFindMe</span><span>|</span><a href="#42554185">prev</a><span>|</span><label class="collapse" for="c-42554996">[-]</label><label class="expand" for="c-42554996">[9 more]</label></div><br/><div class="children"><div class="content">I think this is a terrible analysis with a weak conclusion.<p>There&#x27;s zero mention of how long it took the LLM to write the code vs the human. You have a 300 second runtime limit, but what was your coding time limit? The machine spat out code in, what, a few seconds? And how long did your solutions take to write?<p>Advent of code problems take me longer to just <i>read</i> than it takes an LLM to have a proposed solution ready for evaluation.<p>&gt; <i>they didn’t perform nearly as well as I’d expect</i><p>Is this a joke, though? A machine takes a problem description written as floridly hyperventilated as advent problems are, and, without any opportunity for automated reanalysis, it understands the exact problem domain, it understands exactly what&#x27;s being asked, correctly models the solution, and spits out a correct single-shot solution on 20 of them in no time flat, often with substantially better running time than your own solutions, and that&#x27;s disappointing?<p>&gt; <i>a lot of the submissions had timeout errors, which means that their solutions might work if asked more explicitly for efficient solutions. However the models should know very well what AoC solutions entail</i><p>You made up an arbitrary runtime limit and then kept that limit a secret, and you were surprised when the solutions didn&#x27;t adhere to the secret limit?<p>&gt; <i>Finally, some of the submissions raised some Exceptions, which would likely be fixed with a human reviewing this code and asking for changes.</i><p>How many of your solutions got the correct answer on the first try without going back and fixing something?</div><br/><div id="42555069" class="c"><input type="checkbox" id="c-42555069" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#42554996">parent</a><span>|</span><a href="#42555410">next</a><span>|</span><label class="collapse" for="c-42555069">[-]</label><label class="expand" for="c-42555069">[6 more]</label></div><br/><div class="children"><div class="content">You raise some good points about the &quot;total of hours spent&quot; but I guess you don&#x27;t consider training time included. Also there is no need to quote the author&#x27;s post and have a go at him personally. There are better ways to get your point across by arguing the point made and not the sentences written.</div><br/><div id="42555085" class="c"><input type="checkbox" id="c-42555085" checked=""/><div class="controls bullet"><span class="by">BugsJustFindMe</span><span>|</span><a href="#42554996">root</a><span>|</span><a href="#42555069">parent</a><span>|</span><a href="#42555410">next</a><span>|</span><label class="collapse" for="c-42555085">[-]</label><label class="expand" for="c-42555085">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I guess you don&#x27;t consider training time included</i><p>In the same way that I don&#x27;t consider the time an author spends writing a book when saying how long it took me to read the book. OP lost zero time training ChatGPT.<p>Or do you mean how much time OP spent training themself? Because that&#x27;s a whole new can of worms. How many years should we add to OP&#x27;s development times because probably they learned how to write code before this exercise?</div><br/><div id="42555156" class="c"><input type="checkbox" id="c-42555156" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#42554996">root</a><span>|</span><a href="#42555085">parent</a><span>|</span><a href="#42555410">next</a><span>|</span><label class="collapse" for="c-42555156">[-]</label><label class="expand" for="c-42555156">[4 more]</label></div><br/><div class="children"><div class="content">I was considering the energy used and $, R&amp;D and time spent to train those models, which is colossal compared to the developer and factor that in. Then they don&#x27;t look so impressive. For reference, I&#x27;m not anti-LLM, I use Claude and ChatGPT every day. I&#x27;m just raising those if you want to consider all facts.</div><br/><div id="42557422" class="c"><input type="checkbox" id="c-42557422" checked=""/><div class="controls bullet"><span class="by">menaerus</span><span>|</span><a href="#42554996">root</a><span>|</span><a href="#42555156">parent</a><span>|</span><a href="#42556683">next</a><span>|</span><label class="collapse" for="c-42557422">[-]</label><label class="expand" for="c-42557422">[1 more]</label></div><br/><div class="children"><div class="content">And now also consider the amount of time, school, university, practice and $$$ it took to train a software engineer. Repeat for the population of presumably ~30 million software engineers in the world. Time-wise it doesn&#x27;t scale at all when contrasted to new LLM releases that occur ~once a year per company. $$$-wise is also debatable.</div><br/></div></div><div id="42556683" class="c"><input type="checkbox" id="c-42556683" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#42554996">root</a><span>|</span><a href="#42555156">parent</a><span>|</span><a href="#42557422">prev</a><span>|</span><a href="#42555261">next</a><span>|</span><label class="collapse" for="c-42556683">[-]</label><label class="expand" for="c-42556683">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a weird comparison.<p>Would you also count the number of hours it took for someone to write the OS, the editor, etc? The programmer wouldn&#x27;t be effective without those things.</div><br/></div></div><div id="42555261" class="c"><input type="checkbox" id="c-42555261" checked=""/><div class="controls bullet"><span class="by">lgas</span><span>|</span><a href="#42554996">root</a><span>|</span><a href="#42555156">parent</a><span>|</span><a href="#42556683">prev</a><span>|</span><a href="#42555410">next</a><span>|</span><label class="collapse" for="c-42555261">[-]</label><label class="expand" for="c-42555261">[1 more]</label></div><br/><div class="children"><div class="content">That cost is amortized over all requests made against all copies of the model though.  Fortunately we don&#x27;t have to re-train the model every time we want to perform inference.</div><br/></div></div></div></div></div></div></div></div><div id="42555410" class="c"><input type="checkbox" id="c-42555410" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#42554996">parent</a><span>|</span><a href="#42555069">prev</a><span>|</span><a href="#42556949">next</a><span>|</span><label class="collapse" for="c-42555410">[-]</label><label class="expand" for="c-42555410">[1 more]</label></div><br/><div class="children"><div class="content">I did experiment with local LLM running on my machine, most solutions were generated within the time of 30-60seconds.   My overhead was really copying part 1 of the problem, generating the code, copying and pasting the code, running it, copying the input data, running it.   Entering the result, repeating for part 2 and for most of them that was about 5 minutes from start to finish.   If I automated the process, it would probably be 1 minute or less for the problems it could solve.<p>Not the OP, but I was able to zero shot 27 solutions correctly without any back and forth, and 5 more with a little bit back and forth.   Using local models.</div><br/></div></div><div id="42556949" class="c"><input type="checkbox" id="c-42556949" checked=""/><div class="controls bullet"><span class="by">mvdtnz</span><span>|</span><a href="#42554996">parent</a><span>|</span><a href="#42555410">prev</a><span>|</span><label class="collapse" for="c-42556949">[-]</label><label class="expand" for="c-42556949">[1 more]</label></div><br/><div class="children"><div class="content">&gt; it understands the exact problem domain, it understands exactly what&#x27;s being asked, correctly models the solution<p>It does not &quot;understand&quot; or &quot;model&quot; shit. Good grief you AI shills need to take a breath.</div><br/></div></div></div></div></div></div></div></div></div></body></html>