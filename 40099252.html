<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713690051550" as="style"/><link rel="stylesheet" href="styles.css?v=1713690051550"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://reasoning-tokens.ghost.io/reasoning-tokens/">Self-reasoning tokens: teaching models to think ahead</a> <span class="domain">(<a href="https://reasoning-tokens.ghost.io">reasoning-tokens.ghost.io</a>)</span></div><div class="subtext"><span>fesens</span> | <span>26 comments</span></div><br/><div><div id="40101154" class="c"><input type="checkbox" id="c-40101154" checked=""/><div class="controls bullet"><span class="by">wantsanagent</span><span>|</span><a href="#40099941">next</a><span>|</span><label class="collapse" for="c-40101154">[-]</label><label class="expand" for="c-40101154">[3 more]</label></div><br/><div class="children"><div class="content">&quot;The second token, however, duplicates the input of the first one and does not receive a gradient &quot;answer&quot; from the very next token, only from future tokens; ...&quot;<p>This formulation doesn&#x27;t make a lot of sense to me.<p>I get the motivation here but what you&#x27;re trying to implement is a working memory.<p>Because transformers have perfect retrospective memory within their context window any generation which can be done directly from input tokens will be.<p>At any given point a model might want to write to a working memory, but that does not imply that the next non-working-memory-step will supply useful information to better write to working memory in the future. The model also has to be able to <i>decide</i> when to compare the work done in working memory to the next token.<p>By allowing the model to both exempt output from gradient updates and opt back <i>in</i> to gradient updates, you create a meta-learning loop that could be quite flexible.</div><br/><div id="40101984" class="c"><input type="checkbox" id="c-40101984" checked=""/><div class="controls bullet"><span class="by">sdwr</span><span>|</span><a href="#40101154">parent</a><span>|</span><a href="#40099941">next</a><span>|</span><label class="collapse" for="c-40101984">[-]</label><label class="expand" for="c-40101984">[2 more]</label></div><br/><div class="children"><div class="content">As I understand it, this isn&#x27;t trying to implement <i>actual</i> memory in the form of a cache, but instead some kind of wishy-washy memory-lite.<p>I&#x27;m talking out of my ass here, but I feel like real memory shouldn&#x27;t be that hard to implement on top of chatGPT. Just run it twice per query, the first time as an internal query that fetches from a memory store.<p>The budgeting part would be interesting. How many tokens of the main query do you want to fill with memories? And it wouldn&#x27;t be able to meta learn how to use the system better, you&#x27;d have to update the prompt</div><br/><div id="40103310" class="c"><input type="checkbox" id="c-40103310" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40101154">root</a><span>|</span><a href="#40101984">parent</a><span>|</span><a href="#40099941">next</a><span>|</span><label class="collapse" for="c-40103310">[-]</label><label class="expand" for="c-40103310">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll call it &quot;not even wrong&quot; :P here, they&#x27;re putting it in the model, you&#x27;re describing a common bit of working with LLMs across memory &#x2F; RAG &#x2F; etc.</div><br/></div></div></div></div></div></div><div id="40099941" class="c"><input type="checkbox" id="c-40099941" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40101154">prev</a><span>|</span><a href="#40100250">next</a><span>|</span><label class="collapse" for="c-40099941">[-]</label><label class="expand" for="c-40099941">[4 more]</label></div><br/><div class="children"><div class="content">Ok so my understanding: you can have the network generate a token that can be used as input to future token generation along with each output token it generates<p>These are called reasoning tokens<p>Initial results with gpt2 are promising<p>You can generalize this to let the network decide when to generate reasoning tokens (I&#x27;m unclear on how). There were also multiple lines in the loss graph with reasoning tokens that I don&#x27;t quite understand (what&#x27;s reasoning 1 vs 3? Is it the ratio of reasoning tokens? Something else?)</div><br/><div id="40100203" class="c"><input type="checkbox" id="c-40100203" checked=""/><div class="controls bullet"><span class="by">fesens</span><span>|</span><a href="#40099941">parent</a><span>|</span><a href="#40101603">next</a><span>|</span><label class="collapse" for="c-40100203">[-]</label><label class="expand" for="c-40100203">[2 more]</label></div><br/><div class="children"><div class="content">Reasoning 1 vs. 3 is the number of reasoning tokens between each &quot;text&quot; token. The 1 reasoning token is exactly what you see in the picture explanation in the article.<p>The generalization comes from making the network predict a &lt;&quot;start reasoning token&quot;&gt; and end the sequence only when it predicts a &lt;&quot;end reasoning token&quot;&gt;. The training dataset for the upcoming experiment contains examples like:
&quot;&quot;&quot;
Q: What is 3+2?
A: 3 + 2 is equal to &lt;start reasoning&gt; &lt;reasoning&gt; ... &lt;reasoning&gt; &lt;end reasoning&gt; 5
&quot;&quot;&quot;</div><br/><div id="40102844" class="c"><input type="checkbox" id="c-40102844" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#40099941">root</a><span>|</span><a href="#40100203">parent</a><span>|</span><a href="#40101603">next</a><span>|</span><label class="collapse" for="c-40102844">[-]</label><label class="expand" for="c-40102844">[1 more]</label></div><br/><div class="children"><div class="content">Wasting two tokens on start&#x2F;end reasoning seems expensive to me (a priori)<p>I am curious what that would yield though - in some ways that would be the most fun to analyze (when does it think a lot??)<p>I would also be curious to see at what point you see diminishing returns from reasoning tokens (eg a 1:10 ratio? More?)</div><br/></div></div></div></div><div id="40101603" class="c"><input type="checkbox" id="c-40101603" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#40099941">parent</a><span>|</span><a href="#40100203">prev</a><span>|</span><a href="#40100250">next</a><span>|</span><label class="collapse" for="c-40101603">[-]</label><label class="expand" for="c-40101603">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m just speculating here since I don&#x27;t know what or where the code is but since inference is still autoregressive;<p>given [a b c] sample [d]<p>distribution of [d] could be over [reasoning token] | [vocab token]<p>then at next step you have<p>[a b c d] and each has an embedding vector associated<p>so when you go to sample [e] it&#x27;s a function of [a b c d]</div><br/></div></div></div></div><div id="40100250" class="c"><input type="checkbox" id="c-40100250" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#40099941">prev</a><span>|</span><a href="#40100919">next</a><span>|</span><label class="collapse" for="c-40100250">[-]</label><label class="expand" for="c-40100250">[5 more]</label></div><br/><div class="children"><div class="content">I have definately and frustratingly seen GPT3.5-Turbo do a bunch of anticipation in the outputs.<p>Basically it will create pre-conditions so that the final output aligns to some bias. In my specific case it was the bias to provide an answer to a question. This is noticable sometimes in chain of thought intermediate outputs. I ended up having to create some space between the entangled decisions in the chain of thought output.</div><br/><div id="40101164" class="c"><input type="checkbox" id="c-40101164" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#40100250">parent</a><span>|</span><a href="#40101658">next</a><span>|</span><label class="collapse" for="c-40101164">[-]</label><label class="expand" for="c-40101164">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  In my specific case it was the bias to provide an answer to a question<p>That seems to be a reasonably expected result of the &quot;instruction post-training&quot; finetuning with RLHF or otherwise. If for some reason you don&#x27;t want this behavior, you can avoid this by using a model version that just has the core language modeling without that finetuning, e.g. the llama models have such a version available.</div><br/><div id="40102100" class="c"><input type="checkbox" id="c-40102100" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#40100250">root</a><span>|</span><a href="#40101164">parent</a><span>|</span><a href="#40101658">next</a><span>|</span><label class="collapse" for="c-40102100">[-]</label><label class="expand" for="c-40102100">[1 more]</label></div><br/><div class="children"><div class="content">Well in this specific case, the logic I was asking the model to do was. (Highly paraphrased..)<p>1. Inventory the retrieved items.<p>2. Determine their relevance.<p>3. Pick the most relevant or if none of the retrieved items is relevant return an alternative message.<p>What the model will do is add new items into (1) if none of the retrieved items are relevant. If you add some steps between 1 and 2.. it stops doing that.</div><br/></div></div></div></div><div id="40101658" class="c"><input type="checkbox" id="c-40101658" checked=""/><div class="controls bullet"><span class="by">alt0_</span><span>|</span><a href="#40100250">parent</a><span>|</span><a href="#40101164">prev</a><span>|</span><a href="#40100919">next</a><span>|</span><label class="collapse" for="c-40101658">[-]</label><label class="expand" for="c-40101658">[2 more]</label></div><br/><div class="children"><div class="content">&gt; definately<p>relevant xkcd: <a href="https:&#x2F;&#x2F;xkcd.com&#x2F;2871&#x2F;" rel="nofollow">https:&#x2F;&#x2F;xkcd.com&#x2F;2871&#x2F;</a></div><br/><div id="40102080" class="c"><input type="checkbox" id="c-40102080" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#40100250">root</a><span>|</span><a href="#40101658">parent</a><span>|</span><a href="#40100919">next</a><span>|</span><label class="collapse" for="c-40102080">[-]</label><label class="expand" for="c-40102080">[1 more]</label></div><br/><div class="children"><div class="content">If I wanted it spelled correctly I would have run it through the LLM.</div><br/></div></div></div></div></div></div><div id="40100919" class="c"><input type="checkbox" id="c-40100919" checked=""/><div class="controls bullet"><span class="by">earslap</span><span>|</span><a href="#40100250">prev</a><span>|</span><a href="#40100341">next</a><span>|</span><label class="collapse" for="c-40100919">[-]</label><label class="expand" for="c-40100919">[3 more]</label></div><br/><div class="children"><div class="content">For the existing models is beam-search like methods hopeless due to combinatorial explosion? Are there no smart ways to improve it? Evaluating multiple futures will be slow but if it means that the model can give vastly better output, it might be a worthwhile trade-off in some cases. I feel like our standard way of sampling the output of the LLMs is a bit too simplistic and my hunch is that it should be possible to get a lot more out of them even if it means losing speed.</div><br/><div id="40101639" class="c"><input type="checkbox" id="c-40101639" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40100919">parent</a><span>|</span><a href="#40100341">next</a><span>|</span><label class="collapse" for="c-40101639">[-]</label><label class="expand" for="c-40101639">[2 more]</label></div><br/><div class="children"><div class="content">People are considering that sort of beam-search approach - this is what they call &quot;tree of thoughts&quot; - generate a branching tree of alternate continuations, then pick the best one based on some criteria.<p>This doesn&#x27;t seem an ideal approach though, since it amounts to generating a bunch of shallow responses and picking the best, rather than the preferred thinking more deeply before generating. It&#x27;s not the same as a computer chess program considering N-moves ahead where you are guaranteed that one of those move sequences really is the best one (as long as you don&#x27;t accidentally prune it out). In contrast, if you generate all possible &quot;shallow&quot; N-token responses (bunch of monkeys gibbering), there is no guarantee any of those will be the high quality response you are hoping for.<p>Really planning ahead - reasoning deeply before speaking - would seem harder to implement though, since it&#x27;d involve applying a variable number of reasoning steps (maybe looping), then determining when to stop. This also seems different from the proposed insertion of &quot;reasoning tokens&quot; since those are shallow reasoning steps (normal single pass through transformer&#x27;s layers), when it seems what is really needed is more depth of reasoning (&quot;more layers&quot;), perhaps coupled with some working memory&#x2F;tokens. Both schemes (more tokens vs more depth) are also related to the wish to use a variable amount of compute for different tasks&#x2F;inputs - less compute for simple tasks, more for hard ones.</div><br/><div id="40102598" class="c"><input type="checkbox" id="c-40102598" checked=""/><div class="controls bullet"><span class="by">earslap</span><span>|</span><a href="#40100919">root</a><span>|</span><a href="#40101639">parent</a><span>|</span><a href="#40100341">next</a><span>|</span><label class="collapse" for="c-40102598">[-]</label><label class="expand" for="c-40102598">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, I totally agree. I was inspecting the method as a stopgap solution (especially because it does not require retraining or any other special tricks) until researchers figure out &quot;planning&quot; in a broader sense. It is very inefficient otherwise, but in the meantime, is just simple sampling with a couple parameters to tune from the output softmax the best we can do? is there no low hanging fruit there?</div><br/></div></div></div></div></div></div><div id="40100341" class="c"><input type="checkbox" id="c-40100341" checked=""/><div class="controls bullet"><span class="by">jacobsimon</span><span>|</span><a href="#40100919">prev</a><span>|</span><a href="#40101280">next</a><span>|</span><label class="collapse" for="c-40100341">[-]</label><label class="expand" for="c-40100341">[6 more]</label></div><br/><div class="children"><div class="content">I’ve tried similar experiments before by asking the LLM to generate “internal” and “external” dialog, which I think is sort of the same idea at a higher level—-and might be preferable because it would allow for easy introspection vs a new set of tokens? I’m not enough of an expert to understand whether this proposal is intended more for training or inference.</div><br/><div id="40100619" class="c"><input type="checkbox" id="c-40100619" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40100341">parent</a><span>|</span><a href="#40100552">next</a><span>|</span><label class="collapse" for="c-40100619">[-]</label><label class="expand" for="c-40100619">[3 more]</label></div><br/><div class="children"><div class="content">This method is for training. They are using a stop-gradient to &#x27;shield&#x27; some tokens from contributing to prediction of the immediate next token, and thus producing a stream of tokens that are only used for longer term prediction.<p>This is a bit more low level than the usual prompt engineering approaches, and to my mind, a bit more promising. There&#x27;s more easily measurable results, and I&#x27;ve seen other context where a well placed stop-gradient does wonders...</div><br/><div id="40102199" class="c"><input type="checkbox" id="c-40102199" checked=""/><div class="controls bullet"><span class="by">lucidrains</span><span>|</span><a href="#40100341">root</a><span>|</span><a href="#40100619">parent</a><span>|</span><a href="#40100552">next</a><span>|</span><label class="collapse" for="c-40102199">[-]</label><label class="expand" for="c-40102199">[2 more]</label></div><br/><div class="children"><div class="content">yes, it is a stop gradient mask on the attention matrix, iiuc. worth trying</div><br/><div id="40102347" class="c"><input type="checkbox" id="c-40102347" checked=""/><div class="controls bullet"><span class="by">lucidrains</span><span>|</span><a href="#40100341">root</a><span>|</span><a href="#40102199">parent</a><span>|</span><a href="#40100552">next</a><span>|</span><label class="collapse" for="c-40102347">[-]</label><label class="expand" for="c-40102347">[1 more]</label></div><br/><div class="children"><div class="content">could even try it with a fraction of the attention heads, instead of introducing new tokens</div><br/></div></div></div></div></div></div><div id="40100552" class="c"><input type="checkbox" id="c-40100552" checked=""/><div class="controls bullet"><span class="by">fesens</span><span>|</span><a href="#40100341">parent</a><span>|</span><a href="#40100619">prev</a><span>|</span><a href="#40101280">next</a><span>|</span><label class="collapse" for="c-40100552">[-]</label><label class="expand" for="c-40100552">[2 more]</label></div><br/><div class="children"><div class="content">The main advantage of using a new and constant token for reasoning is that, while we would pay the full price during training, in the inference phase, we could do most, if not all, the &quot;reasoning&quot; in one shot, without having to feed one generation token at a time.</div><br/><div id="40100620" class="c"><input type="checkbox" id="c-40100620" checked=""/><div class="controls bullet"><span class="by">jacobsimon</span><span>|</span><a href="#40100341">root</a><span>|</span><a href="#40100552">parent</a><span>|</span><a href="#40101280">next</a><span>|</span><label class="collapse" for="c-40100620">[-]</label><label class="expand" for="c-40100620">[1 more]</label></div><br/><div class="children"><div class="content">Cool!</div><br/></div></div></div></div></div></div><div id="40101280" class="c"><input type="checkbox" id="c-40101280" checked=""/><div class="controls bullet"><span class="by">exploringBytes</span><span>|</span><a href="#40100341">prev</a><span>|</span><a href="#40102807">next</a><span>|</span><label class="collapse" for="c-40101280">[-]</label><label class="expand" for="c-40101280">[2 more]</label></div><br/><div class="children"><div class="content">First association was to extend the modality of text tokens to concept tokens which could be (logical) relationships. Are you aware of similar works?</div><br/><div id="40101685" class="c"><input type="checkbox" id="c-40101685" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#40101280">parent</a><span>|</span><a href="#40102807">next</a><span>|</span><label class="collapse" for="c-40101685">[-]</label><label class="expand" for="c-40101685">[1 more]</label></div><br/><div class="children"><div class="content">Not exactly the same game but you might be interested in <i>Mathematical Structure of Syntactic Merge, Marcolli, Chomsky, Berwick (2023)</i>.<p>When we speak we give a string. When we think we don&#x27;t have to use a string. But we do have to have a functionality to map something that has no single ordering to something that has an ordering (externalization) - a sentence. And vice versa we have a functionality to turn strings into things without a specific ordering (internalization) - thoughts.</div><br/></div></div></div></div><div id="40102807" class="c"><input type="checkbox" id="c-40102807" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#40101280">prev</a><span>|</span><label class="collapse" for="c-40102807">[-]</label><label class="expand" for="c-40102807">[2 more]</label></div><br/><div class="children"><div class="content">With a little engineering rigor we could do a push-down automata with semantics Girards-Reynolds constrained around polymorphism.</div><br/><div id="40103662" class="c"><input type="checkbox" id="c-40103662" checked=""/><div class="controls bullet"><span class="by">rullelito</span><span>|</span><a href="#40102807">parent</a><span>|</span><label class="collapse" for="c-40103662">[-]</label><label class="expand" for="c-40103662">[1 more]</label></div><br/><div class="children"><div class="content">Utilizing Girard-Reynolds constraints on a polymorphic push-down automata fundamentally misconstrues both computational topology and dynamic system semantics..</div><br/></div></div></div></div></div></div></div></div></div></body></html>