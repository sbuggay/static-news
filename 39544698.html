<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709197271225" as="style"/><link rel="stylesheet" href="styles.css?v=1709197271225"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://medium.com/@zhaodongyu/optimize-sgemm-on-risc-v-platform-b0098630b444">Optimize sgemm on RISC-V platform</a> <span class="domain">(<a href="https://medium.com">medium.com</a>)</span></div><div class="subtext"><span>camel-cdr</span> | <span>29 comments</span></div><br/><div><div id="39546380" class="c"><input type="checkbox" id="c-39546380" checked=""/><div class="controls bullet"><span class="by">taminka</span><span>|</span><a href="#39545424">next</a><span>|</span><label class="collapse" for="c-39546380">[-]</label><label class="expand" for="c-39546380">[5 more]</label></div><br/><div class="children"><div class="content">interesting write-up even outside of the risc-v context, bc most of the optimisations are the same for different platforms<p>one interesting difference is that memory prefetching&#x2F;preloading has such a huge difference, whereas on arm&#x2F;x86 it barely makes any difference (in my experience) that it&#x27;s not even worth doing<p>also, doesn&#x27;t allwinner d1-h have simd registers?</div><br/><div id="39546515" class="c"><input type="checkbox" id="c-39546515" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#39546380">parent</a><span>|</span><a href="#39545424">next</a><span>|</span><label class="collapse" for="c-39546515">[-]</label><label class="expand" for="c-39546515">[4 more]</label></div><br/><div class="children"><div class="content">The chips you&#x27;re used to have had decades of optimization to their automatic memory prefetching. Apples M1 etc do even better (they can speculate prefetch through a pointer which can make lots of data-structures a ton faster. Also for really high performance on x86, prefetching can often be a ~30% boost.</div><br/><div id="39546659" class="c"><input type="checkbox" id="c-39546659" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#39546380">root</a><span>|</span><a href="#39546515">parent</a><span>|</span><a href="#39546629">next</a><span>|</span><label class="collapse" for="c-39546659">[-]</label><label class="expand" for="c-39546659">[2 more]</label></div><br/><div class="children"><div class="content">&gt; M1 etc do even better (they can speculate prefetch through a pointer<p>Interesting.  Do you have a reference or more information on this?</div><br/><div id="39546667" class="c"><input type="checkbox" id="c-39546667" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#39546380">root</a><span>|</span><a href="#39546659">parent</a><span>|</span><a href="#39546629">next</a><span>|</span><label class="collapse" for="c-39546667">[-]</label><label class="expand" for="c-39546667">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe there is any official documentation on this, but <a href="https:&#x2F;&#x2F;github.com&#x2F;JuliaLang&#x2F;julia&#x2F;pull&#x2F;49430">https:&#x2F;&#x2F;github.com&#x2F;JuliaLang&#x2F;julia&#x2F;pull&#x2F;49430</a> for example added prefetching to the marking phase of a GC which saw speedups on x86, but not on M1.<p>It should be pretty easy to find a MWE that shows the behavior. Something like summing an array of int* should do it.</div><br/></div></div></div></div><div id="39546629" class="c"><input type="checkbox" id="c-39546629" checked=""/><div class="controls bullet"><span class="by">taminka</span><span>|</span><a href="#39546380">root</a><span>|</span><a href="#39546515">parent</a><span>|</span><a href="#39546659">prev</a><span>|</span><a href="#39545424">next</a><span>|</span><label class="collapse" for="c-39546629">[-]</label><label class="expand" for="c-39546629">[1 more]</label></div><br/><div class="children"><div class="content">yeah, that&#x27;s probably it<p>i&#x27;m using an m1 laptop, so my sgemm implementation relies on arm neon vector extension, and i saw literally no performance improvements when i added prefetching, which could either be a result of their speculative optimisations or an issue in my own implementation<p>but i have noticed some ppl saying that adding prefetching on x86 has little performance improvement for sgemm, bc it&#x27;s a scenario in which it&#x27;s easier to predict the next memory address that&#x27;s gonna be used</div><br/></div></div></div></div></div></div><div id="39545424" class="c"><input type="checkbox" id="c-39545424" checked=""/><div class="controls bullet"><span class="by">CaliforniaKarl</span><span>|</span><a href="#39546380">prev</a><span>|</span><a href="#39547022">next</a><span>|</span><label class="collapse" for="c-39545424">[-]</label><label class="expand" for="c-39545424">[1 more]</label></div><br/><div class="children"><div class="content">I enjoyed reading this!  Take a problem, in a controlled environment, start with the simple implementation, iterate, and compare results.</div><br/></div></div><div id="39547022" class="c"><input type="checkbox" id="c-39547022" checked=""/><div class="controls bullet"><span class="by">ThouYS</span><span>|</span><a href="#39545424">prev</a><span>|</span><a href="#39545622">next</a><span>|</span><label class="collapse" for="c-39547022">[-]</label><label class="expand" for="c-39547022">[1 more]</label></div><br/><div class="children"><div class="content">cool stuff!</div><br/></div></div><div id="39545622" class="c"><input type="checkbox" id="c-39545622" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39547022">prev</a><span>|</span><label class="collapse" for="c-39545622">[-]</label><label class="expand" for="c-39545622">[21 more]</label></div><br/><div class="children"><div class="content">this is a great tutorial on optimizing linear algebra algorithms; the animations really help a lot and are really well done<p>this sgemm progress is pretty impressive.  i assume &#x27;version 0&#x27; is one of the lower lines on the first graph?  he has a graph lower down that shows it getting under 300 megaflops, closer to 50 megaflops for most problem sizes.  getting up to 2.2 gigaflops is seriously impressive; that&#x27;s 55% of the maximum theoretical possible throughput, which is really challenging on things that aren&#x27;t vector supercomputers (which don&#x27;t have caches!)<p>with respect to the rvv assembly, it&#x27;s worth noting that the allwinner d1 being optimized for here has the pre-ratification version 0.7.1 of the risc-v v extension (rvv) which has some incompatibilities with the ratified 1.0.0.  i don&#x27;t know the instruction set well enough to know if those incompatibilities affect this code<p>probably worth archiving this link: <a href="https:&#x2F;&#x2F;github.com&#x2F;Zhao-Dongyu&#x2F;sgemm_riscv">https:&#x2F;&#x2F;github.com&#x2F;Zhao-Dongyu&#x2F;sgemm_riscv</a><p>— ⁂ —<p>vaguely relatedly, i was astounded yesterday to find that gcc 12.2.0 fails at some basic optimizations on risc-v.  (all of the following is with -Os)<p>this trivial subroutine<p><pre><code>    int sumarray(int *a, int n)
    {
      int total = 0;
      for (int i = 0; i != n; i++) total += a[i];
      return total;
    }
</code></pre>
compiles (with -Os) to a loop containing separate left shift, address-addition, counter-incrementation, and integer addition instructions, 8 instructions in all, plus a ret.  omitting initialization:<p><pre><code>    1: sext.w  a3,a5       ; it&#x27;s my fault i declared i as int instead of size_t so really 7
       bne     a1,a3,2f    ; return if i == n
       ret                              
    2: slli    a3,a5,2     ; convert i to byte offset
       add     a3,a3,a4    ; compute &amp;a[i]
       lw      a3,0(a3)    ; load a[i]
       addi    a5,a5,1     ; increment i  
       addw    a0,a0,a3    ; update total
       j       1b          ; lather, rinse, repeat
</code></pre>
(same in 13.2.0: <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;cd1qa6Yzn" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;cd1qa6Yzn</a>)<p>i can get it to compile to reasonable code (5 instructions in the loop plus a ret) by writing<p><pre><code>    int sumarray_opt(int *a, int n)
    {
      int total = 0;
      for (int *end = a + n; a != end; a++) total += *a;
      return total;
    }
</code></pre>
like the desperate, cornered animal i am<p><pre><code>    1: bne     a5,a1,2f
       ret                                 
    2: lw      a4,0(a5)                    
       addi    a5,a5,4   ; how motherfucking annoying is it that objdump calls this an add
       addw    a0,a0,a4                    
       j       1b
</code></pre>
isn&#x27;t strength-reduction something compilers know how to do already?  are modern cpus so goddamned superscalar that the difference doesn&#x27;t matter?  (i&#x27;m pretty sure less than 0.1% of the billion-plus shipped hardware risc-v cpus out in the field are superscalar.)<p>needing three separate instructions in the first version for converting to a byte offset, adding the byte index to the array base register, and loading from the address is something that could be ameliorated by hardware, but would involve some tradeoffs; the zba bit-manipulation extension to the risc-v isa contains 8 instructions for that sort of thing, including a sh2add that would reduce it to 2.  but reducing it to 0 inside the loop with strength reduction is better.<p>risc-v has a lot of talk about the great importance of macro-op fusion, but that won&#x27;t help here; it could fuse the shift and add maybe but the add isn&#x27;t going to go away until somebody applies some loop induction<p>like, optimizing code for an instruction set with literally thousands of different implementations with different performance characteristics is sort of impossible, but i dare you to find me a risc-v where the first loop runs faster than the second one.  or even the same speed<p>on arm(32) gcc compiles both subroutines to the same code, as any reasonable human being would expect, with the loop being something equivalent to:<p><pre><code>   1: cmp     r3, r1            
      bxeq    lr                
      ldr     r2, [r3], #4      
      add     r0, r0, r2        
      b       1b
</code></pre>
(same in 13.2.0: <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;qTs5vMEjs" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;qTs5vMEjs</a>)<p>that is, <i>the same version of</i> gcc <i>does</i> do the strength-reduction optimization for arm(32) (but not aarch64, but maybe that&#x27;s defensible since aarch64 is usually superscalar: <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;1dbYhEavM" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;1dbYhEavM</a>).  this is still 5 instructions because it can combine the load and increment into a single postincrement load, but the return-if-not-equal costs us two instructions instead of one.  and the actual speed of that will depend a lot more on things like macro-op fusion and branch prediction.  like, it would be easy for the bne to take a lot more than two cycles.<p>for cortex-m4 gcc also generates the same code for both versions, but of course there&#x27;s no bxeq on cortex-m4:<p><pre><code>    1: cmp     r3, r1            
       bne.n   2f
       bx      lr                
    2: ldr.w   r2, [r3], #4      
       add     r0, r2            
       b.n     1b
</code></pre>
(same in 13.2.0: <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;Wc3bv6sWa" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;Wc3bv6sWa</a>)<p>so in conclusion risc-v gcc still needs a lot of work to catch up with arm gcc, even to successfully apply broadly-applicable cross-platform optimizations that it successfully applies on arm, which surprised me a lot<p>postscript: apparently with -O3 instead of -Os gcc does a lot better on risc-v</div><br/><div id="39547028" class="c"><input type="checkbox" id="c-39547028" checked=""/><div class="controls bullet"><span class="by">camel-cdr</span><span>|</span><a href="#39545622">parent</a><span>|</span><a href="#39547329">next</a><span>|</span><label class="collapse" for="c-39547028">[-]</label><label class="expand" for="c-39547028">[2 more]</label></div><br/><div class="children"><div class="content">Porting it from rvv 0.7.1 to rvv 1.0 should be pretty much one to one. The harder part is that this and other gemm implementations often aren&#x27;t vector length agnostic, this is mostly due to gemm requiering specific knowlage about the cpu to get good performance anyways.<p>Btw, with -O3 -march=rv64gcv both gcc and clang can autovectorize both of your example functions. I&#x27;d use -march=rv64gcv_zba_zbb_zbs until profiles are supported in the compilers, then we could so something like -march=rva23.</div><br/><div id="39547092" class="c"><input type="checkbox" id="c-39547092" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39547028">parent</a><span>|</span><a href="#39547329">next</a><span>|</span><label class="collapse" for="c-39547092">[-]</label><label class="expand" for="c-39547092">[1 more]</label></div><br/><div class="children"><div class="content">thanks!<p>yeah, it would be hard to write a function that was easier to autovectorize.  but none of the risc-v hardware i have here actually supports v (or for that matter rv64)</div><br/></div></div></div></div><div id="39547329" class="c"><input type="checkbox" id="c-39547329" checked=""/><div class="controls bullet"><span class="by">jcalvinowens</span><span>|</span><a href="#39545622">parent</a><span>|</span><a href="#39547028">prev</a><span>|</span><a href="#39545693">next</a><span>|</span><label class="collapse" for="c-39547329">[-]</label><label class="expand" for="c-39547329">[1 more]</label></div><br/><div class="children"><div class="content">GCC is sort of infamously pedantic about -Os. This is my favorite example:<p><pre><code>  {0}[calvinow ~] cat test.c
  int foo(int a)
  {
        return a &#x2F; 4;
  }
  {0}[calvinow ~] gcc -c -O2 test.c -o opt.o
  {0}[calvinow ~] objdump -d opt.o 

  opt.o:     file format elf64-x86-64


  Disassembly of section .text:

  0000000000000000 &lt;foo&gt;:
   0:   85 ff                   test   %edi,%edi
   2:   8d 47 03                lea    0x3(%rdi),%eax
   5:   0f 49 c7                cmovns %edi,%eax
   8:   c1 f8 02                sar    $0x2,%eax
   b:   c3                      ret
  {0}[calvinow ~] gcc -c -Os test.c -o size.o
  {0}[calvinow ~] objdump -d size.o 

  size.o:     file format elf64-x86-64


  Disassembly of section .text:

  0000000000000000 &lt;foo&gt;:
   0:   89 f8                   mov    %edi,%eax
   2:   b9 04 00 00 00          mov    $0x4,%ecx
   7:   99                      cltd
   8:   f7 f9                   idiv   %ecx
   a:   c3                      ret
</code></pre>
...because the division saves a single byte on x86! It&#x27;s hard to believe anybody would ever want that trade.</div><br/></div></div><div id="39545693" class="c"><input type="checkbox" id="c-39545693" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#39545622">parent</a><span>|</span><a href="#39547329">prev</a><span>|</span><label class="collapse" for="c-39545693">[-]</label><label class="expand" for="c-39545693">[17 more]</label></div><br/><div class="children"><div class="content">gcc does give the 4-instr loop on all of -O1, -O2, and -O3 for the basic loop. And, to its credit, the worse loop on -Os does result in the function being 2 bytes smaller (if I&#x27;m reading it right), which is what you ask for with -Os.<p>And on -march=rv64gc_zba, the -Os version is 4 bytes smaller than the -O3 one.</div><br/><div id="39545818" class="c"><input type="checkbox" id="c-39545818" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545693">parent</a><span>|</span><a href="#39545744">next</a><span>|</span><label class="collapse" for="c-39545818">[-]</label><label class="expand" for="c-39545818">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve also seen that -Os with GCC produces significantly slower code in pretty much all use cases I&#x27;ve looked at it on (Game rendering code, optimizing shader compiler, GPU drivers). And that&#x27;s on mobile devices where you might expect to be more cache&#x2F;memory bandwidth limited.<p>I believe the general idea that -Os was often the best choice was popularized as it became the default on MacOS, which uses Clang, which may bias it&#x27;s optimization options differently. And Clang has had the -Oz option for if you <i>really</i> want to focus on total size, trading possibly lower performance, for quite a while. GCC only introduced the same relatively recently in gcc 12. So maybe it&#x27;ll change the bias of Os going forward?<p>EDIT: Though I&#x27;m not sure about the disassembler on godbolt - I tried seeing if the c++ std::accumulate function would affect code generation, and the disassembled code looks incomplete, and doesn&#x27;t match the compiler assembler output (by disabling &quot;Compile to Binary Object&quot;) <i>at all</i><p>[0] <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;5443fo75n" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;5443fo75n</a></div><br/><div id="39545968" class="c"><input type="checkbox" id="c-39545968" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545818">parent</a><span>|</span><a href="#39545947">next</a><span>|</span><label class="collapse" for="c-39545968">[-]</label><label class="expand" for="c-39545968">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t bothered to use -Os much (outside of places where performance decidedly does not matter); it seems fairly clear to me that code with like any amount of hot spots (i.e. most code) would greatly suffer.<p>I wonder how different would things be if it was more common to select optimization level per function, instead of for the whole project (gcc allows it, clang has only a thing to disable all optimizations on a function).</div><br/></div></div><div id="39545947" class="c"><input type="checkbox" id="c-39545947" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545818">parent</a><span>|</span><a href="#39545968">prev</a><span>|</span><a href="#39546564">next</a><span>|</span><label class="collapse" for="c-39545947">[-]</label><label class="expand" for="c-39545947">[2 more]</label></div><br/><div class="children"><div class="content">hey, my reply seems to have been lost!  i&#x27;ll try again<p>yes, using -Os does seem to have been my main problem!  possibly in the past gcc&#x27;s optimization wasn&#x27;t as flexible and so couldn&#x27;t trade off performance for code size as effectively<p>but what surprised me in this case was not that -Os was reducing size at the expense of speed; it&#x27;s that it was <i>failing</i> to apply a well-known, widely-applicable optimization that <i>both</i> reduces size <i>and</i> improves speed<p>getting down to the 4-instruction form of the loop does require an optimization that increases size while (usually) improving speed: transforming while (x) y; into if (x) do y; while (x);, which costs four bytes in this case (because the beq&#x2F;bne doesn&#x27;t fit into a compressed instruction; risc-v 2-byte compressed branches can only test a single register, so you only have c.beqz and c.bnez)</div><br/><div id="39545967" class="c"><input type="checkbox" id="c-39545967" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545947">parent</a><span>|</span><a href="#39546564">next</a><span>|</span><label class="collapse" for="c-39545967">[-]</label><label class="expand" for="c-39545967">[1 more]</label></div><br/><div class="children"><div class="content">You can control most of the optimization flags separately, I think most are listed under the various levels here [0], might be interesting to see what causes the difference, or if it&#x27;s one of the unspecified &quot;tuning values&quot;.<p>[0] <a href="https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;gcc&#x2F;Optimize-Options.html" rel="nofollow">https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;gcc&#x2F;Optimize-Options.html</a></div><br/></div></div></div></div><div id="39546564" class="c"><input type="checkbox" id="c-39546564" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545818">parent</a><span>|</span><a href="#39545947">prev</a><span>|</span><a href="#39545744">next</a><span>|</span><label class="collapse" for="c-39546564">[-]</label><label class="expand" for="c-39546564">[1 more]</label></div><br/><div class="children"><div class="content">interesting, i also see truncated (or corrupted?)  output with &#x27;compile to binary object&#x27; on that file.  &#x27;link to binary&#x27; gives better results</div><br/></div></div></div></div><div id="39545744" class="c"><input type="checkbox" id="c-39545744" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545693">parent</a><span>|</span><a href="#39545818">prev</a><span>|</span><label class="collapse" for="c-39545744">[-]</label><label class="expand" for="c-39545744">[11 more]</label></div><br/><div class="children"><div class="content">iiinteresting!  i can&#x27;t see how to get machine code listings on godbolt, but on my own machine (with 12.2.0) the -Os sumarray is 30 bytes, sumarray_opt is 22.  with -O3 sumarray <i>shrinks</i> to 28 bytes but its loop is indeed only 4 instructions long, and sumarray_opt is 26 bytes (with the same inner loop with slightly different register assignments).  so at least that one is smaller.  and it&#x27;s probably true that the 22-byte sumarray_opt with its 5-instruction loop is slower on most machines than the 26-byte version—but the 26-byte version would have been both smaller and faster than what -Os emitted for sumarray originally<p>with -Os (please forgive objdump&#x27;s unreadable disassembly and incorrect mnemonics):<p><pre><code>    00000000000007e8 &lt;sumarray&gt;:
     7e8:       872a                    mv      a4,a0
     7ea:       4781                    li      a5,0
     7ec:       4501                    li      a0,0
     7ee:       0007869b                sext.w  a3,a5
     7f2:       00d59363                bne     a1,a3,7f8 &lt;sumarray+0x10&gt;
     7f6:       8082                    ret
     7f8:       00279693                sll     a3,a5,0x2
     7fc:       96ba                    add     a3,a3,a4
     7fe:       4294                    lw      a3,0(a3)
     800:       0785                    add     a5,a5,1
     802:       9d35                    addw    a0,a0,a3
     804:       b7ed                    j       7ee &lt;sumarray+0x6&gt;

    0000000000000806 &lt;sumarray_opt&gt;:
     806:       058a                    sll     a1,a1,0x2
     808:       87aa                    mv      a5,a0
     80a:       95aa                    add     a1,a1,a0
     80c:       4501                    li      a0,0
     80e:       00b79363                bne     a5,a1,814 &lt;sumarray_opt+0xe&gt;
     812:       8082                    ret
     814:       4398                    lw      a4,0(a5)
     816:       0791                    add     a5,a5,4
     818:       9d39                    addw    a0,a0,a4
     81a:       bfd5                    j       80e &lt;sumarray_opt+0x8&gt;
</code></pre>
with -O3:<p><pre><code>    00000000000007f0 &lt;sumarray&gt;:
     7f0:       cd81                    beqz    a1,808 &lt;sumarray+0x18&gt;
     7f2:       058a                    sll     a1,a1,0x2
     7f4:       87aa                    mv      a5,a0
     7f6:       00b506b3                add     a3,a0,a1
     7fa:       4501                    li      a0,0
     7fc:       4398                    lw      a4,0(a5)
     7fe:       0791                    add     a5,a5,4
     800:       9d39                    addw    a0,a0,a4
     802:       fef69de3                bne     a3,a5,7fc &lt;sumarray+0xc&gt;
     806:       8082                    ret
     808:       4501                    li      a0,0
     80a:       8082                    ret

    000000000000080c &lt;sumarray_opt&gt;:
     80c:       058a                    sll     a1,a1,0x2
     80e:       87aa                    mv      a5,a0
     810:       95aa                    add     a1,a1,a0
     812:       4501                    li      a0,0
     814:       00b78863                beq     a5,a1,824 &lt;sumarray_opt+0x18&gt;
     818:       4398                    lw      a4,0(a5)
     81a:       0791                    add     a5,a5,4
     81c:       9d39                    addw    a0,a0,a4
     81e:       fef59de3                bne     a1,a5,818 &lt;sumarray_opt+0xc&gt;
     822:       8082                    ret
     824:       8082                    ret
</code></pre>
still, aside from risc-v&#x27;s unnecessarily pedagogical syntax, what&#x27;s wrong with this?<p><pre><code>              .globl sumarray
    sumarray: mv a2, a0      # compensate for abi design blunder: first argument (array base address) in return value register
              slli a1, a1, 2 # convert array size to byte count
              li a0, 0       # initialize return value to 0
              add a1, a1, a2 # compute array end
              beq a1, a2, 1f # skip loop if empty
        2:    lw a3, (a2)    # load array item from memory
              add a0, a0, a3 # add to total
              addi a2, a2, 4 # increment array pointer
              bne a1, a2, 2b # continue loop unless done
        1:    ret
</code></pre>
except for the snarky comments, isn&#x27;t this the code you&#x27;d expect from the compiler (without zba)?  i wouldn&#x27;t describe it as <i>thoroughly</i> tested, but it does seem to work.  as i count it, it&#x27;s 24 bytes and so smaller than anything except the original sumarray_opt, and it contains the same 4-instruction loop as the -O3 versions.  i guess it&#x27;s the -O3 sumarray_opt without the useless extra ret instruction, which is presumably some kind of minor bug</div><br/><div id="39545781" class="c"><input type="checkbox" id="c-39545781" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545744">parent</a><span>|</span><a href="#39545897">next</a><span>|</span><label class="collapse" for="c-39545781">[-]</label><label class="expand" for="c-39545781">[4 more]</label></div><br/><div class="children"><div class="content">Output → Compile to binary object.<p>Appears I was testing on a version with the int counter replaced with size_t (namely, <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;sTW64EEYv" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;sTW64EEYv</a> vs <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;zKqjTa5fz" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;zKqjTa5fz</a>). Regardless, indeed, your sumarray_opt one is shorter still, and with less instructions in the loop. Though beating compilers by an instruction or two is nothing particularly new, being possible often even on x86.</div><br/><div id="39545862" class="c"><input type="checkbox" id="c-39545862" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545781">parent</a><span>|</span><a href="#39545897">next</a><span>|</span><label class="collapse" for="c-39545862">[-]</label><label class="expand" for="c-39545862">[3 more]</label></div><br/><div class="children"><div class="content">i did see that option but when i selected it i got no output; maybe i have to look for the output somewhere else?<p>the thing i was surprised by was not really that i could beat it by an instruction or two by rewriting the c code, but that i could reduce the number of instructions in the inner loop by 37% with no apparent tradeoffs by hand-applying an optimization that the compiler already does apply, itself, on arm</div><br/><div id="39545883" class="c"><input type="checkbox" id="c-39545883" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545862">parent</a><span>|</span><a href="#39545897">next</a><span>|</span><label class="collapse" for="c-39545883">[-]</label><label class="expand" for="c-39545883">[2 more]</label></div><br/><div class="children"><div class="content">Seems it doesn&#x27;t like -S being present (and it makes sense as you don&#x27;t want assembly, you want a disassembled object file). In general there&#x27;s no need for -S in compiler explorer.</div><br/><div id="39545894" class="c"><input type="checkbox" id="c-39545894" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545883">parent</a><span>|</span><a href="#39545897">next</a><span>|</span><label class="collapse" for="c-39545894">[-]</label><label class="expand" for="c-39545894">[1 more]</label></div><br/><div class="children"><div class="content">aha, thanks!  this has been very edifying!  i wonder how i ended up with -S in there</div><br/></div></div></div></div></div></div></div></div><div id="39545897" class="c"><input type="checkbox" id="c-39545897" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545744">parent</a><span>|</span><a href="#39545781">prev</a><span>|</span><label class="collapse" for="c-39545897">[-]</label><label class="expand" for="c-39545897">[6 more]</label></div><br/><div class="children"><div class="content">Your custom 24-byte version is just two bytes smaller than the &quot;size_t n&quot; gcc -Os version, probably within a rounding error of what compiler developers care to achieve.</div><br/><div id="39545959" class="c"><input type="checkbox" id="c-39545959" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545897">parent</a><span>|</span><label class="collapse" for="c-39545959">[-]</label><label class="expand" for="c-39545959">[5 more]</label></div><br/><div class="children"><div class="content">yes, agreed!  it wasn&#x27;t that the code size was so important to me in the grand scheme of things, it&#x27;s that i was surprised by the particular way it happened to fail</div><br/><div id="39545992" class="c"><input type="checkbox" id="c-39545992" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545959">parent</a><span>|</span><label class="collapse" for="c-39545992">[-]</label><label class="expand" for="c-39545992">[4 more]</label></div><br/><div class="children"><div class="content">I will agree it&#x27;s slightly weird. But it seems to me it&#x27;s less that it &quot;fails&quot;, more that there&#x27;s heuristics at play other than &quot;speed up loop&quot;. Perhaps on other loops it&#x27;s more reasonable and makes a bigger difference.</div><br/><div id="39546018" class="c"><input type="checkbox" id="c-39546018" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39545992">parent</a><span>|</span><label class="collapse" for="c-39546018">[-]</label><label class="expand" for="c-39546018">[3 more]</label></div><br/><div class="children"><div class="content">well, i haven&#x27;t looked at gcc&#x27;s guts, but how would <i>not</i> strength-reducing pointer arithmetic ever make your loops smaller?  or, what other kinds of heuristics are you thinking might be in play, that happen to have that effect here?  my guess is that it&#x27;s not intentional behavior at that level</div><br/><div id="39546236" class="c"><input type="checkbox" id="c-39546236" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39546018">parent</a><span>|</span><label class="collapse" for="c-39546236">[-]</label><label class="expand" for="c-39546236">[2 more]</label></div><br/><div class="children"><div class="content">hmm, maybe you&#x27;re right. Looking back at the &quot;-Os -march=rv64gc_zba&quot; output, it does the strength reduction, even though the applicability of sh2add is the same regardless of strength reduction or not. A thing that might fit that is wanting to keep the length-0 case short, but I&#x27;d be surprised if that&#x27;s the actual reason.<p>Then there&#x27;s clang which does yet another thing of changing the loop to a decrementing one, which is smaller than all options here so far: <a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;M14rdfzq3;" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;M14rdfzq3;</a> 5-instr loop though, and it stays that way even to -O3.</div><br/><div id="39546406" class="c"><input type="checkbox" id="c-39546406" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#39545622">root</a><span>|</span><a href="#39546236">parent</a><span>|</span><label class="collapse" for="c-39546406">[-]</label><label class="expand" for="c-39546406">[1 more]</label></div><br/><div class="children"><div class="content">interesting!  only 18 bytes!  all compressed instructions!  i&#x27;d thought of doing that (in general it can be a very tricky transformation for a compiler to make safely, since it requires the loop reduction to be commutative, which of course it is in this case) but had come to the conclusion that it wouldn&#x27;t help.  incorrectly, it seems<p>in this case it&#x27;s doing a tricky thing: because it&#x27;s done the strength-reduction on the pointer arithmetic, it&#x27;s free to do whatever it wants with the loop counter—not just eliminate it, but also, say, reverse it.  that means it doesn&#x27;t have to care about the loop reduction being commutative or not<p><a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;M14rdfzq3" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;M14rdfzq3</a> has the trailing semicolon removed, for the convenience of hypothetical other readers of this thread<p>here&#x27;s my untested, hopefully uncorrupted reformatting of it:<p><pre><code>    .globl sumarray
    sumarray:   li a2, 0               # total := 0
                beqz a1, 1f            # skip loop if array empty
        2:      lw a3, (a0)            # load *a
                addw a2, a2, a3        # add it to total
                addi a1, a1, -1        # decrement n-i (previously initialized to n)
                addi a0, a0, 4         # a++
                bnez a1, 2b            # repeat loop unless n-i == 0
        1:      mv a0, a2              # move total into return value slot
                ret
</code></pre>
that&#x27;s 9 instructions, 5 inside the loop, but the only output from one instruction used as an input to another is the output of the lw; plausibly a sufficiently superscalar implementation of risc-v could run an iteration of the loop per cycle?  kind of funny instruction scheduling, though, with those two dependent instructions being right next to each other, which would stall a naïve pipeline.  macro-op fusion won&#x27;t help in this case, you really need superscalar<p>this is an instruction shorter than my version because it doesn&#x27;t need the slli or the extra instruction to compute the loop end, though it does need the extra addi in the loop.  because it&#x27;s only using beqz and bnez, its branches can all be compressed.  i think the main thing i wasn&#x27;t appreciating was how much space i was wasting on the array end computation, because that&#x27;s the kind of thing that never matters<p>the &#x27;pulp&#x27; core (now core-v) got a significant speedup on dsp loops by adding special zero-overhead looping registers, so you don&#x27;t have to waste cycles on decrementing and testing loop counters (on your two inner levels of looping, anyway); part of their claim was that in-order scalar execution took more time per instruction but less picojoules, so if you&#x27;re limited by energy consumption rather than transistor budgets or nonparallelizable algorithms, you&#x27;re better off with more, slower in-order cores. from <a href="https:&#x2F;&#x2F;docs.openhwgroup.org&#x2F;projects&#x2F;cv32e40p-user-manual&#x2F;en&#x2F;latest&#x2F;instruction_set_extensions.html#hardware-loops-operations" rel="nofollow">https:&#x2F;&#x2F;docs.openhwgroup.org&#x2F;projects&#x2F;cv32e40p-user-manual&#x2F;e...</a> i think that would make the loop look like this:<p><pre><code>                .balign 4              # must be 4-byte aligned
                .option norvc          # no compressed insns!
                cv.setup 0, 1f, a1     # loop starts on next insn
                lw a3, (a0)            # load *a
                addw a2, a2, a3        # add it to total
                addi a0, a0, 4         # a++
</code></pre>
but for larger loops the loop setup can take three instructions<p>this is also the kind of thing that the vector extensions ought to help with, of course!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>