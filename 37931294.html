<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697706062620" as="style"/><link rel="stylesheet" href="styles.css?v=1697706062620"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.adept.ai/blog/fuyu-8b">Fuyu-8B: A multimodal architecture for AI agents</a> <span class="domain">(<a href="https://www.adept.ai">www.adept.ai</a>)</span></div><div class="subtext"><span>averylamp</span> | <span>50 comments</span></div><br/><div><div id="37931547" class="c"><input type="checkbox" id="c-37931547" checked=""/><div class="controls bullet"><span class="by">tasdfqwer0897</span><span>|</span><a href="#37935241">next</a><span>|</span><label class="collapse" for="c-37931547">[-]</label><label class="expand" for="c-37931547">[20 more]</label></div><br/><div class="children"><div class="content">Hey I work at Adept and helped make this! 
Happy to answer questions.
The thing I think is especially neat&#x2F;notable is how simple you can make the model architecture while still getting good performance. 
I expect we&#x27;ll continue to see bits of these models get deleted in the next few years<p>Note that you can get the model weights on HuggingFace here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;adept&#x2F;fuyu-8b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;adept&#x2F;fuyu-8b</a></div><br/><div id="37937718" class="c"><input type="checkbox" id="c-37937718" checked=""/><div class="controls bullet"><span class="by">brianjking</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37934217">next</a><span>|</span><label class="collapse" for="c-37937718">[-]</label><label class="expand" for="c-37937718">[1 more]</label></div><br/><div class="children"><div class="content">First off, absolutely incredible work, congrats and thank you.<p>Secondly, do you anticipate Fuyu being made available for commercial access or will it remain NC?</div><br/></div></div><div id="37934217" class="c"><input type="checkbox" id="c-37934217" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37937718">prev</a><span>|</span><a href="#37936164">next</a><span>|</span><label class="collapse" for="c-37934217">[-]</label><label class="expand" for="c-37934217">[10 more]</label></div><br/><div class="children"><div class="content">What’s the situation with the license? Your blog post says you are open sourcing it, but it’s currently only available under a non-commercial license instead. Is an open source release forthcoming?</div><br/><div id="37936623" class="c"><input type="checkbox" id="c-37936623" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37934217">parent</a><span>|</span><a href="#37939760">next</a><span>|</span><label class="collapse" for="c-37936623">[-]</label><label class="expand" for="c-37936623">[1 more]</label></div><br/><div class="children"><div class="content">Yeah... in the blog post, they do explicitly mention &quot;cc-by-nc&quot;, which I find disappointing.<p>Anything that Adept is &quot;excited to see what the community builds on top of it&quot; would <i>only</i> serve Adept and no one else! What incentive does the community have to build on top of Fuyu, when the community can&#x27;t benefit from <i>its own</i> work? If Adept wants to benefit from word-of-mouth discussion of their models and from community contributions that make those models work better, as has happened dramatically with Llama 2, then they need to give the community the opportunity to benefit too.<p>Also weird: if you look at the tags on Hugging Face, you&#x27;ll see it is listed as &quot;cc&quot;. This comes from the README[0] metadata. &quot;cc&quot; is not really a license.<p>[0]: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;adept&#x2F;fuyu-8b&#x2F;blob&#x2F;main&#x2F;README.md?code=true" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;adept&#x2F;fuyu-8b&#x2F;blob&#x2F;main&#x2F;README.md?cod...</a></div><br/></div></div><div id="37939760" class="c"><input type="checkbox" id="c-37939760" checked=""/><div class="controls bullet"><span class="by">schleck8</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37934217">parent</a><span>|</span><a href="#37936623">prev</a><span>|</span><a href="#37935146">next</a><span>|</span><label class="collapse" for="c-37939760">[-]</label><label class="expand" for="c-37939760">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s open source by their definition, that is source available (open). Everyone always thinks the term open source is protected in any way while the entity that has established the commercial usage aspect is the Open Source Foundation. And noone is forced to abide by their ideology<p>FOSS meets the commercial usage requirement much better. Otherwise the term FOSS would be redundant.</div><br/></div></div><div id="37935146" class="c"><input type="checkbox" id="c-37935146" checked=""/><div class="controls bullet"><span class="by">mandelken</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37934217">parent</a><span>|</span><a href="#37939760">prev</a><span>|</span><a href="#37936164">next</a><span>|</span><label class="collapse" for="c-37935146">[-]</label><label class="expand" for="c-37935146">[7 more]</label></div><br/><div class="children"><div class="content">You can download the weights on Hugginface.<p>I believe the copyright on AI model weights in the US is not fully established, but so far it has been held that a list of numbers can not be copyrighted, so likely the same applies to model weights. Note that you don&#x27;t have to enter into an agreement with Adept to use the model.<p>Alternatively, use and download the weights in Japan that has explicitly no copyright on AI models.</div><br/><div id="37935446" class="c"><input type="checkbox" id="c-37935446" checked=""/><div class="controls bullet"><span class="by">ansk</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37935146">parent</a><span>|</span><a href="#37939738">next</a><span>|</span><label class="collapse" for="c-37935446">[-]</label><label class="expand" for="c-37935446">[5 more]</label></div><br/><div class="children"><div class="content">&gt; a list of numbers can not be copyrighted<p>Any digital object can be represented as a list of numbers (this is precisely the origin of the term <i>digit</i>al). Since there is clearly precedent for copyrighted digital objects (media, software, etc), reducing something to &quot;a list of numbers&quot; is not a useful distinction in regard to copyright law.</div><br/><div id="37935682" class="c"><input type="checkbox" id="c-37935682" checked=""/><div class="controls bullet"><span class="by">MattPalmer1086</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37935446">parent</a><span>|</span><a href="#37939738">next</a><span>|</span><label class="collapse" for="c-37935682">[-]</label><label class="expand" for="c-37935682">[4 more]</label></div><br/><div class="children"><div class="content">IANAL but as far as I remember, you can&#x27;t copyright a list of objective facts, for example a phone book containing a list of phone numbers.<p>Model weights are clearly not in that category.  Happy to be corrected if I misremember.</div><br/><div id="37937085" class="c"><input type="checkbox" id="c-37937085" checked=""/><div class="controls bullet"><span class="by">outofpaper</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37935682">parent</a><span>|</span><a href="#37939738">next</a><span>|</span><label class="collapse" for="c-37937085">[-]</label><label class="expand" for="c-37937085">[3 more]</label></div><br/><div class="children"><div class="content">Model weight are akin to markov chains and compressed data. They are direct representations of the data they where created from in the same way that markov chains are created from hidden markov chains and Zipped files are created from files.<p>Zipping a file does not grant the copyright protection of the zipped output beyond the copyright of the original file.<p>Moreover the American federal registrar has officially stated that AI generated artifacts are not eligible for copyright <a href="https:&#x2F;&#x2F;www.federalregister.gov&#x2F;documents&#x2F;2023&#x2F;03&#x2F;16&#x2F;2023-05321&#x2F;copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.federalregister.gov&#x2F;documents&#x2F;2023&#x2F;03&#x2F;16&#x2F;2023-05...</a>.</div><br/><div id="37937945" class="c"><input type="checkbox" id="c-37937945" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37937085">parent</a><span>|</span><a href="#37939738">next</a><span>|</span><label class="collapse" for="c-37937945">[-]</label><label class="expand" for="c-37937945">[2 more]</label></div><br/><div class="children"><div class="content">If you take some copyrighted data, a set of books, for example. And count words in these books and then plot a distribution of top 100 word frequencies. The copyright for that new image would belong to you.</div><br/><div id="37939683" class="c"><input type="checkbox" id="c-37939683" checked=""/><div class="controls bullet"><span class="by">MattPalmer1086</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37937945">parent</a><span>|</span><a href="#37939738">next</a><span>|</span><label class="collapse" for="c-37939683">[-]</label><label class="expand" for="c-37939683">[1 more]</label></div><br/><div class="children"><div class="content">Copyright in the specific image sure, but not the graph itself.  Someone else could do the same thing and make their own graph image.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37939738" class="c"><input type="checkbox" id="c-37939738" checked=""/><div class="controls bullet"><span class="by">schleck8</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37935146">parent</a><span>|</span><a href="#37935446">prev</a><span>|</span><a href="#37936164">next</a><span>|</span><label class="collapse" for="c-37939738">[-]</label><label class="expand" for="c-37939738">[1 more]</label></div><br/><div class="children"><div class="content">I highly doubt that any of this will hold up infront of a court. For intellectual property not just the result is important but also the creation process, and there is enough work going into the data science here</div><br/></div></div></div></div></div></div><div id="37936164" class="c"><input type="checkbox" id="c-37936164" checked=""/><div class="controls bullet"><span class="by">zan2434</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37934217">prev</a><span>|</span><a href="#37936654">next</a><span>|</span><label class="collapse" for="c-37936164">[-]</label><label class="expand" for="c-37936164">[1 more]</label></div><br/><div class="children"><div class="content">Hey! Awesome work. It seems like in theory this encoding scheme should enable the a model like this to generate images as well, by outputting image tokens, is that right?</div><br/></div></div><div id="37936654" class="c"><input type="checkbox" id="c-37936654" checked=""/><div class="controls bullet"><span class="by">abrichr</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37936164">prev</a><span>|</span><a href="#37934545">next</a><span>|</span><label class="collapse" for="c-37936654">[-]</label><label class="expand" for="c-37936654">[2 more]</label></div><br/><div class="children"><div class="content">Thank you for the release!<p>What can you tell us about this:<p>&gt; Our internal models (based on Fuyu) have extra capabilities related to our product. In particular,<p>&gt; 1. They can reliably perform OCR on high-resolution images<p>&gt; 2. They can do fine-grained localization of text and UI elements within those images<p>&gt; 3. They can answer questions about images of UIs<p>Is this just a matter of additional fine tuning, or are there architectural differences?</div><br/><div id="37938880" class="c"><input type="checkbox" id="c-37938880" checked=""/><div class="controls bullet"><span class="by">amks</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37936654">parent</a><span>|</span><a href="#37934545">next</a><span>|</span><label class="collapse" for="c-37938880">[-]</label><label class="expand" for="c-37938880">[1 more]</label></div><br/><div class="children"><div class="content">Even with experiments with just adding additional fine-tuning, we&#x27;ve seen models gain these capabilities!</div><br/></div></div></div></div><div id="37934545" class="c"><input type="checkbox" id="c-37934545" checked=""/><div class="controls bullet"><span class="by">Q6T46nT668w6i3m</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37936654">prev</a><span>|</span><a href="#37931667">next</a><span>|</span><label class="collapse" for="c-37934545">[-]</label><label class="expand" for="c-37934545">[2 more]</label></div><br/><div class="children"><div class="content">Neat idea! Are the batches encoded as tokens into the input sequence? This is something I really like about the multi-modal PALM papers since it enables the multi-modal tokens to be referenced.</div><br/><div id="37935722" class="c"><input type="checkbox" id="c-37935722" checked=""/><div class="controls bullet"><span class="by">ekelsen</span><span>|</span><a href="#37931547">root</a><span>|</span><a href="#37934545">parent</a><span>|</span><a href="#37931667">next</a><span>|</span><label class="collapse" for="c-37935722">[-]</label><label class="expand" for="c-37935722">[1 more]</label></div><br/><div class="children"><div class="content">Image patches are projected directly into an embedding that goes into the decoder Transformer.  The same thing could be done for audio.</div><br/></div></div></div></div><div id="37931667" class="c"><input type="checkbox" id="c-37931667" checked=""/><div class="controls bullet"><span class="by">saran945</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37934545">prev</a><span>|</span><a href="#37934573">next</a><span>|</span><label class="collapse" for="c-37931667">[-]</label><label class="expand" for="c-37931667">[1 more]</label></div><br/><div class="children"><div class="content">Hi, 
Will it work for html&#x2F;APP UI screenshots, Have been trained using UI screenshots ? 
Thank you</div><br/></div></div><div id="37934573" class="c"><input type="checkbox" id="c-37934573" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37931667">prev</a><span>|</span><a href="#37932699">next</a><span>|</span><label class="collapse" for="c-37934573">[-]</label><label class="expand" for="c-37934573">[1 more]</label></div><br/><div class="children"><div class="content">Do you offer paid API access to larger models?</div><br/></div></div><div id="37932699" class="c"><input type="checkbox" id="c-37932699" checked=""/><div class="controls bullet"><span class="by">acanb</span><span>|</span><a href="#37931547">parent</a><span>|</span><a href="#37934573">prev</a><span>|</span><a href="#37935241">next</a><span>|</span><label class="collapse" for="c-37932699">[-]</label><label class="expand" for="c-37932699">[1 more]</label></div><br/><div class="children"><div class="content">can you guys launch a web gradio demo until the transformers PR gets approved? i&#x27;d like to play around with the model</div><br/></div></div></div></div><div id="37935241" class="c"><input type="checkbox" id="c-37935241" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37931547">prev</a><span>|</span><a href="#37936685">next</a><span>|</span><label class="collapse" for="c-37935241">[-]</label><label class="expand" for="c-37935241">[2 more]</label></div><br/><div class="children"><div class="content">The architecture is quite compelling.  I would not have expected it to work as well as it does.  Glancing at the benchmarks it&#x27;s basically on par with other VLMs in its class, despite having no separate image encoder.<p>Is there an associated paper?  Or more specifically, details on the training dataset?  It must have been a mix of text and VLM tasks, otherwise one or the other capability would have rotted during training.  But I wonder if they trained off strictly VLM corpora, or also used plain image-text datasets like CLIP.  It would be interesting if only the former.<p>Also makes me wonder if it could be trained on something like CommonCrawl where all the images are retained and interspersed correctly throughout the text.  This model could theoretically train just fine off that, and it would unlock a whole new dataset effectively.<p>And has there been an inspection of what the model is outputting for predicted image &quot;tokens&quot;?  Is it correctly predicting projected image patches to any degree of accuracy?  And could therefore also generate images inline with text if another de-projection layer was trained?</div><br/><div id="37938643" class="c"><input type="checkbox" id="c-37938643" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#37935241">parent</a><span>|</span><a href="#37936685">next</a><span>|</span><label class="collapse" for="c-37938643">[-]</label><label class="expand" for="c-37938643">[1 more]</label></div><br/><div class="children"><div class="content">I too would like to know about the training dataset, as I just took a look at the one for LLava[0], and found out that they used a pretty big amount of BLIP auto generated captions.<p>This seemed a bit surreal to me, like trying to train an LLM with the outputs of a worse performing smaller LLM.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;haotian-liu&#x2F;LLaVA&#x2F;blob&#x2F;main&#x2F;docs&#x2F;Data.md#pretraining-dataset">https:&#x2F;&#x2F;github.com&#x2F;haotian-liu&#x2F;LLaVA&#x2F;blob&#x2F;main&#x2F;docs&#x2F;Data.md#...</a></div><br/></div></div></div></div><div id="37936685" class="c"><input type="checkbox" id="c-37936685" checked=""/><div class="controls bullet"><span class="by">abrichr</span><span>|</span><a href="#37935241">prev</a><span>|</span><a href="#37937301">next</a><span>|</span><label class="collapse" for="c-37936685">[-]</label><label class="expand" for="c-37936685">[6 more]</label></div><br/><div class="children"><div class="content">Thank you to the amazing team at Adept.ai for making this available!<p>For anyone interested in contributing to a fully open source alternative, join us at <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt">https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt</a><p>Lots of interesting work to be done, including integrating with Fuyu-8B!</div><br/><div id="37937117" class="c"><input type="checkbox" id="c-37937117" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#37936685">parent</a><span>|</span><a href="#37937301">next</a><span>|</span><label class="collapse" for="c-37937117">[-]</label><label class="expand" for="c-37937117">[5 more]</label></div><br/><div class="children"><div class="content">&quot;fully open source&quot;, but there is no license?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;blob&#x2F;30581e47fa9aecc5a4050158cd4b3a361d0f95b0&#x2F;LICENSE">https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;blob&#x2F;30581e47fa9aec...</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;issues&#x2F;246">https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;issues&#x2F;246</a><p>And Fuyu is under a non-commercial license, so there&#x27;s not much to be done with it unless someone trains a new Fuyu-architecture model from scratch.</div><br/><div id="37937348" class="c"><input type="checkbox" id="c-37937348" checked=""/><div class="controls bullet"><span class="by">abrichr</span><span>|</span><a href="#37936685">root</a><span>|</span><a href="#37937117">parent</a><span>|</span><a href="#37937301">next</a><span>|</span><label class="collapse" for="c-37937348">[-]</label><label class="expand" for="c-37937348">[4 more]</label></div><br/><div class="children"><div class="content">Thank you for pointing this out! You are correct that we have not yet decided on a license.<p>I will admit my ignorance on this topic, and I didn&#x27;t want us to rush into selecting a license that is inappropriate.<p>Which one should we choose?</div><br/><div id="37937372" class="c"><input type="checkbox" id="c-37937372" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#37936685">root</a><span>|</span><a href="#37937348">parent</a><span>|</span><a href="#37937947">next</a><span>|</span><label class="collapse" for="c-37937372">[-]</label><label class="expand" for="c-37937372">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Which one should we choose?<p>It depends a lot on what you want the license to do, so I don’t really want to say one way or another.<p>IANAL, but my understanding is that code without a license effectively has an “all rights reserved” license in the U.S., meaning that it can’t be used for anything at all — even non-commercial work.</div><br/></div></div><div id="37937947" class="c"><input type="checkbox" id="c-37937947" checked=""/><div class="controls bullet"><span class="by">webappguy</span><span>|</span><a href="#37936685">root</a><span>|</span><a href="#37937348">parent</a><span>|</span><a href="#37937372">prev</a><span>|</span><a href="#37937301">next</a><span>|</span><label class="collapse" for="c-37937947">[-]</label><label class="expand" for="c-37937947">[2 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s for the win (?), the most permissible is the one you choose. This is a extraordinarily competitive space. The sooner you make the choice and it&#x27;s MIT, the sooner I personally put forth serious contribution time and the faster you grow in the broad and competitive ecosystem. Your main options are GNU All-permissive License, MIT License, BSD licenses, Apple Public Source License and Apache license.<p>It is recommended by this developer you go MIT</div><br/><div id="37938326" class="c"><input type="checkbox" id="c-37938326" checked=""/><div class="controls bullet"><span class="by">abrichr</span><span>|</span><a href="#37936685">root</a><span>|</span><a href="#37937947">parent</a><span>|</span><a href="#37937301">next</a><span>|</span><label class="collapse" for="c-37938326">[-]</label><label class="expand" for="c-37938326">[1 more]</label></div><br/><div class="children"><div class="content">Done: <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;blob&#x2F;main&#x2F;LICENSE">https:&#x2F;&#x2F;github.com&#x2F;OpenAdaptAI&#x2F;OpenAdapt&#x2F;blob&#x2F;main&#x2F;LICENSE</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="37937301" class="c"><input type="checkbox" id="c-37937301" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#37936685">prev</a><span>|</span><a href="#37932290">next</a><span>|</span><label class="collapse" for="c-37937301">[-]</label><label class="expand" for="c-37937301">[1 more]</label></div><br/><div class="children"><div class="content">This looks so cool, and from reading the Hugging Face model card it should be easy enough to run. I do almost all of my work with text, NLP, IR, etc., and I have wanted to try multi-modal models. I just bookmarked the model card page.<p>I am also getting even more excited by the explosion of work on open models. I still haven’t adjusted to how good mistral-7B is, and it runs on my Mac without breaking a sweat.</div><br/></div></div><div id="37932290" class="c"><input type="checkbox" id="c-37932290" checked=""/><div class="controls bullet"><span class="by">thatcherc</span><span>|</span><a href="#37937301">prev</a><span>|</span><a href="#37936210">next</a><span>|</span><label class="collapse" for="c-37932290">[-]</label><label class="expand" for="c-37932290">[4 more]</label></div><br/><div class="children"><div class="content">Really cool that the image patches are converted to tokens with just a linear projection instead of a big embedding model! I wonder if that trick will prove viable for other multimodel media like audio.</div><br/><div id="37934702" class="c"><input type="checkbox" id="c-37934702" checked=""/><div class="controls bullet"><span class="by">rafaelero</span><span>|</span><a href="#37932290">parent</a><span>|</span><a href="#37936210">next</a><span>|</span><label class="collapse" for="c-37934702">[-]</label><label class="expand" for="c-37934702">[3 more]</label></div><br/><div class="children"><div class="content">Not using embeddings&#x2F;lookup table means they can&#x27;t generate image&#x2F;audio, which to me it&#x27;s a severe limitation. Why bother going to the process of generating a multimodal transformer if it&#x27;s able to generate nothing but text?</div><br/><div id="37935095" class="c"><input type="checkbox" id="c-37935095" checked=""/><div class="controls bullet"><span class="by">leodriesch</span><span>|</span><a href="#37932290">root</a><span>|</span><a href="#37934702">parent</a><span>|</span><a href="#37935168">next</a><span>|</span><label class="collapse" for="c-37935095">[-]</label><label class="expand" for="c-37935095">[1 more]</label></div><br/><div class="children"><div class="content">For an AI agent that should navigate a computer (which is Adepts use case IIRC) it should work, as it only has to output commands.</div><br/></div></div><div id="37935168" class="c"><input type="checkbox" id="c-37935168" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#37932290">root</a><span>|</span><a href="#37934702">parent</a><span>|</span><a href="#37935095">prev</a><span>|</span><a href="#37936210">next</a><span>|</span><label class="collapse" for="c-37935168">[-]</label><label class="expand" for="c-37935168">[1 more]</label></div><br/><div class="children"><div class="content">Many applications only need input, not output.</div><br/></div></div></div></div></div></div><div id="37936210" class="c"><input type="checkbox" id="c-37936210" checked=""/><div class="controls bullet"><span class="by">yeldarb</span><span>|</span><a href="#37932290">prev</a><span>|</span><a href="#37936488">next</a><span>|</span><label class="collapse" for="c-37936210">[-]</label><label class="expand" for="c-37936210">[1 more]</label></div><br/><div class="children"><div class="content">This looks epic. Definitely going to explore adding it to Autodistill[1] this weekend. Any chance you&#x27;ll be publicly releasing the internal OCR finetune?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;autodistill&#x2F;autodistill">https:&#x2F;&#x2F;github.com&#x2F;autodistill&#x2F;autodistill</a></div><br/></div></div><div id="37936488" class="c"><input type="checkbox" id="c-37936488" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#37936210">prev</a><span>|</span><a href="#37936945">next</a><span>|</span><label class="collapse" for="c-37936488">[-]</label><label class="expand" for="c-37936488">[1 more]</label></div><br/><div class="children"><div class="content">This looks great! Is there any software that supports these? Llama.cpp, Ollama, LM studio, etc are really convenient, but I don&#x27;t think they have image support yet?</div><br/></div></div><div id="37936945" class="c"><input type="checkbox" id="c-37936945" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#37936488">prev</a><span>|</span><a href="#37936217">next</a><span>|</span><label class="collapse" for="c-37936945">[-]</label><label class="expand" for="c-37936945">[1 more]</label></div><br/><div class="children"><div class="content">Why don‘t these benchmarks judge the likelihood of the example answer? Just taking the MAP predictions seems like a waste of information</div><br/></div></div><div id="37936217" class="c"><input type="checkbox" id="c-37936217" checked=""/><div class="controls bullet"><span class="by">paulkon</span><span>|</span><a href="#37936945">prev</a><span>|</span><a href="#37935210">next</a><span>|</span><label class="collapse" for="c-37936217">[-]</label><label class="expand" for="c-37936217">[1 more]</label></div><br/><div class="children"><div class="content">Can this be used to click around in the browser with text prompts? Maybe after some fine-tuning on screen recordings of specific workflows in browsers.</div><br/></div></div><div id="37935210" class="c"><input type="checkbox" id="c-37935210" checked=""/><div class="controls bullet"><span class="by">devinprater</span><span>|</span><a href="#37936217">prev</a><span>|</span><a href="#37935731">next</a><span>|</span><label class="collapse" for="c-37935210">[-]</label><label class="expand" for="c-37935210">[1 more]</label></div><br/><div class="children"><div class="content">Awesome! I can&#x27;t wait to see how we can make local models for, say, describing images offline, or even getting a few screenshots of, say, a video game and describing what&#x27;s going on.</div><br/></div></div><div id="37935731" class="c"><input type="checkbox" id="c-37935731" checked=""/><div class="controls bullet"><span class="by">thefcpk</span><span>|</span><a href="#37935210">prev</a><span>|</span><a href="#37937269">next</a><span>|</span><label class="collapse" for="c-37935731">[-]</label><label class="expand" for="c-37935731">[3 more]</label></div><br/><div class="children"><div class="content">One thing that puzzles me is the lack of multilingual models... it is a bit sad to see everything through the English language.</div><br/><div id="37935988" class="c"><input type="checkbox" id="c-37935988" checked=""/><div class="controls bullet"><span class="by">snats</span><span>|</span><a href="#37935731">parent</a><span>|</span><a href="#37935901">next</a><span>|</span><label class="collapse" for="c-37935988">[-]</label><label class="expand" for="c-37935988">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but currently there is a project called Aya[1] from Cohere4AI that I think it is trying to create multilingual models.<p>[1] aya.for.ai</div><br/></div></div><div id="37935901" class="c"><input type="checkbox" id="c-37935901" checked=""/><div class="controls bullet"><span class="by">ekelsen</span><span>|</span><a href="#37935731">parent</a><span>|</span><a href="#37935988">prev</a><span>|</span><a href="#37937269">next</a><span>|</span><label class="collapse" for="c-37935901">[-]</label><label class="expand" for="c-37935901">[1 more]</label></div><br/><div class="children"><div class="content">I would try your language of interest...</div><br/></div></div></div></div><div id="37937269" class="c"><input type="checkbox" id="c-37937269" checked=""/><div class="controls bullet"><span class="by">StephenAshmore</span><span>|</span><a href="#37935731">prev</a><span>|</span><a href="#37933686">next</a><span>|</span><label class="collapse" for="c-37937269">[-]</label><label class="expand" for="c-37937269">[1 more]</label></div><br/><div class="children"><div class="content">Fascinating! I love seeing more multimodal ML. Thanks for sharing!</div><br/></div></div><div id="37933686" class="c"><input type="checkbox" id="c-37933686" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37937269">prev</a><span>|</span><a href="#37935607">next</a><span>|</span><label class="collapse" for="c-37933686">[-]</label><label class="expand" for="c-37933686">[4 more]</label></div><br/><div class="children"><div class="content">Oh wow. This seems to be the best released vlm model. The chart&#x2F;UI understanding displayed in particular is superb.</div><br/><div id="37934045" class="c"><input type="checkbox" id="c-37934045" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37933686">parent</a><span>|</span><a href="#37935607">next</a><span>|</span><label class="collapse" for="c-37934045">[-]</label><label class="expand" for="c-37934045">[3 more]</label></div><br/><div class="children"><div class="content">&gt;This is by far the best open source vlm model<p>LLaVA 1.5 is very good, at least at describing images. <a href="http:&#x2F;&#x2F;llava.hliu.cc&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;llava.hliu.cc&#x2F;</a></div><br/><div id="37935058" class="c"><input type="checkbox" id="c-37935058" checked=""/><div class="controls bullet"><span class="by">axiom92</span><span>|</span><a href="#37933686">root</a><span>|</span><a href="#37934045">parent</a><span>|</span><a href="#37935607">next</a><span>|</span><label class="collapse" for="c-37935058">[-]</label><label class="expand" for="c-37935058">[2 more]</label></div><br/><div class="children"><div class="content">Right, but no separate image encoder + half the size could be very helpful for many applications.</div><br/><div id="37935276" class="c"><input type="checkbox" id="c-37935276" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37933686">root</a><span>|</span><a href="#37935058">parent</a><span>|</span><a href="#37935607">next</a><span>|</span><label class="collapse" for="c-37935276">[-]</label><label class="expand" for="c-37935276">[1 more]</label></div><br/><div class="children"><div class="content">The 7B LLaVa model is smaller, even considering the image encoder (CLIP-L).</div><br/></div></div></div></div></div></div></div></div><div id="37935607" class="c"><input type="checkbox" id="c-37935607" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#37933686">prev</a><span>|</span><a href="#37933762">next</a><span>|</span><label class="collapse" for="c-37935607">[-]</label><label class="expand" for="c-37935607">[1 more]</label></div><br/><div class="children"><div class="content">Comparable with llava13b in benchmarks! Great work!</div><br/></div></div><div id="37933762" class="c"><input type="checkbox" id="c-37933762" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#37935607">prev</a><span>|</span><label class="collapse" for="c-37933762">[-]</label><label class="expand" for="c-37933762">[2 more]</label></div><br/><div class="children"><div class="content">Before someone else does, I&#x27;m going to point out that CC-BY-NC is technically not an open source license.</div><br/></div></div></div></div></div></div></div></body></html>