<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1701421258731" as="style"/><link rel="stylesheet" href="styles.css?v=1701421258731"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2311.16989">ChatGPT&#x27;s 1-Year Anniversary: Are Open-Source Large Language Models Catching Up?</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>rkwz</span> | <span>96 comments</span></div><br/><div><div id="38483611" class="c"><input type="checkbox" id="c-38483611" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38483123">next</a><span>|</span><label class="collapse" for="c-38483611">[-]</label><label class="expand" for="c-38483611">[19 more]</label></div><br/><div class="children"><div class="content">A couple big&#x2F;strong open models that have just been released the past few days:<p>* Qwen 72B (and 1.8B) - 32K context, trained on 3T tokens, &lt;100M MAU commercial license, strong benchmark performance: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;huybery&#x2F;status&#x2F;1730127387109781932" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;huybery&#x2F;status&#x2F;1730127387109781932</a><p>* DeepSeek LLM 67B - 4K context, 2T tokens, Apache 2.0 license, strong on code (although DeepSeek Code 33B it benches better) <a href="https:&#x2F;&#x2F;twitter.com&#x2F;deepseek_ai&#x2F;status&#x2F;1729881611234431456" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;deepseek_ai&#x2F;status&#x2F;1729881611234431456</a><p>Also recently released: Yi 34B (with a 100B rumored soon), XVERSE-65B, Aquila2-70B, and Yuan 2.0-102B, interestingly, all coming out of China.<p>Personally, I&#x27;m also looking forward to the larger Mistral releasing soon as mistral-7b-v0.1 was already incredibly strong for its size.</div><br/><div id="38483685" class="c"><input type="checkbox" id="c-38483685" checked=""/><div class="controls bullet"><span class="by">John9</span><span>|</span><a href="#38483611">parent</a><span>|</span><a href="#38483754">next</a><span>|</span><label class="collapse" for="c-38483685">[-]</label><label class="expand" for="c-38483685">[16 more]</label></div><br/><div class="children"><div class="content">Since it&#x27;s not allowed to use ChatGPT in China, there is a huge opportunity to build a local LLM.</div><br/><div id="38483826" class="c"><input type="checkbox" id="c-38483826" checked=""/><div class="controls bullet"><span class="by">wenyuanyu</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483685">parent</a><span>|</span><a href="#38484058">next</a><span>|</span><label class="collapse" for="c-38483826">[-]</label><label class="expand" for="c-38483826">[13 more]</label></div><br/><div class="children"><div class="content">Anyone know what is the reason why both OpenAI and Anthropic proactively banned users from China from using their products...</div><br/><div id="38483857" class="c"><input type="checkbox" id="c-38483857" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483826">parent</a><span>|</span><a href="#38484058">next</a><span>|</span><label class="collapse" for="c-38483857">[-]</label><label class="expand" for="c-38483857">[12 more]</label></div><br/><div class="children"><div class="content">Source for this? I know China&#x27;s government firewall blocks ChatGPT (for obvious reasons) but I wasn&#x27;t aware that OpenAI was blocking them in return.</div><br/><div id="38483958" class="c"><input type="checkbox" id="c-38483958" checked=""/><div class="controls bullet"><span class="by">pototo666</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483857">parent</a><span>|</span><a href="#38484336">next</a><span>|</span><label class="collapse" for="c-38483958">[-]</label><label class="expand" for="c-38483958">[6 more]</label></div><br/><div class="children"><div class="content">Chinese IP are not allowed to use ChatGPT.
Chinese credit card is not allowed for OpenAI API.<p>Source: my own experience.<p>What puzzles me most is the second restriction. My credit card is accepted by AWS, Google, and many other services. It is also accepted by many services which use Stripe to process payments.<p>But OpenAI refuses to take my money.</div><br/><div id="38484193" class="c"><input type="checkbox" id="c-38484193" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483958">parent</a><span>|</span><a href="#38484336">next</a><span>|</span><label class="collapse" for="c-38484193">[-]</label><label class="expand" for="c-38484193">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand, if ChatGPT is blocked by the firewall, how do you know that ChatGPT is blocking IPs in return? Are there chinese IP ranges that are not affected by censorship that a citizen can use?</div><br/><div id="38484262" class="c"><input type="checkbox" id="c-38484262" checked=""/><div class="controls bullet"><span class="by">dantondwa</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484193">parent</a><span>|</span><a href="#38484336">next</a><span>|</span><label class="collapse" for="c-38484262">[-]</label><label class="expand" for="c-38484262">[4 more]</label></div><br/><div class="children"><div class="content">When a website is blocked by the firewall, it doesn’t load.<p>When a website blocks Chinese users, the website loads but you cannot create an account.<p>Yes, the firewall does not block everything, otherwise it would be the same as turning off the internet! There are websites that work.</div><br/><div id="38484289" class="c"><input type="checkbox" id="c-38484289" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484262">parent</a><span>|</span><a href="#38484336">next</a><span>|</span><label class="collapse" for="c-38484289">[-]</label><label class="expand" for="c-38484289">[3 more]</label></div><br/><div class="children"><div class="content">Okay but the point is that ChatGPT is blocked by the firewall.<p>EDIT: I read the comment below about Hong Kong, but I can&#x27;t reply because I&#x27;m typing too fast by HN standards, so I&#x27;m writing it here and yolo: &quot;I&#x27;m from Italy and I remember when ChatGPT was blocked here after the Garante della Privacy complaint, of course the site wasn&#x27;t blocked by Italy but OpenAI complies with local obligations, so maybe it could be a reason about the block. API were also not blocked in Italy.&quot;</div><br/><div id="38484615" class="c"><input type="checkbox" id="c-38484615" checked=""/><div class="controls bullet"><span class="by">deadfoxygrandpa</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484289">parent</a><span>|</span><a href="#38484347">next</a><span>|</span><label class="collapse" for="c-38484615">[-]</label><label class="expand" for="c-38484615">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not blocked by the firewall. i&#x27;m in china and i can load openai&#x27;s website and chatgpt just fine. openai just blocks me from accessing chatgpt or signing up for an account unless i use a VPN and US based phone number for signup<p>as in, if i open chat.openai.com in my browser without a VPN, from behind the firewall, i get an openai error message that says &quot;Unable to load site&quot; with the openai logo on screen<p>if the firewall blocks something the page just doesn&#x27;t load at all and the connection times out</div><br/></div></div><div id="38484347" class="c"><input type="checkbox" id="c-38484347" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484289">parent</a><span>|</span><a href="#38484615">prev</a><span>|</span><a href="#38484336">next</a><span>|</span><label class="collapse" for="c-38484347">[-]</label><label class="expand" for="c-38484347">[1 more]</label></div><br/><div class="children"><div class="content">In so far as Hong Kong IPs are &quot;Chinese IPs&quot;, we can access OpenAI&#x27;s website, but their signup and login pages blocks Hong Kong phone numbers, credit cards and IP addresses.<p>Curiously, the OpenAI API endpoints works flawlessly with Hong Kong IP addresses as long as you have a working API key.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38484336" class="c"><input type="checkbox" id="c-38484336" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483857">parent</a><span>|</span><a href="#38483958">prev</a><span>|</span><a href="#38484432">next</a><span>|</span><label class="collapse" for="c-38484336">[-]</label><label class="expand" for="c-38484336">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI does not allow users from China, including Hong Kong.<p>Hong Kong generally does not have a Great Firewall, so the only thing preventing Hong Kong users from using ChatGPT is Open AI&#x27;s policy. They don&#x27;t allow registration from Hong Kong phone numbers, from Hong Kong credit cards, etc.<p>I&#x27;d say it&#x27;s been pretty deliberate.<p>Reason? Presumably in alignment with US government policies of trying to slow down China&#x27;s development in AI, alongside with the chips bans etc etc.</div><br/><div id="38484644" class="c"><input type="checkbox" id="c-38484644" checked=""/><div class="controls bullet"><span class="by">suslik</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484336">parent</a><span>|</span><a href="#38484432">next</a><span>|</span><label class="collapse" for="c-38484644">[-]</label><label class="expand" for="c-38484644">[1 more]</label></div><br/><div class="children"><div class="content">Sounds plausible - this is in line with the modern trend to posture by sanctioninig innocent people.<p>Of course, the only demographic these restrictions can affect are casuals. Even I know how to cirumvent this; thinking that this could hinder a government agent - who surely have access to all the necessary infrastructure by default - is simply mental.</div><br/></div></div></div></div><div id="38484432" class="c"><input type="checkbox" id="c-38484432" checked=""/><div class="controls bullet"><span class="by">skripp</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483857">parent</a><span>|</span><a href="#38484336">prev</a><span>|</span><a href="#38484058">next</a><span>|</span><label class="collapse" for="c-38484432">[-]</label><label class="expand" for="c-38484432">[3 more]</label></div><br/><div class="children"><div class="content">I live in China. You can&#x27;t use it here easily. Even if you use a VPN you still need a non-Chinese phone number.</div><br/><div id="38484522" class="c"><input type="checkbox" id="c-38484522" checked=""/><div class="controls bullet"><span class="by">finnjohnsen2</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484432">parent</a><span>|</span><a href="#38484058">next</a><span>|</span><label class="collapse" for="c-38484522">[-]</label><label class="expand" for="c-38484522">[2 more]</label></div><br/><div class="children"><div class="content">I would love to know how they technically they can stop you once you run VPN. Do you have any idea on that?</div><br/><div id="38484655" class="c"><input type="checkbox" id="c-38484655" checked=""/><div class="controls bullet"><span class="by">deadfoxygrandpa</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484522">parent</a><span>|</span><a href="#38484058">next</a><span>|</span><label class="collapse" for="c-38484655">[-]</label><label class="expand" for="c-38484655">[1 more]</label></div><br/><div class="children"><div class="content">i know how: you need a verified phone number to open an account, and open ai does not accept chinese phone numbers or known IP phone numbers like google voice.<p>they also block a lot of data center IP addresses, so if you&#x27;re trying to access chatgpt from a VPN running on blacklisted datacenter IP range (a lot of VPN services or common cloud providers that people use to set up their own private VPNs are blacklisted), then it tells you it can&#x27;t access the site and &quot;If you are using a VPN, try turning it off.&quot;</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38484058" class="c"><input type="checkbox" id="c-38484058" checked=""/><div class="controls bullet"><span class="by">magpi3</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483685">parent</a><span>|</span><a href="#38483826">prev</a><span>|</span><a href="#38483754">next</a><span>|</span><label class="collapse" for="c-38484058">[-]</label><label class="expand" for="c-38484058">[2 more]</label></div><br/><div class="children"><div class="content">Baidu has a Chatgpt clone that I use regularly.<p><a href="https:&#x2F;&#x2F;yiyan.baidu.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;yiyan.baidu.com</a><p>I imagine it is good enough for most people.</div><br/><div id="38484159" class="c"><input type="checkbox" id="c-38484159" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38484058">parent</a><span>|</span><a href="#38483754">next</a><span>|</span><label class="collapse" for="c-38484159">[-]</label><label class="expand" for="c-38484159">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious in knowing why you&#x27;ve opted for this model over ChatGPT-3.5. Is it because it performs better in Chinese?</div><br/></div></div></div></div></div></div><div id="38483754" class="c"><input type="checkbox" id="c-38483754" checked=""/><div class="controls bullet"><span class="by">purplecats</span><span>|</span><a href="#38483611">parent</a><span>|</span><a href="#38483685">prev</a><span>|</span><a href="#38483123">next</a><span>|</span><label class="collapse" for="c-38483754">[-]</label><label class="expand" for="c-38483754">[2 more]</label></div><br/><div class="children"><div class="content">when is the new mistral coming out and at what size?</div><br/><div id="38484137" class="c"><input type="checkbox" id="c-38484137" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38483611">root</a><span>|</span><a href="#38483754">parent</a><span>|</span><a href="#38483123">next</a><span>|</span><label class="collapse" for="c-38484137">[-]</label><label class="expand" for="c-38484137">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m hoping that they make it 13B, which is the size I can run locally in 4-bit and still get reasonable performance</div><br/></div></div></div></div></div></div><div id="38483123" class="c"><input type="checkbox" id="c-38483123" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#38483611">prev</a><span>|</span><a href="#38482347">next</a><span>|</span><label class="collapse" for="c-38483123">[-]</label><label class="expand" for="c-38483123">[15 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not mentioned in the paper but this month OpenChat 3.5 released the first 7b model that achieves results comparable to ChatGPT in March 2023 [1]. Only 8k context window, but personally I&#x27;ve been very impressed with it so far. On the chatbot arena leaderboard it ranks above Llama-2-70b-chat [2].<p>In many ways open source LLMs are actually leading the industry, especially in terms of parameter efficiency and shipping useful models that consumers can run on their own hardware.<p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;openchat&#x2F;openchat_3.5" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;openchat&#x2F;openchat_3.5</a><p>[2] <a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;</a></div><br/><div id="38483433" class="c"><input type="checkbox" id="c-38483433" checked=""/><div class="controls bullet"><span class="by">Semaphor</span><span>|</span><a href="#38483123">parent</a><span>|</span><a href="#38484119">next</a><span>|</span><label class="collapse" for="c-38483433">[-]</label><label class="expand" for="c-38483433">[1 more]</label></div><br/><div class="children"><div class="content">Oh wow, and it has far fewer guardrails than either Llama2 (which is horrible in that regard) or GPT3.5, that’s the first time I’m actually really impressed by an open model.</div><br/></div></div><div id="38484119" class="c"><input type="checkbox" id="c-38484119" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38483123">parent</a><span>|</span><a href="#38483433">prev</a><span>|</span><a href="#38484324">next</a><span>|</span><label class="collapse" for="c-38484119">[-]</label><label class="expand" for="c-38484119">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m running the llama.cpp&#x2F;gguf Q8 version, with 30 layers offloaded to the laptop&#x27;s GPU (RTX 3070, 8G VRAM), and I get around 20-25 tokens&#x2F;second.<p>It really feels like I have one the the earlier versions of ChatGPT 3.5 installed on my computer.</div><br/></div></div><div id="38484324" class="c"><input type="checkbox" id="c-38484324" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38483123">parent</a><span>|</span><a href="#38484119">prev</a><span>|</span><a href="#38483155">next</a><span>|</span><label class="collapse" for="c-38484324">[-]</label><label class="expand" for="c-38484324">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;openchat.team&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;openchat.team&#x2F;</a> is the link if you want to test the model online.</div><br/></div></div><div id="38483155" class="c"><input type="checkbox" id="c-38483155" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#38483123">parent</a><span>|</span><a href="#38484324">prev</a><span>|</span><a href="#38482347">next</a><span>|</span><label class="collapse" for="c-38483155">[-]</label><label class="expand" for="c-38483155">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Only 8k context window<p>Is this supposed to be low? All the chat models I&#x27;ve used top out at 4096.</div><br/><div id="38483170" class="c"><input type="checkbox" id="c-38483170" checked=""/><div class="controls bullet"><span class="by">Sai_</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483155">parent</a><span>|</span><a href="#38483899">next</a><span>|</span><label class="collapse" for="c-38483170">[-]</label><label class="expand" for="c-38483170">[9 more]</label></div><br/><div class="children"><div class="content">GPT-4-turbo is at 128k. Claude 2.1 is 200k. But yes, among open source models 8k is roughly middle to top of the pack.</div><br/><div id="38483263" class="c"><input type="checkbox" id="c-38483263" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483170">parent</a><span>|</span><a href="#38483222">next</a><span>|</span><label class="collapse" for="c-38483263">[-]</label><label class="expand" for="c-38483263">[2 more]</label></div><br/><div class="children"><div class="content">The numbers are high, but whether 8k is low depends on your use case. Do you want to process whole book chapters, or feed lots of related documents at the same time? If not, and you&#x27;re just doing a normal question&#x2F;answer session with some priming prompt, 8k is already a lot.</div><br/><div id="38483382" class="c"><input type="checkbox" id="c-38483382" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483263">parent</a><span>|</span><a href="#38483222">next</a><span>|</span><label class="collapse" for="c-38483382">[-]</label><label class="expand" for="c-38483382">[1 more]</label></div><br/><div class="children"><div class="content">8k is very little if you want to add almost any additional data in context, or have a more complicated prompt.<p>Otherwise your knowledge retrieval needs to be almost spot on for llm to provide a proper reply.<p>Ditto with any multi shot prompts.</div><br/></div></div></div></div><div id="38483222" class="c"><input type="checkbox" id="c-38483222" checked=""/><div class="controls bullet"><span class="by">jimmyl02</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483170">parent</a><span>|</span><a href="#38483263">prev</a><span>|</span><a href="#38483279">next</a><span>|</span><label class="collapse" for="c-38483222">[-]</label><label class="expand" for="c-38483222">[4 more]</label></div><br/><div class="children"><div class="content">to be fair, I think the ability of these models to actually use these contexts beyond the standard 8k &#x2F; 16k tokens is pretty weak. RAG based methods are probably a better option for these ultra long contexts</div><br/><div id="38483627" class="c"><input type="checkbox" id="c-38483627" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483222">parent</a><span>|</span><a href="#38483416">next</a><span>|</span><label class="collapse" for="c-38483627">[-]</label><label class="expand" for="c-38483627">[1 more]</label></div><br/><div class="children"><div class="content">Haystack testing on GPT-4&#x27;s 128K context suggests otherwise: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;SteveMoraco&#x2F;status&#x2F;1727370446788530236" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;SteveMoraco&#x2F;status&#x2F;1727370446788530236</a></div><br/></div></div><div id="38483416" class="c"><input type="checkbox" id="c-38483416" checked=""/><div class="controls bullet"><span class="by">jeswin</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483222">parent</a><span>|</span><a href="#38483627">prev</a><span>|</span><a href="#38483387">next</a><span>|</span><label class="collapse" for="c-38483416">[-]</label><label class="expand" for="c-38483416">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think the ability of these models to actually use these contexts beyond the standard 8k &#x2F; 16k tokens is pretty weak.<p>For 32k GPT4 contexts, that&#x27;s not accurate. GPT4 Turbo is a bit weaker than GPT4-32k, but not to the extent that you claim.</div><br/></div></div><div id="38483387" class="c"><input type="checkbox" id="c-38483387" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483222">parent</a><span>|</span><a href="#38483416">prev</a><span>|</span><a href="#38483279">next</a><span>|</span><label class="collapse" for="c-38483387">[-]</label><label class="expand" for="c-38483387">[1 more]</label></div><br/><div class="children"><div class="content">Are you talking about claude or Gpt4 as well? Anybspecific examples where ChatGPT4 fails for long contexts?</div><br/></div></div></div></div><div id="38483279" class="c"><input type="checkbox" id="c-38483279" checked=""/><div class="controls bullet"><span class="by">smeagull</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483170">parent</a><span>|</span><a href="#38483222">prev</a><span>|</span><a href="#38483207">next</a><span>|</span><label class="collapse" for="c-38483279">[-]</label><label class="expand" for="c-38483279">[1 more]</label></div><br/><div class="children"><div class="content">The problem with those numbers is they hit the internal limit before you use all those tokens. There&#x27;s a limit to how many rules or factors their conditional probability model can keep track of. Once you hit that having a bigger context window doesn&#x27;t matter.</div><br/></div></div><div id="38483207" class="c"><input type="checkbox" id="c-38483207" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483170">parent</a><span>|</span><a href="#38483279">prev</a><span>|</span><a href="#38483899">next</a><span>|</span><label class="collapse" for="c-38483207">[-]</label><label class="expand" for="c-38483207">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s insane. The highest I&#x27;ve personally seen in the open-source space is RWKV being trained on (IIRC) 4k but being able to handle much longer context lengths in practice due to being an RNN (you can simply keep feeding it tokens forever and ever). It doesn&#x27;t generalize infinitely by any means but it can be stretched for sure, sometimes up to 16k.<p>It&#x27;s not a transformer model though, and old context fades away much faster &#x2F; is harder to recall because all the new context is layered directly on top of it. But it&#x27;s quite interesting nonetheless.</div><br/></div></div></div></div><div id="38483899" class="c"><input type="checkbox" id="c-38483899" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38483123">root</a><span>|</span><a href="#38483155">parent</a><span>|</span><a href="#38483170">prev</a><span>|</span><a href="#38482347">next</a><span>|</span><label class="collapse" for="c-38483899">[-]</label><label class="expand" for="c-38483899">[1 more]</label></div><br/><div class="children"><div class="content">Most 4K models can use context window extension to get to 8K reasonably, but you&#x27;re starting to see 16K, 32K, 128K (see YaRN for example) tunes become more common, or even a 200K version of Yi-34B.</div><br/></div></div></div></div></div></div><div id="38482347" class="c"><input type="checkbox" id="c-38482347" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38483123">prev</a><span>|</span><a href="#38482536">next</a><span>|</span><label class="collapse" for="c-38482347">[-]</label><label class="expand" for="c-38482347">[5 more]</label></div><br/><div class="children"><div class="content">It depends on what you&#x27;re doing... Just for reference, here is a small showcase of the capabilities that I&#x27;ve trained on a 13 billion parameter llama2 fine tune (done with qlora).<p><a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;186qq92&#x2F;comment&#x2F;kba2d6m&#x2F;?utm_source=share&amp;utm_medium=web2x&amp;context=3" rel="nofollow noreferrer">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;186qq92&#x2F;comment...</a><p>Edit: Embed some of the content instead.<p>Inkbot can create knowledge graphs. The structure returned is proper YAML, and I got much better results with my fine-tune than using GPT4.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Tostino&#x2F;Inkbot-13B-8k-0.2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Tostino&#x2F;Inkbot-13B-8k-0.2</a><p>Simple prompt: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;c3541f3a01d420e771f66c62014e6a24" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;c3541f3a01d420e771f66c62014e...</a><p>Complex prompt: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;44bbc6a6321df5df23ba5b400a01e37d" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;44bbc6a6321df5df23ba5b400a01...</a><p>It also does chunked summarization.<p>Here is an example of chunking:<p>Part 1: chunked summarization - <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;cacb1cecdf2eb7386baf565d157f56a0" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;cacb1cecdf2eb7386baf565d157f...</a><p>Part 2: summary-of-summaries - <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;81eeee9781e519044950332b4e64bef1" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;81eeee9781e519044950332b4e64...</a><p>Here is an example of a single-shot document that fits entirely within context: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;4ba4e7e7988348134a7256fd1cbbf4ff" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Tostino&#x2F;4ba4e7e7988348134a7256fd1cbb...</a></div><br/><div id="38483404" class="c"><input type="checkbox" id="c-38483404" checked=""/><div class="controls bullet"><span class="by">reactive001</span><span>|</span><a href="#38482347">parent</a><span>|</span><a href="#38482766">next</a><span>|</span><label class="collapse" for="c-38483404">[-]</label><label class="expand" for="c-38483404">[2 more]</label></div><br/><div class="children"><div class="content">Amazing work, I&#x27;ve really wanted to get into knowledge graph generation with LLM&#x27;s for the last year but haven&#x27;t found the time. Glad to see someone making good progress on the idea!<p>How are you going about generating training data?</div><br/><div id="38483447" class="c"><input type="checkbox" id="c-38483447" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38482347">root</a><span>|</span><a href="#38483404">parent</a><span>|</span><a href="#38482766">next</a><span>|</span><label class="collapse" for="c-38483447">[-]</label><label class="expand" for="c-38483447">[1 more]</label></div><br/><div class="children"><div class="content">Lots and lots of manual review of very detailed instructions to &quot;more powerful LLMs&quot; with 2-4 prompts to generate the training data.</div><br/></div></div></div></div><div id="38482766" class="c"><input type="checkbox" id="c-38482766" checked=""/><div class="controls bullet"><span class="by">cced</span><span>|</span><a href="#38482347">parent</a><span>|</span><a href="#38483404">prev</a><span>|</span><a href="#38482536">next</a><span>|</span><label class="collapse" for="c-38482766">[-]</label><label class="expand" for="c-38482766">[2 more]</label></div><br/><div class="children"><div class="content">Have any references on how you fine tuned?</div><br/><div id="38482800" class="c"><input type="checkbox" id="c-38482800" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38482347">root</a><span>|</span><a href="#38482766">parent</a><span>|</span><a href="#38482536">next</a><span>|</span><label class="collapse" for="c-38482800">[-]</label><label class="expand" for="c-38482800">[1 more]</label></div><br/><div class="children"><div class="content">Sure thing, I used axolotl, and my training parameters were:<p>sequence_len: 6144
lora_r: 128
lora_alpha: 48
learning_rate: 0.00006
warmup_steps: 600
lr_scheduler: cosine
gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 4
optimizer: paged_adamw_32bit
flash_attention: true
sample_packing: true</div><br/></div></div></div></div></div></div><div id="38482536" class="c"><input type="checkbox" id="c-38482536" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#38482347">prev</a><span>|</span><a href="#38483546">next</a><span>|</span><label class="collapse" for="c-38482536">[-]</label><label class="expand" for="c-38482536">[15 more]</label></div><br/><div class="children"><div class="content">Current ~70B models like LLAMA 2 70B are on par wih ChatGPT 3.5. The best smaller models can appear on par at first glance, but they hallucinate at a much higher rate and lack knowledge of the world. GPT 4 ‘gets’ things at a deeper level and no open source model is even close.<p>A year is a good timeframe to evaluate things: the rest of the world seems to lag behind OpenAI by around 12-18 months, at least with LLMs and image generation.<p>On the other hand open source tech usually has additional features for controlling output that OpenAI never bothers to implement, like llama.cpp’s grammars or ControlNet. So in that sense open source is usually ahead of OpenAI in terms of customizability.</div><br/><div id="38483043" class="c"><input type="checkbox" id="c-38483043" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#38482536">parent</a><span>|</span><a href="#38482984">next</a><span>|</span><label class="collapse" for="c-38483043">[-]</label><label class="expand" for="c-38483043">[6 more]</label></div><br/><div class="children"><div class="content">On the other hand gpt model are converging down. Gpt4 turbo degraded performance so much that now certain 13b produce more consistent results in reasoning. I&#x27;ve a marathon test here for example <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;dfd9b9ae-7214-4dd7-ad20-7ee07abed87c" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;dfd9b9ae-7214-4dd7-ad20-7ee07a...</a> with purposefully open ended and somewhat ambiguous request to see how models perform and gpt4 turbo chat is just not that good it confuses persons out, didn&#x27;t pick the right one for abduction, didn&#x27;t change topic when requested, when recalling persons picked the one from the wrong set, when asked to change language it didn&#x27;t... It know a lot when asked zero shot questions, but when proving it&#x27;s self consistency and attention it is nowhere near gpt4.</div><br/><div id="38483095" class="c"><input type="checkbox" id="c-38483095" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38483043">parent</a><span>|</span><a href="#38482984">next</a><span>|</span><label class="collapse" for="c-38483095">[-]</label><label class="expand" for="c-38483095">[5 more]</label></div><br/><div class="children"><div class="content">Nothing kills someone&#x27;s credibility on LLMs faster than linking to the consumer product that is ChatGPT&#x27;s web interface in a discussion about models.<p>You can&#x27;t even guarantee that you&#x27;re getting the checkpoint that is known as GPT-4-Turbo via the API, and in fact you can almost guarantee you&#x27;re not because now they do side-by-side tests between different versions of their models in the web UI.</div><br/><div id="38483313" class="c"><input type="checkbox" id="c-38483313" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38483095">parent</a><span>|</span><a href="#38482984">next</a><span>|</span><label class="collapse" for="c-38483313">[-]</label><label class="expand" for="c-38483313">[4 more]</label></div><br/><div class="children"><div class="content">The point is exactly that the model people are experiencing is converging down with every subsequent update, and I even mentioned that it&#x27;s nowhere near the orig gpt4, idk possibly read it again slower instead of jumping to credibility and whatnot.</div><br/><div id="38483927" class="c"><input type="checkbox" id="c-38483927" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38483313">parent</a><span>|</span><a href="#38482984">next</a><span>|</span><label class="collapse" for="c-38483927">[-]</label><label class="expand" for="c-38483927">[3 more]</label></div><br/><div class="children"><div class="content">The irony of your comment since it implies you didn&#x27;t catch half the words in mine: I&#x27;m saying you can&#x27;t compare it to the &quot;original&quot; via the web ui<p>The &quot;new&quot; model was being used months before they announced GPT-4 Turbo, you could similarly tell by the speed differential in some of the newer tool using modes.<p>Even the &quot;original&quot; is likely still being used in some responses in the web ui, you can tell because some of the side-by-sides have a drastic speed differential that matches GPT-4 vs GPT-4-Turbo.<p>-<p>You&#x27;re trying to compare models using a tool that doesn&#x27;t actually tell you what models you&#x27;re comparing past which family they belong to, and represents more models than you&#x27;re trying to discuss.<p>In other words you&#x27;re talking about a topic you clearly don&#x27;t understand and the equally clueless mob is glad to roll with it.</div><br/><div id="38484115" class="c"><input type="checkbox" id="c-38484115" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38483927">parent</a><span>|</span><a href="#38482984">next</a><span>|</span><label class="collapse" for="c-38484115">[-]</label><label class="expand" for="c-38484115">[2 more]</label></div><br/><div class="children"><div class="content">&gt; you can&#x27;t compare it to the &quot;original&quot; via the web ui<p>Good thing then that I was comparing the chat offering to 7b and 13b open models then, maybe you didn&#x27;t catch that, try a third reading.<p>Also, this kind of is a chatgpt thread. I know it&#x27;s not a perfect comparison, but still. It&#x27;s a discussion point I&#x27;m presenting, not a research paper.</div><br/><div id="38484632" class="c"><input type="checkbox" id="c-38484632" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38484115">parent</a><span>|</span><a href="#38482984">next</a><span>|</span><label class="collapse" for="c-38484632">[-]</label><label class="expand" for="c-38484632">[1 more]</label></div><br/><div class="children"><div class="content">Good thing that&#x27;s orthogonal to what I said?<p>GPT-4 has an API, when comparing models you use the API otherwise you can&#x27;t even name the checkpoint of the model that you&#x27;re testing.<p>The web ui uses different models, unknown parameters, and even uses RAG... so it&#x27;s not even an unfair comparision, it&#x27;s not a comparison.<p>At the end of the day you&#x27;re free to convince yourself whatever you want on whatever flawed process you want, but when you&#x27;re corrected going full angsty sarcasm isn&#x27;t going to change reality.<p>-<p>AI is in such a strange place now that the masses are interested in it: you can say whatever deluded thing you want as long as it aligns with what people want to hear.<p>I guess while people masturbate to this self-affirming nonsense the rest of us can build useful things on superior tech?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38482984" class="c"><input type="checkbox" id="c-38482984" checked=""/><div class="controls bullet"><span class="by">omeze</span><span>|</span><a href="#38482536">parent</a><span>|</span><a href="#38483043">prev</a><span>|</span><a href="#38482994">next</a><span>|</span><label class="collapse" for="c-38482984">[-]</label><label class="expand" for="c-38482984">[7 more]</label></div><br/><div class="children"><div class="content">I dont think OpenAI is ever going to ahead in image generation, they were lapped very soon after dall-e and every real workflow Ive seen uses Midjourney or Stable Diffusion. The reverse (GPT 4 vision) is well ahead of open source though</div><br/><div id="38483001" class="c"><input type="checkbox" id="c-38483001" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38482984">parent</a><span>|</span><a href="#38483167">next</a><span>|</span><label class="collapse" for="c-38483001">[-]</label><label class="expand" for="c-38483001">[4 more]</label></div><br/><div class="children"><div class="content">The original is leagues behind anything current, but DALL-E version 3 absolutely blows any state of the art generative model out of the water, including mid journey 5.2 and SDXL in terms of pure prompt accuracy and coherence.<p>Midjourney still has the edge in quality, but it&#x27;s a moot point if it takes you 1000 v-rolls to get to your original vision.<p>If all you&#x27;re generating is anime waifus then MJ&#x2F;NovelAI&#x2F;Niji will suffice, but generating prompts particularly featuring relatively complex scenes or actions are amazing on DALL-E 3.<p>And of course unfortunately, it goes without saying that open AI DALL-E is going to be the most restrictive in terms of censorship.<p>I generated these from DALL-E 3 instantly. Try to generate them in any other commercial offering. Go ahead. I&#x27;ll wait...<p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;2GTRjfK" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;2GTRjfK</a><p>Descriptions:<p><i>A 80s photograph of the Koolaid Man breaking through the Berlin Wall.</i><p><i>Comic illustration set at a festive children&#x27;s party. The main focus is on the magician who looks uncannily like a well-known fictional wizard. He&#x27;s trying to say abracadabra but accidentally uses the killing curse.</i></div><br/><div id="38483055" class="c"><input type="checkbox" id="c-38483055" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38483001">parent</a><span>|</span><a href="#38483167">next</a><span>|</span><label class="collapse" for="c-38483055">[-]</label><label class="expand" for="c-38483055">[3 more]</label></div><br/><div class="children"><div class="content">SDXL has controlnet for other kinds of non-text input (like scribbles or just masks). The results are much easier to control in my opinion (a picture is worth thousands of prompt words).<p>For pure prompt coherence though I think ideogram is not far behind dalle 3.</div><br/><div id="38483342" class="c"><input type="checkbox" id="c-38483342" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38483055">parent</a><span>|</span><a href="#38483078">next</a><span>|</span><label class="collapse" for="c-38483342">[-]</label><label class="expand" for="c-38483342">[1 more]</label></div><br/><div class="children"><div class="content">EDIT: okay, I just tried Ideogram. It&#x27;s not terrible and seems to do an okay job on text generation but I&#x27;d still say its a distant second compared to DALL-E 3. However, having the ability to maintain image continuity to make refinements of your initial image based on corrections like: &quot;Make the building larger&quot;, or &quot;He should have a more prominent forehead&quot; is a game changer (e.g. InstructPix2Pix) and DALL-E 3&#x27;s the only one that&#x27;s got it.<p>Ideogram comparisons at bottom:<p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;2GTRjfK" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;2GTRjfK</a></div><br/></div></div><div id="38483078" class="c"><input type="checkbox" id="c-38483078" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38483055">parent</a><span>|</span><a href="#38483342">prev</a><span>|</span><a href="#38483167">next</a><span>|</span><label class="collapse" for="c-38483078">[-]</label><label class="expand" for="c-38483078">[1 more]</label></div><br/><div class="children"><div class="content">SDXL and even some SD 1.5 checkpoints are great. My current workflow is:<p>1. Generate initial draft image in DALL-E 3 (iterate as necessary)<p>It&#x27;s essentially the <i>ONLY</i> good InstructPix2Pix model.<p>2. Bring into InvokeAI<p>Inpaint with stuff that might be considered censored in DALL-E 3.<p>I&#x27;d like to see some proof of Ideogram - it looks... very mobile&#x2F;instagrammy from the landing page. If you have an account, try out my prompts I&#x27;d like to see what you&#x27;re able to produce.</div><br/></div></div></div></div></div></div><div id="38483167" class="c"><input type="checkbox" id="c-38483167" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38482984">parent</a><span>|</span><a href="#38483001">prev</a><span>|</span><a href="#38483093">next</a><span>|</span><label class="collapse" for="c-38483167">[-]</label><label class="expand" for="c-38483167">[1 more]</label></div><br/><div class="children"><div class="content">Strong disagree on this from me as well. DALL-E 3 is miles ahead of the latest Midjourney&#x2F;Stable Diffusion in image generation. The only real area it falls short vs the other options right now is in how nannying it can be.</div><br/></div></div><div id="38483093" class="c"><input type="checkbox" id="c-38483093" checked=""/><div class="controls bullet"><span class="by">kubrickslair</span><span>|</span><a href="#38482536">root</a><span>|</span><a href="#38482984">parent</a><span>|</span><a href="#38483167">prev</a><span>|</span><a href="#38482994">next</a><span>|</span><label class="collapse" for="c-38483093">[-]</label><label class="expand" for="c-38483093">[1 more]</label></div><br/><div class="children"><div class="content">I have found OpenAI to be the most superior in complex prompts especially where written messages like “Get better, Mom” are expected in the images. The distant second would be ideogram.<p>I am using these tools to send custom personal messages to close friends and family.</div><br/></div></div></div></div><div id="38482994" class="c"><input type="checkbox" id="c-38482994" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38482536">parent</a><span>|</span><a href="#38482984">prev</a><span>|</span><a href="#38483546">next</a><span>|</span><label class="collapse" for="c-38482994">[-]</label><label class="expand" for="c-38482994">[1 more]</label></div><br/><div class="children"><div class="content">Function-calling with a JSON schema is about as reliable as llama.cpp’s grammar stuff. I’ve not had any trouble with it.</div><br/></div></div></div></div><div id="38483546" class="c"><input type="checkbox" id="c-38483546" checked=""/><div class="controls bullet"><span class="by">kaycebasques</span><span>|</span><a href="#38482536">prev</a><span>|</span><a href="#38484630">next</a><span>|</span><label class="collapse" for="c-38483546">[-]</label><label class="expand" for="c-38483546">[1 more]</label></div><br/><div class="children"><div class="content">No comment from me on the question in the title (because I don&#x27;t know enough to have an opinion), but since others are discussing various open models I will mention another that I&#x27;ve been enjoying tonight: DeepSeek 67B<p><a href="https:&#x2F;&#x2F;chat.deepseek.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.deepseek.com</a><p>(This chat UI has adequately replaced my ChatGPT needs so far.)<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;deepseek-ai&#x2F;deepseek-llm-67b-base" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;deepseek-ai&#x2F;deepseek-llm-67b-base</a><p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;abacaj&#x2F;status&#x2F;1730019229175312612" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;abacaj&#x2F;status&#x2F;1730019229175312612</a></div><br/></div></div><div id="38484630" class="c"><input type="checkbox" id="c-38484630" checked=""/><div class="controls bullet"><span class="by">jurmous</span><span>|</span><a href="#38483546">prev</a><span>|</span><a href="#38482467">next</a><span>|</span><label class="collapse" for="c-38484630">[-]</label><label class="expand" for="c-38484630">[1 more]</label></div><br/><div class="children"><div class="content">A problem I have with the open source models is that they are all not remotely good in many languages other than English compared to the OpenAI models. I specifically need Dutch and the outputs are unusable for us.</div><br/></div></div><div id="38482467" class="c"><input type="checkbox" id="c-38482467" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#38484630">prev</a><span>|</span><a href="#38483432">next</a><span>|</span><label class="collapse" for="c-38482467">[-]</label><label class="expand" for="c-38482467">[3 more]</label></div><br/><div class="children"><div class="content">I’ve found Mistral OpenOrca is pretty much as good as GPT4-turbo for creative writing&#x2F;analysis. Actually it tends to output very similar text, which is suspicious, but whatever it saves me a lot of money.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Open-Orca&#x2F;Mistral-7B-OpenOrca" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Open-Orca&#x2F;Mistral-7B-OpenOrca</a></div><br/><div id="38482501" class="c"><input type="checkbox" id="c-38482501" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#38482467">parent</a><span>|</span><a href="#38483003">next</a><span>|</span><label class="collapse" for="c-38482501">[-]</label><label class="expand" for="c-38482501">[1 more]</label></div><br/><div class="children"><div class="content">Also openchat, which was trained on gpt4 conversations IIUC.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;imoneoi&#x2F;openchat">https:&#x2F;&#x2F;github.com&#x2F;imoneoi&#x2F;openchat</a></div><br/></div></div><div id="38483003" class="c"><input type="checkbox" id="c-38483003" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#38482467">parent</a><span>|</span><a href="#38482501">prev</a><span>|</span><a href="#38483432">next</a><span>|</span><label class="collapse" for="c-38483003">[-]</label><label class="expand" for="c-38483003">[1 more]</label></div><br/><div class="children"><div class="content">Mistral OpenOrca is very good at task following as well. Its slightly less reliable than GPT 3.5&#x2F;4, but the difference in quality for my text processing tasks is pretty much a toss-up.</div><br/></div></div></div></div><div id="38483432" class="c"><input type="checkbox" id="c-38483432" checked=""/><div class="controls bullet"><span class="by">czk</span><span>|</span><a href="#38482467">prev</a><span>|</span><a href="#38483612">next</a><span>|</span><label class="collapse" for="c-38483432">[-]</label><label class="expand" for="c-38483432">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say they are catching up for sure, especially with how GPT4 has been regressing consistently over the past month.
<a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;c91287ee-9a5e-4c99-b5df-49cc4536e471" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;c91287ee-9a5e-4c99-b5df-49cc45...</a></div><br/><div id="38484229" class="c"><input type="checkbox" id="c-38484229" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38483432">parent</a><span>|</span><a href="#38483612">next</a><span>|</span><label class="collapse" for="c-38484229">[-]</label><label class="expand" for="c-38484229">[1 more]</label></div><br/><div class="children"><div class="content">I suspect a lot of the &quot;catching up&quot; was achieved by using GPT-4 API to generate high quality fine-tune datasets.</div><br/></div></div></div></div><div id="38483612" class="c"><input type="checkbox" id="c-38483612" checked=""/><div class="controls bullet"><span class="by">davidkunz</span><span>|</span><a href="#38483432">prev</a><span>|</span><a href="#38482448">next</a><span>|</span><label class="collapse" for="c-38483612">[-]</label><label class="expand" for="c-38483612">[5 more]</label></div><br/><div class="children"><div class="content">Out of personal experience, open source LLMs did not yet reach the quality of GPT 3.5, despite multiple claims with dubious benchmarks. That said, they are already useful as of today and can even run on your local machine. I regularly use them with my Neovim plugin gen.nvim [1] for simple tasks and they save me a lot of time. I&#x27;m excited about the future!<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;David-Kunz&#x2F;gen.nvim">https:&#x2F;&#x2F;github.com&#x2F;David-Kunz&#x2F;gen.nvim</a></div><br/><div id="38484281" class="c"><input type="checkbox" id="c-38484281" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38483612">parent</a><span>|</span><a href="#38482448">next</a><span>|</span><label class="collapse" for="c-38484281">[-]</label><label class="expand" for="c-38484281">[4 more]</label></div><br/><div class="children"><div class="content">Very interesting.<p>I want to give it a try, but I see that one of the dependencies is &quot;ollama&quot; which is a Mac App and I don&#x27;t have a Mac.<p>I&#x27;m running Llama models locally using llama-cpp-python which provides an OpenAI compatibility layer.</div><br/><div id="38484628" class="c"><input type="checkbox" id="c-38484628" checked=""/><div class="controls bullet"><span class="by">davidkunz</span><span>|</span><a href="#38483612">root</a><span>|</span><a href="#38484281">parent</a><span>|</span><a href="#38484361">next</a><span>|</span><label class="collapse" for="c-38484628">[-]</label><label class="expand" for="c-38484628">[2 more]</label></div><br/><div class="children"><div class="content">It should work on Linux and also with WSL. You could also try running it in Docker.</div><br/><div id="38484638" class="c"><input type="checkbox" id="c-38484638" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38483612">root</a><span>|</span><a href="#38484628">parent</a><span>|</span><a href="#38484361">next</a><span>|</span><label class="collapse" for="c-38484638">[-]</label><label class="expand" for="c-38484638">[1 more]</label></div><br/><div class="children"><div class="content">I see. Does ollama have an http API (this the curl requirement)? If so, is it compatible with OpenAI API?</div><br/></div></div></div></div><div id="38484361" class="c"><input type="checkbox" id="c-38484361" checked=""/><div class="controls bullet"><span class="by">rkwz</span><span>|</span><a href="#38483612">root</a><span>|</span><a href="#38484281">parent</a><span>|</span><a href="#38484628">prev</a><span>|</span><a href="#38482448">next</a><span>|</span><label class="collapse" for="c-38484361">[-]</label><label class="expand" for="c-38484361">[1 more]</label></div><br/><div class="children"><div class="content">Oh, can install Ollama in Linux&#x2F;WSL as well</div><br/></div></div></div></div></div></div><div id="38482448" class="c"><input type="checkbox" id="c-38482448" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#38483612">prev</a><span>|</span><a href="#38482362">next</a><span>|</span><label class="collapse" for="c-38482448">[-]</label><label class="expand" for="c-38482448">[6 more]</label></div><br/><div class="children"><div class="content">Long term it&#x27;s almost unavoidable that open source LLMs start catching up. One factor that&#x27;s worth considering too is cost. The open source community is much more resource constrained and they&#x27;ve really accelerated the pace of development in &lt;30B parameter models.</div><br/><div id="38482587" class="c"><input type="checkbox" id="c-38482587" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38482448">parent</a><span>|</span><a href="#38482832">next</a><span>|</span><label class="collapse" for="c-38482587">[-]</label><label class="expand" for="c-38482587">[2 more]</label></div><br/><div class="children"><div class="content">Google and Meta and all the funded companies also are not even close to GPT 4, so I doubt cost is the biggest factor. Claude is the only model that is decent other than OpenAI&#x27;s.</div><br/><div id="38482626" class="c"><input type="checkbox" id="c-38482626" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#38482448">root</a><span>|</span><a href="#38482587">parent</a><span>|</span><a href="#38482832">next</a><span>|</span><label class="collapse" for="c-38482626">[-]</label><label class="expand" for="c-38482626">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that the models are pretty comparable, but nobody&#x27;s reinforcement training set is not nearly as good as OpenAI&#x27;s, so they&#x27;re able to fine-tune their model to give more accurate results.</div><br/></div></div></div></div><div id="38482832" class="c"><input type="checkbox" id="c-38482832" checked=""/><div class="controls bullet"><span class="by">hyperliner</span><span>|</span><a href="#38482448">parent</a><span>|</span><a href="#38482587">prev</a><span>|</span><a href="#38482362">next</a><span>|</span><label class="collapse" for="c-38482832">[-]</label><label class="expand" for="c-38482832">[3 more]</label></div><br/><div class="children"><div class="content">This is an industry where cost will be an issue. It reminds me of Rackspace and others trying to win with OpenStack “because open.” AWS and Azure won. Even Google is third.<p>The big players will win, and there will be a niche for open tools.</div><br/><div id="38483637" class="c"><input type="checkbox" id="c-38483637" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#38482448">root</a><span>|</span><a href="#38482832">parent</a><span>|</span><a href="#38482362">next</a><span>|</span><label class="collapse" for="c-38483637">[-]</label><label class="expand" for="c-38483637">[2 more]</label></div><br/><div class="children"><div class="content">Google only lost because they couldn’t re-adjust their business for their paid products to not be similar to their advertising products.<p>I can only speak for the European enterprise scene, but AWS came first and in the beginning they went a very “Googley” route of not having very great support and very little patience for local needs. Then Azure came along with their typical Microsoft approach to enterprise, which is where you get excellent support and you get contacts into Microsoft who will actually listen and make changes, well, if the changes align with what Microsoft wants. I know Microsoft isn’t necessarily a popular company amongst people who’ve never interacted with them on an Enterprise level, but they really are an excellent it-business partner because they understand that part of being an Enterprise partner is that they let CTOs tell their organisation that they know X is having issues but that Microsoft headquarters is giving them half-hourly updates by phone. Sort of useless from a technical perspective, immensely useful for the CTO when 2000 employees can’t log into Outlook. Another good example is how when Teams rolled out with being on for all users by default, basically every larger organisation in the world went through the official channels and went “nonononono” and a few hours later it was off by default.<p>Now, when Amazon first entered the European market they were very “Googley” as I said, but once they realized Microsoft business model was losing them customers, they changed. We went from having no contacts to having an assigned AWS person and from not wanting to adopt the GDPR AWS actually became more compliant than even what Azure currently is.<p>Google meanwhile somehow managed to make the one product they were actually selling (education) worse than it was originally, losing billions of dollars on all the European schools who could no longer use it and be GDPR compliant. The Chinese cloud options obviously had similar data privacy issues to Google and never really became valid options. At least not unless China achieves the same sort of diplomatic relationship with the EU that the US has, which is unlikely.<p>So that’s the long story of why only two of the major cloud providers “won”. With the massive price increase, however, more and more companies are especially Azure  for their own setups. This isn’t necessarily a return to having your own iron in the basement, often it’s going to smaller cloud providers and then having a third party vendor set something like Kubernetes up.<p>Right now, Microsoft is winning the AI battle. Not so much because it’s better, but because it comes with Office365. Office365 which was already a sort of monopoly on Office products, but is now even more so. A good example is again how Teams became dominant, even though it wasn’t really the best option for a while and is now only the best option because of how it integrates directly with your Sharepoint online which is where most enterprise orgs store documents these days. So too is copilot currently winking the AI battle for organisations who can’t really use a lot of the other options because of data privacy issues. So while copilot isn’t as good as GPT, it’s still what we are using. But if it ever gets too expensive, it’s not as secure as you may think. Especially not if we start seeing more training sets, or EU and US relations worsens.<p>I think the most likely outcome, at least here in the EU, is that anti-completion laws eventually takes a look at Office365 because of how monopolised it is. Or the EU actually follows through on their “a single vendor is a threat to national security” legislation and force half of the banking&#x2F;energy&#x2F;defense&#x2F;andsoon industries to pick something other than Microsoft. Which will be hilariously hard, but if successful (which it probably won’t be because it’s hilariously hard) will lead to more open products.</div><br/><div id="38483926" class="c"><input type="checkbox" id="c-38483926" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38482448">root</a><span>|</span><a href="#38483637">parent</a><span>|</span><a href="#38482362">next</a><span>|</span><label class="collapse" for="c-38483926">[-]</label><label class="expand" for="c-38483926">[1 more]</label></div><br/><div class="children"><div class="content">&gt; anti completion laws<p>did you mean anti-competitive laws? don&#x27;t scare me with &quot;anti-completion laws&quot;, please, I still want to have AI</div><br/></div></div></div></div></div></div></div></div><div id="38482362" class="c"><input type="checkbox" id="c-38482362" checked=""/><div class="controls bullet"><span class="by">smy20011</span><span>|</span><a href="#38482448">prev</a><span>|</span><a href="#38482528">next</a><span>|</span><label class="collapse" for="c-38482362">[-]</label><label class="expand" for="c-38482362">[1 more]</label></div><br/><div class="children"><div class="content">I tried to use Sakura LLM to translate some JP novels. It&#x27;s really good and half the price of GPT3.5 turbo.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;SakuraLLM&#x2F;Sakura-13B-Galgame&#x2F;tree&#x2F;dev_server">https:&#x2F;&#x2F;github.com&#x2F;SakuraLLM&#x2F;Sakura-13B-Galgame&#x2F;tree&#x2F;dev_ser...</a></div><br/></div></div><div id="38482528" class="c"><input type="checkbox" id="c-38482528" checked=""/><div class="controls bullet"><span class="by">drakenot</span><span>|</span><a href="#38482362">prev</a><span>|</span><a href="#38482751">next</a><span>|</span><label class="collapse" for="c-38482528">[-]</label><label class="expand" for="c-38482528">[11 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been somewhat disappointed with the performance of the open models.<p>The claims of certain models outperforming GPT-3.5-Turbo and approaching GPT-4 fail to hold up to their benchmark results in real-world scenarios, potentially due to data contamination in assessments, based on my testing.<p>As noted in the linked survey paper, some models may outperform 3.5-Turbo in specific, narrow areas, depending on the model. Yet, we still lack a general model that definitively exceeds 3.5-Turbo in all respects.<p>I&#x27;m concerned that while we&#x27;re still striving to reach 3.5-Turbo&#x27;s performance level, OpenAI may unveil a new next-generation model, further widening the performance gap! Back in the summer, I had higher hopes that we would have surpassed the 3.5 threshold by now.<p>The performance gap has been surprisingly large. It is especially noticeable in areas requiring consistent structured output or tool use from the LLM. This is where open models particularly falter.</div><br/><div id="38482927" class="c"><input type="checkbox" id="c-38482927" checked=""/><div class="controls bullet"><span class="by">snowycat</span><span>|</span><a href="#38482528">parent</a><span>|</span><a href="#38482961">next</a><span>|</span><label class="collapse" for="c-38482927">[-]</label><label class="expand" for="c-38482927">[2 more]</label></div><br/><div class="children"><div class="content">There are tools that you can use to force the model to give structured output, such as llama.cpp GBNF grammar for example. They&#x27;re a bit harder to use than ask gpt4 but they do work pretty well for what I use it for.</div><br/><div id="38482966" class="c"><input type="checkbox" id="c-38482966" checked=""/><div class="controls bullet"><span class="by">white_dragon88</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38482927">parent</a><span>|</span><a href="#38482961">next</a><span>|</span><label class="collapse" for="c-38482966">[-]</label><label class="expand" for="c-38482966">[1 more]</label></div><br/><div class="children"><div class="content">what do you use it for?</div><br/></div></div></div></div><div id="38482961" class="c"><input type="checkbox" id="c-38482961" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#38482528">parent</a><span>|</span><a href="#38482927">prev</a><span>|</span><a href="#38482751">next</a><span>|</span><label class="collapse" for="c-38482961">[-]</label><label class="expand" for="c-38482961">[8 more]</label></div><br/><div class="children"><div class="content">&quot;OpenAI has no moat&quot; aged so badly that it&#x27;s almost a satire.</div><br/><div id="38483112" class="c"><input type="checkbox" id="c-38483112" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38482961">parent</a><span>|</span><a href="#38482751">next</a><span>|</span><label class="collapse" for="c-38483112">[-]</label><label class="expand" for="c-38483112">[7 more]</label></div><br/><div class="children"><div class="content">Try out my model vs gpt4 for the same tasks (I explicitly trained on) and compare. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Tostino&#x2F;Inkbot-13B-8k-0.2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Tostino&#x2F;Inkbot-13B-8k-0.2</a><p>It&#x27;s a 13b param model that isn&#x27;t meant to be general purpose, but is meant to excel on the limited tasks I&#x27;ve trained on.<p>You&#x27;ll see more like this soon.</div><br/><div id="38484308" class="c"><input type="checkbox" id="c-38484308" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38483112">parent</a><span>|</span><a href="#38483648">next</a><span>|</span><label class="collapse" for="c-38484308">[-]</label><label class="expand" for="c-38484308">[1 more]</label></div><br/><div class="children"><div class="content">Is this a Llama 2 fine tune?</div><br/></div></div><div id="38483648" class="c"><input type="checkbox" id="c-38483648" checked=""/><div class="controls bullet"><span class="by">choxi</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38483112">parent</a><span>|</span><a href="#38484308">prev</a><span>|</span><a href="#38483189">next</a><span>|</span><label class="collapse" for="c-38483648">[-]</label><label class="expand" for="c-38483648">[2 more]</label></div><br/><div class="children"><div class="content">Any suggestions for creating training data? Did you just manually create your own dataset or did you use any synthetic methods?</div><br/><div id="38483876" class="c"><input type="checkbox" id="c-38483876" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38483648">parent</a><span>|</span><a href="#38483189">next</a><span>|</span><label class="collapse" for="c-38483876">[-]</label><label class="expand" for="c-38483876">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely, pick a complicated problem and keep breaking it down with an existing model (whatever sota) until you have a consistent output for each step of your problem.<p>And then stitch all the outputs together into a coherent single response for your training pipeline.<p>After that you can do things like create q&amp;a pairs about the input and output values that will help the model understand the relationships involved.<p>With that, your training loss should be pretty reasonable for whatever task you are training.<p>The other thing is, don&#x27;t try and embed knowledge. Try and train thought patterns when specific knowledge is available in the context window.</div><br/></div></div></div></div><div id="38483189" class="c"><input type="checkbox" id="c-38483189" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38483112">parent</a><span>|</span><a href="#38483648">prev</a><span>|</span><a href="#38482751">next</a><span>|</span><label class="collapse" for="c-38483189">[-]</label><label class="expand" for="c-38483189">[3 more]</label></div><br/><div class="children"><div class="content">OpenAI has the benefit that it&#x27;s a hosted service. Even if you can set something up at home, not everybody wants to do that.</div><br/><div id="38483308" class="c"><input type="checkbox" id="c-38483308" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38483189">parent</a><span>|</span><a href="#38482751">next</a><span>|</span><label class="collapse" for="c-38483308">[-]</label><label class="expand" for="c-38483308">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not competing with OpenAI... I did a whole bunch of work, and released it for anyone who wants to use it.<p>It does what I trained it on well. Use it if you want to, or don&#x27;t.  Either way.</div><br/><div id="38483707" class="c"><input type="checkbox" id="c-38483707" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#38482528">root</a><span>|</span><a href="#38483308">parent</a><span>|</span><a href="#38482751">next</a><span>|</span><label class="collapse" for="c-38483707">[-]</label><label class="expand" for="c-38483707">[1 more]</label></div><br/><div class="children"><div class="content">Never meant to imply anything against your model. The fact that you released one at all is still more than I have to say for myself.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38482751" class="c"><input type="checkbox" id="c-38482751" checked=""/><div class="controls bullet"><span class="by">matchagaucho</span><span>|</span><a href="#38482528">prev</a><span>|</span><a href="#38483070">next</a><span>|</span><label class="collapse" for="c-38482751">[-]</label><label class="expand" for="c-38482751">[2 more]</label></div><br/><div class="children"><div class="content">If ChatGPT were only 1 LLM, then maybe.<p>But it&#x27;s a Mixture of Experts (MoE) architecture, which I think makes open source comparisons unfair?</div><br/><div id="38483196" class="c"><input type="checkbox" id="c-38483196" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#38482751">parent</a><span>|</span><a href="#38483070">next</a><span>|</span><label class="collapse" for="c-38483196">[-]</label><label class="expand" for="c-38483196">[1 more]</label></div><br/><div class="children"><div class="content">Open source options lacking parts of the ChatGPT approach which make it successful doesn&#x27;t make the comparisons unfair it explains why ChatGPT wins right now. There&#x27;s nothing stopping open source options from using the same MoE architecture, the solutions just don&#x27;t (to the same effect act least) right now.</div><br/></div></div></div></div><div id="38483070" class="c"><input type="checkbox" id="c-38483070" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#38482751">prev</a><span>|</span><a href="#38484358">next</a><span>|</span><label class="collapse" for="c-38483070">[-]</label><label class="expand" for="c-38483070">[1 more]</label></div><br/><div class="children"><div class="content">`shiningvaliant-1.2-Q4_K_M` is my go-to. I appreciate that it doesn&#x27;t top the boards in most metrics vs e.g. GPT-4, but I&#x27;m not in some A&#x2F;B group on the quantization: it&#x27;s more useful to me in practice more of the time.<p>I have it rigged up with a prompt about outputting markdown and wired up to `foo | glow -` and I get GPT-4 out when I want something to write JIRA tickets no one is going to read because it&#x27;s better at that sort of thing.</div><br/></div></div><div id="38484358" class="c"><input type="checkbox" id="c-38484358" checked=""/><div class="controls bullet"><span class="by">intrepidsoldier</span><span>|</span><a href="#38483070">prev</a><span>|</span><a href="#38482651">next</a><span>|</span><label class="collapse" for="c-38484358">[-]</label><label class="expand" for="c-38484358">[1 more]</label></div><br/><div class="children"><div class="content">Big fan of Starcoder</div><br/></div></div><div id="38482651" class="c"><input type="checkbox" id="c-38482651" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#38484358">prev</a><span>|</span><a href="#38483025">next</a><span>|</span><label class="collapse" for="c-38482651">[-]</label><label class="expand" for="c-38482651">[2 more]</label></div><br/><div class="children"><div class="content">Amethyst Mistral 13B q5 gguf is what I’m using most of the time now. Synthetic datasets are great to finetune with, there is no moat for having inaccessible literature data sets<p>I’m offline now because I’ve had too many ideas and domain names registered too soon after conversing with Chat GPT4<p>I’m open to the idea of people reacting to similar stimuli that cause ideas to be done at the same time, but I didn&#x27;t like that experience and I can run these models on my M1 with LM Studio so easily<p>I do think some chats get flagged when the model says something seems novel, like Albert Einstein working at the patent office. Not worth making it my whole identity in wanting to prove, just the catalyst I needed to try 7B and 13B models seriously and I’m quite pleased</div><br/></div></div><div id="38482473" class="c"><input type="checkbox" id="c-38482473" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38482361">prev</a><span>|</span><label class="collapse" for="c-38482473">[-]</label><label class="expand" for="c-38482473">[1 more]</label></div><br/><div class="children"><div class="content">The innovators dilemma.</div><br/></div></div></div></div></div></div></div></body></html>