<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706864456339" as="style"/><link rel="stylesheet" href="styles.css?v=1706864456339"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://allenai.org/olmo/olmo-paper.pdf">OLMo: Accelerating the Science of Language Models [pdf]</a> <span class="domain">(<a href="https://allenai.org">allenai.org</a>)</span></div><div class="subtext"><span>chuckhend</span> | <span>32 comments</span></div><br/><div><div id="39224536" class="c"><input type="checkbox" id="c-39224536" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#39224725">next</a><span>|</span><label class="collapse" for="c-39224536">[-]</label><label class="expand" for="c-39224536">[7 more]</label></div><br/><div class="children"><div class="content">They detail the energy used and therefore estimated carbon emissions which is interesting. When I estimate the raw electricity cost using 7-20 cents per kWh for US commercial rates, then we are only talking about $16-50k for electricity, that seems pretty small! Is my math wrong?<p>Is there any information on how much the computing costs were for renting the clusters?<p>Is the barrier to entry for a 7B model only a couple $100K?<p>EDIT: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39223467#39224534">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39223467#39224534</a><p>Perhaps only $85K total</div><br/><div id="39224808" class="c"><input type="checkbox" id="c-39224808" checked=""/><div class="controls bullet"><span class="by">anonylizard</span><span>|</span><a href="#39224536">parent</a><span>|</span><a href="#39224958">next</a><span>|</span><label class="collapse" for="c-39224808">[-]</label><label class="expand" for="c-39224808">[2 more]</label></div><br/><div class="children"><div class="content">Despite the typical complaints about &quot;X new thing harming the environment!!!&quot;, LLMs are as friendly as it gets, it<p>1. Consumes a minor amount of electricity (Data centers is only 2% of US electricity use, and currently AI is maybe only 5-10% of that). Its trivial compared to say metal smelting.<p>2. Consume water for cooling.<p>That&#x27;s it, there is 0 direct pollution generated from AI, and even the water use is very minor compared to say farming, and can be improved via more water efficient cooling techs.<p>The main concern is the scaling speed. As LLMs scale up 10x, 100x, 1000x, those previously very minor electricity costs can quickly become grid impacting in a decade.</div><br/><div id="39226337" class="c"><input type="checkbox" id="c-39226337" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#39224536">root</a><span>|</span><a href="#39224808">parent</a><span>|</span><a href="#39224958">next</a><span>|</span><label class="collapse" for="c-39226337">[-]</label><label class="expand" for="c-39226337">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t buy this kind of argument anymore.  How about the external effect of AI steering the entire semiconductor industry to increase GPU&#x2F;NPU capacity?</div><br/></div></div></div></div><div id="39224958" class="c"><input type="checkbox" id="c-39224958" checked=""/><div class="controls bullet"><span class="by">StopTheTechies</span><span>|</span><a href="#39224536">parent</a><span>|</span><a href="#39224808">prev</a><span>|</span><a href="#39224725">next</a><span>|</span><label class="collapse" for="c-39224958">[-]</label><label class="expand" for="c-39224958">[4 more]</label></div><br/><div class="children"><div class="content">&gt; we are only talking about $16-50k for electricity, that seems pretty small<p>I suppose this depends greatly on how you view the utility of LLMs. In a capitalist sense, sure—there&#x27;s great utility here persuading VCs to part with their coins and jobs to be replaced with correspondingly larger profit margins. But the opportunity cost of not solving major problems most of humanity can agree on seems nearly incalculably large. Not that capitalists give a shit.</div><br/><div id="39225078" class="c"><input type="checkbox" id="c-39225078" checked=""/><div class="controls bullet"><span class="by">wegfawefgawefg</span><span>|</span><a href="#39224536">root</a><span>|</span><a href="#39224958">parent</a><span>|</span><a href="#39226288">next</a><span>|</span><label class="collapse" for="c-39225078">[-]</label><label class="expand" for="c-39225078">[2 more]</label></div><br/><div class="children"><div class="content">This is a process of exploring new technology. Research is expensive and probably doesnt always yield immediate returns, but when it does you get infinite returns.<p>Imagine how not obvious the first machines must have seemed at the start of the industrial revolution. You only have to feed a man and he can work, but a machine requires iron, oil, water, fuel, engineers, operators. The up front cost for exploring early digging machines must have been absurd. And im sure some people at the time thought: &quot;Wow we could be spending this money on bread for the poor instead.&quot;<p>Arent you glad we didnt.</div><br/></div></div><div id="39226288" class="c"><input type="checkbox" id="c-39226288" checked=""/><div class="controls bullet"><span class="by">FLT8</span><span>|</span><a href="#39224536">root</a><span>|</span><a href="#39224958">parent</a><span>|</span><a href="#39225078">prev</a><span>|</span><a href="#39224725">next</a><span>|</span><label class="collapse" for="c-39226288">[-]</label><label class="expand" for="c-39226288">[1 more]</label></div><br/><div class="children"><div class="content">What if investing in AI tech like LLMs eventually allows knowledge workers to be more productive with fewer resources, and therefore ultimately frees up more people to focus on the so-called major problems?<p>Maybe we can invest more human hours in speeding up the path to zero emissions and energy abundance, or re-planting deserts, or cleaning up forever chemicals &#x2F; microplastics, or helping at-risk kids, etc etc.</div><br/></div></div></div></div></div></div><div id="39224725" class="c"><input type="checkbox" id="c-39224725" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#39224536">prev</a><span>|</span><a href="#39224974">next</a><span>|</span><label class="collapse" for="c-39224725">[-]</label><label class="expand" for="c-39224725">[1 more]</label></div><br/><div class="children"><div class="content">&quot;We intend to follow up on this release with another one soon that includes the following:<p>...<p>Weights &amp; Biases logs for our training runs.&quot;<p>That&#x27;s amazing. I&#x27;ve never seen that before in a paper of this quality. Or, any paper at all.</div><br/></div></div><div id="39224974" class="c"><input type="checkbox" id="c-39224974" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39224725">prev</a><span>|</span><a href="#39223510">next</a><span>|</span><label class="collapse" for="c-39224974">[-]</label><label class="expand" for="c-39224974">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very interesting that they went to the effort of doing complete end-to-end runs on both NVidia and AMD hardware.<p>A pity they didn&#x27;t release the speed of training, but the software is now there for someone else (not under benchmark embargo) to do that.</div><br/></div></div><div id="39223510" class="c"><input type="checkbox" id="c-39223510" checked=""/><div class="controls bullet"><span class="by">gardenfelder</span><span>|</span><a href="#39224974">prev</a><span>|</span><a href="#39224993">next</a><span>|</span><label class="collapse" for="c-39223510">[-]</label><label class="expand" for="c-39223510">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;allenai&#x2F;OLMo-7B" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;allenai&#x2F;OLMo-7B</a><p>Edit: add <a href="https:&#x2F;&#x2F;github.com&#x2F;allenai&#x2F;OLMo">https:&#x2F;&#x2F;github.com&#x2F;allenai&#x2F;OLMo</a></div><br/></div></div><div id="39224993" class="c"><input type="checkbox" id="c-39224993" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39223510">prev</a><span>|</span><a href="#39224201">next</a><span>|</span><label class="collapse" for="c-39224993">[-]</label><label class="expand" for="c-39224993">[4 more]</label></div><br/><div class="children"><div class="content">Who will be the first to do a useful Instruct-trained variant?<p>It&#x27;s a pity the Mistral 7B Instruct 0.2 dataset isn&#x27;t available because I&#x27;ve found that a much higher quality than any of the finetunes around, and I suspect we&#x27;ll have to rely on the same groups doing finetunes for this.</div><br/><div id="39225143" class="c"><input type="checkbox" id="c-39225143" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39224993">parent</a><span>|</span><a href="#39224201">next</a><span>|</span><label class="collapse" for="c-39225143">[-]</label><label class="expand" for="c-39225143">[3 more]</label></div><br/><div class="children"><div class="content">Nous just released their full instruction tuning dataset, so I dunno why someone with enough compute couldn’t do this.</div><br/><div id="39226422" class="c"><input type="checkbox" id="c-39226422" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39224993">root</a><span>|</span><a href="#39225143">parent</a><span>|</span><a href="#39226138">next</a><span>|</span><label class="collapse" for="c-39226422">[-]</label><label class="expand" for="c-39226422">[1 more]</label></div><br/><div class="children"><div class="content">The Nous finetunes of Mistral benchmark well but in practice seem worse than the original Mistral versions IMHO.<p>Of course we don&#x27;t know how to measure this so respect to them for the benchmark performance.</div><br/></div></div><div id="39226138" class="c"><input type="checkbox" id="c-39226138" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#39224993">root</a><span>|</span><a href="#39225143">parent</a><span>|</span><a href="#39226422">prev</a><span>|</span><a href="#39224201">next</a><span>|</span><label class="collapse" for="c-39226138">[-]</label><label class="expand" for="c-39226138">[1 more]</label></div><br/><div class="children"><div class="content">And Capybara be lookin&#x27; fiiine for tuning too. Seriously, though, you&#x27;re right. These are some of the highest quality generative datasets in existence, and I&#x27;m surprised more isn&#x27;t being done with them.</div><br/></div></div></div></div></div></div><div id="39224201" class="c"><input type="checkbox" id="c-39224201" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#39224993">prev</a><span>|</span><a href="#39223760">next</a><span>|</span><label class="collapse" for="c-39224201">[-]</label><label class="expand" for="c-39224201">[2 more]</label></div><br/><div class="children"><div class="content">Pretty cool that it runs on and and Nvidia</div><br/><div id="39224776" class="c"><input type="checkbox" id="c-39224776" checked=""/><div class="controls bullet"><span class="by">shwaj</span><span>|</span><a href="#39224201">parent</a><span>|</span><a href="#39223760">next</a><span>|</span><label class="collapse" for="c-39224776">[-]</label><label class="expand" for="c-39224776">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if you’re downvoted for the typo: “and” instead of “AMD”?</div><br/></div></div></div></div><div id="39223760" class="c"><input type="checkbox" id="c-39223760" checked=""/><div class="controls bullet"><span class="by">artninja1988</span><span>|</span><a href="#39224201">prev</a><span>|</span><label class="collapse" for="c-39223760">[-]</label><label class="expand" for="c-39223760">[15 more]</label></div><br/><div class="children"><div class="content">Feels like there must be 40 or so distinct open source llms now. What gives? We need some more new text to image models too... :(</div><br/><div id="39224534" class="c"><input type="checkbox" id="c-39224534" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39223760">parent</a><span>|</span><a href="#39223915">next</a><span>|</span><label class="collapse" for="c-39224534">[-]</label><label class="expand" for="c-39224534">[9 more]</label></div><br/><div class="children"><div class="content">If you read around, training a 7B model costs on the order of $85,000; the 1.4 stable diffusion release cost around $600,000 to train.<p>You don&#x27;t see a lot of 70B or larger models being released for the same reason; it&#x27;s expensive.<p>We should just be grateful for what we&#x27;re getting right now: basically, people are spending 100s of thousands of dollars on training and <i>giving the results away for free</i>. Hugging face is hosting them for free. ollama is hosting them for free. People are writing free inference engines (eg. llama.cpp) and giving them away.<p>Don&#x27;t complain. We&#x27;ve got it pretty damn good right now.</div><br/><div id="39226456" class="c"><input type="checkbox" id="c-39226456" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39224534">parent</a><span>|</span><a href="#39224557">next</a><span>|</span><label class="collapse" for="c-39226456">[-]</label><label class="expand" for="c-39226456">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you read around, training a 7B model costs on the order of $85,000; the 1.4 stable diffusion release cost around $600,000 to train.<p>SD 1.x is a ~1B parameter model, so its interesting that it cost so much more than a 7B LLM.</div><br/></div></div><div id="39224557" class="c"><input type="checkbox" id="c-39224557" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39224534">parent</a><span>|</span><a href="#39226456">prev</a><span>|</span><a href="#39225297">next</a><span>|</span><label class="collapse" for="c-39224557">[-]</label><label class="expand" for="c-39224557">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you read around, training a 7B model costs on the order of $85,000; the 1.4 stable diffusion release cost around $600,000 to train.<p>That seems remarkably cheap actually and likely getting cheaper  fairly quickly with improvements in training efficiencies I’d imagine.</div><br/></div></div><div id="39225297" class="c"><input type="checkbox" id="c-39225297" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39224534">parent</a><span>|</span><a href="#39224557">prev</a><span>|</span><a href="#39223915">next</a><span>|</span><label class="collapse" for="c-39225297">[-]</label><label class="expand" for="c-39225297">[6 more]</label></div><br/><div class="children"><div class="content">On the other hand, the systems are trained on “free” data so it kinda should be public property by default.<p>Claiming it’s fair use to suck up the entire web and pay wall the derived result is absurd argument.<p>We all created the lifeblood of LLM and we’re entitled to the product.</div><br/><div id="39225744" class="c"><input type="checkbox" id="c-39225744" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39225297">parent</a><span>|</span><a href="#39225394">next</a><span>|</span><label class="collapse" for="c-39225744">[-]</label><label class="expand" for="c-39225744">[1 more]</label></div><br/><div class="children"><div class="content">Whether that’s true or not, the fact remains that a lot of people are spending <i>real money</i> in <i>astonishing large amounts</i> and not asking for anything in return.<p>Seriously, complaining they haven’t spent enough money or didn’t spend 600k making exactly you the model you wanted is…<p>Let’s just say, ungracious.<p>Got some cake for my birthday, but it wasn’t the chocolate deluxe cream cake I wanted.<p>…just remember, the cake is pretty good, and it’s free. :)<p>Over time the cost of training models will come down and bigger open models will turn up, eventually.</div><br/></div></div><div id="39225394" class="c"><input type="checkbox" id="c-39225394" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39225297">parent</a><span>|</span><a href="#39225744">prev</a><span>|</span><a href="#39225323">next</a><span>|</span><label class="collapse" for="c-39225394">[-]</label><label class="expand" for="c-39225394">[2 more]</label></div><br/><div class="children"><div class="content">You’ve just described Google which derives most of its ad revenue from ads it places on the search engine that’s crawling the public web. It has always been thus that derivative products that provide a meaningful transformation of the input is a wholly separate piece of copyright.</div><br/><div id="39225485" class="c"><input type="checkbox" id="c-39225485" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39225394">parent</a><span>|</span><a href="#39225323">next</a><span>|</span><label class="collapse" for="c-39225485">[-]</label><label class="expand" for="c-39225485">[1 more]</label></div><br/><div class="children"><div class="content">No, this is very different. Google will link you to the NYT, you read there, and see ads. If GPT eats the web and pay walls it, they are 100% free riding.<p>Now, I also think the Google model is proven at this point to be a bad model since the web is 90% ads and SEO dogshit.  They strip mined the value, took them a while, but its nearly decimated.</div><br/></div></div></div></div><div id="39225323" class="c"><input type="checkbox" id="c-39225323" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39225297">parent</a><span>|</span><a href="#39225394">prev</a><span>|</span><a href="#39223915">next</a><span>|</span><label class="collapse" for="c-39225323">[-]</label><label class="expand" for="c-39225323">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We all created the lifeblood of LLM and we’re entitled to the product.<p>sounds so nice, yet there are going to be objections, NYT for example doesn&#x27;t think we all should be entitled to the product</div><br/><div id="39225479" class="c"><input type="checkbox" id="c-39225479" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39225323">parent</a><span>|</span><a href="#39223915">next</a><span>|</span><label class="collapse" for="c-39225479">[-]</label><label class="expand" for="c-39225479">[1 more]</label></div><br/><div class="children"><div class="content">Of course, that is partially my point: if OpenAI et al wants to make the argument anything online is fair game, then they should release the weights. If not, they have no leg to stand on.</div><br/></div></div></div></div></div></div></div></div><div id="39223915" class="c"><input type="checkbox" id="c-39223915" checked=""/><div class="controls bullet"><span class="by">chuckhend</span><span>|</span><a href="#39223760">parent</a><span>|</span><a href="#39224534">prev</a><span>|</span><a href="#39225152">next</a><span>|</span><label class="collapse" for="c-39223915">[-]</label><label class="expand" for="c-39223915">[1 more]</label></div><br/><div class="children"><div class="content">The training datasets are also available, which sets them apart a bit IMO.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;allenai&#x2F;dolma" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;allenai&#x2F;dolma</a></div><br/></div></div><div id="39225152" class="c"><input type="checkbox" id="c-39225152" checked=""/><div class="controls bullet"><span class="by">thawab</span><span>|</span><a href="#39223760">parent</a><span>|</span><a href="#39223915">prev</a><span>|</span><a href="#39223967">next</a><span>|</span><label class="collapse" for="c-39225152">[-]</label><label class="expand" for="c-39225152">[1 more]</label></div><br/><div class="children"><div class="content">Open source means i have documentation to reproduce the same results. This is only true with tinyllama and this model. The other models (llama, mistral) are free to use and not open source.</div><br/></div></div><div id="39223967" class="c"><input type="checkbox" id="c-39223967" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#39223760">parent</a><span>|</span><a href="#39225152">prev</a><span>|</span><a href="#39223843">next</a><span>|</span><label class="collapse" for="c-39223967">[-]</label><label class="expand" for="c-39223967">[1 more]</label></div><br/><div class="children"><div class="content">There are few that are &gt;1B params, competitive, and &quot;open source&quot; in the sense that the necessary ingredients to re-train are available. Models like Llama and thus its descendants (including Mistral&#x27;s public models) have weights available but not the training data.</div><br/></div></div><div id="39223843" class="c"><input type="checkbox" id="c-39223843" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39223760">parent</a><span>|</span><a href="#39223967">prev</a><span>|</span><label class="collapse" for="c-39223843">[-]</label><label class="expand" for="c-39223843">[2 more]</label></div><br/><div class="children"><div class="content">Languages, sizes, and degrees of open-ness.</div><br/><div id="39223927" class="c"><input type="checkbox" id="c-39223927" checked=""/><div class="controls bullet"><span class="by">chuckhend</span><span>|</span><a href="#39223760">root</a><span>|</span><a href="#39223843">parent</a><span>|</span><label class="collapse" for="c-39223927">[-]</label><label class="expand" for="c-39223927">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s some more commentary on their open-ness in this blog too
<a href="https:&#x2F;&#x2F;www.interconnects.ai&#x2F;p&#x2F;olmo" rel="nofollow">https:&#x2F;&#x2F;www.interconnects.ai&#x2F;p&#x2F;olmo</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>