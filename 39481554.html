<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708765281071" as="style"/><link rel="stylesheet" href="styles.css?v=1708765281071"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/google/gemma.cpp">Gemma.cpp: lightweight, standalone C++ inference engine for Gemma models</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>mfiguiere</span> | <span>139 comments</span></div><br/><div><div id="39482046" class="c"><input type="checkbox" id="c-39482046" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39484076">next</a><span>|</span><label class="collapse" for="c-39482046">[-]</label><label class="expand" for="c-39482046">[43 more]</label></div><br/><div class="children"><div class="content">Hi, one of the authors austin here. Happy to answer any questions the best I can.<p>To get a few common questions out of the way:<p>- This is separate &#x2F; independent of llama.cpp &#x2F; ggml. I&#x27;m a big fan of that project and it was an inspiration (we say as much in the README). I&#x27;ve been a big advocate of gguf + llama.cpp support for gemma and am happy for people to use that.<p>- how is it different than inference runtime X? gemma.cpp is a direct implementation of gemma, in its current form it&#x27;s aimed at experimentation + research and portability + easy modifiable rather than a general purpose deployment framework.<p>- this initial implementation is cpu simd centric. we&#x27;re exploring options for portable gpu support but the cool thing is it will build and run on a lot of environments you might not expect an llm to run, so long as you have the memory to load the model.<p>- I&#x27;ll let other colleagues answer questions about the Gemma model itself, this is a C++ implementation of the model, but relatively independent of the model training process.<p>- Although this is from Google, we&#x27;re a very small team that wanted such a codebase to exist. We have lots of plans to use it ourselves and we hope other people like it and find it useful.<p>- I wrote a twitter thread on this project here: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;austinvhuang&#x2F;status&#x2F;1760375890448429459" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;austinvhuang&#x2F;status&#x2F;1760375890448429459</a></div><br/><div id="39483290" class="c"><input type="checkbox" id="c-39483290" checked=""/><div class="controls bullet"><span class="by">leminimal</span><span>|</span><a href="#39482046">parent</a><span>|</span><a href="#39482388">next</a><span>|</span><label class="collapse" for="c-39483290">[-]</label><label class="expand" for="c-39483290">[2 more]</label></div><br/><div class="children"><div class="content">Kudos on your release! I know this was just made available but<p>- Somewhere the README, consider adding the need for a `-DWEIGHT_TYPE=hwy::bfloat16_t` flag for non-sfp. Maybe around step 3.<p>- The README should explicitly say somehere that there&#x27;s no GPU support (at the moment)<p>- &quot;Failed to read cache gating_ein_0 (error 294)&quot; is pretty obscure. I think even &quot;(error at line number 294)&quot; would be a big improvement when it fails to FindKey.<p>- There&#x27;s something odd about the 2b vs 7b model. The 2b will claim its trained by Google but the 7b won&#x27;t. Were these trained on the same data?<p>- Are the .sbs weights the same weights as the GGUF? I&#x27;m getting different answers compared to llama.cpp. Do you know of a good way to compare the two? Any way to make both deterministic? Or even dump probability distributions on the first (or any) token to compare?</div><br/><div id="39483776" class="c"><input type="checkbox" id="c-39483776" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39483290">parent</a><span>|</span><a href="#39482388">next</a><span>|</span><label class="collapse" for="c-39483776">[-]</label><label class="expand" for="c-39483776">[1 more]</label></div><br/><div class="children"><div class="content">Yes - thanks for pointing that out. The README is being updated, you can see an  updated WIP in the dev branch: <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp&#x2F;tree&#x2F;dev?tab=readme-ov-file">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp&#x2F;tree&#x2F;dev?tab=readme-ov-f...</a>
and improving error messages is a high priority.<p>The weights should be the same across formats, but it&#x27;s easy for differences to arise due to quantization and&#x2F;or subtle implementation differences. Minor implementation differences has been a pain point in the ML ecosystem for a while (w&#x2F; IRs, onnx, python vs. runtime, etc.), but hopefully the differences aren&#x27;t too significant (if they are, it&#x27;s a bug in one of the implementations).<p>There were quantization fixes like <a href="https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760418864418934922" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760418864418934922</a> and other patches happening, but it may take a few days for patches to work their way through the ecosystem.</div><br/></div></div></div></div><div id="39482388" class="c"><input type="checkbox" id="c-39482388" checked=""/><div class="controls bullet"><span class="by">beoberha</span><span>|</span><a href="#39482046">parent</a><span>|</span><a href="#39483290">prev</a><span>|</span><a href="#39482302">next</a><span>|</span><label class="collapse" for="c-39482388">[-]</label><label class="expand" for="c-39482388">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Although this is from Google, we&#x27;re a very small team that wanted such a codebase to exist. We have lots of plans to use it ourselves and we hope other people like it and find it useful.<p>This is really cool, Austin. Kudos to your team!</div><br/><div id="39482704" class="c"><input type="checkbox" id="c-39482704" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39482388">parent</a><span>|</span><a href="#39482302">next</a><span>|</span><label class="collapse" for="c-39482704">[-]</label><label class="expand" for="c-39482704">[2 more]</label></div><br/><div class="children"><div class="content">Thanks so much!<p>Everyone working on this self-selected into contributing, so I think of it less as my team than ... a team?<p>Specifically want to call out: Jan Wassenberg (author of <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;highway">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;highway</a>) and I started gemma.cpp as a small project just a few months ago + Phil Culliton, Dan Zheng, and Paul Chang + of course the GDM Gemma team.</div><br/><div id="39485061" class="c"><input type="checkbox" id="c-39485061" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39482704">parent</a><span>|</span><a href="#39482302">next</a><span>|</span><label class="collapse" for="c-39485061">[-]</label><label class="expand" for="c-39485061">[1 more]</label></div><br/><div class="children"><div class="content">Huge +1, this has definitely been a self-forming collective of people who love great AI, great research, and the open community.<p>Austin and Jan are truly amazing. The optimization work is genuinely outstanding; I get incredible CPU performance on Gemma.cpp for inference. Thanks for all of the awesomeness, Austin =)</div><br/></div></div></div></div></div></div><div id="39482302" class="c"><input type="checkbox" id="c-39482302" checked=""/><div class="controls bullet"><span class="by">rgbrgb</span><span>|</span><a href="#39482046">parent</a><span>|</span><a href="#39482388">prev</a><span>|</span><a href="#39482342">next</a><span>|</span><label class="collapse" for="c-39482302">[-]</label><label class="expand" for="c-39482302">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for releasing this! What is your use case for this rather than llama.cpp? For the on-device AI stuff I mostly do, llama.cpp is better because of GPU&#x2F;metal offloading.</div><br/><div id="39482581" class="c"><input type="checkbox" id="c-39482581" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39482302">parent</a><span>|</span><a href="#39482342">next</a><span>|</span><label class="collapse" for="c-39482581">[-]</label><label class="expand" for="c-39482581">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp is great, if it fit your needs you can use it. I think at this point llama.cpp is effectively a platform that&#x27;s hardened for production.<p>In its current form, I think of gemma.cpp is more of a direct model implementation (somewhere between the minimalism of llama2.c and the generality of ggml).<p>I tend to think of 3 modes of usage:<p>- hacking on inference internals - there&#x27;s very little indirection, no IRs, the model is just code, so if you want to add support for your own runtime support for sparsity&#x2F;quantization&#x2F;model compression&#x2F;etc. and demo it working with gemma, there&#x27;s minimal barriers to do so<p>- implementing experimental frontends - i&#x27;ll add some examples of this in the very near future. but you&#x27;re free to get pretty creative with terminal UIs, code that interact with model internals like the KV cache, accepting&#x2F;rejecting tokens etc.<p>- interacting with the model locally with a small program - of course there&#x27;s other options for this but hopefully this is one way to play with gemma w&#x2F; minimal fuss.</div><br/></div></div></div></div><div id="39482342" class="c"><input type="checkbox" id="c-39482342" checked=""/><div class="controls bullet"><span class="by">dartharva</span><span>|</span><a href="#39482046">parent</a><span>|</span><a href="#39482302">prev</a><span>|</span><a href="#39483910">next</a><span>|</span><label class="collapse" for="c-39482342">[-]</label><label class="expand" for="c-39482342">[3 more]</label></div><br/><div class="children"><div class="content">So... llamafile release?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile</a></div><br/><div id="39482387" class="c"><input type="checkbox" id="c-39482387" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39482342">parent</a><span>|</span><a href="#39483910">next</a><span>|</span><label class="collapse" for="c-39482387">[-]</label><label class="expand" for="c-39482387">[2 more]</label></div><br/><div class="children"><div class="content">gguf files are out there, so anyone should be able to do this! are people looking for an &quot;official&quot; version?<p>ps i&#x27;m a fan of cosmopolitan as well.</div><br/><div id="39488727" class="c"><input type="checkbox" id="c-39488727" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39482387">parent</a><span>|</span><a href="#39483910">next</a><span>|</span><label class="collapse" for="c-39488727">[-]</label><label class="expand" for="c-39488727">[1 more]</label></div><br/><div class="children"><div class="content">Cosmopolitan is a fan of you :-) great work on gemma.cpp. I&#x27;m really impressed with it so far.</div><br/></div></div></div></div></div></div><div id="39483910" class="c"><input type="checkbox" id="c-39483910" checked=""/><div class="controls bullet"><span class="by">dankle</span><span>|</span><a href="#39482046">parent</a><span>|</span><a href="#39482342">prev</a><span>|</span><a href="#39484345">next</a><span>|</span><label class="collapse" for="c-39483910">[-]</label><label class="expand" for="c-39483910">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the reason to not integrate with llama.cpp instead of a separate app? In what ways this better than llama.cpp?</div><br/><div id="39484166" class="c"><input type="checkbox" id="c-39484166" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39483910">parent</a><span>|</span><a href="#39484345">next</a><span>|</span><label class="collapse" for="c-39484166">[-]</label><label class="expand" for="c-39484166">[2 more]</label></div><br/><div class="children"><div class="content">On uses, see <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39481554#39482302">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39481554#39482302</a> and on llama.cpp support - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39481554">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39481554</a><p>Gemma support has been added to llama.cpp, and we&#x27;re more than happy to see people use it there.</div><br/><div id="39484572" class="c"><input type="checkbox" id="c-39484572" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39484166">parent</a><span>|</span><a href="#39484345">next</a><span>|</span><label class="collapse" for="c-39484572">[-]</label><label class="expand" for="c-39484572">[1 more]</label></div><br/><div class="children"><div class="content">I think on uses you meant to link to <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39482581">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39482581</a> child of <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39481554#39482302">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39481554#39482302</a> ?<p>side note:  imagine how gnarly those urls would be if HN used UUIDs instead of integers for IDs :-D</div><br/></div></div></div></div></div></div><div id="39483679" class="c"><input type="checkbox" id="c-39483679" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39482046">parent</a><span>|</span><a href="#39483680">prev</a><span>|</span><a href="#39482879">next</a><span>|</span><label class="collapse" for="c-39483679">[-]</label><label class="expand" for="c-39483679">[22 more]</label></div><br/><div class="children"><div class="content">Hi Austin, what say you about how the Gemma rollout was handled, issues raised, and atmosphere around the office? :)</div><br/><div id="39485130" class="c"><input type="checkbox" id="c-39485130" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39483679">parent</a><span>|</span><a href="#39486305">next</a><span>|</span><label class="collapse" for="c-39485130">[-]</label><label class="expand" for="c-39485130">[20 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not Austin, but I am Tris, the friendly neighborhood product person on Gemma. Overall, I think that the main feeling is: incredibly relieved to have had the launch go as smoothly as it has! The complexity of the launch is truly astounding:<p>1) Reference implementations in JAX, PyTorch, TF with Keras 3, MaxText&#x2F;JAX, more...<p>2) Full integration at launch with HF including Transformers + optimization therein<p>3) TensorRT-LLM and full NVIDIA opt across the stack in partnership with that team (mentioned on the NVIDIA earnings call by Jensen, even)<p>4) More developer surfaces than you can shake a stick at: Kaggle, Colab, Gemma.cpp,  GGUF<p>5) Comms landing with full coordination from Sundar + Demis + Jeff Dean, not to mention positive articles in NYT, Verge, Fortune, etc.<p>6) Full Google Cloud launches across several major products, including Vertex and GKE<p>7) Launched globally and with a permissive set of terms that enable developers to do awesome stuff<p>Pulling that off without any <i>major</i> SNAFUs is a huge relief for the team. We&#x27;re excited by the potential of using all of those surfaces and the launch momentum to build a lot more great things for you all =)</div><br/><div id="39485264" class="c"><input type="checkbox" id="c-39485264" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485130">parent</a><span>|</span><a href="#39485398">next</a><span>|</span><label class="collapse" for="c-39485264">[-]</label><label class="expand" for="c-39485264">[1 more]</label></div><br/><div class="children"><div class="content">I am not a fan of a lot of what Google does, but congratulations! Thatâs a massive undertaking and it is bringing the field forward. I am glad you could do this, and hope youâll have many other successful releases.<p>Now, Iâm off playing with a new toy :)</div><br/></div></div><div id="39485398" class="c"><input type="checkbox" id="c-39485398" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485130">parent</a><span>|</span><a href="#39485264">prev</a><span>|</span><a href="#39486305">next</a><span>|</span><label class="collapse" for="c-39485398">[-]</label><label class="expand" for="c-39485398">[18 more]</label></div><br/><div class="children"><div class="content">Has there been any negative articles or valid criticism at all in your opinion? =)</div><br/><div id="39485458" class="c"><input type="checkbox" id="c-39485458" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485398">parent</a><span>|</span><a href="#39486305">next</a><span>|</span><label class="collapse" for="c-39485458">[-]</label><label class="expand" for="c-39485458">[17 more]</label></div><br/><div class="children"><div class="content">Always -- anything that comes with the Google name attached always attracts some negativity. There&#x27;s plenty of valid criticism, most of which we hope to address in the coming weeks and months =).</div><br/><div id="39485479" class="c"><input type="checkbox" id="c-39485479" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485458">parent</a><span>|</span><a href="#39486305">next</a><span>|</span><label class="collapse" for="c-39485479">[-]</label><label class="expand" for="c-39485479">[16 more]</label></div><br/><div class="children"><div class="content">Can you without addressing just acknowledge some of it here? Specific examples? =)<p>&gt; not to mention positive articles in NYT, Verge, Fortune, etc.<p>You in fact are mentioning them and only them. I was wondering if you can simply mention the negative ones. Otherwise it sort of sounded at first like its all roses. ;)</div><br/><div id="39485682" class="c"><input type="checkbox" id="c-39485682" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485479">parent</a><span>|</span><a href="#39486305">next</a><span>|</span><label class="collapse" for="c-39485682">[-]</label><label class="expand" for="c-39485682">[15 more]</label></div><br/><div class="children"><div class="content">I mean, many articles will have a negative cast because of the need for clicks -- e.g., the Verge&#x27;s launch article is entitled &quot;Google Gemma: because Google doesnât want to give away Gemini yet&quot; -- which I think is both an unfair characterization (given the free tier of Gemini Pro) and unnecessarily inflammatory.<p>Legitimate criticisms include not working correctly out of the box for llama.cpp due to repetition penalty and vocab size, some snafus on chat templates with huggingface, the fact that they&#x27;re not larger-sized models, etc. Lots of the issues are already fixed, and we&#x27;re committed to making sure these models are great.<p>Honestly, not sure what you&#x27;re trying to get at here -- are you trying to &quot;gotcha&quot; the fact that not everything is perfect? That&#x27;s true for any launch.</div><br/><div id="39487807" class="c"><input type="checkbox" id="c-39487807" checked=""/><div class="controls bullet"><span class="by">Permik</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485682">parent</a><span>|</span><a href="#39488885">next</a><span>|</span><label class="collapse" for="c-39487807">[-]</label><label class="expand" for="c-39487807">[1 more]</label></div><br/><div class="children"><div class="content">I thought that reflecting what went poorly is really informative and inspiring! It really shows how you are taking this seriously, and iterating on a great project that you&#x27;re building better and better, thanks for the insight and for a peek behind the curtain</div><br/></div></div><div id="39488885" class="c"><input type="checkbox" id="c-39488885" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485682">parent</a><span>|</span><a href="#39487807">prev</a><span>|</span><a href="#39488486">next</a><span>|</span><label class="collapse" for="c-39488885">[-]</label><label class="expand" for="c-39488885">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Legitimate criticisms include not working correctly out of the box for llama.cpp<p>I don&#x27;t think that&#x27;s a legitimate criticism. Especially not for something that just launched. You should be helping your own project first, before you help others. For example, it&#x27;d be nice to see AVX2 work as well as AVX512 with gemma.cpp.</div><br/></div></div><div id="39488486" class="c"><input type="checkbox" id="c-39488486" checked=""/><div class="controls bullet"><span class="by">cornel_io</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39485682">parent</a><span>|</span><a href="#39488885">prev</a><span>|</span><a href="#39485795">next</a><span>|</span><label class="collapse" for="c-39488486">[-]</label><label class="expand" for="c-39488486">[1 more]</label></div><br/><div class="children"><div class="content">I think he&#x27;s trying to bring up the racial image gen bias stuff that&#x27;s going on with Gemini, but for some reason won&#x27;t say it. He also doesn&#x27;t appear to realize there is a difference between the two products&#x2F;teams&#x2F;launches...</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39482879" class="c"><input type="checkbox" id="c-39482879" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39482046">parent</a><span>|</span><a href="#39483679">prev</a><span>|</span><a href="#39484076">next</a><span>|</span><label class="collapse" for="c-39482879">[-]</label><label class="expand" for="c-39482879">[5 more]</label></div><br/><div class="children"><div class="content">Cool, any plans on adding K quants, an API server and&#x2F;or a python wrapper? I really doubt most people want to use it as a cpp dependency and run models at FP16.</div><br/><div id="39483050" class="c"><input type="checkbox" id="c-39483050" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39482879">parent</a><span>|</span><a href="#39484076">next</a><span>|</span><label class="collapse" for="c-39483050">[-]</label><label class="expand" for="c-39483050">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a custom 8-bit quantization (SFP), it&#x27;s what we recommend. At 16 bit, we do bfloat16 instead of fp16 thanks to <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;highway">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;highway</a>, even on CPU. Other quants - stay tuned.<p>python wrapper - if you want to run the model in python I feel like there&#x27;s already a lot of more mature options available (see the model variations at <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma</a>) , but if people really want this and have something they want to do with a python wrapper that can&#x27;t be done with existing options let me know. (similar thoughts wrt to API servers).</div><br/><div id="39483420" class="c"><input type="checkbox" id="c-39483420" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39483050">parent</a><span>|</span><a href="#39484076">next</a><span>|</span><label class="collapse" for="c-39483420">[-]</label><label class="expand" for="c-39483420">[3 more]</label></div><br/><div class="children"><div class="content">In my experience there&#x27;s really no reason to run any model above Q6_K, the performance is identical and you shave off almost 2 GB of VRAM of a 7B model compared to Q8. To those of us with single digit amounts, that&#x27;s highly significant. But most people seem to go for 4 bits anyway and it&#x27;s the AWQ standard too. If you think it&#x27;ll make the model look bad, then don&#x27;t worry, it&#x27;s only the relative performance that matters.<p>I would think that having an OpenAI standard compatible API would be a higher priority over a python wrapper, since then it can act as a drop in replacement for most any backend.</div><br/><div id="39483533" class="c"><input type="checkbox" id="c-39483533" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39483420">parent</a><span>|</span><a href="#39484076">next</a><span>|</span><label class="collapse" for="c-39483533">[-]</label><label class="expand" for="c-39483533">[2 more]</label></div><br/><div class="children"><div class="content">A nice side effect of implementing cpu simd is you just need enough regular RAM, which tends to be far less scarce than VRAM. Nonetheless, I get your point that more aggressive quantization is valuable + will share with the modeling team.</div><br/><div id="39483593" class="c"><input type="checkbox" id="c-39483593" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39482046">root</a><span>|</span><a href="#39483533">parent</a><span>|</span><a href="#39484076">next</a><span>|</span><label class="collapse" for="c-39483593">[-]</label><label class="expand" for="c-39483593">[1 more]</label></div><br/><div class="children"><div class="content">True, it&#x27;s the only way I can for example run Mixtral on a 8GB GPU, but main memory will always have more latency so some tradeoff tends to be worth it. And parts like the prompt batch buffer and most of the context generally have to be in VRAM if you want to use cuBLAS, with OpenBLAS it&#x27;s maybe less of a problem, but it is slower.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39484076" class="c"><input type="checkbox" id="c-39484076" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#39482046">prev</a><span>|</span><a href="#39482601">next</a><span>|</span><label class="collapse" for="c-39484076">[-]</label><label class="expand" for="c-39484076">[20 more]</label></div><br/><div class="children"><div class="content">I know a lot of people chide Google for being behind OpenAI in their commercial offerings. We also dunk on them for the over-protective nature of their fine-tuning.<p>But Google is scarily capable on the LLM front and we shouldn&#x27;t count them out. OpenAI might have the advantage of being quick to move, but when the juggernaut gets passed its resting inertia and starts to gain momentum it is going to leave an impression.<p>That became clear to me after watching the recent Jeff Dean video [1] which was posted a few days ago. The depth of institutional knowledge that is going to be unlocked inside Google is actually frightening for me to consider.<p>I hope the continued competition on the open source front, which we can really thank Facebook and Llama for, keeps these behemoths sharing. As OpenAI moves further from its original mission into capitalizing on its technological lead, we have to remember why the original vision they had is important.<p>So thank you, Google, for this.<p>1. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=oSCRZkSQ1CE&amp;ab_channel=RiceKenKennedyInstitute" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=oSCRZkSQ1CE&amp;ab_channel=RiceK...</a></div><br/><div id="39486258" class="c"><input type="checkbox" id="c-39486258" checked=""/><div class="controls bullet"><span class="by">llm_nerd</span><span>|</span><a href="#39484076">parent</a><span>|</span><a href="#39485340">next</a><span>|</span><label class="collapse" for="c-39486258">[-]</label><label class="expand" for="c-39486258">[4 more]</label></div><br/><div class="children"><div class="content">While I generally agree with you, who has ever counted Google out? We&#x27;ve made fun of Google for lagging while they instead spend their engineering time renaming projects and performing algorithmic white-erasure, but we all knew they&#x27;re a potent force.<p>Google has as much or more computing power than anyone. They&#x27;re massively capitalized and have a market cap of almost $2T and colossal cashflow, and have the ability to throw enormous resources at the problem until they have a competitor. They have an enormous, benchmark-setting amount of data across their various projects to train on. That we&#x27;re talking like they&#x27;re some scrappy upstart is super weird.<p>&gt;As OpenAI moves further from its original mission into capitalizing on its technological lead, we have to remember why the original vision they had is important.<p>I&#x27;m way more cynical about the open source models released by the megas, and OpenAI is probably the most honest about their intentions. Meta and Google are releasing these models arguably to kneecap any possible next OpenAI. They want to basically set the market value of anything below state of the art at $0.00, ensuring that there is no breathing room below the $2T cos. These models (Llama, Gemma, etc) are fun toys, but in the end they&#x27;re completely uncompetitive and will yield zero &quot;wins&quot;, so to speak.</div><br/><div id="39486890" class="c"><input type="checkbox" id="c-39486890" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39486258">parent</a><span>|</span><a href="#39486299">next</a><span>|</span><label class="collapse" for="c-39486890">[-]</label><label class="expand" for="c-39486890">[2 more]</label></div><br/><div class="children"><div class="content">I certainly would not count out Google&#x27;s engineering talent.  But all the technical expertise in the world won&#x27;t matter when the leadership is incompetent and dysfunctional.  Rolling out a new product takes vision, and it means taking some risks.  This is diametrically opposed to how Google operates today.  Gemini could be years ahead of ChatGPT (and maybe it is now, if it weren&#x27;t neutered), but Google&#x27;s current leadership would have no idea what to do with it.<p>Google has the technical resources to become a major player here, maybe even the dominant player.  But it won&#x27;t happen under current management.  I won&#x27;t count out Google entirely, and there&#x27;s still time for the company to be saved.  It starts with new leadership.</div><br/><div id="39489230" class="c"><input type="checkbox" id="c-39489230" checked=""/><div class="controls bullet"><span class="by">okdood64</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39486890">parent</a><span>|</span><a href="#39486299">next</a><span>|</span><label class="collapse" for="c-39489230">[-]</label><label class="expand" for="c-39489230">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Google has the technical resources to become a major player here<p>Wait, it&#x27;s not a major player in ML&#x2F;AI?</div><br/></div></div></div></div><div id="39486299" class="c"><input type="checkbox" id="c-39486299" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39486258">parent</a><span>|</span><a href="#39486890">prev</a><span>|</span><a href="#39485340">next</a><span>|</span><label class="collapse" for="c-39486299">[-]</label><label class="expand" for="c-39486299">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Meta and Google are releasing these models arguably to kneecap any possible next OpenAI. They want to basically set the market value of anything below state of the art at $0.00, ensuring that there is no breathing room below the $2T cos<p>Never thought about it that way, but it makes a lot of sense. Itâs also true these models are not up to par with SOTA no matter what the benchmarks say</div><br/></div></div></div></div><div id="39485340" class="c"><input type="checkbox" id="c-39485340" checked=""/><div class="controls bullet"><span class="by">brigadier132</span><span>|</span><a href="#39484076">parent</a><span>|</span><a href="#39486258">prev</a><span>|</span><a href="#39484258">next</a><span>|</span><label class="collapse" for="c-39485340">[-]</label><label class="expand" for="c-39485340">[4 more]</label></div><br/><div class="children"><div class="content">There was a podcast yesterday that explained well why Google is in a tough position.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;-i9AGk3DJ90?t=616" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;-i9AGk3DJ90?t=616</a><p>In essence, Google already rules information retrieval. Their margins are insane. Switching to LLM based search cuts into their margins and increases their costs dramatically. Also, the advantage they&#x27;ve built over decades has been cut down.<p>All of this means there is potential for less profit and a shrinking valuation. A shrinking valuation means issues with employee retention and it could lead to long term stagnation.</div><br/><div id="39486332" class="c"><input type="checkbox" id="c-39486332" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39485340">parent</a><span>|</span><a href="#39486925">next</a><span>|</span><label class="collapse" for="c-39486332">[-]</label><label class="expand" for="c-39486332">[1 more]</label></div><br/><div class="children"><div class="content">The Innovator&#x27;s Dilemma over and over again.</div><br/></div></div><div id="39486925" class="c"><input type="checkbox" id="c-39486925" checked=""/><div class="controls bullet"><span class="by">brikym</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39485340">parent</a><span>|</span><a href="#39486332">prev</a><span>|</span><a href="#39484258">next</a><span>|</span><label class="collapse" for="c-39486925">[-]</label><label class="expand" for="c-39486925">[2 more]</label></div><br/><div class="children"><div class="content">Iâm sure Kodak had the same problem with the digital camera.</div><br/><div id="39489558" class="c"><input type="checkbox" id="c-39489558" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39486925">parent</a><span>|</span><a href="#39484258">next</a><span>|</span><label class="collapse" for="c-39489558">[-]</label><label class="expand" for="c-39489558">[1 more]</label></div><br/><div class="children"><div class="content">They did. They invented and patented the digital camera back in the 70s, refused to improve on it for fear of eating their own market base, and then went bankrupt.<p>* <a href="https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;first-digital-camera-history" rel="nofollow">https:&#x2F;&#x2F;spectrum.ieee.org&#x2F;first-digital-camera-history</a></div><br/></div></div></div></div></div></div><div id="39484258" class="c"><input type="checkbox" id="c-39484258" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39484076">parent</a><span>|</span><a href="#39485340">prev</a><span>|</span><a href="#39484496">next</a><span>|</span><label class="collapse" for="c-39484258">[-]</label><label class="expand" for="c-39484258">[3 more]</label></div><br/><div class="children"><div class="content">Realistically, if Google has all this talent, they should have gotten the juggernaut moving in 2020.<p>Google has had <i>years</i> to get to this stage, and they&#x27;ve lost a lot of the talent that made their initial big splashes to OAI and competitors. Try finding someone on a sparse MoE paper from Google prior to 2022 who is still working there and not at OAI.<p>With respect, they can hardly even beat Mistral, resorting to rounding down a 7.8b model (w&#x2F;o embeddings) to 7b.</div><br/><div id="39484527" class="c"><input type="checkbox" id="c-39484527" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39484258">parent</a><span>|</span><a href="#39484496">next</a><span>|</span><label class="collapse" for="c-39484527">[-]</label><label class="expand" for="c-39484527">[2 more]</label></div><br/><div class="children"><div class="content">Organizational dysfunction can squash&#x2F;squander even the most talented engineers.  Especially in a big org in big tech.  My bet is that their inability to deliver before is probably a result of non-comittal funders&#x2F;decision makers, product whiplash, corporate politics, and other non-technical challenges.<p>Google has been the home of the talent for many years.  They came on my radar in the late 00s when I used Peter Norvig&#x27;s textbook in college, and they hired Ray Kurzweil in like 2012 or 2013 IIRC.  They were hiring ML PhDs with talent for many years, and they pioneered most of the major innovations.  They just got behind on productizing and shipping.</div><br/><div id="39484968" class="c"><input type="checkbox" id="c-39484968" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39484527">parent</a><span>|</span><a href="#39484496">next</a><span>|</span><label class="collapse" for="c-39484968">[-]</label><label class="expand" for="c-39484968">[1 more]</label></div><br/><div class="children"><div class="content">Right, which was fine for them before there was major competition. But starting in 2020, they have basically attrited most of their talented labor force to OAI and competitors who were not similarly dysfunctional.</div><br/></div></div></div></div></div></div><div id="39484496" class="c"><input type="checkbox" id="c-39484496" checked=""/><div class="controls bullet"><span class="by">dguest</span><span>|</span><a href="#39484076">parent</a><span>|</span><a href="#39484258">prev</a><span>|</span><a href="#39486466">next</a><span>|</span><label class="collapse" for="c-39484496">[-]</label><label class="expand" for="c-39484496">[3 more]</label></div><br/><div class="children"><div class="content">Maybe someone who knows google better can answer my question here: are they behind simply because LLMs are not really their core business? In other words, it wasn&#x27;t (and still isn&#x27;t) obvious that LLMs will help them sell add space.<p>And of course writing that gives me a terrible realization: product placement in LLMs is going to be a very big thing in the near future.</div><br/><div id="39484832" class="c"><input type="checkbox" id="c-39484832" checked=""/><div class="controls bullet"><span class="by">elwell</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39484496">parent</a><span>|</span><a href="#39484547">next</a><span>|</span><label class="collapse" for="c-39484832">[-]</label><label class="expand" for="c-39484832">[1 more]</label></div><br/><div class="children"><div class="content">LLM bad because cannibalizes search ads. Wait as long as possible. OpenAI opens pandora&#x27;s box. Now full speed ahead; catch up and overtake.</div><br/></div></div><div id="39484547" class="c"><input type="checkbox" id="c-39484547" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39484496">parent</a><span>|</span><a href="#39484832">prev</a><span>|</span><a href="#39486466">next</a><span>|</span><label class="collapse" for="c-39484547">[-]</label><label class="expand" for="c-39484547">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m an outsider and am speculating based on what I&#x27;ve heard, so maybe I shouldn&#x27;t even comment, but to me it seems like it&#x27;s been entirely corporate&#x2F;organizational reasons.  Non-serious funding, shifting priorities, personnel transfers&#x2F;fluctuations, internal fragmentation, and more.  Lack of talent has never been their problem.</div><br/></div></div></div></div><div id="39486466" class="c"><input type="checkbox" id="c-39486466" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39484076">parent</a><span>|</span><a href="#39484496">prev</a><span>|</span><a href="#39482601">next</a><span>|</span><label class="collapse" for="c-39486466">[-]</label><label class="expand" for="c-39486466">[5 more]</label></div><br/><div class="children"><div class="content">There&#x27;s nothing provided here other than Jeff Dean gave a stock entry-level presentation to students at Rice, therefore &quot;The depth of institutional knowledge that is going to be unlocked inside Google is actually frightening for me to consider.&quot;<p>You should see Google&#x27;s turnover numbers from 4 years ago, much less now.<p>It&#x27;s been years, it&#x27;s broken internally, we see the results.<p>Here, we&#x27;re in awe of 1KLOC of C++ code that runs inference on the CPU.<p>Nobody serious is running inference on CPU unless you&#x27;re on the extreme cutting edge. (ex. I need to on Android and on the Chrome OS Linux VM, but I still use llama.cpp because it does support GPU everywhere else)<p>I&#x27;m not sure what else to say.<p>(n.b. i am a xoogler)</div><br/><div id="39489103" class="c"><input type="checkbox" id="c-39489103" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39486466">parent</a><span>|</span><a href="#39489527">next</a><span>|</span><label class="collapse" for="c-39489103">[-]</label><label class="expand" for="c-39489103">[1 more]</label></div><br/><div class="children"><div class="content">We understand that some teams prefer to use CPU even when mobile GPU would be available.<p>This code is also intended to facilitate research &amp; experimentation, which may not fall under your definition of &#x27;serious&#x27; :)</div><br/></div></div><div id="39489527" class="c"><input type="checkbox" id="c-39489527" checked=""/><div class="controls bullet"><span class="by">iaseiadit</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39486466">parent</a><span>|</span><a href="#39489103">prev</a><span>|</span><a href="#39488900">next</a><span>|</span><label class="collapse" for="c-39489527">[-]</label><label class="expand" for="c-39489527">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You should see Google&#x27;s turnover numbers from 4 years ago, much less now.<p>High turnover was industry-wide a few years back because pay went through the roof and job hopping was the best way to capture that.<p>I suspect itâs lower now, following mass layoffs and substantially fewer openings.</div><br/></div></div><div id="39488900" class="c"><input type="checkbox" id="c-39488900" checked=""/><div class="controls bullet"><span class="by">whitten</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39486466">parent</a><span>|</span><a href="#39489527">prev</a><span>|</span><a href="#39482601">next</a><span>|</span><label class="collapse" for="c-39488900">[-]</label><label class="expand" for="c-39488900">[2 more]</label></div><br/><div class="children"><div class="content">Are there any online transcripts or recordings of the Rice presentation from Jeff Dean ?</div><br/><div id="39488976" class="c"><input type="checkbox" id="c-39488976" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39484076">root</a><span>|</span><a href="#39488900">parent</a><span>|</span><a href="#39482601">next</a><span>|</span><label class="collapse" for="c-39488976">[-]</label><label class="expand" for="c-39488976">[1 more]</label></div><br/><div class="children"><div class="content">Yes, footer of parent: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;oSCRZkSQ1CE?si=Na1d1cK3TApDhkSO" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;oSCRZkSQ1CE?si=Na1d1cK3TApDhkSO</a></div><br/></div></div></div></div></div></div></div></div><div id="39482601" class="c"><input type="checkbox" id="c-39482601" checked=""/><div class="controls bullet"><span class="by">ofermend</span><span>|</span><a href="#39484076">prev</a><span>|</span><a href="#39482751">next</a><span>|</span><label class="collapse" for="c-39482601">[-]</label><label class="expand" for="c-39482601">[3 more]</label></div><br/><div class="children"><div class="content">Awesome work on getting this done so quickly.
We just added Gemma to the HHEM leaderboard - <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;vectara&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;vectara&#x2F;leaderboard</a>, and as you can see there its doing pretty good in terms of low hallucination rate, relative to other small models.</div><br/><div id="39482829" class="c"><input type="checkbox" id="c-39482829" checked=""/><div class="controls bullet"><span class="by">swozey</span><span>|</span><a href="#39482601">parent</a><span>|</span><a href="#39482751">next</a><span>|</span><label class="collapse" for="c-39482829">[-]</label><label class="expand" for="c-39482829">[2 more]</label></div><br/><div class="children"><div class="content">&gt; LLM hallucinations<p>I wasn&#x27;t familiar with the term, good article - <a href="https:&#x2F;&#x2F;masterofcode.com&#x2F;blog&#x2F;hallucinations-in-llms-what-you-need-to-know-before-integration" rel="nofollow">https:&#x2F;&#x2F;masterofcode.com&#x2F;blog&#x2F;hallucinations-in-llms-what-yo...</a></div><br/><div id="39483402" class="c"><input type="checkbox" id="c-39483402" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#39482601">root</a><span>|</span><a href="#39482829">parent</a><span>|</span><a href="#39482751">next</a><span>|</span><label class="collapse" for="c-39483402">[-]</label><label class="expand" for="c-39483402">[1 more]</label></div><br/><div class="children"><div class="content">Karpathy offers a more concise (and whimsical) explanation <a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1733299213503787018" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1733299213503787018</a></div><br/></div></div></div></div></div></div><div id="39482751" class="c"><input type="checkbox" id="c-39482751" checked=""/><div class="controls bullet"><span class="by">swozey</span><span>|</span><a href="#39482601">prev</a><span>|</span><a href="#39482895">next</a><span>|</span><label class="collapse" for="c-39482751">[-]</label><label class="expand" for="c-39482751">[3 more]</label></div><br/><div class="children"><div class="content">The velocity of the LLM open source ecosystem is absolutely insane.<p>I just got into hobby projects with diffusion a week ago and I&#x27;m seeing non-stop releases. It&#x27;s hard to keep up. It&#x27;s a firehose of information, acronyms, code etc.<p>It&#x27;s been a great python refresher.</div><br/><div id="39482877" class="c"><input type="checkbox" id="c-39482877" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482751">parent</a><span>|</span><a href="#39482876">next</a><span>|</span><label class="collapse" for="c-39482877">[-]</label><label class="expand" for="c-39482877">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t be discouraged, you don&#x27;t have to follow everything.<p>In fact it&#x27;s probably better to dive deep into one hobby project like you&#x27;re doing than constantly context switch with every little news item that comes up.<p>While working on gemma.cpp there were definitely a lot of &quot;gee i wish i could clone myself and work on that other thing too&quot;.</div><br/></div></div></div></div><div id="39482895" class="c"><input type="checkbox" id="c-39482895" checked=""/><div class="controls bullet"><span class="by">throwaway19423</span><span>|</span><a href="#39482751">prev</a><span>|</span><a href="#39486156">next</a><span>|</span><label class="collapse" for="c-39482895">[-]</label><label class="expand" for="c-39482895">[6 more]</label></div><br/><div class="children"><div class="content">Can any kind soul explain the difference between GGUF, GGML and all the other model packaging I am seeing these days? Was used to pth and the thing tf uses. Is this all to support inference or quantization? Who manages these formats or are they brewing organically?</div><br/><div id="39483157" class="c"><input type="checkbox" id="c-39483157" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482895">parent</a><span>|</span><a href="#39483748">next</a><span>|</span><label class="collapse" for="c-39483157">[-]</label><label class="expand" for="c-39483157">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s mostly an organic process arising from the ecosystem.<p>My personal way of understanding it is this - the original sin of model weight format complexity is that NNs are both data and computation.<p>Representing the computation as data is the hard part and that&#x27;s where the simplicity falls apart. Do you embed the compute graph? If so, what do you do about different frameworks supporting overlapping but distinct operations. Do you need the artifact to make training reproducible? Well that&#x27;s an even more complex computation that you have to serialize as data. And so on..</div><br/></div></div><div id="39483748" class="c"><input type="checkbox" id="c-39483748" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39482895">parent</a><span>|</span><a href="#39483157">prev</a><span>|</span><a href="#39483939">next</a><span>|</span><label class="collapse" for="c-39483748">[-]</label><label class="expand" for="c-39483748">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s all mostly just inference, though some train LoRAs directly on quantized models too.<p>GGML and GGUF are the same thing, GGUF is the new version that adds more data about the model so it&#x27;s easy to support multiple architectures, and also includes prompt templates. These can run CPU only, be partially or fully offloaded to a GPU. With K quants, you can get anywhere from a 2 bit to an 8 bit GGUF.<p>GPTQ was the GPU-only optimized quantization method that was superseded by AWQ, which is roughly 2x faster and now by EXL2 which is even better. These are usually only 4 bit.<p>Safetensors and pytorch bin files are raw float16 model files, these are only really used for continued fine tuning.</div><br/><div id="39485450" class="c"><input type="checkbox" id="c-39485450" checked=""/><div class="controls bullet"><span class="by">Gracana</span><span>|</span><a href="#39482895">root</a><span>|</span><a href="#39483748">parent</a><span>|</span><a href="#39483939">next</a><span>|</span><label class="collapse" for="c-39485450">[-]</label><label class="expand" for="c-39485450">[2 more]</label></div><br/><div class="children"><div class="content">&gt; and also includes prompt templates<p>That sounds very convenient. What software makes use of the built-in prompt template?</div><br/><div id="39486538" class="c"><input type="checkbox" id="c-39486538" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39482895">root</a><span>|</span><a href="#39485450">parent</a><span>|</span><a href="#39483939">next</a><span>|</span><label class="collapse" for="c-39486538">[-]</label><label class="expand" for="c-39486538">[1 more]</label></div><br/><div class="children"><div class="content">Of the ones I commonly use, I&#x27;ve only seen it read by text-generation-webui, in the GGML days it had a long hardcoded list of known models and which templates they use so they could be auto-selected (which was often wrong), but now it just grabs it from any model directly and sets it when it&#x27;s loaded.</div><br/></div></div></div></div></div></div><div id="39483939" class="c"><input type="checkbox" id="c-39483939" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39482895">parent</a><span>|</span><a href="#39483748">prev</a><span>|</span><a href="#39486156">next</a><span>|</span><label class="collapse" for="c-39483939">[-]</label><label class="expand" for="c-39483939">[1 more]</label></div><br/><div class="children"><div class="content">pth can include Python code (PyTorch code) for inference. TF includes the complete static graph.<p>GGUF is just weights, safetensors the same thing. GGUF doesn&#x27;t need a JSON decoder for the format while safetensors needs that.<p>I personally think having a JSON decoder is not a big deal and make the format more amendable, given GGUF evolves too.</div><br/></div></div></div></div><div id="39486156" class="c"><input type="checkbox" id="c-39486156" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#39482895">prev</a><span>|</span><a href="#39489649">next</a><span>|</span><label class="collapse" for="c-39486156">[-]</label><label class="expand" for="c-39486156">[2 more]</label></div><br/><div class="children"><div class="content">I was discussing LLMs with a non technical person on the plane yesterday. I was explaining why LLMs aren&#x27;t good at math. And, he responded, no, chatgpt is great a multivariate regression, etc.<p>I&#x27;m using LLMs locally almost always and eschewing API backed LLMs like chatgpt. So I&#x27;m not very familiar with plugins, and I&#x27;m assuming chatgpt plugs into a backend when it detects a math problem. So it isn&#x27;t the LLM doing the math but to the user it appears to be.<p>Does anyone here know what LLM projects like llama.cpp or gemma.cpp support a plugin model?<p>I&#x27;m interested in adding to the dungeons and dragons system I built using llama.cpp. Because it doesn&#x27;t do math well, the combat mode is terrible. But I was writing my own layer to break out when combat mode occurs, and I&#x27;m wondering if there is a better way with some kind of plugin approach.</div><br/><div id="39487450" class="c"><input type="checkbox" id="c-39487450" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39486156">parent</a><span>|</span><a href="#39489649">next</a><span>|</span><label class="collapse" for="c-39487450">[-]</label><label class="expand" for="c-39487450">[1 more]</label></div><br/><div class="children"><div class="content">Sillytavern is a front end for local and cloud models. They have a simple scripting language and there&#x27;s been some experiments with adding game functionality with it:<p><a href="https:&#x2F;&#x2F;chub.ai&#x2F;characters&#x2F;creamsan&#x2F;team-neko-e4f1b2f8" rel="nofollow">https:&#x2F;&#x2F;chub.ai&#x2F;characters&#x2F;creamsan&#x2F;team-neko-e4f1b2f8</a><p>This one says it uses javascript as well:<p><a href="https:&#x2F;&#x2F;chub.ai&#x2F;characters&#x2F;creamsan&#x2F;tessa-c4b917f9" rel="nofollow">https:&#x2F;&#x2F;chub.ai&#x2F;characters&#x2F;creamsan&#x2F;tessa-c4b917f9</a><p>Thise are the only two listed as SFW.  There&#x27;s some others if you hit the nsfw toggle and search for the scripted tag.I don&#x27;t know if this is the right approach but you could also write a module for Sillytavern Extras.</div><br/></div></div></div></div><div id="39489649" class="c"><input type="checkbox" id="c-39489649" checked=""/><div class="controls bullet"><span class="by">olegbask</span><span>|</span><a href="#39486156">prev</a><span>|</span><a href="#39482871">next</a><span>|</span><label class="collapse" for="c-39489649">[-]</label><label class="expand" for="c-39489649">[1 more]</label></div><br/><div class="children"><div class="content">It would be amazing to add support for M1 aka Metal: I was able to run Q8 version with llama.cpp and it&#x27;s blazingly fast. The problem: I don&#x27;t know how much accuracy it loses and <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-2b-it&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-2b-it&#x2F;tree&#x2F;main</a> takes too much memory which results in OOMs.<p>Do you have any estimates on getting Metal support similar to how llama.cpp works?<p>Why `<i>.gguf` files are so giant compared to `</i>.sbs`? Is it just because they use fp32?</div><br/></div></div><div id="39482871" class="c"><input type="checkbox" id="c-39482871" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#39489649">prev</a><span>|</span><a href="#39484789">next</a><span>|</span><label class="collapse" for="c-39482871">[-]</label><label class="expand" for="c-39482871">[4 more]</label></div><br/><div class="children"><div class="content">Is this neutered in the way Gemini is (i.e. is the &quot;censorship&quot; built in) or is that a &quot;feature&quot; of the Gemini application?</div><br/><div id="39483973" class="c"><input type="checkbox" id="c-39483973" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39482871">parent</a><span>|</span><a href="#39486819">next</a><span>|</span><label class="collapse" for="c-39483973">[-]</label><label class="expand" for="c-39483973">[1 more]</label></div><br/><div class="children"><div class="content">It depends on the model you load&#x2F;use, the team released both censored and &quot;PT&quot; versions.</div><br/></div></div><div id="39486819" class="c"><input type="checkbox" id="c-39486819" checked=""/><div class="controls bullet"><span class="by">jonpo</span><span>|</span><a href="#39482871">parent</a><span>|</span><a href="#39483973">prev</a><span>|</span><a href="#39483435">next</a><span>|</span><label class="collapse" for="c-39486819">[-]</label><label class="expand" for="c-39486819">[1 more]</label></div><br/><div class="children"><div class="content">These models (Gemma) are very difficult to jailbreak.</div><br/></div></div></div></div><div id="39484789" class="c"><input type="checkbox" id="c-39484789" checked=""/><div class="controls bullet"><span class="by">namtranase</span><span>|</span><a href="#39482871">prev</a><span>|</span><a href="#39481804">next</a><span>|</span><label class="collapse" for="c-39484789">[-]</label><label class="expand" for="c-39484789">[2 more]</label></div><br/><div class="children"><div class="content">Thank the team for the awesome repo. I have navigated gemma.cpp and run it from the first day, it is smooth in my view. So I hope gemma.cpp will continue to add cool features (something like k-quants, server,...) so it can serve more widely.
Actually, I have developed a Python wrapper for it:
<a href="https:&#x2F;&#x2F;github.com&#x2F;namtranase&#x2F;gemma-cpp-python">https:&#x2F;&#x2F;github.com&#x2F;namtranase&#x2F;gemma-cpp-python</a>
The purpose is to use easily and update every new technique from gemma.cpp team.</div><br/><div id="39489370" class="c"><input type="checkbox" id="c-39489370" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39484789">parent</a><span>|</span><a href="#39481804">next</a><span>|</span><label class="collapse" for="c-39489370">[-]</label><label class="expand" for="c-39489370">[1 more]</label></div><br/><div class="children"><div class="content">Nice, this is really cool to see! There were other threads that expressed interest in something like this.</div><br/></div></div></div></div><div id="39481804" class="c"><input type="checkbox" id="c-39481804" checked=""/><div class="controls bullet"><span class="by">a1o</span><span>|</span><a href="#39484789">prev</a><span>|</span><a href="#39489090">next</a><span>|</span><label class="collapse" for="c-39481804">[-]</label><label class="expand" for="c-39481804">[14 more]</label></div><br/><div class="children"><div class="content">If I want to put a Gemma model in a minimalist command line interface, build it to a standalone exe file that runs offline, what is the size of my final executable? I am interested in how small can the size of something like this be and it still be functional.</div><br/><div id="39481908" class="c"><input type="checkbox" id="c-39481908" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#39481804">parent</a><span>|</span><a href="#39481926">next</a><span>|</span><label class="collapse" for="c-39481908">[-]</label><label class="expand" for="c-39481908">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma&#x2F;tags">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma&#x2F;tags</a><p>You can see the various quantizations here, both for the 2B model and the 7B model. The smallest you can go is the q2_K quantization of the 2B model, which is 1.3GB, but I wouldn&#x27;t really call that &quot;functional&quot;. The q4_0 quantization is 1.7GB, and that would probably be functional.<p>The size of anything but the model is going to be rounding error compared to how large the models are, in this context.</div><br/><div id="39482679" class="c"><input type="checkbox" id="c-39482679" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39481908">parent</a><span>|</span><a href="#39481926">next</a><span>|</span><label class="collapse" for="c-39482679">[-]</label><label class="expand" for="c-39482679">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the use case of models this small? Can you use the &quot;knowledge&quot; encoded in them and ask them questions and get relevant answers, or are they used as text processors to summarize documents etc?</div><br/><div id="39485292" class="c"><input type="checkbox" id="c-39485292" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39482679">parent</a><span>|</span><a href="#39481926">next</a><span>|</span><label class="collapse" for="c-39485292">[-]</label><label class="expand" for="c-39485292">[1 more]</label></div><br/><div class="children"><div class="content">Gemma 2B generation quality is excellent in my own very-biased opinion. I asked it to write a response to your comment:<p>--<p>Large language models (LLMs) have achieved significant progress in recent years, with models like GPT-3 and LaMDA demonstrating remarkable abilities in various tasks such as language generation, translation, and question answering.<p>However, 2b parameter models are a much smaller and simpler type of LLM compared to GPT-3. While they are still capable of impressive performance, they have a limited capacity for knowledge representation and reasoning.<p>Despite their size, 2b parameter models can be useful in certain scenarios where the specific knowledge encoded in the model is relevant to the task at hand. For example:<p>- Question answering: 2b parameter models can be used to answer questions by leveraging their ability to generate text that is similar to the question.<p>- Text summarization: 2b parameter models can be used to generate concise summaries of documents by extracting the most important information.<p>- Code generation: While not as common, 2b parameter models can be used to generate code snippets based on the knowledge they have learned.<p>Overall, 2b parameter models are a valuable tool for tasks that require specific knowledge or reasoning capabilities. However, for tasks that involve general language understanding and information retrieval, larger LLMs like GPT-3 may be more suitable.<p>--<p>Generated in under 1s from query to full response on together.ai<p><a href="https:&#x2F;&#x2F;api.together.xyz&#x2F;playground&#x2F;chat&#x2F;google&#x2F;gemma-2b-it" rel="nofollow">https:&#x2F;&#x2F;api.together.xyz&#x2F;playground&#x2F;chat&#x2F;google&#x2F;gemma-2b-it</a></div><br/></div></div></div></div></div></div><div id="39481926" class="c"><input type="checkbox" id="c-39481926" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481804">parent</a><span>|</span><a href="#39481908">prev</a><span>|</span><a href="#39481869">next</a><span>|</span><label class="collapse" for="c-39481926">[-]</label><label class="expand" for="c-39481926">[7 more]</label></div><br/><div class="children"><div class="content">The code is a basically irrelevant fraction of the model weights. The raw FP16 is like 17GB.<p>In practice your priority would be fancy quantization, and just <i>any</i> library that compiles down to an executable (like this, MLC-LLM or llama.cpp)</div><br/><div id="39482091" class="c"><input type="checkbox" id="c-39482091" checked=""/><div class="controls bullet"><span class="by">a1o</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39481926">parent</a><span>|</span><a href="#39481869">next</a><span>|</span><label class="collapse" for="c-39482091">[-]</label><label class="expand" for="c-39482091">[6 more]</label></div><br/><div class="children"><div class="content">17GB looks like a lot. Thanks, I will wait until people figure how to make these smaller before trying to use to make something standalone.</div><br/><div id="39482477" class="c"><input type="checkbox" id="c-39482477" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39482091">parent</a><span>|</span><a href="#39482297">next</a><span>|</span><label class="collapse" for="c-39482477">[-]</label><label class="expand" for="c-39482477">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always going to be a huge quantity of data. Even as efficiency improves, storage and bandwidth are so cheap now that the incentive will be to convert that efficiency towards performance (models with more parameters, ensembles of models, etc) rather than chasing some micro-model that doesn&#x27;t do as well. It might not always be 17GB, but don&#x27;t expect some lesser order of magnitude for anything competitive.<p>As maturity arrives, we&#x27;ll likely see a handful of competing local models shipped as part of the OS or as redistributable third-party bundles (a la the .NET or Java runtimes) so that individual applications don&#x27;t all need to be massive.<p>You&#x27;ll either need to wait for that or bite the bullet and make something chonky. It&#x27;s never going to get that small.</div><br/></div></div><div id="39482297" class="c"><input type="checkbox" id="c-39482297" checked=""/><div class="controls bullet"><span class="by">wg0</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39482091">parent</a><span>|</span><a href="#39482477">prev</a><span>|</span><a href="#39482435">next</a><span>|</span><label class="collapse" for="c-39482297">[-]</label><label class="expand" for="c-39482297">[2 more]</label></div><br/><div class="children"><div class="content">These won&#x27;t be smaller I guess. Given we keep the number of parameters same.<p>Pre LLM era (let&#x27;s say 2020), the hardware used to look decently powerful for most use cases (disks in hundreds of GBs, dozen or two of RAM and quad or hex core processors) but with the advent of LLMs, even disk drives start to look pretty small let alone compute and memory.</div><br/><div id="39482487" class="c"><input type="checkbox" id="c-39482487" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39482297">parent</a><span>|</span><a href="#39482435">next</a><span>|</span><label class="collapse" for="c-39482487">[-]</label><label class="expand" for="c-39482487">[1 more]</label></div><br/><div class="children"><div class="content">And cache! The talk of AI hardware is now &quot;how do we fit these darn things inside SRAM?&quot;</div><br/></div></div></div></div><div id="39482435" class="c"><input type="checkbox" id="c-39482435" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39482091">parent</a><span>|</span><a href="#39482297">prev</a><span>|</span><a href="#39482438">next</a><span>|</span><label class="collapse" for="c-39482435">[-]</label><label class="expand" for="c-39482435">[1 more]</label></div><br/><div class="children"><div class="content">The average PS5 game seems to be around 45GB. Cyberpunk was 250GB.<p>Distributing 17GB isnât a big deal if you shove it into Cloudflare R2.</div><br/></div></div><div id="39482438" class="c"><input type="checkbox" id="c-39482438" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481804">root</a><span>|</span><a href="#39482091">parent</a><span>|</span><a href="#39482435">prev</a><span>|</span><a href="#39481869">next</a><span>|</span><label class="collapse" for="c-39482438">[-]</label><label class="expand" for="c-39482438">[1 more]</label></div><br/><div class="children"><div class="content">In theory quantized weights of smaller models are under a gigabyte.<p>If you are looking for megabytes, yeah, those &quot;chat&quot; llms are pretty unusable at that size.</div><br/></div></div></div></div></div></div><div id="39481869" class="c"><input type="checkbox" id="c-39481869" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39481804">parent</a><span>|</span><a href="#39481926">prev</a><span>|</span><a href="#39481849">next</a><span>|</span><label class="collapse" for="c-39481869">[-]</label><label class="expand" for="c-39481869">[1 more]</label></div><br/><div class="children"><div class="content">Depends how much you quantize the model. For most general-purpose LLMs, the model completely dwarfs the size of the binary code.</div><br/></div></div><div id="39481849" class="c"><input type="checkbox" id="c-39481849" checked=""/><div class="controls bullet"><span class="by">replete</span><span>|</span><a href="#39481804">parent</a><span>|</span><a href="#39481869">prev</a><span>|</span><a href="#39482344">next</a><span>|</span><label class="collapse" for="c-39481849">[-]</label><label class="expand" for="c-39481849">[1 more]</label></div><br/><div class="children"><div class="content">I used gemm:2b with ollama last night and the model was around 1.3gb IIRC</div><br/></div></div><div id="39482344" class="c"><input type="checkbox" id="c-39482344" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#39481804">parent</a><span>|</span><a href="#39481849">prev</a><span>|</span><a href="#39489090">next</a><span>|</span><label class="collapse" for="c-39482344">[-]</label><label class="expand" for="c-39482344">[1 more]</label></div><br/><div class="children"><div class="content">*EDIT*: Nevermind, llamafile hasn&#x27;t been updated in a full month and gemma support was only added to llama.cpp on the 21st of this month. Disregard this post for now and come back when mozilla updates llamafile.<p>---<p>llama.cpp has integrated gemma support. So you can use llamafile for this. It is a standalone executable that is portable across most popular OSes.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases</a><p>So, download the executable from the releases page under assets. You want either just main and server and llava. Don&#x27;t get the huge ones with the model inlined in the file. The executable is about 30MB in size,<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases&#x2F;download&#x2F;0.6.2&#x2F;llamafile-0.6.2">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases&#x2F;download&#x2F;...</a></div><br/></div></div></div></div><div id="39489090" class="c"><input type="checkbox" id="c-39489090" checked=""/><div class="controls bullet"><span class="by">manlobster</span><span>|</span><a href="#39481804">prev</a><span>|</span><a href="#39484925">next</a><span>|</span><label class="collapse" for="c-39489090">[-]</label><label class="expand" for="c-39489090">[1 more]</label></div><br/><div class="children"><div class="content">I wonder why they didn&#x27;t use bazel as their build system.</div><br/></div></div><div id="39484925" class="c"><input type="checkbox" id="c-39484925" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#39489090">prev</a><span>|</span><a href="#39482023">next</a><span>|</span><label class="collapse" for="c-39484925">[-]</label><label class="expand" for="c-39484925">[3 more]</label></div><br/><div class="children"><div class="content">At the risk of being snarky, it&#x27;s interesting that Llama.cpp was a &#x27;grassroots&#x27; effort originating from a Bulgarian hacker google now launches a corporatized effort inspired by it.<p>I wonder if there&#x27;s some analogies to the 80s or 90s in here.</div><br/><div id="39490217" class="c"><input type="checkbox" id="c-39490217" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39484925">parent</a><span>|</span><a href="#39485214">next</a><span>|</span><label class="collapse" for="c-39490217">[-]</label><label class="expand" for="c-39490217">[1 more]</label></div><br/><div class="children"><div class="content">As a Bulgarian from the 80s and 90s, and now a part of the Gemma team, I would say Austin, Jan, and team very much lived up to the ethos of hackers I&#x27;d meet on BBSes back then.<p>They are driven entirely by their own curiosity and a desire to push computers to the limit. Combined with their admirable low-level programming skills, you get a very solid, fun codebase, that they are sharing with the world. :)</div><br/></div></div><div id="39485214" class="c"><input type="checkbox" id="c-39485214" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39484925">parent</a><span>|</span><a href="#39490217">prev</a><span>|</span><a href="#39482023">next</a><span>|</span><label class="collapse" for="c-39485214">[-]</label><label class="expand" for="c-39485214">[1 more]</label></div><br/><div class="children"><div class="content">To be clear, this is not comparable directly to llama.cpp -- Gemma models work on llama.cpp and we encourage people who love llama.cpp to use them there. We&#x27;re also launched with Ollama.<p>Gemma.cpp is a highly optimized and lightweight system. The performance is pretty incredible on CPU, give it a try =)</div><br/></div></div></div></div><div id="39482023" class="c"><input type="checkbox" id="c-39482023" checked=""/><div class="controls bullet"><span class="by">brokensegue</span><span>|</span><a href="#39484925">prev</a><span>|</span><a href="#39481860">next</a><span>|</span><label class="collapse" for="c-39482023">[-]</label><label class="expand" for="c-39482023">[6 more]</label></div><br/><div class="children"><div class="content">does anyone have stats on cpu only inference speed with this?</div><br/><div id="39483249" class="c"><input type="checkbox" id="c-39483249" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482023">parent</a><span>|</span><a href="#39481860">next</a><span>|</span><label class="collapse" for="c-39483249">[-]</label><label class="expand" for="c-39483249">[5 more]</label></div><br/><div class="children"><div class="content">any particular hardware folks are most interested in?</div><br/><div id="39483314" class="c"><input type="checkbox" id="c-39483314" checked=""/><div class="controls bullet"><span class="by">brokensegue</span><span>|</span><a href="#39482023">root</a><span>|</span><a href="#39483249">parent</a><span>|</span><a href="#39481860">next</a><span>|</span><label class="collapse" for="c-39483314">[-]</label><label class="expand" for="c-39483314">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m just looking for ballpark figures. Maybe a common aws instance type</div><br/><div id="39489159" class="c"><input type="checkbox" id="c-39489159" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#39482023">root</a><span>|</span><a href="#39483314">parent</a><span>|</span><a href="#39486492">next</a><span>|</span><label class="collapse" for="c-39489159">[-]</label><label class="expand" for="c-39489159">[1 more]</label></div><br/><div class="children"><div class="content">For the 7B IT and a short factual query I see 5.3 tps on a 5 year old Skylake Gold 6154 CPU @ 3.00GHz, 16 threads.
Expect a slight increase as we improve scalability.<p>FYI using the NUQ (4.5-bit) quantization improves throughput by about 1.4x.</div><br/></div></div><div id="39486492" class="c"><input type="checkbox" id="c-39486492" checked=""/><div class="controls bullet"><span class="by">notum</span><span>|</span><a href="#39482023">root</a><span>|</span><a href="#39483314">parent</a><span>|</span><a href="#39489159">prev</a><span>|</span><a href="#39481860">next</a><span>|</span><label class="collapse" for="c-39486492">[-]</label><label class="expand" for="c-39486492">[2 more]</label></div><br/><div class="children"><div class="content">Not sure if this is of any value to you, but Ryzen 7 generates 2 tokens per second for the 7B-Instruct model.<p>The model itself is very unimpressive and I see no reason to play with it over the worst alternative from Hugging Face.
I can only imagine this was released for some bizarre compliance reasons.</div><br/><div id="39486689" class="c"><input type="checkbox" id="c-39486689" checked=""/><div class="controls bullet"><span class="by">brokensegue</span><span>|</span><a href="#39482023">root</a><span>|</span><a href="#39486492">parent</a><span>|</span><a href="#39481860">next</a><span>|</span><label class="collapse" for="c-39486689">[-]</label><label class="expand" for="c-39486689">[1 more]</label></div><br/><div class="children"><div class="content">the metrics suggest it&#x27;s much better than that</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39481860" class="c"><input type="checkbox" id="c-39481860" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39482023">prev</a><span>|</span><a href="#39485097">next</a><span>|</span><label class="collapse" for="c-39481860">[-]</label><label class="expand" for="c-39481860">[7 more]</label></div><br/><div class="children"><div class="content">...Also, we have eval&#x27;d Gemma 7B internally in a deterministic, zero temperature test, and its error rate is like double Mistral Instruct 0.2. Well below most other 7Bs.<p>Was not very impressed with the chat either.<p>So maybe this is neat for embedded projects, but if it&#x27;s Gemma only, that would be quite a sticking point for me.</div><br/><div id="39482623" class="c"><input type="checkbox" id="c-39482623" checked=""/><div class="controls bullet"><span class="by">Vetch</span><span>|</span><a href="#39481860">parent</a><span>|</span><a href="#39481974">next</a><span>|</span><label class="collapse" for="c-39482623">[-]</label><label class="expand" for="c-39482623">[2 more]</label></div><br/><div class="children"><div class="content">Was it via gemma.cpp or some other library? I&#x27;ve seen a few people note that gemma performance via gemma.cpp is much better than llama.cpp, possible that the non-google implementations are still not quite right?</div><br/><div id="39482760" class="c"><input type="checkbox" id="c-39482760" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481860">root</a><span>|</span><a href="#39482623">parent</a><span>|</span><a href="#39481974">next</a><span>|</span><label class="collapse" for="c-39482760">[-]</label><label class="expand" for="c-39482760">[1 more]</label></div><br/><div class="children"><div class="content">I eval&#x27;d it with vllm.<p>One thing I <i>do</i> suspect people are running into is sampling issues. Gemma probably doesn&#x27;t like llama defaults with its 256K vocab.<p>Many Chinese llms have a similar &quot;default sampling&quot; issue.<p>But our testing was done with zero temperature and constrained single token responses, so that shouldnt be an issue.</div><br/></div></div></div></div><div id="39481974" class="c"><input type="checkbox" id="c-39481974" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39481860">parent</a><span>|</span><a href="#39482623">prev</a><span>|</span><a href="#39485353">next</a><span>|</span><label class="collapse" for="c-39481974">[-]</label><label class="expand" for="c-39481974">[2 more]</label></div><br/><div class="children"><div class="content">That does seem to be the consensus unfortunately. Would have been better for everyone if googleâs foray into open model a la FB made a splash</div><br/><div id="39482058" class="c"><input type="checkbox" id="c-39482058" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481860">root</a><span>|</span><a href="#39481974">parent</a><span>|</span><a href="#39485353">next</a><span>|</span><label class="collapse" for="c-39482058">[-]</label><label class="expand" for="c-39482058">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, especially with how much Google is hyping it.<p>It could have been long context? Or a little bigger, to fill the relative gap in the 13B-30B area? Even if the model itself was mediocre (which you can&#x27;t know until after training), it would have been more interesting.</div><br/></div></div></div></div><div id="39485353" class="c"><input type="checkbox" id="c-39485353" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39481860">parent</a><span>|</span><a href="#39481974">prev</a><span>|</span><a href="#39485097">next</a><span>|</span><label class="collapse" for="c-39485353">[-]</label><label class="expand" for="c-39485353">[2 more]</label></div><br/><div class="children"><div class="content">Any chance you can share more details on your measurement setup and eval protocols? You&#x27;re likely seeing some config snafus, which we&#x27;re trying to track down.</div><br/><div id="39489187" class="c"><input type="checkbox" id="c-39489187" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481860">root</a><span>|</span><a href="#39485353">parent</a><span>|</span><a href="#39485097">next</a><span>|</span><label class="collapse" for="c-39489187">[-]</label><label class="expand" for="c-39489187">[1 more]</label></div><br/><div class="children"><div class="content">I just loaded it in vllm with default settings.<p>I can&#x27;t share the eval, but it&#x27;s pretty simple: it asks a question about some data, and is restricted to only answer yes&#x2F;no (based on the output logits and suggested in the prompt). It&#x27;s called with 0 temperature and only 1 output token, so sampling shouldn&#x27;t be an issue.</div><br/></div></div></div></div></div></div><div id="39485097" class="c"><input type="checkbox" id="c-39485097" checked=""/><div class="controls bullet"><span class="by">natch</span><span>|</span><a href="#39481860">prev</a><span>|</span><a href="#39483269">next</a><span>|</span><label class="collapse" for="c-39485097">[-]</label><label class="expand" for="c-39485097">[1 more]</label></div><br/><div class="children"><div class="content">Apart from the fact that they are different things, since they came out of the same organization I think itâs fair to ask:<p>Do these models have the same kind of odd behavior as Gemini?</div><br/></div></div><div id="39483269" class="c"><input type="checkbox" id="c-39483269" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39485097">prev</a><span>|</span><a href="#39481816">next</a><span>|</span><label class="collapse" for="c-39483269">[-]</label><label class="expand" for="c-39483269">[2 more]</label></div><br/><div class="children"><div class="content">Is it not possible to add Gemma support on Llama.cpp?</div><br/><div id="39483382" class="c"><input type="checkbox" id="c-39483382" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39483269">parent</a><span>|</span><a href="#39481816">next</a><span>|</span><label class="collapse" for="c-39483382">[-]</label><label class="expand" for="c-39483382">[1 more]</label></div><br/><div class="children"><div class="content">Gemma support has been added to llama.cpp, in fact it was added almost immediately after the release:
<a href="https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760293079313973408" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760293079313973408</a><p>However, be aware that there were some quality issues with quantization initially (hopefully they&#x27;re resolved but i haven&#x27;t followed too closely):
<a href="https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760418864418934922" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760418864418934922</a></div><br/></div></div></div></div><div id="39481816" class="c"><input type="checkbox" id="c-39481816" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39483269">prev</a><span>|</span><a href="#39484284">next</a><span>|</span><label class="collapse" for="c-39481816">[-]</label><label class="expand" for="c-39481816">[5 more]</label></div><br/><div class="children"><div class="content">Not to be confused with llama.cpp and the GGML library, which is a seperate project (and almost immediately worked with Gemma).</div><br/><div id="39482925" class="c"><input type="checkbox" id="c-39482925" checked=""/><div class="controls bullet"><span class="by">throwaway19423</span><span>|</span><a href="#39481816">parent</a><span>|</span><a href="#39483750">next</a><span>|</span><label class="collapse" for="c-39482925">[-]</label><label class="expand" for="c-39482925">[3 more]</label></div><br/><div class="children"><div class="content">I am confused how all these things are able to interoperate. Are the creators of these models following the same IO for their models? Won&#x27;t the tokenizer or token embedder be different? I am genuinely confused by how the same code works for so many different models.</div><br/><div id="39483415" class="c"><input type="checkbox" id="c-39483415" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481816">root</a><span>|</span><a href="#39482925">parent</a><span>|</span><a href="#39483750">next</a><span>|</span><label class="collapse" for="c-39483415">[-]</label><label class="expand" for="c-39483415">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s complicated, but basically because <i>most</i> are llama architecture. Meta all but set the standard for open source llms when they released llama1, and anyone trying to deviate from it has run into trouble because the models don&#x27;t work with the hyper optimized llama runtumes.<p>Also, there&#x27;s a lot of magic going on behind the scenes with configs stored in gguf&#x2F;huggingface format models, and the libraries that use them. There are different tokenizers, but they mostly follow the same standards.</div><br/><div id="39485855" class="c"><input type="checkbox" id="c-39485855" checked=""/><div class="controls bullet"><span class="by">null_point</span><span>|</span><a href="#39481816">root</a><span>|</span><a href="#39483415">parent</a><span>|</span><a href="#39483750">next</a><span>|</span><label class="collapse" for="c-39485855">[-]</label><label class="expand" for="c-39485855">[1 more]</label></div><br/><div class="children"><div class="content">I found the magic! <a href="https:&#x2F;&#x2F;github.com&#x2F;search?q=repo%3Aggerganov%2Fggml%20magic&amp;type=code">https:&#x2F;&#x2F;github.com&#x2F;search?q=repo%3Aggerganov%2Fggml%20magic&amp;...</a></div><br/></div></div></div></div></div></div><div id="39483750" class="c"><input type="checkbox" id="c-39483750" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#39481816">parent</a><span>|</span><a href="#39482925">prev</a><span>|</span><a href="#39484284">next</a><span>|</span><label class="collapse" for="c-39483750">[-]</label><label class="expand" for="c-39483750">[1 more]</label></div><br/><div class="children"><div class="content">I doubt there&#x27;d be confusion as the names are totally different</div><br/></div></div></div></div><div id="39484284" class="c"><input type="checkbox" id="c-39484284" checked=""/><div class="controls bullet"><span class="by">kwantaz</span><span>|</span><a href="#39481816">prev</a><span>|</span><a href="#39482190">next</a><span>|</span><label class="collapse" for="c-39484284">[-]</label><label class="expand" for="c-39484284">[1 more]</label></div><br/><div class="children"><div class="content">nice</div><br/></div></div><div id="39482190" class="c"><input type="checkbox" id="c-39482190" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#39484284">prev</a><span>|</span><a href="#39489265">next</a><span>|</span><label class="collapse" for="c-39482190">[-]</label><label class="expand" for="c-39482190">[5 more]</label></div><br/><div class="children"><div class="content">Come on Dejiko, we don&#x27;t have time for this gema!<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9FSAqDVZHhU" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9FSAqDVZHhU</a></div><br/><div id="39482567" class="c"><input type="checkbox" id="c-39482567" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39482190">parent</a><span>|</span><a href="#39482288">next</a><span>|</span><label class="collapse" for="c-39482567">[-]</label><label class="expand" for="c-39482567">[2 more]</label></div><br/><div class="children"><div class="content">Every time I see Gemma all I hear is Jubei screaming Genmaaaa since the n is almost silent. <a href="https:&#x2F;&#x2F;youtu.be&#x2F;TFR9-cZecWo?si=rMED2LEh-fssHeeG" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;TFR9-cZecWo?si=rMED2LEh-fssHeeG</a></div><br/><div id="39483204" class="c"><input type="checkbox" id="c-39483204" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39482190">root</a><span>|</span><a href="#39482567">parent</a><span>|</span><a href="#39482288">next</a><span>|</span><label class="collapse" for="c-39483204">[-]</label><label class="expand" for="c-39483204">[1 more]</label></div><br/><div class="children"><div class="content">lol</div><br/></div></div></div></div><div id="39482288" class="c"><input type="checkbox" id="c-39482288" checked=""/><div class="controls bullet"><span class="by">a-french-anon</span><span>|</span><a href="#39482190">parent</a><span>|</span><a href="#39482567">prev</a><span>|</span><a href="#39489265">next</a><span>|</span><label class="collapse" for="c-39482288">[-]</label><label class="expand" for="c-39482288">[2 more]</label></div><br/><div class="children"><div class="content">Glad I wasn&#x27;t alone.</div><br/><div id="39482404" class="c"><input type="checkbox" id="c-39482404" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#39482190">root</a><span>|</span><a href="#39482288">parent</a><span>|</span><a href="#39489265">next</a><span>|</span><label class="collapse" for="c-39482404">[-]</label><label class="expand" for="c-39482404">[1 more]</label></div><br/><div class="children"><div class="content">Well, it was just so nostalgic for me nyo :-\</div><br/></div></div></div></div></div></div><div id="39481966" class="c"><input type="checkbox" id="c-39481966" checked=""/><div class="controls bullet"><span class="by">colesantiago</span><span>|</span><a href="#39483130">prev</a><span>|</span><a href="#39483134">next</a><span>|</span><label class="collapse" for="c-39481966">[-]</label><label class="expand" for="c-39481966">[5 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t there a huge risk that Google could most likely deprecate Gemini, Gemma and Gemma.cpp? Not really smart to build on anything with Google e.g. Google Cloud for AI.<p>Has this perception changed or pretty much the same?</div><br/><div id="39482009" class="c"><input type="checkbox" id="c-39482009" checked=""/><div class="controls bullet"><span class="by">beoberha</span><span>|</span><a href="#39481966">parent</a><span>|</span><a href="#39482099">next</a><span>|</span><label class="collapse" for="c-39482009">[-]</label><label class="expand" for="c-39482009">[1 more]</label></div><br/><div class="children"><div class="content">Gemini - maybe, though I find it pretty unlikely itâll happen anytime soon.<p>Not sure what you mean about Gemma considering itâs not a service. You can download the model weights and the inference code is on GitHub. Everything is local!</div><br/></div></div><div id="39482099" class="c"><input type="checkbox" id="c-39482099" checked=""/><div class="controls bullet"><span class="by">ertgbnm</span><span>|</span><a href="#39481966">parent</a><span>|</span><a href="#39482009">prev</a><span>|</span><a href="#39482024">next</a><span>|</span><label class="collapse" for="c-39482099">[-]</label><label class="expand" for="c-39482099">[2 more]</label></div><br/><div class="children"><div class="content">The weights are downloadable so there isn&#x27;t much of a risk if Google stops hosting Gemma apart from the fact that it won&#x27;t get new versions that you swap out in the future.</div><br/><div id="39483993" class="c"><input type="checkbox" id="c-39483993" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#39481966">root</a><span>|</span><a href="#39482099">parent</a><span>|</span><a href="#39482024">next</a><span>|</span><label class="collapse" for="c-39483993">[-]</label><label class="expand" for="c-39483993">[1 more]</label></div><br/><div class="children"><div class="content">even if there&#x27;s a new model, I&#x27;m not seeing how these models provide any reliability metric.<p>if you figure out a money making software&#x2F;service, you&#x27;re gonna be tied to that model to some significant degree.</div><br/></div></div></div></div><div id="39482024" class="c"><input type="checkbox" id="c-39482024" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39481966">parent</a><span>|</span><a href="#39482099">prev</a><span>|</span><a href="#39483134">next</a><span>|</span><label class="collapse" for="c-39482024">[-]</label><label class="expand" for="c-39482024">[1 more]</label></div><br/><div class="children"><div class="content">This is not necessarily a production backend, as it mentions in the readme.<p>There are some very interesting efforts in JAX&#x2F;TPU land like <a href="https:&#x2F;&#x2F;github.com&#x2F;erfanzar&#x2F;EasyDeL">https:&#x2F;&#x2F;github.com&#x2F;erfanzar&#x2F;EasyDeL</a></div><br/></div></div></div></div><div id="39483134" class="c"><input type="checkbox" id="c-39483134" checked=""/><div class="controls bullet"><span class="by">Wissan</span><span>|</span><a href="#39481966">prev</a><span>|</span><label class="collapse" for="c-39483134">[-]</label><label class="expand" for="c-39483134">[2 more]</label></div><br/><div class="children"><div class="content">Hello</div><br/><div id="39484672" class="c"><input type="checkbox" id="c-39484672" checked=""/><div class="controls bullet"><span class="by">sintax</span><span>|</span><a href="#39483134">parent</a><span>|</span><label class="collapse" for="c-39484672">[-]</label><label class="expand" for="c-39484672">[1 more]</label></div><br/><div class="children"><div class="content">Demo when model quantized to q0_K?</div><br/></div></div></div></div></div></div></div></div></div></body></html>