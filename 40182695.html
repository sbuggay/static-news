<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714294855675" as="style"/><link rel="stylesheet" href="styles.css?v=1714294855675"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2404.15758">Let&#x27;s Think Dot by Dot: Hidden Computation in Transformer Language Models</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>Jimmc414</span> | <span>27 comments</span></div><br/><div><div id="40185403" class="c"><input type="checkbox" id="c-40185403" checked=""/><div class="controls bullet"><span class="by">razodactyl</span><span>|</span><a href="#40183157">next</a><span>|</span><label class="collapse" for="c-40185403">[-]</label><label class="expand" for="c-40185403">[2 more]</label></div><br/><div class="children"><div class="content">I get the feeling this works due to the following:<p>1. An input is processed with answer generated token-by-token.<p>2. The model can output based on probability the answer or a filler token. Low probability answers are ignored in favour of higher probability filler tokens (I don&#x27;t have a better token to answer with than .....)<p>3. At a certain point, an alignment is made with what was learnt previously triggering a higher probability of outputting a better token.<p>This intuition being that I&#x27;ve noticed models respond differently based on where in context information appears: Can&#x27;t speak for different embedding methods however as I&#x27;m sure this changes my thoughts on above.<p>If instead chain of thought prompting is used, the tokens further generated may interfere with the output probability.<p>So further to this, I&#x27;m thinking filler tokens allow for a purer ability for a model to surface the best answer it has been trained on without introducing more noise. Or we can use methods that resample multiple times to find the highest outputs.<p>These LLMs are practically search engines in disguise.</div><br/></div></div><div id="40183157" class="c"><input type="checkbox" id="c-40183157" checked=""/><div class="controls bullet"><span class="by">diziet</span><span>|</span><a href="#40185403">prev</a><span>|</span><a href="#40184182">next</a><span>|</span><label class="collapse" for="c-40183157">[-]</label><label class="expand" for="c-40183157">[17 more]</label></div><br/><div class="children"><div class="content">This is a surprising result to me, given that (in my mind) the method simply does a few more forward passes, without encoding or transferring meaningful state between each pass.</div><br/><div id="40183556" class="c"><input type="checkbox" id="c-40183556" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40183157">parent</a><span>|</span><a href="#40183860">next</a><span>|</span><label class="collapse" for="c-40183556">[-]</label><label class="expand" for="c-40183556">[3 more]</label></div><br/><div class="children"><div class="content">You get embeddings at every activation layer of the network, at every token. That&#x27;s extra state accessible to the network when running in recurrent &#x27;generate the next token&#x27; mode.</div><br/><div id="40183873" class="c"><input type="checkbox" id="c-40183873" checked=""/><div class="controls bullet"><span class="by">ehsanu1</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40183556">parent</a><span>|</span><a href="#40183860">next</a><span>|</span><label class="collapse" for="c-40183873">[-]</label><label class="expand" for="c-40183873">[2 more]</label></div><br/><div class="children"><div class="content">How much extra state and computation is it per token exactly? Can we account for the improvement in just those terms?</div><br/><div id="40186218" class="c"><input type="checkbox" id="c-40186218" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40183873">parent</a><span>|</span><a href="#40183860">next</a><span>|</span><label class="collapse" for="c-40186218">[-]</label><label class="expand" for="c-40186218">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the point of this paper: investigating whether &#x27;chain of thought&#x27; promoting kinda-works because it actually induces reasoning, or whether it&#x27;s just that more verbose answers give the model more tokens to work with, and this more state in which to hide interesting computations. This work introduced a way to give the model more tokens - and thus compute and state - to work with, independent of the prompt, which makes it easier to separate the computational impacts of verbosity from prompting.</div><br/></div></div></div></div></div></div><div id="40183860" class="c"><input type="checkbox" id="c-40183860" checked=""/><div class="controls bullet"><span class="by">ehsanu1</span><span>|</span><a href="#40183157">parent</a><span>|</span><a href="#40183556">prev</a><span>|</span><a href="#40185585">next</a><span>|</span><label class="collapse" for="c-40183860">[-]</label><label class="expand" for="c-40183860">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only read the abstract, but also find this strange. I wonder if this is just tapping into the computational chains that are already available when tokens are further away, due to the positional encodings being trained that way. If so, that makes the reasoning&#x2F;modeling powers of LLMs even more impressive and inscrutable.</div><br/></div></div><div id="40185585" class="c"><input type="checkbox" id="c-40185585" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#40183157">parent</a><span>|</span><a href="#40183860">prev</a><span>|</span><a href="#40184913">next</a><span>|</span><label class="collapse" for="c-40185585">[-]</label><label class="expand" for="c-40185585">[2 more]</label></div><br/><div class="children"><div class="content">Every token generates a KV cache entry based on this token and all previous KV cache entries. This happens in every layer.
 KV cache is 100k-1MB per token so quite a bit.<p>Edit: also you can forward generate 5 or 10 dots in batch without much overhead compared to a single dot since the main cost is pulling KV cache from VRAM so you have free tensor units.</div><br/></div></div><div id="40184913" class="c"><input type="checkbox" id="c-40184913" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40183157">parent</a><span>|</span><a href="#40185585">prev</a><span>|</span><a href="#40186828">next</a><span>|</span><label class="collapse" for="c-40184913">[-]</label><label class="expand" for="c-40184913">[1 more]</label></div><br/><div class="children"><div class="content">I think the general idea is that since partial embeddings can be copied laterally (between token positions) from one layer of a transformer to the next, then additional work done at filler positions can also be copied to following real token positions. There&#x27;s obviously a limit to how useful this can be since these are added parallel token steps rather than sequential transformer layer ones, and results from different experiments seem to be mixed.<p>Still, I don&#x27;t see how this really works .. more compute &#x2F; embedding transformations are being potentially applied to the prediction, but in what circumstances are these filler positions being used in a useful way? The filler token embeddings themselves presumably aren&#x27;t matching attention keys, but positional encodings for adjacent tokens will be similar, which is maybe what triggers lateral copying into (and perhaps out of) filler positions?</div><br/></div></div><div id="40186828" class="c"><input type="checkbox" id="c-40186828" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40183157">parent</a><span>|</span><a href="#40184913">prev</a><span>|</span><a href="#40184064">next</a><span>|</span><label class="collapse" for="c-40186828">[-]</label><label class="expand" for="c-40186828">[1 more]</label></div><br/><div class="children"><div class="content">Given an algorithm that takes 100 iterations, ask the computer to perform it in ten iterations. It gives you a nonsense answer. Tell it to do it in 100 steps and it might just be able to do it. What this tells us is that context size appears to be a limiting factor as models get bigger and bigger.</div><br/></div></div><div id="40184064" class="c"><input type="checkbox" id="c-40184064" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#40183157">parent</a><span>|</span><a href="#40186828">prev</a><span>|</span><a href="#40184241">next</a><span>|</span><label class="collapse" for="c-40184064">[-]</label><label class="expand" for="c-40184064">[4 more]</label></div><br/><div class="children"><div class="content">You can transfer some state just through dots. The dot count could mean &quot;the first n ideas do not work, analyze the n+1 one, if that&#x27;s bad, emit another dot&quot;</div><br/><div id="40184357" class="c"><input type="checkbox" id="c-40184357" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40184064">parent</a><span>|</span><a href="#40184241">next</a><span>|</span><label class="collapse" for="c-40184357">[-]</label><label class="expand" for="c-40184357">[3 more]</label></div><br/><div class="children"><div class="content">And this works even if we assume the dots don&#x27;t actually transfer information but just slightly mutate the state. First it tries a random approach, if that approach doesn&#x27;t lead to a powerful result it emits a dot to try another random approach in the next round, until you get a sufficiently good path forwards.<p>Essentially a brute-force search. Which is a bit wasteful, but better than just blindly taking the first idea</div><br/><div id="40184571" class="c"><input type="checkbox" id="c-40184571" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40184357">parent</a><span>|</span><a href="#40184241">next</a><span>|</span><label class="collapse" for="c-40184571">[-]</label><label class="expand" for="c-40184571">[2 more]</label></div><br/><div class="children"><div class="content">Kind of like trying nonces until you find one that gives you a hash with lots of leading zeros? Dotchain?</div><br/><div id="40186062" class="c"><input type="checkbox" id="c-40186062" checked=""/><div class="controls bullet"><span class="by">kevindamm</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40184571">parent</a><span>|</span><a href="#40184241">next</a><span>|</span><label class="collapse" for="c-40186062">[-]</label><label class="expand" for="c-40186062">[1 more]</label></div><br/><div class="children"><div class="content">Chain of Dot.</div><br/></div></div></div></div></div></div></div></div><div id="40184241" class="c"><input type="checkbox" id="c-40184241" checked=""/><div class="controls bullet"><span class="by">pyinstallwoes</span><span>|</span><a href="#40183157">parent</a><span>|</span><a href="#40184064">prev</a><span>|</span><a href="#40184182">next</a><span>|</span><label class="collapse" for="c-40184241">[-]</label><label class="expand" for="c-40184241">[4 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t anything be compressed into one word by comparison?</div><br/><div id="40184486" class="c"><input type="checkbox" id="c-40184486" checked=""/><div class="controls bullet"><span class="by">ivankolev</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40184241">parent</a><span>|</span><a href="#40184182">next</a><span>|</span><label class="collapse" for="c-40184486">[-]</label><label class="expand" for="c-40184486">[3 more]</label></div><br/><div class="children"><div class="content">Godel and Heisenberg say no, in the most generalized case. Our universe is not deterministic</div><br/><div id="40185383" class="c"><input type="checkbox" id="c-40185383" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40184486">parent</a><span>|</span><a href="#40186838">next</a><span>|</span><label class="collapse" for="c-40185383">[-]</label><label class="expand" for="c-40185383">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what Godel is doing here but quantum mechanics is consistent with the universe being deterministic<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Superdeterminism" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Superdeterminism</a></div><br/></div></div><div id="40186838" class="c"><input type="checkbox" id="c-40186838" checked=""/><div class="controls bullet"><span class="by">pyinstallwoes</span><span>|</span><a href="#40183157">root</a><span>|</span><a href="#40184486">parent</a><span>|</span><a href="#40185383">prev</a><span>|</span><a href="#40184182">next</a><span>|</span><label class="collapse" for="c-40186838">[-]</label><label class="expand" for="c-40186838">[1 more]</label></div><br/><div class="children"><div class="content">Explain how words do that?</div><br/></div></div></div></div></div></div></div></div><div id="40184182" class="c"><input type="checkbox" id="c-40184182" checked=""/><div class="controls bullet"><span class="by">rgbrgb</span><span>|</span><a href="#40183157">prev</a><span>|</span><a href="#40185182">next</a><span>|</span><label class="collapse" for="c-40184182">[-]</label><label class="expand" for="c-40184182">[1 more]</label></div><br/><div class="children"><div class="content">i found a nice thread-level walkthrough of this paper by the first coauthor here: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;jacob_pfau&#x2F;status&#x2F;1783951795238441449" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;jacob_pfau&#x2F;status&#x2F;1783951795238441449</a></div><br/></div></div><div id="40185182" class="c"><input type="checkbox" id="c-40185182" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#40184182">prev</a><span>|</span><a href="#40185722">next</a><span>|</span><label class="collapse" for="c-40185182">[-]</label><label class="expand" for="c-40185182">[1 more]</label></div><br/><div class="children"><div class="content">What does a prompt for this look like?</div><br/></div></div><div id="40185722" class="c"><input type="checkbox" id="c-40185722" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#40185182">prev</a><span>|</span><a href="#40184672">next</a><span>|</span><label class="collapse" for="c-40185722">[-]</label><label class="expand" for="c-40185722">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t they just not drop the decoder part of the transformer architecture if they need some additional processing on the prompt?</div><br/></div></div><div id="40184672" class="c"><input type="checkbox" id="c-40184672" checked=""/><div class="controls bullet"><span class="by">Vetch</span><span>|</span><a href="#40185722">prev</a><span>|</span><label class="collapse" for="c-40184672">[-]</label><label class="expand" for="c-40184672">[4 more]</label></div><br/><div class="children"><div class="content">This paper reads to me as being about fundamental limitations of Transformers and backdoor risk.<p>The paper starts off by reviewing work which uses an encompassing theoretical model of transformers to prove they&#x27;re limited to only expressing computations in TC^0 (roughly, upperbounded by set of parallelizable problems that can be solved by relatively shallow circuits).<p>There&#x27;s also a reference to a paper which finds that (wrt input problem size), a polynomial number of intermediate scratchpad decoding steps allow transformers to recognize the class of polynomial-time solvable problems, linear steps is context-sensitive languages.<p>This paper now ask about filler tokens, do they help? The answer is negative except for a very clever exception they work out: problems with demonstrations that can be decomposed to be solvable in parallel. This identifies a practical limitation (transformer next token prediction is not expressive enough to capture all of TC^0) at the same as it identifies a theoretical capability. From the paper:<p>&gt; Taken together these findings suggest that although current LLMs are unlikely to benefit from filler tokens, this is not an in-principle limitation of current architectures.<p>If I&#x27;ve understood, this means for learning to use fillers to benefit from CoT data, demonstrations must be structured such that they can be computed in parallel and not as a more natural sequential, instance-adaptive process.<p>&gt; in order to use filler tokens on natural language data, LLMs would need to discover parallelizable algorithmic solutions given access only to CoT demonstrations lacking parallel structure. By training on instance-adaptive chains of thought, we can study whether models can learn to use filler tokens having seen only more naturalistic chain-of-thought data
&gt;...
&gt; We find that models trained on instance-adaptive CoT data fail to use filler tokens. On filler token sequences, the resulting models remain at, or below, no-intermediate-token, baseline performance, Figure 6. This indicates that there is no transfer from serial, instance-adaptive demonstrations to filler tokens for the 3SUM problem.<p>It also appears that the parallelizable problem must have a certain amount of structural complexity before a gap appears versus no filler modes (unless using an impractical amount of filler tokens):<p>&gt; we expect integer addition tasks will not offer suitably rich structures for taking advantage of filler tokens when using large modelsânatural-language tasks may offer alternatives<p>Empirically, other papers have shown that LLM performance on complex tasks deteriorates significantly with input length and distractor text. Anyone who has naively attempted to combine RAG with large contexts might also have first hand experience with this.<p>The reason I consider this to be primarily a backdoor risk is that the kind of data and learning required seems highly unlikely to occur naturally but someone could create documents to introduce triggerable obfuscated computations. While not an issue today, future LLM training might need to filter for data with meaningful parts separated by meaningless patterns of repeated characters.<p>This paper follows a recent trend of marketing excellent theoretical work as LLMs being capable of secretly plotting behind your back, when the realistic implication is backdoor risk.<p>An article currently on the first page is relevant:<p><a href="https:&#x2F;&#x2F;www.strangeloopcanon.com&#x2F;p&#x2F;what-can-llms-never-do" rel="nofollow">https:&#x2F;&#x2F;www.strangeloopcanon.com&#x2F;p&#x2F;what-can-llms-never-do</a></div><br/><div id="40186782" class="c"><input type="checkbox" id="c-40186782" checked=""/><div class="controls bullet"><span class="by">casebash</span><span>|</span><a href="#40184672">parent</a><span>|</span><a href="#40184890">next</a><span>|</span><label class="collapse" for="c-40186782">[-]</label><label class="expand" for="c-40186782">[1 more]</label></div><br/><div class="children"><div class="content">&quot;This paper follows a recent trend of marketing excellent theoretical work as LLMs being capable of secretly plotting behind your back, when the realistic implication is backdoor risk&quot;.<p>Many top computer scientists consider loss of control risks to be a possibility that we need to take seriously.<p>So the question then becomes, is there a way to apply science to gain greater clarity on the possibility of these claims? And this is very tricky, since we&#x27;re trying to evaluate claims not about models that currently exist, but about future models.<p>And I guess what people have realised recently is that, even if we can&#x27;t directly run an experiment to determine the validity of the core claim of concern, we can run experiments on auxiliary claims in order to better inform discussions. For example, the best way to show that a future model could have a capability is to demonstrate that a current model possesses that capability.<p>I&#x27;m guessing you&#x27;d like to see more scientific evidence before you want to take possibilities like deceptive alignment seriously. I think that&#x27;s reasonable. However, work like this is how we gather that evidence.<p>Obviously, each individual result doesn&#x27;t provide much evidence on its own, but the accumulation of results has helped to provide more strategic clarity over time.</div><br/></div></div><div id="40184890" class="c"><input type="checkbox" id="c-40184890" checked=""/><div class="controls bullet"><span class="by">nopromisessir</span><span>|</span><a href="#40184672">parent</a><span>|</span><a href="#40186782">prev</a><span>|</span><label class="collapse" for="c-40184890">[-]</label><label class="expand" for="c-40184890">[2 more]</label></div><br/><div class="children"><div class="content">I would agree that choice of language &#x27;hidden reasoning&#x27; is a poor one.<p>This paper demonstrates a novel training approach which could yield narrow capability growth on a certain class of tasks.<p>The narrow test tube environment in which we see better performance hints at the unknown which when better understood could promise further yields down the road.<p>To my mind, the idea that filler tokens might promote immergent capability leading to broader task complexity capability is more promising than the backdoor risk you lay out. The possible scale in each direction just doesn&#x27;t seem comparable to me(assuming each scenario plays out in a meaningful way).<p>Re the article...<p>A single fundamental breakthrough could make  his entire article obsolete in a single month. We&#x27;ve found a lot of limits to LLMs sure... This is always how it goes over the history of AI right? The pace of fundamental breakthroughs seems of more relevant conversation with respect to the prospects for AGI as framed by his article.</div><br/><div id="40186058" class="c"><input type="checkbox" id="c-40186058" checked=""/><div class="controls bullet"><span class="by">Vetch</span><span>|</span><a href="#40184672">root</a><span>|</span><a href="#40184890">parent</a><span>|</span><label class="collapse" for="c-40186058">[-]</label><label class="expand" for="c-40186058">[1 more]</label></div><br/><div class="children"><div class="content">The paper also proves that this capability, one unlikely to occur naturally, does not help for tasks where one must create sequentially dependent chains of reasoning, a limiting constraint. At least not without overturning what we believe about TCS.<p>&gt; A single fundamental breakthrough<p>Then we&#x27;d no longer be talking about transformers. That something unpredicted could happen is trivially true.<p>&gt; immergent capability<p>It&#x27;s specifically trained in, requires heavy supervision and is hard to learn. It&#x27;s surprising that Transformers can achieve this at all but it&#x27;s not emergent.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>