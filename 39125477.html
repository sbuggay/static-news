<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706173267814" as="style"/><link rel="stylesheet" href="styles.css?v=1706173267814"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ollama.ai/blog/python-javascript-libraries">Ollama releases Python and JavaScript Libraries</a> <span class="domain">(<a href="https://ollama.ai">ollama.ai</a>)</span></div><div class="subtext"><span>adamhowell</span> | <span>74 comments</span></div><br/><div><div id="39126671" class="c"><input type="checkbox" id="c-39126671" checked=""/><div class="controls bullet"><span class="by">ivanfioravanti</span><span>|</span><a href="#39127246">next</a><span>|</span><label class="collapse" for="c-39126671">[-]</label><label class="expand" for="c-39126671">[2 more]</label></div><br/><div class="children"><div class="content">I posted about the Python library few hours after release. Great experience.
Easy, fast and works well.<p>I create a GIST with a quick and dirty way of generating a dataset for fine-tuning Mistral model using Instruction Format on a given topic: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;ivanfioravanti&#x2F;bcacc48ef68b02e9b7a4034161824287" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;ivanfioravanti&#x2F;bcacc48ef68b02e9b7a40...</a></div><br/><div id="39127384" class="c"><input type="checkbox" id="c-39127384" checked=""/><div class="controls bullet"><span class="by">jumperabg</span><span>|</span><a href="#39126671">parent</a><span>|</span><a href="#39127246">next</a><span>|</span><label class="collapse" for="c-39127384">[-]</label><label class="expand" for="c-39127384">[1 more]</label></div><br/><div class="children"><div class="content">How does this fine-tuning work? I can see that you are loading a train.jsonl file and the some instructions but is the output model generated or this is some kind of a new way of training the models?</div><br/></div></div></div></div><div id="39127246" class="c"><input type="checkbox" id="c-39127246" checked=""/><div class="controls bullet"><span class="by">reacharavindh</span><span>|</span><a href="#39126671">prev</a><span>|</span><a href="#39125771">next</a><span>|</span><label class="collapse" for="c-39127246">[-]</label><label class="expand" for="c-39127246">[7 more]</label></div><br/><div class="children"><div class="content">Not directly related to what Ollama aims to achieve. But, I’ll ask nevertheless.<p>Local LLMs are great! But, it would be more useful once we can _easily_ throw our own data for them to use as reference or even as a source of truth. This is where it opens doors that a closed system like OpenAI cannot - I’m never going to upload some  data to ChatGPT for them to train on.<p>Could Ollama make it easier and standardize the way to add documents to local LLMs?<p>I’m not talking about uploading one image or model and asking a question about it. I’m referring to pointing a repository of 1000 text files and asking LLMs questions based on their contents.</div><br/><div id="39127461" class="c"><input type="checkbox" id="c-39127461" checked=""/><div class="controls bullet"><span class="by">sciolist</span><span>|</span><a href="#39127246">parent</a><span>|</span><a href="#39127350">next</a><span>|</span><label class="collapse" for="c-39127461">[-]</label><label class="expand" for="c-39127461">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s two main ways to &quot;add documents to LLMs&quot; - using documents in retrieval augmented generation (RAG) and training&#x2F;finetuning models. I believe you can use RAG with Ollama, however Ollama doesn&#x27;t do the training of models.</div><br/></div></div><div id="39127350" class="c"><input type="checkbox" id="c-39127350" checked=""/><div class="controls bullet"><span class="by">jampekka</span><span>|</span><a href="#39127246">parent</a><span>|</span><a href="#39127461">prev</a><span>|</span><a href="#39127274">next</a><span>|</span><label class="collapse" for="c-39127350">[-]</label><label class="expand" for="c-39127350">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like Retrieval Augmented Generation. This is the technique used by e.g. most customized chatbots.</div><br/></div></div><div id="39127274" class="c"><input type="checkbox" id="c-39127274" checked=""/><div class="controls bullet"><span class="by">emmanueloga_</span><span>|</span><a href="#39127246">parent</a><span>|</span><a href="#39127350">prev</a><span>|</span><a href="#39127292">next</a><span>|</span><label class="collapse" for="c-39127274">[-]</label><label class="expand" for="c-39127274">[1 more]</label></div><br/><div class="children"><div class="content">I don’t know if Ollama can do this but <a href="https:&#x2F;&#x2F;gpt4all.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gpt4all.io&#x2F;</a> can.</div><br/></div></div><div id="39127292" class="c"><input type="checkbox" id="c-39127292" checked=""/><div class="controls bullet"><span class="by">reacharavindh</span><span>|</span><a href="#39127246">parent</a><span>|</span><a href="#39127274">prev</a><span>|</span><a href="#39127400">next</a><span>|</span><label class="collapse" for="c-39127292">[-]</label><label class="expand" for="c-39127292">[2 more]</label></div><br/><div class="children"><div class="content">Basically, I want to do what this product does, but locally with a model running on Ollama. 
<a href="https:&#x2F;&#x2F;www.zenfetch.com&#x2F;">https:&#x2F;&#x2F;www.zenfetch.com&#x2F;</a></div><br/><div id="39127534" class="c"><input type="checkbox" id="c-39127534" checked=""/><div class="controls bullet"><span class="by">NetOpWibby</span><span>|</span><a href="#39127246">root</a><span>|</span><a href="#39127292">parent</a><span>|</span><a href="#39127400">next</a><span>|</span><label class="collapse" for="c-39127534">[-]</label><label class="expand" for="c-39127534">[1 more]</label></div><br/><div class="children"><div class="content">Ooh, I want this too.</div><br/></div></div></div></div><div id="39127400" class="c"><input type="checkbox" id="c-39127400" checked=""/><div class="controls bullet"><span class="by">asterix_pano</span><span>|</span><a href="#39127246">parent</a><span>|</span><a href="#39127292">prev</a><span>|</span><a href="#39125771">next</a><span>|</span><label class="collapse" for="c-39127400">[-]</label><label class="expand" for="c-39127400">[1 more]</label></div><br/><div class="children"><div class="content">Llama_index basically does that. You have even some tuto using Streamlit that creates a UI around it for you.</div><br/></div></div></div></div><div id="39125771" class="c"><input type="checkbox" id="c-39125771" checked=""/><div class="controls bullet"><span class="by">rgbrgb</span><span>|</span><a href="#39127246">prev</a><span>|</span><a href="#39126364">next</a><span>|</span><label class="collapse" for="c-39125771">[-]</label><label class="expand" for="c-39125771">[6 more]</label></div><br/><div class="children"><div class="content">Are these libraries for connecting to an ollama service that the user has already installed or do they work without the user installing anything? Sorry for not checking the code but maybe someone has the same question here.<p>I looked at using ollama when I started making FreeChat [0] but couldn&#x27;t figure out a way to make it work without asking the user to install it first (think I asked in your discord at the time). I wanted FreeChat to be 1-click install from the mac app store so I ended up bundling the llama.cpp server instead which it runs on localhost for inference. At some point I&#x27;d love to swap it out for ollama and take advantage of all the cool model pulling stuff you guys have done, I just need it to be embeddable.<p>My ideal setup would be importing an ollama package in swift which would start the server if the user doesn&#x27;t already have it running. I know this is just js and python to start but a dev can dream :)<p>Either way, congrats on the release!<p>[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;psugihara&#x2F;FreeChat">https:&#x2F;&#x2F;github.com&#x2F;psugihara&#x2F;FreeChat</a></div><br/><div id="39125804" class="c"><input type="checkbox" id="c-39125804" checked=""/><div class="controls bullet"><span class="by">icyfox</span><span>|</span><a href="#39125771">parent</a><span>|</span><a href="#39126364">next</a><span>|</span><label class="collapse" for="c-39125804">[-]</label><label class="expand" for="c-39125804">[5 more]</label></div><br/><div class="children"><div class="content">Just for connecting to an existing service:
<a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama-python&#x2F;blob&#x2F;main&#x2F;ollama&#x2F;_client.py#L109">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama-python&#x2F;blob&#x2F;main&#x2F;ollama&#x2F;_cl...</a></div><br/><div id="39126201" class="c"><input type="checkbox" id="c-39126201" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39125771">root</a><span>|</span><a href="#39125804">parent</a><span>|</span><a href="#39126364">next</a><span>|</span><label class="collapse" for="c-39126201">[-]</label><label class="expand" for="c-39126201">[4 more]</label></div><br/><div class="children"><div class="content">For the client API it&#x27;s pretty clear:<p><pre><code>    from ollama import Client
    client = Client(host=&#x27;http:&#x2F;&#x2F;localhost:11434&#x27;)

</code></pre>
But I don&#x27;t quite get how the example in &quot;Usage&quot; can work:<p><pre><code>    import ollama
    response = ollama.chat(model=&#x27;llama2&#x27;, messages=[
    {
        &#x27;role&#x27;: &#x27;user&#x27;,
        &#x27;content&#x27;: &#x27;Why is the sky blue?&#x27;,
    },
    ])
    print(response[&#x27;message&#x27;][&#x27;content&#x27;])
</code></pre>
Since there is no parameter for host and&#x2F;or port.</div><br/><div id="39126234" class="c"><input type="checkbox" id="c-39126234" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39125771">root</a><span>|</span><a href="#39126201">parent</a><span>|</span><a href="#39126364">next</a><span>|</span><label class="collapse" for="c-39126234">[-]</label><label class="expand" for="c-39126234">[3 more]</label></div><br/><div class="children"><div class="content">Once you have a custom `client` you can use it in place of `ollama`. For example:<p><pre><code>  client = Client(host=&#x27;http:&#x2F;&#x2F;my.ollama.host:11434&#x27;)
  response = client.chat(model=&#x27;llama2&#x27;, messages=[...])</code></pre></div><br/><div id="39126529" class="c"><input type="checkbox" id="c-39126529" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39125771">root</a><span>|</span><a href="#39126234">parent</a><span>|</span><a href="#39126364">next</a><span>|</span><label class="collapse" for="c-39126529">[-]</label><label class="expand" for="c-39126529">[2 more]</label></div><br/><div class="children"><div class="content">Thanks. I don&#x27;t have the service installed on my computer RN, but I assume the former works because it by default uses a host (localhost) and port number that is also the default for ollma service?</div><br/><div id="39126847" class="c"><input type="checkbox" id="c-39126847" checked=""/><div class="controls bullet"><span class="by">flakes</span><span>|</span><a href="#39125771">root</a><span>|</span><a href="#39126529">parent</a><span>|</span><a href="#39126364">next</a><span>|</span><label class="collapse" for="c-39126847">[-]</label><label class="expand" for="c-39126847">[1 more]</label></div><br/><div class="children"><div class="content">Exactly that. Client host options default, <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama-python&#x2F;blob&#x2F;main&#x2F;ollama&#x2F;_client.py#L610-L657">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama-python&#x2F;blob&#x2F;main&#x2F;ollama&#x2F;_cl...</a><p>Also overrideable with OLLAMA_HOST env var. The default imported functions are then based off of a no-arg constructed client <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama-python&#x2F;blob&#x2F;main&#x2F;ollama&#x2F;__init__.py">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama-python&#x2F;blob&#x2F;main&#x2F;ollama&#x2F;__i...</a><p><pre><code>    # ollama-python&#x2F;ollama&#x2F;__init__.py
    _client = Client()

    generate = _client.generate
    chat = _client.chat
    embeddings = _client.embeddings
    ...</code></pre></div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39126364" class="c"><input type="checkbox" id="c-39126364" checked=""/><div class="controls bullet"><span class="by">palashkulsh</span><span>|</span><a href="#39125771">prev</a><span>|</span><a href="#39126403">next</a><span>|</span><label class="collapse" for="c-39126364">[-]</label><label class="expand" for="c-39126364">[4 more]</label></div><br/><div class="children"><div class="content">Noob question, and may be probably being asked at the wrong place.
Is there any way to find out min system requirements for running ollama run commands with different models.</div><br/><div id="39127042" class="c"><input type="checkbox" id="c-39127042" checked=""/><div class="controls bullet"><span class="by">mike978</span><span>|</span><a href="#39126364">parent</a><span>|</span><a href="#39126811">next</a><span>|</span><label class="collapse" for="c-39127042">[-]</label><label class="expand" for="c-39127042">[1 more]</label></div><br/><div class="children"><div class="content">I have a 11th gen intel cpu with 64gb ram and I can run most of big models slowly... so it&#x27;s partly what you can put up with.</div><br/></div></div><div id="39126811" class="c"><input type="checkbox" id="c-39126811" checked=""/><div class="controls bullet"><span class="by">slawr1805</span><span>|</span><a href="#39126364">parent</a><span>|</span><a href="#39127042">prev</a><span>|</span><a href="#39126836">next</a><span>|</span><label class="collapse" for="c-39126811">[-]</label><label class="expand" for="c-39126811">[1 more]</label></div><br/><div class="children"><div class="content">They have a high level summary of ram requirements for the parameter size of each model and how much storage each model uses on their GitHub: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama#model-library">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama#model-library</a></div><br/></div></div></div></div><div id="39126403" class="c"><input type="checkbox" id="c-39126403" checked=""/><div class="controls bullet"><span class="by">porridgeraisin</span><span>|</span><a href="#39126364">prev</a><span>|</span><a href="#39125625">next</a><span>|</span><label class="collapse" for="c-39126403">[-]</label><label class="expand" for="c-39126403">[4 more]</label></div><br/><div class="children"><div class="content">Used ollama as part of a bash pipeline for a tiny throwaway app.<p>It blocks until there is something on the mic, then sends the wav to whisper.cpp, which then sends it to llama which picks out a structured &quot;remind me&quot; object from it, which gets saved to a text file.</div><br/><div id="39127525" class="c"><input type="checkbox" id="c-39127525" checked=""/><div class="controls bullet"><span class="by">awayto</span><span>|</span><a href="#39126403">parent</a><span>|</span><a href="#39127152">next</a><span>|</span><label class="collapse" for="c-39127525">[-]</label><label class="expand" for="c-39127525">[1 more]</label></div><br/><div class="children"><div class="content">I made something pretty similar over winter break so I could have something read books to me. ... Then it turned into a prompting mechanism of course! It uses Whisper, Ollama, and TTS from CoquiAI. It&#x27;s written in shell and should hopefully be &quot;Posix-compliant&quot;, but it does use zenity from Ubuntu; not sure how widely used zenity is.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;jcmccormick&#x2F;runtts">https:&#x2F;&#x2F;github.com&#x2F;jcmccormick&#x2F;runtts</a></div><br/></div></div><div id="39127152" class="c"><input type="checkbox" id="c-39127152" checked=""/><div class="controls bullet"><span class="by">killermouse0</span><span>|</span><a href="#39126403">parent</a><span>|</span><a href="#39127525">prev</a><span>|</span><a href="#39127244">next</a><span>|</span><label class="collapse" for="c-39127152">[-]</label><label class="expand" for="c-39127152">[1 more]</label></div><br/><div class="children"><div class="content">Would you share that code? I&#x27;m not familiar with using the mic in Linux, but interested to do something similar!</div><br/></div></div><div id="39127244" class="c"><input type="checkbox" id="c-39127244" checked=""/><div class="controls bullet"><span class="by">nbbaier</span><span>|</span><a href="#39126403">parent</a><span>|</span><a href="#39127152">prev</a><span>|</span><a href="#39125625">next</a><span>|</span><label class="collapse" for="c-39127244">[-]</label><label class="expand" for="c-39127244">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d also be really interested in seeing this</div><br/></div></div></div></div><div id="39125625" class="c"><input type="checkbox" id="c-39125625" checked=""/><div class="controls bullet"><span class="by">deepsquirrelnet</span><span>|</span><a href="#39126403">prev</a><span>|</span><a href="#39125819">next</a><span>|</span><label class="collapse" for="c-39125625">[-]</label><label class="expand" for="c-39125625">[1 more]</label></div><br/><div class="children"><div class="content">I love the ollama project. Having a local llm running as a service makes sense to me. It works really well for my use.<p>I’ll give this Python library a try. I’ve been wanting to try some fine tuning with LLMs in the loop experiments.</div><br/></div></div><div id="39125819" class="c"><input type="checkbox" id="c-39125819" checked=""/><div class="controls bullet"><span class="by">joaomdmoura</span><span>|</span><a href="#39125625">prev</a><span>|</span><a href="#39125838">next</a><span>|</span><label class="collapse" for="c-39125819">[-]</label><label class="expand" for="c-39125819">[3 more]</label></div><br/><div class="children"><div class="content">So cool! I have bene using Ollama for weeks now and I just love it! Easiest way to run local LLMs, we are actually embedding them into our product right now and super excited about it!</div><br/><div id="39127249" class="c"><input type="checkbox" id="c-39127249" checked=""/><div class="controls bullet"><span class="by">nbbaier</span><span>|</span><a href="#39125819">parent</a><span>|</span><a href="#39126385">next</a><span>|</span><label class="collapse" for="c-39127249">[-]</label><label class="expand" for="c-39127249">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the product?</div><br/></div></div><div id="39126385" class="c"><input type="checkbox" id="c-39126385" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39125819">parent</a><span>|</span><a href="#39127249">prev</a><span>|</span><a href="#39125838">next</a><span>|</span><label class="collapse" for="c-39126385">[-]</label><label class="expand" for="c-39126385">[1 more]</label></div><br/><div class="children"><div class="content">I am using ollama as LLM server + ollama-webui as chat app server. Great UI</div><br/></div></div></div></div><div id="39125838" class="c"><input type="checkbox" id="c-39125838" checked=""/><div class="controls bullet"><span class="by">sqs</span><span>|</span><a href="#39125819">prev</a><span>|</span><a href="#39126800">next</a><span>|</span><label class="collapse" for="c-39125838">[-]</label><label class="expand" for="c-39125838">[20 more]</label></div><br/><div class="children"><div class="content">I posted about my awesome experiences using Ollama a few months ago: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37662915">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37662915</a>. Ollama is definitely the easiest way to run LLMs locally, and that means it’s the best building block for applications that need to use inference. It’s like how Docker made it so any application can execute something kinda portably kinda safely on any machine. With Ollama, any application can run LLM inference on any machine.<p>Since that post, we shipped experimental support in our product for Ollama-based local inference. We had to write our own client in TypeScript but will probably be able to switch to this instead.</div><br/><div id="39126077" class="c"><input type="checkbox" id="c-39126077" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#39125838">parent</a><span>|</span><a href="#39125851">next</a><span>|</span><label class="collapse" for="c-39126077">[-]</label><label class="expand" for="c-39126077">[10 more]</label></div><br/><div class="children"><div class="content">Could you maybe compare it to llama.cpp?<p>All it took for me to get going is `make` and I basically have it working locally as a console app.</div><br/><div id="39126164" class="c"><input type="checkbox" id="c-39126164" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39126077">parent</a><span>|</span><a href="#39126112">next</a><span>|</span><label class="collapse" for="c-39126164">[-]</label><label class="expand" for="c-39126164">[8 more]</label></div><br/><div class="children"><div class="content">Ollama is built around llama.cpp, but it automatically handles templating the chat requests to the format each model expects, and it automatically loads and unloads models on demand based on which model an API client is requesting. Ollama also handles downloading and caching models (including quantized models), so you just request them by name.<p>Recently, it got better (though maybe not perfect yet) at calculating how many layers of any model will fit onto the GPU, letting you get the best performance without a bunch of tedious trial and error.<p>Similar to Dockerfiles, ollama offers Modelfiles that you can use to tweak the existing library of models (the parameters and such), or import gguf files directly if you find a model that isn’t in the library.<p>Ollama is the best way I’ve found to use LLMs locally. I’m not sure how well it would fare for multiuser scenarios, but there are probably better model servers for that anyways.<p>Running “make” on llama.cpp is really only the first step. It’s not comparable.</div><br/><div id="39126318" class="c"><input type="checkbox" id="c-39126318" checked=""/><div class="controls bullet"><span class="by">palmfacehn</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39126164">parent</a><span>|</span><a href="#39127444">next</a><span>|</span><label class="collapse" for="c-39126318">[-]</label><label class="expand" for="c-39126318">[2 more]</label></div><br/><div class="children"><div class="content">This is interesting. I wouldn&#x27;t have given the project a deeper look without this information. The lander is ambiguous. My immediate takeaway was, &quot;Here&#x27;s yet another front end promising ease of use.&quot;</div><br/><div id="39126879" class="c"><input type="checkbox" id="c-39126879" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39126318">parent</a><span>|</span><a href="#39127444">next</a><span>|</span><label class="collapse" for="c-39126879">[-]</label><label class="expand" for="c-39126879">[1 more]</label></div><br/><div class="children"><div class="content">I had similar feelings but last week finally tried it in WSL2.<p>Literally two shell commands and a largish download later I was chatting with mixtral on an aging 1070 at a positively surprising tokens&#x2F;s (almost reading speed, kinda like the first chatgpt). Felt like magic.</div><br/></div></div></div></div><div id="39127444" class="c"><input type="checkbox" id="c-39127444" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39126164">parent</a><span>|</span><a href="#39126318">prev</a><span>|</span><a href="#39126413">next</a><span>|</span><label class="collapse" for="c-39127444">[-]</label><label class="expand" for="c-39127444">[1 more]</label></div><br/><div class="children"><div class="content">For me, the critical thing was that ollama got the GPU offload for Mixtral right on a single 4090, where vLLM consistently failed with out of memory issues.<p>It&#x27;s annoying that it seems to have its own model cache, but I can live with that.</div><br/></div></div></div></div><div id="39126112" class="c"><input type="checkbox" id="c-39126112" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39126077">parent</a><span>|</span><a href="#39126164">prev</a><span>|</span><a href="#39125851">next</a><span>|</span><label class="collapse" for="c-39126112">[-]</label><label class="expand" for="c-39126112">[1 more]</label></div><br/><div class="children"><div class="content">For me the big deal with Ollama is the ease of instantly setting up a local inference API. I&#x27;ve got a beefy machine with a GPU downstairs, but Ollama allows me to easily use it from a Raspberry Pi on the main floor.</div><br/></div></div></div></div><div id="39125851" class="c"><input type="checkbox" id="c-39125851" checked=""/><div class="controls bullet"><span class="by">sqs</span><span>|</span><a href="#39125838">parent</a><span>|</span><a href="#39126077">prev</a><span>|</span><a href="#39127154">next</a><span>|</span><label class="collapse" for="c-39125851">[-]</label><label class="expand" for="c-39125851">[3 more]</label></div><br/><div class="children"><div class="content">Also one feature request - if the library (or another related library) could also transparently spin up a local Ollama instance if the user doesn’t have one already. “Transparent-on-demand-Ollama” or something.</div><br/><div id="39126200" class="c"><input type="checkbox" id="c-39126200" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39125851">parent</a><span>|</span><a href="#39126043">next</a><span>|</span><label class="collapse" for="c-39126200">[-]</label><label class="expand" for="c-39126200">[1 more]</label></div><br/><div class="children"><div class="content">I have been working on something similar to that in Msty [1]. I haven’t announced the app anywhere (including my friends as I got a few things in pipeline that I want to get out first :)<p>[1]: <a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/></div></div><div id="39126043" class="c"><input type="checkbox" id="c-39126043" checked=""/><div class="controls bullet"><span class="by">zenlikethat</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39125851">parent</a><span>|</span><a href="#39126200">prev</a><span>|</span><a href="#39127154">next</a><span>|</span><label class="collapse" for="c-39126043">[-]</label><label class="expand" for="c-39126043">[1 more]</label></div><br/><div class="children"><div class="content">That gets into process management which can get dicey, but I agree, a &quot;daemonless&quot; mode could be really interesting</div><br/></div></div></div></div><div id="39127154" class="c"><input type="checkbox" id="c-39127154" checked=""/><div class="controls bullet"><span class="by">donpdonp</span><span>|</span><a href="#39125838">parent</a><span>|</span><a href="#39125851">prev</a><span>|</span><a href="#39126564">next</a><span>|</span><label class="collapse" for="c-39127154">[-]</label><label class="expand" for="c-39127154">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to see a comparison to nitro <a href="https:&#x2F;&#x2F;github.com&#x2F;janhq&#x2F;nitro">https:&#x2F;&#x2F;github.com&#x2F;janhq&#x2F;nitro</a> which has been fantastic for running a local LLM.</div><br/></div></div><div id="39126564" class="c"><input type="checkbox" id="c-39126564" checked=""/><div class="controls bullet"><span class="by">acd10j</span><span>|</span><a href="#39125838">parent</a><span>|</span><a href="#39127154">prev</a><span>|</span><a href="#39125876">next</a><span>|</span><label class="collapse" for="c-39126564">[-]</label><label class="expand" for="c-39126564">[1 more]</label></div><br/><div class="children"><div class="content">In my experience award for easiest to run locally will go to llamafile models <a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile</a>.</div><br/></div></div><div id="39125876" class="c"><input type="checkbox" id="c-39125876" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39125838">parent</a><span>|</span><a href="#39126564">prev</a><span>|</span><a href="#39126800">next</a><span>|</span><label class="collapse" for="c-39125876">[-]</label><label class="expand" for="c-39125876">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Ollama is definitely the easiest way to run LLMs locally<p>Nitro outstripped them, 3 MB executable with OpenAI HTTP server and persistent model load</div><br/><div id="39126399" class="c"><input type="checkbox" id="c-39126399" checked=""/><div class="controls bullet"><span class="by">evantbyrne</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39125876">parent</a><span>|</span><a href="#39125951">next</a><span>|</span><label class="collapse" for="c-39126399">[-]</label><label class="expand" for="c-39126399">[1 more]</label></div><br/><div class="children"><div class="content">Who cares about executable size when the models are measured in gigabytes lol. I would prefer a Go&#x2F;Node&#x2F;Python&#x2F;etc server for a HTTP service even at 10x the size over some guy&#x27;s bespoke c++ any day of the week. Also, measuring the size of an executable after zipping is a nonsense benchmark in of itself</div><br/></div></div><div id="39125951" class="c"><input type="checkbox" id="c-39125951" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39125876">parent</a><span>|</span><a href="#39126399">prev</a><span>|</span><a href="#39126800">next</a><span>|</span><label class="collapse" for="c-39125951">[-]</label><label class="expand" for="c-39125951">[2 more]</label></div><br/><div class="children"><div class="content">Persistent model loading will be possible with: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;2146">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;2146</a> – sorry it isn&#x27;t yet! More to come on filesize and API improvements</div><br/><div id="39126224" class="c"><input type="checkbox" id="c-39126224" checked=""/><div class="controls bullet"><span class="by">akulbe</span><span>|</span><a href="#39125838">root</a><span>|</span><a href="#39125951">parent</a><span>|</span><a href="#39126800">next</a><span>|</span><label class="collapse" for="c-39126224">[-]</label><label class="expand" for="c-39126224">[1 more]</label></div><br/><div class="children"><div class="content">I just wanted to say <i>thank you</i> for being communicative and approachable and nice.</div><br/></div></div></div></div></div></div></div></div><div id="39126800" class="c"><input type="checkbox" id="c-39126800" checked=""/><div class="controls bullet"><span class="by">techn00</span><span>|</span><a href="#39125838">prev</a><span>|</span><a href="#39125905">next</a><span>|</span><label class="collapse" for="c-39126800">[-]</label><label class="expand" for="c-39126800">[2 more]</label></div><br/><div class="children"><div class="content">Does Ollama support GBNF grammars?</div><br/><div id="39127491" class="c"><input type="checkbox" id="c-39127491" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39126800">parent</a><span>|</span><a href="#39125905">next</a><span>|</span><label class="collapse" for="c-39127491">[-]</label><label class="expand" for="c-39127491">[1 more]</label></div><br/><div class="children"><div class="content">No, but it does support json formatting</div><br/></div></div></div></div><div id="39125905" class="c"><input type="checkbox" id="c-39125905" checked=""/><div class="controls bullet"><span class="by">3Sophons</span><span>|</span><a href="#39126800">prev</a><span>|</span><a href="#39126151">next</a><span>|</span><label class="collapse" for="c-39125905">[-]</label><label class="expand" for="c-39125905">[5 more]</label></div><br/><div class="children"><div class="content">The Rust+Wasm stack provides a strong alternative to Python in AI inference.<p>* Lightweight. Total runtime size is 30MB as opposed 4GB for Python and 350MB for Ollama. 
* Fast. Full native speed on GPUs. 
* Portable. Single cross-platform binary on different CPUs, GPUs and OSes.
* Secure. Sandboxed and isolated execution on untrusted devices. 
* Modern languages for inference apps. 
* Container-ready. Supported in Docker, containerd, Podman, and Kubernetes. 
* OpenAI compatible. Seamlessly integrate into the OpenAI tooling ecosystem.<p>Give it a try --- <a href="https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;wasm-runtime-agi&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.secondstate.io&#x2F;articles&#x2F;wasm-runtime-agi&#x2F;</a></div><br/><div id="39126406" class="c"><input type="checkbox" id="c-39126406" checked=""/><div class="controls bullet"><span class="by">anhldbk</span><span>|</span><a href="#39125905">parent</a><span>|</span><a href="#39126433">next</a><span>|</span><label class="collapse" for="c-39126406">[-]</label><label class="expand" for="c-39126406">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. But the gguf file for llama2 is 4.78 GB in size.<p>For ollama, llama2:7b is 3.8 GB. See: <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;llama2&#x2F;tags">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;llama2&#x2F;tags</a>. Still I see ollama requires less RAM to run llama 2</div><br/></div></div><div id="39126433" class="c"><input type="checkbox" id="c-39126433" checked=""/><div class="controls bullet"><span class="by">fillskills</span><span>|</span><a href="#39125905">parent</a><span>|</span><a href="#39126406">prev</a><span>|</span><a href="#39126151">next</a><span>|</span><label class="collapse" for="c-39126433">[-]</label><label class="expand" for="c-39126433">[3 more]</label></div><br/><div class="children"><div class="content">Why would anyone downvote this? There is nothing against HN rules and the comment itself is adding new and relevant information.</div><br/><div id="39126808" class="c"><input type="checkbox" id="c-39126808" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#39125905">root</a><span>|</span><a href="#39126433">parent</a><span>|</span><a href="#39126866">next</a><span>|</span><label class="collapse" for="c-39126808">[-]</label><label class="expand" for="c-39126808">[1 more]</label></div><br/><div class="children"><div class="content">From the HN Guidelines:<p>“Please don&#x27;t use HN primarily for promotion. It&#x27;s ok to post your own stuff part of the time, but the primary use of the site should be for curiosity.”<p>That user almost exclusively links to what appears to be their own product, which is self promotion. They also do it without clarifying their involvement, which could come across as astroturfing.<p>Self promotion sometimes (not all the time) is fine, but it should also be clearly stated as such. Doing it in a thread about a competing product is not ideal. If it came up naturally, that would be different from just interjecting a sales pitch.<p>I haven’t downvoted them, but I came close.</div><br/></div></div></div></div></div></div><div id="39126151" class="c"><input type="checkbox" id="c-39126151" checked=""/><div class="controls bullet"><span class="by">imrehg</span><span>|</span><a href="#39125905">prev</a><span>|</span><a href="#39125891">next</a><span>|</span><label class="collapse" for="c-39126151">[-]</label><label class="expand" for="c-39126151">[1 more]</label></div><br/><div class="children"><div class="content">This should be nice to be easier to integrate with things like Vanna.ai, that was on HN recently.<p>There a bunch of methods need to be implemented to work, but then usual OpenAI buts can be switched out to anything else, e.g. see the code stub in <a href="https:&#x2F;&#x2F;vanna.ai&#x2F;docs&#x2F;bigquery-other-llm-vannadb.html" rel="nofollow">https:&#x2F;&#x2F;vanna.ai&#x2F;docs&#x2F;bigquery-other-llm-vannadb.html</a><p>Looking forward to more remixes for other tools too.</div><br/></div></div><div id="39125891" class="c"><input type="checkbox" id="c-39125891" checked=""/><div class="controls bullet"><span class="by">Kostic</span><span>|</span><a href="#39126151">prev</a><span>|</span><a href="#39126510">next</a><span>|</span><label class="collapse" for="c-39125891">[-]</label><label class="expand" for="c-39125891">[4 more]</label></div><br/><div class="children"><div class="content">I used this half a year ago, love the UX but it was not possible to accelerate the workloads using an AMD GPU. How&#x27;s the support for AMD GPUs under Ollama today?</div><br/><div id="39125918" class="c"><input type="checkbox" id="c-39125918" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#39125891">parent</a><span>|</span><a href="#39127141">next</a><span>|</span><label class="collapse" for="c-39125918">[-]</label><label class="expand" for="c-39125918">[1 more]</label></div><br/><div class="children"><div class="content">Hi, I&#x27;m one of the maintainers on Ollama. We are working on supporting ROCm in the official releases.<p>If you do build from source, it should work (Instructions below):<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;development.md#linux-rocm-amd">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;development....</a><p>The reason why it&#x27;s not in released builds is because we are still testing ROCm.</div><br/></div></div><div id="39127141" class="c"><input type="checkbox" id="c-39127141" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39125891">parent</a><span>|</span><a href="#39125918">prev</a><span>|</span><a href="#39126157">next</a><span>|</span><label class="collapse" for="c-39127141">[-]</label><label class="expand" for="c-39127141">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately &quot;AMD&quot; and &quot;easy&quot; are mutually exclusive right now.<p>You can be a linux&#x2F;python dev and set up rocm.<p>Or you can run llama.cpp&#x27;s very slow OpenCL backend, but with easy setup.<p>Or you can run MLC&#x27;s very fast Vulkan backend, but with no model splitting and medium-hard setup.</div><br/></div></div><div id="39126157" class="c"><input type="checkbox" id="c-39126157" checked=""/><div class="controls bullet"><span class="by">accelbred</span><span>|</span><a href="#39125891">parent</a><span>|</span><a href="#39127141">prev</a><span>|</span><a href="#39126510">next</a><span>|</span><label class="collapse" for="c-39126157">[-]</label><label class="expand" for="c-39126157">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using it on an AMD GPU with the clblast backend.</div><br/></div></div></div></div><div id="39126510" class="c"><input type="checkbox" id="c-39126510" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#39125891">prev</a><span>|</span><a href="#39125833">next</a><span>|</span><label class="collapse" for="c-39126510">[-]</label><label class="expand" for="c-39126510">[2 more]</label></div><br/><div class="children"><div class="content">API wise, it looks very similar to the OpenAI python SDK but not quite the same. I was hoping I could swap out one client for another. Can anyone confirm they’re intentionally using an incompatible interface?</div><br/><div id="39127228" class="c"><input type="checkbox" id="c-39127228" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#39126510">parent</a><span>|</span><a href="#39125833">next</a><span>|</span><label class="collapse" for="c-39127228">[-]</label><label class="expand" for="c-39127228">[1 more]</label></div><br/><div class="children"><div class="content">There is an issue for this: [1]. I think it&#x27;s more of priority issue.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;305">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;305</a></div><br/></div></div></div></div><div id="39125833" class="c"><input type="checkbox" id="c-39125833" checked=""/><div class="controls bullet"><span class="by">WhackyIdeas</span><span>|</span><a href="#39126510">prev</a><span>|</span><a href="#39125828">next</a><span>|</span><label class="collapse" for="c-39125833">[-]</label><label class="expand" for="c-39125833">[1 more]</label></div><br/><div class="children"><div class="content">This is going to make my current project a million times easier. Nice.</div><br/></div></div><div id="39125828" class="c"><input type="checkbox" id="c-39125828" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39125833">prev</a><span>|</span><a href="#39125935">next</a><span>|</span><label class="collapse" for="c-39125828">[-]</label><label class="expand" for="c-39125828">[2 more]</label></div><br/><div class="children"><div class="content">What I hate about ollama is that it makes server configuration a PITA. ollama relies on llama.cpp to run GGUF models but while llama.cpp can keep the model in memory using `mlock` (helpful to reduce inference times), ollama simply won&#x27;t let you do that:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;1536">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;1536</a><p>Not to mention, they hide all the server configs in favor of their own &quot;sane defaults&quot;.</div><br/><div id="39125943" class="c"><input type="checkbox" id="c-39125943" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39125828">parent</a><span>|</span><a href="#39125935">next</a><span>|</span><label class="collapse" for="c-39125943">[-]</label><label class="expand" for="c-39125943">[1 more]</label></div><br/><div class="children"><div class="content">Sorry this isn&#x27;t easier!<p>You can enable mlock manually in the &#x2F;api&#x2F;generate and &#x2F;api&#x2F;chat endpoints by specifying the &quot;use_mlock&quot; option:<p>{“options”: {“use_mlock”: true}}<p>Many other sever configurations are also available there: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;api.md#request-5">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;api.md#reque...</a></div><br/></div></div></div></div><div id="39125935" class="c"><input type="checkbox" id="c-39125935" checked=""/><div class="controls bullet"><span class="by">awongh</span><span>|</span><a href="#39125828">prev</a><span>|</span><a href="#39125916">next</a><span>|</span><label class="collapse" for="c-39125935">[-]</label><label class="expand" for="c-39125935">[2 more]</label></div><br/><div class="children"><div class="content">Wow, I guess I wouldn’t have thought there would be GPU support. What’s the mechanism for this?</div><br/><div id="39127199" class="c"><input type="checkbox" id="c-39127199" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39125935">parent</a><span>|</span><a href="#39125916">next</a><span>|</span><label class="collapse" for="c-39127199">[-]</label><label class="expand" for="c-39127199">[1 more]</label></div><br/><div class="children"><div class="content">Via llama.cpp&#x27;s GPU support.</div><br/></div></div></div></div><div id="39125916" class="c"><input type="checkbox" id="c-39125916" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#39125935">prev</a><span>|</span><a href="#39126933">next</a><span>|</span><label class="collapse" for="c-39125916">[-]</label><label class="expand" for="c-39125916">[2 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re using TypeScript I highly recommend modelfusion <a href="https:&#x2F;&#x2F;modelfusion.dev&#x2F;guide&#x2F;" rel="nofollow">https:&#x2F;&#x2F;modelfusion.dev&#x2F;guide&#x2F;</a><p>It is far more robust, integrates with any LLM local or hosted, supports multi-modal, retries, structure parsing using zod and more.</div><br/><div id="39127053" class="c"><input type="checkbox" id="c-39127053" checked=""/><div class="controls bullet"><span class="by">kvz</span><span>|</span><a href="#39125916">parent</a><span>|</span><a href="#39126933">next</a><span>|</span><label class="collapse" for="c-39127053">[-]</label><label class="expand" for="c-39127053">[1 more]</label></div><br/><div class="children"><div class="content">This looks really nice but it’s good to point out that this project can use the Ollama HTTP API or any other API, but does not run models itself. So not a replacement to Ollama, but rather to the Ollama npm. Perhaps that was obvious because the post is about that, but I briefly thought this could run models too.</div><br/></div></div></div></div><div id="39126933" class="c"><input type="checkbox" id="c-39126933" checked=""/><div class="controls bullet"><span class="by">malux85</span><span>|</span><a href="#39125916">prev</a><span>|</span><a href="#39126613">next</a><span>|</span><label class="collapse" for="c-39126933">[-]</label><label class="expand" for="c-39126933">[2 more]</label></div><br/><div class="children"><div class="content">I love ollama, the engine underneath is llama.cpp, and they have the first version of self-extend about to me merged into main, so with any luck it will be available in ollama soon too!</div><br/><div id="39127202" class="c"><input type="checkbox" id="c-39127202" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39126933">parent</a><span>|</span><a href="#39126613">next</a><span>|</span><label class="collapse" for="c-39127202">[-]</label><label class="expand" for="c-39127202">[1 more]</label></div><br/><div class="children"><div class="content">A lot of the new models coming out are long context anyway. Check out Yi, InternLM and Mixtral.<p>Also, you really want to wait until flash attention is merged before using mega context with llama.cpp. The 8 bit KV cache would be ideal too.</div><br/></div></div></div></div><div id="39126613" class="c"><input type="checkbox" id="c-39126613" checked=""/><div class="controls bullet"><span class="by">leansensei</span><span>|</span><a href="#39126933">prev</a><span>|</span><a href="#39125896">next</a><span>|</span><label class="collapse" for="c-39126613">[-]</label><label class="expand" for="c-39126613">[1 more]</label></div><br/><div class="children"><div class="content">There is also an Elixir library: <a href="https:&#x2F;&#x2F;overbring.com&#x2F;blog&#x2F;2024-01-14-ollamex-ollama-api-embeddings&#x2F;" rel="nofollow">https:&#x2F;&#x2F;overbring.com&#x2F;blog&#x2F;2024-01-14-ollamex-ollama-api-emb...</a></div><br/></div></div><div id="39125896" class="c"><input type="checkbox" id="c-39125896" checked=""/><div class="controls bullet"><span class="by">jdlyga</span><span>|</span><a href="#39126613">prev</a><span>|</span><label class="collapse" for="c-39125896">[-]</label><label class="expand" for="c-39125896">[2 more]</label></div><br/><div class="children"><div class="content">Thanks Ollama</div><br/></div></div></div></div></div></div></div></body></html>