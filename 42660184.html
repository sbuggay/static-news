<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736845251939" as="style"/><link rel="stylesheet" href="styles.css?v=1736845251939"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://olup-blog.pages.dev/stories/image-detection-cars">How we used GPT-4o for image detection with 350 similar illustrations</a> <span class="domain">(<a href="https://olup-blog.pages.dev">olup-blog.pages.dev</a>)</span></div><div class="subtext"><span>olup</span> | <span>56 comments</span></div><br/><div><div id="42694032" class="c"><input type="checkbox" id="c-42694032" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#42690392">next</a><span>|</span><label class="collapse" for="c-42694032">[-]</label><label class="expand" for="c-42694032">[7 more]</label></div><br/><div class="children"><div class="content">This has been my experience. Foundation models have completely changed the game of ML. Previously, companies might have needed to hire ML engineers familiar with ML training, architectures etc to get mediocre results. Now companies can just hire a regular software engineer familiar with foundation model API’s to get excellent results. In some ways it is sad, but in other ways the result you get is so much better than we achieved before.<p>My example was an image segmentation model. I managed to create an dataset of 100,000+ images and was training UNets and other advanced models on it, always reached a good validation loss but my data was simply not diverse enough and I faced a lot of issues in actual deployment, where the data distribution kept changing on a day to day basis. Then, I tried DINO v2 from Meta, finetuned on 4 images and it solved the problem, handled all the variations in lighting etc with far higher accuracy than I ever achieved. It makes sense, DINO was train on 100M + images, I would never be able to compete with that.<p>In this case, the company still needed my expertise, because Meta just released the weights and so someone had to setup the fine-tuning pipeline. But I can imagine a fine tuning API like OpenAI’s requiring no expertise outside of simple coding. If AI results depend on scale, it naturally follows that only a few well funded companies, will build AI that actually works, and everyone else will just use their models. The only way this trend reverses, is if compute becomes so cheap and ubiquitous, that everyone can achieve the necessary scale.</div><br/><div id="42694305" class="c"><input type="checkbox" id="c-42694305" checked=""/><div class="controls bullet"><span class="by">pmontra</span><span>|</span><a href="#42694032">parent</a><span>|</span><a href="#42694923">next</a><span>|</span><label class="collapse" for="c-42694305">[-]</label><label class="expand" for="c-42694305">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The only way this trend reverses, is if compute becomes so cheap and ubiquitous, that everyone can achieve the necessary scale.<p>We would still need the 100 M+ images with accurate labels. That work can be performed collectively and open sourced but it must be maintained etc. I don&#x27;t think it will be easy.</div><br/><div id="42694688" class="c"><input type="checkbox" id="c-42694688" checked=""/><div class="controls bullet"><span class="by">goldemerald</span><span>|</span><a href="#42694032">root</a><span>|</span><a href="#42694305">parent</a><span>|</span><a href="#42694923">next</a><span>|</span><label class="collapse" for="c-42694688">[-]</label><label class="expand" for="c-42694688">[1 more]</label></div><br/><div class="children"><div class="content">DinoV2 is an unsupervised model. It learns both a high quality global image representation and local representations with no labels. It&#x27;s becoming strikingly clear that foundation models are the go to choice for common data types of natural images, text, video, and audio. The labels are effectively free, the hard part now is extracting quality from massive datasets.</div><br/></div></div></div></div><div id="42694923" class="c"><input type="checkbox" id="c-42694923" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#42694032">parent</a><span>|</span><a href="#42694305">prev</a><span>|</span><a href="#42694275">next</a><span>|</span><label class="collapse" for="c-42694923">[-]</label><label class="expand" for="c-42694923">[1 more]</label></div><br/><div class="children"><div class="content">This was exactly my experience being the ML engineer on a predictive maintenance project. We detected broken traffic signs in video feeds from trucks; first you segment, then you classify.<p>Simply yeeting every &quot;object of interest&quot; into DINOv2 and running any cheap classifier on that was a game changer.</div><br/></div></div><div id="42694275" class="c"><input type="checkbox" id="c-42694275" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#42694032">parent</a><span>|</span><a href="#42694923">prev</a><span>|</span><a href="#42694315">next</a><span>|</span><label class="collapse" for="c-42694275">[-]</label><label class="expand" for="c-42694275">[2 more]</label></div><br/><div class="children"><div class="content">Could DINO or some other model be used to identify fillable form fields in webforms and&#x2F;or PDF forms and&#x2F;or desktop apps?<p>Or does it likely just work on real world photos and cartoons and stuff?</div><br/><div id="42694314" class="c"><input type="checkbox" id="c-42694314" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#42694032">root</a><span>|</span><a href="#42694275">parent</a><span>|</span><a href="#42694315">next</a><span>|</span><label class="collapse" for="c-42694314">[-]</label><label class="expand" for="c-42694314">[1 more]</label></div><br/><div class="children"><div class="content">There are dedicated models for recognizing UI elements such as form fields. One example is <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;OmniParser">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;OmniParser</a></div><br/></div></div></div></div><div id="42694315" class="c"><input type="checkbox" id="c-42694315" checked=""/><div class="controls bullet"><span class="by">pj_mukh</span><span>|</span><a href="#42694032">parent</a><span>|</span><a href="#42694275">prev</a><span>|</span><a href="#42690392">next</a><span>|</span><label class="collapse" for="c-42694315">[-]</label><label class="expand" for="c-42694315">[1 more]</label></div><br/><div class="children"><div class="content">Just 4 images?! Damn. I’ve had to do at least in the 100’s. I guess it depends on the complexity of the segmentation.</div><br/></div></div></div></div><div id="42690392" class="c"><input type="checkbox" id="c-42690392" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#42694032">prev</a><span>|</span><a href="#42694924">next</a><span>|</span><label class="collapse" for="c-42690392">[-]</label><label class="expand" for="c-42690392">[12 more]</label></div><br/><div class="children"><div class="content">It&#x27;s tough to judge without seeing examples of the targets and the user photos, but I&#x27;m curious if this could be done with just old-school SIFT. If it really is exactly the same image in the in the corpus and on the wall, does a neural embedding model really buy you a lot? A small number of high confidence tie points  seems like it&#x27;d be all you need, but it probably depends a lot on just how challenging the user photos are.</div><br/><div id="42690601" class="c"><input type="checkbox" id="c-42690601" checked=""/><div class="controls bullet"><span class="by">Morizero</span><span>|</span><a href="#42690392">parent</a><span>|</span><a href="#42693224">next</a><span>|</span><label class="collapse" for="c-42690601">[-]</label><label class="expand" for="c-42690601">[7 more]</label></div><br/><div class="children"><div class="content">I find a lot of applied AI use-cases to be &quot;same as this other method, but more expensive&quot;.</div><br/><div id="42693229" class="c"><input type="checkbox" id="c-42693229" checked=""/><div class="controls bullet"><span class="by">relativ575</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42690601">parent</a><span>|</span><a href="#42693955">next</a><span>|</span><label class="collapse" for="c-42693229">[-]</label><label class="expand" for="c-42693229">[2 more]</label></div><br/><div class="children"><div class="content">Use cases such as?</div><br/><div id="42693422" class="c"><input type="checkbox" id="c-42693422" checked=""/><div class="controls bullet"><span class="by">Morizero</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42693229">parent</a><span>|</span><a href="#42693955">next</a><span>|</span><label class="collapse" for="c-42693422">[-]</label><label class="expand" for="c-42693422">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m in an AI focused education research group, and most &quot;smart&#x2F;personalized tutors&quot; on the market have similar processes and outcomes as paper flashcards.</div><br/></div></div></div></div><div id="42693955" class="c"><input type="checkbox" id="c-42693955" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42690601">parent</a><span>|</span><a href="#42693229">prev</a><span>|</span><a href="#42690663">next</a><span>|</span><label class="collapse" for="c-42693955">[-]</label><label class="expand" for="c-42693955">[1 more]</label></div><br/><div class="children"><div class="content">That was happening even when they were still calling it machine learning in the papers. Longer before that still. It’s the way some people reliably get papers out for better or worse. Find a known phenomenon with existing published methods, use the same dataset potentially using new method of the day, show there’s a little agreement between the old “gold standard” and your method, and boom, new paper for your cv on $hotnewmethod you can now land jobs with. Never mind no one will cite it. That’s not the point here.</div><br/></div></div><div id="42690663" class="c"><input type="checkbox" id="c-42690663" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42690601">parent</a><span>|</span><a href="#42693955">prev</a><span>|</span><a href="#42693224">next</a><span>|</span><label class="collapse" for="c-42690663">[-]</label><label class="expand" for="c-42690663">[3 more]</label></div><br/><div class="children"><div class="content">Better to spend $100 in op-ex money than spend $1 in cap-ex money reading a journal paper, especially if it lets you tell investors &quot;AI.&quot; :p</div><br/><div id="42692216" class="c"><input type="checkbox" id="c-42692216" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42690663">parent</a><span>|</span><a href="#42693224">next</a><span>|</span><label class="collapse" for="c-42692216">[-]</label><label class="expand" for="c-42692216">[2 more]</label></div><br/><div class="children"><div class="content">Your engineers cost &lt;$1&#x2F;hr and understand journal papers?</div><br/><div id="42693718" class="c"><input type="checkbox" id="c-42693718" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42692216">parent</a><span>|</span><a href="#42693224">next</a><span>|</span><label class="collapse" for="c-42693718">[-]</label><label class="expand" for="c-42693718">[1 more]</label></div><br/><div class="children"><div class="content">The 100-vs-1 is a ratio.</div><br/></div></div></div></div></div></div></div></div><div id="42693224" class="c"><input type="checkbox" id="c-42693224" checked=""/><div class="controls bullet"><span class="by">relativ575</span><span>|</span><a href="#42690392">parent</a><span>|</span><a href="#42690601">prev</a><span>|</span><a href="#42694924">next</a><span>|</span><label class="collapse" for="c-42693224">[-]</label><label class="expand" for="c-42693224">[4 more]</label></div><br/><div class="children"><div class="content">From TFA:<p>&gt; LLMs and the platforms powering them are quickly becoming one-stop shops for any ML-related tasks. From my perspective, the real revolution is not the chat ability or the knowledge embedded in these models, but rather the versatility they bring in a single system.<p>Why use another piece of software if LLM is good enough?</div><br/><div id="42693958" class="c"><input type="checkbox" id="c-42693958" checked=""/><div class="controls bullet"><span class="by">comex</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42693224">parent</a><span>|</span><a href="#42693292">next</a><span>|</span><label class="collapse" for="c-42693958">[-]</label><label class="expand" for="c-42693958">[1 more]</label></div><br/><div class="children"><div class="content">Performance.  A museum visitor may not have a good internet connection, so any solution that involves uploading a photo to a server will probably be (much) slower than client-side detection.  There’s a thin line between a magical experience and an annoying gimmick.  Making people wait for something to load is a sure way to cross that line.<p>Also privacy.  Do museum visitors know their camera data is being sent to the United States?  Is that even legal (without consent) where the museum is located?  Yes, visitors are supposed to be pointing their phone at a wall, but I suspect there will often be other people in view.</div><br/></div></div><div id="42693292" class="c"><input type="checkbox" id="c-42693292" checked=""/><div class="controls bullet"><span class="by">titzer</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42693224">parent</a><span>|</span><a href="#42693958">prev</a><span>|</span><a href="#42694924">next</a><span>|</span><label class="collapse" for="c-42693292">[-]</label><label class="expand" for="c-42693292">[2 more]</label></div><br/><div class="children"><div class="content">Cost. Same reason you don&#x27;t deliver UPS packages with B-2 bombers.</div><br/><div id="42693525" class="c"><input type="checkbox" id="c-42693525" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#42690392">root</a><span>|</span><a href="#42693292">parent</a><span>|</span><a href="#42694924">next</a><span>|</span><label class="collapse" for="c-42693525">[-]</label><label class="expand" for="c-42693525">[1 more]</label></div><br/><div class="children"><div class="content">The cost of LLM inference is cheap and will continue to decrease. More traditional methods take up far more of an engineer&#x27;s time (which also costs money).<p>If I have a project with a low enough lifetime inputs I&#x27;m not wasting my time labelling data and training a model. That time could be better spent working on something else. As long as the evaluation is thorough, it doesn&#x27;t matter. But I still like doing some labelling manually to get a feel for the problem space.</div><br/></div></div></div></div></div></div></div></div><div id="42694924" class="c"><input type="checkbox" id="c-42694924" checked=""/><div class="controls bullet"><span class="by">suriya-ganesh</span><span>|</span><a href="#42690392">prev</a><span>|</span><a href="#42692335">next</a><span>|</span><label class="collapse" for="c-42694924">[-]</label><label class="expand" for="c-42694924">[1 more]</label></div><br/><div class="children"><div class="content">This tracks with my experience.
We built a complex processing pipeline for an NLP classification, search and comprehension task. Using vector database of Proprietary data etc.<p>We ran a benchmark of our system against an LLM call and the LLM performed much better for so much cheaper, in terms of dev time, complexity, and compute.
Incredible time to be in working in the space seeing traditional problems eaten away by new paradigms</div><br/></div></div><div id="42692335" class="c"><input type="checkbox" id="c-42692335" checked=""/><div class="controls bullet"><span class="by">JayShower</span><span>|</span><a href="#42694924">prev</a><span>|</span><a href="#42660185">next</a><span>|</span><label class="collapse" for="c-42692335">[-]</label><label class="expand" for="c-42692335">[1 more]</label></div><br/><div class="children"><div class="content">Alternative solution that would require less heavy lifting of ML but a little more upfront programming:
 It sounds like the cars are arranged in a grid on the wall. Maybe it would be possible to narrow down which car the user took a photo of by looking at the photos of the surrounding cars as well, and hardcoding into the system the position of each car relative to one another?
Could potentially do that locally very quickly (maybe even at the level of QR-code speed) versus doing an embedding + LLM.<p>Con of this approach would be that it’s requires maintenance if they ever decide to change the illustration positions.</div><br/></div></div><div id="42660185" class="c"><input type="checkbox" id="c-42660185" checked=""/><div class="controls bullet"><span class="by">olup</span><span>|</span><a href="#42692335">prev</a><span>|</span><a href="#42693977">next</a><span>|</span><label class="collapse" for="c-42660185">[-]</label><label class="expand" for="c-42660185">[6 more]</label></div><br/><div class="children"><div class="content">First time for me posting this kind of story - I thought it would make an interesting case on solving a hard computer vision problem with a crafty product engineer team.</div><br/><div id="42689898" class="c"><input type="checkbox" id="c-42689898" checked=""/><div class="controls bullet"><span class="by">caioariede</span><span>|</span><a href="#42660185">parent</a><span>|</span><a href="#42692808">next</a><span>|</span><label class="collapse" for="c-42689898">[-]</label><label class="expand" for="c-42689898">[4 more]</label></div><br/><div class="children"><div class="content">Just a small feedback… I have switched to the reader mode because the font used is very challenging to read for me.</div><br/><div id="42690399" class="c"><input type="checkbox" id="c-42690399" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42660185">root</a><span>|</span><a href="#42689898">parent</a><span>|</span><a href="#42692808">next</a><span>|</span><label class="collapse" for="c-42690399">[-]</label><label class="expand" for="c-42690399">[3 more]</label></div><br/><div class="children"><div class="content">Also, having a blog post about image detection, and not showing a single picture in the whole post was quite frustrating.</div><br/><div id="42690472" class="c"><input type="checkbox" id="c-42690472" checked=""/><div class="controls bullet"><span class="by">Oarch</span><span>|</span><a href="#42660185">root</a><span>|</span><a href="#42690399">parent</a><span>|</span><a href="#42692808">next</a><span>|</span><label class="collapse" for="c-42690472">[-]</label><label class="expand" for="c-42690472">[2 more]</label></div><br/><div class="children"><div class="content">Especially given the detailed description surely the author could just generate a similar image</div><br/><div id="42691058" class="c"><input type="checkbox" id="c-42691058" checked=""/><div class="controls bullet"><span class="by">bl4ckneon</span><span>|</span><a href="#42660185">root</a><span>|</span><a href="#42690472">parent</a><span>|</span><a href="#42692808">next</a><span>|</span><label class="collapse" for="c-42691058">[-]</label><label class="expand" for="c-42691058">[1 more]</label></div><br/><div class="children"><div class="content">Just thinking that. Spend a few minutes trying to have chatgpt generate some images with Dall-E 3. Flux would probably be better to get all the specific details but ya</div><br/></div></div></div></div></div></div></div></div><div id="42692808" class="c"><input type="checkbox" id="c-42692808" checked=""/><div class="controls bullet"><span class="by">yannis</span><span>|</span><a href="#42660185">parent</a><span>|</span><a href="#42689898">prev</a><span>|</span><a href="#42693977">next</a><span>|</span><label class="collapse" for="c-42692808">[-]</label><label class="expand" for="c-42692808">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing. Interesting approach. As other commenters mentioned, article could do well with some hypothetical images. Maybe on a follow-up blog post? Also since you mentioning your Company&#x27;s name you missing an opportunity for marketing by not providing a link.</div><br/></div></div></div></div><div id="42693977" class="c"><input type="checkbox" id="c-42693977" checked=""/><div class="controls bullet"><span class="by">rldjbpin</span><span>|</span><a href="#42660185">prev</a><span>|</span><a href="#42691135">next</a><span>|</span><label class="collapse" for="c-42693977">[-]</label><label class="expand" for="c-42693977">[1 more]</label></div><br/><div class="children"><div class="content">reads to me like 95% of the &quot;conventional AI&quot; was applied to the problem and then using llm in the end seems to work like a lucky three-faced dice.<p>when &quot;embeddings&quot; are used to perform closeness test, you are using a pretrained computer vision model behind the scenes. it is doing the far majority of tasks of filtering out hundreds of images down to a handful.<p>visual llm works on textual descriptions that seem far too close for similar images. regardless, more power to the team for finding something that works for them.</div><br/></div></div><div id="42691135" class="c"><input type="checkbox" id="c-42691135" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#42693977">prev</a><span>|</span><a href="#42690387">next</a><span>|</span><label class="collapse" for="c-42691135">[-]</label><label class="expand" for="c-42691135">[3 more]</label></div><br/><div class="children"><div class="content">Interesting approach to a a very interesting challenge, given how close the images supposedly are.<p>With the limited training data they have I&#x27;m surprised they don&#x27;t mention any attempts at synthetic training data. Make (or buy) a couple museum scenes in blender, hang one of the images there, take images from a lot of angles, repeat for more scenes, lighting conditions and all 350 images. Should be easy to script. Then train YOLO on those images, or if that still fails use their embedding approach with those training images.</div><br/><div id="42691315" class="c"><input type="checkbox" id="c-42691315" checked=""/><div class="controls bullet"><span class="by">brody_hamer</span><span>|</span><a href="#42691135">parent</a><span>|</span><a href="#42690387">next</a><span>|</span><label class="collapse" for="c-42691315">[-]</label><label class="expand" for="c-42691315">[2 more]</label></div><br/><div class="children"><div class="content">They did.<p>&gt; “ To address this limitation, we turned to data augmentation, artificially creating new versions of each image by modifying colors, adding noise, applying distortion, or rotating images. By the end, we had generated 600 augmented images per car.”</div><br/><div id="42692941" class="c"><input type="checkbox" id="c-42692941" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#42691135">root</a><span>|</span><a href="#42691315">parent</a><span>|</span><a href="#42690387">next</a><span>|</span><label class="collapse" for="c-42692941">[-]</label><label class="expand" for="c-42692941">[1 more]</label></div><br/><div class="children"><div class="content">Those are pretty standard. A standard YOLO training run applies more transformations than that, and there are ready-made modules that do the same in keras and pytorch (for their mobilenet and VGG16). I&#x27;m not sure if anyone is training any serious vision algorithm without that kind of data augmentation.<p>What I am talking about is that they want to recognize scenes containing the images, but only have the images as training data. They have a good idea what those scenes will look like. Going there to take actual training pictures was evidently not viable, but generating approximations of them might have been.</div><br/></div></div></div></div></div></div><div id="42690387" class="c"><input type="checkbox" id="c-42690387" checked=""/><div class="controls bullet"><span class="by">kredd</span><span>|</span><a href="#42691135">prev</a><span>|</span><a href="#42693892">next</a><span>|</span><label class="collapse" for="c-42690387">[-]</label><label class="expand" for="c-42690387">[6 more]</label></div><br/><div class="children"><div class="content">A bit tangential, but I think we will see a good chunk of small teams building competing products in different software business segments, by just doubling on productivity and offering a cheaper option due to less operational overhead (reads: paying engineers). I can think of at least two businesses that can be competed in costs if the team can automate a good chunk of it.</div><br/><div id="42690466" class="c"><input type="checkbox" id="c-42690466" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#42690387">parent</a><span>|</span><a href="#42691598">next</a><span>|</span><label class="collapse" for="c-42690466">[-]</label><label class="expand" for="c-42690466">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I can think of at least two businesses that can be competed in costs if the team can automate a good chunk of it.<p>And which would those be?</div><br/><div id="42690585" class="c"><input type="checkbox" id="c-42690585" checked=""/><div class="controls bullet"><span class="by">kredd</span><span>|</span><a href="#42690387">root</a><span>|</span><a href="#42690466">parent</a><span>|</span><a href="#42690762">next</a><span>|</span><label class="collapse" for="c-42690585">[-]</label><label class="expand" for="c-42690585">[1 more]</label></div><br/><div class="children"><div class="content">We both know I didn&#x27;t write it down with the hopes that I&#x27;ll act on the at some point in the near future, and want to avoid my imaginary competitors. Even though, in reality, I will ponder about it for another week or two, give up without actually getting anything done, then regret for never trying :)</div><br/></div></div><div id="42690762" class="c"><input type="checkbox" id="c-42690762" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#42690387">root</a><span>|</span><a href="#42690466">parent</a><span>|</span><a href="#42690585">prev</a><span>|</span><a href="#42691598">next</a><span>|</span><label class="collapse" for="c-42690762">[-]</label><label class="expand" for="c-42690762">[2 more]</label></div><br/><div class="children"><div class="content">Job applications, recruiter outreach and initial screening calls. I heard of an AI interviewer via voice chat on a reddit thread recently.</div><br/><div id="42691172" class="c"><input type="checkbox" id="c-42691172" checked=""/><div class="controls bullet"><span class="by">rad_gruchalski</span><span>|</span><a href="#42690387">root</a><span>|</span><a href="#42690762">parent</a><span>|</span><a href="#42691598">next</a><span>|</span><label class="collapse" for="c-42691172">[-]</label><label class="expand" for="c-42691172">[1 more]</label></div><br/><div class="children"><div class="content">„AI” talking to an „AI”. What a time to be alive.</div><br/></div></div></div></div></div></div></div></div><div id="42693892" class="c"><input type="checkbox" id="c-42693892" checked=""/><div class="controls bullet"><span class="by">babyent</span><span>|</span><a href="#42690387">prev</a><span>|</span><a href="#42691839">next</a><span>|</span><label class="collapse" for="c-42693892">[-]</label><label class="expand" for="c-42693892">[1 more]</label></div><br/><div class="children"><div class="content">This was a fun read. I’m not a AI expert by any means. I’m also ESL. Please bear with me.<p>However the inaccuracy threshold seems fine for a museum, but in enterprise operations inaccuracy can mean lost revenue or worse lost trust and future business flow.<p>I’m struggling with some more advanced AI use cases in my collaborative work platform. I use AI (LLMs) for things like summarizations, communication, finding information using embedding. However, sometimes it is completely wrong.<p>To test this I spent a few days (doing something unrelated) building up a recipes database and then trying to query it for things like “I want to make a quick and easy drink”. I ran the data through classification and other steps to get as good data as I could. The results would still include fries or some other food result when I’m asking for drinks.<p>So I have to ask what the heck am I doing wrong? Again, for things like sending messages and reminders or coming up with descriptions, and finding old messages that match some input - no problem.<p>But if I have data that I’m augmenting with additional information (trying to attach more information that maybe missing but possible to deduce from what’s available) to try and enable richer workflows I’m always being bit in the butt. I feel like if I can figure this out I can provide way more value.<p>Not sure if what I said makes sense.</div><br/></div></div><div id="42691839" class="c"><input type="checkbox" id="c-42691839" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42693892">prev</a><span>|</span><a href="#42690210">next</a><span>|</span><label class="collapse" for="c-42691839">[-]</label><label class="expand" for="c-42691839">[2 more]</label></div><br/><div class="children"><div class="content">Calling an llm and a cv model by the same name to give the appearance of agi is a pet peeve of mine.<p>And someone that&#x27;s not openai buying into this naming convention is just unpaid propaganda</div><br/><div id="42693336" class="c"><input type="checkbox" id="c-42693336" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42691839">parent</a><span>|</span><a href="#42690210">next</a><span>|</span><label class="collapse" for="c-42693336">[-]</label><label class="expand" for="c-42693336">[1 more]</label></div><br/><div class="children"><div class="content">How would you prefer people talk about it? &quot;Multimodal LLM&quot;? My understanding is the vision portion is indeed wired directly to (and trained alongside) the language portion.<p>&gt; give the appearance of agi<p>Can you point out where specifically they&#x27;re doing this? Best I can tell, they give a decent summary of the effectiveness of multi-modal LLM&#x27;s with support for vision, and then talk about using it to solve an incredibly narrow task. The only diction I could see that hints at &quot;agi&quot; is when they describe the versatility of this approach; but how could you possibly argue against that? It&#x27;s objectively more versatile (if not wasteful and more expensive).</div><br/></div></div></div></div><div id="42690210" class="c"><input type="checkbox" id="c-42690210" checked=""/><div class="controls bullet"><span class="by">saint_yossarian</span><span>|</span><a href="#42691839">prev</a><span>|</span><a href="#42689911">next</a><span>|</span><label class="collapse" for="c-42690210">[-]</label><label class="expand" for="c-42690210">[8 more]</label></div><br/><div class="children"><div class="content">I mean, cool tech, but why not just print a QR code next to each illustration?</div><br/><div id="42695028" class="c"><input type="checkbox" id="c-42695028" checked=""/><div class="controls bullet"><span class="by">olup</span><span>|</span><a href="#42690210">parent</a><span>|</span><a href="#42694395">next</a><span>|</span><label class="collapse" for="c-42695028">[-]</label><label class="expand" for="c-42695028">[1 more]</label></div><br/><div class="children"><div class="content">Poster here. We would have loved that, and it was one of our first proposal - a QR code or some kind of marker. However, the client is understandably very controlling on the aesthetics of their wall as a central element of their scenography. We would have pushed for it again in the last resort, but would probably have lost the contract.</div><br/></div></div><div id="42694395" class="c"><input type="checkbox" id="c-42694395" checked=""/><div class="controls bullet"><span class="by">cuu508</span><span>|</span><a href="#42690210">parent</a><span>|</span><a href="#42695028">prev</a><span>|</span><a href="#42694957">next</a><span>|</span><label class="collapse" for="c-42694395">[-]</label><label class="expand" for="c-42694395">[1 more]</label></div><br/><div class="children"><div class="content">Or just a human-readable label with the model and year on it. Visitors would not need to mess with gadgets to read the labels which would be a huge usability win.</div><br/></div></div><div id="42694957" class="c"><input type="checkbox" id="c-42694957" checked=""/><div class="controls bullet"><span class="by">nthingtohide</span><span>|</span><a href="#42690210">parent</a><span>|</span><a href="#42694395">prev</a><span>|</span><a href="#42692316">next</a><span>|</span><label class="collapse" for="c-42694957">[-]</label><label class="expand" for="c-42694957">[1 more]</label></div><br/><div class="children"><div class="content">Or just geo tag the room itself.</div><br/></div></div><div id="42692316" class="c"><input type="checkbox" id="c-42692316" checked=""/><div class="controls bullet"><span class="by">JayShower</span><span>|</span><a href="#42690210">parent</a><span>|</span><a href="#42694957">prev</a><span>|</span><a href="#42691705">next</a><span>|</span><label class="collapse" for="c-42692316">[-]</label><label class="expand" for="c-42692316">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like the client cared a lot about the user experience being smooth (they declined the solution of presenting the user with the narrowed-down choices of which car they took a picture of), and I think adding a bunch of QR codes to this aesthetic wall of car illustrations would not align with that goal.</div><br/></div></div><div id="42691705" class="c"><input type="checkbox" id="c-42691705" checked=""/><div class="controls bullet"><span class="by">urbandw311er</span><span>|</span><a href="#42690210">parent</a><span>|</span><a href="#42692316">prev</a><span>|</span><a href="#42690408">next</a><span>|</span><label class="collapse" for="c-42691705">[-]</label><label class="expand" for="c-42691705">[2 more]</label></div><br/><div class="children"><div class="content">This feels like one of those “NASA spent millions developing a space pen, Russians took a pencil” moments.</div><br/><div id="42693947" class="c"><input type="checkbox" id="c-42693947" checked=""/><div class="controls bullet"><span class="by">webmaven</span><span>|</span><a href="#42690210">root</a><span>|</span><a href="#42691705">parent</a><span>|</span><a href="#42690408">next</a><span>|</span><label class="collapse" for="c-42693947">[-]</label><label class="expand" for="c-42693947">[1 more]</label></div><br/><div class="children"><div class="content">... which ignores the hazards of pencil shavings in a zero-g environment, especially the graphite, a good electrical conductor.</div><br/></div></div></div></div><div id="42690408" class="c"><input type="checkbox" id="c-42690408" checked=""/><div class="controls bullet"><span class="by">nnnnico</span><span>|</span><a href="#42690210">parent</a><span>|</span><a href="#42691705">prev</a><span>|</span><a href="#42689911">next</a><span>|</span><label class="collapse" for="c-42690408">[-]</label><label class="expand" for="c-42690408">[1 more]</label></div><br/><div class="children"><div class="content">just in: using gpt4o to read QRs</div><br/></div></div></div></div><div id="42689911" class="c"><input type="checkbox" id="c-42689911" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42690210">prev</a><span>|</span><a href="#42690626">next</a><span>|</span><label class="collapse" for="c-42689911">[-]</label><label class="expand" for="c-42689911">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the “bitter lesson” news from the frontlines. Curious; did you experiment with 4o as the sole pipeline? And of course as I think you mention, it would be interesting to know if say llama 8b could do a similar job as well.<p>Congrats on shipping.</div><br/><div id="42691681" class="c"><input type="checkbox" id="c-42691681" checked=""/><div class="controls bullet"><span class="by">numba888</span><span>|</span><a href="#42689911">parent</a><span>|</span><a href="#42690626">next</a><span>|</span><label class="collapse" for="c-42691681">[-]</label><label class="expand" for="c-42691681">[1 more]</label></div><br/><div class="children"><div class="content">they don&#x27;t self-host the models, neither embedding nor last step llm. taking into account low load self-hosting likely would be more expensive. if so why not to use the best models.</div><br/></div></div></div></div><div id="42690626" class="c"><input type="checkbox" id="c-42690626" checked=""/><div class="controls bullet"><span class="by">gunalx</span><span>|</span><a href="#42689911">prev</a><span>|</span><a href="#42690028">next</a><span>|</span><label class="collapse" for="c-42690626">[-]</label><label class="expand" for="c-42690626">[1 more]</label></div><br/><div class="children"><div class="content">Cool real life use Case.  
Don&#x27;t think lmms usually get applied reasonably where they should be and I am glad that a generic knn model also was used to simplify costs and also just more suitable.</div><br/></div></div><div id="42690028" class="c"><input type="checkbox" id="c-42690028" checked=""/><div class="controls bullet"><span class="by">schappim</span><span>|</span><a href="#42690626">prev</a><span>|</span><a href="#42690172">next</a><span>|</span><label class="collapse" for="c-42690028">[-]</label><label class="expand" for="c-42690028">[1 more]</label></div><br/><div class="children"><div class="content">I would love to see the prompt &#x2F; image data sent to GPT-4o!</div><br/></div></div><div id="42690172" class="c"><input type="checkbox" id="c-42690172" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#42690028">prev</a><span>|</span><a href="#42690161">next</a><span>|</span><label class="collapse" for="c-42690172">[-]</label><label class="expand" for="c-42690172">[1 more]</label></div><br/><div class="children"><div class="content">Is there a reason to choose VGG16 over more modern models?</div><br/></div></div><div id="42690161" class="c"><input type="checkbox" id="c-42690161" checked=""/><div class="controls bullet"><span class="by">gazchop</span><span>|</span><a href="#42690172">prev</a><span>|</span><a href="#42689982">next</a><span>|</span><label class="collapse" for="c-42690161">[-]</label><label class="expand" for="c-42690161">[1 more]</label></div><br/><div class="children"><div class="content">I hear a lot of qualitative speak but nothing quantitative.</div><br/></div></div></div></div></div></div></div></body></html>