<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691917258801" as="style"/><link rel="stylesheet" href="styles.css?v=1691917258801"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anyscale.com/blog/num-every-llm-developer-should-know">Numbers every LLM Developer should know</a> <span class="domain">(<a href="https://www.anyscale.com">www.anyscale.com</a>)</span></div><div class="subtext"><span>davidwu</span> | <span>16 comments</span></div><br/><div><div id="37106736" class="c"><input type="checkbox" id="c-37106736" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#37107110">next</a><span>|</span><label class="collapse" for="c-37106736">[-]</label><label class="expand" for="c-37106736">[5 more]</label></div><br/><div class="children"><div class="content">This is honestly a bit gross, as it&#x27;s just a marketing piece.<p>The original numbers every programmer should know is a profound piece of pedagogy, aimed at helping programmers be better at their craft.<p>This is just an excerpt from a pitch deck.</div><br/><div id="37107045" class="c"><input type="checkbox" id="c-37107045" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#37106736">parent</a><span>|</span><a href="#37107190">prev</a><span>|</span><a href="#37107110">next</a><span>|</span><label class="collapse" for="c-37107045">[-]</label><label class="expand" for="c-37107045">[3 more]</label></div><br/><div class="children"><div class="content">Where can I find the original?</div><br/><div id="37107096" class="c"><input type="checkbox" id="c-37107096" checked=""/><div class="controls bullet"><span class="by">dijit</span><span>|</span><a href="#37106736">root</a><span>|</span><a href="#37107045">parent</a><span>|</span><a href="#37107110">next</a><span>|</span><label class="collapse" for="c-37107096">[-]</label><label class="expand" for="c-37107096">[2 more]</label></div><br/><div class="children"><div class="content">Its linked in the article: <a href="http:&#x2F;&#x2F;brenocon.com&#x2F;dean_perf.html" rel="nofollow noreferrer">http:&#x2F;&#x2F;brenocon.com&#x2F;dean_perf.html</a></div><br/><div id="37107595" class="c"><input type="checkbox" id="c-37107595" checked=""/><div class="controls bullet"><span class="by">chaosite</span><span>|</span><a href="#37106736">root</a><span>|</span><a href="#37107096">parent</a><span>|</span><a href="#37107110">next</a><span>|</span><label class="collapse" for="c-37107595">[-]</label><label class="expand" for="c-37107595">[1 more]</label></div><br/><div class="children"><div class="content">I like the interactive version: <a href="https:&#x2F;&#x2F;colin-scott.github.io&#x2F;personal_website&#x2F;research&#x2F;interactive_latency.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;colin-scott.github.io&#x2F;personal_website&#x2F;research&#x2F;inte...</a></div><br/></div></div></div></div></div></div></div></div><div id="37107110" class="c"><input type="checkbox" id="c-37107110" checked=""/><div class="controls bullet"><span class="by">crashocaster</span><span>|</span><a href="#37106736">prev</a><span>|</span><a href="#37106890">next</a><span>|</span><label class="collapse" for="c-37107110">[-]</label><label class="expand" for="c-37107110">[1 more]</label></div><br/><div class="children"><div class="content">Actually, the only numbers every LLM developer should know are their accelerator specs.
For example:<p>A100 specs:<p>- 312e12 BF16 FLOPS<p>- 1555e9 GB&#x2F;s HBM bandwidth<p>H100:<p>- 1000e12&#x2F;2000e12 BF16&#x2F;INT8 FLOPS<p>(apply ~0.7 flops efficiency multiplier because h100s power throttle extremely quickly)<p>- 3000 GB&#x2F;s HBM bandwidth<p>---<p>For a 13B model on an A100, this nets:<p>13e9 * 2 bytes per param = 26 GB HBM required (at bf16)<p>26e9&#x2F;1555e9 = 17ms &#x2F; token small-batch latency (~60 tokens &#x2F; second)<p>What about large batches?<p>latency for some batch size B is 13e9 * 2 FLOP per param * B &#x2F; 312e12<p>We want B such that we&#x27;re just about no longer HBM bound:
26e9&#x2F;312e12 * B = 17ms<p>&lt;=&gt; 17e-3&#x2F;(26e9&#x2F;312e12)<p>giving a batch size of 204.<p>At that batch size (and all larger batch sizes), the a100 delivers a throughput of
B * 1&#x2F;17ms = 12000 tokens &#x2F; second<p>---<p>KV caching, multi-gpu and -node comms and matmul efficiencies left as an exercise to the reader :)</div><br/></div></div><div id="37106890" class="c"><input type="checkbox" id="c-37106890" checked=""/><div class="controls bullet"><span class="by">vikp</span><span>|</span><a href="#37107110">prev</a><span>|</span><a href="#37107942">next</a><span>|</span><label class="collapse" for="c-37106890">[-]</label><label class="expand" for="c-37106890">[2 more]</label></div><br/><div class="children"><div class="content">I clicked because I thought they were defining LLM developer as &quot;someone training LLMs&quot;, but instead they define it as &quot;someone integrating LLMs into their application&quot;.<p>If you also had the same initial thought as me, this is an excellent article - <a href="https:&#x2F;&#x2F;blog.eleuther.ai&#x2F;transformer-math&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.eleuther.ai&#x2F;transformer-math&#x2F;</a> .</div><br/><div id="37107199" class="c"><input type="checkbox" id="c-37107199" checked=""/><div class="controls bullet"><span class="by">luckyt</span><span>|</span><a href="#37106890">parent</a><span>|</span><a href="#37107942">next</a><span>|</span><label class="collapse" for="c-37107199">[-]</label><label class="expand" for="c-37107199">[1 more]</label></div><br/><div class="children"><div class="content">I had the same thought, but overall, there is probably an order of magnitude more people using LLMs in applications or fine-tuning them compared to those trying to pretrain LLMs from scratch.</div><br/></div></div></div></div><div id="37107942" class="c"><input type="checkbox" id="c-37107942" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37106890">prev</a><span>|</span><a href="#37106628">next</a><span>|</span><label class="collapse" for="c-37107942">[-]</label><label class="expand" for="c-37107942">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t think I&#x27;ve ever heard anyone call it &quot;GRAM&quot; instead of VRAM.<p>Another cost saving tip: On API, do combo calls where possible to dual use the input tokens. e.g.<p>&quot;&quot;&quot;You are an AI assistant that summarizes text given.<p>After the summarized text, add the word END.<p>After that answer the following questions with Yes or NO:<p>Is the text about Donald Trump?<p>Is the text about Space?
&quot;&quot;&quot;<p>Down side is now you need code to parse the output pieces &amp; error handling around that</div><br/></div></div><div id="37106628" class="c"><input type="checkbox" id="c-37106628" checked=""/><div class="controls bullet"><span class="by">marban</span><span>|</span><a href="#37107942">prev</a><span>|</span><a href="#37107291">next</a><span>|</span><label class="collapse" for="c-37106628">[-]</label><label class="expand" for="c-37106628">[1 more]</label></div><br/><div class="children"><div class="content">This will not age well.</div><br/></div></div><div id="37107291" class="c"><input type="checkbox" id="c-37107291" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37106628">prev</a><span>|</span><a href="#37106795">next</a><span>|</span><label class="collapse" for="c-37107291">[-]</label><label class="expand" for="c-37107291">[1 more]</label></div><br/><div class="children"><div class="content">The main thing every LLM should know is that ARM will eat x86_64&#x27;s lunch in ML. Why? Because of the shared&#x2F;unified memory model. M2 Ultra from apple can use up to 192GB of RAM. Even your smartphone thanks to this model can run networks a lot bigger than you would expect.</div><br/></div></div><div id="37106795" class="c"><input type="checkbox" id="c-37106795" checked=""/><div class="controls bullet"><span class="by">regecks</span><span>|</span><a href="#37107291">prev</a><span>|</span><a href="#37106951">next</a><span>|</span><label class="collapse" for="c-37106795">[-]</label><label class="expand" for="c-37106795">[1 more]</label></div><br/><div class="children"><div class="content">What’s this “neural information retrieval system” thing about?<p>I’m just hacking away and presenting the LLM with some JSON data from our metrics database and making it answer user questions as a completion.<p>Is this embedding thing relevant for what I’m doing? Where should I start reading?</div><br/></div></div><div id="37106951" class="c"><input type="checkbox" id="c-37106951" checked=""/><div class="controls bullet"><span class="by">fullstackchris</span><span>|</span><a href="#37106795">prev</a><span>|</span><label class="collapse" for="c-37106951">[-]</label><label class="expand" for="c-37106951">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious about the point on the embedding lookup cost... in my experience for an embedding lookup to be accurate, you have to include your entire document dataset to be queried against... obviously this can be just as expensive as querying a full cloud model if your dataset is very large. Interested if anyone had thoughts about this.</div><br/><div id="37107126" class="c"><input type="checkbox" id="c-37107126" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#37106951">parent</a><span>|</span><a href="#37107062">next</a><span>|</span><label class="collapse" for="c-37107126">[-]</label><label class="expand" for="c-37107126">[1 more]</label></div><br/><div class="children"><div class="content">Yes. I think the point is that the price per token for creating the  embeddings using e.g. OpenAI&#x27;s text-embedding-ada-002 api might be low, this will add up to some significant cost for a large document corpus. The suggestion to roll your own based on freely available embedding models is sound IMHO.<p>Now how to chunk those documents into semantically coherent pieces for context retrieval, that is the real challange though.</div><br/></div></div><div id="37107062" class="c"><input type="checkbox" id="c-37107062" checked=""/><div class="controls bullet"><span class="by">phreeza</span><span>|</span><a href="#37106951">parent</a><span>|</span><a href="#37107126">prev</a><span>|</span><label class="collapse" for="c-37107062">[-]</label><label class="expand" for="c-37107062">[1 more]</label></div><br/><div class="children"><div class="content">There are very efficient algorithms for doing this, but of course it may still be expensive if your dataset is very large. See <a href="https:&#x2F;&#x2F;ann-benchmarks.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ann-benchmarks.com&#x2F;</a> for some of the algorithms</div><br/></div></div></div></div></div></div></div></div></div></body></html>