<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726822877095" as="style"/><link rel="stylesheet" href="styles.css?v=1726822877095"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/news/contextual-retrieval">Anthropic – Introducing Contextual Retrieval</a> <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>loganfrederick</span> | <span>19 comments</span></div><br/><div><div id="41599782" class="c"><input type="checkbox" id="c-41599782" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41599341">next</a><span>|</span><label class="collapse" for="c-41599782">[-]</label><label class="expand" for="c-41599782">[5 more]</label></div><br/><div class="children"><div class="content">To add some context, this isn&#x27;t that novel of an approach. A common approach to improve RAG results is to &quot;expand&quot; the underlying chunks using an llm, so as to increase the semantic surface area to match against. You can further improve your results by running query expansion using HyDE[1], though it&#x27;s not always an improvement. I use it as a fallback.<p>I&#x27;m not sure what Anthropic is introducing here. I looked at the cookbook code and it&#x27;s just showing the process of producing said context, but there&#x27;s no actual change to their API regarding &quot;contextual retrieval&quot;.<p>The one change is prompt caching, introduced a month back, which allows you to very cheaply add better context to individual chunks by providing the entire (long) document as context. Caching is an awesome feature to expose to developers and I don&#x27;t want to take anything away from that.<p>However, other than that, the only thing I see introduced is just a cookbook on how to do a particular rag workflow.<p>As an aside, Cohere may be my favorite API to work with. (no affiliation) Their RAG API is a delight, and unlike anything else provided by other providers. I highly recommend it.<p>1: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10496" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10496</a></div><br/><div id="41599799" class="c"><input type="checkbox" id="c-41599799" checked=""/><div class="controls bullet"><span class="by">resiros</span><span>|</span><a href="#41599782">parent</a><span>|</span><a href="#41599341">next</a><span>|</span><label class="collapse" for="c-41599799">[-]</label><label class="expand" for="c-41599799">[4 more]</label></div><br/><div class="children"><div class="content">I think the innovation is using caching as so to make the cost of the approach manageable. The way they implemented it is that each time you create a chunk, you ask the llm to create an atomic chunk from the whole context. You need to do this for all tens of thousands of chunks in your data. This costs a lot. By caching the documents, you can spare costs</div><br/><div id="41599845" class="c"><input type="checkbox" id="c-41599845" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41599799">parent</a><span>|</span><a href="#41599841">next</a><span>|</span><label class="collapse" for="c-41599845">[-]</label><label class="expand" for="c-41599845">[1 more]</label></div><br/><div class="children"><div class="content">Yup. Caching is very nice.. but the framing is weird. &quot;Introducing&quot; to me, connotes a product release, not a new tutorial.</div><br/></div></div><div id="41599841" class="c"><input type="checkbox" id="c-41599841" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41599799">parent</a><span>|</span><a href="#41599845">prev</a><span>|</span><a href="#41599341">next</a><span>|</span><label class="collapse" for="c-41599841">[-]</label><label class="expand" for="c-41599841">[2 more]</label></div><br/><div class="children"><div class="content">You could also just save the first outputted atomic chunk and store it then re-use it each time yourself. Easier and more consistent.</div><br/><div id="41599858" class="c"><input type="checkbox" id="c-41599858" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41599841">parent</a><span>|</span><a href="#41599341">next</a><span>|</span><label class="collapse" for="c-41599858">[-]</label><label class="expand" for="c-41599858">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, that only works if you keep chunk windows static.</div><br/></div></div></div></div></div></div></div></div><div id="41599341" class="c"><input type="checkbox" id="c-41599341" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#41599782">prev</a><span>|</span><a href="#41599929">next</a><span>|</span><label class="collapse" for="c-41599341">[-]</label><label class="expand" for="c-41599341">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a fan of this technique. I agree the scenario they lay out is a common problem, but the proposed solution feels odd.<p>Vector embeddings have bag-of-words compression properties and can over-index on the first newline separated text block to the extent that certain indices in the resulting vector end up much closer to 0 than they otherwise would. With quantization, they can eventually become 0 and cause you to lose out on lots of precision with the dense vectors. IDF search overcomes this to some extent, but not enough.<p>You can &quot;semantically boost&quot; embeddings such that they move closer to your document&#x27;s title, summary, abstract, etc. and get the recall benefits of this &quot;context&quot; prepend without polluting the underlying vector. Implementation wise it&#x27;s a weighted sum. During the augmentation step where you put things in the context window, you can always inject the summary chunk when the doc matches as well. Much cleaner solution imo.<p>Description of &quot;semantic boost&quot; in the Trieve API[1]:<p>&gt;semantic_boost: Semantic boost is useful for moving the embedding vector of the chunk in the direction of the distance phrase. I.e. you can push a chunk with a chunk_html of &quot;iphone&quot; 25% closer to the term &quot;flagship&quot; by using the distance phrase &quot;flagship&quot; and a distance factor of 0.25. Conceptually it&#x27;s drawing a line (euclidean&#x2F;L2 distance) between the vector for the innerText of the chunk_html and distance_phrase then moving the vector of the chunk_html distance_factorL2Distance closer to or away from the distance_phrase point along the line between the two points.<p>[1]:<a href="https:&#x2F;&#x2F;docs.trieve.ai&#x2F;api-reference&#x2F;chunk&#x2F;create-or-upsert-chunk-or-chunks">https:&#x2F;&#x2F;docs.trieve.ai&#x2F;api-reference&#x2F;chunk&#x2F;create-or-upsert-...</a></div><br/></div></div><div id="41599929" class="c"><input type="checkbox" id="c-41599929" checked=""/><div class="controls bullet"><span class="by">_bramses</span><span>|</span><a href="#41599341">prev</a><span>|</span><a href="#41599388">next</a><span>|</span><label class="collapse" for="c-41599929">[-]</label><label class="expand" for="c-41599929">[1 more]</label></div><br/><div class="children"><div class="content">The technique I find most useful is to implement a “linked list” strategy where a chunk has multiple pointers to the entry it is referenced by. This task is done manually, but the diversity of the ways you can reference a particular node go up dramatically.<p>Another way to look at it, comments. Imagine every comment under this post is a pointer back to the original post. Some will be close in distance, and others will be farther, due to the perception of the authors of the comments themselves. But if you assign each comment a “parent_id”, your access to the post multiplies.<p>You can see an example of this technique here [1]. I don’t attempt to mind read what the end user will query for, I simply let them tell me, and then index that as a pointer. There are only a finite number of options to represent a given object. But some representations are very, very, very far from the semantic meaning of the core object.<p>[1] - <a href="https:&#x2F;&#x2F;x.com&#x2F;yourcommonbase&#x2F;status&#x2F;1833262865194557505" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;yourcommonbase&#x2F;status&#x2F;1833262865194557505</a></div><br/></div></div><div id="41599388" class="c"><input type="checkbox" id="c-41599388" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41599929">prev</a><span>|</span><a href="#41599736">next</a><span>|</span><label class="collapse" for="c-41599388">[-]</label><label class="expand" for="c-41599388">[6 more]</label></div><br/><div class="children"><div class="content">My favorite thing about this is the way it takes advantage of prompt caching.<p>That&#x27;s priced at around 1&#x2F;10th of what the prompts would normally cost if they weren&#x27;t cached, which means that tricks like this (running every single chunk against a full copy of the original document) become feasible where previously they wouldn&#x27;t have financially made sense.<p>I bet there are all sorts of other neat tricks like this which are opened up by caching cost savings.<p>My notes on contextual retrieval: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;20&#x2F;introducing-contextual-retrieval&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;20&#x2F;introducing-contextual...</a> and prompt caching: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Aug&#x2F;14&#x2F;prompt-caching-with-claude&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Aug&#x2F;14&#x2F;prompt-caching-with-cl...</a></div><br/><div id="41599662" class="c"><input type="checkbox" id="c-41599662" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#41599388">parent</a><span>|</span><a href="#41599736">next</a><span>|</span><label class="collapse" for="c-41599662">[-]</label><label class="expand" for="c-41599662">[5 more]</label></div><br/><div class="children"><div class="content">You could do a lot of stuff with pre-calculating things for your embeddings. Why cache when you can pre-calculate. That brings into play a whole lot of things people commonly do as part of ETL.<p>I come from a traditional search back ground. It&#x27;s quite obvious to me that RAG is a bit of a naive strategy if you limit it to just using vector search with some off the shelf embedding model. Vector search simply isn&#x27;t that good. You need additional information retrieval strategies if you want to improve the context you provide to the LLM. That is effectively what they are doing here.<p>Microsoft published an interesting paper on graph RAG some time ago where they combine RAG with vector search based on a conceptual graph that they construct from the indexed data using entity extraction. This allows them to pull in contextually relevant information for matching chunks.<p>I have a hunch that you could probably get quite far without doing any vector search at all. It would be a lot cheaper too. Simply use a traditional search engine and some tuned query. The trick is of course query tuning. Which may not work that well for general purpose use cases but it could work for more specialized use cases.</div><br/><div id="41599889" class="c"><input type="checkbox" id="c-41599889" checked=""/><div class="controls bullet"><span class="by">TmpstsTrrctta</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599662">parent</a><span>|</span><a href="#41599937">next</a><span>|</span><label class="collapse" for="c-41599889">[-]</label><label class="expand" for="c-41599889">[1 more]</label></div><br/><div class="children"><div class="content">I have experience in traditional search as well and I think this is doing some limiting of my imagination when it comes to vector search. In the post, I did like the introduction of the Contextual BM25 compared to other hybrid approaches then doing rrf.<p>For question answering, vector&#x2F;semantic search is clearly a better fit in my mind, and I can see how the contextual models can enable and bolster that. However, because I’ve implemented and used so many keyword based systems, that just doesn’t seem to be how my brain works.<p>An example I’m thinking of is finding a sushi restaurant near me with availability this weekend around dinner time. I’d love to be able to search for this as I’ve written it. How I would search for it would be search for sushi restaurant, sort by distance and hope the application does a proper job of surfacing time filtering.<p>Conversely, this is mostly how I would build this system. Perhaps with a layer to determine user intention to pull out restaurant type, location sorting, and time filtering.<p>I could see using semantic search for filtering down the restaurants to related to sushi, but do we then drop back into traditional search for filtering and sorting? Utilize function calling to have the LLM parameterize our search query?<p>As stated, perhaps I’m not thinking of these the right way because of my experiences with existing systems, which I find seem to give me better results when well built</div><br/></div></div><div id="41599937" class="c"><input type="checkbox" id="c-41599937" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599662">parent</a><span>|</span><a href="#41599889">prev</a><span>|</span><a href="#41599736">next</a><span>|</span><label class="collapse" for="c-41599937">[-]</label><label class="expand" for="c-41599937">[3 more]</label></div><br/><div class="children"><div class="content">Graph RAG is very cool and outstanding at filling some niches. IIRC, Perplexity&#x27;s actual search is just BM25 (based a lex fridman interview of the founder).</div><br/><div id="41600054" class="c"><input type="checkbox" id="c-41600054" checked=""/><div class="controls bullet"><span class="by">_hfqa</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599937">parent</a><span>|</span><a href="#41599998">next</a><span>|</span><label class="collapse" for="c-41600054">[-]</label><label class="expand" for="c-41600054">[1 more]</label></div><br/><div class="children"><div class="content">Do you have the link and the time in the video where he mentions it?</div><br/></div></div><div id="41599998" class="c"><input type="checkbox" id="c-41599998" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599937">parent</a><span>|</span><a href="#41600054">prev</a><span>|</span><a href="#41599736">next</a><span>|</span><label class="collapse" for="c-41599998">[-]</label><label class="expand" for="c-41599998">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense; perplexity is really responsive and fast usually.<p>I need to check out that interview with Lex Fridman.</div><br/></div></div></div></div></div></div></div></div><div id="41599736" class="c"><input type="checkbox" id="c-41599736" checked=""/><div class="controls bullet"><span class="by">valstu</span><span>|</span><a href="#41599388">prev</a><span>|</span><a href="#41599750">next</a><span>|</span><label class="collapse" for="c-41599736">[-]</label><label class="expand" for="c-41599736">[3 more]</label></div><br/><div class="children"><div class="content">We&#x27;re doing something similar. We first chunk the documents based on h1,h2,h3 headings. Then we add headers in the beginning of the chunk as a context. As an imagenary example, instead of one chunk being:<p><pre><code>  The usual dose for adults is one or two 200mg tablets or 
  capsules 3 times a day.
</code></pre>
It is now something like:<p><pre><code>  # Fever
  ## Treatment
  ---
  The usual dose for adults is one or two 200mg tablets or 
  capsules 3 times a day.
</code></pre>
This seems to work pretty well, and doesn&#x27;t require any LLMs when indexing documents.<p>(Edited formatting)</div><br/><div id="41599769" class="c"><input type="checkbox" id="c-41599769" checked=""/><div class="controls bullet"><span class="by">cabidaher</span><span>|</span><a href="#41599736">parent</a><span>|</span><a href="#41599750">next</a><span>|</span><label class="collapse" for="c-41599769">[-]</label><label class="expand" for="c-41599769">[2 more]</label></div><br/><div class="children"><div class="content">Did you experiment with different ways to format those included headers? Asking because I am doing something similar to that as well.</div><br/><div id="41599805" class="c"><input type="checkbox" id="c-41599805" checked=""/><div class="controls bullet"><span class="by">valstu</span><span>|</span><a href="#41599736">root</a><span>|</span><a href="#41599769">parent</a><span>|</span><a href="#41599750">next</a><span>|</span><label class="collapse" for="c-41599805">[-]</label><label class="expand" for="c-41599805">[1 more]</label></div><br/><div class="children"><div class="content">Nope, not yet. We have sticked with markdownish syntax so far.</div><br/></div></div></div></div></div></div><div id="41599750" class="c"><input type="checkbox" id="c-41599750" checked=""/><div class="controls bullet"><span class="by">timwaagh</span><span>|</span><a href="#41599736">prev</a><span>|</span><a href="#41598787">next</a><span>|</span><label class="collapse" for="c-41599750">[-]</label><label class="expand" for="c-41599750">[1 more]</label></div><br/><div class="children"><div class="content">I guess this does give some insights. Using a more space efficient language for your codebase will mean more functionality in the ais context window when working with Claude and code.</div><br/></div></div><div id="41598787" class="c"><input type="checkbox" id="c-41598787" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41599750">prev</a><span>|</span><label class="collapse" for="c-41598787">[-]</label><label class="expand" for="c-41598787">[1 more]</label></div><br/><div class="children"><div class="content">This sounds a lot like how we used to do research, by reading books and writing any interesting quotes on index cards, along with where they came from. I wonder if prompting for that would result in better chunks? It might make it easier to review if you wanted to do it manually.</div><br/></div></div></div></div></div></div></div></body></html>