<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726909253206" as="style"/><link rel="stylesheet" href="styles.css?v=1726909253206"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/news/contextual-retrieval">Contextual Retrieval</a> <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>loganfrederick</span> | <span>68 comments</span></div><br/><div><div id="41600451" class="c"><input type="checkbox" id="c-41600451" checked=""/><div class="controls bullet"><span class="by">underlines</span><span>|</span><a href="#41599782">next</a><span>|</span><label class="collapse" for="c-41600451">[-]</label><label class="expand" for="c-41600451">[11 more]</label></div><br/><div class="children"><div class="content">We build a corporate RAG for a government entity. What I&#x27;ve learned so far by applying an experimental A&#x2F;B testing approach to RAG using RAGAS metrics:<p>- Hybrid Retrieval (semantic + vector) and then LLM based Reranking made no significant change using synthetic eva-questions<p>- HyDE decreased answer quality and retrieval quality severly when measured with RAGAS using synthetic eval-questions<p>(we still have to do a RAGAS eval using expert and real user questions)<p>So yes, hybrid retrieval is always good - that&#x27;s no news to anyone building production ready or enterprise RAG solutions. But one method doesn&#x27;t always win. We found semantic search of Azure AI Search being sufficient as a second method, next to vector similarity. Others might find BM25 great, or a fine tuned query post processing SLM. Depends on the use case. Test, test, test.<p>Next things we&#x27;re going to try:<p>- RAPTOR<p>- SelfRAG<p>- Agentic RAG<p>- Query Refinement (expansion and sub-queries)<p>- GraphRAG<p>Learning so far:<p>- Always use a baseline and an experiment to try to refute your null hypothesis using measures like RAGAS or others.<p>- Use three types of evaluation questions&#x2F;answers: 1. Expert written q&amp;a, 2. Real user questions (from logs), 3. Synthetic q&amp;a generated from your source documents</div><br/><div id="41601219" class="c"><input type="checkbox" id="c-41601219" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#41600451">parent</a><span>|</span><a href="#41605316">next</a><span>|</span><label class="collapse" for="c-41601219">[-]</label><label class="expand" for="c-41601219">[9 more]</label></div><br/><div class="children"><div class="content">Could you explain or link to explanations of all of the acronyms you’ve used in your comment?</div><br/><div id="41608439" class="c"><input type="checkbox" id="c-41608439" checked=""/><div class="controls bullet"><span class="by">santiagobasulto</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41601219">parent</a><span>|</span><a href="#41601858">next</a><span>|</span><label class="collapse" for="c-41608439">[-]</label><label class="expand" for="c-41608439">[1 more]</label></div><br/><div class="children"><div class="content">These are all &quot;techniques&quot; on top of the foundations of RAG. It&#x27;s similar to &quot;Chain of Thought&quot; in prompt engineering. You have an underlying technology, and then come up with techniques&#x2F;frameworks on top. What MVC was for Web dev +15 years ago.<p>RAPTOR for example is a technique that groups and clusters documents together, summarizes them, and creates embeddings defining a sort of a Tree. Paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2401.18059v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2401.18059v1</a><p>Agentic RAG is creating an agent that can decide to augment &quot;conversations&quot; (or other LLM tools) with RAG searches and analyze its relevance. Pretty useful, but hard to implement right.<p>You can google the others, they&#x27;re all more or less these &quot;techniques&quot; to improve an old-fashioned RAG search.</div><br/></div></div><div id="41601858" class="c"><input type="checkbox" id="c-41601858" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41601219">parent</a><span>|</span><a href="#41608439">prev</a><span>|</span><a href="#41605316">next</a><span>|</span><label class="collapse" for="c-41601858">[-]</label><label class="expand" for="c-41601858">[7 more]</label></div><br/><div class="children"><div class="content">It makes me chuckle a bit to see this kind of request in a tech forum, particularly when discussing advanced LLM-related topics.<p>This is akin to a HN comment asking someone to search the Internet for something on their behalf, while discussing search engine algorithms!</div><br/><div id="41605358" class="c"><input type="checkbox" id="c-41605358" checked=""/><div class="controls bullet"><span class="by">_kb</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41601858">parent</a><span>|</span><a href="#41602037">next</a><span>|</span><label class="collapse" for="c-41605358">[-]</label><label class="expand" for="c-41605358">[4 more]</label></div><br/><div class="children"><div class="content">A lot of people here (myself included) work across different specialisations and are here to learn from discussion that is intentionally unfamiliar.</div><br/><div id="41605519" class="c"><input type="checkbox" id="c-41605519" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41605358">parent</a><span>|</span><a href="#41602037">next</a><span>|</span><label class="collapse" for="c-41605519">[-]</label><label class="expand" for="c-41605519">[3 more]</label></div><br/><div class="children"><div class="content">Yes, but ChatGPT knows these things! Just ask it to expand the acronyms.<p>This is the new “can you Google that for me?”</div><br/><div id="41605704" class="c"><input type="checkbox" id="c-41605704" checked=""/><div class="controls bullet"><span class="by">_kb</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41605519">parent</a><span>|</span><a href="#41602037">next</a><span>|</span><label class="collapse" for="c-41605704">[-]</label><label class="expand" for="c-41605704">[2 more]</label></div><br/><div class="children"><div class="content">يمكن لـ ChatGPT أيضًا الترجمة من العربية إلى الإنجليزية، ولكن سيكون من المزعج استخدامه للمحادثة في هذا السياق</div><br/><div id="41606368" class="c"><input type="checkbox" id="c-41606368" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41605704">parent</a><span>|</span><a href="#41602037">next</a><span>|</span><label class="collapse" for="c-41606368">[-]</label><label class="expand" for="c-41606368">[1 more]</label></div><br/><div class="children"><div class="content">Annyira lusta vagyok, hogy nem akarok néhány gombot megnyomni, ezért kérlek, írj nekem egy oldal szöveget.</div><br/></div></div></div></div></div></div></div></div><div id="41602037" class="c"><input type="checkbox" id="c-41602037" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41601858">parent</a><span>|</span><a href="#41605358">prev</a><span>|</span><a href="#41605316">next</a><span>|</span><label class="collapse" for="c-41602037">[-]</label><label class="expand" for="c-41602037">[2 more]</label></div><br/><div class="children"><div class="content">It adds useful context to the discussion and spurs further conversation.</div><br/><div id="41602110" class="c"><input type="checkbox" id="c-41602110" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#41600451">root</a><span>|</span><a href="#41602037">parent</a><span>|</span><a href="#41605316">next</a><span>|</span><label class="collapse" for="c-41602110">[-]</label><label class="expand" for="c-41602110">[1 more]</label></div><br/><div class="children"><div class="content">HyDE: Hypothetical Document Embeddings [1]<p>RAGAS: RAG Assessment [2]<p>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval [3]<p>Self-RAG: Self-Reflective Retrieval-Augmented Generation [4]<p>Agentic RAG: Agentic Retrieval-Augmented Generation [5]<p>GraphRAG: Graph Retrieval-Augmented Generation [6]<p>[1] <a href="https:&#x2F;&#x2F;docs.haystack.deepset.ai&#x2F;docs&#x2F;hypothetical-document-embeddings-hyde" rel="nofollow">https:&#x2F;&#x2F;docs.haystack.deepset.ai&#x2F;docs&#x2F;hypothetical-document-...</a><p>[2] <a href="https:&#x2F;&#x2F;docs.ragas.io&#x2F;en&#x2F;stable&#x2F;">https:&#x2F;&#x2F;docs.ragas.io&#x2F;en&#x2F;stable&#x2F;</a><p>[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2401.18059v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2401.18059v1</a><p>[4] <a href="https:&#x2F;&#x2F;selfrag.github.io" rel="nofollow">https:&#x2F;&#x2F;selfrag.github.io</a><p>[5] <a href="https:&#x2F;&#x2F;langchain-ai.github.io&#x2F;langgraph&#x2F;tutorials&#x2F;rag&#x2F;langgraph_agentic_rag&#x2F;" rel="nofollow">https:&#x2F;&#x2F;langchain-ai.github.io&#x2F;langgraph&#x2F;tutorials&#x2F;rag&#x2F;langg...</a><p>[6] <a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;graphrag-unlocking-llm-discovery-on-narrative-private-data&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;graphrag-unloc...</a></div><br/></div></div></div></div></div></div></div></div><div id="41605316" class="c"><input type="checkbox" id="c-41605316" checked=""/><div class="controls bullet"><span class="by">turing_complete</span><span>|</span><a href="#41600451">parent</a><span>|</span><a href="#41601219">prev</a><span>|</span><a href="#41599782">next</a><span>|</span><label class="collapse" for="c-41605316">[-]</label><label class="expand" for="c-41605316">[1 more]</label></div><br/><div class="children"><div class="content">What do you think of HippoRAG? Did you try it or plan to do?</div><br/></div></div></div></div><div id="41599782" class="c"><input type="checkbox" id="c-41599782" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41600451">prev</a><span>|</span><a href="#41599388">next</a><span>|</span><label class="collapse" for="c-41599782">[-]</label><label class="expand" for="c-41599782">[9 more]</label></div><br/><div class="children"><div class="content">To add some context, this isn&#x27;t that novel of an approach. A common approach to improve RAG results is to &quot;expand&quot; the underlying chunks using an llm, so as to increase the semantic surface area to match against. You can further improve your results by running query expansion using HyDE[1], though it&#x27;s not always an improvement. I use it as a fallback.<p>I&#x27;m not sure what Anthropic is introducing here. I looked at the cookbook code and it&#x27;s just showing the process of producing said context, but there&#x27;s no actual change to their API regarding &quot;contextual retrieval&quot;.<p>The one change is prompt caching, introduced a month back, which allows you to very cheaply add better context to individual chunks by providing the entire (long) document as context. Caching is an awesome feature to expose to developers and I don&#x27;t want to take anything away from that.<p>However, other than that, the only thing I see introduced is just a cookbook on how to do a particular rag workflow.<p>As an aside, Cohere may be my favorite API to work with. (no affiliation) Their RAG API is a delight, and unlike anything else provided by other providers. I highly recommend it.<p>1: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10496" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10496</a></div><br/><div id="41599799" class="c"><input type="checkbox" id="c-41599799" checked=""/><div class="controls bullet"><span class="by">resiros</span><span>|</span><a href="#41599782">parent</a><span>|</span><a href="#41600436">next</a><span>|</span><label class="collapse" for="c-41599799">[-]</label><label class="expand" for="c-41599799">[5 more]</label></div><br/><div class="children"><div class="content">I think the innovation is using caching as so to make the cost of the approach manageable. The way they implemented it is that each time you create a chunk, you ask the llm to create an atomic chunk from the whole context. You need to do this for all tens of thousands of chunks in your data. This costs a lot. By caching the documents, you can spare costs</div><br/><div id="41599841" class="c"><input type="checkbox" id="c-41599841" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41599799">parent</a><span>|</span><a href="#41599845">next</a><span>|</span><label class="collapse" for="c-41599841">[-]</label><label class="expand" for="c-41599841">[3 more]</label></div><br/><div class="children"><div class="content">You could also just save the first outputted atomic chunk and store it then re-use it each time yourself. Easier and more consistent.</div><br/><div id="41601028" class="c"><input type="checkbox" id="c-41601028" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41599841">parent</a><span>|</span><a href="#41599858">next</a><span>|</span><label class="collapse" for="c-41601028">[-]</label><label class="expand" for="c-41601028">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how that helps here. They&#x27;re not regenerating each chunk every time, this is about caching the state after running a large doc through a model. You can only do this kind of thing if you have access to the model itself, or it&#x27;s provided by the API you use.</div><br/></div></div><div id="41599858" class="c"><input type="checkbox" id="c-41599858" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41599841">parent</a><span>|</span><a href="#41601028">prev</a><span>|</span><a href="#41599845">next</a><span>|</span><label class="collapse" for="c-41599858">[-]</label><label class="expand" for="c-41599858">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, that only works if you keep chunk windows static.</div><br/></div></div></div></div><div id="41599845" class="c"><input type="checkbox" id="c-41599845" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41599799">parent</a><span>|</span><a href="#41599841">prev</a><span>|</span><a href="#41600436">next</a><span>|</span><label class="collapse" for="c-41599845">[-]</label><label class="expand" for="c-41599845">[1 more]</label></div><br/><div class="children"><div class="content">Yup. Caching is very nice.. but the framing is weird. &quot;Introducing&quot; to me, connotes a product release, not a new tutorial.</div><br/></div></div></div></div><div id="41600436" class="c"><input type="checkbox" id="c-41600436" checked=""/><div class="controls bullet"><span class="by">bayesianbot</span><span>|</span><a href="#41599782">parent</a><span>|</span><a href="#41599799">prev</a><span>|</span><a href="#41599388">next</a><span>|</span><label class="collapse" for="c-41600436">[-]</label><label class="expand" for="c-41600436">[3 more]</label></div><br/><div class="children"><div class="content">I was trying to do this using Prompt Caching like a month ago, but then noticed there&#x27;s five minute maximum lifetime for the cached prompts - doesn&#x27;t really work for my RAG needs (or probably most), where the queries would be ran during the next month or a year. I can&#x27;t see any changes to that policy. Little surprised to see them talk about Prompt Caching relating to RAG.</div><br/><div id="41602393" class="c"><input type="checkbox" id="c-41602393" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41600436">parent</a><span>|</span><a href="#41599388">next</a><span>|</span><label class="collapse" for="c-41602393">[-]</label><label class="expand" for="c-41602393">[2 more]</label></div><br/><div class="children"><div class="content">They aren’t using the prompt caching on the query side, only on the embedding side… so you cache the document in the context window when ingesting it, but not during retrieval.</div><br/><div id="41603857" class="c"><input type="checkbox" id="c-41603857" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#41599782">root</a><span>|</span><a href="#41602393">parent</a><span>|</span><a href="#41599388">next</a><span>|</span><label class="collapse" for="c-41603857">[-]</label><label class="expand" for="c-41603857">[1 more]</label></div><br/><div class="children"><div class="content">It seems a little odd to make multiple requests instead of using one request to create all the context for all the chunks.</div><br/></div></div></div></div></div></div></div></div><div id="41599388" class="c"><input type="checkbox" id="c-41599388" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41599782">prev</a><span>|</span><a href="#41599736">next</a><span>|</span><label class="collapse" for="c-41599388">[-]</label><label class="expand" for="c-41599388">[14 more]</label></div><br/><div class="children"><div class="content">My favorite thing about this is the way it takes advantage of prompt caching.<p>That&#x27;s priced at around 1&#x2F;10th of what the prompts would normally cost if they weren&#x27;t cached, which means that tricks like this (running every single chunk against a full copy of the original document) become feasible where previously they wouldn&#x27;t have financially made sense.<p>I bet there are all sorts of other neat tricks like this which are opened up by caching cost savings.<p>My notes on contextual retrieval: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;20&#x2F;introducing-contextual-retrieval&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;20&#x2F;introducing-contextual...</a> and prompt caching: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Aug&#x2F;14&#x2F;prompt-caching-with-claude&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Aug&#x2F;14&#x2F;prompt-caching-with-cl...</a></div><br/><div id="41600486" class="c"><input type="checkbox" id="c-41600486" checked=""/><div class="controls bullet"><span class="by">thruway516</span><span>|</span><a href="#41599388">parent</a><span>|</span><a href="#41599662">next</a><span>|</span><label class="collapse" for="c-41600486">[-]</label><label class="expand" for="c-41600486">[2 more]</label></div><br/><div class="children"><div class="content">I follow your blog and read almost everything you write about Llms. Just curious (if you havent already written about it somewhere and I missed it), how much do you spend monthly, exploring all the various Llms and their features? (I think its a useful context for having a grasp of how much I would have to spend to keep up to date with the models out there and the latest features)</div><br/><div id="41601252" class="c"><input type="checkbox" id="c-41601252" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41600486">parent</a><span>|</span><a href="#41599662">next</a><span>|</span><label class="collapse" for="c-41601252">[-]</label><label class="expand" for="c-41601252">[1 more]</label></div><br/><div class="children"><div class="content">Most months I spend less than $10 total across the OpenAI, Anthropic and Google APIs - for the kind of stuff I do I’m just not racking up really high token counts.<p>I spend $20&#x2F;month on ChatGPT plus and $20&#x2F;month on Claude Pro. I get GitHub Copilot for free as an open source maintainer.</div><br/></div></div></div></div><div id="41599662" class="c"><input type="checkbox" id="c-41599662" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#41599388">parent</a><span>|</span><a href="#41600486">prev</a><span>|</span><a href="#41602993">next</a><span>|</span><label class="collapse" for="c-41599662">[-]</label><label class="expand" for="c-41599662">[9 more]</label></div><br/><div class="children"><div class="content">You could do a lot of stuff with pre-calculating things for your embeddings. Why cache when you can pre-calculate. That brings into play a whole lot of things people commonly do as part of ETL.<p>I come from a traditional search back ground. It&#x27;s quite obvious to me that RAG is a bit of a naive strategy if you limit it to just using vector search with some off the shelf embedding model. Vector search simply isn&#x27;t that good. You need additional information retrieval strategies if you want to improve the context you provide to the LLM. That is effectively what they are doing here.<p>Microsoft published an interesting paper on graph RAG some time ago where they combine RAG with vector search based on a conceptual graph that they construct from the indexed data using entity extraction. This allows them to pull in contextually relevant information for matching chunks.<p>I have a hunch that you could probably get quite far without doing any vector search at all. It would be a lot cheaper too. Simply use a traditional search engine and some tuned query. The trick is of course query tuning. Which may not work that well for general purpose use cases but it could work for more specialized use cases.</div><br/><div id="41599889" class="c"><input type="checkbox" id="c-41599889" checked=""/><div class="controls bullet"><span class="by">TmpstsTrrctta</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599662">parent</a><span>|</span><a href="#41600144">next</a><span>|</span><label class="collapse" for="c-41599889">[-]</label><label class="expand" for="c-41599889">[2 more]</label></div><br/><div class="children"><div class="content">I have experience in traditional search as well and I think this is doing some limiting of my imagination when it comes to vector search. In the post, I did like the introduction of the Contextual BM25 compared to other hybrid approaches then doing rrf.<p>For question answering, vector&#x2F;semantic search is clearly a better fit in my mind, and I can see how the contextual models can enable and bolster that. However, because I’ve implemented and used so many keyword based systems, that just doesn’t seem to be how my brain works.<p>An example I’m thinking of is finding a sushi restaurant near me with availability this weekend around dinner time. I’d love to be able to search for this as I’ve written it. How I would search for it would be search for sushi restaurant, sort by distance and hope the application does a proper job of surfacing time filtering.<p>Conversely, this is mostly how I would build this system. Perhaps with a layer to determine user intention to pull out restaurant type, location sorting, and time filtering.<p>I could see using semantic search for filtering down the restaurants to related to sushi, but do we then drop back into traditional search for filtering and sorting? Utilize function calling to have the LLM parameterize our search query?<p>As stated, perhaps I’m not thinking of these the right way because of my experiences with existing systems, which I find seem to give me better results when well built</div><br/><div id="41603032" class="c"><input type="checkbox" id="c-41603032" checked=""/><div class="controls bullet"><span class="by">ValentinA23</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599889">parent</a><span>|</span><a href="#41600144">next</a><span>|</span><label class="collapse" for="c-41603032">[-]</label><label class="expand" for="c-41603032">[1 more]</label></div><br/><div class="children"><div class="content">Another approach I saw is to build a conceptual graph using entity extraction and have the LLM suggest search paths through that graph to enhance the retrieval step. The LMM is fine-tuned on the conceptual graph for this specific task. Could work in your case, but you need to deal with an ontology that suits your use case, in other words it must already contain restaurant location, type of dishes served and opening hours.</div><br/></div></div></div></div><div id="41600144" class="c"><input type="checkbox" id="c-41600144" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599662">parent</a><span>|</span><a href="#41599889">prev</a><span>|</span><a href="#41599937">next</a><span>|</span><label class="collapse" for="c-41600144">[-]</label><label class="expand" for="c-41600144">[1 more]</label></div><br/><div class="children"><div class="content">GraphRAG requires you define upfront the schema of entity and relation types. This works when you are in a known domain, but in general, when you want to just answer questions from a large reference, you don&#x27;t know what you need to put in the graph.</div><br/></div></div><div id="41599937" class="c"><input type="checkbox" id="c-41599937" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599662">parent</a><span>|</span><a href="#41600144">prev</a><span>|</span><a href="#41602718">next</a><span>|</span><label class="collapse" for="c-41599937">[-]</label><label class="expand" for="c-41599937">[4 more]</label></div><br/><div class="children"><div class="content">Graph RAG is very cool and outstanding at filling some niches. IIRC, Perplexity&#x27;s actual search is just BM25 (based a lex fridman interview of the founder).</div><br/><div id="41599998" class="c"><input type="checkbox" id="c-41599998" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599937">parent</a><span>|</span><a href="#41600054">next</a><span>|</span><label class="collapse" for="c-41599998">[-]</label><label class="expand" for="c-41599998">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense; perplexity is really responsive and fast usually.<p>I need to check out that interview with Lex Fridman.</div><br/></div></div><div id="41600054" class="c"><input type="checkbox" id="c-41600054" checked=""/><div class="controls bullet"><span class="by">_hfqa</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599937">parent</a><span>|</span><a href="#41599998">prev</a><span>|</span><a href="#41602718">next</a><span>|</span><label class="collapse" for="c-41600054">[-]</label><label class="expand" for="c-41600054">[2 more]</label></div><br/><div class="children"><div class="content">Do you have the link and the time in the video where he mentions it?</div><br/><div id="41601675" class="c"><input type="checkbox" id="c-41601675" checked=""/><div class="controls bullet"><span class="by">rty32</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41600054">parent</a><span>|</span><a href="#41602718">next</a><span>|</span><label class="collapse" for="c-41601675">[-]</label><label class="expand" for="c-41601675">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtu.be&#x2F;e-gwvmhyU7A?t=2h5m41s" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;e-gwvmhyU7A?t=2h5m41s</a></div><br/></div></div></div></div></div></div><div id="41602718" class="c"><input type="checkbox" id="c-41602718" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41599662">parent</a><span>|</span><a href="#41599937">prev</a><span>|</span><a href="#41602993">next</a><span>|</span><label class="collapse" for="c-41602718">[-]</label><label class="expand" for="c-41602718">[1 more]</label></div><br/><div class="children"><div class="content">This was my exact question. Why do an LLM rewrite, when you can add a context vector to a chunk vector, and for plaintext indexing, add a context string (eg, tfidf)?<p>The article claimed other context augmentation fails, and that you are better off paying anthropic to run an LLM on all your data, but it seems quite handwavy. What vector+text search nuance does a full document cache LLM rewrite catch that cheapo methods miss? Reminds me of &quot;It is difficult to get a man to understand something when his salary depends on his not understanding it&quot;. (We process enough data that we try to limit LLMs to the retrieval step, and only embeddings &amp; light LLMs to the indexing step, so it&#x27;s a $$$ distinction for our customers.)<p>The context caching is neat in general, so I have to wonder if this use case is more about paying for ease than quality, and its value for quality is elsewhere.</div><br/></div></div></div></div><div id="41602993" class="c"><input type="checkbox" id="c-41602993" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41599388">parent</a><span>|</span><a href="#41599662">prev</a><span>|</span><a href="#41599736">next</a><span>|</span><label class="collapse" for="c-41602993">[-]</label><label class="expand" for="c-41602993">[2 more]</label></div><br/><div class="children"><div class="content">Cost is one aspect, but what about ingest time? You’re adding significant processing time to your pipeline with this method right?</div><br/><div id="41604002" class="c"><input type="checkbox" id="c-41604002" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41599388">root</a><span>|</span><a href="#41602993">parent</a><span>|</span><a href="#41599736">next</a><span>|</span><label class="collapse" for="c-41604002">[-]</label><label class="expand" for="c-41604002">[1 more]</label></div><br/><div class="children"><div class="content">I expect most implementations of RAG don&#x27;t mind this too much - if you&#x27;re dealing with only a few hundred more pages of documents a day the ingestion time from using fancy tricks like this is going to be measured in minutes.</div><br/></div></div></div></div></div></div><div id="41599736" class="c"><input type="checkbox" id="c-41599736" checked=""/><div class="controls bullet"><span class="by">valstu</span><span>|</span><a href="#41599388">prev</a><span>|</span><a href="#41599341">next</a><span>|</span><label class="collapse" for="c-41599736">[-]</label><label class="expand" for="c-41599736">[5 more]</label></div><br/><div class="children"><div class="content">We&#x27;re doing something similar. We first chunk the documents based on h1,h2,h3 headings. Then we add headers in the beginning of the chunk as a context. As an imagenary example, instead of one chunk being:<p><pre><code>  The usual dose for adults is one or two 200mg tablets or 
  capsules 3 times a day.
</code></pre>
It is now something like:<p><pre><code>  # Fever
  ## Treatment
  ---
  The usual dose for adults is one or two 200mg tablets or 
  capsules 3 times a day.
</code></pre>
This seems to work pretty well, and doesn&#x27;t require any LLMs when indexing documents.<p>(Edited formatting)</div><br/><div id="41604466" class="c"><input type="checkbox" id="c-41604466" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#41599736">parent</a><span>|</span><a href="#41600175">next</a><span>|</span><label class="collapse" for="c-41604466">[-]</label><label class="expand" for="c-41604466">[1 more]</label></div><br/><div class="children"><div class="content">I used to always wonder how do llms know whether a particular long article or audio transcript was written by say Alan Watts. Basically these kind of metadata annotation would be common while preparing training data for Llama models and so on. This could also be reason for the genesis for the argument that ChatGPT got slower in December. That &quot;date&quot; metadata would &quot;inform&quot; ChatGPT to be unhelpful.</div><br/></div></div><div id="41600175" class="c"><input type="checkbox" id="c-41600175" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41599736">parent</a><span>|</span><a href="#41604466">prev</a><span>|</span><a href="#41599769">next</a><span>|</span><label class="collapse" for="c-41600175">[-]</label><label class="expand" for="c-41600175">[1 more]</label></div><br/><div class="children"><div class="content">I am working on question answering based on long documents &#x2F; bundles of documents, 100+ pages, and I took a similar approach. I first summarize each page, give it a title and extract a list of subsections. Then I put all the summaries together and I ask the model to provide a hierarchical index. It will organize the whole bundle into a tree. At querying time I combine the path in the tree as additional context.</div><br/></div></div><div id="41599769" class="c"><input type="checkbox" id="c-41599769" checked=""/><div class="controls bullet"><span class="by">cabidaher</span><span>|</span><a href="#41599736">parent</a><span>|</span><a href="#41600175">prev</a><span>|</span><a href="#41599341">next</a><span>|</span><label class="collapse" for="c-41599769">[-]</label><label class="expand" for="c-41599769">[2 more]</label></div><br/><div class="children"><div class="content">Did you experiment with different ways to format those included headers? Asking because I am doing something similar to that as well.</div><br/><div id="41599805" class="c"><input type="checkbox" id="c-41599805" checked=""/><div class="controls bullet"><span class="by">valstu</span><span>|</span><a href="#41599736">root</a><span>|</span><a href="#41599769">parent</a><span>|</span><a href="#41599341">next</a><span>|</span><label class="collapse" for="c-41599805">[-]</label><label class="expand" for="c-41599805">[1 more]</label></div><br/><div class="children"><div class="content">Nope, not yet. We have sticked with markdownish syntax so far.</div><br/></div></div></div></div></div></div><div id="41599341" class="c"><input type="checkbox" id="c-41599341" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#41599736">prev</a><span>|</span><a href="#41599929">next</a><span>|</span><label class="collapse" for="c-41599341">[-]</label><label class="expand" for="c-41599341">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a fan of this technique. I agree the scenario they lay out is a common problem, but the proposed solution feels odd.<p>Vector embeddings have bag-of-words compression properties and can over-index on the first newline separated text block to the extent that certain indices in the resulting vector end up much closer to 0 than they otherwise would. With quantization, they can eventually become 0 and cause you to lose out on lots of precision with the dense vectors. IDF search overcomes this to some extent, but not enough.<p>You can &quot;semantically boost&quot; embeddings such that they move closer to your document&#x27;s title, summary, abstract, etc. and get the recall benefits of this &quot;context&quot; prepend without polluting the underlying vector. Implementation wise it&#x27;s a weighted sum. During the augmentation step where you put things in the context window, you can always inject the summary chunk when the doc matches as well. Much cleaner solution imo.<p>Description of &quot;semantic boost&quot; in the Trieve API[1]:<p>&gt;semantic_boost: Semantic boost is useful for moving the embedding vector of the chunk in the direction of the distance phrase. I.e. you can push a chunk with a chunk_html of &quot;iphone&quot; 25% closer to the term &quot;flagship&quot; by using the distance phrase &quot;flagship&quot; and a distance factor of 0.25. Conceptually it&#x27;s drawing a line (euclidean&#x2F;L2 distance) between the vector for the innerText of the chunk_html and distance_phrase then moving the vector of the chunk_html distance_factorL2Distance closer to or away from the distance_phrase point along the line between the two points.<p>[1]:<a href="https:&#x2F;&#x2F;docs.trieve.ai&#x2F;api-reference&#x2F;chunk&#x2F;create-or-upsert-chunk-or-chunks">https:&#x2F;&#x2F;docs.trieve.ai&#x2F;api-reference&#x2F;chunk&#x2F;create-or-upsert-...</a></div><br/><div id="41600358" class="c"><input type="checkbox" id="c-41600358" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#41599341">parent</a><span>|</span><a href="#41599929">next</a><span>|</span><label class="collapse" for="c-41600358">[-]</label><label class="expand" for="c-41600358">[5 more]</label></div><br/><div class="children"><div class="content">Sorry random question - do vector dbs work across models? I&#x27;d guess no, since embeddings are models specific afaik, but that means that a vector db would lock you into using a single LLM and even within that, a single version, like Claude-3.5 Sonnet, and you couldn&#x27;t move to 3.5 Haiku, Opus etc., never mind ChatGPT or Llama without reindexing.</div><br/><div id="41600475" class="c"><input type="checkbox" id="c-41600475" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#41599341">root</a><span>|</span><a href="#41600358">parent</a><span>|</span><a href="#41604423">next</a><span>|</span><label class="collapse" for="c-41600475">[-]</label><label class="expand" for="c-41600475">[3 more]</label></div><br/><div class="children"><div class="content">In short: no.<p>The vector databases are here to store vectors and calculating distance between vectors.<p>The embeddings model is the model that you pick to generate these vectors from a string or an image.<p>You give &quot;bart simpson&quot; to an embeddings model and it becomes (43, -23, 2, 3, 4, 843, 34, 230, 324, 234, ...)<p>You can imagine it like geometric points in space (well, it&#x27;s a vector though), except that instead of being 2D, or 3D-space, they are typically in higher-number of dimensions (e.g: 768).<p>When you want to find similar entries, you just generate a new vector &quot;homer simpson&quot; (64, -13, 2, 3, 4, 843, 34, 230, 324, 234, ...) and send it to the vector database and it will return you all the nearest neighbors (= the existing entries with the smallest distance).<p>To generate these vectors, you can use any model that you want, however, you have to stay consistent.<p>It means that once you are using one embedding model, you are &quot;forever&quot; stuck with it, as there is no practical way to project from one vector space to another.</div><br/><div id="41600615" class="c"><input type="checkbox" id="c-41600615" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#41599341">root</a><span>|</span><a href="#41600475">parent</a><span>|</span><a href="#41604423">next</a><span>|</span><label class="collapse" for="c-41600615">[-]</label><label class="expand" for="c-41600615">[2 more]</label></div><br/><div class="children"><div class="content">that sucks :(. I wonder if there are other approaches to this, like simple word lookup, with storing a few synonyms, and prompting the LLM to always use the proper technical terms when performing a lookup.</div><br/><div id="41602621" class="c"><input type="checkbox" id="c-41602621" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#41599341">root</a><span>|</span><a href="#41600615">parent</a><span>|</span><a href="#41604423">next</a><span>|</span><label class="collapse" for="c-41602621">[-]</label><label class="expand" for="c-41602621">[1 more]</label></div><br/><div class="children"><div class="content">Back of the book index or inverted indexes can be stored in a set store and give decent results that compare to vector lookups. The issue with them is you have to do an extraction inference to get the keywords.</div><br/></div></div></div></div></div></div><div id="41604423" class="c"><input type="checkbox" id="c-41604423" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#41599341">root</a><span>|</span><a href="#41600358">parent</a><span>|</span><a href="#41600475">prev</a><span>|</span><a href="#41599929">next</a><span>|</span><label class="collapse" for="c-41604423">[-]</label><label class="expand" for="c-41604423">[1 more]</label></div><br/><div class="children"><div class="content">Embedding is a transformation which allows us to find semantically relevant chunks from a catalogue given a query. Through some nearness criteria, you would retrieve &quot;semantically relevant&quot; chunks which along with query would be fed to LLMs and ask them to synthesize the best answer. Vespa docs are very great if you are thinking of building in this space. Retrieval part is independent of synthesis, hence it has its separate leaderboard on huggingface.<p><a href="https:&#x2F;&#x2F;docs.vespa.ai&#x2F;en&#x2F;embedding.html" rel="nofollow">https:&#x2F;&#x2F;docs.vespa.ai&#x2F;en&#x2F;embedding.html</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard</a></div><br/></div></div></div></div></div></div><div id="41599929" class="c"><input type="checkbox" id="c-41599929" checked=""/><div class="controls bullet"><span class="by">_bramses</span><span>|</span><a href="#41599341">prev</a><span>|</span><a href="#41602883">next</a><span>|</span><label class="collapse" for="c-41599929">[-]</label><label class="expand" for="c-41599929">[1 more]</label></div><br/><div class="children"><div class="content">The technique I find most useful is to implement a “linked list” strategy where a chunk has multiple pointers to the entry it is referenced by. This task is done manually, but the diversity of the ways you can reference a particular node go up dramatically.<p>Another way to look at it, comments. Imagine every comment under this post is a pointer back to the original post. Some will be close in distance, and others will be farther, due to the perception of the authors of the comments themselves. But if you assign each comment a “parent_id”, your access to the post multiplies.<p>You can see an example of this technique here [1]. I don’t attempt to mind read what the end user will query for, I simply let them tell me, and then index that as a pointer. There are only a finite number of options to represent a given object. But some representations are very, very, very far from the semantic meaning of the core object.<p>[1] - <a href="https:&#x2F;&#x2F;x.com&#x2F;yourcommonbase&#x2F;status&#x2F;1833262865194557505" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;yourcommonbase&#x2F;status&#x2F;1833262865194557505</a></div><br/></div></div><div id="41602883" class="c"><input type="checkbox" id="c-41602883" checked=""/><div class="controls bullet"><span class="by">ValentinA23</span><span>|</span><a href="#41599929">prev</a><span>|</span><a href="#41601208">next</a><span>|</span><label class="collapse" for="c-41602883">[-]</label><label class="expand" for="c-41602883">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. One problem I&#x27;m facing is using RAG to retrieve applicable rules instead of knowledge (chunks): only rules that may apply to the context should be injected into the context. I haven&#x27;t done any experiment, but one approach that I think could work would be to train small classifiers to determine whether a specific rule <i>could</i> apply. The main LLM would be tasked with determining whether the rule indeed applies or not for the current context.<p>An example: let&#x27;s suppose you&#x27;re using an LLM to play a multi user dungeon. In the past your character has behaved badly with taxis so that the game has decided to create a rule that says that whenever you try to enter a taxi you&#x27;re kicked out: &quot;we know who you are, we refuse to have you as a client until you formally apologize to the taxi company director&quot;. Upon apologizing, the rule is removed. Note that the director of the taxi company could be another player and be the one who issued the rule in the first place, to be enforced by his NPC fleet of taxis.<p>I&#x27;m wondering how well this could scale (with respect of number of active rules) and to which extent traditional RAG could be applied. It seems deciding whether a rule applies or not is a problem that is more abstract and difficult than deciding whether a chunk of knowledge is relevant or not.<p>In particular the main problem I have identified that makes it more difficult is the following dependency loop that doesn&#x27;t appear with knowledge retrieval: you need to retrieve a rule to identify whether it applies or not. Does anyone know how this problem could be solved ?</div><br/></div></div><div id="41601208" class="c"><input type="checkbox" id="c-41601208" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#41602883">prev</a><span>|</span><a href="#41607058">next</a><span>|</span><label class="collapse" for="c-41601208">[-]</label><label class="expand" for="c-41601208">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If your knowledge base is smaller than 200,000 tokens (about 500 pages of material)<p>I would prefer that anthropic just release their tokeniser so we don&#x27;t have to make guesses.</div><br/><div id="41607957" class="c"><input type="checkbox" id="c-41607957" checked=""/><div class="controls bullet"><span class="by">whereismyacc</span><span>|</span><a href="#41601208">parent</a><span>|</span><a href="#41607058">next</a><span>|</span><label class="collapse" for="c-41607957">[-]</label><label class="expand" for="c-41607957">[1 more]</label></div><br/><div class="children"><div class="content">shouldn&#x27;t this be possible to reverse-engineer since they stream the responses token-by-token?</div><br/></div></div></div></div><div id="41607058" class="c"><input type="checkbox" id="c-41607058" checked=""/><div class="controls bullet"><span class="by">layoric</span><span>|</span><a href="#41601208">prev</a><span>|</span><a href="#41602629">next</a><span>|</span><label class="collapse" for="c-41607058">[-]</label><label class="expand" for="c-41607058">[1 more]</label></div><br/><div class="children"><div class="content">The statement about just throwing 200k tokens to get best answer for smaller datasets goes against my experience. I commonly find as my prompt gets larger, the less consistent the output becomes, and the poorer following instructions becomes. Does anyone else experience this or a well known way to avoid this? It seems to happen at much less than even 25k tokens.</div><br/></div></div><div id="41602629" class="c"><input type="checkbox" id="c-41602629" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#41607058">prev</a><span>|</span><a href="#41603374">next</a><span>|</span><label class="collapse" for="c-41602629">[-]</label><label class="expand" for="c-41602629">[2 more]</label></div><br/><div class="children"><div class="content">Waiting for the day when the entire AI industry goes back full circle to TF-IDF.</div><br/><div id="41602977" class="c"><input type="checkbox" id="c-41602977" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41602629">parent</a><span>|</span><a href="#41603374">next</a><span>|</span><label class="collapse" for="c-41602977">[-]</label><label class="expand" for="c-41602977">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it did make me chuckle. I’m guessing products like elasticsearch support all the classic text matching algos out of the box anyway?</div><br/></div></div></div></div><div id="41603374" class="c"><input type="checkbox" id="c-41603374" checked=""/><div class="controls bullet"><span class="by">will-burner</span><span>|</span><a href="#41602629">prev</a><span>|</span><a href="#41602956">next</a><span>|</span><label class="collapse" for="c-41603374">[-]</label><label class="expand" for="c-41603374">[1 more]</label></div><br/><div class="children"><div class="content">I wish they included the datasets they used for the evaluations. As far as I can tell, in appendix II they include some sample questions, answers, and golden chunks but they do not give the entire dataset or give an explicit information on exactly what the datasets are.<p>Does anyone know if the datasets they used for the evaluation are publicly available or if they give more information on the datasets than what&#x27;s in appendix II?<p>There are standard publically available datasets for this type of evaluation, like MTEB (<a href="https:&#x2F;&#x2F;github.com&#x2F;embeddings-benchmark&#x2F;mteb">https:&#x2F;&#x2F;github.com&#x2F;embeddings-benchmark&#x2F;mteb</a>). I wonder how this technique does on the MTEB dataset.</div><br/></div></div><div id="41602956" class="c"><input type="checkbox" id="c-41602956" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41603374">prev</a><span>|</span><a href="#41598787">next</a><span>|</span><label class="collapse" for="c-41602956">[-]</label><label class="expand" for="c-41602956">[1 more]</label></div><br/><div class="children"><div class="content">Even with prompt caching this adds a huge extra time to your vector database create&#x2F;update, right? That may be okay for some use cases but I’m always wary of adding multiple LLM layers into these kinds of applications. It’s nice for the cloud LLM providers of course.<p>I wonder how it would work if you generated the contexts yourself algorithmically. Depending on how well structured your docs are this could be quite trivial (eg for an html doc insert the title &gt; h1 &gt; h2 &gt; chunk).</div><br/></div></div><div id="41598787" class="c"><input type="checkbox" id="c-41598787" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41602956">prev</a><span>|</span><a href="#41601173">next</a><span>|</span><label class="collapse" for="c-41598787">[-]</label><label class="expand" for="c-41598787">[3 more]</label></div><br/><div class="children"><div class="content">This sounds a lot like how we used to do research, by reading books and writing any interesting quotes on index cards, along with where they came from. I wonder if prompting for that would result in better chunks? It might make it easier to review if you wanted to do it manually.</div><br/><div id="41600182" class="c"><input type="checkbox" id="c-41600182" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41598787">parent</a><span>|</span><a href="#41601173">next</a><span>|</span><label class="collapse" for="c-41600182">[-]</label><label class="expand" for="c-41600182">[2 more]</label></div><br/><div class="children"><div class="content">The fundamental problem of both keyword and embedding based retrieval is that they only access surface level features. If your document contains 5+5 and you search &quot;where is the result 10&quot; you won&#x27;t find the answer. That is why all texts need to be &quot;digested&quot; with LLM before indexing, to draw out implicit information and make it explicit. It&#x27;s also what Anthropic proposes we do to improve RAG.<p>&quot;study your data before indexing it&quot;</div><br/><div id="41604255" class="c"><input type="checkbox" id="c-41604255" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41598787">root</a><span>|</span><a href="#41600182">parent</a><span>|</span><a href="#41601173">next</a><span>|</span><label class="collapse" for="c-41604255">[-]</label><label class="expand" for="c-41604255">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense. It seems after retrieval, both would be useful - both the exact quote and a summary of its context.</div><br/></div></div></div></div></div></div><div id="41601173" class="c"><input type="checkbox" id="c-41601173" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#41598787">prev</a><span>|</span><a href="#41603480">next</a><span>|</span><label class="collapse" for="c-41601173">[-]</label><label class="expand" for="c-41601173">[1 more]</label></div><br/><div class="children"><div class="content">I just took the time to read through all source code and docs. Nice ideas. I like to experiment with LLMs running on my local computer so I will probably convert this example to use the light weight Python library Rank-BM25 instead of Elastic Search, and a long context model running on Ollama. I wouldn’t have prompt caching though.<p>This example is well written and documented, easy to understand. Well done.</div><br/></div></div><div id="41603480" class="c"><input type="checkbox" id="c-41603480" checked=""/><div class="controls bullet"><span class="by">justanotheratom</span><span>|</span><a href="#41601173">prev</a><span>|</span><a href="#41600204">next</a><span>|</span><label class="collapse" for="c-41603480">[-]</label><label class="expand" for="c-41603480">[1 more]</label></div><br/><div class="children"><div class="content">Looking forward to some guidance on &quot;chunking&quot;:<p>&quot;Chunk boundaries: Consider how you split your documents into chunks. The choice of chunk size, chunk boundary, and chunk overlap can affect retrieval performance1.&quot;</div><br/></div></div><div id="41600204" class="c"><input type="checkbox" id="c-41600204" checked=""/><div class="controls bullet"><span class="by">vendiddy</span><span>|</span><a href="#41603480">prev</a><span>|</span><a href="#41600803">next</a><span>|</span><label class="collapse" for="c-41600204">[-]</label><label class="expand" for="c-41600204">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know anything about AI but I&#x27;ve always wished I could just upload a bunch of documents&#x2F;books and the AI would perform some basic keyword searches to figure out what is relevant, then auto include that in the prompt.</div><br/><div id="41607962" class="c"><input type="checkbox" id="c-41607962" checked=""/><div class="controls bullet"><span class="by">whereismyacc</span><span>|</span><a href="#41600204">parent</a><span>|</span><a href="#41600285">next</a><span>|</span><label class="collapse" for="c-41607962">[-]</label><label class="expand" for="c-41607962">[1 more]</label></div><br/><div class="children"><div class="content">i&#x27;ve noticed that copilot seems to do this pretty well, i ask about a function and it correctly looks up the relevant lines of code</div><br/></div></div><div id="41600285" class="c"><input type="checkbox" id="c-41600285" checked=""/><div class="controls bullet"><span class="by">average_r_user</span><span>|</span><a href="#41600204">parent</a><span>|</span><a href="#41607962">prev</a><span>|</span><a href="#41600803">next</a><span>|</span><label class="collapse" for="c-41600285">[-]</label><label class="expand" for="c-41600285">[2 more]</label></div><br/><div class="children"><div class="content">It would help if you tried Notebooklm by Google. It does this, you can upload a document&#x2F;PDF whatever, and ask questions. The model replies to you giving also a reference to your material</div><br/><div id="41601323" class="c"><input type="checkbox" id="c-41601323" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#41600204">root</a><span>|</span><a href="#41600285">parent</a><span>|</span><a href="#41600803">next</a><span>|</span><label class="collapse" for="c-41601323">[-]</label><label class="expand" for="c-41601323">[1 more]</label></div><br/><div class="children"><div class="content">+1 Google’s NotebookLM is amazing. In addition to the functionality you mention, I tried loading the PDF for my entire Practical AI Programming with Clojure book and had it generate an 8 minute podcast that was very nuanced - to be honest, it seriously blew my mind how well it works. Here is a link to the audio file it automatically generated <a href="https:&#x2F;&#x2F;markwatson.com&#x2F;audio&#x2F;AIClojureBook.wav" rel="nofollow">https:&#x2F;&#x2F;markwatson.com&#x2F;audio&#x2F;AIClojureBook.wav</a><p>NotebookLM is currently free to use and was so good I almost immediately started paying Google $20 a month to get access to their pro version of Gemini.<p>I still think the Groq APIs for open weight models are the best value for the money, but the way OpenAI, Google, Anthropic, etc. are productizing LLMs is very impressive.</div><br/></div></div></div></div></div></div><div id="41600803" class="c"><input type="checkbox" id="c-41600803" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#41600204">prev</a><span>|</span><a href="#41599750">next</a><span>|</span><label class="collapse" for="c-41600803">[-]</label><label class="expand" for="c-41600803">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been wondering for a while if having ElasticSearch as just another function to call might be interesting.  If the LLM can just generate queries it&#x27;s an easy deployment.</div><br/></div></div><div id="41599750" class="c"><input type="checkbox" id="c-41599750" checked=""/><div class="controls bullet"><span class="by">timwaagh</span><span>|</span><a href="#41600803">prev</a><span>|</span><a href="#41600795">next</a><span>|</span><label class="collapse" for="c-41599750">[-]</label><label class="expand" for="c-41599750">[1 more]</label></div><br/><div class="children"><div class="content">I guess this does give some insights. Using a more space efficient language for your codebase will mean more functionality in the ais context window when working with Claude and code.</div><br/></div></div><div id="41600795" class="c"><input type="checkbox" id="c-41600795" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#41599750">prev</a><span>|</span><label class="collapse" for="c-41600795">[-]</label><label class="expand" for="c-41600795">[2 more]</label></div><br/><div class="children"><div class="content">Can someone explain simply how these benchmarks work?<p>What exactly is a &quot;failure rate&quot; and how is it computed?</div><br/><div id="41603728" class="c"><input type="checkbox" id="c-41603728" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41600795">parent</a><span>|</span><label class="collapse" for="c-41603728">[-]</label><label class="expand" for="c-41603728">[1 more]</label></div><br/><div class="children"><div class="content">They simply ask the AI a question about a large document (or set of docs). It either gets the answer right or wrong. They count the number of hits and misses.</div><br/></div></div></div></div></div></div></div></div></div></body></html>