<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725699671405" as="style"/><link rel="stylesheet" href="styles.css?v=1725699671405"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://tomhazledine.com/cosine-similarity/">How does cosine similarity work?</a> <span class="domain">(<a href="https://tomhazledine.com">tomhazledine.com</a>)</span></div><div class="subtext"><span>tomhazledine</span> | <span>96 comments</span></div><br/><div><div id="41470593" class="c"><input type="checkbox" id="c-41470593" checked=""/><div class="controls bullet"><span class="by">GuB-42</span><span>|</span><a href="#41469612">next</a><span>|</span><label class="collapse" for="c-41470593">[-]</label><label class="expand" for="c-41470593">[26 more]</label></div><br/><div class="children"><div class="content">I think the use of the term &quot;cosine&quot; here is needlessly confusing. It is the dot product of normalized vectors. Sure, when you do the maths, it gives out a cosine, but since we are not doing geometry here, so it isn&#x27;t really helpful for a beginner to know that. Especially considering that these vectors have many dimensions and anything above 3D is super confusing when you think about it geometrically.<p>Instead just try to think about what it is: the sum of term-by-term products of normalized vectors. A product is the soft version of a logic AND, and it makes intuitive sense that vectors A and B are similar if there are a lot of traits that are present in both A <i>AND</i> B (represented by the sum) relative to the total number of traits that A and B have (that&#x27;s the normalization process).<p>Forget about angles and geometry unless you are comfortable with N-dimensional space with N&gt;&gt;3. Most people aren&#x27;t.</div><br/><div id="41470715" class="c"><input type="checkbox" id="c-41470715" checked=""/><div class="controls bullet"><span class="by">adw</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41470912">next</a><span>|</span><label class="collapse" for="c-41470715">[-]</label><label class="expand" for="c-41470715">[8 more]</label></div><br/><div class="children"><div class="content">&gt; we are not doing geometry here<p>we absolutely <i>are</i> doing geometry here, given we&#x27;re talking about metrics in a vector space – and this is trigonometry you learned by the first year of high school.</div><br/><div id="41472119" class="c"><input type="checkbox" id="c-41472119" checked=""/><div class="controls bullet"><span class="by">yzydserd</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41470715">parent</a><span>|</span><a href="#41471339">next</a><span>|</span><label class="collapse" for="c-41472119">[-]</label><label class="expand" for="c-41472119">[1 more]</label></div><br/><div class="children"><div class="content">Where I live, where many people live, we enter high school aged 11. We haven’t  been introduced at school to geometry yet.<p>I suspect you’re using American terminology. When talking about school years it’s often useful to talk about the year or grade of school, like “9th grade” or “year 9” as it’s more universal.</div><br/></div></div><div id="41471339" class="c"><input type="checkbox" id="c-41471339" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41470715">parent</a><span>|</span><a href="#41472119">prev</a><span>|</span><a href="#41470912">next</a><span>|</span><label class="collapse" for="c-41471339">[-]</label><label class="expand" for="c-41471339">[6 more]</label></div><br/><div class="children"><div class="content">You might like to think of vectors in their geometric interpretation but vectors are not inherently geometric - vectors are just lists of numbers, which we sometimes interpret geometrically because it helps us comprehend them. High dimensional vectors grow increasingly ungeometric as we have to wrestle with increasingly implausible numbers of orthogonal spatial dimensions in order to render them ‘geometric’.<p>In the end, vectors (long lists of numbers <i>a</i>1, <i>a</i>2, <i>a</i>3, … <i>a</i>n) start looking more like discrete functions <i>f</i>(<i>i</i>) = <i>a</i>i. And you can extend the same concept all the way to continuous functions - they’re like infinite dimensional vectors. For continuous functions over a finite interval the dot product (usually called the inner product in this domain) is just the integral of the product of two functions, and the ‘magnitude’ of a function is its RMS, and that means <i>functions</i> have a ‘cosine similarity’ which is not remotely geometric. There isn’t any geometric sense in which there is an ‘angle between’ cos(x) and sin(x) except it turns out that they have a cosine similarity of 0 so it implies the ‘angle between’ them is 90°, which actually makes a lot of sense. But in this same sense there’s an ‘angle between’ <i>any</i> two functions (over an interval).<p>But we are not doing geometry here.</div><br/><div id="41472572" class="c"><input type="checkbox" id="c-41472572" checked=""/><div class="controls bullet"><span class="by">klodolph</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471339">parent</a><span>|</span><a href="#41471453">next</a><span>|</span><label class="collapse" for="c-41472572">[-]</label><label class="expand" for="c-41472572">[1 more]</label></div><br/><div class="children"><div class="content">&gt; that means functions have a ‘cosine similarity’ which is not remotely geometric.<p>It obeys the normal rules you learned in geometry. For example, pick three functions a,b,c. The functions form a triangle. The triangle obeys the triangle inequality—the distances satisfy d(a,b) ≤ d(a,c) + d(c,b). The angles of the triangle sum to 180°.<p>This sounds an awful lot like geometry to me.</div><br/></div></div><div id="41471453" class="c"><input type="checkbox" id="c-41471453" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471339">parent</a><span>|</span><a href="#41472572">prev</a><span>|</span><a href="#41472052">next</a><span>|</span><label class="collapse" for="c-41471453">[-]</label><label class="expand" for="c-41471453">[2 more]</label></div><br/><div class="children"><div class="content">&gt; increasingly implausible numbers of orthogonal spatial dimensions in order to render them ‘geometric’.<p>Implausible how? “geometric” doesn’t mean “embeds nicely in 3D space”.<p>What’s wrong with talking about the angle between two L^2 functions defined on an interval?
Geometric reasoning still works? If you take a span of two functions, you have a plane. What’s the issue?</div><br/><div id="41471982" class="c"><input type="checkbox" id="c-41471982" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471453">parent</a><span>|</span><a href="#41472052">next</a><span>|</span><label class="collapse" for="c-41471982">[-]</label><label class="expand" for="c-41471982">[1 more]</label></div><br/><div class="children"><div class="content">In this case can people just prepend &quot;hyper-&quot; as in hyperplane etc? Hyper-line, hyper-angle. (Speaking as someone who has heard &#x27;hyperplane&#x27; a few times but not others)</div><br/></div></div></div></div><div id="41472052" class="c"><input type="checkbox" id="c-41472052" checked=""/><div class="controls bullet"><span class="by">bowsamic</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471339">parent</a><span>|</span><a href="#41471453">prev</a><span>|</span><a href="#41470912">next</a><span>|</span><label class="collapse" for="c-41472052">[-]</label><label class="expand" for="c-41472052">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You might like to think of vectors in their geometric interpretation but vectors are not inherently geometric - vectors are just lists of numbers<p>No. They can be expressed as lists of numbers in a basis if the vector space is equipped with a scalar product but the vector itself is an object that transcends the specific numbers it is expressed in.<p>What you’re saying here is totally wrong and I recommend you check out the Wikipedia page on vector spaces. The geometrical object “a vector” is the more fundamental thing than the list of numbers</div><br/><div id="41472428" class="c"><input type="checkbox" id="c-41472428" checked=""/><div class="controls bullet"><span class="by">xigoi</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41472052">parent</a><span>|</span><a href="#41470912">next</a><span>|</span><label class="collapse" for="c-41472428">[-]</label><label class="expand" for="c-41472428">[1 more]</label></div><br/><div class="children"><div class="content">Tuples of numbers are a special case of a vector space, which even comes with a canonical basis and inner product for free. And since the article is about word embeddings, which map words to tuples of numbers, there’s no need to mention other vector spaces in this context.</div><br/></div></div></div></div></div></div></div></div><div id="41470912" class="c"><input type="checkbox" id="c-41470912" checked=""/><div class="controls bullet"><span class="by">steve_adams_86</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41470715">prev</a><span>|</span><a href="#41471232">next</a><span>|</span><label class="collapse" for="c-41470912">[-]</label><label class="expand" for="c-41470912">[5 more]</label></div><br/><div class="children"><div class="content">Interesting, I think it’s actually far more intuitive to think of it geometrically. I’m not sure what my brain is doing in order for this mental projection to help, but this is exactly how I made dot products “click” for me. I started to think of them in multidimensional space, almost physically (though in a very limited sense since my brain came from a monkey and generally fires on a couple cylinders).</div><br/><div id="41472454" class="c"><input type="checkbox" id="c-41472454" checked=""/><div class="controls bullet"><span class="by">bgnn</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41470912">parent</a><span>|</span><a href="#41471738">next</a><span>|</span><label class="collapse" for="c-41472454">[-]</label><label class="expand" for="c-41472454">[1 more]</label></div><br/><div class="children"><div class="content">One cannot intuitively think about higher than 3 dimensions. Even for most their intuition is often wrong in 3D space. It&#x27;s quite accurate for 1D and 2D.<p>Richard Hamming has a whole section lecture to make everyone realize precisely this [1]. This was an eye opener to me.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=uU_Q2a0S0zI" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=uU_Q2a0S0zI</a></div><br/></div></div><div id="41471738" class="c"><input type="checkbox" id="c-41471738" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41470912">parent</a><span>|</span><a href="#41472454">prev</a><span>|</span><a href="#41471232">next</a><span>|</span><label class="collapse" for="c-41471738">[-]</label><label class="expand" for="c-41471738">[3 more]</label></div><br/><div class="children"><div class="content">I expect it’s like how learning to play by ear is more intuitive than sheet music. That’s great if you’re an amateur. If you’re dealing with tensors or somesuch trying to design a fusion reactor that’s probably a crutch.</div><br/><div id="41471882" class="c"><input type="checkbox" id="c-41471882" checked=""/><div class="controls bullet"><span class="by">gnulinux</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471738">parent</a><span>|</span><a href="#41471232">next</a><span>|</span><label class="collapse" for="c-41471882">[-]</label><label class="expand" for="c-41471882">[2 more]</label></div><br/><div class="children"><div class="content">This is a very odd statement and depicts the different ways human brain works. As a musician, I find playing (or thinking music in terms of) sheet music so much more intuitive than play by ear. It feels like the very reason people notate, write music is because anything written down is easier to think&#x2F;play than anything listened.</div><br/><div id="41472110" class="c"><input type="checkbox" id="c-41472110" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471882">parent</a><span>|</span><a href="#41471232">next</a><span>|</span><label class="collapse" for="c-41472110">[-]</label><label class="expand" for="c-41472110">[1 more]</label></div><br/><div class="children"><div class="content">I can sing along to <i>songs I never liked</i> that haven’t been on the radio for twenty years.<p>So I tend to sympathize with the by ear folks.</div><br/></div></div></div></div></div></div></div></div><div id="41471232" class="c"><input type="checkbox" id="c-41471232" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41470912">prev</a><span>|</span><a href="#41471569">next</a><span>|</span><label class="collapse" for="c-41471232">[-]</label><label class="expand" for="c-41471232">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but since we are not doing geometry here, so it isn&#x27;t really helpful for a beginner to know that.<p>This article isn&#x27;t talking geometry, it&#x27;s trigonometry.  And half the article is visual anyway.</div><br/></div></div><div id="41471569" class="c"><input type="checkbox" id="c-41471569" checked=""/><div class="controls bullet"><span class="by">dilawar</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41471232">prev</a><span>|</span><a href="#41472154">next</a><span>|</span><label class="collapse" for="c-41471569">[-]</label><label class="expand" for="c-41471569">[1 more]</label></div><br/><div class="children"><div class="content">Hmmm.. I heard in a conference that most well understood engineering principles or theories have a neat geometric interpretation. Personally I find a theory with geometric interpretation far easier to grasp. On the other hand, the higher dimensions geometry confuses me a lot: most random sparse vectors are orthogonal to each other, and most volumes of a sphere in that dimension are concentrated in a place.</div><br/></div></div><div id="41472154" class="c"><input type="checkbox" id="c-41472154" checked=""/><div class="controls bullet"><span class="by">Jaxan</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41471569">prev</a><span>|</span><a href="#41471810">next</a><span>|</span><label class="collapse" for="c-41472154">[-]</label><label class="expand" for="c-41472154">[1 more]</label></div><br/><div class="children"><div class="content">I agree on the first point. But I find the dot product much more geometrical than the cosine. So in my mind your argument is in favour of geometry!</div><br/></div></div><div id="41471810" class="c"><input type="checkbox" id="c-41471810" checked=""/><div class="controls bullet"><span class="by">danieldk</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41472154">prev</a><span>|</span><a href="#41470950">next</a><span>|</span><label class="collapse" for="c-41471810">[-]</label><label class="expand" for="c-41471810">[1 more]</label></div><br/><div class="children"><div class="content">It’s quite interesting that we end up using cosine similarity. Most networks are trained with a softmax layer at the end (e.g. next word prediction). Given the close relation between softmax and logistic regression, it might make more sense to use <i>σ(u.v)</i> as the similarity function.</div><br/></div></div><div id="41470950" class="c"><input type="checkbox" id="c-41470950" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41471810">prev</a><span>|</span><a href="#41470752">next</a><span>|</span><label class="collapse" for="c-41470950">[-]</label><label class="expand" for="c-41470950">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Forget about angles and geometry unless you are comfortable with N-dimensional space with N&gt;&gt;3. Most people aren&#x27;t.<p>The whole point of measuring similarity this way is that any two vectors exist in a two-dimensional space, which is where you measure the angle between them. Why would you need to be comfortable with high-dimensional spaces?</div><br/><div id="41471458" class="c"><input type="checkbox" id="c-41471458" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41470950">parent</a><span>|</span><a href="#41470999">next</a><span>|</span><label class="collapse" for="c-41471458">[-]</label><label class="expand" for="c-41471458">[2 more]</label></div><br/><div class="children"><div class="content">for one reason, if you&#x27;re just thinking about it as fancy 2d, you will miss a lot of phenomena that occur in higher dimensional spaces. for example, almost all vectors are almost completely orthogonal which isn&#x27;t true at all in low dimensional spaces</div><br/><div id="41471991" class="c"><input type="checkbox" id="c-41471991" checked=""/><div class="controls bullet"><span class="by">kaffekaka</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471458">parent</a><span>|</span><a href="#41470999">next</a><span>|</span><label class="collapse" for="c-41471991">[-]</label><label class="expand" for="c-41471991">[1 more]</label></div><br/><div class="children"><div class="content">Phrased like that it sounds like a qualitative difference between &quot;low&quot; and &quot;high&quot; dimensional spaces. But isn&#x27;t it simply a consequence of the fact that the more dimensions you have, the less likely that randomly distributed, sparse non-zeros will end up in the same positions?<p>I.e. simply a quantitive difference.</div><br/></div></div></div></div><div id="41470999" class="c"><input type="checkbox" id="c-41470999" checked=""/><div class="controls bullet"><span class="by">RichardLake</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41470950">parent</a><span>|</span><a href="#41471458">prev</a><span>|</span><a href="#41470752">next</a><span>|</span><label class="collapse" for="c-41470999">[-]</label><label class="expand" for="c-41470999">[4 more]</label></div><br/><div class="children"><div class="content">By `two vectors exist in a two-dimensional space` are you talking about how two (linearly independent) n dimensional vectors will span a 2d space?</div><br/><div id="41471012" class="c"><input type="checkbox" id="c-41471012" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41470999">parent</a><span>|</span><a href="#41470752">next</a><span>|</span><label class="collapse" for="c-41471012">[-]</label><label class="expand" for="c-41471012">[3 more]</label></div><br/><div class="children"><div class="content">No, I&#x27;m talking about the fact that the space spanned by two vectors is sufficient to contain those vectors. All of the analysis you could ever theoretically want to do on them can be done within that space. If you only have two vectors, you never need to consider a space with higher dimensionality than 2. Each vector is a dimension of the space, and that&#x27;s it.</div><br/><div id="41471394" class="c"><input type="checkbox" id="c-41471394" checked=""/><div class="controls bullet"><span class="by">Chinjut</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471012">parent</a><span>|</span><a href="#41470752">next</a><span>|</span><label class="collapse" for="c-41471394">[-]</label><label class="expand" for="c-41471394">[2 more]</label></div><br/><div class="children"><div class="content">That is the same thing as what is being said in the comment you are replying &quot;No&quot; to.</div><br/><div id="41472040" class="c"><input type="checkbox" id="c-41472040" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41470593">root</a><span>|</span><a href="#41471394">parent</a><span>|</span><a href="#41470752">next</a><span>|</span><label class="collapse" for="c-41472040">[-]</label><label class="expand" for="c-41472040">[1 more]</label></div><br/><div class="children"><div class="content">No, these are not at all the same claim:<p>(A) Look at this space. Every point within it can be reached by combining these two vectors.<p>(B) Look at this space. <i>No point outside it</i> can be reached by combining these two vectors.<p>Saying that two vectors span a space is claim (A). Saying that the space they span contains them is... much weaker than claim (B), but it&#x27;s related to claim (B) and not to claim (A).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41470752" class="c"><input type="checkbox" id="c-41470752" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#41470593">parent</a><span>|</span><a href="#41470950">prev</a><span>|</span><a href="#41469612">next</a><span>|</span><label class="collapse" for="c-41470752">[-]</label><label class="expand" for="c-41470752">[1 more]</label></div><br/><div class="children"><div class="content">I bet it&#x27;s whether your primary background is programming or mathematics. From the latter, the cosine is very natural (scalar projection etc.) and it&#x27;s lots of steps to get to your thing. I&#x27;d say this was intuitive for us post high-school because of that pedagogical background.</div><br/></div></div></div></div><div id="41469612" class="c"><input type="checkbox" id="c-41469612" checked=""/><div class="controls bullet"><span class="by">naijaboiler</span><span>|</span><a href="#41470593">prev</a><span>|</span><a href="#41472285">next</a><span>|</span><label class="collapse" for="c-41469612">[-]</label><label class="expand" for="c-41469612">[9 more]</label></div><br/><div class="children"><div class="content">Imagine 2 points in 3 dimensional space with a vector being the line from the origin to the point. So you have 2 vectors pointing going to the 2 points from the origin.<p>If those points are really close together, then angle between the two vector lines is very small. Loosely speaking cosine is a way to quantize how close two lines with a shared origin is. If both lines are the same, the angle between them is 0, and the cosine of 0 is 1. If two lines are 90 degrees apart, their cosine is 0. If two lines are 180 degrees apart, their cosine is -1. 
So cosine is a way to quantify the closeness of two lines which share to same origin<p>To go back with 2 points in space that we started with, we can measure how close those 2 points are by taking the cosine of the lines going from origin to the two points. If they are close, the angle between them is small. If they are the exact same point, the angle between the lines is 0. That line is called a vector<p>Cosine similarity measures how closes two vectors are in Euclidean space. That’s we end up using it a lot. It’s no the only way to measure closeness. There are many others</div><br/><div id="41469945" class="c"><input type="checkbox" id="c-41469945" checked=""/><div class="controls bullet"><span class="by">stouset</span><span>|</span><a href="#41469612">parent</a><span>|</span><a href="#41470300">next</a><span>|</span><label class="collapse" for="c-41469945">[-]</label><label class="expand" for="c-41469945">[7 more]</label></div><br/><div class="children"><div class="content">Are all the points in question one unit distant from the origin?</div><br/><div id="41470702" class="c"><input type="checkbox" id="c-41470702" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41469612">root</a><span>|</span><a href="#41469945">parent</a><span>|</span><a href="#41471123">next</a><span>|</span><label class="collapse" for="c-41470702">[-]</label><label class="expand" for="c-41470702">[1 more]</label></div><br/><div class="children"><div class="content">I vaguely remember some paper where they didn’t even bother normalizing the vectors, because they expected zeros to be very close to zero, and anything else was considered a one.<p>I have no idea if this a common optimization or if it was something very niche. It was for a heuristic matrix reordering strategy, so I think they were willing to accept some mistakes.</div><br/></div></div><div id="41471123" class="c"><input type="checkbox" id="c-41471123" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41469612">root</a><span>|</span><a href="#41469945">parent</a><span>|</span><a href="#41470702">prev</a><span>|</span><a href="#41470220">next</a><span>|</span><label class="collapse" for="c-41471123">[-]</label><label class="expand" for="c-41471123">[3 more]</label></div><br/><div class="children"><div class="content">In cosine similarity, yes..  iirc there is a recent paper arguing that this causes cosine similarity to perform poorly in high dimensional (aka ML) vector spaces.</div><br/><div id="41471479" class="c"><input type="checkbox" id="c-41471479" checked=""/><div class="controls bullet"><span class="by">lainga</span><span>|</span><a href="#41469612">root</a><span>|</span><a href="#41471123">parent</a><span>|</span><a href="#41470220">next</a><span>|</span><label class="collapse" for="c-41471479">[-]</label><label class="expand" for="c-41471479">[2 more]</label></div><br/><div class="children"><div class="content">...How recent? Bellman coined <i>curse of dimensionality</i> in 1957.</div><br/><div id="41471983" class="c"><input type="checkbox" id="c-41471983" checked=""/><div class="controls bullet"><span class="by">zzleeper</span><span>|</span><a href="#41469612">root</a><span>|</span><a href="#41471479">parent</a><span>|</span><a href="#41470220">next</a><span>|</span><label class="collapse" for="c-41471983">[-]</label><label class="expand" for="c-41471983">[1 more]</label></div><br/><div class="children"><div class="content">I think he is talking about the 2024 arxiv paper by some netflix (?) researchers that say that it&#x27;s best not to normalize the dot products (so instead of cosine similarity you just have a dot product).<p>For most commercial embeddings (openai etc) this is not a problem as the embeddings are already normalized</div><br/></div></div></div></div></div></div><div id="41470220" class="c"><input type="checkbox" id="c-41470220" checked=""/><div class="controls bullet"><span class="by">viciousvoxel</span><span>|</span><a href="#41469612">root</a><span>|</span><a href="#41469945">parent</a><span>|</span><a href="#41471123">prev</a><span>|</span><a href="#41470122">next</a><span>|</span><label class="collapse" for="c-41470220">[-]</label><label class="expand" for="c-41470220">[1 more]</label></div><br/><div class="children"><div class="content">yes, cosine similarity involves normalizing the points (by the L2 norm) and then dot product. In other words the points lie on the unit (hyper)sphere.</div><br/></div></div></div></div><div id="41470300" class="c"><input type="checkbox" id="c-41470300" checked=""/><div class="controls bullet"><span class="by">jFatFinger</span><span>|</span><a href="#41469612">parent</a><span>|</span><a href="#41469945">prev</a><span>|</span><a href="#41472285">next</a><span>|</span><label class="collapse" for="c-41470300">[-]</label><label class="expand" for="c-41470300">[1 more]</label></div><br/><div class="children"><div class="content">Good explanation, can you also explain how a sentence ends up as a point next to another point where the sentences has similar meaning. What does it mean for two sentences to be similar?</div><br/></div></div></div></div><div id="41472285" class="c"><input type="checkbox" id="c-41472285" checked=""/><div class="controls bullet"><span class="by">niemandhier</span><span>|</span><a href="#41469612">prev</a><span>|</span><a href="#41468910">next</a><span>|</span><label class="collapse" for="c-41472285">[-]</label><label class="expand" for="c-41472285">[1 more]</label></div><br/><div class="children"><div class="content">In high dimensional Spaces the distances between nearest and farthest points from query points with respect to normal metrics become almost equal.<p>Cosine similarity still works though, since it only look at how aligned vectors are.<p>The thing that people tend to overlook is, that there is no need for embeddings to be a vector space endowed with an inner product.<p>Words don’t have this structure, we define it on the image of the mapping from words to n-tuples and the embeddings we use coevolved in such a way that we assume the cosine similarity to be meaningful.</div><br/></div></div><div id="41468910" class="c"><input type="checkbox" id="c-41468910" checked=""/><div class="controls bullet"><span class="by">dbfclark</span><span>|</span><a href="#41472285">prev</a><span>|</span><a href="#41469983">next</a><span>|</span><label class="collapse" for="c-41468910">[-]</label><label class="expand" for="c-41468910">[2 more]</label></div><br/><div class="children"><div class="content">A good way to understand why cosine similarity is so common in NLP is to think in terms of a keyword search. A bag-of-words vector represents a document as a sparse vector of its word counts; counting the number of occurrences of some set of query words is the dot product of the query vector with the document vector; normalizing for length gives you cosine similarity. If you have word embedding vectors instead of discrete words, you can think of the same game, just now the “count” of a word with another word is the similarity of the word embeddings instead of a 0&#x2F;1. Finally, LLMs give sentence embeddings as weighted sums of contextual word vectors, so it’s all just fuzzy word counting again.</div><br/></div></div><div id="41469983" class="c"><input type="checkbox" id="c-41469983" checked=""/><div class="controls bullet"><span class="by">cproctor</span><span>|</span><a href="#41468910">prev</a><span>|</span><a href="#41471809">next</a><span>|</span><label class="collapse" for="c-41469983">[-]</label><label class="expand" for="c-41469983">[25 more]</label></div><br/><div class="children"><div class="content">One thing I&#x27;ve wondered for a while: Is there a principled reason (e.g. explainable in terms of embedding training) why a vector&#x27;s magnitude can be ignored within a pretrained embedding, such that cosine similarity is a good measure of semantic distance? Or is it just a computationally-inexpensive trick that works well in practice?<p>For example, if I have a set of words and I want to consider their relative location on an axis between two anchor words (e.g. &quot;good&quot; and &quot;evil&quot;), it makes sense to me to project all the words onto the vector from &quot;good&quot; to &quot;evil.&quot; Would comparing each word&#x27;s &quot;good&quot; and &quot;evil&quot; cosine similarity be equivalent, or even preferable? (I know there are questions about the interpretability of this kind of geometry.)</div><br/><div id="41470177" class="c"><input type="checkbox" id="c-41470177" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#41469983">parent</a><span>|</span><a href="#41470109">next</a><span>|</span><label class="collapse" for="c-41470177">[-]</label><label class="expand" for="c-41470177">[20 more]</label></div><br/><div class="children"><div class="content">Some embedding models are explicitly trained on cosine similarity. Otherwise, if you have a 512D vector, discarding magnitude is like discarding just a single dimension (i.e. you get 511 independent dimensions).</div><br/><div id="41470240" class="c"><input type="checkbox" id="c-41470240" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470177">parent</a><span>|</span><a href="#41470109">next</a><span>|</span><label class="collapse" for="c-41470240">[-]</label><label class="expand" for="c-41470240">[19 more]</label></div><br/><div class="children"><div class="content">This is not quite right; you are actually losing information about each of the dimensions and your mental model of reducing the dimensionality by one is misleading.<p>Consider [1,0] and [x,x]
Normalised we get [1,0] and [sqrt(.5),sqrt(.5)] — clearly something has changed because the first vector is now larger in dimension zero than the second, despite starting off as an arbitrary value, x, which could have been smaller than 1. As such we have lost information about x’s magnitude which we cannot recover from just the normalized vector.</div><br/><div id="41470392" class="c"><input type="checkbox" id="c-41470392" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470240">parent</a><span>|</span><a href="#41470816">prev</a><span>|</span><a href="#41470391">next</a><span>|</span><label class="collapse" for="c-41470392">[-]</label><label class="expand" for="c-41470392">[3 more]</label></div><br/><div class="children"><div class="content">Well, depends. For some models (especially two tower style models that use a dot product), you&#x27;re definitely right and it makes a huge difference. In my very limited experience with LLM embeddings, it doesn&#x27;t seem to make a difference.</div><br/><div id="41470503" class="c"><input type="checkbox" id="c-41470503" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470392">parent</a><span>|</span><a href="#41470391">next</a><span>|</span><label class="collapse" for="c-41470503">[-]</label><label class="expand" for="c-41470503">[2 more]</label></div><br/><div class="children"><div class="content">Interesting, I hadn’t heard of two tower modes before!<p>Yes, I guess it’s curious that the information lost doesn’t seem very significant (this also matches my experience!)</div><br/><div id="41471219" class="c"><input type="checkbox" id="c-41471219" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470503">parent</a><span>|</span><a href="#41470391">next</a><span>|</span><label class="collapse" for="c-41471219">[-]</label><label class="expand" for="c-41471219">[1 more]</label></div><br/><div class="children"><div class="content">Two tower models (and various variants thereof) are popular for early stages of recommendation system pipelines and search engine pipelines.</div><br/></div></div></div></div></div></div><div id="41470391" class="c"><input type="checkbox" id="c-41470391" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470240">parent</a><span>|</span><a href="#41470392">prev</a><span>|</span><a href="#41470109">next</a><span>|</span><label class="collapse" for="c-41470391">[-]</label><label class="expand" for="c-41470391">[14 more]</label></div><br/><div class="children"><div class="content">That‘s exactly the point no? We lost 1 dim (magnitude). Not so nice in 2d but no biggie in 512d</div><br/><div id="41470517" class="c"><input type="checkbox" id="c-41470517" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470391">parent</a><span>|</span><a href="#41470869">next</a><span>|</span><label class="collapse" for="c-41470517">[-]</label><label class="expand" for="c-41470517">[9 more]</label></div><br/><div class="children"><div class="content">Magnitude is not a dimension, it’s information about each value that is lost when you normalize it. To prove this normalize any vector and then try to de-normalize it again.</div><br/><div id="41470819" class="c"><input type="checkbox" id="c-41470819" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470517">parent</a><span>|</span><a href="#41470534">next</a><span>|</span><label class="collapse" for="c-41470819">[-]</label><label class="expand" for="c-41470819">[1 more]</label></div><br/><div class="children"><div class="content">Magnitude is a dimension. Any 2-dimensional vector can be explicitly transformed into the polar (r, theta) coordinate system where one of the dimensions is magnitude. Any 3-dimensional vector can be transformed into the spherical (r, theta, phi) coordinate where one of the dimensions is magnitude. This is high school mathematics. (Okay I concede that maybe the spherical coordinate system isn&#x27;t exactly high school material, then just think about longitude, latitude, and distance from the center.)</div><br/></div></div><div id="41470534" class="c"><input type="checkbox" id="c-41470534" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470517">parent</a><span>|</span><a href="#41470819">prev</a><span>|</span><a href="#41471305">next</a><span>|</span><label class="collapse" for="c-41470534">[-]</label><label class="expand" for="c-41470534">[6 more]</label></div><br/><div class="children"><div class="content">Impossible because... you lost a dimension.</div><br/><div id="41470775" class="c"><input type="checkbox" id="c-41470775" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470534">parent</a><span>|</span><a href="#41471305">next</a><span>|</span><label class="collapse" for="c-41470775">[-]</label><label class="expand" for="c-41470775">[5 more]</label></div><br/><div class="children"><div class="content">That’s not mathematically accurate though, is it? We haven’t reduced the dimension of the vector by one.<p>Pray tell, which dimension do we lose when we normalize, say a 2D vector?</div><br/><div id="41471186" class="c"><input type="checkbox" id="c-41471186" checked=""/><div class="controls bullet"><span class="by">creata</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470775">parent</a><span>|</span><a href="#41472526">next</a><span>|</span><label class="collapse" for="c-41471186">[-]</label><label class="expand" for="c-41471186">[1 more]</label></div><br/><div class="children"><div class="content">Mathematically, it&#x27;s fine to say that you&#x27;ve lost the magnitude dimension.<p>Before normalization, the vector lies in R^n, which is an n-dimensional manifold.<p>After normalization, the vector lies in the unit sphere in R^n, which is an (n-1)-dimensional manifold.</div><br/></div></div><div id="41472526" class="c"><input type="checkbox" id="c-41472526" checked=""/><div class="controls bullet"><span class="by">langcss</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470775">parent</a><span>|</span><a href="#41471186">prev</a><span>|</span><a href="#41470962">next</a><span>|</span><label class="collapse" for="c-41472526">[-]</label><label class="expand" for="c-41472526">[1 more]</label></div><br/><div class="children"><div class="content">A circle circumference is a line, is 1D?</div><br/></div></div><div id="41470962" class="c"><input type="checkbox" id="c-41470962" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470775">parent</a><span>|</span><a href="#41472526">prev</a><span>|</span><a href="#41470825">next</a><span>|</span><label class="collapse" for="c-41470962">[-]</label><label class="expand" for="c-41470962">[1 more]</label></div><br/><div class="children"><div class="content">Magnitude, obviously.<p>&gt;&gt;&gt; Magnitude is not a dimension [...] To prove this normalize any vector and then try to de-normalize it again.<p>Say you have the vector (18, -5) in a normal Euclidean <i>x, y</i> plane.<p>Now project that vector onto the y-axis.<p>Now try to un-project it again.<p>What do you think you just proved?</div><br/></div></div></div></div></div></div><div id="41471305" class="c"><input type="checkbox" id="c-41471305" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470517">parent</a><span>|</span><a href="#41470534">prev</a><span>|</span><a href="#41470869">next</a><span>|</span><label class="collapse" for="c-41471305">[-]</label><label class="expand" for="c-41471305">[1 more]</label></div><br/><div class="children"><div class="content">you dont lose anything when you normalize things. not sure what you are tallking about.</div><br/></div></div></div></div><div id="41470869" class="c"><input type="checkbox" id="c-41470869" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470391">parent</a><span>|</span><a href="#41470517">prev</a><span>|</span><a href="#41470109">next</a><span>|</span><label class="collapse" for="c-41470869">[-]</label><label class="expand" for="c-41470869">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s something wrong with the picture here but I can&#x27;t put my finger on it because my mathematical background here is too old. The space of k dimension vectors all normalized isn&#x27;t a vector space itself. It&#x27;s well-behaved in many ways but you lose the 0 vector (may not be relevant). Addition isn&#x27;t defined anymore, and if you try to keep it inside by normalization post addition, distribution becomes weird. I have no idea what this transformation means for word2vec and friends.<p>But the intuitive notion is that if you take all 3D and flatten it &#x2F; expand it to be just the surface of the 3D sphere, then paste yourself onto it Flatland style, it&#x27;s not the same as if you were to Flatland yourself into the 2D plane. The obvious thing is that triangles won&#x27;t sum to 180, but also parallel lines will intersect, and all sorts of differing strange things will happen.<p>I mean, it might still work in practice, but it&#x27;s obviously different from some method of dimensionality reduction because you&#x27;re changing the curvature of the space.</div><br/><div id="41470917" class="c"><input type="checkbox" id="c-41470917" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470869">parent</a><span>|</span><a href="#41470109">next</a><span>|</span><label class="collapse" for="c-41470917">[-]</label><label class="expand" for="c-41470917">[3 more]</label></div><br/><div class="children"><div class="content">The space of all normalized k-dimensional vector is just a unit k-sphere. You can deal with it directly, or you can use the standard inverse stereographic projection to map every point (except for one) onto a plane.<p>&gt; triangles won&#x27;t sum to 180<p>Exactly. Spherical triangles have the sum of their interior angles exceed 180 degrees.<p>&gt; parallel lines will intersect<p>Yes because parallel &quot;lines&quot; are really great circles on the sphere.</div><br/><div id="41471153" class="c"><input type="checkbox" id="c-41471153" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470917">parent</a><span>|</span><a href="#41470109">next</a><span>|</span><label class="collapse" for="c-41471153">[-]</label><label class="expand" for="c-41471153">[2 more]</label></div><br/><div class="children"><div class="content">So is it actually the case that normalizing down and then mapping to the k-1 plane yields a useful (for this purpose) k-1 space? Something feels wrong about the whole thing but I must just have broken intuition.</div><br/><div id="41471209" class="c"><input type="checkbox" id="c-41471209" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41471153">parent</a><span>|</span><a href="#41470109">next</a><span>|</span><label class="collapse" for="c-41471209">[-]</label><label class="expand" for="c-41471209">[1 more]</label></div><br/><div class="children"><div class="content">I do not understand the purpose that you are referring to in this comment or the earlier comment. But it is useful for some purposes.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41470109" class="c"><input type="checkbox" id="c-41470109" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#41469983">parent</a><span>|</span><a href="#41470177">prev</a><span>|</span><a href="#41470013">next</a><span>|</span><label class="collapse" for="c-41470109">[-]</label><label class="expand" for="c-41470109">[2 more]</label></div><br/><div class="children"><div class="content">So I first learned about cosine similarity in the context of traditional information retrieval, and the simplified models used in that field before the development of LLMs, TensorFlow, and large-scale machine learning might prove instructive.<p>Imagine you have a simple bag-of-words model of a document, where you just count the number of occurrences of each word in the document.  Numerically, this is represented as a vector where each dimension is one <i>token</i> (so, you might have one number for the word &quot;number&quot;, another for &quot;cosine&quot;, another for &quot;the&quot;, and so on), and the magnitude of that component is the count of the number of times it occurs.  Intuitively, cosine similarity is a measure of <i>how frequently the same word appears in both documents</i>.  Words that appear in both documents get multiplied together, but words that are only in one get multiplied by zero and drop out of the cosine sum.  So because &quot;cosine&quot;, &quot;number&quot;, and &quot;vector&quot; appear frequently in my post, it will appear similar to other documents about math.  Because &quot;words&quot; and &quot;documents&quot; appear frequently, it will appear similar to other documents about metalanguage or information retrieval.<p>And intuitively, the reason the magnitude doesn&#x27;t matter is that <i>those counts will be much higher in longer documents, but the length of the document doesn&#x27;t say much about what the document is about</i>.  The reason you take the cosine (which has a denominator of magnitude-squared) is a form of length normalization, so that you can get sensible results without biasing toward shorter or longer documents.<p>Most machine-learned embeddings are similar.  The components of the vector are <i>features</i> that your ML model has determined are important.  If the product of the same dimension of two items is large, it indicates that they are similar in that dimension.  If it&#x27;s zero, it indicates that that feature is not particularly representative of the item.  Embeddings are often normalized, and for normalized vectors the fact that magnitude drops out doesn&#x27;t really matter.  But it doesn&#x27;t hurt either: the magnitude will be one, so magnitude^2 is also 1 and you just take the pair-wise product of the vectors.</div><br/><div id="41470178" class="c"><input type="checkbox" id="c-41470178" checked=""/><div class="controls bullet"><span class="by">d110af5ccf</span><span>|</span><a href="#41469983">root</a><span>|</span><a href="#41470109">parent</a><span>|</span><a href="#41470013">next</a><span>|</span><label class="collapse" for="c-41470178">[-]</label><label class="expand" for="c-41470178">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the reason the magnitude doesn&#x27;t matter is that those counts will be much higher in longer documents ...<p>To be a bit more explicit (of my intuition). The vector is encoding a ratio, isn&#x27;t it? You want to treat 3:2, 6:4, 12:8, ... as equivalent in this case; normalization does exactly that.</div><br/></div></div></div></div><div id="41470013" class="c"><input type="checkbox" id="c-41470013" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#41469983">parent</a><span>|</span><a href="#41470109">prev</a><span>|</span><a href="#41470114">next</a><span>|</span><label class="collapse" for="c-41470013">[-]</label><label class="expand" for="c-41470013">[1 more]</label></div><br/><div class="children"><div class="content">Dunno if I have the full answer, but it seems in high dimensional spaces, you can typically throw away a lot of information and still preserve distance.<p>The J-L lemma is at least somewhat related, even though it doesn&#x27;t to my understanding quite describe the same transformation.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstrauss_lemma" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstraus...</a><p>see also <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Random_projection" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Random_projection</a></div><br/></div></div><div id="41470114" class="c"><input type="checkbox" id="c-41470114" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41469983">parent</a><span>|</span><a href="#41470013">prev</a><span>|</span><a href="#41471809">next</a><span>|</span><label class="collapse" for="c-41470114">[-]</label><label class="expand" for="c-41470114">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been wondering the same.<p>When I dabbled with latent semantic indexing[1], using cosine similarity made sense as the dimensions of the input vectors were words, for example a 1 if a word was present or 0 if not. So one would expect vectors that point in a similar direction to be related.<p>I haven&#x27;t studied LLM embedding layers in depth, so yeah been wondering about using certain norms[2] instead to determine if two embeddings are similar. Does it depends on the embedding layer for example?<p>Should be noted it&#x27;s been many years since I learned linear algebra, so getting somewhat rusty.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Latent_semantic_analysis" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Latent_semantic_analysis</a><p>[2]: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Norm_(mathematics)" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Norm_(mathematics)</a></div><br/></div></div></div></div><div id="41471809" class="c"><input type="checkbox" id="c-41471809" checked=""/><div class="controls bullet"><span class="by">SomewhatLikely</span><span>|</span><a href="#41469983">prev</a><span>|</span><a href="#41472357">next</a><span>|</span><label class="collapse" for="c-41471809">[-]</label><label class="expand" for="c-41471809">[2 more]</label></div><br/><div class="children"><div class="content">Something worth mentioning is that if your vectors all have the same length then cosine similarity and Euclidean distance will order most (all?) neighbors in the same order. Think of your query vector as a point on a unit sphere.  The Euclidean distance to a neighbor will be a chord from the query point to the neighbor.  Just as with the angle between the query-to-origin and the neighbor-to-origin vectors, the farther you move the neighbor from the query point on the surface of the sphere, the longer the chord between those points gets too.<p>EDIT: Here&#x27;s a better treatment, and it is the case that they give the exact same orderings: <a href="https:&#x2F;&#x2F;ajayp.app&#x2F;posts&#x2F;2020&#x2F;05&#x2F;relationship-between-cosine-similarity-and-euclidean-distance&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ajayp.app&#x2F;posts&#x2F;2020&#x2F;05&#x2F;relationship-between-cosine-...</a></div><br/><div id="41471819" class="c"><input type="checkbox" id="c-41471819" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#41471809">parent</a><span>|</span><a href="#41472357">next</a><span>|</span><label class="collapse" for="c-41471819">[-]</label><label class="expand" for="c-41471819">[1 more]</label></div><br/><div class="children"><div class="content">Cosine similarity is equal to the dot product of each vector normalized</div><br/></div></div></div></div><div id="41469739" class="c"><input type="checkbox" id="c-41469739" checked=""/><div class="controls bullet"><span class="by">derbOac</span><span>|</span><a href="#41472357">prev</a><span>|</span><a href="#41470605">next</a><span>|</span><label class="collapse" for="c-41469739">[-]</label><label class="expand" for="c-41469739">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I missed this but I was surprised they didn&#x27;t mention the connection to correlation. Cosine similarity can be thought of as a correlation, and some bivariate distributions (normal I think?) can be rexpressed in terms of cosine similarity.<p>There&#x27;s also some generalizations to higher dimensional notions of cosines that are kind of interesting.</div><br/></div></div><div id="41470605" class="c"><input type="checkbox" id="c-41470605" checked=""/><div class="controls bullet"><span class="by">anArbitraryOne</span><span>|</span><a href="#41469739">prev</a><span>|</span><a href="#41472541">next</a><span>|</span><label class="collapse" for="c-41470605">[-]</label><label class="expand" for="c-41470605">[3 more]</label></div><br/><div class="children"><div class="content">Cosine similarly is the epitome of status quo bias. How many DS or ML people actually think through similarity metrics that might be appropriate, then choose cosine? Gods forbid they have to justify using a different measure to their colleagues</div><br/><div id="41471901" class="c"><input type="checkbox" id="c-41471901" checked=""/><div class="controls bullet"><span class="by">gnulinux</span><span>|</span><a href="#41470605">parent</a><span>|</span><a href="#41471456">next</a><span>|</span><label class="collapse" for="c-41471901">[-]</label><label class="expand" for="c-41471901">[1 more]</label></div><br/><div class="children"><div class="content">Simplicity is a feature, unless there is a good justification, it&#x27;s not always better to have a more complex model.</div><br/></div></div><div id="41471456" class="c"><input type="checkbox" id="c-41471456" checked=""/><div class="controls bullet"><span class="by">whiterknight</span><span>|</span><a href="#41470605">parent</a><span>|</span><a href="#41471901">prev</a><span>|</span><a href="#41472541">next</a><span>|</span><label class="collapse" for="c-41471456">[-]</label><label class="expand" for="c-41471456">[1 more]</label></div><br/><div class="children"><div class="content">Seems more like easy to understand and gets the job done bias. Are there common pitfalls you have identified?</div><br/></div></div></div></div><div id="41472541" class="c"><input type="checkbox" id="c-41472541" checked=""/><div class="controls bullet"><span class="by">jheriko</span><span>|</span><a href="#41470605">prev</a><span>|</span><a href="#41469498">next</a><span>|</span><label class="collapse" for="c-41472541">[-]</label><label class="expand" for="c-41472541">[1 more]</label></div><br/><div class="children"><div class="content">so normalised dot product.<p>the notation here is bad. the bottom of the division looks like a cross product<p>as a games and graphics programmer i find it amazing that this would be a mystery... understanding the dot product is utterly foundational, and is some high-school level basics.</div><br/></div></div><div id="41469498" class="c"><input type="checkbox" id="c-41469498" checked=""/><div class="controls bullet"><span class="by">acjohnson55</span><span>|</span><a href="#41472541">prev</a><span>|</span><a href="#41469124">next</a><span>|</span><label class="collapse" for="c-41469498">[-]</label><label class="expand" for="c-41469498">[4 more]</label></div><br/><div class="children"><div class="content">I might have missed this, but I think the post might bury the lede that in a high dimensional space, two randomly chosen vectors are very unlikely to have high cosine similarity. Or maybe another way to put it is that the expected value of the cosine of two random vectors approaches zero as the dimensionality increases.<p>Most similarity metrics will be very low if vectors don&#x27;t even point in the same direction, so cosine similarity is a cheap way to filter out the vast majority of the data set.<p>It&#x27;s been a while since I&#x27;ve studied this stuff, so I might be off target.</div><br/><div id="41469823" class="c"><input type="checkbox" id="c-41469823" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41469498">parent</a><span>|</span><a href="#41470187">next</a><span>|</span><label class="collapse" for="c-41469823">[-]</label><label class="expand" for="c-41469823">[1 more]</label></div><br/><div class="children"><div class="content">Even if two random vectors don&#x27;t have high cosine similarity, and I have not had this issue in 3000 dimensions, the cosine similarity is still usable in relative terms, i.e. relative to other items in the dataset. This keeps it useful.</div><br/></div></div><div id="41470187" class="c"><input type="checkbox" id="c-41470187" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#41469498">parent</a><span>|</span><a href="#41469823">prev</a><span>|</span><a href="#41469124">next</a><span>|</span><label class="collapse" for="c-41470187">[-]</label><label class="expand" for="c-41470187">[2 more]</label></div><br/><div class="children"><div class="content">Nitpick: The expected value of the cosine is 0 even in low-dimensional spaces. It’s the expected <i>square</i> of that (i.e. the variance) which gets smaller with the dimension.</div><br/><div id="41471815" class="c"><input type="checkbox" id="c-41471815" checked=""/><div class="controls bullet"><span class="by">acjohnson55</span><span>|</span><a href="#41469498">root</a><span>|</span><a href="#41470187">parent</a><span>|</span><a href="#41469124">next</a><span>|</span><label class="collapse" for="c-41471815">[-]</label><label class="expand" for="c-41471815">[1 more]</label></div><br/><div class="children"><div class="content">That totally makes a sense, thanks!</div><br/></div></div></div></div></div></div><div id="41469124" class="c"><input type="checkbox" id="c-41469124" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#41469498">prev</a><span>|</span><a href="#41468824">next</a><span>|</span><label class="collapse" for="c-41469124">[-]</label><label class="expand" for="c-41469124">[1 more]</label></div><br/><div class="children"><div class="content">Three separate passes over JavaScript arrays are quite costly, especially for high-dimensional vectors. I&#x27;d recommend using `TypedArray` with vanilla `for` loops. It will make things faster, and will allow using C extensions, if you want to benefit from modern hardware features, while still implementing the logic in JavaScript: <a href="https:&#x2F;&#x2F;ashvardanian.com&#x2F;posts&#x2F;javascript-ai-vector-search&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ashvardanian.com&#x2F;posts&#x2F;javascript-ai-vector-search&#x2F;</a></div><br/></div></div><div id="41468824" class="c"><input type="checkbox" id="c-41468824" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41469124">prev</a><span>|</span><a href="#41470015">next</a><span>|</span><label class="collapse" for="c-41468824">[-]</label><label class="expand" for="c-41468824">[4 more]</label></div><br/><div class="children"><div class="content">For those who don&#x27;t want to use a full-blown RAG database, scipy.spatial.distance has a convenient cosine distance function. And for those who don&#x27;t even want to use SciPy, the formula in the linked post.<p>For anyone new to the topic, note that the monotonic interpretation of cosine distance is opposite to that of cosine similarity.</div><br/><div id="41469104" class="c"><input type="checkbox" id="c-41469104" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#41468824">parent</a><span>|</span><a href="#41468968">next</a><span>|</span><label class="collapse" for="c-41469104">[-]</label><label class="expand" for="c-41469104">[2 more]</label></div><br/><div class="children"><div class="content">SciPy distances module has its own problems. It&#x27;s pretty slow, and constantly overflows in mixed precision scenarios. It also raises the wrong type of errors when it overflows, and uses general purpose `math` package instead of `numpy` for square roots. So use it with caution.<p>I&#x27;ve outlined some of the related issues here: <a href="https:&#x2F;&#x2F;github.com&#x2F;ashvardanian&#x2F;SimSIMD#cosine-similarity-reciprocal-square-root-and-newton-raphson-iteration">https:&#x2F;&#x2F;github.com&#x2F;ashvardanian&#x2F;SimSIMD#cosine-similarity-re...</a></div><br/><div id="41469803" class="c"><input type="checkbox" id="c-41469803" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41468824">root</a><span>|</span><a href="#41469104">parent</a><span>|</span><a href="#41468968">next</a><span>|</span><label class="collapse" for="c-41469803">[-]</label><label class="expand" for="c-41469803">[1 more]</label></div><br/><div class="children"><div class="content">Noted, and thanks for your great work. My experience with it is limited to working with LLM embeddings, which I believe have been cleanly between 0 and 1. As such, I am yet to encounter these issues.<p>Regarding the speed, yes, I wouldn&#x27;t use it with big data. Up to a few thousand items has been fine for me, or perhaps a few hundred if pairwise.</div><br/></div></div></div></div></div></div><div id="41470015" class="c"><input type="checkbox" id="c-41470015" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#41468824">prev</a><span>|</span><a href="#41470565">next</a><span>|</span><label class="collapse" for="c-41470015">[-]</label><label class="expand" for="c-41470015">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just a normalized dot product. People use &quot;cosine similarity&quot; to sound knowledgeable</div><br/><div id="41470826" class="c"><input type="checkbox" id="c-41470826" checked=""/><div class="controls bullet"><span class="by">youssefabdelm</span><span>|</span><a href="#41470015">parent</a><span>|</span><a href="#41470380">next</a><span>|</span><label class="collapse" for="c-41470826">[-]</label><label class="expand" for="c-41470826">[1 more]</label></div><br/><div class="children"><div class="content">Hm for me interactive visualizations are more illuminating:<p><a href="https:&#x2F;&#x2F;www.falstad.com&#x2F;dotproduct&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.falstad.com&#x2F;dotproduct&#x2F;</a><p><a href="https:&#x2F;&#x2F;wordsandbuttons.online&#x2F;interactive_mnemonics_for_dot_and_cross_vector_products.html" rel="nofollow">https:&#x2F;&#x2F;wordsandbuttons.online&#x2F;interactive_mnemonics_for_dot...</a><p>In essence then, not as confusing to the beginner who might even know what a dot product &#x27;is&#x27; operationally but not what it &#x27;does&#x27;.<p>So level 1 &#x27;it&#x27;s just a normalized dot product&#x27;, level 2 more immediately intuitive: &#x27;is arrow 1 pointing in the same direction as arrow 2?&#x27; or &#x27;how close is arrow 1&#x27;s direction to arrow 2&#x27;s direction?&#x27;<p>Now what&#x27;s left after that is &#x27;Why is it so? Why did we decide on this in embeddings?&#x27;</div><br/></div></div><div id="41470380" class="c"><input type="checkbox" id="c-41470380" checked=""/><div class="controls bullet"><span class="by">ttoinou</span><span>|</span><a href="#41470015">parent</a><span>|</span><a href="#41470826">prev</a><span>|</span><a href="#41471244">next</a><span>|</span><label class="collapse" for="c-41470380">[-]</label><label class="expand" for="c-41470380">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. It’s almost like Wildberger‘s rational trigonometry</div><br/></div></div><div id="41471244" class="c"><input type="checkbox" id="c-41471244" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#41470015">parent</a><span>|</span><a href="#41470380">prev</a><span>|</span><a href="#41470565">next</a><span>|</span><label class="collapse" for="c-41471244">[-]</label><label class="expand" for="c-41471244">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Cosine similarity&quot; includes that particular kind of normalization, it does actually impart more information than &quot;normalized dot product&quot;.</div><br/></div></div></div></div><div id="41470565" class="c"><input type="checkbox" id="c-41470565" checked=""/><div class="controls bullet"><span class="by">anArbitraryOne</span><span>|</span><a href="#41470015">prev</a><span>|</span><a href="#41469961">next</a><span>|</span><label class="collapse" for="c-41470565">[-]</label><label class="expand" for="c-41470565">[1 more]</label></div><br/><div class="children"><div class="content">Not enthused about X notating the dot product as opposed to cross product</div><br/></div></div><div id="41469961" class="c"><input type="checkbox" id="c-41469961" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#41470565">prev</a><span>|</span><a href="#41469902">next</a><span>|</span><label class="collapse" for="c-41469961">[-]</label><label class="expand" for="c-41469961">[1 more]</label></div><br/><div class="children"><div class="content">It works like the dot product in the case of spherical embeddings. Normalizing the embeddings makes it easier to understand too.</div><br/></div></div><div id="41469902" class="c"><input type="checkbox" id="c-41469902" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#41469961">prev</a><span>|</span><a href="#41470288">next</a><span>|</span><label class="collapse" for="c-41469902">[-]</label><label class="expand" for="c-41469902">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t properly explain what it says it explains. To explain it correctly, you have to explain why the dot product of two vectors computed as the sum of the products of the coefficients of an orthonormal basis is a scalar equal to the product of the Euclidean magnitudes of the vectors and the cosine of the angle between them. The Wikipedia article on dot product explains this reasonably well, so just read that.</div><br/></div></div><div id="41470288" class="c"><input type="checkbox" id="c-41470288" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41469902">prev</a><span>|</span><a href="#41470019">next</a><span>|</span><label class="collapse" for="c-41470288">[-]</label><label class="expand" for="c-41470288">[1 more]</label></div><br/><div class="children"><div class="content">Cosine similarity for unit vectors looks like the angle between hands on a high dimensional clock. 12 and 6 have -1 cosine sim. 5 and 6 are pretty close.<p>Cosine similarity works if the model has been deliberately trained with cosine similarity as the distance metric. If they were trained with Euclidean distance the results aren’t reliable.<p>Example: (0,1) and (0,2) have a cosine similarity of 1 but nonzero Euclidean distance.</div><br/></div></div><div id="41470019" class="c"><input type="checkbox" id="c-41470019" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#41470288">prev</a><span>|</span><a href="#41471132">next</a><span>|</span><label class="collapse" for="c-41470019">[-]</label><label class="expand" for="c-41470019">[1 more]</label></div><br/><div class="children"><div class="content">dot product is easiest to understand as the projection of one vector to the other. the rest of it is self explanatory</div><br/></div></div><div id="41471132" class="c"><input type="checkbox" id="c-41471132" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41470019">prev</a><span>|</span><a href="#41469053">next</a><span>|</span><label class="collapse" for="c-41471132">[-]</label><label class="expand" for="c-41471132">[1 more]</label></div><br/><div class="children"><div class="content">I was always curious as to why we don&#x27;t encode ML parameters and activations using complex numbers.<p>A dot product between two complex numbers naturally encodes confidence in the result in the magnitude.<p>I blame numpy</div><br/></div></div><div id="41469053" class="c"><input type="checkbox" id="c-41469053" checked=""/><div class="controls bullet"><span class="by">heyitsguay</span><span>|</span><a href="#41471132">prev</a><span>|</span><a href="#41444634">next</a><span>|</span><label class="collapse" for="c-41469053">[-]</label><label class="expand" for="c-41469053">[4 more]</label></div><br/><div class="children"><div class="content">Sorta related -- whenever I&#x27;m doing something with embeddings, i just normalize them to length one, at which point cosine similarity becomes a simple dot product. Is there ever a reason to not normalize embedding length? An application where that length matters?</div><br/><div id="41469225" class="c"><input type="checkbox" id="c-41469225" checked=""/><div class="controls bullet"><span class="by">psyklic</span><span>|</span><a href="#41469053">parent</a><span>|</span><a href="#41469255">next</a><span>|</span><label class="collapse" for="c-41469225">[-]</label><label class="expand" for="c-41469225">[1 more]</label></div><br/><div class="children"><div class="content">For the LLM itself, length matters. For example, the final logits are computed as the un-normalized dot product, making them a function of both direction and magnitude. This means that if you embed then immediately un-embed (using the same embeddings for both), a different token might be obtained. In models such as GPT2, the embedding vector magnitude is loosely correlated with token frequency.</div><br/></div></div><div id="41469255" class="c"><input type="checkbox" id="c-41469255" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#41469053">parent</a><span>|</span><a href="#41469225">prev</a><span>|</span><a href="#41469593">next</a><span>|</span><label class="collapse" for="c-41469255">[-]</label><label class="expand" for="c-41469255">[1 more]</label></div><br/><div class="children"><div class="content">On the practical side, dot products are great, but break in mixed precision and integer representations, where accurately normalizing to unit length isn&#x27;t feasible.<p>In other cases people prefer L2 distances for embeddings, where the magnitude can have a serious impact on the distance between a pair of points.</div><br/></div></div><div id="41469593" class="c"><input type="checkbox" id="c-41469593" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41469053">parent</a><span>|</span><a href="#41469255">prev</a><span>|</span><a href="#41444634">next</a><span>|</span><label class="collapse" for="c-41469593">[-]</label><label class="expand" for="c-41469593">[1 more]</label></div><br/><div class="children"><div class="content">If you’re feeling guilty about it you can usually store the un-normalized lengths separately.</div><br/></div></div></div></div></div></div></div></div></div></body></html>