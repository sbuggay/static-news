<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714899712614" as="style"/><link rel="stylesheet" href="styles.css?v=1714899712614"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.03175">The Matrix: A Bayesian learning model for LLMs</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>stoniejohnson</span> | <span>14 comments</span></div><br/><div><div id="40256840" class="c"><input type="checkbox" id="c-40256840" checked=""/><div class="controls bullet"><span class="by">dosinga</span><span>|</span><a href="#40260450">next</a><span>|</span><label class="collapse" for="c-40256840">[-]</label><label class="expand" for="c-40256840">[3 more]</label></div><br/><div class="children"><div class="content">Conclusion from the paper is:<p>In this paper we present a new model to explain the behavior of Large Language Models. Our frame of reference is an abstract probability matrix, which contains the multinomial probabilities for next token prediction in each row, where the row represents a specific prompt. We then demonstrate that LLM text generation is consistent with a compact representation of this abstract matrix through a combination of embeddings and Bayesian learning. Our model explains (the emergence of) In-Context learning with scale of the LLMs, as also other phenomena like Chain of Thought reasoning and the problem with large context windows. Finally, we outline implications of our model and some directions for future exploration.<p>Where does the &quot;Cannot Recursively Improve&quot; come from?</div><br/><div id="40259038" class="c"><input type="checkbox" id="c-40259038" checked=""/><div class="controls bullet"><span class="by">ajb</span><span>|</span><a href="#40256840">parent</a><span>|</span><a href="#40260450">next</a><span>|</span><label class="collapse" for="c-40259038">[-]</label><label class="expand" for="c-40259038">[2 more]</label></div><br/><div class="children"><div class="content">Looks like someone edited the title of this thread. I assume &quot;Cannot Recursively Improve&quot; was in the old one?</div><br/><div id="40259176" class="c"><input type="checkbox" id="c-40259176" checked=""/><div class="controls bullet"><span class="by">dosinga</span><span>|</span><a href="#40256840">root</a><span>|</span><a href="#40259038">parent</a><span>|</span><a href="#40260450">next</a><span>|</span><label class="collapse" for="c-40259176">[-]</label><label class="expand" for="c-40259176">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, looks like it. This is better!</div><br/></div></div></div></div></div></div><div id="40260450" class="c"><input type="checkbox" id="c-40260450" checked=""/><div class="controls bullet"><span class="by">avi_vallarapu</span><span>|</span><a href="#40256840">prev</a><span>|</span><a href="#40261299">next</a><span>|</span><label class="collapse" for="c-40260450">[-]</label><label class="expand" for="c-40260450">[1 more]</label></div><br/><div class="children"><div class="content">Theoretically this sounds great. I would worry about scalability issues with the Bayesian learning models practical implementation when dealing with the vast parameter space and data requirements of state of the-art models like GPT-3 and beyond.<p>Would love to see practical implementations on large-scale datasets and in varied contexts. I Liked the use of Dirichlet distributions to approximate any prior over multinomial distributions.</div><br/></div></div><div id="40261299" class="c"><input type="checkbox" id="c-40261299" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#40260450">prev</a><span>|</span><a href="#40256833">next</a><span>|</span><label class="collapse" for="c-40261299">[-]</label><label class="expand" for="c-40261299">[2 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t read through the paper (just the abstract), but isn&#x27;t the whole point of the KL divergence loss to get the best compression, which is equivalent to Bayesian learning? I don&#x27;t really see how this is novel, like I&#x27;m sure people were doing this with Markov chains back in the 90s.</div><br/><div id="40262591" class="c"><input type="checkbox" id="c-40262591" checked=""/><div class="controls bullet"><span class="by">jksk61</span><span>|</span><a href="#40261299">parent</a><span>|</span><a href="#40256833">next</a><span>|</span><label class="collapse" for="c-40262591">[-]</label><label class="expand" for="c-40262591">[1 more]</label></div><br/><div class="children"><div class="content">in fact it is nothing new.</div><br/></div></div></div></div><div id="40256833" class="c"><input type="checkbox" id="c-40256833" checked=""/><div class="controls bullet"><span class="by">drbig</span><span>|</span><a href="#40261299">prev</a><span>|</span><a href="#40256824">next</a><span>|</span><label class="collapse" for="c-40256833">[-]</label><label class="expand" for="c-40256833">[2 more]</label></div><br/><div class="children"><div class="content">The title of the paper is actually `The Matrix: A Bayesian learning model for LLMs` and the conclusion presented in the title of this post is not to be found in the abstract... Just a heads up y&#x27;all.</div><br/><div id="40256850" class="c"><input type="checkbox" id="c-40256850" checked=""/><div class="controls bullet"><span class="by">zoky</span><span>|</span><a href="#40256833">parent</a><span>|</span><a href="#40256824">next</a><span>|</span><label class="collapse" for="c-40256850">[-]</label><label class="expand" for="c-40256850">[1 more]</label></div><br/><div class="children"><div class="content">I don’t really care, I just want some vague reassurance that we’re probably not on the verge of launching Skynet…</div><br/></div></div></div></div><div id="40256824" class="c"><input type="checkbox" id="c-40256824" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#40256833">prev</a><span>|</span><a href="#40257007">next</a><span>|</span><label class="collapse" for="c-40256824">[-]</label><label class="expand" for="c-40256824">[1 more]</label></div><br/><div class="children"><div class="content">Completely editorialized title. The article talks about LLMs, not transformers.</div><br/></div></div><div id="40260502" class="c"><input type="checkbox" id="c-40260502" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#40257007">prev</a><span>|</span><label class="collapse" for="c-40260502">[-]</label><label class="expand" for="c-40260502">[3 more]</label></div><br/><div class="children"><div class="content">Upvote bait for LessWrong&#x2F;EA advocates?<p>*runs before stones are cast*</div><br/><div id="40261505" class="c"><input type="checkbox" id="c-40261505" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40260502">parent</a><span>|</span><a href="#40261723">next</a><span>|</span><label class="collapse" for="c-40261505">[-]</label><label class="expand" for="c-40261505">[1 more]</label></div><br/><div class="children"><div class="content">Could you please stop posting unsubstantive comments and flamebait? It&#x27;s not what this site is for, and destroys what it is for.<p>If you wouldn&#x27;t mind reviewing <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html</a> and taking the intended spirit of the site more to heart, we&#x27;d be grateful.</div><br/></div></div></div></div></div></div></div></div></div></body></html>