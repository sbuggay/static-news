<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734685260945" as="style"/><link rel="stylesheet" href="styles.css?v=1734685260945"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</a> <span class="domain">(<a href="https://distill.pub">distill.pub</a>)</span></div><div class="subtext"><span>misonic</span> | <span>7 comments</span></div><br/><div><div id="42468798" class="c"><input type="checkbox" id="c-42468798" checked=""/><div class="controls bullet"><span class="by">samsartor</span><span>|</span><a href="#42468950">next</a><span>|</span><label class="collapse" for="c-42468798">[-]</label><label class="expand" for="c-42468798">[4 more]</label></div><br/><div class="children"><div class="content">GNNs have been a bit of a disappointment to me. I&#x27;ve tried to apply them a couple times to my research but it has never worked out.<p>For a long time GNNs were pitched as a generalization of CNNs. But CNNs are more powerful because the &quot;adjacency weights&quot; (so to speak) are more meaningful: they learn relative positional relationships. GNNs usually resort to pooling, like described here. And you can output an image with a CNN. Good luck getting a GNN to output a graph. Topology still has to be decided up front, sometimes even during training. And the nail in the coffin is performance. It is incredible how slow GNNs are compared to CNNs.<p>These days I feel like attention has kinda eclipsed GNNs for a lot of those reasons. You can make GNNs that use attention instead of pooling, but there isn&#x27;t much point. The graph is usually only traversed in order to create the mask matrix (ie attend between nth neighbors) and otherwise you are using a regular old transformer. Often you don&#x27;t even need the graph adjacencies because some kind of distance metric is already available.<p>I&#x27;m sure GNNs are extremely useful to someone somewhere but my experience has been a hammer looking for a nail.</div><br/><div id="42469313" class="c"><input type="checkbox" id="c-42469313" checked=""/><div class="controls bullet"><span class="by">igorkraw</span><span>|</span><a href="#42468798">parent</a><span>|</span><a href="#42468882">next</a><span>|</span><label class="collapse" for="c-42469313">[-]</label><label class="expand" for="c-42469313">[1 more]</label></div><br/><div class="children"><div class="content">GNNs are useful at least in one case, when your data a set of atoms that define your datum through their interactions, specifically a set that is that is high cardinality (so you can&#x27;t YOLO it with attention) with some notion of neighbourhood (i.e. geometry) within your set (defined by the interactions) which if maintained makes the data permutation equivariant, BUT you can&#x27;t find a meaningful way to represent that geometry implicitly (for example because it changes between samples) =&gt;  you YOLO it by just passing the neighourhood&#x2F;interaction structure in as an input.<p>In almost every other case, you can exploit additional structure to be more efficient (can you define an order? sequence model. is it euclidean&#x2F;riemanian? CNN or manifold aware models. no need to have global state? pointcloud networks. you have an explicit hierarchy? Unet version of your underlying modality.  etc)<p>The reason why I find GNNs cool is that 1) they encode the very notion of _relations_ and 2) they have a very nice relationship to completely general discretized differential equations, which as a complex systems&#x2F;dynamical systems guy is cool (but if you can specialize, there&#x27;s again easier ways)</div><br/></div></div><div id="42468882" class="c"><input type="checkbox" id="c-42468882" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#42468798">parent</a><span>|</span><a href="#42469313">prev</a><span>|</span><a href="#42468874">next</a><span>|</span><label class="collapse" for="c-42468882">[-]</label><label class="expand" for="c-42468882">[1 more]</label></div><br/><div class="children"><div class="content">Are you doing some regular like vision?<p>For the reasons you&#x27;re saying, I don&#x27;t think it&#x27;s an accident that GNNs are popular mostly in domains like recommendations that feel graph-y for their domain model so getting to a useful topology isn&#x27;t as big a leap.<p>A frustration for me has been more that many of these graph-y domains are about behavioral machine&#x2F;people data like logs that contain a large amount of categorical dimensions. The graph part can help, but it is just as import to key on the categorical dimensions, and doing well there often end up outside of the model - random forest etc. It&#x27;s easier to start with those, and then is a lot of work for the GNN part (though we &amp; others have been trying to simplify) for &quot;a bit more lift&quot;.<p>Of course, if this is your core business and this means many millions of dollars, it can be justified... but still, it&#x27;s hard for most production teams. In practice, we often just do something with pygraphistry users like xgboost + umap and move on. Even getting an RGCN to perform well takes work..</div><br/></div></div><div id="42468874" class="c"><input type="checkbox" id="c-42468874" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#42468798">parent</a><span>|</span><a href="#42468882">prev</a><span>|</span><a href="#42468950">next</a><span>|</span><label class="collapse" for="c-42468874">[-]</label><label class="expand" for="c-42468874">[1 more]</label></div><br/><div class="children"><div class="content">Same! I’ve seen many proposals to use a GNN for some problem for which we used a “flat” model, e.g., taking into account HTML structure when predicting labels for pages. Even when it seemingly made a lot of sense to use them, it didn’t work.</div><br/></div></div></div></div><div id="42468950" class="c"><input type="checkbox" id="c-42468950" checked=""/><div class="controls bullet"><span class="by">helltone</span><span>|</span><a href="#42468798">prev</a><span>|</span><label class="collapse" for="c-42468950">[-]</label><label class="expand" for="c-42468950">[2 more]</label></div><br/><div class="children"><div class="content">It seems GNNs operate on a fixed topology. What if I want to approximate some transformation of the topology of the graph? For example learning how to layout a graph, or converting program abstract syntax trees to data flow graphs.</div><br/><div id="42469251" class="c"><input type="checkbox" id="c-42469251" checked=""/><div class="controls bullet"><span class="by">igorkraw</span><span>|</span><a href="#42468950">parent</a><span>|</span><label class="collapse" for="c-42469251">[-]</label><label class="expand" for="c-42469251">[1 more]</label></div><br/><div class="children"><div class="content">The whole point of GNNs is that they generalize to arbitrary topologies by explicitly conditioning the idea of &quot;neighbours&quot; on the graph specifying the topology. Graph layout has been tried here <a href="https:&#x2F;&#x2F;github.com&#x2F;limbo018&#x2F;DREAMPlace">https:&#x2F;&#x2F;github.com&#x2F;limbo018&#x2F;DREAMPlace</a> to great fanfare although there is recent drama about it <a href="https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;The-False-Dawn%3A-Reevaluating-Google&#x27;s-Reinforcement-Markov&#x2F;c43045140f4b30def64f04d1bc24a14030c0798b" rel="nofollow">https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;The-False-Dawn%3A-Reev...</a> . Graph transformations are a thing as well <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2012.01470" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2012.01470</a> but it&#x27;s a tricky problem because you implicitly need to solve the graph matching problem</div><br/></div></div></div></div></div></div></div></div></div></body></html>