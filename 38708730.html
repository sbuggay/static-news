<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703149258625" as="style"/><link rel="stylesheet" href="styles.css?v=1703149258625"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/johnma2006/mamba-minimal">Implementation of Mamba in one file of PyTorch</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>johnma2006</span> | <span>109 comments</span></div><br/><div><div id="38710691" class="c"><input type="checkbox" id="c-38710691" checked=""/><div class="controls bullet"><span class="by">danieldk</span><span>|</span><a href="#38709375">next</a><span>|</span><label class="collapse" for="c-38710691">[-]</label><label class="expand" for="c-38710691">[4 more]</label></div><br/><div class="children"><div class="content">Nice! For what it is worth, a colleague and I made a library a while ago that factors out most shared model code, with which many models can be implemented in about 100 lines (excluding Python import ceremony and comments). E.g.:<p>BERT:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;curated_transformers&#x2F;models&#x2F;bert&#x2F;encoder.py">https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;...</a><p>Llama 1&#x2F;2:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;curated_transformers&#x2F;models&#x2F;llama&#x2F;decoder.py">https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;...</a><p>MPT:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;curated_transformers&#x2F;models&#x2F;mpt&#x2F;decoder.py">https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;curated-transformers&#x2F;blob&#x2F;main&#x2F;...</a><p>With various stuff enabled, including support for TorchScript JIT, PyTorch flash attention, etc.</div><br/><div id="38716613" class="c"><input type="checkbox" id="c-38716613" checked=""/><div class="controls bullet"><span class="by">deepsquirrelnet</span><span>|</span><a href="#38710691">parent</a><span>|</span><a href="#38712513">next</a><span>|</span><label class="collapse" for="c-38716613">[-]</label><label class="expand" for="c-38716613">[1 more]</label></div><br/><div class="children"><div class="content">I’m floored by this library. Great concept.<p>I’ve never been a fan of HFs implementation. This is a beautiful API at exactly the right level of abstraction.<p>I’ll give this a try on my next project.</div><br/></div></div><div id="38712513" class="c"><input type="checkbox" id="c-38712513" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#38710691">parent</a><span>|</span><a href="#38716613">prev</a><span>|</span><a href="#38709375">next</a><span>|</span><label class="collapse" for="c-38712513">[-]</label><label class="expand" for="c-38712513">[2 more]</label></div><br/><div class="children"><div class="content">Nice. I will definitely be taking a look at this. Have you looked at the xformers library ? They are looking at the same problem as you but their focus is more on providing performant transformer modules using triton. Using specific components from the library though is not as simple. I kept running into runtime errors so I&#x27;ve kept it aside for now. I am building something based on the Bert architecture so I will give this a look. Thanks for all the work!</div><br/><div id="38712636" class="c"><input type="checkbox" id="c-38712636" checked=""/><div class="controls bullet"><span class="by">danieldk</span><span>|</span><a href="#38710691">root</a><span>|</span><a href="#38712513">parent</a><span>|</span><a href="#38709375">next</a><span>|</span><label class="collapse" for="c-38712636">[-]</label><label class="expand" for="c-38712636">[1 more]</label></div><br/><div class="children"><div class="content">I would&#x27;ve loved to look at xFormers, but I avoided looking at other implementations to make sure that ours is a clean room implementation.<p>Curated Transformers started as a very small library just for spaCy (spaCy 3.7 transformer pipelines use Curated Transformers) with just the older encoder models (BERT, RoBERTa, etc.). spaCy used Hugging Face Transformers prior for the provided transformer models, but we wanted something where we could easily hook into different parts of the model (e.g. for distillation).<p>After the functionality needed for spaCy was done, Matt @ Explosion encouraged us to extend it into a more general PyTorch library that would also support decoder architectures, generation, etc.</div><br/></div></div></div></div></div></div><div id="38709375" class="c"><input type="checkbox" id="c-38709375" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#38710691">prev</a><span>|</span><a href="#38710356">next</a><span>|</span><label class="collapse" for="c-38709375">[-]</label><label class="expand" for="c-38709375">[7 more]</label></div><br/><div class="children"><div class="content">The original mamba code has a lot of speed optimizations and other stuff that make it difficult to immediately get so this will help with learning.<p>I can&#x27;t help but also plug my own Mamba inference implementation. <a href="https:&#x2F;&#x2F;github.com&#x2F;rbitr&#x2F;llm.f90&#x2F;tree&#x2F;master&#x2F;ssm">https:&#x2F;&#x2F;github.com&#x2F;rbitr&#x2F;llm.f90&#x2F;tree&#x2F;master&#x2F;ssm</a><p>For inference one token at a time everything simplifies considerably.</div><br/><div id="38713074" class="c"><input type="checkbox" id="c-38713074" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#38709375">parent</a><span>|</span><a href="#38710356">next</a><span>|</span><label class="collapse" for="c-38713074">[-]</label><label class="expand" for="c-38713074">[6 more]</label></div><br/><div class="children"><div class="content">Fortran! If you don&#x27;t mind me asking, why Fortran?<p>I know it underpins a <i>lot</i> of time-tested scientific code, often wrapped by libraries like PyTorch and Numpy, but Fortran isn&#x27;t exactly a popular language nowadays. What&#x27;s your rationale for using it?</div><br/><div id="38713298" class="c"><input type="checkbox" id="c-38713298" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#38709375">root</a><span>|</span><a href="#38713074">parent</a><span>|</span><a href="#38718164">next</a><span>|</span><label class="collapse" for="c-38713298">[-]</label><label class="expand" for="c-38713298">[4 more]</label></div><br/><div class="children"><div class="content">Tdlr, Fortran is low level-ish, compiled, but otherwise almost identical to numpy syntax wise.<p>It supports all the common array and matrix operations and it doesn&#x27;t need memory and pointer management the way C does. But it still compiles down to something very fast, you can link in BLAS and GPU libraries, supports easy parallelism...<p>When I compare with e.g. Karpathy&#x27;s llama2.c, I think Fortran is easy to work with implementing basic transformer inference because of how it handles arrays.<p>The downside is that while there are efforts to modernize it, I find it more cumbersome for non-numerical stuff, particularly strings. But I think for the actual linear algebra implementation, it can&#x27;t be beat.<p>I should add, I know it&#x27;s a bit of an uphill battle, I expect fewer people will use code that I write in Fortran vs basically anything else. But I&#x27;m hoping to pull some people in and get a critical mass of interest because I think it has a lot of promise. That&#x27;s actually one of the reasons I wanted to get a Mamba implementation quickly (though now that there&#x27;s a basic python one I think I&#x27;ll have lost some potential users to it :)</div><br/><div id="38713524" class="c"><input type="checkbox" id="c-38713524" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#38709375">root</a><span>|</span><a href="#38713298">parent</a><span>|</span><a href="#38718164">next</a><span>|</span><label class="collapse" for="c-38713524">[-]</label><label class="expand" for="c-38713524">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for the thoughtful response.<p>Unfortunately, I too think it will be a bit of an uphill battle for you.<p>If you haven&#x27;t already, take a look at Mojo and Julia. Both offer many of the benefits of Fortran, but unlike it, they are seeing growing adoption.</div><br/><div id="38713947" class="c"><input type="checkbox" id="c-38713947" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#38709375">root</a><span>|</span><a href="#38713524">parent</a><span>|</span><a href="#38718164">next</a><span>|</span><label class="collapse" for="c-38713947">[-]</label><label class="expand" for="c-38713947">[2 more]</label></div><br/><div class="children"><div class="content">An uphill battle is fine</div><br/></div></div></div></div></div></div><div id="38718164" class="c"><input type="checkbox" id="c-38718164" checked=""/><div class="controls bullet"><span class="by">cztomsik</span><span>|</span><a href="#38709375">root</a><span>|</span><a href="#38713074">parent</a><span>|</span><a href="#38713298">prev</a><span>|</span><a href="#38710356">next</a><span>|</span><label class="collapse" for="c-38718164">[-]</label><label class="expand" for="c-38718164">[1 more]</label></div><br/><div class="children"><div class="content">I only heard good things about fortran :)</div><br/></div></div></div></div></div></div><div id="38710356" class="c"><input type="checkbox" id="c-38710356" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38709375">prev</a><span>|</span><a href="#38709750">next</a><span>|</span><label class="collapse" for="c-38710356">[-]</label><label class="expand" for="c-38710356">[11 more]</label></div><br/><div class="children"><div class="content">things I&#x27;d like a non-ML-researcher explanation of about Mamba:<p>1. what is the overall insight of state space models beyond transformers? (i know this is somewhat covered in the paper but still a bit inaccessible)<p>2. what was the incremental innovation&#x2F;result that is making Mamba more successful&#x2F;interesting than its predecessors? (S4, H3, Monarch etc)<p>3. what are the implications beyond subquadratic scaling of context? say if i don&#x27;t really care about context length &gt; 100k tokens. what other benefits are there - for example, is Mamba potentially more compute-efficient to train for a similar size of model&#x2F;dataset?<p>just offering 3 prompts for knowledgeable people to drop some alpha</div><br/><div id="38712467" class="c"><input type="checkbox" id="c-38712467" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#38710356">parent</a><span>|</span><a href="#38715842">next</a><span>|</span><label class="collapse" for="c-38712467">[-]</label><label class="expand" for="c-38712467">[2 more]</label></div><br/><div class="children"><div class="content">My IQ is orders of magnitude lower than the authors of the paper, but I did my best to work through it anyway. I studied CE and have the basic control theory background and undergrad level discrete time systems intuition. It would take much additional studying to understand state space models enough to really parse this paper. But I tried anyway. Take my comment here with a big grain of salt.<p>The overall insight of Mamba is to solve a longstanding problem with state space models. They are good at compressing the input context, but the compression of input into a hidden state erases information needed to make use of the context effectively as Transformers do.<p>Their solution to this problem is to create what they call a selection mechanism. The mechanism is input-dependent, allowing the model to adjust its output at each step as the input changes. How they do this is by making a few of the state space variables input-dependent instead of input-invariant. They choose a few of the state space variables and attach linear layers and such to project the input onto the state space variable at each time step. The linear layers (etc) are obviously trained so that they know how to transform the input appropriately so that the model spits out useful output.<p>But making the state space variables input dependent creates a problem in terms of computation overhead. They fix the computation problem by designing a machine architecture-aware algorithm that makes the most of modern GPU memory architecture, avoiding moving things in and out of HBM as much as possible.<p>Tri Dao came up with Flash Attention, which is basically a way to use hardware more efficiently in a Transformer. So this is his jam 100%.<p>I know this doesn’t add much to understanding the paper, but hopefully it’s better than nothing.</div><br/><div id="38713727" class="c"><input type="checkbox" id="c-38713727" checked=""/><div class="controls bullet"><span class="by">SpaceManNabs</span><span>|</span><a href="#38710356">root</a><span>|</span><a href="#38712467">parent</a><span>|</span><a href="#38715842">next</a><span>|</span><label class="collapse" for="c-38713727">[-]</label><label class="expand" for="c-38713727">[1 more]</label></div><br/><div class="children"><div class="content">Is this similar to subset selection with the concrete distribution?</div><br/></div></div></div></div><div id="38715842" class="c"><input type="checkbox" id="c-38715842" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#38710356">parent</a><span>|</span><a href="#38712467">prev</a><span>|</span><a href="#38711246">next</a><span>|</span><label class="collapse" for="c-38715842">[-]</label><label class="expand" for="c-38715842">[2 more]</label></div><br/><div class="children"><div class="content">1. Attention is quadratic with context length; RNN with gating (LSTM, GRU, etc) are linear, as are all these new architectures. Early RNN used gating to avoid exploding gradients, these new ideas use theory from dynamical systems that guarantees stability so the gating can focus on memory, rather than solving two problems at once.<p>2. The models released in the last couple of weeks running up to neurIPS23 (Mamba and Based) included a multi-query associative recall (MQAR) and data-dependence in the gating&#x2F;selection inspired by multi-headed attention. It turned out these were the main missing ingredients compared to earlier state-space (Hyena and earlier) architectures and made these new models as good as attention in associative recall tasks, and potentially even slightly better than attention in other non-lookup tasks.  Of course the huge detail in mamba is the efficient implementation on CUDA; without it the architecture may not make much sense for tasks where transformers are already appropriate.<p>3. If one does not have to worry too much about context length, a lot of new domains open up: DNA-sequence analysis is a linear task with long dependence; think of analyzing images, videos, or higher dimensional info in terms of streams of tokens (scan the pixels in the way of an old CRT monitor). The early dreams of AI included a continuously evolving single learning trajectory of an agent interacting with an environment continuously, so maybe such dreams will be easier to realize with these infinite-context-length models.<p>bonus: you didn&#x27;t ask for it, but as of today the downstream applications of these models for important&#x2F;practical tasks are largely untested&#x2F;untuned compared to the rather mature applications of attention, so there may be a little delay before people figure out all the tricks for how to use large pre-trained models of these types.  The analogy to the old RNN helps to a degree, but people had super specialized to attention and transformers the last 5 years, so there is a lot of momentum in favor of transformers.</div><br/><div id="38717236" class="c"><input type="checkbox" id="c-38717236" checked=""/><div class="controls bullet"><span class="by">adhi01</span><span>|</span><a href="#38710356">root</a><span>|</span><a href="#38715842">parent</a><span>|</span><a href="#38711246">next</a><span>|</span><label class="collapse" for="c-38717236">[-]</label><label class="expand" for="c-38717236">[1 more]</label></div><br/><div class="children"><div class="content">Can you cite what&#x27;s the &quot;Based&quot; paper in here.</div><br/></div></div></div></div><div id="38711246" class="c"><input type="checkbox" id="c-38711246" checked=""/><div class="controls bullet"><span class="by">pk-protect-ai</span><span>|</span><a href="#38710356">parent</a><span>|</span><a href="#38715842">prev</a><span>|</span><a href="#38712937">next</a><span>|</span><label class="collapse" for="c-38711246">[-]</label><label class="expand" for="c-38711246">[2 more]</label></div><br/><div class="children"><div class="content">&gt; is Mamba potentially more compute-efficient to train for a similar size of model&#x2F;dataset?<p>I would like to understand it too as well ...<p>Here is the citation from original paper:<p>&quot;Computation. After the parameters have been transformed from (∆, A, B, C) ↦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3). Commonly, the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where the inputs are seen one timestep at a time).&quot;<p>So the training is parallelizable, like in RetNet with parallel forward mode.
By default inference is done in the recurrent mode, to have a longest possible context. No chunking available, so it is difficult for me to say how much RAM and VRAM it will consume during the inference ...</div><br/><div id="38715387" class="c"><input type="checkbox" id="c-38715387" checked=""/><div class="controls bullet"><span class="by">pk-protect-ai</span><span>|</span><a href="#38710356">root</a><span>|</span><a href="#38711246">parent</a><span>|</span><a href="#38712937">next</a><span>|</span><label class="collapse" for="c-38715387">[-]</label><label class="expand" for="c-38715387">[1 more]</label></div><br/><div class="children"><div class="content">I did some minimal testing, mamba uses about 60% of VRAM in comparison to RetNet (parallel forward mode) with the model of the same size and the vocabulary of same size during inference.</div><br/></div></div></div></div><div id="38712937" class="c"><input type="checkbox" id="c-38712937" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#38710356">parent</a><span>|</span><a href="#38711246">prev</a><span>|</span><a href="#38714839">next</a><span>|</span><label class="collapse" for="c-38712937">[-]</label><label class="expand" for="c-38712937">[1 more]</label></div><br/><div class="children"><div class="content">I think this video is exactly what you’re looking for.<p>He explains the paper but also gives a lot of context, how it fits into the big picture, etc.<p>It’s actual kind of exciting hearing the plot unfold.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;ouF-H35atOY?si=y2Ckp9MCFd7ulLL3" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;ouF-H35atOY?si=y2Ckp9MCFd7ulLL3</a></div><br/></div></div><div id="38714839" class="c"><input type="checkbox" id="c-38714839" checked=""/><div class="controls bullet"><span class="by">opprobium</span><span>|</span><a href="#38710356">parent</a><span>|</span><a href="#38712937">prev</a><span>|</span><a href="#38712671">next</a><span>|</span><label class="collapse" for="c-38714839">[-]</label><label class="expand" for="c-38714839">[1 more]</label></div><br/><div class="children"><div class="content">Re 3) Even if you don&#x27;t care about long context length, Mamba is much cheaper per token of auto-regressive output. Each token has to only compute the next step of a linear RNN, the transformer has to attend back over all previous outputs, which rapidly grows in cost and memory.</div><br/></div></div><div id="38712671" class="c"><input type="checkbox" id="c-38712671" checked=""/><div class="controls bullet"><span class="by">sjkoelle</span><span>|</span><a href="#38710356">parent</a><span>|</span><a href="#38714839">prev</a><span>|</span><a href="#38710788">next</a><span>|</span><label class="collapse" for="c-38712671">[-]</label><label class="expand" for="c-38712671">[1 more]</label></div><br/><div class="children"><div class="content">my loose understanding<p>1) transformers create an input x input size attention matrix that is unnecessarily large.  state space models somehow compress this.<p>2) &quot;The main difference is simply making several parameters [in the state space model] functions of the input&quot;<p>3) i think it might be more sample efficient (requires less data)</div><br/></div></div><div id="38710788" class="c"><input type="checkbox" id="c-38710788" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#38710356">parent</a><span>|</span><a href="#38712671">prev</a><span>|</span><a href="#38709750">next</a><span>|</span><label class="collapse" for="c-38710788">[-]</label><label class="expand" for="c-38710788">[1 more]</label></div><br/><div class="children"><div class="content">For 2, Mamba makes some A B C weights that in S4 are time invariant become functions of the input, which makes it more powerful.</div><br/></div></div></div></div><div id="38709750" class="c"><input type="checkbox" id="c-38709750" checked=""/><div class="controls bullet"><span class="by">boredumb</span><span>|</span><a href="#38710356">prev</a><span>|</span><a href="#38713189">next</a><span>|</span><label class="collapse" for="c-38709750">[-]</label><label class="expand" for="c-38709750">[5 more]</label></div><br/><div class="children"><div class="content">&quot;Mamba is the world&#x27;s longest venomous snake with an estimated length of over 150 m&quot;<p>Had a laugh at that. Really great stuff though, it was nice to have referencing to the arxiv paper so someone like me who generally consumes these things instead of translating them from papers could sort of peak behind the curtains.</div><br/><div id="38710413" class="c"><input type="checkbox" id="c-38710413" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38709750">parent</a><span>|</span><a href="#38712479">next</a><span>|</span><label class="collapse" for="c-38710413">[-]</label><label class="expand" for="c-38710413">[2 more]</label></div><br/><div class="children"><div class="content">Mamba has a great name ... [S]elective [S]tructured [S]tate [S]pace [S]equence models.. makes sSSSS, like a snake</div><br/><div id="38711035" class="c"><input type="checkbox" id="c-38711035" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#38709750">root</a><span>|</span><a href="#38710413">parent</a><span>|</span><a href="#38712479">next</a><span>|</span><label class="collapse" for="c-38711035">[-]</label><label class="expand" for="c-38711035">[1 more]</label></div><br/><div class="children"><div class="content">If only the &quot;mamba&quot; name were not ugly.</div><br/></div></div></div></div><div id="38712479" class="c"><input type="checkbox" id="c-38712479" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#38709750">parent</a><span>|</span><a href="#38710413">prev</a><span>|</span><a href="#38713189">next</a><span>|</span><label class="collapse" for="c-38712479">[-]</label><label class="expand" for="c-38712479">[2 more]</label></div><br/><div class="children"><div class="content">Wait I thought that was the king cobra? The longest venomous snake ? At least that was what a simple Google search showed me.<p>Would be funny if they had to issue a correction for that sentence later on</div><br/><div id="38712608" class="c"><input type="checkbox" id="c-38712608" checked=""/><div class="controls bullet"><span class="by">fwip</span><span>|</span><a href="#38709750">root</a><span>|</span><a href="#38712479">parent</a><span>|</span><a href="#38713189">next</a><span>|</span><label class="collapse" for="c-38712608">[-]</label><label class="expand" for="c-38712608">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also not 150 meters long (nearly 500 feet), which I think is also part of why it was funny to include the sentence in the README.</div><br/></div></div></div></div></div></div><div id="38713189" class="c"><input type="checkbox" id="c-38713189" checked=""/><div class="controls bullet"><span class="by">iskander</span><span>|</span><a href="#38709750">prev</a><span>|</span><a href="#38712269">next</a><span>|</span><label class="collapse" for="c-38713189">[-]</label><label class="expand" for="c-38713189">[1 more]</label></div><br/><div class="children"><div class="content">I expected the core of the algorithm to be a parallel prefix scan though (isn&#x27;t that the point of Mamba?):<p><pre><code>    for i in range(l):
            x = deltaA[:, :, i] \* x + deltaB_u[:, :, i]
            y = einsum(x, C[:, i, :], &#x27;b d_in n , b n -&gt; b d_in&#x27;)
            ys.append(y)</code></pre></div><br/></div></div><div id="38712269" class="c"><input type="checkbox" id="c-38712269" checked=""/><div class="controls bullet"><span class="by">epaulson</span><span>|</span><a href="#38713189">prev</a><span>|</span><a href="#38711245">next</a><span>|</span><label class="collapse" for="c-38712269">[-]</label><label class="expand" for="c-38712269">[2 more]</label></div><br/><div class="children"><div class="content">This is a dumb question but how hard is it to train the mamba models that are on huggingface? It looks like the largest one is 2.8b - how many GPUs for how long do you need to train that up using a dataset like The Pile?</div><br/><div id="38717977" class="c"><input type="checkbox" id="c-38717977" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#38712269">parent</a><span>|</span><a href="#38711245">next</a><span>|</span><label class="collapse" for="c-38717977">[-]</label><label class="expand" for="c-38717977">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a great question and I would like to know too. It looks like the answer is substantially faster than an equally sized Transformer, and the end result will score better than a Transformer on basically every benchmark. Also it will do inference 3-5x faster in half the RAM.</div><br/></div></div></div></div><div id="38711245" class="c"><input type="checkbox" id="c-38711245" checked=""/><div class="controls bullet"><span class="by">DarmokJalad1701</span><span>|</span><a href="#38712269">prev</a><span>|</span><a href="#38711253">next</a><span>|</span><label class="collapse" for="c-38711245">[-]</label><label class="expand" for="c-38711245">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for this. I took a stab at unraveling the official CUDA version and never really got around to it after my initial attempt failed. This seems a lot nicer.</div><br/></div></div><div id="38711253" class="c"><input type="checkbox" id="c-38711253" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38711245">prev</a><span>|</span><a href="#38710983">next</a><span>|</span><label class="collapse" for="c-38711253">[-]</label><label class="expand" for="c-38711253">[3 more]</label></div><br/><div class="children"><div class="content">Oh my gosh, another one-file PyTorch implementation. This is fantastic. I&#x27;d like to hope that some of my previous work (hlb-CIFAR10 and related projects, along with other influences before it like minGPT, DawnBench, etc.) has been able to help push the &#x27;simple, single-file, reduced-complexity&#x27; format forward a bit. I personally think that this kind of work is critical to efficient ML research, and that is possibly one of the most important things that we can do for the field today.<p>Research progresses at the speed of innovation, which progresses with the inverse of experiment runtime, which is definitely and absolutely related to the underlying Kolmogorov Complexity of the code w.r.t. a research&#x2F;simple-hackery-focused objective.<p>I really cannot stress enough how important to research tools like this are and how much they&#x27;ve sped up the knowledge discovery process for me personally. Being able to quickly sketch out ideas, often in minutes, and get immediate, high-snr results back has become an indispensable part of my research progress. While we seem to really good at some of the specifics of some of the detailsresearch, and somehow have extremely information-efficient training processes, we have not applied the same logic seemingly on the whole to the entire research field!<p>Knowledge distillation and&#x2F;or the MDL (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minimum_description_length" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minimum_description_length</a>) are excessively important I think to reversing  a lot of the constant fluff, cruft, and overly dense thrash-and-hope-you-don&#x27;t-get-scooped-by-other-researchers-on-marginal-value-topics trend that I think has largely been encouraged by the current paper submission&#x2F;review&#x2F;etc process.<p>I&#x27;ve been wanting to try to get around this and move a bit more towards a slightly better scaling solution recently. One of these things is that I&#x27;ve started distributing my code in 1-file, self-contained, short rough gists as &#x27;code sketches&#x27;, which shortens dev time and gets rough, unpolished, working code for a concept in people&#x27;s hands. It seems to work pretty well so far, I hope to continue doing it! &lt;3 :&#x27;))))<p>In any case, this is extremely exciting stuff, and everyone -- please! More code like this! We&#x27;re researchers on learning data in a largely-scaled way, let&#x27;s be data-efficient in how we disseminate information as well! It&#x27;s a dream come true to see a lot more of this stuff coming down the pipeline, fantastic work and keep it coming! &lt;3 :&#x27;)))) Woop woop woop!!!!<p>Excellent stuff. &lt;3 :&#x27;))))</div><br/><div id="38711348" class="c"><input type="checkbox" id="c-38711348" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38711253">parent</a><span>|</span><a href="#38713334">next</a><span>|</span><label class="collapse" for="c-38711348">[-]</label><label class="expand" for="c-38711348">[1 more]</label></div><br/><div class="children"><div class="content">Minor potential performance benefit -- it looks like you might be able to fuse the x_proj and dt_proj weights here as x_proj has no bias. This is a thing that&#x27;s possibly doable simply at runtime if there&#x27;s any weight-fiddling reqs, I&#x27;m guessing the single kernel + bias will still run faster in the end (not sure though! &lt;3 :&#x27;)))) )</div><br/></div></div><div id="38713334" class="c"><input type="checkbox" id="c-38713334" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#38711253">parent</a><span>|</span><a href="#38711348">prev</a><span>|</span><a href="#38710983">next</a><span>|</span><label class="collapse" for="c-38713334">[-]</label><label class="expand" for="c-38713334">[1 more]</label></div><br/><div class="children"><div class="content">It’s been an exciting 2023 year in no small part because of watching AI research unfold at these crazy speeds. Like you’ve said, these enablers like ArXiV, PyTorch, GitHub, Huggingface, and terse Python code that’s open source are dramatically accelerating the development of this new field.<p>It’s probably the fastest the human race has ever developed anything of substantial complexity!<p>The only other place I see this king of velocity is SpaceX, which also launched two cutting edge rockets this year.<p>I wonder what 2024 will bring…</div><br/></div></div></div></div><div id="38710983" class="c"><input type="checkbox" id="c-38710983" checked=""/><div class="controls bullet"><span class="by">pk-protect-ai</span><span>|</span><a href="#38711253">prev</a><span>|</span><a href="#38711108">next</a><span>|</span><label class="collapse" for="c-38710983">[-]</label><label class="expand" for="c-38710983">[3 more]</label></div><br/><div class="children"><div class="content">Is there an original paper discussion? I seem to have missed it. It&#x27;s quite interesting. I didn&#x27;t catch on to this part:<p>&quot;We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that can also be interpreted as SSMs, due to a lack of efficient implementation leading to out-of-memory or unrealistic computation requirements.&quot;<p>RetNet doesn&#x27;t really consume much memory, and with the chunkwise forward implementation, it restricts the VRAM usage to the chunk size. This is the part to test the context length.<p>Has anyone done some tests on the original Mamba model? How fast is the training on this one in comparison with RetNet in parallel forward mode?</div><br/><div id="38711644" class="c"><input type="checkbox" id="c-38711644" checked=""/><div class="controls bullet"><span class="by">error9348</span><span>|</span><a href="#38710983">parent</a><span>|</span><a href="#38717980">next</a><span>|</span><label class="collapse" for="c-38711644">[-]</label><label class="expand" for="c-38711644">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38522428">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38522428</a><p><a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=AL1fq05o7H" rel="nofollow noreferrer">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=AL1fq05o7H</a></div><br/></div></div><div id="38717980" class="c"><input type="checkbox" id="c-38717980" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#38710983">parent</a><span>|</span><a href="#38711644">prev</a><span>|</span><a href="#38711108">next</a><span>|</span><label class="collapse" for="c-38717980">[-]</label><label class="expand" for="c-38717980">[1 more]</label></div><br/><div class="children"><div class="content">Faster training, much faster inference, and about half the VRAM usage during inference.</div><br/></div></div></div></div><div id="38711108" class="c"><input type="checkbox" id="c-38711108" checked=""/><div class="controls bullet"><span class="by">allanrbo</span><span>|</span><a href="#38710983">prev</a><span>|</span><a href="#38711702">next</a><span>|</span><label class="collapse" for="c-38711108">[-]</label><label class="expand" for="c-38711108">[1 more]</label></div><br/><div class="children"><div class="content">Love it when complex things are distilled down to just the essentials!</div><br/></div></div><div id="38711702" class="c"><input type="checkbox" id="c-38711702" checked=""/><div class="controls bullet"><span class="by">jdeaton</span><span>|</span><a href="#38711108">prev</a><span>|</span><a href="#38717494">next</a><span>|</span><label class="collapse" for="c-38711702">[-]</label><label class="expand" for="c-38711702">[3 more]</label></div><br/><div class="children"><div class="content">Very cool ive read this line of paper originating from hippo, s4, hyena, mamba etc but can someone please explain how this isnt just an RNN&#x2F;LSTM variant??</div><br/><div id="38711755" class="c"><input type="checkbox" id="c-38711755" checked=""/><div class="controls bullet"><span class="by">rhaps0dy</span><span>|</span><a href="#38711702">parent</a><span>|</span><a href="#38717494">next</a><span>|</span><label class="collapse" for="c-38711755">[-]</label><label class="expand" for="c-38711755">[2 more]</label></div><br/><div class="children"><div class="content">Its latent space transition is linear, instead of nonlinear, so there&#x27;s a more parallelizable algorithm for advancing time in it. This makes it much more efficient to train and do inference with in GPUs.<p>The way it keeps all the representation power of LSTMs is by having the transition vary with the input (but still be linear).</div><br/><div id="38712137" class="c"><input type="checkbox" id="c-38712137" checked=""/><div class="controls bullet"><span class="by">jdeaton</span><span>|</span><a href="#38711702">root</a><span>|</span><a href="#38711755">parent</a><span>|</span><a href="#38717494">next</a><span>|</span><label class="collapse" for="c-38712137">[-]</label><label class="expand" for="c-38712137">[1 more]</label></div><br/><div class="children"><div class="content">Thanks thats helpful. One place where the parallelizability of this method falls short of the transformer is not being able to pack multiple varying length examples into the same array during training with block diagonal attention pattern. If I understand correctly thats not possible with this architecture and its an important practical concern in large scale transformer training.</div><br/></div></div></div></div></div></div><div id="38717494" class="c"><input type="checkbox" id="c-38717494" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#38711702">prev</a><span>|</span><a href="#38710532">next</a><span>|</span><label class="collapse" for="c-38717494">[-]</label><label class="expand" for="c-38717494">[1 more]</label></div><br/><div class="children"><div class="content">Cool share!</div><br/></div></div><div id="38710532" class="c"><input type="checkbox" id="c-38710532" checked=""/><div class="controls bullet"><span class="by">mcemilg</span><span>|</span><a href="#38717494">prev</a><span>|</span><a href="#38713075">next</a><span>|</span><label class="collapse" for="c-38710532">[-]</label><label class="expand" for="c-38710532">[3 more]</label></div><br/><div class="children"><div class="content">Looks wonderful. But I would like to add this, I hate einops, it doesn&#x27;t make it simple to read unfortunately.</div><br/><div id="38711295" class="c"><input type="checkbox" id="c-38711295" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#38710532">parent</a><span>|</span><a href="#38712576">next</a><span>|</span><label class="collapse" for="c-38711295">[-]</label><label class="expand" for="c-38711295">[1 more]</label></div><br/><div class="children"><div class="content">I re-implemented Mamba myself and this was the first time I had ever worked with einops&#x2F;einsum. I&#x27;m 50&#x2F;50 on them after this. I found them relatively easy to look at and understand the intent (possibly more so than other representations), but talking extra time to transforms into other primitives (loops, multiplication, etc). I belive using torch.einsum is generally well optimized as well compared to naively looping. All said, I don&#x27;t know if I&#x27;d use it myself working from scratch but it&#x27;s interesting to know and if I was working in python I might try comparing the speed of einops&#x2F;sum vs other ways.</div><br/></div></div><div id="38712576" class="c"><input type="checkbox" id="c-38712576" checked=""/><div class="controls bullet"><span class="by">sjkoelle</span><span>|</span><a href="#38710532">parent</a><span>|</span><a href="#38711295">prev</a><span>|</span><a href="#38713075">next</a><span>|</span><label class="collapse" for="c-38712576">[-]</label><label class="expand" for="c-38712576">[1 more]</label></div><br/><div class="children"><div class="content">disagree</div><br/></div></div></div></div><div id="38713075" class="c"><input type="checkbox" id="c-38713075" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#38710532">prev</a><span>|</span><a href="#38717057">next</a><span>|</span><label class="collapse" for="c-38713075">[-]</label><label class="expand" for="c-38713075">[1 more]</label></div><br/><div class="children"><div class="content">Hm I&#x27;d take a stab at a Jax version based on this. Thanks</div><br/></div></div><div id="38717057" class="c"><input type="checkbox" id="c-38717057" checked=""/><div class="controls bullet"><span class="by">Nimitz14</span><span>|</span><a href="#38713075">prev</a><span>|</span><a href="#38712536">next</a><span>|</span><label class="collapse" for="c-38717057">[-]</label><label class="expand" for="c-38717057">[1 more]</label></div><br/><div class="children"><div class="content">Very nice. Love the glossary!</div><br/></div></div><div id="38712536" class="c"><input type="checkbox" id="c-38712536" checked=""/><div class="controls bullet"><span class="by">uejfiweun</span><span>|</span><a href="#38717057">prev</a><span>|</span><a href="#38711147">next</a><span>|</span><label class="collapse" for="c-38712536">[-]</label><label class="expand" for="c-38712536">[3 more]</label></div><br/><div class="children"><div class="content">How long does it generally take between model architectures like Mamba being proposed and the use of these architectures in SotA mega models like GPT or Gemini? IIUC Mamba basically eliminates restrictions on context length which would be awesome to see in the super-mega high performance models.</div><br/><div id="38714353" class="c"><input type="checkbox" id="c-38714353" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#38712536">parent</a><span>|</span><a href="#38711147">next</a><span>|</span><label class="collapse" for="c-38714353">[-]</label><label class="expand" for="c-38714353">[2 more]</label></div><br/><div class="children"><div class="content">GPT-5 would have this enhancement</div><br/><div id="38717997" class="c"><input type="checkbox" id="c-38717997" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#38712536">root</a><span>|</span><a href="#38714353">parent</a><span>|</span><a href="#38711147">next</a><span>|</span><label class="collapse" for="c-38717997">[-]</label><label class="expand" for="c-38717997">[1 more]</label></div><br/><div class="children"><div class="content">GPT-5 will not, because the T in GPT stands for Transformer and Mamba&#x2F;SSMs&#x2F;S6 are not Transformers.<p>But I would bet that we see a SOTA S6 LLM from Meta by this Spring.</div><br/></div></div></div></div></div></div><div id="38711147" class="c"><input type="checkbox" id="c-38711147" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#38712536">prev</a><span>|</span><a href="#38709535">next</a><span>|</span><label class="collapse" for="c-38711147">[-]</label><label class="expand" for="c-38711147">[2 more]</label></div><br/><div class="children"><div class="content">I love one file implementations. I hate all these implementations with preprocess_utils.py that imports stuff from model.py that imports stuff again from preprocess_utils.py that imports stuff from ...</div><br/><div id="38712809" class="c"><input type="checkbox" id="c-38712809" checked=""/><div class="controls bullet"><span class="by">fassssst</span><span>|</span><a href="#38711147">parent</a><span>|</span><a href="#38709535">next</a><span>|</span><label class="collapse" for="c-38712809">[-]</label><label class="expand" for="c-38712809">[1 more]</label></div><br/><div class="children"><div class="content">Feels like a useful preprocessor script: turn this repo into a single file</div><br/></div></div></div></div><div id="38709535" class="c"><input type="checkbox" id="c-38709535" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#38711147">prev</a><span>|</span><a href="#38710239">next</a><span>|</span><label class="collapse" for="c-38709535">[-]</label><label class="expand" for="c-38709535">[9 more]</label></div><br/><div class="children"><div class="content">This looks <i>really nice</i>. Thank you for sharing it on HN!<p>In case you didn&#x27;t know, you can parallelize the slow Python loop in <i>selective_scan</i> that computes all the x&#x27;s:<p><pre><code>  x = torch.zeros((b, d_in, n))
  for i in range(l):
      x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
      ⋮ 
</code></pre>
with only two calls to the PyTorch API. See the examples here: <a href="https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_sequence&#x2F;blob&#x2F;main&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;glassroom&#x2F;heinsen_sequence&#x2F;blob&#x2F;main&#x2F;READ...</a> .[a]<p>You can then compute all the y&#x27;s with one einsum, instead of l sequential einsums.<p>---<p>[a] Previous discussion on HN: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38556669">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38556669</a></div><br/><div id="38712673" class="c"><input type="checkbox" id="c-38712673" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#38709535">parent</a><span>|</span><a href="#38709970">next</a><span>|</span><label class="collapse" for="c-38712673">[-]</label><label class="expand" for="c-38712673">[5 more]</label></div><br/><div class="children"><div class="content">OP&#x27;s code is much easier to understand, though, which is the main (only) purpose of their code</div><br/><div id="38712722" class="c"><input type="checkbox" id="c-38712722" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#38709535">root</a><span>|</span><a href="#38712673">parent</a><span>|</span><a href="#38709970">next</a><span>|</span><label class="collapse" for="c-38712722">[-]</label><label class="expand" for="c-38712722">[4 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t argue with that! :-)<p>For what it&#x27;s worth, you can keep both, and make parallel vs sequential execution an option, with a boolean flag.<p>You can also leave the sequential code as a comment explaining what the parallel code does.<p>Or, if slow execution doesn&#x27;t bother you, leave it as is.</div><br/><div id="38713118" class="c"><input type="checkbox" id="c-38713118" checked=""/><div class="controls bullet"><span class="by">bradfitz</span><span>|</span><a href="#38709535">root</a><span>|</span><a href="#38712722">parent</a><span>|</span><a href="#38709970">next</a><span>|</span><label class="collapse" for="c-38713118">[-]</label><label class="expand" for="c-38713118">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re replying to somebody who was arguing for readability being its virtue and you&#x27;re proposing ... adding options and alternate code paths? :)</div><br/><div id="38713569" class="c"><input type="checkbox" id="c-38713569" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#38709535">root</a><span>|</span><a href="#38713118">parent</a><span>|</span><a href="#38713249">next</a><span>|</span><label class="collapse" for="c-38713569">[-]</label><label class="expand" for="c-38713569">[1 more]</label></div><br/><div class="children"><div class="content"><i>Touché.</i> I just updated my comment :-)</div><br/></div></div><div id="38713249" class="c"><input type="checkbox" id="c-38713249" checked=""/><div class="controls bullet"><span class="by">anytime5704</span><span>|</span><a href="#38709535">root</a><span>|</span><a href="#38713118">parent</a><span>|</span><a href="#38713569">prev</a><span>|</span><a href="#38709970">next</a><span>|</span><label class="collapse" for="c-38713249">[-]</label><label class="expand" for="c-38713249">[1 more]</label></div><br/><div class="children"><div class="content">Via a boolean parameter, no less.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38710239" class="c"><input type="checkbox" id="c-38710239" checked=""/><div class="controls bullet"><span class="by">y42</span><span>|</span><a href="#38709535">prev</a><span>|</span><a href="#38713350">next</a><span>|</span><label class="collapse" for="c-38710239">[-]</label><label class="expand" for="c-38710239">[41 more]</label></div><br/><div class="children"><div class="content">slightly OT:<p>I really struggle with dozens and dozens of vocabulary that is being used in the field of machine learning and especially AI. I&#x27;m not a beginner  at all, but I wonder if there is a comprehensive guide for all those terms that not  necessarily explains the technology behind them in detail, but shows their position and relation to each other. like some kind of landscape.<p>&quot;everyone&quot; seems to know Mamba. I never heard of Mamba. There are constantly new kind of llm popping up, talking about stuff that seems to be  obvious.<p>So, is there some kind of resource like that, not aiming at beginners, but experienced users, coming from other fields of IT?</div><br/><div id="38710455" class="c"><input type="checkbox" id="c-38710455" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710287">next</a><span>|</span><label class="collapse" for="c-38710455">[-]</label><label class="expand" for="c-38710455">[9 more]</label></div><br/><div class="children"><div class="content">In fast evolving fields it’s always all about sociology, not canon or pedagogy. Meaning in new fields is created in community (constructionism).<p>You need to plug into the community and overhear what people are talking about (HN is such a community). You’ll also get a sense of the linguistic subculture (acronyms, lingo etc) much like you learn to talk hip hop if you’re into the hip hop subculture.  Much of it will be noise but overall you’ll get a sense of what the community cares about, which helps you narrow what you need to focus on. The subreddit r&#x2F;localllama is the watering hole for hobbyists right now.<p>If you need a primer, this is a good guide.<p><a href="https:&#x2F;&#x2F;flyte.org&#x2F;blog&#x2F;getting-started-with-large-language-models-key-things-to-know" rel="nofollow noreferrer">https:&#x2F;&#x2F;flyte.org&#x2F;blog&#x2F;getting-started-with-large-language-m...</a><p>In this particular case, I find it helpful to do syntopical reading (per Mortimer Adler) around LLMs not AI in general. Mamba is interesting to me because I have a background in optimal control and state space models are my bread an butter and it’s fascinating to see them applied in this way.<p>Side: I’m in my 40s and this isn’t my first rodeo. There will always be new fields and trends emerging — I’ve been through several waves of this (cloud, big data, ML, data science etc) where posts like yours are commonplace. But there is no need to be frustrated. Overhearing conversations is one way to make sense of them instead of feeling lost and waiting for someone to summarize and explain everything to you.<p>The same applies to academic fields.<p>Ps also consider you might not need to be on the cutting edge. If you’re not trying to build leading edge stuff, it’s good to wait for the dust to settle — you’ll waste less time following dead ends while the community is figuring out what’s good.</div><br/><div id="38711068" class="c"><input type="checkbox" id="c-38711068" checked=""/><div class="controls bullet"><span class="by">pshc</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710455">parent</a><span>|</span><a href="#38712857">next</a><span>|</span><label class="collapse" for="c-38711068">[-]</label><label class="expand" for="c-38711068">[7 more]</label></div><br/><div class="children"><div class="content">Perhaps the community at r&#x2F;localllama could train an LLM that knows about the latest developments and explains jargon and papers, updated weekly. Free idea for karma.</div><br/><div id="38711206" class="c"><input type="checkbox" id="c-38711206" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711068">parent</a><span>|</span><a href="#38712857">next</a><span>|</span><label class="collapse" for="c-38711206">[-]</label><label class="expand" for="c-38711206">[6 more]</label></div><br/><div class="children"><div class="content">Not a bad idea.<p>I actually read papers with the help of ChatGPT-4 and Claude. It helps me quickly understand papers that I don’t have a background in.<p>For instance when I see something I don’t understand I ask it “can you break that down for me?” Or “is this similar to (concept I know)?”<p>It’s the new way of doing syntopical reading — but faster and more efficient.<p>(For the uninitiated, it’s a technique from Mortimer Adler’s How to read a book)</div><br/><div id="38712040" class="c"><input type="checkbox" id="c-38712040" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711206">parent</a><span>|</span><a href="#38712550">next</a><span>|</span><label class="collapse" for="c-38712040">[-]</label><label class="expand" for="c-38712040">[1 more]</label></div><br/><div class="children"><div class="content">This is a great way to consume papers. If there’s one thing LLMs know, it’s machine learning literature!</div><br/></div></div><div id="38712550" class="c"><input type="checkbox" id="c-38712550" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711206">parent</a><span>|</span><a href="#38712040">prev</a><span>|</span><a href="#38712857">next</a><span>|</span><label class="collapse" for="c-38712550">[-]</label><label class="expand" for="c-38712550">[4 more]</label></div><br/><div class="children"><div class="content">How do you feed a recent arxiv paper directly to ChatGPT?</div><br/><div id="38712760" class="c"><input type="checkbox" id="c-38712760" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38712550">parent</a><span>|</span><a href="#38716789">next</a><span>|</span><label class="collapse" for="c-38712760">[-]</label><label class="expand" for="c-38712760">[1 more]</label></div><br/><div class="children"><div class="content">A few options are:<p>1. Select abstract or select all text then copy&#x2F;paste.<p>2. Save the PDF and upload with ChatGPT’s document feature.<p>3. Ask for it, “what’s that well known LLM paper about context and getting lost in the middle?”. It will web search as needed.<p>You can also do more than summarize. Ask about equations, ask it to make analogies, challenge the key findings as devil’s advocate to learn from different angles. Propose your own ideas.<p>Use voice to digest topics during your commute and ask tons of questions until you understand.</div><br/></div></div><div id="38716789" class="c"><input type="checkbox" id="c-38716789" checked=""/><div class="controls bullet"><span class="by">b4ke</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38712550">parent</a><span>|</span><a href="#38712760">prev</a><span>|</span><a href="#38712678">next</a><span>|</span><label class="collapse" for="c-38716789">[-]</label><label class="expand" for="c-38716789">[1 more]</label></div><br/><div class="children"><div class="content">You can visit the page and use edge browser copilot feature. It uses gpt4 and doesn’t cost anything ;)</div><br/></div></div><div id="38712678" class="c"><input type="checkbox" id="c-38712678" checked=""/><div class="controls bullet"><span class="by">Min0taur</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38712550">parent</a><span>|</span><a href="#38716789">prev</a><span>|</span><a href="#38712857">next</a><span>|</span><label class="collapse" for="c-38712678">[-]</label><label class="expand" for="c-38712678">[1 more]</label></div><br/><div class="children"><div class="content">If you have the + subscription you can upload pdfs directly&#x2F;ask it to ingest.</div><br/></div></div></div></div></div></div></div></div><div id="38712857" class="c"><input type="checkbox" id="c-38712857" checked=""/><div class="controls bullet"><span class="by">y42</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710455">parent</a><span>|</span><a href="#38711068">prev</a><span>|</span><a href="#38710287">next</a><span>|</span><label class="collapse" for="c-38712857">[-]</label><label class="expand" for="c-38712857">[1 more]</label></div><br/><div class="children"><div class="content">Good point, thanks for the link!
(one of the links there leads to this wonderful post: Highly recommended: <a href="http:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;</a>)</div><br/></div></div></div></div><div id="38710287" class="c"><input type="checkbox" id="c-38710287" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710455">prev</a><span>|</span><a href="#38710484">next</a><span>|</span><label class="collapse" for="c-38710287">[-]</label><label class="expand" for="c-38710287">[8 more]</label></div><br/><div class="children"><div class="content">&gt;&quot;everyone&quot; seems to know Mamba. I never heard of Mamba<p>Only the &quot;everybody who knows what mamba is&quot; are the ones upvoting and commenting. Think of all the people who ignore it. For me, Mamba is the faster version of Conda [1], and that&#x27;s why I clicked on the article.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;mamba-org&#x2F;mamba">https:&#x2F;&#x2F;github.com&#x2F;mamba-org&#x2F;mamba</a></div><br/><div id="38711032" class="c"><input type="checkbox" id="c-38711032" checked=""/><div class="controls bullet"><span class="by">3-cheese-sundae</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710287">parent</a><span>|</span><a href="#38711627">next</a><span>|</span><label class="collapse" for="c-38711032">[-]</label><label class="expand" for="c-38711032">[6 more]</label></div><br/><div class="children"><div class="content">Ah yes, Conda, definitely something else I&#x27;ve heard of.</div><br/><div id="38712436" class="c"><input type="checkbox" id="c-38712436" checked=""/><div class="controls bullet"><span class="by">supermatt</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711032">parent</a><span>|</span><a href="#38711131">next</a><span>|</span><label class="collapse" for="c-38712436">[-]</label><label class="expand" for="c-38712436">[1 more]</label></div><br/><div class="children"><div class="content">Its extremely common to manage python environments with conda (although it can do much more). If you are unaware of conda, it is unlikely you work with python, and therefore unlikely to be doing much with ML (and LLMs) anyway - its even part of the &quot;getting started&quot; documentation for pytorch.</div><br/></div></div><div id="38711131" class="c"><input type="checkbox" id="c-38711131" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711032">parent</a><span>|</span><a href="#38712436">prev</a><span>|</span><a href="#38711239">next</a><span>|</span><label class="collapse" for="c-38711131">[-]</label><label class="expand" for="c-38711131">[1 more]</label></div><br/><div class="children"><div class="content">Conda has been around for a decade and it used to be the primary package manager for everything related to numpy&#x2F;scipy. Most ML and data science people have heard of it even if they haven&#x27;t used it.</div><br/></div></div><div id="38711239" class="c"><input type="checkbox" id="c-38711239" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711032">parent</a><span>|</span><a href="#38711131">prev</a><span>|</span><a href="#38711627">next</a><span>|</span><label class="collapse" for="c-38711239">[-]</label><label class="expand" for="c-38711239">[3 more]</label></div><br/><div class="children"><div class="content">Conda is the latest LLM cli frontend that&#x27;s a MOE of Mistral 7B, LLama 17B, Falcon 32C, and the Yamaha YZ50 quad bike.</div><br/><div id="38718059" class="c"><input type="checkbox" id="c-38718059" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711239">parent</a><span>|</span><a href="#38713287">next</a><span>|</span><label class="collapse" for="c-38718059">[-]</label><label class="expand" for="c-38718059">[1 more]</label></div><br/><div class="children"><div class="content">Mamba is a PoC of the latest SSM architecture for LLMs named S6 and is a dense counterpart to Transformers trained for 300B tokens of the Pile in sizes up to 2.7B. Mamba proves that S6 LLMs train faster, run faster, use less VRAM, result in lower perplexity and better benchmark scores with the same exact training data.<p>That is actually accurate but probably sounds just as outlandish.<p>The approachable version is: Mamba is a proof of concept language model which showcases a new LLM architecture called S6 which is a competitor to the Transformer architecture (the &#x27;T&#x27; in ChatGPT) and it is better in every measurable way.</div><br/></div></div><div id="38713287" class="c"><input type="checkbox" id="c-38713287" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711239">parent</a><span>|</span><a href="#38718059">prev</a><span>|</span><a href="#38711627">next</a><span>|</span><label class="collapse" for="c-38713287">[-]</label><label class="expand" for="c-38713287">[1 more]</label></div><br/><div class="children"><div class="content">&gt; and the Yamaha YZ50 quad bike.<p>Well played.</div><br/></div></div></div></div></div></div><div id="38711627" class="c"><input type="checkbox" id="c-38711627" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710287">parent</a><span>|</span><a href="#38711032">prev</a><span>|</span><a href="#38710484">next</a><span>|</span><label class="collapse" for="c-38711627">[-]</label><label class="expand" for="c-38711627">[1 more]</label></div><br/><div class="children"><div class="content">That is not &quot;a new LLVM architecture&quot;... It&#x27;s talking about a different Mamba.</div><br/></div></div></div></div><div id="38710484" class="c"><input type="checkbox" id="c-38710484" checked=""/><div class="controls bullet"><span class="by">countWSS</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710287">prev</a><span>|</span><a href="#38710322">next</a><span>|</span><label class="collapse" for="c-38710484">[-]</label><label class="expand" for="c-38710484">[2 more]</label></div><br/><div class="children"><div class="content">Its a new LLM type: instead of transformers it use state-space machines,
which are orders of magnitude faster.
Its currently very new and less coherent than GPT-2.</div><br/><div id="38711076" class="c"><input type="checkbox" id="c-38711076" checked=""/><div class="controls bullet"><span class="by">senseiV</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710484">parent</a><span>|</span><a href="#38710322">next</a><span>|</span><label class="collapse" for="c-38711076">[-]</label><label class="expand" for="c-38711076">[1 more]</label></div><br/><div class="children"><div class="content">? its better than GPT 2 for sure...</div><br/></div></div></div></div><div id="38710322" class="c"><input type="checkbox" id="c-38710322" checked=""/><div class="controls bullet"><span class="by">orbifold</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710484">prev</a><span>|</span><a href="#38710346">next</a><span>|</span><label class="collapse" for="c-38710322">[-]</label><label class="expand" for="c-38710322">[3 more]</label></div><br/><div class="children"><div class="content">It is a very fad driven field. Everyone brands everything. It isn&#x27;t enough to give things boring titles like, stacked open linear dynamical system with selective observations and learned timestep.</div><br/><div id="38713238" class="c"><input type="checkbox" id="c-38713238" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710322">parent</a><span>|</span><a href="#38712631">next</a><span>|</span><label class="collapse" for="c-38713238">[-]</label><label class="expand" for="c-38713238">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s half of it, the other half is pure social linguistics.<p>try talking about stacked open linear dynamical system for more than three times and you&#x27;re bound to figure out a token that conveys the same but  is quicker to produce<p>it&#x27;s turtles all the way down with LLM And your comment. people are just trying to maximize their token conversations</div><br/></div></div><div id="38712631" class="c"><input type="checkbox" id="c-38712631" checked=""/><div class="controls bullet"><span class="by">Ar-Curunir</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710322">parent</a><span>|</span><a href="#38713238">prev</a><span>|</span><a href="#38710346">next</a><span>|</span><label class="collapse" for="c-38712631">[-]</label><label class="expand" for="c-38712631">[1 more]</label></div><br/><div class="children"><div class="content">I mean, Mamba is much easier to remember than what you said. It’s good to have short names for techniques.</div><br/></div></div></div></div><div id="38710346" class="c"><input type="checkbox" id="c-38710346" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710322">prev</a><span>|</span><a href="#38710365">next</a><span>|</span><label class="collapse" for="c-38710346">[-]</label><label class="expand" for="c-38710346">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know Mamba but the bottom of the page lists comprehensive references.<p>If you mean the &quot;branding&quot; that is common in ML, which is often criticized, I much prefer it over the jargon used in other fields, e.g. Mathematics. It is nice to have distinguished words to talk about different concepts.</div><br/></div></div><div id="38710365" class="c"><input type="checkbox" id="c-38710365" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710346">prev</a><span>|</span><a href="#38712177">next</a><span>|</span><label class="collapse" for="c-38710365">[-]</label><label class="expand" for="c-38710365">[4 more]</label></div><br/><div class="children"><div class="content">&gt;  I never heard of Mamba.<p>Just came out a few days ago. It&#x27;s new for everyone.</div><br/><div id="38710934" class="c"><input type="checkbox" id="c-38710934" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710365">parent</a><span>|</span><a href="#38712177">next</a><span>|</span><label class="collapse" for="c-38710934">[-]</label><label class="expand" for="c-38710934">[3 more]</label></div><br/><div class="children"><div class="content">Mamba is also the name of a package management system, similar to Conda.<p>Just to make it a little extra confusing :)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;mamba-org&#x2F;mamba">https:&#x2F;&#x2F;github.com&#x2F;mamba-org&#x2F;mamba</a></div><br/><div id="38711103" class="c"><input type="checkbox" id="c-38711103" checked=""/><div class="controls bullet"><span class="by">Tao3300</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710934">parent</a><span>|</span><a href="#38712177">next</a><span>|</span><label class="collapse" for="c-38711103">[-]</label><label class="expand" for="c-38711103">[2 more]</label></div><br/><div class="children"><div class="content">Should have picked a different snake, like... I dunno, Asp? Wait, no, not that one...</div><br/><div id="38712821" class="c"><input type="checkbox" id="c-38712821" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38711103">parent</a><span>|</span><a href="#38712177">next</a><span>|</span><label class="collapse" for="c-38712821">[-]</label><label class="expand" for="c-38712821">[1 more]</label></div><br/><div class="children"><div class="content">Python!</div><br/></div></div></div></div></div></div></div></div><div id="38712177" class="c"><input type="checkbox" id="c-38712177" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710365">prev</a><span>|</span><a href="#38710314">next</a><span>|</span><label class="collapse" for="c-38712177">[-]</label><label class="expand" for="c-38712177">[2 more]</label></div><br/><div class="children"><div class="content">The people that are constantly up to date on this stuff tend to be AI&#x2F;ML researchers and engineers. In academia, industry research groups, or startups.<p>They literally get paid to read papers, and implement models on a day-to-day basis.<p>I wouldn&#x27;t worry too much not being up to date or things sounding a bit foreign. The names themselves are just that, names, the models themselves tend to be incremental versions of some previous model.</div><br/><div id="38712245" class="c"><input type="checkbox" id="c-38712245" checked=""/><div class="controls bullet"><span class="by">toasted-subs</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38712177">parent</a><span>|</span><a href="#38710314">next</a><span>|</span><label class="collapse" for="c-38712245">[-]</label><label class="expand" for="c-38712245">[1 more]</label></div><br/><div class="children"><div class="content">Most of the startups I&#x27;ve chatted with seem to prioritize finding people who build products. The complaint&#x2F;regret I&#x27;ve heard from 3-5 organizations was hiring researchers.<p>Researcher is more for highly funded organizations. Starrups can get by with off the shelf models.</div><br/></div></div></div></div><div id="38710314" class="c"><input type="checkbox" id="c-38710314" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38712177">prev</a><span>|</span><a href="#38710758">next</a><span>|</span><label class="collapse" for="c-38710314">[-]</label><label class="expand" for="c-38710314">[2 more]</label></div><br/><div class="children"><div class="content">the field just moves fast. I have curated a list of non-hypey writers and youtubers who explain these things for a typical SWE audience if you are interested. <a href="https:&#x2F;&#x2F;github.com&#x2F;swyxio&#x2F;ai-notes&#x2F;blob&#x2F;main&#x2F;Resources&#x2F;Good%20AI%20Podcasts%20and%20Newsletters.md">https:&#x2F;&#x2F;github.com&#x2F;swyxio&#x2F;ai-notes&#x2F;blob&#x2F;main&#x2F;Resources&#x2F;Good%...</a></div><br/><div id="38712800" class="c"><input type="checkbox" id="c-38712800" checked=""/><div class="controls bullet"><span class="by">y42</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710314">parent</a><span>|</span><a href="#38710758">next</a><span>|</span><label class="collapse" for="c-38712800">[-]</label><label class="expand" for="c-38712800">[1 more]</label></div><br/><div class="children"><div class="content">Will check it, thank you!</div><br/></div></div></div></div><div id="38710758" class="c"><input type="checkbox" id="c-38710758" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710314">prev</a><span>|</span><a href="#38710300">next</a><span>|</span><label class="collapse" for="c-38710758">[-]</label><label class="expand" for="c-38710758">[2 more]</label></div><br/><div class="children"><div class="content">&gt; in the field of machine learning and especially AI<p>Sorry for getting semantical here, but isn&#x27;t ML a subfield of AI? In other words, I would have expected &quot;... in the field of machine learning and AI in general&quot;</div><br/><div id="38711134" class="c"><input type="checkbox" id="c-38711134" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38710239">root</a><span>|</span><a href="#38710758">parent</a><span>|</span><a href="#38710300">next</a><span>|</span><label class="collapse" for="c-38711134">[-]</label><label class="expand" for="c-38711134">[1 more]</label></div><br/><div class="children"><div class="content">AI is often being used recently for specifically <i>generative</i> AI, which is a subfield of machine learning, which is a subfield of AI in the broader sense.</div><br/></div></div></div></div><div id="38710300" class="c"><input type="checkbox" id="c-38710300" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710758">prev</a><span>|</span><a href="#38710340">next</a><span>|</span><label class="collapse" for="c-38710300">[-]</label><label class="expand" for="c-38710300">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not aware of such a glossary.<p>But I did notice the &quot;References&quot; section in the bottom of the README, which does explain what Mamba is by linking to the original paper: &quot;Mamba: Linear-Time Sequence Modeling with Selective State Spaces&quot; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752</a></div><br/></div></div><div id="38710340" class="c"><input type="checkbox" id="c-38710340" checked=""/><div class="controls bullet"><span class="by">yuppiepuppie</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710300">prev</a><span>|</span><a href="#38713953">next</a><span>|</span><label class="collapse" for="c-38710340">[-]</label><label class="expand" for="c-38710340">[1 more]</label></div><br/><div class="children"><div class="content">Heavily agree. Ive been following this space quite closely, like most people, only for the past year. But it seems to be still in its experimental phase which in turn brings academics and researchers who tend toward this type of language.</div><br/></div></div><div id="38713953" class="c"><input type="checkbox" id="c-38713953" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710340">prev</a><span>|</span><a href="#38710991">next</a><span>|</span><label class="collapse" for="c-38713953">[-]</label><label class="expand" for="c-38713953">[1 more]</label></div><br/><div class="children"><div class="content">Everybody doesn&#x27;t know Mamba. You can&#x27;t stay on top of everything in ML so stop trying. Since you asked, Mamba is a neural architecture based on structured state space models (SSMs) that aims to replace Transformers. For me right now just know that counts as staying on top of things. If I need to know more than that I can have the computer summarize it for me.</div><br/></div></div><div id="38710991" class="c"><input type="checkbox" id="c-38710991" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38713953">prev</a><span>|</span><a href="#38710308">next</a><span>|</span><label class="collapse" for="c-38710991">[-]</label><label class="expand" for="c-38710991">[1 more]</label></div><br/><div class="children"><div class="content">You are now in the loop! Your colleagues will think the same “this person how does he&#x2F;she keep up with all the LLM stuff?”.</div><br/></div></div><div id="38710308" class="c"><input type="checkbox" id="c-38710308" checked=""/><div class="controls bullet"><span class="by">bananaflag</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710991">prev</a><span>|</span><a href="#38713168">next</a><span>|</span><label class="collapse" for="c-38710308">[-]</label><label class="expand" for="c-38710308">[1 more]</label></div><br/><div class="children"><div class="content">I knew about Mamba from r&#x2F;singularity and following AI researchers on Twitter.<p>I don&#x27;t work in AI at all (and don&#x27;t plan to), but it&#x27;s fun to know about stuff a little before they become mainstream.</div><br/></div></div><div id="38713168" class="c"><input type="checkbox" id="c-38713168" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#38710239">parent</a><span>|</span><a href="#38710308">prev</a><span>|</span><a href="#38713791">next</a><span>|</span><label class="collapse" for="c-38713168">[-]</label><label class="expand" for="c-38713168">[1 more]</label></div><br/><div class="children"><div class="content">Don’t feel bad, Mamba is <i>very new</i> technology. I only just heard about it for the first time last week!</div><br/></div></div></div></div><div id="38713350" class="c"><input type="checkbox" id="c-38713350" checked=""/><div class="controls bullet"><span class="by">ekiauhce</span><span>|</span><a href="#38710239">prev</a><span>|</span><a href="#38711074">next</a><span>|</span><label class="collapse" for="c-38713350">[-]</label><label class="expand" for="c-38713350">[3 more]</label></div><br/><div class="children"><div class="content">If a variable contains batch size, then name it accordingly — batch_size.<p>And no glossary needed, KISS<p><a href="https:&#x2F;&#x2F;github.com&#x2F;johnma2006&#x2F;mamba-minimal&#x2F;blob&#x2F;82efa90919c3b5066674216f3edcebb3414a7b8f&#x2F;model.py#L11">https:&#x2F;&#x2F;github.com&#x2F;johnma2006&#x2F;mamba-minimal&#x2F;blob&#x2F;82efa90919c...</a></div><br/><div id="38715537" class="c"><input type="checkbox" id="c-38715537" checked=""/><div class="controls bullet"><span class="by">DarmokJalad1701</span><span>|</span><a href="#38713350">parent</a><span>|</span><a href="#38713802">next</a><span>|</span><label class="collapse" for="c-38715537">[-]</label><label class="expand" for="c-38715537">[1 more]</label></div><br/><div class="children"><div class="content">I think the glossary is defining variable names as given in the paper. I found this confusing when I originally read the paper as the authors assume that the reader knows what B, L, D and N stand for. I had to use explainpaper to figure it out.</div><br/></div></div></div></div><div id="38711154" class="c"><input type="checkbox" id="c-38711154" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#38711074">prev</a><span>|</span><label class="collapse" for="c-38711154">[-]</label><label class="expand" for="c-38711154">[2 more]</label></div><br/><div class="children"><div class="content">Is number of files in a project a meaningful metric...?</div><br/><div id="38715732" class="c"><input type="checkbox" id="c-38715732" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#38711154">parent</a><span>|</span><label class="collapse" for="c-38715732">[-]</label><label class="expand" for="c-38715732">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it is here. This is an implementation designed for education: the main purpose here is to understand the model architecture in a practical sense.<p>So lines of code and number of files are both meaningful. This is 1 short Python file, which makes it a lot easier to understand than a full optimized implementation.</div><br/></div></div></div></div></div></div></div></div></div></body></html>