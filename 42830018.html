<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1738054866593" as="style"/><link rel="stylesheet" href="styles.css?v=1738054866593"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Ask HN: What do you use for content moderation of UGC?</a> </div><div class="subtext"><span>jhunter1016</span> | <span>14 comments</span></div><br/><div><div id="42850140" class="c"><input type="checkbox" id="c-42850140" checked=""/><div class="controls bullet"><span class="by">nbadg</span><span>|</span><a href="#42850258">next</a><span>|</span><label class="collapse" for="c-42850140">[-]</label><label class="expand" for="c-42850140">[1 more]</label></div><br/><div class="children"><div class="content">Follow-up question: what work has been done on client-side moderation? I know this gets dangerously close to the kind of content scanning that eg apple has tried (to very detrimental results), but hear me out: I really think this is a prerequisite to end-to-end encryption on a social network (there has to be some level of protection; even if 100% of users report 100% of bad content, imagine scrolling a feed and stumbling upon CSAM simply because you were the first person to see it). I also also think it&#x27;s possible to strike a balance that preserves user agency while still protecting them, by simply inserting a manual reporting step. So, for example, potentially problematic content gets put behind an interstitial with a content warning and options to view, hide, report, etc. But again, this requires client-side content classification.<p>I&#x27;m aware of eg NSFWJS, which is a tensorflowJS model [1]. Is there anything else that, say, can also do violence&#x2F;gore detection?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;infinitered&#x2F;nsfwjs">https:&#x2F;&#x2F;github.com&#x2F;infinitered&#x2F;nsfwjs</a></div><br/></div></div><div id="42850258" class="c"><input type="checkbox" id="c-42850258" checked=""/><div class="controls bullet"><span class="by">AyyEye</span><span>|</span><a href="#42850140">prev</a><span>|</span><a href="#42849940">next</a><span>|</span><label class="collapse" for="c-42850258">[-]</label><label class="expand" for="c-42850258">[1 more]</label></div><br/><div class="children"><div class="content">We use a combo of AI and the cheapest African contractors we could find.</div><br/></div></div><div id="42849940" class="c"><input type="checkbox" id="c-42849940" checked=""/><div class="controls bullet"><span class="by">Freak_NL</span><span>|</span><a href="#42850258">prev</a><span>|</span><a href="#42850020">next</a><span>|</span><label class="collapse" for="c-42849940">[-]</label><label class="expand" for="c-42849940">[1 more]</label></div><br/><div class="children"><div class="content">Is a static site hosting platform required to proactively monitor which content paying users host in your jurisdiction?<p>Wouldn&#x27;t a solid set of processes to handle content complaints and knowing who your customers are in case the hosting country&#x27;s law enforcement has a case suffice?<p>Or are you having some free tier where users can anonymously upload stuff?<p>In the latter case — a free place to stash megabytes — you&#x27;ll need to detect password protected archives in addition to unencrypted content. Get ready for a perpetual game of whack-a-mole though.</div><br/></div></div><div id="42850020" class="c"><input type="checkbox" id="c-42850020" checked=""/><div class="controls bullet"><span class="by">BrunoBernardino</span><span>|</span><a href="#42849940">prev</a><span>|</span><a href="#42841664">next</a><span>|</span><label class="collapse" for="c-42850020">[-]</label><label class="expand" for="c-42850020">[1 more]</label></div><br/><div class="children"><div class="content">How serendipitous! I did an Ask HN last week [1] trying to get platform creators to talk about this without much success. In any case, I&#x27;ve built a solution for links and emails, with an API [2], in case that helps. No subscription and I&#x27;m happy to provide some credits for free, for you to test it. Reach out if you&#x27;re interested!<p>[1]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42780265">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42780265</a>
[2]: <a href="https:&#x2F;&#x2F;oxcheck.com&#x2F;safe-api" rel="nofollow">https:&#x2F;&#x2F;oxcheck.com&#x2F;safe-api</a></div><br/></div></div><div id="42841664" class="c"><input type="checkbox" id="c-42841664" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#42850020">prev</a><span>|</span><a href="#42833033">next</a><span>|</span><label class="collapse" for="c-42841664">[-]</label><label class="expand" for="c-42841664">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI&#x27;s moderation API<p><a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;moderation" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;moderation</a></div><br/></div></div><div id="42833033" class="c"><input type="checkbox" id="c-42833033" checked=""/><div class="controls bullet"><span class="by">brudgers</span><span>|</span><a href="#42841664">prev</a><span>|</span><a href="#42837095">next</a><span>|</span><label class="collapse" for="c-42833033">[-]</label><label class="expand" for="c-42833033">[1 more]</label></div><br/><div class="children"><div class="content">If you care about moderation, a lot of it has to be done manually. Manual moderation requires placing a high level of trust in the moderators. That means that either you pay them well enough to care, or you build a community which user-moderators will protect. Or both.<p>That makes <i>approximately</i> all <i>business</i> ideas to host user generated content non-viable. The conflict is dynamic and you are the Maginot Line...except that any breach of laws creates a potential attack by state enforcement agencies too.<p>To put it another way, ASCII files and a teletype were enough to see pictures of naked ladies. Good luck.</div><br/></div></div><div id="42837095" class="c"><input type="checkbox" id="c-42837095" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#42833033">prev</a><span>|</span><a href="#42849022">next</a><span>|</span><label class="collapse" for="c-42837095">[-]</label><label class="expand" for="c-42837095">[5 more]</label></div><br/><div class="children"><div class="content">I hate to be that guy.  But this seems like the perfect use case for an LLM.  First put content through your script and then through a decently prompted LLM.  Anything it catches, put in a queue for manual review.</div><br/><div id="42850064" class="c"><input type="checkbox" id="c-42850064" checked=""/><div class="controls bullet"><span class="by">notatoad</span><span>|</span><a href="#42837095">parent</a><span>|</span><a href="#42839618">next</a><span>|</span><label class="collapse" for="c-42850064">[-]</label><label class="expand" for="c-42850064">[3 more]</label></div><br/><div class="children"><div class="content">i&#x27;m pretty sure most content moderation strategies operate on a budget below what most LLMs would cost.<p>for any site hosting user-generated content, how efficiently you can run your moderation system is essentially your unique selling point - LLM is the baseline.</div><br/><div id="42850192" class="c"><input type="checkbox" id="c-42850192" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42837095">root</a><span>|</span><a href="#42850064">parent</a><span>|</span><a href="#42850158">next</a><span>|</span><label class="collapse" for="c-42850192">[-]</label><label class="expand" for="c-42850192">[1 more]</label></div><br/><div class="children"><div class="content">You may be shocked at how inexpensive some of the LLMs are these days.<p>Google Gemini 1.5 Flash 8B charges $0.04&#x2F;million input tokens and $0.15&#x2F;million output tokens.<p>If a piece of content that needs to be moderated is 1,000 tokens (that&#x27;s pretty long!) and you expect a 10 token return it will cost you 0.0039 cents - that&#x27;s not a dollar amount, that&#x27;s less than a 250th of a single cent.<p>So 1 cent will moderate 250 items of content. $1 will moderate 25,000 items of content.<p>LLMs are dirt cheap. You can play around with pricing across different models using my calculator here: <a href="https:&#x2F;&#x2F;tools.simonwillison.net&#x2F;llm-prices" rel="nofollow">https:&#x2F;&#x2F;tools.simonwillison.net&#x2F;llm-prices</a></div><br/></div></div><div id="42850158" class="c"><input type="checkbox" id="c-42850158" checked=""/><div class="controls bullet"><span class="by">anshumankmr</span><span>|</span><a href="#42837095">root</a><span>|</span><a href="#42850064">parent</a><span>|</span><a href="#42850192">prev</a><span>|</span><a href="#42839618">next</a><span>|</span><label class="collapse" for="c-42850158">[-]</label><label class="expand" for="c-42850158">[1 more]</label></div><br/><div class="children"><div class="content">you could use some cheaper LLMs for this, even GPT-3.5turbo could excel at this or the moderations API (but that in my experience was more trained to obey the laws of USA, for example guns were okay to be asked about but are pretty much illegal in my country). The simplest way is a blocklist of terms which was what we had previously used containing an insane number of terms beyond the regular abusive terms, but it would need some updating from time to time.</div><br/></div></div></div></div><div id="42839618" class="c"><input type="checkbox" id="c-42839618" checked=""/><div class="controls bullet"><span class="by">socrateslee</span><span>|</span><a href="#42837095">parent</a><span>|</span><a href="#42850064">prev</a><span>|</span><a href="#42849022">next</a><span>|</span><label class="collapse" for="c-42839618">[-]</label><label class="expand" for="c-42839618">[1 more]</label></div><br/><div class="children"><div class="content">I agree that LLM could do most of the moderation work. You could use a multi-modal LLM for image moderation.</div><br/></div></div></div></div></div></div></div></div></div></body></html>