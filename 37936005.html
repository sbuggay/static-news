<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697706062620" as="style"/><link rel="stylesheet" href="styles.css?v=1697706062620"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2310.02989">xVal: A continuous number encoding for large language models</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>og_kalu</span> | <span>7 comments</span></div><br/><div><div id="37937630" class="c"><input type="checkbox" id="c-37937630" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#37936490">next</a><span>|</span><label class="collapse" for="c-37937630">[-]</label><label class="expand" for="c-37937630">[2 more]</label></div><br/><div class="children"><div class="content">So, this is a fairly interesting innovation. With what looks like really good results. The paper sort of buries the lede IMO, in that xVal looks like it could <i>possibly</i> be something that turns next-gen LLMs into zero-shot numeric prediction models.<p>The big idea is to take any corpus and instead of making tokenizations for numbers that are digits (GPT-2 era) or weirdo floating point range things, instead every number goes to a single token: [NUM]. They then keep a shadow tensor&#x2F;vector where the [NUM] is given an actual floating point (maybe fixed point?) number.<p>When the model predicts a [NUM] token, there&#x27;s a number prediction layer that chooses a number. This lets them train against predicting whether or not a number will be next in a generated text (the model turns out to be really good at this, not surprising). And, how cool -- their loss function can check the actual number created by the number layer, and give a high quality result as to how good the number guess was.<p>This works really really well out of the box for a bunch of math problems up to 5 digit multiplication, like super human levels of accuracy, and vastly beats SOTA from other Foundation models.<p>They then go on to throw fine tuning tasks of scientific data at it, and it absolutely does <i>not</i> shit the bed. Which is extremely profound to me, and something they do not make a big deal of. I would expect some scaled up models using this tech could be generally highly useful for a broad range of numeric &#x2F; stats &#x2F; prediction in the same way that GPT-3 and on have been highly useful as &quot;text calculators&quot;.<p>Worth a read.</div><br/><div id="37939653" class="c"><input type="checkbox" id="c-37939653" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#37937630">parent</a><span>|</span><a href="#37936490">next</a><span>|</span><label class="collapse" for="c-37939653">[-]</label><label class="expand" for="c-37939653">[1 more]</label></div><br/><div class="children"><div class="content">Err... interesting, maybe -- useful? I don&#x27;t see why.<p>There are a near infinite number of function approximators to choose from, and theory-building science has little use for any. Empirical, or heuristic, science already uses much faster and arbitrarily accurate ones.</div><br/></div></div></div></div><div id="37938746" class="c"><input type="checkbox" id="c-37938746" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#37936490">prev</a><span>|</span><a href="#37936456">next</a><span>|</span><label class="collapse" for="c-37938746">[-]</label><label class="expand" for="c-37938746">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how this is useful. People don&#x27;t calculate numbers in brain, they learned to use calculator. LLM should just learn to access external API every time they need to make a calculation instead of spending precious neurones on primitive tech already solved by transistors.</div><br/><div id="37938870" class="c"><input type="checkbox" id="c-37938870" checked=""/><div class="controls bullet"><span class="by">alexcannan</span><span>|</span><a href="#37938746">parent</a><span>|</span><a href="#37936456">next</a><span>|</span><label class="collapse" for="c-37938870">[-]</label><label class="expand" for="c-37938870">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m personally excited about LLMs becoming more number-fluent for regression tasks, e.g. read this paper and give it a grade 0-100</div><br/></div></div></div></div><div id="37936456" class="c"><input type="checkbox" id="c-37936456" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#37938746">prev</a><span>|</span><label class="collapse" for="c-37936456">[-]</label><label class="expand" for="c-37936456">[1 more]</label></div><br/><div class="children"><div class="content">Submitted title was &quot;Single Digit Tokenization improves LLM Math abilities by up to 70x&quot;.<p>Submitters: &quot;<i>Please use the original title, unless it is misleading or linkbait; don&#x27;t editorialize.</i>&quot; - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html</a><p>If you want to say what you think is important about an article, that&#x27;s fine, but do it by adding a comment to the thread. Then your view will be on a level playing field with everyone else&#x27;s: <a href="https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&amp;page=0&amp;prefix=false&amp;sort=byDate&amp;type=comment&amp;query=%22level%20playing%20field%22%20by:dang" rel="nofollow noreferrer">https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&amp;page=0&amp;prefix=false&amp;so...</a></div><br/></div></div></div></div></div></div></div></body></html>