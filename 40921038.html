<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720602064561" as="style"/><link rel="stylesheet" href="styles.css?v=1720602064561"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://aiencoder.substack.com/p/graphrag-analysis-part-1-how-indexing">Knowledge Graphs in RAG: Hype vs. Ragas Analysis</a> <span class="domain">(<a href="https://aiencoder.substack.com">aiencoder.substack.com</a>)</span></div><div class="subtext"><span>rooftopzen</span> | <span>12 comments</span></div><br/><div><div id="40924009" class="c"><input type="checkbox" id="c-40924009" checked=""/><div class="controls bullet"><span class="by">piizei</span><span>|</span><a href="#40923699">next</a><span>|</span><label class="collapse" for="c-40924009">[-]</label><label class="expand" for="c-40924009">[2 more]</label></div><br/><div class="children"><div class="content">Looks like the test-setup confuses knowledge graphs with graph databases. The code just creates a neo4j database from a document, not a knowledge graph (basically uses neo4j as vector database). A knowledge graph would be created by a LLM as a preprocessing step (and queried similary by an LLM). This is a different approach than was tested, an approach that trades preprocessing time and domain knowledge for accuracy. Reference: <a href="https:&#x2F;&#x2F;python.langchain.com&#x2F;v0.1&#x2F;docs&#x2F;use_cases&#x2F;graph&#x2F;constructing&#x2F;" rel="nofollow">https:&#x2F;&#x2F;python.langchain.com&#x2F;v0.1&#x2F;docs&#x2F;use_cases&#x2F;graph&#x2F;const...</a></div><br/><div id="40924351" class="c"><input type="checkbox" id="c-40924351" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#40924009">parent</a><span>|</span><a href="#40923699">next</a><span>|</span><label class="collapse" for="c-40924351">[-]</label><label class="expand" for="c-40924351">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I think the dataset is flawed. GraphRAG appears to be aimed at navigating the Microsoft 365 document and people graph that you get in an organization setting, not doing a homogenous search.</div><br/></div></div></div></div><div id="40923699" class="c"><input type="checkbox" id="c-40923699" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40924009">prev</a><span>|</span><a href="#40923684">next</a><span>|</span><label class="collapse" for="c-40923699">[-]</label><label class="expand" for="c-40923699">[1 more]</label></div><br/><div class="children"><div class="content">The Microsoft GraphRAG paper focuses on global sensemaking through hierarchical summarization, which is a fundamental aspect of their approach. The blog post analysis, however, doesn&#x27;t address this core feature at all. Another issue is the corpus size, the paper focuses on sizes on the order of 1M tokens, while the reference text used in the blog post is probably shorter. On shorter text a simple LLM call could do summarization directly.</div><br/></div></div><div id="40923684" class="c"><input type="checkbox" id="c-40923684" checked=""/><div class="controls bullet"><span class="by">itkovian_</span><span>|</span><a href="#40923699">prev</a><span>|</span><a href="#40923506">next</a><span>|</span><label class="collapse" for="c-40923684">[-]</label><label class="expand" for="c-40923684">[4 more]</label></div><br/><div class="children"><div class="content">Knowledge graphs where created to solve the problem of making natural,free flowing text machine processable. We now have a technology that completely understands natural free flowing text and can extract meaning. Why would going back to structure help when that structure can never be as rich as just text. I get it if the kb has new information, that&#x27;s not what I&#x27;m saying.</div><br/><div id="40923731" class="c"><input type="checkbox" id="c-40923731" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40923684">parent</a><span>|</span><a href="#40923722">next</a><span>|</span><label class="collapse" for="c-40923731">[-]</label><label class="expand" for="c-40923731">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why would going back to structure help<p>When your corpus is large it is useful to split it up and hierarchically combine. In their place I would do both bottom-up and top-down summarization passes, so information can percolate from a leaf to the root and from the root to a different leaf. Global context can illuminate local summaries, for example think of the twist in a novel, it sheds new light on everything.</div><br/></div></div><div id="40923722" class="c"><input type="checkbox" id="c-40923722" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40923684">parent</a><span>|</span><a href="#40923731">prev</a><span>|</span><a href="#40924026">next</a><span>|</span><label class="collapse" for="c-40923722">[-]</label><label class="expand" for="c-40923722">[1 more]</label></div><br/><div class="children"><div class="content">But RAG without graphs just relies on similarity search, which isn&#x27;t very smart.</div><br/></div></div></div></div><div id="40923506" class="c"><input type="checkbox" id="c-40923506" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#40923684">prev</a><span>|</span><a href="#40924201">next</a><span>|</span><label class="collapse" for="c-40923506">[-]</label><label class="expand" for="c-40923506">[2 more]</label></div><br/><div class="children"><div class="content">This is a nice sandbox walkthrough of the author&#x27;s objective which was to test MSFT claims in the paper -- but with all due respect the buzz of graphs is because they add whole third layer in a combined approach like Reciprocal Rank Fusion (RRF).  You do a BM25 search then you do a vector based nearest neighbors search and now you can add a KG search then all combined with local and global reranking etc the expectation is this produces a better final outcome.  These findings aside, it still makes sense that adding KG to a hybrid search pipeline is going to be useful.</div><br/></div></div><div id="40924201" class="c"><input type="checkbox" id="c-40924201" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#40923506">prev</a><span>|</span><a href="#40921039">next</a><span>|</span><label class="collapse" for="c-40924201">[-]</label><label class="expand" for="c-40924201">[1 more]</label></div><br/><div class="children"><div class="content">I don’t believe the author read the GraphRAG paper as there is nothing in this “deep dive” that implements anything remotely close.</div><br/></div></div></div></div></div></div></div></body></html>