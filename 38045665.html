<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1698483652248" as="style"/><link rel="stylesheet" href="styles.css?v=1698483652248"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html">When gradient descent is a kernel method</a> <span class="domain">(<a href="https://cgad.ski">cgad.ski</a>)</span></div><div class="subtext"><span>cgadski</span> | <span>20 comments</span></div><br/><div><div id="38048229" class="c"><input type="checkbox" id="c-38048229" checked=""/><div class="controls bullet"><span class="by">zeec123</span><span>|</span><a href="#38047886">next</a><span>|</span><label class="collapse" for="c-38048229">[-]</label><label class="expand" for="c-38048229">[1 more]</label></div><br/><div class="children"><div class="content">You may be interested in the book Gaussian Processes for Machine Learning: <a href="http:&#x2F;&#x2F;gaussianprocess.org&#x2F;gpml&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;gaussianprocess.org&#x2F;gpml&#x2F;</a> (freely available)</div><br/></div></div><div id="38047886" class="c"><input type="checkbox" id="c-38047886" checked=""/><div class="controls bullet"><span class="by">eachro</span><span>|</span><a href="#38048229">prev</a><span>|</span><a href="#38047165">next</a><span>|</span><label class="collapse" for="c-38047886">[-]</label><label class="expand" for="c-38047886">[1 more]</label></div><br/><div class="children"><div class="content">This really brings me back to my days in college, when this was the exact sort of stuff that ML classes focused on. You could have an entire course on various interpretations and extensions of kernel based methods. I wonder how much these insights are worth anymore in the age of LLMs, deep neural networks. I havent kept up too much with the NTK literature but it seems like the theoretical understanding of kernel based methods, gaussian processes does not confer you any advantage in being a better modern ML (specifically working on LLMs) engineer, where the skill set is more heavily geared towards systems engineering and&#x2F;or devops for babysitting all your experiments.</div><br/></div></div><div id="38047165" class="c"><input type="checkbox" id="c-38047165" checked=""/><div class="controls bullet"><span class="by">yagyu</span><span>|</span><a href="#38047886">prev</a><span>|</span><a href="#38046663">next</a><span>|</span><label class="collapse" for="c-38047165">[-]</label><label class="expand" for="c-38047165">[2 more]</label></div><br/><div class="children"><div class="content">This seems like a young talent that we’ll see more of. I like your to the point writing style and obvious passion for mathematical clarity.  Keep it up and best wishes for your phd studies.</div><br/><div id="38047337" class="c"><input type="checkbox" id="c-38047337" checked=""/><div class="controls bullet"><span class="by">yagyu</span><span>|</span><a href="#38047165">parent</a><span>|</span><a href="#38046663">next</a><span>|</span><label class="collapse" for="c-38047337">[-]</label><label class="expand" for="c-38047337">[1 more]</label></div><br/><div class="children"><div class="content">I’d be interested in your thoughts on the case where the f_i are optimizable: f_i(t) = K(t, z_i), i=1..m &lt;&lt; N. Like the representer thm but much fewer terms than you have data points to fit.  The points z are usually called inducing points and may be optimized by gradient descent.<p>There is literature on approximating exact GP inference with (something like)  these objects when m &lt;&lt; N (variational inference).<p>However, I’m not aware of anyone drawing a clear picture of the other direction, starting from the optimization picture and explaining it in terms of inference, similar to what TFA does.<p>In TFA the number of functions is large, so the system is underdetermined. In the variational inference the system is overdetermined and I wonder what inference, if any, gradient descent does..<p>Caveat: 1am and a few drinks deep so if I’m not making sense that’s ok</div><br/></div></div></div></div><div id="38046663" class="c"><input type="checkbox" id="c-38046663" checked=""/><div class="controls bullet"><span class="by">archmaster</span><span>|</span><a href="#38047165">prev</a><span>|</span><a href="#38046418">next</a><span>|</span><label class="collapse" for="c-38046663">[-]</label><label class="expand" for="c-38046663">[1 more]</label></div><br/><div class="children"><div class="content">Interesting article at first glance, although I definitely have to reread when I&#x27;m actually... awake<p>Is that my Water.css color scheme from so many years ago I see? :)</div><br/></div></div><div id="38046418" class="c"><input type="checkbox" id="c-38046418" checked=""/><div class="controls bullet"><span class="by">globalnode</span><span>|</span><a href="#38046663">prev</a><span>|</span><a href="#38047103">next</a><span>|</span><label class="collapse" for="c-38046418">[-]</label><label class="expand" for="c-38046418">[3 more]</label></div><br/><div class="children"><div class="content">I had to ask gpt every line about the meaning of terms :D, i know im not the target audience, but i find this stuff interesting at a conceptual level.<p>Is it basically about using stats to improve on linear models?</div><br/><div id="38046435" class="c"><input type="checkbox" id="c-38046435" checked=""/><div class="controls bullet"><span class="by">smitty1e</span><span>|</span><a href="#38046418">parent</a><span>|</span><a href="#38047103">next</a><span>|</span><label class="collapse" for="c-38046435">[-]</label><label class="expand" for="c-38046435">[2 more]</label></div><br/><div class="children"><div class="content">I kept waiting for this kernel discussion to converge on a scheduler or memory manager improvement, but joy proved evasive.</div><br/></div></div></div></div><div id="38047103" class="c"><input type="checkbox" id="c-38047103" checked=""/><div class="controls bullet"><span class="by">bt1a</span><span>|</span><a href="#38046418">prev</a><span>|</span><a href="#38046436">next</a><span>|</span><label class="collapse" for="c-38047103">[-]</label><label class="expand" for="c-38047103">[1 more]</label></div><br/><div class="children"><div class="content">i can&#x27;t comprehend this, but i enjoyed your main page and seeing the quine shuffle as i zoomed in and out with my scrollwheel</div><br/></div></div><div id="38046436" class="c"><input type="checkbox" id="c-38046436" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#38047103">prev</a><span>|</span><a href="#38046626">next</a><span>|</span><label class="collapse" for="c-38046436">[-]</label><label class="expand" for="c-38046436">[2 more]</label></div><br/><div class="children"><div class="content">Excellent article and a really clear view on statistical model training dynamics. This perspective will no doubt contribute to the development of deep learning theory.<p>I&#x27;m interested especially in the lessons we can learn about the success of overparametrization. As mentioned at the beginning of the article:<p>&gt; To use the picturesque idea of a &quot;loss landscape&quot; over parameter space, our problem will have a ridge of equally performing parameters rather than just a single optimal peak.<p>It has always been my intuition that overparametrization makes this ridge an overwhelming statistical majority of the parameter space, which would explain the success in training. What is less clear, as mentioned at the end, is why it hedges against overfitting. Could it be that &quot;simple&quot; function combinations are also overwhelmingly statistically likely vs more complicated ones? I&#x27;m imagining a hypersphere-in-many-dimensions kind of situation, where the &quot;corners&quot; are just too sharp to stay in for long before descending back into the &quot;bulk&quot;.<p>Interested to hear others&#x27; perspectives or pointers to research on this in the context of a kernel-based interpretation. I hope understanding overparametrization may also go some way toward explaining the unreasonable effective of analog-based learning systems such as human brains.</div><br/><div id="38047281" class="c"><input type="checkbox" id="c-38047281" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#38046436">parent</a><span>|</span><a href="#38046626">next</a><span>|</span><label class="collapse" for="c-38047281">[-]</label><label class="expand" for="c-38047281">[1 more]</label></div><br/><div class="children"><div class="content">Reflecting a bit more on the article I think the key lies close to this notion, quoted from the article:<p>&gt; Since ker⁡Π can be described as the orthogonal complement to the set {Kti}, the orthogonal complement to ker⁡Π is exactly the closure of the span of the vectors Kti.<p>{Kti} is going to be <i>very</i> large in the overparametrized case, so the orthogonal complement will be small. Note also this part:<p>&gt; Because v is chosen with minimal norm [in the context of the corresponding RKHS], it cannot be made smaller by adjusting it by an element of ker⁡Π...<p>So it sounds like all the &quot;capacity&quot; is taken up by representing the function itself and seemingly paradoxically the parameters λi are <i>more</i> constrained by the implicit regularization imposed by gradient descent (hypothetically enforcing the minimal-norm constraint). So the parameter space of functions that can possibly fit is tiny. The rub in practical applications is many combinations of NN parameters can correspond to one set of parameters in this kernel space, so the connection between <i>p</i> and <i>λ</i> (via <i>f</i>?) seems key to understanding the core of the issue.</div><br/></div></div></div></div><div id="38046626" class="c"><input type="checkbox" id="c-38046626" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38046436">prev</a><span>|</span><a href="#38047200">next</a><span>|</span><label class="collapse" for="c-38046626">[-]</label><label class="expand" for="c-38046626">[6 more]</label></div><br/><div class="children"><div class="content">That feeling when the article is clearly high quality (so upvotes) but few have the expertise to say why (few comments).</div><br/><div id="38047098" class="c"><input type="checkbox" id="c-38047098" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38046626">parent</a><span>|</span><a href="#38047200">next</a><span>|</span><label class="collapse" for="c-38047098">[-]</label><label class="expand" for="c-38047098">[5 more]</label></div><br/><div class="children"><div class="content">I would need 20 hours hard study minimum to understand it at any level. I need the quanta mag version (or Karpathy video) before I comment.<p>Although excellent the article could start off with the practical implications (maybe it is faster GPT training?) so that at least there is a motivation.</div><br/><div id="38047419" class="c"><input type="checkbox" id="c-38047419" checked=""/><div class="controls bullet"><span class="by">virtualbluesky</span><span>|</span><a href="#38046626">root</a><span>|</span><a href="#38047098">parent</a><span>|</span><a href="#38047299">next</a><span>|</span><label class="collapse" for="c-38047419">[-]</label><label class="expand" for="c-38047419">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s quite a few takeaways that can be had without fully understanding the ah.. esoteria.<p>1. Gradient descent is path-dependent and doesn&#x27;t forget the initial conditions. Intuitively reasonable - the method can only make local decisions, and figures out &#x27;correct&#x27; by looking at the size of its steps. There&#x27;s no &#x27;right answer&#x27; to discover, and each initial condition follows a subtly different path to &#x27;slow enough&#x27;...<p>because...<p>2. With enough simplification the path taken by each optimization process can be modeled using a matrix (their covariance matrix, K) with defined properties. This acts as a curvature of the mathematical space, and has some side-effects like being able to use eigen-magic to justify why the optimization process locks some parameters in place quickly, but others take a long time to settle.<p>which is fine, but doesn&#x27;t help explain why wild over-fitting doesn&#x27;t plague high-dimensional models (would you even notice if it did?). Enter implicit regularization, stage left. And mostly passing me by on the way in, but:<p>3. Because they decided to use random noise to generate the functions they combined to solve their optimization problem there is an additional layer of interpretation that they put on the properties of the aforementioned matrix that imply the result will only use each constituent function &#x27;as necessary&#x27; (i.e. regularized, rather than wildly amplifying pairs of coefficients)<p>And then something something baysian, which I&#x27;m happy to admit I&#x27;m not across</div><br/><div id="38047701" class="c"><input type="checkbox" id="c-38047701" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38046626">root</a><span>|</span><a href="#38047419">parent</a><span>|</span><a href="#38047299">next</a><span>|</span><label class="collapse" for="c-38047701">[-]</label><label class="expand" for="c-38047701">[1 more]</label></div><br/><div class="children"><div class="content">Thanks that is a brilliant explainer!</div><br/></div></div></div></div><div id="38047299" class="c"><input type="checkbox" id="c-38047299" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#38046626">root</a><span>|</span><a href="#38047098">parent</a><span>|</span><a href="#38047419">prev</a><span>|</span><a href="#38047200">next</a><span>|</span><label class="collapse" for="c-38047299">[-]</label><label class="expand" for="c-38047299">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty close to the theory end of the theory-application spectrum, so it could touch on many aspects of implementation. I suggest it may inform architectural improvements and&#x2F;or better justifications for gradient descent training schedules.</div><br/><div id="38047698" class="c"><input type="checkbox" id="c-38047698" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38046626">root</a><span>|</span><a href="#38047299">parent</a><span>|</span><a href="#38047200">next</a><span>|</span><label class="collapse" for="c-38047698">[-]</label><label class="expand" for="c-38047698">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. I think when I saw kernel my mind jumped to the conclusion that it was a GPU kernel so was expecting something a bit more software engineeringy!</div><br/></div></div></div></div></div></div></div></div><div id="38047022" class="c"><input type="checkbox" id="c-38047022" checked=""/><div class="controls bullet"><span class="by">superhumanuser</span><span>|</span><a href="#38047200">prev</a><span>|</span><label class="collapse" for="c-38047022">[-]</label><label class="expand" for="c-38047022">[1 more]</label></div><br/><div class="children"><div class="content">Is scrolling incredibly jerky on mobile for anyone else?</div><br/></div></div></div></div></div></div></div></body></html>