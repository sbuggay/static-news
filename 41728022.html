<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728205269160" as="style"/><link rel="stylesheet" href="styles.css?v=1728205269160"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/when-data-is-missing-scientists-guess-then-guess-again-20241002/">Statisticians use a technique that leverages randomness to deal with the unknown</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>Duximo</span> | <span>32 comments</span></div><br/><div><div id="41755819" class="c"><input type="checkbox" id="c-41755819" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#41753954">next</a><span>|</span><label class="collapse" for="c-41755819">[-]</label><label class="expand" for="c-41755819">[1 more]</label></div><br/><div class="children"><div class="content">It reminds me somewhat of dithering in signal processing.</div><br/></div></div><div id="41753954" class="c"><input type="checkbox" id="c-41753954" checked=""/><div class="controls bullet"><span class="by">Jun8</span><span>|</span><a href="#41755819">prev</a><span>|</span><a href="#41755657">next</a><span>|</span><label class="collapse" for="c-41753954">[-]</label><label class="expand" for="c-41753954">[3 more]</label></div><br/><div class="children"><div class="content">Not one mention of the EM algorithm, which is, as far as I can understand, is being described here (<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Expectation%E2%80%93maximization_algorithm" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Expectation%E2%80%93maximiza...</a>). It has so many applications, among which is estimating number of clusters for a Gaussian mixture model.<p>An ELI5 intro: <a href="https:&#x2F;&#x2F;abidlabs.github.io&#x2F;EM-Algorithm&#x2F;" rel="nofollow">https:&#x2F;&#x2F;abidlabs.github.io&#x2F;EM-Algorithm&#x2F;</a></div><br/><div id="41754385" class="c"><input type="checkbox" id="c-41754385" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#41753954">parent</a><span>|</span><a href="#41754304">next</a><span>|</span><label class="collapse" for="c-41754385">[-]</label><label class="expand" for="c-41754385">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It has so many applications, among which is estimating number of clusters for a Gaussian mixture model<p>Any sources for that? As far as I remember, EM is used to calculate actual cluster parameters (means, covariances etc), but I&#x27;m not aware of any usage to estimate what number of clusters works best.<p>Source: I&#x27;ve implemented EM for GMMs for a college assignment once, but I&#x27;m a bit hazy on the details.</div><br/></div></div><div id="41754304" class="c"><input type="checkbox" id="c-41754304" checked=""/><div class="controls bullet"><span class="by">Sniffnoy</span><span>|</span><a href="#41753954">parent</a><span>|</span><a href="#41754385">prev</a><span>|</span><a href="#41755657">next</a><span>|</span><label class="collapse" for="c-41754304">[-]</label><label class="expand" for="c-41754304">[1 more]</label></div><br/><div class="children"><div class="content">It does not appear to be what&#x27;s being described here?  Could you perhaps expand on the equivalence between the two if it is?</div><br/></div></div></div></div><div id="41755657" class="c"><input type="checkbox" id="c-41755657" checked=""/><div class="controls bullet"><span class="by">bgnn</span><span>|</span><a href="#41753954">prev</a><span>|</span><a href="#41753488">next</a><span>|</span><label class="collapse" for="c-41755657">[-]</label><label class="expand" for="c-41755657">[1 more]</label></div><br/><div class="children"><div class="content">Why not interpolate the missing data points with similar patients data?<p>This must be about the confidence of the approach. Maybe interpolation would be overconfident too.</div><br/></div></div><div id="41753488" class="c"><input type="checkbox" id="c-41753488" checked=""/><div class="controls bullet"><span class="by">clircle</span><span>|</span><a href="#41755657">prev</a><span>|</span><a href="#41754710">next</a><span>|</span><label class="collapse" for="c-41753488">[-]</label><label class="expand" for="c-41753488">[4 more]</label></div><br/><div class="children"><div class="content">Does any living statistician come close to the level of Donald Rubin in terms of research impact? Missing data analysis, causal inference, EM algorithm, any probably more. He just walks around creating new subfields.</div><br/><div id="41753632" class="c"><input type="checkbox" id="c-41753632" checked=""/><div class="controls bullet"><span class="by">selimthegrim</span><span>|</span><a href="#41753488">parent</a><span>|</span><a href="#41755051">next</a><span>|</span><label class="collapse" for="c-41753632">[-]</label><label class="expand" for="c-41753632">[2 more]</label></div><br/><div class="children"><div class="content">Efron?</div><br/><div id="41755513" class="c"><input type="checkbox" id="c-41755513" checked=""/><div class="controls bullet"><span class="by">richrichie</span><span>|</span><a href="#41753488">root</a><span>|</span><a href="#41753632">parent</a><span>|</span><a href="#41755051">next</a><span>|</span><label class="collapse" for="c-41755513">[-]</label><label class="expand" for="c-41755513">[1 more]</label></div><br/><div class="children"><div class="content">&amp; Tibshirani</div><br/></div></div></div></div><div id="41755051" class="c"><input type="checkbox" id="c-41755051" checked=""/><div class="controls bullet"><span class="by">aquafox</span><span>|</span><a href="#41753488">parent</a><span>|</span><a href="#41753632">prev</a><span>|</span><a href="#41754710">next</a><span>|</span><label class="collapse" for="c-41755051">[-]</label><label class="expand" for="c-41755051">[1 more]</label></div><br/><div class="children"><div class="content">Andrew Gelman?</div><br/></div></div></div></div><div id="41754710" class="c"><input type="checkbox" id="c-41754710" checked=""/><div class="controls bullet"><span class="by">TaurenHunter</span><span>|</span><a href="#41753488">prev</a><span>|</span><a href="#41753082">next</a><span>|</span><label class="collapse" for="c-41754710">[-]</label><label class="expand" for="c-41754710">[1 more]</label></div><br/><div class="children"><div class="content">Donald Rubin is kind of a modern day Leibniz...<p>Rubin Causal Model<p>Propensity Score Matching<p>Contributions to<p>Bayesian Inference<p>Missing data mechanisms<p>Survey sampling<p>Causal inference in observations<p>Multiple comparisons and hypothesis testing</div><br/></div></div><div id="41753082" class="c"><input type="checkbox" id="c-41753082" checked=""/><div class="controls bullet"><span class="by">xiaodai</span><span>|</span><a href="#41754710">prev</a><span>|</span><a href="#41755489">next</a><span>|</span><label class="collapse" for="c-41753082">[-]</label><label class="expand" for="c-41753082">[6 more]</label></div><br/><div class="children"><div class="content">I don’t know. I find quanta articles very high noise. It’s always hyping something</div><br/><div id="41753186" class="c"><input type="checkbox" id="c-41753186" checked=""/><div class="controls bullet"><span class="by">jll29</span><span>|</span><a href="#41753082">parent</a><span>|</span><a href="#41753203">next</a><span>|</span><label class="collapse" for="c-41753186">[-]</label><label class="expand" for="c-41753186">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t find the language of the article full of &quot;hype&quot;; they describe the history of different forms of imputation from single to multiple to ML-based.<p>The table is particularly useful as it describes what the article is all about in a way that can stick to students&#x27; minds. I&#x27;m very grateful for QuantaMagazine for its popular science reporting.</div><br/><div id="41755332" class="c"><input type="checkbox" id="c-41755332" checked=""/><div class="controls bullet"><span class="by">billfruit</span><span>|</span><a href="#41753082">root</a><span>|</span><a href="#41753186">parent</a><span>|</span><a href="#41753203">next</a><span>|</span><label class="collapse" for="c-41755332">[-]</label><label class="expand" for="c-41755332">[1 more]</label></div><br/><div class="children"><div class="content">The Quanta articles usually have a gossipy style and are very low information density.</div><br/></div></div></div></div><div id="41753203" class="c"><input type="checkbox" id="c-41753203" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#41753082">parent</a><span>|</span><a href="#41753186">prev</a><span>|</span><a href="#41754981">next</a><span>|</span><label class="collapse" for="c-41753203">[-]</label><label class="expand" for="c-41753203">[2 more]</label></div><br/><div class="children"><div class="content">I agree with that. I skip the Quanta magazine articles, mainly because the titles seem to be a little to hyped for my taste and don&#x27;t represent the content as well as they should.</div><br/><div id="41753615" class="c"><input type="checkbox" id="c-41753615" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#41753082">root</a><span>|</span><a href="#41753203">parent</a><span>|</span><a href="#41754981">next</a><span>|</span><label class="collapse" for="c-41753615">[-]</label><label class="expand" for="c-41753615">[1 more]</label></div><br/><div class="children"><div class="content">Yes, typically a short conversation with an LLM gives me more info and understanding of a topic than reading a Quanta article.</div><br/></div></div></div></div><div id="41754981" class="c"><input type="checkbox" id="c-41754981" checked=""/><div class="controls bullet"><span class="by">MiddleMan5</span><span>|</span><a href="#41753082">parent</a><span>|</span><a href="#41753203">prev</a><span>|</span><a href="#41755489">next</a><span>|</span><label class="collapse" for="c-41754981">[-]</label><label class="expand" for="c-41754981">[1 more]</label></div><br/><div class="children"><div class="content">Curious, what sites would you recommend?</div><br/></div></div></div></div><div id="41755489" class="c"><input type="checkbox" id="c-41755489" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#41753082">prev</a><span>|</span><a href="#41753718">next</a><span>|</span><label class="collapse" for="c-41755489">[-]</label><label class="expand" for="c-41755489">[1 more]</label></div><br/><div class="children"><div class="content">life is nothing but shaped noise</div><br/></div></div><div id="41753718" class="c"><input type="checkbox" id="c-41753718" checked=""/><div class="controls bullet"><span class="by">paulpauper</span><span>|</span><a href="#41755489">prev</a><span>|</span><a href="#41753122">next</a><span>|</span><label class="collapse" for="c-41753718">[-]</label><label class="expand" for="c-41753718">[3 more]</label></div><br/><div class="children"><div class="content">why not use regression on the existing entries to infer what the missing ones should be?</div><br/><div id="41754030" class="c"><input type="checkbox" id="c-41754030" checked=""/><div class="controls bullet"><span class="by">ivan_ah</span><span>|</span><a href="#41753718">parent</a><span>|</span><a href="#41753122">next</a><span>|</span><label class="collapse" for="c-41754030">[-]</label><label class="expand" for="c-41754030">[2 more]</label></div><br/><div class="children"><div class="content">That would push things towards the mean... not necessarily a bad thing, but presumably later steps of the analysis will be pooling&#x2F;averaging data together so not that useful.<p>A more interesting approach, let&#x27;s call it OPTION2, would be to sample from the predictive distribution of a regression (regression mean + noise), which would result in more variability in the imputations, although random so might not what you want.<p>The multiple imputation approach seems to be a resampling methods of obtaining OPTION2, w&#x2F;o need to assume linear regression model.</div><br/><div id="41755405" class="c"><input type="checkbox" id="c-41755405" checked=""/><div class="controls bullet"><span class="by">stdbrouw</span><span>|</span><a href="#41753718">root</a><span>|</span><a href="#41754030">parent</a><span>|</span><a href="#41753122">next</a><span>|</span><label class="collapse" for="c-41755405">[-]</label><label class="expand" for="c-41755405">[1 more]</label></div><br/><div class="children"><div class="content">Multiple imputation simply means you impute multiple times and run the analysis on each complete (imputed) dataset so you can incorporate the uncertainty that comes from guessing at missing values into your final confidence intervals and such. How you actually do the imputation will depend on the type of variable, the amount of missingness etc. A draw from the predictive distribution of a linear model of other variables without missing data is definitely a common method, but in a state-of-the-art multiple imputation package like mi in R you can choose from dozens.</div><br/></div></div></div></div></div></div><div id="41753122" class="c"><input type="checkbox" id="c-41753122" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#41753718">prev</a><span>|</span><label class="collapse" for="c-41753122">[-]</label><label class="expand" for="c-41753122">[11 more]</label></div><br/><div class="children"><div class="content">I wish they actually engaged with this issue instead of writing a fluff piece. There are plenty of problems with multiple imputation.<p>Not the least of which is that it&#x27;s far too easy to do the equivalent of p hacking and get your data to be significant by playing games with how you do the imputation. Garbage in, garbage out.<p>I think all of these methods should be abolished from the curriculum entirely. When I review papers in the ML&#x2F;AI I automatically reject any paper or dataset that uses imputation.<p>This is all a consequence of the terrible statics used in most fields. Bayesian methods don&#x27;t need to do this.</div><br/><div id="41753159" class="c"><input type="checkbox" id="c-41753159" checked=""/><div class="controls bullet"><span class="by">jll29</span><span>|</span><a href="#41753122">parent</a><span>|</span><a href="#41754988">next</a><span>|</span><label class="collapse" for="c-41753159">[-]</label><label class="expand" for="c-41753159">[2 more]</label></div><br/><div class="children"><div class="content">There are plenty of legit. articles that discuss&#x2F;survey imputation in ML&#x2F;AI:
 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?hl=de&amp;as_sdt=0%2C5&amp;q=%22machine+learning%22+imputation&amp;btnG=" rel="nofollow">https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?hl=de&amp;as_sdt=0%2C5&amp;q=%22m...</a></div><br/><div id="41753455" class="c"><input type="checkbox" id="c-41753455" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#41753122">root</a><span>|</span><a href="#41753159">parent</a><span>|</span><a href="#41754988">next</a><span>|</span><label class="collapse" for="c-41753455">[-]</label><label class="expand" for="c-41753455">[1 more]</label></div><br/><div class="children"><div class="content">The prestigious journal &quot;Artificial intelligence in medicine&quot;? No. Just because it&#x27;s on Google scholar doesn&#x27;t mean it&#x27;s worth anything. These are almost all trash. On the first page there&#x27;s one maybe legit paper in an ok venue as far as ML is concerned (KDD; an adjacent field to ML) that&#x27;s 30 years old.<p>No. AI&#x2F;ML folks don&#x27;t do imputation on our datasets. I cannot think of a single major dataset in vision, nlp, or robotics that does so. Despite missing data being a huge issue in those fields. It&#x27;s an antiqued method for an antiqued idea of how statistics should work that is doing far more damage than good.</div><br/></div></div></div></div><div id="41754988" class="c"><input type="checkbox" id="c-41754988" checked=""/><div class="controls bullet"><span class="by">aabaker99</span><span>|</span><a href="#41753122">parent</a><span>|</span><a href="#41753159">prev</a><span>|</span><a href="#41753308">next</a><span>|</span><label class="collapse" for="c-41754988">[-]</label><label class="expand" for="c-41754988">[1 more]</label></div><br/><div class="children"><div class="content">My intuition would be that there are certain conditions under which Bayesian inference for the missing data and multiple imputation lead to the same results.<p>What is the distinction?<p>The scenario described in the paper could be represented in a Bayesian method or not. “For a given missing value in one copy, randomly assign a guess from your distribution.” Here “my distribution” could be Bayesian or not but either way it’s still up to the statistician to make good choices about the model. The Bayesian can p hack here all the same.</div><br/></div></div><div id="41753308" class="c"><input type="checkbox" id="c-41753308" checked=""/><div class="controls bullet"><span class="by">DAGdug</span><span>|</span><a href="#41753122">parent</a><span>|</span><a href="#41754988">prev</a><span>|</span><a href="#41753955">next</a><span>|</span><label class="collapse" for="c-41753308">[-]</label><label class="expand" for="c-41753308">[4 more]</label></div><br/><div class="children"><div class="content">Maybe in academia, where sketchy incentives rule. In industry, p-hacking is great till you’re eventually caught for doing nonsense that isn’t driving real impact (still, the lead time is enough to mint money).</div><br/><div id="41753398" class="c"><input type="checkbox" id="c-41753398" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#41753122">root</a><span>|</span><a href="#41753308">parent</a><span>|</span><a href="#41753955">next</a><span>|</span><label class="collapse" for="c-41753398">[-]</label><label class="expand" for="c-41753398">[3 more]</label></div><br/><div class="children"><div class="content">Very doubtful. There are plenty of drugs that get approved and are of questionable value. Plenty of procedures that turn out to be not useful. The incentives in industry are even worse because everything depends on lying with data if you can do it.</div><br/><div id="41753467" class="c"><input type="checkbox" id="c-41753467" checked=""/><div class="controls bullet"><span class="by">hggigg</span><span>|</span><a href="#41753122">root</a><span>|</span><a href="#41753398">parent</a><span>|</span><a href="#41753547">next</a><span>|</span><label class="collapse" for="c-41753467">[-]</label><label class="expand" for="c-41753467">[1 more]</label></div><br/><div class="children"><div class="content">Indeed. Even worse some entire academic fields are built on pillars of lies. I was married to a researcher in one of them. Anything that compromises the existence of the field just gets written off. The end game is this fed into life changing healthcare decisions so one should never assume academia is harmless. This was utterly painful watching it from the perspective of a mathematician.</div><br/></div></div><div id="41753547" class="c"><input type="checkbox" id="c-41753547" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#41753122">root</a><span>|</span><a href="#41753398">parent</a><span>|</span><a href="#41753467">prev</a><span>|</span><a href="#41753955">next</a><span>|</span><label class="collapse" for="c-41753547">[-]</label><label class="expand" for="c-41753547">[1 more]</label></div><br/><div class="children"><div class="content">I assume by &quot;in industry&quot; they meant in jobs where you are doing data analysis to support decisions that your employer is making. This would be any typical &quot;data scientist&quot; job nowadays. There the consequences of BSing are felt by the entity that pays you, and will eventually come back around to you.<p>The incentives in medicine are more similar to those in academia, where your job is to cook up data that convinces <i>someone else</i> of your results, with highly imbalanced incentives that reward fraud.</div><br/></div></div></div></div></div></div><div id="41753955" class="c"><input type="checkbox" id="c-41753955" checked=""/><div class="controls bullet"><span class="by">parpfish</span><span>|</span><a href="#41753122">parent</a><span>|</span><a href="#41753308">prev</a><span>|</span><a href="#41753901">next</a><span>|</span><label class="collapse" for="c-41753955">[-]</label><label class="expand" for="c-41753955">[2 more]</label></div><br/><div class="children"><div class="content">I feel like multiple imputation is fine when you have data missing at random.<p>The problem is that data is <i>never</i> actually missing at random and there’s always some sort of interesting variable that confounds which pieces are missing</div><br/><div id="41754774" class="c"><input type="checkbox" id="c-41754774" checked=""/><div class="controls bullet"><span class="by">underbiding</span><span>|</span><a href="#41753122">root</a><span>|</span><a href="#41753955">parent</a><span>|</span><a href="#41753901">next</a><span>|</span><label class="collapse" for="c-41754774">[-]</label><label class="expand" for="c-41754774">[1 more]</label></div><br/><div class="children"><div class="content">True true but how do you account for missing data based on variables you care about and those you don&#x27;t?<p>More specifically, how do you determine if the pattern you seem to be identifying is actually related to the phenomenon being measured and not an error in the measurement tools themselves?<p>For example, a significant pattern of answers to &quot;Yes &#x2F; No: have you ever been assaulted?&quot; are blank. This could be (A), respondents who were assaulted are more likely to leave it blank out of shame or (B) someone handling the spreadsheet accidentally dropped some rows in the data (because lets be serious here, its all spreadsheets and emails...).<p>While you could say that (B) should be theoretically &quot;more truly random&quot;, we can&#x27;t assume that there isn&#x27;t a pattern to the way those rows were dropped (i.e. a pattern imposed on some algorithm that bugged out and dropped those rows).</div><br/></div></div></div></div><div id="41753901" class="c"><input type="checkbox" id="c-41753901" checked=""/><div class="controls bullet"><span class="by">fn-mote</span><span>|</span><a href="#41753122">parent</a><span>|</span><a href="#41753955">prev</a><span>|</span><label class="collapse" for="c-41753901">[-]</label><label class="expand" for="c-41753901">[1 more]</label></div><br/><div class="children"><div class="content">Clearly you know your stuff. Are there any not-super-technical references where an argument against using imputation is clearly explained?</div><br/></div></div></div></div></div></div></div></div></div></body></html>