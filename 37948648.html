<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697792455735" as="style"/><link rel="stylesheet" href="styles.css?v=1697792455735"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cdn.openai.com/papers/dall-e-3.pdf">Improving image generation with better captions [pdf]</a> <span class="domain">(<a href="https://cdn.openai.com">cdn.openai.com</a>)</span></div><div class="subtext"><span>alievk</span> | <span>29 comments</span></div><br/><div><div id="37948649" class="c"><input type="checkbox" id="c-37948649" checked=""/><div class="controls bullet"><span class="by">alievk</span><span>|</span><a href="#37950904">next</a><span>|</span><label class="collapse" for="c-37948649">[-]</label><label class="expand" for="c-37948649">[2 more]</label></div><br/><div class="children"><div class="content">Core takeaways:<p>- major improvement is from detailed image captioning<p>- they trained an image captioning model to produce short and detailed captions<p>- they use T5 text encoder<p>- they use GPT-4 to &quot;upsample&quot; short user prompts<p>- they train a U-net decoder and distill it to 2 denoising steps<p>- text rendering is still unreliable, they believe the model has hard time mapping word tokens into letters in image</div><br/><div id="37949727" class="c"><input type="checkbox" id="c-37949727" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37948649">parent</a><span>|</span><a href="#37950904">next</a><span>|</span><label class="collapse" for="c-37949727">[-]</label><label class="expand" for="c-37949727">[1 more]</label></div><br/><div class="children"><div class="content">They have only distilled the diffusion model used as a replacement for the VAE decoder, not the main diffusion model.</div><br/></div></div></div></div><div id="37950904" class="c"><input type="checkbox" id="c-37950904" checked=""/><div class="controls bullet"><span class="by">gzer0</span><span>|</span><a href="#37948649">prev</a><span>|</span><a href="#37950364">next</a><span>|</span><label class="collapse" for="c-37950904">[-]</label><label class="expand" for="c-37950904">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>During testing, we have noticed that this capability is unreliable as words are have missing or extra characters. We suspect this may have to do with the T5 text encoder we used: when the model encounters text in a prompt, it actually sees tokens that represent whole words and must map those to letters in an image.</i><p>Ah, interesting remark regarding text rendering.</div><br/><div id="37951522" class="c"><input type="checkbox" id="c-37951522" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#37950904">parent</a><span>|</span><a href="#37950364">next</a><span>|</span><label class="collapse" for="c-37951522">[-]</label><label class="expand" for="c-37951522">[3 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t a new problem. A Google paper from late 2022 mentioned this fact; it went away when they used a byte&#x2F;character-level text encoder (vs. tokenizing text first).</div><br/><div id="37952471" class="c"><input type="checkbox" id="c-37952471" checked=""/><div class="controls bullet"><span class="by">ollin</span><span>|</span><a href="#37950904">root</a><span>|</span><a href="#37951522">parent</a><span>|</span><a href="#37951876">next</a><span>|</span><label class="collapse" for="c-37952471">[-]</label><label class="expand" for="c-37952471">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Character-Aware Models Improve Visual Text Rendering&quot; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10562" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.10562</a> for anyone curious</div><br/></div></div><div id="37951876" class="c"><input type="checkbox" id="c-37951876" checked=""/><div class="controls bullet"><span class="by">mycall</span><span>|</span><a href="#37950904">root</a><span>|</span><a href="#37951522">parent</a><span>|</span><a href="#37952471">prev</a><span>|</span><a href="#37950364">next</a><span>|</span><label class="collapse" for="c-37951876">[-]</label><label class="expand" for="c-37951876">[1 more]</label></div><br/><div class="children"><div class="content">It still is an amazing state machine.</div><br/></div></div></div></div></div></div><div id="37950364" class="c"><input type="checkbox" id="c-37950364" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#37950904">prev</a><span>|</span><a href="#37952166">next</a><span>|</span><label class="collapse" for="c-37950364">[-]</label><label class="expand" for="c-37950364">[14 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty unreal how much better DALL-E 3 is at adhering to prompt accuracy versus its largest for-profit competitor, Midjourney. Quality-wise I think MJ still has the edge, but if it takes 2000 v-rolls to get there (assuming you can at all), then I&#x27;d say MJ has a steep hill to climb.</div><br/><div id="37950670" class="c"><input type="checkbox" id="c-37950670" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37950364">parent</a><span>|</span><a href="#37952122">next</a><span>|</span><label class="collapse" for="c-37950670">[-]</label><label class="expand" for="c-37950670">[12 more]</label></div><br/><div class="children"><div class="content">If you haven&#x27;t tried Ideogram or DeepFloyd, those are better yet at the specific case of &quot;writing verbatim text in your prompt&quot;. To the point Ideogram&#x27;s trending page is entirely taken over by images of Latina women&#x27;s names with bling effects and Disney characters last I looked.<p>DALLE3 is definitely amazingly higher quality but I still feel it&#x27;s kind of… useless. It&#x27;s too hard to control in conversation form, because it&#x27;s not a multimodal LLM but rather just works by rewriting text prompts. ChatGPT doesn&#x27;t really know what dalle can and can&#x27;t do, the actual dalle model still frequently fights you and just generates whatever it wants, when it generates prompts it sometimes leaves out or writes conflicting details, and it writes every prompt in &quot;friendly harmless AI voice&quot; so there&#x27;s superfluous hype adjectives.<p>This is odd because GPT4V actually is a multimodal model, and asking it to describe images as text works really well IME.<p>Common failure modes I saw playing with it:<p>* if you set something in eg &quot;France&quot;, ChatGPT will rewrite it to be diverse and inclusive by literally saying &quot;diverse people&quot; a lot, but then also adds a lot of super-stereotypical details, and then dalle ignores half of it. So you get sometimes-diverse French people who are all wearing striped shirts and berets in front of the Eiffel Tower.<p>* You can have a conversation with it and tell it to edit images, which it does by writing new prompts, but it&#x27;s dumb and thinks the prompts are part of the conversation. So a second round prompt will sometimes leave out all the details and just say &quot;The girl from before&quot; and produce a useless image.<p>* Composition is usually boring (very center-aligned and symmetrical) and if you try to control it by using words like &quot;camera&quot;, it will put cameras in the picture half the time.<p>* Typical AI failure modes: &quot;anime&quot; generates a kind of commercial&#x2F;fanart style that doesn&#x27;t exist rather than frames from a TV show. You can&#x27;t generate things if they&#x27;re hard to explain in English, like &quot;a keyboard whose keys are house keys&quot;.<p>But it&#x27;s really good at putting things in Minecraft.</div><br/><div id="37951004" class="c"><input type="checkbox" id="c-37951004" checked=""/><div class="controls bullet"><span class="by">gzer0</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37950670">parent</a><span>|</span><a href="#37951174">next</a><span>|</span><label class="collapse" for="c-37951004">[-]</label><label class="expand" for="c-37951004">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Composition is usually boring (very center-aligned and symmetrical) and if you try to control it by using words like &quot;camera&quot;, it will put cameras in the picture half the time.</i><p>- Be more descriptive and specific with your prompts. I always add &quot;shot from afar&quot; or &quot;wide-angled&quot; or &quot;shot with 85mm lens&quot; - never had an issue of boring composition.<p>&gt; <i>Typical AI failure modes: &quot;anime&quot; generates a kind of commercial&#x2F;fanart style that doesn&#x27;t exist rather than frames from a TV show.</i><p>- Again, be more descriptive and specific with your prompts. ie; &#x27;a character in the style of &#x27;Naruto&#x27;. Need to specify the show or artists instead of using the broad term &quot;anime&quot;.<p>&gt; <i>You can&#x27;t generate things if they&#x27;re hard to explain in English, like &quot;a keyboard whose keys are house keys&quot;.</i><p>- One more time, you just need to be more specific. do: &#x27;a computer keyboard, but instead of regular keys, replace each one with a metal house key.&#x27;</div><br/><div id="37951848" class="c"><input type="checkbox" id="c-37951848" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37951004">parent</a><span>|</span><a href="#37951174">next</a><span>|</span><label class="collapse" for="c-37951848">[-]</label><label class="expand" for="c-37951848">[1 more]</label></div><br/><div class="children"><div class="content">DALLE3 is explicitly designed around not having to do this; the point is that it&#x27;ll write the detailed prompt for you in four different ways so you get more variation.<p>&quot;Wide angle&quot; and &quot;fisheye&quot; do work, but &quot;lens&quot; is dangerous; anything that can be read as an object will tend to cause that object to appear instead of being used metaphorically. (Though trying a bit more, that doesn&#x27;t usually happen, if only because it rewrites the prompt to not mention it.)<p>&gt; - Again, be more descriptive and specific with your prompts. ie; &#x27;a character in the style of &#x27;Naruto&#x27;. Need to specify the show or artists instead of using the broad term &quot;anime&quot;.<p>Explicitly against the rules. It&#x27;ll block you if you try to use any real person&#x27;s name or anything it thinks is copyrighted. You can paraphrase though, or argue with it which weirdly sometimes works.<p>&gt; One more time, you just need to be more specific. do: &#x27;a computer keyboard, but instead of regular keys, replace each one with a metal house key.&#x27;<p>Tried it, doesn&#x27;t work well. If it gets to dalle it&#x27;s not smart enough to reliably do &quot;instead of&quot; (or generally anything that&#x27;s a &quot;deletion&quot;). It&#x27;ll just put in all three concepts.<p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;l8RS1Lu" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;l8RS1Lu</a><p>Synonyms can help if there is one in English. Emoji do interesting things but can&#x27;t tell how well they work.</div><br/></div></div></div></div><div id="37951174" class="c"><input type="checkbox" id="c-37951174" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37950670">parent</a><span>|</span><a href="#37951004">prev</a><span>|</span><a href="#37951902">next</a><span>|</span><label class="collapse" for="c-37951174">[-]</label><label class="expand" for="c-37951174">[1 more]</label></div><br/><div class="children"><div class="content">My typical use cases don&#x27;t usually involve actual text in the pictures themselves, but I definitely have seen lots of situations where it gets confused and tries to insert the description of the image into random speech bubbles. I can &quot;usually&quot; fix this most of the time by explicitly stating that there should be no text in the image.<p>I&#x27;ve gotten some pretty accurate images in one shot though that would&#x27;ve taken me 1000 rolls&#x2F;mangling to get MJ to do:<p>- Historical 1980s photograph of the Kool-Aid man busting through the Berlin Wall<p>I haven&#x27;t tried DeepFloyd or Ideogram, so maybe I&#x27;ll give them a shot. From what I&#x27;ve gathered 99% of people who use these just use them to either generate anime, or facial portrait type stuff so most of the models out there are tuned for that kind of thing. DALL-E 3 is the first model I&#x27;ve used (including SDXL, etc) that can actually get pretty close to matching my prompt.</div><br/></div></div><div id="37951902" class="c"><input type="checkbox" id="c-37951902" checked=""/><div class="controls bullet"><span class="by">crustaceansoup</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37950670">parent</a><span>|</span><a href="#37951174">prev</a><span>|</span><a href="#37950963">next</a><span>|</span><label class="collapse" for="c-37951902">[-]</label><label class="expand" for="c-37951902">[2 more]</label></div><br/><div class="children"><div class="content">I can fix most issues by characterizing the issues to ChatGPT. I&#x27;ll tell it what DALL-E&#x27;s limitations are and why its prompts don&#x27;t work and it (usually) &quot;listens&quot; and fixes things. It &quot;knows&quot; what a prompt is, so you could probably just tell it that it needs to be explicit with characters each time and it will be.<p>I&#x27;ve been just playing with it and generating silly images, I find working with the LLM to generate prompts is really entertaining and can go in directions I wouldn&#x27;t. I can just ask for &quot;software development memes&quot;, then &quot;make some more but reminiscent of famous memes&quot;, and then maybe ask it to &quot;create some images blending game development with cosmic horror&quot;, then &quot;I like prompt #2, create some variants of that but in the style of Junji Ito&quot; and on and on.</div><br/><div id="37952043" class="c"><input type="checkbox" id="c-37952043" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37951902">parent</a><span>|</span><a href="#37950963">next</a><span>|</span><label class="collapse" for="c-37952043">[-]</label><label class="expand" for="c-37952043">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, it&#x27;s good at that. It&#x27;s very good at combining concepts it is familiar with and can do a decent job at comics.<p>I&#x27;ve seen other people do comics with &quot;meme characters&quot; like Pepe in, but I don&#x27;t know the trick; it usually complains about proper names and when I tried a few paraphrases it inexplicably produced Reddit ragefaces.</div><br/></div></div></div></div><div id="37950963" class="c"><input type="checkbox" id="c-37950963" checked=""/><div class="controls bullet"><span class="by">thom</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37950670">parent</a><span>|</span><a href="#37951902">prev</a><span>|</span><a href="#37951166">next</a><span>|</span><label class="collapse" for="c-37950963">[-]</label><label class="expand" for="c-37950963">[1 more]</label></div><br/><div class="children"><div class="content">I recognise some of what you&#x27;re saying here. I&#x27;ve used DALL-E 3 in anger recently and while the results have really impressed me, every time I&#x27;ve tried to actually tweak and improve something I&#x27;ve ended up getting further and further away from what I want.</div><br/></div></div><div id="37951166" class="c"><input type="checkbox" id="c-37951166" checked=""/><div class="controls bullet"><span class="by">thequadehunter</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37950670">parent</a><span>|</span><a href="#37950963">prev</a><span>|</span><a href="#37951014">next</a><span>|</span><label class="collapse" for="c-37951166">[-]</label><label class="expand" for="c-37951166">[1 more]</label></div><br/><div class="children"><div class="content">Is there evidence that bing creator does this? I remember when it first rolled out there the generations were lower quality but didn&#x27;t stray as far as Chatgpt. It&#x27;s just anecdotal though, so might not be true.</div><br/></div></div><div id="37951014" class="c"><input type="checkbox" id="c-37951014" checked=""/><div class="controls bullet"><span class="by">bbstats</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37950670">parent</a><span>|</span><a href="#37951166">prev</a><span>|</span><a href="#37951074">next</a><span>|</span><label class="collapse" for="c-37951014">[-]</label><label class="expand" for="c-37951014">[2 more]</label></div><br/><div class="children"><div class="content">Quick test on Dall-E 3 (bing.com&#x2F;create) vs ideogram, and ideogram is not even close in terms of quality.</div><br/><div id="37951863" class="c"><input type="checkbox" id="c-37951863" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37951014">parent</a><span>|</span><a href="#37951074">next</a><span>|</span><label class="collapse" for="c-37951863">[-]</label><label class="expand" for="c-37951863">[1 more]</label></div><br/><div class="children"><div class="content">Image quality no, it&#x27;s just especially good at being able to spell words compared to Midjourney.<p>Deepfloyd is a local model sponsored by Stability that&#x27;s older than SDXL.</div><br/></div></div></div></div><div id="37951074" class="c"><input type="checkbox" id="c-37951074" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37950670">parent</a><span>|</span><a href="#37951014">prev</a><span>|</span><a href="#37952122">next</a><span>|</span><label class="collapse" for="c-37951074">[-]</label><label class="expand" for="c-37951074">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Typical AI failure modes: &quot;anime&quot; generates a kind of commercial&#x2F;fanart style that doesn&#x27;t exist rather than frames from a TV show.<p>People usually want anime artwork, not frames from a show. If you want that take the effort to write it.</div><br/><div id="37951877" class="c"><input type="checkbox" id="c-37951877" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37950364">root</a><span>|</span><a href="#37951074">parent</a><span>|</span><a href="#37952122">next</a><span>|</span><label class="collapse" for="c-37951877">[-]</label><label class="expand" for="c-37951877">[1 more]</label></div><br/><div class="children"><div class="content">I mention this one because Dalle2 actually does work if you say &quot;anime screenshot&quot; (it knows studio names too.) 3 does not understand &quot;screenshot&quot; (makes pictures of people watching something on a TV) and blocks you if you try to reference anything copyrighted, so no &quot;N64 game&quot; either. Time periods like &quot;80s anime&quot; kinda work though.<p>But the main point is that it makes it online English centric. That style isn&#x27;t called &quot;anime&quot; in Chinese&#x2F;Japanese, but if you try to prompt in those languages it translates it first. Basically, if you could control it with images as well as text that&#x27;d help.</div><br/></div></div></div></div></div></div><div id="37952122" class="c"><input type="checkbox" id="c-37952122" checked=""/><div class="controls bullet"><span class="by">robbrown451</span><span>|</span><a href="#37950364">parent</a><span>|</span><a href="#37950670">prev</a><span>|</span><a href="#37952166">next</a><span>|</span><label class="collapse" for="c-37952122">[-]</label><label class="expand" for="c-37952122">[1 more]</label></div><br/><div class="children"><div class="content">Personally I think DALL-E does better quality, especially at photorealistic stuff.<p>Here&#x27;s a few of mine, many photorealistic.<p><a href="https:&#x2F;&#x2F;www.karmatics.com&#x2F;stuff&#x2F;dalle.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.karmatics.com&#x2F;stuff&#x2F;dalle.html</a></div><br/></div></div></div></div><div id="37952166" class="c"><input type="checkbox" id="c-37952166" checked=""/><div class="controls bullet"><span class="by">totetsu</span><span>|</span><a href="#37950364">prev</a><span>|</span><a href="#37952818">next</a><span>|</span><label class="collapse" for="c-37952166">[-]</label><label class="expand" for="c-37952166">[1 more]</label></div><br/><div class="children"><div class="content">I have to say that the GPT4 Dalle plugin is very good at making what you ask for. I was a bit disappointed to lose access to the new model directly through the dalle interface, because i enjoyed pushing its limits to make wierd images, But its amazing to be able to do things like, say &quot;hey lets come up with a yaml schema to describe an outfit, and then make me some images of that&quot; and get very very consistent results. When I was writing prompts myself I felt I was more just jumping into the soup image vector space.</div><br/></div></div><div id="37952818" class="c"><input type="checkbox" id="c-37952818" checked=""/><div class="controls bullet"><span class="by">aargh_aargh</span><span>|</span><a href="#37952166">prev</a><span>|</span><a href="#37951348">next</a><span>|</span><label class="collapse" for="c-37952818">[-]</label><label class="expand" for="c-37952818">[3 more]</label></div><br/><div class="children"><div class="content">Here you can try part of the process yourself (image to text):<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;fffiloni&#x2F;CLIP-Interrogator-2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;fffiloni&#x2F;CLIP-Interrogator-2</a></div><br/><div id="37952911" class="c"><input type="checkbox" id="c-37952911" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#37952818">parent</a><span>|</span><a href="#37951348">next</a><span>|</span><label class="collapse" for="c-37952911">[-]</label><label class="expand" for="c-37952911">[2 more]</label></div><br/><div class="children"><div class="content">Well, not quite. The linked hf space uses the ViT-H-14 OpenCLIP model, which was trained on the Laion-2B dataset[0], which I&#x27;d categorize as fitting the reports description of &quot;noisy and inaccurate image captions&quot; perfectly.<p>[0] <a href="https:&#x2F;&#x2F;laion.ai&#x2F;blog&#x2F;large-openclip&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;laion.ai&#x2F;blog&#x2F;large-openclip&#x2F;</a></div><br/><div id="37952992" class="c"><input type="checkbox" id="c-37952992" checked=""/><div class="controls bullet"><span class="by">aargh_aargh</span><span>|</span><a href="#37952818">root</a><span>|</span><a href="#37952911">parent</a><span>|</span><a href="#37951348">next</a><span>|</span><label class="collapse" for="c-37952992">[-]</label><label class="expand" for="c-37952992">[1 more]</label></div><br/><div class="children"><div class="content">I see, so they relabelled a smaller (more representative) subset for the captioner manually but more diligently, then used that to relabel their large set (analogous to LAION) with more descriptive captions, then trained DALL-E 3 on that.<p>By the way, in the case of CLIP-Interrogator-2, I wonder how they came up with their wordlists (included in the repo). Are they just all unique terms from OpenCLIP?</div><br/></div></div></div></div></div></div><div id="37951348" class="c"><input type="checkbox" id="c-37951348" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#37952818">prev</a><span>|</span><a href="#37951869">next</a><span>|</span><label class="collapse" for="c-37951348">[-]</label><label class="expand" for="c-37951348">[1 more]</label></div><br/><div class="children"><div class="content">Just want to call out this is fresh from OpenAI that actually talks about data preparation, their models (to some extents) and some negative learnings. Still not as complete as what&#x27;s from their peers (Stability AI, Meta, adept.ai etc). Probably it just shows their core focus is on LLM or there is a change of heart?</div><br/></div></div><div id="37951869" class="c"><input type="checkbox" id="c-37951869" checked=""/><div class="controls bullet"><span class="by">amanzi</span><span>|</span><a href="#37951348">prev</a><span>|</span><a href="#37952345">next</a><span>|</span><label class="collapse" for="c-37951869">[-]</label><label class="expand" for="c-37951869">[1 more]</label></div><br/><div class="children"><div class="content">I re-generated some of these images using the same prompts. It&#x27;s truly amazing how good some of the artwork is. Most AI-generated images I&#x27;ve seen look good on the surface, but when you look closer there are obvious blemishes and weird things going on. But I&#x27;ve generated a bunch of images in Dall-E 3 and have been amazed at the results.<p>e.g. my version of the &quot;fierce garden gnome warrior&quot; in widescreen: <a href="https:&#x2F;&#x2F;1drv.ms&#x2F;i&#x2F;s!Avl8TQnojpIQrsRCFFRQQDpNcyNKNw?e=VAZfVe" rel="nofollow noreferrer">https:&#x2F;&#x2F;1drv.ms&#x2F;i&#x2F;s!Avl8TQnojpIQrsRCFFRQQDpNcyNKNw?e=VAZfVe</a></div><br/></div></div><div id="37952345" class="c"><input type="checkbox" id="c-37952345" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37951869">prev</a><span>|</span><label class="collapse" for="c-37952345">[-]</label><label class="expand" for="c-37952345">[2 more]</label></div><br/><div class="children"><div class="content">When I used GPT4-Image to get a full detailed description of an image, I was sure it was going to be used as a method for generating training data.</div><br/><div id="37953768" class="c"><input type="checkbox" id="c-37953768" checked=""/><div class="controls bullet"><span class="by">cocoflunchy</span><span>|</span><a href="#37952345">parent</a><span>|</span><label class="collapse" for="c-37953768">[-]</label><label class="expand" for="c-37953768">[1 more]</label></div><br/><div class="children"><div class="content">I just tried this and wow, I&#x27;m impressed. Input &#x2F; output: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;nTSkJUK" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;nTSkJUK</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>