<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691312456414" as="style"/><link rel="stylesheet" href="styles.css?v=1691312456414"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://mkone.ai/blog/introducing-mk1">MK-1</a> <span class="domain">(<a href="https://mkone.ai">mkone.ai</a>)</span></div><div class="subtext"><span>ejz</span> | <span>48 comments</span></div><br/><div><div id="37016753" class="c"><input type="checkbox" id="c-37016753" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#37016840">next</a><span>|</span><label class="collapse" for="c-37016753">[-]</label><label class="expand" for="c-37016753">[16 more]</label></div><br/><div class="children"><div class="content">It&#x27;s weird that not once do they mention or compare their results to the already-available quantization methods. I normally try to give benefit of the doubt, but there&#x27;s really no way they&#x27;re not aware that there are already widely used techniques for accomplishing this same thing, so the comparison benchmarks <i>really</i> should be there.<p>To fill in the gap, here&#x27;s llama.cpp&#x27;s comparison chart[0] for the different quantizations available for Llama 1. We can&#x27;t compare directly with their Llama 2 metrics, but just comparing the percent change in speed and perplexity, MK-1 looks very similar to Q5_1. There&#x27;s a small but not insignificant hit to perplexity, and a just over 2x speedup.<p>If these numbers are accurate, you can download pre-quantized Llama 2 models from Hugging Face that will perform essentially the same as what MK-1 is offering, with the Q5 files here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-13B-GGML&#x2F;tree&#x2F;main" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-13B-GGML&#x2F;tree&#x2F;main</a><p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization</a></div><br/><div id="37016971" class="c"><input type="checkbox" id="c-37016971" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#37016753">parent</a><span>|</span><a href="#37018989">next</a><span>|</span><label class="collapse" for="c-37016971">[-]</label><label class="expand" for="c-37016971">[3 more]</label></div><br/><div class="children"><div class="content">Also, using the word &quot;codecs&quot; kind of puts a bad taste in my mouth. It&#x27;s like they&#x27;re trying to sound like they invented an entirely new paradigm, with their own fancy name that reminds people of video compression.</div><br/><div id="37018120" class="c"><input type="checkbox" id="c-37018120" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37016971">parent</a><span>|</span><a href="#37017306">next</a><span>|</span><label class="collapse" for="c-37018120">[-]</label><label class="expand" for="c-37018120">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d go so far as to say this entire post is grossly misleading and should be flagged.</div><br/></div></div><div id="37017306" class="c"><input type="checkbox" id="c-37017306" checked=""/><div class="controls bullet"><span class="by">throwanem</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37016971">parent</a><span>|</span><a href="#37018120">prev</a><span>|</span><a href="#37018989">next</a><span>|</span><label class="collapse" for="c-37017306">[-]</label><label class="expand" for="c-37017306">[1 more]</label></div><br/><div class="children"><div class="content">Sure. How else are they supposed to sell it?</div><br/></div></div></div></div><div id="37018989" class="c"><input type="checkbox" id="c-37018989" checked=""/><div class="controls bullet"><span class="by">paul_mk1</span><span>|</span><a href="#37016753">parent</a><span>|</span><a href="#37016971">prev</a><span>|</span><a href="#37018150">next</a><span>|</span><label class="collapse" for="c-37018989">[-]</label><label class="expand" for="c-37018989">[6 more]</label></div><br/><div class="children"><div class="content">Hi, one of the founders here.<p>Attempting to address some of the comments in a single message.<p>To help understand why we decided not to compare to existing methods: I think it would be difficult to do so fairly, since there are many tradeoffs and different use cases. It&#x27;s not always the case that one technique is bad and the other is good, it&#x27;s more about the targeted design point (say, cloud vs local). We are openly offering our numbers &#x2F; benchmarks and looking for early partners that are aligned with our current value proposition (hence the closed beta).<p>A good example is that llama.cpp is a fantastic framework to run models locally for the single-user case (batch=1). While llama.cpp supports different backends (RPi, CPU, GPU), I don&#x27;t think it would be particularly fair to compare and show that MKML is better at a given perplexity, compression ratio, and speed on GPU for a multi-user case (batch &gt;&gt; 1), when that is not llama.cpp’s targeted use case (afaik). For example MKML achieves ~2700 tok&#x2F;sec at batch 32 (i.e. 32 prompts in parallel) on a 4090 for a Llama-2 7B, with a ~4̶.̶2̶G̶B 5.2GB memory footprint, and perplexity that is ~fp16.<p>Also, we&#x27;re not currently wrapping any open source tools or techniques for quantization. Everything is our own and there’s more news to come soon.<p>If anyone has specific technical questions I&#x27;d be happy to answer as best I can.<p>Cheers,
Paul Merolla</div><br/><div id="37019172" class="c"><input type="checkbox" id="c-37019172" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37018989">parent</a><span>|</span><a href="#37019545">next</a><span>|</span><label class="collapse" for="c-37019172">[-]</label><label class="expand" for="c-37019172">[3 more]</label></div><br/><div class="children"><div class="content">&gt; A good example is that llama.cpp is a fantastic framework to run models locally for the single-user case (batch=1). While llama.cpp supports different backends (RPi, CPU, GPU), I don&#x27;t think it would be particularly fair to compare and show that MKML is better at a given perplexity, compression ratio, and speed on GPU for a multi-user case (batch &gt;&gt; 1), when that is not llama.cpp’s targeted use case (afaik).<p>Maybe I&#x27;m misunderstanding MKML. As I understood it, MKML is a compression step that then feeds into another framework like HF&#x27;s Transformers or PyTorch. If that&#x27;s the case, then comparing MKML to llama.cpp is apples to oranges—the correct comparison would be to GGML and the various quantization methods. The inference engine and its intended use cases aren&#x27;t what&#x27;s in question here.<p>If a model compressed with MKML outperforms a standard quantized model in a batch setting, that&#x27;s useful information! It would not be at all unfair for you to cite that as a strength, and it would increase your credibility because you wouldn&#x27;t seem to be dodging the question of how you compare to your substitutes.</div><br/><div id="37019899" class="c"><input type="checkbox" id="c-37019899" checked=""/><div class="controls bullet"><span class="by">paul_mk1</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37019172">parent</a><span>|</span><a href="#37019270">next</a><span>|</span><label class="collapse" for="c-37019899">[-]</label><label class="expand" for="c-37019899">[1 more]</label></div><br/><div class="children"><div class="content">Appreciate your response.<p>We compared MKML mk600 (5.2GB) against llama.cpp Q5_1 (4.7GB) and Q6_k (5.1GB) on a 4090 for llama-7B. The test is the same in all cases: we generate 128 tokens from a single token prompt (batch=1) and measure performance of the forward pass during auto-regression.<p>(llama-7B, single prompt, batch=1)<p>MKML      mk600:  125t&#x2F;s<p>Llama.cpp Q5_1:   84 t&#x2F;s<p>Llama.cpp Q6_k:   78 t&#x2F;s<p>Our llama.cpp test:
Build (<a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#cublas">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#cublas</a>):<p>make -j12 LLAMA_CUBLAS=1<p>Run:<p>.&#x2F;main -t 16 -ngl 32 -m llama-2-7b-chat.ggmlv3.q6_K.bin -p &quot;?&quot; -n 128<p>Please feel free to post your llama.cpp results if they are different.<p>&gt;Maybe I&#x27;m misunderstanding MKML. As I understood it, MKML is a compression step that then feeds into another framework like HF&#x27;s Transformers or PyTorch.<p>MKML is not a compression tool that feeds into another framework. It is an inference runtime (like FasterTransformers or vllm) except that MKML is also plug and play with existing frameworks like Hugging Face.</div><br/></div></div><div id="37019270" class="c"><input type="checkbox" id="c-37019270" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37019172">parent</a><span>|</span><a href="#37019899">prev</a><span>|</span><a href="#37019545">next</a><span>|</span><label class="collapse" for="c-37019270">[-]</label><label class="expand" for="c-37019270">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. This apples-to-apples comparison being obviously missing here is quite telling.</div><br/></div></div></div></div><div id="37019545" class="c"><input type="checkbox" id="c-37019545" checked=""/><div class="controls bullet"><span class="by">polishgladiator</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37018989">parent</a><span>|</span><a href="#37019172">prev</a><span>|</span><a href="#37019478">next</a><span>|</span><label class="collapse" for="c-37019545">[-]</label><label class="expand" for="c-37019545">[1 more]</label></div><br/><div class="children"><div class="content">&gt; [...] llama.cpp is a fantastic framework to run models locally for the single-user case (batch=1) 
&gt; [...] I don&#x27;t think it would be particularly fair to compare and show that MKML is better at a given perplexity, compression ratio, and speed on GPU for a multi-user case (batch &gt;&gt; 1)<p>Ok so you agree that llama.cpp etc are great for batch==1, right?<p>And I agree their targeted use case is not batch==32 (because who is doing that really?)<p>But if we extended llama.cpp or some other faster batch==1 implementation to support batch==32, why do you suppose it wouldn&#x27;t still be faster than MKML? It seems to me that if you can do batch==1 faster, you could easily do batch&gt;&gt;1 faster too -- it is just that no one really needed that (yet?)</div><br/></div></div><div id="37019478" class="c"><input type="checkbox" id="c-37019478" checked=""/><div class="controls bullet"><span class="by">polishgladiator</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37018989">parent</a><span>|</span><a href="#37019545">prev</a><span>|</span><a href="#37018150">next</a><span>|</span><label class="collapse" for="c-37019478">[-]</label><label class="expand" for="c-37019478">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If anyone has specific technical questions I&#x27;d be happy to answer as best I can.<p>What is the context size for these measurements? Is it the full 4k for llama-2? And just to be clear, when you say memory footprint, this is the entire memory foorprint right? Weights, 4k KV cache etc?<p>And more generally, I&#x27;m curious about the use case for running puny models like Llama-2 7B in the cloud on desktops GPUs (like 4090) with batch==32?</div><br/></div></div></div></div><div id="37018150" class="c"><input type="checkbox" id="c-37018150" checked=""/><div class="controls bullet"><span class="by">polishgladiator</span><span>|</span><a href="#37016753">parent</a><span>|</span><a href="#37018989">prev</a><span>|</span><a href="#37018181">next</a><span>|</span><label class="collapse" for="c-37018150">[-]</label><label class="expand" for="c-37018150">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been doing some hacking with Llama2 on an AMD 7900 XTX this weekend, using llama.cpp and q5_k_s quantization.<p>Compared to MK600 on an RTX 4090 in their data, I am measuring higher throughput and lower perplexity (again, note that I am using a cheaper GPU!)...</div><br/></div></div><div id="37018181" class="c"><input type="checkbox" id="c-37018181" checked=""/><div class="controls bullet"><span class="by">kapildev</span><span>|</span><a href="#37016753">parent</a><span>|</span><a href="#37018150">prev</a><span>|</span><a href="#37016807">next</a><span>|</span><label class="collapse" for="c-37018181">[-]</label><label class="expand" for="c-37018181">[2 more]</label></div><br/><div class="children"><div class="content">MKML says that they reduced the size of Llama2-13B model from 26GB to 10.5GB. Similar offering from TheBloke (your first link) is a 10.7GB Q6_K model. Maybe, they are using GGML and llama.cpp and packaging it in an attractive way while making people believe it is some proprietary tech.</div><br/><div id="37018232" class="c"><input type="checkbox" id="c-37018232" checked=""/><div class="controls bullet"><span class="by">polishgladiator</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37018181">parent</a><span>|</span><a href="#37016807">next</a><span>|</span><label class="collapse" for="c-37018232">[-]</label><label class="expand" for="c-37018232">[1 more]</label></div><br/><div class="children"><div class="content">Based on the integration examples, I don&#x27;t think they are simply repackaging llama.cpp<p>Rather it looks like they are reimplementing their own quantization scheme, in such a way that it is a little easier to integrate for basic python users, at the cost of performance (compared to llama.cpp and others).<p>Given that the bar for integrating something with higher perf like llama.cpp isn&#x27;t very high (and that&#x27;s the way the world is heading -- ask any 15 year old interested in this stuff), I can&#x27;t see anything of value here.</div><br/></div></div></div></div><div id="37016807" class="c"><input type="checkbox" id="c-37016807" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37016753">parent</a><span>|</span><a href="#37018181">prev</a><span>|</span><a href="#37017888">next</a><span>|</span><label class="collapse" for="c-37016807">[-]</label><label class="expand" for="c-37016807">[2 more]</label></div><br/><div class="children"><div class="content">Q5_1 is already old news too, K quants are faster and more space efficient for the same perplexity loss.<p><a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;142q5k5&#x2F;updated_relative_comparison_of_ggml_quantization&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;142q5k5&#x2F;updated...</a></div><br/><div id="37016823" class="c"><input type="checkbox" id="c-37016823" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#37016753">root</a><span>|</span><a href="#37016807">parent</a><span>|</span><a href="#37017888">next</a><span>|</span><label class="collapse" for="c-37016823">[-]</label><label class="expand" for="c-37016823">[1 more]</label></div><br/><div class="children"><div class="content">For sure, but I couldn&#x27;t find numbers for the K quants that included inference speeds, so I settled on the older one. If MK-1 were trying to be honest they&#x27;d definitely want to benchmark against the newest methods!</div><br/></div></div></div></div></div></div><div id="37016840" class="c"><input type="checkbox" id="c-37016840" checked=""/><div class="controls bullet"><span class="by">xianshou</span><span>|</span><a href="#37016753">prev</a><span>|</span><a href="#37017455">next</a><span>|</span><label class="collapse" for="c-37016840">[-]</label><label class="expand" for="c-37016840">[1 more]</label></div><br/><div class="children"><div class="content">Not a single mention of existing quantization techniques? Ten bucks says this is just a wrapper around bitsandbytes or ggml.</div><br/></div></div><div id="37017455" class="c"><input type="checkbox" id="c-37017455" checked=""/><div class="controls bullet"><span class="by">lyapunova</span><span>|</span><a href="#37016840">prev</a><span>|</span><a href="#37016929">next</a><span>|</span><label class="collapse" for="c-37017455">[-]</label><label class="expand" for="c-37017455">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think I can use this if it&#x27;s not open source... sorry.<p>The field moves too fast and the convenience is just not there otherwise.<p>edit: also the branding makes me think of MK-ultra which is probably something to avoid</div><br/></div></div><div id="37016929" class="c"><input type="checkbox" id="c-37016929" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#37017455">prev</a><span>|</span><a href="#37016604">next</a><span>|</span><label class="collapse" for="c-37016929">[-]</label><label class="expand" for="c-37016929">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve worked on ML model quantization. The open source 4-bit or 8-bit quantization isn&#x27;t as good as one can get - there are much fancier techniques to keep predictive performance while squeezing size.<p>Some techniques (like quantization-aware training) involve changes to training.</div><br/><div id="37016957" class="c"><input type="checkbox" id="c-37016957" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#37016929">parent</a><span>|</span><a href="#37016982">next</a><span>|</span><label class="collapse" for="c-37016957">[-]</label><label class="expand" for="c-37016957">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure there are better methods! But in this case, MKML&#x27;s numbers just don&#x27;t look impressive when placed alongside the prominent quantization techniques already in use. According to this chart [0] it&#x27;s most similar in size to a Q6_K quantization, and if anything has slightly worse perplexity.<p>If their technique <i>were</i> better, I imagine that the company would acknowledge the existence of the open source techniques and show them in their comparisons, instead of pretending the only other option is the raw fp16 model.<p>[0] <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;142q5k5&#x2F;updated_relative_comparison_of_ggml_quantization&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;142q5k5&#x2F;updated...</a></div><br/><div id="37017225" class="c"><input type="checkbox" id="c-37017225" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#37016929">root</a><span>|</span><a href="#37016957">parent</a><span>|</span><a href="#37016982">next</a><span>|</span><label class="collapse" for="c-37017225">[-]</label><label class="expand" for="c-37017225">[1 more]</label></div><br/><div class="children"><div class="content">From what I remember, non-power-of-2 compression schemes tank inference speed (assuming Q6_k is 6-bit; I haven&#x27;t actually verified if ggml q6_K llama is slow). Meanwhile, the site claims a speed-up.<p>But I do actually agree with you - they should really be benchmarking against popular competitors. In my experience, fancier quantization is a _lot_ of work for fairly little gain (at least for neural nets). I also think that ML techniques such as quantization (or fancy param sweeps, feature pruning, that kind of stuff) tend to either get in-housed (i.e. the model will come quantized from the source) or get open-sourced.<p>In-housing of ML techniques tends to happen more often if there&#x27;s a money-making model where the hardware running the model costs money, but running the model brings in money.</div><br/></div></div></div></div><div id="37016982" class="c"><input type="checkbox" id="c-37016982" checked=""/><div class="controls bullet"><span class="by">KRAKRISMOTT</span><span>|</span><a href="#37016929">parent</a><span>|</span><a href="#37016957">prev</a><span>|</span><a href="#37016604">next</a><span>|</span><label class="collapse" for="c-37016982">[-]</label><label class="expand" for="c-37016982">[2 more]</label></div><br/><div class="children"><div class="content">What about Unum&#x27;s quantization methods?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;usearch">https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;usearch</a></div><br/><div id="37017129" class="c"><input type="checkbox" id="c-37017129" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#37016929">root</a><span>|</span><a href="#37016982">parent</a><span>|</span><a href="#37016604">next</a><span>|</span><label class="collapse" for="c-37017129">[-]</label><label class="expand" for="c-37017129">[1 more]</label></div><br/><div class="children"><div class="content">Not familiar with Unum. From a quick glance, it seems that they truncate Least Significant Bits, which is the simplest but fastest quantization method.</div><br/></div></div></div></div></div></div><div id="37016604" class="c"><input type="checkbox" id="c-37016604" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#37016929">prev</a><span>|</span><a href="#37016860">next</a><span>|</span><label class="collapse" for="c-37016604">[-]</label><label class="expand" for="c-37016604">[4 more]</label></div><br/><div class="children"><div class="content">...isn&#x27;t this just quantization?</div><br/><div id="37017008" class="c"><input type="checkbox" id="c-37017008" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37016604">parent</a><span>|</span><a href="#37016722">next</a><span>|</span><label class="collapse" for="c-37017008">[-]</label><label class="expand" for="c-37017008">[1 more]</label></div><br/><div class="children"><div class="content">If you look at the demo video, the output is exactly the same for both cases, so I doubt it uses quantization.</div><br/></div></div><div id="37016722" class="c"><input type="checkbox" id="c-37016722" checked=""/><div class="controls bullet"><span class="by">atlas_hugged</span><span>|</span><a href="#37016604">parent</a><span>|</span><a href="#37017008">prev</a><span>|</span><a href="#37016644">next</a><span>|</span><label class="collapse" for="c-37016722">[-]</label><label class="expand" for="c-37016722">[1 more]</label></div><br/><div class="children"><div class="content">Exactly what I was thinking. Everyone already does this. Unless they’re doing something else, they’ll have to show why it’s better than just quickly quantizing to 8 bits or 4 bits or whatever.</div><br/></div></div><div id="37016644" class="c"><input type="checkbox" id="c-37016644" checked=""/><div class="controls bullet"><span class="by">bhouston</span><span>|</span><a href="#37016604">parent</a><span>|</span><a href="#37016722">prev</a><span>|</span><a href="#37016860">next</a><span>|</span><label class="collapse" for="c-37016644">[-]</label><label class="expand" for="c-37016644">[1 more]</label></div><br/><div class="children"><div class="content">Whatever it is, it will likely be copied into the open source tooling like llama.cop soonish or something similar will arrive in llama.cpp.  It doesn’t seem defensive advantage.  It seems like a feature and fighting against fast moving open source alternatives.</div><br/></div></div></div></div><div id="37016860" class="c"><input type="checkbox" id="c-37016860" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#37016604">prev</a><span>|</span><a href="#37017094">next</a><span>|</span><label class="collapse" for="c-37016860">[-]</label><label class="expand" for="c-37016860">[2 more]</label></div><br/><div class="children"><div class="content">Too bad it&#x27;s not an open source effort.<p>I&#x27;m not a fan of proprietary dependencies in my stack, full stop.</div><br/><div id="37016869" class="c"><input type="checkbox" id="c-37016869" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#37016860">parent</a><span>|</span><a href="#37017094">next</a><span>|</span><label class="collapse" for="c-37016869">[-]</label><label class="expand" for="c-37016869">[1 more]</label></div><br/><div class="children"><div class="content">I seriously doubt this will go anywhere. The open source community has already achieved basically the same performance improvements via quantization. This feels like someone has repackaged those libraries and is going to try to sell them to unwary and uninformed AI startups.</div><br/></div></div></div></div><div id="37017094" class="c"><input type="checkbox" id="c-37017094" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37016860">prev</a><span>|</span><a href="#37018801">next</a><span>|</span><label class="collapse" for="c-37017094">[-]</label><label class="expand" for="c-37017094">[5 more]</label></div><br/><div class="children"><div class="content">How does this compare to mlc-llm with 4 bit quantization? It runs llama2 13B incredibly fast on my 4090. Multiples of the speed of llama.cpp even on GPU with the same 4 bit quantization.</div><br/><div id="37018436" class="c"><input type="checkbox" id="c-37018436" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37017094">parent</a><span>|</span><a href="#37017249">next</a><span>|</span><label class="collapse" for="c-37018436">[-]</label><label class="expand" for="c-37018436">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, that TVM Vulkan autotuning is incredible. And its not even using the matmul Vulkan extension, I dont think.<p>MLC&#x27;s 4 bit quantization is &quot;dumb&quot; compared to llama.cpp, which reduces perplexity (and also explains some of the speed difference), but the biggest missing feature is CPU offloading (which would allow you to run 70B reasonably well on a 4090).<p>I think the holy grail of local llm inference is llama 70B, run in TVM, split between the GPU and IGP. It feels like we are inches away... All the pieces are there, but there are no front end devs connecting those dots.</div><br/><div id="37018511" class="c"><input type="checkbox" id="c-37018511" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37017094">root</a><span>|</span><a href="#37018436">parent</a><span>|</span><a href="#37017249">next</a><span>|</span><label class="collapse" for="c-37018511">[-]</label><label class="expand" for="c-37018511">[2 more]</label></div><br/><div class="children"><div class="content">Wow, using the IGP for the parts that don&#x27;t fit on the discrete GPU is a great idea.</div><br/><div id="37018895" class="c"><input type="checkbox" id="c-37018895" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37017094">root</a><span>|</span><a href="#37018511">parent</a><span>|</span><a href="#37017249">next</a><span>|</span><label class="collapse" for="c-37018895">[-]</label><label class="expand" for="c-37018895">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. It might be possible in llama.cpp soon, but the vulkan implementation may or may not be fast on the IGP.</div><br/></div></div></div></div></div></div></div></div><div id="37018801" class="c"><input type="checkbox" id="c-37018801" checked=""/><div class="controls bullet"><span class="by">radicaldreamer</span><span>|</span><a href="#37017094">prev</a><span>|</span><a href="#37018178">next</a><span>|</span><label class="collapse" for="c-37018801">[-]</label><label class="expand" for="c-37018801">[1 more]</label></div><br/><div class="children"><div class="content">You can do this stuff on a MacBook Pro these days... not sure why you&#x27;d want to be locked into another vendor here. Either use the best (OpenAI, Anthropic) or just roll your own.</div><br/></div></div><div id="37018178" class="c"><input type="checkbox" id="c-37018178" checked=""/><div class="controls bullet"><span class="by">hardwaresofton</span><span>|</span><a href="#37018801">prev</a><span>|</span><a href="#37019921">next</a><span>|</span><label class="collapse" for="c-37018178">[-]</label><label class="expand" for="c-37018178">[1 more]</label></div><br/><div class="children"><div class="content">Is this the true effect of Ultra Instinct^H^H Llama2?<p>Facebook is effectively supercharging the ecosystems and tool builders and <i>smaller</i> inference services.<p>This company had access to a credible, popular model (with an actual OSS license), and the relevant weights so they could optimize on it and sell the optimization without worrying about the license&#x2F;restrictions on the weights themselves.</div><br/></div></div><div id="37019921" class="c"><input type="checkbox" id="c-37019921" checked=""/><div class="controls bullet"><span class="by">ushakov</span><span>|</span><a href="#37018178">prev</a><span>|</span><a href="#37016743">next</a><span>|</span><label class="collapse" for="c-37019921">[-]</label><label class="expand" for="c-37019921">[1 more]</label></div><br/><div class="children"><div class="content">This seems more like a VC Pitchdeck rather than a technical paper explaining why their approach is better</div><br/></div></div><div id="37016743" class="c"><input type="checkbox" id="c-37016743" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#37019921">prev</a><span>|</span><a href="#37017075">next</a><span>|</span><label class="collapse" for="c-37016743">[-]</label><label class="expand" for="c-37016743">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t FasterTransformer (NVidia, OSS) and text-generation-inference (Huggingface, not OSS) are faster than this?</div><br/></div></div><div id="37017075" class="c"><input type="checkbox" id="c-37017075" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#37016743">prev</a><span>|</span><a href="#37016868">next</a><span>|</span><label class="collapse" for="c-37017075">[-]</label><label class="expand" for="c-37017075">[1 more]</label></div><br/><div class="children"><div class="content">Another AI startup grift, using GGML and closing it up to beg for VC cash.<p>Yet another AI wrapper company doing the same thing and jumping on the LLM hype train before it dries up.<p>If it is not open source and it is closed, it is immediately dead in the water.</div><br/></div></div><div id="37016868" class="c"><input type="checkbox" id="c-37016868" checked=""/><div class="controls bullet"><span class="by">drtournier</span><span>|</span><a href="#37017075">prev</a><span>|</span><a href="#37016624">next</a><span>|</span><label class="collapse" for="c-37016868">[-]</label><label class="expand" for="c-37016868">[1 more]</label></div><br/><div class="children"><div class="content">MKML == abstractions and wrappers for GGML?</div><br/></div></div><div id="37016624" class="c"><input type="checkbox" id="c-37016624" checked=""/><div class="controls bullet"><span class="by">pestatije</span><span>|</span><a href="#37016868">prev</a><span>|</span><a href="#37017916">next</a><span>|</span><label class="collapse" for="c-37016624">[-]</label><label class="expand" for="c-37016624">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Today, we’re announcing our first product, MKML. MKML is a software package that can reduce LLM inference costs on GPUs by 2x with just a few lines of Python code. And it is plug and play with popular ecosystems like Hugging Face and PyTorch</div><br/><div id="37016827" class="c"><input type="checkbox" id="c-37016827" checked=""/><div class="controls bullet"><span class="by">cududa</span><span>|</span><a href="#37016624">parent</a><span>|</span><a href="#37017916">next</a><span>|</span><label class="collapse" for="c-37016827">[-]</label><label class="expand" for="c-37016827">[3 more]</label></div><br/><div class="children"><div class="content">No judgement, but I’m genuinely curious why you saw the need to comment with a random sentence in their post?</div><br/><div id="37016870" class="c"><input type="checkbox" id="c-37016870" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#37016624">root</a><span>|</span><a href="#37016827">parent</a><span>|</span><a href="#37019266">next</a><span>|</span><label class="collapse" for="c-37016870">[-]</label><label class="expand" for="c-37016870">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a random sentence, it&#x27;s the main sentence everyone wants to read. They posted it to be helpful.</div><br/></div></div><div id="37019266" class="c"><input type="checkbox" id="c-37019266" checked=""/><div class="controls bullet"><span class="by">pestatije</span><span>|</span><a href="#37016624">root</a><span>|</span><a href="#37016827">parent</a><span>|</span><a href="#37016870">prev</a><span>|</span><a href="#37017916">next</a><span>|</span><label class="collapse" for="c-37019266">[-]</label><label class="expand" for="c-37019266">[1 more]</label></div><br/><div class="children"><div class="content">No judgement taken...i try to go through hn article headers as quickly as possible. If theres an idiot that thinks &quot;MK-1&quot; is an appropriate title id prefer they dont bother to be honest. Missing that, i go through some comments to find out what is it about. If i have to waste minutes to find out what it is about then ill go and comment a summary</div><br/></div></div></div></div></div></div><div id="37017916" class="c"><input type="checkbox" id="c-37017916" checked=""/><div class="controls bullet"><span class="by">mugivarra69</span><span>|</span><a href="#37016624">prev</a><span>|</span><a href="#37017887">next</a><span>|</span><label class="collapse" for="c-37017916">[-]</label><label class="expand" for="c-37017916">[1 more]</label></div><br/><div class="children"><div class="content">we have no moat and so does not openai is getting better by time.</div><br/></div></div></div></div></div></div></div></body></html>