<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704790865241" as="style"/><link rel="stylesheet" href="styles.css?v=1704790865241"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.solvermax.com/blog/solver-performance-1989-vs-2024">Solver Performance: 1989 vs. 2024</a> <span class="domain">(<a href="https://www.solvermax.com">www.solvermax.com</a>)</span></div><div class="subtext"><span>stncls</span> | <span>66 comments</span></div><br/><div><div id="38917206" class="c"><input type="checkbox" id="c-38917206" checked=""/><div class="controls bullet"><span class="by">ngruhn</span><span>|</span><a href="#38917301">next</a><span>|</span><label class="collapse" for="c-38917206">[-]</label><label class="expand" for="c-38917206">[34 more]</label></div><br/><div class="children"><div class="content">These solvers really show that NP-hardness is no reason to give up. For example, they can solve surprisingly large Traveling Salesmen instances to proven optimality.</div><br/><div id="38917468" class="c"><input type="checkbox" id="c-38917468" checked=""/><div class="controls bullet"><span class="by">codeflo</span><span>|</span><a href="#38917206">parent</a><span>|</span><a href="#38917243">next</a><span>|</span><label class="collapse" for="c-38917468">[-]</label><label class="expand" for="c-38917468">[13 more]</label></div><br/><div class="children"><div class="content">Solvers aren&#x27;t magic. But it turns out that many naturally occurring instances of NP-hard problems aren&#x27;t the &quot;really hard&quot; instances that the NP-hardness proof depends on.<p>Solvers can also fail for really tiny problems. You simply have to try to figure out how hard (or how well adapted to the solver&#x27;s heuristics) your particular problem instance is.</div><br/><div id="38917905" class="c"><input type="checkbox" id="c-38917905" checked=""/><div class="controls bullet"><span class="by">ngruhn</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917468">parent</a><span>|</span><a href="#38920708">next</a><span>|</span><label class="collapse" for="c-38917905">[-]</label><label class="expand" for="c-38917905">[6 more]</label></div><br/><div class="children"><div class="content">I’m saying this because most people got from their CS degree that NP hardness is practically a death sentence. But it‘s really not true. Even if the problem is proven to be NP hard (or worse) and every previous approach has failed. There can still be some trick or technique (non heuristic!) that brings a breakthrough. Maybe you still wait 10^whatever years for a solution in some cases. But if you get an answer in seconds in 99.9% of cases then its not a big deal in practice.<p>Even the famous SAT problem can almost be considered solved nowadays with the solvers we have.</div><br/><div id="38918325" class="c"><input type="checkbox" id="c-38918325" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917905">parent</a><span>|</span><a href="#38917947">next</a><span>|</span><label class="collapse" for="c-38918325">[-]</label><label class="expand" for="c-38918325">[4 more]</label></div><br/><div class="children"><div class="content">The true benefit of being able to tell NP-complete&#x2F;NP-hard from the garden variety bucket-o-bytes moving is not in giving up when you encounter them, as you correctly identified, but in knowing that even attempting an optimal solution is futile and proceeding to looking for approximate solutions to the business problem more or less instantly.<p>People unfamiliar with the matter may attempt to solve the problem, might even get rewarded for effort and hard work if management is also unfamiliar with the matter. Maybe it&#x27;s fine though...?</div><br/><div id="38918862" class="c"><input type="checkbox" id="c-38918862" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918325">parent</a><span>|</span><a href="#38920730">next</a><span>|</span><label class="collapse" for="c-38918862">[-]</label><label class="expand" for="c-38918862">[2 more]</label></div><br/><div class="children"><div class="content">Optimal is overblown for business problems in general. Knowing there’s a mathematically optimal solution, people want it. Even if it’s practically impossible to get. It feels like if you have the optimal, you don’t have to consider trade offs (not usually true).<p>Having a solution within 1% of optimal with ten nines of confidence is usually indistinguishable.<p>Anyone ever notice how these CS problems are rarely famous business problems? It’s because the pure problems don’t usually matter and the approximate versions are usually quite easy. You almost never need the shortest route for a salesman, or need to pick a secretary with no chance of going back.</div><br/><div id="38923758" class="c"><input type="checkbox" id="c-38923758" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918862">parent</a><span>|</span><a href="#38920730">next</a><span>|</span><label class="collapse" for="c-38923758">[-]</label><label class="expand" for="c-38923758">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m quite convinced that if you had a 1% better solution to the salesman problem than what fedex or ups currently have, they&#x27;ll pay good money, though :)</div><br/></div></div></div></div><div id="38920730" class="c"><input type="checkbox" id="c-38920730" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918325">parent</a><span>|</span><a href="#38918862">prev</a><span>|</span><a href="#38917947">next</a><span>|</span><label class="collapse" for="c-38920730">[-]</label><label class="expand" for="c-38920730">[1 more]</label></div><br/><div class="children"><div class="content">Yes, exactly this. You don&#x27;t need the optimal solution in most cases, you just need <i>a</i> solution, and if it is 9x% there then the optimum solution can be approximated by burning more cycles but from an economics perspective you may well already have a viable solution in hand.</div><br/></div></div></div></div><div id="38917947" class="c"><input type="checkbox" id="c-38917947" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917905">parent</a><span>|</span><a href="#38918325">prev</a><span>|</span><a href="#38920708">next</a><span>|</span><label class="collapse" for="c-38917947">[-]</label><label class="expand" for="c-38917947">[1 more]</label></div><br/><div class="children"><div class="content">It also depends on if you actually need the optimal solution. Getting a solution in your time budget, even if not guaranteed to be optimal, can be appropriate for many problems.</div><br/></div></div></div></div><div id="38920708" class="c"><input type="checkbox" id="c-38920708" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917468">parent</a><span>|</span><a href="#38917905">prev</a><span>|</span><a href="#38917944">next</a><span>|</span><label class="collapse" for="c-38920708">[-]</label><label class="expand" for="c-38920708">[1 more]</label></div><br/><div class="children"><div class="content">NP hard for a pathological case doesn&#x27;t mean <i>all</i> practical cases are pathological. Many of the cases have optimal solutions in a reasonable amount of time without resorting to brute force because you can use structure in the dataset to limit the search space.<p>That you can construct a pathological case makes something NP hard for an arbitrary case but not for all cases. Compare with QS: it&#x27;s very fast for most practical cases but you <i>can</i> construct a case where it performs quite bad, much worse than you&#x27;d expect given the name. But in practice that isn&#x27;t all that relevant.</div><br/></div></div><div id="38917944" class="c"><input type="checkbox" id="c-38917944" checked=""/><div class="controls bullet"><span class="by">thechao</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917468">parent</a><span>|</span><a href="#38920708">prev</a><span>|</span><a href="#38918795">next</a><span>|</span><label class="collapse" for="c-38917944">[-]</label><label class="expand" for="c-38917944">[1 more]</label></div><br/><div class="children"><div class="content">This reminds me of the random polynomial time bound for the simplex algorithm; an algorithm that is, naively, exponential time[1].<p>I seem to vaguely remember — this is ~17 years ago, now — that it&#x27;s possible to &quot;characterize&quot; the really bad edge-cases for simplex and work around them (the whole point of the paper).<p>[1] <a href="https:&#x2F;&#x2F;cstheory.stackexchange.com&#x2F;questions&#x2F;2373&#x2F;complexity-of-the-simplex-algorithm#2374" rel="nofollow">https:&#x2F;&#x2F;cstheory.stackexchange.com&#x2F;questions&#x2F;2373&#x2F;complexity...</a></div><br/></div></div><div id="38918795" class="c"><input type="checkbox" id="c-38918795" checked=""/><div class="controls bullet"><span class="by">ur-whale</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917468">parent</a><span>|</span><a href="#38917944">prev</a><span>|</span><a href="#38917243">next</a><span>|</span><label class="collapse" for="c-38918795">[-]</label><label class="expand" for="c-38918795">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Solvers can also fail for really tiny problems.<p>What that tells me is that the current metric we use for problem complexity (e.g. big-o) is woefully inadequate at measuring the actual complexity of problems.</div><br/><div id="38923339" class="c"><input type="checkbox" id="c-38923339" checked=""/><div class="controls bullet"><span class="by">blt</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918795">parent</a><span>|</span><a href="#38919134">next</a><span>|</span><label class="collapse" for="c-38923339">[-]</label><label class="expand" for="c-38923339">[1 more]</label></div><br/><div class="children"><div class="content">Lots of CS theorists are working on non worst case analysis and have been for some time. The CS research community recognizes the limitations of worst case.<p>It makes sense to teach worst case in undergrad classes because it&#x27;s easier to understand and basically a prerequisite for other kinds of analysis.</div><br/></div></div><div id="38919134" class="c"><input type="checkbox" id="c-38919134" checked=""/><div class="controls bullet"><span class="by">kaba0</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918795">parent</a><span>|</span><a href="#38923339">prev</a><span>|</span><a href="#38920766">next</a><span>|</span><label class="collapse" for="c-38919134">[-]</label><label class="expand" for="c-38919134">[1 more]</label></div><br/><div class="children"><div class="content">Why would they be inadequate? They are a very good metric that lets one instantly recognize the way the problem’s certain properties change with respect to some other factor. Does it give a complete picture for everything? No. Why would it? But to say that they are woefully inadequate is just ridiculous.</div><br/></div></div><div id="38920766" class="c"><input type="checkbox" id="c-38920766" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918795">parent</a><span>|</span><a href="#38919134">prev</a><span>|</span><a href="#38917243">next</a><span>|</span><label class="collapse" for="c-38920766">[-]</label><label class="expand" for="c-38920766">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the current metric we use for problem complexity (e.g. big-o) is woefully inadequate at measuring the actual complexity of problems.<p>The complexity of <i>all</i> problems. But big-O isn&#x27;t the only complexity metric available.<p>It&#x27;s extremely useful and very adequate in <i>almost</i> all cases, but it doesn&#x27;t work well when the numbers are very small and the problems are part of a small subset of all available problems.<p>But those are edge cases. In practice those are fairly rare and when the datasets are small enough normally all solutions are more or less viable. But as soon as your data set is non-trivial big-O is the right tool to apply at the outset.<p>Right tool for the job... small dataset, tricky problem: big-O may not apply.</div><br/></div></div></div></div></div></div><div id="38917243" class="c"><input type="checkbox" id="c-38917243" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#38917206">parent</a><span>|</span><a href="#38917468">prev</a><span>|</span><a href="#38918106">next</a><span>|</span><label class="collapse" for="c-38917243">[-]</label><label class="expand" for="c-38917243">[14 more]</label></div><br/><div class="children"><div class="content">NP-hard problems commonly come up as human-solvable puzzles. Like Sudoku... or perhaps a more applicable problem... layout and routing of electronic components on a PCB and&#x2F;or chip. Or even assembly-language register allocation (coloring and packing problem).<p>Trained Humans are surprisingly good at these problems, far better than expected given how much computational power we have today. So its clear we don&#x27;t understand something fundamental with regards to NP-hardness. And that&#x27;s why the research continues, to bring human-like intuition into these problems and provide more automation to these tasks.</div><br/><div id="38917666" class="c"><input type="checkbox" id="c-38917666" checked=""/><div class="controls bullet"><span class="by">mistercow</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917243">parent</a><span>|</span><a href="#38919225">next</a><span>|</span><label class="collapse" for="c-38917666">[-]</label><label class="expand" for="c-38917666">[4 more]</label></div><br/><div class="children"><div class="content">I’m not sure there’s necessarily going to be a satisfying general answer to find here though. I can invent an NP-complete problem with very easy instances by combining a linear time problem with an NP-complete problem, e.g. “Does this list of cities have a complete route shorter than X OR is one of the cities New York?”<p>All of the “yes New York” solutions are easy, but there’s no deep, satisfying reason for that. It’s just a hard problem glued onto an easy problem. For all we know, a lot of NP-hard problems could be like that, and the structure of the easy instances could be essentially unrelated between problems.</div><br/><div id="38917873" class="c"><input type="checkbox" id="c-38917873" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917666">parent</a><span>|</span><a href="#38919225">next</a><span>|</span><label class="collapse" for="c-38917873">[-]</label><label class="expand" for="c-38917873">[3 more]</label></div><br/><div class="children"><div class="content">Maybe the answer then is to find what these hard cases are and use them in heuristics or approximate algorithms? If we could tell when part of the problem is hard, and maybe bound the error for them, and use an exact algorithm for other parts of the problem, you could get a better result in most cases without it blowing up on you.</div><br/><div id="38919171" class="c"><input type="checkbox" id="c-38919171" checked=""/><div class="controls bullet"><span class="by">kaba0</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917873">parent</a><span>|</span><a href="#38919225">next</a><span>|</span><label class="collapse" for="c-38919171">[-]</label><label class="expand" for="c-38919171">[2 more]</label></div><br/><div class="children"><div class="content">Well, you can just start an exact solver on the problem in one thread, and an approximate algorithm on another. If the former doesn’t finish in n seconds, you cancel it and use the result of the latter.</div><br/><div id="38923108" class="c"><input type="checkbox" id="c-38923108" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38919171">parent</a><span>|</span><a href="#38919225">next</a><span>|</span><label class="collapse" for="c-38923108">[-]</label><label class="expand" for="c-38923108">[1 more]</label></div><br/><div class="children"><div class="content">The advantage of combining them though is that you might be able to treat different subparts of the problem differently (which is the hard part), so that you could use an approximate algorithm for the hard part of it, and an exact algorithm for the easy part.<p>Thi of course assumes the problem can be divided in this way, which is fairly speculative.</div><br/></div></div></div></div></div></div></div></div><div id="38919225" class="c"><input type="checkbox" id="c-38919225" checked=""/><div class="controls bullet"><span class="by">kaba0</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917243">parent</a><span>|</span><a href="#38917666">prev</a><span>|</span><a href="#38917350">next</a><span>|</span><label class="collapse" for="c-38919225">[-]</label><label class="expand" for="c-38919225">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Trained Humans are surprisingly good at these problems<p>Are we? I don’t think we would even start working on problems with big enough `n` where the complexity actually ramps up.<p>Like, optimally scheduling even just a couple of things will have a shitton of combinations, and I really doubt we would be good at it.<p>Good at iteratively decreasing a cost function? Yeah, I guess with a good interface we could move around tasks to be scheduled on a timeline and optimize it. Finding the optimum? No chance.</div><br/><div id="38920878" class="c"><input type="checkbox" id="c-38920878" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38919225">parent</a><span>|</span><a href="#38917350">next</a><span>|</span><label class="collapse" for="c-38920878">[-]</label><label class="expand" for="c-38920878">[1 more]</label></div><br/><div class="children"><div class="content">Our monkey brains can get within a few % of optimal on written TSP problems.<p>For a lot of these problems, having a tight upper bound lets you narrow the search space, regardless of whether you&#x27;re looking for the ideal answer or simply a less-bad one (Would you turn down saving $500,000 on fuel and labor in a year just because someone thinks $613,000 is the maximum savings achievable?)<p>The more we can get automation to do approximations as well as humans, the better.</div><br/></div></div></div></div><div id="38917350" class="c"><input type="checkbox" id="c-38917350" checked=""/><div class="controls bullet"><span class="by">azornathogron</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917243">parent</a><span>|</span><a href="#38919225">prev</a><span>|</span><a href="#38917595">next</a><span>|</span><label class="collapse" for="c-38917350">[-]</label><label class="expand" for="c-38917350">[3 more]</label></div><br/><div class="children"><div class="content">I do find this interesting...<p>&gt; Trained Humans are surprisingly good at these problems<p>Surprising why, and by what metric? (speed? Or quality of solution?)</div><br/><div id="38917430" class="c"><input type="checkbox" id="c-38917430" checked=""/><div class="controls bullet"><span class="by">codeflo</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917350">parent</a><span>|</span><a href="#38917595">next</a><span>|</span><label class="collapse" for="c-38917430">[-]</label><label class="expand" for="c-38917430">[2 more]</label></div><br/><div class="children"><div class="content">A problem is NP hard if there are instances that other hard problems reduce to. That is, only <i>some</i> instances of the problem have to be hard. NP hardness says nothing about &quot;average&quot; problem instances and even less about hand-picked ones.<p>And that&#x27;s what Sudoku puzzles are. They are made for humans to solve. They are specifically crafted to contain challenging, but possible, solution paths.</div><br/><div id="38917612" class="c"><input type="checkbox" id="c-38917612" checked=""/><div class="controls bullet"><span class="by">azornathogron</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917430">parent</a><span>|</span><a href="#38917595">next</a><span>|</span><label class="collapse" for="c-38917612">[-]</label><label class="expand" for="c-38917612">[1 more]</label></div><br/><div class="children"><div class="content">I was asking about why specifically it&#x27;s surprising that humans are good at such problems. I think that to be surprising there must be some prior expectation about how good humans should be and then evidence that they&#x27;re actually better than that. Both sides of that are interesting and I&#x27;d like to know more about it: What is the prior expectation of human capabilities here (and why), and how does it compare to actual observed capabilities.<p>I&#x27;m not sure sudoku is a good example since the problem size there is so small that I don&#x27;t have any intuitive sense that it should be something humans can&#x27;t do.</div><br/></div></div></div></div></div></div><div id="38917595" class="c"><input type="checkbox" id="c-38917595" checked=""/><div class="controls bullet"><span class="by">cyberax</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917243">parent</a><span>|</span><a href="#38917350">prev</a><span>|</span><a href="#38917607">next</a><span>|</span><label class="collapse" for="c-38917595">[-]</label><label class="expand" for="c-38917595">[1 more]</label></div><br/><div class="children"><div class="content">Hmm.... A neural network for register allocation? Might be an interesting experiment to try.</div><br/></div></div><div id="38917607" class="c"><input type="checkbox" id="c-38917607" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917243">parent</a><span>|</span><a href="#38917595">prev</a><span>|</span><a href="#38918106">next</a><span>|</span><label class="collapse" for="c-38917607">[-]</label><label class="expand" for="c-38917607">[3 more]</label></div><br/><div class="children"><div class="content">Sudoku puzzles are usually set by humans.</div><br/><div id="38918118" class="c"><input type="checkbox" id="c-38918118" checked=""/><div class="controls bullet"><span class="by">verve_rat</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917607">parent</a><span>|</span><a href="#38918106">next</a><span>|</span><label class="collapse" for="c-38918118">[-]</label><label class="expand" for="c-38918118">[2 more]</label></div><br/><div class="children"><div class="content">Not really, as I understand it. The big blow up of sudoku books full of puzzles came about because some guy figured out how to create them with a program, rather than by hand. Earned the guy millions I believe.<p>Edit: Wayne Gould <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Wayne_Gould" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Wayne_Gould</a></div><br/><div id="38918993" class="c"><input type="checkbox" id="c-38918993" checked=""/><div class="controls bullet"><span class="by">mrloba</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918118">parent</a><span>|</span><a href="#38918106">next</a><span>|</span><label class="collapse" for="c-38918993">[-]</label><label class="expand" for="c-38918993">[1 more]</label></div><br/><div class="children"><div class="content">People who are more into it usually prefer human made puzzles since they often have a logical path that you&#x27;re supposed to find, which can be quite satisfying. Generating sudoku puzzles is actually quite easy. Just put random numbers on the board until you have a unique solution. It runs surprisingly fast. The tricky part is deciding on the difficulty. I made a program that would identify all the different sudoku techniques needed to solve each puzzle (x wings, pairs, all the way up to chains), then set the difficulty based on what techniques were required. Code is here for anyone interested: <a href="https:&#x2F;&#x2F;github.com&#x2F;magnusjt&#x2F;sudoku&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;magnusjt&#x2F;sudoku&#x2F;</a>
Sadly I don&#x27;t think anyone would pay millions for this anymore</div><br/></div></div></div></div></div></div></div></div><div id="38918106" class="c"><input type="checkbox" id="c-38918106" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#38917206">parent</a><span>|</span><a href="#38917243">prev</a><span>|</span><a href="#38917757">next</a><span>|</span><label class="collapse" for="c-38918106">[-]</label><label class="expand" for="c-38918106">[4 more]</label></div><br/><div class="children"><div class="content">&gt; they can solve surprisingly large Traveling Salesmen instances to proven optimality.<p>For someone who studies computational complexity theory, is the ability to solve some instances of NP-hard problems efficiently due more to these instances having lower average-case complexity than worst-case complexity, or because they possess structural properties that allow for more effective algorithmic exploitation?<p>More formally, are certain NP-hard problems easier to solve because the expected time to solve a randomly selected instance from their language is polynomial? Or is it because real-world instances are not uniformly distributed across the language, and these instances possess some amount of Kolmogorov compressibility that leads to better-than-expected performance?</div><br/><div id="38918579" class="c"><input type="checkbox" id="c-38918579" checked=""/><div class="controls bullet"><span class="by">tooltower</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918106">parent</a><span>|</span><a href="#38917757">next</a><span>|</span><label class="collapse" for="c-38918579">[-]</label><label class="expand" for="c-38918579">[3 more]</label></div><br/><div class="children"><div class="content">The latter. The real-world instances are not uniformly distributed across the language. It is pretty easy to randomly generate problems that are hard for current solvers.<p>Caveats:<p>* &quot;uniform distribution&quot; is a tricky concept over infinite countable sets. Famously, it doesn&#x27;t exist. You have to restrict the length or something.<p>* I have no idea if there&#x27;s a concrete result linking Kolmogorov
 complexity and solving ease. I suspect no, since even a complex but known problem can be solved rather quickly by special-casing.</div><br/><div id="38919510" class="c"><input type="checkbox" id="c-38919510" checked=""/><div class="controls bullet"><span class="by">danbruc</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918579">parent</a><span>|</span><a href="#38919499">next</a><span>|</span><label class="collapse" for="c-38919510">[-]</label><label class="expand" for="c-38919510">[1 more]</label></div><br/><div class="children"><div class="content">I played quite a bit with vertex three coloring in the past and it has a surprisingly sharp phase transition. If you randomly generate graphs by including each possible edge with probability p, then the graph will have average degree p×n. Don&#x27;t quote me on the exact numbers, but something like if the average degree is about 3, then the graph is usually hard to color. If it is only 2.9, then the graph is usually easy to color, if it is 3.1 then there is usually either no valid coloring at all or it is easy to find one. So of all the graphs with average degree from 0 to n, mostly only graphs with average degree in a narrow band around 3 are hard.</div><br/></div></div><div id="38919499" class="c"><input type="checkbox" id="c-38919499" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38918579">parent</a><span>|</span><a href="#38919510">prev</a><span>|</span><a href="#38917757">next</a><span>|</span><label class="collapse" for="c-38919499">[-]</label><label class="expand" for="c-38919499">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I have no idea if there&#x27;s a concrete result linking Kolmogorov complexity and solving ease<p>I’ve only heard of the incompressibility method but don’t know too much about the details: 
<a href="https:&#x2F;&#x2F;core.ac.uk&#x2F;download&#x2F;pdf&#x2F;301634196.pdf" rel="nofollow">https:&#x2F;&#x2F;core.ac.uk&#x2F;download&#x2F;pdf&#x2F;301634196.pdf</a></div><br/></div></div></div></div></div></div><div id="38917757" class="c"><input type="checkbox" id="c-38917757" checked=""/><div class="controls bullet"><span class="by">TimPC</span><span>|</span><a href="#38917206">parent</a><span>|</span><a href="#38918106">prev</a><span>|</span><a href="#38917301">next</a><span>|</span><label class="collapse" for="c-38917757">[-]</label><label class="expand" for="c-38917757">[2 more]</label></div><br/><div class="children"><div class="content">Most instances of a class of problems that are NP-hard are in fact easy. Usually NP-hard is something resembling exponential blow-up in a problematic edge case.</div><br/><div id="38920963" class="c"><input type="checkbox" id="c-38920963" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38917206">root</a><span>|</span><a href="#38917757">parent</a><span>|</span><a href="#38917301">next</a><span>|</span><label class="collapse" for="c-38920963">[-]</label><label class="expand" for="c-38920963">[1 more]</label></div><br/><div class="children"><div class="content">One of my favorite terms in this space is &#x27;relaxation&#x27;.<p>When you set yourself against the worst case scenario, you are destined to have a very bad time. But there are subsets of the problem where one or two details are treated as trivialities, while still keeping the rest of the problem &#x27;interesting&#x27; or &#x27;useful&#x27;.<p>Compression cannot compress white noise. But it turns out humans don&#x27;t find noise that interesting (except in the negative). We value signal, and meaning. Most of the communication we care to exchange has a much more straightforward message than the medium, and so we continue to find new ways to condense both the message, and the most valuable nuance, down.</div><br/></div></div></div></div></div></div><div id="38917301" class="c"><input type="checkbox" id="c-38917301" checked=""/><div class="controls bullet"><span class="by">ayhanfuat</span><span>|</span><a href="#38917206">prev</a><span>|</span><a href="#38916995">next</a><span>|</span><label class="collapse" for="c-38917301">[-]</label><label class="expand" for="c-38917301">[7 more]</label></div><br/><div class="children"><div class="content">Sadly, the field is still mostly dominated by commercial solvers. There are a few open source ones but their performance is nowhere near the commercial ones which is kind of expected given the number of people working on them and the little funding they have. It is really a pity that the OR world hasn&#x27;t embraced open source as much as ML world.</div><br/><div id="38917756" class="c"><input type="checkbox" id="c-38917756" checked=""/><div class="controls bullet"><span class="by">Chio</span><span>|</span><a href="#38917301">parent</a><span>|</span><a href="#38917404">next</a><span>|</span><label class="collapse" for="c-38917756">[-]</label><label class="expand" for="c-38917756">[1 more]</label></div><br/><div class="children"><div class="content">This is true, but there is HiGHS [1] which is &quot;good enough&quot; to use for a lot of real problems (though the commercial solvers are still better).<p>[1] <a href="https:&#x2F;&#x2F;highs.dev&#x2F;" rel="nofollow">https:&#x2F;&#x2F;highs.dev&#x2F;</a></div><br/></div></div><div id="38917404" class="c"><input type="checkbox" id="c-38917404" checked=""/><div class="controls bullet"><span class="by">crsn</span><span>|</span><a href="#38917301">parent</a><span>|</span><a href="#38917756">prev</a><span>|</span><a href="#38917552">next</a><span>|</span><label class="collapse" for="c-38917404">[-]</label><label class="expand" for="c-38917404">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m working actively on this area – anyone interested in helping should please get in touch! carson@spindle.app</div><br/></div></div><div id="38917552" class="c"><input type="checkbox" id="c-38917552" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#38917301">parent</a><span>|</span><a href="#38917404">prev</a><span>|</span><a href="#38917586">next</a><span>|</span><label class="collapse" for="c-38917552">[-]</label><label class="expand" for="c-38917552">[1 more]</label></div><br/><div class="children"><div class="content">The pricing of some of this software is absurd — easily enough for a company using more than one or two server licenses to dedicate an entire FTE toward open source.</div><br/></div></div><div id="38917586" class="c"><input type="checkbox" id="c-38917586" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#38917301">parent</a><span>|</span><a href="#38917552">prev</a><span>|</span><a href="#38916995">next</a><span>|</span><label class="collapse" for="c-38917586">[-]</label><label class="expand" for="c-38917586">[3 more]</label></div><br/><div class="children"><div class="content">The thing is, ML building blocks are very simple and composable, I don&#x27;t think that holds for solvers.</div><br/><div id="38923407" class="c"><input type="checkbox" id="c-38923407" checked=""/><div class="controls bullet"><span class="by">petters</span><span>|</span><a href="#38917301">root</a><span>|</span><a href="#38917586">parent</a><span>|</span><a href="#38920938">next</a><span>|</span><label class="collapse" for="c-38923407">[-]</label><label class="expand" for="c-38923407">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I think this is true. I&#x27;ve worked in both fields. But we should be really happy about the fact that we did not end up in the same place with ML.</div><br/></div></div><div id="38920938" class="c"><input type="checkbox" id="c-38920938" checked=""/><div class="controls bullet"><span class="by">owlbite</span><span>|</span><a href="#38917301">root</a><span>|</span><a href="#38917586">parent</a><span>|</span><a href="#38923407">prev</a><span>|</span><a href="#38916995">next</a><span>|</span><label class="collapse" for="c-38920938">[-]</label><label class="expand" for="c-38920938">[1 more]</label></div><br/><div class="children"><div class="content">Also ML gets a lot more funding (academic or commercial) than MILP.</div><br/></div></div></div></div></div></div><div id="38916995" class="c"><input type="checkbox" id="c-38916995" checked=""/><div class="controls bullet"><span class="by">Epa095</span><span>|</span><a href="#38917301">prev</a><span>|</span><a href="#38923549">next</a><span>|</span><label class="collapse" for="c-38916995">[-]</label><label class="expand" for="c-38916995">[3 more]</label></div><br/><div class="children"><div class="content">A bit disappointed that there were no reimplementation and real benchmarking happening.</div><br/><div id="38923687" class="c"><input type="checkbox" id="c-38923687" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#38916995">parent</a><span>|</span><a href="#38917084">next</a><span>|</span><label class="collapse" for="c-38923687">[-]</label><label class="expand" for="c-38923687">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. In particular here I suspect the perf is dramatically worse than 1800s&#x2F;20e9=90ns. Constant factors are real-world annoyance :)</div><br/></div></div><div id="38917084" class="c"><input type="checkbox" id="c-38917084" checked=""/><div class="controls bullet"><span class="by">Jtsummers</span><span>|</span><a href="#38916995">parent</a><span>|</span><a href="#38923687">prev</a><span>|</span><a href="#38923549">next</a><span>|</span><label class="collapse" for="c-38917084">[-]</label><label class="expand" for="c-38917084">[1 more]</label></div><br/><div class="children"><div class="content">Next article apparently. I was very much looking forward to that, too.<p>&gt; In our next article, we&#x27;ll look to compile crosswords using a MILP model.<p>and<p>&gt; In the next article, we attempt to formulate and solve a MILP to compile crossword puzzles. This is still a difficult problem, though given the improvement in computer and solver speed, there is some basis for optimism that the task is now possible.</div><br/></div></div></div></div><div id="38923549" class="c"><input type="checkbox" id="c-38923549" checked=""/><div class="controls bullet"><span class="by">LunaSea</span><span>|</span><a href="#38916995">prev</a><span>|</span><a href="#38921093">next</a><span>|</span><label class="collapse" for="c-38923549">[-]</label><label class="expand" for="c-38923549">[1 more]</label></div><br/><div class="children"><div class="content">Are there good reference books on solver implementations?<p>I tried diving into the subject using online references but found them lacking in context and explanations sometimes.</div><br/></div></div><div id="38921093" class="c"><input type="checkbox" id="c-38921093" checked=""/><div class="controls bullet"><span class="by">markwkw</span><span>|</span><a href="#38923549">prev</a><span>|</span><a href="#38919724">next</a><span>|</span><label class="collapse" for="c-38921093">[-]</label><label class="expand" for="c-38921093">[1 more]</label></div><br/><div class="children"><div class="content">I wish they tried to solve the 1989 4x4 crossword puzzle optimization with a modern solver, but a small memory limit (~8MB) and perhaps a severely underclocked CPU  to showcase the algorithm improvements.</div><br/></div></div><div id="38919724" class="c"><input type="checkbox" id="c-38919724" checked=""/><div class="controls bullet"><span class="by">soulbadguy</span><span>|</span><a href="#38921093">prev</a><span>|</span><a href="#38916935">next</a><span>|</span><label class="collapse" for="c-38919724">[-]</label><label class="expand" for="c-38919724">[1 more]</label></div><br/><div class="children"><div class="content">Anyone has a good literature review (or anything similar) on AI technics applied to ILP&#x2F;constraints solver ?<p>I have seen a couple of result for specific domain ( like place and route ) but I am wondering how those new technics fair in more general settings</div><br/></div></div><div id="38916935" class="c"><input type="checkbox" id="c-38916935" checked=""/><div class="controls bullet"><span class="by">datadrivenangel</span><span>|</span><a href="#38919724">prev</a><span>|</span><a href="#38923688">next</a><span>|</span><label class="collapse" for="c-38916935">[-]</label><label class="expand" for="c-38916935">[10 more]</label></div><br/><div class="children"><div class="content">&quot;Combining the computer hardware speed increase of 4,000 times with the solver software performance improvement of 5 million times, the total improvement from 1989 to 2024 is a factor of 20 billion times faster!&quot;<p>No wonder people have started making jokes that programmers no longer know how to program! We can get away with a lot more now.</div><br/><div id="38917011" class="c"><input type="checkbox" id="c-38917011" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#38916935">parent</a><span>|</span><a href="#38917941">next</a><span>|</span><label class="collapse" for="c-38917011">[-]</label><label class="expand" for="c-38917011">[3 more]</label></div><br/><div class="children"><div class="content">The software improvements are on an order of 1000x more than the hardware improvements.<p>IE: The software matters more. As in, today&#x27;s programmers know more about this subject.</div><br/><div id="38917322" class="c"><input type="checkbox" id="c-38917322" checked=""/><div class="controls bullet"><span class="by">theLiminator</span><span>|</span><a href="#38916935">root</a><span>|</span><a href="#38917011">parent</a><span>|</span><a href="#38920921">next</a><span>|</span><label class="collapse" for="c-38917322">[-]</label><label class="expand" for="c-38917322">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say on average programmers know less about writing optimal code now, but certainly the best in the field know a lot more.</div><br/></div></div><div id="38920921" class="c"><input type="checkbox" id="c-38920921" checked=""/><div class="controls bullet"><span class="by">owlbite</span><span>|</span><a href="#38916935">root</a><span>|</span><a href="#38917011">parent</a><span>|</span><a href="#38917322">prev</a><span>|</span><a href="#38917941">next</a><span>|</span><label class="collapse" for="c-38920921">[-]</label><label class="expand" for="c-38920921">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say that the algorithm matters more. As in, today&#x27;s mathematicians know more about the subject.<p>I suspect the actual implementation of said algorithms probably achieves a lower % of peak performance than the older ones (though to be fair they are &#x2F;much&#x2F; more complex algorithms).</div><br/></div></div></div></div><div id="38917941" class="c"><input type="checkbox" id="c-38917941" checked=""/><div class="controls bullet"><span class="by">wiredfool</span><span>|</span><a href="#38916935">parent</a><span>|</span><a href="#38917011">prev</a><span>|</span><a href="#38921062">next</a><span>|</span><label class="collapse" for="c-38917941">[-]</label><label class="expand" for="c-38917941">[4 more]</label></div><br/><div class="children"><div class="content">I went to a Guest lecture in ~96 on performance of supercomputing and applications to FEA, so basically matrix factoring.<p>In the time from the Cray 1 -&gt; then, there were 6 orders of magnitude of hardware gains, and 6 orders of magnitude in software as well.</div><br/><div id="38919529" class="c"><input type="checkbox" id="c-38919529" checked=""/><div class="controls bullet"><span class="by">nwallin</span><span>|</span><a href="#38916935">root</a><span>|</span><a href="#38917941">parent</a><span>|</span><a href="#38921062">next</a><span>|</span><label class="collapse" for="c-38919529">[-]</label><label class="expand" for="c-38919529">[3 more]</label></div><br/><div class="children"><div class="content">Matrix factoring as in LU, Cholesky, QR, SVD etc? 6 orders of magnitude from mid-70s to mid-90s?<p>Unless I&#x27;m misunderstanding I&#x27;m shocked that there was that much left on the table.</div><br/><div id="38919656" class="c"><input type="checkbox" id="c-38919656" checked=""/><div class="controls bullet"><span class="by">wiredfool</span><span>|</span><a href="#38916935">root</a><span>|</span><a href="#38919529">parent</a><span>|</span><a href="#38920931">next</a><span>|</span><label class="collapse" for="c-38919656">[-]</label><label class="expand" for="c-38919656">[1 more]</label></div><br/><div class="children"><div class="content">I think it went from naïve gaussian through LU and SVD to approximate iterative forms for the top eigenvectors&#x2F;values.  So a good portion of that was not computing the higher order terms that didn&#x27;t significantly contribute to the results.<p>Hazy memory though, as it was 25 years back and I&#x27;ve been out of the FEA side of things for 20+ years now.<p>I will say though -- I was doing some stuff at the time that was burying SuperSparcs for 24 hours at a time, and would now probably run realtime on a watch or phone. (Again, a big mix of hardware advancement, reduced precision for insignificant terms, and generally optimized algos)</div><br/></div></div><div id="38920931" class="c"><input type="checkbox" id="c-38920931" checked=""/><div class="controls bullet"><span class="by">owlbite</span><span>|</span><a href="#38916935">root</a><span>|</span><a href="#38919529">parent</a><span>|</span><a href="#38919656">prev</a><span>|</span><a href="#38921062">next</a><span>|</span><label class="collapse" for="c-38920931">[-]</label><label class="expand" for="c-38920931">[1 more]</label></div><br/><div class="children"><div class="content">FEAs probably involve sparse matrices, which have a lot more complexity than simple dense matrices. For example compute optimal reordering of a generic sparse matrix is iirc NP-complete.</div><br/></div></div></div></div></div></div><div id="38921062" class="c"><input type="checkbox" id="c-38921062" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38916935">parent</a><span>|</span><a href="#38917941">prev</a><span>|</span><a href="#38917139">next</a><span>|</span><label class="collapse" for="c-38921062">[-]</label><label class="expand" for="c-38921062">[1 more]</label></div><br/><div class="children"><div class="content">The problem with factorials is that 20 billion doesn&#x27;t necessarily mean much.<p>Let&#x27;s say we could solve a problem of 100 elements 35 years ago, with 100! operations. If the 2x10^10 multiplier only made the exact same calculation but faster, that would let you solve n = 105 in the same time. Business problems don&#x27;t grow that slowly. Not over 35 years.<p>You have to get better at solving the problem in less than n! steps by culling impossible scenarios, and doing it more aggressively.</div><br/></div></div><div id="38917139" class="c"><input type="checkbox" id="c-38917139" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#38916935">parent</a><span>|</span><a href="#38921062">prev</a><span>|</span><a href="#38923688">next</a><span>|</span><label class="collapse" for="c-38917139">[-]</label><label class="expand" for="c-38917139">[1 more]</label></div><br/><div class="children"><div class="content">Of course, it takes knowing how to program to take advantage of the solver improvements.  I find that most programs do not, in fact, use any solver techniques.  More, I think I can program, but I would also have trouble using solvers in a lot of my programs.</div><br/></div></div></div></div><div id="38923688" class="c"><input type="checkbox" id="c-38923688" checked=""/><div class="controls bullet"><span class="by">selimnairb</span><span>|</span><a href="#38916935">prev</a><span>|</span><a href="#38918856">next</a><span>|</span><label class="collapse" for="c-38923688">[-]</label><label class="expand" for="c-38923688">[1 more]</label></div><br/><div class="children"><div class="content">Yet more reasons to try more central planning rather than rely on analog markets. If it’s good enough for large Capitalist firms, it should be good enough for sectors of the economy.</div><br/></div></div><div id="38918856" class="c"><input type="checkbox" id="c-38918856" checked=""/><div class="controls bullet"><span class="by">ur-whale</span><span>|</span><a href="#38923688">prev</a><span>|</span><a href="#38918834">next</a><span>|</span><label class="collapse" for="c-38918856">[-]</label><label class="expand" for="c-38918856">[3 more]</label></div><br/><div class="children"><div class="content">So solvers are now much faster, but I haven&#x27;t found a single hint in the article  as to <i>how</i> they got faster (aside from the &quot;more memory&quot;, &quot;more CPU&quot; aspect).<p>Was there a major theoretical development in the solver field that allowed this to happen ?<p>Or is it a bunch of tiny tuning heuristics ?<p>If so, how are those even possible given that the solver is supposed to be a generic tool applicable to a large class of problem ?<p>Are there problems whose structure fit a recognizable pattern where optimizations are possible ?<p>I confess to being left hanging by the article.</div><br/><div id="38919415" class="c"><input type="checkbox" id="c-38919415" checked=""/><div class="controls bullet"><span class="by">ngruhn</span><span>|</span><a href="#38918856">parent</a><span>|</span><a href="#38919977">next</a><span>|</span><label class="collapse" for="c-38919415">[-]</label><label class="expand" for="c-38919415">[1 more]</label></div><br/><div class="children"><div class="content">The details are pretty math heavy but what these solvers are doing is they try to find an optimal assignment to a bunch of variables x,y,z,etc while respecting a bunch of constraints like<p>3x + 2y &lt;= 10<p>4z &gt;= 3.5<p>Additionally, there is an “objective function” that defines what optimal means. Something like:<p>maximize (3x + 10y - 2z)<p>It’s not obvious but all kinds of problems can be modeled in this framework, like scheduling-, graph-, routing- problems. A big application is logistics: maximize profit &#x2F; minimize costs under certain constraints.<p>So the solvers are just dealing with this inequality solving business. And this is where a lot of theoretical advances have happened. It’s a large field and I barely scratch the surface but it’s very interesting. Some keywords are: Operations Research, Mixed Integer Programming, Simplex Method.</div><br/></div></div><div id="38919977" class="c"><input type="checkbox" id="c-38919977" checked=""/><div class="controls bullet"><span class="by">stncls</span><span>|</span><a href="#38918856">parent</a><span>|</span><a href="#38919415">prev</a><span>|</span><a href="#38918834">next</a><span>|</span><label class="collapse" for="c-38919977">[-]</label><label class="expand" for="c-38919977">[1 more]</label></div><br/><div class="children"><div class="content">I would answer &quot;yes&quot; to all your questions.<p>&gt; Was there a major theoretical development in the solver field that allowed this to happen ?<p>A few major theoretical developments did happen, although the really big ones are 25+ years ago (see Figure 4 in the OP): 5x in 1994 with the incorporation of the dual simplex method, 10x in 1998, mostly because of cutting planes, Gomory cuts specifically.<p>&gt; Or is it a bunch of tiny tuning heuristics ?<p>Also yes. Bob Bixby, co-founder of CPLEX and Gurobi, describes mixed-integer programming as &quot;a bag of tricks&quot;. And of course, there is a whole spectrum between pure theory and heuristic trickery, it&#x27;s not black-and-white.<p>&gt; If so, how are those even possible given that the solver is supposed to be a generic tool applicable to a large class of problem ?
&gt; Are there problems whose structure fit a recognizable pattern where optimizations are possible ?<p>Yes, plenty! Commercial solver developers have a business to run. Clients got problems, they need to solve them, regardless of the algorithm. The canonical example of problem-structure-detection is knapsack problems. CPLEX and Gurobi both detect when their input is a knapsack, and they then run a completely different algorithm to solve it.<p>At a smaller scale (but larger impact overall), there are a wide range of &quot;presolve&quot; techniques that each detect some microstructures in problems and simplify them [1]. Most of these techniques affect &lt;20% of problems, but together they are extremely powerful.<p>Another example of half-theoretical half-tricky technique that has a great impact on a few instances by detecting structure: symmetry detection. The theory behind it is serious stuff. Implementing the techniques requires serious (and unpublished) engineering efforts. Most problem instances aren&#x27;t affected at all. But when it works, you can expect a 10x speedup.<p>[1] <a href="https:&#x2F;&#x2F;opus4.kobv.de&#x2F;opus4-zib&#x2F;files&#x2F;6037&#x2F;Presolve.pdf" rel="nofollow">https:&#x2F;&#x2F;opus4.kobv.de&#x2F;opus4-zib&#x2F;files&#x2F;6037&#x2F;Presolve.pdf</a></div><br/></div></div></div></div><div id="38918834" class="c"><input type="checkbox" id="c-38918834" checked=""/><div class="controls bullet"><span class="by">genman</span><span>|</span><a href="#38918856">prev</a><span>|</span><a href="#38918957">next</a><span>|</span><label class="collapse" for="c-38918834">[-]</label><label class="expand" for="c-38918834">[1 more]</label></div><br/><div class="children"><div class="content">I would side with their grain of salt until they have actually completed their promise to implement a crossword puzzle solver using integer programming.</div><br/></div></div><div id="38918957" class="c"><input type="checkbox" id="c-38918957" checked=""/><div class="controls bullet"><span class="by">lowbloodsugar</span><span>|</span><a href="#38918834">prev</a><span>|</span><label class="collapse" for="c-38918957">[-]</label><label class="expand" for="c-38918957">[3 more]</label></div><br/><div class="children"><div class="content">1989: Odd that the superminicomputer that cost hundreds of thousands had 8MB RAM and managed 1 MIPS, while my Acorn Archimedes cost $2000 had 1MB (and you could get up to 4MB) and managed 8 MIPS.</div><br/><div id="38919022" class="c"><input type="checkbox" id="c-38919022" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#38918957">parent</a><span>|</span><label class="collapse" for="c-38919022">[-]</label><label class="expand" for="c-38919022">[2 more]</label></div><br/><div class="children"><div class="content">Good catch. That might be a typo: the Prime 750 seems to date from 19_7_9.</div><br/><div id="38919867" class="c"><input type="checkbox" id="c-38919867" checked=""/><div class="controls bullet"><span class="by">Jtsummers</span><span>|</span><a href="#38918957">root</a><span>|</span><a href="#38919022">parent</a><span>|</span><label class="collapse" for="c-38919867">[-]</label><label class="expand" for="c-38919867">[1 more]</label></div><br/><div class="children"><div class="content">The paper was written in 1988 and published in 1989 (1 Jan 1989, so just). The Prime 750 is specifically named in the paper, it was probably the best system he had access to when doing the work.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>