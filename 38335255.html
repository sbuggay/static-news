<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700470865350" as="style"/><link rel="stylesheet" href="styles.css?v=1700470865350"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/yl4579/StyleTTS2">StyleTTS2 – open-source Eleven-Labs-quality Text To Speech</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>sandslides</span> | <span>168 comments</span></div><br/><div><div id="38339045" class="c"><input type="checkbox" id="c-38339045" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38336502">next</a><span>|</span><label class="collapse" for="c-38339045">[-]</label><label class="expand" for="c-38339045">[43 more]</label></div><br/><div class="children"><div class="content">I made a 100% local voice chatbot using StyleTTS2 and other open source pieces (Whisper and OpenHermes2-Mistral-7B). It responds <i>so</i> much faster than ChatGPT. You can have a real conversation with it instead of the stilted Siri-style interaction you have with other voice assistants. Fun to play with!<p>Anyone who has a Windows gaming PC with a 12 GB Nvidia GPU (tested on 3060 12GB) can install and converse with StyleTTS2 with one click, no fiddling with Python or CUDA needed: <a href="https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7" rel="nofollow noreferrer">https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7</a><p>The demo is janky in various ways (requires headphones, runs as a console app, etc), but it&#x27;s a sneak peek at what will soon be possible to run on a normal gaming PC just by putting together open source pieces. The models are improving rapidly, there are already several improved models I haven&#x27;t yet incorporated.</div><br/><div id="38339115" class="c"><input type="checkbox" id="c-38339115" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38340240">next</a><span>|</span><label class="collapse" for="c-38339115">[-]</label><label class="expand" for="c-38339115">[19 more]</label></div><br/><div class="children"><div class="content">How hard on your end does the task of making the chatbot converse naturally look? Specifically I&#x27;m thinking about interruptions, if it&#x27;s talking too long I would like to be able to start talking and interrupt it like in a normal conversation, or if I&#x27;m saying something it could quickly interject something. Once you&#x27;ve got the extremely high speed, theoretically faster than real time, you can start doing that stuff right?<p>There is another thing remaining after that for fully natural conversation, which is making the AI context aware like a human would be. Basically giving it eyes so it can see your face and judge body language to know if it&#x27;s talking too long and needs to be more brief, the same way a human talks.</div><br/><div id="38339222" class="c"><input type="checkbox" id="c-38339222" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339115">parent</a><span>|</span><a href="#38340240">next</a><span>|</span><label class="collapse" for="c-38339222">[-]</label><label class="expand" for="c-38339222">[18 more]</label></div><br/><div class="children"><div class="content">Yes, I implemented the ability to interrupt the chatbot while it is talking. It wasn&#x27;t too hard, although it does require you to wear headphones so the bot doesn&#x27;t hear itself and get interrupted.<p>The other way around (bot interrupting the user) is hard. Currently the bot starts processing a response after every word that the voice recognition outputs, to reduce latency. When new words come in before the response is ready it starts over. If it finishes its response before any more words arrive (~1 second usually) it starts speaking. This is not ideal because the user might not be done speaking, of course. If the user continues speaking the bot will stop and listen. But deciding when the user is done speaking, or if the bot should interrupt before the user is done, is a hard problem. It could possibly be done zero-shot using prompting of a LLM but you&#x27;d want a GPT-4 level LLM to do a good job and GPT-4 is too slow for instant response right now. A better idea would be to train a dedicated turn-taking model that directly predicts who should speak next in conversations. I haven&#x27;t thought much about how to source a dataset and train a model for that yet.<p>Ultimately the end state of this type of system is a complete end-to-end audio-to-audio language model. There should be only one model, it should take audio directly as input and produce audio directly as output. I believe that having TTS and voice recognition and language modeling all as separate systems will not get us to 100% natural human conversation. I think that such a system would be within reach of today&#x27;s hardware too, all you need is the right training dataset&#x2F;procedure and some architecture bits to make it efficient.<p>As for giving the model eyes, actually there are already open source vision-language models that could be used for this today! I&#x27;d love to implement one in my chatbot. It probably wouldn&#x27;t have social intelligence to read body language yet, but it could definitely answer questions about things you present to the webcam, read text, maybe even look at your computer screen and have conversations about what&#x27;s on your screen. The latter could potentially be very useful, the endgame there is like GitHub Copilot for everything you do on your computer, not just typing code.</div><br/><div id="38339982" class="c"><input type="checkbox" id="c-38339982" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339222">parent</a><span>|</span><a href="#38344120">next</a><span>|</span><label class="collapse" for="c-38339982">[-]</label><label class="expand" for="c-38339982">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, fascinating insights. I think an everything-to-everything multimodal model could work if it&#x27;s big enough because of transfer learning (but then there are latency issues), and so could a refined system built on LLMs&#x2F;LMMs with TTS (like what you are using), but I haven&#x27;t seen any good research on audio-to-audio language models. My suspicion is that that would take a lot of compute, much more than text, and that the amount of semantically meaningful accessible data might be much lower as well. And if you do manage to get to the same level of quality as text, what is latency like then? Not 100% sure, just intuitions, but I doubt it&#x27;s great.<p>I like the idea of an RL predictor for interruption timing, although I think it might struggle with factual-correction interruptions. It could be a good way to make a very fast system, and if latency on the rest of the system is low enough you could probably start slipping in your &quot;Of course&quot;, &quot;Yeah, I agree&quot;, and &quot;It was in March, but yeah&quot; for truly natural speech. If latency is low you could just use the RL system to find opportunities to interrupt, give them to the LLM&#x2F;LMM, and it decides how to interrupt, all the way from &quot;mhm&quot;, to &quot;Yep, sounds good to me&quot;, to &quot;Not quite, it was the 3rd entry, but yeah otherwise it makes sense&quot;, to &quot;Actually can I quickly jump on that? I just wanted to quickly [make a point]&#x2F;[ask a question] about [some thing that requires exploration before the conversation continues]&quot;.<p>Tuning a system like this would be the most annoying activity in human history, but something like this has to be achieved for truly natural conversation so we gotta do it lol.</div><br/></div></div><div id="38344120" class="c"><input type="checkbox" id="c-38344120" checked=""/><div class="controls bullet"><span class="by">slow_numbnut</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339222">parent</a><span>|</span><a href="#38339982">prev</a><span>|</span><a href="#38340023">next</a><span>|</span><label class="collapse" for="c-38344120">[-]</label><label class="expand" for="c-38344120">[2 more]</label></div><br/><div class="children"><div class="content">Instead of sacrificing flexibility by building one monolith model that does Audio to audio in one go, wouldn&#x27;t it be better to train a model that handles conversing with the user (knows when the user is done talking, when it&#x27;s hearing itself, etc) and leave the thinking to other, more generic models?</div><br/><div id="38344179" class="c"><input type="checkbox" id="c-38344179" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38344120">parent</a><span>|</span><a href="#38340023">next</a><span>|</span><label class="collapse" for="c-38344179">[-]</label><label class="expand" for="c-38344179">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t lose flexibility with an end to end model. You lose controllability. But there are ways to mitigate that.</div><br/></div></div></div></div><div id="38340023" class="c"><input type="checkbox" id="c-38340023" checked=""/><div class="controls bullet"><span class="by">fintechie</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339222">parent</a><span>|</span><a href="#38344120">prev</a><span>|</span><a href="#38339881">next</a><span>|</span><label class="collapse" for="c-38340023">[-]</label><label class="expand" for="c-38340023">[4 more]</label></div><br/><div class="children"><div class="content">&gt; although it does require you to wear headphones so the bot doesn&#x27;t hear itself and get interrupted.<p>Maybe you can use some sort of speaker identification to sort this out?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;whisper&#x2F;discussions&#x2F;264">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;whisper&#x2F;discussions&#x2F;264</a></div><br/><div id="38342508" class="c"><input type="checkbox" id="c-38342508" checked=""/><div class="controls bullet"><span class="by">woodson</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38340023">parent</a><span>|</span><a href="#38341138">next</a><span>|</span><label class="collapse" for="c-38342508">[-]</label><label class="expand" for="c-38342508">[2 more]</label></div><br/><div class="children"><div class="content">A simple correlation of audio chunks from microphone and from the TTS should be enough to tell which parts in the input stream are re-recorded TTS. Much simpler, no?</div><br/><div id="38342631" class="c"><input type="checkbox" id="c-38342631" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38342508">parent</a><span>|</span><a href="#38341138">next</a><span>|</span><label class="collapse" for="c-38342631">[-]</label><label class="expand" for="c-38342631">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not so simple when the impulse response of the room and mic and speakers are all unknown, possibly changing, plus unknown background sounds as well, possibly at a very high level. and there&#x27;s also unknown latency which can be quite large especially in the networked case, and maybe some codecs, and maybe some audio &quot;enhancement&quot;  software the OEM installed on the user&#x27;s machine. Also, ideally the computer would be able to hear the user even while it is speaking.<p>Echo cancellation is non-trivial for sure.</div><br/></div></div></div></div><div id="38341138" class="c"><input type="checkbox" id="c-38341138" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38340023">parent</a><span>|</span><a href="#38342508">prev</a><span>|</span><a href="#38339881">next</a><span>|</span><label class="collapse" for="c-38341138">[-]</label><label class="expand" for="c-38341138">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this is a good idea. Too many good ideas, not enough time!</div><br/></div></div></div></div><div id="38339881" class="c"><input type="checkbox" id="c-38339881" checked=""/><div class="controls bullet"><span class="by">globalnode</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339222">parent</a><span>|</span><a href="#38340023">prev</a><span>|</span><a href="#38341029">next</a><span>|</span><label class="collapse" for="c-38339881">[-]</label><label class="expand" for="c-38339881">[4 more]</label></div><br/><div class="children"><div class="content">you&#x27;d have to do something along the lines of what voice comm does to combat the output feedback problem. i think it involves an fft to analyse the two signals and cancel out the feedback, im not 100% sure on the details.</div><br/><div id="38341002" class="c"><input type="checkbox" id="c-38341002" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339881">parent</a><span>|</span><a href="#38341029">next</a><span>|</span><label class="collapse" for="c-38341002">[-]</label><label class="expand" for="c-38341002">[3 more]</label></div><br/><div class="children"><div class="content">I plan to change the audio input to use WebRTC, then I get echo cancellation and network transparency for free. Although dealing with WebRTC is a headache harder than doing the AI parts.</div><br/><div id="38341081" class="c"><input type="checkbox" id="c-38341081" checked=""/><div class="controls bullet"><span class="by">Sean-Der</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341002">parent</a><span>|</span><a href="#38341029">next</a><span>|</span><label class="collapse" for="c-38341081">[-]</label><label class="expand" for="c-38341081">[2 more]</label></div><br/><div class="children"><div class="content">What do you find hard about WebRTC?<p>I would love to help. Would even code up a prototype if you wanted :)</div><br/><div id="38341155" class="c"><input type="checkbox" id="c-38341155" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341081">parent</a><span>|</span><a href="#38341029">next</a><span>|</span><label class="collapse" for="c-38341155">[-]</label><label class="expand" for="c-38341155">[1 more]</label></div><br/><div class="children"><div class="content">For starters, every WebRTC demo I&#x27;ve tried has at least 400ms of round trip latency even on a loopback connection. Shoot me an email if you know WebRTC, would be good to chat with someone who knows stuff!</div><br/></div></div></div></div></div></div></div></div><div id="38341029" class="c"><input type="checkbox" id="c-38341029" checked=""/><div class="controls bullet"><span class="by">joshspankit</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339222">parent</a><span>|</span><a href="#38339881">prev</a><span>|</span><a href="#38341415">next</a><span>|</span><label class="collapse" for="c-38341029">[-]</label><label class="expand" for="c-38341029">[4 more]</label></div><br/><div class="children"><div class="content">Short-term could it be configured as push to talk?</div><br/><div id="38341047" class="c"><input type="checkbox" id="c-38341047" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341029">parent</a><span>|</span><a href="#38341415">next</a><span>|</span><label class="collapse" for="c-38341047">[-]</label><label class="expand" for="c-38341047">[3 more]</label></div><br/><div class="children"><div class="content">Certainly, but then it has little advantage over e.g. ChatGPT voice mode. I guess running locally is an advantage but the voice and answer quality is worse. The much better latency and more natural conversation is what I like about it.</div><br/><div id="38341813" class="c"><input type="checkbox" id="c-38341813" checked=""/><div class="controls bullet"><span class="by">joshspankit</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341047">parent</a><span>|</span><a href="#38341415">next</a><span>|</span><label class="collapse" for="c-38341813">[-]</label><label class="expand" for="c-38341813">[2 more]</label></div><br/><div class="children"><div class="content">Am I wrong to think it would have a couple major advantages? Like using speakers without having to worry about echo cancellation, having a distinct interrupt signal, and still getting all the latency benefits (possibly even more once you get used to it since the conversational style has to assume the end of the user’s sentence instead of knowing the second they let go of the button)</div><br/><div id="38342007" class="c"><input type="checkbox" id="c-38342007" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341813">parent</a><span>|</span><a href="#38341415">next</a><span>|</span><label class="collapse" for="c-38342007">[-]</label><label class="expand" for="c-38342007">[1 more]</label></div><br/><div class="children"><div class="content">You wouldn&#x27;t quite have all the latency benefits because you&#x27;d have the additional delay between when you stop speaking and when you release the button (or cut off speech if you release too early). It wouldn&#x27;t respond any faster because it&#x27;s already responding at the fastest possible speed right now, it doesn&#x27;t wait at all. And it wouldn&#x27;t be hands free, it wouldn&#x27;t feel like a natural conversation which is what I&#x27;m going for.<p>I&#x27;d rather use speaker diarization and&#x2F;or echo cancellation to solve the problem without needing the user to press any buttons.</div><br/></div></div></div></div></div></div></div></div><div id="38341415" class="c"><input type="checkbox" id="c-38341415" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339222">parent</a><span>|</span><a href="#38341029">prev</a><span>|</span><a href="#38340240">next</a><span>|</span><label class="collapse" for="c-38341415">[-]</label><label class="expand" for="c-38341415">[2 more]</label></div><br/><div class="children"><div class="content">Deciding when someone is done speaking is hard to do well and impossible to do perfectly. Some people finish speaking, then think of something else to say and pretend they were still talking.</div><br/><div id="38341440" class="c"><input type="checkbox" id="c-38341440" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341415">parent</a><span>|</span><a href="#38340240">next</a><span>|</span><label class="collapse" for="c-38341440">[-]</label><label class="expand" for="c-38341440">[1 more]</label></div><br/><div class="children"><div class="content">True, perfection isn&#x27;t achievable but human level performance is all you need and it may be possible to do better than that.</div><br/></div></div></div></div></div></div></div></div><div id="38340240" class="c"><input type="checkbox" id="c-38340240" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38339115">prev</a><span>|</span><a href="#38342555">next</a><span>|</span><label class="collapse" for="c-38340240">[-]</label><label class="expand" for="c-38340240">[6 more]</label></div><br/><div class="children"><div class="content">Tried it but it seems it only works with Cuda 11 and I have 12 installed. Not really willing to potentially screw up my Cuda environment to try it.</div><br/><div id="38340904" class="c"><input type="checkbox" id="c-38340904" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38340240">parent</a><span>|</span><a href="#38340315">next</a><span>|</span><label class="collapse" for="c-38340904">[-]</label><label class="expand" for="c-38340904">[4 more]</label></div><br/><div class="children"><div class="content">Thanks for trying, what error message did you get? It works without CUDA installed at all on my test machine.</div><br/><div id="38341254" class="c"><input type="checkbox" id="c-38341254" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38340904">parent</a><span>|</span><a href="#38340315">next</a><span>|</span><label class="collapse" for="c-38341254">[-]</label><label class="expand" for="c-38341254">[3 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  Process Process-2:
  Traceback (most recent call last):
    File &quot;multiprocessing\process.py&quot;, line 314, in _bootstrap
    File &quot;multiprocessing\process.py&quot;, line 108, in run
    File &quot;chirp.py&quot;, line 126, in whisper_process
    File &quot;chirp.py&quot;, line 126, in &lt;listcomp&gt;
    File &quot;faster_whisper\transcribe.py&quot;, line 426, in generate_segments
    File &quot;faster_whisper\transcribe.py&quot;, line 610, in encode
  RuntimeError: Library cublas64_11.dll is not found or cannot be loaded
  tts initialized</code></pre></div><br/><div id="38341363" class="c"><input type="checkbox" id="c-38341363" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341254">parent</a><span>|</span><a href="#38340315">next</a><span>|</span><label class="collapse" for="c-38341363">[-]</label><label class="expand" for="c-38341363">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, the dll is included in the app package but maybe there is a conflict with other installed DLLs on some machines. When releasing PC software I always expect this type of issue unfortunately. I plan to move away from faster_whisper which may fix this.<p>I have to say that the Python ecosystem is just awful for distribution purposes and I spent a lot longer on packaging issues than I did on the actual AI parts. And clearly didn&#x27;t find all of the issues :)</div><br/><div id="38341503" class="c"><input type="checkbox" id="c-38341503" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341363">parent</a><span>|</span><a href="#38340315">next</a><span>|</span><label class="collapse" for="c-38341503">[-]</label><label class="expand" for="c-38341503">[1 more]</label></div><br/><div class="children"><div class="content">Agree completely. But in this case the fault is with CUDA which never ever works without a struggle. It’s insane how hard it is to get stuff that works cross-platform without a lot of work using CUDA. Even PyTorch has an awkward way of dealing with it and they have more resources to figure it out than just about anyone.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38342555" class="c"><input type="checkbox" id="c-38342555" checked=""/><div class="controls bullet"><span class="by">shon</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38340240">prev</a><span>|</span><a href="#38340635">next</a><span>|</span><label class="collapse" for="c-38342555">[-]</label><label class="expand" for="c-38342555">[2 more]</label></div><br/><div class="children"><div class="content">Cool work! I tested it and got some mixed results:<p>1) it throws an error if it&#x27;s installed to any drive other than C:\ --I moved it to C: and it works fine.<p>2) I&#x27;m seeing huge latency on an EVGA 3080Ti with 12GB. Also seeing it repeat the parsed input, even though I only spoke once, it appears to process the same input many times with slightly different predictions sometimes. Here&#x27;s some logs:<p>Latency to LLM response: 4.59
latency to speaking: 5.31
speaking 4: Hi Jim!
user spoke: Hi Jim.
user spoke recently, prompting LLM. last word time: 77.81 time: 78.11742429999867
latency to prompting: 0.31<p>Latency to LLM response: 2.09
latency to speaking: 3.83
speaking 5: So what have you been up to lately?
user spoke: So what have you been up to lately?
user spoke recently, prompting LLM. last word time: 83.9 time: 84.09415280001122
latency to prompting: 0.19
user spoke: So what have you been up to lately? No, I&#x27;m watching.
user spoke a while ago, ignoring. last word time: 86.9 time: 88.92142140000942
user spoke: So what have you been up to lately? No, just watching TV.
user spoke a while ago, ignoring. last word time: 87.9 time: 90.76665070001036
user spoke: So what have you been up to lately? No, I&#x27;m just watching TV.
user spoke a while ago, ignoring. last word time: 87.9 time: 94.16581820001011
user spoke: So what have you been up to lately? No, I&#x27;m just watching TV.
user spoke a while ago, ignoring. last word time: 88.9 time: 97.85854300000938
user spoke: So what have you been up to lately? No, I&#x27;m just watching TV.
user spoke a while ago, ignoring. last word time: 87.9 time: 101.54986060000374
user spoke: No, I just bought you a TV.
user spoke a while ago, ignoring. last word time: 87.8 time: 104.51332219998585
user spoke: No, I&#x27;ll just watch you TV.
user spoke a while ago, ignoring. last word time: 87.41 time: 106.60086529998807
Latency to LLM response: 46.09
latency to speaking: 50.49<p>Thanks for posting it!<p>Edit:<p>3) It&#x27;s hearing itself and responding to itself...</div><br/><div id="38342771" class="c"><input type="checkbox" id="c-38342771" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38342555">parent</a><span>|</span><a href="#38340635">next</a><span>|</span><label class="collapse" for="c-38342771">[-]</label><label class="expand" for="c-38342771">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for trying it and thanks for the feedback! Yes, right now you need to use headphones so it doesn&#x27;t hear itself. Sometimes Whisper inexplicably fails to recognize speech promptly. It seems to depend on what you say, so try saying something else. I have improvements that I haven&#x27;t had time to release yet that should improve the situation, and a lot more work is definitely needed, this is definitely MVP level stuff right now. This stuff is fixable but it&#x27;ll take time.</div><br/></div></div></div></div><div id="38340635" class="c"><input type="checkbox" id="c-38340635" checked=""/><div class="controls bullet"><span class="by">samsepi0l121</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38342555">prev</a><span>|</span><a href="#38341514">next</a><span>|</span><label class="collapse" for="c-38340635">[-]</label><label class="expand" for="c-38340635">[2 more]</label></div><br/><div class="children"><div class="content">But whisper does not support input streaming, so you have to wait for the whole llm response to trigger the transcription or not?</div><br/><div id="38341013" class="c"><input type="checkbox" id="c-38341013" checked=""/><div class="controls bullet"><span class="by">yencabulator</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38340635">parent</a><span>|</span><a href="#38341514">next</a><span>|</span><label class="collapse" for="c-38341013">[-]</label><label class="expand" for="c-38341013">[1 more]</label></div><br/><div class="children"><div class="content">Apparently, by running it on windows of audio very often:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38340938">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38340938</a></div><br/></div></div></div></div><div id="38341514" class="c"><input type="checkbox" id="c-38341514" checked=""/><div class="controls bullet"><span class="by">aik</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38340635">prev</a><span>|</span><a href="#38339779">next</a><span>|</span><label class="collapse" for="c-38341514">[-]</label><label class="expand" for="c-38341514">[2 more]</label></div><br/><div class="children"><div class="content">Hey modeless.  Love it.  Is your project open source by any chance?  Would love to see it.</div><br/><div id="38341779" class="c"><input type="checkbox" id="c-38341779" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341514">parent</a><span>|</span><a href="#38339779">next</a><span>|</span><label class="collapse" for="c-38341779">[-]</label><label class="expand" for="c-38341779">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t decided yet what I&#x27;m going to do with it. I think ideally I would open source it for people who have GPUs but also run it as a paid service for people who don&#x27;t have GPUs. Open source that also makes money is always the holy grail :) I&#x27;ll post updates on my Twitter&#x2F;X account.</div><br/></div></div></div></div><div id="38339779" class="c"><input type="checkbox" id="c-38339779" checked=""/><div class="controls bullet"><span class="by">tomp</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38341514">prev</a><span>|</span><a href="#38341684">next</a><span>|</span><label class="collapse" for="c-38339779">[-]</label><label class="expand" for="c-38339779">[4 more]</label></div><br/><div class="children"><div class="content">How do you get Whisper to be fast?<p>Isn&#x27;t it quite non-realtime?</div><br/><div id="38340938" class="c"><input type="checkbox" id="c-38340938" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339779">parent</a><span>|</span><a href="#38339848">next</a><span>|</span><label class="collapse" for="c-38340938">[-]</label><label class="expand" for="c-38340938">[1 more]</label></div><br/><div class="children"><div class="content">Great question! Whisper processes audio in 30 second chunks. But on a fast GPU it can finish in only 100 milliseconds or so. So you can run it 10+ times per second and get around 100ms latency. Even better actually because Whisper will predict past the end of the audio sometimes.<p>This is an advantage of running locally. Running whisper this way is inefficient but I have a whole GPU sitting there dedicated to one user, so it&#x27;s not a problem as long as it is fast enough. It wouldn&#x27;t work well for a cloud service trying to optimize GPU use. But there are other ways of doing real time speech recognition that could be used there.</div><br/></div></div><div id="38339848" class="c"><input type="checkbox" id="c-38339848" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339779">parent</a><span>|</span><a href="#38340938">prev</a><span>|</span><a href="#38339932">next</a><span>|</span><label class="collapse" for="c-38339848">[-]</label><label class="expand" for="c-38339848">[1 more]</label></div><br/><div class="children"><div class="content">The community upgrades to whisper are far faster than real-time, especially if you have a powerful gpu</div><br/></div></div><div id="38339932" class="c"><input type="checkbox" id="c-38339932" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38339779">parent</a><span>|</span><a href="#38339848">prev</a><span>|</span><a href="#38341684">next</a><span>|</span><label class="collapse" for="c-38339932">[-]</label><label class="expand" for="c-38339932">[1 more]</label></div><br/><div class="children"><div class="content">use whisper-distil, it&#x27;s like 5-8x faster</div><br/></div></div></div></div><div id="38341684" class="c"><input type="checkbox" id="c-38341684" checked=""/><div class="controls bullet"><span class="by">funtech</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38339779">prev</a><span>|</span><a href="#38340076">next</a><span>|</span><label class="collapse" for="c-38341684">[-]</label><label class="expand" for="c-38341684">[5 more]</label></div><br/><div class="children"><div class="content">Is 12GB the minimum? got an out of memory error with 8GB</div><br/><div id="38341707" class="c"><input type="checkbox" id="c-38341707" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341684">parent</a><span>|</span><a href="#38340076">next</a><span>|</span><label class="collapse" for="c-38341707">[-]</label><label class="expand" for="c-38341707">[4 more]</label></div><br/><div class="children"><div class="content">Yes, unfortunately these models take a lot of VRAM. It may be possible to do an 8GB version but it will have to compromise on quality of voice recognition and the language model so it might not be a good experience.</div><br/><div id="38341821" class="c"><input type="checkbox" id="c-38341821" checked=""/><div class="controls bullet"><span class="by">joshspankit</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341707">parent</a><span>|</span><a href="#38340076">next</a><span>|</span><label class="collapse" for="c-38341821">[-]</label><label class="expand" for="c-38341821">[3 more]</label></div><br/><div class="children"><div class="content">This might be silly because of how few people it benefits, but could it be broken up on to multiple 8GB cards on the same system?</div><br/><div id="38341987" class="c"><input type="checkbox" id="c-38341987" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341821">parent</a><span>|</span><a href="#38340076">next</a><span>|</span><label class="collapse" for="c-38341987">[-]</label><label class="expand" for="c-38341987">[2 more]</label></div><br/><div class="children"><div class="content">Yes, it absolutely could. You&#x27;re right that this configuration is rare. Although people have been putting together machines with multiple 24GB cards in order to split and run larger models like llama2-70B.</div><br/><div id="38343357" class="c"><input type="checkbox" id="c-38343357" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38341987">parent</a><span>|</span><a href="#38340076">next</a><span>|</span><label class="collapse" for="c-38343357">[-]</label><label class="expand" for="c-38343357">[1 more]</label></div><br/><div class="children"><div class="content">The latest large models are 120B and 100k context such as Goliath and Tess XL</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38340076" class="c"><input type="checkbox" id="c-38340076" checked=""/><div class="controls bullet"><span class="by">xena</span><span>|</span><a href="#38339045">parent</a><span>|</span><a href="#38341684">prev</a><span>|</span><a href="#38336502">next</a><span>|</span><label class="collapse" for="c-38340076">[-]</label><label class="expand" for="c-38340076">[2 more]</label></div><br/><div class="children"><div class="content">It threw a python exception for me and didn&#x27;t generate speech</div><br/><div id="38340905" class="c"><input type="checkbox" id="c-38340905" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339045">root</a><span>|</span><a href="#38340076">parent</a><span>|</span><a href="#38336502">next</a><span>|</span><label class="collapse" for="c-38340905">[-]</label><label class="expand" for="c-38340905">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for trying, what exception did you get?</div><br/></div></div></div></div></div></div><div id="38336502" class="c"><input type="checkbox" id="c-38336502" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38339045">prev</a><span>|</span><a href="#38337562">next</a><span>|</span><label class="collapse" for="c-38336502">[-]</label><label class="expand" for="c-38336502">[16 more]</label></div><br/><div class="children"><div class="content">I tested StyleTTS2 last month, my step-by-step notes that might be useful for people doing local setup (not too hard): <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;howto-guides&#x2F;page&#x2F;styletts-2" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;howto-guides&#x2F;page&#x2F;styletts-2</a><p>Also I did a little speed&#x2F;quality shootoff with the LJSpeech model (vs VITS and XTTS). StyleTTS2 was pretty good and very fast: <a href="https:&#x2F;&#x2F;fediverse.randomfoo.net&#x2F;notice&#x2F;AaOgprU715gcT5GrZ2" rel="nofollow noreferrer">https:&#x2F;&#x2F;fediverse.randomfoo.net&#x2F;notice&#x2F;AaOgprU715gcT5GrZ2</a></div><br/><div id="38336728" class="c"><input type="checkbox" id="c-38336728" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#38336502">parent</a><span>|</span><a href="#38342280">next</a><span>|</span><label class="collapse" for="c-38336728">[-]</label><label class="expand" for="c-38336728">[14 more]</label></div><br/><div class="children"><div class="content">&gt;  inferences at up to 15-95X (!) RT on my 4090<p>That&#x27;s incredible!<p>Are infill and outpainting equivalents possible? Super-RT TTS at this level of quality opens up a diverse array of uses esp for indie&#x2F;experimental gamedev that I&#x27;m excited for.</div><br/><div id="38339863" class="c"><input type="checkbox" id="c-38339863" checked=""/><div class="controls bullet"><span class="by">JonathanFly</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38336728">parent</a><span>|</span><a href="#38338355">next</a><span>|</span><label class="collapse" for="c-38339863">[-]</label><label class="expand" for="c-38339863">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Are infill and outpainting equivalents possible?<p>Do you mean outpainting as in you still what words to do, or the model just extends the audio unconditionally the way some image models just expand past an image borders without a specific prompt (in audio like <a href="https:&#x2F;&#x2F;twitter.com&#x2F;jonathanfly&#x2F;status&#x2F;1650001584485552130" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;jonathanfly&#x2F;status&#x2F;1650001584485552130</a>)</div><br/></div></div><div id="38338355" class="c"><input type="checkbox" id="c-38338355" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38336728">parent</a><span>|</span><a href="#38339863">prev</a><span>|</span><a href="#38337224">next</a><span>|</span><label class="collapse" for="c-38338355">[-]</label><label class="expand" for="c-38338355">[2 more]</label></div><br/><div class="children"><div class="content">It is theoretically possible to train a model that, given some speech, attempts to continue the speech, e.g. Spectron: <a href="https:&#x2F;&#x2F;michelleramanovich.github.io&#x2F;spectron&#x2F;spectron&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;michelleramanovich.github.io&#x2F;spectron&#x2F;spectron&#x2F;</a>. Similarly, it is possible to train a model to edit the content, a la Voicebox: <a href="https:&#x2F;&#x2F;voicebox.metademolab.com&#x2F;edit.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;voicebox.metademolab.com&#x2F;edit.html</a>.</div><br/><div id="38341447" class="c"><input type="checkbox" id="c-38341447" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38338355">parent</a><span>|</span><a href="#38337224">next</a><span>|</span><label class="collapse" for="c-38341447">[-]</label><label class="expand" for="c-38341447">[1 more]</label></div><br/><div class="children"><div class="content">Great. :P<p>Me: Won’t it be great when AI can-<p>Computer: Finish your sentences for you? OMG that’s exactly what I was thinking!</div><br/></div></div></div></div><div id="38337224" class="c"><input type="checkbox" id="c-38337224" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38336728">parent</a><span>|</span><a href="#38338355">prev</a><span>|</span><a href="#38342280">next</a><span>|</span><label class="collapse" for="c-38337224">[-]</label><label class="expand" for="c-38337224">[10 more]</label></div><br/><div class="children"><div class="content">Not sure what you mean:
If you mean could inpainting and out painting with image models be faster, its a &quot;not even wrong&quot; question, similar to asking  if the United Airlines app could get faster because American Airlines did. (Yes, getting faster is an option available to ~all code)<p>If you mean could you inpaint and outpaint text...yes, by inserting and deleting characters.<p>If you mean could you use an existing voice clip to generate speech by the same speaker in the clip, yes, part of the article is demonstrating generating speech by speakers not seen at training time</div><br/><div id="38337411" class="c"><input type="checkbox" id="c-38337411" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38337224">parent</a><span>|</span><a href="#38338343">next</a><span>|</span><label class="collapse" for="c-38337411">[-]</label><label class="expand" for="c-38337411">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure I understand what you mean to say. To me it&#x27;s a reasonable question asking whether text to speech models can complete a missing part of some existing speech audio, or make it go on for longer, rather than only generating speech from scratch. I don&#x27;t see a connection to your faster apps analogy.<p>Fwiw, I imagine this is possible, at least to some extent. I was recently playing with xtts and it can generate speaker embeddings from short periods of speech, so you could use those to provide a logical continuation to existing audio. However, I&#x27;m not sure it&#x27;s possible or easy to manage the &quot;seams&quot; between what is generated and what is preexisting very easily yet.<p>It&#x27;s certainly not a misguided question to me. Perhaps you could be less curt and offer your domain knowledge to contribute to the discussion?<p>Edit: I see you&#x27;ve edited your post to be more informative, thanks for sharing more of your thoughts.</div><br/><div id="38338810" class="c"><input type="checkbox" id="c-38338810" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38337411">parent</a><span>|</span><a href="#38338343">next</a><span>|</span><label class="collapse" for="c-38338810">[-]</label><label class="expand" for="c-38338810">[5 more]</label></div><br/><div class="children"><div class="content">It imposes a cost on others when when you makes false claims like I said or felt the question was unreasonable.<p>I didn&#x27;t and don&#x27;t.<p>It is a hard question to understand and an interesting mind-bender to answer.<p>Less policing of the metacontext and more focusing on the discussion at hand will help ensure there&#x27;s interlocutors around to, at the very least, continue policing.</div><br/><div id="38339462" class="c"><input type="checkbox" id="c-38339462" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38338810">parent</a><span>|</span><a href="#38338343">next</a><span>|</span><label class="collapse" for="c-38339462">[-]</label><label class="expand" for="c-38339462">[4 more]</label></div><br/><div class="children"><div class="content">Sorry but it was pretty obvious what he meant.</div><br/><div id="38341161" class="c"><input type="checkbox" id="c-38341161" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38339462">parent</a><span>|</span><a href="#38340198">prev</a><span>|</span><a href="#38338343">next</a><span>|</span><label class="collapse" for="c-38341161">[-]</label><label class="expand" for="c-38341161">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not, at all.<p>He could have meant speed, text, audio, words, or phonemes, with least probably images.<p>He probably didn&#x27;t mean phonemes or he wouldn&#x27;t be asking.<p>He probably didn&#x27;t mean arbitrarily slicing &#x27;real&#x27; audio and stitching on fake audio - he made repeated references to a video game.<p>He probably didn&#x27;t mean inpainting and outpainting imagery, even though he made reference to a video game, because its an audio model.<p>Thank you for explaining I deserve to get downvoted through  the floor multiple times for asking a question because it&#x27;s &quot;obvious&quot;. Maybe you can explain to the rest of the class what he meant then? If it was obviously phonemes, will you then advocate for them being downvoted through the floor since the answer was obvious? Or is it only people who assume good faith and ask what they meant who deserve downvotes?</div><br/><div id="38343771" class="c"><input type="checkbox" id="c-38343771" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38341161">parent</a><span>|</span><a href="#38338343">next</a><span>|</span><label class="collapse" for="c-38343771">[-]</label><label class="expand" for="c-38343771">[1 more]</label></div><br/><div class="children"><div class="content">Inpainting and outpainting of images is when the model generates bits inside or outside the image that don&#x27;t exist. By analogy he was talking about generating sound inside (I.e. filling gaps) or outside (extrapolating beyond the end) the audio.<p>I don&#x27;t know why you would think he was talking about inpainting images, words. This whole discussion is about speech synthesis.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38338343" class="c"><input type="checkbox" id="c-38338343" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38337224">parent</a><span>|</span><a href="#38337411">prev</a><span>|</span><a href="#38342280">next</a><span>|</span><label class="collapse" for="c-38338343">[-]</label><label class="expand" for="c-38338343">[3 more]</label></div><br/><div class="children"><div class="content">Ignore the speed comment; it is unrelated to my question.<p>What I mean is, can output be conditioned on antecedent audio as well as text analogous to how image diffusion models can condition inpainting and outpatient on static parts of an image and clip embeddings?</div><br/><div id="38338763" class="c"><input type="checkbox" id="c-38338763" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38338343">parent</a><span>|</span><a href="#38342280">next</a><span>|</span><label class="collapse" for="c-38338763">[-]</label><label class="expand" for="c-38338763">[2 more]</label></div><br/><div class="children"><div class="content">Yes, the paper and Eleven Labs have a major feature of &quot;given $AUDIO_SET, generate speech for $TEXT in the same style of $AUDIO_SET&quot;<p>No, in that, you can&#x27;t cut it at an arbitrary midword point, say at &quot;what tim&quot; in &quot;what time is it bejing&quot;, and give it the string &quot;what time is it in beijing&quot;, and have it recover seamlessly.<p>Yes, in that, you can cut it at an arbirtrary phoneme boundary, say &#x27;this, I.S. a; good: test! ok?&#x27; in IPA is 
&#x27;ðˈɪs, ˌaɪˌɛsˈeɪ; ɡˈʊd: tˈɛst!
ˌoʊkˈeɪ?&#x27;, and I can cut it &#x27;between&#x27; a phoneme, give it the  and have it complete.</div><br/><div id="38339302" class="c"><input type="checkbox" id="c-38339302" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#38336502">root</a><span>|</span><a href="#38338763">parent</a><span>|</span><a href="#38342280">next</a><span>|</span><label class="collapse" for="c-38339302">[-]</label><label class="expand" for="c-38339302">[1 more]</label></div><br/><div class="children"><div class="content">Perfect! Thank you</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38342280" class="c"><input type="checkbox" id="c-38342280" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#38336502">parent</a><span>|</span><a href="#38336728">prev</a><span>|</span><a href="#38337562">next</a><span>|</span><label class="collapse" for="c-38342280">[-]</label><label class="expand" for="c-38342280">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. Following the instructions now. BTW mamba is no longer recommended (for those like me who aren&#x27;t already using it), and the #mambaforge anchor in the link didn&#x27;t work.</div><br/></div></div></div></div><div id="38337562" class="c"><input type="checkbox" id="c-38337562" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38336502">prev</a><span>|</span><a href="#38336480">next</a><span>|</span><label class="collapse" for="c-38337562">[-]</label><label class="expand" for="c-38337562">[16 more]</label></div><br/><div class="children"><div class="content">Was somewhat annoying to get everything to work as the documentation is a bit spotty, but after ~20 minutes it&#x27;s all working well for me on WSL Ubuntu 22.04. Sound quality is very good, much better than other open source TTS projects I&#x27;ve seen. It&#x27;s also SUPER fast (at least using a 4090 GPU).<p>Not sure it&#x27;s quite up to Eleven Labs quality. But to me, what makes Eleven so cool is that they have a large library of high quality voices that are easy to choose from. I don&#x27;t yet see any way with this library to get a different voice from the default female voice.<p>Also, the real special sauce for Eleven is the near instant voice cloning with just a single 5 minute sample, which works shockingly (even spookily) well. Can&#x27;t wait to have that all available in a fully open source project! The services that provide this as an API are just too expensive for many use cases. Even the OpenAI one which is on the cheaper side costs ~10 cents for a couple thousand word generation.</div><br/><div id="38338932" class="c"><input type="checkbox" id="c-38338932" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38337562">parent</a><span>|</span><a href="#38337625">next</a><span>|</span><label class="collapse" for="c-38338932">[-]</label><label class="expand" for="c-38338932">[3 more]</label></div><br/><div class="children"><div class="content">To save people some time, this is tested on Ubuntu 22.04 (google is being annoying about the download link, saying too many people have downloaded it in the past 24 hours, but if you wait a bit it should work again):<p><pre><code>  git clone https:&#x2F;&#x2F;github.com&#x2F;yl4579&#x2F;StyleTTS2.git
  cd StyleTTS2
  python3 -m venv venv
  source venv&#x2F;bin&#x2F;activate
  python3 -m pip install --upgrade pip
  python3 -m pip install wheel
  pip install -r requirements.txt
  pip install phonemizer
  sudo apt-get install -y espeak-ng
  pip install gdown
  gdown https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id=1K3jt1JEbtohBLUA0X75KLw36TW7U1yxq
  7z x Models.zip
  rm Models.zip
  gdown https:&#x2F;&#x2F;drive.google.com&#x2F;uc?id=1jK_VV3TnGM9dkrIMsdQ_upov8FrIymr7
  7z x Models.zip
  rm Models.zip
  pip install ipykernel pickleshare nltk SoundFile
  python -c &quot;import nltk; nltk.download(&#x27;punkt&#x27;)&quot;
  pip install --upgrade jupyter ipywidgets librosa
  python -m ipykernel install --user --name=venv --display-name=&quot;Python (venv)&quot;
  jupyter notebook
  </code></pre>
Then navigate to &#x2F;Demo and open either `Inference_LJSpeech.ipynb` or `Inference_LibriTTS.ipynb` and they should work.</div><br/><div id="38342711" class="c"><input type="checkbox" id="c-38342711" checked=""/><div class="controls bullet"><span class="by">degobah</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38338932">parent</a><span>|</span><a href="#38340492">prev</a><span>|</span><a href="#38337625">next</a><span>|</span><label class="collapse" for="c-38342711">[-]</label><label class="expand" for="c-38342711">[1 more]</label></div><br/><div class="children"><div class="content">Very helpful, thanks!</div><br/></div></div></div></div><div id="38337625" class="c"><input type="checkbox" id="c-38337625" checked=""/><div class="controls bullet"><span class="by">wczekalski</span><span>|</span><a href="#38337562">parent</a><span>|</span><a href="#38338932">prev</a><span>|</span><a href="#38337692">next</a><span>|</span><label class="collapse" for="c-38337625">[-]</label><label class="expand" for="c-38337625">[2 more]</label></div><br/><div class="children"><div class="content">One thing I&#x27;ve seen done for style cloning is a high quality fine tuned TTS -&gt; RVC pipeline to &quot;enhance&quot; the output. TTS for intonation + pronunciation, RVC for voice texture. With StyleTTS and this pipeline you should get close to ElevenLabs.</div><br/><div id="38338184" class="c"><input type="checkbox" id="c-38338184" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38337625">parent</a><span>|</span><a href="#38337692">next</a><span>|</span><label class="collapse" for="c-38338184">[-]</label><label class="expand" for="c-38338184">[1 more]</label></div><br/><div class="children"><div class="content">I suspect they are doing many more things to make it sounds better. I certainly hope open source solutions can approach that level of quality, but so far I&#x27;ve been very disappointed.</div><br/></div></div></div></div><div id="38337692" class="c"><input type="checkbox" id="c-38337692" checked=""/><div class="controls bullet"><span class="by">sandslides</span><span>|</span><a href="#38337562">parent</a><span>|</span><a href="#38337625">prev</a><span>|</span><a href="#38337595">next</a><span>|</span><label class="collapse" for="c-38337692">[-]</label><label class="expand" for="c-38337692">[8 more]</label></div><br/><div class="children"><div class="content">The LibriTTS demo clones unseen speakers from a five second or so clip</div><br/><div id="38337869" class="c"><input type="checkbox" id="c-38337869" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38337692">parent</a><span>|</span><a href="#38337595">next</a><span>|</span><label class="collapse" for="c-38337869">[-]</label><label class="expand" for="c-38337869">[7 more]</label></div><br/><div class="children"><div class="content">Ah ok, thanks. I tried the other demo.</div><br/><div id="38338169" class="c"><input type="checkbox" id="c-38338169" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38337869">parent</a><span>|</span><a href="#38337595">next</a><span>|</span><label class="collapse" for="c-38338169">[-]</label><label class="expand" for="c-38338169">[6 more]</label></div><br/><div class="children"><div class="content">I tried it. Sounds absolutely nothing like my voice or my wife&#x27;s voice. I used the same sample files as I used 2 days ago on the Eleven Labs website, and they worked flawlessly there. So this is very, very far from being close to &quot;Eleven Labs quality&quot; when it comes to voice cloning.</div><br/><div id="38339033" class="c"><input type="checkbox" id="c-38339033" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38338169">parent</a><span>|</span><a href="#38339603">next</a><span>|</span><label class="collapse" for="c-38339033">[-]</label><label class="expand" for="c-38339033">[1 more]</label></div><br/><div class="children"><div class="content">Ah that&#x27;s disappointing, have you tried <a href="https:&#x2F;&#x2F;git.ecker.tech&#x2F;mrq&#x2F;ai-voice-cloning" rel="nofollow noreferrer">https:&#x2F;&#x2F;git.ecker.tech&#x2F;mrq&#x2F;ai-voice-cloning</a> ? I&#x27;ve had decent results with that, but inference is quite slow.</div><br/></div></div><div id="38339603" class="c"><input type="checkbox" id="c-38339603" checked=""/><div class="controls bullet"><span class="by">lewismenelaws</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38338169">parent</a><span>|</span><a href="#38339033">prev</a><span>|</span><a href="#38339038">next</a><span>|</span><label class="collapse" for="c-38339603">[-]</label><label class="expand" for="c-38339603">[2 more]</label></div><br/><div class="children"><div class="content">Yep. Tried as well. Tried a little clip of Tony Sopranos and it came out as a british guy.<p>xTTSv2 does it much better. But the quality on the trained voices are great though.</div><br/><div id="38339734" class="c"><input type="checkbox" id="c-38339734" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38339603">parent</a><span>|</span><a href="#38339038">next</a><span>|</span><label class="collapse" for="c-38339734">[-]</label><label class="expand" for="c-38339734">[1 more]</label></div><br/><div class="children"><div class="content">Yes, same for my voice. Made me sound British and didn&#x27;t capture anything special about my voice that makes it recognizable.</div><br/></div></div></div></div><div id="38339038" class="c"><input type="checkbox" id="c-38339038" checked=""/><div class="controls bullet"><span class="by">jsjmch</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38338169">parent</a><span>|</span><a href="#38339603">prev</a><span>|</span><a href="#38338426">next</a><span>|</span><label class="collapse" for="c-38339038">[-]</label><label class="expand" for="c-38339038">[1 more]</label></div><br/><div class="children"><div class="content">ElevenLabs are based on Tortoise-TTS which was already pre-trained on millions of hours of data, but this one was only trained on LibriTTS which was 500 hours at best. If you have seen millions of voices, there are definitely gonna be some of them that sound like you. It is just a matter of training data, but it is very difficult to have someone collect these large amounts of data and train on it.</div><br/></div></div><div id="38338426" class="c"><input type="checkbox" id="c-38338426" checked=""/><div class="controls bullet"><span class="by">sandslides</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38338169">parent</a><span>|</span><a href="#38339038">prev</a><span>|</span><a href="#38337595">next</a><span>|</span><label class="collapse" for="c-38338426">[-]</label><label class="expand" for="c-38338426">[1 more]</label></div><br/><div class="children"><div class="content">The speech generated is the best I&#x27;ve heard from an open source model. The one test I made didn&#x27;t make an exact clone either but this is still early days. There&#x27;s likely something not quite right. The cloned voice does speak without any artifacts or other weirdness that most TTS systems suffer from.</div><br/></div></div></div></div></div></div></div></div><div id="38337595" class="c"><input type="checkbox" id="c-38337595" checked=""/><div class="controls bullet"><span class="by">wczekalski</span><span>|</span><a href="#38337562">parent</a><span>|</span><a href="#38337692">prev</a><span>|</span><a href="#38336480">next</a><span>|</span><label class="collapse" for="c-38337595">[-]</label><label class="expand" for="c-38337595">[2 more]</label></div><br/><div class="children"><div class="content">have you tested longer utterances with both ElevenLabs and with StyleTTS? Short audio synthesis is a ~solved problem in the TTS world but things start falling apart once you want to do something like create an audiobook with text to speech.</div><br/><div id="38339155" class="c"><input type="checkbox" id="c-38339155" checked=""/><div class="controls bullet"><span class="by">wingworks</span><span>|</span><a href="#38337562">root</a><span>|</span><a href="#38337595">parent</a><span>|</span><a href="#38336480">next</a><span>|</span><label class="collapse" for="c-38339155">[-]</label><label class="expand" for="c-38339155">[1 more]</label></div><br/><div class="children"><div class="content">I can say that the paid service from ElevenLabs can do long form TTS very well. I used it for a while to convert long articles to voice to listen to later instead of reading. It works very well. 
I only stopped because it gets a little pricey.</div><br/></div></div></div></div></div></div><div id="38336480" class="c"><input type="checkbox" id="c-38336480" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#38337562">prev</a><span>|</span><a href="#38340674">next</a><span>|</span><label class="collapse" for="c-38336480">[-]</label><label class="expand" for="c-38336480">[8 more]</label></div><br/><div class="children"><div class="content">Funnily enough, the TTS2 examples sound <i>better</i> than the ground truth [0]. For example, the &quot;Then leaving the corpse within the house [...]&quot; example has the ground truth pronounce &quot;house&quot; weirdly, with some change in the tonality that sounds higher, but the TTS2 version sounds more natural.<p>I&#x27;m excited to use this for all my ePub files, many of which don&#x27;t have corresponding audiobooks, such as a lot of Japanese light novels. I am currently using Moon+ Reader on Android which has TTS but it is very robotic.<p>[0] <a href="https:&#x2F;&#x2F;styletts2.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;styletts2.github.io&#x2F;</a></div><br/><div id="38342600" class="c"><input type="checkbox" id="c-38342600" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#38336480">parent</a><span>|</span><a href="#38337711">next</a><span>|</span><label class="collapse" for="c-38342600">[-]</label><label class="expand" for="c-38342600">[1 more]</label></div><br/><div class="children"><div class="content">First Wife is a professional voice-over actor. I saw someone left her a bad review saying &quot;Clearly an AI.&quot;<p>2023. There is no way to win.</div><br/></div></div><div id="38337711" class="c"><input type="checkbox" id="c-38337711" checked=""/><div class="controls bullet"><span class="by">KolmogorovComp</span><span>|</span><a href="#38336480">parent</a><span>|</span><a href="#38342600">prev</a><span>|</span><a href="#38336832">next</a><span>|</span><label class="collapse" for="c-38337711">[-]</label><label class="expand" for="c-38337711">[1 more]</label></div><br/><div class="children"><div class="content">The pace is better, but imho you there is still a very noticeable “metalic” tone which makes it inferior to the real thing.<p>Impressive results nonetheless, and superior to all other TTS.</div><br/></div></div><div id="38336832" class="c"><input type="checkbox" id="c-38336832" checked=""/><div class="controls bullet"><span class="by">risho</span><span>|</span><a href="#38336480">parent</a><span>|</span><a href="#38337711">prev</a><span>|</span><a href="#38340674">next</a><span>|</span><label class="collapse" for="c-38336832">[-]</label><label class="expand" for="c-38336832">[5 more]</label></div><br/><div class="children"><div class="content">how are you planning on using this with epubs? i&#x27;m in a similar boat. would really like to leverage something like this for ebooks.</div><br/><div id="38336893" class="c"><input type="checkbox" id="c-38336893" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#38336480">root</a><span>|</span><a href="#38336832">parent</a><span>|</span><a href="#38340674">next</a><span>|</span><label class="collapse" for="c-38336893">[-]</label><label class="expand" for="c-38336893">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if you can add a TTS engine to Android as an app or plugin, then make Moon+ Reader or another reader to use that custom engine. That&#x27;s probably how I&#x27;d do it for the easiest approach, but if that doesn&#x27;t work, I might just have to make my own app.</div><br/><div id="38337715" class="c"><input type="checkbox" id="c-38337715" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#38336480">root</a><span>|</span><a href="#38336893">parent</a><span>|</span><a href="#38338126">next</a><span>|</span><label class="collapse" for="c-38337715">[-]</label><label class="expand" for="c-38337715">[2 more]</label></div><br/><div class="children"><div class="content">I’m planning on making a self-host solution where you can upload files and the host sends back the audio to play, as a first pass on this tech. I’ll open source the repo after fiddling and prototyping. I’ve needed this kinda thing for a long time!</div><br/><div id="38339262" class="c"><input type="checkbox" id="c-38339262" checked=""/><div class="controls bullet"><span class="by">risho</span><span>|</span><a href="#38336480">root</a><span>|</span><a href="#38337715">parent</a><span>|</span><a href="#38338126">next</a><span>|</span><label class="collapse" for="c-38339262">[-]</label><label class="expand" for="c-38339262">[1 more]</label></div><br/><div class="children"><div class="content">Please make sure to link it back to HN so that we can check it out!</div><br/></div></div></div></div><div id="38338126" class="c"><input type="checkbox" id="c-38338126" checked=""/><div class="controls bullet"><span class="by">jrpear</span><span>|</span><a href="#38336480">root</a><span>|</span><a href="#38336893">parent</a><span>|</span><a href="#38337715">prev</a><span>|</span><a href="#38340674">next</a><span>|</span><label class="collapse" for="c-38338126">[-]</label><label class="expand" for="c-38338126">[1 more]</label></div><br/><div class="children"><div class="content">You can! [rhvoice](<a href="https:&#x2F;&#x2F;rhvoice.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;rhvoice.org&#x2F;</a>) is an open source example.</div><br/></div></div></div></div></div></div></div></div><div id="38340674" class="c"><input type="checkbox" id="c-38340674" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#38336480">prev</a><span>|</span><a href="#38336750">next</a><span>|</span><label class="collapse" for="c-38340674">[-]</label><label class="expand" for="c-38340674">[7 more]</label></div><br/><div class="children"><div class="content">Out of curiosity - to folks that have had success with this...<p>This voice cloning is... nothing like XTTSv2, let alone ElevenLabs.<p>It doesn&#x27;t seem to care about accents at all. It does pretty well with pitch and cadence, and that&#x27;s about it.<p>I&#x27;ve tried all kinds of different values for alpha, beta, embedding scale, diffusion steps.<p>Anyone else have better luck?<p>Sure it&#x27;s fast and the sound quality is pretty good, but I can&#x27;t get the voice cloning to work at all.</div><br/><div id="38342108" class="c"><input type="checkbox" id="c-38342108" checked=""/><div class="controls bullet"><span class="by">jsjmch</span><span>|</span><a href="#38340674">parent</a><span>|</span><a href="#38341083">next</a><span>|</span><label class="collapse" for="c-38342108">[-]</label><label class="expand" for="c-38342108">[4 more]</label></div><br/><div class="children"><div class="content">See my previous comment about this point. ElevenLabs are based on Tortoise-TTS which was already pre-trained on millions of hours of data, but this one was only trained on LibriTTS which was 500 hours at best. XTTS was also trained with probably millions of speakers in more than 20 languages.<p>If you have seen millions of voices, there are definitely gonna be some of them that sound like you. It is just a matter of training data, but it is very difficult to have someone collect these large amounts of data and train on it.</div><br/><div id="38343436" class="c"><input type="checkbox" id="c-38343436" checked=""/><div class="controls bullet"><span class="by">wczekalski</span><span>|</span><a href="#38340674">root</a><span>|</span><a href="#38342108">parent</a><span>|</span><a href="#38342778">next</a><span>|</span><label class="collapse" for="c-38343436">[-]</label><label class="expand" for="c-38343436">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s your basis for the claim that they are based on TorToiSe? I have seen this claim made (and rebutted) many times.</div><br/><div id="38344543" class="c"><input type="checkbox" id="c-38344543" checked=""/><div class="controls bullet"><span class="by">jsjmch</span><span>|</span><a href="#38340674">root</a><span>|</span><a href="#38343436">parent</a><span>|</span><a href="#38342778">next</a><span>|</span><label class="collapse" for="c-38344543">[-]</label><label class="expand" for="c-38344543">[1 more]</label></div><br/><div class="children"><div class="content">Very similar features, quite slow inference speed, and various rumors.</div><br/></div></div></div></div><div id="38342778" class="c"><input type="checkbox" id="c-38342778" checked=""/><div class="controls bullet"><span class="by">lossolo</span><span>|</span><a href="#38340674">root</a><span>|</span><a href="#38342108">parent</a><span>|</span><a href="#38343436">prev</a><span>|</span><a href="#38341083">next</a><span>|</span><label class="collapse" for="c-38342778">[-]</label><label class="expand" for="c-38342778">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It is just a matter of training data, but it is very difficult to have someone collect these large amounts of data and train on it.<p>It&#x27;s really not that difficult, they are trained mostly on audiobooks and high quality audio from yt videos. If we talk about EV model then we are talking about around 500k hours of audio, but Tortoise-TTS is only around 50k from what I remember.</div><br/></div></div></div></div><div id="38341083" class="c"><input type="checkbox" id="c-38341083" checked=""/><div class="controls bullet"><span class="by">dsrtslnd23</span><span>|</span><a href="#38340674">parent</a><span>|</span><a href="#38342108">prev</a><span>|</span><a href="#38341031">next</a><span>|</span><label class="collapse" for="c-38341083">[-]</label><label class="expand" for="c-38341083">[1 more]</label></div><br/><div class="children"><div class="content">See the conclusion remarks in the paper - they acknowledge that voice cloning is not that good (yet).</div><br/></div></div><div id="38341031" class="c"><input type="checkbox" id="c-38341031" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#38340674">parent</a><span>|</span><a href="#38341083">prev</a><span>|</span><a href="#38336750">next</a><span>|</span><label class="collapse" for="c-38341031">[-]</label><label class="expand" for="c-38341031">[1 more]</label></div><br/><div class="children"><div class="content">I had the same experience as what you described (with a lot of experimentation with alpha and beta, as well as uploading different audio clips).</div><br/></div></div></div></div><div id="38336750" class="c"><input type="checkbox" id="c-38336750" checked=""/><div class="controls bullet"><span class="by">gjm11</span><span>|</span><a href="#38340674">prev</a><span>|</span><a href="#38338421">next</a><span>|</span><label class="collapse" for="c-38336750">[-]</label><label class="expand" for="c-38336750">[7 more]</label></div><br/><div class="children"><div class="content">HN title at present is &quot;StyleTTS2 – open-source Eleven Labs quality Text To Speech&quot;. Actual title at the far end doesn&#x27;t name any particular other product; arXiv paper linked from there doesn&#x27;t mention Eleven Labs either. I thought this sort of editorializing was frowned on.</div><br/><div id="38338933" class="c"><input type="checkbox" id="c-38338933" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38336750">parent</a><span>|</span><a href="#38337188">next</a><span>|</span><label class="collapse" for="c-38338933">[-]</label><label class="expand" for="c-38338933">[1 more]</label></div><br/><div class="children"><div class="content">It is editorializing and it is an exaggeration. However I&#x27;ve been using StyleTTS2 myself and IMO it is the best open source TTS by far and definitely deserves a spot on the top of HN for a while.</div><br/></div></div><div id="38337188" class="c"><input type="checkbox" id="c-38337188" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#38336750">parent</a><span>|</span><a href="#38338933">prev</a><span>|</span><a href="#38337330">next</a><span>|</span><label class="collapse" for="c-38337188">[-]</label><label class="expand" for="c-38337188">[4 more]</label></div><br/><div class="children"><div class="content">Eleven Labs is the gold standard for voice synthesis. There is nothing better out there.<p>So it is extremely notable for an open source system to be able to approach this level of quality, which is why I&#x27;d imagine most would appreciate the comparison. I know it caught my attention.</div><br/><div id="38338228" class="c"><input type="checkbox" id="c-38338228" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38336750">root</a><span>|</span><a href="#38337188">parent</a><span>|</span><a href="#38342574">next</a><span>|</span><label class="collapse" for="c-38338228">[-]</label><label class="expand" for="c-38338228">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI&#x27;s TTS is better than Eleven Labs, but they don&#x27;t let you train it to have a particular voice out of fear of the consequences.</div><br/><div id="38338384" class="c"><input type="checkbox" id="c-38338384" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#38336750">root</a><span>|</span><a href="#38338228">parent</a><span>|</span><a href="#38342574">next</a><span>|</span><label class="collapse" for="c-38338384">[-]</label><label class="expand" for="c-38338384">[1 more]</label></div><br/><div class="children"><div class="content">I concur that, for the use cases that OpenAI&#x27;s voices cover, it is significantly better than Eleven.</div><br/></div></div></div></div><div id="38342574" class="c"><input type="checkbox" id="c-38342574" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#38336750">root</a><span>|</span><a href="#38337188">parent</a><span>|</span><a href="#38338228">prev</a><span>|</span><a href="#38337330">next</a><span>|</span><label class="collapse" for="c-38342574">[-]</label><label class="expand" for="c-38342574">[1 more]</label></div><br/><div class="children"><div class="content">But is this even approaching Eleven? Doesn&#x27;t seem like it from the other comments here.</div><br/></div></div></div></div><div id="38337330" class="c"><input type="checkbox" id="c-38337330" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38336750">parent</a><span>|</span><a href="#38337188">prev</a><span>|</span><a href="#38338421">next</a><span>|</span><label class="collapse" for="c-38337330">[-]</label><label class="expand" for="c-38337330">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s against the guidelines. In fact, when I read the title, I didn&#x27;t think it was a new research paper but a random GitHub project.</div><br/></div></div></div></div><div id="38338421" class="c"><input type="checkbox" id="c-38338421" checked=""/><div class="controls bullet"><span class="by">wg0</span><span>|</span><a href="#38336750">prev</a><span>|</span><a href="#38344226">next</a><span>|</span><label class="collapse" for="c-38338421">[-]</label><label class="expand" for="c-38338421">[3 more]</label></div><br/><div class="children"><div class="content">The quality is really really INSANE and pretty much unimaginable in early 2000s.<p>Could have interesting prospects for games where you have LLM assuming a character and such TTS giving those NPCs voice.</div><br/><div id="38338539" class="c"><input type="checkbox" id="c-38338539" checked=""/><div class="controls bullet"><span class="by">abraae</span><span>|</span><a href="#38338421">parent</a><span>|</span><a href="#38344226">next</a><span>|</span><label class="collapse" for="c-38338539">[-]</label><label class="expand" for="c-38338539">[2 more]</label></div><br/><div class="children"><div class="content">This is a big thing for one area I&#x27;m interested in - golf simulation.<p>Currently playing in a golf simulator has a bit of a post-apocalyptian vibe. The birds are cheeping, the grass is rustling, the game play is realistic, but there&#x27;s not a human to be seen. Just so different from the smacktalking of a real round, or the crowd noise at a big game.<p>It&#x27;s begging for some LLM-fuelled banter to be added.</div><br/><div id="38339165" class="c"><input type="checkbox" id="c-38339165" checked=""/><div class="controls bullet"><span class="by">billylo</span><span>|</span><a href="#38338421">root</a><span>|</span><a href="#38338539">parent</a><span>|</span><a href="#38344226">next</a><span>|</span><label class="collapse" for="c-38339165">[-]</label><label class="expand" for="c-38339165">[1 more]</label></div><br/><div class="children"><div class="content">Or the occasional &quot;Fore!!&quot;s.  :-)</div><br/></div></div></div></div></div></div><div id="38344226" class="c"><input type="checkbox" id="c-38344226" checked=""/><div class="controls bullet"><span class="by">deknos</span><span>|</span><a href="#38338421">prev</a><span>|</span><a href="#38344075">next</a><span>|</span><label class="collapse" for="c-38344226">[-]</label><label class="expand" for="c-38344226">[1 more]</label></div><br/><div class="children"><div class="content">Is this really opensource and&#x2F;or free software? like code, data(set&#x2F;s) and models?<p>I am quite tired to see some &quot;open-source&quot; advertisement, where the half or more is not really free.<p>general psa: please be honest in your announcements :|</div><br/></div></div><div id="38344075" class="c"><input type="checkbox" id="c-38344075" checked=""/><div class="controls bullet"><span class="by">kats</span><span>|</span><a href="#38344226">prev</a><span>|</span><a href="#38338147">next</a><span>|</span><label class="collapse" for="c-38344075">[-]</label><label class="expand" for="c-38344075">[6 more]</label></div><br/><div class="children"><div class="content">This is really harmful and unethical work. It will be used to hurt millions of elderly people with scams. That&#x27;s the real application that will happen 100x more than anything else. It&#x27;s unethical and harmful to release tools that will be overwhelmingly used to hurt elderly people. What they should do about it is: Stop releasing models. Only release a service so that scammers will not use it. Also, only released audio that is watermarked, so that apps can tell that a phone call might be a scam. When they share models with researchers, use previous best practices: post a Google Form to request access.</div><br/><div id="38344268" class="c"><input type="checkbox" id="c-38344268" checked=""/><div class="controls bullet"><span class="by">slow_numbnut</span><span>|</span><a href="#38344075">parent</a><span>|</span><a href="#38344118">next</a><span>|</span><label class="collapse" for="c-38344268">[-]</label><label class="expand" for="c-38344268">[2 more]</label></div><br/><div class="children"><div class="content">Just imagine if this line of thinking was used elsewhere.<p>This tech is already out of the bag and I thank the author(s) for the contribution to humanity. The correct solution here is not to shove your head in the sand and ignore reality, but to get your government to penalize any country or company that facilitates this crime. If they can force severe penalties for other financial crimes and funding terrorism, they can do the same here.</div><br/><div id="38344443" class="c"><input type="checkbox" id="c-38344443" checked=""/><div class="controls bullet"><span class="by">kats</span><span>|</span><a href="#38344075">root</a><span>|</span><a href="#38344268">parent</a><span>|</span><a href="#38344118">next</a><span>|</span><label class="collapse" for="c-38344443">[-]</label><label class="expand" for="c-38344443">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s funny because just yesterday I posted:<p>&gt; soon as it&#x27;s out, a whole bunch of extremely privileged ML people will throw their hands up and say, &quot;oh well, cats out of the bag.&quot;<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;context?id=38324742">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;context?id=38324742</a></div><br/></div></div></div></div><div id="38344118" class="c"><input type="checkbox" id="c-38344118" checked=""/><div class="controls bullet"><span class="by">flarg</span><span>|</span><a href="#38344075">parent</a><span>|</span><a href="#38344268">prev</a><span>|</span><a href="#38338147">next</a><span>|</span><label class="collapse" for="c-38344118">[-]</label><label class="expand" for="c-38344118">[3 more]</label></div><br/><div class="children"><div class="content">Millions of elderly people are already getting scammed by overseas call centers so unless we do something more significant this tech will not make one iota of a difference.</div><br/><div id="38344137" class="c"><input type="checkbox" id="c-38344137" checked=""/><div class="controls bullet"><span class="by">kats</span><span>|</span><a href="#38344075">root</a><span>|</span><a href="#38344118">parent</a><span>|</span><a href="#38338147">next</a><span>|</span><label class="collapse" for="c-38344137">[-]</label><label class="expand" for="c-38344137">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not really true, most scammers have a male voice with a heavy accent. When they have tools that easily disguise their voice, scammers can reach many more elderly people.</div><br/><div id="38344354" class="c"><input type="checkbox" id="c-38344354" checked=""/><div class="controls bullet"><span class="by">slow_numbnut</span><span>|</span><a href="#38344075">root</a><span>|</span><a href="#38344137">parent</a><span>|</span><a href="#38338147">next</a><span>|</span><label class="collapse" for="c-38344354">[-]</label><label class="expand" for="c-38344354">[1 more]</label></div><br/><div class="children"><div class="content">That might have been true about a year ago, but I&#x27;ve been getting calls from well-spoken native-level scammers for about two months now. They are so frequent that I can put them on speaker during family gatherings to raise awareness.<p>Sample sizes of 1 are never representative but they definitely have full access to native speakers or tech that can generate very passable speech.</div><br/></div></div></div></div></div></div></div></div><div id="38338147" class="c"><input type="checkbox" id="c-38338147" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#38344075">prev</a><span>|</span><a href="#38337608">next</a><span>|</span><label class="collapse" for="c-38338147">[-]</label><label class="expand" for="c-38338147">[1 more]</label></div><br/><div class="children"><div class="content">Curious if we&#x27;ll see a Civitai-style LoRA[1] marketplace for text-to-speech models.<p>1 = <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;LoRA">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;LoRA</a></div><br/></div></div><div id="38337608" class="c"><input type="checkbox" id="c-38337608" checked=""/><div class="controls bullet"><span class="by">Evidlo</span><span>|</span><a href="#38338147">prev</a><span>|</span><a href="#38339158">next</a><span>|</span><label class="collapse" for="c-38337608">[-]</label><label class="expand" for="c-38337608">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s a ballpark estimate for inference time on a modern CPU?</div><br/></div></div><div id="38339158" class="c"><input type="checkbox" id="c-38339158" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#38337608">prev</a><span>|</span><a href="#38337751">next</a><span>|</span><label class="collapse" for="c-38339158">[-]</label><label class="expand" for="c-38339158">[1 more]</label></div><br/><div class="children"><div class="content">Having now tried it (the linked repo links to pre-built colab notebooks):<p>1) It does a fantastic job of text-to-speech.<p>2) I have had no success in getting any meaningful zero-shot voice cloning working. It technically runs and produces a voice, but it sounds nothing like the target voice. (This includes trying their microphone-based self-voice-cloning option.)<p>Presumably fine-tuning is needed - but I am curious if anyone had better luck with the zero-shot approach.</div><br/></div></div><div id="38337751" class="c"><input type="checkbox" id="c-38337751" checked=""/><div class="controls bullet"><span class="by">beltsazar</span><span>|</span><a href="#38339158">prev</a><span>|</span><a href="#38340991">next</a><span>|</span><label class="collapse" for="c-38337751">[-]</label><label class="expand" for="c-38337751">[11 more]</label></div><br/><div class="children"><div class="content">If AI will render some jobs obsolete, I suppose the first one will be audio book narrators and voice actors.</div><br/><div id="38339343" class="c"><input type="checkbox" id="c-38339343" checked=""/><div class="controls bullet"><span class="by">riquito</span><span>|</span><a href="#38337751">parent</a><span>|</span><a href="#38338504">next</a><span>|</span><label class="collapse" for="c-38339343">[-]</label><label class="expand" for="c-38339343">[3 more]</label></div><br/><div class="children"><div class="content">I can see a future where the label &quot;100% narrated by a human&quot; (and similar in other industries) will be a thing</div><br/><div id="38339467" class="c"><input type="checkbox" id="c-38339467" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38339343">parent</a><span>|</span><a href="#38340384">next</a><span>|</span><label class="collapse" for="c-38339467">[-]</label><label class="expand" for="c-38339467">[1 more]</label></div><br/><div class="children"><div class="content">A la, A Young Lady&#x27;s Illustrated Primer.</div><br/></div></div><div id="38340384" class="c"><input type="checkbox" id="c-38340384" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38339343">parent</a><span>|</span><a href="#38339467">prev</a><span>|</span><a href="#38338504">next</a><span>|</span><label class="collapse" for="c-38340384">[-]</label><label class="expand" for="c-38340384">[1 more]</label></div><br/><div class="children"><div class="content">&quot;No humans were fired in the making of this film&quot;</div><br/></div></div></div></div><div id="38338504" class="c"><input type="checkbox" id="c-38338504" checked=""/><div class="controls bullet"><span class="by">washadjeffmad</span><span>|</span><a href="#38337751">parent</a><span>|</span><a href="#38339343">prev</a><span>|</span><a href="#38340991">next</a><span>|</span><label class="collapse" for="c-38338504">[-]</label><label class="expand" for="c-38338504">[7 more]</label></div><br/><div class="children"><div class="content">Hardly. Imagine licensing your voice to Amazon so that any customer could stream any book narrated in your likeness without you having to commit the time to record. You could still work as a custom voice artist, all with a &quot;no clone&quot; clause if you chose. You could profit from your performance and craft in a fraction of the time, focusing as your own agent on the management of your assets. Or, you could just keep and commit to your day job.<p>Just imagine hearing the final novel of ASoIaF narrated by Roy Dotrice and knowing that a royalty went to his family and estate, or if David Attenborough willed the digital likeness of his voice and its performance to the BBC for use in nature documentaries after his death.<p>The advent of recorded audio didn&#x27;t put artists out of business, it expanded the industries that relied on them by allowing more of them to work. Film and tape didn&#x27;t put artists out of business, it expanded the industries that relied on them by allowing more of them to work. Audio digitization and the internet didn&#x27;t put artists out of business; it expanded the industries that relied on them by allowing more of them to work.<p>And TTS won&#x27;t put artists out of business, but it will create yet another new market with another niche that people will have to figure out how to monetize, even though 98% of the revenues will still somehow end up with the distributors.</div><br/><div id="38338738" class="c"><input type="checkbox" id="c-38338738" checked=""/><div class="controls bullet"><span class="by">nikkwong</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38338504">parent</a><span>|</span><a href="#38342270">next</a><span>|</span><label class="collapse" for="c-38338738">[-]</label><label class="expand" for="c-38338738">[4 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re not considering here is that a large majority of this industry is made up of no-name voice actors who have a pleasant (but perfectly substitutible) voice which is now something that AI can do perfectly and at a fraction of the price.<p>Sure, celebrities and other well-known figures will have more to gain here as they can license out their voice; but the majority of voice actors won&#x27;t be able to capitalize on this. So this is actually even more perverse because it again creates a system where all assets will accumulate at the top and there won&#x27;t be any distributions for everyone else.</div><br/><div id="38339910" class="c"><input type="checkbox" id="c-38339910" checked=""/><div class="controls bullet"><span class="by">washadjeffmad</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38338738">parent</a><span>|</span><a href="#38342270">next</a><span>|</span><label class="collapse" for="c-38339910">[-]</label><label class="expand" for="c-38339910">[3 more]</label></div><br/><div class="children"><div class="content">No, I am. I work with them, and I&#x27;ve been one (am one, rarely).<p>I listed just one possible use, but I also see voice cloning and advanced TTS expanding access for evocative instruction, as an aid to study style and expand range.<p>Don&#x27;t be afraid on their behalf. The dooming you&#x27;re talking about applied to every one of the technological changes I already listed, and we employ more performers and artists today than ever in history.<p>When animation went digital, we graduated more storyboard artists and digital animators. When music notation software and sampling could replace musicians and orchestras, we graduated more musicians and composers trained on those tools. Now it&#x27;s the performing arts, and no one in industry is going to shrink their pool of available talent (or risk ire) by daring conflate authenticity and performance with virtual impersonation. Performance capture and vfx also didn&#x27;t kill or consolidate the movie industry - it allowed it to expand.<p>Art evolves, and so does its business. People who love art want to see people who do art succeed. I&#x27;m optimistic.</div><br/><div id="38344015" class="c"><input type="checkbox" id="c-38344015" checked=""/><div class="controls bullet"><span class="by">hgomersall</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38339910">parent</a><span>|</span><a href="#38342677">next</a><span>|</span><label class="collapse" for="c-38344015">[-]</label><label class="expand" for="c-38344015">[1 more]</label></div><br/><div class="children"><div class="content">TTS actually allows scope for far more different artists&#x27; likenesses to be incorporated. An book can be read with all the characters having a different voice entirely. This is difficult currently and relies on the skill of the performer.</div><br/></div></div><div id="38342677" class="c"><input type="checkbox" id="c-38342677" checked=""/><div class="controls bullet"><span class="by">nikkwong</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38339910">parent</a><span>|</span><a href="#38344015">prev</a><span>|</span><a href="#38342270">next</a><span>|</span><label class="collapse" for="c-38342677">[-]</label><label class="expand" for="c-38342677">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know, I feel like the work produced through voice acting is more of a commodity than work in the other industries that you&#x27;re describing. Sure, a voice actor can add a lot of emotion and verbal nuance in a way that is differentiating, but I&#x27;m not sure if the difference is enough to matter for most people for the vast majority of cases. (Or I may be too dense to realize it). This is in contradiction to say performing arts, where there are in my opinion many more dimensions to the creative output which makes it less perfectly substitutable.</div><br/></div></div></div></div></div></div><div id="38342270" class="c"><input type="checkbox" id="c-38342270" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38338504">parent</a><span>|</span><a href="#38338738">prev</a><span>|</span><a href="#38338754">next</a><span>|</span><label class="collapse" for="c-38342270">[-]</label><label class="expand" for="c-38342270">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not really thinking it through. I have friends involved in the VA business, and it&#x27;s only gotten more competitive as time has progressed - this is partially because it&#x27;s rare that we need a voice actor that needs to create a crazy Looney Tunes sounding voice, the majority of VA work is surprisingly just close to the natural sounding voice of the VA themselves.<p>It&#x27;s rare that you need a talent like Dan Castellaneta, Mel Blanc, etc.<p>Secondly, yes, VA licensing will become a thing – but that means that jobs that would previously be available to other lesser known voice actors, because the major players simply didn&#x27;t have enough time to take those gigs, can no longer take them. A TTSVA can do unlimited recordings.<p>Thirdly, major studios that would require hundreds of voices for video games and other things don&#x27;t have to license known voices at all, they can just create generate brand new ones and pay zero licensing fees.</div><br/></div></div><div id="38338754" class="c"><input type="checkbox" id="c-38338754" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#38337751">root</a><span>|</span><a href="#38338504">parent</a><span>|</span><a href="#38342270">prev</a><span>|</span><a href="#38340991">next</a><span>|</span><label class="collapse" for="c-38338754">[-]</label><label class="expand" for="c-38338754">[1 more]</label></div><br/><div class="children"><div class="content">The point is no one will pay for any of that if you can just clone someone&#x27;s voice locally. Or just tell the AI how you want it to sound. Your argument literally ignores the entire elephant in the room.</div><br/></div></div></div></div></div></div><div id="38340991" class="c"><input type="checkbox" id="c-38340991" checked=""/><div class="controls bullet"><span class="by">wanderingmind</span><span>|</span><a href="#38337751">prev</a><span>|</span><a href="#38335257">next</a><span>|</span><label class="collapse" for="c-38340991">[-]</label><label class="expand" for="c-38340991">[1 more]</label></div><br/><div class="children"><div class="content">As a tangent away from LLMs, is there an integration available to be used in Android as TTS Engine?. The TTS voice that I have now (RHVoice) for OSMAnd is really driving me crazy and almost makes me want to go back to Google Maps.</div><br/></div></div><div id="38335257" class="c"><input type="checkbox" id="c-38335257" checked=""/><div class="controls bullet"><span class="by">sandslides</span><span>|</span><a href="#38340991">prev</a><span>|</span><a href="#38336525">next</a><span>|</span><label class="collapse" for="c-38335257">[-]</label><label class="expand" for="c-38335257">[7 more]</label></div><br/><div class="children"><div class="content">Just tried the collab notebooks.  Seems to be very good quality.  It also supports voice cloning.</div><br/><div id="38338341" class="c"><input type="checkbox" id="c-38338341" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#38335257">parent</a><span>|</span><a href="#38335679">next</a><span>|</span><label class="collapse" for="c-38338341">[-]</label><label class="expand" for="c-38338341">[1 more]</label></div><br/><div class="children"><div class="content">I skimmed the github but didn&#x27;t see any info on this, how long does it take to finetune to a particular voice?</div><br/></div></div><div id="38335679" class="c"><input type="checkbox" id="c-38335679" checked=""/><div class="controls bullet"><span class="by">fullstackchris</span><span>|</span><a href="#38335257">parent</a><span>|</span><a href="#38338341">prev</a><span>|</span><a href="#38336525">next</a><span>|</span><label class="collapse" for="c-38335679">[-]</label><label class="expand" for="c-38335679">[5 more]</label></div><br/><div class="children"><div class="content">Great stuff, took a look through the README but... what are the minimum hardware requirements to run this? Is this gonna blow up my CPU &#x2F; harddrive?</div><br/><div id="38335768" class="c"><input type="checkbox" id="c-38335768" checked=""/><div class="controls bullet"><span class="by">sandslides</span><span>|</span><a href="#38335257">root</a><span>|</span><a href="#38335679">parent</a><span>|</span><a href="#38336525">next</a><span>|</span><label class="collapse" for="c-38335768">[-]</label><label class="expand" for="c-38335768">[4 more]</label></div><br/><div class="children"><div class="content">Not sure. The only inference demos are colab notebooks. The models are approx 700mb each so I imagine it will run on modest gpu</div><br/><div id="38336139" class="c"><input type="checkbox" id="c-38336139" checked=""/><div class="controls bullet"><span class="by">bbbruno222</span><span>|</span><a href="#38335257">root</a><span>|</span><a href="#38335768">parent</a><span>|</span><a href="#38336525">next</a><span>|</span><label class="collapse" for="c-38336139">[-]</label><label class="expand" for="c-38336139">[3 more]</label></div><br/><div class="children"><div class="content">Would it run in a cheap non-GPU server?</div><br/><div id="38337326" class="c"><input type="checkbox" id="c-38337326" checked=""/><div class="controls bullet"><span class="by">dmw_ng</span><span>|</span><a href="#38335257">root</a><span>|</span><a href="#38336139">parent</a><span>|</span><a href="#38337379">next</a><span>|</span><label class="collapse" for="c-38337326">[-]</label><label class="expand" for="c-38337326">[1 more]</label></div><br/><div class="children"><div class="content">Seems to run about &quot;2x realtime&quot; on 2015 4 core i7-6700HQ laptop, that is, 5 seconds to generate 10 seconds of output. Can imagine that being 4x or greater on a real machine</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38336525" class="c"><input type="checkbox" id="c-38336525" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#38335257">prev</a><span>|</span><a href="#38337346">next</a><span>|</span><label class="collapse" for="c-38336525">[-]</label><label class="expand" for="c-38336525">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been playing with XTTSv2 and on my 3080ti, and it&#x27;s sightly faster than the length of the final audio. It&#x27;s also good quality, but these samples sound better.<p>Excited to try it out!</div><br/></div></div><div id="38337346" class="c"><input type="checkbox" id="c-38337346" checked=""/><div class="controls bullet"><span class="by">victorbjorklund</span><span>|</span><a href="#38336525">prev</a><span>|</span><a href="#38339828">next</a><span>|</span><label class="collapse" for="c-38337346">[-]</label><label class="expand" for="c-38337346">[3 more]</label></div><br/><div class="children"><div class="content">This only works for English voices right?</div><br/><div id="38338386" class="c"><input type="checkbox" id="c-38338386" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#38337346">parent</a><span>|</span><a href="#38339828">next</a><span>|</span><label class="collapse" for="c-38338386">[-]</label><label class="expand" for="c-38338386">[2 more]</label></div><br/><div class="children"><div class="content">No? From the readme:<p>In Utils folder, there are three pre-trained models:<p><pre><code>    ASR folder: It contains the pre-trained text aligner, which was pre-trained on English (LibriTTS), Japanese (JVS), and Chinese (AiShell) corpus. It works well for most other languages without fine-tuning, but you can always train your own text aligner with the code here: yl4579&#x2F;AuxiliaryASR.

    JDC folder: It contains the pre-trained pitch extractor, which was pre-trained on English (LibriTTS) corpus only. However, it works well for other languages too because F0 is independent of language. If you want to train on singing corpus, it is recommended to train a new pitch extractor with the code here: yl4579&#x2F;PitchExtractor.

    PLBERT folder: It contains the pre-trained PL-BERT model, which was pre-trained on English (Wikipedia) corpus only. It probably does not work very well on other languages, so you will need to train a different PL-BERT for different languages using the repo here: yl4579&#x2F;PL-BERT. You can also replace this module with other phoneme BERT models like XPhoneBERT which is pre-trained on more than 100 languages.</code></pre></div><br/><div id="38338959" class="c"><input type="checkbox" id="c-38338959" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38337346">root</a><span>|</span><a href="#38338386">parent</a><span>|</span><a href="#38339828">next</a><span>|</span><label class="collapse" for="c-38338959">[-]</label><label class="expand" for="c-38338959">[1 more]</label></div><br/><div class="children"><div class="content">Those are just parts of the system and don&#x27;t make a complete TTS. In theory you could train a complete StyleTTS2 for other languages but currently the pretrained models are English only.</div><br/></div></div></div></div></div></div><div id="38339828" class="c"><input type="checkbox" id="c-38339828" checked=""/><div class="controls bullet"><span class="by">exizt88</span><span>|</span><a href="#38337346">prev</a><span>|</span><a href="#38342918">next</a><span>|</span><label class="collapse" for="c-38339828">[-]</label><label class="expand" for="c-38339828">[1 more]</label></div><br/><div class="children"><div class="content">The weights aren’t MIT-licensed, so this is not usable in commercial applications, right?</div><br/></div></div><div id="38342918" class="c"><input type="checkbox" id="c-38342918" checked=""/><div class="controls bullet"><span class="by">lfmunoz4</span><span>|</span><a href="#38339828">prev</a><span>|</span><a href="#38338879">next</a><span>|</span><label class="collapse" for="c-38342918">[-]</label><label class="expand" for="c-38342918">[1 more]</label></div><br/><div class="children"><div class="content">Been looking for a speech to text that can work in real time and run locally, anyone know which are the best options available?</div><br/></div></div><div id="38338879" class="c"><input type="checkbox" id="c-38338879" checked=""/><div class="controls bullet"><span class="by">zsoltkacsandi</span><span>|</span><a href="#38342918">prev</a><span>|</span><a href="#38337529">next</a><span>|</span><label class="collapse" for="c-38338879">[-]</label><label class="expand" for="c-38338879">[5 more]</label></div><br/><div class="children"><div class="content">Is it possible to optimize somehow the model to run a Raspberry with 4 GB of RAM?</div><br/><div id="38339976" class="c"><input type="checkbox" id="c-38339976" checked=""/><div class="controls bullet"><span class="by">zsoltkacsandi</span><span>|</span><a href="#38338879">parent</a><span>|</span><a href="#38339967">next</a><span>|</span><label class="collapse" for="c-38339976">[-]</label><label class="expand" for="c-38339976">[3 more]</label></div><br/><div class="children"><div class="content">I was able to get it work with libjemalloc.</div><br/><div id="38340459" class="c"><input type="checkbox" id="c-38340459" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38338879">root</a><span>|</span><a href="#38339976">parent</a><span>|</span><a href="#38339967">next</a><span>|</span><label class="collapse" for="c-38340459">[-]</label><label class="expand" for="c-38340459">[2 more]</label></div><br/><div class="children"><div class="content">How fast is it on your raspberry?</div><br/><div id="38342367" class="c"><input type="checkbox" id="c-38342367" checked=""/><div class="controls bullet"><span class="by">zsoltkacsandi</span><span>|</span><a href="#38338879">root</a><span>|</span><a href="#38340459">parent</a><span>|</span><a href="#38339967">next</a><span>|</span><label class="collapse" for="c-38342367">[-]</label><label class="expand" for="c-38342367">[1 more]</label></div><br/><div class="children"><div class="content">Super slow. On my Mac Mini the inference was running in seconds, on Raspberry, minutes.</div><br/></div></div></div></div></div></div></div></div><div id="38337529" class="c"><input type="checkbox" id="c-38337529" checked=""/><div class="controls bullet"><span class="by">svapnil</span><span>|</span><a href="#38338879">prev</a><span>|</span><a href="#38339766">next</a><span>|</span><label class="collapse" for="c-38337529">[-]</label><label class="expand" for="c-38337529">[2 more]</label></div><br/><div class="children"><div class="content">How fast is inference with this model?<p>For reference, I&#x27;m using 11Labs to synthesize short messages - maybe a sentence or something, using voice cloning, and I&#x27;m getting it at around 400 - 500ms response times.<p>Is there any OS solution that gets me to around the same inference time?</div><br/><div id="38337579" class="c"><input type="checkbox" id="c-38337579" checked=""/><div class="controls bullet"><span class="by">wczekalski</span><span>|</span><a href="#38337529">parent</a><span>|</span><a href="#38339766">next</a><span>|</span><label class="collapse" for="c-38337579">[-]</label><label class="expand" for="c-38337579">[1 more]</label></div><br/><div class="children"><div class="content">It depends on hardware but IIRC on V100s it took 0.01-0.03s for 1s of audio.</div><br/></div></div></div></div><div id="38339766" class="c"><input type="checkbox" id="c-38339766" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38337529">prev</a><span>|</span><a href="#38338325">next</a><span>|</span><label class="collapse" for="c-38339766">[-]</label><label class="expand" for="c-38339766">[2 more]</label></div><br/><div class="children"><div class="content">Yes, please integrate it with Mistral and Whisper. This has got to get into the LLM frontends.</div><br/><div id="38341340" class="c"><input type="checkbox" id="c-38341340" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#38339766">parent</a><span>|</span><a href="#38338325">next</a><span>|</span><label class="collapse" for="c-38341340">[-]</label><label class="expand" for="c-38341340">[1 more]</label></div><br/><div class="children"><div class="content">Done: <a href="https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7" rel="nofollow noreferrer">https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7</a><p>It&#x27;s mostly just a demo for now and a little bit janky but it&#x27;s fun to chat with and you can see the promise for 100% local voice AI in the future.</div><br/></div></div></div></div><div id="38338325" class="c"><input type="checkbox" id="c-38338325" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38339766">prev</a><span>|</span><a href="#38340398">next</a><span>|</span><label class="collapse" for="c-38338325">[-]</label><label class="expand" for="c-38338325">[1 more]</label></div><br/><div class="children"><div class="content">silicon valley is very leaky, eleven labs is widely rumored to have raised a huge round recently. great timing because with OpenAI&#x27;s TTS and now this thing the options in the market have just expanded greatly.</div><br/></div></div><div id="38340398" class="c"><input type="checkbox" id="c-38340398" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#38338325">prev</a><span>|</span><a href="#38337240">next</a><span>|</span><label class="collapse" for="c-38340398">[-]</label><label class="expand" for="c-38340398">[1 more]</label></div><br/><div class="children"><div class="content">Those sound incredibly good.<p>Though would def like to clone a pleasant voice on it before using. Those sound good but not my cup of tea</div><br/></div></div><div id="38337240" class="c"><input type="checkbox" id="c-38337240" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#38340398">prev</a><span>|</span><a href="#38338080">next</a><span>|</span><label class="collapse" for="c-38337240">[-]</label><label class="expand" for="c-38337240">[10 more]</label></div><br/><div class="children"><div class="content">I really want to try this but making the venv to install all the torch dependencies is starting to get old lol.<p>How are other people dealing with this? Is there an easy way to get multiple venvs to share like a common torch venv? I can do this manually but I&#x27;m wondering if there&#x27;s a tool out there that does this.</div><br/><div id="38337279" class="c"><input type="checkbox" id="c-38337279" checked=""/><div class="controls bullet"><span class="by">wczekalski</span><span>|</span><a href="#38337240">parent</a><span>|</span><a href="#38339317">next</a><span>|</span><label class="collapse" for="c-38337279">[-]</label><label class="expand" for="c-38337279">[1 more]</label></div><br/><div class="children"><div class="content">I use nix to setup the python env (python version + poetry + sometimes python packages that are difficult to install with poetry) and use poetry for the rest.<p>The workflow is:<p><pre><code>  &gt; nix flake init -t github:dialohq&#x2F;flake-templates#python
  &gt; nix develop -c $SHELL
  &gt; # I&#x27;m in the shell with poetry env, I have a shell hook in the nix devenv that does poetry install and poetry activate.</code></pre></div><br/></div></div><div id="38339317" class="c"><input type="checkbox" id="c-38339317" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#38337240">parent</a><span>|</span><a href="#38337279">prev</a><span>|</span><a href="#38337610">next</a><span>|</span><label class="collapse" for="c-38339317">[-]</label><label class="expand" for="c-38339317">[1 more]</label></div><br/><div class="children"><div class="content">I generally try to use Docker for this stuff, but yeah, it&#x27;s the main reason why I pass on these, even though I&#x27;ve been looking for something like this. It&#x27;s just too hard to figure out the dependencies.</div><br/></div></div><div id="38337610" class="c"><input type="checkbox" id="c-38337610" checked=""/><div class="controls bullet"><span class="by">lukasga</span><span>|</span><a href="#38337240">parent</a><span>|</span><a href="#38339317">prev</a><span>|</span><a href="#38337693">next</a><span>|</span><label class="collapse" for="c-38337610">[-]</label><label class="expand" for="c-38337610">[2 more]</label></div><br/><div class="children"><div class="content">Can relate to this problem a lot. I have considered starting using a Docker dev container and making a base image for shared dependencies which I then can customize in a dockerfile for each new project, not sure if there&#x27;s a better alternative though.</div><br/><div id="38343476" class="c"><input type="checkbox" id="c-38343476" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#38337240">root</a><span>|</span><a href="#38337610">parent</a><span>|</span><a href="#38337693">next</a><span>|</span><label class="collapse" for="c-38343476">[-]</label><label class="expand" for="c-38343476">[1 more]</label></div><br/><div class="children"><div class="content">Yeah there is the official Nvidia container with torch+cuda pre-installed that some projects use.<p>I feel more projects should start with that as the base instead of pinning on whatever variants. Most aren&#x27;t using specialized CUDA kernels after all.<p>Suppose there&#x27;s the answer, just pick the specific torch+CUDA base that matches the major version of the project you want to run. Then cross your fingers and hope the dependencies mesh :p.</div><br/></div></div></div></div><div id="38337693" class="c"><input type="checkbox" id="c-38337693" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38337240">parent</a><span>|</span><a href="#38337610">prev</a><span>|</span><a href="#38340408">next</a><span>|</span><label class="collapse" for="c-38337693">[-]</label><label class="expand" for="c-38337693">[3 more]</label></div><br/><div class="children"><div class="content">Same here. I&#x27;m using conda and eyeing simply installing a pytorch into the base conda env</div><br/><div id="38338882" class="c"><input type="checkbox" id="c-38338882" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38337240">root</a><span>|</span><a href="#38337693">parent</a><span>|</span><a href="#38340408">next</a><span>|</span><label class="collapse" for="c-38338882">[-]</label><label class="expand" for="c-38338882">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think &quot;base&quot; works like that  (while it can be a fallback for some dependencies, afaik, Python packages are isolated&#x2F;not in path). But even if you could, don&#x27;t do it. Different packages usually have different pytorch dependencies (often CUDA as well) and it will definitely bite you.<p>The biggest optimization I&#x27;ve found is to use mamba for everything. It&#x27;s ridiculously faster than conda for package resolution. With everything cached, you&#x27;re mostly just waiting for your SSD at that point.<p>(I suppose you <i>could</i> add the base env&#x27;s lib path to the end of your PYTHONPATH, but that sounds like a sure way to get bitten by weird dependency&#x2F;reproducibility issues down the line.)</div><br/><div id="38339619" class="c"><input type="checkbox" id="c-38339619" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#38337240">root</a><span>|</span><a href="#38338882">parent</a><span>|</span><a href="#38340408">next</a><span>|</span><label class="collapse" for="c-38339619">[-]</label><label class="expand" for="c-38339619">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! First time I come across. Looks very promising</div><br/></div></div></div></div></div></div><div id="38340408" class="c"><input type="checkbox" id="c-38340408" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#38337240">parent</a><span>|</span><a href="#38337693">prev</a><span>|</span><a href="#38338080">next</a><span>|</span><label class="collapse" for="c-38340408">[-]</label><label class="expand" for="c-38340408">[2 more]</label></div><br/><div class="children"><div class="content">&gt; is starting to get old lol.<p>If it&#x27;s starting to get old, then this means that an LLM like Copilot should be able to do it for you, no?</div><br/><div id="38343308" class="c"><input type="checkbox" id="c-38343308" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#38337240">root</a><span>|</span><a href="#38340408">parent</a><span>|</span><a href="#38338080">next</a><span>|</span><label class="collapse" for="c-38343308">[-]</label><label class="expand" for="c-38343308">[1 more]</label></div><br/><div class="children"><div class="content">I mean that I already have like 10 different torch venvs for different projects all with various pinned versions and CUDA variants.<p>Still worth the trade-off of not having to deal with dependency hell, but you start to wonder if there is a better way. All together this is many GBs of duplicated libs, wasted bandwidth and compute.</div><br/></div></div></div></div></div></div><div id="38338080" class="c"><input type="checkbox" id="c-38338080" checked=""/><div class="controls bullet"><span class="by">tomcam</span><span>|</span><a href="#38337240">prev</a><span>|</span><a href="#38341891">next</a><span>|</span><label class="collapse" for="c-38338080">[-]</label><label class="expand" for="c-38338080">[1 more]</label></div><br/><div class="children"><div class="content">Very impressive. It would take me a long time to even guess that some of these are text to speech.</div><br/></div></div><div id="38341891" class="c"><input type="checkbox" id="c-38341891" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#38338080">prev</a><span>|</span><a href="#38339065">next</a><span>|</span><label class="collapse" for="c-38341891">[-]</label><label class="expand" for="c-38341891">[1 more]</label></div><br/><div class="children"><div class="content">Wow this thing is wicked fast!</div><br/></div></div><div id="38339065" class="c"><input type="checkbox" id="c-38339065" checked=""/><div class="controls bullet"><span class="by">causality0</span><span>|</span><a href="#38341891">prev</a><span>|</span><a href="#38338371">next</a><span>|</span><label class="collapse" for="c-38339065">[-]</label><label class="expand" for="c-38339065">[1 more]</label></div><br/><div class="children"><div class="content">What are the chances this gets packaged into something a little more streamlined to use? I have a lot of ebooks I&#x27;d love to generate audio versions of.</div><br/></div></div><div id="38338371" class="c"><input type="checkbox" id="c-38338371" checked=""/><div class="controls bullet"><span class="by">readyplayernull</span><span>|</span><a href="#38339065">prev</a><span>|</span><a href="#38336357">next</a><span>|</span><label class="collapse" for="c-38338371">[-]</label><label class="expand" for="c-38338371">[1 more]</label></div><br/><div class="children"><div class="content">Someone please create a TTS with marked-down emotions&#x2F;intonations.</div><br/></div></div><div id="38336357" class="c"><input type="checkbox" id="c-38336357" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38338371">prev</a><span>|</span><label class="collapse" for="c-38336357">[-]</label><label class="expand" for="c-38336357">[7 more]</label></div><br/><div class="children"><div class="content">Why name it Style&lt;anything&gt; if it isn&#x27;t a StyleGAN? Looks like the first one wasn&#x27;t either. Interesting to see moves away from flows, especially when none of the flows were modern.<p>Also, is no one clicking on the audio links? There are some... questionable ones... and I&#x27;m pretty sure lots of mistakes.</div><br/><div id="38336715" class="c"><input type="checkbox" id="c-38336715" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38336357">parent</a><span>|</span><a href="#38336710">next</a><span>|</span><label class="collapse" for="c-38336715">[-]</label><label class="expand" for="c-38336715">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not called a GAN TTS right? StyleGAN is called what it is because of a &quot;style-based&quot; approach and StyleTTS&#x2F;2 seems to be doing the same (applying style transfer) through different method (and disentangling style from the rest of the voice synthesis).<p>(Actually, looked at the original StyleTTS paper and it actually even partially uses AdaIN in the decoder, which is the same way that StyleGAN injected style information? Still, I think is besides the point for the naming.)</div><br/><div id="38338156" class="c"><input type="checkbox" id="c-38338156" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38336357">root</a><span>|</span><a href="#38336715">parent</a><span>|</span><a href="#38336710">next</a><span>|</span><label class="collapse" for="c-38338156">[-]</label><label class="expand" for="c-38338156">[3 more]</label></div><br/><div class="children"><div class="content">Yeah no I get this but the naming convention has become so prolific that anyone working in generative space hears &quot;Style&lt;thing&gt;&quot; and you should think &quot;GAN&quot;. (I work in generative vision btw)<p>My point is not that it is technically right, it is that the name is strongly related with the concept now. Such that if you use a style based network and don&#x27;t name it StyleX that it&#x27;s odd and might look like you&#x27;re trying to claim you&#x27;ve done more. Not that there aren&#x27;t plenty of GANs that are using Karras&#x27;s code and called something else.<p>&gt; AdaIN<p>Yes, StyleGAN (version 1) uses AdaIN but StyleGAN2 (and beyond) doesn&#x27;t. AdaIN stands for Adaptive Instance Normalization. While they use it in that network, to be clear, they did not invent AdaIN and the technique isn&#x27;t explicit to style, it&#x27;s a normalization technique. One that StyleGAN2 modifies because the standard one creates strong and localized spikes in the statistics which results in image artifacts.</div><br/><div id="38338761" class="c"><input type="checkbox" id="c-38338761" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#38336357">root</a><span>|</span><a href="#38338156">parent</a><span>|</span><a href="#38336710">next</a><span>|</span><label class="collapse" for="c-38338761">[-]</label><label class="expand" for="c-38338761">[2 more]</label></div><br/><div class="children"><div class="content">So what I&#x27;m hearing is... no one should use &quot;style&quot; in its name anymore to describe style transfers because it&#x27;s too closely associated with a set of models in a sub-field that uses a different concept to apply style that used &quot;style&quot; in its name, unless it also uses that unrelated concept in its implementation?  Is that the gist of it, because that sounds a bit mental.<p>(I&#x27;m half kidding, I get what you mean, but also, think about it. The alternative is worse.)</div><br/><div id="38340378" class="c"><input type="checkbox" id="c-38340378" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38336357">root</a><span>|</span><a href="#38338761">parent</a><span>|</span><a href="#38336710">next</a><span>|</span><label class="collapse" for="c-38340378">[-]</label><label class="expand" for="c-38340378">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m half kidding, I get what you mean<p>I mean yeah, I&#x27;m not saying that they shouldn&#x27;t be able to use the name. There&#x27;s no control of &quot;StyleX&quot; but it certainly is a poor choice that can lead to confusion. That&#x27;s all I&#x27;m getting at. 100% this is an opinion (would be insane if believed to be anything else).<p>I don&#x27;t think it is just a &quot;sub-field&quot; as you mention and it definitely isn&#x27;t like StyleGAN isn&#x27;t known by nearly every person that learns ML (I have seen very few courses that do not mention it, but those tend to be ones that don&#x27;t discuss generation at all). StyleGAN is one of the most well known models that exist. Up there with GPT, YOLO, and ViT. Realistically we use these names as a style of model now rather than the actual original model themselves (or somewhat interchangeably).<p>The original StyleTTS&#x27;s abstract has the line<p>&gt; Here, we propose StyleTTS, a
style-based generative model for parallel TTS<p>And I certainly would not blame anyone for thinking &quot;Oh, they&#x27;re using a StyleGAN&quot;. That&#x27;s all I&#x27;m saying. Their style encoder looks nothing like the StyleGAN&#x27;s style encoder. It looks a bit closer to the synthesis network but that&#x27;s just because they&#x27;re using Leaky ReLUs and AdaIN, but like we said before, that&#x27;s not really a StyleGAN specific thing. There are also other parts we could say look similar but they are pretty generic sections that I wouldn&#x27;t particularly think uniquely pertains to StyleGAN architectures (or StyleDiffusion ones that do make this callback).<p>It other words, it&#x27;s like naming something iX. Sure, Apple doesn&#x27;t have complete control over a leading letter but I also understand Apple&#x27;s claim that such a naming pattern can confuse people. Certainly a name collision. Hell, I&#x27;ll say that the authors that made this paper knew what they were doing <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.01452" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.01452</a><p>I just think they can come up with a better name that has worse chance of collision. It&#x27;s not like StyleTTS is a particularly creative name or even that apt of a description either. Names are important because they do mean things. You may think it is not a poor choice of naming and that&#x27;s okay too. But we also work in different fields too and I&#x27;d argue that research papers are aimed at other researchers, where I would be surprised if anyone works in generations (image, voice, language, data, whatever) is not well aware of the Style based networks. Because I can use that sentence and it make sense to most ML people.</div><br/></div></div></div></div></div></div></div></div><div id="38336710" class="c"><input type="checkbox" id="c-38336710" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#38336357">parent</a><span>|</span><a href="#38336715">prev</a><span>|</span><label class="collapse" for="c-38336710">[-]</label><label class="expand" for="c-38336710">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  Looks like the first one wasn&#x27;t either.<p>The first one says it uses AdaIN layers to help control style? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.15439.pdf#page=2" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.15439.pdf#page=2</a> Seems as justifiable as the original StyleGAN calling itself StyleX...</div><br/><div id="38338173" class="c"><input type="checkbox" id="c-38338173" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38336357">root</a><span>|</span><a href="#38336710">parent</a><span>|</span><label class="collapse" for="c-38338173">[-]</label><label class="expand" for="c-38338173">[1 more]</label></div><br/><div class="children"><div class="content">See my other comment. StyleGAN isn&#x27;t about AdaIN. StyleGAN2 even modified it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>