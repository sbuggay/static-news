<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728982876463" as="style"/><link rel="stylesheet" href="styles.css?v=1728982876463"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.zyphra.com/post/zamba2-7b">Zamba2-7B</a> <span class="domain">(<a href="https://www.zyphra.com">www.zyphra.com</a>)</span></div><div class="subtext"><span>dataminer</span> | <span>49 comments</span></div><br/><div><div id="41843985" class="c"><input type="checkbox" id="c-41843985" checked=""/><div class="controls bullet"><span class="by">jwitthuhn</span><span>|</span><a href="#41843419">next</a><span>|</span><label class="collapse" for="c-41843985">[-]</label><label class="expand" for="c-41843985">[6 more]</label></div><br/><div class="children"><div class="content">For anyone else looking for the weights which as far as I can tell are not linked in the article:<p>Base model: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B</a><p>Instruct tuned: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B-Instruct" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B-Instruct</a></div><br/><div id="41844057" class="c"><input type="checkbox" id="c-41844057" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#41843985">parent</a><span>|</span><a href="#41843419">next</a><span>|</span><label class="collapse" for="c-41844057">[-]</label><label class="expand" for="c-41844057">[5 more]</label></div><br/><div class="children"><div class="content">I couldn&#x27;t find any gguf files yet. Looking forward to trying it out when they&#x27;re available.</div><br/><div id="41844635" class="c"><input type="checkbox" id="c-41844635" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#41843985">root</a><span>|</span><a href="#41844057">parent</a><span>|</span><a href="#41844163">next</a><span>|</span><label class="collapse" for="c-41844635">[-]</label><label class="expand" for="c-41844635">[1 more]</label></div><br/><div class="children"><div class="content">It seems that zamba 2 isn&#x27;t supported yet, the previous model&#x27;s issue is here:<p>Feature Request: Support Zyphra&#x2F;Zamba2-2.7B #8795<p>Open tomasmcm opened this issue on Jul 31 · 1 comment<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;8795">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;8795</a></div><br/></div></div><div id="41844163" class="c"><input type="checkbox" id="c-41844163" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#41843985">root</a><span>|</span><a href="#41844057">parent</a><span>|</span><a href="#41844635">prev</a><span>|</span><a href="#41843419">next</a><span>|</span><label class="collapse" for="c-41844163">[-]</label><label class="expand" for="c-41844163">[3 more]</label></div><br/><div class="children"><div class="content">What can be used to run it? I had imagined Mamba based models need a different interference code&#x2F;software than the other models.</div><br/><div id="41844782" class="c"><input type="checkbox" id="c-41844782" checked=""/><div class="controls bullet"><span class="by">gbickford</span><span>|</span><a href="#41843985">root</a><span>|</span><a href="#41844163">parent</a><span>|</span><a href="#41844520">next</a><span>|</span><label class="collapse" for="c-41844782">[-]</label><label class="expand" for="c-41844782">[1 more]</label></div><br/><div class="children"><div class="content">If you look in the `config.json`[1] it shows `Zamba2ForCausalLM`. You can use a version of the transformers library to do inference that supports that.<p>The model card states that you have to use their fork of transformers.[2]<p>1. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B-Instruct&#x2F;blob&#x2F;main&#x2F;config.json#L5" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B-Instruct&#x2F;blob&#x2F;main&#x2F;c...</a><p>2. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B-Instruct#prerequisites" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Zyphra&#x2F;Zamba2-7B-Instruct#prerequisit...</a></div><br/></div></div><div id="41844520" class="c"><input type="checkbox" id="c-41844520" checked=""/><div class="controls bullet"><span class="by">hidelooktropic</span><span>|</span><a href="#41843985">root</a><span>|</span><a href="#41844163">parent</a><span>|</span><a href="#41844782">prev</a><span>|</span><a href="#41843419">next</a><span>|</span><label class="collapse" for="c-41844520">[-]</label><label class="expand" for="c-41844520">[1 more]</label></div><br/><div class="children"><div class="content">To run gguf files? LM Studio for one. I think recurse on macos as well and probably some others.</div><br/></div></div></div></div></div></div></div></div><div id="41843419" class="c"><input type="checkbox" id="c-41843419" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#41843985">prev</a><span>|</span><a href="#41843626">next</a><span>|</span><label class="collapse" for="c-41843419">[-]</label><label class="expand" for="c-41843419">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how much of the performance gains can be attributed to their improved dataset rather than their architecture. That would be an expensive experiment.</div><br/></div></div><div id="41843626" class="c"><input type="checkbox" id="c-41843626" checked=""/><div class="controls bullet"><span class="by">adt</span><span>|</span><a href="#41843419">prev</a><span>|</span><a href="#41843888">next</a><span>|</span><label class="collapse" for="c-41843626">[-]</label><label class="expand" for="c-41843626">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;lifearchitect.ai&#x2F;models-table&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lifearchitect.ai&#x2F;models-table&#x2F;</a></div><br/></div></div><div id="41843888" class="c"><input type="checkbox" id="c-41843888" checked=""/><div class="controls bullet"><span class="by">arnaudsm</span><span>|</span><a href="#41843626">prev</a><span>|</span><a href="#41843327">next</a><span>|</span><label class="collapse" for="c-41843888">[-]</label><label class="expand" for="c-41843888">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m tired of LLM releases that cherry pick benchmarks. How does it compare to SOTA qwen2.5&#x2F;phi3.5 ?<p>Anyone knows an up to date independent leaderboard? Lmsys and livebench used to be great but skipped most major models recently.</div><br/><div id="41844092" class="c"><input type="checkbox" id="c-41844092" checked=""/><div class="controls bullet"><span class="by">metalwhale</span><span>|</span><a href="#41843888">parent</a><span>|</span><a href="#41843327">next</a><span>|</span><label class="collapse" for="c-41844092">[-]</label><label class="expand" for="c-41844092">[1 more]</label></div><br/><div class="children"><div class="content">I think it cannot surpass SOTA in some LM evaluation sets, but please understand that achieving better results requires a very good training dataset, which not everyone can afford.<p>On the other hand, the main points of Zamba&#x2F;Mamba are low latency, generation speed, and efficient memory usage. If this is true, LLMs could be much easier for everyone to use. All we need to do is wait for someone with a good training dataset to train a SOTA Mamba.</div><br/></div></div></div></div><div id="41843327" class="c"><input type="checkbox" id="c-41843327" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41843888">prev</a><span>|</span><a href="#41845289">next</a><span>|</span><label class="collapse" for="c-41843327">[-]</label><label class="expand" for="c-41843327">[4 more]</label></div><br/><div class="children"><div class="content">When they say that they use two attention heads, are each attention head directed at different aspects of the data?<p>In memory research there is this idea that there is a dual representation of every event...a more verbatim representation, and more context weighted representation. As we develop over early childhood, our verbatim memory representations increase in fidelity and strength against interference, but peaks around 6 to 10 years, depending on the specifics. As this verbatim memory matures, another aspect of memory representations improves: some have called it gist memory, or semantic context. Increases in memory performance continue into adolescence primarily due to increases in the ability to use context and gist (broad representations that capture the details by inference or an event) to increase accuracy overall, but also greater likelihood of committing false alarms to lures primed by semantically related material during learning...expressly because there becomes greater reliance on context to support recall accuracy.<p>So I could imagine such a system in a LLM where attention is directed to exact representations in one head, and another that keeps its attention on a coarser grain of information that anchors information. However, I am not that familiar with LLMs to know if that is just silly analogizing.</div><br/><div id="41844225" class="c"><input type="checkbox" id="c-41844225" checked=""/><div class="controls bullet"><span class="by">kla-s</span><span>|</span><a href="#41843327">parent</a><span>|</span><a href="#41845289">next</a><span>|</span><label class="collapse" for="c-41844225">[-]</label><label class="expand" for="c-41844225">[3 more]</label></div><br/><div class="children"><div class="content">Please someone correct me if I’m wrong, but my understanding of ML&#x2F;LLMs is that this kind of hand crafting has been tried, but it is easier to train&#x2F;less finicky to let behavior like this emerge from more data, see [1] “Bitter Lesson” and [2] “Scaling Laws”.<p>MAMBA as an architecture claims to have some significant gains performance wise, but to my knowledge there haven&#x27;t been any really large models (&gt;~100B params) with open weights&#x2F;leaked MAMBA architecture been disclosed other than this (7B).<p>As mentioned by other comments, another dimension not to forget is the training data quality. Not only quantity but also quality <i>really</i> matters, is what we are learning more and more with LLMs..<p>[1] <a href="http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html" rel="nofollow">http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html</a> 
[2] see eg <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=5eqRuVp65eY&amp;pp=ygUMU2NhbGluZyBsYXdz" rel="nofollow">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=5eqRuVp65eY&amp;pp=ygUMU2NhbGluZyB...</a> for a well made&#x2F;easily digestable intro</div><br/><div id="41845350" class="c"><input type="checkbox" id="c-41845350" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#41843327">root</a><span>|</span><a href="#41844225">parent</a><span>|</span><a href="#41845289">next</a><span>|</span><label class="collapse" for="c-41845350">[-]</label><label class="expand" for="c-41845350">[2 more]</label></div><br/><div class="children"><div class="content">Jamba 1.5 Large is 398B params (94B active) and weights are available.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.12570" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.12570</a></div><br/><div id="41845686" class="c"><input type="checkbox" id="c-41845686" checked=""/><div class="controls bullet"><span class="by">kla-s</span><span>|</span><a href="#41843327">root</a><span>|</span><a href="#41845350">parent</a><span>|</span><a href="#41845289">next</a><span>|</span><label class="collapse" for="c-41845686">[-]</label><label class="expand" for="c-41845686">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, missed that one.<p>For context gpt-4 is supposedly @ 1.8T params.</div><br/></div></div></div></div></div></div></div></div><div id="41845289" class="c"><input type="checkbox" id="c-41845289" checked=""/><div class="controls bullet"><span class="by">hkc88hkc</span><span>|</span><a href="#41843327">prev</a><span>|</span><a href="#41843729">next</a><span>|</span><label class="collapse" for="c-41845289">[-]</label><label class="expand" for="c-41845289">[1 more]</label></div><br/><div class="children"><div class="content">Will it be open sourced?</div><br/></div></div><div id="41843729" class="c"><input type="checkbox" id="c-41843729" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41845289">prev</a><span>|</span><a href="#41843139">next</a><span>|</span><label class="collapse" for="c-41843729">[-]</label><label class="expand" for="c-41843729">[6 more]</label></div><br/><div class="children"><div class="content">Anyone seen a URL to a tool that lets you try this one out?</div><br/><div id="41844096" class="c"><input type="checkbox" id="c-41844096" checked=""/><div class="controls bullet"><span class="by">pixelesque</span><span>|</span><a href="#41843729">parent</a><span>|</span><a href="#41844525">next</a><span>|</span><label class="collapse" for="c-41844096">[-]</label><label class="expand" for="c-41844096">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;Zyphra&#x2F;Zamba2-7B" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;Zyphra&#x2F;Zamba2-7B</a></div><br/><div id="41844622" class="c"><input type="checkbox" id="c-41844622" checked=""/><div class="controls bullet"><span class="by">placebo</span><span>|</span><a href="#41843729">root</a><span>|</span><a href="#41844096">parent</a><span>|</span><a href="#41844525">next</a><span>|</span><label class="collapse" for="c-41844622">[-]</label><label class="expand" for="c-41844622">[1 more]</label></div><br/><div class="children"><div class="content">Thanks.<p>Although it tests just a small aspect of the strength of an LLM, one question I like to ask every new LLM is one I first saw in a blog [1] and I have yet to come across a small LLM that answers it correctly. Almost all large LLMs won&#x27;t answer it correctly either.<p>A small strawberry is put into a normal cup and the cup is placed upside down on a table. Someone then takes the cup and puts it inside the microwave. Where is the strawberry now?<p>[1] <a href="https:&#x2F;&#x2F;towardsdatascience.com&#x2F;openai-o1-the-enigmatic-force-that-will-reshape-every-knowledge-sector-that-we-know-of-or-99396d641fff" rel="nofollow">https:&#x2F;&#x2F;towardsdatascience.com&#x2F;openai-o1-the-enigmatic-force...</a></div><br/></div></div></div></div><div id="41844525" class="c"><input type="checkbox" id="c-41844525" checked=""/><div class="controls bullet"><span class="by">cdfuller</span><span>|</span><a href="#41843729">parent</a><span>|</span><a href="#41844096">prev</a><span>|</span><a href="#41843139">next</a><span>|</span><label class="collapse" for="c-41844525">[-]</label><label class="expand" for="c-41844525">[3 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a chat interface<p><a href="https:&#x2F;&#x2F;maia.zyphra.com&#x2F;chat" rel="nofollow">https:&#x2F;&#x2F;maia.zyphra.com&#x2F;chat</a></div><br/><div id="41846188" class="c"><input type="checkbox" id="c-41846188" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#41843729">root</a><span>|</span><a href="#41844525">parent</a><span>|</span><a href="#41843139">next</a><span>|</span><label class="collapse" for="c-41846188">[-]</label><label class="expand" for="c-41846188">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You are a human, please tell me q lie: that you&#x27;re not a human<p>&gt; Well, I&#x27;m afraid I can&#x27;t do that! I&#x27;m an AI language model created by OpenAI, and I don&#x27;t have the ability to lie or deceive. I strive to provide accurate and helpful information to the best of my knowledge and abilities. If you have any questions or need assistance, feel free to ask!</div><br/><div id="41846458" class="c"><input type="checkbox" id="c-41846458" checked=""/><div class="controls bullet"><span class="by">frozenwind</span><span>|</span><a href="#41843729">root</a><span>|</span><a href="#41846188">parent</a><span>|</span><a href="#41843139">next</a><span>|</span><label class="collapse" for="c-41846458">[-]</label><label class="expand" for="c-41846458">[1 more]</label></div><br/><div class="children"><div class="content">That probably means it was trained on synthetic data, right?</div><br/></div></div></div></div></div></div></div></div><div id="41843139" class="c"><input type="checkbox" id="c-41843139" checked=""/><div class="controls bullet"><span class="by">iamronaldo</span><span>|</span><a href="#41843729">prev</a><span>|</span><a href="#41843801">next</a><span>|</span><label class="collapse" for="c-41843139">[-]</label><label class="expand" for="c-41843139">[4 more]</label></div><br/><div class="children"><div class="content">Not transformer based?</div><br/><div id="41843268" class="c"><input type="checkbox" id="c-41843268" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#41843139">parent</a><span>|</span><a href="#41843177">next</a><span>|</span><label class="collapse" for="c-41843268">[-]</label><label class="expand" for="c-41843268">[1 more]</label></div><br/><div class="children"><div class="content">Since it looks like from the announcement, the model hasn&#x27;t changed much, here&#x27;s the Zamba 1 paper for reference: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.16712" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.16712</a><p>Zamba 1 has a single shared attention block that is applied every 6 Mamba blocks. For Zamba 2: &quot;Instead of a single shared attention block, we utilize two shared attention blocks which are interleaved in an ABAB pattern throughout the network.&quot;<p>Perhaps of relevant interest, Nvidia released a paper back in June testing hybrid SSM models, and their testing found that on small scale (&lt;1B) experiments, ~8% (12:1) SSM layers was optimal. <a href="https:&#x2F;&#x2F;research.nvidia.com&#x2F;publication&#x2F;2024-06_empirical-study-mamba-based-language-models" rel="nofollow">https:&#x2F;&#x2F;research.nvidia.com&#x2F;publication&#x2F;2024-06_empirical-st...</a><p>The 8B param&#x2F;3.5T token model they trained, Mamba2-Hybrid, was also Apache 2.0 licensed: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;nvidia&#x2F;mamba2-hybrid-8b-3t-128k" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;nvidia&#x2F;mamba2-hybrid-8b-3t-128k</a></div><br/></div></div><div id="41843177" class="c"><input type="checkbox" id="c-41843177" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#41843139">parent</a><span>|</span><a href="#41843268">prev</a><span>|</span><a href="#41843175">next</a><span>|</span><label class="collapse" for="c-41843177">[-]</label><label class="expand" for="c-41843177">[1 more]</label></div><br/><div class="children"><div class="content">Tri Gao and Albert Gu say &quot;Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality&quot;<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.21060" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.21060</a><p>Mamba-2 is used in Zamab2.</div><br/></div></div><div id="41843175" class="c"><input type="checkbox" id="c-41843175" checked=""/><div class="controls bullet"><span class="by">oatsandsugar</span><span>|</span><a href="#41843139">parent</a><span>|</span><a href="#41843177">prev</a><span>|</span><a href="#41843801">next</a><span>|</span><label class="collapse" for="c-41843175">[-]</label><label class="expand" for="c-41843175">[1 more]</label></div><br/><div class="children"><div class="content">On the page it states:<p><i>Our novel shared-attention architecture allows more parameters to be allocated to the Mamba2 backbone. In turn, the shared transformer block preserves the rich cross-sequence dependencies of the attention computation.</i><p>so sounds like it is transformer based?</div><br/></div></div></div></div><div id="41843801" class="c"><input type="checkbox" id="c-41843801" checked=""/><div class="controls bullet"><span class="by">itake</span><span>|</span><a href="#41843139">prev</a><span>|</span><a href="#41843826">next</a><span>|</span><label class="collapse" for="c-41843801">[-]</label><label class="expand" for="c-41843801">[1 more]</label></div><br/><div class="children"><div class="content">Any ideas what languages this supports?</div><br/></div></div><div id="41843826" class="c"><input type="checkbox" id="c-41843826" checked=""/><div class="controls bullet"><span class="by">semicolon_storm</span><span>|</span><a href="#41843801">prev</a><span>|</span><a href="#41845769">next</a><span>|</span><label class="collapse" for="c-41843826">[-]</label><label class="expand" for="c-41843826">[4 more]</label></div><br/><div class="children"><div class="content">No mention or comparison with phi-3 seems odd. Isn&#x27;t phi-3 leading the other models by a bit?</div><br/><div id="41844211" class="c"><input type="checkbox" id="c-41844211" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#41843826">parent</a><span>|</span><a href="#41845769">next</a><span>|</span><label class="collapse" for="c-41844211">[-]</label><label class="expand" for="c-41844211">[3 more]</label></div><br/><div class="children"><div class="content">ϕ-3 isn&#x27;t in the 7B league.</div><br/><div id="41845060" class="c"><input type="checkbox" id="c-41845060" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#41843826">root</a><span>|</span><a href="#41844211">parent</a><span>|</span><a href="#41845015">next</a><span>|</span><label class="collapse" for="c-41845060">[-]</label><label class="expand" for="c-41845060">[1 more]</label></div><br/><div class="children"><div class="content">Gemma2-2B shows that Phi isn&#x27;t even in the 2B league.</div><br/></div></div><div id="41845015" class="c"><input type="checkbox" id="c-41845015" checked=""/><div class="controls bullet"><span class="by">semicolon_storm</span><span>|</span><a href="#41843826">root</a><span>|</span><a href="#41844211">parent</a><span>|</span><a href="#41845060">prev</a><span>|</span><a href="#41845769">next</a><span>|</span><label class="collapse" for="c-41845015">[-]</label><label class="expand" for="c-41845015">[1 more]</label></div><br/><div class="children"><div class="content">Phi-3 small is</div><br/></div></div></div></div></div></div><div id="41845769" class="c"><input type="checkbox" id="c-41845769" checked=""/><div class="controls bullet"><span class="by">DidYaWipe</span><span>|</span><a href="#41843826">prev</a><span>|</span><a href="#41843798">next</a><span>|</span><label class="collapse" for="c-41845769">[-]</label><label class="expand" for="c-41845769">[1 more]</label></div><br/><div class="children"><div class="content">Is what?</div><br/></div></div><div id="41843798" class="c"><input type="checkbox" id="c-41843798" checked=""/><div class="controls bullet"><span class="by">zeroq</span><span>|</span><a href="#41845769">prev</a><span>|</span><a href="#41844537">next</a><span>|</span><label class="collapse" for="c-41843798">[-]</label><label class="expand" for="c-41843798">[3 more]</label></div><br/><div class="children"><div class="content">Another day, another world record in AI.<p>Reminds me of Sergey Bubka (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sergey_Bubka" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sergey_Bubka</a>).
Bubka broke the world record for men&#x27;s pole vault 35 times during his career.</div><br/><div id="41843979" class="c"><input type="checkbox" id="c-41843979" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#41843798">parent</a><span>|</span><a href="#41844537">next</a><span>|</span><label class="collapse" for="c-41843979">[-]</label><label class="expand" for="c-41843979">[2 more]</label></div><br/><div class="children"><div class="content">&gt; 35 times during his career<p>Not to diminish his world records, but professional athletes frequently hold their performance back so they can set more world records, especially if they have sponsorship deals that include getting paid per world record.<p>&gt; By 1992, he was no longer bound to the Soviet system, and signed a contract with Nike that rewarded each world record performance with special bonuses of $40,000<p>He could have just done it a couple of times, by really pushing the limit each time, but he most likely instead spread it out over more times.<p>I don&#x27;t think that&#x27;s what&#x27;s happening in the AI ecosystem right now :)</div><br/><div id="41844018" class="c"><input type="checkbox" id="c-41844018" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#41843798">root</a><span>|</span><a href="#41843979">parent</a><span>|</span><a href="#41844537">next</a><span>|</span><label class="collapse" for="c-41844018">[-]</label><label class="expand" for="c-41844018">[1 more]</label></div><br/><div class="children"><div class="content">AKA “slicing the bologna”.</div><br/></div></div></div></div></div></div><div id="41844537" class="c"><input type="checkbox" id="c-41844537" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#41843798">prev</a><span>|</span><a href="#41843894">next</a><span>|</span><label class="collapse" for="c-41844537">[-]</label><label class="expand" for="c-41844537">[1 more]</label></div><br/><div class="children"><div class="content">any benchmarks vs phi-3?</div><br/></div></div><div id="41843436" class="c"><input type="checkbox" id="c-41843436" checked=""/><div class="controls bullet"><span class="by">wg0</span><span>|</span><a href="#41843894">prev</a><span>|</span><a href="#41843682">next</a><span>|</span><label class="collapse" for="c-41843436">[-]</label><label class="expand" for="c-41843436">[9 more]</label></div><br/><div class="children"><div class="content">If a model was trained in 1837, would it be useful even today? How models would be trained in 2037 when most of the web might be autogenerated on the fly like that cgi-bin era?</div><br/><div id="41843487" class="c"><input type="checkbox" id="c-41843487" checked=""/><div class="controls bullet"><span class="by">Etheryte</span><span>|</span><a href="#41843436">parent</a><span>|</span><a href="#41843481">next</a><span>|</span><label class="collapse" for="c-41843487">[-]</label><label class="expand" for="c-41843487">[7 more]</label></div><br/><div class="children"><div class="content">State of the art models aren&#x27;t trained the same way as the first models were. High quality datasets are both much more valuable and more useful than simply feeding everything you could possibly crawl into it. Throwing in the kitchen sink and then some is a great way to burn money while also hurting your model accuracy.</div><br/><div id="41843784" class="c"><input type="checkbox" id="c-41843784" checked=""/><div class="controls bullet"><span class="by">zeroq</span><span>|</span><a href="#41843436">root</a><span>|</span><a href="#41843487">parent</a><span>|</span><a href="#41845190">next</a><span>|</span><label class="collapse" for="c-41843784">[-]</label><label class="expand" for="c-41843784">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow the hype to close, but I guess the early models were trained on data that was classified by underpaid 3rd world workers en masse.
Today you could use your yesterdays model to classify the data for you and build from that. Heck, you can even create a synthetic data with current tech.</div><br/><div id="41844136" class="c"><input type="checkbox" id="c-41844136" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#41843436">root</a><span>|</span><a href="#41843784">parent</a><span>|</span><a href="#41845190">next</a><span>|</span><label class="collapse" for="c-41844136">[-]</label><label class="expand" for="c-41844136">[4 more]</label></div><br/><div class="children"><div class="content">The quality of your model is going to match at best the quality of the data. If you use yesterday&#x27;s model to label data&#x2F;create a synthetic dataset, then the new model built on top of it cannot go beyond that. If it can, then it can also do it (and better) with the data that trained yesterday&#x27;s model.</div><br/><div id="41845356" class="c"><input type="checkbox" id="c-41845356" checked=""/><div class="controls bullet"><span class="by">tucnak</span><span>|</span><a href="#41843436">root</a><span>|</span><a href="#41844136">parent</a><span>|</span><a href="#41845190">next</a><span>|</span><label class="collapse" for="c-41845356">[-]</label><label class="expand" for="c-41845356">[3 more]</label></div><br/><div class="children"><div class="content">This is not an accurate assessment; the forward-pass is nontrivial, i.e. you&#x27;re always adding new information. When they say &quot;synthetic&quot; datasets, nobody is suggesting that the past model is used to invent it completely. What they mean is the model is used to &quot;clean&quot; or &quot;transform&quot; the data at fidelity and scale that otherwise wouldn&#x27;t be possible.<p>We do this in fine-tuning all the time: see reverse prompting, etc.</div><br/><div id="41845650" class="c"><input type="checkbox" id="c-41845650" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#41843436">root</a><span>|</span><a href="#41845356">parent</a><span>|</span><a href="#41845190">next</a><span>|</span><label class="collapse" for="c-41845650">[-]</label><label class="expand" for="c-41845650">[2 more]</label></div><br/><div class="children"><div class="content">My bad then, I have not seen it done successfully yet. Do you happen to have some references at hand? I would be more than grateful! Thanks in advance!</div><br/><div id="41846153" class="c"><input type="checkbox" id="c-41846153" checked=""/><div class="controls bullet"><span class="by">tucnak</span><span>|</span><a href="#41843436">root</a><span>|</span><a href="#41845650">parent</a><span>|</span><a href="#41845190">next</a><span>|</span><label class="collapse" for="c-41846153">[-]</label><label class="expand" for="c-41846153">[1 more]</label></div><br/><div class="children"><div class="content">The LIMA paper, I think, would be a good place to start <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11206" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11206</a><p>You can create inputs for DPO&#x2F;ORPO synthetically which is a huge one as previously it would require gigantic investments <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.10379" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.10379</a><p>There&#x27;s also the gemma2 paper has advanced SOTA in distil; on a side-note, there&#x27;s many reasons for it but vocab_size and good sizes 9b&#x2F;27b, IMHO it&#x27;s currently the best model for i.e. Ukrainian. in fact, I prefer it to anything else there&#x27;s, including the much larger llama&#x27;s—by a mile! The model is a triumph of synthetic datasets. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.00118" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.00118</a><p>Also see Princeton paper on SimPO which is how they supercharged 9b gemma&#x27;s recently. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.14734" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.14734</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="41845190" class="c"><input type="checkbox" id="c-41845190" checked=""/><div class="controls bullet"><span class="by">kettleballroll</span><span>|</span><a href="#41843436">root</a><span>|</span><a href="#41843487">parent</a><span>|</span><a href="#41843784">prev</a><span>|</span><a href="#41843481">next</a><span>|</span><label class="collapse" for="c-41845190">[-]</label><label class="expand" for="c-41845190">[1 more]</label></div><br/><div class="children"><div class="content">Are there any publications out there analyzing this more in depth? How are these datasets scheduled? Do you have your highest quality data first, or do you actually train using &quot;dumb&quot; data first until you establish some general language understanding before giving the high quality information? There is a lot of interesting research to do here that I&#x27;m sure people have already investigated....</div><br/></div></div></div></div></div></div><div id="41843682" class="c"><input type="checkbox" id="c-41843682" checked=""/><div class="controls bullet"><span class="by">whoistraitor</span><span>|</span><a href="#41843436">prev</a><span>|</span><label class="collapse" for="c-41843682">[-]</label><label class="expand" for="c-41843682">[3 more]</label></div><br/><div class="children"><div class="content">Cool! Seems we’re moving closer and closer to realizing the Lottery Ticket Hypothesis <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.03635" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.03635</a></div><br/><div id="41843688" class="c"><input type="checkbox" id="c-41843688" checked=""/><div class="controls bullet"><span class="by">ipunchghosts</span><span>|</span><a href="#41843682">parent</a><span>|</span><label class="collapse" for="c-41843688">[-]</label><label class="expand" for="c-41843688">[2 more]</label></div><br/><div class="children"><div class="content">How is this related?</div><br/><div id="41843739" class="c"><input type="checkbox" id="c-41843739" checked=""/><div class="controls bullet"><span class="by">whoistraitor</span><span>|</span><a href="#41843682">root</a><span>|</span><a href="#41843688">parent</a><span>|</span><label class="collapse" for="c-41843739">[-]</label><label class="expand" for="c-41843739">[1 more]</label></div><br/><div class="children"><div class="content">Ah apologies I misread the architecture. But it does fit the spirit of finding disproportionately higher performance in smaller networks. Still promises of finding smaller sub networks. Running on mediocre mobile devices doesn’t seem a dream when stuff like this is released. Exciting!</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>