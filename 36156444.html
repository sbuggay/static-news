<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685696458384" as="style"/><link rel="stylesheet" href="styles.css?v=1685696458384"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cvw.cac.cornell.edu/vector/default">Vectorization: Introduction</a> <span class="domain">(<a href="https://cvw.cac.cornell.edu">cvw.cac.cornell.edu</a>)</span></div><div class="subtext"><span>gsav</span> | <span>71 comments</span></div><br/><div><div id="36158438" class="c"><input type="checkbox" id="c-36158438" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36158286">next</a><span>|</span><label class="collapse" for="c-36158438">[-]</label><label class="expand" for="c-36158438">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Vectorization is a process by which floating-point computations in scientific code are compiled into special instructions that execute elementary operations (+,-,*, etc.) or functions (exp, cos, etc.) in parallel on fixed-size vector arrays.<p>I guess this is a scientific computing course or something but I feel like even so it’s important to point out that most processors have many vector instructions that operate on integers, bitfields, characters, and the like. The fundamental premise of “do a thing on a bunch of data at once” isn’t limited to just floating point operations.</div><br/><div id="36161091" class="c"><input type="checkbox" id="c-36161091" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#36158438">parent</a><span>|</span><a href="#36159017">next</a><span>|</span><label class="collapse" for="c-36161091">[-]</label><label class="expand" for="c-36161091">[2 more]</label></div><br/><div class="children"><div class="content">It seems odd in a scientific computing class to not mention classic cray style vectors. I mean, they haven’t been popular for a while, but you never know, RISC-V might bring them back.</div><br/><div id="36162351" class="c"><input type="checkbox" id="c-36162351" checked=""/><div class="controls bullet"><span class="by">Joker_vD</span><span>|</span><a href="#36158438">root</a><span>|</span><a href="#36161091">parent</a><span>|</span><a href="#36159017">next</a><span>|</span><label class="collapse" for="c-36162351">[-]</label><label class="expand" for="c-36162351">[1 more]</label></div><br/><div class="children"><div class="content">I thought I was crazy when looking at all those vector extensions piling up made me think &quot;why not just have some generic-sized vector instructions, and then let the CPU to sort out what is the largest chunk size it actually supports internally&quot; but apparently no, I am not! It was actually been done before and we might even have that again in the future. That&#x27;s nice.</div><br/></div></div></div></div><div id="36159017" class="c"><input type="checkbox" id="c-36159017" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#36158438">parent</a><span>|</span><a href="#36161091">prev</a><span>|</span><a href="#36158286">next</a><span>|</span><label class="collapse" for="c-36159017">[-]</label><label class="expand" for="c-36159017">[1 more]</label></div><br/><div class="children"><div class="content">Vectorization (disambiguation)
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vectorization" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vectorization</a> :<p>&gt; Array programming, <i>a style of computer programming where operations are applied to whole arrays instead of individual elements</i><p>&gt; Automatic vectorization, <i>a compiler optimization that transforms loops to vector operations</i><p>&gt; Image tracing, <i>the creation of vector from raster graphics</i><p>&gt; Word embedding, <i>mapping words to vectors, in natural language processing</i><p>&gt; Vectorization (mathematics), <i>a linear transformation which converts a matrix into a column vector</i><p>Vector (disambiguation) <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector</a><p>&gt; Vector (mathematics and physics):<p>&gt; <i>Row and column vectors</i>, single row or column matrices<p>&gt; <i>Vector space</i><p>&gt; <i>Vector field</i>, a vector for each point<p>And then there are a number of CS usages of the word vector for 1D arrays.<p>Compute kernel: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Compute_kernel" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Compute_kernel</a><p>GPGPU &gt; Vectorization, Stream Processing &gt; Compute kernels: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;General-purpose_computing_on_graphics_processing_units#Vectorization" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;General-purpose_computing_on_g...</a><p>sympy.utilities.lambdify.lambdify()  <a href="https:&#x2F;&#x2F;github.com&#x2F;sympy&#x2F;sympy&#x2F;blob&#x2F;a76b02fcd3a8b7f79b3a88df50c19eb7aee33c17&#x2F;sympy&#x2F;utilities&#x2F;lambdify.py#L182">https:&#x2F;&#x2F;github.com&#x2F;sympy&#x2F;sympy&#x2F;blob&#x2F;a76b02fcd3a8b7f79b3a88df...</a> :<p>&gt; <i>&quot;&quot;&quot;Convert a SymPy expression into a function that allows for fast numeric evaluation</i> [with the CPython math module, mpmath, NumPy, SciPy, CuPy, JAX, TensorFlow, SymPt, numexpr,]<p>pyorch lambdify PR, sympytorch: <a href="https:&#x2F;&#x2F;github.com&#x2F;sympy&#x2F;sympy&#x2F;pull&#x2F;20516#issuecomment-784285458">https:&#x2F;&#x2F;github.com&#x2F;sympy&#x2F;sympy&#x2F;pull&#x2F;20516#issuecomment-78428...</a><p>sympytorch: <a href="https:&#x2F;&#x2F;github.com&#x2F;patrick-kidger&#x2F;sympytorch">https:&#x2F;&#x2F;github.com&#x2F;patrick-kidger&#x2F;sympytorch</a> :<p>&gt; <i>Turn SymPy expressions into PyTorch Modules.</i><p>&gt; <i>SymPy floats (optionally) become trainable parameters. SymPy symbols are inputs to the Module.</i><p>sympy2jax <a href="https:&#x2F;&#x2F;github.com&#x2F;MilesCranmer&#x2F;sympy2jax">https:&#x2F;&#x2F;github.com&#x2F;MilesCranmer&#x2F;sympy2jax</a> :<p>&gt; <i>Turn SymPy expressions into parametrized, differentiable, vectorizable, JAX functions.</i><p>&gt; <i>All SymPy floats become trainable input parameters. SymPy symbols become columns of a passed matrix.</i></div><br/></div></div></div></div><div id="36158286" class="c"><input type="checkbox" id="c-36158286" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#36158438">prev</a><span>|</span><a href="#36159633">next</a><span>|</span><label class="collapse" for="c-36158286">[-]</label><label class="expand" for="c-36158286">[21 more]</label></div><br/><div class="children"><div class="content">Auto-vectorization is easier to get into than other SIMD-frameworks like CUDA, OpenCL, ROCm, Intel&#x27;s ISPC and whatnot. But in my experience, auto-vectorizers are just not as flexible as the proper SIMD-tools.<p>I&#x27;d say auto-vectorization should still be learned by modern high-performance programmers, because its very low-hanging fruit. You barely have to do anything and suddenly your for-loops are AVX512 optimized, though maybe not to the fullest extent possible.<p>Still, I suggest that programmers also learn how to properly make SIMD code. Maybe intrinsics are too hard in practice, but ISPC, CUDA, and other SIMD-programming environments make things far easier to learn than you might expect.<p>------------<p>ISPC in particular is Intel&#x27;s SIMD programming language, much akin to CUDA except it compiles into AVX512. So for AVX512-like code execution environments, using the ISPC language&#x2F;compiler is exceptionally useful.<p>Its harder to learn a new language than to learn a few compiler-options to enable auto-vectorization however. So in practice, auto-vectorization will continue to be used. But for tasks that specifically would benefit from SIMD-thinking, the dedicated ISPC language should be beneficial.</div><br/><div id="36161954" class="c"><input type="checkbox" id="c-36161954" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#36158286">parent</a><span>|</span><a href="#36158341">next</a><span>|</span><label class="collapse" for="c-36161954">[-]</label><label class="expand" for="c-36161954">[1 more]</label></div><br/><div class="children"><div class="content">Even more important than learning how to vectorize is learning the basics of computer architecture: register, caches, pipelining, branch prediction, and superscalar execution.<p>One of the common interview questions I ask is computing the sum of an array of doubles. Barely 1% of applicants can even get the basics correctly such as having multiple partial sums.<p>Once you do the work such that your code actually allows execution to happen in parallel, then leveraging superscalar and simd happens automatically.</div><br/></div></div><div id="36158341" class="c"><input type="checkbox" id="c-36158341" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36158286">parent</a><span>|</span><a href="#36161954">prev</a><span>|</span><a href="#36162149">next</a><span>|</span><label class="collapse" for="c-36158341">[-]</label><label class="expand" for="c-36158341">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;d say auto-vectorization should still be learned by modern high-performance programmers, because its very low-hanging fruit.<p>Somewhat related: 55 GiB&#x2F;s FizzBuzz <a href="https:&#x2F;&#x2F;codegolf.stackexchange.com&#x2F;questions&#x2F;215216&#x2F;high-throughput-fizz-buzz&#x2F;236630#236630" rel="nofollow">https:&#x2F;&#x2F;codegolf.stackexchange.com&#x2F;questions&#x2F;215216&#x2F;high-thr...</a> (discussion: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29031488" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29031488</a>)</div><br/><div id="36158404" class="c"><input type="checkbox" id="c-36158404" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36158341">parent</a><span>|</span><a href="#36162149">next</a><span>|</span><label class="collapse" for="c-36158404">[-]</label><label class="expand" for="c-36158404">[1 more]</label></div><br/><div class="children"><div class="content">Definitely not autovectorized.</div><br/></div></div></div></div><div id="36162149" class="c"><input type="checkbox" id="c-36162149" checked=""/><div class="controls bullet"><span class="by">Genghis_Khan</span><span>|</span><a href="#36158286">parent</a><span>|</span><a href="#36158341">prev</a><span>|</span><a href="#36160981">next</a><span>|</span><label class="collapse" for="c-36162149">[-]</label><label class="expand" for="c-36162149">[1 more]</label></div><br/><div class="children"><div class="content">&gt; auto-vectorizers are just not as flexible as the proper SIMD-tools<p>What is a &quot;proper&quot; SIMD tool?</div><br/></div></div><div id="36160981" class="c"><input type="checkbox" id="c-36160981" checked=""/><div class="controls bullet"><span class="by">iamcreasy</span><span>|</span><a href="#36158286">parent</a><span>|</span><a href="#36162149">prev</a><span>|</span><a href="#36158420">next</a><span>|</span><label class="collapse" for="c-36160981">[-]</label><label class="expand" for="c-36160981">[8 more]</label></div><br/><div class="children"><div class="content">What would happen if I try to run a program contains AVX instruction on a system that does not have it? Do I need to recompile the program?</div><br/><div id="36161153" class="c"><input type="checkbox" id="c-36161153" checked=""/><div class="controls bullet"><span class="by">kookamamie</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36160981">parent</a><span>|</span><a href="#36161077">next</a><span>|</span><label class="collapse" for="c-36161153">[-]</label><label class="expand" for="c-36161153">[1 more]</label></div><br/><div class="children"><div class="content">E.g. ISPC can compile for multiple targets at once. The runtime system selects the best compatible implementation for the machine running the code.</div><br/></div></div><div id="36161077" class="c"><input type="checkbox" id="c-36161077" checked=""/><div class="controls bullet"><span class="by">stevefan1999</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36160981">parent</a><span>|</span><a href="#36161153">prev</a><span>|</span><a href="#36158420">next</a><span>|</span><label class="collapse" for="c-36161077">[-]</label><label class="expand" for="c-36161077">[6 more]</label></div><br/><div class="children"><div class="content">segfault with illegal opcode</div><br/><div id="36161731" class="c"><input type="checkbox" id="c-36161731" checked=""/><div class="controls bullet"><span class="by">iamcreasy</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36161077">parent</a><span>|</span><a href="#36158420">next</a><span>|</span><label class="collapse" for="c-36161731">[-]</label><label class="expand" for="c-36161731">[5 more]</label></div><br/><div class="children"><div class="content">So when I am downloading binary - i.e. Krita&#x2F;GIMP or any open source project - how do the dev ensure that the common binary will utilize these newer instructions without hitting &#x27;illegal opcode&#x27; error?</div><br/><div id="36161941" class="c"><input type="checkbox" id="c-36161941" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36161731">parent</a><span>|</span><a href="#36158420">next</a><span>|</span><label class="collapse" for="c-36161941">[-]</label><label class="expand" for="c-36161941">[4 more]</label></div><br/><div class="children"><div class="content">Depends on the binary. Compiling to the lowest common denominator is one strategy. The other is to to have the code detect what&#x27;s available and only use that subset.</div><br/><div id="36162241" class="c"><input type="checkbox" id="c-36162241" checked=""/><div class="controls bullet"><span class="by">iamcreasy</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36161941">parent</a><span>|</span><a href="#36158420">next</a><span>|</span><label class="collapse" for="c-36162241">[-]</label><label class="expand" for="c-36162241">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. How do the devs decide on the lowest common denominator? Do you happen to know any blog or resources that explains the process?</div><br/><div id="36162584" class="c"><input type="checkbox" id="c-36162584" checked=""/><div class="controls bullet"><span class="by">stevefan1999</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36162241">parent</a><span>|</span><a href="#36162307">next</a><span>|</span><label class="collapse" for="c-36162584">[-]</label><label class="expand" for="c-36162584">[1 more]</label></div><br/><div class="children"><div class="content">Most devs don&#x27;t care, so it depends on the compiler default. However some people who are eagering for optimization will turn on -march=native to get the best profile out of the current running processor family. This is where the troubles come in</div><br/></div></div><div id="36162307" class="c"><input type="checkbox" id="c-36162307" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36162241">parent</a><span>|</span><a href="#36162584">prev</a><span>|</span><a href="#36158420">next</a><span>|</span><label class="collapse" for="c-36162307">[-]</label><label class="expand" for="c-36162307">[1 more]</label></div><br/><div class="children"><div class="content">I touched that question in &quot;Hardware Support&quot; section, on page 6 of that article <a href="http:&#x2F;&#x2F;const.me&#x2F;articles&#x2F;simd&#x2F;simd.pdf" rel="nofollow">http:&#x2F;&#x2F;const.me&#x2F;articles&#x2F;simd&#x2F;simd.pdf</a><p>TLDR: SSE1 and SSE2 are parts of the base AMD64 ISA, they are always available while compiling for 64 bits. SSE3, SSE4.1 are old, market penetration is very close to 100%, always available in practice. For the newer extensions like AVX and FMA, the optimal tradeoff depends on the product and the market.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36158420" class="c"><input type="checkbox" id="c-36158420" checked=""/><div class="controls bullet"><span class="by">jackmott42</span><span>|</span><a href="#36158286">parent</a><span>|</span><a href="#36160981">prev</a><span>|</span><a href="#36159633">next</a><span>|</span><label class="collapse" for="c-36158420">[-]</label><label class="expand" for="c-36158420">[8 more]</label></div><br/><div class="children"><div class="content">What do you mean by learning auto vectorization? Do you mean learning how to structure your code so that your compiler of choice has a good chance of pulling it off?</div><br/><div id="36162209" class="c"><input type="checkbox" id="c-36162209" checked=""/><div class="controls bullet"><span class="by">Matumio</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36158420">parent</a><span>|</span><a href="#36158540">next</a><span>|</span><label class="collapse" for="c-36162209">[-]</label><label class="expand" for="c-36162209">[1 more]</label></div><br/><div class="children"><div class="content">Compilers are pretty good at restructuring code. As a programmer you need know some common situations when the compiler would like to reorder your code, but can&#x27;t.<p>E.g. when you do summation using a for loop, and the compiler is trying to preserve the rounding-correctness your float operations. Or because you created a dependency graph with a for loop where each iteration requires the previous iteration&#x27;s result.</div><br/></div></div><div id="36158540" class="c"><input type="checkbox" id="c-36158540" checked=""/><div class="controls bullet"><span class="by">Veliladon</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36158420">parent</a><span>|</span><a href="#36162209">prev</a><span>|</span><a href="#36158623">next</a><span>|</span><label class="collapse" for="c-36158540">[-]</label><label class="expand" for="c-36158540">[5 more]</label></div><br/><div class="children"><div class="content">Pretty much.<p>If one wants to sum two arrays instead of for looping through the elements of two arrays one might instead use iterate by chunks so that the compiler can easily tie them all together as a single operation which can then easily vectorize.</div><br/><div id="36158695" class="c"><input type="checkbox" id="c-36158695" checked=""/><div class="controls bullet"><span class="by">pasmafaute</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36158540">parent</a><span>|</span><a href="#36158623">next</a><span>|</span><label class="collapse" for="c-36158695">[-]</label><label class="expand" for="c-36158695">[4 more]</label></div><br/><div class="children"><div class="content">If I recall, you can absolutely loop through the elements in a tight loop and the compiler (e.g. GCC) will auto-vectorize for you (if you have the relevant optimization flag set).<p>The trick with coding for auto-vectorization is to keep your loops small and free of clutter.<p>I don&#x27;t have the documentation handy but I think you only need to follow a couple rules:<p>- loop must have a defined size (for-loop instead of while-loop)<p>- don&#x27;t muck with pointers inside the loop (simple pointer increment is okay)<p>- don&#x27;t modify other variables (only the array should be modified)</div><br/><div id="36158819" class="c"><input type="checkbox" id="c-36158819" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36158695">parent</a><span>|</span><a href="#36159742">next</a><span>|</span><label class="collapse" for="c-36158819">[-]</label><label class="expand" for="c-36158819">[1 more]</label></div><br/><div class="children"><div class="content">The linked document describes Intel&#x27;s autovectorizer, it&#x27;s warnings and compiler flags that point out which loops autovectorized or not, as well as listing specific reason codes why.<p>Microsoft, GCC and Clang all do this too, though with different compiler flags and messages.<p>I&#x27;d say that the whole point of this document listed here is to build up the programmer to understanding these error messages and specifically know how to fix the errors that causes a autovectorization-fail.</div><br/></div></div><div id="36159742" class="c"><input type="checkbox" id="c-36159742" checked=""/><div class="controls bullet"><span class="by">galangalalgol</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36158695">parent</a><span>|</span><a href="#36158819">prev</a><span>|</span><a href="#36158623">next</a><span>|</span><label class="collapse" for="c-36159742">[-]</label><label class="expand" for="c-36159742">[2 more]</label></div><br/><div class="children"><div class="content">In rust I&#x27;ve noticed using iterators instead of for loops tends to trigger auto vectorization more often.</div><br/><div id="36160714" class="c"><input type="checkbox" id="c-36160714" checked=""/><div class="controls bullet"><span class="by">Veliladon</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36159742">parent</a><span>|</span><a href="#36158623">next</a><span>|</span><label class="collapse" for="c-36160714">[-]</label><label class="expand" for="c-36160714">[1 more]</label></div><br/><div class="children"><div class="content">In Rust using iterators is almost always better than for loops. The compiler is virtually guaranteed to optimize out bounds checks vs indexing in.</div><br/></div></div></div></div></div></div></div></div><div id="36158623" class="c"><input type="checkbox" id="c-36158623" checked=""/><div class="controls bullet"><span class="by">killingtime74</span><span>|</span><a href="#36158286">root</a><span>|</span><a href="#36158420">parent</a><span>|</span><a href="#36158540">prev</a><span>|</span><a href="#36159633">next</a><span>|</span><label class="collapse" for="c-36158623">[-]</label><label class="expand" for="c-36158623">[1 more]</label></div><br/><div class="children"><div class="content">Here are some examples, or you could use this library itself in rust <a href="https:&#x2F;&#x2F;github.com&#x2F;vorner&#x2F;slipstream">https:&#x2F;&#x2F;github.com&#x2F;vorner&#x2F;slipstream</a></div><br/></div></div></div></div></div></div><div id="36159633" class="c"><input type="checkbox" id="c-36159633" checked=""/><div class="controls bullet"><span class="by">sundarurfriend</span><span>|</span><a href="#36158286">prev</a><span>|</span><a href="#36161085">next</a><span>|</span><label class="collapse" for="c-36159633">[-]</label><label class="expand" for="c-36159633">[3 more]</label></div><br/><div class="children"><div class="content">I wonder how much these apply to the Julia compiler: based on the docs for `@simd` and the writing here [1], only the first two criteria on this page [2] seem to be really a requirement in Julia&#x27;s case.<p>The one about avoiding function calls is a no-go, since things like + and * are function calls too. But is it still required for all function calls to be inline-able?<p>Indirect indexing isn&#x27;t mentioned in Julia&#x27;s case, but is it one of the things that would disable auto-vectorization and require an explicit @simd annotation?<p>[1] <a href="https:&#x2F;&#x2F;viralinstruction.com&#x2F;posts&#x2F;hardware&#x2F;#74a3ddb4-8af1-11eb-186e-4d80402adfcf" rel="nofollow">https:&#x2F;&#x2F;viralinstruction.com&#x2F;posts&#x2F;hardware&#x2F;#74a3ddb4-8af1-1...</a>
[2] <a href="https:&#x2F;&#x2F;cvw.cac.cornell.edu&#x2F;vector&#x2F;coding_vectorizable" rel="nofollow">https:&#x2F;&#x2F;cvw.cac.cornell.edu&#x2F;vector&#x2F;coding_vectorizable</a></div><br/><div id="36162657" class="c"><input type="checkbox" id="c-36162657" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#36159633">parent</a><span>|</span><a href="#36160358">next</a><span>|</span><label class="collapse" for="c-36162657">[-]</label><label class="expand" for="c-36162657">[1 more]</label></div><br/><div class="children"><div class="content">In the case of the Julia compiler, auto-vectorization is indeed influenced by the criteria mentioned in the sources you provided. However, the Julia compiler has its own set of rules and optimizations that may differ from other compilers. For instance, according to MirrorThink.ai, the Julia compiler is capable of inlining small functions, including arithmetic operations like + and *.<p>This inlining can help improve the chances of auto-vectorization. However, it is not a strict requirement for all function calls to be inline-able. The compiler will make decisions based on the specific code and the potential performance gains.<p>As for indirect indexing, it can indeed impact auto-vectorization in Julia. Indirect indexing can introduce data dependencies that make it difficult for the compiler to determine if vectorization is safe or beneficial. In such cases, using the @simd annotation can provide a hint to the compiler that vectorization is desired, but it is still up to the compiler to determine if it is possible and beneficial.</div><br/></div></div><div id="36160358" class="c"><input type="checkbox" id="c-36160358" checked=""/><div class="controls bullet"><span class="by">celrod</span><span>|</span><a href="#36159633">parent</a><span>|</span><a href="#36162657">prev</a><span>|</span><a href="#36161085">next</a><span>|</span><label class="collapse" for="c-36160358">[-]</label><label class="expand" for="c-36160358">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it is required for all functions to be inlined in Julia. Note that Julia 1.8 and newer supports callsite inlining, so you can, for example, do `@inline exp(x[i])` inside your `for` loop, and this will cause `exp` to get inlined there, and should enable vectorization.
For functions like `log` or `sin`, this is not enough, but you could try `@turbo` from LoopVectorization.jl, which supports these and some others.<p>`@simd ivdep` may be needed when you have indirect indexing.
Do not forget the `ivdep`, but only if you know it is actually safe. This can lead to hard to track down bugs if it is not.</div><br/></div></div></div></div><div id="36161085" class="c"><input type="checkbox" id="c-36161085" checked=""/><div class="controls bullet"><span class="by">dev_tty01</span><span>|</span><a href="#36159633">prev</a><span>|</span><a href="#36157503">next</a><span>|</span><label class="collapse" for="c-36161085">[-]</label><label class="expand" for="c-36161085">[3 more]</label></div><br/><div class="children"><div class="content">When I was in grad school programming on a Cray, &quot;vectorization&quot; was about chaining operations across an array and getting the inner-loop dependencies right, rather than parallel operations across elements of wide words.  Interesting how the definitions have changed with architectural changes.</div><br/><div id="36161855" class="c"><input type="checkbox" id="c-36161855" checked=""/><div class="controls bullet"><span class="by">agalunar</span><span>|</span><a href="#36161085">parent</a><span>|</span><a href="#36161119">next</a><span>|</span><label class="collapse" for="c-36161855">[-]</label><label class="expand" for="c-36161855">[1 more]</label></div><br/><div class="children"><div class="content">From what I recall, in vector machines, operations are still done in parallel due to chaining; the difference is that you&#x27;re doing e.g. add(v[1]) and mul(v[0]) in parallel, then add(v[2]) and mul(v[1]), and so on, rather than add(v[0]) and add(v[1]) in parallel, then mul(v[0]) and mul(v[1]), &amp;c, which makes support for any vector length trivial.<p>(I&#x27;m sure you know this; this is just for the benefit of anyone else perusing the comment section.)</div><br/></div></div><div id="36161119" class="c"><input type="checkbox" id="c-36161119" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#36161085">parent</a><span>|</span><a href="#36161855">prev</a><span>|</span><a href="#36157503">next</a><span>|</span><label class="collapse" for="c-36161119">[-]</label><label class="expand" for="c-36161119">[1 more]</label></div><br/><div class="children"><div class="content">This presentation seems very “let’s get ya going on the Xeon’s we’ve got in Stampede today” focused. Which, to be fair, is probably what lots of researchers who are interested in using (rather than studying) HPC need to know.<p>RISC-V has, or at least had, some classic vector extension proposal. Not sure if it actually got implemented.</div><br/></div></div></div></div><div id="36157503" class="c"><input type="checkbox" id="c-36157503" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#36161085">prev</a><span>|</span><a href="#36158436">next</a><span>|</span><label class="collapse" for="c-36157503">[-]</label><label class="expand" for="c-36157503">[24 more]</label></div><br/><div class="children"><div class="content">We need also something like Tensorization:Introduction</div><br/><div id="36158050" class="c"><input type="checkbox" id="c-36158050" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#36157503">parent</a><span>|</span><a href="#36159038">next</a><span>|</span><label class="collapse" for="c-36158050">[-]</label><label class="expand" for="c-36158050">[10 more]</label></div><br/><div class="children"><div class="content">We really need to name tensors differently. &quot;Array&quot; would be a more fitting name.<p>Tensors are really different mathematical objects with a far more rich structure than those used in the Deep Learning context.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Tensor" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Tensor</a></div><br/><div id="36158105" class="c"><input type="checkbox" id="c-36158105" checked=""/><div class="controls bullet"><span class="by">itishappy</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158050">parent</a><span>|</span><a href="#36158297">next</a><span>|</span><label class="collapse" for="c-36158105">[-]</label><label class="expand" for="c-36158105">[3 more]</label></div><br/><div class="children"><div class="content">Same with vectors!<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector_(mathematics_and_physics)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector_(mathematics_and_physic...</a></div><br/><div id="36161244" class="c"><input type="checkbox" id="c-36161244" checked=""/><div class="controls bullet"><span class="by">antognini</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158105">parent</a><span>|</span><a href="#36158297">next</a><span>|</span><label class="collapse" for="c-36161244">[-]</label><label class="expand" for="c-36161244">[2 more]</label></div><br/><div class="children"><div class="content">As a transplant to the ML world who migrated from physics I do find it funny that people in ML will insist that gradients are vectors.  Once you get to general relativity your physics professors will beat into you the fact that gradients are <i>not</i> vectors.  (They are one-forms.)  But in the ML context it&#x27;s a distinction without a difference, unless you&#x27;re in a really specialized corner of the field like information geometry.</div><br/><div id="36162481" class="c"><input type="checkbox" id="c-36162481" checked=""/><div class="controls bullet"><span class="by">ReleaseCandidat</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36161244">parent</a><span>|</span><a href="#36158297">next</a><span>|</span><label class="collapse" for="c-36162481">[-]</label><label class="expand" for="c-36162481">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Once you get to general relativity your physics professors will beat into you the fact that gradients are not vectors.<p>In linear algebra you should learn this as soon as you hear about it.
Is the english Wikipedia page correct in that you always use nabla for the gradient?
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gradient#Generalizations" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gradient#Generalizations</a><p>Because `grad f = ∇f` holds in scalar fields f only (not in vector or tensor fields (tensors that aren&#x27;t scalars)).</div><br/></div></div></div></div></div></div><div id="36158297" class="c"><input type="checkbox" id="c-36158297" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158050">parent</a><span>|</span><a href="#36158105">prev</a><span>|</span><a href="#36158170">next</a><span>|</span><label class="collapse" for="c-36158297">[-]</label><label class="expand" for="c-36158297">[2 more]</label></div><br/><div class="children"><div class="content">Names can have different meanings based on context, even within pure mathematics. I think  a mathematician would have little trouble discerning which kind of tensor is meant, as this sort of reusing existing terms is pretty common.<p>Just calling it an array might be underselling it, at this point. Perhaps a tensor in the context of CompSci is just a particular type of array with certain expected properties.</div><br/><div id="36159756" class="c"><input type="checkbox" id="c-36159756" checked=""/><div class="controls bullet"><span class="by">rsfern</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158297">parent</a><span>|</span><a href="#36158170">next</a><span>|</span><label class="collapse" for="c-36159756">[-]</label><label class="expand" for="c-36159756">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Just calling it an array might be underselling it, at this point<p>I like the NDArray terminology  used by numpy. I think it gets the point across more clearly than “tensor”, and conceptually they’re pretty much equivalent</div><br/></div></div></div></div><div id="36158170" class="c"><input type="checkbox" id="c-36158170" checked=""/><div class="controls bullet"><span class="by">opportune</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158050">parent</a><span>|</span><a href="#36158297">prev</a><span>|</span><a href="#36158913">next</a><span>|</span><label class="collapse" for="c-36158170">[-]</label><label class="expand" for="c-36158170">[2 more]</label></div><br/><div class="children"><div class="content">You could apply that argument to “isomorphic” functions, c++ “vectors” or compute “vectorizarion”, “integer” types… even though the semantics aren’t exactly the same between the computing and math terms, they’re related enough that they do help with understanding to those coming from a math background (which to be fair is probably less and less programmers over time).</div><br/><div id="36158313" class="c"><input type="checkbox" id="c-36158313" checked=""/><div class="controls bullet"><span class="by">myownpetard</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158170">parent</a><span>|</span><a href="#36158913">next</a><span>|</span><label class="collapse" for="c-36158313">[-]</label><label class="expand" for="c-36158313">[1 more]</label></div><br/><div class="children"><div class="content">Isomorphic javascript is one of the most egregious.</div><br/></div></div></div></div><div id="36158913" class="c"><input type="checkbox" id="c-36158913" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158050">parent</a><span>|</span><a href="#36158170">prev</a><span>|</span><a href="#36158214">next</a><span>|</span><label class="collapse" for="c-36158913">[-]</label><label class="expand" for="c-36158913">[1 more]</label></div><br/><div class="children"><div class="content">Tensor processing units sounds far more cutting edge than number array processing units.</div><br/></div></div><div id="36158214" class="c"><input type="checkbox" id="c-36158214" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158050">parent</a><span>|</span><a href="#36158913">prev</a><span>|</span><a href="#36159038">next</a><span>|</span><label class="collapse" for="c-36158214">[-]</label><label class="expand" for="c-36158214">[1 more]</label></div><br/><div class="children"><div class="content">I think that naming battle is lost.<p>Would indeed be cool to have <i>true</i> tensor processing units :-)<p>PKD level of cool as you could argue those chips directly process spacetime chunks</div><br/></div></div></div></div><div id="36159038" class="c"><input type="checkbox" id="c-36159038" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36157503">parent</a><span>|</span><a href="#36158050">prev</a><span>|</span><a href="#36157532">next</a><span>|</span><label class="collapse" for="c-36159038">[-]</label><label class="expand" for="c-36159038">[4 more]</label></div><br/><div class="children"><div class="content">PyTorch internals: <a href="http:&#x2F;&#x2F;blog.ezyang.com&#x2F;2019&#x2F;05&#x2F;pytorch-internals&#x2F;" rel="nofollow">http:&#x2F;&#x2F;blog.ezyang.com&#x2F;2019&#x2F;05&#x2F;pytorch-internals&#x2F;</a><p>A good basic thing to know is a tensor is a single dimensional array under the hood. At least as far as I know in my mental model - I have not yet looked at the code :-)<p>Therefore &quot;converting&quot; a 2 * 5 * 10 tensor into a 2 * 50 one is an immediate operation, it doesn&#x27;t have to scan. Just change some metadata.</div><br/><div id="36162109" class="c"><input type="checkbox" id="c-36162109" checked=""/><div class="controls bullet"><span class="by">jabl</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36159038">parent</a><span>|</span><a href="#36157532">next</a><span>|</span><label class="collapse" for="c-36162109">[-]</label><label class="expand" for="c-36162109">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s pretty much how every serious dense matrix (or more generally, multi-dimensional array) representation has been done since, well, forever. It&#x27;s only C programmers writing &quot;My First Matrix Program&quot; that insist on allocating each row separately due to the lack of native multidimensional arrays in that language.</div><br/><div id="36162581" class="c"><input type="checkbox" id="c-36162581" checked=""/><div class="controls bullet"><span class="by">ReleaseCandidat</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36162109">parent</a><span>|</span><a href="#36162181">next</a><span>|</span><label class="collapse" for="c-36162581">[-]</label><label class="expand" for="c-36162581">[1 more]</label></div><br/><div class="children"><div class="content">Fortran did it:<p><pre><code>    A 2-dimensional array A will, in the object program, be stored sequentially in
    the order Ai,i, A2,i Am l , A| i2 , A2f2 Am, 2&gt; , Am,„. Thus
    it is stored “columnwise”, with the first of its subscripts varying most 
    rapidly, and the last varying least rapidly. The same is true of 3-dimensional arrays.
   1 -dimensional arrays are of course simply stored sequentially. All arrays are stored backwards in storage; i.e. the above sequence is in the order of decreasing absolute location.
</code></pre>
<a href="https:&#x2F;&#x2F;ia800807.us.archive.org&#x2F;0&#x2F;items&#x2F;history-of-fortran&#x2F;Image072217151026_text.pdf" rel="nofollow">https:&#x2F;&#x2F;ia800807.us.archive.org&#x2F;0&#x2F;items&#x2F;history-of-fortran&#x2F;I...</a></div><br/></div></div><div id="36162181" class="c"><input type="checkbox" id="c-36162181" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36162109">parent</a><span>|</span><a href="#36162581">prev</a><span>|</span><a href="#36157532">next</a><span>|</span><label class="collapse" for="c-36162181">[-]</label><label class="expand" for="c-36162181">[1 more]</label></div><br/><div class="children"><div class="content">Yes! I was surprised how simple it was. Obvious once you know how. I never really considered multidimensional storage until looking into Machine Learning recently. My only brush with it is relational database tuning, but that is very different, as the number of &quot;dimensions&quot; are small and they are usually not homogenous.</div><br/></div></div></div></div></div></div><div id="36157532" class="c"><input type="checkbox" id="c-36157532" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#36157503">parent</a><span>|</span><a href="#36159038">prev</a><span>|</span><a href="#36157945">next</a><span>|</span><label class="collapse" for="c-36157532">[-]</label><label class="expand" for="c-36157532">[5 more]</label></div><br/><div class="children"><div class="content">Tensor ops are usually implemented using the principles of vectorizing code.<p>After all most tensors are implemented as 1D arrays with strides for each dimension with the innermost dimension always contiguous.</div><br/><div id="36157697" class="c"><input type="checkbox" id="c-36157697" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36157532">parent</a><span>|</span><a href="#36157951">next</a><span>|</span><label class="collapse" for="c-36157697">[-]</label><label class="expand" for="c-36157697">[2 more]</label></div><br/><div class="children"><div class="content">I think the bigger difficulty lies in how to store and process this stuff on a GPU&#x2F;other accelerator.<p>If the OP cares about implementation details about how an API like PyTorch is made, I think the MiniTorch &#x27;book&#x27; is a pretty good intro:<p><a href="https:&#x2F;&#x2F;minitorch.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;minitorch.github.io&#x2F;</a></div><br/><div id="36158862" class="c"><input type="checkbox" id="c-36158862" checked=""/><div class="controls bullet"><span class="by">lying4fun</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36157697">parent</a><span>|</span><a href="#36157951">next</a><span>|</span><label class="collapse" for="c-36158862">[-]</label><label class="expand" for="c-36158862">[1 more]</label></div><br/><div class="children"><div class="content">The MiniTorch book seems amazing, thanks</div><br/></div></div></div></div><div id="36157951" class="c"><input type="checkbox" id="c-36157951" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36157532">parent</a><span>|</span><a href="#36157697">prev</a><span>|</span><a href="#36157599">next</a><span>|</span><label class="collapse" for="c-36157951">[-]</label><label class="expand" for="c-36157951">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the principles of vectorizing code.
&gt; 1D arrays<p>I think accelerators mostly do computations over small matrices and not vectors, so it is &quot;matricized&quot; code, but I am not an expert in this area.</div><br/></div></div><div id="36157599" class="c"><input type="checkbox" id="c-36157599" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36157532">parent</a><span>|</span><a href="#36157951">prev</a><span>|</span><a href="#36157945">next</a><span>|</span><label class="collapse" for="c-36157599">[-]</label><label class="expand" for="c-36157599">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT interpretation:<p>Sure! Let me break it down for you:<p>When we talk about &quot;tensor ops,&quot; we&#x27;re referring to operations performed on tensors, which are multidimensional arrays of numbers commonly used in mathematics and computer science.<p>Now, these tensor operations are typically implemented using a technique called &quot;vectorizing code.&quot; In simple terms, vectorizing code means performing operations on entire arrays of data instead of looping through each element one by one. It&#x27;s like doing multiple calculations at once, which can be more efficient and faster.<p>Tensors are usually represented as 1D arrays, meaning all the elements are arranged in a single line. Each dimension of the tensor has a concept called &quot;stride,&quot; which represents how many elements we need to skip to move to the next element in that dimension. This helps us efficiently access and manipulate the data in the tensor.<p>Additionally, the innermost dimension of a tensor is always &quot;contiguous,&quot; which means the elements are stored sequentially without any gaps. This arrangement also aids in efficient processing of the tensor data.<p>So, in summary, tensor ops involve performing operations on multidimensional arrays, and we use vectorized code to do these operations more efficiently. Tensors are represented as 1D arrays with strides for each dimension, and the innermost dimension is always stored sequentially without any gaps.</div><br/></div></div></div></div><div id="36157945" class="c"><input type="checkbox" id="c-36157945" checked=""/><div class="controls bullet"><span class="by">corsix</span><span>|</span><a href="#36157503">parent</a><span>|</span><a href="#36157532">prev</a><span>|</span><a href="#36158339">next</a><span>|</span><label class="collapse" for="c-36157945">[-]</label><label class="expand" for="c-36157945">[3 more]</label></div><br/><div class="children"><div class="content">From a hardware perspective, vector instructions operate on small 1D vectors, whereas tensor instructions operate on small 2D matrices. I say “instructions”, but it’s really only matrix multiply or matrix multiply and accumulate - most other instructions are fine staying as 1D.</div><br/><div id="36158068" class="c"><input type="checkbox" id="c-36158068" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36157945">parent</a><span>|</span><a href="#36158339">next</a><span>|</span><label class="collapse" for="c-36158068">[-]</label><label class="expand" for="c-36158068">[2 more]</label></div><br/><div class="children"><div class="content">If there is matrix multiply at hardware level its fair to have another name than vectorization. For example the dimensions and partitioning of large matrices to fit would be specific to that design and very different from rolling things out on 1D arrays</div><br/><div id="36161140" class="c"><input type="checkbox" id="c-36161140" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#36157503">root</a><span>|</span><a href="#36158068">parent</a><span>|</span><a href="#36158339">next</a><span>|</span><label class="collapse" for="c-36161140">[-]</label><label class="expand" for="c-36161140">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they should call it a Matrix Multiplication Xcelerator just to mess with people.</div><br/></div></div></div></div></div></div><div id="36158339" class="c"><input type="checkbox" id="c-36158339" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#36157503">parent</a><span>|</span><a href="#36157945">prev</a><span>|</span><a href="#36158436">next</a><span>|</span><label class="collapse" for="c-36158339">[-]</label><label class="expand" for="c-36158339">[1 more]</label></div><br/><div class="children"><div class="content">This is mostly going to be the same. The main difference in tensorization is that your vectors are not contiguous. So you lose some speedup. How you typically optimize this is by flattening your tensor. Then your data is contiguious and all the same rules apply.<p>This is overly simplified though. Things get different when we start talking about {S,M}I{S,M}D (page addresses SIMD) or GPUs. Parallel computing is a whole other game, and CUDA takes it to the next level (lots of people can write CUDA kernels, not a lot of people can write GOOD CUDA kernels (I&#x27;m definitely not a pro, but know some)). Parallelism adds another level of complexity due to being able to access different memory locations simultaneously. If you think concurrent optimization is magic, parallel optimization is wizardry (I still believe this is true)</div><br/></div></div></div></div><div id="36158411" class="c"><input type="checkbox" id="c-36158411" checked=""/><div class="controls bullet"><span class="by">photochemsyn</span><span>|</span><a href="#36159008">prev</a><span>|</span><a href="#36159699">next</a><span>|</span><label class="collapse" for="c-36158411">[-]</label><label class="expand" for="c-36158411">[3 more]</label></div><br/><div class="children"><div class="content">This looks like a curious case, non-fixed-length vectorization code in RISC-V:<p>&gt; &quot;Perhaps the most interesting part of the open RISC-V instruction set architecture (ISA) is the vector extension (RISC-V &quot;V&quot;). In contrast to the average single-instruction multipe-data (SIMD) instruction set, RISC-V vector instructions are vector length agnostic (VLA). Thus, a RISC-V &quot;V&quot; CPU is flexible in choosing a vector register size while RISC-V &quot;V&quot; binary code is portable between different CPU implementations.&quot;<p><a href="https:&#x2F;&#x2F;gms.tf&#x2F;riscv-vector.html" rel="nofollow">https:&#x2F;&#x2F;gms.tf&#x2F;riscv-vector.html</a></div><br/><div id="36161214" class="c"><input type="checkbox" id="c-36161214" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#36158411">parent</a><span>|</span><a href="#36159699">next</a><span>|</span><label class="collapse" for="c-36161214">[-]</label><label class="expand" for="c-36161214">[2 more]</label></div><br/><div class="children"><div class="content">Classical vector machines (think Cray) had flexible vector sizes. But eventually they were buried under a tide of consumer processors, which were fundamentally scalar. Then as these machines took over the world, they had to go into every niche, so the MMX style “fixed length vector instruction” was invented, for CPUs that might have to do some math while still fitting into that paradigm.<p>RISC-V also has the P extension, which follows this pattern. It will be very cool if RISC-V proves flexible enough to really enable a resurgence of classic vectors though.</div><br/><div id="36162201" class="c"><input type="checkbox" id="c-36162201" checked=""/><div class="controls bullet"><span class="by">jabl</span><span>|</span><a href="#36158411">root</a><span>|</span><a href="#36161214">parent</a><span>|</span><a href="#36159699">next</a><span>|</span><label class="collapse" for="c-36162201">[-]</label><label class="expand" for="c-36162201">[1 more]</label></div><br/><div class="children"><div class="content">No, classic Cray did have a fixed vector size (4096 bits, or 64 double precision floating point values) both in the HW and the ISA. What it did have, however, which most contemporary SIMD ISA&#x27;s lack, was a vector length register which allowed masking the end of the vector register. That meant the compiler didn&#x27;t have to generate a scalar loop tail to handle the final iterations of a vectorized loop, it just set the vector length register to mask of the end of the vector data register, which meant that load&#x2F;store&#x2F;arithmetic instructions only worked on the first N elements in the vector rather than the full 64.<p>I think the RISC-V V extension, as well as ARM SVE, both have something similar as well. What RVV and SVE have, that Cray vector ISA didn&#x27;t, is the ability to support different HW vector register lengths with the same ISA.</div><br/></div></div></div></div></div></div><div id="36159699" class="c"><input type="checkbox" id="c-36159699" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#36158411">prev</a><span>|</span><a href="#36157741">next</a><span>|</span><label class="collapse" for="c-36159699">[-]</label><label class="expand" for="c-36159699">[1 more]</label></div><br/><div class="children"><div class="content">Sorry if this is a different sensd of that same word…<p>I can understand almost everything in the explanations of how ChatGPT, neural networks and fine-tuning works<p>But one thing isn’t being explained…<p>How do you embed the words as vectors????<p>Do you just make up your own scheme?<p>I understand about vector databases being used for search, to retrieve snippets and stuff them into a prompt. I don’t mean that.<p>I mean how did people embed the words WHEN THEY USE THE EMBEDDINGS API?<p>I think this whole vector database and pinecone stuff will be phased out once the LLM windows get larger, like with the new Claude from Anthropic. And the ability to fine tune the model on your data. Then the LLM can make its own lower-rank model of your entire corpus of data, rather than having to do vector lookups and stuffing it into a prompt.</div><br/></div></div><div id="36157741" class="c"><input type="checkbox" id="c-36157741" checked=""/><div class="controls bullet"><span class="by">NKosmatos</span><span>|</span><a href="#36159699">prev</a><span>|</span><label class="collapse" for="c-36157741">[-]</label><label class="expand" for="c-36157741">[9 more]</label></div><br/><div class="children"><div class="content">I know I’m probably going to get downvoted for this, but I’m going to post it anyhow :-)<p><a href="https:&#x2F;&#x2F;jvns.ca&#x2F;blog&#x2F;2023&#x2F;02&#x2F;08&#x2F;why-does-0-1-plus-0-2-equal-0-30000000000000004&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jvns.ca&#x2F;blog&#x2F;2023&#x2F;02&#x2F;08&#x2F;why-does-0-1-plus-0-2-equal-...</a></div><br/><div id="36157908" class="c"><input type="checkbox" id="c-36157908" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36157741">parent</a><span>|</span><label class="collapse" for="c-36157908">[-]</label><label class="expand" for="c-36157908">[8 more]</label></div><br/><div class="children"><div class="content">Perhaps you could point out in which way you think this is relevant?</div><br/><div id="36158016" class="c"><input type="checkbox" id="c-36158016" checked=""/><div class="controls bullet"><span class="by">NKosmatos</span><span>|</span><a href="#36157741">root</a><span>|</span><a href="#36157908">parent</a><span>|</span><label class="collapse" for="c-36158016">[-]</label><label class="expand" for="c-36158016">[7 more]</label></div><br/><div class="children"><div class="content">From the introductory text “The ultimate goal of vectorization is an increase in floating-point performance…”.
I find it a bit funny that by vectorizing float executions we’ll be able to calculate wrong answers faster ;-)</div><br/><div id="36158385" class="c"><input type="checkbox" id="c-36158385" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36157741">root</a><span>|</span><a href="#36158016">parent</a><span>|</span><a href="#36158419">next</a><span>|</span><label class="collapse" for="c-36158385">[-]</label><label class="expand" for="c-36158385">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a wrong answer, anymore than writing down ⅓ as 0.333, multiplying that by 3, and discovering that the result is 0.999 and not 1.<p>And there&#x27;s nothing in vectorization that requires floating-point numbers; it&#x27;s just that the algorithms that tend to demand the most out of a computer tend to be large numerical algorithms, which use floating-point.</div><br/><div id="36158415" class="c"><input type="checkbox" id="c-36158415" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36157741">root</a><span>|</span><a href="#36158385">parent</a><span>|</span><a href="#36158419">next</a><span>|</span><label class="collapse" for="c-36158415">[-]</label><label class="expand" for="c-36158415">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the result is 0.999 and not 1.<p>But 0.999... is 1.<p>Edit: Nevermind, I see you meant &quot;writing down 1&#x2F;3 as 0.333&quot; literally.</div><br/></div></div></div></div><div id="36158419" class="c"><input type="checkbox" id="c-36158419" checked=""/><div class="controls bullet"><span class="by">comicjk</span><span>|</span><a href="#36157741">root</a><span>|</span><a href="#36158016">parent</a><span>|</span><a href="#36158385">prev</a><span>|</span><a href="#36158432">next</a><span>|</span><label class="collapse" for="c-36158419">[-]</label><label class="expand" for="c-36158419">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not wrong answers, they&#x27;re floating-point answers. What&#x27;s wrong is the assumption that infinite precision will fit in a handful of bytes.</div><br/></div></div><div id="36158432" class="c"><input type="checkbox" id="c-36158432" checked=""/><div class="controls bullet"><span class="by">shoo</span><span>|</span><a href="#36157741">root</a><span>|</span><a href="#36158016">parent</a><span>|</span><a href="#36158419">prev</a><span>|</span><a href="#36158374">next</a><span>|</span><label class="collapse" for="c-36158432">[-]</label><label class="expand" for="c-36158432">[1 more]</label></div><br/><div class="children"><div class="content">for many practical applications where we compute things, the input data isn&#x27;t fully accurate, it&#x27;s an estimate with significant error or uncertainty. when we consider how the outputs of computations are used, in many cases it isn&#x27;t useful for outputs to be arbitrarily precise.  maybe the output is used to make a decision and the expected cost or benefit decision won&#x27;t significantly change even if the output is wrong by 1%, for example.<p>one person&#x27;s &quot;wrong&quot; is another person&#x27;s approximation. by reducing accuracy we can often get very large performance improvements. good engineering makes an appropriate tradeoff.<p>there&#x27;s even more scope for this kind of thing inside some numerical algorithms where having a &quot;pretty good&quot; guess gives a massive speed boost, but the algorithm can be iterated to find quite precise answers even if the &quot;pretty good&quot; guess is an approximation that&#x27;s off by 20%<p>there&#x27;s yet more scope for this if we consider if we&#x27;re trying to mathematically model reality -- even if we perform all the math with arbitrary precision calculations, the theoretical model itself is still an approximation of reality, there&#x27;s some error there.  there&#x27;s limited value in solving a model exactly, as there&#x27;s always some approximation error between the model and reality.<p>it&#x27;s amazing we have such good support for high-performance scientific quality ieee floating point math everywhere in commodity hardware. it&#x27;s a great tool to have in the toolbelt</div><br/></div></div><div id="36158374" class="c"><input type="checkbox" id="c-36158374" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#36157741">root</a><span>|</span><a href="#36158016">parent</a><span>|</span><a href="#36158432">prev</a><span>|</span><a href="#36158584">next</a><span>|</span><label class="collapse" for="c-36158374">[-]</label><label class="expand" for="c-36158374">[1 more]</label></div><br/><div class="children"><div class="content">How is this relevant? This is more of a critique of floats than it is about vectorization.</div><br/></div></div><div id="36158584" class="c"><input type="checkbox" id="c-36158584" checked=""/><div class="controls bullet"><span class="by">windsignaling</span><span>|</span><a href="#36157741">root</a><span>|</span><a href="#36158016">parent</a><span>|</span><a href="#36158374">prev</a><span>|</span><label class="collapse" for="c-36158584">[-]</label><label class="expand" for="c-36158584">[1 more]</label></div><br/><div class="children"><div class="content">Still irrelevant.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>