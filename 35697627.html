<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="apple-mobile-web-app-capable" content="yes"/><link rel="preload" href="styles.css?v=1682522832960" as="style"/><link rel="stylesheet" href="styles.css?v=1682522832960"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://e2eml.school/transformers.html">Transformers from Scratch</a> <span class="domain">(<a href="https://e2eml.school">e2eml.school</a>)</span></div><div class="subtext"><span>jasim</span> | <span>29 comments</span></div><br/><div><div id="35715205" class="c"><input type="checkbox" id="c-35715205" checked=""/><div class="controls bullet"><span class="by">metalloid</span><span>|</span><a href="#35711789">next</a><span>|</span><label class="collapse" for="c-35715205">[-]</label><label class="expand" for="c-35715205">[1 more]</label></div><br/><div class="children"><div class="content">The author of the article should had provided an implementation of the transformer using only numpy or pure C++.</div><br/></div></div><div id="35711789" class="c"><input type="checkbox" id="c-35711789" checked=""/><div class="controls bullet"><span class="by">dsubburam</span><span>|</span><a href="#35715205">prev</a><span>|</span><a href="#35715028">next</a><span>|</span><label class="collapse" for="c-35711789">[-]</label><label class="expand" for="c-35711789">[5 more]</label></div><br/><div class="children"><div class="content">An early explainer of transformers, which is a quicker read, that I found very useful when they were still new to me, is The Illustrated Transformer[1], by Jay Alammar.<p>A more recent academic but high-level explanation of transformers, very good for detail on the different flow flavors (e.g. encoder-decoder vs decoder only), is Formal Algorithms for Transformers[2], from DeepMind.<p>[1] <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;</a>
[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.09238" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.09238</a></div><br/><div id="35713993" class="c"><input type="checkbox" id="c-35713993" checked=""/><div class="controls bullet"><span class="by">noobcoder</span><span>|</span><a href="#35711789">parent</a><span>|</span><a href="#35712334">next</a><span>|</span><label class="collapse" for="c-35713993">[-]</label><label class="expand" for="c-35713993">[1 more]</label></div><br/><div class="children"><div class="content">I remember looking into this article. It was really helpful for me to understand transformers. Although the OP&#x27;s article is detailed, this one is concise. Here&#x27;s the link: <a href="https:&#x2F;&#x2F;blue-season.github.io&#x2F;transformer-in-5-minutes" rel="nofollow">https:&#x2F;&#x2F;blue-season.github.io&#x2F;transformer-in-5-minutes</a></div><br/></div></div><div id="35712334" class="c"><input type="checkbox" id="c-35712334" checked=""/><div class="controls bullet"><span class="by">driscoll42</span><span>|</span><a href="#35711789">parent</a><span>|</span><a href="#35713993">prev</a><span>|</span><a href="#35713556">next</a><span>|</span><label class="collapse" for="c-35712334">[-]</label><label class="expand" for="c-35712334">[1 more]</label></div><br/><div class="children"><div class="content">The Illustrated Transformer is fantastic, but I would suggest that those going into it really should read the previous articles in the series to get a foundation to understand it more, plus later articles that go into GPT and BERT, here&#x27;s the list:<p>A Visual and Interactive Guide to the Basics of Neural Networks - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;visual-interactive-guide-basics-neural-networks&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;visual-interactive-guide-basics-n...</a><p>A Visual And Interactive Look at Basic Neural Network Math - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;feedforward-neural-networks-visual-interactive&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;feedforward-neural-networks-visua...</a><p>Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;visualizing-neural-machine-transl...</a><p>The Illustrated Transformer - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;</a><p>The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-bert&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-bert&#x2F;</a><p>The Illustrated GPT-2 (Visualizing Transformer Language Models) - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-gpt2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-gpt2&#x2F;</a><p>How GPT3 Works - Visualizations and Animations - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;how-gpt3-works-visualizations-animations&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;how-gpt3-works-visualizations-ani...</a><p>The Illustrated Retrieval Transformer - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-retrieval-transformer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-retrieval-transformer...</a><p>The Illustrated Stable Diffusion - <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-stable-diffusion&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-stable-diffusion&#x2F;</a><p>If you want to learn how to code them, this book is great: <a href="https:&#x2F;&#x2F;d2l.ai&#x2F;chapter_attention-mechanisms-and-transformers&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;d2l.ai&#x2F;chapter_attention-mechanisms-and-transformers...</a></div><br/></div></div><div id="35713556" class="c"><input type="checkbox" id="c-35713556" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#35711789">parent</a><span>|</span><a href="#35712334">prev</a><span>|</span><a href="#35715028">next</a><span>|</span><label class="collapse" for="c-35713556">[-]</label><label class="expand" for="c-35713556">[2 more]</label></div><br/><div class="children"><div class="content">Can anyone please say how much value there is in learning the fundamentals of LLMs for someone who uses them in practice?</div><br/><div id="35713744" class="c"><input type="checkbox" id="c-35713744" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35711789">root</a><span>|</span><a href="#35713556">parent</a><span>|</span><a href="#35715028">next</a><span>|</span><label class="collapse" for="c-35713744">[-]</label><label class="expand" for="c-35713744">[1 more]</label></div><br/><div class="children"><div class="content">A little understand would give you why certain prompts work or don’t work.  A high level should do.  It could help you make better prompts or trouble shoot, although you don’t need it for 80% of the cases</div><br/></div></div></div></div></div></div><div id="35715028" class="c"><input type="checkbox" id="c-35715028" checked=""/><div class="controls bullet"><span class="by">dingosity</span><span>|</span><a href="#35711789">prev</a><span>|</span><a href="#35711576">next</a><span>|</span><label class="collapse" for="c-35715028">[-]</label><label class="expand" for="c-35715028">[1 more]</label></div><br/><div class="children"><div class="content">Did anyone make the obvious &quot;Robots in Smalltalk&quot; joke yet?<p>Okay... here goes...<p>When I first read that title I thought the author was talking about Robots in Smalltalk.</div><br/></div></div><div id="35711576" class="c"><input type="checkbox" id="c-35711576" checked=""/><div class="controls bullet"><span class="by">stared</span><span>|</span><a href="#35715028">prev</a><span>|</span><a href="#35713090">next</a><span>|</span><label class="collapse" for="c-35711576">[-]</label><label class="expand" for="c-35711576">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing!<p>For the &quot;from scratch&quot; version, I recommend &quot;The GPT-3 Architecture, on a Napkin&quot; <a href="https:&#x2F;&#x2F;dugas.ch&#x2F;artificial_curiosity&#x2F;GPT_architecture.html" rel="nofollow">https:&#x2F;&#x2F;dugas.ch&#x2F;artificial_curiosity&#x2F;GPT_architecture.html</a>, which was there as well (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33942597" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33942597</a>).<p>Then, to actually dive into details, &quot;The Annotated Transformer&quot;, i.e. a walktrough &quot;Attention Is All You Need&quot;, with code in PyTorch, <a href="https:&#x2F;&#x2F;nlp.seas.harvard.edu&#x2F;2018&#x2F;04&#x2F;03&#x2F;attention.html" rel="nofollow">https:&#x2F;&#x2F;nlp.seas.harvard.edu&#x2F;2018&#x2F;04&#x2F;03&#x2F;attention.html</a>.</div><br/></div></div><div id="35713090" class="c"><input type="checkbox" id="c-35713090" checked=""/><div class="controls bullet"><span class="by">erwincoumans</span><span>|</span><a href="#35711576">prev</a><span>|</span><a href="#35714522">next</a><span>|</span><label class="collapse" for="c-35713090">[-]</label><label class="expand" for="c-35713090">[1 more]</label></div><br/><div class="children"><div class="content">Andrej Karpathy&#x27;s 2 hour video and code is really good to understand the details of Transformers:<p>&quot;Let&#x27;s build GPT: from scratch, in code, spelled out.&quot;<p><a href="https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=kCc8FmEb1nY">https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=kCc8FmEb1nY</a></div><br/></div></div><div id="35714522" class="c"><input type="checkbox" id="c-35714522" checked=""/><div class="controls bullet"><span class="by">lucidrains</span><span>|</span><a href="#35713090">prev</a><span>|</span><a href="#35711854">next</a><span>|</span><label class="collapse" for="c-35714522">[-]</label><label class="expand" for="c-35714522">[1 more]</label></div><br/><div class="children"><div class="content">besides everything that was mentioned here, what made it finally click for me early in my journey was running through this excellent tutorial by Peter Bloem multiple times <a href="https:&#x2F;&#x2F;peterbloem.nl&#x2F;blog&#x2F;transformers" rel="nofollow">https:&#x2F;&#x2F;peterbloem.nl&#x2F;blog&#x2F;transformers</a> highly recommend</div><br/></div></div><div id="35711854" class="c"><input type="checkbox" id="c-35711854" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#35714522">prev</a><span>|</span><a href="#35712880">next</a><span>|</span><label class="collapse" for="c-35711854">[-]</label><label class="expand" for="c-35711854">[9 more]</label></div><br/><div class="children"><div class="content">This article describes positional encodings based on several sine waves with different frequencies, but I&#x27;ve also seen positional &quot;embeddings&quot; used, where the position (the position is an integer value) is used to select an differentiable embedding from an embedding table. Thus, the model learns its own positional encoding. Does anyone know how these compare?<p>I&#x27;ve also wondered why we add the positional encoding to the value, rather than concatenating them?<p>Also, the terms encoding, embedding, projection, and others are all starting to sound the same to me. I&#x27;m not sure exactly what the difference is. Linear projections start to look like embeddings start to look like encodings start to look like projections, etc.  I guess that&#x27;s just the nature of linear algebra? It&#x27;s all the same? The data is the computation, and the computation is the data. Numbers in, numbers out, and if the wrong numbers come out then God help you.<p>I digress. Is there a distinction between encoding, embedding, and projection I should be aware of?<p>I recently read in &quot;The Little Learner&quot; book that finding the right parameters <i>is</i> learning. That&#x27;s the point. Everything we do in deep learning is focused on choosing the right sequence of numbers and we call those numbers <i>parameters</i>. Every parameter has a specific role in our model. <i>Parameters</i> are our choice, those are the nobs that we (as a personified machine learning algorithm) get to adjust. Ever since then the word &quot;parameters&quot; has been much more meaningful to me. I&#x27;m hoping for similar clarity with these other words.</div><br/><div id="35712203" class="c"><input type="checkbox" id="c-35712203" checked=""/><div class="controls bullet"><span class="by">whoateallthepy</span><span>|</span><a href="#35711854">parent</a><span>|</span><a href="#35712020">next</a><span>|</span><label class="collapse" for="c-35712203">[-]</label><label class="expand" for="c-35712203">[5 more]</label></div><br/><div class="children"><div class="content">This is a great set of comments&#x2F;questions! To try and answer this a bit briefly:<p>The input string is <i>tokenized</i> into a sequence of <i>token indices</i> (integers) as the first step of processing the input. For example, &quot;Hello World&quot; is tokenized to:<p><pre><code>  [15496, 2159]
</code></pre>
The first step in a transformer network is to <i>embed</i> the tokens. Each token index is mapped to a (learned or fixed) embedding (a vector of floats) via the embeddings table. The <i>Embeddings</i> module from PyTorch is commonly used. After mapping, the matrix of embeddings will look something like:<p><pre><code>  [[-0.147, 2.861, ..., -0.447],
   [-0.517, -0.698, ..., -0.558]]
</code></pre>
where the number of columns is the <i>model dimension</i>.<p>A single <i>transformer block</i> takes a matrix of embeddings and transforms them to a matrix of identical dimensions. An important property of the block is that if you reorder the rows of the matrix (which can be done by reordering the input tokens), the output will be reordered but otherwise identical too. (The formal name for this is <i>permutation equivariance</i>).<p>In problems related to language it seems inappropriate to have the order of tokens not matter, so to solve for this we need to adjust the embeddings of the tokens initially based on their position.<p>There are a few common ways you might see this done, but they broadly work by assigning fixed or learned embeddings to each position in the input token sequence. These embeddings can be added to our matrix above so that the first row gets the embedding for the first position added to it, the second row gets the embedding for the second position, and so on. Now if the tokens are reordered, the combined embedding matrix will <i>not</i> be the same. Alternatively, these embeddings can be concatenated horizontally to our matrix: this guarantees the positional information is kept entirely separate from the linguistic (at the cost of having a larger model dimension).<p>I put together this repository at the end of last year to better help visualize the internals of a transformer block when applied to a toy problem: <a href="https:&#x2F;&#x2F;github.com&#x2F;rstebbing&#x2F;workshop&#x2F;tree&#x2F;main&#x2F;experiments&#x2F;transformer_sequence_classification">https:&#x2F;&#x2F;github.com&#x2F;rstebbing&#x2F;workshop&#x2F;tree&#x2F;main&#x2F;experiments&#x2F;...</a>. It is not super long, and the point is to try and better distinguish between the quantities you referred to by seeing them (which is possible when embeddings are in a low dimension).<p>I hope this helps!</div><br/><div id="35713078" class="c"><input type="checkbox" id="c-35713078" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#35711854">root</a><span>|</span><a href="#35712203">parent</a><span>|</span><a href="#35712427">next</a><span>|</span><label class="collapse" for="c-35713078">[-]</label><label class="expand" for="c-35713078">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Alternatively, these embeddings can be concatenated horizontally to our matrix: this guarantees the positional information is kept entirely separate from the linguistic (at the cost of having a larger model dimension).<p>Yes, the entire description is helpful, but I especially appreciate this validation that concatenating the position encoding is a valid option.<p>I&#x27;ve been thinking a lot about aggregation functions, usually summation since it&#x27;s the most basic aggregation function. After adding the token embedding and the positional encoding together, it seems information has been lost, because the resulting sum cannot be separated back into the original values. And yet, that seems to be what they do in most transformers, so it must be worth the trade-off.<p>It reminds me of being a kid, when you first realize that zipping a file produces a smaller file and you think &quot;well, what if I zip the zip file?&quot; At first you wonder if you can eventually compress everything down to a single byte. I wonder the same with aggregation &#x2F; summation, &quot;if I can add the position to the embedding, and things still work, can I just keep adding things together until I have a single number?&quot; Obviously there are some limits, but I&#x27;m not sure where those are. Maybe nobody knows? I&#x27;m hoping to study linear algebra more and perhaps I will find some answers there?</div><br/></div></div><div id="35712427" class="c"><input type="checkbox" id="c-35712427" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#35711854">root</a><span>|</span><a href="#35712203">parent</a><span>|</span><a href="#35713078">prev</a><span>|</span><a href="#35712020">next</a><span>|</span><label class="collapse" for="c-35712427">[-]</label><label class="expand" for="c-35712427">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The input string is tokenized into a sequence of token indices (integers)<p>How is this tokenization done? Sometimes a single word can be two tokens. My understanding is that the token indices are also learned, but by whom? The same transformer? Another neural network?</div><br/><div id="35712666" class="c"><input type="checkbox" id="c-35712666" checked=""/><div class="controls bullet"><span class="by">whoateallthepy</span><span>|</span><a href="#35711854">root</a><span>|</span><a href="#35712427">parent</a><span>|</span><a href="#35712728">next</a><span>|</span><label class="collapse" for="c-35712666">[-]</label><label class="expand" for="c-35712666">[1 more]</label></div><br/><div class="children"><div class="content">The tokenization is done by the tokenizer which can be thought of as just a function that maps strings to integers <i>before</i> the neural network. Tokenizers can be hand-specified or learned, but in either case this is typically done separately from training the model. It is also less frequently necessary unless you are dealing with an entirely new input type&#x2F;language.<p>Tokenizers can be quite gnarly internally. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;chapter6&#x2F;5?fw=pt" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;chapter6&#x2F;5?fw=pt</a> is a good resource on BPE tokenization.</div><br/></div></div><div id="35712728" class="c"><input type="checkbox" id="c-35712728" checked=""/><div class="controls bullet"><span class="by">montebicyclelo</span><span>|</span><a href="#35711854">root</a><span>|</span><a href="#35712427">parent</a><span>|</span><a href="#35712666">prev</a><span>|</span><a href="#35712020">next</a><span>|</span><label class="collapse" for="c-35712728">[-]</label><label class="expand" for="c-35712728">[1 more]</label></div><br/><div class="children"><div class="content">Huggingface have good guides on tokenization, and tokenizer training. BPE (e.g. used by gpt) and wordpiece (e.g. used by bert) are two commonly used methods <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;chapter6&#x2F;5?fw=pt" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;chapter6&#x2F;5?fw=pt</a></div><br/></div></div></div></div></div></div><div id="35712020" class="c"><input type="checkbox" id="c-35712020" checked=""/><div class="controls bullet"><span class="by">giovannibonetti</span><span>|</span><a href="#35711854">parent</a><span>|</span><a href="#35712203">prev</a><span>|</span><a href="#35713030">next</a><span>|</span><label class="collapse" for="c-35712020">[-]</label><label class="expand" for="c-35712020">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I recently read in &quot;The Little Learner&quot; book that finding the right parameters is learning. That&#x27;s the point. Everything we do in deep learning is focused on choosing the right sequence of numbers and we call those numbers parameters. Every parameter has a specific role in our model. Parameters are our choice, those are the nobs that we (as a personified machine learning algorithm) get to adjust.<p>Be careful not to mistake parameters for hyperparameters.
- Parameters are the result of the training phase, as you mentioned. They start with random values and are discovered by the training algorithm;
- Hyperparameters, on the other hand, are the knobs you tweak to make the training process arrive at the &quot;right&quot; parameters. You can think of them as meta-parameters;<p>Also, it is important to think on the ML architecture - transformers, neural networks, random forests and so on - as the parameters change completely depending on which one you&#x27;re using.</div><br/><div id="35712921" class="c"><input type="checkbox" id="c-35712921" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#35711854">root</a><span>|</span><a href="#35712020">parent</a><span>|</span><a href="#35713030">next</a><span>|</span><label class="collapse" for="c-35712921">[-]</label><label class="expand" for="c-35712921">[1 more]</label></div><br/><div class="children"><div class="content">Yes, hyper-parameters are parameters about the parameters. Parameters we get to choose which control the parameters that the learning algorithm chooses.<p>The other set of data the book called &quot;arguments&quot;, which is the term they use to describe the data you are training on. That seems like an unnecessarily confusing term, and I haven&#x27;t heard it anywhere else.<p>I didn&#x27;t learn anything truly new in all this, but it helped me sort my own thoughts to realize there is data, parameters, and hyperparameters. Data comes from the world and we cannot change it. Parameters are chosen by us indirectly through the learning algorithm, they are the most important outcome of successful learning. Hyperparameters are chosen by us directly and control the model and learning algorithm, and the resulting parameters.</div><br/></div></div></div></div><div id="35713030" class="c"><input type="checkbox" id="c-35713030" checked=""/><div class="controls bullet"><span class="by">Silverback_VII</span><span>|</span><a href="#35711854">parent</a><span>|</span><a href="#35712020">prev</a><span>|</span><a href="#35712880">next</a><span>|</span><label class="collapse" for="c-35713030">[-]</label><label class="expand" for="c-35713030">[1 more]</label></div><br/><div class="children"><div class="content">Also, the terms encoding, embedding, projection, and others are all starting to sound the same to me.<p>Well, projection is used to create the embedding with which the symbol is encoded.<p>It greatly reduces computational cost as the encoding carries already a lot of information.</div><br/></div></div></div></div><div id="35712880" class="c"><input type="checkbox" id="c-35712880" checked=""/><div class="controls bullet"><span class="by">adriantam</span><span>|</span><a href="#35711854">prev</a><span>|</span><a href="#35711803">next</a><span>|</span><label class="collapse" for="c-35712880">[-]</label><label class="expand" for="c-35712880">[1 more]</label></div><br/><div class="children"><div class="content">If you want a TensorFlow implementation, here it is: <a href="https:&#x2F;&#x2F;machinelearningmastery.com&#x2F;building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days&#x2F;" rel="nofollow">https:&#x2F;&#x2F;machinelearningmastery.com&#x2F;building-transformer-mode...</a></div><br/></div></div><div id="35711803" class="c"><input type="checkbox" id="c-35711803" checked=""/><div class="controls bullet"><span class="by">cuuupid</span><span>|</span><a href="#35712880">prev</a><span>|</span><a href="#35711413">next</a><span>|</span><label class="collapse" for="c-35711803">[-]</label><label class="expand" for="c-35711803">[1 more]</label></div><br/><div class="children"><div class="content">This is cool, I highly recommend Jay Alammar’s Illustrated Transformer series to anyone wanting to get an understanding of the different types of transformers and how self-attention works.<p>The math behind self-attention is also cool and easy to extend to e.g. dual attention</div><br/></div></div><div id="35711413" class="c"><input type="checkbox" id="c-35711413" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35711803">prev</a><span>|</span><a href="#35711756">next</a><span>|</span><label class="collapse" for="c-35711413">[-]</label><label class="expand" for="c-35711413">[2 more]</label></div><br/><div class="children"><div class="content">so I’m on the same journey of trying to teach myself ML and I do find most of the resources go over things very quickly and leave a lot you to figure out yourself.<p>Having had a quick look at this one, it looks very beginner, friendly, and also very careful to explain things slowly, so I will definitely added to my reading list.<p>Thanks to the author for this!</div><br/><div id="35711582" class="c"><input type="checkbox" id="c-35711582" checked=""/><div class="controls bullet"><span class="by">KyeRussell</span><span>|</span><a href="#35711413">parent</a><span>|</span><a href="#35711756">next</a><span>|</span><label class="collapse" for="c-35711582">[-]</label><label class="expand" for="c-35711582">[1 more]</label></div><br/><div class="children"><div class="content">Ultimately because it’s such a hot topic the “market” is flooded with people that want to crank out content without understanding what they’re talking about.</div><br/></div></div></div></div><div id="35711756" class="c"><input type="checkbox" id="c-35711756" checked=""/><div class="controls bullet"><span class="by">toyg</span><span>|</span><a href="#35711413">prev</a><span>|</span><a href="#35711570">next</a><span>|</span><label class="collapse" for="c-35711756">[-]</label><label class="expand" for="c-35711756">[3 more]</label></div><br/><div class="children"><div class="content">MORE THAN MEETS THE EYE!<p>... oh, not <i>those</i> Transformers. Meh.</div><br/><div id="35713731" class="c"><input type="checkbox" id="c-35713731" checked=""/><div class="controls bullet"><span class="by">MisterTea</span><span>|</span><a href="#35711756">parent</a><span>|</span><a href="#35712690">next</a><span>|</span><label class="collapse" for="c-35713731">[-]</label><label class="expand" for="c-35713731">[1 more]</label></div><br/><div class="children"><div class="content">I was hoping it was an article on designing and building an electrical transformer complete with pictures of a home made, hand wound transformer. I was very disappointed.</div><br/></div></div><div id="35712690" class="c"><input type="checkbox" id="c-35712690" checked=""/><div class="controls bullet"><span class="by">zabzonk</span><span>|</span><a href="#35711756">parent</a><span>|</span><a href="#35713731">prev</a><span>|</span><a href="#35711570">next</a><span>|</span><label class="collapse" for="c-35712690">[-]</label><label class="expand" for="c-35712690">[1 more]</label></div><br/><div class="children"><div class="content">that was my first glance reading too - shape-changing toys written in the scratch language! how cool could that be?</div><br/></div></div></div></div></div></div></div></div></div></body></html>