<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729414851290" as="style"/><link rel="stylesheet" href="styles.css?v=1729414851290"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.01502">Why do random forests work? They are self-regularizing adaptive smoothers</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>sebg</span> | <span>31 comments</span></div><br/><div><div id="41890415" class="c"><input type="checkbox" id="c-41890415" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#41890447">next</a><span>|</span><label class="collapse" for="c-41890415">[-]</label><label class="expand" for="c-41890415">[7 more]</label></div><br/><div class="children"><div class="content">Nice article, although I find it&#x27;s a bit overly complex if your are not familiar with ML and mathematics at the same time.<p>I will leave here a geometrical intuition of why they work in case it can help someone:<p>To simplify things, I will talk about regression, and say we want to predict some value y, that we know depends on x, and we have some noisy measurements of y.<p>y = f(x)
y_observed = f(x) + noise<p>If you want some mental image, think about f(x)=sin(x).<p>Now, a (over fitted) regression tree in this case is just a step function where the value at x is y_observed. If there is no noise, we now that by doing more measurements, we can approximate y with as much precision as we want. But if there is noise, the regression tree will over fit the noisy values, creating some artificial spikes.<p>If you want to avoid this over fitting, you sample a lot of times the values of X, and for each sample you build a regression tree, and then average them. When you average them, every tree will contain its own particular artificial spikes, and if they are noise, they won&#x27;t appear in the majority of the other trees. So when you average them, the spikes will attenuate, creating the smoother behaviour that the article talks about.<p>I hope it helps!</div><br/><div id="41892756" class="c"><input type="checkbox" id="c-41892756" checked=""/><div class="controls bullet"><span class="by">math_dandy</span><span>|</span><a href="#41890415">parent</a><span>|</span><a href="#41893790">next</a><span>|</span><label class="collapse" for="c-41892756">[-]</label><label class="expand" for="c-41892756">[2 more]</label></div><br/><div class="children"><div class="content">This is good intuition for why ensembling overparametrized is a good idea. Doesn’t speak to why ensembles of tree-structured estimators in particular perform so well compared to ensembles of other nonparametric estimators.</div><br/><div id="41893498" class="c"><input type="checkbox" id="c-41893498" checked=""/><div class="controls bullet"><span class="by">youoy</span><span>|</span><a href="#41890415">root</a><span>|</span><a href="#41892756">parent</a><span>|</span><a href="#41893790">next</a><span>|</span><label class="collapse" for="c-41893498">[-]</label><label class="expand" for="c-41893498">[1 more]</label></div><br/><div class="children"><div class="content">If you look at what makes it work well in the example, I would say it is being able to easily approximate a function with whatever degree of precision that you want, which translates to being able to isolate spikes in the approximation.<p>For example, one could ask, what if instead of an approximation by step functions, we use a piecewise linear approximation (which is as good)? You can do that with a fully connected artificial neural network with ReLU nonlinearity, and if you check it experimentally, you will see that the results are equivalent.<p>Why do people often use ensembles of tree structures? The ensembling part is included in the programming packages and that is not the case for ANN, so it is quicker to experiment with. Appart from that, if you have some features that behave like categorical variables, trees also behave better in training.</div><br/></div></div></div></div><div id="41893790" class="c"><input type="checkbox" id="c-41893790" checked=""/><div class="controls bullet"><span class="by">sillying</span><span>|</span><a href="#41890415">parent</a><span>|</span><a href="#41892756">prev</a><span>|</span><a href="#41890442">next</a><span>|</span><label class="collapse" for="c-41893790">[-]</label><label class="expand" for="c-41893790">[1 more]</label></div><br/><div class="children"><div class="content">So it seems that when you have different sources of errors the average of them cancel the noise. I think some property about the sources of error is necessary so in some sense the sources should be independent.</div><br/></div></div><div id="41890442" class="c"><input type="checkbox" id="c-41890442" checked=""/><div class="controls bullet"><span class="by">hammock</span><span>|</span><a href="#41890415">parent</a><span>|</span><a href="#41893790">prev</a><span>|</span><a href="#41893558">next</a><span>|</span><label class="collapse" for="c-41890442">[-]</label><label class="expand" for="c-41890442">[1 more]</label></div><br/><div class="children"><div class="content">Thanks that helps. The way I think about your example is it’s like (not the same obv) taking a bunch of moving averages of different durations at different starting points, and throwing those into your regression model along with the actual data</div><br/></div></div><div id="41893558" class="c"><input type="checkbox" id="c-41893558" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#41890415">parent</a><span>|</span><a href="#41890442">prev</a><span>|</span><a href="#41892542">next</a><span>|</span><label class="collapse" for="c-41893558">[-]</label><label class="expand" for="c-41893558">[1 more]</label></div><br/><div class="children"><div class="content">It would be also interesting to see the limitations of random forest and where they struggle to learn and produce good results.</div><br/></div></div><div id="41892542" class="c"><input type="checkbox" id="c-41892542" checked=""/><div class="controls bullet"><span class="by">dbetteridge</span><span>|</span><a href="#41890415">parent</a><span>|</span><a href="#41893558">prev</a><span>|</span><a href="#41890447">next</a><span>|</span><label class="collapse" for="c-41892542">[-]</label><label class="expand" for="c-41892542">[1 more]</label></div><br/><div class="children"><div class="content">Some overlay lots of really squiggly lines, average (most points in common) is the actual function you&#x27;re looking for?</div><br/></div></div></div></div><div id="41890447" class="c"><input type="checkbox" id="c-41890447" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#41890415">prev</a><span>|</span><a href="#41891058">next</a><span>|</span><label class="collapse" for="c-41890447">[-]</label><label class="expand" for="c-41890447">[3 more]</label></div><br/><div class="children"><div class="content">Good to see more research exploring the connection between trees, ensembles, and smoothing. Way back in Trevor Hastie&#x27;s ESL book there&#x27;s a section on how gradient boosting using &quot;stumps&quot; (trees with only one split) is equivalent to an additive spline model (GAM, technically) with a step function as a spline basis and adaptive knot placement.<p>I&#x27;ve always thought there should be a deep connection between ReLU neural nets and regularized adaptive smoothers as well, since the ReLU function is itself a spline basis (a so-called truncated linear spline) and happens to span the same functional basis as B-splines of the same degree.</div><br/><div id="41893418" class="c"><input type="checkbox" id="c-41893418" checked=""/><div class="controls bullet"><span class="by">abhgh</span><span>|</span><a href="#41890447">parent</a><span>|</span><a href="#41892797">next</a><span>|</span><label class="collapse" for="c-41893418">[-]</label><label class="expand" for="c-41893418">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know if you have come across this: <i>A Spline Theory of Deep Networks</i> [1].
Has been on my To-Read list forever :(<p>[1] <a href="http:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v80&#x2F;balestriero18b&#x2F;balestriero18b.pdf" rel="nofollow">http:&#x2F;&#x2F;proceedings.mlr.press&#x2F;v80&#x2F;balestriero18b&#x2F;balestriero1...</a></div><br/></div></div><div id="41892797" class="c"><input type="checkbox" id="c-41892797" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#41890447">parent</a><span>|</span><a href="#41893418">prev</a><span>|</span><a href="#41891058">next</a><span>|</span><label class="collapse" for="c-41892797">[-]</label><label class="expand" for="c-41892797">[1 more]</label></div><br/><div class="children"><div class="content">One of my biggest pet peeves is flagrant overuse of &quot;deep&quot;. Everything is so deep around around here these days...<p>&gt; since the ReLU function is itself a spline basis (a so-called truncated linear spline) and happens to span the same functional basis as B-splines of the same degree.<p>... you literally just spelled out the entire &quot;depth&quot; of it.</div><br/></div></div></div></div><div id="41891058" class="c"><input type="checkbox" id="c-41891058" checked=""/><div class="controls bullet"><span class="by">bonoboTP</span><span>|</span><a href="#41890447">prev</a><span>|</span><a href="#41891942">next</a><span>|</span><label class="collapse" for="c-41891058">[-]</label><label class="expand" for="c-41891058">[2 more]</label></div><br/><div class="children"><div class="content">The same team wrote another interesting paper arguing that there&#x27;s no &quot;double descent&quot; in linear regression, trees, and boosting, despite what many argued before (in this paper they don&#x27;t tackle deep learning double descent, but remark that the case may be similar regarding the existence of different complexity measures being conflated).</div><br/><div id="41892261" class="c"><input type="checkbox" id="c-41892261" checked=""/><div class="controls bullet"><span class="by">selimthegrim</span><span>|</span><a href="#41891058">parent</a><span>|</span><a href="#41891942">next</a><span>|</span><label class="collapse" for="c-41892261">[-]</label><label class="expand" for="c-41892261">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.18988" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.18988</a></div><br/></div></div></div></div><div id="41891942" class="c"><input type="checkbox" id="c-41891942" checked=""/><div class="controls bullet"><span class="by">foundry27</span><span>|</span><a href="#41891058">prev</a><span>|</span><a href="#41891352">next</a><span>|</span><label class="collapse" for="c-41891942">[-]</label><label class="expand" for="c-41891942">[1 more]</label></div><br/><div class="children"><div class="content">I like this article. Randomness in system design is one of the most practical ways to handle the messiness of real world inputs, and I think random forests nail this by using randomness to produce useful outputs to various inputs without overfitting and adapt to the unexpected. Yeah, you can always engineer a system that explicitly handles every possible situation, but the important question is “how long&#x2F;costly will that process be?”. Deterministic  systems aren’t good on that front, and when edge cases hit, sometimes those rigid models crack. Controlled randomness (load balancing, feature selection, etc.) makes systems more flexible and resilient. You don’t get stuck in the same predictable ruts, and that’s exactly why randomness works where pure determinism fails</div><br/></div></div><div id="41891352" class="c"><input type="checkbox" id="c-41891352" checked=""/><div class="controls bullet"><span class="by">StableAlkyne</span><span>|</span><a href="#41891942">prev</a><span>|</span><a href="#41892387">next</a><span>|</span><label class="collapse" for="c-41891352">[-]</label><label class="expand" for="c-41891352">[2 more]</label></div><br/><div class="children"><div class="content">Random forests are incredible cool algorithms.<p>The idea that you can take hundreds of bad models that over fit (the individual decision trees), add even more randomness by randomly picking training data and features*, and averaging them together - it&#x27;s frankly amazing that this leads to consistently OK models. Often not the best but rarely the worst. There&#x27;s a reason they&#x27;re such a common baseline to compare against.<p>*Unless you&#x27;re using Sklearn, whose implementation of RandomForestRegressor is not a random forest. It&#x27;s actually bagged trees because they don&#x27;t randomly select features. Why they kept the misleading classname is beyond me.</div><br/><div id="41891898" class="c"><input type="checkbox" id="c-41891898" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#41891352">parent</a><span>|</span><a href="#41892387">next</a><span>|</span><label class="collapse" for="c-41891898">[-]</label><label class="expand" for="c-41891898">[1 more]</label></div><br/><div class="children"><div class="content">With a relatively small variant to make it gradient boosted trees it’s pretty much as good as it gets for tabular data these days</div><br/></div></div></div></div><div id="41892387" class="c"><input type="checkbox" id="c-41892387" checked=""/><div class="controls bullet"><span class="by">SomaticPirate</span><span>|</span><a href="#41891352">prev</a><span>|</span><a href="#41890745">next</a><span>|</span><label class="collapse" for="c-41892387">[-]</label><label class="expand" for="c-41892387">[3 more]</label></div><br/><div class="children"><div class="content">Any suggestions for a 101 introduction to random forests? In university I encountered some ML courses but never random forests.</div><br/><div id="41892681" class="c"><input type="checkbox" id="c-41892681" checked=""/><div class="controls bullet"><span class="by">sunshinesnacks</span><span>|</span><a href="#41892387">parent</a><span>|</span><a href="#41892414">next</a><span>|</span><label class="collapse" for="c-41892681">[-]</label><label class="expand" for="c-41892681">[1 more]</label></div><br/><div class="children"><div class="content">Statquest is really good. Maybe a little basic if you’ve taken ML courses, though?<p><a href="https:&#x2F;&#x2F;youtube.com&#x2F;playlist?list=PLblh5JKOoLUIE96dI3U7oxHaCAbZgfhHk&amp;si=mjR1sGi97wVQDR-I" rel="nofollow">https:&#x2F;&#x2F;youtube.com&#x2F;playlist?list=PLblh5JKOoLUIE96dI3U7oxHaC...</a></div><br/></div></div><div id="41892414" class="c"><input type="checkbox" id="c-41892414" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41892387">parent</a><span>|</span><a href="#41892681">prev</a><span>|</span><a href="#41890745">next</a><span>|</span><label class="collapse" for="c-41892414">[-]</label><label class="expand" for="c-41892414">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I did a deep dive on them here -- doesn&#x27;t require any particular math background beyond high school:
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=blyXCk4sgEg" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=blyXCk4sgEg</a></div><br/></div></div></div></div><div id="41890745" class="c"><input type="checkbox" id="c-41890745" checked=""/><div class="controls bullet"><span class="by">tylerneylon</span><span>|</span><a href="#41892387">prev</a><span>|</span><a href="#41891421">next</a><span>|</span><label class="collapse" for="c-41890745">[-]</label><label class="expand" for="c-41890745">[2 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s some context and a partial summary (youoy also has a nice summary) --<p>Context:<p>A random forest is an ML model that can be trained to predict an output value based on a list of input features: eg, predicting a house&#x27;s value based on square footage, location, etc. This paper focuses on regression models, meaning the output value is a real number (or a vector thereof). Classical ML theory suggests that models with many learned parameters are more likely to overfit the training data, meaning that when you predict an output for a test (non-training) input, the predicted value is less likely to be correct because the model is not generalizing well (it does well on training data, but not on test data - aka, it has memorized, but not understood).<p>Historically, a surprise is that random forests can have many parameters yet don&#x27;t overfit. This paper explores the surprise.<p>What the paper says:<p>The perspective of the paper is to see random forests (and related models) as _smoothers_, which is a kind of model that essentially memorizes the training data and then makes predictions by combining training output values that are relevant to the prediction-time (new) input values. For example, k-nearest neighbors is a simple kind of smoother. A single decision tree counts as a smoother because each final&#x2F;leaf node in the tree predicts a value based on combining training outputs that could possibly reach that node. The same can be said for forests.<p>So the authors see a random forest as a way to use a subset of training data and a subset of (or set of weights on) training features, to provide an averaged output. While a single decision tree can overfit (become &quot;spikey&quot;) because some leaf nodes can be based on single training examples, a forest gives a smoother prediction function since it is averaging across many trees, and often other trees won&#x27;t be spikey for the same input (their leaf node may be based on many training points, not a single one).<p>Finally, the authors refer to random forests as _adaptive smoothers_ to point out that random forests become even better at smoothing in locations in the input space that either have high variation (intuitively, that have a higher slope), or that are far from the training data. The word &quot;adaptive&quot; indicates that the predicted function changes behavior based on the nature of the data — eg, with k-NN, an adaptive version might increase the value of k at some places in the input space.<p>The way random forests act adaptively is that (a) the prediction function is naturally more dense (can change value more quickly) in <i>areas of high variability</i> because those locations will have more leaf nodes, and (b) the prediction function is typically a combination of a wider variety of possible values <i>when the input is far from the training data</i> because in that case the trees are likely to provide a variety of output values. These are both ways to avoid overfitting to training data and to generalize better to new inputs.<p>Disclaimer: I did not carefully read the paper; this is my quick understanding.</div><br/><div id="41891012" class="c"><input type="checkbox" id="c-41891012" checked=""/><div class="controls bullet"><span class="by">abetusk</span><span>|</span><a href="#41890745">parent</a><span>|</span><a href="#41891421">next</a><span>|</span><label class="collapse" for="c-41891012">[-]</label><label class="expand" for="c-41891012">[1 more]</label></div><br/><div class="children"><div class="content">I think this is specifically coming to terms with an insight that&#x27;s taught to statisticians about a bias-variance tradeoff.<p>From my understanding, in a statistical setting, low variability in bias leads to high variability in variance whereas low variability in variance leads to high variability in bias. The example I saw was with K-means, where K = N leads to high variance (the predicted cluster is highly variable) but low bias (take an input point, you get that exact input point back), vs. K=1 low variance (there&#x27;s only one cluster) but bad bias (input point is far away from the cluster center&#x2F;representative point).<p>I&#x27;m not sure I&#x27;ve characterized it well but there&#x27;s a Twitter post from Alicia Curth that explains it [0] as well as a paper that goes into it [1].<p>[0] <a href="https:&#x2F;&#x2F;x.com&#x2F;AliciaCurth&#x2F;status&#x2F;1841817856142348529" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;AliciaCurth&#x2F;status&#x2F;1841817856142348529</a><p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.18842" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.18842</a></div><br/></div></div></div></div><div id="41891421" class="c"><input type="checkbox" id="c-41891421" checked=""/><div class="controls bullet"><span class="by">alexfromapex</span><span>|</span><a href="#41890745">prev</a><span>|</span><a href="#41890125">next</a><span>|</span><label class="collapse" for="c-41891421">[-]</label><label class="expand" for="c-41891421">[1 more]</label></div><br/><div class="children"><div class="content">Quantized random sampling regression</div><br/></div></div><div id="41890125" class="c"><input type="checkbox" id="c-41890125" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#41891421">prev</a><span>|</span><label class="collapse" for="c-41890125">[-]</label><label class="expand" for="c-41890125">[9 more]</label></div><br/><div class="children"><div class="content">Reading the abstract alone, I have no idea whether it&#x27;s talking about algorithmic trees or, like, the big brown things with small green bits.</div><br/><div id="41890247" class="c"><input type="checkbox" id="c-41890247" checked=""/><div class="controls bullet"><span class="by">krystofee</span><span>|</span><a href="#41890125">parent</a><span>|</span><a href="#41890186">next</a><span>|</span><label class="collapse" for="c-41890247">[-]</label><label class="expand" for="c-41890247">[1 more]</label></div><br/><div class="children"><div class="content">You have to know some machine learning fundamentals to figure that out - “Random Forest” is a specific machine learning algorithm, which does not need a further explanation. To take it a step further, they should really not describe “Machine learning”, no, its not like the machine takes a book and learns, its a term.</div><br/></div></div><div id="41890186" class="c"><input type="checkbox" id="c-41890186" checked=""/><div class="controls bullet"><span class="by">vatys</span><span>|</span><a href="#41890125">parent</a><span>|</span><a href="#41890247">prev</a><span>|</span><a href="#41890628">next</a><span>|</span><label class="collapse" for="c-41890186">[-]</label><label class="expand" for="c-41890186">[3 more]</label></div><br/><div class="children"><div class="content">I had the exact same reaction: biology or computers?<p>The only hint I can see anywhere on the page is &quot;Statistics &gt; Machine Learning&quot; above the abstract title.<p>I really want it to be about actual biological trees being studied on the scale of forests growing with smooth edges over long periods of time, but I suspect that&#x27;s not what it is about.</div><br/><div id="41891115" class="c"><input type="checkbox" id="c-41891115" checked=""/><div class="controls bullet"><span class="by">bonoboTP</span><span>|</span><a href="#41890125">root</a><span>|</span><a href="#41890186">parent</a><span>|</span><a href="#41891128">next</a><span>|</span><label class="collapse" for="c-41891115">[-]</label><label class="expand" for="c-41891115">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also &quot;Subjects: Machine Learning (stat.ML); Machine Learning (cs.LG)&quot;<p>Also, the very first sentence of the actual paper (after the abstract) is<p>&gt; Random forests (Breiman, 2001) have emerged as one of the most reliable off-the-shelf supervised learning algorithms [...]<p>arxiv.org is overwhelmingly used for math and computer science papers, though not exclusively.<p>The paper will also likely be submitted to a machine learning venue.</div><br/></div></div><div id="41891128" class="c"><input type="checkbox" id="c-41891128" checked=""/><div class="controls bullet"><span class="by">ibgeek</span><span>|</span><a href="#41890125">root</a><span>|</span><a href="#41890186">parent</a><span>|</span><a href="#41891115">prev</a><span>|</span><a href="#41890628">next</a><span>|</span><label class="collapse" for="c-41891128">[-]</label><label class="expand" for="c-41891128">[1 more]</label></div><br/><div class="children"><div class="content">Biological trees don’t make predictions.  Second or third sentence contains the phrase “randomized tree ensembles not only make predictions.”</div><br/></div></div></div></div><div id="41890628" class="c"><input type="checkbox" id="c-41890628" checked=""/><div class="controls bullet"><span class="by">bigmadshoe</span><span>|</span><a href="#41890125">parent</a><span>|</span><a href="#41890186">prev</a><span>|</span><a href="#41890231">next</a><span>|</span><label class="collapse" for="c-41890628">[-]</label><label class="expand" for="c-41890628">[2 more]</label></div><br/><div class="children"><div class="content">The tree is an incredibly common data structure in computer science. Decision trees are well known. Random forests are ubiquitous in Machine Learning. Should the authors really have to dumb their paper down so people who don’t work in this domain avoid confusing it with work in arborism?</div><br/><div id="41890664" class="c"><input type="checkbox" id="c-41890664" checked=""/><div class="controls bullet"><span class="by">avazhi</span><span>|</span><a href="#41890125">root</a><span>|</span><a href="#41890628">parent</a><span>|</span><a href="#41890231">next</a><span>|</span><label class="collapse" for="c-41890664">[-]</label><label class="expand" for="c-41890664">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure the guy you’re replying to was half-joking, but adding the words ‘machine learning’ in the first sentence would have cleared this up pretty simply and wouldn’t have resulted in dumbing down anything.</div><br/></div></div></div></div><div id="41890231" class="c"><input type="checkbox" id="c-41890231" checked=""/><div class="controls bullet"><span class="by">mhuffman</span><span>|</span><a href="#41890125">parent</a><span>|</span><a href="#41890628">prev</a><span>|</span><label class="collapse" for="c-41890231">[-]</label><label class="expand" for="c-41890231">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s talking about these[0]<p>[0]<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Random_forest" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Random_forest</a></div><br/><div id="41890955" class="c"><input type="checkbox" id="c-41890955" checked=""/><div class="controls bullet"><span class="by">defjm</span><span>|</span><a href="#41890125">root</a><span>|</span><a href="#41890231">parent</a><span>|</span><label class="collapse" for="c-41890955">[-]</label><label class="expand" for="c-41890955">[1 more]</label></div><br/><div class="children"><div class="content">Jeremy Howard has a great video explaining Random Forests: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;blyXCk4sgEg" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;blyXCk4sgEg</a> .</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>