<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728810068115" as="style"/><link rel="stylesheet" href="styles.css?v=1728810068115"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://nathanzhao.cc/explore-exploit">The Explore vs. Exploit Dilemma</a> <span class="domain">(<a href="https://nathanzhao.cc">nathanzhao.cc</a>)</span></div><div class="subtext"><span>nzhaa</span> | <span>5 comments</span></div><br/><div><div id="41825910" class="c"><input type="checkbox" id="c-41825910" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41825498">next</a><span>|</span><label class="collapse" for="c-41825910">[-]</label><label class="expand" for="c-41825910">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using evolutionary techniques with Pareto front optimization to deal with this tradeoff. If two (or more) objectives are in direct conflict, but otherwise don&#x27;t dominate one another, we simply take them all. Then, the problem becomes one of making sure you have enough resources to maintain the frontier across the generations. If you do have to collapse the frontier, you can use things like crowding score to maintain diversity.<p>Example use case - minimizing total neuron activations in a spiking neural simulation while simultaneously maximizing activation coverage and output correctness scores. You don&#x27;t necessarily know if the cheaper to simulate, but less correct, candidate represents a better evolutionary path until you go down it a few steps.<p>This also gives you an idea of what your horizon looks like on the fitness landscape. If your Pareto front is small, you are typically deep into exploitation. A simple forward path. If it is large, you are putting more resources into exploration, but you aren&#x27;t giving up exploitation - it just takes longer to get through the jungle bits. This can be used as a heuristic to inform restarting or other strategic corrective measures in the evolutionary algorithm. If we are stuck in jungle for too long, we might decide to abandon the run and pick new hyperparameters.</div><br/></div></div><div id="41825498" class="c"><input type="checkbox" id="c-41825498" checked=""/><div class="controls bullet"><span class="by">FailMore</span><span>|</span><a href="#41825910">prev</a><span>|</span><a href="#41824062">next</a><span>|</span><label class="collapse" for="c-41825498">[-]</label><label class="expand" for="c-41825498">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for writing, a problem I struggle with. I think confidence in oneself impacts the decision. You seem to be highly employable, meaning you can err on the side of exploration. I’m less convinced about that for myself, which makes me anxious to keep exploring. Although I find it very difficult to resist that part of my nature!</div><br/></div></div><div id="41824062" class="c"><input type="checkbox" id="c-41824062" checked=""/><div class="controls bullet"><span class="by">nmca</span><span>|</span><a href="#41825498">prev</a><span>|</span><a href="#41824088">next</a><span>|</span><label class="collapse" for="c-41824062">[-]</label><label class="expand" for="c-41824062">[1 more]</label></div><br/><div class="children"><div class="content">A wonderful treatise on the same topic, “Reinforcement Learning Bit by Bit”, for anyone looking for a more advanced treatment of explore&#x2F;exploit.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2103.04047" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2103.04047</a></div><br/></div></div><div id="41824088" class="c"><input type="checkbox" id="c-41824088" checked=""/><div class="controls bullet"><span class="by">matheist</span><span>|</span><a href="#41824062">prev</a><span>|</span><label class="collapse" for="c-41824088">[-]</label><label class="expand" for="c-41824088">[1 more]</label></div><br/><div class="children"><div class="content">See also Thompson sampling[+] for a different approach to multi-armed bandits that doesn&#x27;t depend on explicitly distinguishing between explore-exploit.<p>[+] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thompson_sampling" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thompson_sampling</a></div><br/></div></div></div></div></div></div></div></body></html>