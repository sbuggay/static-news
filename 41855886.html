<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729069267399" as="style"/><link rel="stylesheet" href="styles.css?v=1729069267399"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://geek.sg/blog/how-i-self-hosted-llama-32-with-coolify-on-my-home-server-a-step-by-step-guide">I Self-Hosted Llama 3.2 with Coolify on My Home Server</a> <span class="domain">(<a href="https://geek.sg">geek.sg</a>)</span></div><div class="subtext"><span>whitefables</span> | <span>41 comments</span></div><br/><div><div id="41857017" class="c"><input type="checkbox" id="c-41857017" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#41856567">next</a><span>|</span><label class="collapse" for="c-41857017">[-]</label><label class="expand" for="c-41857017">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I decided to explore self-hosting some of my non-critical applications</i><p>Self-hosting static or almost-static websites is now really easy with a Cloudflare front. I just closed my account on SmugMug and published my images locally using my NAS; this costs no extra money (is basically free) since the photos were already on the NAS, and the NAS is already powered on 24-7.<p>The NAS I use is an Asustor so it&#x27;s not really Linux and you can&#x27;t install what you want on it, but it has Apache, Python and PHP with Sqlite extension, which is more than enough for basic websites.<p>Cloudflare free is like magic. Response times are near instantaneous and setup is minimal. You don&#x27;t even have to configure an SSL certificate locally, it&#x27;s all handled for you and works for wildcard subdomains.<p>And of course if one puts a real server behind it, like in the post, anything&#x27;s possible.</div><br/></div></div><div id="41856567" class="c"><input type="checkbox" id="c-41856567" checked=""/><div class="controls bullet"><span class="by">taosx</span><span>|</span><a href="#41857017">prev</a><span>|</span><a href="#41856729">next</a><span>|</span><label class="collapse" for="c-41856567">[-]</label><label class="expand" for="c-41856567">[12 more]</label></div><br/><div class="children"><div class="content">For the people who self-host LLMs at home: what use cases do you have?<p>Personally, I have some notes and bookmarks that I&#x27;d like to scrape, then have an LLM summarize, generate hierarchical tags, and store in a database. For the notes part at least, I wouldn&#x27;t want to give them to another provider; even for the bookmarks, I wouldn&#x27;t be comfortable passing my reading profile to anyone.</div><br/><div id="41856653" class="c"><input type="checkbox" id="c-41856653" checked=""/><div class="controls bullet"><span class="by">xyc</span><span>|</span><a href="#41856567">parent</a><span>|</span><a href="#41856881">next</a><span>|</span><label class="collapse" for="c-41856653">[-]</label><label class="expand" for="c-41856653">[4 more]</label></div><br/><div class="children"><div class="content">llama3.2 1b &amp; 3b is really useful for quick tasks like creating some quick scripts from some text, then pasting them to execute as it&#x27;s super fast &amp; replaces a lot of temporary automation needs. If you don&#x27;t feel like invest time into automation, sometimes you can just feed into an LLM.<p>This is one of the reason why recently I added floating chat to <a href="https:&#x2F;&#x2F;recurse.chat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;recurse.chat&#x2F;</a> to quickly access local LLM.<p>Here&#x27;s a demo:
<a href="https:&#x2F;&#x2F;x.com&#x2F;recursechat&#x2F;status&#x2F;1846309980091330815" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;recursechat&#x2F;status&#x2F;1846309980091330815</a></div><br/><div id="41856827" class="c"><input type="checkbox" id="c-41856827" checked=""/><div class="controls bullet"><span class="by">taosx</span><span>|</span><a href="#41856567">root</a><span>|</span><a href="#41856653">parent</a><span>|</span><a href="#41856881">next</a><span>|</span><label class="collapse" for="c-41856827">[-]</label><label class="expand" for="c-41856827">[3 more]</label></div><br/><div class="children"><div class="content">Looks very nice, saved it for later. Last week, I worked on implementing always-on speech-to-text functionality for automating tasks. I&#x27;ve made significant progress, achieving decent accuracy, but I imposed some self-imposed constraints to implement certain parts from scratch to deliver a single binary deployable solution, which means I still have work to do (audio processing is new territory for me). However, I&#x27;m optimistic about its potential.<p>That being said, I think the more straightforward approach would be to utilize an existing library like <a href="https:&#x2F;&#x2F;github.com&#x2F;collabora&#x2F;WhisperLive&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;collabora&#x2F;WhisperLive&#x2F;</a> within a Docker container. This way, you can call it via WebSocket and integrate it with my LLM, which could also serve as a nice feature in your product.</div><br/><div id="41856926" class="c"><input type="checkbox" id="c-41856926" checked=""/><div class="controls bullet"><span class="by">xyc</span><span>|</span><a href="#41856567">root</a><span>|</span><a href="#41856827">parent</a><span>|</span><a href="#41856881">next</a><span>|</span><label class="collapse" for="c-41856926">[-]</label><label class="expand" for="c-41856926">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! lmk when&#x2F;if you wanna give it a spin as free trial hasn&#x27;t been updated with the latest but I&#x27;ll try to do it this week.<p>I&#x27;ve actually been playing around with speech to text recently. Thank you for the pointer, docker is a bit too heavy to deploy for desktop app use case but it&#x27;s good to know about the repo. Building binaries with Pyinstaller could be an option though.<p>Real time transcription seems a bit complicated as it involves VAD so a feasible path for me is to first ship simple transcription with whisper.cpp. large-v3-turbo looks fast enough :D</div><br/><div id="41856983" class="c"><input type="checkbox" id="c-41856983" checked=""/><div class="controls bullet"><span class="by">taosx</span><span>|</span><a href="#41856567">root</a><span>|</span><a href="#41856926">parent</a><span>|</span><a href="#41856881">next</a><span>|</span><label class="collapse" for="c-41856983">[-]</label><label class="expand" for="c-41856983">[1 more]</label></div><br/><div class="children"><div class="content">Yes it&#x27;s fast enough, especially if you don&#x27;t need something live.</div><br/></div></div></div></div></div></div></div></div><div id="41856881" class="c"><input type="checkbox" id="c-41856881" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#41856567">parent</a><span>|</span><a href="#41856653">prev</a><span>|</span><a href="#41856970">next</a><span>|</span><label class="collapse" for="c-41856881">[-]</label><label class="expand" for="c-41856881">[2 more]</label></div><br/><div class="children"><div class="content">I keep an 8b running with ollama&#x2F;openwebui to ask it to format things, summarization, and to generate SQL&#x2F;simple bash commands and what not.</div><br/><div id="41856966" class="c"><input type="checkbox" id="c-41856966" checked=""/><div class="controls bullet"><span class="by">worldsayshi</span><span>|</span><a href="#41856567">root</a><span>|</span><a href="#41856881">parent</a><span>|</span><a href="#41856970">next</a><span>|</span><label class="collapse" for="c-41856966">[-]</label><label class="expand" for="c-41856966">[1 more]</label></div><br/><div class="children"><div class="content">So 8b is really smart enough to write scripts for you? How often does it fail?</div><br/></div></div></div></div><div id="41856970" class="c"><input type="checkbox" id="c-41856970" checked=""/><div class="controls bullet"><span class="by">laniakean</span><span>|</span><a href="#41856567">parent</a><span>|</span><a href="#41856881">prev</a><span>|</span><a href="#41856992">next</a><span>|</span><label class="collapse" for="c-41856970">[-]</label><label class="expand" for="c-41856970">[1 more]</label></div><br/><div class="children"><div class="content">I mostly use it to write some quick scripts or generate texts if it follows some pattern. Also, getting it up running with LM studio is pretty straightforward.</div><br/></div></div><div id="41856992" class="c"><input type="checkbox" id="c-41856992" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#41856567">parent</a><span>|</span><a href="#41856970">prev</a><span>|</span><a href="#41856701">next</a><span>|</span><label class="collapse" for="c-41856992">[-]</label><label class="expand" for="c-41856992">[1 more]</label></div><br/><div class="children"><div class="content">I run Mistral Large on 2xA6000. 9 times out of 10 the response is the same quality as GPT 4o. My employer does not allow the use of GPT for privacy related reasons. So I just use a private Mistral for that</div><br/></div></div><div id="41856701" class="c"><input type="checkbox" id="c-41856701" checked=""/><div class="controls bullet"><span class="by">segalord</span><span>|</span><a href="#41856567">parent</a><span>|</span><a href="#41856992">prev</a><span>|</span><a href="#41856729">next</a><span>|</span><label class="collapse" for="c-41856701">[-]</label><label class="expand" for="c-41856701">[3 more]</label></div><br/><div class="children"><div class="content">I use it exclusively for users on my personal website to chat with my data. I&#x27;ve given the setup tools to have read access my files and data</div><br/><div id="41856740" class="c"><input type="checkbox" id="c-41856740" checked=""/><div class="controls bullet"><span class="by">netdevnet</span><span>|</span><a href="#41856567">root</a><span>|</span><a href="#41856701">parent</a><span>|</span><a href="#41856729">next</a><span>|</span><label class="collapse" for="c-41856740">[-]</label><label class="expand" for="c-41856740">[2 more]</label></div><br/><div class="children"><div class="content">Is this not something that you can with non-hosted LLMs like ChatGPT? If you expose your data, it should be able to access it iirc</div><br/><div id="41856978" class="c"><input type="checkbox" id="c-41856978" checked=""/><div class="controls bullet"><span class="by">worldsayshi</span><span>|</span><a href="#41856567">root</a><span>|</span><a href="#41856740">parent</a><span>|</span><a href="#41856729">next</a><span>|</span><label class="collapse" for="c-41856978">[-]</label><label class="expand" for="c-41856978">[1 more]</label></div><br/><div class="children"><div class="content">You can absolutely do that but then you pay by the token instead of a big upfront hardware cost. It feels different I suppose. Sunk cost and all that.</div><br/></div></div></div></div></div></div></div></div><div id="41856729" class="c"><input type="checkbox" id="c-41856729" checked=""/><div class="controls bullet"><span class="by">netdevnet</span><span>|</span><a href="#41856567">prev</a><span>|</span><a href="#41856480">next</a><span>|</span><label class="collapse" for="c-41856729">[-]</label><label class="expand" for="c-41856729">[6 more]</label></div><br/><div class="children"><div class="content">Am I right thinking that a self-hosted llama wouldn&#x27;t have the kind restrictions ChatGPT has since it has no initial system prompt?</div><br/><div id="41856777" class="c"><input type="checkbox" id="c-41856777" checked=""/><div class="controls bullet"><span class="by">dtquad</span><span>|</span><a href="#41856729">parent</a><span>|</span><a href="#41856872">next</a><span>|</span><label class="collapse" for="c-41856777">[-]</label><label class="expand" for="c-41856777">[2 more]</label></div><br/><div class="children"><div class="content">All the self-hosted LLM and text-to-image models come with some restrictions trained into them [1]. However there are plenty of people who have made uncensored &quot;forks&quot; of these models where the restrictions have been &quot;trained away&quot; (mostly by fine-tuning).<p>You can find plenty of uncensored LLM models here:<p><a href="https:&#x2F;&#x2F;ollama.com&#x2F;library">https:&#x2F;&#x2F;ollama.com&#x2F;library</a><p>[1]: I personally suspect that many LLMs are still trained on WebText, derivatives of WebText, or using synthetic data generated by LLMs trained on WebText. This might be why they feel so &quot;censored&quot;:<p>&gt;WebText was generated by scraping only pages linked to by Reddit posts that had received at least three upvotes prior to December 2017. The corpus was subsequently cleaned<p>The implications of so much AI trained on content upvoted by 2015-2017 redditors is not talked about enough.</div><br/><div id="41856879" class="c"><input type="checkbox" id="c-41856879" checked=""/><div class="controls bullet"><span class="by">nubinetwork</span><span>|</span><a href="#41856729">root</a><span>|</span><a href="#41856777">parent</a><span>|</span><a href="#41856872">next</a><span>|</span><label class="collapse" for="c-41856879">[-]</label><label class="expand" for="c-41856879">[1 more]</label></div><br/><div class="children"><div class="content">&gt; All the self-hosted [...] text-to-image models come with some restrictions trained into them<p><a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;diffusers&#x2F;issues&#x2F;3422">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;diffusers&#x2F;issues&#x2F;3422</a></div><br/></div></div></div></div><div id="41856872" class="c"><input type="checkbox" id="c-41856872" checked=""/><div class="controls bullet"><span class="by">nubinetwork</span><span>|</span><a href="#41856729">parent</a><span>|</span><a href="#41856777">prev</a><span>|</span><a href="#41856734">next</a><span>|</span><label class="collapse" for="c-41856872">[-]</label><label class="expand" for="c-41856872">[1 more]</label></div><br/><div class="children"><div class="content">That depends on the frontend, you can supply a system prompt if you want to... whether it follows it to the letter is another problem...</div><br/></div></div><div id="41856734" class="c"><input type="checkbox" id="c-41856734" checked=""/><div class="controls bullet"><span class="by">Kudos</span><span>|</span><a href="#41856729">parent</a><span>|</span><a href="#41856872">prev</a><span>|</span><a href="#41856779">next</a><span>|</span><label class="collapse" for="c-41856734">[-]</label><label class="expand" for="c-41856734">[1 more]</label></div><br/><div class="children"><div class="content">Many protections are baked into the models themselves.</div><br/></div></div><div id="41856779" class="c"><input type="checkbox" id="c-41856779" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41856729">parent</a><span>|</span><a href="#41856734">prev</a><span>|</span><a href="#41856480">next</a><span>|</span><label class="collapse" for="c-41856779">[-]</label><label class="expand" for="c-41856779">[1 more]</label></div><br/><div class="children"><div class="content">It has a sanitised output. You might want to look for &quot;abliterated&quot; models, where the general performance might drop a bit but the guard-rails have been diminished.</div><br/></div></div></div></div><div id="41856480" class="c"><input type="checkbox" id="c-41856480" checked=""/><div class="controls bullet"><span class="by">varun_ch</span><span>|</span><a href="#41856729">prev</a><span>|</span><a href="#41856737">next</a><span>|</span><label class="collapse" for="c-41856480">[-]</label><label class="expand" for="c-41856480">[10 more]</label></div><br/><div class="children"><div class="content">I’m curious about how good the performance with local LLMs is on ‘outdated’ hardware like the author’s 2060. I have a desktop with a 2070 super that it could be fun to turn into an “AI server” if I had the time…</div><br/><div id="41856609" class="c"><input type="checkbox" id="c-41856609" checked=""/><div class="controls bullet"><span class="by">khafra</span><span>|</span><a href="#41856480">parent</a><span>|</span><a href="#41856558">next</a><span>|</span><label class="collapse" for="c-41856609">[-]</label><label class="expand" for="c-41856609">[2 more]</label></div><br/><div class="children"><div class="content">If you want to set up an AI server for your own use, it&#x27;s exceedingly easy to install LM Studio and hit the &quot;serve an API&quot; button.<p>Testing performance this way, I got about 0.5-1.5 tokens per second with an 8GB 4bit quantized model on an old DL360 rack-mount server with 192GB RAM and 2 E5-2670 CPUs. I got about 20-50 tokens per second on my laptop with a mobile RTX 4080.</div><br/><div id="41856694" class="c"><input type="checkbox" id="c-41856694" checked=""/><div class="controls bullet"><span class="by">taosx</span><span>|</span><a href="#41856480">root</a><span>|</span><a href="#41856609">parent</a><span>|</span><a href="#41856558">next</a><span>|</span><label class="collapse" for="c-41856694">[-]</label><label class="expand" for="c-41856694">[1 more]</label></div><br/><div class="children"><div class="content">LM studio is so nice, I&#x27;m up and running in 5 minutes. ty</div><br/></div></div></div></div><div id="41856558" class="c"><input type="checkbox" id="c-41856558" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41856480">parent</a><span>|</span><a href="#41856609">prev</a><span>|</span><a href="#41856875">next</a><span>|</span><label class="collapse" for="c-41856558">[-]</label><label class="expand" for="c-41856558">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been playing with some LLMs like Llama 3 and Gemma on my 2080Ti. If it fits in GPU memory the inference speed is quite decent.<p>However I&#x27;ve found quality of smaller models to be quite lacking. The Llama 3.2 3B for example is much worse than Gemma2 9B, which is the one I found performs best while fitting comfortably.<p>Actual sentences are fine, but it doesn&#x27;t follow prompts as well and it doesn&#x27;t &quot;understand&quot; the context very well.<p>Quantization brings down memory cost, but there seems to be a sharp decline below 5 bits for those I tried. So a larger but heavily quantized model usually performs worse, at least with the models I&#x27;ve tried so far.<p>So with only 6GB of GPU memory I think you either have to accept the hit on inference speed by only partially offloading, or accept fairly low model quality.<p>Doesn&#x27;t mean the smaller models can&#x27;t be useful, but don&#x27;t expect ChatGPT 4o at home.<p>That said if you got a beefy CPU then it can be reasonable to have it do a few of the layers.<p>Personally I found Gemma2 9B quantized to 6 bit IIRC to be quite useful. YMMV.</div><br/></div></div><div id="41856875" class="c"><input type="checkbox" id="c-41856875" checked=""/><div class="controls bullet"><span class="by">dtquad</span><span>|</span><a href="#41856480">parent</a><span>|</span><a href="#41856558">prev</a><span>|</span><a href="#41856894">next</a><span>|</span><label class="collapse" for="c-41856875">[-]</label><label class="expand" for="c-41856875">[1 more]</label></div><br/><div class="children"><div class="content">I am using an old laptop with a GTX 1060 6 GB VRAM to run a home server with Ubuntu and Ollama. Because of quantization Ollama can run 7B&#x2F;8B models on an 8 year old laptop GPU with 6 GB VRAM.</div><br/></div></div><div id="41856894" class="c"><input type="checkbox" id="c-41856894" checked=""/><div class="controls bullet"><span class="by">nubinetwork</span><span>|</span><a href="#41856480">parent</a><span>|</span><a href="#41856875">prev</a><span>|</span><a href="#41856559">next</a><span>|</span><label class="collapse" for="c-41856894">[-]</label><label class="expand" for="c-41856894">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m happy with a Radeon VII, unless the model is bigger than 16gb...</div><br/></div></div><div id="41856559" class="c"><input type="checkbox" id="c-41856559" checked=""/><div class="controls bullet"><span class="by">whitefables</span><span>|</span><a href="#41856480">parent</a><span>|</span><a href="#41856894">prev</a><span>|</span><a href="#41856521">next</a><span>|</span><label class="collapse" for="c-41856559">[-]</label><label class="expand" for="c-41856559">[3 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s how it looks like in real time: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;3vhJ6fNW8AI" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;3vhJ6fNW8AI</a></div><br/><div id="41856601" class="c"><input type="checkbox" id="c-41856601" checked=""/><div class="controls bullet"><span class="by">thisguyagain</span><span>|</span><a href="#41856480">root</a><span>|</span><a href="#41856559">parent</a><span>|</span><a href="#41856521">next</a><span>|</span><label class="collapse" for="c-41856601">[-]</label><label class="expand" for="c-41856601">[2 more]</label></div><br/><div class="children"><div class="content">What’d you use to record that? Looks really great.</div><br/><div id="41856691" class="c"><input type="checkbox" id="c-41856691" checked=""/><div class="controls bullet"><span class="by">whitefables</span><span>|</span><a href="#41856480">root</a><span>|</span><a href="#41856601">parent</a><span>|</span><a href="#41856521">next</a><span>|</span><label class="collapse" for="c-41856691">[-]</label><label class="expand" for="c-41856691">[1 more]</label></div><br/><div class="children"><div class="content">Screen studio</div><br/></div></div></div></div></div></div><div id="41856521" class="c"><input type="checkbox" id="c-41856521" checked=""/><div class="controls bullet"><span class="by">taosx</span><span>|</span><a href="#41856480">parent</a><span>|</span><a href="#41856559">prev</a><span>|</span><a href="#41856737">next</a><span>|</span><label class="collapse" for="c-41856521">[-]</label><label class="expand" for="c-41856521">[1 more]</label></div><br/><div class="children"><div class="content">Last time I tried a local llm was about a year ago with a 2070S and 3950x and the performance was quite slow for anything beyond phi 3.5 and the small models quality feels worse than what some providers offer for cheap or free so it doesn&#x27;t seem worth it with my current hardware.<p>Edit: I&#x27;ve loaded llama 3.1 8b instruct GGUF and I got 12.61 tok&#x2F;sec and 80tok&#x2F;sec for 3.2 3b.</div><br/></div></div></div></div><div id="41856737" class="c"><input type="checkbox" id="c-41856737" checked=""/><div class="controls bullet"><span class="by">ragebol</span><span>|</span><a href="#41856480">prev</a><span>|</span><a href="#41856395">next</a><span>|</span><label class="collapse" for="c-41856737">[-]</label><label class="expand" for="c-41856737">[4 more]</label></div><br/><div class="children"><div class="content">Probably saves a bit on the gas bill for heating too</div><br/><div id="41856987" class="c"><input type="checkbox" id="c-41856987" checked=""/><div class="controls bullet"><span class="by">CraigJPerry</span><span>|</span><a href="#41856737">parent</a><span>|</span><a href="#41856994">next</a><span>|</span><label class="collapse" for="c-41856987">[-]</label><label class="expand" for="c-41856987">[1 more]</label></div><br/><div class="children"><div class="content">I don’t know, it’s kind of amazing how good the lighter weight self hosted models are now.<p>Given a 16gb system with cpu inference only, I’m hosting gemma2 9b at q8 for llm tasks and SDXL turbo for image work and besides the memory usage creeping up for a second or so while i invoke a prompt, they’re basically undetectable in the background.</div><br/></div></div><div id="41856994" class="c"><input type="checkbox" id="c-41856994" checked=""/><div class="controls bullet"><span class="by">rglullis</span><span>|</span><a href="#41856737">parent</a><span>|</span><a href="#41856987">prev</a><span>|</span><a href="#41856985">next</a><span>|</span><label class="collapse" for="c-41856994">[-]</label><label class="expand" for="c-41856994">[1 more]</label></div><br/><div class="children"><div class="content">Snark aside, even in Germany (where electricity is very expensive) it is more economical to self host than to pay for a subscription to any of the commercial providers.</div><br/></div></div><div id="41856985" class="c"><input type="checkbox" id="c-41856985" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#41856737">parent</a><span>|</span><a href="#41856994">prev</a><span>|</span><a href="#41856395">next</a><span>|</span><label class="collapse" for="c-41856985">[-]</label><label class="expand" for="c-41856985">[1 more]</label></div><br/><div class="children"><div class="content">If only we had heat-pump computers</div><br/></div></div></div></div><div id="41856395" class="c"><input type="checkbox" id="c-41856395" checked=""/><div class="controls bullet"><span class="by">seungwoolee518</span><span>|</span><a href="#41856737">prev</a><span>|</span><a href="#41856662">next</a><span>|</span><label class="collapse" for="c-41856395">[-]</label><label class="expand" for="c-41856395">[2 more]</label></div><br/><div class="children"><div class="content">Great post!<p>However, Do I need to Install CUDA toolkit on host?<p>I haven&#x27;t install CUDA toolkit when I use on Containerized platform (like docker)</div><br/><div id="41856420" class="c"><input type="checkbox" id="c-41856420" checked=""/><div class="controls bullet"><span class="by">thangngoc89</span><span>|</span><a href="#41856395">parent</a><span>|</span><a href="#41856662">next</a><span>|</span><label class="collapse" for="c-41856420">[-]</label><label class="expand" for="c-41856420">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need to install CUDA toolkit on host system.<p>Nvidia driver + Nvidia container toolkit would do the job. You could check official instructions at [0]<p>[0] <a href="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;datacenter&#x2F;cloud-native&#x2F;container-toolkit&#x2F;latest&#x2F;install-guide.html" rel="nofollow">https:&#x2F;&#x2F;docs.nvidia.com&#x2F;datacenter&#x2F;cloud-native&#x2F;container-to...</a></div><br/></div></div></div></div><div id="41856662" class="c"><input type="checkbox" id="c-41856662" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#41856395">prev</a><span>|</span><a href="#41856491">next</a><span>|</span><label class="collapse" for="c-41856662">[-]</label><label class="expand" for="c-41856662">[3 more]</label></div><br/><div class="children"><div class="content">I love Coolify, used to use v3, anyone know how their v4 is going? I thought it was still a beta release from what I saw on GitHub.</div><br/><div id="41856812" class="c"><input type="checkbox" id="c-41856812" checked=""/><div class="controls bullet"><span class="by">j12a</span><span>|</span><a href="#41856662">parent</a><span>|</span><a href="#41856754">next</a><span>|</span><label class="collapse" for="c-41856812">[-]</label><label class="expand" for="c-41856812">[1 more]</label></div><br/><div class="children"><div class="content">Coolify is quite nice, have been running some things with the v4 beta.<p>It reminds a bit of making web sites with a page builder. Easy to install and click around to get something running without thinking too much about it fairly quickly.<p>Problems are quite similar also, training wheels getting stuck in the woods more easily, hehe.</div><br/></div></div><div id="41856754" class="c"><input type="checkbox" id="c-41856754" checked=""/><div class="controls bullet"><span class="by">whitefables</span><span>|</span><a href="#41856662">parent</a><span>|</span><a href="#41856812">prev</a><span>|</span><a href="#41856491">next</a><span>|</span><label class="collapse" for="c-41856754">[-]</label><label class="expand" for="c-41856754">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using v4 beta in the blog post. Didn&#x27;t try v3 so there&#x27;s no point of comparison but I&#x27;m loving it so far!<p>It was so easy to get other non-AI stuffs running!</div><br/></div></div></div></div><div id="41856491" class="c"><input type="checkbox" id="c-41856491" checked=""/><div class="controls bullet"><span class="by">keriati1</span><span>|</span><a href="#41856662">prev</a><span>|</span><a href="#41856618">next</a><span>|</span><label class="collapse" for="c-41856491">[-]</label><label class="expand" for="c-41856491">[1 more]</label></div><br/><div class="children"><div class="content">What model size is used here?
How much memory does the GPU have?</div><br/></div></div><div id="41856618" class="c"><input type="checkbox" id="c-41856618" checked=""/><div class="controls bullet"><span class="by">_blk</span><span>|</span><a href="#41856491">prev</a><span>|</span><label class="collapse" for="c-41856618">[-]</label><label class="expand" for="c-41856618">[1 more]</label></div><br/><div class="children"><div class="content">Why disable LVM for a smoother reboot experience? For encryption I get it since you need a key to mount, but all my setups have LVM or ZFS and I&#x27;d say my reboots are smooth enough.</div><br/></div></div></div></div></div></div></div></body></html>