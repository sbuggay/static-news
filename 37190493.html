<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692522055730" as="style"/><link rel="stylesheet" href="styles.css?v=1692522055730"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/openzfs/zfs/pull/15022">OpenZFS – add disks to existing RAIDZ</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>shrubble</span> | <span>120 comments</span></div><br/><div><div id="37192101" class="c"><input type="checkbox" id="c-37192101" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#37191398">next</a><span>|</span><label class="collapse" for="c-37192101">[-]</label><label class="expand" for="c-37192101">[24 more]</label></div><br/><div class="children"><div class="content">I wish Apple and Oracle would have just sorted things out and made ZFS the main filesystem for the Mac.  Way back in the day when they first designed Time Machine, it was supposed to just be a GUI for ZFS snapshots.<p>How cool would it be if we had a great GUI for ZFS (snapshots, volume management, etc.).  I could buy a new external disk, add it to a pool, have seamless storage expansion.<p>It would be great.  Ah, what could have been.</div><br/><div id="37193208" class="c"><input type="checkbox" id="c-37193208" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37193774">next</a><span>|</span><label class="collapse" for="c-37193208">[-]</label><label class="expand" for="c-37193208">[2 more]</label></div><br/><div class="children"><div class="content">&gt; How cool would it be if we had a great GUI for ZFS (snapshots, volume management, etc.).<p>How cool would it be if we had a great <i>TUI</i> for ZFS...<p>Live in the now: <a href="https:&#x2F;&#x2F;github.com&#x2F;kimono-koans&#x2F;httm">https:&#x2F;&#x2F;github.com&#x2F;kimono-koans&#x2F;httm</a></div><br/><div id="37197108" class="c"><input type="checkbox" id="c-37197108" checked=""/><div class="controls bullet"><span class="by">SpecialistK</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37193208">parent</a><span>|</span><a href="#37193774">next</a><span>|</span><label class="collapse" for="c-37197108">[-]</label><label class="expand" for="c-37197108">[1 more]</label></div><br/><div class="children"><div class="content">Now that&#x27;s a smooth segue to self-promo. I respect it.</div><br/></div></div></div></div><div id="37193774" class="c"><input type="checkbox" id="c-37193774" checked=""/><div class="controls bullet"><span class="by">xoa</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37193208">prev</a><span>|</span><a href="#37193941">next</a><span>|</span><label class="collapse" for="c-37193774">[-]</label><label class="expand" for="c-37193774">[2 more]</label></div><br/><div class="children"><div class="content">Yes, this will always depress me, and to me personally be one of the ultimate evils of the court invented idea of &quot;software patents&quot;. I&#x27;ve used ZFS with Macs sicne 2011, but without Apple onboard it&#x27;s never been as smooth as it should have been and has gotten more difficult in some respects. There was a small window where we might have had a really universal, really solid FS with great data guarantees and features. All sorts of things with OS could be so much better, trivial (and even automated) update rollbacks for example. Sigh :(. I hope zvols finally someday get some performance attention so that if nothing else running APFS (or any other OS of course) on top of one is a better experience.</div><br/><div id="37197304" class="c"><input type="checkbox" id="c-37197304" checked=""/><div class="controls bullet"><span class="by">gjvc</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37193774">parent</a><span>|</span><a href="#37193941">next</a><span>|</span><label class="collapse" for="c-37197304">[-]</label><label class="expand" for="c-37197304">[1 more]</label></div><br/><div class="children"><div class="content">1000x this.  A universal filesystem for UNIX would have been a wonderful thing, and would have been the best legacy Sun could have left.</div><br/></div></div></div></div><div id="37193941" class="c"><input type="checkbox" id="c-37193941" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37193774">prev</a><span>|</span><a href="#37195106">next</a><span>|</span><label class="collapse" for="c-37193941">[-]</label><label class="expand" for="c-37193941">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>How cool would it be if we had a great GUI for ZFS (snapshots, volume management, etc.). I could buy a new external disk, add it to a pool, have seamless storage expansion.</i><p>See QNAP HERO 5:<p><a href="https:&#x2F;&#x2F;www.qnap.com&#x2F;static&#x2F;landing&#x2F;2021&#x2F;quts-hero-5.0&#x2F;en&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.qnap.com&#x2F;static&#x2F;landing&#x2F;2021&#x2F;quts-hero-5.0&#x2F;en&#x2F;in...</a><p>NAS Options:<p><a href="https:&#x2F;&#x2F;www.qnap.com&#x2F;en-us&#x2F;product&#x2F;?conditions=4-3" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.qnap.com&#x2F;en-us&#x2F;product&#x2F;?conditions=4-3</a><p>&#x2F;&#x2F; This was about MacOS, and yes, that would be cool.  But QNAP is remarkably MacOS-friendly, including Thunderbolt and Time Machine support.</div><br/><div id="37195833" class="c"><input type="checkbox" id="c-37195833" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37193941">parent</a><span>|</span><a href="#37195500">next</a><span>|</span><label class="collapse" for="c-37195833">[-]</label><label class="expand" for="c-37195833">[1 more]</label></div><br/><div class="children"><div class="content">Worth noting QNAP&#x27;s pools are 500% not ever going to be importable on stock OpenZFS.<p>If you&#x27;re happily using their devices, this may not matter to you, but since the post is about OpenZFS...</div><br/></div></div><div id="37195500" class="c"><input type="checkbox" id="c-37195500" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37193941">parent</a><span>|</span><a href="#37195833">prev</a><span>|</span><a href="#37193996">next</a><span>|</span><label class="collapse" for="c-37195500">[-]</label><label class="expand" for="c-37195500">[1 more]</label></div><br/><div class="children"><div class="content">If you follow Qnap you may be aware of their security nightmare in recent years.<p>And HERO OS is only available on the Enterprise range of NAS, not even Prosumer class NAS.</div><br/></div></div><div id="37193996" class="c"><input type="checkbox" id="c-37193996" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37193941">parent</a><span>|</span><a href="#37195500">prev</a><span>|</span><a href="#37195106">next</a><span>|</span><label class="collapse" for="c-37193996">[-]</label><label class="expand" for="c-37193996">[1 more]</label></div><br/><div class="children"><div class="content">As far as I can tell I can&#x27;t use that as my root filesystem, right?</div><br/></div></div></div></div><div id="37195106" class="c"><input type="checkbox" id="c-37195106" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37193941">prev</a><span>|</span><a href="#37194067">next</a><span>|</span><label class="collapse" for="c-37195106">[-]</label><label class="expand" for="c-37195106">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I wonder if it was really caused by Jonathan Schwartz bragging to the media about it and pissing Steve Jobs off, as was rumoured at the time. Or if there was more behind it.<p>I would certainly not put it past Jobs. While he had some genius qualities, his personality was pretty deeply flawed. But on the other hand I wonder if his sense for business would not have prevailed.<p>If only Apple had absorbed sun instead of oracle, their legacy would have been better off. Of course it couldn&#x27;t have been worse.<p>Which also made me wonder, of course Jobs and Ellison were big friends so perhaps Jobs just left it for him to take? Oracle always wanted to get their hands on java obviously.<p>Ps the whole ZFS on Mac adventure came a little after time machine was already in production.<p>ZFS is indeed an amazing desktop filesystem. I&#x27;m using it now on FreeBSD, I moved away from Mac because it became ever more closed down. I wanted more control, not less.</div><br/><div id="37196326" class="c"><input type="checkbox" id="c-37196326" checked=""/><div class="controls bullet"><span class="by">mekster</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37195106">parent</a><span>|</span><a href="#37194067">next</a><span>|</span><label class="collapse" for="c-37196326">[-]</label><label class="expand" for="c-37196326">[1 more]</label></div><br/><div class="children"><div class="content">Not sure why you limit that to desktop. It&#x27;s more important that server files can have an instant snapshot, compression and performant incremental backup to remote servers.</div><br/></div></div></div></div><div id="37194067" class="c"><input type="checkbox" id="c-37194067" checked=""/><div class="controls bullet"><span class="by">phs318u</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37195106">prev</a><span>|</span><a href="#37195179">next</a><span>|</span><label class="collapse" for="c-37194067">[-]</label><label class="expand" for="c-37194067">[1 more]</label></div><br/><div class="children"><div class="content">I had the same thoughts about Apple and (at the time) Sun, 15 years ago.<p><a href="https:&#x2F;&#x2F;macoverdrive.blogspot.com&#x2F;2008&#x2F;10&#x2F;using-zfs-to-manage-your-vm-zoo.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;macoverdrive.blogspot.com&#x2F;2008&#x2F;10&#x2F;using-zfs-to-manag...</a></div><br/></div></div><div id="37195179" class="c"><input type="checkbox" id="c-37195179" checked=""/><div class="controls bullet"><span class="by">postmodest</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37194067">prev</a><span>|</span><a href="#37195555">next</a><span>|</span><label class="collapse" for="c-37195179">[-]</label><label class="expand" for="c-37195179">[2 more]</label></div><br/><div class="children"><div class="content">Apple wanted one FS across all devices and ZFS&#x27;s ARC makes that less than useful in memory-starved phones (and base model Airs amirite?)<p>ZFS&#x27;s separate cache meant that the better choice was APFS. Once they got around to writing it.<p>And on drives at least, TM backups ARE APFS snapshots.</div><br/><div id="37196060" class="c"><input type="checkbox" id="c-37196060" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37195179">parent</a><span>|</span><a href="#37195555">next</a><span>|</span><label class="collapse" for="c-37196060">[-]</label><label class="expand" for="c-37196060">[1 more]</label></div><br/><div class="children"><div class="content">Basically every OS has a file cache and you can tune the ARC how you please</div><br/></div></div></div></div><div id="37195555" class="c"><input type="checkbox" id="c-37195555" checked=""/><div class="controls bullet"><span class="by">nortonham</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37195179">prev</a><span>|</span><a href="#37192310">next</a><span>|</span><label class="collapse" for="c-37195555">[-]</label><label class="expand" for="c-37195555">[1 more]</label></div><br/><div class="children"><div class="content">&gt;How cool would it be if we had a great GUI for ZFS (snapshots, volume management, etc.)<p>I believe that was something built into the file manager in OpenSolaris, and then into illumos OS&#x27;s. OpenIndiana has it in their file manager within their themed MATE desktop environment.</div><br/></div></div><div id="37192310" class="c"><input type="checkbox" id="c-37192310" checked=""/><div class="controls bullet"><span class="by">risho</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37195555">prev</a><span>|</span><a href="#37195113">next</a><span>|</span><label class="collapse" for="c-37192310">[-]</label><label class="expand" for="c-37192310">[3 more]</label></div><br/><div class="children"><div class="content">yeah imagine a world where you could use time machine to backup 500gb parallels volumes where only the diff was stored between snapshots rather than needing to back up the whole 500gb volume every single time.</div><br/><div id="37192335" class="c"><input type="checkbox" id="c-37192335" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37192310">parent</a><span>|</span><a href="#37195113">next</a><span>|</span><label class="collapse" for="c-37192335">[-]</label><label class="expand" for="c-37192335">[2 more]</label></div><br/><div class="children"><div class="content">Right, that would be nice wouldn&#x27;t it?<p>As a workaround, you can create a sparsevolume to store your parallels volume.  Sparsevolumes are stores in bands, and only bands that change get backed up. It might be slightly more efficient.</div><br/><div id="37192439" class="c"><input type="checkbox" id="c-37192439" checked=""/><div class="controls bullet"><span class="by">risho</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37192335">parent</a><span>|</span><a href="#37195113">next</a><span>|</span><label class="collapse" for="c-37192439">[-]</label><label class="expand" for="c-37192439">[1 more]</label></div><br/><div class="children"><div class="content">wow that sounds like an interesting solution!</div><br/></div></div></div></div></div></div><div id="37195113" class="c"><input type="checkbox" id="c-37195113" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37192310">prev</a><span>|</span><a href="#37193910">next</a><span>|</span><label class="collapse" for="c-37195113">[-]</label><label class="expand" for="c-37195113">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I could buy a new external disk, add it to a pool, have seamless storage expansion.</i><p>You still can&#x27;t do that with ZFS though, that&#x27;s what this PR is for.</div><br/><div id="37195158" class="c"><input type="checkbox" id="c-37195158" checked=""/><div class="controls bullet"><span class="by">postmodest</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37195113">parent</a><span>|</span><a href="#37193910">next</a><span>|</span><label class="collapse" for="c-37195158">[-]</label><label class="expand" for="c-37195158">[3 more]</label></div><br/><div class="children"><div class="content">You can add devices as vdevs to a pool, though. You can&#x27;t add disks to a vdev. (IIRC) It&#x27;s no different to LVM in that aspect.</div><br/><div id="37196759" class="c"><input type="checkbox" id="c-37196759" checked=""/><div class="controls bullet"><span class="by">Lex-2008</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37195158">parent</a><span>|</span><a href="#37193910">next</a><span>|</span><label class="collapse" for="c-37196759">[-]</label><label class="expand" for="c-37196759">[2 more]</label></div><br/><div class="children"><div class="content">IIRC, the pool is effectively RAID0 over vdevs: if any single vdev dies - whole pool dies. Do i remember this correctly?<p>So if over the years I got, say, five disks of varying quality - I shouldn&#x27;t add them to a pool as five individual vdevs :)</div><br/><div id="37197203" class="c"><input type="checkbox" id="c-37197203" checked=""/><div class="controls bullet"><span class="by">calcifer</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37196759">parent</a><span>|</span><a href="#37193910">next</a><span>|</span><label class="collapse" for="c-37197203">[-]</label><label class="expand" for="c-37197203">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if any single vdev dies - whole pool dies. Do i remember this correctly?<p>Yes, that&#x27;s why you don&#x27;t have single-disk vdevs.</div><br/></div></div></div></div></div></div></div></div><div id="37193910" class="c"><input type="checkbox" id="c-37193910" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#37192101">parent</a><span>|</span><a href="#37195113">prev</a><span>|</span><a href="#37191398">next</a><span>|</span><label class="collapse" for="c-37193910">[-]</label><label class="expand" for="c-37193910">[2 more]</label></div><br/><div class="children"><div class="content">TrueNAS? You can manage the volumes with a web UI. It has a thing you can enable for SMB shares that let you see the previous versions of files on Windows. Or perhaps I don&#x27;t understand what you&#x27;re after.</div><br/><div id="37194001" class="c"><input type="checkbox" id="c-37194001" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#37192101">root</a><span>|</span><a href="#37193910">parent</a><span>|</span><a href="#37191398">next</a><span>|</span><label class="collapse" for="c-37194001">[-]</label><label class="expand" for="c-37194001">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m looking for that kind of featureset for my root filesystem on MacOs.  :&#x2F;</div><br/></div></div></div></div></div></div><div id="37191398" class="c"><input type="checkbox" id="c-37191398" checked=""/><div class="controls bullet"><span class="by">eminence32</span><span>|</span><a href="#37192101">prev</a><span>|</span><a href="#37192076">next</a><span>|</span><label class="collapse" for="c-37191398">[-]</label><label class="expand" for="c-37191398">[4 more]</label></div><br/><div class="children"><div class="content">The big news here seems to be that iXsystems (the company behind FreeNAS&#x2F;TrueNAS) is sponsoring this work now.  This PR supersedes ones that was opened back in 2021<p><a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;12225#issuecomment-1610169213">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;pull&#x2F;12225#issuecomment-16101...</a></div><br/><div id="37195184" class="c"><input type="checkbox" id="c-37195184" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#37191398">parent</a><span>|</span><a href="#37191567">next</a><span>|</span><label class="collapse" for="c-37195184">[-]</label><label class="expand" for="c-37195184">[2 more]</label></div><br/><div class="children"><div class="content">I hope this corporate sponsorship won&#x27;t go the way that wireguard went. <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2021&#x2F;03&#x2F;buffer-overruns-license-violations-and-bad-code-freebsd-13s-close-call&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2021&#x2F;03&#x2F;buffer-overruns-lice...</a><p>What worries me the most is them doubling down on it after it was shown to be a mess and totally insecure. I imagine to protect the corporate image. But such corporate interests don&#x27;t belong in FreeBSD. Just admit you hired the wrong guy (or the right guy at the wrong time) and take the hit. Because it wasn&#x27;t even the company&#x27;s fault really.<p>The lack of serious review on the freebsd side was also a big one but I find the &#x27;strings attached&#x27; the biggest issue myself because it made mitigation of serious issues such a problem.<p>The large amount of corporate kernel work in Linux is why I went for FreeBSD and to see whatever little go so wrong was worrying.</div><br/><div id="37197158" class="c"><input type="checkbox" id="c-37197158" checked=""/><div class="controls bullet"><span class="by">SpecialistK</span><span>|</span><a href="#37191398">root</a><span>|</span><a href="#37195184">parent</a><span>|</span><a href="#37191567">next</a><span>|</span><label class="collapse" for="c-37197158">[-]</label><label class="expand" for="c-37197158">[1 more]</label></div><br/><div class="children"><div class="content">iXsystems are an entirely different beast than Netgate (pfSense).<p>One can trace its legacy to BSD while it was still at Berkley and has worked with many of the most respected names in the BSD space.<p>The other bought a domain name of a fork and used it to post disparaging messages and Hitler &quot;Downfall&quot; memes slandering them. Source: <a href="https:&#x2F;&#x2F;www.wipo.int&#x2F;amc&#x2F;en&#x2F;domains&#x2F;search&#x2F;text.jsp?case=D2017-1828" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.wipo.int&#x2F;amc&#x2F;en&#x2F;domains&#x2F;search&#x2F;text.jsp?case=D20...</a><p>While I have some disagreements with iXsystems&#x27; pivot to ZFS-on-Linux offerings and the like, they&#x27;re not the clowns that Netgate &#x2F; pfSense are. 10cm to my right are TrueNAS and opnSense boxes - you won&#x27;t catch me dead using pfSense in my network after the crap they&#x27;ve pulled.</div><br/></div></div></div></div><div id="37191567" class="c"><input type="checkbox" id="c-37191567" checked=""/><div class="controls bullet"><span class="by">seltzered_</span><span>|</span><a href="#37191398">parent</a><span>|</span><a href="#37195184">prev</a><span>|</span><a href="#37192076">next</a><span>|</span><label class="collapse" for="c-37191567">[-]</label><label class="expand" for="c-37191567">[1 more]</label></div><br/><div class="children"><div class="content">Yep, see also <a href="https:&#x2F;&#x2F;freebsdfoundation.org&#x2F;blog&#x2F;raid-z-expansion-feature-for-zfs&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;freebsdfoundation.org&#x2F;blog&#x2F;raid-z-expansion-feature-...</a> (2022)<p>(Via 
 <a href="https:&#x2F;&#x2F;lobste.rs&#x2F;s&#x2F;5ahxj1&#x2F;raid_z_expansion_feature_for_zfs_goes_live" rel="nofollow noreferrer">https:&#x2F;&#x2F;lobste.rs&#x2F;s&#x2F;5ahxj1&#x2F;raid_z_expansion_feature_for_zfs_...</a> )</div><br/></div></div></div></div><div id="37192076" class="c"><input type="checkbox" id="c-37192076" checked=""/><div class="controls bullet"><span class="by">crote</span><span>|</span><a href="#37191398">prev</a><span>|</span><a href="#37190494">next</a><span>|</span><label class="collapse" for="c-37192076">[-]</label><label class="expand" for="c-37192076">[12 more]</label></div><br/><div class="children"><div class="content">&gt; After the expansion completes, old blocks remain with their old data-to-parity ratio (e.g. 5-wide RAIDZ2, has 3 data to 2 parity), but distributed among the larger set of disks. New blocks will be written with the new data-to-parity
ratio (e.g. a 5-wide RAIDZ2 which has been expanded once to 6-wide, has 4 data to 2 parity).<p>Does anyone know why this is the case? When expanding an array which is getting full this will result in a far smaller capacity gain than desired.<p>Let&#x27;s assume we are using 5x 10TB disks which are 90% full. Before the process, each disk will contain 5.4TB of data, 3.6TB of parity, and 1TB of free space. After the process and converting it to 6x 10TB, each disk will contain 4.5TB of data, 3TB of parity, and 2.5TB of free space. We can fill this free space with 1.66TB of data and 0.83TB of parity per disk - after which our entire array will contain 36.96TB of data.<p>If we made a new 6-wide Z2 array, it would be able to contain 40TB of data - so adding a disk this way made us lose over 3TB in capacity! Considering the process is already reading and rewriting basically the entire array, why not recalculate the parity as well?</div><br/><div id="37194166" class="c"><input type="checkbox" id="c-37194166" checked=""/><div class="controls bullet"><span class="by">allanjude</span><span>|</span><a href="#37192076">parent</a><span>|</span><a href="#37192697">next</a><span>|</span><label class="collapse" for="c-37194166">[-]</label><label class="expand" for="c-37194166">[5 more]</label></div><br/><div class="children"><div class="content">That is not how this will work.<p>The reason the parity ratio stays the same, is that all of the references to the data are by DVA (Data Virtual Address, effectively the LBA within the RAID-Z vdev).<p>So the data will occupy the same amount of space and parity as it did before.<p>All stripes in RAID-Z are dynamic, so if your stripe is 5 wide and your array is 6 wide, the 2nd stripe will start on the last disk and wrap around.<p>So if your 5x10 TB disks are 90% full, after the expansion they will contain the same 5.4 TB of data and 3.6 TB of parity, and the pool will now be 10 TB bigger.<p>New writes, will be 4+2 instead, but the old data won&#x27;t change (they is how this feature is able to work without needing block-pointer rewrite).<p>See this presentation: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=yF2KgQGmUic">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=yF2KgQGmUic</a></div><br/><div id="37194900" class="c"><input type="checkbox" id="c-37194900" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#37192076">root</a><span>|</span><a href="#37194166">parent</a><span>|</span><a href="#37195148">next</a><span>|</span><label class="collapse" for="c-37194900">[-]</label><label class="expand" for="c-37194900">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So the data will occupy the same amount of space and parity as it did before.<p>So you lose data capacity compared to &quot;dumb&quot; RAID6 on mdadm.<p>If you expand RAID6 from 4+2 to 5+2, you go from using 33.3% data for parity to 28.5% on parity<p>If you expand RAIDZ from 4+2 to 5+2, your new data will use 28.5%, but your old (which is majority, because if it wasn&#x27;t you wouldn&#x27;t be expanding) would still use 33.3% on parity.</div><br/></div></div><div id="37195148" class="c"><input type="checkbox" id="c-37195148" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#37192076">root</a><span>|</span><a href="#37194166">parent</a><span>|</span><a href="#37194900">prev</a><span>|</span><a href="#37192697">next</a><span>|</span><label class="collapse" for="c-37195148">[-]</label><label class="expand" for="c-37195148">[3 more]</label></div><br/><div class="children"><div class="content">Could you force a complete rewrite if you wanted to? That would be handy. Without copying all the data elsewhere of course. I don&#x27;t have another 90TB of spare disks :P<p>Edit: I suppose I could cover this with a shell script needing only the spare space of the largest file. Nice!</div><br/><div id="37197021" class="c"><input type="checkbox" id="c-37197021" checked=""/><div class="controls bullet"><span class="by">lloeki</span><span>|</span><a href="#37192076">root</a><span>|</span><a href="#37195148">parent</a><span>|</span><a href="#37195401">next</a><span>|</span><label class="collapse" for="c-37197021">[-]</label><label class="expand" for="c-37197021">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Could you force a complete rewrite if you wanted to?<p>On btrfs that&#x27;s a rebalance, and part of how one expands an array (btrfs add + btrs balance)<p>(Not sure if ZFS has a similar operation, but from my understanding resilvering would not be it)<p>Not that it matters much though as RAID5 and RAID6 aren&#x27;t dependable upon, and the array failure modes are weird in practice, so in context of expanding storage it really only matters for RAID0 and RAID10.<p><a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2021&#x2F;09&#x2F;examining-btrfs-linuxs-perpetually-half-finished-filesystem&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2021&#x2F;09&#x2F;examining-btrfs-linu...</a><p><a href="https:&#x2F;&#x2F;www.unixsheikh.com&#x2F;articles&#x2F;battle-testing-zfs-btrfs-and-mdadm-dm.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.unixsheikh.com&#x2F;articles&#x2F;battle-testing-zfs-btrfs...</a></div><br/></div></div><div id="37195401" class="c"><input type="checkbox" id="c-37195401" checked=""/><div class="controls bullet"><span class="by">keeperofdakeys</span><span>|</span><a href="#37192076">root</a><span>|</span><a href="#37195148">parent</a><span>|</span><a href="#37197021">prev</a><span>|</span><a href="#37192697">next</a><span>|</span><label class="collapse" for="c-37195401">[-]</label><label class="expand" for="c-37195401">[1 more]</label></div><br/><div class="children"><div class="content">The easiest approach is to make a new subvolume, and move one file at a time. (Normal mv is copy + remove which doesn&#x27;t quite work here, so you&#x27;d probably want something using find -type f and xargs with mv).</div><br/></div></div></div></div></div></div><div id="37192697" class="c"><input type="checkbox" id="c-37192697" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#37192076">parent</a><span>|</span><a href="#37194166">prev</a><span>|</span><a href="#37192896">next</a><span>|</span><label class="collapse" for="c-37192697">[-]</label><label class="expand" for="c-37192697">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Does anyone know why this is the case? 
&gt; Considering the process is already reading and rewriting basically the entire array, why not recalculate the parity as well?<p>IANA expert but my guess is -- because, here, you don&#x27;t have to modify block pointers, etc.<p>ZFS RAIDZ is not like traditional RAID, as it&#x27;s not just a sequence of arbitrary bits, data plus parity.  RAIDZ stripe width is variable&#x2F;dynamic, written in blocks (imagine a 128K block, compressed to ~88K), and there is no way to quickly tell where the parity data is within a written block, where the end of any written block is, etc.<p>If you had to instead, modify the block pointers, I&#x27;d assume you have to also change each block in the live tree and all dependent (including snapshot) blocks at the same time?  That sounds extraordinarily complicated (and this is the data integrity FS!), and much slower, than just blasting through the data, in order.<p>To do what you want, you can do what one could always do -- zfs send&#x2F;recv between a filesystem between and old and new filesystem.</div><br/><div id="37195083" class="c"><input type="checkbox" id="c-37195083" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37192076">root</a><span>|</span><a href="#37192697">parent</a><span>|</span><a href="#37192896">next</a><span>|</span><label class="collapse" for="c-37195083">[-]</label><label class="expand" for="c-37195083">[1 more]</label></div><br/><div class="children"><div class="content">&gt; To do what you want, you can do what one could always do -- zfs send&#x2F;recv between a filesystem between and old and new filesystem.<p>Sure, but that involves having enough spare disks, enough places to put them, and enough places to connect them.<p>This way, while the initial expansion is not ideal, it works. If you really need the space gains from a wider distribution, you can do this expansion and then do the make a new copy, then replace old copy with a new copy dance... although that&#x27;s counter productive if you have snapshots.</div><br/></div></div></div></div><div id="37192896" class="c"><input type="checkbox" id="c-37192896" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#37192076">parent</a><span>|</span><a href="#37192697">prev</a><span>|</span><a href="#37194533">next</a><span>|</span><label class="collapse" for="c-37192896">[-]</label><label class="expand" for="c-37192896">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Considering the process is already reading and rewriting basically the entire array, why not recalculate the parity as well?<p>Because snapshots might refer to the old blocks. Sure you could recompute, but then any snapshots would mean those old blocks would have to stay around so now you&#x27;ve taken up ~twice the space.</div><br/></div></div><div id="37194533" class="c"><input type="checkbox" id="c-37194533" checked=""/><div class="controls bullet"><span class="by">katbyte</span><span>|</span><a href="#37192076">parent</a><span>|</span><a href="#37192896">prev</a><span>|</span><a href="#37193498">next</a><span>|</span><label class="collapse" for="c-37194533">[-]</label><label class="expand" for="c-37194533">[2 more]</label></div><br/><div class="children"><div class="content">I imagine there would eventually be a way &#x2F; option to automatically rewrite old blocks. Or at least I would hope so because usually when your adding new disks the array is going to be near full</div><br/><div id="37195862" class="c"><input type="checkbox" id="c-37195862" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#37192076">root</a><span>|</span><a href="#37194533">parent</a><span>|</span><a href="#37193498">next</a><span>|</span><label class="collapse" for="c-37195862">[-]</label><label class="expand" for="c-37195862">[1 more]</label></div><br/><div class="children"><div class="content">Almost certainly not in any useful fashion that doesn&#x27;t duplicate data versus old snapshots.<p>ZFS really deeply assumes you&#x27;re not gonna be rewriting history for a bunch of features, and you&#x27;d have written a good chunk of a new filesystem to reimplement everything without those assumptions.</div><br/></div></div></div></div><div id="37193498" class="c"><input type="checkbox" id="c-37193498" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#37192076">parent</a><span>|</span><a href="#37194533">prev</a><span>|</span><a href="#37190494">next</a><span>|</span><label class="collapse" for="c-37193498">[-]</label><label class="expand" for="c-37193498">[1 more]</label></div><br/><div class="children"><div class="content">The key issue is that you basically have to rewrite all existing data to regain that lost 3TB. This takes a huge amount of time and the ZFS developers have decided not to automate this as part of this feature.<p>You can do this yourself though when convenient to get those lost TB back.<p>The RAID VDEV expansion feature actually was quite stale and wasn’t being worked on afaik until this sponsorship.</div><br/></div></div></div></div><div id="37190494" class="c"><input type="checkbox" id="c-37190494" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#37192076">prev</a><span>|</span><a href="#37193135">next</a><span>|</span><label class="collapse" for="c-37190494">[-]</label><label class="expand" for="c-37190494">[1 more]</label></div><br/><div class="children"><div class="content">&quot;This feature allows disks to be added one at a time to a RAID-Z group, expanding its capacity incrementally. This feature is especially useful for small pools (typically with only one RAID-Z group), where there isn&#x27;t sufficient hardware to add capacity by adding a whole new RAID-Z group (typically doubling the number of disks).&quot;<p>A feature I am excited to see is being added!</div><br/></div></div><div id="37193135" class="c"><input type="checkbox" id="c-37193135" checked=""/><div class="controls bullet"><span class="by">ErneX</span><span>|</span><a href="#37190494">prev</a><span>|</span><a href="#37192381">next</a><span>|</span><label class="collapse" for="c-37193135">[-]</label><label class="expand" for="c-37193135">[9 more]</label></div><br/><div class="children"><div class="content">I’m not an expert whatsoever but what I’ve been doing for my NAS is using mirrored VDEVs. Started with one and later on added a couple more drives for a second mirror.<p>Coincidentally one of the drives of my 1st mirror died few days ago after rebooting the host machine for updates and I replaced it today, it’s been resilvering for a while.</div><br/><div id="37193798" class="c"><input type="checkbox" id="c-37193798" checked=""/><div class="controls bullet"><span class="by">edmundsauto</span><span>|</span><a href="#37193135">parent</a><span>|</span><a href="#37192381">next</a><span>|</span><label class="collapse" for="c-37193798">[-]</label><label class="expand" for="c-37193798">[8 more]</label></div><br/><div class="children"><div class="content">I’ve read this is suboptimal because you are now stressing the drive that has the only copy of your data to rebuild. what are your thoughts?</div><br/><div id="37197294" class="c"><input type="checkbox" id="c-37197294" checked=""/><div class="controls bullet"><span class="by">ErneX</span><span>|</span><a href="#37193135">root</a><span>|</span><a href="#37193798">parent</a><span>|</span><a href="#37194072">next</a><span>|</span><label class="collapse" for="c-37197294">[-]</label><label class="expand" for="c-37197294">[1 more]</label></div><br/><div class="children"><div class="content">I am backing up the whole NAS in case such a failure happens, and as was mentioned in other replies getting the replacement drive in the pool is way quicker on a mirrored vdev.</div><br/></div></div><div id="37194072" class="c"><input type="checkbox" id="c-37194072" checked=""/><div class="controls bullet"><span class="by">deadbunny</span><span>|</span><a href="#37193135">root</a><span>|</span><a href="#37193798">parent</a><span>|</span><a href="#37197294">prev</a><span>|</span><a href="#37194920">next</a><span>|</span><label class="collapse" for="c-37194072">[-]</label><label class="expand" for="c-37194072">[3 more]</label></div><br/><div class="children"><div class="content">Mirrored vdevs resilver a lot faster than zX vdevs. Much less chance of the remaining drive dying during a resilver if it takes hours rather than days.</div><br/><div id="37194934" class="c"><input type="checkbox" id="c-37194934" checked=""/><div class="controls bullet"><span class="by">edmundsauto</span><span>|</span><a href="#37193135">root</a><span>|</span><a href="#37194072">parent</a><span>|</span><a href="#37194920">next</a><span>|</span><label class="collapse" for="c-37194934">[-]</label><label class="expand" for="c-37194934">[2 more]</label></div><br/><div class="children"><div class="content">Is the amount of data read&#x2F;written the same? I&#x27;m not clear why wall time is the relevant metric, unless that&#x27;s the critical driver of failure likelihood. I would have guessed it&#x27;s not time but bytes.</div><br/><div id="37195485" class="c"><input type="checkbox" id="c-37195485" checked=""/><div class="controls bullet"><span class="by">keeperofdakeys</span><span>|</span><a href="#37193135">root</a><span>|</span><a href="#37194934">parent</a><span>|</span><a href="#37194920">next</a><span>|</span><label class="collapse" for="c-37195485">[-]</label><label class="expand" for="c-37195485">[1 more]</label></div><br/><div class="children"><div class="content">A mirror resilver is a relatively linear and sequential rewrite, so its very fast. Raidz resilvers require lots of random reads and writes across all the drives, and requires waiting for all drives to read data before it can be written to the replaced drive - &quot;herding cats&quot; sounds appropriate here.</div><br/></div></div></div></div></div></div><div id="37194920" class="c"><input type="checkbox" id="c-37194920" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#37193135">root</a><span>|</span><a href="#37193798">parent</a><span>|</span><a href="#37194072">prev</a><span>|</span><a href="#37192381">next</a><span>|</span><label class="collapse" for="c-37194920">[-]</label><label class="expand" for="c-37194920">[3 more]</label></div><br/><div class="children"><div class="content">....vs stressing all of them for parity rebuild.</div><br/><div id="37194928" class="c"><input type="checkbox" id="c-37194928" checked=""/><div class="controls bullet"><span class="by">edmundsauto</span><span>|</span><a href="#37193135">root</a><span>|</span><a href="#37194920">parent</a><span>|</span><a href="#37192381">next</a><span>|</span><label class="collapse" for="c-37194928">[-]</label><label class="expand" for="c-37194928">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t it be better to apply stress over more drives to minimize the chances that you lose a second drive during a rebuild?</div><br/><div id="37197228" class="c"><input type="checkbox" id="c-37197228" checked=""/><div class="controls bullet"><span class="by">ErneX</span><span>|</span><a href="#37193135">root</a><span>|</span><a href="#37194928">parent</a><span>|</span><a href="#37192381">next</a><span>|</span><label class="collapse" for="c-37197228">[-]</label><label class="expand" for="c-37197228">[1 more]</label></div><br/><div class="children"><div class="content">I believe this is the article I read when I started:<p><a href="https:&#x2F;&#x2F;jrs-s.net&#x2F;2015&#x2F;02&#x2F;06&#x2F;zfs-you-should-use-mirror-vdevs-not-raidz&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;jrs-s.net&#x2F;2015&#x2F;02&#x2F;06&#x2F;zfs-you-should-use-mirror-vdevs...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="37192381" class="c"><input type="checkbox" id="c-37192381" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#37193135">prev</a><span>|</span><a href="#37191552">next</a><span>|</span><label class="collapse" for="c-37192381">[-]</label><label class="expand" for="c-37192381">[23 more]</label></div><br/><div class="children"><div class="content">I’m frustrated because this feature was mentioned by Schwartz when it was still in beta. I thought a new era of home computing was about to start. It didn’t, and instead we got The Cloud, which feels like decentralization but is in fact massive centralization (organizational, rather than geographical).<p>Some of us think people should be hosting stuff from home, accessible from their mobile devices. But the first and to me one of the biggest hurdles is managing storage. And that requires a storage appliance that is simpler than using a laptop, not requiring the skills of an IT professional.<p>Drobo tried to make a storage appliance, but once you got to the fine print it had the same set of problems that ZFS still does.<p>All professional storage solutions are built on an assumption of symmetry of hardware. I have n identical (except not the same batch?) drives which I will smear files out across.<p>Consumers will <i>never</i> have drive symmetry. That’s a huge expenditure that few can justify, or  much afford. My Synology didn’t like most of my old drives so by the time I had a working array I’d spent practically a laptop on it. For a weirdly shaped computer I couldn’t actually use directly. I’m a developer, I can afford it. None of my friends can. Mom definitely can’t.<p>A consumer solution needs to assume drive asymmetry. That day it is first plugged in, it will contain a couple new drives, and every hard drive the consumer can scrounge up from junk drawers - save two: their current backup drive and an extra copy. Once the array (with one open slot) is built and verified, then one of the backups can go into the array for additional space and speed.<p>From then on, the owner will likely buy one or two new drives every year, at whatever price point they’re willing to pay, and swap out the smallest or slowest drive in the array. Meaning the array will always contain 2-3 different generation of hard drives. Never the same speed and never the same capacity. And they expect that if a rebuild fails, some of their data will still be retrievable. Without a professional data recovery company.<p>which rules out all RAID levels except 0, which is nuts. An algorithm that can handle this scenario is consistent hashing. Weighted consistent hashing can handle disparate resources, by assigning more buckets to faster or larger machines. And it can grow and shrink (in a drive array, the two are sequential or simultaneous).<p>Small and old businesses begin to resemble consumer purchasing patterns. They can’t afford a shiny new array all at once. It’s scrounging and piecemeal. So this isn’t strictly about chasing consumers.<p>I thought ZFS was on a similar path, but the delays in sprouting these features make me wonder.</div><br/><div id="37194968" class="c"><input type="checkbox" id="c-37194968" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#37192381">parent</a><span>|</span><a href="#37193094">next</a><span>|</span><label class="collapse" for="c-37194968">[-]</label><label class="expand" for="c-37194968">[1 more]</label></div><br/><div class="children"><div class="content">Unraid &quot;solves&quot; that althought needs some user knowledge (IIRC parity drives must be the biggest one in array and for some reason that isn&#x27;t automatic)<p>Ceph actually works quite well for that, althought obviously far more complex than anything for home use. You just tell it &quot;x chunks with N parity&quot; and it will spread it over available drives. Just need more drives than x + n and not too egregious size differences.<p>&gt; Weighted consistent hashing can handle disparate resources, by assigning more buckets to faster or larger machines. And it can grow and shrink (in a drive array, the two are sequential or simultaneous).<p>It would need to be more complex than that. Putting chunks on say 2, 6, 8, 12, 22 TB (which might be what you&#x27;d get if you just buy &quot;cheapest GB&#x2F;$&quot; for your NAS over last 10 years!) is more complex than that.</div><br/></div></div><div id="37193094" class="c"><input type="checkbox" id="c-37193094" checked=""/><div class="controls bullet"><span class="by">gregmac</span><span>|</span><a href="#37192381">parent</a><span>|</span><a href="#37194968">prev</a><span>|</span><a href="#37194325">next</a><span>|</span><label class="collapse" for="c-37193094">[-]</label><label class="expand" for="c-37193094">[3 more]</label></div><br/><div class="children"><div class="content">I can afford it, but have a hard time justifying the costs, not to mention scrapped (working) hardware and inconvenience (of swapping to a whole new array).<p>I started using snapraid [1] several years ago, after finding zfs couldn&#x27;t expand. Often when I went to add space the &quot;sweet spot&quot; disk size (best $&#x2F;TB) was 2-3x the size of the previous biggest disk I ran. This was very economical compared to replacing the whole array every couple years.<p>It works by having &quot;data&quot; and &quot;parity&quot; drives. Data drives are totally normal filesystems, and joined with unionfs. In fact you can mount them independently and access whatever files are on it. Parity drives are just a big file that snapraid updates nightly.<p>The big downside is it&#x27;s not realtime redundant: you can lose a day&#x27;s worth of data from a (data) drive failure. For my use case this is acceptable.<p>A huge upside is rebuilds are fairly painless. Rebuilding a parity drive has zero downtime, just degraded performance. Rebuilding a data drive leaves it offline, but the rest work fine (I think the individual files are actually accessible as they&#x27;re restored though). In the worst case you can mount each data drive independently on any system and recover its contents.<p>I&#x27;ve been running the &quot;same&quot; array for a decade, but at this point every disk has been swapped out at least once (for a larger one), and it&#x27;s been in at least two different host systems.<p>[1] <a href="https:&#x2F;&#x2F;www.snapraid.it&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.snapraid.it&#x2F;</a></div><br/><div id="37195706" class="c"><input type="checkbox" id="c-37195706" checked=""/><div class="controls bullet"><span class="by">RulerOf</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37193094">parent</a><span>|</span><a href="#37194325">next</a><span>|</span><label class="collapse" for="c-37195706">[-]</label><label class="expand" for="c-37195706">[2 more]</label></div><br/><div class="children"><div class="content">I gave snapraid a serious look a few months back and decided it might not be for me because the act of balancing writes out to the &quot;array&quot; member disks appeared to be manual.<p>I didn&#x27;t want to point applications at 100 T of &quot;free&quot; space only for attires to start blocking after 8.<p>Am I mistaken about that?</div><br/><div id="37196293" class="c"><input type="checkbox" id="c-37196293" checked=""/><div class="controls bullet"><span class="by">gregmac</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37195706">parent</a><span>|</span><a href="#37194325">next</a><span>|</span><label class="collapse" for="c-37196293">[-]</label><label class="expand" for="c-37196293">[1 more]</label></div><br/><div class="children"><div class="content">Sorry I said unionfs but it&#x27;s actually handled by mergerfs [1], and it&#x27;s all automatic. There are a whole boatload of policies [2] to control writes.<p>I use &quot;existing path, least free space&quot;. Once a path is created, it keeps using it for new files in that path. If it runs out of space, it creates that same path on another drive. If the path exists on both drives for some reason, my rationale is this keeps <i>most</i> of the related files (same path) together on the same drive.<p>I see there&#x27;s some newer &quot;most shared path&quot; options I don&#x27;t remember that might even make more sense for me, so maybe that&#x27;s something I&#x27;ll change next time I need to touch it.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;trapexit&#x2F;mergerfs">https:&#x2F;&#x2F;github.com&#x2F;trapexit&#x2F;mergerfs</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;trapexit&#x2F;mergerfs#policy-descriptions">https:&#x2F;&#x2F;github.com&#x2F;trapexit&#x2F;mergerfs#policy-descriptions</a></div><br/></div></div></div></div></div></div><div id="37194325" class="c"><input type="checkbox" id="c-37194325" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#37192381">parent</a><span>|</span><a href="#37193094">prev</a><span>|</span><a href="#37195074">next</a><span>|</span><label class="collapse" for="c-37194325">[-]</label><label class="expand" for="c-37194325">[9 more]</label></div><br/><div class="children"><div class="content">&gt; which rules out all RAID levels except 0, which is nuts.<p>RAID 1, you mean? Because that way you still have a complete copy of your data if one drive fails.<p>BTRFS is <i>excellent</i> for the use case of a wide variety of mismatched drives, because it supports adding and removing drives and rebalancing the array. But for the moment only the RAID 1 modes are really trustworthy. I have a NAS consisting of drives whose advertised capacities in GB are: 1000, 1000, 1920, 2000, 2048, 2050, 3840, 3840, 4000. But it started as a pile of infamously unreliable 3TB hard drives.</div><br/><div id="37195540" class="c"><input type="checkbox" id="c-37195540" checked=""/><div class="controls bullet"><span class="by">keeperofdakeys</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37194325">parent</a><span>|</span><a href="#37194825">next</a><span>|</span><label class="collapse" for="c-37195540">[-]</label><label class="expand" for="c-37195540">[3 more]</label></div><br/><div class="children"><div class="content">The BTRFS raid1 feature is very badly named, its best called two-copy. Skipping some details, two copies of data are written to the two drives with the most free space. So you can have multiple mismatched drives.<p>However you get no striping, and data is only read from one drive, so performance is limited to that of one drive for reads and writes. Plus with mismatched drives, smaller drives go unused unless you write enough data.</div><br/><div id="37195633" class="c"><input type="checkbox" id="c-37195633" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37195540">parent</a><span>|</span><a href="#37194825">next</a><span>|</span><label class="collapse" for="c-37195633">[-]</label><label class="expand" for="c-37195633">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The BTRFS raid1 feature is very badly named,<p>You&#x27;re going to have to support that claim a bit better. The core idea of RAID 1 is mirroring data, which BTRFS RAID 1 mode <i>definitely</i> does. Striping is not an essential part of RAID 1 (hence RAID 10), and reading data from two disks in parallel is an optional performance optimization that is not performed by all RAID 1 implementations (but <i>could</i> be implemented for BTRFS RAID 1: <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;55408256&#x2F;btrfs-raid-1-which-device-gets-the-reads" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;55408256&#x2F;btrfs-raid-1-wh...</a> ).<p>&gt; Plus with mismatched drives, smaller drives go unused unless you write enough data.<p>Yes, the allocation is suboptimal from a performance perspective, as I&#x27;ve already said. But it is simple and straightforward and reasonably good at avoiding putting you into a situation where manually issuing a rebalance command is necessary. If you do need better performance, there&#x27;s a RAID 10 mode. But since my NAS is currently a motley pile of SSDs, I don&#x27;t need to to anything extra to have decent performance.</div><br/><div id="37195999" class="c"><input type="checkbox" id="c-37195999" checked=""/><div class="controls bullet"><span class="by">keeperofdakeys</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37195633">parent</a><span>|</span><a href="#37194825">next</a><span>|</span><label class="collapse" for="c-37195999">[-]</label><label class="expand" for="c-37195999">[1 more]</label></div><br/><div class="children"><div class="content">RAID1 is about mirroring disks, BTRFS RAID1 mirrors block groups. Plus traditionally a RAID1 of 3 disks will mirror the same data on all disks, which is different to how the RAID1 mode on BTRFS acts. So the name leads to misunderstandings since it doesn&#x27;t act like RAID1 at all.<p>It&#x27;d be way easier to talk about if it had a unique name, and you could say &quot;It&#x27;s like RAID1&quot;.<p>Despite all that I do like the mode, and use it in a few places.</div><br/></div></div></div></div></div></div><div id="37194825" class="c"><input type="checkbox" id="c-37194825" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37194325">parent</a><span>|</span><a href="#37195540">prev</a><span>|</span><a href="#37195074">next</a><span>|</span><label class="collapse" for="c-37194825">[-]</label><label class="expand" for="c-37194825">[5 more]</label></div><br/><div class="children"><div class="content">Raid 1 does not work with heterogenous disk arrays.</div><br/><div id="37195075" class="c"><input type="checkbox" id="c-37195075" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37194825">parent</a><span>|</span><a href="#37194994">next</a><span>|</span><label class="collapse" for="c-37195075">[-]</label><label class="expand" for="c-37195075">[1 more]</label></div><br/><div class="children"><div class="content">BTRFS RAID 1 works just fine with heterogeneous disks with no extra configuration required; I would not have been able to provide examples from experience otherwise. When writing new data, it allocates two blocks on two separate devices, preferring to allocate space on whatever devices have the most free space. This may not be optimal from a performance perspective, but it does mean there&#x27;s minimal wasted capacity in most configurations. (The RAID1c3 and RAID1c4 modes work the same, but with three or four copies of each block, all on separate devices.)<p>Were you assuming that BTRFS RAID 1 meant every block of data gets mirrored across <i>every</i> device? I&#x27;ve seen that assumption before, from people who don&#x27;t realize there are <i>two</i> ways to generalize RAID1 to more than two devices.</div><br/></div></div><div id="37194994" class="c"><input type="checkbox" id="c-37194994" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37194825">parent</a><span>|</span><a href="#37195075">prev</a><span>|</span><a href="#37195074">next</a><span>|</span><label class="collapse" for="c-37194994">[-]</label><label class="expand" for="c-37194994">[3 more]</label></div><br/><div class="children"><div class="content">Eh, it can kinda work. If you have 1, 2,3,4TB drives you can make RAID10 out of first GB, then stitch 2TB RAID0 from second and third and RAID1 it with 2TB out of 4th drive. then raid1 out of remaining GB on 3rd and 4th.<p>Then glue it together with LVM and hope for best.</div><br/><div id="37195378" class="c"><input type="checkbox" id="c-37195378" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37194994">parent</a><span>|</span><a href="#37195090">next</a><span>|</span><label class="collapse" for="c-37195378">[-]</label><label class="expand" for="c-37195378">[1 more]</label></div><br/><div class="children"><div class="content">There is what a man can do, and what he should do. Nobody normal is going to do any of this.</div><br/></div></div><div id="37195090" class="c"><input type="checkbox" id="c-37195090" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37194994">parent</a><span>|</span><a href="#37195378">prev</a><span>|</span><a href="#37195074">next</a><span>|</span><label class="collapse" for="c-37195090">[-]</label><label class="expand" for="c-37195090">[1 more]</label></div><br/><div class="children"><div class="content">Or you can just put all those devices into one BTRFS RAID 1 filesystem and not have to worry about it.</div><br/></div></div></div></div></div></div></div></div><div id="37195074" class="c"><input type="checkbox" id="c-37195074" checked=""/><div class="controls bullet"><span class="by">Tijdreiziger</span><span>|</span><a href="#37192381">parent</a><span>|</span><a href="#37194325">prev</a><span>|</span><a href="#37192673">next</a><span>|</span><label class="collapse" for="c-37195074">[-]</label><label class="expand" for="c-37195074">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;kb.synology.com&#x2F;en-global&#x2F;DSM&#x2F;tutorial&#x2F;What_is_Synology_Hybrid_RAID_SHR" rel="nofollow noreferrer">https:&#x2F;&#x2F;kb.synology.com&#x2F;en-global&#x2F;DSM&#x2F;tutorial&#x2F;What_is_Synol...</a></div><br/><div id="37195371" class="c"><input type="checkbox" id="c-37195371" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37195074">parent</a><span>|</span><a href="#37192673">next</a><span>|</span><label class="collapse" for="c-37195371">[-]</label><label class="expand" for="c-37195371">[1 more]</label></div><br/><div class="children"><div class="content">That creates multiple arrays from the same disks. That’s intimately no simpler than buying a string of 2 disk raid enclosures.<p>Do any of you people actually do UX or DX for a living? This is starting to worry me.</div><br/></div></div></div></div><div id="37192673" class="c"><input type="checkbox" id="c-37192673" checked=""/><div class="controls bullet"><span class="by">mrighele</span><span>|</span><a href="#37192381">parent</a><span>|</span><a href="#37195074">prev</a><span>|</span><a href="#37192567">next</a><span>|</span><label class="collapse" for="c-37192673">[-]</label><label class="expand" for="c-37192673">[5 more]</label></div><br/><div class="children"><div class="content">&gt; I’m a developer, I can afford it. None of my friends can. Mom definitely can’t.<p>The only thing that can work for your mom and your friend is, in my opinion, a pair of disks in mirror. When the space finished, buy another box with two other disk in mirror. Anything more than this is not only too complex for the average user but also too expensive.</div><br/><div id="37193663" class="c"><input type="checkbox" id="c-37193663" checked=""/><div class="controls bullet"><span class="by">bartvk</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37192673">parent</a><span>|</span><a href="#37192901">next</a><span>|</span><label class="collapse" for="c-37193663">[-]</label><label class="expand" for="c-37193663">[1 more]</label></div><br/><div class="children"><div class="content">Even that sounds complex.<p>If they want network attached storage, I’d just use a single disk NAS, and remotely back it up.</div><br/></div></div><div id="37192901" class="c"><input type="checkbox" id="c-37192901" checked=""/><div class="controls bullet"><span class="by">Quekid5</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37192673">parent</a><span>|</span><a href="#37193663">prev</a><span>|</span><a href="#37192864">next</a><span>|</span><label class="collapse" for="c-37192901">[-]</label><label class="expand" for="c-37192901">[2 more]</label></div><br/><div class="children"><div class="content">I think I would advise against a direct mirroring -- instead, I&#x27;d do sync-every-24-hours or something similar.<p>Both schemes are vulnerable to the (admittedly rarer) errors where both drives fail simultaneously (e.g. mobo fried them) or are just ... destroyed by a fire or whatever.<p>A periodic sync (while harder to set up) <i>will</i> occasionally save you from the deleting the wrong files which mirroring doesn&#x27;t.<p>Either way: Any truly important data (family photos&#x2F;videos, etc.) needs to be saved periodically to remote storage. There&#x27;s no getting around that if you <i>really</i> care about the data.</div><br/><div id="37195168" class="c"><input type="checkbox" id="c-37195168" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37192901">parent</a><span>|</span><a href="#37192864">next</a><span>|</span><label class="collapse" for="c-37195168">[-]</label><label class="expand" for="c-37195168">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>A periodic sync (while harder to set up) will occasionally save you from the deleting the wrong files which mirroring doesn&#x27;t</i><p>Which ever solution for disks you end up with, you should definitely always be using something that keeps around periodic snapshots.</div><br/></div></div></div></div><div id="37192864" class="c"><input type="checkbox" id="c-37192864" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37192673">parent</a><span>|</span><a href="#37192901">prev</a><span>|</span><a href="#37192567">next</a><span>|</span><label class="collapse" for="c-37192864">[-]</label><label class="expand" for="c-37192864">[1 more]</label></div><br/><div class="children"><div class="content">Keeping track of a heterogenous drive array is just as big an imposition.</div><br/></div></div></div></div><div id="37192567" class="c"><input type="checkbox" id="c-37192567" checked=""/><div class="controls bullet"><span class="by">Osiris</span><span>|</span><a href="#37192381">parent</a><span>|</span><a href="#37192673">prev</a><span>|</span><a href="#37191552">next</a><span>|</span><label class="collapse" for="c-37192567">[-]</label><label class="expand" for="c-37192567">[2 more]</label></div><br/><div class="children"><div class="content">I completely agree. To build my array I had to buy several drives at the same time. To expand I had to buy a new drive, move the data onto the array, and then I’m left with the extra drive I had too buy to temporarily store the data because I can’t add it to the array.<p>I would love to have more options for expandable redundancy.</div><br/><div id="37195002" class="c"><input type="checkbox" id="c-37195002" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#37192381">root</a><span>|</span><a href="#37192567">parent</a><span>|</span><a href="#37191552">next</a><span>|</span><label class="collapse" for="c-37195002">[-]</label><label class="expand" for="c-37195002">[1 more]</label></div><br/><div class="children"><div class="content">I just...leave it then create new RAID out of free space once I get more than 2-3 disks that are bigger. But yeah, PITA.</div><br/></div></div></div></div></div></div><div id="37191552" class="c"><input type="checkbox" id="c-37191552" checked=""/><div class="controls bullet"><span class="by">scheme271</span><span>|</span><a href="#37192381">prev</a><span>|</span><a href="#37193145">next</a><span>|</span><label class="collapse" for="c-37191552">[-]</label><label class="expand" for="c-37191552">[4 more]</label></div><br/><div class="children"><div class="content">This has been floating around for 2 years at this point so might be a long while until it gets in.  Interesting, QNAP somehow added this feature into the code that their QuTS Hero NASes uses.  I&#x27;m not sure how solid or tested the QNAP code is but it&#x27;s solid enough that they&#x27;re shipping it in production.</div><br/><div id="37191943" class="c"><input type="checkbox" id="c-37191943" checked=""/><div class="controls bullet"><span class="by">jtriangle</span><span>|</span><a href="#37191552">parent</a><span>|</span><a href="#37193145">next</a><span>|</span><label class="collapse" for="c-37191943">[-]</label><label class="expand" for="c-37191943">[3 more]</label></div><br/><div class="children"><div class="content">Qnap does recommend a full backup before doing so, which tells me it&#x27;s not exactly production ready as you and I would think of it.</div><br/><div id="37192380" class="c"><input type="checkbox" id="c-37192380" checked=""/><div class="controls bullet"><span class="by">phpisthebest</span><span>|</span><a href="#37191552">root</a><span>|</span><a href="#37191943">parent</a><span>|</span><a href="#37192035">next</a><span>|</span><label class="collapse" for="c-37192380">[-]</label><label class="expand" for="c-37192380">[1 more]</label></div><br/><div class="children"><div class="content">I would expect any storage array to recommend a full backup any time you are messing with the physical disks.  even &quot;production ready&quot; features one would not add, remove, or do anything with the array with out a full backup.<p>&quot;Production&quot; systems should not even be considered production unless you have a backup of them,</div><br/></div></div><div id="37192035" class="c"><input type="checkbox" id="c-37192035" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#37191552">root</a><span>|</span><a href="#37191943">parent</a><span>|</span><a href="#37192380">prev</a><span>|</span><a href="#37193145">next</a><span>|</span><label class="collapse" for="c-37192035">[-]</label><label class="expand" for="c-37192035">[1 more]</label></div><br/><div class="children"><div class="content">QNAP&#x27;s source drops are also kind of wild, in that they branched a looooooooong time ago and have been implementing their own versions of features they wanted since, AFAICT.</div><br/></div></div></div></div></div></div><div id="37193145" class="c"><input type="checkbox" id="c-37193145" checked=""/><div class="controls bullet"><span class="by">tambourine_man</span><span>|</span><a href="#37191552">prev</a><span>|</span><a href="#37192555">next</a><span>|</span><label class="collapse" for="c-37193145">[-]</label><label class="expand" for="c-37193145">[1 more]</label></div><br/><div class="children"><div class="content">If you don’t need real-time redundancy, you maybe better served by something like SnapRAID. It’s more flexible, can mismatch disk sizes, performance requirements are much lower, etc.</div><br/></div></div><div id="37192555" class="c"><input type="checkbox" id="c-37192555" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#37193145">prev</a><span>|</span><a href="#37192893">next</a><span>|</span><label class="collapse" for="c-37192555">[-]</label><label class="expand" for="c-37192555">[2 more]</label></div><br/><div class="children"><div class="content">So is it safe to use btrfs for a basic Raid-1 yet?</div><br/><div id="37192844" class="c"><input type="checkbox" id="c-37192844" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#37192555">parent</a><span>|</span><a href="#37192893">next</a><span>|</span><label class="collapse" for="c-37192844">[-]</label><label class="expand" for="c-37192844">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the non-parity RAID modes have been safe for a pretty long time, as long as you RTFM when something goes wrong instead of assuming the recovery procedures match what you&#x27;d expect coming from a background of traditional RAID or how ZFS does it. I&#x27;ve been using RAID-1 (with RAID1c3 for metadata since that feature became available) on a NAS for over a decade now without loss of data despite loss of more drives over the years than the array started out with.</div><br/></div></div></div></div><div id="37192893" class="c"><input type="checkbox" id="c-37192893" checked=""/><div class="controls bullet"><span class="by">canvascritic</span><span>|</span><a href="#37192555">prev</a><span>|</span><a href="#37191217">next</a><span>|</span><label class="collapse" for="c-37192893">[-]</label><label class="expand" for="c-37192893">[3 more]</label></div><br/><div class="children"><div class="content">this is a really neat addition to raid-z. i recall setting up my zfs pool in the early 2000s and grappling with disk counts because of how rigid expansion was. Good times. this would&#x27;ve made things so much simpler. small nitpick: in the &quot;during expansion&quot; bit, I thought he could have elaborated a touch on restoring the &quot;health of the raidz vdev&quot; part, didn&#x27;t really follow his reasoning there. but overall, looking forward to this update. nice work.</div><br/><div id="37194315" class="c"><input type="checkbox" id="c-37194315" checked=""/><div class="controls bullet"><span class="by">mgerdts</span><span>|</span><a href="#37192893">parent</a><span>|</span><a href="#37191217">next</a><span>|</span><label class="collapse" for="c-37194315">[-]</label><label class="expand" for="c-37194315">[2 more]</label></div><br/><div class="children"><div class="content">I remember in the decade+ before zfs was introduced and in several of the years since then trying to figure out the optimal stripe size for RAID 10 volumes and the optimal stripe size and count for RAID 5 volumes with Solaris Disk Suite, VxVM, Linux md, etc. Fixing bad decisions has typically meant backup twice, test a restore, fixe the volume, and pray one of your backups is good. :)<p>ZFS has generally been easier to go from zero to working well once you accept that life doesn’t need to be so complicated as legacy md, lvm, and fs layers made it. It also has a pretty big footgun that causes me to carefully read the man page before using “zpool add” or “zpool attach”. Vdev removal is a big help for this.</div><br/><div id="37194475" class="c"><input type="checkbox" id="c-37194475" checked=""/><div class="controls bullet"><span class="by">canvascritic</span><span>|</span><a href="#37192893">root</a><span>|</span><a href="#37194315">parent</a><span>|</span><a href="#37191217">next</a><span>|</span><label class="collapse" for="c-37194475">[-]</label><label class="expand" for="c-37194475">[1 more]</label></div><br/><div class="children"><div class="content">oh, the memories of trying to optimize stripe sizes in the pre-zfs days! i can totally relate to the struggles with solaris disk suite and linux md. the pain of fixing poor decisions and the nail-biting backup processes are all too familiar. zfs really did simplify things in many ways. though i agree, it&#x27;s always a good idea to tread carefully with commands like `zpool add` and `zpool attach`. glad vdev removal has made that process a bit smoother. it&#x27;s pretty cool to see how storage management has evolved over the years. cheers for the trip down memory lane. :)</div><br/></div></div></div></div></div></div><div id="37191217" class="c"><input type="checkbox" id="c-37191217" checked=""/><div class="controls bullet"><span class="by">znpy</span><span>|</span><a href="#37192893">prev</a><span>|</span><a href="#37194042">next</a><span>|</span><label class="collapse" for="c-37191217">[-]</label><label class="expand" for="c-37191217">[31 more]</label></div><br/><div class="children"><div class="content">It bothers me so much that zfs is not into mainline linux. I know it’s due to the license incompatibility… :(</div><br/><div id="37191412" class="c"><input type="checkbox" id="c-37191412" checked=""/><div class="controls bullet"><span class="by">tux3</span><span>|</span><a href="#37191217">parent</a><span>|</span><a href="#37191719">next</a><span>|</span><label class="collapse" for="c-37191412">[-]</label><label class="expand" for="c-37191412">[14 more]</label></div><br/><div class="children"><div class="content">We may get a successor filesystem before that particular situation is sorted out..<p>By all accounts mainline is at best not interested, if not actively against ZFS on Linux. The last few kerfuffles around symbols used by the out-of-tree module laid out the position rather unambiguously.</div><br/><div id="37191690" class="c"><input type="checkbox" id="c-37191690" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191412">parent</a><span>|</span><a href="#37191624">next</a><span>|</span><label class="collapse" for="c-37191690">[-]</label><label class="expand" for="c-37191690">[8 more]</label></div><br/><div class="children"><div class="content">&gt; The last few kerfuffles around symbols used by the out-of-tree module laid out the position rather unambiguously.<p>Source, for someone who isn&#x27;t following kernel mailing lists?</div><br/><div id="37191794" class="c"><input type="checkbox" id="c-37191794" checked=""/><div class="controls bullet"><span class="by">tux3</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191690">parent</a><span>|</span><a href="#37195127">next</a><span>|</span><label class="collapse" for="c-37191794">[-]</label><label class="expand" for="c-37191794">[6 more]</label></div><br/><div class="children"><div class="content">I was thinking of this thread from 5.0 in particular (2019, time flies!)<p><a href="https:&#x2F;&#x2F;lore.kernel.org&#x2F;all&#x2F;20190110182413.GA6932@kroah.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lore.kernel.org&#x2F;all&#x2F;20190110182413.GA6932@kroah.com&#x2F;</a></div><br/><div id="37192136" class="c"><input type="checkbox" id="c-37192136" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191794">parent</a><span>|</span><a href="#37195020">next</a><span>|</span><label class="collapse" for="c-37192136">[-]</label><label class="expand" for="c-37192136">[2 more]</label></div><br/><div class="children"><div class="content">It’s sad to see that free software under a license (and movement) that was born out of someone’s frustration with closed-source printer drivers (acting as DRM, albeit inadvertently) appears to include similar DRM whose sole purpose is to restrict usage of a (seemingly arbitrary) selection of symbols.</div><br/><div id="37192293" class="c"><input type="checkbox" id="c-37192293" checked=""/><div class="controls bullet"><span class="by">tux3</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37192136">parent</a><span>|</span><a href="#37195020">next</a><span>|</span><label class="collapse" for="c-37192293">[-]</label><label class="expand" for="c-37192293">[1 more]</label></div><br/><div class="children"><div class="content">It is. That said, I can&#x27;t fault people too much for being afraid of the lawnmower. People have been mowed for much less.</div><br/></div></div></div></div><div id="37195020" class="c"><input type="checkbox" id="c-37195020" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191794">parent</a><span>|</span><a href="#37192136">prev</a><span>|</span><a href="#37195127">next</a><span>|</span><label class="collapse" for="c-37195020">[-]</label><label class="expand" for="c-37195020">[3 more]</label></div><br/><div class="children"><div class="content">So, why OpenZFS can&#x27;t fix their license ?</div><br/><div id="37195191" class="c"><input type="checkbox" id="c-37195191" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37195020">parent</a><span>|</span><a href="#37195195">next</a><span>|</span><label class="collapse" for="c-37195191">[-]</label><label class="expand" for="c-37195191">[1 more]</label></div><br/><div class="children"><div class="content">OpenZFS is a fork of the Sun&#x2F;Oracle release of ZFS. The OpenZFS maintainers cannot change the license without consent of the copyright owners, and Oracle is unlikely to consent. Oracle could potentially change the terms in a future release of CDDL, and that might work, too.<p>Similarly, Linux probably has too many contributors, some of which are no longer living, to come to an agreement on a fixed license either.<p>Although, license changes do happen; OpenSSL did one recently, but I think there are fewer contributors.</div><br/></div></div><div id="37195195" class="c"><input type="checkbox" id="c-37195195" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37195020">parent</a><span>|</span><a href="#37195191">prev</a><span>|</span><a href="#37195127">next</a><span>|</span><label class="collapse" for="c-37195195">[-]</label><label class="expand" for="c-37195195">[1 more]</label></div><br/><div class="children"><div class="content">Because it uses code released by Sun under the CDDL. Oracle has since closed sourced the whole thing again so they&#x27;re not going to change the license of this old code, they don&#x27;t even want it out there but they can&#x27;t take it back.<p>Doing a clean room rewrite would be a huge job.</div><br/></div></div></div></div></div></div><div id="37195127" class="c"><input type="checkbox" id="c-37195127" checked=""/><div class="controls bullet"><span class="by">mappu</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191690">parent</a><span>|</span><a href="#37191794">prev</a><span>|</span><a href="#37191624">next</a><span>|</span><label class="collapse" for="c-37195127">[-]</label><label class="expand" for="c-37195127">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;939842&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;939842&#x2F;</a> was the most recent one (August 2023). The article focuses on the NVIDIA proprietary driver but the bulk of the comments are about the impact on ZFS.<p>ZFS is already a very awkward citizen on Linux (e.g. can&#x27;t use the page cache so it has to duplicate all that in the ARC; won&#x27;t be able to take advantage of multi-page folios because of the lowest-common-denominator SPL; ...) and it will get worse, not better.</div><br/></div></div></div></div><div id="37191624" class="c"><input type="checkbox" id="c-37191624" checked=""/><div class="controls bullet"><span class="by">betaby</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191412">parent</a><span>|</span><a href="#37191690">prev</a><span>|</span><a href="#37191719">next</a><span>|</span><label class="collapse" for="c-37191624">[-]</label><label class="expand" for="c-37191624">[5 more]</label></div><br/><div class="children"><div class="content">&gt; successor filesystem<p>Which one?</div><br/><div id="37191655" class="c"><input type="checkbox" id="c-37191655" checked=""/><div class="controls bullet"><span class="by">pa7ch</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191624">parent</a><span>|</span><a href="#37191719">next</a><span>|</span><label class="collapse" for="c-37191655">[-]</label><label class="expand" for="c-37191655">[4 more]</label></div><br/><div class="children"><div class="content">bcachefs presumably</div><br/><div id="37191733" class="c"><input type="checkbox" id="c-37191733" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191655">parent</a><span>|</span><a href="#37191719">next</a><span>|</span><label class="collapse" for="c-37191733">[-]</label><label class="expand" for="c-37191733">[3 more]</label></div><br/><div class="children"><div class="content">we have heard the same with btrfs.</div><br/><div id="37191937" class="c"><input type="checkbox" id="c-37191937" checked=""/><div class="controls bullet"><span class="by">chungy</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191733">parent</a><span>|</span><a href="#37195135">next</a><span>|</span><label class="collapse" for="c-37191937">[-]</label><label class="expand" for="c-37191937">[1 more]</label></div><br/><div class="children"><div class="content">bcachefs has had time to actually mature instead of being kneecapped early on by an angry Linus Torvalds when btrfs&#x27;s on disk format changed and broke his Fedora install.</div><br/></div></div><div id="37195135" class="c"><input type="checkbox" id="c-37195135" checked=""/><div class="controls bullet"><span class="by">mappu</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191733">parent</a><span>|</span><a href="#37191937">prev</a><span>|</span><a href="#37191719">next</a><span>|</span><label class="collapse" for="c-37195135">[-]</label><label class="expand" for="c-37195135">[1 more]</label></div><br/><div class="children"><div class="content">Btrfs on mdadm is an excellent ZFS alternative.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37191719" class="c"><input type="checkbox" id="c-37191719" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#37191217">parent</a><span>|</span><a href="#37191412">prev</a><span>|</span><a href="#37191306">next</a><span>|</span><label class="collapse" for="c-37191719">[-]</label><label class="expand" for="c-37191719">[11 more]</label></div><br/><div class="children"><div class="content">Has the whole license incompatibility thing actually been tested&#x2F;litigated in court? I heard Canonical has (at least at some point) shipped prebuilt ZFS. I understand that in-tree inclusion brings its own set of problems, but I’m just asking about redistribution of binaries - the same binaries you are allowed to build locally.<p>It would be nice to have a precedent deciding on this bullshit argument once and for all so distros can freely ship prebuilt binary modules and bring Linux to the modern times when it comes to filesystems.<p>The whole situation is ridiculous. I&#x27;d understand if this was about money, but who exactly gets hurt by users getting a prebuilt module from somewhere, vs building exactly the same thing locally from freely-available source?</div><br/><div id="37191920" class="c"><input type="checkbox" id="c-37191920" checked=""/><div class="controls bullet"><span class="by">chungy</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191719">parent</a><span>|</span><a href="#37192022">next</a><span>|</span><label class="collapse" for="c-37191920">[-]</label><label class="expand" for="c-37191920">[5 more]</label></div><br/><div class="children"><div class="content">The only entity to start a litigation cycle is Oracle, and they&#x27;ve either been uninterested or know they can&#x27;t win. Canonical is the only entity they have any chance of going after; Canonical&#x27;s lawyers already decided there was no license conflict.<p>Linus Torvalds doesn&#x27;t feel like being the guinea pig by risking ZFS in the mainline kernel. A totally reasonable position while the CDDL+GPL resolution is still ultimately unknown. (And honestly, with OpenZFS supporting a very wide range of Linux versions and FreeBSD at the same time, I have the feeling that mainline inclusion in Linux might not be the best outcome anyway.)</div><br/><div id="37192032" class="c"><input type="checkbox" id="c-37192032" checked=""/><div class="controls bullet"><span class="by">Nextgrid</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191920">parent</a><span>|</span><a href="#37192422">next</a><span>|</span><label class="collapse" for="c-37192032">[-]</label><label class="expand" for="c-37192032">[3 more]</label></div><br/><div class="children"><div class="content">I wonder what Oracle would litigate over though? My understanding is that licenses are generally used by copyright holders to restrict what others can do with a work so the holder can profit off the work and&#x2F;or keep a competitive advantage.<p>Here I do not see this argument applying since the source is freely available to use and extend; the license explicitly allows someone to compile it and use it. In this case providing prebuilt binaries is more akin to providing a &quot;cache&quot; for something you can (and are allowed to) build locally (using ZFS-DKMS for example) using source you are once again allowed to acquire and use.<p>What prejudice does it cause to Oracle that the “make” command is ran on Ubuntu’s build servers as opposed to users’ individual machines? Have similar cases been litigated before where the argument was about who runs the make command, with source that either party has otherwise a right to download &amp; use?</div><br/><div id="37195246" class="c"><input type="checkbox" id="c-37195246" checked=""/><div class="controls bullet"><span class="by">wkat4242</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37192032">parent</a><span>|</span><a href="#37193511">next</a><span>|</span><label class="collapse" for="c-37195246">[-]</label><label class="expand" for="c-37195246">[1 more]</label></div><br/><div class="children"><div class="content">Playing legal chicken with Oracle is a really dumb thing to do because catching their customers in obscure license violations and then suing the hell out of them is their entire business model. They specialise in this stuff.</div><br/></div></div><div id="37193511" class="c"><input type="checkbox" id="c-37193511" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37192032">parent</a><span>|</span><a href="#37195246">prev</a><span>|</span><a href="#37192422">next</a><span>|</span><label class="collapse" for="c-37193511">[-]</label><label class="expand" for="c-37193511">[1 more]</label></div><br/><div class="children"><div class="content">Copying and distributing those binaries is covered by copyright.  You have to follow the license if you want to do it.  It doesn&#x27;t matter that end users could legally get the same files in some other manner.  Distribution itself is part of the legal framework.</div><br/></div></div></div></div><div id="37192422" class="c"><input type="checkbox" id="c-37192422" checked=""/><div class="controls bullet"><span class="by">bubblethink</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191920">parent</a><span>|</span><a href="#37192032">prev</a><span>|</span><a href="#37192022">next</a><span>|</span><label class="collapse" for="c-37192422">[-]</label><label class="expand" for="c-37192422">[1 more]</label></div><br/><div class="children"><div class="content">Not to mention, bcachefs is making progress towards mainline.</div><br/></div></div></div></div><div id="37192022" class="c"><input type="checkbox" id="c-37192022" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191719">parent</a><span>|</span><a href="#37191920">prev</a><span>|</span><a href="#37191907">next</a><span>|</span><label class="collapse" for="c-37192022">[-]</label><label class="expand" for="c-37192022">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think Linux would like to ship a mass of code that size that&#x27;s not GPLed, even if court cases say CDDL is GPL-compatible.</div><br/><div id="37193722" class="c"><input type="checkbox" id="c-37193722" checked=""/><div class="controls bullet"><span class="by">jonhohle</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37192022">parent</a><span>|</span><a href="#37191907">next</a><span>|</span><label class="collapse" for="c-37193722">[-]</label><label class="expand" for="c-37193722">[2 more]</label></div><br/><div class="children"><div class="content">It isn’t compatible because it adds additional restrictions regarding patented code. The CDDL was made to be used in mixed license distributions and only affects individual files, but the GPL taints anything linked (which is why LGPL exists). Since the terms of the CDDL can’t be respected in a GPL’d distribution, I can’t see a way for it to ever be included in the kernel repo.<p>I don’t think there’s any issue with canonical shipping a kmod, but similar to 3ᴿᴰ party binary drivers, it would need to be treated as a different “work”.</div><br/><div id="37195851" class="c"><input type="checkbox" id="c-37195851" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37193722">parent</a><span>|</span><a href="#37191907">next</a><span>|</span><label class="collapse" for="c-37195851">[-]</label><label class="expand" for="c-37195851">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m aware of the surrounding arguments, I promise; my remark was that I doubted Linux would like to ship a large mass of code with any additional encumberments even if it didn&#x27;t somehow fail the GPL compatible test. (I&#x27;m aware that this is an impossible condition barring court precedent that ignores the definition of the constraint.)</div><br/></div></div></div></div></div></div><div id="37191907" class="c"><input type="checkbox" id="c-37191907" checked=""/><div class="controls bullet"><span class="by">askiiart</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191719">parent</a><span>|</span><a href="#37192022">prev</a><span>|</span><a href="#37191977">next</a><span>|</span><label class="collapse" for="c-37191907">[-]</label><label class="expand" for="c-37191907">[1 more]</label></div><br/><div class="children"><div class="content">I can confirm that Ubuntu ships prebuilt ZFS, I used a live Ubuntu USB to copy some data off a ZFS pool just a couple weeks ago.</div><br/></div></div><div id="37191977" class="c"><input type="checkbox" id="c-37191977" checked=""/><div class="controls bullet"><span class="by">2OEH8eoCRo0</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191719">parent</a><span>|</span><a href="#37191907">prev</a><span>|</span><a href="#37191306">next</a><span>|</span><label class="collapse" for="c-37191977">[-]</label><label class="expand" for="c-37191977">[1 more]</label></div><br/><div class="children"><div class="content">Because it will be hella fun backing ZFS out of the kernel source once it&#x27;s been there for a few years.</div><br/></div></div></div></div><div id="37191306" class="c"><input type="checkbox" id="c-37191306" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#37191217">parent</a><span>|</span><a href="#37191719">prev</a><span>|</span><a href="#37194042">next</a><span>|</span><label class="collapse" for="c-37191306">[-]</label><label class="expand" for="c-37191306">[5 more]</label></div><br/><div class="children"><div class="content">Same. Maybe one day. Though I don’t know what will happen first: it gets mainlined into the kernel or we get HL3</div><br/><div id="37191513" class="c"><input type="checkbox" id="c-37191513" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191306">parent</a><span>|</span><a href="#37195364">next</a><span>|</span><label class="collapse" for="c-37191513">[-]</label><label class="expand" for="c-37191513">[3 more]</label></div><br/><div class="children"><div class="content">So long as the kernel developers are actively hostile to ZFS…<p>You will take your Btrfs and you will like it.</div><br/><div id="37191550" class="c"><input type="checkbox" id="c-37191550" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191513">parent</a><span>|</span><a href="#37195364">next</a><span>|</span><label class="collapse" for="c-37191550">[-]</label><label class="expand" for="c-37191550">[2 more]</label></div><br/><div class="children"><div class="content">I’m holding out for bcachefs and still building ZFS via dkms on current kernels like a madman.</div><br/><div id="37191584" class="c"><input type="checkbox" id="c-37191584" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#37191217">root</a><span>|</span><a href="#37191550">parent</a><span>|</span><a href="#37195364">next</a><span>|</span><label class="collapse" for="c-37191584">[-]</label><label class="expand" for="c-37191584">[1 more]</label></div><br/><div class="children"><div class="content">I’m running bcachefs on my desktop right now.<p>It’s promising, but there’s… bugs. Right now only performance oriented ones, that I’ve noticed, but I’d wait a bit longer.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37194042" class="c"><input type="checkbox" id="c-37194042" checked=""/><div class="controls bullet"><span class="by">matheusmoreira</span><span>|</span><a href="#37191217">prev</a><span>|</span><a href="#37191188">next</a><span>|</span><label class="collapse" for="c-37194042">[-]</label><label class="expand" for="c-37194042">[1 more]</label></div><br/><div class="children"><div class="content">So happy to see this. Incremental expansion is extremely important for consumers, homelabs. Now we can gradually expand capacity.</div><br/></div></div><div id="37191188" class="c"><input type="checkbox" id="c-37191188" checked=""/><div class="controls bullet"><span class="by">lyjia</span><span>|</span><a href="#37194042">prev</a><span>|</span><a href="#37191699">next</a><span>|</span><label class="collapse" for="c-37191188">[-]</label><label class="expand" for="c-37191188">[3 more]</label></div><br/><div class="children"><div class="content">Wow, I&#x27;ve been hearing this has been in the works for a while. I am glad to see it released! My RAIDZ array awaits new disks!</div><br/><div id="37191384" class="c"><input type="checkbox" id="c-37191384" checked=""/><div class="controls bullet"><span class="by">cassianoleal</span><span>|</span><a href="#37191188">parent</a><span>|</span><a href="#37191699">next</a><span>|</span><label class="collapse" for="c-37191384">[-]</label><label class="expand" for="c-37191384">[2 more]</label></div><br/><div class="children"><div class="content">What do you mean by released? It hasn&#x27;t even been merged yet. :)</div><br/><div id="37196307" class="c"><input type="checkbox" id="c-37196307" checked=""/><div class="controls bullet"><span class="by">lyjia</span><span>|</span><a href="#37191188">root</a><span>|</span><a href="#37191384">parent</a><span>|</span><a href="#37191699">next</a><span>|</span><label class="collapse" for="c-37196307">[-]</label><label class="expand" for="c-37196307">[1 more]</label></div><br/><div class="children"><div class="content">Misread the pull request. But this means it is close to release! It has been in the works for some time.</div><br/></div></div></div></div></div></div><div id="37191699" class="c"><input type="checkbox" id="c-37191699" checked=""/><div class="controls bullet"><span class="by">atmosx</span><span>|</span><a href="#37191188">prev</a><span>|</span><label class="collapse" for="c-37191699">[-]</label><label class="expand" for="c-37191699">[1 more]</label></div><br/><div class="children"><div class="content">Finally :-)</div><br/></div></div></div></div></div></div></div></body></html>