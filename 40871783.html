<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720083676209" as="style"/><link rel="stylesheet" href="styles.css?v=1720083676209"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://boyuan.space/diffusion-forcing/">Diffusion Forcing: Next-Token Prediction Meets Full-Sequence Diffusion</a>Â <span class="domain">(<a href="https://boyuan.space">boyuan.space</a>)</span></div><div class="subtext"><span>magoghm</span> | <span>4 comments</span></div><br/><div><div id="40873378" class="c"><input type="checkbox" id="c-40873378" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40872702">next</a><span>|</span><label class="collapse" for="c-40873378">[-]</label><label class="expand" for="c-40873378">[2 more]</label></div><br/><div class="children"><div class="content">Anyone know of research or tools for using an existing text generating LLM with diffusion like techniques with no new pre-training, or at most, a bit of fine-tuning, such that it works with a small GPT &#x2F; Phi 3 &#x2F; Gwen model, for example?
I know about Tree of Thoughts with MCTS etc, that are somewhat similar (though often with a different reward learned goal) but I&#x27;m interested in something closer to token level generation.
Is this possible?</div><br/><div id="40873426" class="c"><input type="checkbox" id="c-40873426" checked=""/><div class="controls bullet"><span class="by">42lux</span><span>|</span><a href="#40873378">parent</a><span>|</span><a href="#40872702">next</a><span>|</span><label class="collapse" for="c-40873426">[-]</label><label class="expand" for="c-40873426">[1 more]</label></div><br/><div class="children"><div class="content">The Pixart team (now at Nvidia) has some models with of the shelf llms as textencoders but like the rest they settled for T1 in their last models.</div><br/></div></div></div></div><div id="40872702" class="c"><input type="checkbox" id="c-40872702" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#40873378">prev</a><span>|</span><label class="collapse" for="c-40872702">[-]</label><label class="expand" for="c-40872702">[1 more]</label></div><br/><div class="children"><div class="content">Am I missing something about training time? Does adding per token noise cause training to slow significantly? Cool paper though!</div><br/></div></div></div></div></div></div></div></body></html>