<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696410064615" as="style"/><link rel="stylesheet" href="styles.css?v=1696410064615"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2307.08197">Self-Assembling Artificial Neural Networks Through Neural Developmental Programs</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>hardmaru</span> | <span>6 comments</span></div><br/><div><div id="37760884" class="c"><input type="checkbox" id="c-37760884" checked=""/><div class="controls bullet"><span class="by">great_psy</span><span>|</span><label class="collapse" for="c-37760884">[-]</label><label class="expand" for="c-37760884">[5 more]</label></div><br/><div class="children"><div class="content">Seems promising. I guess the one catch is that all this extra computation required for self assembly might not be received back in the form of a better system compared to using that computation to better train a human designed neural network.</div><br/><div id="37761116" class="c"><input type="checkbox" id="c-37761116" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37760884">parent</a><span>|</span><label class="collapse" for="c-37761116">[-]</label><label class="expand" for="c-37761116">[4 more]</label></div><br/><div class="children"><div class="content">Designing neural net architectures seems like exactly the sort of problem that neural nets ought to beat humans at handily. I&#x27;d be surprised if some kind of self-optimizing architecture wasn&#x27;t standard in a decade or two.</div><br/><div id="37761412" class="c"><input type="checkbox" id="c-37761412" checked=""/><div class="controls bullet"><span class="by">signa11</span><span>|</span><a href="#37760884">root</a><span>|</span><a href="#37761116">parent</a><span>|</span><a href="#37761311">next</a><span>|</span><label class="collapse" for="c-37761412">[-]</label><label class="expand" for="c-37761412">[2 more]</label></div><br/><div class="children"><div class="content">&gt; ... Designing neural net architectures seems like exactly the sort of problem that neural nets ought to beat humans at handily. I&#x27;d be surprised if some kind of self-optimizing architecture wasn&#x27;t standard in a decade or two.<p>wouldn&#x27;t evolutionary algorithms be better suited for this ? evolution is how we got here after all ? perhaps different constraints on the &#x27;state space&#x27; can &#x27;guide&#x27; the whole process...</div><br/><div id="37762605" class="c"><input type="checkbox" id="c-37762605" checked=""/><div class="controls bullet"><span class="by">buffalobuffalo</span><span>|</span><a href="#37760884">root</a><span>|</span><a href="#37761412">parent</a><span>|</span><a href="#37761311">next</a><span>|</span><label class="collapse" for="c-37762605">[-]</label><label class="expand" for="c-37762605">[1 more]</label></div><br/><div class="children"><div class="content">There are several evolutionary algorithms that are used to design network topography as well as weights.  NEAT is a popular one: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuroevolution_of_augmenting_topologies" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuroevolution_of_augmenting_t...</a><p>There are a few problems with this approach though.  For one, there are often many local maxima that a genetic algorithm can easily converge on.<p>Secondly, the state space is often enormous, and testing&#x2F;discarding millions of generations of poorly constructed networks is a very inefficient way of sampling it.<p>These problems are often enough to prohibit GA use.</div><br/></div></div></div></div><div id="37761311" class="c"><input type="checkbox" id="c-37761311" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37760884">root</a><span>|</span><a href="#37761116">parent</a><span>|</span><a href="#37761412">prev</a><span>|</span><label class="collapse" for="c-37761311">[-]</label><label class="expand" for="c-37761311">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Designing neural net architectures seems like exactly the sort of problem that neural nets ought to beat humans at handily.<p>Is it? The “easy” tasks for neural nets to beat humans at are things that are harder for humans to do than for humans to provide volumes of high-quality training data by way of which neural nets can be trained.<p>It’s not clear to ne that designing neural nets really fits that.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>