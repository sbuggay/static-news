<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700211657985" as="style"/><link rel="stylesheet" href="styles.css?v=1700211657985"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://dl.acm.org/doi/10.1145/2367376.2367378">Disks lie. And the controllers that run them are partners in crime (2012)</a> <span class="domain">(<a href="https://dl.acm.org">dl.acm.org</a>)</span></div><div class="subtext"><span>ecliptik</span> | <span>21 comments</span></div><br/><div><div id="38300859" class="c"><input type="checkbox" id="c-38300859" checked=""/><div class="controls bullet"><span class="by">nurple</span><span>|</span><a href="#38300548">next</a><span>|</span><label class="collapse" for="c-38300859">[-]</label><label class="expand" for="c-38300859">[2 more]</label></div><br/><div class="children"><div class="content">What I get from this is that a major method of rent seeking used by disk manufacturers, to differentiate expensive enterprise disk from cheap consumer disk, was colluding to keep the simple and effective write queue reporting from being shipped to the latter. It didn&#x27;t require nvram, or complex microcode that knew how to process leftover wire requests, it only had to report back when each chunk it was already tracking in its buffers got persisted.<p>To me it sounds like it&#x27;s the manufacturers were the true partners in crime here.</div><br/><div id="38301076" class="c"><input type="checkbox" id="c-38301076" checked=""/><div class="controls bullet"><span class="by">dan-robertson</span><span>|</span><a href="#38300859">parent</a><span>|</span><a href="#38300548">next</a><span>|</span><label class="collapse" for="c-38301076">[-]</label><label class="expand" for="c-38301076">[1 more]</label></div><br/><div class="children"><div class="content">The article is from 10 years ago. Most people use flash now which is different (see eg <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3064176.3064187" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3064176.3064187</a>).<p>I think higher prices for enterprise-grade disks are more about reliability than NCQ behaviour.</div><br/></div></div></div></div><div id="38300548" class="c"><input type="checkbox" id="c-38300548" checked=""/><div class="controls bullet"><span class="by">johnbender</span><span>|</span><a href="#38300859">prev</a><span>|</span><a href="#38300461">next</a><span>|</span><label class="collapse" for="c-38300548">[-]</label><label class="expand" for="c-38300548">[2 more]</label></div><br/><div class="children"><div class="content">If you view a file system as running concurrently with another instance of itself where it could be preempted at any time indefinitely and where your algorithm for ensuring your crash protocol invariant must be lock free (two threads helps here) then you get a good sense for the complexity of the problem.</div><br/><div id="38301098" class="c"><input type="checkbox" id="c-38301098" checked=""/><div class="controls bullet"><span class="by">dan-robertson</span><span>|</span><a href="#38300548">parent</a><span>|</span><a href="#38300461">next</a><span>|</span><label class="collapse" for="c-38301098">[-]</label><label class="expand" for="c-38301098">[1 more]</label></div><br/><div class="children"><div class="content">… and filesystems themselves are multithreaded, and your writes can get reordered quite dramatically.</div><br/></div></div></div></div><div id="38300461" class="c"><input type="checkbox" id="c-38300461" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#38300548">prev</a><span>|</span><a href="#38300739">next</a><span>|</span><label class="collapse" for="c-38300461">[-]</label><label class="expand" for="c-38300461">[3 more]</label></div><br/><div class="children"><div class="content">Disks are foreign nodes in a distributed system. Treat them as such.</div><br/><div id="38300581" class="c"><input type="checkbox" id="c-38300581" checked=""/><div class="controls bullet"><span class="by">pyrolistical</span><span>|</span><a href="#38300461">parent</a><span>|</span><a href="#38300739">next</a><span>|</span><label class="collapse" for="c-38300581">[-]</label><label class="expand" for="c-38300581">[2 more]</label></div><br/><div class="children"><div class="content">Then what does this mean for consensus algorithm like raft? Wouldn’t you need to power cycle disks and read back what you just wrote to ensure it actually got written and not just buffered?</div><br/><div id="38301005" class="c"><input type="checkbox" id="c-38301005" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#38300461">root</a><span>|</span><a href="#38300581">parent</a><span>|</span><a href="#38300739">next</a><span>|</span><label class="collapse" for="c-38301005">[-]</label><label class="expand" for="c-38301005">[1 more]</label></div><br/><div class="children"><div class="content">You’ll always get back what you wrote even if it wasn’t written durably. It’s kind of how that works. With consensus you make the following kind of argument:<p>1. The kind of drives you’re using if you’re running a distributed consensus in production is going to be enterprise where these kind of games are played a lot less<p>2. Establishing consensus is time consuming - by the time peers ack it, you will have flushed to disk. In a common 3 replica scenario, that means 1&#x2F;3 nodes is taken care of.<p>3. The probability of 2 disks failing simultaneously in ways that cause a problem is pretty small.<p>But yeah, if your concensus has to be durably written and your flash controller is lying to you, your concensus is worthless (even reading what you wrote isn’t good enough because again - controller can lie to you and satisfy reads from the buffer too)</div><br/></div></div></div></div></div></div><div id="38300739" class="c"><input type="checkbox" id="c-38300739" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#38300461">prev</a><span>|</span><a href="#38300700">next</a><span>|</span><label class="collapse" for="c-38300739">[-]</label><label class="expand" for="c-38300739">[8 more]</label></div><br/><div class="children"><div class="content">I guess I&#x27;ve been living under a rock, because I had the souvenir that disks with volatile write caches did have a small power bank, able to sustain the disk for the few seconds it needed to flush its caches (what it called a BBU?)<p>Which leads me to another question, should it really be the responsibility of each component to gracefully handle a random, instantaneous power loss?<p>Couldn&#x27;t we have a small power bank on the motherboard keeping, say, 2 seconds of power and signaling emergency shutdown to components?<p>In a sense, that is very similar to sending a SIGTERM to a process to allow it to gracefully stop.</div><br/><div id="38300792" class="c"><input type="checkbox" id="c-38300792" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#38300739">parent</a><span>|</span><a href="#38300764">next</a><span>|</span><label class="collapse" for="c-38300792">[-]</label><label class="expand" for="c-38300792">[5 more]</label></div><br/><div class="children"><div class="content">Consumer drives are generally junk.<p>Personally I think it should be the responsibility of the operating system. All filesystem operations on modern computers should be atomic, and either entirely succeed or entirely fail. And the OS should work with hard disk manufacturers to make sure their devices meet whatever expectation the OS makes of them.<p>Hilariously, the OS already uses transactions for its own filesystem metadata operations. But they don&#x27;t share that capability with end user applications, which end up either being quietly broken in the event of power loss (most apps) or horribly complex to make up for the OS&#x27;s shortfalls (most databases).<p>There are dozens of solutions in userland, from git&#x27;s content-addressing to on-disk B+trees. There should just be a simple, effective, correct transactional filesystem API provided by the OS that applications can simply use. The best part of this proposal: a solution implemented by the OS directly would probably be faster than what most people would implement in userland anyway. Its ridiculous this doesn&#x27;t already exist.<p>There&#x27;s simply no structural reason for power failure (or application crashes) to be able to put hard disk data into an inconsistent, corrupt state.</div><br/><div id="38301074" class="c"><input type="checkbox" id="c-38301074" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#38300739">root</a><span>|</span><a href="#38300792">parent</a><span>|</span><a href="#38300968">next</a><span>|</span><label class="collapse" for="c-38301074">[-]</label><label class="expand" for="c-38301074">[1 more]</label></div><br/><div class="children"><div class="content">&gt; All filesystem operations on modern computers should be atomic<p>Well that seems a bit over the top. You would pay the price of full consistency for every write, while just one write every 3 years will fail. It would be insanely slow and wasteful.<p>Not to mention, that would basically disallow any kind of OS or hardware caching, which is immensely important for 99% of use cases.<p>&gt; the OS should work with hard disk manufacturers to make sure their devices meet whatever expectation the OS makes of them<p>This is a bit of wishful thinking. We&#x27;ve seen already on numerous cases (Nvidia?) that OS developpers have very little leverage on hardware manufacturers. The best they could do is issue some kind of &quot;Linux certified&quot; label, which honestly nobody would care about.<p>Also, I guess there is some form of optimum that users and providers reach in terms of stability versus performance. If one OS provides x10 performance for 99.99% of uses while an other one provides 100% safety for &#x2F;10 performance, I&#x27;m not sure users would choose the latter.</div><br/></div></div><div id="38300968" class="c"><input type="checkbox" id="c-38300968" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#38300739">root</a><span>|</span><a href="#38300792">parent</a><span>|</span><a href="#38301074">prev</a><span>|</span><a href="#38300948">next</a><span>|</span><label class="collapse" for="c-38300968">[-]</label><label class="expand" for="c-38300968">[2 more]</label></div><br/><div class="children"><div class="content">I think this kind of design would have crap performance (or at least enforce it). The reason is that the disk controller has its own set of RAM that it employs on the other end of the PCIe bus which could be battery backed and be designed to keep the controller alive long enough to flush everything. That’s not possible if the OS is in charge (the power requirements for the full OS are orders of magnitude higher). Additionally, you can’t have hybrid NVMe spinning rust disks with that kind of design.<p>Finally, it’s unlikely that implementing it in the OS would be faster because there’s a bunch of flash translation logic and garbage collection that has to happen - much faster if all of that happens on the other side of the PCIe bus with a dedicated coprocessor with a dedicated SPI directly to raw flash.</div><br/><div id="38301049" class="c"><input type="checkbox" id="c-38301049" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#38300739">root</a><span>|</span><a href="#38300968">parent</a><span>|</span><a href="#38300948">next</a><span>|</span><label class="collapse" for="c-38301049">[-]</label><label class="expand" for="c-38301049">[1 more]</label></div><br/><div class="children"><div class="content">I’m imagining a software solution using some form of journaling &#x2F; checkpoints. Instead of overwriting data in-place, send all writes to fresh, unused storage and then “commit” by updating a pointer to avoid write amplification. You’d need a bit of extra machinery to handle reads on the old data. As I understand it, NVMe drives already send all writes to unused space and then update pointers to reclaim old capacity.<p>We could start with a nice implementation in the OS. Probably with a way for applications to opt in and out. And then optimize over time. Optimizations could include better integration with the way NVMe devices actually work, rather than the way they pretend to work to the rest of your computer.</div><br/></div></div></div></div><div id="38300948" class="c"><input type="checkbox" id="c-38300948" checked=""/><div class="controls bullet"><span class="by">fmajid</span><span>|</span><a href="#38300739">root</a><span>|</span><a href="#38300792">parent</a><span>|</span><a href="#38300968">prev</a><span>|</span><a href="#38300764">next</a><span>|</span><label class="collapse" for="c-38300948">[-]</label><label class="expand" for="c-38300948">[1 more]</label></div><br/><div class="children"><div class="content">McKusick is an OS writer (4.2BSD and 4.3BSD), he is writing about the challenges systems programmers face with deceptive hardware.</div><br/></div></div></div></div><div id="38300764" class="c"><input type="checkbox" id="c-38300764" checked=""/><div class="controls bullet"><span class="by">gavinsyancey</span><span>|</span><a href="#38300739">parent</a><span>|</span><a href="#38300792">prev</a><span>|</span><a href="#38300700">next</a><span>|</span><label class="collapse" for="c-38300764">[-]</label><label class="expand" for="c-38300764">[2 more]</label></div><br/><div class="children"><div class="content">My understanding is that disks use the inertia of their spinning platters to power themselves long enough to flush their cache in the event of sudden power loss.</div><br/><div id="38300794" class="c"><input type="checkbox" id="c-38300794" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#38300739">root</a><span>|</span><a href="#38300764">parent</a><span>|</span><a href="#38300700">next</a><span>|</span><label class="collapse" for="c-38300794">[-]</label><label class="expand" for="c-38300794">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I just had a look and it seems BBUs don&#x27;t actually power the disk. They just keep the volatile cache alive until the disk is powered to flush them.</div><br/></div></div></div></div></div></div><div id="38300700" class="c"><input type="checkbox" id="c-38300700" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#38300739">prev</a><span>|</span><label class="collapse" for="c-38300700">[-]</label><label class="expand" for="c-38300700">[5 more]</label></div><br/><div class="children"><div class="content">What we singularly call a computer is actually a <i>collection</i> of computers.<p>Each HDD and&#x2F;or SSD is its own computer, with its own CPU (controller) and RAM (cache). Each video card is its own computer with its own CPU (GPU) and RAM (video RAM). Every USB controller is its own computer with its own CPU and RAM. Each NIC is its own computer with its own CPU and RAM.<p>And so on and so forth.<p>Do disks lie? Fuck yeah, and so does everything else. Much like the human world, everyone (everything) lies so the system as a whole can (hopefully) keep running smoothly.</div><br/><div id="38300871" class="c"><input type="checkbox" id="c-38300871" checked=""/><div class="controls bullet"><span class="by">nurple</span><span>|</span><a href="#38300700">parent</a><span>|</span><a href="#38300846">next</a><span>|</span><label class="collapse" for="c-38300871">[-]</label><label class="expand" for="c-38300871">[2 more]</label></div><br/><div class="children"><div class="content">So my Kubernetes cluster is actually a collection of a collection of someone else&#x27;s computers?</div><br/><div id="38301036" class="c"><input type="checkbox" id="c-38301036" checked=""/><div class="controls bullet"><span class="by">groestl</span><span>|</span><a href="#38300700">root</a><span>|</span><a href="#38300871">parent</a><span>|</span><a href="#38300846">next</a><span>|</span><label class="collapse" for="c-38301036">[-]</label><label class="expand" for="c-38301036">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much yes</div><br/></div></div></div></div><div id="38300846" class="c"><input type="checkbox" id="c-38300846" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#38300700">parent</a><span>|</span><a href="#38300871">prev</a><span>|</span><label class="collapse" for="c-38300846">[-]</label><label class="expand" for="c-38300846">[2 more]</label></div><br/><div class="children"><div class="content">&gt;everyone (everything) lies so the system as a whole can (hopefully) keep running smoothly.<p>That&#x27;s not how you make a smoothly running system, it&#x27;s how you make one that insists everything is fine while it&#x27;s all going to shit.</div><br/><div id="38300901" class="c"><input type="checkbox" id="c-38300901" checked=""/><div class="controls bullet"><span class="by">jurgenaut23</span><span>|</span><a href="#38300700">root</a><span>|</span><a href="#38300846">parent</a><span>|</span><label class="collapse" for="c-38300901">[-]</label><label class="expand" for="c-38300901">[1 more]</label></div><br/><div class="children"><div class="content">Great analogy of the wider world, by the way.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>