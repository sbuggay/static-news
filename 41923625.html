<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729760475229" as="style"/><link rel="stylesheet" href="styles.css?v=1729760475229"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cybernetist.com/2024/10/21/you-should-probably-pay-attention-to-tokenizers/">Probably pay attention to tokenizers</a> <span class="domain">(<a href="https://cybernetist.com">cybernetist.com</a>)</span></div><div class="subtext"><span>ingve</span> | <span>75 comments</span></div><br/><div><div id="41926790" class="c"><input type="checkbox" id="c-41926790" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#41927351">next</a><span>|</span><label class="collapse" for="c-41926790">[-]</label><label class="expand" for="c-41926790">[35 more]</label></div><br/><div class="children"><div class="content">Tokenizers aren&#x27;t considered the &quot;sexy&quot; part of LLMs, but where others see boring, I see opportunity. Papers like xVal[1], point toward specialization strategies in tokenization. Spelling and letter tasks are another problem that could benefit from innovation on the tokenization.<p>LLMs are notoriously bad at counting letters in words or performing simply oulipos of letter omission. GPT-4o, for example, writes a small python program and executes it in order to count letter instances. We all know that tokenization effectively erases knowledge about letters in prompts and directly negatively impacts performance at these tasks, yet we haven&#x27;t found a way to solve it.<p>1. <a href="https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;2310.02989" rel="nofollow">https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;2310.02989</a></div><br/><div id="41928821" class="c"><input type="checkbox" id="c-41928821" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#41926790">parent</a><span>|</span><a href="#41927778">next</a><span>|</span><label class="collapse" for="c-41928821">[-]</label><label class="expand" for="c-41928821">[1 more]</label></div><br/><div class="children"><div class="content">This was ages ago, in the pre-transformer era, and I can&#x27;t find the link anymore. But once upon a time I read a great paper that demonstrated that most of the performance differences being reported among popular embedding models of the time were better explained by text cleaning and tokenization than they were by the embedding model itself.<p>In other words, if you train a model using word2vec&#x27;s preprocessing and GloVe&#x27;s algorithm, the result looks more like a &quot;standard-issue&quot; word2vec model than a &quot;standard-issue&quot; GloVe model.</div><br/></div></div><div id="41927778" class="c"><input type="checkbox" id="c-41927778" checked=""/><div class="controls bullet"><span class="by">screye</span><span>|</span><a href="#41926790">parent</a><span>|</span><a href="#41928821">prev</a><span>|</span><a href="#41926850">next</a><span>|</span><label class="collapse" for="c-41927778">[-]</label><label class="expand" for="c-41927778">[1 more]</label></div><br/><div class="children"><div class="content">Tokenizers face an odd compute issue.<p>Since they&#x27;re part of the pre-processing pipeline, you can&#x27;t quickly test them out for effectiveness. You have to restart a pretraining run to test downstream effectiveness.<p>Separately,<p>As much as an attention module can do universal nonlinear transformations....I wonder if it makes sense to add specifuc modules for some math primitives as well. I remember that the executor paper [1] (slightly precursor to the attention is allyou need paper)  created self contained modules for operations like less than, count, sum and then explicitly orchestrated them in the decoder.<p>I&#x27;m surprised we haven&#x27;t seen such solutions produce sota results from math-ai or code-ai research communities.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1705.03633" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1705.03633</a></div><br/></div></div><div id="41926850" class="c"><input type="checkbox" id="c-41926850" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#41926790">parent</a><span>|</span><a href="#41927778">prev</a><span>|</span><a href="#41927742">next</a><span>|</span><label class="collapse" for="c-41926850">[-]</label><label class="expand" for="c-41926850">[21 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the issue with character-level tokenization(I assume this would be much better at count-the-letter tasks)? The article mentions it as an option but doesn&#x27;t talk about why subword tokenization is preferred by most of the big LLMs out there.</div><br/><div id="41927079" class="c"><input type="checkbox" id="c-41927079" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41926850">parent</a><span>|</span><a href="#41926909">next</a><span>|</span><label class="collapse" for="c-41927079">[-]</label><label class="expand" for="c-41927079">[16 more]</label></div><br/><div class="children"><div class="content">Using subwords makes your sequences shorter, which makes them cost less.<p>Besides that, for alphabetic languages, there exists almost no relation between form and meaning. I.e.: “ring” and “wing” differ by one letter but have no real common meaning. By picking the character or byte as your choice of representation, the model basically has to learn to distinguish ring and wing in context. This is a lot of work!<p>So, while working on the character or byte level saves you some embeddings and thus makes your model smaller, it puts all of the work of distinguishing similar sequences with divergent meanings on the model itself, which means you need a larger model.<p>By having subwords, a part of this distinguishing work already has been done by the vocabulary itself. As the article points out, this sometimes fails.</div><br/><div id="41929527" class="c"><input type="checkbox" id="c-41929527" checked=""/><div class="controls bullet"><span class="by">sundarurfriend</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41927079">parent</a><span>|</span><a href="#41929332">next</a><span>|</span><label class="collapse" for="c-41929527">[-]</label><label class="expand" for="c-41929527">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Besides that, for alphabetic languages, there exists almost no relation between form and meaning.<p>Also true for Abugida-based languages, for eg. சரம் (saram = string) vs மரம் (maram = tree), and many more. I think your intention with specifying &quot;alphabetic languages&quot; was to say &quot;non-logographic languages&quot;, right?</div><br/><div id="41929677" class="c"><input type="checkbox" id="c-41929677" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41929527">parent</a><span>|</span><a href="#41931796">next</a><span>|</span><label class="collapse" for="c-41929677">[-]</label><label class="expand" for="c-41929677">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll do you one more and say &quot;non-Chinese languages&quot;. Written Japanese - including the kanji portion of the script - has the same characteristic.<p>And even in Chinese it&#x27;s a fairly weak relationship. A large portion of the meanings of individual characters come from sound loan. For example the 英 in 英雄 means &quot;hero&quot;, in 英语 means &quot;England&quot;, an in 精英 means &quot;flower&quot;. The relationship there is simple homophony.<p>On the other hand, one thing you do get with written Chinese is that &quot;1 character = 1 morpheme&quot; very nearly works. So mechanistically breaking a text into a sequence of morphemes can be done pretty reliably without the aid of a semantic model or exhaustive hard-coded mapping. I think that for many other languages you can&#x27;t even get close using only syntactic analysis.</div><br/><div id="41930827" class="c"><input type="checkbox" id="c-41930827" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41929677">parent</a><span>|</span><a href="#41931796">next</a><span>|</span><label class="collapse" for="c-41930827">[-]</label><label class="expand" for="c-41930827">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ll do you one more and say &quot;non-Chinese languages&quot;. Written Japanese - including the kanji portion of the script - has the same characteristic.<p>Written Japanese is much <i>more</i> ideographic than written Chinese. Japanese spelling is determined, such as it is, by semantics. Chinese spelling is determined by sound. Thus, 女的, 娘们, and 妮子, all meaning &#x27;girl&#x27; or &#x27;woman&#x27;, have no spelling in common because they are different words, while Japanese uses 女 for &quot;jo&quot; and &quot;onna&quot; despite a total lack of any relationship between those words.</div><br/></div></div></div></div><div id="41931796" class="c"><input type="checkbox" id="c-41931796" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41929527">parent</a><span>|</span><a href="#41929677">prev</a><span>|</span><a href="#41929332">next</a><span>|</span><label class="collapse" for="c-41931796">[-]</label><label class="expand" for="c-41931796">[1 more]</label></div><br/><div class="children"><div class="content">I was trying to say “at least for alphabetic languages”. I don’t like to say things about languages I can’t speak or write. So, no, it wasn’t my intention to say “non-logographic languages”</div><br/></div></div></div></div><div id="41929332" class="c"><input type="checkbox" id="c-41929332" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41927079">parent</a><span>|</span><a href="#41929527">prev</a><span>|</span><a href="#41928842">next</a><span>|</span><label class="collapse" for="c-41929332">[-]</label><label class="expand" for="c-41929332">[9 more]</label></div><br/><div class="children"><div class="content">Has anyone tried to combine a token embedding with some representation of the characters in the (sub)word? For example, use a 512 long vector to represent a token,  and reserve the last 12 values to spell out the word.</div><br/><div id="41929537" class="c"><input type="checkbox" id="c-41929537" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41929332">parent</a><span>|</span><a href="#41929378">next</a><span>|</span><label class="collapse" for="c-41929537">[-]</label><label class="expand" for="c-41929537">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not following - spell out the word how? Like put the actual bytes as numerical input to the transformer layer?</div><br/><div id="41930237" class="c"><input type="checkbox" id="c-41930237" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41929537">parent</a><span>|</span><a href="#41929378">next</a><span>|</span><label class="collapse" for="c-41930237">[-]</label><label class="expand" for="c-41930237">[3 more]</label></div><br/><div class="children"><div class="content">Yes</div><br/><div id="41931816" class="c"><input type="checkbox" id="c-41931816" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41930237">parent</a><span>|</span><a href="#41929378">next</a><span>|</span><label class="collapse" for="c-41931816">[-]</label><label class="expand" for="c-41931816">[2 more]</label></div><br/><div class="children"><div class="content">Not that I know of, but encoding orthography in a fixed width vector usually carries the assumption that words with the same prefix are more similar. So there’s an alignment problem. You usually solve this using dynamic programming, but that doesn’t work in a vector.<p>For example “parent” and “parents” are aligned, they share letters in the same position, but “skew” and “askew” share no letters in the same position.</div><br/><div id="41932137" class="c"><input type="checkbox" id="c-41932137" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41931816">parent</a><span>|</span><a href="#41929378">next</a><span>|</span><label class="collapse" for="c-41932137">[-]</label><label class="expand" for="c-41932137">[1 more]</label></div><br/><div class="children"><div class="content">The other 500 values in the skew&#x2F;askew vectors will be similar though. The 12 character values don’t need to be aligned, their function is to provide spelling. Adding such info will probably help LLM answer questions requiring character level knowledge (e.g. counting ‘r’s in ‘strawberry’).</div><br/></div></div></div></div></div></div></div></div><div id="41929378" class="c"><input type="checkbox" id="c-41929378" checked=""/><div class="controls bullet"><span class="by">RicoElectrico</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41929332">parent</a><span>|</span><a href="#41929537">prev</a><span>|</span><a href="#41928842">next</a><span>|</span><label class="collapse" for="c-41929378">[-]</label><label class="expand" for="c-41929378">[4 more]</label></div><br/><div class="children"><div class="content">Well, fastText uses character n-grams to compute embeddings for out-of-vocabulary words. This is pre-transformers work BTW.</div><br/><div id="41931809" class="c"><input type="checkbox" id="c-41931809" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41929378">parent</a><span>|</span><a href="#41928842">next</a><span>|</span><label class="collapse" for="c-41931809">[-]</label><label class="expand" for="c-41931809">[3 more]</label></div><br/><div class="children"><div class="content">IIRC, overlapping ngram vectors are summed to form the token embedding - doesn’t it effectively destroy any character level representation of the token? Doesn’t really make sense to me.</div><br/><div id="41931831" class="c"><input type="checkbox" id="c-41931831" checked=""/><div class="controls bullet"><span class="by">stephantul</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41931809">parent</a><span>|</span><a href="#41928842">next</a><span>|</span><label class="collapse" for="c-41931831">[-]</label><label class="expand" for="c-41931831">[2 more]</label></div><br/><div class="children"><div class="content">It works because they use really large ngram values, up to 6. So most character-level information is in these subwords.</div><br/><div id="41932206" class="c"><input type="checkbox" id="c-41932206" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41931831">parent</a><span>|</span><a href="#41928842">next</a><span>|</span><label class="collapse" for="c-41932206">[-]</label><label class="expand" for="c-41932206">[1 more]</label></div><br/><div class="children"><div class="content">Let’s say we want to use 6-grams and build an embedding vector for the word “because”: we add integer vectors for “becaus” and “ecause”, right? For example: [1,2,3,4,5,6] + [2,3,4,5,6,2] = [3,5,7,9,11,8]. Obviously we cannot use this resulting numerical vector to spell the input word. Pretty much all character level info is lost.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41928842" class="c"><input type="checkbox" id="c-41928842" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41927079">parent</a><span>|</span><a href="#41929332">prev</a><span>|</span><a href="#41926909">next</a><span>|</span><label class="collapse" for="c-41928842">[-]</label><label class="expand" for="c-41928842">[2 more]</label></div><br/><div class="children"><div class="content">I suspect that the holy grail here is figuring out how to break the input into a sequence of morphemes and non-morpheme lexical units.</div><br/><div id="41930854" class="c"><input type="checkbox" id="c-41930854" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41928842">parent</a><span>|</span><a href="#41926909">next</a><span>|</span><label class="collapse" for="c-41930854">[-]</label><label class="expand" for="c-41930854">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean by non-morpheme lexical units? Syntactic particles, units too small to be morphemes? Lexical items that contain multiple morphemes?<p>In either case, isn&#x27;t this something we already do well?</div><br/></div></div></div></div></div></div><div id="41926909" class="c"><input type="checkbox" id="c-41926909" checked=""/><div class="controls bullet"><span class="by">SEGyges</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41926850">parent</a><span>|</span><a href="#41927079">prev</a><span>|</span><a href="#41926927">next</a><span>|</span><label class="collapse" for="c-41926909">[-]</label><label class="expand" for="c-41926909">[2 more]</label></div><br/><div class="children"><div class="content">tokens are on average four characters and the number of residual streams (and therefore RAM) the LLM allocates to a given sequence is proportionate to the number of units of input. the flops is proportionate to their <i>square</i> in the attention calculation.<p>you can hypothetically try to ameliorate this by other means, but if you just naively drop from tokenization to character or byte level models this is what goes wrong</div><br/><div id="41932231" class="c"><input type="checkbox" id="c-41932231" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41926909">parent</a><span>|</span><a href="#41926927">next</a><span>|</span><label class="collapse" for="c-41932231">[-]</label><label class="expand" for="c-41932231">[1 more]</label></div><br/><div class="children"><div class="content">4x seq length expansion doesn’t sound that bad.</div><br/></div></div></div></div><div id="41926927" class="c"><input type="checkbox" id="c-41926927" checked=""/><div class="controls bullet"><span class="by">Centigonal</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41926850">parent</a><span>|</span><a href="#41926909">prev</a><span>|</span><a href="#41928442">next</a><span>|</span><label class="collapse" for="c-41926927">[-]</label><label class="expand" for="c-41926927">[1 more]</label></div><br/><div class="children"><div class="content">I think it has to do with both performance (smaller tokens means more tokens per sentence read and more runs per sentence generated) and with how embeddings work. You need a token for &quot;dog&quot; and a token for &quot;puppy&quot; to represent the relationship between the two as a dimension in latent space.</div><br/></div></div><div id="41928442" class="c"><input type="checkbox" id="c-41928442" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41926850">parent</a><span>|</span><a href="#41926927">prev</a><span>|</span><a href="#41927742">next</a><span>|</span><label class="collapse" for="c-41928442">[-]</label><label class="expand" for="c-41928442">[1 more]</label></div><br/><div class="children"><div class="content">Context length performance and memory scales N^2.  Smaller tokens mean worse scaling, up to a point.</div><br/></div></div></div></div><div id="41927742" class="c"><input type="checkbox" id="c-41927742" checked=""/><div class="controls bullet"><span class="by">kaycebasques</span><span>|</span><a href="#41926790">parent</a><span>|</span><a href="#41926850">prev</a><span>|</span><a href="#41928304">next</a><span>|</span><label class="collapse" for="c-41927742">[-]</label><label class="expand" for="c-41927742">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but where others see boring, I see opportunity<p>I feel this way about embeddings<p>This line of thought seems related to the old wisdom of finding innovative solutions by mucking around in the layer below whatever the &quot;tools of the trade&quot; are for your domain</div><br/></div></div><div id="41928304" class="c"><input type="checkbox" id="c-41928304" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#41926790">parent</a><span>|</span><a href="#41927742">prev</a><span>|</span><a href="#41928753">next</a><span>|</span><label class="collapse" for="c-41928304">[-]</label><label class="expand" for="c-41928304">[8 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs are notoriously bad at counting letters in words or performing simply oulipos of letter omission.<p>If it were so simple, why hasn’t this already been dealt with?<p>Multimodal VQA models also have had a hard time generalizing counting. Counting is not as simple as changing the tokenizer.</div><br/><div id="41928382" class="c"><input type="checkbox" id="c-41928382" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41928304">parent</a><span>|</span><a href="#41930459">next</a><span>|</span><label class="collapse" for="c-41928382">[-]</label><label class="expand" for="c-41928382">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m saying the oulipo rule is simple, not the task given current tokenization methods</div><br/></div></div><div id="41930459" class="c"><input type="checkbox" id="c-41930459" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41928304">parent</a><span>|</span><a href="#41928382">prev</a><span>|</span><a href="#41928753">next</a><span>|</span><label class="collapse" for="c-41930459">[-]</label><label class="expand" for="c-41930459">[6 more]</label></div><br/><div class="children"><div class="content">Should the number 23 be tokenized as one token or two tokens?</div><br/><div id="41931658" class="c"><input type="checkbox" id="c-41931658" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41930459">parent</a><span>|</span><a href="#41930516">next</a><span>|</span><label class="collapse" for="c-41931658">[-]</label><label class="expand" for="c-41931658">[2 more]</label></div><br/><div class="children"><div class="content">It doesn’t matter. The challenge with counting doesn’t have to do with tokenization. Why this got into the zeitgeist, I don’t know.</div><br/><div id="41932924" class="c"><input type="checkbox" id="c-41932924" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41931658">parent</a><span>|</span><a href="#41930516">next</a><span>|</span><label class="collapse" for="c-41932924">[-]</label><label class="expand" for="c-41932924">[1 more]</label></div><br/><div class="children"><div class="content">No LLM struggles with two digit arithmetic. 100 digit addition is possible with the use of state of the art position encodings. Counting is not bottlenecked by arithmetic at all.<p>When you ask an LLM to count the number of &quot;r&quot; in the word Strawberry, the LLM will output a random number. If you ask it to separate the letters into S t r a w b e r r y, then each letter is tokenized independently and the attention mechanism is capable of performing the task.<p>What you are doing is essentially denying that the problem exists.</div><br/></div></div></div></div><div id="41930516" class="c"><input type="checkbox" id="c-41930516" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41930459">parent</a><span>|</span><a href="#41931658">prev</a><span>|</span><a href="#41930870">next</a><span>|</span><label class="collapse" for="c-41930516">[-]</label><label class="expand" for="c-41930516">[1 more]</label></div><br/><div class="children"><div class="content">We already solved that with binary representation ;-)</div><br/></div></div><div id="41930870" class="c"><input type="checkbox" id="c-41930870" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41930459">parent</a><span>|</span><a href="#41930516">prev</a><span>|</span><a href="#41928753">next</a><span>|</span><label class="collapse" for="c-41930870">[-]</label><label class="expand" for="c-41930870">[2 more]</label></div><br/><div class="children"><div class="content">Two. That&#x27;s the reality.<p>You interpret the token sequence by constructing a parse tree, but that doesn&#x27;t require you to forget that the tokens exist.</div><br/><div id="41931177" class="c"><input type="checkbox" id="c-41931177" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#41926790">root</a><span>|</span><a href="#41930870">parent</a><span>|</span><a href="#41928753">next</a><span>|</span><label class="collapse" for="c-41931177">[-]</label><label class="expand" for="c-41931177">[1 more]</label></div><br/><div class="children"><div class="content">If you use standard BPE, you likely won&#x27;t tokenize every number by it&#x27;s digits, depending on the data set used to create the tokenizer.<p>The point is, you have a choice. You can do the tokenization however you like. The reason 23 is interesting is that there is a case to be made that a model will more likely understand 23 is related to Jordan if it&#x27;s one token, and if it&#x27;s two tokens it&#x27;s more difficult. The opposite is true for math problems.<p>The reality is whatever we want to make it. It&#x27;s likely that current schemes are... sub optimal. In practice it would be great if every token was geometrically well spaced after embedding, and preserve semantic information, among other things. The &quot;other things&quot; have taken precedent thus far.</div><br/></div></div></div></div></div></div></div></div><div id="41928753" class="c"><input type="checkbox" id="c-41928753" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41926790">parent</a><span>|</span><a href="#41928304">prev</a><span>|</span><a href="#41930268">next</a><span>|</span><label class="collapse" for="c-41928753">[-]</label><label class="expand" for="c-41928753">[1 more]</label></div><br/><div class="children"><div class="content">I wrote a whole paper about this exact topic! (Syntactic, phonetic, and related constraints)<p><a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2022.cai-1.2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2022.cai-1.2&#x2F;</a></div><br/></div></div><div id="41930268" class="c"><input type="checkbox" id="c-41930268" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#41926790">parent</a><span>|</span><a href="#41928753">prev</a><span>|</span><a href="#41927351">next</a><span>|</span><label class="collapse" for="c-41930268">[-]</label><label class="expand" for="c-41930268">[1 more]</label></div><br/><div class="children"><div class="content">And decoders.</div><br/></div></div></div></div><div id="41927351" class="c"><input type="checkbox" id="c-41927351" checked=""/><div class="controls bullet"><span class="by">Joker_vD</span><span>|</span><a href="#41926790">prev</a><span>|</span><a href="#41929132">next</a><span>|</span><label class="collapse" for="c-41927351">[-]</label><label class="expand" for="c-41927351">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You need to understand [the input data] before you can do anything meaningful with it.<p>IMHO that&#x27;s the main reason people turn to any sort of automated data-processing tools in the first place: they don&#x27;t want to look at the input data. They&#x27;d rather have &quot;the computer&quot; look at it and maybe query them back with some additional info gathering requests. But thinking on their own? Ugh.<p>So I boldly propose the new definition of AGI: it&#x27;s the data-processing entity that will (at last!) reliably liberate you from having to look at your data before you start shoving this data into that processing entity.</div><br/><div id="41928711" class="c"><input type="checkbox" id="c-41928711" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#41927351">parent</a><span>|</span><a href="#41929132">next</a><span>|</span><label class="collapse" for="c-41928711">[-]</label><label class="expand" for="c-41928711">[1 more]</label></div><br/><div class="children"><div class="content">Over the past year I&#x27;ve encountered so many situations where a person&#x27;s opinion of how well an LLM accomplishes a task actually says more about that person&#x27;s reading comprehension skills than it does the LLM&#x27;s performance. This applies to both positive and negative opinions.</div><br/></div></div></div></div><div id="41929132" class="c"><input type="checkbox" id="c-41929132" checked=""/><div class="controls bullet"><span class="by">HanClinto</span><span>|</span><a href="#41927351">prev</a><span>|</span><a href="#41932357">next</a><span>|</span><label class="collapse" for="c-41929132">[-]</label><label class="expand" for="c-41929132">[6 more]</label></div><br/><div class="children"><div class="content">I really appreciated this blog post, and in particular I appreciated the segment talking about typos.<p>We were discussing this earlier this week -- I&#x27;m helping with a RAG-like application for a project right now, and we&#x27;re concerned with how much small typos or formatting differences in users&#x27; queries can throw off our embedding distances.<p>One thought was: Should we be augmenting our training data (or at the very least, our pretraining data) with intentional typos &#x2F; substitutions &#x2F; capitalizations, just to help it learn that &quot;wrk&quot; and &quot;work&quot; are probably synonyms? I looked briefly around for typo augmentation for (pre)training, and didn&#x27;t see anything at first blush, so I&#x27;m guessing that if this is a common practice, that it&#x27;s called something else.</div><br/><div id="41929182" class="c"><input type="checkbox" id="c-41929182" checked=""/><div class="controls bullet"><span class="by">tmikaeld</span><span>|</span><a href="#41929132">parent</a><span>|</span><a href="#41929497">next</a><span>|</span><label class="collapse" for="c-41929182">[-]</label><label class="expand" for="c-41929182">[3 more]</label></div><br/><div class="children"><div class="content">I work with full text search where this is common. Here is some points.<p>Stemming: Reducing words to their base or root form (e.g., “working,” “worked” becoming “work”).<p>Lemmatization: Similar to stemming, but more sophisticated, accounting for context (e.g., “better” lemmatizes to “good”).<p>Token normalization: Standardizing tokens, such as converting “wrk” to “work” through predefined rules (case folding, character replacement).<p>Fuzzy matching: Allowing approximate matches based on edit distance (e.g., “wrk” matches “work” due to minimal character difference).<p>Phonetic matching: Matching words that sound similar, sometimes used to match abbreviations or common misspellings.<p>Thesaurus-based search: Using a predefined list of synonyms or alternative spellings to expand search queries.<p>Most of these are open and free lists you can use, check the sources on manticore search for example.</div><br/><div id="41929887" class="c"><input type="checkbox" id="c-41929887" checked=""/><div class="controls bullet"><span class="by">soared</span><span>|</span><a href="#41929132">root</a><span>|</span><a href="#41929182">parent</a><span>|</span><a href="#41930878">next</a><span>|</span><label class="collapse" for="c-41929887">[-]</label><label class="expand" for="c-41929887">[1 more]</label></div><br/><div class="children"><div class="content">Porter stemming is currently widely used in adtech for keywords.</div><br/></div></div><div id="41930878" class="c"><input type="checkbox" id="c-41930878" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#41929132">root</a><span>|</span><a href="#41929182">parent</a><span>|</span><a href="#41929887">prev</a><span>|</span><a href="#41929497">next</a><span>|</span><label class="collapse" for="c-41930878">[-]</label><label class="expand" for="c-41930878">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Lemmatization: Similar to stemming, but more sophisticated, accounting for context (e.g., “better” lemmatizes to “good”).<p>I don&#x27;t understand. How is that different from stemming? What&#x27;s the base form of &quot;better&quot; if not &quot;good&quot;? The nature of the relationship between &quot;better&quot; and &quot;good&quot; is no different from that between &quot;work&quot; and &quot;worked&quot;.</div><br/></div></div></div></div><div id="41929497" class="c"><input type="checkbox" id="c-41929497" checked=""/><div class="controls bullet"><span class="by">andix</span><span>|</span><a href="#41929132">parent</a><span>|</span><a href="#41929182">prev</a><span>|</span><a href="#41929659">next</a><span>|</span><label class="collapse" for="c-41929497">[-]</label><label class="expand" for="c-41929497">[1 more]</label></div><br/><div class="children"><div class="content">For queries there is an easy solution: give the question&#x2F;search term to a LLM and let it rephrase it. A lot of basic RAG examples do that.<p>This might also work for indexing your data, but has the potential to get really expensive quickly.</div><br/></div></div><div id="41929659" class="c"><input type="checkbox" id="c-41929659" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#41929132">parent</a><span>|</span><a href="#41929497">prev</a><span>|</span><a href="#41932357">next</a><span>|</span><label class="collapse" for="c-41929659">[-]</label><label class="expand" for="c-41929659">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad this is mentioned. I&#x27;ve suspected that using correct grammar, punctuation and spelling greatly impacts response quality. It&#x27;s hard to objectify so I&#x27;ve just decided to write my prompts in perfect English just to be sure. I have a friend who prompts like he texts and I&#x27;ve always felt he was getting lower quality responses. Not unusable, just a little worse, and he needs to correct it more.</div><br/></div></div></div></div><div id="41932357" class="c"><input type="checkbox" id="c-41932357" checked=""/><div class="controls bullet"><span class="by">cranium</span><span>|</span><a href="#41929132">prev</a><span>|</span><a href="#41932762">next</a><span>|</span><label class="collapse" for="c-41932357">[-]</label><label class="expand" for="c-41932357">[1 more]</label></div><br/><div class="children"><div class="content">I finally understood the weirdness of tokenizers after watching the video Andrej Karpathy made: &quot;Let&#x27;s build the GPT Tokenizer&quot; (<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=zduSFxRajkE" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=zduSFxRajkE</a>).<p>He goes through why we need them instead of raw byte sequences (too expensive) and how the Byte Pair Encoding algorithm works. Worth spending 2h for the deeper understanding if you deal with LLMs.</div><br/></div></div><div id="41932762" class="c"><input type="checkbox" id="c-41932762" checked=""/><div class="controls bullet"><span class="by">maytc</span><span>|</span><a href="#41932357">prev</a><span>|</span><a href="#41927851">next</a><span>|</span><label class="collapse" for="c-41932762">[-]</label><label class="expand" for="c-41932762">[1 more]</label></div><br/><div class="children"><div class="content">The difference in the dates example seems right to me
20 October 2024 and 2024-20-10 are not the same.<p>Months in different locales can be written as yyyy-MM-dd. It can also be a catalog&#x2F;reference number. So, it seems right that their embedding similarity is not perfectly aligned.<p>So, it&#x27;s not a tokenizer problem. The text meant different things according to the LLM.</div><br/></div></div><div id="41927851" class="c"><input type="checkbox" id="c-41927851" checked=""/><div class="controls bullet"><span class="by">yoelhacks</span><span>|</span><a href="#41932762">prev</a><span>|</span><a href="#41927926">next</a><span>|</span><label class="collapse" for="c-41927851">[-]</label><label class="expand" for="c-41927851">[4 more]</label></div><br/><div class="children"><div class="content">I used to work on an app that very heavily leaned on Elasticsearch to do advanced text querying for similarities between a 1-2 sentence input and a corpus of paragraph+ length documents.<p>It was fascinating how much tokenization strategies could affect a particular subset of queries. A really great example is a &quot;W-4&quot; or &quot;W4&quot; Standard tokenization might split on the &quot;-&quot; or split on letter &#x2F; number boundaries. That input now becomes completely unidentifiable in the index, when it otherwise would have been a very rich factor in matching HR &#x2F; salary &#x2F; tax related content.<p>Different domain, but this doesn&#x27;t shock me at all.</div><br/><div id="41928878" class="c"><input type="checkbox" id="c-41928878" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#41927851">parent</a><span>|</span><a href="#41930515">next</a><span>|</span><label class="collapse" for="c-41928878">[-]</label><label class="expand" for="c-41928878">[2 more]</label></div><br/><div class="children"><div class="content">The trained embedding vectors for the token equivalents of W4 and W-4 would be mapped to a similar space due to their appearance in the same contexts.</div><br/><div id="41930019" class="c"><input type="checkbox" id="c-41930019" checked=""/><div class="controls bullet"><span class="by">dangerlibrary</span><span>|</span><a href="#41927851">root</a><span>|</span><a href="#41928878">parent</a><span>|</span><a href="#41930515">next</a><span>|</span><label class="collapse" for="c-41930019">[-]</label><label class="expand" for="c-41930019">[1 more]</label></div><br/><div class="children"><div class="content">The point of the GP post is that the &quot;w-4&quot; token had very different results from [&quot;w&quot;, &quot;-4&quot;] or similar algorithms where the &quot;w&quot; and &quot;4&quot; wound up in separate tokens.</div><br/></div></div></div></div><div id="41930515" class="c"><input type="checkbox" id="c-41930515" checked=""/><div class="controls bullet"><span class="by">AStrangeMorrow</span><span>|</span><a href="#41927851">parent</a><span>|</span><a href="#41928878">prev</a><span>|</span><a href="#41927926">next</a><span>|</span><label class="collapse" for="c-41930515">[-]</label><label class="expand" for="c-41930515">[1 more]</label></div><br/><div class="children"><div class="content">Yes, used to work on a system that has elasticsearch and also some custom Word2Vec models. What had the most impact on the quality of the search is ES and on the quality of our W2V model were tokenization and a custom ngrams system.</div><br/></div></div></div></div><div id="41927926" class="c"><input type="checkbox" id="c-41927926" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#41927851">prev</a><span>|</span><a href="#41927789">next</a><span>|</span><label class="collapse" for="c-41927926">[-]</label><label class="expand" for="c-41927926">[1 more]</label></div><br/><div class="children"><div class="content">&gt; One of the things I noticed over the past year is how a lot of developers who are used to developing in the traditional (deterministic) space fail to change the way they should think about problems in the statistical space which is ultimately what LLM apps are.<p>I’m a developer and don’t struggle with this, where I really struggle is trying to explain this to users.</div><br/></div></div><div id="41927789" class="c"><input type="checkbox" id="c-41927789" checked=""/><div class="controls bullet"><span class="by">bcherry</span><span>|</span><a href="#41927926">prev</a><span>|</span><a href="#41931053">next</a><span>|</span><label class="collapse" for="c-41927789">[-]</label><label class="expand" for="c-41927789">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of interesting because I think most people implementing RAG aren&#x27;t even thinking about tokenization at all. They&#x27;re thinking about embeddings:<p>1. chunk the corpus of data (various strategies but they&#x27;re all somewhat intuitive)<p>2. compute embedding for each chunk<p>3. generate search query&#x2F;queries<p>4. compute embedding for each query<p>5. rank corpus chunks by distance to query (vector search)<p>6. construct return values (e.g chunk + surrounding context, or whole doc, etc)<p>So this article really gets at the importance of a hidden, relatively mundane-feeling, operation that occurs which can have an outsized impact on the performance of the system. I do wish it had more concrete recommendations in the last section and code sample of a robust project with normalization, fine-tuning, and eval.</div><br/></div></div><div id="41931053" class="c"><input type="checkbox" id="c-41931053" checked=""/><div class="controls bullet"><span class="by">r_hanz</span><span>|</span><a href="#41927789">prev</a><span>|</span><a href="#41928196">next</a><span>|</span><label class="collapse" for="c-41931053">[-]</label><label class="expand" for="c-41931053">[1 more]</label></div><br/><div class="children"><div class="content">Very nicely written article. Personally, I find RAG (and more abstractly, vector search) the only mildly interesting development in the latest LLM fad, and have always felt that LLMs sit way too far down the diminishing returns curve to be interesting. However, I can’t believe tokenization and embeddings in general, are not broadly considered the absolutely most paramount aspect of all deep learning. The latent space your model captures is the most important aspect of the whole pipeline, or else what is any deep learning model even doing?</div><br/></div></div><div id="41928196" class="c"><input type="checkbox" id="c-41928196" checked=""/><div class="controls bullet"><span class="by">halyax7</span><span>|</span><a href="#41931053">prev</a><span>|</span><a href="#41929153">next</a><span>|</span><label class="collapse" for="c-41928196">[-]</label><label class="expand" for="c-41928196">[4 more]</label></div><br/><div class="children"><div class="content">an issue I&#x27;ve seen in several RAG implementations is assuming that the target documents, however cleverly they&#x27;re chunked, will be good search keys for incoming queries. Unless your incoming search text looks semantically like the documents you&#x27;re searching over (not the case in general), you&#x27;ll get bad hits. On a recent project, we saw a big improvement in retrieval relevance when we separated the search keys from the returned values (chunked documents), and we used an LM to generate appropriate keys which were then embedded. Appropriate in this case means &quot;sentences like what the user might input if theyre expecting this chunk back&quot;</div><br/><div id="41929136" class="c"><input type="checkbox" id="c-41929136" checked=""/><div class="controls bullet"><span class="by">marlott</span><span>|</span><a href="#41928196">parent</a><span>|</span><a href="#41929153">next</a><span>|</span><label class="collapse" for="c-41929136">[-]</label><label class="expand" for="c-41929136">[3 more]</label></div><br/><div class="children"><div class="content">Interesting! So you basically got a LM to rephrase the search phrase&#x2F;keys into the style of the target documents, then used that in the RAG pipeline? Did you do an initial search first to limit the documents?</div><br/><div id="41929827" class="c"><input type="checkbox" id="c-41929827" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#41928196">root</a><span>|</span><a href="#41929136">parent</a><span>|</span><a href="#41929153">next</a><span>|</span><label class="collapse" for="c-41929827">[-]</label><label class="expand" for="c-41929827">[2 more]</label></div><br/><div class="children"><div class="content">IIUC they&#x27;re doing some sort of &quot;q&#x2F;a&quot; for each chunk from documents, where they ask an LLM to &quot;play the user role and ask a question that would be answered by this chunk&quot;. They then embed those questions, and match live user queries with those questions first, then maybe re-rank on the document chunks retrieved.</div><br/></div></div></div></div></div></div><div id="41929153" class="c"><input type="checkbox" id="c-41929153" checked=""/><div class="controls bullet"><span class="by">woolr</span><span>|</span><a href="#41928196">prev</a><span>|</span><a href="#41929465">next</a><span>|</span><label class="collapse" for="c-41929153">[-]</label><label class="expand" for="c-41929153">[2 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t repro some of the numbers in this blog post, for example:<p><pre><code>  from sentence_transformers import SentenceTransformer
  from sentence_transformers import util

  model = SentenceTransformer(&#x27;all-MiniLM-L6-v2&#x27;)

  data_to_check = [
    &quot;I have recieved wrong package&quot;,
    &quot;I hve recieved wrong package&quot;
  ]
  embeddings = model.encode(data_to_check)
  util.cos_sim(embeddings, embeddings)
</code></pre>
Outputs:<p><pre><code>  tensor([[1.0000, 0.9749],
        [0.9749, 1.0000]])</code></pre></div><br/><div id="41929263" class="c"><input type="checkbox" id="c-41929263" checked=""/><div class="controls bullet"><span class="by">1986</span><span>|</span><a href="#41929153">parent</a><span>|</span><a href="#41929465">next</a><span>|</span><label class="collapse" for="c-41929263">[-]</label><label class="expand" for="c-41929263">[1 more]</label></div><br/><div class="children"><div class="content">Your data differs from theirs - they have  &quot;I have received wrong package&quot; vs &quot;I hve received wrong pckage&quot;, you misspelled &quot;received&quot; in both and didn&#x27;t omit an &quot;a&quot; from &quot;package&quot; in the &quot;bad&quot; data</div><br/></div></div></div></div><div id="41929465" class="c"><input type="checkbox" id="c-41929465" checked=""/><div class="controls bullet"><span class="by">andix</span><span>|</span><a href="#41929153">prev</a><span>|</span><a href="#41927601">next</a><span>|</span><label class="collapse" for="c-41929465">[-]</label><label class="expand" for="c-41929465">[1 more]</label></div><br/><div class="children"><div class="content">This is an awesome article, but I’m missing the part where solutions for each of the problems were discussed.<p>Run a spell check before tokenizing? Maybe even tokenize the misspelled word and the potential corrected word next to each other like „misspld (misspelled)“?<p>For the issue with the brand names the tokenizer doesn’t know, I have no idea how to handle it. This problem is probably even worse in less common languages, or in languages which use a lot of compound words.</div><br/></div></div><div id="41927601" class="c"><input type="checkbox" id="c-41927601" checked=""/><div class="controls bullet"><span class="by">ratedgene</span><span>|</span><a href="#41929465">prev</a><span>|</span><a href="#41928909">next</a><span>|</span><label class="collapse" for="c-41927601">[-]</label><label class="expand" for="c-41927601">[3 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t someone expand on this<p>&gt; Chunking is more or less a fixable problem with some clever techniques: these are pretty well documented around the internet;<p>Curious about what chunking solutions are out there for different sets of data&#x2F;problems</div><br/><div id="41928641" class="c"><input type="checkbox" id="c-41928641" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#41927601">parent</a><span>|</span><a href="#41928351">next</a><span>|</span><label class="collapse" for="c-41928641">[-]</label><label class="expand" for="c-41928641">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s only &quot;solved&quot; if you&#x27;re okay with a 50-90% retrieval rate or have particularly nice data. There&#x27;s a lot of stuff like &quot;referencing the techniques from Chapter 2 we do &lt;blah&gt;&quot; in the wild, and any chunking solution is unlikely to correctly answer queries involving both Chapter 2 and &lt;blah&gt;, at least not without significant false positive rates.<p>That said, the chunking people are doing is worse than the SOTA. The core thing you want to do is understand your data well enough to ensure that any question, as best as possible, has relevant data within a single chunk. Details vary (maybe the details are what you&#x27;re asking for?).</div><br/></div></div><div id="41928351" class="c"><input type="checkbox" id="c-41928351" checked=""/><div class="controls bullet"><span class="by">pphysch</span><span>|</span><a href="#41927601">parent</a><span>|</span><a href="#41928641">prev</a><span>|</span><a href="#41928909">next</a><span>|</span><label class="collapse" for="c-41928351">[-]</label><label class="expand" for="c-41928351">[1 more]</label></div><br/><div class="children"><div class="content">Most data has semantic boundaries: whether tokens, words, lines, paragraphs, blocks, sections, articles, chapters, versions, etc. and ideally the chunking algorithm will align with those boundaries in the actual data. But there is a lot of variety.</div><br/></div></div></div></div><div id="41928909" class="c"><input type="checkbox" id="c-41928909" checked=""/><div class="controls bullet"><span class="by">quirkot</span><span>|</span><a href="#41927601">prev</a><span>|</span><a href="#41926786">next</a><span>|</span><label class="collapse" for="c-41928909">[-]</label><label class="expand" for="c-41928909">[6 more]</label></div><br/><div class="children"><div class="content">Is this true?<p>&gt;&gt; Do not panic! A lot of the large LLM vocabularies are pretty huge (30k-300k tokens large)<p>Seems small by an order of magnitude (at least). English alone is 1+ millions words</div><br/><div id="41929657" class="c"><input type="checkbox" id="c-41929657" checked=""/><div class="controls bullet"><span class="by">macleginn</span><span>|</span><a href="#41928909">parent</a><span>|</span><a href="#41928967">next</a><span>|</span><label class="collapse" for="c-41929657">[-]</label><label class="expand" for="c-41929657">[2 more]</label></div><br/><div class="children"><div class="content">Most of these 1+ million words are almost never used, so 200k is plenty for English. Optimistically, we hope that rarer words would be longer and to some degree compositional (optim-ism, optim-istic, etc.), but unfortunately this is not what tokenisers arrive at (and you are more likely to get &quot;opt-i-mis-m&quot; or something like that). People have tried to optimise tokenisation and the main part of LLM training jointly, which leads to more sensible results, but this is unworkable for larger models, so we are stuck with inflated basic vocabularies.<p>It is also probably possible now to go even for larger vocabularies, in the 1-2 million range (by factorising the embedding matrix, for example), but this does not lead to noticeable improvements in performance, AFAIK.</div><br/><div id="41930151" class="c"><input type="checkbox" id="c-41930151" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41928909">root</a><span>|</span><a href="#41929657">parent</a><span>|</span><a href="#41928967">next</a><span>|</span><label class="collapse" for="c-41930151">[-]</label><label class="expand" for="c-41930151">[1 more]</label></div><br/><div class="children"><div class="content">Performance would be massively improved on constrained text tasks. That alone makes it worth it to expand the vocabulary size.</div><br/></div></div></div></div><div id="41928967" class="c"><input type="checkbox" id="c-41928967" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#41928909">parent</a><span>|</span><a href="#41929657">prev</a><span>|</span><a href="#41931564">next</a><span>|</span><label class="collapse" for="c-41928967">[-]</label><label class="expand" for="c-41928967">[1 more]</label></div><br/><div class="children"><div class="content">Tokens are often sub-word, all the way down to bytes (which are implicitly understood as UTF8 but models will sometimes generate invalid UTF8...).</div><br/></div></div><div id="41931564" class="c"><input type="checkbox" id="c-41931564" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#41928909">parent</a><span>|</span><a href="#41928967">prev</a><span>|</span><a href="#41928950">next</a><span>|</span><label class="collapse" for="c-41931564">[-]</label><label class="expand" for="c-41931564">[1 more]</label></div><br/><div class="children"><div class="content">BPE is complete.  Every valid Unicode string can be encoded with any BPE tokenizer.<p>BPE basically starts with a token for every valid value for a Unicode byte and then creates new tokens by looking at common pairs of bytes (‘t’ followed by ‘h’ becomes a new token ’th’)</div><br/></div></div></div></div><div id="41926786" class="c"><input type="checkbox" id="c-41926786" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#41928909">prev</a><span>|</span><a href="#41926706">next</a><span>|</span><label class="collapse" for="c-41926786">[-]</label><label class="expand" for="c-41926786">[3 more]</label></div><br/><div class="children"><div class="content">I think I take something different away from the article, yes tokenizers are important but they&#x27;re a means to get at something much much bigger which is how to clean up and normalize unstructured data. It&#x27;s a current endeavor of mine at $dayjob for how to do this in a way that can work reasonably well even for badly mangled documents. I don&#x27;t have any silver bullets, at least nothing worthy of a blog-post yet, but since this is needed when dealing with OCR documents so &quot;post-ocr correction&quot; turns up quite a few different approaches.<p>And this is an aside, but I see folks using LLMs to do this correction in the first place. I don&#x27;t think using LLMs to do correction in a multi-pass system is inherently bad but I haven&#x27;t been able to get good results out of &quot;call&#x2F;response&quot; (i.e. a prompt to clean up this text). The best results are when you&#x27;re running an LLM locally and cleaning incrementally by using token probabilities to help guide you. You get some candidate words from your wordlist based on the fuzzy match of the text you do have, and candidate words predicted from the previous text and when both align -- ding! It&#x27;s (obviously) not the fastest method however.</div><br/><div id="41926979" class="c"><input type="checkbox" id="c-41926979" checked=""/><div class="controls bullet"><span class="by">SEGyges</span><span>|</span><a href="#41926786">parent</a><span>|</span><a href="#41927868">next</a><span>|</span><label class="collapse" for="c-41926979">[-]</label><label class="expand" for="c-41926979">[1 more]</label></div><br/><div class="children"><div class="content">you might have better luck giving the LM the original document and having it generate its own OCR independently, then asking the llm to tiebreak between its own generation and the OCR output while the image is still in the context window until it is satisfied that it got things correct</div><br/></div></div><div id="41927868" class="c"><input type="checkbox" id="c-41927868" checked=""/><div class="controls bullet"><span class="by">7thpower</span><span>|</span><a href="#41926786">parent</a><span>|</span><a href="#41926979">prev</a><span>|</span><a href="#41926706">next</a><span>|</span><label class="collapse" for="c-41927868">[-]</label><label class="expand" for="c-41927868">[1 more]</label></div><br/><div class="children"><div class="content">This is interesting. What types of content are you using this approach on and how does it handle semi structured data? For instance, embedded tables.</div><br/></div></div></div></div></div></div></div></div></div></body></html>