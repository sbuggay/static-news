<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730624464106" as="style"/><link rel="stylesheet" href="styles.css?v=1730624464106"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2111.08566">Spann: Highly-Efficient Billion-Scale Approximate Nearest Neighbor Search (2021)</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>ksec</span> | <span>26 comments</span></div><br/><div><div id="42030281" class="c"><input type="checkbox" id="c-42030281" checked=""/><div class="controls bullet"><span class="by">rbranson</span><span>|</span><a href="#42029673">next</a><span>|</span><label class="collapse" for="c-42030281">[-]</label><label class="expand" for="c-42030281">[7 more]</label></div><br/><div class="children"><div class="content">One of the only (the only?) commercial grade implementations was launched recently by us at PlanetScale:<p><a href="https:&#x2F;&#x2F;planetscale.com&#x2F;blog&#x2F;announcing-planetscale-vectors-public-beta" rel="nofollow">https:&#x2F;&#x2F;planetscale.com&#x2F;blog&#x2F;announcing-planetscale-vectors-...</a></div><br/><div id="42031724" class="c"><input type="checkbox" id="c-42031724" checked=""/><div class="controls bullet"><span class="by">Sirupsen</span><span>|</span><a href="#42030281">parent</a><span>|</span><a href="#42030820">next</a><span>|</span><label class="collapse" for="c-42031724">[-]</label><label class="expand" for="c-42031724">[1 more]</label></div><br/><div class="children"><div class="content">It works great. We’ve had SPANN in production since October of 2023 at <a href="https:&#x2F;&#x2F;turbopuffer.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;turbopuffer.com&#x2F;</a></div><br/></div></div><div id="42030820" class="c"><input type="checkbox" id="c-42030820" checked=""/><div class="controls bullet"><span class="by">noahbp</span><span>|</span><a href="#42030281">parent</a><span>|</span><a href="#42031724">prev</a><span>|</span><a href="#42031811">next</a><span>|</span><label class="collapse" for="c-42030820">[-]</label><label class="expand" for="c-42030820">[4 more]</label></div><br/><div class="children"><div class="content">No ability to host offline, and for 1&#x2F;8th CPU + 1GB RAM + 800 GB storage, the price is $1,224&#x2F;month?<p>I&#x27;m sure it works great, but at that price point, I&#x27;m stuck with self-hosting Postgres+pgvector.</div><br/><div id="42031087" class="c"><input type="checkbox" id="c-42031087" checked=""/><div class="controls bullet"><span class="by">bddicken</span><span>|</span><a href="#42030281">root</a><span>|</span><a href="#42030820">parent</a><span>|</span><a href="#42030833">next</a><span>|</span><label class="collapse" for="c-42031087">[-]</label><label class="expand" for="c-42031087">[1 more]</label></div><br/><div class="children"><div class="content">Just pointing out that what you&#x27;re paying for is actually 3x these resources. By default you get a primary server and two replicas with whatever specification you choose. This is primarily for data durability, but you can also send queries to your replicas.</div><br/></div></div><div id="42030833" class="c"><input type="checkbox" id="c-42030833" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#42030281">root</a><span>|</span><a href="#42030820">parent</a><span>|</span><a href="#42031087">prev</a><span>|</span><a href="#42031811">next</a><span>|</span><label class="collapse" for="c-42030833">[-]</label><label class="expand" for="c-42030833">[2 more]</label></div><br/><div class="children"><div class="content">Which works completely fine as long as you know how to manage your own db without getting wrecked!<p>But yes, I it seems extereme. But it is also cheaper than hiring a dedicated postgres&#x2F;db guy who will cost 5 to 10x more per month.</div><br/><div id="42030969" class="c"><input type="checkbox" id="c-42030969" checked=""/><div class="controls bullet"><span class="by">mhuffman</span><span>|</span><a href="#42030281">root</a><span>|</span><a href="#42030833">parent</a><span>|</span><a href="#42031811">next</a><span>|</span><label class="collapse" for="c-42030969">[-]</label><label class="expand" for="c-42030969">[1 more]</label></div><br/><div class="children"><div class="content">There are plenty of set-it-and-forget-it vector dbs right now, maybe too many![0]<p>[0]<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41985176">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41985176</a></div><br/></div></div></div></div></div></div></div></div><div id="42029673" class="c"><input type="checkbox" id="c-42029673" checked=""/><div class="controls bullet"><span class="by">aaronblohowiak</span><span>|</span><a href="#42030281">prev</a><span>|</span><a href="#42030901">next</a><span>|</span><label class="collapse" for="c-42029673">[-]</label><label class="expand" for="c-42029673">[14 more]</label></div><br/><div class="children"><div class="content">Kinda related, hopefully someone here in comments can help: what’s your favorite precise nn search that works on arm Macs for in memory dataset; 100k times &#x2F; 300  float32 dims per item ? Ideally supporting cosine similarity<p>Faiss seems big to get going, tried n2 but doesn’t seem to want to install via pip.. if anyone has a go-to I’d be grateful. Thanks.</div><br/><div id="42030675" class="c"><input type="checkbox" id="c-42030675" checked=""/><div class="controls bullet"><span class="by">lmcinnes</span><span>|</span><a href="#42029673">parent</a><span>|</span><a href="#42031386">next</a><span>|</span><label class="collapse" for="c-42030675">[-]</label><label class="expand" for="c-42030675">[1 more]</label></div><br/><div class="children"><div class="content">If you just want in-memory then PyNNDescent (<a href="https:&#x2F;&#x2F;github.com&#x2F;lmcinnes&#x2F;pynndescent">https:&#x2F;&#x2F;github.com&#x2F;lmcinnes&#x2F;pynndescent</a>) can work pretty well. It should install easily with pip, works well at the scales you mention, and supports a large number of metrics, including cosine.</div><br/></div></div><div id="42031386" class="c"><input type="checkbox" id="c-42031386" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#42029673">parent</a><span>|</span><a href="#42030675">prev</a><span>|</span><a href="#42030338">next</a><span>|</span><label class="collapse" for="c-42031386">[-]</label><label class="expand" for="c-42031386">[1 more]</label></div><br/><div class="children"><div class="content">For just 100K items why don&#x27;t you simply load the embeds into numpy and use cosine similarity directly? It&#x27;s like 2 lines of code and works well for &quot;small&quot; number of documents. This would be exact NN search.<p>Use approximate NN search when you have high volume of searches over millions of vectors.</div><br/></div></div><div id="42030338" class="c"><input type="checkbox" id="c-42030338" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#42029673">parent</a><span>|</span><a href="#42031386">prev</a><span>|</span><a href="#42030996">next</a><span>|</span><label class="collapse" for="c-42030338">[-]</label><label class="expand" for="c-42030338">[5 more]</label></div><br/><div class="children"><div class="content">Out of interest is nearest neighbour even remotely effective with 300 dimensions?<p>Seems to me that <i>unless</i> most of the variation is in only a couple of directions, pretty much <i>no</i> points are going to be anywhere near one another.<p>So with cosine similarity you&#x27;re either going to get low scores for pretty much everything or a basic PCA should be able to reduce the dimensionality significantly.</div><br/><div id="42030533" class="c"><input type="checkbox" id="c-42030533" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#42029673">root</a><span>|</span><a href="#42030338">parent</a><span>|</span><a href="#42031332">next</a><span>|</span><label class="collapse" for="c-42030533">[-]</label><label class="expand" for="c-42030533">[2 more]</label></div><br/><div class="children"><div class="content">I think you are referring to what&#x27;s known as the &quot;curse of dimensionality,&quot; where as dimensionality increases, the distance between points tends to become more uniform and large. However, nearest neighbor search can still work effectively because of several key factors:<p>1. Real data rarely occupies the full high-dimensional space uniformly. Instead, it typically lies on or near a lower-dimensional manifold embedded within the high-dimensional space. This is often called the &quot;manifold hypothesis.&quot;<p>2. While distances may be large in absolute terms, _relative_ distances still maintain meaningful relationships. If point A is closer to point B than to point C in this high-dimensional space, that proximity often still indicates semantic similarity.<p>3. The data points that matter for a given problem often cluster in meaningful ways. Even in high dimensions, these clusters can maintain separation that makes nearest neighbor search useful.<p>Let me give a concrete example: Consider a dataset of images. While an image might be represented in a very high-dimensional space (e.g., thousands of pixels), images of dogs will tend to be closer to other dog images than to images of cars, even in this high-dimensional space. The meaningful features create a structure that nearest neighbor search can exploit.<p>Spam filtering is another area where nearest neighbor is used to good effect. When you know that a certain embedding representing a spam message (in any medium - email, comments, whatever), then if other messages come along and are _relatively_ close to that one, you may conclude that they are on the right side of the manifold to be considered spam.<p>You could train a special model to define this manifold, but spam changes all the time and constant training doesn’t work well.</div><br/><div id="42031455" class="c"><input type="checkbox" id="c-42031455" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#42029673">root</a><span>|</span><a href="#42030533">parent</a><span>|</span><a href="#42031332">next</a><span>|</span><label class="collapse" for="c-42031455">[-]</label><label class="expand" for="c-42031455">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; as the &quot;curse of dimensionality,&quot; where as dimensionality increases, the distance between points tends to become more uniform and large<p>Since embeddings are the middle layer of an ANN, doesn&#x27;t this suggest that there are too many dimensions used during training. I would think a training goal would be to have relatively uniform coverage of the space</div><br/></div></div></div></div><div id="42031332" class="c"><input type="checkbox" id="c-42031332" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#42029673">root</a><span>|</span><a href="#42030338">parent</a><span>|</span><a href="#42030533">prev</a><span>|</span><a href="#42030990">next</a><span>|</span><label class="collapse" for="c-42031332">[-]</label><label class="expand" for="c-42031332">[1 more]</label></div><br/><div class="children"><div class="content">When dimensionality increases so does distance, but distance doesn&#x27;t matter, we only care about relative distance compared to different points.<p>If clustering works or not has nothing to do with the dimensionality of the space, and everything to do with the distribution of the points.</div><br/></div></div><div id="42030990" class="c"><input type="checkbox" id="c-42030990" checked=""/><div class="controls bullet"><span class="by">mhuffman</span><span>|</span><a href="#42029673">root</a><span>|</span><a href="#42030338">parent</a><span>|</span><a href="#42031332">prev</a><span>|</span><a href="#42030996">next</a><span>|</span><label class="collapse" for="c-42030990">[-]</label><label class="expand" for="c-42030990">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Out of interest is nearest neighbour even remotely effective with 300 dimensions?<p>They are&#x2F;can be anyway. I had data with 50,000 dimensions, which after applying dimensionality reduction techniques got it &quot;down to&quot; around 300! ANN worked very well on those vectors. This was prior to the glut of vector dbs we have available now, so it was all in-memory and used direct library calls to find neighbors.</div><br/></div></div></div></div><div id="42030996" class="c"><input type="checkbox" id="c-42030996" checked=""/><div class="controls bullet"><span class="by">mhuffman</span><span>|</span><a href="#42029673">parent</a><span>|</span><a href="#42030338">prev</a><span>|</span><a href="#42030038">next</a><span>|</span><label class="collapse" for="c-42030996">[-]</label><label class="expand" for="c-42030996">[2 more]</label></div><br/><div class="children"><div class="content">Annoy is old, but works surprisingly well and is fast.</div><br/><div id="42031364" class="c"><input type="checkbox" id="c-42031364" checked=""/><div class="controls bullet"><span class="by">wood_spirit</span><span>|</span><a href="#42029673">root</a><span>|</span><a href="#42030996">parent</a><span>|</span><a href="#42030038">next</a><span>|</span><label class="collapse" for="c-42031364">[-]</label><label class="expand" for="c-42031364">[1 more]</label></div><br/><div class="children"><div class="content">And nowadays Spotify uses voyager <a href="https:&#x2F;&#x2F;engineering.atspotify.com&#x2F;2023&#x2F;10&#x2F;introducing-voyager-spotifys-new-nearest-neighbor-search-library&#x2F;" rel="nofollow">https:&#x2F;&#x2F;engineering.atspotify.com&#x2F;2023&#x2F;10&#x2F;introducing-voyage...</a></div><br/></div></div></div></div><div id="42030038" class="c"><input type="checkbox" id="c-42030038" checked=""/><div class="controls bullet"><span class="by">ebursztein</span><span>|</span><a href="#42029673">parent</a><span>|</span><a href="#42030996">prev</a><span>|</span><a href="#42030433">next</a><span>|</span><label class="collapse" for="c-42030038">[-]</label><label class="expand" for="c-42030038">[1 more]</label></div><br/><div class="children"><div class="content">Try Usearch - it&#x27;s really fast and under rated <a href="https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;usearch">https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;usearch</a></div><br/></div></div><div id="42030433" class="c"><input type="checkbox" id="c-42030433" checked=""/><div class="controls bullet"><span class="by">peterldowns</span><span>|</span><a href="#42029673">parent</a><span>|</span><a href="#42030038">prev</a><span>|</span><a href="#42029922">next</a><span>|</span><label class="collapse" for="c-42030433">[-]</label><label class="expand" for="c-42030433">[1 more]</label></div><br/><div class="children"><div class="content">Annoy</div><br/></div></div><div id="42029710" class="c"><input type="checkbox" id="c-42029710" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#42029673">parent</a><span>|</span><a href="#42029922">prev</a><span>|</span><a href="#42030901">next</a><span>|</span><label class="collapse" for="c-42029710">[-]</label><label class="expand" for="c-42029710">[1 more]</label></div><br/><div class="children"><div class="content">Why in memory? What are your latency requirements? I found pgvector to be surprisingly performant.</div><br/></div></div></div></div><div id="42030901" class="c"><input type="checkbox" id="c-42030901" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#42029673">prev</a><span>|</span><a href="#42031791">next</a><span>|</span><label class="collapse" for="c-42030901">[-]</label><label class="expand" for="c-42030901">[1 more]</label></div><br/><div class="children"><div class="content">Can we build an OS version of this and make it easy for solo dev to self host &#x2F; roll their own?</div><br/></div></div><div id="42031791" class="c"><input type="checkbox" id="c-42031791" checked=""/><div class="controls bullet"><span class="by">graycat</span><span>|</span><a href="#42030901">prev</a><span>|</span><a href="#42029724">next</a><span>|</span><label class="collapse" for="c-42031791">[-]</label><label class="expand" for="c-42031791">[1 more]</label></div><br/><div class="children"><div class="content">Hmm, how to do a statistical hypothesis of nearest neighbor data?  Distribution free?</div><br/></div></div><div id="42029724" class="c"><input type="checkbox" id="c-42029724" checked=""/><div class="controls bullet"><span class="by">singhrac</span><span>|</span><a href="#42031791">prev</a><span>|</span><a href="#42029866">next</a><span>|</span><label class="collapse" for="c-42029724">[-]</label><label class="expand" for="c-42029724">[1 more]</label></div><br/><div class="children"><div class="content">Maybe worth a (2021) tag.</div><br/></div></div></div></div></div></div></div></body></html>