<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726131657750" as="style"/><link rel="stylesheet" href="styles.css?v=1726131657750"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/felafax/felafax">Show HN: Tune LLaMa3.1 on Google Cloud TPUs</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>felarof</span> | <span>45 comments</span></div><br/><div><div id="41518714" class="c"><input type="checkbox" id="c-41518714" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#41518871">next</a><span>|</span><label class="collapse" for="c-41518714">[-]</label><label class="expand" for="c-41518714">[1 more]</label></div><br/><div class="children"><div class="content">Very cool! Unlocking TPU training is a big win.<p>FWIW, if this helps prioritize: personally I&#x27;d find LoRA training for Llama 3.1 most useful (which it sounds like currently isn&#x27;t well-supported with Felafax?) since with something like vLLM you can serve large numbers of LoRAs that share the same underlying GPU resources (assuming they&#x27;re based on the same base model), vs full finetunes where each model will need to deploy on its own set of GPUs. In general I would guess that full finetunes are going to be less cost effective for most enterprise use cases: finetuning — whether full-finetuning or PEFT — generally improves only task-specific performance, so assuming you&#x27;ve got more than one task you want to use a model for in your business, it&#x27;ll pretty quickly become dramatically cheaper to do the tasks with LoRAs rather than full finetunes unless you&#x27;re saturating the boxes for each specific task. So, I&#x27;m hoping you guys build support for LoRA training with JAX in addition to finetuning!</div><br/></div></div><div id="41518871" class="c"><input type="checkbox" id="c-41518871" checked=""/><div class="controls bullet"><span class="by">faangguyindia</span><span>|</span><a href="#41518714">prev</a><span>|</span><a href="#41518299">next</a><span>|</span><label class="collapse" for="c-41518871">[-]</label><label class="expand" for="c-41518871">[1 more]</label></div><br/><div class="children"><div class="content">For 99% case flash is enough. Period.</div><br/></div></div><div id="41518299" class="c"><input type="checkbox" id="c-41518299" checked=""/><div class="controls bullet"><span class="by">fbn79</span><span>|</span><a href="#41518871">prev</a><span>|</span><a href="#41516535">next</a><span>|</span><label class="collapse" for="c-41518299">[-]</label><label class="expand" for="c-41518299">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m totally new to AI. If I take for example LLaMa 3.1 (small size 8B), what&#x27;s the rough budget to fine tune it against for example 1GB of extra text data, in any cloud GPU service? (if compute time is not a problem, I can wait)</div><br/><div id="41518748" class="c"><input type="checkbox" id="c-41518748" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#41518299">parent</a><span>|</span><a href="#41516535">next</a><span>|</span><label class="collapse" for="c-41518748">[-]</label><label class="expand" for="c-41518748">[2 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s assume that the average token size in your 1GB file is 4 characters (which is the average that the OpenAI tokenizer generally will get; I assume the Llama tokenizer is similar). 4 chars is 4 bytes, assuming here that you&#x27;re using UTF-8 and your characters are in the Latin range, so that means your training data is about 264MM tokens.<p>Let&#x27;s assume you&#x27;re doing a single-epoch LoRA training run. A single H100 should be enough to train Llama 3.1 8B, and it should crank through 264MM tokens in a couple hours, IMO. Since you&#x27;re not doing multi-GPU training, a PCIe H100 should be fine, and those go for about $2.50&#x2F;hr on Runpod.<p>So, about $5 for a custom model, that&#x27;s probably the best in the world at whatever your task is! (Even if it might be a little dumber at other tasks.) Insanely cheap when you think about it.<p>TPUs won&#x27;t beat H100s on price for on-demand personal use cases, but for reserved capacity (i.e. businesses) they&#x27;re slightly cheaper.</div><br/><div id="41518864" class="c"><input type="checkbox" id="c-41518864" checked=""/><div class="controls bullet"><span class="by">fbn79</span><span>|</span><a href="#41518299">root</a><span>|</span><a href="#41518748">parent</a><span>|</span><a href="#41516535">next</a><span>|</span><label class="collapse" for="c-41518864">[-]</label><label class="expand" for="c-41518864">[1 more]</label></div><br/><div class="children"><div class="content">Really incredible :O I was imagining numbers with two extra zeros</div><br/></div></div></div></div></div></div><div id="41516535" class="c"><input type="checkbox" id="c-41516535" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#41518299">prev</a><span>|</span><a href="#41517819">next</a><span>|</span><label class="collapse" for="c-41516535">[-]</label><label class="expand" for="c-41516535">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure anyone finetuning Lllama now on a regular basis is using <a href="https:&#x2F;&#x2F;github.com&#x2F;unslothai&#x2F;unsloth">https:&#x2F;&#x2F;github.com&#x2F;unslothai&#x2F;unsloth</a> so comparisons should be against that. The open source version is ~2x faster than default implementations. NVidia only, although the kernels are in Triton so might be portable.</div><br/><div id="41516588" class="c"><input type="checkbox" id="c-41516588" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#41516535">parent</a><span>|</span><a href="#41518105">next</a><span>|</span><label class="collapse" for="c-41516588">[-]</label><label class="expand" for="c-41516588">[2 more]</label></div><br/><div class="children"><div class="content">I remember seeing them on HN when the first started! I never understood what’s the price you pay, how did they get such a big speed up and less memory usage?</div><br/><div id="41516662" class="c"><input type="checkbox" id="c-41516662" checked=""/><div class="controls bullet"><span class="by">randomcatuser</span><span>|</span><a href="#41516535">root</a><span>|</span><a href="#41516588">parent</a><span>|</span><a href="#41518105">next</a><span>|</span><label class="collapse" for="c-41516662">[-]</label><label class="expand" for="c-41516662">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s previous comments, apparently the founder did a lot of math re-deriving things from scratch :)<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39672070">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39672070</a><p><a href="https:&#x2F;&#x2F;unsloth.ai&#x2F;blog&#x2F;gemma-bugs">https:&#x2F;&#x2F;unsloth.ai&#x2F;blog&#x2F;gemma-bugs</a></div><br/></div></div></div></div><div id="41518105" class="c"><input type="checkbox" id="c-41518105" checked=""/><div class="controls bullet"><span class="by">pilooch</span><span>|</span><a href="#41516535">parent</a><span>|</span><a href="#41516588">prev</a><span>|</span><a href="#41517819">next</a><span>|</span><label class="collapse" for="c-41518105">[-]</label><label class="expand" for="c-41518105">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, a lora finetune of llama 3.1 8B works on a single 24GB GPU and takes from a few hours to a few days depending on the dataset size.</div><br/></div></div></div></div><div id="41517819" class="c"><input type="checkbox" id="c-41517819" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#41516535">prev</a><span>|</span><a href="#41515307">next</a><span>|</span><label class="collapse" for="c-41517819">[-]</label><label class="expand" for="c-41517819">[4 more]</label></div><br/><div class="children"><div class="content">&gt; For example, training and serving Llama 3.1 on Google TPUs is about 30% cheaper than NVIDIA GPUs<p>When you say this, you should specify which Nvidia GPU you mean (I assume h100 SXM) and that price you are assuming for such GPU.<p>One can&#x27;t simply compare based on the on demand price on GCP, because the Nvidia GPUs there are extremely overpriced.</div><br/><div id="41518674" class="c"><input type="checkbox" id="c-41518674" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#41517819">parent</a><span>|</span><a href="#41517886">next</a><span>|</span><label class="collapse" for="c-41518674">[-]</label><label class="expand" for="c-41518674">[1 more]</label></div><br/><div class="children"><div class="content">Runpod charges $3.49&#x2F;hr for an H100 SXM, which is fairly cheap as far as on-demand H100s go. A v5p TPU is $4.20&#x2F;hr, but has 95GB RAM instead of 80GB on the H100 — so you&#x27;ll need fewer TPUs to get the same amount of RAM.<p>Runpod is ever-so-slightly cheaper than Google TPUs on-demand on a per-GB basis: about 4.3 cents an hour per GB for Runpod vs 4.4 cents an hour per GB for a TPU. But let&#x27;s look at how they compare with reserved pricing. Runpod is $2.79&#x2F;hr with a 3-month commitment (the longest commitment period they offer), whereas Google offers v5p TPUs for $2.94&#x2F;hr for a 1-year commitment (the shortest period they offer; and to be honest, you probably don&#x27;t want to make 3-year commitments in this space, since there are large perf gains in successive generations).<p>If you&#x27;re willing to do reserved capacity, Google is cheaper than Runpod per GB of RAM you need to run training or inference: Runpod is about 3.4 cents per GB per hour vs Google for about 3.09 cents per GB per hour. Additionally, Google presumably has a lot more TPU capacity than Runpod has GPU capacity, and doing multi-node training is a pain with GPUs and less so with TPUs.<p>Another cheap option to benchmark against is Lambda Labs. Now, Lambda is pretty slow to boot, and considerably more annoying to work with (e.g. they only offer preconfigured VMs, so you&#x27;ll need to do some kind of management on top of them). They offer H100s for $2.99&#x2F;hr &quot;on-demand&quot; (although in my experience, prepare to wait 20+ minutes for the machines to boot); if cold boot times don&#x27;t matter to you, they&#x27;re even better than Runpod if you need large machines (they only offer 8xH100 nodes, though: nothing smaller). For a 1-year commit, they&#x27;ll drop prices to $2.49&#x2F;hr... Which is still more expensive on a per-GB basis than TPUs — 3.11 cents per GB per hour vs 3.09 cents per GB per hour — and again I&#x27;d trust Google&#x27;s TPU capacity more than Lambda&#x27;s H100 capacity.<p>It&#x27;s not dramatically cheaper than the cheapest GPU options available, but it <i>is</i> cheaper if you&#x27;re working with reserved capacity — and probably more reliably available in large quantities.</div><br/></div></div><div id="41517886" class="c"><input type="checkbox" id="c-41517886" checked=""/><div class="controls bullet"><span class="by">spullara</span><span>|</span><a href="#41517819">parent</a><span>|</span><a href="#41518674">prev</a><span>|</span><a href="#41515307">next</a><span>|</span><label class="collapse" for="c-41517886">[-]</label><label class="expand" for="c-41517886">[2 more]</label></div><br/><div class="children"><div class="content">GCP is one of the cheapest places you can get them at scale.</div><br/><div id="41518601" class="c"><input type="checkbox" id="c-41518601" checked=""/><div class="controls bullet"><span class="by">danvdb</span><span>|</span><a href="#41517819">root</a><span>|</span><a href="#41517886">parent</a><span>|</span><a href="#41515307">next</a><span>|</span><label class="collapse" for="c-41518601">[-]</label><label class="expand" for="c-41518601">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t really say it&#x27;s the cheapest option...there are other providers like Lambda Labs or Ori.co where you can find them way cheaper</div><br/></div></div></div></div></div></div><div id="41515307" class="c"><input type="checkbox" id="c-41515307" checked=""/><div class="controls bullet"><span class="by">axpy906</span><span>|</span><a href="#41517819">prev</a><span>|</span><a href="#41514020">next</a><span>|</span><label class="collapse" for="c-41515307">[-]</label><label class="expand" for="c-41515307">[2 more]</label></div><br/><div class="children"><div class="content">I am actually not surprised by JAX converting better to XLA. Also deep respect for anybody in this space as their is lot of complexity (?) to deal with at the framework and compiler level.</div><br/><div id="41515709" class="c"><input type="checkbox" id="c-41515709" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41515307">parent</a><span>|</span><a href="#41514020">next</a><span>|</span><label class="collapse" for="c-41515709">[-]</label><label class="expand" for="c-41515709">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! Yeah, there are a few complexities and very little documentation around JAX, plus a lot of missing libraries.</div><br/></div></div></div></div><div id="41514020" class="c"><input type="checkbox" id="c-41514020" checked=""/><div class="controls bullet"><span class="by">mandoline</span><span>|</span><a href="#41515307">prev</a><span>|</span><a href="#41514231">next</a><span>|</span><label class="collapse" for="c-41514020">[-]</label><label class="expand" for="c-41514020">[2 more]</label></div><br/><div class="children"><div class="content">Do you have any apples-to-apples speed and cost comparisons across Nvidia vs. non-NVIDIA chips (as you mentioned: TPUs, Trainium, AMD GPUs)?</div><br/><div id="41514894" class="c"><input type="checkbox" id="c-41514894" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41514020">parent</a><span>|</span><a href="#41514231">next</a><span>|</span><label class="collapse" for="c-41514894">[-]</label><label class="expand" for="c-41514894">[1 more]</label></div><br/><div class="children"><div class="content">Google published this benchmark a year or so ago comparing TPU vs NVIDIA (<a href="https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blob&#x2F;main&#x2F;community-content&#x2F;vertex_model_garden&#x2F;benchmarking_reports&#x2F;jax_vit_benchmarking_report.md">https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blo...</a>)<p>Conclusion is at the bottom, but TLDR was TPUs were 33% cheaper (performance per dollar) and JAX scales very well compared to PyTorch.<p>If you are curious, there was a thorough comparison done by Cohere and they published their paper <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07181" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07181</a> -- TPU+JAX turned out to be more performant and more fault tolerant (less weird errors).</div><br/></div></div></div></div><div id="41514231" class="c"><input type="checkbox" id="c-41514231" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#41514020">prev</a><span>|</span><a href="#41515829">next</a><span>|</span><label class="collapse" for="c-41514231">[-]</label><label class="expand" for="c-41514231">[2 more]</label></div><br/><div class="children"><div class="content">What was the estimate for how much time you guys took to translate the torch to Jax vs how much you spent on XLA?</div><br/><div id="41514563" class="c"><input type="checkbox" id="c-41514563" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41514231">parent</a><span>|</span><a href="#41515829">next</a><span>|</span><label class="collapse" for="c-41514563">[-]</label><label class="expand" for="c-41514563">[1 more]</label></div><br/><div class="children"><div class="content">It took roughly 2-3 weeks to translate Torch to JAX, but I had past experience writing JAX from my time at Google.<p>We spent nearly 4 weeks getting PyTorch XLA working on TPU. Hope that answers your question!</div><br/></div></div></div></div><div id="41515829" class="c"><input type="checkbox" id="c-41515829" checked=""/><div class="controls bullet"><span class="by">tcdent</span><span>|</span><a href="#41514231">prev</a><span>|</span><a href="#41514882">next</a><span>|</span><label class="collapse" for="c-41515829">[-]</label><label class="expand" for="c-41515829">[2 more]</label></div><br/><div class="children"><div class="content">Where in the codebase is the logic specific to TPU vs. CUDA?</div><br/><div id="41516825" class="c"><input type="checkbox" id="c-41516825" checked=""/><div class="controls bullet"><span class="by">hi</span><span>|</span><a href="#41515829">parent</a><span>|</span><a href="#41514882">next</a><span>|</span><label class="collapse" for="c-41516825">[-]</label><label class="expand" for="c-41516825">[1 more]</label></div><br/><div class="children"><div class="content">The codebase heavily uses PyTorch XLA libraries (torch_xla.*), which are specific to TPU. Key TPU-specific elements include XLA device initialization, SPMD execution mode, TPU-specific data loading, and mesh-based model partitioning.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax&#x2F;blob&#x2F;main&#x2F;llama3_pytorch_xla&#x2F;llama3_70b_train.ipynb">https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax&#x2F;blob&#x2F;main&#x2F;llama3_pytorch_...</a><p>[1] <a href="https:&#x2F;&#x2F;pytorch.org&#x2F;xla&#x2F;master&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;xla&#x2F;master&#x2F;</a></div><br/></div></div></div></div><div id="41514882" class="c"><input type="checkbox" id="c-41514882" checked=""/><div class="controls bullet"><span class="by">ricw</span><span>|</span><a href="#41515829">prev</a><span>|</span><a href="#41514897">next</a><span>|</span><label class="collapse" for="c-41514882">[-]</label><label class="expand" for="c-41514882">[12 more]</label></div><br/><div class="children"><div class="content">I’m surprised how it’s only 30% cheaper vs nvidia. How come? This seems to indicate that the nvidia premium isn’t as high as everybody makes it out to be.</div><br/><div id="41515113" class="c"><input type="checkbox" id="c-41515113" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41514882">parent</a><span>|</span><a href="#41514966">next</a><span>|</span><label class="collapse" for="c-41515113">[-]</label><label class="expand" for="c-41515113">[9 more]</label></div><br/><div class="children"><div class="content">30% is a conservative estimate (to be precise, we went with this benchmark: <a href="https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blob&#x2F;main&#x2F;community-content&#x2F;vertex_model_garden&#x2F;benchmarking_reports&#x2F;jax_vit_benchmarking_report.md">https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blo...</a>). However, the actual difference we observe ranges from 30-70%.<p>Also, calculating GPU costs is getting quite nuanced, with a wide range of prices (<a href="https:&#x2F;&#x2F;cloud-gpus.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cloud-gpus.com&#x2F;</a>) and other variables that makes it harder to do apples-to-apples comparison.</div><br/><div id="41515219" class="c"><input type="checkbox" id="c-41515219" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41515113">parent</a><span>|</span><a href="#41514966">next</a><span>|</span><label class="collapse" for="c-41515219">[-]</label><label class="expand" for="c-41515219">[8 more]</label></div><br/><div class="children"><div class="content">Did you try running this task (finetuning Llama) on Nvidia GPUs? If yes, can you provide details (which cloud instance and time)?<p>I’m curious about your  reported 30-70% speedup.</div><br/><div id="41515262" class="c"><input type="checkbox" id="c-41515262" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41515219">parent</a><span>|</span><a href="#41514966">next</a><span>|</span><label class="collapse" for="c-41515262">[-]</label><label class="expand" for="c-41515262">[7 more]</label></div><br/><div class="children"><div class="content">I think you slightly misunderstood, and I wasn&#x27;t clear enough—sorry! It&#x27;s not a 30-70% speedup; it&#x27;s 30-70% more cost-efficient. This is mainly due to non-NVIDIA chipsets (e.g., Google TPU) being cheaper, with some additional efficiency gains from JAX being more closely integrated with the XLA architecture.<p>No, we haven&#x27;t run our JAX + XLA on NVIDIA chipsets yet. I&#x27;m not sure if NVIDIA has good XLA backend support.</div><br/><div id="41515364" class="c"><input type="checkbox" id="c-41515364" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41515262">parent</a><span>|</span><a href="#41514966">next</a><span>|</span><label class="collapse" for="c-41515364">[-]</label><label class="expand" for="c-41515364">[6 more]</label></div><br/><div class="children"><div class="content">Then how did you compute the 30-70% cost efficiency numbers compared to Nvidia if you haven’t run this Llama finetuning task on Nvidia GPUs?</div><br/><div id="41515482" class="c"><input type="checkbox" id="c-41515482" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41515364">parent</a><span>|</span><a href="#41514966">next</a><span>|</span><label class="collapse" for="c-41515482">[-]</label><label class="expand" for="c-41515482">[5 more]</label></div><br/><div class="children"><div class="content">Check out this benchmark where they did an analysis: <a href="https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blob&#x2F;main&#x2F;community-content&#x2F;vertex_model_garden&#x2F;benchmarking_reports&#x2F;jax_vit_benchmarking_report.md">https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blo...</a>.<p>At the bottom, it shows the calculations around the 30% cost efficiency of TPU vs GPU.<p>Our range of 30-70% is based on some numbers we collected from running fine-tuning runs on TPU and comparing them to similar runs on NVIDIA (though not using our code but other OSS libraries).</div><br/><div id="41515597" class="c"><input type="checkbox" id="c-41515597" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41515482">parent</a><span>|</span><a href="#41514966">next</a><span>|</span><label class="collapse" for="c-41515597">[-]</label><label class="expand" for="c-41515597">[4 more]</label></div><br/><div class="children"><div class="content">It would be a lot more convincing if you actually ran it yourself and did a proper apples to apples comparison, especially considering that’s the whole idea behind your project.</div><br/><div id="41516267" class="c"><input type="checkbox" id="c-41516267" checked=""/><div class="controls bullet"><span class="by">KaoruAoiShiho</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41515597">parent</a><span>|</span><a href="#41515702">next</a><span>|</span><label class="collapse" for="c-41516267">[-]</label><label class="expand" for="c-41516267">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also comparing prices on google cloud, which has its own markup, a lot more expensive than say runpod. Runpod is $1.64&#x2F;hr for the A100 on secure cloud while the A100 on Google is $4.44&#x2F;hr. A lot more expensive... yeah. So in that context a 30% price beat is actually a huge loss overall.</div><br/><div id="41517891" class="c"><input type="checkbox" id="c-41517891" checked=""/><div class="controls bullet"><span class="by">spullara</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41516267">parent</a><span>|</span><a href="#41515702">next</a><span>|</span><label class="collapse" for="c-41517891">[-]</label><label class="expand" for="c-41517891">[1 more]</label></div><br/><div class="children"><div class="content">who trains on a100 at this point lol</div><br/></div></div></div></div><div id="41515702" class="c"><input type="checkbox" id="c-41515702" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41515597">parent</a><span>|</span><a href="#41516267">prev</a><span>|</span><a href="#41514966">next</a><span>|</span><label class="collapse" for="c-41515702">[-]</label><label class="expand" for="c-41515702">[1 more]</label></div><br/><div class="children"><div class="content">Totally agree, thanks for feedback! This is one of the TODOs on our radar.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41514966" class="c"><input type="checkbox" id="c-41514966" checked=""/><div class="controls bullet"><span class="by">cherioo</span><span>|</span><a href="#41514882">parent</a><span>|</span><a href="#41515113">prev</a><span>|</span><a href="#41514897">next</a><span>|</span><label class="collapse" for="c-41514966">[-]</label><label class="expand" for="c-41514966">[2 more]</label></div><br/><div class="children"><div class="content">Nvidia margin is like 70%. Using google TPU is certainly going to erase some of that.</div><br/><div id="41516268" class="c"><input type="checkbox" id="c-41516268" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41514882">root</a><span>|</span><a href="#41514966">parent</a><span>|</span><a href="#41514897">next</a><span>|</span><label class="collapse" for="c-41516268">[-]</label><label class="expand" for="c-41516268">[1 more]</label></div><br/><div class="children"><div class="content">They sell cards and they are selling out</div><br/></div></div></div></div></div></div><div id="41514897" class="c"><input type="checkbox" id="c-41514897" checked=""/><div class="controls bullet"><span class="by">khimaros</span><span>|</span><a href="#41514882">prev</a><span>|</span><a href="#41515693">next</a><span>|</span><label class="collapse" for="c-41514897">[-]</label><label class="expand" for="c-41514897">[6 more]</label></div><br/><div class="children"><div class="content">an interesting thread with speculation about how to eventually do this on local TPUs with llama.cpp and GGUF infrastructure: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;12o96hf&#x2F;has_anyone_used_llama_with_a_tpu_instead_of_gpu&#x2F;?sort=new" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;12o96hf&#x2F;has_any...</a></div><br/><div id="41516379" class="c"><input type="checkbox" id="c-41516379" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#41514897">parent</a><span>|</span><a href="#41514947">next</a><span>|</span><label class="collapse" for="c-41516379">[-]</label><label class="expand" for="c-41516379">[4 more]</label></div><br/><div class="children"><div class="content">That’s not happening. The coral edge tpus are ancient, slow and don’t have enough mem to be meaningful and somehow still manage to be relatively expensive even 2nd hand.<p>They have some good uses but LLMs aint it</div><br/><div id="41516696" class="c"><input type="checkbox" id="c-41516696" checked=""/><div class="controls bullet"><span class="by">pogue</span><span>|</span><a href="#41514897">root</a><span>|</span><a href="#41516379">parent</a><span>|</span><a href="#41514947">next</a><span>|</span><label class="collapse" for="c-41516696">[-]</label><label class="expand" for="c-41516696">[3 more]</label></div><br/><div class="children"><div class="content">Are those the TPUs Google sells to consumers? I&#x27;ve been thinking of buying one &amp; hooking it up to a Pi just to play around with LLMs or Stable Diffusion. But I didn&#x27;t realize they were slower&#x2F;worse than other options.</div><br/><div id="41516722" class="c"><input type="checkbox" id="c-41516722" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41514897">root</a><span>|</span><a href="#41516696">parent</a><span>|</span><a href="#41514947">next</a><span>|</span><label class="collapse" for="c-41516722">[-]</label><label class="expand" for="c-41516722">[2 more]</label></div><br/><div class="children"><div class="content">The Coral TPUs have not been updated for several years. They were last updated long before the current LLM craze. They are good for simple things like object detection in photos.<p>They have almost nothing in common with Cloud TPUs.</div><br/></div></div></div></div></div></div><div id="41514947" class="c"><input type="checkbox" id="c-41514947" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41514897">parent</a><span>|</span><a href="#41516379">prev</a><span>|</span><a href="#41515693">next</a><span>|</span><label class="collapse" for="c-41514947">[-]</label><label class="expand" for="c-41514947">[1 more]</label></div><br/><div class="children"><div class="content">Ahh, the reddit thread is referring to edge TPU devices, will check it out.<p>Google also has Cloud TPUs, which are their server-side accelerators, and this is what we are initially trying to build for!</div><br/></div></div></div></div><div id="41515693" class="c"><input type="checkbox" id="c-41515693" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#41514897">prev</a><span>|</span><a href="#41515535">next</a><span>|</span><label class="collapse" for="c-41515693">[-]</label><label class="expand" for="c-41515693">[2 more]</label></div><br/><div class="children"><div class="content">Anyone want to comment on this versus the fine tune speedups from llama3.1 with unsloth?</div><br/><div id="41515939" class="c"><input type="checkbox" id="c-41515939" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41515693">parent</a><span>|</span><a href="#41515535">next</a><span>|</span><label class="collapse" for="c-41515939">[-]</label><label class="expand" for="c-41515939">[1 more]</label></div><br/><div class="children"><div class="content">Unsloth is great! They focus on single-GPU and LoRA fine-tuning on NVIDIA GPUs. We are initially trying to target multi-node, multi-TPU, full-precision training use cases.<p>That said, in terms of single-GPU speed, we believe we would be behind but not too far off, thanks to JAX+TPU&#x27;s more performant stack. Additionally, we can do larger-scale multi-node training on TPUs.<p>There are still more optimizations we need to do for Llama 3.1, such as adding Pallas memory attention kernels, etc</div><br/></div></div></div></div><div id="41515535" class="c"><input type="checkbox" id="c-41515535" checked=""/><div class="controls bullet"><span class="by">stroupwaffle</span><span>|</span><a href="#41515693">prev</a><span>|</span><a href="#41517566">next</a><span>|</span><label class="collapse" for="c-41515535">[-]</label><label class="expand" for="c-41515535">[2 more]</label></div><br/><div class="children"><div class="content">You might want to change Road Runner logo because it’s definitely copyrighted</div><br/><div id="41515791" class="c"><input type="checkbox" id="c-41515791" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41515535">parent</a><span>|</span><a href="#41517566">next</a><span>|</span><label class="collapse" for="c-41515791">[-]</label><label class="expand" for="c-41515791">[1 more]</label></div><br/><div class="children"><div class="content">Haha, yeah, good point. I&#x27;ll remove it.</div><br/></div></div></div></div></div></div></div></div></div></body></html>