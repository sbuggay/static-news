<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1710838876782" as="style"/><link rel="stylesheet" href="styles.css?v=1710838876782"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://rysana.com/inversion">Inversion: Fast, Reliable Structured LLMs</a> <span class="domain">(<a href="https://rysana.com">rysana.com</a>)</span></div><div class="subtext"><span>swyx</span> | <span>11 comments</span></div><br/><div><div id="39753393" class="c"><input type="checkbox" id="c-39753393" checked=""/><div class="controls bullet"><span class="by">fzysingularity</span><span>|</span><a href="#39752864">next</a><span>|</span><label class="collapse" for="c-39753393">[-]</label><label class="expand" for="c-39753393">[6 more]</label></div><br/><div class="children"><div class="content">I was initially impressed with the landing page, but it does look a bit suspect when things are claimed to be 100x faster without much info on the HW acceleration or the model sizes.<p>My best guess is that they&#x27;re using two approaches to get this running faster:<p>- structured generation techniques from sglang (<a href="https:&#x2F;&#x2F;github.com&#x2F;sgl-project&#x2F;sglang">https:&#x2F;&#x2F;github.com&#x2F;sgl-project&#x2F;sglang</a>) that allow them to generate faster JSON (with look-ahead &#x2F; pre-fill) with strong guarantees on the output (i.e. 100% reliable, without requiring any retries).<p>- distilling a gpt-3.5 turbo-esque model from GPT-4 JSON outputs, and using it in conjuction with above to give the additional performance boosts on inference.<p>It doesn&#x27;t seem like they&#x27;re deploying on any custom silicon, nor have they optimized GPU kernels to suggest that the speed ups came there.</div><br/><div id="39753476" class="c"><input type="checkbox" id="c-39753476" checked=""/><div class="controls bullet"><span class="by">zuck_vs_musk</span><span>|</span><a href="#39753393">parent</a><span>|</span><a href="#39753527">next</a><span>|</span><label class="collapse" for="c-39753476">[-]</label><label class="expand" for="c-39753476">[3 more]</label></div><br/><div class="children"><div class="content">&gt; strong guarantees on the output (i.e. 100% reliable, without requiring any retries).<p>Has anyone seen a good JSON library that can handle slightly broken JSON? e.g. trailing commas, unescaped newlines, etc.? I have not found a good one.</div><br/><div id="39753604" class="c"><input type="checkbox" id="c-39753604" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39753393">root</a><span>|</span><a href="#39753476">parent</a><span>|</span><a href="#39753882">next</a><span>|</span><label class="collapse" for="c-39753604">[-]</label><label class="expand" for="c-39753604">[1 more]</label></div><br/><div class="children"><div class="content">Entirely broken JSON, no — I would be surprised if one existed. If you want slightly laxer semantics like trailing commas, JSON5 [1] is a pretty good spec and is JSON-compatible. I used to use it for LLMs (while telling them to emit JSON — no need to confuse them by explaining JSON5), in order to handle things like trailing commas, but in my experience LLMs have gotten good enough over the last year I mostly don&#x27;t even bother anymore.<p>1: <a href="https:&#x2F;&#x2F;json5.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;json5.org&#x2F;</a></div><br/></div></div><div id="39753882" class="c"><input type="checkbox" id="c-39753882" checked=""/><div class="controls bullet"><span class="by">catlifeonmars</span><span>|</span><a href="#39753393">root</a><span>|</span><a href="#39753476">parent</a><span>|</span><a href="#39753604">prev</a><span>|</span><a href="#39753527">next</a><span>|</span><label class="collapse" for="c-39753882">[-]</label><label class="expand" for="c-39753882">[1 more]</label></div><br/><div class="children"><div class="content">Should be easy to build on top of a lexer as a pre-parsing pass.</div><br/></div></div></div></div><div id="39753527" class="c"><input type="checkbox" id="c-39753527" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#39753393">parent</a><span>|</span><a href="#39753476">prev</a><span>|</span><a href="#39752864">next</a><span>|</span><label class="collapse" for="c-39753527">[-]</label><label class="expand" for="c-39753527">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been doing a lot of indie work with structured generation and llama.cpp, you can get extremely fast responses with caching and deterministic token skipping.<p>When generating json, calling the llm when you already know that after<p><pre><code>    { &quot;brand&quot;: &quot;Toyota&quot;
</code></pre>
Comes<p><pre><code>     , &quot;year&quot;: 
</code></pre>
Is a massive waste. If the data itself is constrained too you can skip most of that too! You&#x27;ll go down from needing 20 calls to the llm to just three for a simple piece of data like<p><pre><code>    { &quot;brand&quot;: &quot;Toyota&quot;, &quot;year&quot;: 1995 }
</code></pre>
If they combine these techniques with a model that&#x27;s specifically trained for structured output, along with a novel inference-time pruning technique that they were talking about in the post I can definitely see them getting these kinds of inference speeds.<p>I&#x27;m experimenting with a self hosted api that is fast enough to not even need a gpu for single user use cases (because the latency is good, but not batching). Once I&#x27;m done with the finishing touches I&#x27;ll rent a GPU server for actual hosting.</div><br/><div id="39753694" class="c"><input type="checkbox" id="c-39753694" checked=""/><div class="controls bullet"><span class="by">Incipient</span><span>|</span><a href="#39753393">root</a><span>|</span><a href="#39753527">parent</a><span>|</span><a href="#39752864">next</a><span>|</span><label class="collapse" for="c-39753694">[-]</label><label class="expand" for="c-39753694">[1 more]</label></div><br/><div class="children"><div class="content">Have you seen any blog posts that have some details on this? I can imagine, roughly, the concept but it sounds interesting and I&#x27;d like to get a better understanding.</div><br/></div></div></div></div></div></div><div id="39752864" class="c"><input type="checkbox" id="c-39752864" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#39753393">prev</a><span>|</span><label class="collapse" for="c-39752864">[-]</label><label class="expand" for="c-39752864">[4 more]</label></div><br/><div class="children"><div class="content">Despite reading it twice I couldn&#x27;t come away with why they chose char&#x2F;s or Hz as an appropriate measure. They also provided no benchmarks or model sizes except for a relative comparison with models 10x or 100x in size, which leads me to assume this is a small model maybe?</div><br/><div id="39753634" class="c"><input type="checkbox" id="c-39753634" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39752864">parent</a><span>|</span><a href="#39752960">next</a><span>|</span><label class="collapse" for="c-39753634">[-]</label><label class="expand" for="c-39753634">[1 more]</label></div><br/><div class="children"><div class="content">My guess is they&#x27;re generating the structure of the JSON programmatically (i.e. keys, commas, braces), and doing JSON escaping for the strings programatically, and not handling JSON in the LLM at all. Hence they&#x27;re comparing char&#x2F;s: first of all it&#x27;s not just generating tokens, and secondly it&#x27;s better for their benchmarks to compare char&#x2F;s (since they don&#x27;t hit the LLM for a lot of their characters) rather than LLM tokens&#x2F;s (which are probably somewhat faster, but not 100x faster).</div><br/></div></div><div id="39752960" class="c"><input type="checkbox" id="c-39752960" checked=""/><div class="controls bullet"><span class="by">padolsey</span><span>|</span><a href="#39752864">parent</a><span>|</span><a href="#39753634">prev</a><span>|</span><label class="collapse" for="c-39752960">[-]</label><label class="expand" for="c-39752960">[2 more]</label></div><br/><div class="children"><div class="content">Yeh it’s weird they don’t mention parameter size or other reasoning metrics. It’s a very cool approach to getting structured output from an LLM, but the benchmarks don’t show us the whole picture. I’m wondering if their approach can be used to delegate to different models at each step in a structured output. If it could be run with mistral 8x7B and still maintain its performance then that’s awesome.</div><br/><div id="39753261" class="c"><input type="checkbox" id="c-39753261" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#39752864">root</a><span>|</span><a href="#39752960">parent</a><span>|</span><label class="collapse" for="c-39753261">[-]</label><label class="expand" for="c-39753261">[1 more]</label></div><br/><div class="children"><div class="content">The last bit is already under way with speculative decoding but wonder what exactly are they proposing</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>