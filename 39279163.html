<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707296468241" as="style"/><link rel="stylesheet" href="styles.css?v=1707296468241"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures">Demystifying GPU compute architectures</a>Â <span class="domain">(<a href="https://thechipletter.substack.com">thechipletter.substack.com</a>)</span></div><div class="subtext"><span>rbanffy</span> | <span>37 comments</span></div><br/><div><div id="39285991" class="c"><input type="checkbox" id="c-39285991" checked=""/><div class="controls bullet"><span class="by">ribit</span><span>|</span><a href="#39282195">next</a><span>|</span><label class="collapse" for="c-39285991">[-]</label><label class="expand" for="c-39285991">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s really great to see these kind of articles! Of course, this is just scratching the surface. I think the most challenging bit about understanding GPUs is breaking through the marketing claims and trying to understand what is really going on in the background. How are the instructions scheduled, how does the execution actually look like and so on. And this is where information can be very difficult to come by. Nvidia documents the &quot;front-end&quot; of their GPUs very well, but the details are often shrouded in mystery. It&#x27;s easier with AMD, since they publish the ISA, but it can be still difficult to map their marketing claims (like 2x compute on RDNA3) to the actual hardware reality. Interestingly enough, we know quite a lot about Apple GPUs, since their compute architecture is very streamlined compared to other vendors.</div><br/></div></div><div id="39282195" class="c"><input type="checkbox" id="c-39282195" checked=""/><div class="controls bullet"><span class="by">fancyfredbot</span><span>|</span><a href="#39285991">prev</a><span>|</span><a href="#39284392">next</a><span>|</span><label class="collapse" for="c-39282195">[-]</label><label class="expand" for="c-39282195">[6 more]</label></div><br/><div class="children"><div class="content">NVIDIA are really keen you understand their hardware, to the extent they will give you insanely detailed tutorials on things like avoiding shared memory bank conflicts. It&#x27;s rare to find so much detail from CPU vendors in my experience. Perhaps because CPUs, with their out of order execution and branch prediction, are just much much harder to predict and understand than the comparatively simple in order GPU cores.</div><br/><div id="39284080" class="c"><input type="checkbox" id="c-39284080" checked=""/><div class="controls bullet"><span class="by">Remnant44</span><span>|</span><a href="#39282195">parent</a><span>|</span><a href="#39285125">next</a><span>|</span><label class="collapse" for="c-39284080">[-]</label><label class="expand" for="c-39284080">[1 more]</label></div><br/><div class="children"><div class="content">At least for x86, there&#x27;s an incredible wealth of architectural details out there, both from the vendors themselves and from people who have worked tirelessly to characterize them.<p>Along the lines of another comment on this post, part of the problem is the GPU compute model is a lot more abstract that what is presented for the CPU.<p>That abstraction is really helpful for being able to simply write parallel code. But it also hides the tremendous differences in performance possible...</div><br/></div></div><div id="39285125" class="c"><input type="checkbox" id="c-39285125" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39282195">parent</a><span>|</span><a href="#39284080">prev</a><span>|</span><a href="#39282814">next</a><span>|</span><label class="collapse" for="c-39285125">[-]</label><label class="expand" for="c-39285125">[3 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t GPUs also have out of order execution and instruction level parallelism?<p>I think the reason why Nvidia publishes these resources is because the GPUs are worth nothing if people can&#x27;t get a reasonable fraction of the advertisable FLOPs with reasonable effort. CUDA wouldn&#x27;t have taken off, if it were harder than it absolutely needs to be.</div><br/><div id="39286061" class="c"><input type="checkbox" id="c-39286061" checked=""/><div class="controls bullet"><span class="by">ribit</span><span>|</span><a href="#39282195">root</a><span>|</span><a href="#39285125">parent</a><span>|</span><a href="#39285463">next</a><span>|</span><label class="collapse" for="c-39286061">[-]</label><label class="expand" for="c-39286061">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Don&#x27;t GPUs also have out of order execution and instruction level parallelism?<p>Not any contemporary mainstream GPU I am aware of. Sure, the way these GPUs are marketed does sound like they have superscalar execution, but if you dig a bit deeper this is either about interleaving execution of many programs (similar to SMT) or a SIMD-within-SIMD. Two examples:<p>1. Nvidia claims they have simultaneous execution of FP and INT operations. What this actually means is that they can schedule an FP and and INT operation simultaneously, but they have to come from different programs. What this actually actually means is that they only schedule one instruction per clock but it takes two clocks to actually issue, so it kind of looks like issuing two instructions per clock if you squint hard enough. The trick is that their ALUs are 16-wide, but they pretend that they are 32-wide. I hope this makes sense.<p>2. AMD claims they have superscalar execution, but what they really have is a packed instruction that can do two operations using a limited selection of arguments. Which is why RDNA3 performance improvements even on compute-dense code are much more modest. Since these packed instructions have limitations, the compiler is not always able to emit them.</div><br/></div></div><div id="39285463" class="c"><input type="checkbox" id="c-39285463" checked=""/><div class="controls bullet"><span class="by">wtracy</span><span>|</span><a href="#39282195">root</a><span>|</span><a href="#39285125">parent</a><span>|</span><a href="#39286061">prev</a><span>|</span><a href="#39282814">next</a><span>|</span><label class="collapse" for="c-39285463">[-]</label><label class="expand" for="c-39285463">[1 more]</label></div><br/><div class="children"><div class="content">My understanding was that GPU instruction level parallelism is quite limited compared to CPUs (since multiple &quot;threads&quot; are running on each hardware core) and I wasn&#x27;t aware that GPUs had any meaningful OOO execution.<p>If I&#x27;m wrong, I&#x27;d be happy to learn more.</div><br/></div></div></div></div><div id="39282814" class="c"><input type="checkbox" id="c-39282814" checked=""/><div class="controls bullet"><span class="by">qrios</span><span>|</span><a href="#39282195">parent</a><span>|</span><a href="#39285125">prev</a><span>|</span><a href="#39284392">next</a><span>|</span><label class="collapse" for="c-39282814">[-]</label><label class="expand" for="c-39282814">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t now if NVIDIA is keen to understand their hardware, but they are obviously very interested in users understanding their hardware. I originally started i86 assembler and stopped after the i860, which as far as I remember was the first Intel processor with branch prediction. It&#x27;s a nightmare for control freaks, especially on CISC processors with variable clock cycles.<p>GPU programming with CUDA and PTX feels like programming on a single core CPU without tasks and threads with deterministic behavior but in a multidimensional space. And every hour spent avoiding an &#x27;if&#x27; pays off in terms of synchronization and therefore speed.</div><br/></div></div></div></div><div id="39284392" class="c"><input type="checkbox" id="c-39284392" checked=""/><div class="controls bullet"><span class="by">phendrenad2</span><span>|</span><a href="#39282195">prev</a><span>|</span><a href="#39281712">next</a><span>|</span><label class="collapse" for="c-39284392">[-]</label><label class="expand" for="c-39284392">[6 more]</label></div><br/><div class="children"><div class="content">I wonder if at some point G(raphics)PUs and G(^HMachine-Learning)PUs will diverge to the point that we can start calling them different things. I don&#x27;t particularly buy into the idea that 3D rendering is just an application of a bunch of compute units with a rasterizer at the end. There&#x27;s certainly some alpha in graphics-dedicated hardware. Hey! Maybe we can call them &quot;video cards&quot; again!</div><br/><div id="39286080" class="c"><input type="checkbox" id="c-39286080" checked=""/><div class="controls bullet"><span class="by">ribit</span><span>|</span><a href="#39284392">parent</a><span>|</span><a href="#39284599">next</a><span>|</span><label class="collapse" for="c-39286080">[-]</label><label class="expand" for="c-39286080">[1 more]</label></div><br/><div class="children"><div class="content">Hardware rasterizers are likely to disappear before long. We still use them because they are energy and area-efficient when working with large triangles. But already achieving high primitive count is difficult with hardware rasterizers, which is why some modern rendering approaches rasterize using computer shaders (e.g. UE3 Nanite). And of course, there is ray tracing which will replace the rasterization pipelines within the next decade or so.<p>As to ML, it absolutely needs different hardware than your traditional GPUs. The reason we associate the two is because GPUs are parallel processors and thus naturally suit the task. But as we move forward things like in-memory processing will become much more important. High-performance, energy efficient matrix multiplication also requires very different hardware layouts than your standard wide GPU SIMDs.</div><br/></div></div><div id="39284599" class="c"><input type="checkbox" id="c-39284599" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39284392">parent</a><span>|</span><a href="#39286080">prev</a><span>|</span><a href="#39285581">next</a><span>|</span><label class="collapse" for="c-39284599">[-]</label><label class="expand" for="c-39284599">[1 more]</label></div><br/><div class="children"><div class="content">AMD diverged RDNA (graphics) and CDNA (HPC&#x2F;AI) already. Nvidia removed the graphics-specific hardware from Hopper although it appears to be a small fraction of the chip.</div><br/></div></div><div id="39285581" class="c"><input type="checkbox" id="c-39285581" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#39284392">parent</a><span>|</span><a href="#39284599">prev</a><span>|</span><a href="#39285292">next</a><span>|</span><label class="collapse" for="c-39285581">[-]</label><label class="expand" for="c-39285581">[1 more]</label></div><br/><div class="children"><div class="content">I think the terms GPUs (for &quot;regular pipeline&quot; work - direct, opengl) and GP-GPU (for &quot;custom pipeline&quot; work - cuda, custom built) were used when I was working in the area some 15y ago.</div><br/></div></div><div id="39285292" class="c"><input type="checkbox" id="c-39285292" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39284392">parent</a><span>|</span><a href="#39285581">prev</a><span>|</span><a href="#39281712">next</a><span>|</span><label class="collapse" for="c-39285292">[-]</label><label class="expand" for="c-39285292">[2 more]</label></div><br/><div class="children"><div class="content">Then when neural rendering becomes a thing in games we&#x27;ll have come full circle</div><br/><div id="39286153" class="c"><input type="checkbox" id="c-39286153" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39284392">root</a><span>|</span><a href="#39285292">parent</a><span>|</span><a href="#39281712">next</a><span>|</span><label class="collapse" for="c-39286153">[-]</label><label class="expand" for="c-39286153">[1 more]</label></div><br/><div class="children"><div class="content">They can coexist, you can rasterize a surface and then neural-render its appearance</div><br/></div></div></div></div></div></div><div id="39281712" class="c"><input type="checkbox" id="c-39281712" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#39284392">prev</a><span>|</span><a href="#39283794">next</a><span>|</span><label class="collapse" for="c-39281712">[-]</label><label class="expand" for="c-39281712">[19 more]</label></div><br/><div class="children"><div class="content">The main problem with GPU architectures is that the manufacturers hide their details from us. Hell, they don&#x27;t even document the instruction sets! At least NVIDIA doesn&#x27;t. And they often don&#x27;t tell us what the hardware actually _does_, and rather offer a mental model of how we can think about it.<p>Specifically, those structural diagrams of functional-units within SM&#x27;s you see in the blog post? That comes from NVIDIA. And they explicitly state that they do _not_ guarantee that this is what the hardware is actually like. The hardware works &quot;as-if&quot; it were made up of this kind of units. And even more specifically - it&#x27;s not clear whether there even is such a thing as a &quot;tensor core&quot;, or whether it&#x27;s just some hack for doing lower-precision FP math somewhat faster.<p>-----<p>Anyway, if the architectures weren&#x27;t mostly _hidden_, we would make them far less mysterious within a rather short period of time.</div><br/><div id="39284698" class="c"><input type="checkbox" id="c-39284698" checked=""/><div class="controls bullet"><span class="by">Arelius</span><span>|</span><a href="#39281712">parent</a><span>|</span><a href="#39281898">next</a><span>|</span><label class="collapse" for="c-39284698">[-]</label><label class="expand" for="c-39284698">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, Nvidia has been awful about sharing documentation. The best docs I know of out Nvidia, are actually provided by Nintendo. AMD on the otherhand, while is far from perfect, does try to provide documentation.<p>CDNA docs, for instance: <a href="https:&#x2F;&#x2F;www.amd.com&#x2F;content&#x2F;dam&#x2F;amd&#x2F;en&#x2F;documents&#x2F;instinct-tech-docs&#x2F;white-papers&#x2F;amd-cdna-3-white-paper.pdf" rel="nofollow">https:&#x2F;&#x2F;www.amd.com&#x2F;content&#x2F;dam&#x2F;amd&#x2F;en&#x2F;documents&#x2F;instinct-te...</a></div><br/></div></div><div id="39281898" class="c"><input type="checkbox" id="c-39281898" checked=""/><div class="controls bullet"><span class="by">wolletd</span><span>|</span><a href="#39281712">parent</a><span>|</span><a href="#39284698">prev</a><span>|</span><a href="#39284002">next</a><span>|</span><label class="collapse" for="c-39281898">[-]</label><label class="expand" for="c-39281898">[2 more]</label></div><br/><div class="children"><div class="content">Just to be fair: as also stated at the beginning of the post, CPUs are far more complex nowadays than they represent themselves to the outside.<p>They don&#x27;t work like a von-Neumann machine, they act like one. Granted, we know a lot more about the inner workings of modern CPUs thn GPUs, but a lot of real-life work still assumes that the CPU works &quot;as-if&quot; it was a computer from the 70s, just really, really fast.</div><br/><div id="39286165" class="c"><input type="checkbox" id="c-39286165" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39281898">parent</a><span>|</span><a href="#39284002">next</a><span>|</span><label class="collapse" for="c-39286165">[-]</label><label class="expand" for="c-39286165">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div></div></div><div id="39284002" class="c"><input type="checkbox" id="c-39284002" checked=""/><div class="controls bullet"><span class="by">atq2119</span><span>|</span><a href="#39281712">parent</a><span>|</span><a href="#39281898">prev</a><span>|</span><a href="#39281919">next</a><span>|</span><label class="collapse" for="c-39284002">[-]</label><label class="expand" for="c-39284002">[1 more]</label></div><br/><div class="children"><div class="content">On the claim that instruction sets are undocumented: <a href="https:&#x2F;&#x2F;gpuopen.com&#x2F;amd-isa-documentation&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gpuopen.com&#x2F;amd-isa-documentation&#x2F;</a><p>Of course this doesn&#x27;t apply to Nvidia, but MI-300 seems to be pretty viable for machine learning, so if you want things to change across the industry, more people need to put their money where their mouths are.</div><br/></div></div><div id="39281919" class="c"><input type="checkbox" id="c-39281919" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#39281712">parent</a><span>|</span><a href="#39284002">prev</a><span>|</span><a href="#39283840">next</a><span>|</span><label class="collapse" for="c-39281919">[-]</label><label class="expand" for="c-39281919">[12 more]</label></div><br/><div class="children"><div class="content">GPU instructions sets change every year&#x2F;generation.<p>Your software would not run next year if you directly targeted the instruction set.<p>NVIDIA does document their PTX instruction set (a level above what the hardware actually runs):<p><a href="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;parallel-thread-execution&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;parallel-thread-execution&#x2F;index...</a></div><br/><div id="39281977" class="c"><input type="checkbox" id="c-39281977" checked=""/><div class="controls bullet"><span class="by">Xeamek</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39281919">parent</a><span>|</span><a href="#39282352">next</a><span>|</span><label class="collapse" for="c-39281977">[-]</label><label class="expand" for="c-39281977">[7 more]</label></div><br/><div class="children"><div class="content">...But literally the exact same is the case for CPUs and yet we do have public and constant instruction set for decades now?<p>Altough ofcourse CPU&#x27;s instruction are also just a frontend api that behind the scenes is implemented using microcode, which probably is much less stable.<p>But the point is, if we could move one level &#x27;closer&#x27; on gpus, just like we have it on cpus, it would stop the big buisness gate-keeping that exsists when it comes to current day GPU apis&#x2F;libraries</div><br/><div id="39282117" class="c"><input type="checkbox" id="c-39282117" checked=""/><div class="controls bullet"><span class="by">fancyfredbot</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39281977">parent</a><span>|</span><a href="#39282039">next</a><span>|</span><label class="collapse" for="c-39282117">[-]</label><label class="expand" for="c-39282117">[1 more]</label></div><br/><div class="children"><div class="content">PTX is more or less the same as x86. What&#x27;s even better is that unlike x86, Nvidia allows you to see and step through the SASS instructions which PTX compiles into. Although SASS is not documented, seeing it alongside the PTX makes it possible to infer the meaning. In contrast to this you can&#x27;t see the micro-ops running on x86 cpus (although people have inferred quite a lot about them).</div><br/></div></div><div id="39282039" class="c"><input type="checkbox" id="c-39282039" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39281977">parent</a><span>|</span><a href="#39282117">prev</a><span>|</span><a href="#39282352">next</a><span>|</span><label class="collapse" for="c-39282039">[-]</label><label class="expand" for="c-39282039">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not true, you can run a 30 years old binary on modern CPUs. The machine code you feed to the CPU didn&#x27;t change much.<p>That&#x27;s not true for GPUs, the machine code changes very frequently. You feed your &quot;binary&quot; (PTX, ...) to the driver, and the driver compiles it to the actual machine code of your actual GPU.</div><br/><div id="39282246" class="c"><input type="checkbox" id="c-39282246" checked=""/><div class="controls bullet"><span class="by">Xeamek</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39282039">parent</a><span>|</span><a href="#39282352">next</a><span>|</span><label class="collapse" for="c-39282246">[-]</label><label class="expand" for="c-39282246">[4 more]</label></div><br/><div class="children"><div class="content">But that&#x27;s not inherent due to gpus completely revolutionizing their aproach every generation, is it?<p>The main difference is that with cpu, the translation unit is hidden inside the cpu itself.
With gpus, the translation is moved from the device to the driver.<p>Old openGL code also will run in card that is newer then code itself.<p>The only difference is that with the cpus, it&#x27;s open standard what is the instruction set, while on gpus, instruction sets are defined by third parties (DX12,Vulkan,OpenGL) while it falls to nvidia to implement them.</div><br/><div id="39282805" class="c"><input type="checkbox" id="c-39282805" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39282246">parent</a><span>|</span><a href="#39282352">next</a><span>|</span><label class="collapse" for="c-39282805">[-]</label><label class="expand" for="c-39282805">[3 more]</label></div><br/><div class="children"><div class="content">OpenGL code is actually running on the CPU. From this CPU running code you can send shaders to the driver, in source code form or bytecode form, and the driver does the final compilation to GPU machine code.</div><br/><div id="39283260" class="c"><input type="checkbox" id="c-39283260" checked=""/><div class="controls bullet"><span class="by">Xeamek</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39282805">parent</a><span>|</span><a href="#39282352">next</a><span>|</span><label class="collapse" for="c-39283260">[-]</label><label class="expand" for="c-39283260">[2 more]</label></div><br/><div class="children"><div class="content">Pedantry.
Yes you are running your code on the cpu, but you are using opengl defined api to do so.
Having a text generation and parsing, or having a function calls passed to&#x2F;from drivers- those a simply implementation details</div><br/><div id="39285138" class="c"><input type="checkbox" id="c-39285138" checked=""/><div class="controls bullet"><span class="by">toasterlovin</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39283260">parent</a><span>|</span><a href="#39282352">next</a><span>|</span><label class="collapse" for="c-39285138">[-]</label><label class="expand" for="c-39285138">[1 more]</label></div><br/><div class="children"><div class="content">Modern CPUs being able to run 30 year old binaries as-is is pretty different than a GPU requiring a driver to compile byte code into machine code at runtime.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39282352" class="c"><input type="checkbox" id="c-39282352" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39281919">parent</a><span>|</span><a href="#39281977">prev</a><span>|</span><a href="#39283840">next</a><span>|</span><label class="collapse" for="c-39282352">[-]</label><label class="expand" for="c-39282352">[4 more]</label></div><br/><div class="children"><div class="content">&gt; GPU instructions sets change every year&#x2F;generation.<p>1. It&#x27;s every two-to-three years.<p>2. It&#x27;s not like they change into something completely different.<p>3. PTX is not a hardware instruction set, it&#x27;s just an LLVM IR variant<p>4. I don&#x27;t need to only target an instruction set directly, but it does help to know what instructions the hardware actually executes. And this is just like for CPUs (ok, not just like, because CPUs have u-ops, and I don&#x27;t know that GPUs have those).</div><br/><div id="39282722" class="c"><input type="checkbox" id="c-39282722" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39282352">parent</a><span>|</span><a href="#39283840">next</a><span>|</span><label class="collapse" for="c-39282722">[-]</label><label class="expand" for="c-39282722">[3 more]</label></div><br/><div class="children"><div class="content">4. The actual instruction set is called SASS. You can view it with Nsight, which can show all 3 levels, CUDA+PTX+SASS:<p><a href="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;nsight-visual-studio-edition&#x2F;3.2&#x2F;Content&#x2F;Images&#x2F;Debugger.CUDA.Disassembly.Breakpoints.001.png" rel="nofollow">https:&#x2F;&#x2F;docs.nvidia.com&#x2F;nsight-visual-studio-edition&#x2F;3.2&#x2F;Con...</a></div><br/><div id="39282928" class="c"><input type="checkbox" id="c-39282928" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39282722">parent</a><span>|</span><a href="#39283840">next</a><span>|</span><label class="collapse" for="c-39282928">[-]</label><label class="expand" for="c-39282928">[2 more]</label></div><br/><div class="children"><div class="content">You can view it, but there&#x27;s no documentation for it, nor a listing of all instructions. You need to guess what the instructions actually do. Sometimes it&#x27;s not so difficult, like IADD3; but sometimes it&#x27;s not at all trivial.</div><br/><div id="39285086" class="c"><input type="checkbox" id="c-39285086" checked=""/><div class="controls bullet"><span class="by">av3csr</span><span>|</span><a href="#39281712">root</a><span>|</span><a href="#39282928">parent</a><span>|</span><a href="#39283840">next</a><span>|</span><label class="collapse" for="c-39285086">[-]</label><label class="expand" for="c-39285086">[1 more]</label></div><br/><div class="children"><div class="content">The only &quot;documentation&quot; we have is in the form of the binary utility docs, which has a list of SASS instructions<p><a href="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;cuda-binary-utilities&#x2F;index.html#ampere-and-ada-instruction-set" rel="nofollow">https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;cuda-binary-utilities&#x2F;index.htm...</a><p>though there is no guarantee this is exhaustive, no opcodes either (though you could reverse engineer it using cuobjdump -sass and a hex editing like I&#x27;ve been doing). I&#x27;m pretty sure some of the instructions in the list are deprecated as well (95% percent sure that PMTRIG does nothing &gt;Volta)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39283840" class="c"><input type="checkbox" id="c-39283840" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39281712">parent</a><span>|</span><a href="#39281919">prev</a><span>|</span><a href="#39283189">next</a><span>|</span><label class="collapse" for="c-39283840">[-]</label><label class="expand" for="c-39283840">[1 more]</label></div><br/><div class="children"><div class="content">4 hours after posting, this is currently the second comment.<p>The first comment assures me &quot;NVIDIA are really keen you understand their hardware, to the extent they will give you insanely detailed tutorials on things like avoiding shared memory bank conflicts.&quot;<p>I don&#x27;t know who to believe!</div><br/></div></div></div></div><div id="39283794" class="c"><input type="checkbox" id="c-39283794" checked=""/><div class="controls bullet"><span class="by">ssivark</span><span>|</span><a href="#39281712">prev</a><span>|</span><label class="collapse" for="c-39283794">[-]</label><label class="expand" for="c-39283794">[4 more]</label></div><br/><div class="children"><div class="content">All this stuff about warps and streaming multi-processors â is this still the best mental model for targeting modern GPUs with ML workloads? It seems cleaner to think in terms of GEMM, kernels, etc.</div><br/><div id="39283918" class="c"><input type="checkbox" id="c-39283918" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#39283794">parent</a><span>|</span><label class="collapse" for="c-39283918">[-]</label><label class="expand" for="c-39283918">[3 more]</label></div><br/><div class="children"><div class="content">You donât have to go to that level of detail but it is still relevant. For example FlashAttention is built on optimizing for this level of detail.</div><br/><div id="39285741" class="c"><input type="checkbox" id="c-39285741" checked=""/><div class="controls bullet"><span class="by">TechnicolorByte</span><span>|</span><a href="#39283794">root</a><span>|</span><a href="#39283918">parent</a><span>|</span><label class="collapse" for="c-39285741">[-]</label><label class="expand" for="c-39285741">[2 more]</label></div><br/><div class="children"><div class="content">Any specifics you can describe or somewhere to read about this?</div><br/><div id="39286176" class="c"><input type="checkbox" id="c-39286176" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39283794">root</a><span>|</span><a href="#39285741">parent</a><span>|</span><label class="collapse" for="c-39286176">[-]</label><label class="expand" for="c-39286176">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>