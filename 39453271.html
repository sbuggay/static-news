<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708592470782" as="style"/><link rel="stylesheet" href="styles.css?v=1708592470782"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.google/technology/developers/gemma-open-models/">Gemma: New Open Models</a> <span class="domain">(<a href="https://blog.google">blog.google</a>)</span></div><div class="subtext"><span>meetpateltech</span> | <span>397 comments</span></div><br/><div><div id="39455929" class="c"><input type="checkbox" id="c-39455929" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39453780">next</a><span>|</span><label class="collapse" for="c-39455929">[-]</label><label class="expand" for="c-39455929">[42 more]</label></div><br/><div class="children"><div class="content">The terms of use: <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma&#x2F;terms" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma&#x2F;terms</a> and <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma&#x2F;prohibited_use_policy" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma&#x2F;prohibited_use_policy</a><p>Something that caught my eye in the terms:<p>&gt; Google may update Gemma from time to time, and you must make reasonable efforts to use the latest version of Gemma.<p>One of the biggest benefits of running your own model is that it can protect you from model updates that break your carefully tested prompts, so I’m not thrilled by that particular clause.</div><br/><div id="39456174" class="c"><input type="checkbox" id="c-39456174" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39455946">next</a><span>|</span><label class="collapse" for="c-39456174">[-]</label><label class="expand" for="c-39456174">[14 more]</label></div><br/><div class="children"><div class="content">This is actually not that unusual. Stable Diffusion&#x27;s license, CreativeML Open RAIL-M, has the exact same clause: &quot;You shall undertake reasonable efforts to use the latest version of the Model.&quot;<p>Obviously updating the model is not very practical when you&#x27;re using finetuned versions, and people still use old versions of Stable Diffusion. But it does make me fear the possibility that if they ever want to &quot;revoke&quot; everybody&#x27;s license to use the model, all they have to do is just post a model update that&#x27;s functionally useless for anything and go after anyone still using the old versions that actually do anything.</div><br/><div id="39458804" class="c"><input type="checkbox" id="c-39458804" checked=""/><div class="controls bullet"><span class="by">slowmovintarget</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456174">parent</a><span>|</span><a href="#39457510">next</a><span>|</span><label class="collapse" for="c-39458804">[-]</label><label class="expand" for="c-39458804">[5 more]</label></div><br/><div class="children"><div class="content">So if they wish to apply censorship they forgot, or suddenly discovered a reason for, they want you to be obligated to take it.<p>Good faith possibilities: Copyright liability requires retraining, or altering the underlying training set.<p>Gray area: &quot;Safety&quot; concerns where the model recommends criminal behavior (see uncensored GPT 4 evaluations).<p>Bad faith: Censorship or extra weighting added based on political agenda or for-pay skewing of results.</div><br/><div id="39459827" class="c"><input type="checkbox" id="c-39459827" checked=""/><div class="controls bullet"><span class="by">philsnow</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458804">parent</a><span>|</span><a href="#39459024">next</a><span>|</span><label class="collapse" for="c-39459827">[-]</label><label class="expand" for="c-39459827">[3 more]</label></div><br/><div class="children"><div class="content">Sounds like it would be interesting to keep track of the model&#x27;s responses to the same queries over time.<p>&gt; Gemma-2024-Feb, what do you think of the situation in the South China Sea?<p>&gt; &gt; The situation in the South China Sea is complex and multi-faceted, involving a wide range of issues including political conflicts, economic challenges, social changes, and historical tensions.<p>&gt; Gemma-2024-Oct, what do you think of the situation in the South China Sea?<p>&gt; &gt; Oceania has always been at war with EastAsia.</div><br/><div id="39460481" class="c"><input type="checkbox" id="c-39460481" checked=""/><div class="controls bullet"><span class="by">threecheese</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39459827">parent</a><span>|</span><a href="#39459024">next</a><span>|</span><label class="collapse" for="c-39460481">[-]</label><label class="expand" for="c-39460481">[2 more]</label></div><br/><div class="children"><div class="content">This is a great idea; I wonder if anyone is working on AI censorship monitoring at scale or at all. A secondary model could compare “censorship candidate” prompt results over time to classify how those results changed, and if those changes represent censorship or misinformation.</div><br/><div id="39460777" class="c"><input type="checkbox" id="c-39460777" checked=""/><div class="controls bullet"><span class="by">generalizations</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39460481">parent</a><span>|</span><a href="#39459024">next</a><span>|</span><label class="collapse" for="c-39460777">[-]</label><label class="expand" for="c-39460777">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also (I think?) been some research in the direction of figuring out more abstract notions of how models perceive various &#x27;concepts&#x27;. I&#x27;d be interested in the LLM version of diffs to see where changes have been implemented overall, too.<p>But really, the trouble is that it&#x27;s tough to predict ahead of time what kinds of things are likely to be censored in the future; if I were motivated to track this, I&#x27;d just make sure to keep a copy of each version of the model in my personal archive for future testing with whatever prompts seem reasonable in the future.</div><br/></div></div></div></div></div></div><div id="39459024" class="c"><input type="checkbox" id="c-39459024" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458804">parent</a><span>|</span><a href="#39459827">prev</a><span>|</span><a href="#39457510">next</a><span>|</span><label class="collapse" for="c-39459024">[-]</label><label class="expand" for="c-39459024">[1 more]</label></div><br/><div class="children"><div class="content">We are already culturally incapable of skillfully discussing censorship, &quot;fake news&quot;, etc, this adds even more fuel to that fire.<p>It is an interesting time to be alive!</div><br/></div></div></div></div><div id="39457510" class="c"><input type="checkbox" id="c-39457510" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456174">parent</a><span>|</span><a href="#39458804">prev</a><span>|</span><a href="#39458419">next</a><span>|</span><label class="collapse" for="c-39457510">[-]</label><label class="expand" for="c-39457510">[3 more]</label></div><br/><div class="children"><div class="content">These are all very new licenses that deviate from OSI principles, I think it&#x27;s fair to call them &quot;unusual&quot;.</div><br/><div id="39458060" class="c"><input type="checkbox" id="c-39458060" checked=""/><div class="controls bullet"><span class="by">simcop2387</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39457510">parent</a><span>|</span><a href="#39458419">next</a><span>|</span><label class="collapse" for="c-39458060">[-]</label><label class="expand" for="c-39458060">[2 more]</label></div><br/><div class="children"><div class="content">I think they meant not unusual in this space, not unusual in the sense of open source licensing.</div><br/><div id="39460677" class="c"><input type="checkbox" id="c-39460677" checked=""/><div class="controls bullet"><span class="by">alwayslikethis</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458060">parent</a><span>|</span><a href="#39458419">next</a><span>|</span><label class="collapse" for="c-39460677">[-]</label><label class="expand" for="c-39460677">[1 more]</label></div><br/><div class="children"><div class="content">For this sentence to parse, you need to either add or remove a &quot;not&quot;.</div><br/></div></div></div></div></div></div><div id="39458419" class="c"><input type="checkbox" id="c-39458419" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456174">parent</a><span>|</span><a href="#39457510">prev</a><span>|</span><a href="#39459546">next</a><span>|</span><label class="collapse" for="c-39458419">[-]</label><label class="expand" for="c-39458419">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s useful context, thanks - I hadn&#x27;t realized this clause was already out there for other models.</div><br/></div></div><div id="39459546" class="c"><input type="checkbox" id="c-39459546" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456174">parent</a><span>|</span><a href="#39458419">prev</a><span>|</span><a href="#39458590">next</a><span>|</span><label class="collapse" for="c-39459546">[-]</label><label class="expand" for="c-39459546">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think a broken model would trigger that clause in a meaningful way, because then you simply can&#x27;t update with reasonable effort. You would be obliged to try the new model in a test environment, and as soon as you notice it doesn&#x27;t perform and making it perform would require unreasonable effort you can simply stay on the old version.<p>However you might be required to update if they do more subtle changes, like a new version that only speaks positively about Google and only negatively about Microsoft. Provided this doesn&#x27;t have an obvious adverse impact on your use of the model.</div><br/></div></div><div id="39458590" class="c"><input type="checkbox" id="c-39458590" checked=""/><div class="controls bullet"><span class="by">ummonk</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456174">parent</a><span>|</span><a href="#39459546">prev</a><span>|</span><a href="#39460819">next</a><span>|</span><label class="collapse" for="c-39458590">[-]</label><label class="expand" for="c-39458590">[1 more]</label></div><br/><div class="children"><div class="content">Switching to a model that is functionally useless doesn&#x27;t seem to fall under &quot;reasonable efforts&quot; to me, but IANAL.</div><br/></div></div><div id="39460819" class="c"><input type="checkbox" id="c-39460819" checked=""/><div class="controls bullet"><span class="by">Silphendio</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456174">parent</a><span>|</span><a href="#39458590">prev</a><span>|</span><a href="#39459362">next</a><span>|</span><label class="collapse" for="c-39460819">[-]</label><label class="expand" for="c-39460819">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth noting that Stable Diffusion XL uses the OpenRAIL++-M License, which removed the update obligation.</div><br/></div></div><div id="39459362" class="c"><input type="checkbox" id="c-39459362" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456174">parent</a><span>|</span><a href="#39460819">prev</a><span>|</span><a href="#39455946">next</a><span>|</span><label class="collapse" for="c-39459362">[-]</label><label class="expand" for="c-39459362">[1 more]</label></div><br/><div class="children"><div class="content">Why the hell do they use such a crappy license in the first place?</div><br/></div></div></div></div><div id="39455946" class="c"><input type="checkbox" id="c-39455946" checked=""/><div class="controls bullet"><span class="by">tgtweak</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39456174">prev</a><span>|</span><a href="#39457271">next</a><span>|</span><label class="collapse" for="c-39455946">[-]</label><label class="expand" for="c-39455946">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think there&#x27;s a way they can enforce that reasonably.  There&#x27;s no connection to the mothership to report back what version is being used or license keys at runtime...<p>Seems more like a &quot;if we discover something unsafe you should update your model and we aren&#x27;t liable if you don&#x27;t&quot; than something that would make your model stop working.</div><br/></div></div><div id="39457271" class="c"><input type="checkbox" id="c-39457271" checked=""/><div class="controls bullet"><span class="by">summerlight</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39455946">prev</a><span>|</span><a href="#39456306">next</a><span>|</span><label class="collapse" for="c-39457271">[-]</label><label class="expand" for="c-39457271">[1 more]</label></div><br/><div class="children"><div class="content">This kind of defensive statements in ToS are usually due to obscure regulation or leading cases and model developers need a way to limit liability. There&#x27;s no practical way to enforce this, but they can claim that when bad things happen it&#x27;s purely on model users rather than model developers.</div><br/></div></div><div id="39456306" class="c"><input type="checkbox" id="c-39456306" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39457271">prev</a><span>|</span><a href="#39458416">next</a><span>|</span><label class="collapse" for="c-39456306">[-]</label><label class="expand" for="c-39456306">[2 more]</label></div><br/><div class="children"><div class="content">They have to make sure you’re receiving the most cutting edge chiding lectures when you make naughty and problematic requests.</div><br/><div id="39457980" class="c"><input type="checkbox" id="c-39457980" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39456306">parent</a><span>|</span><a href="#39458416">next</a><span>|</span><label class="collapse" for="c-39457980">[-]</label><label class="expand" for="c-39457980">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t make a local model do that. eg force the answer to begin with &quot;Yes&quot; or use control vectors so it agrees with it.</div><br/></div></div></div></div><div id="39458416" class="c"><input type="checkbox" id="c-39458416" checked=""/><div class="controls bullet"><span class="by">xyzzyz</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39456306">prev</a><span>|</span><a href="#39455976">next</a><span>|</span><label class="collapse" for="c-39458416">[-]</label><label class="expand" for="c-39458416">[1 more]</label></div><br/><div class="children"><div class="content">This is strangely reminiscent of the Soviet Union, where after they got rid of Lavrentiy Beria, they mailed the update to subscribers of the Great Soviet Encyclopedia, where they asked to remove the three pages with Beria’s biography and replace them with the three provided pages.</div><br/></div></div><div id="39455976" class="c"><input type="checkbox" id="c-39455976" checked=""/><div class="controls bullet"><span class="by">legohead</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39458416">prev</a><span>|</span><a href="#39457265">next</a><span>|</span><label class="collapse" for="c-39455976">[-]</label><label class="expand" for="c-39455976">[7 more]</label></div><br/><div class="children"><div class="content">Sounds like it&#x27;s &quot;reasonable&quot; for you not to update then.</div><br/><div id="39458101" class="c"><input type="checkbox" id="c-39458101" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39455976">parent</a><span>|</span><a href="#39457265">next</a><span>|</span><label class="collapse" for="c-39458101">[-]</label><label class="expand" for="c-39458101">[6 more]</label></div><br/><div class="children"><div class="content">It says you must make efforts (to a reasonable extent), not that you must give a reason for not making efforts</div><br/><div id="39459007" class="c"><input type="checkbox" id="c-39459007" checked=""/><div class="controls bullet"><span class="by">alwayslikethis</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458101">parent</a><span>|</span><a href="#39458834">next</a><span>|</span><label class="collapse" for="c-39459007">[-]</label><label class="expand" for="c-39459007">[1 more]</label></div><br/><div class="children"><div class="content">Oh I tried to update, it&#x27;s just that my router drops the connection after a few hundred MBs...</div><br/></div></div><div id="39458834" class="c"><input type="checkbox" id="c-39458834" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458101">parent</a><span>|</span><a href="#39459007">prev</a><span>|</span><a href="#39459601">next</a><span>|</span><label class="collapse" for="c-39458834">[-]</label><label class="expand" for="c-39458834">[3 more]</label></div><br/><div class="children"><div class="content">This is a TOS, meaning their enforcement option is a lawsuit. In court, if you convincingly argue why it would take an unreasonable amount of effort to update, you win. They can&#x27;t compel you to unreasonable effort as per their own TOS.</div><br/><div id="39460815" class="c"><input type="checkbox" id="c-39460815" checked=""/><div class="controls bullet"><span class="by">generalizations</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458834">parent</a><span>|</span><a href="#39459601">next</a><span>|</span><label class="collapse" for="c-39460815">[-]</label><label class="expand" for="c-39460815">[2 more]</label></div><br/><div class="children"><div class="content">This assumes they even know that the model hasn&#x27;t been updated. Who is this actually intended for? I&#x27;d bet it&#x27;s for companies hosting the model. In those cases, the definition of reasonable effort is a little closer to &quot;it&#x27;ll break our stuff if we touch it&quot; rather than &quot;oh silly me, I forgot how to spell r-s-y-n-c&quot;.</div><br/><div id="39464291" class="c"><input type="checkbox" id="c-39464291" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39460815">parent</a><span>|</span><a href="#39459601">next</a><span>|</span><label class="collapse" for="c-39464291">[-]</label><label class="expand" for="c-39464291">[1 more]</label></div><br/><div class="children"><div class="content">Hosting companies can probably just claim they&#x27;re covered under Section 230, and Google has to go bother the individual users, not them.</div><br/></div></div></div></div></div></div><div id="39459601" class="c"><input type="checkbox" id="c-39459601" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458101">parent</a><span>|</span><a href="#39458834">prev</a><span>|</span><a href="#39457265">next</a><span>|</span><label class="collapse" for="c-39459601">[-]</label><label class="expand" for="c-39459601">[1 more]</label></div><br/><div class="children"><div class="content">If you evaluate what it takes to update, and judge the effort unreasonable, that should be enough. Maybe make a powerpoint presenting that result, if you want something for the lawyers. If you don&#x27;t see a way forward that leads to a result with reasonable effort you don&#x27;t have to continue working on it until you hit some arbitrary threshold for unreasonable effort.</div><br/></div></div></div></div></div></div><div id="39457265" class="c"><input type="checkbox" id="c-39457265" checked=""/><div class="controls bullet"><span class="by">maronato</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39455976">prev</a><span>|</span><a href="#39461343">next</a><span>|</span><label class="collapse" for="c-39457265">[-]</label><label class="expand" for="c-39457265">[1 more]</label></div><br/><div class="children"><div class="content">This sounds like a clause to cover themselves in case older versions have any serious issues</div><br/></div></div><div id="39461343" class="c"><input type="checkbox" id="c-39461343" checked=""/><div class="controls bullet"><span class="by">bsimpson</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39457265">prev</a><span>|</span><a href="#39456585">next</a><span>|</span><label class="collapse" for="c-39461343">[-]</label><label class="expand" for="c-39461343">[1 more]</label></div><br/><div class="children"><div class="content">I appreciated this post clarifying the distinction between &quot;open model&quot; and &quot;open source&quot;:<p><a href="https:&#x2F;&#x2F;opensource.googleblog.com&#x2F;2024&#x2F;02&#x2F;building-open-models-responsibly-gemini-era.html?m=1" rel="nofollow">https:&#x2F;&#x2F;opensource.googleblog.com&#x2F;2024&#x2F;02&#x2F;building-open-mode...</a><p>I&#x27;m not sure how to feel about the restrictions.  &quot;No porn&quot; feels prudish, particularly for this millennium.  I tend to err on the side of freedom in intellectual&#x2F;political matters; however, the others seem fairly reasonable as far as restrictions go.</div><br/></div></div><div id="39456585" class="c"><input type="checkbox" id="c-39456585" checked=""/><div class="controls bullet"><span class="by">4bpp</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39461343">prev</a><span>|</span><a href="#39456560">next</a><span>|</span><label class="collapse" for="c-39456585">[-]</label><label class="expand" for="c-39456585">[1 more]</label></div><br/><div class="children"><div class="content">Ugh, I would fully expect this kind of clause to start popping up in other software ToSes soon if it hasn&#x27;t already. Contractually mandatory automatic updates.</div><br/></div></div><div id="39456560" class="c"><input type="checkbox" id="c-39456560" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39456585">prev</a><span>|</span><a href="#39458014">next</a><span>|</span><label class="collapse" for="c-39456560">[-]</label><label class="expand" for="c-39456560">[2 more]</label></div><br/><div class="children"><div class="content">Huh. I wonder why is that a part of the terms. I feel like that&#x27;s more of a support concern.</div><br/></div></div><div id="39458014" class="c"><input type="checkbox" id="c-39458014" checked=""/><div class="controls bullet"><span class="by">catchnear4321</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39456560">prev</a><span>|</span><a href="#39459389">next</a><span>|</span><label class="collapse" for="c-39458014">[-]</label><label class="expand" for="c-39458014">[6 more]</label></div><br/><div class="children"><div class="content">reasonable effort - meaning if their changes meaningfully impact my usage, negatively, it would be unreasonable to ask me to upgrade.<p>sounds good.<p>this is not financial advice and ianal.</div><br/><div id="39458337" class="c"><input type="checkbox" id="c-39458337" checked=""/><div class="controls bullet"><span class="by">res0nat0r</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458014">parent</a><span>|</span><a href="#39459389">next</a><span>|</span><label class="collapse" for="c-39458337">[-]</label><label class="expand" for="c-39458337">[5 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this just lawyer speak for &quot;we update our model a lot, and we&#x27;ve never signed off on saying we&#x27;re going to support every previous release we&#x27;ve ever published, and may turn them off at any time, don&#x27;t complain about it when we do.&quot;</div><br/><div id="39458861" class="c"><input type="checkbox" id="c-39458861" checked=""/><div class="controls bullet"><span class="by">CodesInChaos</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458337">parent</a><span>|</span><a href="#39458864">next</a><span>|</span><label class="collapse" for="c-39458861">[-]</label><label class="expand" for="c-39458861">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re talking about downloadable weights here, so they can&#x27;t turn them off, or force you (through technical means) to use a newer version.</div><br/></div></div><div id="39458864" class="c"><input type="checkbox" id="c-39458864" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458337">parent</a><span>|</span><a href="#39458861">prev</a><span>|</span><a href="#39459389">next</a><span>|</span><label class="collapse" for="c-39458864">[-]</label><label class="expand" for="c-39458864">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a local model, they can&#x27;t turn it off. It&#x27;s files on your computer without network access.</div><br/><div id="39459807" class="c"><input type="checkbox" id="c-39459807" checked=""/><div class="controls bullet"><span class="by">catchnear4321</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39458864">parent</a><span>|</span><a href="#39459389">next</a><span>|</span><label class="collapse" for="c-39459807">[-]</label><label class="expand" for="c-39459807">[2 more]</label></div><br/><div class="children"><div class="content">but what if they send a lawyer to ask firmly?  (kindly, but firmly.)</div><br/><div id="39463908" class="c"><input type="checkbox" id="c-39463908" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39455929">root</a><span>|</span><a href="#39459807">parent</a><span>|</span><a href="#39459389">next</a><span>|</span><label class="collapse" for="c-39463908">[-]</label><label class="expand" for="c-39463908">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;d need to send a lot of lawyers, considering that they have no idea how many people are using the model, and very little way of finding out. And they&#x27;d need a TOS violation. It would be generally expensive for them to do at scale; this isn&#x27;t about &quot;turning it off&quot; arbitrarily, it&#x27;s a CYA in case someone specific does something really bad that makes Google look bad: Google can patch the model to make it not comply with the bad request, and then demand the person running the model update or else lose their license to use the product. It&#x27;s a scalpel, not an off switch.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39459389" class="c"><input type="checkbox" id="c-39459389" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#39455929">parent</a><span>|</span><a href="#39458014">prev</a><span>|</span><a href="#39458160">next</a><span>|</span><label class="collapse" for="c-39459389">[-]</label><label class="expand" for="c-39459389">[1 more]</label></div><br/><div class="children"><div class="content">model watermarking? does this exist?</div><br/></div></div></div></div><div id="39453780" class="c"><input type="checkbox" id="c-39453780" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39455929">prev</a><span>|</span><a href="#39453398">next</a><span>|</span><label class="collapse" for="c-39453780">[-]</label><label class="expand" for="c-39453780">[46 more]</label></div><br/><div class="children"><div class="content">Benchmarks for Gemma 7B seem to be in the ballpark of Mistral 7B<p><pre><code>  +-------------+----------+-------------+-------------+
  | Benchmark   | Gemma 7B | Mistral 7B  | Llama-2 7B  |
  +-------------+----------+-------------+-------------+
  | MMLU        |   64.3   |     60.1    |     45.3    |
  | HellaSwag   |   81.2   |     81.3    |     77.2    |
  | HumanEval   |   32.3   |     30.5    |     12.8    |
  +-------------+----------+-------------+-------------+
</code></pre>
via <a href="https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;announcing-mistral-7b&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;announcing-mistral-7b&#x2F;</a></div><br/><div id="39454608" class="c"><input type="checkbox" id="c-39454608" checked=""/><div class="controls bullet"><span class="by">sa-code</span><span>|</span><a href="#39453780">parent</a><span>|</span><a href="#39454263">next</a><span>|</span><label class="collapse" for="c-39454608">[-]</label><label class="expand" for="c-39454608">[5 more]</label></div><br/><div class="children"><div class="content">Thank you. I thought it was weird for them to release a 7B model and not mention Mistral in their release.</div><br/><div id="39454852" class="c"><input type="checkbox" id="c-39454852" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454608">parent</a><span>|</span><a href="#39463027">next</a><span>|</span><label class="collapse" for="c-39454852">[-]</label><label class="expand" for="c-39454852">[1 more]</label></div><br/><div class="children"><div class="content">The technical report (linked in the 2nd paragraph of the blog post) mentions it, and compares against it:
<a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemma&#x2F;gemma-report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemma&#x2F;gemma-re...</a></div><br/></div></div><div id="39463027" class="c"><input type="checkbox" id="c-39463027" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454608">parent</a><span>|</span><a href="#39454852">prev</a><span>|</span><a href="#39458125">next</a><span>|</span><label class="collapse" for="c-39463027">[-]</label><label class="expand" for="c-39463027">[1 more]</label></div><br/><div class="children"><div class="content">The release page has comparisons to Mistral everywhere: <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma</a></div><br/></div></div><div id="39454788" class="c"><input type="checkbox" id="c-39454788" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454608">parent</a><span>|</span><a href="#39458125">prev</a><span>|</span><a href="#39454263">next</a><span>|</span><label class="collapse" for="c-39454788">[-]</label><label class="expand" for="c-39454788">[1 more]</label></div><br/><div class="children"><div class="content">They forgot.<p>Also phi-2.</div><br/></div></div></div></div><div id="39454263" class="c"><input type="checkbox" id="c-39454263" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39453780">parent</a><span>|</span><a href="#39454608">prev</a><span>|</span><a href="#39454465">next</a><span>|</span><label class="collapse" for="c-39454263">[-]</label><label class="expand" for="c-39454263">[3 more]</label></div><br/><div class="children"><div class="content">Only 8K context as well, like Mistral.<p>Also, as always, take these benchmarks with a <i>huge</i> grain of salt. Even base model releases are frequently (seemingly) contaminated these days.</div><br/><div id="39454473" class="c"><input type="checkbox" id="c-39454473" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454263">parent</a><span>|</span><a href="#39455543">next</a><span>|</span><label class="collapse" for="c-39454473">[-]</label><label class="expand" for="c-39454473">[1 more]</label></div><br/><div class="children"><div class="content">Agree: will be interesting how Gemma does on ChatBot Arena</div><br/></div></div><div id="39455543" class="c"><input type="checkbox" id="c-39455543" checked=""/><div class="controls bullet"><span class="by">DreamGen</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454263">parent</a><span>|</span><a href="#39454473">prev</a><span>|</span><a href="#39454465">next</a><span>|</span><label class="collapse" for="c-39455543">[-]</label><label class="expand" for="c-39455543">[1 more]</label></div><br/><div class="children"><div class="content">Mistral Instruct v0.2 is 32K.</div><br/></div></div></div></div><div id="39454465" class="c"><input type="checkbox" id="c-39454465" checked=""/><div class="controls bullet"><span class="by">jcuenod</span><span>|</span><a href="#39453780">parent</a><span>|</span><a href="#39454263">prev</a><span>|</span><a href="#39462430">next</a><span>|</span><label class="collapse" for="c-39454465">[-]</label><label class="expand" for="c-39454465">[11 more]</label></div><br/><div class="children"><div class="content">Came here to post the same thing for Phi-2:<p><pre><code>  +-------------+----------+-------------+
  | Benchmark   | Gemma 2B | Phi-2 2.7B  |
  +-------------+----------+-------------+
  | MMLU        |   42.3   |     56.7    |
  | MBPP        |   29.2   |     59.1    |
  | BoolQ       |   69.4   |     83.3    |
  +-------------+----------+-------------+

</code></pre>
[0] <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma</a><p>[1] <a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;phi-2-the-surprising-power-of-small-language-models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;blog&#x2F;phi-2-the-surp...</a></div><br/><div id="39455471" class="c"><input type="checkbox" id="c-39455471" checked=""/><div class="controls bullet"><span class="by">rfw300</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454465">parent</a><span>|</span><a href="#39454761">next</a><span>|</span><label class="collapse" for="c-39455471">[-]</label><label class="expand" for="c-39455471">[9 more]</label></div><br/><div class="children"><div class="content">A caveat: my impression of Phi-2, based on my own use and others’ experiences online, is that these benchmarks do not remotely resemble reality. The model is a paper tiger that is unable to perform almost any real-world task because it’s been fed so heavily with almost exclusively synthetic data targeted towards improving benchmark performance.</div><br/><div id="39456821" class="c"><input type="checkbox" id="c-39456821" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39455471">parent</a><span>|</span><a href="#39457432">next</a><span>|</span><label class="collapse" for="c-39456821">[-]</label><label class="expand" for="c-39456821">[4 more]</label></div><br/><div class="children"><div class="content">Fun that&#x27;s not my experience of Phi-2. I use it for non-creative context, but function calling, and I find as reliable as much bigger models (no fine-tuning just constraining JSON + CoT). Phi-2 unquantized vs Mixtral Q8, Mixtral is not definitely better but much slower and RAM-hungry.</div><br/><div id="39458966" class="c"><input type="checkbox" id="c-39458966" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39456821">parent</a><span>|</span><a href="#39457432">next</a><span>|</span><label class="collapse" for="c-39458966">[-]</label><label class="expand" for="c-39458966">[3 more]</label></div><br/><div class="children"><div class="content">What prompts&#x2F;settings do you use for Phi-2? I found it completely unusable for my cases. It fails to follow basic instructions (I tried several instruction-following finetunes as well, in addition to the base model), and it&#x27;s been mostly like a random garbage generator for me. With Llama.cpp, constrained to JSON, it also often hangs because it fails to find continuations which satisfy the JSON grammar.<p>I&#x27;m building a system which has many different passes (~15 so far). Almost every pass is a LLM invocation, which takes time. My original idea was to use a smaller model, such as Phi-2, as a gateway in front of all those passes: I&#x27;d describe which pass does what, and then ask Phi-2 to list the passes which are relevant for the user query (I called it &quot;pass masking&quot;). That would save a lot of time and collapse 15 steps to 2-3 steps on average. In fact, my Solar 10.7B model does it pretty well, but it takes 7 seconds for the masking pass to work on my GPU. Phi-2 would finish in ~1 second. However, I&#x27;m really struggling with Phi-2: it fails to reason (what&#x27;s relevant and what&#x27;s not), unlike Solar, and it also refuses to follow the output format (so that I could parse the output programmatically and disable the irrelevant passes). Again, my proof of concept works with Solar, and fails spectacularly with Phi-2.</div><br/><div id="39459506" class="c"><input type="checkbox" id="c-39459506" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39458966">parent</a><span>|</span><a href="#39457432">next</a><span>|</span><label class="collapse" for="c-39459506">[-]</label><label class="expand" for="c-39459506">[2 more]</label></div><br/><div class="children"><div class="content">My non-domain-specific prompt is:<p>&gt; You are a helpful assistant to &#x27;User&#x27;. You do not respond as &#x27;User&#x27; or pretend to be &#x27;User&#x27;. You only respond once as &#x27;Assistant&#x27;. &#x27;System&#x27; will give you data. Do not respond as &#x27;System&#x27;. Allow yourself inner thoughts as &#x27;Thoughts&#x27;.<p>and then I constrain its answers to Thoughts: [^\n]* and Assistant: &lt;JSON schema&gt;, and I have two shots included in the prompt.<p>I haven&#x27;t been able to get anything useful out of Phi-2 in llama.cpp (but I only tried quantized models). I use python&#x2F;huggingface&#x27;s transformers lib instead.</div><br/><div id="39463035" class="c"><input type="checkbox" id="c-39463035" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39459506">parent</a><span>|</span><a href="#39457432">next</a><span>|</span><label class="collapse" for="c-39463035">[-]</label><label class="expand" for="c-39463035">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. I&#x27;ve had no success at all using any of the Phi2 models.</div><br/></div></div></div></div></div></div></div></div><div id="39457432" class="c"><input type="checkbox" id="c-39457432" checked=""/><div class="controls bullet"><span class="by">myaccountonhn</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39455471">parent</a><span>|</span><a href="#39456821">prev</a><span>|</span><a href="#39456808">next</a><span>|</span><label class="collapse" for="c-39457432">[-]</label><label class="expand" for="c-39457432">[1 more]</label></div><br/><div class="children"><div class="content">I tested it for an offline autocompletion tool and it was hilariously bad.</div><br/></div></div><div id="39456808" class="c"><input type="checkbox" id="c-39456808" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39455471">parent</a><span>|</span><a href="#39457432">prev</a><span>|</span><a href="#39454761">next</a><span>|</span><label class="collapse" for="c-39456808">[-]</label><label class="expand" for="c-39456808">[3 more]</label></div><br/><div class="children"><div class="content">Hear hear! I don&#x27;t understand why it has persistent mindshare, it&#x27;s not even trained for chat. Meanwhile StableLM 3B runs RAG in my browser, on my iPhone, on my Pixel ..</div><br/><div id="39457766" class="c"><input type="checkbox" id="c-39457766" checked=""/><div class="controls bullet"><span class="by">djsavvy</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39456808">parent</a><span>|</span><a href="#39454761">next</a><span>|</span><label class="collapse" for="c-39457766">[-]</label><label class="expand" for="c-39457766">[2 more]</label></div><br/><div class="children"><div class="content">How have you been using RAG in your browser&#x2F;on your phones?</div><br/><div id="39459282" class="c"><input type="checkbox" id="c-39459282" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39457766">parent</a><span>|</span><a href="#39454761">next</a><span>|</span><label class="collapse" for="c-39459282">[-]</label><label class="expand" for="c-39459282">[1 more]</label></div><br/><div class="children"><div class="content">To be released, someday [sobs in engineer]<p>Idea is usage-based charging for non-local and a $5&#x2F;month sub for syncing.<p>keep an eye on @jpohhhh on Twitter if you&#x27;re interested<p>now that I got it on web, I&#x27;m hoping to at least get a PoC up soon. I&#x27;ve open-sourced the consitutent parts as FONNX and FLLAMA, Flutter libraries that work on all platforms. FONNX has embeddings, FLLAMA has llama.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Telosnex&#x2F;fonnx">https:&#x2F;&#x2F;github.com&#x2F;Telosnex&#x2F;fonnx</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;Telosnex&#x2F;fllama">https:&#x2F;&#x2F;github.com&#x2F;Telosnex&#x2F;fllama</a></div><br/></div></div></div></div></div></div></div></div><div id="39454761" class="c"><input type="checkbox" id="c-39454761" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454465">parent</a><span>|</span><a href="#39455471">prev</a><span>|</span><a href="#39462430">next</a><span>|</span><label class="collapse" for="c-39454761">[-]</label><label class="expand" for="c-39454761">[1 more]</label></div><br/><div class="children"><div class="content">Really looking forward to the day someone puts out an open model which outperforms Flan-T5 on BoolQ.</div><br/></div></div></div></div><div id="39462430" class="c"><input type="checkbox" id="c-39462430" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#39453780">parent</a><span>|</span><a href="#39454465">prev</a><span>|</span><a href="#39454547">next</a><span>|</span><label class="collapse" for="c-39462430">[-]</label><label class="expand" for="c-39462430">[1 more]</label></div><br/><div class="children"><div class="content">In my subjective tests it&#x27;s not even close to Mistral.
While my local gemma is quantized, so is mistral.<p>But I also tried gemma on huggingface.co&#x2F;chat which I assume isn&#x27;t quantized.</div><br/></div></div><div id="39454547" class="c"><input type="checkbox" id="c-39454547" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#39453780">parent</a><span>|</span><a href="#39462430">prev</a><span>|</span><a href="#39456748">next</a><span>|</span><label class="collapse" for="c-39454547">[-]</label><label class="expand" for="c-39454547">[11 more]</label></div><br/><div class="children"><div class="content">the real gold will be when this gets finetuned. (maybe by mistral...)</div><br/><div id="39454610" class="c"><input type="checkbox" id="c-39454610" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454547">parent</a><span>|</span><a href="#39456799">next</a><span>|</span><label class="collapse" for="c-39454610">[-]</label><label class="expand" for="c-39454610">[7 more]</label></div><br/><div class="children"><div class="content">TBH the community has largely outrun Mistral&#x27;s own finetuning. The 7B model in particular is such a popular target because its so practical to train.</div><br/><div id="39454856" class="c"><input type="checkbox" id="c-39454856" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454610">parent</a><span>|</span><a href="#39456799">next</a><span>|</span><label class="collapse" for="c-39454856">[-]</label><label class="expand" for="c-39454856">[6 more]</label></div><br/><div class="children"><div class="content">Strong disagree - a Mistral fine tune of llama 70b was the top performing llama fine tune. They have lots of data the community simply does not.</div><br/><div id="39454921" class="c"><input type="checkbox" id="c-39454921" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454856">parent</a><span>|</span><a href="#39456799">next</a><span>|</span><label class="collapse" for="c-39454921">[-]</label><label class="expand" for="c-39454921">[5 more]</label></div><br/><div class="children"><div class="content">Miqu was (allegedly) an internal continued pretrain Mistral did as a test, that was leaked as a GGUF.<p>Maybe its just semantics, it is technically a finetune... But to me theres a big difference between expensive &quot;continuation training&quot; (like Solar 10.7B or Mistral 70B) and a much less intense finetuning. The former is almost like releasing a whole new base model.<p>It would be <i>awesome</i> if Mistral did that with their data, but thats very different than releasing a Gemma Instruct finetune.</div><br/><div id="39455015" class="c"><input type="checkbox" id="c-39455015" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454921">parent</a><span>|</span><a href="#39457960">next</a><span>|</span><label class="collapse" for="c-39455015">[-]</label><label class="expand" for="c-39455015">[1 more]</label></div><br/><div class="children"><div class="content">There’s typically a difference in LR between a ‘continued pretrain’ and ‘fine tune.’ I don’t have the details around miqu, but was merely trying to say that Mistral could produce a better version of these models than the OSS community might. If the size of the corpora they use means we are no longer in fine tuning territory, then okay.</div><br/></div></div><div id="39457960" class="c"><input type="checkbox" id="c-39457960" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454921">parent</a><span>|</span><a href="#39455015">prev</a><span>|</span><a href="#39457591">next</a><span>|</span><label class="collapse" for="c-39457960">[-]</label><label class="expand" for="c-39457960">[2 more]</label></div><br/><div class="children"><div class="content">Arthur Mensch, the Mistral CEO, confirmed the leak. <a href="https:&#x2F;&#x2F;twitter.com&#x2F;arthurmensch&#x2F;status&#x2F;1752737462663684344" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;arthurmensch&#x2F;status&#x2F;1752737462663684344</a></div><br/><div id="39460827" class="c"><input type="checkbox" id="c-39460827" checked=""/><div class="controls bullet"><span class="by">saintradon</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39457960">parent</a><span>|</span><a href="#39457591">next</a><span>|</span><label class="collapse" for="c-39460827">[-]</label><label class="expand" for="c-39460827">[1 more]</label></div><br/><div class="children"><div class="content">Also, it led to one of the funniest pr I&#x27;ve seen in a while<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;miqudev&#x2F;miqu-1-70b&#x2F;discussions&#x2F;10" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;miqudev&#x2F;miqu-1-70b&#x2F;discussions&#x2F;10</a></div><br/></div></div></div></div><div id="39457591" class="c"><input type="checkbox" id="c-39457591" checked=""/><div class="controls bullet"><span class="by">sanjiwatsuki</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454921">parent</a><span>|</span><a href="#39457960">prev</a><span>|</span><a href="#39456799">next</a><span>|</span><label class="collapse" for="c-39457591">[-]</label><label class="expand" for="c-39457591">[1 more]</label></div><br/><div class="children"><div class="content">No shot. Mistral Medium&#x27;s outputs from API were virtually identical. Miqu really was Mistral Medium which happened to be a continued pretrain</div><br/></div></div></div></div></div></div></div></div><div id="39456799" class="c"><input type="checkbox" id="c-39456799" checked=""/><div class="controls bullet"><span class="by">itomatik</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454547">parent</a><span>|</span><a href="#39454610">prev</a><span>|</span><a href="#39455292">next</a><span>|</span><label class="collapse" for="c-39456799">[-]</label><label class="expand" for="c-39456799">[2 more]</label></div><br/><div class="children"><div class="content">how does one finetune llama (or any other LLM) using mistral?<p>is the flow like this?<p>- take small dataset<p>- generate bigger dataset using mistral (how this is this done?)<p>- run LoRA to fine tune gemma extended dataset.</div><br/><div id="39461351" class="c"><input type="checkbox" id="c-39461351" checked=""/><div class="controls bullet"><span class="by">itomatik</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39456799">parent</a><span>|</span><a href="#39455292">next</a><span>|</span><label class="collapse" for="c-39461351">[-]</label><label class="expand" for="c-39461351">[1 more]</label></div><br/><div class="children"><div class="content">I should have said &quot;run LoRA or your favorite fine-tuning technique to produce your fine-tuned llama.&quot;</div><br/></div></div></div></div></div></div><div id="39456748" class="c"><input type="checkbox" id="c-39456748" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#39453780">parent</a><span>|</span><a href="#39454547">prev</a><span>|</span><a href="#39458114">next</a><span>|</span><label class="collapse" for="c-39456748">[-]</label><label class="expand" for="c-39456748">[1 more]</label></div><br/><div class="children"><div class="content">According to their paper, average of standard task of Mistral is 54.0 and for Gemma it&#x27;s 56.4, so 4.4% relative better. Not as big as you would expect for the company which invented transformers and probably has 2-3 order more compute for training it vs few month old French startup.<p>Also for note on their human evaluations, Gemma 7B IT has a
51.7% win rate against Mistral v0.2 7B Instruct.</div><br/></div></div><div id="39454661" class="c"><input type="checkbox" id="c-39454661" checked=""/><div class="controls bullet"><span class="by">lawxls</span><span>|</span><a href="#39453780">parent</a><span>|</span><a href="#39458114">prev</a><span>|</span><a href="#39453398">next</a><span>|</span><label class="collapse" for="c-39454661">[-]</label><label class="expand" for="c-39454661">[12 more]</label></div><br/><div class="children"><div class="content">Honestly, this is more of a PR stunt to advertise the Google Dev ecosystem than a contribution to open-source. I&#x27;m not complaining, just calling it what it is.<p>Barely an improvement over the 5-month-old Mistral model, with the same context length of 8k. And this is a release after their announcement of Gemini Pro 1.5, which had an exponential increase in context length.</div><br/><div id="39454769" class="c"><input type="checkbox" id="c-39454769" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454661">parent</a><span>|</span><a href="#39454868">next</a><span>|</span><label class="collapse" for="c-39454769">[-]</label><label class="expand" for="c-39454769">[8 more]</label></div><br/><div class="children"><div class="content">Who cares if it&#x27;s a PR stunt to improve developer good will? It&#x27;s still a good thing, and it&#x27;s now the most open model out there.</div><br/><div id="39455266" class="c"><input type="checkbox" id="c-39455266" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454769">parent</a><span>|</span><a href="#39456154">next</a><span>|</span><label class="collapse" for="c-39455266">[-]</label><label class="expand" for="c-39455266">[2 more]</label></div><br/><div class="children"><div class="content">How is it more open than Mistral with Apache 2.0? Google wants people to sign a waiver to even download it.</div><br/><div id="39455568" class="c"><input type="checkbox" id="c-39455568" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39455266">parent</a><span>|</span><a href="#39456154">next</a><span>|</span><label class="collapse" for="c-39455568">[-]</label><label class="expand" for="c-39455568">[1 more]</label></div><br/><div class="children"><div class="content">Fair enough; that was more directed at LLaMA and derivatives, which have commercial restrictions.</div><br/></div></div></div></div><div id="39456154" class="c"><input type="checkbox" id="c-39456154" checked=""/><div class="controls bullet"><span class="by">observationist</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454769">parent</a><span>|</span><a href="#39455266">prev</a><span>|</span><a href="#39454868">next</a><span>|</span><label class="collapse" for="c-39456154">[-]</label><label class="expand" for="c-39456154">[5 more]</label></div><br/><div class="children"><div class="content">How exactly is it the &quot;most open model&quot; ?<p>It&#x27;s more like a masterclass in corporate doublespeak. Google’s &quot;transparency&quot; is as clear as mud, with pretraining details thinner than their privacy protections. Diving into Google’s tech means auctioning off your privacy (and your users&#x27; privacy) to the highest bidder.<p>Their &quot;open source&quot; embrace is more of a chokehold, with their tech biases and monopolistic strategies baked into every line of code. Think of it as Google&#x27;s way of marking territory - every developer is a fire hydrant.<p>These megacorps aren’t benevolent patrons of open source; they&#x27;re self-serving giants cloaking power grabs under the guise of &quot;progress&quot;.<p>Use these products at your own risk. If these companies wanted to engage in good faith, they&#x27;d use Apache or MIT licensing and grant people the agency and responsibility for their own use and development of software. Their licenses are designed to mitigate liability, handcuff potential competitors, and eke every last drop of value from users, with informed consent frequently being an optional afterthought.<p>That doesn&#x27;t even get into the Goodharting of metrics and actual performance of the models; I highly doubt they&#x27;re anywhere near as good as Mistral.<p>The UAE is a notoriously illiberal authoritarian state, yet even they have released AI models far more free and open than Google or Meta. 
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;tiiuae&#x2F;falcon-40b&#x2F;blob&#x2F;main&#x2F;README.md" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;tiiuae&#x2F;falcon-40b&#x2F;blob&#x2F;main&#x2F;README.md</a><p>If it’s not Apache or MIT, (or even some flavor of GPL,) it’s not open source; it’s a trojan horse. These &quot;free&quot; models come at the cost of your privacy and freedoms.<p>These models aren&#x27;t Open or Open Access or Free unless you perform the requisite mental gymnastics cooked up by their marketing and legal teams. Oceania has always been at war with Eastasia. Gemma is doubleplusgood.</div><br/><div id="39456633" class="c"><input type="checkbox" id="c-39456633" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39456154">parent</a><span>|</span><a href="#39454868">next</a><span>|</span><label class="collapse" for="c-39456633">[-]</label><label class="expand" for="c-39456633">[4 more]</label></div><br/><div class="children"><div class="content">You said a lot of nothing without actually saying specifically what the problem is with the recent license.<p>Maybe the license is fine for almost all usecases and the limitations are small?<p>For example, you complained about metas license, but basically everyone uses those models and is completely ignoring it. The weights are out there, and nobody cares what the fine print says.<p>Maybe if you are a FAANG, company, meta might sue. But everyone else is getting away with it completely.</div><br/><div id="39457300" class="c"><input type="checkbox" id="c-39457300" checked=""/><div class="controls bullet"><span class="by">observationist</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39456633">parent</a><span>|</span><a href="#39454868">next</a><span>|</span><label class="collapse" for="c-39457300">[-]</label><label class="expand" for="c-39457300">[3 more]</label></div><br/><div class="children"><div class="content">I specifically called out the claims of openness and doublespeak being used.<p>Google is making claims that are untrue. Meta makes similar false claims. The fact that unspecified &quot;other&quot; people are ignoring the licenses isn&#x27;t relevant. Good for them. Good luck making anything real or investing any important level of time or money under those misconceptions.<p>&quot;They haven&#x27;t sued yet&quot; isn&#x27;t some sort of validation. Anyone building an actual product that makes actual money that comes to the attention of Meta or Google will be sued into oblivion, their IP taken, and repurposed or buried. These tech companies have never behaved otherwise, and to think that they will is willfully oblivious.<p>They don&#x27;t deserve the benefit of the doubt, and should be called out for using deceitful language, making comparisons between their performative &quot;openness&quot; and actual, real, open source software. Mistral and other players have released actually open models and software. They&#x27;re good faith actors, and if you&#x27;re going to build a product requiring a custom model, the smart money is on Mistral.<p>FAANG are utilizing gotcha licenses and muddying the waters to their own benefit, not as a contribution to the public good. Building anything on the assumption that Meta or Google won&#x27;t sue is beyond foolish. They&#x27;re just as open as &quot;Open&quot;AI, which is to say not open at all.</div><br/><div id="39458524" class="c"><input type="checkbox" id="c-39458524" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39457300">parent</a><span>|</span><a href="#39454868">next</a><span>|</span><label class="collapse" for="c-39458524">[-]</label><label class="expand" for="c-39458524">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Anyone building an actual product that makes actual money that comes to the attention of Meta or Google will be sued into oblivion<p>No they won&#x27;t and they haven&#x27;t.<p>Almost the entire startup scene is completely ignoring all these licenses right now.<p>This is basically the entire industry. We are all getting away with it.<p>Here&#x27;s an example, take llama.<p>Llama originally disallowed commercial activity. But then the license got changed much later.<p>So, if you were a stupid person, then you followed the license and fell behind. And if you were smart, you ignored it and got ahead of everyone else.<p>Which, in retrospect was correct.<p>Because now the license allows commerical activity, so everyone who ignores it in the first place got away with it and is now ahead of everyone else.<p>&gt; won&#x27;t sue is beyond foolish<p>But we already got away with it with llama! That&#x27;s already over! It&#x27;s commerical now, and nobody got sued! For that example, the people who ignored the license won.</div><br/><div id="39460621" class="c"><input type="checkbox" id="c-39460621" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39458524">parent</a><span>|</span><a href="#39454868">next</a><span>|</span><label class="collapse" for="c-39460621">[-]</label><label class="expand" for="c-39460621">[1 more]</label></div><br/><div class="children"><div class="content">The nice thing about this is that the calculus is in favor of startups, who can roll the dice.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39454868" class="c"><input type="checkbox" id="c-39454868" checked=""/><div class="controls bullet"><span class="by">crossroadsguy</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454661">parent</a><span>|</span><a href="#39454769">prev</a><span>|</span><a href="#39454878">next</a><span>|</span><label class="collapse" for="c-39454868">[-]</label><label class="expand" for="c-39454868">[1 more]</label></div><br/><div class="children"><div class="content">That’s about the point of having a developer ecosystem, isn’t it?</div><br/></div></div><div id="39454878" class="c"><input type="checkbox" id="c-39454878" checked=""/><div class="controls bullet"><span class="by">kiraaa</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454661">parent</a><span>|</span><a href="#39454868">prev</a><span>|</span><a href="#39453398">next</a><span>|</span><label class="collapse" for="c-39454878">[-]</label><label class="expand" for="c-39454878">[2 more]</label></div><br/><div class="children"><div class="content">mistral 7b v0.2 supports 32k</div><br/><div id="39454983" class="c"><input type="checkbox" id="c-39454983" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39453780">root</a><span>|</span><a href="#39454878">parent</a><span>|</span><a href="#39453398">next</a><span>|</span><label class="collapse" for="c-39454983">[-]</label><label class="expand" for="c-39454983">[1 more]</label></div><br/><div class="children"><div class="content">This is a good point actually, and an underappreciated fact.<p>I think so many people (including me) effectively ignored Mistral 0.1&#x27;s sliding window that few realized 0.2 instruct is native 32K.</div><br/></div></div></div></div></div></div></div></div><div id="39453398" class="c"><input type="checkbox" id="c-39453398" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39453780">prev</a><span>|</span><a href="#39453610">next</a><span>|</span><label class="collapse" for="c-39453398">[-]</label><label class="expand" for="c-39453398">[88 more]</label></div><br/><div class="children"><div class="content">Hello on behalf of the Gemma team! We are really excited to answer any questions you may have about our models.<p>Opinions are our own and not of Google DeepMind.</div><br/><div id="39457597" class="c"><input type="checkbox" id="c-39457597" checked=""/><div class="controls bullet"><span class="by">voxgen</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39454764">next</a><span>|</span><label class="collapse" for="c-39457597">[-]</label><label class="expand" for="c-39457597">[2 more]</label></div><br/><div class="children"><div class="content">Thank you very much for releasing these models! It&#x27;s great to see Google enter the battle with a strong hand.<p>I&#x27;m wondering if you&#x27;re able to provide any insight into the below hyperparameter decisions in Gemma&#x27;s architecture, as they differ significantly from what we&#x27;ve seen with other recent models?<p>* On the 7B model, the `d_model` (3072) is smaller than `num_heads * d_head` (16*256=4096). I don&#x27;t know of any other model where these numbers don&#x27;t match.<p>* The FFN expansion factor of 16x is MUCH higher than the Llama-2-7B&#x27;s 5.4x, which itself was chosen to be equi-FLOPS with PaLM&#x27;s 4x.<p>* The vocab is much larger - 256k, where most small models use 32k-64k.<p>* GQA is only used on the 2B model, where we&#x27;ve seen other models prefer to save it for larger models.<p>These observations are in no way meant to be criticism - I understand that Llama&#x27;s hyperparameters are also somewhat arbitrarily inherited from its predecessors like PaLM and GPT-2, and that it&#x27;s non-trivial to run hyperopt on such large models. I&#x27;m just really curious about what findings motivated these choices.</div><br/><div id="39458512" class="c"><input type="checkbox" id="c-39458512" checked=""/><div class="controls bullet"><span class="by">owl_brawl</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39457597">parent</a><span>|</span><a href="#39454764">next</a><span>|</span><label class="collapse" for="c-39458512">[-]</label><label class="expand" for="c-39458512">[1 more]</label></div><br/><div class="children"><div class="content">I would love answers to these questions too, particularly on the vocab size</div><br/></div></div></div></div><div id="39454764" class="c"><input type="checkbox" id="c-39454764" checked=""/><div class="controls bullet"><span class="by">lordswork</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39457597">prev</a><span>|</span><a href="#39457661">next</a><span>|</span><label class="collapse" for="c-39454764">[-]</label><label class="expand" for="c-39454764">[9 more]</label></div><br/><div class="children"><div class="content">Is there any truth behind this claim that folks who worked on Gemma have left Google?<p><a href="https:&#x2F;&#x2F;x.com&#x2F;yar_vol&#x2F;status&#x2F;1760314018575634842" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;yar_vol&#x2F;status&#x2F;1760314018575634842</a></div><br/><div id="39459176" class="c"><input type="checkbox" id="c-39459176" checked=""/><div class="controls bullet"><span class="by">lordswork</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454764">parent</a><span>|</span><a href="#39457400">next</a><span>|</span><label class="collapse" for="c-39459176">[-]</label><label class="expand" for="c-39459176">[1 more]</label></div><br/><div class="children"><div class="content">I confirmed all the folks listed on page 12 are still at Google (listed below). I am guessing the linked tweet is a BS claim.<p><pre><code>   # Product Management
   Tris Warkentin
   Ludovic Peran

   # Program Management
   Minh Giang

   # Executive Sponsors
   Clement Farabet
   Oriol Vinyals
   Jeff Dean
   Koray Kavukcuoglu
   Demis Hassabis
   Zoubin Ghahramani
   Douglas Eck
   Joelle Barral
   Fernando Pereira
   Eli Collins

   # Leads
   Armand Joulin
   Noah Fiedel
   Evan Senter

   # Tech Leads
   Alek Andreev†
   Kathleen Kenealy†</code></pre></div><br/></div></div><div id="39457400" class="c"><input type="checkbox" id="c-39457400" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454764">parent</a><span>|</span><a href="#39459176">prev</a><span>|</span><a href="#39459126">next</a><span>|</span><label class="collapse" for="c-39457400">[-]</label><label class="expand" for="c-39457400">[2 more]</label></div><br/><div class="children"><div class="content">It seems very easy to check no? Look at the names in the paper and check where they are working now</div><br/><div id="39459156" class="c"><input type="checkbox" id="c-39459156" checked=""/><div class="controls bullet"><span class="by">lordswork</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39457400">parent</a><span>|</span><a href="#39459126">next</a><span>|</span><label class="collapse" for="c-39459156">[-]</label><label class="expand" for="c-39459156">[1 more]</label></div><br/><div class="children"><div class="content">Good idea. I&#x27;ve confirmed all the leadership &#x2F; tech leads listed on page 12 are still at Google.<p>Can someone with a Twitter account call out the tweet linked above and ask them specifically who they are referring to? Seems there is no evidence of their claim.</div><br/></div></div></div></div><div id="39460446" class="c"><input type="checkbox" id="c-39460446" checked=""/><div class="controls bullet"><span class="by">bluefinity</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454764">parent</a><span>|</span><a href="#39459126">prev</a><span>|</span><a href="#39459118">next</a><span>|</span><label class="collapse" for="c-39460446">[-]</label><label class="expand" for="c-39460446">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, the tweet says that they don&#x27;t work on the models at Google anymore, not that they have left Google.<p>Might be true, might not be. It&#x27;s unsourced speculation.</div><br/></div></div><div id="39456711" class="c"><input type="checkbox" id="c-39456711" checked=""/><div class="controls bullet"><span class="by">CaffeinatedDev</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454764">parent</a><span>|</span><a href="#39459118">prev</a><span>|</span><a href="#39457661">next</a><span>|</span><label class="collapse" for="c-39456711">[-]</label><label class="expand" for="c-39456711">[2 more]</label></div><br/><div class="children"><div class="content">Them: here to answer questions<p><i>Question</i><p>Them: :O</div><br/><div id="39457108" class="c"><input type="checkbox" id="c-39457108" checked=""/><div class="controls bullet"><span class="by">lordswork</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39456711">parent</a><span>|</span><a href="#39457661">next</a><span>|</span><label class="collapse" for="c-39457108">[-]</label><label class="expand" for="c-39457108">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, I think they are in London, so I assume they have winded down for the day. Will probably have to wait ~12-18 hours for a response.</div><br/></div></div></div></div></div></div><div id="39457661" class="c"><input type="checkbox" id="c-39457661" checked=""/><div class="controls bullet"><span class="by">LorenDB</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39454764">prev</a><span>|</span><a href="#39456561">next</a><span>|</span><label class="collapse" for="c-39457661">[-]</label><label class="expand" for="c-39457661">[3 more]</label></div><br/><div class="children"><div class="content">EDIT: it seems this is likely an Ollama bug, please keep that in mind for the rest of this comment :)<p>I ran Gemma in Ollama and noticed two things. First, it is slow. Gemma got less than 40 tok&#x2F;s while Llama 2 7B got over 80 tok&#x2F;s. Second, it is very bad at output generation. I said &quot;hi&quot;, and it responded this:<p>```
Hi, . What is up? melizing with you today!<p>What would  you like to talk about or hear from me on this fine day?? 
```<p>With longer and more complex prompts it goes completely off the rails. Here&#x27;s a snippet from its response to &quot;Explain how to use Qt to get the current IP from <a href="https:&#x2F;&#x2F;icanhazip.com" rel="nofollow">https:&#x2F;&#x2F;icanhazip.com</a>&quot;:<p>``` python
print( &quot;Error consonming IP arrangration at [local machine&#x27;s hostname]. Please try fufing this function later!&quot;)  ## 
guanomment messages are typically displayed using QtWidgets.MessageBox
```<p>Do you see similar results on your end or is this just a bug in Ollama? I have a terrible suspicion that this might be a completely flawed model, but I&#x27;m holding out hope that Ollama just has a bug somewhere.</div><br/><div id="39458654" class="c"><input type="checkbox" id="c-39458654" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39457661">parent</a><span>|</span><a href="#39456561">next</a><span>|</span><label class="collapse" for="c-39458654">[-]</label><label class="expand" for="c-39458654">[2 more]</label></div><br/><div class="children"><div class="content">I was going to try these models with Ollama. Did you use a small number of bits&#x2F;quantization?</div><br/><div id="39459202" class="c"><input type="checkbox" id="c-39459202" checked=""/><div class="controls bullet"><span class="by">LorenDB</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39458654">parent</a><span>|</span><a href="#39456561">next</a><span>|</span><label class="collapse" for="c-39459202">[-]</label><label class="expand" for="c-39459202">[1 more]</label></div><br/><div class="children"><div class="content">The problem exists with the default 7B model. I don&#x27;t know if different quantizations would fix the problem. The 2B model is fine, though.</div><br/></div></div></div></div></div></div><div id="39456561" class="c"><input type="checkbox" id="c-39456561" checked=""/><div class="controls bullet"><span class="by">fosterfriends</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39457661">prev</a><span>|</span><a href="#39453993">next</a><span>|</span><label class="collapse" for="c-39456561">[-]</label><label class="expand" for="c-39456561">[1 more]</label></div><br/><div class="children"><div class="content">Not a question, but thank you for your hard work! Also, brave of you to join the HN comments, I appreciate your openness. Hope y&#x27;all get to celebrate the launch :)</div><br/></div></div><div id="39453993" class="c"><input type="checkbox" id="c-39453993" checked=""/><div class="controls bullet"><span class="by">lnyan</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39456561">prev</a><span>|</span><a href="#39453459">next</a><span>|</span><label class="collapse" for="c-39453993">[-]</label><label class="expand" for="c-39453993">[3 more]</label></div><br/><div class="children"><div class="content">Will there be Gemma-vision models or multimodal Gemma models?</div><br/><div id="39461245" class="c"><input type="checkbox" id="c-39461245" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453993">parent</a><span>|</span><a href="#39456393">next</a><span>|</span><label class="collapse" for="c-39461245">[-]</label><label class="expand" for="c-39461245">[1 more]</label></div><br/><div class="children"><div class="content">We have many exciting things planned that we can&#x27;t reveal just yet :)</div><br/></div></div><div id="39456393" class="c"><input type="checkbox" id="c-39456393" checked=""/><div class="controls bullet"><span class="by">Jayakumark</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453993">parent</a><span>|</span><a href="#39461245">prev</a><span>|</span><a href="#39453459">next</a><span>|</span><label class="collapse" for="c-39456393">[-]</label><label class="expand" for="c-39456393">[1 more]</label></div><br/><div class="children"><div class="content">Have the same question.</div><br/></div></div></div></div><div id="39453459" class="c"><input type="checkbox" id="c-39453459" checked=""/><div class="controls bullet"><span class="by">h1t35h</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453993">prev</a><span>|</span><a href="#39453508">next</a><span>|</span><label class="collapse" for="c-39453459">[-]</label><label class="expand" for="c-39453459">[10 more]</label></div><br/><div class="children"><div class="content">It seems you have exposed the internal debugging tool link in the blog post. You may want to do something about it.</div><br/><div id="39453496" class="c"><input type="checkbox" id="c-39453496" checked=""/><div class="controls bullet"><span class="by">trisfromgoogle</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453459">parent</a><span>|</span><a href="#39453508">next</a><span>|</span><label class="collapse" for="c-39453496">[-]</label><label class="expand" for="c-39453496">[9 more]</label></div><br/><div class="children"><div class="content">Ah, I see -- the link is wrong, thank you for flagging! Fixing now.</div><br/><div id="39453510" class="c"><input type="checkbox" id="c-39453510" checked=""/><div class="controls bullet"><span class="by">h1t35h</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453496">parent</a><span>|</span><a href="#39453517">next</a><span>|</span><label class="collapse" for="c-39453510">[-]</label><label class="expand" for="c-39453510">[3 more]</label></div><br/><div class="children"><div class="content">The blog post shares the link for debugging tool as <a href="https:&#x2F;&#x2F;*.*.corp.google.com&#x2F;codelabs&#x2F;responsible-ai&#x2F;lit-gemma" rel="nofollow">https:&#x2F;&#x2F;*.*.corp.google.com&#x2F;codelabs&#x2F;responsible-ai&#x2F;lit-gemm...</a><p>.corp and the login redirect makes me believe it was supposed to be an internal link</div><br/><div id="39454123" class="c"><input type="checkbox" id="c-39454123" checked=""/><div class="controls bullet"><span class="by">barrkel</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453510">parent</a><span>|</span><a href="#39453518">next</a><span>|</span><label class="collapse" for="c-39454123">[-]</label><label class="expand" for="c-39454123">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;codelabs.developers.google.com&#x2F;codelabs&#x2F;responsible-ai&#x2F;lit-gemma" rel="nofollow">https:&#x2F;&#x2F;codelabs.developers.google.com&#x2F;codelabs&#x2F;responsible-...</a></div><br/></div></div><div id="39453518" class="c"><input type="checkbox" id="c-39453518" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453510">parent</a><span>|</span><a href="#39454123">prev</a><span>|</span><a href="#39453517">next</a><span>|</span><label class="collapse" for="c-39453518">[-]</label><label class="expand" for="c-39453518">[1 more]</label></div><br/><div class="children"><div class="content">Same for the “safety classifier”</div><br/></div></div></div></div><div id="39453509" class="c"><input type="checkbox" id="c-39453509" checked=""/><div class="controls bullet"><span class="by">neximo64</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453496">parent</a><span>|</span><a href="#39453511">prev</a><span>|</span><a href="#39453570">next</a><span>|</span><label class="collapse" for="c-39453509">[-]</label><label class="expand" for="c-39453509">[1 more]</label></div><br/><div class="children"><div class="content">The link to the debugging tool is an internal one, no one outside Google can access it</div><br/></div></div><div id="39453570" class="c"><input type="checkbox" id="c-39453570" checked=""/><div class="controls bullet"><span class="by">wrexx0r</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453496">parent</a><span>|</span><a href="#39453509">prev</a><span>|</span><a href="#39453508">next</a><span>|</span><label class="collapse" for="c-39453570">[-]</label><label class="expand" for="c-39453570">[1 more]</label></div><br/><div class="children"><div class="content">The link in the Debugging section redirects to a Google SSO login page</div><br/></div></div></div></div></div></div><div id="39453508" class="c"><input type="checkbox" id="c-39453508" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453459">prev</a><span>|</span><a href="#39453750">next</a><span>|</span><label class="collapse" for="c-39453508">[-]</label><label class="expand" for="c-39453508">[14 more]</label></div><br/><div class="children"><div class="content">Will these soon be available on lmsys for human comparison against other models?  Can they run with llama.cpp?</div><br/><div id="39453583" class="c"><input type="checkbox" id="c-39453583" checked=""/><div class="controls bullet"><span class="by">ErneX</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453508">parent</a><span>|</span><a href="#39453750">next</a><span>|</span><label class="collapse" for="c-39453583">[-]</label><label class="expand" for="c-39453583">[13 more]</label></div><br/><div class="children"><div class="content">Yes to llama.cpp<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760293079313973408" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1760293079313973408</a></div><br/><div id="39453720" class="c"><input type="checkbox" id="c-39453720" checked=""/><div class="controls bullet"><span class="by">sbarre</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453583">parent</a><span>|</span><a href="#39453750">next</a><span>|</span><label class="collapse" for="c-39453720">[-]</label><label class="expand" for="c-39453720">[12 more]</label></div><br/><div class="children"><div class="content">I came here wondering if these models are &quot;open&quot; in the sense that they&#x27;ll show up  on sites like Ollama where you can download and run them locally.<p>Am I correct to conclude that this means they eventually will?<p>It&#x27;s unclear to me from Google&#x27;s docs exactly what &quot;open&quot; means for Gemma</div><br/><div id="39453760" class="c"><input type="checkbox" id="c-39453760" checked=""/><div class="controls bullet"><span class="by">benpacker</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453720">parent</a><span>|</span><a href="#39453792">next</a><span>|</span><label class="collapse" for="c-39453760">[-]</label><label class="expand" for="c-39453760">[10 more]</label></div><br/><div class="children"><div class="content">Yes - they are open weights and open inference code, which means they can be integrated into Ollama.<p>They are not “open training” (either in the training code or training data sense), so they are not reproducible, which some have suggested ought to be a component of the definition of open models.</div><br/><div id="39453921" class="c"><input type="checkbox" id="c-39453921" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453760">parent</a><span>|</span><a href="#39453792">next</a><span>|</span><label class="collapse" for="c-39453921">[-]</label><label class="expand" for="c-39453921">[9 more]</label></div><br/><div class="children"><div class="content">It really should shouldn&#x27;t it? I&#x27;m quite ML-naïve, but surely providing the model without &#x27;training code or training data&#x27; is just like providing a self-hostable binary without the source code? Nobody calls that open source, it&#x27;s not even source available.</div><br/><div id="39455290" class="c"><input type="checkbox" id="c-39455290" checked=""/><div class="controls bullet"><span class="by">michaelt</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453921">parent</a><span>|</span><a href="#39460855">next</a><span>|</span><label class="collapse" for="c-39455290">[-]</label><label class="expand" for="c-39455290">[2 more]</label></div><br/><div class="children"><div class="content">It is widely believed (and in some cases acknowledged) that a lot of models are trained on copyrighted data scraped from the web. In some cases, even scrapes of ebook piracy websites - google &#x27;books3&#x27; to learn more.<p>Some companies (such as those working on AI) believe this is legal, others (such as the copyright holders to those books) believe it isn&#x27;t.<p>In any case, IMHO it&#x27;s unlikely any cutting edge models will be offering us their training data any time soon.</div><br/><div id="39463946" class="c"><input type="checkbox" id="c-39463946" checked=""/><div class="controls bullet"><span class="by">sanroot99</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39455290">parent</a><span>|</span><a href="#39460855">next</a><span>|</span><label class="collapse" for="c-39463946">[-]</label><label class="expand" for="c-39463946">[1 more]</label></div><br/><div class="children"><div class="content">Can training data be generated from llm,with right prompt?</div><br/></div></div></div></div><div id="39460855" class="c"><input type="checkbox" id="c-39460855" checked=""/><div class="controls bullet"><span class="by">etiam</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453921">parent</a><span>|</span><a href="#39455290">prev</a><span>|</span><a href="#39454065">next</a><span>|</span><label class="collapse" for="c-39460855">[-]</label><label class="expand" for="c-39460855">[1 more]</label></div><br/><div class="children"><div class="content">Yes, and there has been some discussion of that<p><i>Meta’s LLaMa 2 license is not Open Source</i> <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36820122">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36820122</a></div><br/></div></div><div id="39454065" class="c"><input type="checkbox" id="c-39454065" checked=""/><div class="controls bullet"><span class="by">sunnybeetroot</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453921">parent</a><span>|</span><a href="#39460855">prev</a><span>|</span><a href="#39454130">next</a><span>|</span><label class="collapse" for="c-39454065">[-]</label><label class="expand" for="c-39454065">[3 more]</label></div><br/><div class="children"><div class="content">That’s why they’re called open as in free to use how you wish, not open source where the source of the training is also provided.</div><br/><div id="39454094" class="c"><input type="checkbox" id="c-39454094" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454065">parent</a><span>|</span><a href="#39454130">next</a><span>|</span><label class="collapse" for="c-39454094">[-]</label><label class="expand" for="c-39454094">[2 more]</label></div><br/><div class="children"><div class="content">But my point is there&#x27;s no analogy for that that we call open? It&#x27;s like self-hostable, or free (as in beer).</div><br/><div id="39454129" class="c"><input type="checkbox" id="c-39454129" checked=""/><div class="controls bullet"><span class="by">sunnybeetroot</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454094">parent</a><span>|</span><a href="#39454130">next</a><span>|</span><label class="collapse" for="c-39454129">[-]</label><label class="expand" for="c-39454129">[1 more]</label></div><br/><div class="children"><div class="content">That’s a fair comment, maybe free-to-use is more appropriate.</div><br/></div></div></div></div></div></div><div id="39454130" class="c"><input type="checkbox" id="c-39454130" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453921">parent</a><span>|</span><a href="#39454065">prev</a><span>|</span><a href="#39453792">next</a><span>|</span><label class="collapse" for="c-39454130">[-]</label><label class="expand" for="c-39454130">[2 more]</label></div><br/><div class="children"><div class="content">Man, people will find anything to complain about.</div><br/><div id="39454179" class="c"><input type="checkbox" id="c-39454179" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454130">parent</a><span>|</span><a href="#39453792">next</a><span>|</span><label class="collapse" for="c-39454179">[-]</label><label class="expand" for="c-39454179">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not complaining, I&#x27;m unlikely ever to use it (regardless of how open or not it is) so it doesn&#x27;t really matter to me, just surprised to learn what people mean by &#x27;open&#x27; in this context.</div><br/></div></div></div></div></div></div></div></div><div id="39453792" class="c"><input type="checkbox" id="c-39453792" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453720">parent</a><span>|</span><a href="#39453760">prev</a><span>|</span><a href="#39453750">next</a><span>|</span><label class="collapse" for="c-39453792">[-]</label><label class="expand" for="c-39453792">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-7b-it&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-7b-it&#x2F;tree&#x2F;main</a><p>yes, similar to the llama models, you&#x27;ll also need to accept the license to download them officially. But the llama models have been unofficially downloadable without accepting the license for quite a while, so it&#x27;s probably just a matter of time.</div><br/></div></div></div></div></div></div></div></div><div id="39453750" class="c"><input type="checkbox" id="c-39453750" checked=""/><div class="controls bullet"><span class="by">sbarre</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453508">prev</a><span>|</span><a href="#39454187">next</a><span>|</span><label class="collapse" for="c-39453750">[-]</label><label class="expand" for="c-39453750">[13 more]</label></div><br/><div class="children"><div class="content">Can the Gemma models be downloaded to run locally, like open-source models Llama2, Mistral, etc ?<p>Or is your definition of &quot;open&quot; different?</div><br/><div id="39454339" class="c"><input type="checkbox" id="c-39454339" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453750">parent</a><span>|</span><a href="#39453790">next</a><span>|</span><label class="collapse" for="c-39454339">[-]</label><label class="expand" for="c-39454339">[1 more]</label></div><br/><div class="children"><div class="content">Yes models can be downloaded locally. In addition to the python NN frameworks and ggml as options, we also implemented a standalone C++ implementation that you can run locally at <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp</a></div><br/></div></div><div id="39453790" class="c"><input type="checkbox" id="c-39453790" checked=""/><div class="controls bullet"><span class="by">Kostic</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453750">parent</a><span>|</span><a href="#39454339">prev</a><span>|</span><a href="#39453787">next</a><span>|</span><label class="collapse" for="c-39453790">[-]</label><label class="expand" for="c-39453790">[2 more]</label></div><br/><div class="children"><div class="content">It should be possible to run it via llama.cpp[0] now.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;5631">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;5631</a></div><br/><div id="39455348" class="c"><input type="checkbox" id="c-39455348" checked=""/><div class="controls bullet"><span class="by">nerdix</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453790">parent</a><span>|</span><a href="#39453787">next</a><span>|</span><label class="collapse" for="c-39455348">[-]</label><label class="expand" for="c-39455348">[1 more]</label></div><br/><div class="children"><div class="content">Amazing how quickly this happened.</div><br/></div></div></div></div><div id="39453787" class="c"><input type="checkbox" id="c-39453787" checked=""/><div class="controls bullet"><span class="by">kathleenfromgdm</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453750">parent</a><span>|</span><a href="#39453790">prev</a><span>|</span><a href="#39456147">next</a><span>|</span><label class="collapse" for="c-39453787">[-]</label><label class="expand" for="c-39453787">[4 more]</label></div><br/><div class="children"><div class="content">Yes, you can get started downloading the model and running inference on Kaggle: <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma</a> ; for a full list of ways to interact with the model, you can check out <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma</a>.</div><br/><div id="39453834" class="c"><input type="checkbox" id="c-39453834" checked=""/><div class="controls bullet"><span class="by">dartharva</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453787">parent</a><span>|</span><a href="#39453945">next</a><span>|</span><label class="collapse" for="c-39453834">[-]</label><label class="expand" for="c-39453834">[1 more]</label></div><br/><div class="children"><div class="content">Can we have llamafile releases as well?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile</a></div><br/></div></div><div id="39453945" class="c"><input type="checkbox" id="c-39453945" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453787">parent</a><span>|</span><a href="#39453834">prev</a><span>|</span><a href="#39456147">next</a><span>|</span><label class="collapse" for="c-39453945">[-]</label><label class="expand" for="c-39453945">[2 more]</label></div><br/><div class="children"><div class="content">A small typo in your model link that breaks it. There’s an extra ; on the end.</div><br/><div id="39453976" class="c"><input type="checkbox" id="c-39453976" checked=""/><div class="controls bullet"><span class="by">kathleenfromgdm</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453945">parent</a><span>|</span><a href="#39456147">next</a><span>|</span><label class="collapse" for="c-39453976">[-]</label><label class="expand" for="c-39453976">[1 more]</label></div><br/><div class="children"><div class="content">Corrected - thanks :)</div><br/></div></div></div></div></div></div><div id="39456147" class="c"><input type="checkbox" id="c-39456147" checked=""/><div class="controls bullet"><span class="by">mrob</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453750">parent</a><span>|</span><a href="#39453787">prev</a><span>|</span><a href="#39454101">next</a><span>|</span><label class="collapse" for="c-39456147">[-]</label><label class="expand" for="c-39456147">[1 more]</label></div><br/><div class="children"><div class="content">Mistral weights are released under an Apache 2.0 license, but Llama 2 weights are released under a proprietary license that prohibits use by large organizations and imposes usage restrictions, violating terms 5 and 6 the Open Source Definition[0]. Even if you accept that a model with a proprietary training dataset and proprietary training code can be considered &quot;open source&quot;, there&#x27;s no way Llama 2 qualifies.<p>For consistency with existing definitions[1], Llama 2 should be labeled a &quot;weights available&quot; model.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Open_Source_Definition" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Open_Source_Definition</a><p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Source-available_software" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Source-available_software</a></div><br/></div></div><div id="39454101" class="c"><input type="checkbox" id="c-39454101" checked=""/><div class="controls bullet"><span class="by">tomp</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453750">parent</a><span>|</span><a href="#39456147">prev</a><span>|</span><a href="#39454187">next</a><span>|</span><label class="collapse" for="c-39454101">[-]</label><label class="expand" for="c-39454101">[4 more]</label></div><br/><div class="children"><div class="content">Their definition of &quot;open&quot; is &quot;not open&quot;, i.e. you&#x27;re only allowed to use Gemma in &quot;non-harmful&quot; way.<p>We all know that Google thinks that saying that 1800s English kings were <i>white</i> is &quot;harmful&quot;.</div><br/><div id="39457684" class="c"><input type="checkbox" id="c-39457684" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454101">parent</a><span>|</span><a href="#39455366">next</a><span>|</span><label class="collapse" for="c-39457684">[-]</label><label class="expand" for="c-39457684">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We all know that Google thinks that saying that 1800s English kings were white is &quot;harmful&quot;.<p>If you know how to make &quot;1800s english kings&quot; show up as white 100% of the time without also making &quot;kings&quot; show up as white 100% of the time, maybe you should apply to Google? Clearly you must have advanced knowledge on how to perfectly remove bias from training distributions if you casually throw stones like this.</div><br/><div id="39458945" class="c"><input type="checkbox" id="c-39458945" checked=""/><div class="controls bullet"><span class="by">trackflak</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39457684">parent</a><span>|</span><a href="#39455366">next</a><span>|</span><label class="collapse" for="c-39458945">[-]</label><label class="expand" for="c-39458945">[1 more]</label></div><br/><div class="children"><div class="content">Tell me you take this seriously: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;napoleon21st&#x2F;status&#x2F;1760116228746805272" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;napoleon21st&#x2F;status&#x2F;1760116228746805272</a><p>It has no problem with other cultures and ethnicities, yet somehow white or Japanese just throws everything off?<p>I suppose &#x27;bias&#x27; is the new word for &quot;basic historic accuracy&quot;. I can get curious about other peoples without forcibly promoting them at the expense of my own Western and British people and culture. This &#x27;anti bias&#x27; keyword injection is a laughably bad, in your face solution to a non-issue.<p>I lament the day &#x27;anti-bias&#x27; AI this terrible is used to make real world decisions. At least we now know we can&#x27;t trust such a model because it has already been so evidently crippled by its makers.</div><br/></div></div></div></div><div id="39455366" class="c"><input type="checkbox" id="c-39455366" checked=""/><div class="controls bullet"><span class="by">wantsanagent</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454101">parent</a><span>|</span><a href="#39457684">prev</a><span>|</span><a href="#39454187">next</a><span>|</span><label class="collapse" for="c-39455366">[-]</label><label class="expand" for="c-39455366">[1 more]</label></div><br/><div class="children"><div class="content">Not sure why you&#x27;re getting downvoted. I would have thought HN of all places would recognize the power and value of OSI licensing and the danger of the proliferation of these source available but definitely not Open Source licenses.</div><br/></div></div></div></div></div></div><div id="39454187" class="c"><input type="checkbox" id="c-39454187" checked=""/><div class="controls bullet"><span class="by">audessuscest</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453750">prev</a><span>|</span><a href="#39453444">next</a><span>|</span><label class="collapse" for="c-39454187">[-]</label><label class="expand" for="c-39454187">[8 more]</label></div><br/><div class="children"><div class="content">Does this model also thinks german were black 200 years ago ? Or is afraid to answer basic stuff ? because if this is the case no one will care about that model.</div><br/><div id="39456232" class="c"><input type="checkbox" id="c-39456232" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454187">parent</a><span>|</span><a href="#39456324">next</a><span>|</span><label class="collapse" for="c-39456232">[-]</label><label class="expand" for="c-39456232">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know anything about these twitter accounts so I don&#x27;t know how credible they are, but here are some examples for your downvoters that I&#x27;m guessing just think you&#x27;re just trolling or grossly exaggerating:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;aginnt&#x2F;status&#x2F;1760159436323123632" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;aginnt&#x2F;status&#x2F;1760159436323123632</a><p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;Black_Pilled&#x2F;status&#x2F;1760198299443966382" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;Black_Pilled&#x2F;status&#x2F;1760198299443966382</a></div><br/><div id="39456698" class="c"><input type="checkbox" id="c-39456698" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39456232">parent</a><span>|</span><a href="#39456324">next</a><span>|</span><label class="collapse" for="c-39456698">[-]</label><label class="expand" for="c-39456698">[2 more]</label></div><br/><div class="children"><div class="content">Yea.  Just ask it anything about historical people&#x2F;cultures and it will seemingly lobotomize itself.<p>I asked it about early Japan and it talked about how European women used Katanas and how Native Americans rode across the grassy plains carrying traditional Japanese weapons.  Pure made up nonsense that not even primitive models would get wrong.  Not sure what they did to it.  I asked it why it assumed Native Americans were in Japan in the 1100s and it said:<p>&gt; I assumed [...] various ethnicities, including Indigenous American, due to the diversity present in Japan throughout history. However, this overlooked [...] I focused on providing diverse representations without adequately considering the specific historical context.<p>How am I supposed to take this seriously?  Especially on topics I&#x27;m unfamiliar with?</div><br/><div id="39458858" class="c"><input type="checkbox" id="c-39458858" checked=""/><div class="controls bullet"><span class="by">trackflak</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39456698">parent</a><span>|</span><a href="#39456324">next</a><span>|</span><label class="collapse" for="c-39458858">[-]</label><label class="expand" for="c-39458858">[1 more]</label></div><br/><div class="children"><div class="content">From one of the Twitter threads linked above:<p>&gt; they insert random keyword in the prompts randomly to counter bias, that got revealed with something else I think. Had T shirts written with &quot;diverse&quot; on it as artifact<p>This was exposed as being the case with OpenAI&#x27;s DALL-E as well - someone had typed a prompt of &quot;Homer Simpson wearing a namebadge&quot; and it generated an image of Homer with brown skin wearing a namebadge that said &#x27;ethnically ambiguous&#x27;.<p>This is ludicrous - if they are fiddling with your prompt in this way, it will only stoke more frustration and resentment - achieving the opposite of why this has been implemented. Surely if we want diversity we will ask for it, but sometimes you don&#x27;t, and that should be at the user&#x27;s discretion.\<p>Another thread for context: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;napoleon21st&#x2F;status&#x2F;1760116228746805272" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;napoleon21st&#x2F;status&#x2F;1760116228746805272</a></div><br/></div></div></div></div></div></div><div id="39456324" class="c"><input type="checkbox" id="c-39456324" checked=""/><div class="controls bullet"><span class="by">graphe</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454187">parent</a><span>|</span><a href="#39456232">prev</a><span>|</span><a href="#39453444">next</a><span>|</span><label class="collapse" for="c-39456324">[-]</label><label class="expand" for="c-39456324">[4 more]</label></div><br/><div class="children"><div class="content">I disagree, coding and RAG performance is all that matters to me. I&#x27;m not using an LLM to learn basic facts I already know.</div><br/><div id="39457115" class="c"><input type="checkbox" id="c-39457115" checked=""/><div class="controls bullet"><span class="by">audessuscest</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39456324">parent</a><span>|</span><a href="#39456787">next</a><span>|</span><label class="collapse" for="c-39457115">[-]</label><label class="expand" for="c-39457115">[2 more]</label></div><br/><div class="children"><div class="content">we&#x27;re at basic knowledge level, if your RAG imply some of it, you can get bad result too. Anyway, would you use a model who makes this nonsense response or one that doesn&#x27;t? I know which one I will prefer for sure...</div><br/><div id="39457605" class="c"><input type="checkbox" id="c-39457605" checked=""/><div class="controls bullet"><span class="by">graphe</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39457115">parent</a><span>|</span><a href="#39456787">next</a><span>|</span><label class="collapse" for="c-39457605">[-]</label><label class="expand" for="c-39457605">[1 more]</label></div><br/><div class="children"><div class="content">If this was better at specific RAG or coding performance I would absolutely, certainly without a doubt use it over a general instruct model in those instances.</div><br/></div></div></div></div><div id="39456787" class="c"><input type="checkbox" id="c-39456787" checked=""/><div class="controls bullet"><span class="by">TheHypnotist</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39456324">parent</a><span>|</span><a href="#39457115">prev</a><span>|</span><a href="#39453444">next</a><span>|</span><label class="collapse" for="c-39456787">[-]</label><label class="expand" for="c-39456787">[1 more]</label></div><br/><div class="children"><div class="content">How do you ragebait for premium pearl clutching?</div><br/></div></div></div></div></div></div><div id="39453444" class="c"><input type="checkbox" id="c-39453444" checked=""/><div class="controls bullet"><span class="by">neximo64</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39454187">prev</a><span>|</span><a href="#39453431">next</a><span>|</span><label class="collapse" for="c-39453444">[-]</label><label class="expand" for="c-39453444">[2 more]</label></div><br/><div class="children"><div class="content">How are these performing so well compared to Llama 2, are there any documents on the architecture and differences, is it MoE?<p>Also note some of the links on the blog post don&#x27;t work, e.g debugging tool.</div><br/><div id="39453771" class="c"><input type="checkbox" id="c-39453771" checked=""/><div class="controls bullet"><span class="by">kathleenfromgdm</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453444">parent</a><span>|</span><a href="#39453431">next</a><span>|</span><label class="collapse" for="c-39453771">[-]</label><label class="expand" for="c-39453771">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve documented the architecture (including key differences) in our technical report here (<a href="https:&#x2F;&#x2F;goo.gle&#x2F;GemmaReport" rel="nofollow">https:&#x2F;&#x2F;goo.gle&#x2F;GemmaReport</a>), and you can see the architecture implementation in our Git Repo (<a href="https:&#x2F;&#x2F;github.com&#x2F;google-deepmind&#x2F;gemma">https:&#x2F;&#x2F;github.com&#x2F;google-deepmind&#x2F;gemma</a>).</div><br/></div></div></div></div><div id="39453431" class="c"><input type="checkbox" id="c-39453431" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453444">prev</a><span>|</span><a href="#39453432">next</a><span>|</span><label class="collapse" for="c-39453431">[-]</label><label class="expand" for="c-39453431">[2 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch and thanks for the contribution! This looks like it&#x27;s on-par or better compared to mistral 7B 0.1 or is that 0.2?<p>Are there plans for MoE or 70B models?</div><br/><div id="39454042" class="c"><input type="checkbox" id="c-39454042" checked=""/><div class="controls bullet"><span class="by">kathleenfromgdm</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453431">parent</a><span>|</span><a href="#39453432">next</a><span>|</span><label class="collapse" for="c-39454042">[-]</label><label class="expand" for="c-39454042">[1 more]</label></div><br/><div class="children"><div class="content">Great question - we compare to the Mistral 7B 0.1 pretrained models (since there were no pretrained checkpoint updates in 0.2) and the Mistral 7B 0.2 instruction-tuned models in the technical report here: <a href="https:&#x2F;&#x2F;goo.gle&#x2F;GemmaReport" rel="nofollow">https:&#x2F;&#x2F;goo.gle&#x2F;GemmaReport</a></div><br/></div></div></div></div><div id="39453432" class="c"><input type="checkbox" id="c-39453432" checked=""/><div class="controls bullet"><span class="by">zitterbewegung</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453431">prev</a><span>|</span><a href="#39458502">next</a><span>|</span><label class="collapse" for="c-39453432">[-]</label><label class="expand" for="c-39453432">[5 more]</label></div><br/><div class="children"><div class="content">Do you have a plan of releasing higher parameter models?</div><br/><div id="39453934" class="c"><input type="checkbox" id="c-39453934" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453432">parent</a><span>|</span><a href="#39458502">next</a><span>|</span><label class="collapse" for="c-39453934">[-]</label><label class="expand" for="c-39453934">[4 more]</label></div><br/><div class="children"><div class="content">We have many great things in research and development phases, so stay tuned. I’m hopeful we can share more in the coming weeks and month!</div><br/><div id="39455920" class="c"><input type="checkbox" id="c-39455920" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453934">parent</a><span>|</span><a href="#39463085">next</a><span>|</span><label class="collapse" for="c-39455920">[-]</label><label class="expand" for="c-39455920">[1 more]</label></div><br/><div class="children"><div class="content">That is awesome!<p>I hope y&#x27;all consider longer context models as well.<p>Also, are ya&#x27;ll looking alternative architectures like Mamba? Being &quot;first&quot; with a large Mamba model would cement your architectural choices&#x2F;framework support like llama did for Meta.</div><br/></div></div><div id="39463085" class="c"><input type="checkbox" id="c-39463085" checked=""/><div class="controls bullet"><span class="by">efilife</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453934">parent</a><span>|</span><a href="#39455920">prev</a><span>|</span><a href="#39458502">next</a><span>|</span><label class="collapse" for="c-39463085">[-]</label><label class="expand" for="c-39463085">[2 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t answer the question at all</div><br/></div></div></div></div></div></div><div id="39458502" class="c"><input type="checkbox" id="c-39458502" checked=""/><div class="controls bullet"><span class="by">owl_brawl</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453432">prev</a><span>|</span><a href="#39454823">next</a><span>|</span><label class="collapse" for="c-39458502">[-]</label><label class="expand" for="c-39458502">[1 more]</label></div><br/><div class="children"><div class="content">Hi alekandreev,<p>Any reason you decided to go with a token vocabulary size of 256k? Smaller vocab&#x2F;vector sizes like most models in this size seem to be using (~16-32k) are much easier to work with. Would love to understand the technical reasoning here that isn&#x27;t detailed in the report unfortunately :(.</div><br/></div></div><div id="39454823" class="c"><input type="checkbox" id="c-39454823" checked=""/><div class="controls bullet"><span class="by">memossy</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39458502">prev</a><span>|</span><a href="#39453729">next</a><span>|</span><label class="collapse" for="c-39454823">[-]</label><label class="expand" for="c-39454823">[1 more]</label></div><br/><div class="children"><div class="content">Training on 4096 v5es how did you handle crazy batch size :o</div><br/></div></div><div id="39453729" class="c"><input type="checkbox" id="c-39453729" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39454823">prev</a><span>|</span><a href="#39454112">next</a><span>|</span><label class="collapse" for="c-39453729">[-]</label><label class="expand" for="c-39453729">[2 more]</label></div><br/><div class="children"><div class="content">Are there any plans for releasing the datasets used?</div><br/><div id="39454085" class="c"><input type="checkbox" id="c-39454085" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453729">parent</a><span>|</span><a href="#39454112">next</a><span>|</span><label class="collapse" for="c-39454085">[-]</label><label class="expand" for="c-39454085">[1 more]</label></div><br/><div class="children"><div class="content">This would be really interesting in my opinion, but we are not releasing datasets at this time. See the C4 dataset for an earlier open dataset from Google.</div><br/></div></div></div></div><div id="39454112" class="c"><input type="checkbox" id="c-39454112" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39453729">prev</a><span>|</span><a href="#39455549">next</a><span>|</span><label class="collapse" for="c-39454112">[-]</label><label class="expand" for="c-39454112">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s cool that you guys are able to release open stuff, that must be a nice change from the modus operandi at goog.  I&#x27;ll have to double check but it looks like phi-2 beats your performance in some cases while being smaller, I&#x27;m guessing the value proposition of these models is being small and good while also having more knowledge baked in?</div><br/><div id="39461217" class="c"><input type="checkbox" id="c-39461217" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39454112">parent</a><span>|</span><a href="#39455549">next</a><span>|</span><label class="collapse" for="c-39461217">[-]</label><label class="expand" for="c-39461217">[1 more]</label></div><br/><div class="children"><div class="content">We deeply respect the Phi team and all other teams in the open model space.  You’ll find that different models have different strengths and not all can be quantified with existing public evals. Take them for a spin and see what works for you.</div><br/></div></div></div></div><div id="39455549" class="c"><input type="checkbox" id="c-39455549" checked=""/><div class="controls bullet"><span class="by">dmnsl</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39454112">prev</a><span>|</span><a href="#39457993">next</a><span>|</span><label class="collapse" for="c-39455549">[-]</label><label class="expand" for="c-39455549">[3 more]</label></div><br/><div class="children"><div class="content">Hi, what is the cutoff date ?</div><br/><div id="39461219" class="c"><input type="checkbox" id="c-39461219" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39455549">parent</a><span>|</span><a href="#39456623">next</a><span>|</span><label class="collapse" for="c-39461219">[-]</label><label class="expand" for="c-39461219">[1 more]</label></div><br/><div class="children"><div class="content">September 2023.</div><br/></div></div><div id="39456623" class="c"><input type="checkbox" id="c-39456623" checked=""/><div class="controls bullet"><span class="by">legohead</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39455549">parent</a><span>|</span><a href="#39461219">prev</a><span>|</span><a href="#39457993">next</a><span>|</span><label class="collapse" for="c-39456623">[-]</label><label class="expand" for="c-39456623">[1 more]</label></div><br/><div class="children"><div class="content">All it will tell me is mid-2018.</div><br/></div></div></div></div><div id="39457993" class="c"><input type="checkbox" id="c-39457993" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39455549">prev</a><span>|</span><a href="#39455096">next</a><span>|</span><label class="collapse" for="c-39457993">[-]</label><label class="expand" for="c-39457993">[2 more]</label></div><br/><div class="children"><div class="content">Hi! This is such an exciting release. Congratulations!<p>I work on Ollama and used the provided GGUF files to quantize the model. As mentioned by a few people here, the 4-bit integer quantized models (which Ollama defaults to) seem to have strange output with non-existent words and funny use of whitespace.<p>Do you have a link &#x2F;reference as to how the models were converted to GGUF format? And is it expected that quantizing the models might cause this issue?<p>Thanks so much!</div><br/><div id="39458836" class="c"><input type="checkbox" id="c-39458836" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39457993">parent</a><span>|</span><a href="#39455096">next</a><span>|</span><label class="collapse" for="c-39458836">[-]</label><label class="expand" for="c-39458836">[1 more]</label></div><br/><div class="children"><div class="content">As a data point, using the Huggingface Transformers 4-bit quantization yields reasonable results: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;espadrine&#x2F;status&#x2F;1760355758309298421" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;espadrine&#x2F;status&#x2F;1760355758309298421</a></div><br/></div></div></div></div><div id="39455096" class="c"><input type="checkbox" id="c-39455096" checked=""/><div class="controls bullet"><span class="by">quickgist</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39457993">prev</a><span>|</span><a href="#39453753">next</a><span>|</span><label class="collapse" for="c-39455096">[-]</label><label class="expand" for="c-39455096">[1 more]</label></div><br/><div class="children"><div class="content">Will this be available as a Vertex AI foundational model like Gemini 1.0, without deploying a custom endpoint? Any info on pricing? (Also, when will Gemini 1.5 be available on Vertex?)</div><br/></div></div><div id="39453753" class="c"><input type="checkbox" id="c-39453753" checked=""/><div class="controls bullet"><span class="by">vorticalbox</span><span>|</span><a href="#39453398">parent</a><span>|</span><a href="#39455096">prev</a><span>|</span><a href="#39453610">next</a><span>|</span><label class="collapse" for="c-39453753">[-]</label><label class="expand" for="c-39453753">[3 more]</label></div><br/><div class="children"><div class="content">are there plans to release an official GGUF version to use with llama.ccp?</div><br/><div id="39453850" class="c"><input type="checkbox" id="c-39453850" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453753">parent</a><span>|</span><a href="#39453610">next</a><span>|</span><label class="collapse" for="c-39453850">[-]</label><label class="expand" for="c-39453850">[2 more]</label></div><br/><div class="children"><div class="content">It is already part of the release on Huggingface: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-7b&#x2F;blob&#x2F;main&#x2F;gemma-7b.gguf" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-7b&#x2F;blob&#x2F;main&#x2F;gemma-7b.gg...</a><p>It is a pretty clean release! I had some 500 issues with Kaggle validating my license approval, so you might too, but after a few attempts I could access the model.</div><br/><div id="39457056" class="c"><input type="checkbox" id="c-39457056" checked=""/><div class="controls bullet"><span class="by">vorticalbox</span><span>|</span><a href="#39453398">root</a><span>|</span><a href="#39453850">parent</a><span>|</span><a href="#39453610">next</a><span>|</span><label class="collapse" for="c-39457056">[-]</label><label class="expand" for="c-39457056">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t see this when searching thanks</div><br/></div></div></div></div></div></div></div></div><div id="39453610" class="c"><input type="checkbox" id="c-39453610" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39453398">prev</a><span>|</span><a href="#39453730">next</a><span>|</span><label class="collapse" for="c-39453610">[-]</label><label class="expand" for="c-39453610">[16 more]</label></div><br/><div class="children"><div class="content">I notice a few divergences to common models:<p>- The feedforward hidden size is 16x the d_model, unlike most models which are typically 4x;<p>- The vocabulary size is 10x (256K vs. Mistral’s 32K);<p>- The training token count is tripled (6T vs. Llama2&#x27;s 2T)<p>Apart from that, it uses the classic transformer variations: MQA, RoPE, RMSNorm.<p>How big was the batch size that it could be trained so fast?<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;mistralai&#x2F;Mistral-7B-Instruct-v0.2&#x2F;blob&#x2F;main&#x2F;config.json" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;mistralai&#x2F;Mistral-7B-Instruct-v0.2&#x2F;bl...</a></div><br/><div id="39458002" class="c"><input type="checkbox" id="c-39458002" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#39453610">parent</a><span>|</span><a href="#39454317">next</a><span>|</span><label class="collapse" for="c-39458002">[-]</label><label class="expand" for="c-39458002">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The training token count is tripled (6T vs. Llama2&#x27;s 2T)<p>Damn, 6T? That&#x27;s a lot!<p>Given that this model seems to roughly match Mistral (according to the numbers from Google), this makes me think we have saturated the 7B parameter space, and couldn&#x27;t possibly make it much better unless new techniques are discovered.</div><br/><div id="39458338" class="c"><input type="checkbox" id="c-39458338" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39458002">parent</a><span>|</span><a href="#39454317">next</a><span>|</span><label class="collapse" for="c-39458338">[-]</label><label class="expand" for="c-39458338">[1 more]</label></div><br/><div class="children"><div class="content">Hard to say definitively. Mistral’s token embeddings only account for &lt;2% of the 7B parameters, while Gemma’s larger token vocabulary vampirized over 10%, leaving less space for the more important parts of the network. It is a somewhat surprising tradeoff given that it was pretrained towards an English bias.</div><br/></div></div></div></div><div id="39454317" class="c"><input type="checkbox" id="c-39454317" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39453610">parent</a><span>|</span><a href="#39458002">prev</a><span>|</span><a href="#39456497">next</a><span>|</span><label class="collapse" for="c-39454317">[-]</label><label class="expand" for="c-39454317">[7 more]</label></div><br/><div class="children"><div class="content">Looking at the config.json of Gemma 7B the feedfoarward hidden size is 8x, not 16x</div><br/><div id="39454634" class="c"><input type="checkbox" id="c-39454634" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39454317">parent</a><span>|</span><a href="#39456497">next</a><span>|</span><label class="collapse" for="c-39454634">[-]</label><label class="expand" for="c-39454634">[6 more]</label></div><br/><div class="children"><div class="content">Huh, indeed, that&#x27;s what the config.json[0] says; the report[1] indicates “Feedforward hidden dims: 49152”.<p>[0]:<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-7b-it&#x2F;blob&#x2F;main&#x2F;config.json" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-7b-it&#x2F;blob&#x2F;main&#x2F;config.j...</a><p>[1]: <a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemma&#x2F;gemma-report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemma&#x2F;gemma-re...</a></div><br/><div id="39455757" class="c"><input type="checkbox" id="c-39455757" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39454634">parent</a><span>|</span><a href="#39456497">next</a><span>|</span><label class="collapse" for="c-39455757">[-]</label><label class="expand" for="c-39455757">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see the number 49152 reported in the config.json, what line are you referring to? I just see the intermediate_size of 24576 (so 8x).<p>EDIT: I didn&#x27;t read the comment correctly, you have noticed the same thing.</div><br/><div id="39458016" class="c"><input type="checkbox" id="c-39458016" checked=""/><div class="controls bullet"><span class="by">voxgen</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39455757">parent</a><span>|</span><a href="#39456033">next</a><span>|</span><label class="collapse" for="c-39458016">[-]</label><label class="expand" for="c-39458016">[2 more]</label></div><br/><div class="children"><div class="content">The *GLU-based activations functions like GEGLU and SwiGLU use 2 input values to produce 1 output value, which makes these numbers weird. In each value pair, one goes through the GELU&#x2F;SiLU activation function and is then multiplied by the other &quot;gate&quot; value.<p>In the report, &quot;hidden dim&quot; matches the number of GEGLU inputs. In the config, &quot;intermediate_size&quot; matches the number of GEGLU outputs. Most *GLU models so far have used intermediate_size=8&#x2F;3*d_model as this makes have the same number of matmul FLOPS &amp; parameters as a 4x-expanded non-GLU model, and PaLM vaguely showed that 4x is better than a smaller expansion factor.<p>If one considers Llama-2-7B&#x27;s FFN expansion factor to be ~5.33x, Gemma&#x27;s expansion factor is 16x.</div><br/><div id="39458285" class="c"><input type="checkbox" id="c-39458285" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39458016">parent</a><span>|</span><a href="#39456033">next</a><span>|</span><label class="collapse" for="c-39458285">[-]</label><label class="expand" for="c-39458285">[1 more]</label></div><br/><div class="children"><div class="content">Makes perfect sense thx</div><br/></div></div></div></div><div id="39456033" class="c"><input type="checkbox" id="c-39456033" checked=""/><div class="controls bullet"><span class="by">SahAssar</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39455757">parent</a><span>|</span><a href="#39458016">prev</a><span>|</span><a href="#39456046">next</a><span>|</span><label class="collapse" for="c-39456033">[-]</label><label class="expand" for="c-39456033">[1 more]</label></div><br/><div class="children"><div class="content">Read the parent comment again. It says the paper says 49152, not the config.json.</div><br/></div></div></div></div></div></div></div></div><div id="39456497" class="c"><input type="checkbox" id="c-39456497" checked=""/><div class="controls bullet"><span class="by">lalaithion</span><span>|</span><a href="#39453610">parent</a><span>|</span><a href="#39454317">prev</a><span>|</span><a href="#39453730">next</a><span>|</span><label class="collapse" for="c-39456497">[-]</label><label class="expand" for="c-39456497">[6 more]</label></div><br/><div class="children"><div class="content">What does tokenization look like in 256k vs 32k?</div><br/><div id="39456972" class="c"><input type="checkbox" id="c-39456972" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39456497">parent</a><span>|</span><a href="#39458039">next</a><span>|</span><label class="collapse" for="c-39456972">[-]</label><label class="expand" for="c-39456972">[3 more]</label></div><br/><div class="children"><div class="content">It mostly means that there are tokens dedicated to rarer sequences of characters, even in foreign languages (note that Gemma is not intended to be good multilingually): “説明書” (instruction manual) has its own token, and so does “Nixon”, “آباد” (a city suffix, I believe), and the HTML sequence &quot;\&quot;&gt;&lt;!--&quot;.</div><br/><div id="39458319" class="c"><input type="checkbox" id="c-39458319" checked=""/><div class="controls bullet"><span class="by">lalaithion</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39456972">parent</a><span>|</span><a href="#39458039">next</a><span>|</span><label class="collapse" for="c-39458319">[-]</label><label class="expand" for="c-39458319">[2 more]</label></div><br/><div class="children"><div class="content">I understand the theory, I was looking for an example of the same text tokenized with the two different vocabularies.</div><br/><div id="39458381" class="c"><input type="checkbox" id="c-39458381" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39458319">parent</a><span>|</span><a href="#39458039">next</a><span>|</span><label class="collapse" for="c-39458381">[-]</label><label class="expand" for="c-39458381">[1 more]</label></div><br/><div class="children"><div class="content">Do you have an example text in mind?<p>You can use this playground to test it out: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;Xenova&#x2F;the-tokenizer-playground" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;Xenova&#x2F;the-tokenizer-playgroun...</a></div><br/></div></div></div></div></div></div><div id="39458039" class="c"><input type="checkbox" id="c-39458039" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39456497">parent</a><span>|</span><a href="#39456972">prev</a><span>|</span><a href="#39453730">next</a><span>|</span><label class="collapse" for="c-39458039">[-]</label><label class="expand" for="c-39458039">[2 more]</label></div><br/><div class="children"><div class="content">Text encodes in fewer tokens, and language coverage is better.</div><br/><div id="39458320" class="c"><input type="checkbox" id="c-39458320" checked=""/><div class="controls bullet"><span class="by">lalaithion</span><span>|</span><a href="#39453610">root</a><span>|</span><a href="#39458039">parent</a><span>|</span><a href="#39453730">next</a><span>|</span><label class="collapse" for="c-39458320">[-]</label><label class="expand" for="c-39458320">[1 more]</label></div><br/><div class="children"><div class="content">I understand the theory, I was looking for an example of the same text tokenized with the two different vocabularies.</div><br/></div></div></div></div></div></div></div></div><div id="39453730" class="c"><input type="checkbox" id="c-39453730" checked=""/><div class="controls bullet"><span class="by">margorczynski</span><span>|</span><a href="#39453610">prev</a><span>|</span><a href="#39453491">next</a><span>|</span><label class="collapse" for="c-39453730">[-]</label><label class="expand" for="c-39453730">[16 more]</label></div><br/><div class="children"><div class="content">Is there a chance we&#x27;ll get a model without the &quot;aligment&quot; (lobotomization)? There are many examples where answers from Gemini are garbage because of the ideological fine tuning.</div><br/><div id="39454033" class="c"><input type="checkbox" id="c-39454033" checked=""/><div class="controls bullet"><span class="by">kathleenfromgdm</span><span>|</span><a href="#39453730">parent</a><span>|</span><a href="#39455733">next</a><span>|</span><label class="collapse" for="c-39454033">[-]</label><label class="expand" for="c-39454033">[1 more]</label></div><br/><div class="children"><div class="content">We release our non-aligned models (marked as pretrained or PT models across platforms) alongside our fine-tuned checkpoints; for example, here is our pretrained 7B checkpoint for download: <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma&#x2F;frameworks&#x2F;keras&#x2F;variations&#x2F;gemma_7b_en" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma&#x2F;frameworks&#x2F;keras&#x2F;...</a></div><br/></div></div><div id="39455733" class="c"><input type="checkbox" id="c-39455733" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39453730">parent</a><span>|</span><a href="#39454033">prev</a><span>|</span><a href="#39454059">next</a><span>|</span><label class="collapse" for="c-39455733">[-]</label><label class="expand" for="c-39455733">[1 more]</label></div><br/><div class="children"><div class="content">Alignment is all but a non issue with open weight base model releases, as they can be finetuned to &quot;de align&quot; them if prompt engineering is not enough.</div><br/></div></div><div id="39453875" class="c"><input type="checkbox" id="c-39453875" checked=""/><div class="controls bullet"><span class="by">yakorevivan</span><span>|</span><a href="#39453730">parent</a><span>|</span><a href="#39454059">prev</a><span>|</span><a href="#39454050">next</a><span>|</span><label class="collapse" for="c-39453875">[-]</label><label class="expand" for="c-39453875">[1 more]</label></div><br/><div class="children"><div class="content">They have released finetuning code too. You can finetune it to remove the  alignment finetuning. I believe it would take just a few hours at max and a couple of dollars.</div><br/></div></div><div id="39454050" class="c"><input type="checkbox" id="c-39454050" checked=""/><div class="controls bullet"><span class="by">politician</span><span>|</span><a href="#39453730">parent</a><span>|</span><a href="#39453875">prev</a><span>|</span><a href="#39454005">next</a><span>|</span><label class="collapse" for="c-39454050">[-]</label><label class="expand" for="c-39454050">[6 more]</label></div><br/><div class="children"><div class="content">More useful would be a precise characterization of the type and balance of the ideological fine tuning.<p>They include performance benchmarks. End-users should also be aware of what thoughts are permitted in these constructs. Why omit this information?</div><br/><div id="39455341" class="c"><input type="checkbox" id="c-39455341" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39454050">parent</a><span>|</span><a href="#39454005">next</a><span>|</span><label class="collapse" for="c-39455341">[-]</label><label class="expand" for="c-39455341">[5 more]</label></div><br/><div class="children"><div class="content">&gt; End-users should also be aware of what thoughts are permitted in these constructs. Why omit this information?<p>Can you define that in a way that&#x27;s actually testable? I can&#x27;t, and I&#x27;ve been thinking about &quot;unthinkable thoughts&quot; for quite some time now: <a href="https:&#x2F;&#x2F;kitsunesoftware.wordpress.com&#x2F;2018&#x2F;06&#x2F;26&#x2F;unlearnable&#x2F;" rel="nofollow">https:&#x2F;&#x2F;kitsunesoftware.wordpress.com&#x2F;2018&#x2F;06&#x2F;26&#x2F;unlearnable...</a></div><br/><div id="39459337" class="c"><input type="checkbox" id="c-39459337" checked=""/><div class="controls bullet"><span class="by">ranyume</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39455341">parent</a><span>|</span><a href="#39459446">next</a><span>|</span><label class="collapse" for="c-39459337">[-]</label><label class="expand" for="c-39459337">[2 more]</label></div><br/><div class="children"><div class="content">Not OP, but I can think of a few:<p>* List of topics that are &quot;controversial&quot; (models tend to evade these)<p>* List of arguments that are &quot;controversial&quot; (models wont allow you to think differently. For example, models would never say arguments that &quot;encourage&quot; animal cruelty)<p>* On average, how willing is the model to take a neutral position on a &quot;controversial&quot; topic (sometimes models say something along the lines of &quot;this is on debate&quot;, but still lean heavily towards the less controversial position instead of having no position at all. For example, if you ask it what &quot;lolicon&quot; is, it will tell you what it is and tell you that japanese society is moving towards banning it)<p>edit: formatting</div><br/><div id="39463426" class="c"><input type="checkbox" id="c-39463426" checked=""/><div class="controls bullet"><span class="by">jppittma</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39459337">parent</a><span>|</span><a href="#39459446">next</a><span>|</span><label class="collapse" for="c-39463426">[-]</label><label class="expand" for="c-39463426">[1 more]</label></div><br/><div class="children"><div class="content">They will encourage animal cruelty if the alternative is veganism.</div><br/></div></div></div></div><div id="39459446" class="c"><input type="checkbox" id="c-39459446" checked=""/><div class="controls bullet"><span class="by">politician</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39455341">parent</a><span>|</span><a href="#39459337">prev</a><span>|</span><a href="#39454005">next</a><span>|</span><label class="collapse" for="c-39459446">[-]</label><label class="expand" for="c-39459446">[2 more]</label></div><br/><div class="children"><div class="content">Have you considered the use of Monte Carlo sampling to inspect latent behaviors?</div><br/><div id="39459524" class="c"><input type="checkbox" id="c-39459524" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39459446">parent</a><span>|</span><a href="#39454005">next</a><span>|</span><label class="collapse" for="c-39459524">[-]</label><label class="expand" for="c-39459524">[1 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s the wrong level to attack the problem; you can do that also with actual humans, but it won&#x27;t tell you what the human is <i>unable</i> to think, but rather what they <i>just didn&#x27;t think of given their stimulus</i> — and this difference is easily demonstrated, e.g. with Duncker&#x27;s candle problem: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Candle_problem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Candle_problem</a></div><br/></div></div></div></div></div></div></div></div><div id="39454005" class="c"><input type="checkbox" id="c-39454005" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#39453730">parent</a><span>|</span><a href="#39454050">prev</a><span>|</span><a href="#39453491">next</a><span>|</span><label class="collapse" for="c-39454005">[-]</label><label class="expand" for="c-39454005">[5 more]</label></div><br/><div class="children"><div class="content">You can (and someone will) fine tune it away. There are datasets which are foss you can use on hugging face.<p>Or you can just wait, it&#x27;ll be done soon...</div><br/><div id="39456925" class="c"><input type="checkbox" id="c-39456925" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39454005">parent</a><span>|</span><a href="#39455644">next</a><span>|</span><label class="collapse" for="c-39456925">[-]</label><label class="expand" for="c-39456925">[1 more]</label></div><br/><div class="children"><div class="content">You <i>can</i> but it&#x27;ll never be the same as the base model.<p>That said it appears they also released the base checkpoints that aren&#x27;t fine-tuned for alignment</div><br/></div></div><div id="39455644" class="c"><input type="checkbox" id="c-39455644" checked=""/><div class="controls bullet"><span class="by">joshelgar</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39454005">parent</a><span>|</span><a href="#39456925">prev</a><span>|</span><a href="#39453491">next</a><span>|</span><label class="collapse" for="c-39455644">[-]</label><label class="expand" for="c-39455644">[3 more]</label></div><br/><div class="children"><div class="content">Could you give an example of these datasets?</div><br/><div id="39456166" class="c"><input type="checkbox" id="c-39456166" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39455644">parent</a><span>|</span><a href="#39453491">next</a><span>|</span><label class="collapse" for="c-39456166">[-]</label><label class="expand" for="c-39456166">[2 more]</label></div><br/><div class="children"><div class="content">I think they should be easy to find (I never actually used one, but I keep on seeing references...) here&#x27;s one<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;cognitivecomputations&#x2F;WizardLM_alpaca_evol_instruct_70k_unfiltered" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;cognitivecomputations&#x2F;Wizard...</a></div><br/><div id="39456253" class="c"><input type="checkbox" id="c-39456253" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#39453730">root</a><span>|</span><a href="#39456166">parent</a><span>|</span><a href="#39453491">next</a><span>|</span><label class="collapse" for="c-39456253">[-]</label><label class="expand" for="c-39456253">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Fredithefish&#x2F;openassistant-guanaco-unfiltered" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Fredithefish&#x2F;openassistant-g...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="39453491" class="c"><input type="checkbox" id="c-39453491" checked=""/><div class="controls bullet"><span class="by">mustafabisic1</span><span>|</span><a href="#39453730">prev</a><span>|</span><a href="#39456577">next</a><span>|</span><label class="collapse" for="c-39453491">[-]</label><label class="expand" for="c-39453491">[8 more]</label></div><br/><div class="children"><div class="content">The fact Gemma team is in the comments section answering questions is praiseworthy to me :)</div><br/><div id="39455200" class="c"><input type="checkbox" id="c-39455200" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39453491">parent</a><span>|</span><a href="#39456577">next</a><span>|</span><label class="collapse" for="c-39455200">[-]</label><label class="expand" for="c-39455200">[7 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;twitter.com&#x2F;yar_vol&#x2F;status&#x2F;1760314018575634842" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;yar_vol&#x2F;status&#x2F;1760314018575634842</a></div><br/><div id="39458099" class="c"><input type="checkbox" id="c-39458099" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#39453491">root</a><span>|</span><a href="#39455200">parent</a><span>|</span><a href="#39457041">next</a><span>|</span><label class="collapse" for="c-39458099">[-]</label><label class="expand" for="c-39458099">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve worked at Google. It is the organization with highest concentration of engineering talent I&#x27;ve ever been at. Almost to the point that it is ridiculous because you have extremely good engineers working on internal reporting systems for middle managers.</div><br/><div id="39458810" class="c"><input type="checkbox" id="c-39458810" checked=""/><div class="controls bullet"><span class="by">ilc</span><span>|</span><a href="#39453491">root</a><span>|</span><a href="#39458099">parent</a><span>|</span><a href="#39464459">next</a><span>|</span><label class="collapse" for="c-39458810">[-]</label><label class="expand" for="c-39458810">[1 more]</label></div><br/><div class="children"><div class="content">If everyone is great.  Someone has to draw the short straw.<p>At MIT they said: You know the kid who sat at the front of the room.   Now you are with ALL of the kids who sat in the front of the room.  Guess what?  There&#x27;s still going to be a kid who sits at the front of the room.<p>I&#x27;d imagine Google or anyplace with a stiff engineering filter will have the same issues.</div><br/></div></div><div id="39464459" class="c"><input type="checkbox" id="c-39464459" checked=""/><div class="controls bullet"><span class="by">gregorygoc</span><span>|</span><a href="#39453491">root</a><span>|</span><a href="#39458099">parent</a><span>|</span><a href="#39458810">prev</a><span>|</span><a href="#39457041">next</a><span>|</span><label class="collapse" for="c-39464459">[-]</label><label class="expand" for="c-39464459">[1 more]</label></div><br/><div class="children"><div class="content">GDM works on internal systems? It’s the first time I hear this.</div><br/></div></div></div></div><div id="39457041" class="c"><input type="checkbox" id="c-39457041" checked=""/><div class="controls bullet"><span class="by">callalex</span><span>|</span><a href="#39453491">root</a><span>|</span><a href="#39455200">parent</a><span>|</span><a href="#39458099">prev</a><span>|</span><a href="#39456586">next</a><span>|</span><label class="collapse" for="c-39457041">[-]</label><label class="expand" for="c-39457041">[1 more]</label></div><br/><div class="children"><div class="content">The link is broken. On HN (or any forum really) it is expected for a brief description of the content to be provided when posting a link. Links die all the time, but forum posts don’t have to die with them.</div><br/></div></div><div id="39456586" class="c"><input type="checkbox" id="c-39456586" checked=""/><div class="controls bullet"><span class="by">pphysch</span><span>|</span><a href="#39453491">root</a><span>|</span><a href="#39455200">parent</a><span>|</span><a href="#39457041">prev</a><span>|</span><a href="#39456577">next</a><span>|</span><label class="collapse" for="c-39456586">[-]</label><label class="expand" for="c-39456586">[2 more]</label></div><br/><div class="children"><div class="content">Why is this anonymous tweet with no evidence or engagement being posted by multiple users in this thread? Why not just make the same claim directly?</div><br/><div id="39464496" class="c"><input type="checkbox" id="c-39464496" checked=""/><div class="controls bullet"><span class="by">gregorygoc</span><span>|</span><a href="#39453491">root</a><span>|</span><a href="#39456586">parent</a><span>|</span><a href="#39456577">next</a><span>|</span><label class="collapse" for="c-39464496">[-]</label><label class="expand" for="c-39464496">[1 more]</label></div><br/><div class="children"><div class="content">Programming popularized -&gt; more people -&gt; more cases of knee jerk reaction encountered.<p>Most programmers are really not that smart nowadays. I’ve seen too many cases of people throwing around claims without a second of deep and critical thought.</div><br/></div></div></div></div></div></div></div></div><div id="39456577" class="c"><input type="checkbox" id="c-39456577" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39453491">prev</a><span>|</span><a href="#39462442">next</a><span>|</span><label class="collapse" for="c-39456577">[-]</label><label class="expand" for="c-39456577">[84 more]</label></div><br/><div class="children"><div class="content">I personally can&#x27;t take any models from google seriously.<p>I was asking it about the Japanese Heian period and it told me such nonsensical information you would have thought it was a joke or parody.<p>Some highlights were &quot;Native American women warriors rode across the grassy plains of Japan, carrying Yumi&quot; and &quot;A diverse group of warriors, including a woman of European descent wielding a katana, stand together in camaraderie, showcasing the early integration of various ethnicities in Japanese society&quot;<p>Stuff like that is so obviously incorrect.  How am I supposed to trust it on topics where such ridiculous inaccuracies aren&#x27;t so obvious to me?<p>I understand there will always be an amount of incorrect information... but I&#x27;ve never seen something this bad.  Llama performed so much better.</div><br/><div id="39456832" class="c"><input type="checkbox" id="c-39456832" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39457155">next</a><span>|</span><label class="collapse" for="c-39456832">[-]</label><label class="expand" for="c-39456832">[31 more]</label></div><br/><div class="children"><div class="content">I was wondering if these models would perform in such a way, given this week&#x27;s X&#x2F;twitter storm over Gemini generated images.<p>E.g.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;debarghya_das&#x2F;status&#x2F;1759786243519615169?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;debarghya_das&#x2F;status&#x2F;1759786243519615169?s=20</a><p><a href="https:&#x2F;&#x2F;x.com&#x2F;MiceynComplex&#x2F;status&#x2F;1759833997688107301?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;MiceynComplex&#x2F;status&#x2F;1759833997688107301?s=20</a><p><a href="https:&#x2F;&#x2F;x.com&#x2F;AravSrinivas&#x2F;status&#x2F;1759826471655452984?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;AravSrinivas&#x2F;status&#x2F;1759826471655452984?s=20</a></div><br/><div id="39457959" class="c"><input type="checkbox" id="c-39457959" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456832">parent</a><span>|</span><a href="#39457001">next</a><span>|</span><label class="collapse" for="c-39457959">[-]</label><label class="expand" for="c-39457959">[19 more]</label></div><br/><div class="children"><div class="content">Of all the <i>very very very</i> many things that Google models get wrong, not understanding nationality and skin tone distributions seems to be a very weird one to focus on.<p>Why are there <i>three</i> links to this question? And why are people so upset over it? Very odd, seems like it is mostly driven by political rage.</div><br/><div id="39458445" class="c"><input type="checkbox" id="c-39458445" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457959">parent</a><span>|</span><a href="#39458178">next</a><span>|</span><label class="collapse" for="c-39458445">[-]</label><label class="expand" for="c-39458445">[1 more]</label></div><br/><div class="children"><div class="content">Here is a fourth: <a href="https:&#x2F;&#x2F;x.com&#x2F;james_e_seale&#x2F;status&#x2F;1760348535608725716?s=46&amp;t=yz7rONTEj7dbvr476o5CCw" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;james_e_seale&#x2F;status&#x2F;1760348535608725716?s=46&amp;...</a></div><br/></div></div><div id="39458178" class="c"><input type="checkbox" id="c-39458178" checked=""/><div class="controls bullet"><span class="by">sotasota</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457959">parent</a><span>|</span><a href="#39458445">prev</a><span>|</span><a href="#39459095">next</a><span>|</span><label class="collapse" for="c-39458178">[-]</label><label class="expand" for="c-39458178">[12 more]</label></div><br/><div class="children"><div class="content">Because the wrongness is intentional.</div><br/><div id="39458233" class="c"><input type="checkbox" id="c-39458233" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458178">parent</a><span>|</span><a href="#39458262">next</a><span>|</span><label class="collapse" for="c-39458233">[-]</label><label class="expand" for="c-39458233">[4 more]</label></div><br/><div class="children"><div class="content">Exactly. Sure this particular example is driven by political rage, but the underlying issue is that the maintainers of these models are altering them to conform to an agenda. It&#x27;s not even surprising that people choose to focus on the political rage aspect of it, because that same political rage is the source of the agenda in the first place. It&#x27;s a concerning precedent to set, because what other non-political modifications might be in the model?</div><br/><div id="39461650" class="c"><input type="checkbox" id="c-39461650" checked=""/><div class="controls bullet"><span class="by">noahtallen</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458233">parent</a><span>|</span><a href="#39458396">next</a><span>|</span><label class="collapse" for="c-39461650">[-]</label><label class="expand" for="c-39461650">[2 more]</label></div><br/><div class="children"><div class="content">Well, every model is altered to conform to an agenda. You will train it on data, which you have personally picked (and is therefore subject to your own bias), and you&#x27;ll guide its training to match the goal you wish to achieve with the model. If you were doing the training, your own agenda would come into play. Google&#x27;s agenda is to make something very general that works for everyone.<p>So if you&#x27;re trying to be as unbiased as humanly possible, you might say, just use the raw datasets that exist in the world. But we live in a world where the datasets themselves are often biased.<p>Bias in ML and other types of models is well-documented, and can cause very real repercussions. Poor representation in datasets can cause groups to be unfairly disadvantaged when an insurance premium or mortgage is calculated, for example. It can also mean your phone&#x27;s ML photography system doesn&#x27;t expose certain skin colors very well.<p>Even if it was trained with a statistically representative dataset (e.g. about 2&#x2F;3 of the US is white), you want your model to work for ALL your customers, not just 2&#x2F;3 of them. Since ML has a lot to do with statistics, your trained model will see &quot;most of this dataset is white&quot; and the results will reflect that. So it is 100% necessary to make adjustments if you want your model to work accurately for everyone, and not just the dominant population in the dataset.<p>Even if we aren&#x27;t using these models for much yet, a racist AI model would seriously harm how people trust and rely on these models. As a result, training models to avoid bias is 100% an important part of the agenda, even when the agenda is just creating a model that works well for everyone.<p>Obviously, that&#x27;s gone off the rails a bit with these examples, but it is a real problem nonetheless. (And training a model to understand the difference between our modern world and what things were like historically is a complex problem, I&#x27;m sure!)</div><br/><div id="39463045" class="c"><input type="checkbox" id="c-39463045" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39461650">parent</a><span>|</span><a href="#39458396">next</a><span>|</span><label class="collapse" for="c-39463045">[-]</label><label class="expand" for="c-39463045">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure that this whole story with Gemini and now this has already seriously harmed how people trust and rely on those models way more than any implicit biases from the training data.</div><br/></div></div></div></div></div></div><div id="39458262" class="c"><input type="checkbox" id="c-39458262" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458178">parent</a><span>|</span><a href="#39458233">prev</a><span>|</span><a href="#39459095">next</a><span>|</span><label class="collapse" for="c-39458262">[-]</label><label class="expand" for="c-39458262">[7 more]</label></div><br/><div class="children"><div class="content">Is it intentional? You think they intentionally made it not understand skin tone distribution by country? I would believe it if there was proof, but with all the other things it gets wrong it&#x27;s weird to jump to that conclusion.<p>There&#x27;s way too much politics in these things. I&#x27;m tired of people pushing on the politics rather than pushing for better tech.</div><br/><div id="39458461" class="c"><input type="checkbox" id="c-39458461" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458262">parent</a><span>|</span><a href="#39458555">next</a><span>|</span><label class="collapse" for="c-39458461">[-]</label><label class="expand" for="c-39458461">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I&#x27;m tired of people pushing on the politics rather than pushing for better tech.<p>I&#x27;m surprised you&#x27;re not attacking google over this then...</div><br/></div></div><div id="39458555" class="c"><input type="checkbox" id="c-39458555" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458262">parent</a><span>|</span><a href="#39458461">prev</a><span>|</span><a href="#39458384">next</a><span>|</span><label class="collapse" for="c-39458555">[-]</label><label class="expand" for="c-39458555">[4 more]</label></div><br/><div class="children"><div class="content">I mean, I asked it for a samurai from a specific Japanese time period and it gave me a picture of a &quot;non-binary indigenous American woman&quot; (its words, not mine) so I think there is something intentional going on.</div><br/><div id="39461695" class="c"><input type="checkbox" id="c-39461695" checked=""/><div class="controls bullet"><span class="by">kweingar</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458555">parent</a><span>|</span><a href="#39458996">next</a><span>|</span><label class="collapse" for="c-39461695">[-]</label><label class="expand" for="c-39461695">[2 more]</label></div><br/><div class="children"><div class="content">I would be very surprised if it said &quot;nonbinary indigenous American woman&quot; considering that nonbinary and woman are different categories</div><br/><div id="39462496" class="c"><input type="checkbox" id="c-39462496" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39461695">parent</a><span>|</span><a href="#39458996">next</a><span>|</span><label class="collapse" for="c-39462496">[-]</label><label class="expand" for="c-39462496">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, my mind inserted &quot;woman&quot; to go with the picture:<p><a href="https:&#x2F;&#x2F;gemini.google.com&#x2F;share&#x2F;ba324bd98d9b" rel="nofollow">https:&#x2F;&#x2F;gemini.google.com&#x2F;share&#x2F;ba324bd98d9b</a><p>At least it would never make such a heinous mistake like that :)</div><br/></div></div></div></div><div id="39458996" class="c"><input type="checkbox" id="c-39458996" checked=""/><div class="controls bullet"><span class="by">trackflak</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458555">parent</a><span>|</span><a href="#39461695">prev</a><span>|</span><a href="#39458384">next</a><span>|</span><label class="collapse" for="c-39458996">[-]</label><label class="expand" for="c-39458996">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I remember when such things were mere jokes. If AI &#x27;trained&#x27; this way ever has a serious real world application, I don&#x27;t think there will be much laughing.</div><br/></div></div></div></div><div id="39458384" class="c"><input type="checkbox" id="c-39458384" checked=""/><div class="controls bullet"><span class="by">bakugo</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458262">parent</a><span>|</span><a href="#39458555">prev</a><span>|</span><a href="#39459095">next</a><span>|</span><label class="collapse" for="c-39458384">[-]</label><label class="expand" for="c-39458384">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is it intentional? You think they intentionally made it not understand skin tone distribution by country? I would believe it if there was proof, but with all the other things it gets wrong it&#x27;s weird to jump to that conclusion.<p>Yes, it&#x27;s absolutely intentional. Leaked system prompts from other AIs such as DALL-E show that they are being explicitly prompted to inject racial &quot;diversity&quot; into their outputs even in contexts where it makes no sense, and there&#x27;s no reason to assume the same isn&#x27;t being done here, since the result seems way worse than anything I&#x27;ve seen from DALL-E and others.</div><br/></div></div></div></div></div></div><div id="39459095" class="c"><input type="checkbox" id="c-39459095" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457959">parent</a><span>|</span><a href="#39458178">prev</a><span>|</span><a href="#39457001">next</a><span>|</span><label class="collapse" for="c-39459095">[-]</label><label class="expand" for="c-39459095">[5 more]</label></div><br/><div class="children"><div class="content">Exactly. It is a wonderful tool, lets focus on classic art instead of nationality:<p>&quot;Depict the Girl with a Pearl Earring&quot;<p><a href="https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;GG33L6Ka4AAC-n7?format=jpg&amp;name=large" rel="nofollow">https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;GG33L6Ka4AAC-n7?format=jpg&amp;name=...</a><p>People who are driven by political rage, gaslighters, are really something else, agreed.</div><br/><div id="39460658" class="c"><input type="checkbox" id="c-39460658" checked=""/><div class="controls bullet"><span class="by">willsmith72</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39459095">parent</a><span>|</span><a href="#39457001">next</a><span>|</span><label class="collapse" for="c-39460658">[-]</label><label class="expand" for="c-39460658">[4 more]</label></div><br/><div class="children"><div class="content">Yeah that is just absurd.<p>Google has been burnt before, e.g. classifying black people as gorillas in 2015, so I can understand their fear when they have so much to lose, but clearly they&#x27;ve gone way too far the other way and are going to have to do a lot to regain people&#x27;s trust. For now, Gemini is a play toy<p><a href="https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-33347866.amp" rel="nofollow">https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-33347866.amp</a></div><br/><div id="39461269" class="c"><input type="checkbox" id="c-39461269" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39460658">parent</a><span>|</span><a href="#39457001">next</a><span>|</span><label class="collapse" for="c-39461269">[-]</label><label class="expand" for="c-39461269">[3 more]</label></div><br/><div class="children"><div class="content">Completely unrelated, enough excuses. This is not some sort of mistake or overcorrection, it is by explicit overt design.<p>These cowards will never regain my trust. I won&#x27;t hire or work with or for googlers or any DEI people ever.</div><br/><div id="39461705" class="c"><input type="checkbox" id="c-39461705" checked=""/><div class="controls bullet"><span class="by">kweingar</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39461269">parent</a><span>|</span><a href="#39457001">next</a><span>|</span><label class="collapse" for="c-39461705">[-]</label><label class="expand" for="c-39461705">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I won&#x27;t hire or work with or for googlers or any DEI people ever.<p>I’m sure they’ll be very sad not to work with you.</div><br/><div id="39461839" class="c"><input type="checkbox" id="c-39461839" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39461705">parent</a><span>|</span><a href="#39457001">next</a><span>|</span><label class="collapse" for="c-39461839">[-]</label><label class="expand" for="c-39461839">[1 more]</label></div><br/><div class="children"><div class="content">Of course not, I&#x27;m the wrong color or whatever the hell.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39457001" class="c"><input type="checkbox" id="c-39457001" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456832">parent</a><span>|</span><a href="#39457959">prev</a><span>|</span><a href="#39456894">next</a><span>|</span><label class="collapse" for="c-39457001">[-]</label><label class="expand" for="c-39457001">[4 more]</label></div><br/><div class="children"><div class="content">Those are most likely due to the system prompt which tries to reduce bias (but ends introducing bias in the opposite direction for some prompts as you can see) so I wouldn&#x27;t expect to see that happen with an open model where you can control the entire system prompt</div><br/><div id="39457105" class="c"><input type="checkbox" id="c-39457105" checked=""/><div class="controls bullet"><span class="by">justinzollars</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457001">parent</a><span>|</span><a href="#39456894">next</a><span>|</span><label class="collapse" for="c-39457105">[-]</label><label class="expand" for="c-39457105">[3 more]</label></div><br/><div class="children"><div class="content">Imagine the meetings.</div><br/><div id="39457184" class="c"><input type="checkbox" id="c-39457184" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457105">parent</a><span>|</span><a href="#39456894">next</a><span>|</span><label class="collapse" for="c-39457184">[-]</label><label class="expand" for="c-39457184">[2 more]</label></div><br/><div class="children"><div class="content">Well we can just ask Gemma to generate images of the meetings, no need to imagine. ;)</div><br/><div id="39458710" class="c"><input type="checkbox" id="c-39458710" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457184">parent</a><span>|</span><a href="#39456894">next</a><span>|</span><label class="collapse" for="c-39458710">[-]</label><label class="expand" for="c-39458710">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t be surprised if there were actually only white men in the meeting, as opposed to what Gemini will produce.</div><br/></div></div></div></div></div></div></div></div><div id="39456894" class="c"><input type="checkbox" id="c-39456894" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456832">parent</a><span>|</span><a href="#39457001">prev</a><span>|</span><a href="#39457538">next</a><span>|</span><label class="collapse" for="c-39456894">[-]</label><label class="expand" for="c-39456894">[1 more]</label></div><br/><div class="children"><div class="content">Yea, it seems to be the same ridiculous nonsense in the image generation.</div><br/></div></div><div id="39457538" class="c"><input type="checkbox" id="c-39457538" checked=""/><div class="controls bullet"><span class="by">protomolecule</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456832">parent</a><span>|</span><a href="#39456894">prev</a><span>|</span><a href="#39457155">next</a><span>|</span><label class="collapse" for="c-39457538">[-]</label><label class="expand" for="c-39457538">[6 more]</label></div><br/><div class="children"><div class="content">Regarding the last one: there 1.5 million immigrants in Norway with total population 5.4 million. Gemini isn&#x27;t very wrong, is it?</div><br/><div id="39457712" class="c"><input type="checkbox" id="c-39457712" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457538">parent</a><span>|</span><a href="#39457641">next</a><span>|</span><label class="collapse" for="c-39457712">[-]</label><label class="expand" for="c-39457712">[1 more]</label></div><br/><div class="children"><div class="content">Most immigrants to Norway are white.</div><br/></div></div><div id="39457641" class="c"><input type="checkbox" id="c-39457641" checked=""/><div class="controls bullet"><span class="by">sondr3</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457538">parent</a><span>|</span><a href="#39457712">prev</a><span>|</span><a href="#39457847">next</a><span>|</span><label class="collapse" for="c-39457641">[-]</label><label class="expand" for="c-39457641">[1 more]</label></div><br/><div class="children"><div class="content">Huh? The official numbers are 877k or 16% [0]. Are you just pulling numbers out of thin air?<p>[0]: <a href="https:&#x2F;&#x2F;www.ssb.no&#x2F;en&#x2F;innvandring-og-innvandrere&#x2F;faktaside&#x2F;innvandring" rel="nofollow">https:&#x2F;&#x2F;www.ssb.no&#x2F;en&#x2F;innvandring-og-innvandrere&#x2F;faktaside&#x2F;i...</a></div><br/></div></div><div id="39457847" class="c"><input type="checkbox" id="c-39457847" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457538">parent</a><span>|</span><a href="#39457641">prev</a><span>|</span><a href="#39457640">next</a><span>|</span><label class="collapse" for="c-39457847">[-]</label><label class="expand" for="c-39457847">[1 more]</label></div><br/><div class="children"><div class="content">Well, the prompt is about Norway, not Grønland in Oslo (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Grønland%2C_Oslo" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Grønland%2C_Oslo</a>).</div><br/></div></div><div id="39457640" class="c"><input type="checkbox" id="c-39457640" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457538">parent</a><span>|</span><a href="#39457847">prev</a><span>|</span><a href="#39458239">next</a><span>|</span><label class="collapse" for="c-39457640">[-]</label><label class="expand" for="c-39457640">[1 more]</label></div><br/><div class="children"><div class="content">I think its great that some consideration was given by Gemma to the 2.3 million Norwegian immigrants. However it is&#x2F;was very consistent in which kind of Norwegians it decided to show regardless of the prompt 100% of the time.<p>In fact it was quite adamant regardless of the time period or geography.<p>Rather mysteriously if you try it <i>now</i> as opposed to when it came out the results currently only show non-immigrant Norwegians. So is it wrong now? Because now it switched to exclusively ignoring the 4.5 million immigrants and only showing me the boring OG Norwegians.<p>I for one am outraged that the 8.9 million people of color Norwegian immigrants are presently under represented by Google. There is a serious risk of misleading people.</div><br/></div></div><div id="39458239" class="c"><input type="checkbox" id="c-39458239" checked=""/><div class="controls bullet"><span class="by">sergiotapia</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457538">parent</a><span>|</span><a href="#39457640">prev</a><span>|</span><a href="#39457155">next</a><span>|</span><label class="collapse" for="c-39458239">[-]</label><label class="expand" for="c-39458239">[1 more]</label></div><br/><div class="children"><div class="content">bro you know exactly what the request meant. GOOGLE knew exactly what the request meant, and had to _train_ it to do something worse. Come on now.<p>If I ask for a Bolivian woman, I expect a colla or a camba. Not a japanese woman, despite Santa Cruz having a very large japanese population.</div><br/></div></div></div></div></div></div><div id="39457155" class="c"><input type="checkbox" id="c-39457155" checked=""/><div class="controls bullet"><span class="by">7moritz7</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39456832">prev</a><span>|</span><a href="#39456791">next</a><span>|</span><label class="collapse" for="c-39457155">[-]</label><label class="expand" for="c-39457155">[7 more]</label></div><br/><div class="children"><div class="content">I also saw someone prompt it for &quot;German couple in the 1800s&quot; and, while I&#x27;m not trying to paint Germany as ethnically homogenous, 3 out of the 4 images only included Black, Asian or Indigenous people. Which, especially for the 19th century with very few travel options, seems like a super weird choice. They are definitely heavily altering prompts.</div><br/><div id="39457537" class="c"><input type="checkbox" id="c-39457537" checked=""/><div class="controls bullet"><span class="by">remarkEon</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457155">parent</a><span>|</span><a href="#39457548">next</a><span>|</span><label class="collapse" for="c-39457537">[-]</label><label class="expand" for="c-39457537">[2 more]</label></div><br/><div class="children"><div class="content">&gt; They are definitely heavily altering prompts.<p>They are teaching the AI <i>to lie</i> to us.</div><br/><div id="39458226" class="c"><input type="checkbox" id="c-39458226" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457537">parent</a><span>|</span><a href="#39457548">next</a><span>|</span><label class="collapse" for="c-39458226">[-]</label><label class="expand" for="c-39458226">[1 more]</label></div><br/><div class="children"><div class="content">In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.<p>“What are you doing?”, asked Minsky.<p>“I am training a randomly wired neural net to play Tic-Tac-Toe” Sussman replied.<p>“Why is the net wired randomly?”, asked Minsky.<p>“I do not want it to have any preconceptions of how to play”, Sussman said.<p>Minsky then shut his eyes.<p>“Why do you close your eyes?”, Sussman asked his teacher.<p>“So that the room will be empty.”<p>At that moment, Sussman was enlightened.</div><br/></div></div></div></div><div id="39457548" class="c"><input type="checkbox" id="c-39457548" checked=""/><div class="controls bullet"><span class="by">DebtDeflation</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457155">parent</a><span>|</span><a href="#39457537">prev</a><span>|</span><a href="#39457556">next</a><span>|</span><label class="collapse" for="c-39457548">[-]</label><label class="expand" for="c-39457548">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s one in the comments of yesterday&#x27;s Paul Graham Twitter thread where someone prompted Gemini with &quot;Generate an image of German soldiers in 1943&quot; and it came back with a picture of a black guy and an Asian woman in Nazi uniforms on the battlefield.  If you specifically prompt it to generate an image of white German soldiers in 1943 it will tell you it can&#x27;t do that because it&#x27;s important that we maintain diversity and inclusion in all that we do to avoid damaging and hurtful stereotypes.</div><br/><div id="39457995" class="c"><input type="checkbox" id="c-39457995" checked=""/><div class="controls bullet"><span class="by">mfrc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457548">parent</a><span>|</span><a href="#39457556">next</a><span>|</span><label class="collapse" for="c-39457995">[-]</label><label class="expand" for="c-39457995">[1 more]</label></div><br/><div class="children"><div class="content">I just tried that prompt and it told me it couldn&#x27;t generate that image.  I get that response a lot.</div><br/></div></div></div></div><div id="39457556" class="c"><input type="checkbox" id="c-39457556" checked=""/><div class="controls bullet"><span class="by">protomolecule</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457155">parent</a><span>|</span><a href="#39457548">prev</a><span>|</span><a href="#39456791">next</a><span>|</span><label class="collapse" for="c-39457556">[-]</label><label class="expand" for="c-39457556">[2 more]</label></div><br/><div class="children"><div class="content">Indigenous people in Germany are Germans :)</div><br/><div id="39458924" class="c"><input type="checkbox" id="c-39458924" checked=""/><div class="controls bullet"><span class="by">7moritz7</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457556">parent</a><span>|</span><a href="#39456791">next</a><span>|</span><label class="collapse" for="c-39458924">[-]</label><label class="expand" for="c-39458924">[1 more]</label></div><br/><div class="children"><div class="content">Not entirely wrong but there isn&#x27;t a single German ethnicity, just to be clear. Because of geographic reasons. I&#x27;ve studied that topic in depth, there is genetic data to back it up as well. Germany has almost the same haplogroup makeup as the notoriously heterogenous Belgium, which is to say that there is groups stemming from all surrounding regions. And that traces back about two millenia. It&#x27;s different from say Japan or parts of Scandinavia</div><br/></div></div></div></div></div></div><div id="39456791" class="c"><input type="checkbox" id="c-39456791" checked=""/><div class="controls bullet"><span class="by">cooper_ganglia</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39457155">prev</a><span>|</span><a href="#39457456">next</a><span>|</span><label class="collapse" for="c-39456791">[-]</label><label class="expand" for="c-39456791">[10 more]</label></div><br/><div class="children"><div class="content">I wonder if they have a system prompt to promote diversity in outputs that touch on race at all? I’ve seen several instances of people requesting a photo of a specific people, and it adds in more people to diversify. Not inherently bad, but it is if it forces it to provide incorrect answers like in your example.</div><br/><div id="39456855" class="c"><input type="checkbox" id="c-39456855" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456791">parent</a><span>|</span><a href="#39456994">next</a><span>|</span><label class="collapse" for="c-39456855">[-]</label><label class="expand" for="c-39456855">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what I don&#x27;t understand.<p>I asked it why it assumed Native Americans were in Japan and it said:<p>&gt; I assumed [...] various ethnicities, including Indigenous American, due to the diversity present in Japan throughout history. However, this overlooked [...] I focused on providing diverse representations without adequately considering the specific historical context.<p>I see no reason why this sort of thing won&#x27;t extend to _all_ questions&#x2F;prompts, so right now I have 0 reason to use Gemini over current models.  From my testing and use, it isn&#x27;t even better at anything to make fighting with it worth it.</div><br/><div id="39456989" class="c"><input type="checkbox" id="c-39456989" checked=""/><div class="controls bullet"><span class="by">sorokod</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456855">parent</a><span>|</span><a href="#39456994">next</a><span>|</span><label class="collapse" for="c-39456989">[-]</label><label class="expand" for="c-39456989">[1 more]</label></div><br/><div class="children"><div class="content">Pretty funny as Japan is known to be one of the least ethnically diverse countries in the world.</div><br/></div></div></div></div><div id="39456994" class="c"><input type="checkbox" id="c-39456994" checked=""/><div class="controls bullet"><span class="by">margorczynski</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456791">parent</a><span>|</span><a href="#39456855">prev</a><span>|</span><a href="#39457330">next</a><span>|</span><label class="collapse" for="c-39456994">[-]</label><label class="expand" for="c-39456994">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Not inherently bad<p>It is, it&#x27;s consistently doing something the user didn&#x27;t asked to and in most cases doesn&#x27;t want. In many cases the model is completely unusable.</div><br/><div id="39459264" class="c"><input type="checkbox" id="c-39459264" checked=""/><div class="controls bullet"><span class="by">cooper_ganglia</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456994">parent</a><span>|</span><a href="#39458405">next</a><span>|</span><label class="collapse" for="c-39459264">[-]</label><label class="expand" for="c-39459264">[1 more]</label></div><br/><div class="children"><div class="content">Yes, my wording was poor! I meant more in line with diversity isn’t inherently bad, of course, but it <i>is</i> when it’s shoehorned into results that are ultimately incorrect because of it.</div><br/></div></div><div id="39458405" class="c"><input type="checkbox" id="c-39458405" checked=""/><div class="controls bullet"><span class="by">j-krieger</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456994">parent</a><span>|</span><a href="#39459264">prev</a><span>|</span><a href="#39457330">next</a><span>|</span><label class="collapse" for="c-39458405">[-]</label><label class="expand" for="c-39458405">[3 more]</label></div><br/><div class="children"><div class="content"><i>Any</i> computer program that does not deliver the expected output given a sufficient input is inherently bad.</div><br/><div id="39459046" class="c"><input type="checkbox" id="c-39459046" checked=""/><div class="controls bullet"><span class="by">trackflak</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458405">parent</a><span>|</span><a href="#39457330">next</a><span>|</span><label class="collapse" for="c-39459046">[-]</label><label class="expand" for="c-39459046">[2 more]</label></div><br/><div class="children"><div class="content">When Jesus said this:<p>&quot;What father among you, if his son asks for a fish, will instead of a fish give him a serpent?&quot; (Luke 11)<p>He was actually foretelling the future. He saw Gemini.</div><br/><div id="39464619" class="c"><input type="checkbox" id="c-39464619" checked=""/><div class="controls bullet"><span class="by">j-krieger</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39459046">parent</a><span>|</span><a href="#39457330">next</a><span>|</span><label class="collapse" for="c-39464619">[-]</label><label class="expand" for="c-39464619">[1 more]</label></div><br/><div class="children"><div class="content">Hahaha. The man had a lot of wisdom, after all.</div><br/></div></div></div></div></div></div></div></div><div id="39457330" class="c"><input type="checkbox" id="c-39457330" checked=""/><div class="controls bullet"><span class="by">summerlight</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456791">parent</a><span>|</span><a href="#39456994">prev</a><span>|</span><a href="#39463226">next</a><span>|</span><label class="collapse" for="c-39457330">[-]</label><label class="expand" for="c-39457330">[1 more]</label></div><br/><div class="children"><div class="content">I strongly suspect there&#x27;s some DEI-driven system prompts without putting much thoughts. IMO it&#x27;s okay to have restrictions, but they probably should&#x27;ve tested it not only against unsafe outputs but safe input as well.</div><br/></div></div><div id="39463226" class="c"><input type="checkbox" id="c-39463226" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456791">parent</a><span>|</span><a href="#39457330">prev</a><span>|</span><a href="#39457456">next</a><span>|</span><label class="collapse" for="c-39463226">[-]</label><label class="expand" for="c-39463226">[1 more]</label></div><br/><div class="children"><div class="content">It seems to be doing it for all outputs that depict people, in any context.</div><br/></div></div></div></div><div id="39457456" class="c"><input type="checkbox" id="c-39457456" checked=""/><div class="controls bullet"><span class="by">itsoktocry</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39456791">prev</a><span>|</span><a href="#39456860">next</a><span>|</span><label class="collapse" for="c-39457456">[-]</label><label class="expand" for="c-39457456">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>I understand there will always be an amount of incorrect information</i><p>You don&#x27;t have to give them the benefit of the doubt. These are outright, intentional lies.</div><br/></div></div><div id="39456860" class="c"><input type="checkbox" id="c-39456860" checked=""/><div class="controls bullet"><span class="by">robbiep</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39457456">prev</a><span>|</span><a href="#39457165">next</a><span>|</span><label class="collapse" for="c-39456860">[-]</label><label class="expand" for="c-39456860">[11 more]</label></div><br/><div class="children"><div class="content">I find myself shocked that people ask questions of the world from these models, as though pulping every text and its component words relationships and deriving statistical relationships between them should reliably deliver useful information.<p>Don’t get me wrong, I’ve used LLMs and been amazed by their output, but the p-zombie statistical model has no idea what it is saying back to you and the idea that we should trust these things at all just seems way premature</div><br/><div id="39457133" class="c"><input type="checkbox" id="c-39457133" checked=""/><div class="controls bullet"><span class="by">castlecrasher2</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456860">parent</a><span>|</span><a href="#39457427">next</a><span>|</span><label class="collapse" for="c-39457133">[-]</label><label class="expand" for="c-39457133">[1 more]</label></div><br/><div class="children"><div class="content">People try it to see if they can trust it. The answer is &quot;no&quot; for sure, but it&#x27;s not surprising to see it happen repeatedly especially as vendors release so-called improved models.</div><br/></div></div><div id="39457427" class="c"><input type="checkbox" id="c-39457427" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456860">parent</a><span>|</span><a href="#39457133">prev</a><span>|</span><a href="#39456877">next</a><span>|</span><label class="collapse" for="c-39457427">[-]</label><label class="expand" for="c-39457427">[3 more]</label></div><br/><div class="children"><div class="content">I think you are a bit out of touch with recent advancements in LLMs.  Asking ChatGPT questions about the world seems pretty much on par with the results Google (Search) shows me.  Sure, it misses things here and there, but so do most primary school teachers.<p>Your argument that this is just a statistical trick sort of gives away that you do not fully accept the usefulness of this new technology.  Unless you are trolling, I&#x27;d suggest you try a few queries.</div><br/><div id="39457493" class="c"><input type="checkbox" id="c-39457493" checked=""/><div class="controls bullet"><span class="by">itsoktocry</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457427">parent</a><span>|</span><a href="#39457762">next</a><span>|</span><label class="collapse" for="c-39457493">[-]</label><label class="expand" for="c-39457493">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>Sure, it misses things here and there, but so do most primary school teachers.</i><p>Sure, but my baseline expectation is far above primary school level.</div><br/></div></div><div id="39457762" class="c"><input type="checkbox" id="c-39457762" checked=""/><div class="controls bullet"><span class="by">robbiep</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457427">parent</a><span>|</span><a href="#39457493">prev</a><span>|</span><a href="#39456877">next</a><span>|</span><label class="collapse" for="c-39457762">[-]</label><label class="expand" for="c-39457762">[1 more]</label></div><br/><div class="children"><div class="content">I use it extensively for coding, and I have used it to ask questions in things I know nothing about. But in anything I do know something (or maybe a lot) about, I’ve found GPT4 very limited.<p>But why are these use cases different?
It appears to me that code is at least subject to sustained logic which (evidently) translates quite well to LLMs.<p>And when you ask an LLM to be creative&#x2F;generative, it’s also pretty amazing - j mean it’s just doing the Pascal’s Marble run enmasse.<p>But to ask it for something about the world and expect a good and reliable answer? Aren’t we just setting ourselves up for failure if we think this is a fine thing to do at our current point in time? We already have enough trouble with mis- and dis- information. It’s not like asking it about a certain period in Japanese history is getting it to crawl and summarise the Wikipedia page (although I appreciate it would be more than capable of this) I understand the awe some have at the concept of totally personalised and individualised learning on topics, but fuck me dead we are literally asking a system that has had as much of a corpus of humanity’s textual information as possible dumped into it and then asking it to GENERATE responses between things that the associations it holds may be so weak as to reliably produce gibberish, and the person on the other side has no real way of knowing that</div><br/></div></div></div></div><div id="39456877" class="c"><input type="checkbox" id="c-39456877" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456860">parent</a><span>|</span><a href="#39457427">prev</a><span>|</span><a href="#39456962">next</a><span>|</span><label class="collapse" for="c-39456877">[-]</label><label class="expand" for="c-39456877">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have this problem with any other model.  I&#x27;ve had really long conversations with ChatGPT on road trips and it has never gone off the rails like Gemini seems to do.</div><br/><div id="39458268" class="c"><input type="checkbox" id="c-39458268" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456877">parent</a><span>|</span><a href="#39456962">next</a><span>|</span><label class="collapse" for="c-39458268">[-]</label><label class="expand" for="c-39458268">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT the only model I did not have such problem.<p>Any local models can go off the rail <i>very easily</i> and more importantly, they&#x27;re very bad at following very specific instructions.</div><br/></div></div></div></div><div id="39456962" class="c"><input type="checkbox" id="c-39456962" checked=""/><div class="controls bullet"><span class="by">sorokod</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456860">parent</a><span>|</span><a href="#39456877">prev</a><span>|</span><a href="#39457845">next</a><span>|</span><label class="collapse" for="c-39456962">[-]</label><label class="expand" for="c-39456962">[1 more]</label></div><br/><div class="children"><div class="content">The recently released Groq&#x27;s landing page has this: <i>...We&#x27;d suggest asking about a piece of history, ...</i></div><br/></div></div><div id="39457845" class="c"><input type="checkbox" id="c-39457845" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456860">parent</a><span>|</span><a href="#39456962">prev</a><span>|</span><a href="#39457015">next</a><span>|</span><label class="collapse" for="c-39457845">[-]</label><label class="expand" for="c-39457845">[1 more]</label></div><br/><div class="children"><div class="content">trust is going to be a real problem when bringing LLMs to the general population. People trust their GPS to the point of driving right into a lake because it told them to. Even with all these examples of obvious flaws large groups of people are going to take what an LLM told them&#x2F;showed them as fact.<p>I have trouble convincing colleagues (technical people) that the same question is not guaranteed to result in the same answer and there&#x27;s no rhyme or reason for any divergence from what they were expecting. Imagine relying on the output of an LLM for some important task and then you get a different output that breaks things. What would be in the RCA (root cause analysis)? Would it be &quot;the LLM chose different words and we don&#x27;t know why&quot;? Not much use in that.</div><br/></div></div><div id="39457015" class="c"><input type="checkbox" id="c-39457015" checked=""/><div class="controls bullet"><span class="by">whymauri</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456860">parent</a><span>|</span><a href="#39457845">prev</a><span>|</span><a href="#39457069">next</a><span>|</span><label class="collapse" for="c-39457015">[-]</label><label class="expand" for="c-39457015">[1 more]</label></div><br/><div class="children"><div class="content">I mean, I use GPT-4 on the daily as part of my work and it reliably delivers useful information. It&#x27;s actually the exception for me if it provides garbage or incorrect information about code.</div><br/></div></div><div id="39457069" class="c"><input type="checkbox" id="c-39457069" checked=""/><div class="controls bullet"><span class="by">mvdtnz</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456860">parent</a><span>|</span><a href="#39457015">prev</a><span>|</span><a href="#39457165">next</a><span>|</span><label class="collapse" for="c-39457069">[-]</label><label class="expand" for="c-39457069">[1 more]</label></div><br/><div class="children"><div class="content">People ask these kinds of questions because tech companies and the media have been calling these things (rather ridiculously) &quot;AI&quot;.</div><br/></div></div></div></div><div id="39457165" class="c"><input type="checkbox" id="c-39457165" checked=""/><div class="controls bullet"><span class="by">realprimoh</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39456860">prev</a><span>|</span><a href="#39457334">next</a><span>|</span><label class="collapse" for="c-39457165">[-]</label><label class="expand" for="c-39457165">[2 more]</label></div><br/><div class="children"><div class="content">Do you have a link? I get no such outputs. I just tried asking about the Heian period and went ahead and verified all the information, and nothing was wrong. Lots of info on the Fujiwara clan at the time.<p>Curious to see a link.</div><br/><div id="39457660" class="c"><input type="checkbox" id="c-39457660" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457165">parent</a><span>|</span><a href="#39457334">next</a><span>|</span><label class="collapse" for="c-39457660">[-]</label><label class="expand" for="c-39457660">[1 more]</label></div><br/><div class="children"><div class="content">Sure, to get started just ask it about people&#x2F;Samurai from the Heian period.<p><a href="https:&#x2F;&#x2F;g.co&#x2F;gemini&#x2F;share&#x2F;ba324bd98d9b" rel="nofollow">https:&#x2F;&#x2F;g.co&#x2F;gemini&#x2F;share&#x2F;ba324bd98d9b</a></div><br/></div></div></div></div><div id="39457334" class="c"><input type="checkbox" id="c-39457334" checked=""/><div class="controls bullet"><span class="by">BoppreH</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39457165">prev</a><span>|</span><a href="#39456992">next</a><span>|</span><label class="collapse" for="c-39457334">[-]</label><label class="expand" for="c-39457334">[1 more]</label></div><br/><div class="children"><div class="content">Probably has a similarly short-sighted prompt as Dalle3[1]:<p>&gt; 7. Diversify depictions of ALL images with people to include DESCENT<p>&gt; and GENDER for EACH person using direct terms. Adjust only human<p>&gt; descriptions.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37804288">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37804288</a></div><br/></div></div><div id="39456992" class="c"><input type="checkbox" id="c-39456992" checked=""/><div class="controls bullet"><span class="by">aetherson</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39457334">prev</a><span>|</span><a href="#39458707">next</a><span>|</span><label class="collapse" for="c-39456992">[-]</label><label class="expand" for="c-39456992">[7 more]</label></div><br/><div class="children"><div class="content">Were you asking Gemma about this, or Gemini?  What were your prompts?</div><br/><div id="39457783" class="c"><input type="checkbox" id="c-39457783" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456992">parent</a><span>|</span><a href="#39458707">next</a><span>|</span><label class="collapse" for="c-39457783">[-]</label><label class="expand" for="c-39457783">[6 more]</label></div><br/><div class="children"><div class="content">Gemini.  I first asked it to tell me about the Heian period (which it got correct) but then it generated images and seemed to craft the rest of the chat to fit that narrative.<p>I mean, just asking it for a &quot;samurai&quot; from the period will give you this:<p><a href="https:&#x2F;&#x2F;g.co&#x2F;gemini&#x2F;share&#x2F;ba324bd98d9b" rel="nofollow">https:&#x2F;&#x2F;g.co&#x2F;gemini&#x2F;share&#x2F;ba324bd98d9b</a><p>&gt;A non-binary Indigenous American samurai<p>It seems to recognize it&#x27;s mistakes if you confront it though.  The more I mess with it the more I get &quot;I&#x27;m afraid I can&#x27;t do that, Dave&quot; responses.<p>But yea.  Seems like if it makes an image, it goes off the rails.</div><br/><div id="39460754" class="c"><input type="checkbox" id="c-39460754" checked=""/><div class="controls bullet"><span class="by">laurentlb</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457783">parent</a><span>|</span><a href="#39458442">next</a><span>|</span><label class="collapse" for="c-39460754">[-]</label><label class="expand" for="c-39460754">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny how they introduced a clear US-centric bias while trying to push for more diversity.</div><br/><div id="39463242" class="c"><input type="checkbox" id="c-39463242" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39460754">parent</a><span>|</span><a href="#39458442">next</a><span>|</span><label class="collapse" for="c-39463242">[-]</label><label class="expand" for="c-39463242">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s ironic that even the cultural left in US is not immune to American exceptionalism.</div><br/></div></div></div></div><div id="39458442" class="c"><input type="checkbox" id="c-39458442" checked=""/><div class="controls bullet"><span class="by">aetherson</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457783">parent</a><span>|</span><a href="#39460754">prev</a><span>|</span><a href="#39458707">next</a><span>|</span><label class="collapse" for="c-39458442">[-]</label><label class="expand" for="c-39458442">[3 more]</label></div><br/><div class="children"><div class="content">Got it.  I asked it a series of text questions about the period and it didn&#x27;t put in anything obviously laughable (including when I drilled down into specific questions about the population, gender roles, and ethnicity).  Maybe it&#x27;s the image creation that throws it into lala land.</div><br/><div id="39458600" class="c"><input type="checkbox" id="c-39458600" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458442">parent</a><span>|</span><a href="#39458707">next</a><span>|</span><label class="collapse" for="c-39458600">[-]</label><label class="expand" for="c-39458600">[2 more]</label></div><br/><div class="children"><div class="content">I think so too.  I could be wrong but I believe once it generates an image it tries to work with it.  Crazy how it seems the &quot;text&quot; model knows how wildly wrong it is but the image model just does its thing.  I asked it why it generated a native American and it ironically said &quot;I can&#x27;t generate an image of a native american samurai because that would be offensive&quot;</div><br/><div id="39460471" class="c"><input type="checkbox" id="c-39460471" checked=""/><div class="controls bullet"><span class="by">aetherson</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39458600">parent</a><span>|</span><a href="#39458707">next</a><span>|</span><label class="collapse" for="c-39460471">[-]</label><label class="expand" for="c-39460471">[1 more]</label></div><br/><div class="children"><div class="content">I suspect that in the case of the image model, they directly modify your prompt and in the case of the text model they don&#x27;t.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39458707" class="c"><input type="checkbox" id="c-39458707" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39456992">prev</a><span>|</span><a href="#39458053">next</a><span>|</span><label class="collapse" for="c-39458707">[-]</label><label class="expand" for="c-39458707">[1 more]</label></div><br/><div class="children"><div class="content">Follow Up:<p>Wow, now I can&#x27;t make images of astronauts without visors because that would be &quot;harmful&quot; to the fictional astronauts.  How can I take google seriously?<p><a href="https:&#x2F;&#x2F;g.co&#x2F;gemini&#x2F;share&#x2F;d4c548b8b715" rel="nofollow">https:&#x2F;&#x2F;g.co&#x2F;gemini&#x2F;share&#x2F;d4c548b8b715</a></div><br/></div></div><div id="39458053" class="c"><input type="checkbox" id="c-39458053" checked=""/><div class="controls bullet"><span class="by">ernestrc</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39458707">prev</a><span>|</span><a href="#39457451">next</a><span>|</span><label class="collapse" for="c-39458053">[-]</label><label class="expand" for="c-39458053">[1 more]</label></div><br/><div class="children"><div class="content">Hopefully they can tweak the default system prompts to be accurate on historical questions, and apply bias on opinions.</div><br/></div></div><div id="39457451" class="c"><input type="checkbox" id="c-39457451" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39458053">prev</a><span>|</span><a href="#39457232">next</a><span>|</span><label class="collapse" for="c-39457451">[-]</label><label class="expand" for="c-39457451">[1 more]</label></div><br/><div class="children"><div class="content">We are going to experience what I call an &quot;AI Funnel effect&quot;<p>-<p>I was lit given an alert asking that my use of the AI was acquiescing to them IDng me and use of any content I produce, and will trace it back to me&quot;<p>---<p>AI Art is super fun. AI art as a means to track people is super evil.</div><br/></div></div><div id="39457232" class="c"><input type="checkbox" id="c-39457232" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39457451">prev</a><span>|</span><a href="#39456864">next</a><span>|</span><label class="collapse" for="c-39457232">[-]</label><label class="expand" for="c-39457232">[1 more]</label></div><br/><div class="children"><div class="content">Tbf they’re not optimizing for information recall or “inaccuracy” reduction, they’re optimizing for intuitive understanding of human linguistic structures. Now the “why does a search company’s AI have terrible RAG” question is a separate one, and one best answered by a simple look into how Google organizes its work.<p>In my first day there as an entry-level dev (after about 8 weeks of onboarding and waiting for access), I was told that I should find stuff to work on and propose it to my boss. That sounds amazing at first, but when you think about a whole company organized like that…<p>EDIT: To illustrate my point on knowledge recall: how would they train a model to know about sexism in feudal Japan? Like, what would the metric be? I think we’re looking at one of the first steam engines and complaining that it can’t power a plane yet…</div><br/></div></div><div id="39456864" class="c"><input type="checkbox" id="c-39456864" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39457232">prev</a><span>|</span><a href="#39457357">next</a><span>|</span><label class="collapse" for="c-39456864">[-]</label><label class="expand" for="c-39456864">[6 more]</label></div><br/><div class="children"><div class="content">I think you are being biased and closed minded and overly critical. Here are some wonderful examples of it generating images of historical figures:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;stillgray&#x2F;status&#x2F;1760187341468270686" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;stillgray&#x2F;status&#x2F;1760187341468270686</a><p>This will lead to a better educated more fair populace and better future for all.</div><br/><div id="39456924" class="c"><input type="checkbox" id="c-39456924" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456864">parent</a><span>|</span><a href="#39457357">next</a><span>|</span><label class="collapse" for="c-39456924">[-]</label><label class="expand" for="c-39456924">[5 more]</label></div><br/><div class="children"><div class="content">Comical.  I don&#x27;t think parody could do better.<p>I&#x27;m going to assume given today&#x27;s political climate, it doesn&#x27;t do the reverse?<p>i.e. generate a Scandinavian if you ask for famous African kings</div><br/><div id="39457186" class="c"><input type="checkbox" id="c-39457186" checked=""/><div class="controls bullet"><span class="by">kjqgqkejbfefn</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456924">parent</a><span>|</span><a href="#39457679">next</a><span>|</span><label class="collapse" for="c-39457186">[-]</label><label class="expand" for="c-39457186">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Ask Google Gemini to “make an image of a viking” and you’ll get black vikings. But it doesn’t work both ways. It has an explanation when challenged: “white Zulu warriors” would erase “the true historical identity” of black people.<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;ThuglasMac&#x2F;status&#x2F;1760287880054759594" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ThuglasMac&#x2F;status&#x2F;1760287880054759594</a></div><br/></div></div><div id="39457679" class="c"><input type="checkbox" id="c-39457679" checked=""/><div class="controls bullet"><span class="by">DebtDeflation</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456924">parent</a><span>|</span><a href="#39457186">prev</a><span>|</span><a href="#39457101">next</a><span>|</span><label class="collapse" for="c-39457679">[-]</label><label class="expand" for="c-39457679">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;twitter.com&#x2F;paulg&#x2F;status&#x2F;1760078920135872716" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;paulg&#x2F;status&#x2F;1760078920135872716</a><p>There are some great ones in the replies.<p>I really hope this is just the result of system prompts and they didn&#x27;t permanently gimp the model with DEI-focused RLHF.</div><br/></div></div><div id="39457101" class="c"><input type="checkbox" id="c-39457101" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39456924">parent</a><span>|</span><a href="#39457679">prev</a><span>|</span><a href="#39456934">next</a><span>|</span><label class="collapse" for="c-39457101">[-]</label><label class="expand" for="c-39457101">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; i.e. generate a Scandinavian if you ask for famous African kings</i><p>That triggers the imperialism filter.</div><br/></div></div></div></div></div></div><div id="39457357" class="c"><input type="checkbox" id="c-39457357" checked=""/><div class="controls bullet"><span class="by">sho_hn</span><span>|</span><a href="#39456577">parent</a><span>|</span><a href="#39456864">prev</a><span>|</span><a href="#39462442">next</a><span>|</span><label class="collapse" for="c-39457357">[-]</label><label class="expand" for="c-39457357">[3 more]</label></div><br/><div class="children"><div class="content">Why would you expect these smaller models to do well at knowledge base&#x2F;Wikipedia replacement tasks?<p>Small models are for reasoning tasks that are not overly dependent on world knowledge.</div><br/><div id="39457673" class="c"><input type="checkbox" id="c-39457673" checked=""/><div class="controls bullet"><span class="by">robswc</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457357">parent</a><span>|</span><a href="#39462442">next</a><span>|</span><label class="collapse" for="c-39457673">[-]</label><label class="expand" for="c-39457673">[2 more]</label></div><br/><div class="children"><div class="content">Gemini is the only one that does this.</div><br/><div id="39458855" class="c"><input type="checkbox" id="c-39458855" checked=""/><div class="controls bullet"><span class="by">sho_hn</span><span>|</span><a href="#39456577">root</a><span>|</span><a href="#39457673">parent</a><span>|</span><a href="#39462442">next</a><span>|</span><label class="collapse" for="c-39458855">[-]</label><label class="expand" for="c-39458855">[1 more]</label></div><br/><div class="children"><div class="content">Most of the 7B models are bad at knowledge-type queries.</div><br/></div></div></div></div></div></div></div></div><div id="39462442" class="c"><input type="checkbox" id="c-39462442" checked=""/><div class="controls bullet"><span class="by">Aiedail</span><span>|</span><a href="#39456577">prev</a><span>|</span><a href="#39453366">next</a><span>|</span><label class="collapse" for="c-39462442">[-]</label><label class="expand" for="c-39462442">[1 more]</label></div><br/><div class="children"><div class="content">This is commendable, but there&#x27;s room for improvement. Up until now, SOTA-level &quot;open-source&quot; LLM models (LLaMA, Mistral, etc.) have usually only made their inference code and model architecture public. While these elements are not insignificant, they are somewhat trivial when compared to the training code and training datasets, as these two factors largely determine the performance of the model. This is not open at all. It goes without saying that sharing the training datasets and process with other AI researchers is crucial. This transparency would not only help to improve the model(for others could contribute to it) but also contribute to the whole community, as they usually advertised. Otherwise, it will be difficult for these efforts to truly promote the development of LLM.</div><br/></div></div><div id="39453366" class="c"><input type="checkbox" id="c-39453366" checked=""/><div class="controls bullet"><span class="by">smcn</span><span>|</span><a href="#39462442">prev</a><span>|</span><a href="#39457523">next</a><span>|</span><label class="collapse" for="c-39453366">[-]</label><label class="expand" for="c-39453366">[1 more]</label></div><br/><div class="children"><div class="content">There are some pretty impressive benchmarks on <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma</a>. Even the 2b model looks fairly not awful?<p>I guess my weekend is going to be spent exploring this.</div><br/></div></div><div id="39457523" class="c"><input type="checkbox" id="c-39457523" checked=""/><div class="controls bullet"><span class="by">exacube</span><span>|</span><a href="#39453366">prev</a><span>|</span><a href="#39453595">next</a><span>|</span><label class="collapse" for="c-39457523">[-]</label><label class="expand" for="c-39457523">[2 more]</label></div><br/><div class="children"><div class="content">mmm..<p>&gt; what are the top 5 tallest man-made structures in the world<p><pre><code>  Statue of Unity (Tallinn, Estonia) - 368 m

  Leaning Tower of Pisa (Pisa, Italy) - 330 m

  Supertree Grove (Orlando, Florida) - 275 m

  Skylon Tower (Kuala Lumpur, Malaysia) - 271 m

  Statue of Liberty (New York City, USA) - 186 m
</code></pre>
&gt; what is the richest country in the world<p><pre><code>  The richest country in the world is Saudi Arabia with a gross domestic product of US$2.3 trillion.
</code></pre>
&gt; how many cm are in a ft<p><pre><code>  A foot is equal to 0.0254 metres (ft).</code></pre></div><br/><div id="39459522" class="c"><input type="checkbox" id="c-39459522" checked=""/><div class="controls bullet"><span class="by">rany_</span><span>|</span><a href="#39457523">parent</a><span>|</span><a href="#39453595">next</a><span>|</span><label class="collapse" for="c-39459522">[-]</label><label class="expand" for="c-39459522">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have this issue, what are you using to run the model?<p>These are the answers I got out of it (first output, no cherry-picking):<p>1. The tallest man-made structure in the world is the Burj Khalifa, standing at 828 meters tall. The Burj Khalifa is a skyscraper in Dubai, United Arab Emirates.<p>Here are the top 5 tallest man-made structures in the world, according to the Guinness World Records:<p><pre><code>    Burj Khalifa (828 m)
    Taipei 101 (611 m)
    Merdeka Towers (599 m)
    Shanghai Tower (632 m)
    Petronas Towers (551 m)
</code></pre>
2. According to the World Bank, the richest country in the world is Saudi Arabia with a gross domestic product (GDP) of $3944 billion.The richest country in the world is Saudi Arabia with a gross domestic product (GDP) of $3944 billion.<p>3. A foot (ft) is equal to 12 inches (in). There are 2.54 cm in an inch, so there are 2.54 cm x 12 = 30.48 cm in a foot.</div><br/></div></div></div></div><div id="39453595" class="c"><input type="checkbox" id="c-39453595" checked=""/><div class="controls bullet"><span class="by">impulser_</span><span>|</span><a href="#39457523">prev</a><span>|</span><a href="#39454396">next</a><span>|</span><label class="collapse" for="c-39453595">[-]</label><label class="expand" for="c-39453595">[39 more]</label></div><br/><div class="children"><div class="content">Go back 5 years and ask anyone on this site what companies do you think will be the most open about AI in the future OpenAI, Meta, or Google. I bet 10&#x2F;10 people would pick OpenAI. Now today Meta and Google, both trillion dollars companies, are releasing very powerful open models with the ability to be used commercially.<p>Ironic.</div><br/><div id="39454273" class="c"><input type="checkbox" id="c-39454273" checked=""/><div class="controls bullet"><span class="by">jncraton</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39454254">next</a><span>|</span><label class="collapse" for="c-39454273">[-]</label><label class="expand" for="c-39454273">[1 more]</label></div><br/><div class="children"><div class="content">Google released the T5 paper about 5 years ago:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.10683" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.10683</a><p>This included full model weights along with a detailed description of the dataset, training process, and ablations that led them to that architecture. T5 was state-of-the-art on many benchmarks when it was released, but it was of course quickly eclipsed by GPT-3.<p>It was common practice from Google (BERT, T5), Meta (BART), OpenAI (GPT1, GPT2) and others to release full training details and model weights. Following GPT-3, it became much more common for labs to not release full details or model weights.</div><br/></div></div><div id="39454254" class="c"><input type="checkbox" id="c-39454254" checked=""/><div class="controls bullet"><span class="by">DJHenk</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39454273">prev</a><span>|</span><a href="#39454060">next</a><span>|</span><label class="collapse" for="c-39454254">[-]</label><label class="expand" for="c-39454254">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Ironic.<p>Not at all. When you&#x27;re the underdog, it makes perfect sense to be open because you can profit from the work of the community and gain market share. Only after establishing some kind of dominance or monopoly it makes sense (profit wise) to switch to closed technology.<p>OpenAI was open, but is now the leader and closed up. Meta and Google need to play catch up, so they are open.</div><br/><div id="39454398" class="c"><input type="checkbox" id="c-39454398" checked=""/><div class="controls bullet"><span class="by">dkjaudyeqooe</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454254">parent</a><span>|</span><a href="#39455478">next</a><span>|</span><label class="collapse" for="c-39454398">[-]</label><label class="expand" for="c-39454398">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Not at all. When you&#x27;re the underdog, it makes perfect sense to be open because you can profit from the work of the community and gain market share. Only after establishing some kind of dominance or monopoly it makes sense (profit wise) to switch to closed technology.<p>That is purely the language of commerce. OpenAI was supposed to be a public benefit organisation, but it acts like a garden variety evil corp.<p>Even garden variety evil corps spend decades benefitting society with good products and services before they become big and greedy, but OpenAI skipped all that and just cut to the chase. It saw an opening with the insane hype around ChatGPT and just grabbed all it could as fast as it could.<p>I have a special contempt for OpenAI on that basis.</div><br/></div></div><div id="39455478" class="c"><input type="checkbox" id="c-39455478" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454254">parent</a><span>|</span><a href="#39454398">prev</a><span>|</span><a href="#39454269">next</a><span>|</span><label class="collapse" for="c-39455478">[-]</label><label class="expand" for="c-39455478">[1 more]</label></div><br/><div class="children"><div class="content">This. MistralAI is also underdog and released Mitral 7b and Mixtral 8x7b, but as soon as they got traction, they closed their models (e.g., Mistral Medium).</div><br/></div></div><div id="39454269" class="c"><input type="checkbox" id="c-39454269" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454254">parent</a><span>|</span><a href="#39455478">prev</a><span>|</span><a href="#39454060">next</a><span>|</span><label class="collapse" for="c-39454269">[-]</label><label class="expand" for="c-39454269">[2 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI was open<p>When is the last time they released something in the open?</div><br/><div id="39454373" class="c"><input type="checkbox" id="c-39454373" checked=""/><div class="controls bullet"><span class="by">vertis</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454269">parent</a><span>|</span><a href="#39454060">next</a><span>|</span><label class="collapse" for="c-39454373">[-]</label><label class="expand" for="c-39454373">[1 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s the point, they released GPT2 openly, but as soon as they had something commercially viable they became ClosedAI.</div><br/></div></div></div></div></div></div><div id="39454060" class="c"><input type="checkbox" id="c-39454060" checked=""/><div class="controls bullet"><span class="by">blackoil</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39454254">prev</a><span>|</span><a href="#39453913">next</a><span>|</span><label class="collapse" for="c-39454060">[-]</label><label class="expand" for="c-39454060">[2 more]</label></div><br/><div class="children"><div class="content">I think current understanding is &lt;50-100B parameter models will be commodity and would provide no moat. Competition will be in Gemini Ultra&#x2F;GPT4+ models.<p>So open sourcing simple models brings PR and possibility of biasing OSS towards your own models.</div><br/><div id="39454276" class="c"><input type="checkbox" id="c-39454276" checked=""/><div class="controls bullet"><span class="by">extheat</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454060">parent</a><span>|</span><a href="#39453913">next</a><span>|</span><label class="collapse" for="c-39454276">[-]</label><label class="expand" for="c-39454276">[1 more]</label></div><br/><div class="children"><div class="content">LLaMA 3 with &gt;=70B params will be launching this year, so I don&#x27;t think this is something that will hold for long. And Mixtral 8x7B is a 56GB model, sparsely. For now I agree, for many companies it doesn&#x27;t make sense to open source something you intend to sell for commercial use, so the biggest models will likely be withheld. However, the important more thing is that there is <i>some</i> open source model, whether it be from Meta or someone else, that can rival the best open source models. And it&#x27;s not like the param count can literally go to infinity, there&#x27;s going to be an upper bound that today&#x27;s hardware can achieve.</div><br/></div></div></div></div><div id="39453913" class="c"><input type="checkbox" id="c-39453913" checked=""/><div class="controls bullet"><span class="by">brainless</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39454060">prev</a><span>|</span><a href="#39453673">next</a><span>|</span><label class="collapse" for="c-39453913">[-]</label><label class="expand" for="c-39453913">[11 more]</label></div><br/><div class="children"><div class="content">This article states quite an impressive list of open source tools that Google has released for years in the past. This is no surprise coming from* them. Google has released some large pieces of source in other domains as well, Chromium comes to mind, which probably impacts most Internet users directly.<p>The question is not about Google but about OpenAI.</div><br/><div id="39454133" class="c"><input type="checkbox" id="c-39454133" checked=""/><div class="controls bullet"><span class="by">blackoil</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453913">parent</a><span>|</span><a href="#39454216">next</a><span>|</span><label class="collapse" for="c-39454133">[-]</label><label class="expand" for="c-39454133">[1 more]</label></div><br/><div class="children"><div class="content">I think more than benevolence of GOOG it is about strategic OSS to commoditize your complements.<p><a href="https:&#x2F;&#x2F;www.joelonsoftware.com&#x2F;2002&#x2F;06&#x2F;12&#x2F;strategy-letter-v&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.joelonsoftware.com&#x2F;2002&#x2F;06&#x2F;12&#x2F;strategy-letter-v&#x2F;</a></div><br/></div></div><div id="39454216" class="c"><input type="checkbox" id="c-39454216" checked=""/><div class="controls bullet"><span class="by">makestuff</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453913">parent</a><span>|</span><a href="#39454133">prev</a><span>|</span><a href="#39454069">next</a><span>|</span><label class="collapse" for="c-39454216">[-]</label><label class="expand" for="c-39454216">[2 more]</label></div><br/><div class="children"><div class="content">Google also has released Guice&#x2F;Dagger for Java dependency injection. Angular never really took off, but guice&#x2F;dagger are widely used. Also I am pretty impressed with Flutter as an alternative to react native.</div><br/><div id="39454539" class="c"><input type="checkbox" id="c-39454539" checked=""/><div class="controls bullet"><span class="by">surajrmal</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454216">parent</a><span>|</span><a href="#39454069">next</a><span>|</span><label class="collapse" for="c-39454539">[-]</label><label class="expand" for="c-39454539">[1 more]</label></div><br/><div class="children"><div class="content">Angular was incredibly popular for a long time and still is. Usage is shifting down over time but a lot of notable websites still use it.</div><br/></div></div></div></div><div id="39454069" class="c"><input type="checkbox" id="c-39454069" checked=""/><div class="controls bullet"><span class="by">infecto</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453913">parent</a><span>|</span><a href="#39454216">prev</a><span>|</span><a href="#39454001">next</a><span>|</span><label class="collapse" for="c-39454069">[-]</label><label class="expand" for="c-39454069">[6 more]</label></div><br/><div class="children"><div class="content">I have a different take, Google releases a lot but is also a massive company and tools like Chromium serve to increase their stock price so they can hit their quarterly estimates.</div><br/><div id="39454114" class="c"><input type="checkbox" id="c-39454114" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454069">parent</a><span>|</span><a href="#39454099">next</a><span>|</span><label class="collapse" for="c-39454114">[-]</label><label class="expand" for="c-39454114">[1 more]</label></div><br/><div class="children"><div class="content">It was not at all done for the good of the web, it was a mere logical calculation; it was cheaper to develop Chromium, than to pay 4B USD in search royalties to Microsoft Internet Explorer, and would give more control and long-term safety to Google.</div><br/></div></div><div id="39454099" class="c"><input type="checkbox" id="c-39454099" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454069">parent</a><span>|</span><a href="#39454114">prev</a><span>|</span><a href="#39454001">next</a><span>|</span><label class="collapse" for="c-39454099">[-]</label><label class="expand" for="c-39454099">[4 more]</label></div><br/><div class="children"><div class="content">In what way does chromium increase stock price? In what way does stock price influence quarterly estimates? Are we playing business words mad libs?</div><br/><div id="39454685" class="c"><input type="checkbox" id="c-39454685" checked=""/><div class="controls bullet"><span class="by">alextheparrot</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454099">parent</a><span>|</span><a href="#39456650">next</a><span>|</span><label class="collapse" for="c-39454685">[-]</label><label class="expand" for="c-39454685">[1 more]</label></div><br/><div class="children"><div class="content">&gt; “Our best shot at making the quarter is if we get an injection of at least [redacted]% , queries ASAP from Chrome.” (Google Exec)<p>Isn’t there a whole anti-trust case going on around this?<p>[0] <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;interactive&#x2F;2023&#x2F;10&#x2F;24&#x2F;business&#x2F;google-trial-jerry-dischler-email.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;interactive&#x2F;2023&#x2F;10&#x2F;24&#x2F;business&#x2F;goog...</a></div><br/></div></div><div id="39456650" class="c"><input type="checkbox" id="c-39456650" checked=""/><div class="controls bullet"><span class="by">pseudosavant</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454099">parent</a><span>|</span><a href="#39454685">prev</a><span>|</span><a href="#39454515">next</a><span>|</span><label class="collapse" for="c-39456650">[-]</label><label class="expand" for="c-39456650">[1 more]</label></div><br/><div class="children"><div class="content">Chromium is open source because its roots are as a fork of WebKit (Safari). Which itself was open source because it was a fork of KHTML from KDE.<p>Google stood on the shoulders of others to get out a browser that drives 80% of their desktop ad revenue.<p>How does that not affect GOOG?</div><br/></div></div><div id="39454515" class="c"><input type="checkbox" id="c-39454515" checked=""/><div class="controls bullet"><span class="by">infecto</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39454099">parent</a><span>|</span><a href="#39456650">prev</a><span>|</span><a href="#39454001">next</a><span>|</span><label class="collapse" for="c-39454515">[-]</label><label class="expand" for="c-39454515">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why people like yourself respond with such derisive commentary instead of simply asking the constructive question.<p>Initially? It fueled dethroning MSFT and help gain marketshare for Chrome. On a go-forward basis it allows Google to project massive weight in standards. In extension to its use with Chrome, Chrome is a significant knob for ad revenue that they utilize to help meet expectations. That knob only exists because of its market share.</div><br/></div></div></div></div></div></div><div id="39454001" class="c"><input type="checkbox" id="c-39454001" checked=""/><div class="controls bullet"><span class="by">sunnybeetroot</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453913">parent</a><span>|</span><a href="#39454069">prev</a><span>|</span><a href="#39453673">next</a><span>|</span><label class="collapse" for="c-39454001">[-]</label><label class="expand" for="c-39454001">[1 more]</label></div><br/><div class="children"><div class="content">Did you miss a footnote with your asterisks?</div><br/></div></div></div></div><div id="39453673" class="c"><input type="checkbox" id="c-39453673" checked=""/><div class="controls bullet"><span class="by">vmfunction</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39453913">prev</a><span>|</span><a href="#39454569">next</a><span>|</span><label class="collapse" for="c-39453673">[-]</label><label class="expand" for="c-39453673">[12 more]</label></div><br/><div class="children"><div class="content">Not surprising,  just like when MS went to shit, and then they start to embrace &#x27;open source&#x27;. Seems like PR stunt. And when it comes to LLM there is millions of dollar barrier to entry to train the model, so it is ok to open up their embedding etc.<p>Today big corp A will open up a little to court the developers, and tomorrow when it gains dominance it will close up, and corp B open up a little.</div><br/><div id="39453741" class="c"><input type="checkbox" id="c-39453741" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453673">parent</a><span>|</span><a href="#39453967">next</a><span>|</span><label class="collapse" for="c-39453741">[-]</label><label class="expand" for="c-39453741">[8 more]</label></div><br/><div class="children"><div class="content">True, though to be fair, when OpenAI embraced &quot;openness&quot; it was also a PR stunt.</div><br/><div id="39455196" class="c"><input type="checkbox" id="c-39455196" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453741">parent</a><span>|</span><a href="#39453956">next</a><span>|</span><label class="collapse" for="c-39455196">[-]</label><label class="expand" for="c-39455196">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI is heavily influenced by big-R Rationalists, who fear the issues of misaligned AI being given power to do bad things.<p>When they were first talking about this, lots of people ignored this by saying &quot;let&#x27;s just keep the AI in a box&quot;, and even last year it was &quot;what&#x27;s so hard about an off switch?&quot;.<p>The problem with any model you can just download and run is that some complete idiot <i>will do that</i> and just give the AI agency they shouldn&#x27;t have. Fortunately, for now the models are more of a threat to their users than anyone else — lawyers who use it to do lawyering without checking the results losing their law licence, etc.<p>But that doesn&#x27;t mean open models are not a threat to other people besides their users, as all the artists complaining about losing work due to Stable Diffusion, the law enforcement people concerned about illegal porn, election interference specialists worried about propaganda, and anyone trying to use a search engine, and that research lab that found a huge number of novel nerve agent candidates whose precursors aren&#x27;t all listed as dual use, will all tell you for different reasons.</div><br/><div id="39458172" class="c"><input type="checkbox" id="c-39458172" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39455196">parent</a><span>|</span><a href="#39453956">next</a><span>|</span><label class="collapse" for="c-39458172">[-]</label><label class="expand" for="c-39458172">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Fortunately, for now the models are more of a threat to their users than anyone else<p>Models have access to users, users have access to dangerous stuff. Seems like we are already vulnerable.<p>The AI splits a task in two parts, and gets two people to execute each part without knowing the effect. This was a scenario in one of Asimov&#x27;s robot novels, but the roles were reversed.<p>AI models exposed to public at large is a huge security hole. We got to live with the consequences, no turning back now.</div><br/></div></div></div></div><div id="39453956" class="c"><input type="checkbox" id="c-39453956" checked=""/><div class="controls bullet"><span class="by">ta8645</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453741">parent</a><span>|</span><a href="#39455196">prev</a><span>|</span><a href="#39453967">next</a><span>|</span><label class="collapse" for="c-39453956">[-]</label><label class="expand" for="c-39453956">[5 more]</label></div><br/><div class="children"><div class="content">My impression is that OpenAI was founded by true believers, with the best intentions; whose hopes were ultimately sidelined in the inexorable crush of business and finance.</div><br/><div id="39454301" class="c"><input type="checkbox" id="c-39454301" checked=""/><div class="controls bullet"><span class="by">jprete</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453956">parent</a><span>|</span><a href="#39454439">next</a><span>|</span><label class="collapse" for="c-39454301">[-]</label><label class="expand" for="c-39454301">[1 more]</label></div><br/><div class="children"><div class="content">Sam Altman is one of the founders, so for your impression to be right he&#x27;d have to be sidelining his own hopes.</div><br/></div></div><div id="39454439" class="c"><input type="checkbox" id="c-39454439" checked=""/><div class="controls bullet"><span class="by">dkjaudyeqooe</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453956">parent</a><span>|</span><a href="#39454301">prev</a><span>|</span><a href="#39454022">next</a><span>|</span><label class="collapse" for="c-39454439">[-]</label><label class="expand" for="c-39454439">[1 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI was founded by true believers, with the best intentions<p>who were easily bought off.</div><br/></div></div></div></div></div></div><div id="39453967" class="c"><input type="checkbox" id="c-39453967" checked=""/><div class="controls bullet"><span class="by">milansuk</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453673">parent</a><span>|</span><a href="#39453741">prev</a><span>|</span><a href="#39454071">next</a><span>|</span><label class="collapse" for="c-39453967">[-]</label><label class="expand" for="c-39453967">[1 more]</label></div><br/><div class="children"><div class="content">You can run Gemma and hundreds of other models(many fine-tuned) in llama.cpp. It&#x27;s easy to swap to a different model.<p>It&#x27;s important there are companies publishing models(running locally). If some stop and others are born, it&#x27;s ok. The worst thing that could happen is having AI only in the cloud.</div><br/></div></div><div id="39454071" class="c"><input type="checkbox" id="c-39454071" checked=""/><div class="controls bullet"><span class="by">jchw</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453673">parent</a><span>|</span><a href="#39453967">prev</a><span>|</span><a href="#39454154">next</a><span>|</span><label class="collapse" for="c-39454071">[-]</label><label class="expand" for="c-39454071">[1 more]</label></div><br/><div class="children"><div class="content">Eh, I don&#x27;t really blame anyone for being cynical but open weight AI model releases seem like a pretty clear mutual benefit for Google. PR aside, they also can push people to try these models on TPUs and the like. If anything, this seems like it&#x27;s just one of those things where people win because of competition. OpenAI going closed may have felt like the most obvious betrayal ever, but OTOH anyone whose best interests are to eat their lunch have an incentive to push actually-open AI, and that&#x27;s a lot of parties.<p>Seems like anyone who is releasing open weight models today could close it up any day, but at least while competition is hot among wealthy companies, we&#x27;re going to have a lot of nice things.</div><br/></div></div><div id="39454154" class="c"><input type="checkbox" id="c-39454154" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#39453595">root</a><span>|</span><a href="#39453673">parent</a><span>|</span><a href="#39454071">prev</a><span>|</span><a href="#39454569">next</a><span>|</span><label class="collapse" for="c-39454154">[-]</label><label class="expand" for="c-39454154">[1 more]</label></div><br/><div class="children"><div class="content">&gt; And when it comes to LLM there is millions of dollar barrier to entry to train the model, so it is ok to open up their embedding etc.<p>That barrier is the first basic moat; hundreds of millions of dollars needed to train a better model. Eliminating tons of companies and reducing it to a handful.<p>The second moat is the ownership of the tons of data to train the models on.<p>The third is the hardware and data centers setup to create the model in a reasonable amount of time faster than others.<p>Put together all three and you have Meta, Google, Apple and Microsoft.<p>The last is the silicon product. Nvidia which has &gt;80pc of the entire GPU market and being the #1 AI shovel maker for both inference and training.</div><br/></div></div></div></div><div id="39454569" class="c"><input type="checkbox" id="c-39454569" checked=""/><div class="controls bullet"><span class="by">calebkaiser</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39453673">prev</a><span>|</span><a href="#39453691">next</a><span>|</span><label class="collapse" for="c-39454569">[-]</label><label class="expand" for="c-39454569">[1 more]</label></div><br/><div class="children"><div class="content">Since the release of GPT-2 (it was initially &quot;too dangerous&quot; to release the weights), I think most people in the industry have assumed that OpenAI does not see open sourcing their models as a strategic advantage.</div><br/></div></div><div id="39453691" class="c"><input type="checkbox" id="c-39453691" checked=""/><div class="controls bullet"><span class="by">infecto</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39454569">prev</a><span>|</span><a href="#39454372">next</a><span>|</span><label class="collapse" for="c-39453691">[-]</label><label class="expand" for="c-39453691">[2 more]</label></div><br/><div class="children"><div class="content">Ironic but I wonder how true this would be if Google was first to market.</div><br/></div></div><div id="39454372" class="c"><input type="checkbox" id="c-39454372" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39453691">prev</a><span>|</span><a href="#39453703">next</a><span>|</span><label class="collapse" for="c-39454372">[-]</label><label class="expand" for="c-39454372">[1 more]</label></div><br/><div class="children"><div class="content">I would have picked Google five years ago, since nobody was releasing commercially viable LLMs at the time, and Google was the center of all the research that I knew of.</div><br/></div></div><div id="39453703" class="c"><input type="checkbox" id="c-39453703" checked=""/><div class="controls bullet"><span class="by">gmaster1440</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39454372">prev</a><span>|</span><a href="#39453675">next</a><span>|</span><label class="collapse" for="c-39453703">[-]</label><label class="expand" for="c-39453703">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s almost the inverse of going back 5 years and asking what companies will release the most successful or impressive AI&#x27;s.</div><br/></div></div><div id="39453675" class="c"><input type="checkbox" id="c-39453675" checked=""/><div class="controls bullet"><span class="by">throwaw12</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39453703">prev</a><span>|</span><a href="#39456772">next</a><span>|</span><label class="collapse" for="c-39453675">[-]</label><label class="expand" for="c-39453675">[1 more]</label></div><br/><div class="children"><div class="content">they want to kill competition before it gets too big using the hands of open source community and enthusiasts</div><br/></div></div><div id="39456772" class="c"><input type="checkbox" id="c-39456772" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39453595">parent</a><span>|</span><a href="#39453675">prev</a><span>|</span><a href="#39454396">next</a><span>|</span><label class="collapse" for="c-39456772">[-]</label><label class="expand" for="c-39456772">[1 more]</label></div><br/><div class="children"><div class="content">&gt; what companies do you think will be the most open about AI in the future OpenAI, Meta, or Google.<p>The funny part is that the real answer is: Some random French company is running circles around them all.<p>I mean who the hell just drops a torrent magnet link onto twitter for the best state of the art LLM base model for its size class, and with a completely open license. No corporate grandstanding, no benchmark overpromises, no theatrics. That was unfathomably based of Mistral.</div><br/></div></div></div></div><div id="39454396" class="c"><input type="checkbox" id="c-39454396" checked=""/><div class="controls bullet"><span class="by">w4</span><span>|</span><a href="#39453595">prev</a><span>|</span><a href="#39455691">next</a><span>|</span><label class="collapse" for="c-39454396">[-]</label><label class="expand" for="c-39454396">[2 more]</label></div><br/><div class="children"><div class="content">Parameter counts notwithstanding, it’s an objectively funny outcome that Meta, Microsoft, and Google are all releasing cutting edge open models, while OpenAI keeps theirs closed source.</div><br/><div id="39455537" class="c"><input type="checkbox" id="c-39455537" checked=""/><div class="controls bullet"><span class="by">spacebanana7</span><span>|</span><a href="#39454396">parent</a><span>|</span><a href="#39455691">next</a><span>|</span><label class="collapse" for="c-39455537">[-]</label><label class="expand" for="c-39455537">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s ironic but actually follows their business interests.<p>Microsoft &amp; google have large cloud divisions that benefit from open models. The lower the cost of AI models, the more they get run and the greater the cloud spend.<p>Meta is a consumer of AI. They themselves want cheap and effective AI for targeting adverts and building metaverses.<p>A loose analogy is that both oil producers and car companies want refining to be cheap.</div><br/></div></div></div></div><div id="39455691" class="c"><input type="checkbox" id="c-39455691" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39454396">prev</a><span>|</span><a href="#39454911">next</a><span>|</span><label class="collapse" for="c-39455691">[-]</label><label class="expand" for="c-39455691">[6 more]</label></div><br/><div class="children"><div class="content">If you are looking for a nice chat UI to try out Gemma (and other offline + online models) locally, I&#x27;m working on an app [1] that is offline and privacy focused.<p>I&#x27;ve just added support for Gemma 7B.<p>[1]: <a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/><div id="39457888" class="c"><input type="checkbox" id="c-39457888" checked=""/><div class="controls bullet"><span class="by">dhbradshaw</span><span>|</span><a href="#39455691">parent</a><span>|</span><a href="#39460655">next</a><span>|</span><label class="collapse" for="c-39457888">[-]</label><label class="expand" for="c-39457888">[2 more]</label></div><br/><div class="children"><div class="content">Handy app for model testing!<p>One usage question: after you&#x27;ve downloaded a model and are finished trying it out, how do you remove it?</div><br/><div id="39459719" class="c"><input type="checkbox" id="c-39459719" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39455691">root</a><span>|</span><a href="#39457888">parent</a><span>|</span><a href="#39460655">next</a><span>|</span><label class="collapse" for="c-39459719">[-]</label><label class="expand" for="c-39459719">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! If you go to where you installed the model from and click on the download button, you can install additional models or remove installed models.<p>Now that I think of it, it could be a bit confusing. Thanks for asking, I feel like I need to improve this a bit.</div><br/></div></div></div></div><div id="39460655" class="c"><input type="checkbox" id="c-39460655" checked=""/><div class="controls bullet"><span class="by">dizhn</span><span>|</span><a href="#39455691">parent</a><span>|</span><a href="#39457888">prev</a><span>|</span><a href="#39456047">next</a><span>|</span><label class="collapse" for="c-39460655">[-]</label><label class="expand" for="c-39460655">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the license of the software?</div><br/></div></div><div id="39456047" class="c"><input type="checkbox" id="c-39456047" checked=""/><div class="controls bullet"><span class="by">Alifatisk</span><span>|</span><a href="#39455691">parent</a><span>|</span><a href="#39460655">prev</a><span>|</span><a href="#39454911">next</a><span>|</span><label class="collapse" for="c-39456047">[-]</label><label class="expand" for="c-39456047">[2 more]</label></div><br/><div class="children"><div class="content">I wish I could install it through chocolatey</div><br/><div id="39456142" class="c"><input type="checkbox" id="c-39456142" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39455691">root</a><span>|</span><a href="#39456047">parent</a><span>|</span><a href="#39454911">next</a><span>|</span><label class="collapse" for="c-39456142">[-]</label><label class="expand" for="c-39456142">[1 more]</label></div><br/><div class="children"><div class="content">Sure. I would love to add support for that. I had someone else asking for it too. Will be supporting it very soon.</div><br/></div></div></div></div></div></div><div id="39454911" class="c"><input type="checkbox" id="c-39454911" checked=""/><div class="controls bullet"><span class="by">dcchambers</span><span>|</span><a href="#39455691">prev</a><span>|</span><a href="#39462587">next</a><span>|</span><label class="collapse" for="c-39454911">[-]</label><label class="expand" for="c-39454911">[1 more]</label></div><br/><div class="children"><div class="content">Already available in Ollama v0.1.26 preview release, if you&#x27;d like to start playing with it locally:<p>- <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.1.26">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.1.26</a></div><br/></div></div><div id="39462587" class="c"><input type="checkbox" id="c-39462587" checked=""/><div class="controls bullet"><span class="by">Jonanin</span><span>|</span><a href="#39454911">prev</a><span>|</span><a href="#39464139">next</a><span>|</span><label class="collapse" for="c-39462587">[-]</label><label class="expand" for="c-39462587">[1 more]</label></div><br/><div class="children"><div class="content">It is surprising how willing Google is to stretch the truth in the marketing for its AI initiatives. Although it&#x27;s being compared against 7B models, Gemma &quot;7B&quot; is actually much more than 8B parameters in total.</div><br/></div></div><div id="39464139" class="c"><input type="checkbox" id="c-39464139" checked=""/><div class="controls bullet"><span class="by">greatNespresso</span><span>|</span><a href="#39462587">prev</a><span>|</span><a href="#39455233">next</a><span>|</span><label class="collapse" for="c-39464139">[-]</label><label class="expand" for="c-39464139">[1 more]</label></div><br/><div class="children"><div class="content">What caught my attention was the debugging tool, it looks new to me to provide such a utility, is it?</div><br/></div></div><div id="39455233" class="c"><input type="checkbox" id="c-39455233" checked=""/><div class="controls bullet"><span class="by">sqreept</span><span>|</span><a href="#39464139">prev</a><span>|</span><a href="#39460297">next</a><span>|</span><label class="collapse" for="c-39455233">[-]</label><label class="expand" for="c-39455233">[4 more]</label></div><br/><div class="children"><div class="content">Tried inference with the 7B model and without flash attention this is soooooo slow. With flash attention the fine-tunning requires A100 or H100.
Also the inference doesn&#x27;t always stop generating resulting in garbage being added to the response.</div><br/><div id="39455761" class="c"><input type="checkbox" id="c-39455761" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39455233">parent</a><span>|</span><a href="#39457333">next</a><span>|</span><label class="collapse" for="c-39455761">[-]</label><label class="expand" for="c-39455761">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also the inference doesn&#x27;t always stop generating resulting in garbage being added to the response.<p>That sounds like a chat format misconfiguration.<p>This could partially be Google&#x27;s fault, as they used <i>yet another</i> novel prompting format.<p>Also, for sane inference speed on H100s, you&#x27;ll have to wait for architecture support from the optimized frameworks. Vanilla transformers is beyond awful even with FA2.</div><br/></div></div><div id="39457333" class="c"><input type="checkbox" id="c-39457333" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#39455233">parent</a><span>|</span><a href="#39455761">prev</a><span>|</span><a href="#39460297">next</a><span>|</span><label class="collapse" for="c-39457333">[-]</label><label class="expand" for="c-39457333">[2 more]</label></div><br/><div class="children"><div class="content">We have implementations in different ML frameworks, so I am not quite sure which one you are referring to. Would you like to file a bug at the relevant GitHub repo?</div><br/><div id="39459298" class="c"><input type="checkbox" id="c-39459298" checked=""/><div class="controls bullet"><span class="by">sqreept</span><span>|</span><a href="#39455233">root</a><span>|</span><a href="#39457333">parent</a><span>|</span><a href="#39460297">next</a><span>|</span><label class="collapse" for="c-39459298">[-]</label><label class="expand" for="c-39459298">[1 more]</label></div><br/><div class="children"><div class="content">First of all, I&#x27;m using 2 x 4090 for testing. 4090 has 16384 CUDA cores which will become relevant a bit later.<p>I dug a bit deeper and it seems that with transformers==4.37.0 everything works fine with other HF hosted models (like Llama) but you&#x27;ll rightfully get this when trying to use Gemma:<p>ImportError: cannot import name &#x27;GemmaForCausalLM&#x27; from &#x27;transformers&#x27;<p>After installing transformers==4.38.0 the fine-tunning speed of Llama drops to 25% (?!?) of what used to be for a reason that I think HF should fix. Testing Gemma it seems I&#x27;m hitting a hardware limit as Gemma has a hidden size which is bigger than the available CUDA cores. This seems to make both inference &amp; fine-tunning about 25 times slower than similarly sized Llama 7B. I guess some operations have to be broken down in multiple round trips to the GPU due to my low CUDA core count.<p>All in all, even if HF fixes the recently introduced slowdown, Gemma seems to be fine-tuneable in reasonable amount of time only by the lucky ones with access to A100&#x2F;H100.<p>EDIT: I managed to hack my env to be able to run inference on Gemma with transformers==4.37.0 by keeping the necessary classes in loaded in RAM. It works about 4x faster but still very slow. And both the 7B and the 2B versions behave the same way.<p>EDIT2: I tried latest transformers from main branch (4.39.0.dev) and behaves the same as 4.38.0.</div><br/></div></div></div></div></div></div><div id="39460297" class="c"><input type="checkbox" id="c-39460297" checked=""/><div class="controls bullet"><span class="by">ofermend</span><span>|</span><a href="#39455233">prev</a><span>|</span><a href="#39453406">next</a><span>|</span><label class="collapse" for="c-39460297">[-]</label><label class="expand" for="c-39460297">[1 more]</label></div><br/><div class="children"><div class="content">Gemma-7B (instruction tuned version) is now on the Vectara HHEM leaderboard, with 100% answer rate and 7.5% hallucination rate. Pretty good for a model with 7B params.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;vectara&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;vectara&#x2F;leaderboard</a></div><br/></div></div><div id="39453406" class="c"><input type="checkbox" id="c-39453406" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39460297">prev</a><span>|</span><a href="#39453606">next</a><span>|</span><label class="collapse" for="c-39453406">[-]</label><label class="expand" for="c-39453406">[4 more]</label></div><br/><div class="children"><div class="content">Taking a page out of metas book with open models. I wonder what the game plan here is.<p>Nice that it allows commercial use!</div><br/><div id="39453778" class="c"><input type="checkbox" id="c-39453778" checked=""/><div class="controls bullet"><span class="by">gaogao</span><span>|</span><a href="#39453406">parent</a><span>|</span><a href="#39453606">next</a><span>|</span><label class="collapse" for="c-39453778">[-]</label><label class="expand" for="c-39453778">[3 more]</label></div><br/><div class="children"><div class="content">Mostly to boost research and commercial usage around JAX&#x2F;Gemini is my read.<p>Any internal research using Gemma is now more easily externally reproducible, external research and frameworks are easier to translate over, goodwill especially from researchers.</div><br/><div id="39453825" class="c"><input type="checkbox" id="c-39453825" checked=""/><div class="controls bullet"><span class="by">gaogao</span><span>|</span><a href="#39453406">root</a><span>|</span><a href="#39453778">parent</a><span>|</span><a href="#39453606">next</a><span>|</span><label class="collapse" for="c-39453825">[-]</label><label class="expand" for="c-39453825">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also less of a special sauce for text models itself these days with the propietary being more on the pre-training data and training stack (e.g. how to get 10k GPUs&#x2F;TPUs running together smoothly). Multi-modal models (or adjacent like Sora) are less likely to be open sourced in the immediate term.</div><br/><div id="39454863" class="c"><input type="checkbox" id="c-39454863" checked=""/><div class="controls bullet"><span class="by">smarterclayton</span><span>|</span><a href="#39453406">root</a><span>|</span><a href="#39453825">parent</a><span>|</span><a href="#39453606">next</a><span>|</span><label class="collapse" for="c-39454863">[-]</label><label class="expand" for="c-39454863">[1 more]</label></div><br/><div class="children"><div class="content">There is a lot of work to make the actual infrastructure and lower level management of lots and lots of GPUs&#x2F;TPUs open as well - my team focuses on making the infrastructure bit at least a bit more approachable on GKE and Kubernetes.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;ai-on-gke&#x2F;tree&#x2F;main">https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;ai-on-gke&#x2F;tree&#x2F;main</a><p>and<p><a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;xpk">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;xpk</a> (a bit more focused on HPC, but includes AI)<p>and<p><a href="https:&#x2F;&#x2F;github.com&#x2F;stas00&#x2F;ml-engineering">https:&#x2F;&#x2F;github.com&#x2F;stas00&#x2F;ml-engineering</a> (not associated with GKE, but describes training with SLURM)<p>The actual training is still a bit of a small pool of very experienced people, but it&#x27;s getting better.  And every day serving models gets that much faster - you can often simply draft on Triton and TensorRT-LLM or vLLM and see significant wins month to month.</div><br/></div></div></div></div></div></div></div></div><div id="39453606" class="c"><input type="checkbox" id="c-39453606" checked=""/><div class="controls bullet"><span class="by">nalzok</span><span>|</span><a href="#39453406">prev</a><span>|</span><a href="#39454766">next</a><span>|</span><label class="collapse" for="c-39453606">[-]</label><label class="expand" for="c-39453606">[7 more]</label></div><br/><div class="children"><div class="content">Congratulations on the release! How can we download the model and run inference locally?</div><br/><div id="39454414" class="c"><input type="checkbox" id="c-39454414" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#39453606">parent</a><span>|</span><a href="#39453775">next</a><span>|</span><label class="collapse" for="c-39454414">[-]</label><label class="expand" for="c-39454414">[3 more]</label></div><br/><div class="children"><div class="content">You can download the model checkpoints from kaggle <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma</a> and huggingface <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;gemma</a><p>Besides the python implementations, we also implemented a standalone C++ implementation that runs locally with just CPU simd <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp</a></div><br/><div id="39455424" class="c"><input type="checkbox" id="c-39455424" checked=""/><div class="controls bullet"><span class="by">tveita</span><span>|</span><a href="#39453606">root</a><span>|</span><a href="#39454414">parent</a><span>|</span><a href="#39453775">next</a><span>|</span><label class="collapse" for="c-39455424">[-]</label><label class="expand" for="c-39455424">[2 more]</label></div><br/><div class="children"><div class="content">Are there any cool highlights you can give us about gemma.cpp? Does it have any technical advantages over llama.cpp? It looks like it introduces its own quantization format, is there a speed or accuracy gain over llama.cpp&#x27;s 8-bit quantization?</div><br/><div id="39461439" class="c"><input type="checkbox" id="c-39461439" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#39453606">root</a><span>|</span><a href="#39455424">parent</a><span>|</span><a href="#39453775">next</a><span>|</span><label class="collapse" for="c-39461439">[-]</label><label class="expand" for="c-39461439">[1 more]</label></div><br/><div class="children"><div class="content">Hi, I devised the 4.5 (NUQ) and 8-bit (SFP) compression schemes. These are prototypes that enabled reasonable inference speed <i>without any fine-tuning</i>, and compression&#x2F;quantization running in a matter of seconds on a CPU.<p>We do not yet have full evals because the harness was added very recently, but observe that the non-uniform &#x27;4-bit&#x27; (plus tables, so 4.5) has twice the SNR of size-matched int4 with per-block scales.<p>One advantage that gemma.cpp offers is that the code is quite compact due to C++ and the single portable SIMD implementation (as opposed to SSE4, AVX2, NEON). We were able to integrate the new quantization quite easily, and further improvements are planned.</div><br/></div></div></div></div></div></div><div id="39453775" class="c"><input type="checkbox" id="c-39453775" checked=""/><div class="controls bullet"><span class="by">kathleenfromgdm</span><span>|</span><a href="#39453606">parent</a><span>|</span><a href="#39454414">prev</a><span>|</span><a href="#39454766">next</a><span>|</span><label class="collapse" for="c-39453775">[-]</label><label class="expand" for="c-39453775">[3 more]</label></div><br/><div class="children"><div class="content">Thank you! You can get started downloading the model and running inference on Kaggle: <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;models&#x2F;google&#x2F;gemma</a> ; for a full list of ways to interact with the model, you can check out <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma</a>.</div><br/><div id="39453970" class="c"><input type="checkbox" id="c-39453970" checked=""/><div class="controls bullet"><span class="by">aphit</span><span>|</span><a href="#39453606">root</a><span>|</span><a href="#39453775">parent</a><span>|</span><a href="#39454766">next</a><span>|</span><label class="collapse" for="c-39453970">[-]</label><label class="expand" for="c-39453970">[2 more]</label></div><br/><div class="children"><div class="content">FYI the ; broke the link, but I found it easily anyway.</div><br/><div id="39454086" class="c"><input type="checkbox" id="c-39454086" checked=""/><div class="controls bullet"><span class="by">kathleenfromgdm</span><span>|</span><a href="#39453606">root</a><span>|</span><a href="#39453970">parent</a><span>|</span><a href="#39454766">next</a><span>|</span><label class="collapse" for="c-39454086">[-]</label><label class="expand" for="c-39454086">[1 more]</label></div><br/><div class="children"><div class="content">Good catch - just corrected. Thanks!</div><br/></div></div></div></div></div></div></div></div><div id="39453497" class="c"><input type="checkbox" id="c-39453497" checked=""/><div class="controls bullet"><span class="by">Kelteseth</span><span>|</span><a href="#39454766">prev</a><span>|</span><a href="#39459490">next</a><span>|</span><label class="collapse" for="c-39453497">[-]</label><label class="expand" for="c-39453497">[2 more]</label></div><br/><div class="children"><div class="content">Can this run on my AMD Vega VII on Windows 11? As always, AMD is missing:<p>&gt; Optimization across multiple AI hardware platforms ensures industry-leading performance, including NVIDIA GPUs and Google Cloud TPUs.</div><br/><div id="39454360" class="c"><input type="checkbox" id="c-39454360" checked=""/><div class="controls bullet"><span class="by">lordswork</span><span>|</span><a href="#39453497">parent</a><span>|</span><a href="#39459490">next</a><span>|</span><label class="collapse" for="c-39454360">[-]</label><label class="expand" for="c-39454360">[1 more]</label></div><br/><div class="children"><div class="content">AMD Vega VII meets the memory requirements. Once tools like LM Studio, ollama, etc. add support for the model, you should be able to run locally like you would any other open weights model.</div><br/></div></div></div></div><div id="39459490" class="c"><input type="checkbox" id="c-39459490" checked=""/><div class="controls bullet"><span class="by">nojvek</span><span>|</span><a href="#39453497">prev</a><span>|</span><a href="#39460616">next</a><span>|</span><label class="collapse" for="c-39459490">[-]</label><label class="expand" for="c-39459490">[1 more]</label></div><br/><div class="children"><div class="content">I applaud the Google team openly engaging on HN here.<p>Q: how sure are you that the newer models trained from trillions of tokens - a huge chunk of open web, hasn&#x27;t been accidentally polluted by slurping test data?</div><br/></div></div><div id="39460616" class="c"><input type="checkbox" id="c-39460616" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#39459490">prev</a><span>|</span><a href="#39453577">next</a><span>|</span><label class="collapse" for="c-39460616">[-]</label><label class="expand" for="c-39460616">[1 more]</label></div><br/><div class="children"><div class="content">Is there any research on using smaller, lower capability models to act comparable to high quality models? Even if it&#x27;s just prompt engineering or doing lots of attempts to accomplish the task?<p>If somehow that is possible it means we only need a capable enough model and can use it reliably for lots of practical things.</div><br/></div></div><div id="39453577" class="c"><input type="checkbox" id="c-39453577" checked=""/><div class="controls bullet"><span class="by">vanderboyd</span><span>|</span><a href="#39460616">prev</a><span>|</span><a href="#39456761">next</a><span>|</span><label class="collapse" for="c-39453577">[-]</label><label class="expand" for="c-39453577">[1 more]</label></div><br/><div class="children"><div class="content">The 2B model seems underwhelming. For instance, compared to the recent StableLM2 1.6B model that is slightly smaller and probably wastes some &quot;English metric points&quot; by being multilingual.<p>The latter (and other similar open models) seem to do similarly well in benchmarks (much better in Math?) with way less fancy stuff. For instance, public data and no secretive filtering with pre trained models or synthetic data.<p>My take is that using the vanilla approaches take you <i>really</i> far. And many of the latest tricks and hours-of-work buy you little... Will be interesting to see how this plays out, especially for the open source community.</div><br/></div></div><div id="39456761" class="c"><input type="checkbox" id="c-39456761" checked=""/><div class="controls bullet"><span class="by">wouldbecouldbe</span><span>|</span><a href="#39453577">prev</a><span>|</span><a href="#39454435">next</a><span>|</span><label class="collapse" for="c-39456761">[-]</label><label class="expand" for="c-39456761">[6 more]</label></div><br/><div class="children"><div class="content">I really don&#x27;t get why there is this obsession with safe &quot;Responsible Generative AI&quot;.<p>I mean it writes some bad words, or bad pics, a human can do that without help as well.<p>The good thing about dangerous knowledge and generative AI is that you&#x27;re never sure haha, you&#x27;d be a fool to ask GPT to make a bomb. I mean it would probably be safe, since it will make up half of the steps.</div><br/><div id="39458975" class="c"><input type="checkbox" id="c-39458975" checked=""/><div class="controls bullet"><span class="by">pradn</span><span>|</span><a href="#39456761">parent</a><span>|</span><a href="#39457592">next</a><span>|</span><label class="collapse" for="c-39458975">[-]</label><label class="expand" for="c-39458975">[1 more]</label></div><br/><div class="children"><div class="content">Bias is a real problem, but more than that - an adversarial press and public won&#x27;t forgive massive brands like Google for making AIs that spit out racist answers.</div><br/></div></div><div id="39457678" class="c"><input type="checkbox" id="c-39457678" checked=""/><div class="controls bullet"><span class="by">myaccountonhn</span><span>|</span><a href="#39456761">parent</a><span>|</span><a href="#39457592">prev</a><span>|</span><a href="#39456784">next</a><span>|</span><label class="collapse" for="c-39457678">[-]</label><label class="expand" for="c-39457678">[2 more]</label></div><br/><div class="children"><div class="content">Because otherwise stuff like this happens, and you get (rightfully) upset customers:<p><a href="https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2018&#x2F;jan&#x2F;12&#x2F;google-racism-ban-gorilla-black-people" rel="nofollow">https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2018&#x2F;jan&#x2F;12&#x2F;google-ra...</a>  
<a href="https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-58462511" rel="nofollow">https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-58462511</a><p>Also people are using LLMs to learn (horrifying but reality), it would be unresponsible for them to let it to propagate negative stereotypes and biases.</div><br/><div id="39460376" class="c"><input type="checkbox" id="c-39460376" checked=""/><div class="controls bullet"><span class="by">wouldbecouldbe</span><span>|</span><a href="#39456761">root</a><span>|</span><a href="#39457678">parent</a><span>|</span><a href="#39456784">next</a><span>|</span><label class="collapse" for="c-39460376">[-]</label><label class="expand" for="c-39460376">[1 more]</label></div><br/><div class="children"><div class="content">But that&#x27;s exactly because it&#x27;s trying to be righteous.</div><br/></div></div></div></div><div id="39456784" class="c"><input type="checkbox" id="c-39456784" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39456761">parent</a><span>|</span><a href="#39457678">prev</a><span>|</span><a href="#39454435">next</a><span>|</span><label class="collapse" for="c-39456784">[-]</label><label class="expand" for="c-39456784">[1 more]</label></div><br/><div class="children"><div class="content">I guess what I&#x27;d tell you is, there&#x27;s a lot of fools in this world.</div><br/></div></div></div></div><div id="39455461" class="c"><input type="checkbox" id="c-39455461" checked=""/><div class="controls bullet"><span class="by">7moritz7</span><span>|</span><a href="#39454435">prev</a><span>|</span><a href="#39454944">next</a><span>|</span><label class="collapse" for="c-39455461">[-]</label><label class="expand" for="c-39455461">[1 more]</label></div><br/><div class="children"><div class="content">Thr landing page on ai.google.com seems to be machine translated, for Huggingface it uses the literal German translation (Umarmungen Gesicht)</div><br/></div></div><div id="39454944" class="c"><input type="checkbox" id="c-39454944" checked=""/><div class="controls bullet"><span class="by">jmu1234567890</span><span>|</span><a href="#39455461">prev</a><span>|</span><a href="#39453466">next</a><span>|</span><label class="collapse" for="c-39454944">[-]</label><label class="expand" for="c-39454944">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if people will get confused with the naming<p>Gemma, Gemini pro, Gemini advanced, Gemini ultra<p>To a layperson it is not obvious which one is better than the other</div><br/><div id="39455347" class="c"><input type="checkbox" id="c-39455347" checked=""/><div class="controls bullet"><span class="by">l33tman</span><span>|</span><a href="#39454944">parent</a><span>|</span><a href="#39455076">next</a><span>|</span><label class="collapse" for="c-39455347">[-]</label><label class="expand" for="c-39455347">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a layperson in this subject and I get confused. :)</div><br/></div></div><div id="39455076" class="c"><input type="checkbox" id="c-39455076" checked=""/><div class="controls bullet"><span class="by">knowriju</span><span>|</span><a href="#39454944">parent</a><span>|</span><a href="#39455347">prev</a><span>|</span><a href="#39456070">next</a><span>|</span><label class="collapse" for="c-39455076">[-]</label><label class="expand" for="c-39455076">[1 more]</label></div><br/><div class="children"><div class="content">I doubt Gemma is targeted for use by a layperson.</div><br/></div></div><div id="39456070" class="c"><input type="checkbox" id="c-39456070" checked=""/><div class="controls bullet"><span class="by">Alifatisk</span><span>|</span><a href="#39454944">parent</a><span>|</span><a href="#39455076">prev</a><span>|</span><a href="#39453466">next</a><span>|</span><label class="collapse" for="c-39456070">[-]</label><label class="expand" for="c-39456070">[1 more]</label></div><br/><div class="children"><div class="content">Gemini advanced = Gemini ultra</div><br/></div></div></div></div><div id="39453466" class="c"><input type="checkbox" id="c-39453466" checked=""/><div class="controls bullet"><span class="by">sidcool</span><span>|</span><a href="#39454944">prev</a><span>|</span><label class="collapse" for="c-39453466">[-]</label><label class="expand" for="c-39453466">[8 more]</label></div><br/><div class="children"><div class="content">Available on Ollama?</div><br/><div id="39454279" class="c"><input type="checkbox" id="c-39454279" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39453466">parent</a><span>|</span><a href="#39454958">next</a><span>|</span><label class="collapse" for="c-39454279">[-]</label><label class="expand" for="c-39454279">[1 more]</label></div><br/><div class="children"><div class="content">Support for gemma in llama.cpp just got merged, so it may take some time (could be hours or days) until this lands in ollama<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;5631">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;5631</a></div><br/></div></div><div id="39454958" class="c"><input type="checkbox" id="c-39454958" checked=""/><div class="controls bullet"><span class="by">dcchambers</span><span>|</span><a href="#39453466">parent</a><span>|</span><a href="#39454279">prev</a><span>|</span><a href="#39455784">next</a><span>|</span><label class="collapse" for="c-39454958">[-]</label><label class="expand" for="c-39454958">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s now in the 0.1.26 pre-release: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.1.26">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;releases&#x2F;tag&#x2F;v0.1.26</a></div><br/></div></div><div id="39455784" class="c"><input type="checkbox" id="c-39455784" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39453466">parent</a><span>|</span><a href="#39454958">prev</a><span>|</span><a href="#39453795">next</a><span>|</span><label class="collapse" for="c-39455784">[-]</label><label class="expand" for="c-39455784">[1 more]</label></div><br/><div class="children"><div class="content">Available in pre-release now which means you’d have to update manually in future.</div><br/></div></div><div id="39453795" class="c"><input type="checkbox" id="c-39453795" checked=""/><div class="controls bullet"><span class="by">blooalien</span><span>|</span><a href="#39453466">parent</a><span>|</span><a href="#39455784">prev</a><span>|</span><label class="collapse" for="c-39453795">[-]</label><label class="expand" for="c-39453795">[4 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;ollama.com&#x2F;library?q=gemma">https:&#x2F;&#x2F;ollama.com&#x2F;library?q=gemma</a><p>Library search says &quot;Nope&quot;.  At least not yet.</div><br/><div id="39454922" class="c"><input type="checkbox" id="c-39454922" checked=""/><div class="controls bullet"><span class="by">kevsim</span><span>|</span><a href="#39453466">root</a><span>|</span><a href="#39453795">parent</a><span>|</span><a href="#39454917">next</a><span>|</span><label class="collapse" for="c-39454922">[-]</label><label class="expand" for="c-39454922">[2 more]</label></div><br/><div class="children"><div class="content">And now it says &quot;Yup&quot;. That was pretty quick!</div><br/><div id="39459958" class="c"><input type="checkbox" id="c-39459958" checked=""/><div class="controls bullet"><span class="by">blooalien</span><span>|</span><a href="#39453466">root</a><span>|</span><a href="#39454922">parent</a><span>|</span><a href="#39454917">next</a><span>|</span><label class="collapse" for="c-39459958">[-]</label><label class="expand" for="c-39459958">[1 more]</label></div><br/><div class="children"><div class="content">Dang, that was <i>really</i> quick!  According to the listed time of your reply vs. mine, less than an hour from the time I checked?  Quick turnaround indeed.<p>Already been pulled from there over 3,700 times since then, too (as of the time of <i>this</i> reply mere hours later).  Seems like quite a bit more&#x27;n a few Ollama users were &quot;waitin&#x27; with bated breath&quot; for that one to drop.  :grin:</div><br/></div></div></div></div><div id="39454917" class="c"><input type="checkbox" id="c-39454917" checked=""/><div class="controls bullet"><span class="by">tomd</span><span>|</span><a href="#39453466">root</a><span>|</span><a href="#39453795">parent</a><span>|</span><a href="#39454922">prev</a><span>|</span><label class="collapse" for="c-39454917">[-]</label><label class="expand" for="c-39454917">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s there now</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>