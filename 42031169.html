<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730710881540" as="style"/><link rel="stylesheet" href="styles.css?v=1730710881540"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cloud.google.com/blog/products/networking/speed-scale-reliability-25-years-of-data-center-networking">Speed, scale and reliability: 25 years of Google datacenter networking evolution</a>Â <span class="domain">(<a href="https://cloud.google.com">cloud.google.com</a>)</span></div><div class="subtext"><span>sandwichsphinx</span> | <span>83 comments</span></div><br/><div><div id="42032298" class="c"><input type="checkbox" id="c-42032298" checked=""/><div class="controls bullet"><span class="by">cletus</span><span>|</span><a href="#42031636">next</a><span>|</span><label class="collapse" for="c-42032298">[-]</label><label class="expand" for="c-42032298">[7 more]</label></div><br/><div class="children"><div class="content">This mentions Jupiter generations, which I think is about 10-15 years old at this point. It doesn&#x27;t really talk about what existed before so it&#x27;s not really 25 years of history here. I want to say &quot;Watchtower&quot; was before Jupiter? but honestly it&#x27;s been about a decade since I read anything about it.<p>Google&#x27;s DC networking is interesting because of how deeply integrated it is into the entire software stack. Click on some of the links and you&#x27;ll see it mentions SDN (Software Defined Network). This is so Borg instances can talk to each other within the same service at high throughput and low latency. 8-10 years ago this was (IIRC) 40Gbps connections. It&#x27;s probably 100Gbps now but that&#x27;s just a guess.<p>But the networking is also integrated into global services like traffic management to handle, say, DDoS attacks.<p>Anyway, from reading this it doesn&#x27;t sound like Google is abandoning their custom TPU silicon (ie it talks about the upcoming A3 Ultra and Trillium). So where does NVidia ConnectX fit in? AFAICT that&#x27;s just the NIC they&#x27;re plugging into Jupiter. That&#x27;s probably what enables (or will enable) 100Gbps connections between servers. Yes, 100GbE optical NICs have existed for a long time. I would assume that NVidia produce better ones in terms of price, performance, size, power usage and&#x2F;or heat produced.<p>Disclaimer: Xoogler. I didn&#x27;t work in networking though.</div><br/><div id="42036074" class="c"><input type="checkbox" id="c-42036074" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#42032298">parent</a><span>|</span><a href="#42033580">next</a><span>|</span><label class="collapse" for="c-42036074">[-]</label><label class="expand" for="c-42036074">[2 more]</label></div><br/><div class="children"><div class="content">The past few years there has been a weird situation where Google and AWS have had worse GPU&#x27;s than smaller providers like Coreweave + Lambda Labs. This is because they didn&#x27;t want to buy into Nvidias proprietary Infiniband stack for GPU-GPU networking, and instead wanted to make it work on top of their ethernet (but still pretty proprietary) stack.<p>The outcome was really bad GPU-GPU latency &amp; bandwidth between machines. My understanding is ConnectX is Nvidias supported (and probably still very profitable) way for these hyperscalers to use their proprietary networks without buying Infiniband switches and without paying the latency cost of moving bytes from the GPU to the CPU.</div><br/><div id="42036166" class="c"><input type="checkbox" id="c-42036166" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42032298">root</a><span>|</span><a href="#42036074">parent</a><span>|</span><a href="#42033580">next</a><span>|</span><label class="collapse" for="c-42036166">[-]</label><label class="expand" for="c-42036166">[1 more]</label></div><br/><div class="children"><div class="content">Your understanding is correct. Part of the other issue is that at one point, there was a huge shortage of availability of IB switches... lead times of 1+ years... another solution had to be found.<p>RoCE is IB over Ethernet. All the underlying documentation and settings to put this stuff together are the same. It doesn&#x27;t require ConnectX NIC&#x27;s though. We do the same with 8x Broadcom Thor 2 NIC&#x27;s (into a Broadcom Tomahawk 5 based Dell Z9864F switch) for our own 400G cluster.</div><br/></div></div></div></div><div id="42033580" class="c"><input type="checkbox" id="c-42033580" checked=""/><div class="controls bullet"><span class="by">neomantra</span><span>|</span><a href="#42032298">parent</a><span>|</span><a href="#42036074">prev</a><span>|</span><a href="#42032842">next</a><span>|</span><label class="collapse" for="c-42033580">[-]</label><label class="expand" for="c-42033580">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia got ConnectX from their Mellanox acquisition -- they were experts in RMDA, particularly with Infiniband but eventually pushing Ethernet (RoCE).  These NICs have hardware-acceleration of RDMA.   Over the RDMA fabric, GPUs can communicate with each other without much CPU usage (the &quot;GPU-to-GPU&quot; mentioned in the article).<p>[I know nothing about Jupiter, and little about RDMA in practice, but used ConnectX for VMA, its hardware-accelerated, kernel-bypass tech.]</div><br/></div></div><div id="42032842" class="c"><input type="checkbox" id="c-42032842" checked=""/><div class="controls bullet"><span class="by">virtuallynathan</span><span>|</span><a href="#42032298">parent</a><span>|</span><a href="#42033580">prev</a><span>|</span><a href="#42033830">next</a><span>|</span><label class="collapse" for="c-42032842">[-]</label><label class="expand" for="c-42032842">[1 more]</label></div><br/><div class="children"><div class="content">This latest revision of Jupiter is apparently 400G, as is the ConnectX-7, A3 Ultra will have 8 of them!</div><br/></div></div><div id="42033830" class="c"><input type="checkbox" id="c-42033830" checked=""/><div class="controls bullet"><span class="by">ceph_</span><span>|</span><a href="#42032298">parent</a><span>|</span><a href="#42032842">prev</a><span>|</span><a href="#42034803">next</a><span>|</span><label class="collapse" for="c-42033830">[-]</label><label class="expand" for="c-42033830">[1 more]</label></div><br/><div class="children"><div class="content">From memory: Firehose &gt; Watchtower &gt; WCC &gt; SCC &gt; Jupiter v1</div><br/></div></div><div id="42034803" class="c"><input type="checkbox" id="c-42034803" checked=""/><div class="controls bullet"><span class="by">CBLT</span><span>|</span><a href="#42032298">parent</a><span>|</span><a href="#42033830">prev</a><span>|</span><a href="#42031636">next</a><span>|</span><label class="collapse" for="c-42034803">[-]</label><label class="expand" for="c-42034803">[1 more]</label></div><br/><div class="children"><div class="content">I would guess the Nvidia ConnectX is part of a secondary networking plane, not plugged into Jupiter.  Current-gen Google NICs are custom hardware with a _lot_ of Google-specific functionality, such as running the borglet on the NIC to free up all CPU cores for guests.</div><br/></div></div></div></div><div id="42031636" class="c"><input type="checkbox" id="c-42031636" checked=""/><div class="controls bullet"><span class="by">alex_young</span><span>|</span><a href="#42032298">prev</a><span>|</span><a href="#42031796">next</a><span>|</span><label class="collapse" for="c-42031636">[-]</label><label class="expand" for="c-42031636">[3 more]</label></div><br/><div class="children"><div class="content">Like most discussions of the last 25 years, this one starts 9 years ago. Good times.</div><br/><div id="42032305" class="c"><input type="checkbox" id="c-42032305" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42031636">parent</a><span>|</span><a href="#42031945">prev</a><span>|</span><a href="#42031796">next</a><span>|</span><label class="collapse" for="c-42032305">[-]</label><label class="expand" for="c-42032305">[1 more]</label></div><br/><div class="children"><div class="content">The Further Resources section goes a bit further back.</div><br/></div></div></div></div><div id="42031796" class="c"><input type="checkbox" id="c-42031796" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#42031636">prev</a><span>|</span><a href="#42032481">next</a><span>|</span><label class="collapse" for="c-42031796">[-]</label><label class="expand" for="c-42031796">[22 more]</label></div><br/><div class="children"><div class="content">It seems all cutting edge datacenters like x.ai Colossus are using Nvidia networking. Now Google is upgrading to Nvidia networking, too.<p>Since Nvidia owns most of the Gpgpu products, they have top notch networking and interconnect, I wonder if they don&#x27;t have a plan to own all datacenter hardware in the future. Maybe they plan to also release CPUs, motherboards, storage and whatever else is needed.</div><br/><div id="42031949" class="c"><input type="checkbox" id="c-42031949" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42031796">parent</a><span>|</span><a href="#42032160">next</a><span>|</span><label class="collapse" for="c-42031949">[-]</label><label class="expand" for="c-42031949">[3 more]</label></div><br/><div class="children"><div class="content">I read this slightly differently, that specific machine types with Nvidia GPU hardware also have Nvidia networking for tying together those GPUs.<p>Google has its own TPUs and donât really use GPUs except to sell them to end customers on cloud I think. So using Nvidia networking for Nvidia GPUs across many machines on cloud is really just a reflection of what external customers want to buy.<p>Disclaimer, I work at Google but have no non public info about this.</div><br/><div id="42034347" class="c"><input type="checkbox" id="c-42034347" checked=""/><div class="controls bullet"><span class="by">dmacedo</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42031949">parent</a><span>|</span><a href="#42032160">next</a><span>|</span><label class="collapse" for="c-42034347">[-]</label><label class="expand" for="c-42034347">[2 more]</label></div><br/><div class="children"><div class="content">Having just worked with some of the Thread folks at M&amp;S, thought I&#x27;d reach out and say hello. Seems like it was an awesome team! (=</div><br/><div id="42036306" class="c"><input type="checkbox" id="c-42036306" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42034347">parent</a><span>|</span><a href="#42032160">next</a><span>|</span><label class="collapse" for="c-42036306">[-]</label><label class="expand" for="c-42036306">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re lucky to be working with them, an amazing team.</div><br/></div></div></div></div></div></div><div id="42032160" class="c"><input type="checkbox" id="c-42032160" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#42031796">parent</a><span>|</span><a href="#42031949">prev</a><span>|</span><a href="#42036763">next</a><span>|</span><label class="collapse" for="c-42032160">[-]</label><label class="expand" for="c-42032160">[5 more]</label></div><br/><div class="children"><div class="content">Nvidia networking is what used to be called Mellanox networking, which was already dominant in datacenters.</div><br/><div id="42033111" class="c"><input type="checkbox" id="c-42033111" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42032160">parent</a><span>|</span><a href="#42036763">next</a><span>|</span><label class="collapse" for="c-42033111">[-]</label><label class="expand" for="c-42033111">[4 more]</label></div><br/><div class="children"><div class="content">Only within supercomputers (including the smaller GPU ones used to train AI). Normal data centers use Cisco or Juniper or similarly.well known Ethernet equipment, and they still do. The Mellanox&#x2F;Nvidia Infiniband networks are specifically used for supercomputer-like clusters.</div><br/><div id="42033379" class="c"><input type="checkbox" id="c-42033379" checked=""/><div class="controls bullet"><span class="by">wbl</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42033111">parent</a><span>|</span><a href="#42037008">next</a><span>|</span><label class="collapse" for="c-42033379">[-]</label><label class="expand" for="c-42033379">[1 more]</label></div><br/><div class="children"><div class="content">Mellanox Ethernet NIC got used a bunch of places due to better programmability.</div><br/></div></div><div id="42037008" class="c"><input type="checkbox" id="c-42037008" checked=""/><div class="controls bullet"><span class="by">crmd</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42033111">parent</a><span>|</span><a href="#42033379">prev</a><span>|</span><a href="#42036372">next</a><span>|</span><label class="collapse" for="c-42037008">[-]</label><label class="expand" for="c-42037008">[1 more]</label></div><br/><div class="children"><div class="content">Mellanox IB were ubiquitous in storage networking. None of the storage systems I worked on would have been possible without mellanox tech.</div><br/></div></div><div id="42036372" class="c"><input type="checkbox" id="c-42036372" checked=""/><div class="controls bullet"><span class="by">dafugg</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42033111">parent</a><span>|</span><a href="#42037008">prev</a><span>|</span><a href="#42036763">next</a><span>|</span><label class="collapse" for="c-42036372">[-]</label><label class="expand" for="c-42036372">[1 more]</label></div><br/><div class="children"><div class="content">You seem to have a narrow definition of ânormalâ for datacenters. Meta were using OCP mellanox NICs for common hardware platforms a decade ago and still are.</div><br/></div></div></div></div></div></div><div id="42036763" class="c"><input type="checkbox" id="c-42036763" checked=""/><div class="controls bullet"><span class="by">jonas21</span><span>|</span><a href="#42031796">parent</a><span>|</span><a href="#42032160">prev</a><span>|</span><a href="#42031852">next</a><span>|</span><label class="collapse" for="c-42036763">[-]</label><label class="expand" for="c-42036763">[1 more]</label></div><br/><div class="children"><div class="content">I believe this is what they plan on doing. See, for example:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;Y2F8yisiS6E?si=GbyzzIG8w-mtS7s-&amp;t=1598" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;Y2F8yisiS6E?si=GbyzzIG8w-mtS7s-...</a></div><br/></div></div><div id="42031852" class="c"><input type="checkbox" id="c-42031852" checked=""/><div class="controls bullet"><span class="by">Kab1r</span><span>|</span><a href="#42031796">parent</a><span>|</span><a href="#42036763">prev</a><span>|</span><a href="#42033390">next</a><span>|</span><label class="collapse" for="c-42031852">[-]</label><label class="expand" for="c-42031852">[1 more]</label></div><br/><div class="children"><div class="content">Grace Hopper already includes Arm based CPUs (and reference motherboards)</div><br/></div></div><div id="42033390" class="c"><input type="checkbox" id="c-42033390" checked=""/><div class="controls bullet"><span class="by">mikeyouse</span><span>|</span><a href="#42031796">parent</a><span>|</span><a href="#42031852">prev</a><span>|</span><a href="#42031906">next</a><span>|</span><label class="collapse" for="c-42033390">[-]</label><label class="expand" for="c-42033390">[4 more]</label></div><br/><div class="children"><div class="content">Yeah thereâs a bit of industry worry about that very eventuality â hence the ultra Ethernet consortium trying to work on open source alternatives to the mellanox&#x2F;nvidia lock-in.<p><a href="https:&#x2F;&#x2F;ultraethernet.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ultraethernet.org&#x2F;</a></div><br/><div id="42036376" class="c"><input type="checkbox" id="c-42036376" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42033390">parent</a><span>|</span><a href="#42031906">next</a><span>|</span><label class="collapse" for="c-42036376">[-]</label><label class="expand" for="c-42036376">[3 more]</label></div><br/><div class="children"><div class="content">Interesting Nvidia is on the steering committee</div><br/><div id="42037984" class="c"><input type="checkbox" id="c-42037984" checked=""/><div class="controls bullet"><span class="by">jclulow</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42036376">parent</a><span>|</span><a href="#42031906">next</a><span>|</span><label class="collapse" for="c-42037984">[-]</label><label class="expand" for="c-42037984">[2 more]</label></div><br/><div class="children"><div class="content">Cisco have sat on the steering committees for a lot of things where they had a proprietary initial version of something.  It&#x27;s not that unusual, and also, it&#x27;s often frankly not actually that open; e.g., see the rent seeking racket for access to PCI documentation, or USB-IF actively seeking to prevent open source hardware from existing, etc.</div><br/><div id="42038564" class="c"><input type="checkbox" id="c-42038564" checked=""/><div class="controls bullet"><span class="by">mikeyouse</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42037984">parent</a><span>|</span><a href="#42031906">next</a><span>|</span><label class="collapse" for="c-42038564">[-]</label><label class="expand" for="c-42038564">[1 more]</label></div><br/><div class="children"><div class="content">Eh, the UEC effort is a standards org through the Linux Foundation so it won&#x27;t be subject to any of the usual chicanery.  And actually, it looks like Nvidia is jsut a general member and not one of the Steering Committee members;<p><a href="https:&#x2F;&#x2F;ultraethernet.org&#x2F;wp-content&#x2F;uploads&#x2F;sites&#x2F;20&#x2F;2023&#x2F;09&#x2F;23.08.10-UEC-Overview-Presentation-FINAL-2.pdf" rel="nofollow">https:&#x2F;&#x2F;ultraethernet.org&#x2F;wp-content&#x2F;uploads&#x2F;sites&#x2F;20&#x2F;2023&#x2F;0...</a></div><br/></div></div></div></div></div></div></div></div><div id="42031906" class="c"><input type="checkbox" id="c-42031906" checked=""/><div class="controls bullet"><span class="by">timenova</span><span>|</span><a href="#42031796">parent</a><span>|</span><a href="#42033390">prev</a><span>|</span><a href="#42032189">next</a><span>|</span><label class="collapse" for="c-42031906">[-]</label><label class="expand" for="c-42031906">[1 more]</label></div><br/><div class="children"><div class="content">That was their plan with trying to buy ARM...</div><br/></div></div><div id="42032189" class="c"><input type="checkbox" id="c-42032189" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42031796">parent</a><span>|</span><a href="#42031906">prev</a><span>|</span><a href="#42032481">next</a><span>|</span><label class="collapse" for="c-42032189">[-]</label><label class="expand" for="c-42032189">[6 more]</label></div><br/><div class="children"><div class="content">I have to wonder if Nvidia has reached a point where it hesitates to develop new products because it would hurt their margins. Sure they could probably release a profitable networking product but if they did their net margins would decrease even as profit increased. This may actually hurt their market cap as investors absolutely love high margins.</div><br/><div id="42032303" class="c"><input type="checkbox" id="c-42032303" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42032189">parent</a><span>|</span><a href="#42032481">next</a><span>|</span><label class="collapse" for="c-42032303">[-]</label><label class="expand" for="c-42032303">[5 more]</label></div><br/><div class="children"><div class="content">They can always release capital back to investors, and then those investors can put the money into different companies that eg produce networking equipment.</div><br/><div id="42032417" class="c"><input type="checkbox" id="c-42032417" checked=""/><div class="controls bullet"><span class="by">thrw42A8N</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42032303">parent</a><span>|</span><a href="#42032481">next</a><span>|</span><label class="collapse" for="c-42032417">[-]</label><label class="expand" for="c-42032417">[4 more]</label></div><br/><div class="children"><div class="content">Why would they release money if they can invest it and return much more?</div><br/><div id="42038876" class="c"><input type="checkbox" id="c-42038876" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42032417">parent</a><span>|</span><a href="#42032481">next</a><span>|</span><label class="collapse" for="c-42038876">[-]</label><label class="expand" for="c-42038876">[3 more]</label></div><br/><div class="children"><div class="content">I was working under HDThoreaun&#x27;s assumption that the margins would be lower.<p>If they have other opportunities for investment with higher margins, they should seize those, of course.  And perhaps even call up investors for more capital, if required.</div><br/><div id="42039002" class="c"><input type="checkbox" id="c-42039002" checked=""/><div class="controls bullet"><span class="by">lovich</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42038876">parent</a><span>|</span><a href="#42032481">next</a><span>|</span><label class="collapse" for="c-42039002">[-]</label><label class="expand" for="c-42039002">[2 more]</label></div><br/><div class="children"><div class="content">When the agents employed by investors would be harmed by releasing capital back, which is guaranteed since so many peopleâs compensation is in the form of stock and returning capital leads to decreasing stock value, why would those agents ever return the capital voluntarily?</div><br/><div id="42039629" class="c"><input type="checkbox" id="c-42039629" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#42031796">root</a><span>|</span><a href="#42039002">parent</a><span>|</span><a href="#42032481">next</a><span>|</span><label class="collapse" for="c-42039629">[-]</label><label class="expand" for="c-42039629">[1 more]</label></div><br/><div class="children"><div class="content">Have you heard of stock buybacks?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42032481" class="c"><input type="checkbox" id="c-42032481" checked=""/><div class="controls bullet"><span class="by">maz1b</span><span>|</span><a href="#42031796">prev</a><span>|</span><a href="#42031705">next</a><span>|</span><label class="collapse" for="c-42032481">[-]</label><label class="expand" for="c-42032481">[2 more]</label></div><br/><div class="children"><div class="content">Pretty crazy. Supporting 1.5mbps video calls for each human on earth? Did I read that right?<p>Just goes to show how drastic and extraordinary levels of scale can be.</div><br/><div id="42036684" class="c"><input type="checkbox" id="c-42036684" checked=""/><div class="controls bullet"><span class="by">sethammons</span><span>|</span><a href="#42032481">parent</a><span>|</span><a href="#42031705">next</a><span>|</span><label class="collapse" for="c-42036684">[-]</label><label class="expand" for="c-42036684">[1 more]</label></div><br/><div class="children"><div class="content">Scale means different things to different people</div><br/></div></div></div></div><div id="42031705" class="c"><input type="checkbox" id="c-42031705" checked=""/><div class="controls bullet"><span class="by">jerzmacow</span><span>|</span><a href="#42032481">prev</a><span>|</span><a href="#42033171">next</a><span>|</span><label class="collapse" for="c-42031705">[-]</label><label class="expand" for="c-42031705">[3 more]</label></div><br/><div class="children"><div class="content">Wow and it doesn&#x27;t open with a picture of their lego server? Wasn&#x27;t that their first one, 25 years ago?</div><br/><div id="42031768" class="c"><input type="checkbox" id="c-42031768" checked=""/><div class="controls bullet"><span class="by">teractiveodular</span><span>|</span><a href="#42031705">parent</a><span>|</span><a href="#42033171">next</a><span>|</span><label class="collapse" for="c-42031768">[-]</label><label class="expand" for="c-42031768">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a marketing piece, they don&#x27;t particularly want to emphasize the hacky early days for an audience of Serious Enterprise Customers.</div><br/></div></div></div></div><div id="42033171" class="c"><input type="checkbox" id="c-42033171" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42031705">prev</a><span>|</span><a href="#42033000">next</a><span>|</span><label class="collapse" for="c-42033171">[-]</label><label class="expand" for="c-42033171">[1 more]</label></div><br/><div class="children"><div class="content">They managed to double from 6 Petabit per second in 2022 to 13 Pbps in 2023. I assume with ConnectX-8 this could be 26 Pbps in 2025&#x2F;26. The ConnextX-8 is PCI-e 6 so I assume we could get 1.6Tbps ConnextX-9 with PCI-e 7.0 which is not far away.<p>Cant wait to see the FreeBSD Netflix version of that post.<p>This also goes back to how increasing throughput is relatively easy and has a very strong roadmap. While increasing storage is difficult. I notice YouTube has been serving higher bitrate video in recent years with H.264. Instead of storing yet another copy of video files in VP9 or AV1 unless they are 2K+.</div><br/></div></div><div id="42033000" class="c"><input type="checkbox" id="c-42033000" checked=""/><div class="controls bullet"><span class="by">dangoodmanUT</span><span>|</span><a href="#42033171">prev</a><span>|</span><a href="#42032506">next</a><span>|</span><label class="collapse" for="c-42033000">[-]</label><label class="expand" for="c-42033000">[2 more]</label></div><br/><div class="children"><div class="content">Does gcp have the worst networking for gpu training though?</div><br/><div id="42033424" class="c"><input type="checkbox" id="c-42033424" checked=""/><div class="controls bullet"><span class="by">dweekly</span><span>|</span><a href="#42033000">parent</a><span>|</span><a href="#42032506">next</a><span>|</span><label class="collapse" for="c-42033424">[-]</label><label class="expand" for="c-42033424">[1 more]</label></div><br/><div class="children"><div class="content">For TPU pods they use 3D torus topology with multi-terabit cross connects. For GPU, A3 Ultra instances offer &quot;non-blocking 3.2 Tbps per server of GPU-to-GPU traffic over RoCE&quot;.<p>Is that the worst for training? Namely: do superior solutions exist?</div><br/></div></div></div></div><div id="42034964" class="c"><input type="checkbox" id="c-42034964" checked=""/><div class="controls bullet"><span class="by">reaperducer</span><span>|</span><a href="#42032075">prev</a><span>|</span><a href="#42032535">next</a><span>|</span><label class="collapse" for="c-42034964">[-]</label><label class="expand" for="c-42034964">[3 more]</label></div><br/><div class="children"><div class="content"><i>Speed, scale and reliability</i><p>Choose any two.</div><br/><div id="42038774" class="c"><input type="checkbox" id="c-42038774" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#42034964">parent</a><span>|</span><a href="#42035078">next</a><span>|</span><label class="collapse" for="c-42038774">[-]</label><label class="expand" for="c-42038774">[1 more]</label></div><br/><div class="children"><div class="content">In any decision making matrix you need a constraint that get consumed (economics, size, etc) to force a &quot;choose any two&quot; type situation.<p>You absolutely can have speed, scale and reliability. You can&#x27;t have speed, scale, reliability and low cost.</div><br/></div></div><div id="42035078" class="c"><input type="checkbox" id="c-42035078" checked=""/><div class="controls bullet"><span class="by">teractiveodular</span><span>|</span><a href="#42034964">parent</a><span>|</span><a href="#42038774">prev</a><span>|</span><a href="#42032535">next</a><span>|</span><label class="collapse" for="c-42035078">[-]</label><label class="expand" for="c-42035078">[1 more]</label></div><br/><div class="children"><div class="content">Which of those is Google&#x27;s network missing?</div><br/></div></div></div></div><div id="42032535" class="c"><input type="checkbox" id="c-42032535" checked=""/><div class="controls bullet"><span class="by">486sx33</span><span>|</span><a href="#42034964">prev</a><span>|</span><a href="#42032556">next</a><span>|</span><label class="collapse" for="c-42032535">[-]</label><label class="expand" for="c-42032535">[1 more]</label></div><br/><div class="children"><div class="content">The most amazing surveillance machine ever â¦</div><br/></div></div><div id="42032556" class="c"><input type="checkbox" id="c-42032556" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#42032535">prev</a><span>|</span><label class="collapse" for="c-42032556">[-]</label><label class="expand" for="c-42032556">[30 more]</label></div><br/><div class="children"><div class="content">Awesome Google... Now learn what an availability zone is and stop creating them with firewalls across the same data center.<p>Oh and make your data centers smaller. Not so big they can be seen in Google Maps. Because otherwise, you will be unable to move those whale sized workloads to an alternative.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;mDNHK-SzXEM?t=564" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;mDNHK-SzXEM?t=564</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35713001">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35713001</a><p>&quot;Unmasking Google Cloud: How to Determine if a Region Supports Physical Zone Separation&quot; - <a href="https:&#x2F;&#x2F;cagataygurturk.medium.com&#x2F;unmasking-google-cloud-how-to-determine-if-a-region-supports-physical-zone-separation-1a7bfbb3d280" rel="nofollow">https:&#x2F;&#x2F;cagataygurturk.medium.com&#x2F;unmasking-google-cloud-how...</a></div><br/><div id="42038795" class="c"><input type="checkbox" id="c-42038795" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#42032556">parent</a><span>|</span><a href="#42032825">next</a><span>|</span><label class="collapse" for="c-42038795">[-]</label><label class="expand" for="c-42038795">[2 more]</label></div><br/><div class="children"><div class="content">To address the availability point of your comment, Google&#x27;s terminology is slightly different to AWS.<p>On GCP it sounds like you want to have a multi <i>region</i> architecture, not multi-zone (if you want firewalls outside the same data center).<p>&gt; Resources that live in a zone, such as virtual machine instances or zonal persistent disks, are referred to as zonal resources. Other resources, like static external IP addresses, are regional. Regional resources can be used by any resource in that region, regardless of zone, while zonal resources can only be used by other resources in the same zone.<p><a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;regions-zones" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;regions-zones</a><p>(No affiliation with Google, just had a similar confusion at one point)</div><br/><div id="42038817" class="c"><input type="checkbox" id="c-42038817" checked=""/><div class="controls bullet"><span class="by">erik_seaberg</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42038795">parent</a><span>|</span><a href="#42032825">next</a><span>|</span><label class="collapse" for="c-42038817">[-]</label><label class="expand" for="c-42038817">[1 more]</label></div><br/><div class="children"><div class="content">You also need to go multi-region with AWS. I liked their AZ story but in practice it hasn&#x27;t avoided multi-zone outages (maybe deploys?)</div><br/></div></div></div></div><div id="42032825" class="c"><input type="checkbox" id="c-42032825" checked=""/><div class="controls bullet"><span class="by">tecleandor</span><span>|</span><a href="#42032556">parent</a><span>|</span><a href="#42038795">prev</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42032825">[-]</label><label class="expand" for="c-42032825">[12 more]</label></div><br/><div class="children"><div class="content">Making a datacenter not visible from Google Maps, at least on most big cities where Google zones are deployed, would mean making them smaller than a car. Or even smaller than a dishwasher.<p>If I check London (where europe-west2 is kinda located) on Google Maps right now, I can easily discern manhole covers or people. If I check Jakarta (Asia-southeast2) things smaller than a car get confusing, but you can definitely see them.</div><br/><div id="42034659" class="c"><input type="checkbox" id="c-42034659" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42032825">parent</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42034659">[-]</label><label class="expand" for="c-42034659">[11 more]</label></div><br/><div class="children"><div class="content">Your comment does not address the essence of the point I was trying to make. If you have a monstrous data-center, instead of many smaller, in relative size, you are putting too many eggs on a giant basket.</div><br/><div id="42035164" class="c"><input type="checkbox" id="c-42035164" checked=""/><div class="controls bullet"><span class="by">joshuamorton</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42034659">parent</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42035164">[-]</label><label class="expand" for="c-42035164">[10 more]</label></div><br/><div class="children"><div class="content">What if you have dozens of big data centers?</div><br/><div id="42036268" class="c"><input type="checkbox" id="c-42036268" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42035164">parent</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42036268">[-]</label><label class="expand" for="c-42036268">[9 more]</label></div><br/><div class="children"><div class="content">To reinforce your point:<p>The scale of cloud data centres reflects the scale of their customer base, not the size of the basket for each individual customer.<p>Larger data centres actually improve availability through several mechanisms: more power components such as generators means the failure of any one is just a few percent instead of a total blackout. You can also partition core infrastructure like routers and power rails into more fault domains and update domains.<p>Some large clouds have two update domains and five fault domains <i>on top of</i> three zones that are more than 10km apart. You canât beat ~30 individual partitions with your data centres at a reasonable cost!</div><br/><div id="42036397" class="c"><input type="checkbox" id="c-42036397" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42036268">parent</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42036397">[-]</label><label class="expand" for="c-42036397">[8 more]</label></div><br/><div class="children"><div class="content">I provided three different references. Despite the massive downvotes on my comment I guess by Google engineers, as a troll...:-)I take comfort on the fact nobody was able to advance a reference to prove me wrong.</div><br/><div id="42038803" class="c"><input type="checkbox" id="c-42038803" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42036397">parent</a><span>|</span><a href="#42036522">next</a><span>|</span><label class="collapse" for="c-42038803">[-]</label><label class="expand" for="c-42038803">[1 more]</label></div><br/><div class="children"><div class="content">AWS Zone is sort-roughly-kinda a GCP Region. It sounds like you want multi-region: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;regions-zones" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;compute&#x2F;docs&#x2F;regions-zones</a></div><br/></div></div><div id="42036522" class="c"><input type="checkbox" id="c-42036522" checked=""/><div class="controls bullet"><span class="by">joshuamorton</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42036397">parent</a><span>|</span><a href="#42038803">prev</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42036522">[-]</label><label class="expand" for="c-42036522">[6 more]</label></div><br/><div class="children"><div class="content">You haven&#x27;t actually made an argument.<p>It is true that the nomenclature &quot;AWS Availability Zone&quot; has a different meaning than &quot;GCP Zone&quot; when discussing the physical separation between zones within the same region.<p>It&#x27;s unclear why this is inherently a bad thing, as long as them same overall level of reliability is achieved.</div><br/><div id="42036595" class="c"><input type="checkbox" id="c-42036595" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42036522">parent</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42036595">[-]</label><label class="expand" for="c-42036595">[5 more]</label></div><br/><div class="children"><div class="content">The phrase &quot;as long as the same overall level of reliability is achieved&quot; is logically flawed when discussing physically co-located vs. geographically separated infrastructure.</div><br/><div id="42036704" class="c"><input type="checkbox" id="c-42036704" checked=""/><div class="controls bullet"><span class="by">joshuamorton</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42036595">parent</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42036704">[-]</label><label class="expand" for="c-42036704">[4 more]</label></div><br/><div class="children"><div class="content">Justify that claim.<p>In my experience, the set of issues that would affect 2 buildings close to each other, but not two buildings a mile apart, is vanishingly small, usually <i>just</i> last mile fiber cuts or power issues (which are rare and mitigated by having multiple independent providers), as well as issues like building fires (which are exceedingly rare, we know of, perhaps two of notable impact in more than a decade across the big three cloud providers).<p>Everything else is done at the zone level no matter what (onsite repair work, rollouts, upgrades, control plane changes, etc.) or can impact an entire region (non-last mile fiber or power cuts, inclement weather, regional power starvation, etc.)<p>There is a potential gain from physical zone isolation, but it protects against a relatively small set of issues. Is it really better to invest in that, or to invest the resources in other safety improvements?</div><br/><div id="42037361" class="c"><input type="checkbox" id="c-42037361" checked=""/><div class="controls bullet"><span class="by">anewplace</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42036704">parent</a><span>|</span><a href="#42037118">next</a><span>|</span><label class="collapse" for="c-42037361">[-]</label><label class="expand" for="c-42037361">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re undermining the seriousness of a physical event like a fire. Even if the likelihood of these things is &quot;vanishingly small&quot;, the impact is so large that it more than offsets it. Taking the OVH data center fire as an example, multiple companies completely lost their data and are effectively dead now. When you&#x27;re talking about a company-ending-event, many people would consider even just two examples per decade as a completely unacceptable failure rate. And it&#x27;s more than just fires: we&#x27;re also talking about tornados, floods, hurricanes, terrorist attacks, etc.<p>Google even recognizes this, and suggests that for disaster recovery planning, you should use multiple regions. AWS on the other hand does acknowledge some use cases for multiple regions (mostly performance or data sovereignty), but maintains the stance that if your only concern is DR, then a single region should be enough for the vast majority of workloads.<p>There&#x27;s more to the story though, of course. GCP makes it easier to use multiple regions, including things like dual-region storage buckets, or just making more regions available for use. For example GCP has ~3 times as many regions in the US as AWS does (although each region is comparatively smaller). I&#x27;m not sure if there&#x27;s consensus on which is the &quot;right&quot; way to do it. They both have pros and cons.</div><br/></div></div><div id="42037118" class="c"><input type="checkbox" id="c-42037118" checked=""/><div class="controls bullet"><span class="by">retinaros</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42036704">parent</a><span>|</span><a href="#42037361">prev</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42037118">[-]</label><label class="expand" for="c-42037118">[2 more]</label></div><br/><div class="children"><div class="content">what happened in gcp paris region then?</div><br/><div id="42037252" class="c"><input type="checkbox" id="c-42037252" checked=""/><div class="controls bullet"><span class="by">joshuamorton</span><span>|</span><a href="#42032556">root</a><span>|</span><a href="#42037118">parent</a><span>|</span><a href="#42033856">next</a><span>|</span><label class="collapse" for="c-42037252">[-]</label><label class="expand" for="c-42037252">[1 more]</label></div><br/><div class="children"><div class="content">One of the vanishingly small set of issues I mentioned.<p>It is true, and obvious, that GCP and AWS and Azure use different architectures. It does not obviously follow that any of those architectures are inherently more reliable. And even if it did, it doesn&#x27;t obviously follow that any of the platforms are inherently more reliable due to a specific architectural decision.<p>Like, all cloud providers still have regional outages.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>