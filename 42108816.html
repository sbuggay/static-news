<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731402069680" as="style"/><link rel="stylesheet" href="styles.css?v=1731402069680"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://zanussbaum.substack.com/p/optimizing-a-webgpu-matmul-kernel">Optimizing a WebGPU Matmul Kernel for 1 TFLOP</a> <span class="domain">(<a href="https://zanussbaum.substack.com">zanussbaum.substack.com</a>)</span></div><div class="subtext"><span>zanussbaum</span> | <span>50 comments</span></div><br/><div><div id="42109780" class="c"><input type="checkbox" id="c-42109780" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#42110257">next</a><span>|</span><label class="collapse" for="c-42109780">[-]</label><label class="expand" for="c-42109780">[11 more]</label></div><br/><div class="children"><div class="content">Great article!<p>For context: this WebGPU version achieves ~17% of peak theoretical performance of M2. With CUDA (i.e. CuBLAS), you can reach ~75% of peak performance for same matrix config (without tensor core).</div><br/><div id="42112716" class="c"><input type="checkbox" id="c-42112716" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#42109780">parent</a><span>|</span><a href="#42111822">next</a><span>|</span><label class="collapse" for="c-42112716">[-]</label><label class="expand" for="c-42112716">[1 more]</label></div><br/><div class="children"><div class="content">how are you running CUDA on the integrated Apple silicon GPU these days?</div><br/></div></div><div id="42111822" class="c"><input type="checkbox" id="c-42111822" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#42109780">parent</a><span>|</span><a href="#42112716">prev</a><span>|</span><a href="#42110127">next</a><span>|</span><label class="collapse" for="c-42111822">[-]</label><label class="expand" for="c-42111822">[2 more]</label></div><br/><div class="children"><div class="content">&gt; you can reach ~75% of peak performance for same matrix config<p>Not on the same computer, CUDA doesn’t run on the integrated GPU of the Apple M2 Pro.</div><br/><div id="42113581" class="c"><input type="checkbox" id="c-42113581" checked=""/><div class="controls bullet"><span class="by">ladyanita22</span><span>|</span><a href="#42109780">root</a><span>|</span><a href="#42111822">parent</a><span>|</span><a href="#42110127">next</a><span>|</span><label class="collapse" for="c-42113581">[-]</label><label class="expand" for="c-42113581">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly what I was wondering. That cannot be.</div><br/></div></div></div></div><div id="42110127" class="c"><input type="checkbox" id="c-42110127" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#42109780">parent</a><span>|</span><a href="#42111822">prev</a><span>|</span><a href="#42109791">next</a><span>|</span><label class="collapse" for="c-42110127">[-]</label><label class="expand" for="c-42110127">[5 more]</label></div><br/><div class="children"><div class="content">75% can&#x27;t be the best we can do. What would reach 100% or nearly 100%? Handcoded assembly?</div><br/><div id="42110154" class="c"><input type="checkbox" id="c-42110154" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42109780">root</a><span>|</span><a href="#42110127">parent</a><span>|</span><a href="#42110408">next</a><span>|</span><label class="collapse" for="c-42110154">[-]</label><label class="expand" for="c-42110154">[2 more]</label></div><br/><div class="children"><div class="content">With GPUs it&#x27;s not uncommon to run out of memory bandwidth before you max out the theoretical FLOPS. They may have a ton of bandwidth but it&#x27;s never enough.<p>That can lead you to some pretty counter-intuitive optimizations because it&#x27;s often faster to do <i>more</i> compute work if it means you touch less memory in the process.</div><br/><div id="42112071" class="c"><input type="checkbox" id="c-42112071" checked=""/><div class="controls bullet"><span class="by">wbl</span><span>|</span><a href="#42109780">root</a><span>|</span><a href="#42110154">parent</a><span>|</span><a href="#42110408">next</a><span>|</span><label class="collapse" for="c-42112071">[-]</label><label class="expand" for="c-42112071">[1 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t the roofline inform capacity assessments?</div><br/></div></div></div></div><div id="42110408" class="c"><input type="checkbox" id="c-42110408" checked=""/><div class="controls bullet"><span class="by">david-gpu</span><span>|</span><a href="#42109780">root</a><span>|</span><a href="#42110127">parent</a><span>|</span><a href="#42110154">prev</a><span>|</span><a href="#42109791">next</a><span>|</span><label class="collapse" for="c-42110408">[-]</label><label class="expand" for="c-42110408">[2 more]</label></div><br/><div class="children"><div class="content">The parameters of the matrix multiply, such as the size of the matrices, impose some limits to how close you can get to the peak theoretical performance in a particular GPU. Not all possible matrix multiplies are equally valuable to optimize <i>a priori</i>, so the hardware is designed to perform best on problems that are financially significant, such as modern LLMs.<p>As for handcoded assembly, do you believe that it would be financially sound to hand code and maintain thousands of kernels that way, even if you believed that they would be faster?</div><br/><div id="42113587" class="c"><input type="checkbox" id="c-42113587" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42109780">root</a><span>|</span><a href="#42110408">parent</a><span>|</span><a href="#42109791">next</a><span>|</span><label class="collapse" for="c-42113587">[-]</label><label class="expand" for="c-42113587">[1 more]</label></div><br/><div class="children"><div class="content">&gt; As for handcoded assembly, do you believe that it would be financially sound to hand code and maintain thousands of kernels that way, even if you believed that they would be faster?<p>Why not? We do so for cryptographic primitives and video codecs. And why are you talking about “thousands of kernels”? AI programs only need a small amount of different kernels so it doesn&#x27;t sound intractable.</div><br/></div></div></div></div></div></div><div id="42109791" class="c"><input type="checkbox" id="c-42109791" checked=""/><div class="controls bullet"><span class="by">zanussbaum</span><span>|</span><a href="#42109780">parent</a><span>|</span><a href="#42110127">prev</a><span>|</span><a href="#42110348">next</a><span>|</span><label class="collapse" for="c-42109791">[-]</label><label class="expand" for="c-42109791">[1 more]</label></div><br/><div class="children"><div class="content">thanks! and yes definitely not at CUDA levels :)</div><br/></div></div></div></div><div id="42110257" class="c"><input type="checkbox" id="c-42110257" checked=""/><div class="controls bullet"><span class="by">inglor</span><span>|</span><a href="#42109780">prev</a><span>|</span><a href="#42111476">next</a><span>|</span><label class="collapse" for="c-42110257">[-]</label><label class="expand" for="c-42110257">[7 more]</label></div><br/><div class="children"><div class="content">Can you explain why you did the naive algorithm here and not any of the fast matrix multiplication ones that trade multiplications for more additions? Just for educational purposes or is there a performance benefit in the technique?</div><br/><div id="42112836" class="c"><input type="checkbox" id="c-42112836" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42110257">parent</a><span>|</span><a href="#42110803">next</a><span>|</span><label class="collapse" for="c-42112836">[-]</label><label class="expand" for="c-42112836">[1 more]</label></div><br/><div class="children"><div class="content">Because those algorithms are generally not worth implementing even though their algorithmic complexity is theoretically lower.</div><br/></div></div><div id="42110803" class="c"><input type="checkbox" id="c-42110803" checked=""/><div class="controls bullet"><span class="by">zanussbaum</span><span>|</span><a href="#42110257">parent</a><span>|</span><a href="#42112836">prev</a><span>|</span><a href="#42110302">next</a><span>|</span><label class="collapse" for="c-42110803">[-]</label><label class="expand" for="c-42110803">[4 more]</label></div><br/><div class="children"><div class="content">at least on my m2, the compiled kernel ends up using fast math anyways so using WGSL&#x27;s fma didn&#x27;t change anything about the actual kernel that gets run</div><br/><div id="42111509" class="c"><input type="checkbox" id="c-42111509" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#42110257">root</a><span>|</span><a href="#42110803">parent</a><span>|</span><a href="#42110302">next</a><span>|</span><label class="collapse" for="c-42111509">[-]</label><label class="expand" for="c-42111509">[3 more]</label></div><br/><div class="children"><div class="content">inglor is probably referring to Strassen or Coppersmith–Winograd.</div><br/><div id="42112079" class="c"><input type="checkbox" id="c-42112079" checked=""/><div class="controls bullet"><span class="by">wbl</span><span>|</span><a href="#42110257">root</a><span>|</span><a href="#42111509">parent</a><span>|</span><a href="#42111667">next</a><span>|</span><label class="collapse" for="c-42112079">[-]</label><label class="expand" for="c-42112079">[1 more]</label></div><br/><div class="children"><div class="content">Last I checked the extra mems really hurt on a lot of cases especially for the more complex ones, but I&#x27;m no expert.</div><br/></div></div><div id="42111667" class="c"><input type="checkbox" id="c-42111667" checked=""/><div class="controls bullet"><span class="by">zanussbaum</span><span>|</span><a href="#42110257">root</a><span>|</span><a href="#42111509">parent</a><span>|</span><a href="#42112079">prev</a><span>|</span><a href="#42110302">next</a><span>|</span><label class="collapse" for="c-42111667">[-]</label><label class="expand" for="c-42111667">[1 more]</label></div><br/><div class="children"><div class="content">oh in that case it was because i didn&#x27;t know about them :) something to try next!</div><br/></div></div></div></div></div></div></div></div><div id="42111476" class="c"><input type="checkbox" id="c-42111476" checked=""/><div class="controls bullet"><span class="by">FL33TW00D</span><span>|</span><a href="#42110257">prev</a><span>|</span><a href="#42112232">next</a><span>|</span><label class="collapse" for="c-42111476">[-]</label><label class="expand" for="c-42111476">[2 more]</label></div><br/><div class="children"><div class="content">I wrote something similar a while back: <a href="https:&#x2F;&#x2F;github.com&#x2F;FL33TW00D&#x2F;wgpu-mm">https:&#x2F;&#x2F;github.com&#x2F;FL33TW00D&#x2F;wgpu-mm</a><p>Also does quantized matmuls.</div><br/><div id="42112602" class="c"><input type="checkbox" id="c-42112602" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#42111476">parent</a><span>|</span><a href="#42112232">next</a><span>|</span><label class="collapse" for="c-42112602">[-]</label><label class="expand" for="c-42112602">[1 more]</label></div><br/><div class="children"><div class="content">would be fun to do a leaderboard of some specific size (e.g. 4096x4096x4096) just to get all the code and tricks in one spot for folks to learn about things</div><br/></div></div></div></div><div id="42112232" class="c"><input type="checkbox" id="c-42112232" checked=""/><div class="controls bullet"><span class="by">mkeeter</span><span>|</span><a href="#42111476">prev</a><span>|</span><a href="#42110103">next</a><span>|</span><label class="collapse" for="c-42112232">[-]</label><label class="expand" for="c-42112232">[4 more]</label></div><br/><div class="children"><div class="content">For a very deep dive into the subject, this is a great writeup:<p>How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance
(<a href="https:&#x2F;&#x2F;siboehm.com&#x2F;articles&#x2F;22&#x2F;CUDA-MMM" rel="nofollow">https:&#x2F;&#x2F;siboehm.com&#x2F;articles&#x2F;22&#x2F;CUDA-MMM</a>)<p>(It&#x27;s CUDA-specific, so there may be aspects that can&#x27;t yet be ported to WGPU)</div><br/><div id="42112340" class="c"><input type="checkbox" id="c-42112340" checked=""/><div class="controls bullet"><span class="by">zanussbaum</span><span>|</span><a href="#42112232">parent</a><span>|</span><a href="#42112906">next</a><span>|</span><label class="collapse" for="c-42112340">[-]</label><label class="expand" for="c-42112340">[1 more]</label></div><br/><div class="children"><div class="content">this was a huge inspiration for the post! i tried to highlight it in the blog but it might have gotten buried<p>there are a few things that i wasn&#x27;t able to figure out how to get access to&#x2F;i wasn&#x27;t sure if they were possible. for example, a lot of Simon&#x27;s article takes advantage of the warp scheduler and warp tiling.<p>i had a hard time finding information on if that&#x27;s even possible with my M2&#x2F;metal and the general memory access patterns. it seems like CUDA does have better documentation in this regard</div><br/></div></div><div id="42112906" class="c"><input type="checkbox" id="c-42112906" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#42112232">parent</a><span>|</span><a href="#42112340">prev</a><span>|</span><a href="#42110103">next</a><span>|</span><label class="collapse" for="c-42112906">[-]</label><label class="expand" for="c-42112906">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a nice tutorial but just to be clear: that is not a deep dive in any sense. It&#x27;s just the bog standard tricks. It doesn&#x27;t cover MMA and WMMA, which today is table stakes for fast matmul. Also doesn&#x27;t cover software pipelining. It&#x27;s basically a good summary of the basics.</div><br/><div id="42113469" class="c"><input type="checkbox" id="c-42113469" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42112232">root</a><span>|</span><a href="#42112906">parent</a><span>|</span><a href="#42110103">next</a><span>|</span><label class="collapse" for="c-42113469">[-]</label><label class="expand" for="c-42113469">[1 more]</label></div><br/><div class="children"><div class="content">It’s a deep dive as of like 2015 probably. I don’t know if anyone has done something similar for modern GEMMs. Maybe the CUTLASS or Colfax people?</div><br/></div></div></div></div></div></div><div id="42110103" class="c"><input type="checkbox" id="c-42110103" checked=""/><div class="controls bullet"><span class="by">coffeeaddict1</span><span>|</span><a href="#42112232">prev</a><span>|</span><a href="#42111367">next</a><span>|</span><label class="collapse" for="c-42110103">[-]</label><label class="expand" for="c-42110103">[1 more]</label></div><br/><div class="children"><div class="content">You can do slightly better fairly easily I think. See here for example <a href="https:&#x2F;&#x2F;github.com&#x2F;AnswerDotAI&#x2F;gpu.cpp&#x2F;pull&#x2F;35">https:&#x2F;&#x2F;github.com&#x2F;AnswerDotAI&#x2F;gpu.cpp&#x2F;pull&#x2F;35</a></div><br/></div></div><div id="42111367" class="c"><input type="checkbox" id="c-42111367" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42110103">prev</a><span>|</span><a href="#42109582">next</a><span>|</span><label class="collapse" for="c-42111367">[-]</label><label class="expand" for="c-42111367">[6 more]</label></div><br/><div class="children"><div class="content">To clarify the title: TFLOP&#x2F;s is the unit the author goes after, not TFLOP.  People in the threads compare CUDA performance on GPUs to WebAssembly performance: please recall that H100 has a theoretical performance of about 1000 TFLOP&#x2F;s for bfloat16, and even moderately complicated algorithms in typical modern transformer architectures can reach about half of that performance.</div><br/><div id="42112845" class="c"><input type="checkbox" id="c-42112845" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42111367">parent</a><span>|</span><a href="#42109582">next</a><span>|</span><label class="collapse" for="c-42112845">[-]</label><label class="expand" for="c-42112845">[5 more]</label></div><br/><div class="children"><div class="content">H100 can do well over 1500 TFLOPS in fp16.</div><br/><div id="42113463" class="c"><input type="checkbox" id="c-42113463" checked=""/><div class="controls bullet"><span class="by">nulltype</span><span>|</span><a href="#42111367">root</a><span>|</span><a href="#42112845">parent</a><span>|</span><a href="#42109582">next</a><span>|</span><label class="collapse" for="c-42113463">[-]</label><label class="expand" for="c-42113463">[4 more]</label></div><br/><div class="children"><div class="content">Which H100 and how much over 1500 TFLOP&#x2F;s?<p>The datasheet for the H100 SXM seems to indicate that it can only do ~1000 TFLOP&#x2F;s peak.</div><br/><div id="42113478" class="c"><input type="checkbox" id="c-42113478" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42111367">root</a><span>|</span><a href="#42113463">parent</a><span>|</span><a href="#42109582">next</a><span>|</span><label class="collapse" for="c-42113478">[-]</label><label class="expand" for="c-42113478">[3 more]</label></div><br/><div class="children"><div class="content">I just went to Nvidia’s site and downloaded the data sheet: <a href="https:&#x2F;&#x2F;resources.nvidia.com&#x2F;en-us-tensor-core&#x2F;nvidia-tensor-core-gpu-datasheet" rel="nofollow">https:&#x2F;&#x2F;resources.nvidia.com&#x2F;en-us-tensor-core&#x2F;nvidia-tensor...</a>. It says 1600&#x2F;1900 in half precision?</div><br/><div id="42113656" class="c"><input type="checkbox" id="c-42113656" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42111367">root</a><span>|</span><a href="#42113478">parent</a><span>|</span><a href="#42109582">next</a><span>|</span><label class="collapse" for="c-42113656">[-]</label><label class="expand" for="c-42113656">[2 more]</label></div><br/><div class="children"><div class="content">Read the fine print: &quot;With sparsity&quot;. They double the claimed throughput by assuming that half of the FLOPs can be skipped.</div><br/><div id="42113693" class="c"><input type="checkbox" id="c-42113693" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42111367">root</a><span>|</span><a href="#42113656">parent</a><span>|</span><a href="#42109582">next</a><span>|</span><label class="collapse" for="c-42113693">[-]</label><label class="expand" for="c-42113693">[1 more]</label></div><br/><div class="children"><div class="content">Oh, that is really annoying. Thanks for catching that!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42109582" class="c"><input type="checkbox" id="c-42109582" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#42111367">prev</a><span>|</span><a href="#42109405">next</a><span>|</span><label class="collapse" for="c-42109582">[-]</label><label class="expand" for="c-42109582">[4 more]</label></div><br/><div class="children"><div class="content">Couple years ago, I wanted about the same thing in HLSL language, for a Direct3D 11.0 compute shader. Here’s the fastest version I managed to make back then: <a href="https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;blob&#x2F;master&#x2F;Mistral&#x2F;MistralShaders&#x2F;mulMatTiled.hlsl">https:&#x2F;&#x2F;github.com&#x2F;Const-me&#x2F;Cgml&#x2F;blob&#x2F;master&#x2F;Mistral&#x2F;Mistral...</a><p>As you see, I have implemented 32×32 tiling, using thread groups of 32×8 threads, two groupshared buffers to load tiles of the input matrices, and I accumulate numbers into local variables, 32 &#x2F; 8 = 4 accumulators per thread.</div><br/></div></div><div id="42109405" class="c"><input type="checkbox" id="c-42109405" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#42109582">prev</a><span>|</span><a href="#42109848">next</a><span>|</span><label class="collapse" for="c-42109405">[-]</label><label class="expand" for="c-42109405">[7 more]</label></div><br/><div class="children"><div class="content">WebGPU doesn&#x27;t seem to talk about bank conflict, hiding some hardware details that might be necessary to write the best kernel. will it be able to match the perf of Cuda on the same hardware?</div><br/><div id="42109680" class="c"><input type="checkbox" id="c-42109680" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#42109405">parent</a><span>|</span><a href="#42109512">next</a><span>|</span><label class="collapse" for="c-42109680">[-]</label><label class="expand" for="c-42109680">[5 more]</label></div><br/><div class="children"><div class="content">WebGPU cannot even come close unfortunately since they don&#x27;t have support for hardware specific memory or warp-level primitives (like TMA or tensorcores).  it&#x27;s not like it gets 80% of perf, it gets &lt; 30% of the peak perf for anything related to heavy compute matrix multiplications</div><br/><div id="42109758" class="c"><input type="checkbox" id="c-42109758" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#42109405">root</a><span>|</span><a href="#42109680">parent</a><span>|</span><a href="#42109788">next</a><span>|</span><label class="collapse" for="c-42109758">[-]</label><label class="expand" for="c-42109758">[2 more]</label></div><br/><div class="children"><div class="content">&gt; don&#x27;t have support for hardware specific memory<p>I have no experience with WebGPU but if you mean group shared memory, I think the support is available. See the demo: <a href="https:&#x2F;&#x2F;compute.toys&#x2F;view&#x2F;25" rel="nofollow">https:&#x2F;&#x2F;compute.toys&#x2F;view&#x2F;25</a></div><br/><div id="42109784" class="c"><input type="checkbox" id="c-42109784" checked=""/><div class="controls bullet"><span class="by">zanussbaum</span><span>|</span><a href="#42109405">root</a><span>|</span><a href="#42109758">parent</a><span>|</span><a href="#42109788">next</a><span>|</span><label class="collapse" for="c-42109784">[-]</label><label class="expand" for="c-42109784">[1 more]</label></div><br/><div class="children"><div class="content">i tried using workgroup shared memory and found it slower than just recomputing everything in each thread although i may have been doing something dumb<p>i&#x27;m excited to try subgroups though: <a href="https:&#x2F;&#x2F;developer.chrome.com&#x2F;blog&#x2F;new-in-webgpu-128#experimenting_with_subgroups" rel="nofollow">https:&#x2F;&#x2F;developer.chrome.com&#x2F;blog&#x2F;new-in-webgpu-128#experime...</a></div><br/></div></div></div></div><div id="42109788" class="c"><input type="checkbox" id="c-42109788" checked=""/><div class="controls bullet"><span class="by">kayvr</span><span>|</span><a href="#42109405">root</a><span>|</span><a href="#42109680">parent</a><span>|</span><a href="#42109758">prev</a><span>|</span><a href="#42109757">next</a><span>|</span><label class="collapse" for="c-42109788">[-]</label><label class="expand" for="c-42109788">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard the WebGPU workgroup wants to close the gap on tensor core support.</div><br/></div></div><div id="42109757" class="c"><input type="checkbox" id="c-42109757" checked=""/><div class="controls bullet"><span class="by">zanussbaum</span><span>|</span><a href="#42109405">root</a><span>|</span><a href="#42109680">parent</a><span>|</span><a href="#42109788">prev</a><span>|</span><a href="#42109512">next</a><span>|</span><label class="collapse" for="c-42109757">[-]</label><label class="expand" for="c-42109757">[1 more]</label></div><br/><div class="children"><div class="content">you&#x27;re definitely right, 80% was a bit of an overestimation, especially with respect to CUDA<p>it would be cool to see if there&#x27;s some way to get better access to those lower-level primitives but would be surprised<p>it does seem like subgroup support are a step in the right direction though!</div><br/></div></div></div></div><div id="42109512" class="c"><input type="checkbox" id="c-42109512" checked=""/><div class="controls bullet"><span class="by">zanussbaum</span><span>|</span><a href="#42109405">parent</a><span>|</span><a href="#42109680">prev</a><span>|</span><a href="#42109848">next</a><span>|</span><label class="collapse" for="c-42109512">[-]</label><label class="expand" for="c-42109512">[1 more]</label></div><br/><div class="children"><div class="content">great question, to me webGPU sits a hair high level than CUDA or Vulkan. so you don&#x27;t have the exact same level of control but can get to 80% performance of it without having to write different kernels specific to the hardware</div><br/></div></div></div></div><div id="42109848" class="c"><input type="checkbox" id="c-42109848" checked=""/><div class="controls bullet"><span class="by">maelito</span><span>|</span><a href="#42109405">prev</a><span>|</span><a href="#42112605">next</a><span>|</span><label class="collapse" for="c-42109848">[-]</label><label class="expand" for="c-42109848">[6 more]</label></div><br/><div class="children"><div class="content">WebGPU will make Web maps even more competitive than they are already.<p>The smoothness of an iPhone map zoom, on any device.</div><br/><div id="42109916" class="c"><input type="checkbox" id="c-42109916" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42109848">parent</a><span>|</span><a href="#42112605">next</a><span>|</span><label class="collapse" for="c-42109916">[-]</label><label class="expand" for="c-42109916">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The smoothness of an iPhone map zoom, on any device.<p>Any device except an iPhone, until Apple finally gets around to shipping WebGPU in Safari. Any year now...</div><br/><div id="42110252" class="c"><input type="checkbox" id="c-42110252" checked=""/><div class="controls bullet"><span class="by">astlouis44</span><span>|</span><a href="#42109848">root</a><span>|</span><a href="#42109916">parent</a><span>|</span><a href="#42112605">next</a><span>|</span><label class="collapse" for="c-42110252">[-]</label><label class="expand" for="c-42110252">[4 more]</label></div><br/><div class="children"><div class="content">Safari is officially enabling support for WebGPU in iOS 18.2, which is rolling out within the first weeks of December.</div><br/><div id="42110336" class="c"><input type="checkbox" id="c-42110336" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42109848">root</a><span>|</span><a href="#42110252">parent</a><span>|</span><a href="#42110648">next</a><span>|</span><label class="collapse" for="c-42110336">[-]</label><label class="expand" for="c-42110336">[2 more]</label></div><br/><div class="children"><div class="content">Where&#x27;d you hear that? It&#x27;s not listed here:<p><a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;safari-release-notes&#x2F;safari-18_2-release-notes" rel="nofollow">https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;safari-release-not...</a></div><br/><div id="42111158" class="c"><input type="checkbox" id="c-42111158" checked=""/><div class="controls bullet"><span class="by">astlouis44</span><span>|</span><a href="#42109848">root</a><span>|</span><a href="#42110336">parent</a><span>|</span><a href="#42110648">next</a><span>|</span><label class="collapse" for="c-42111158">[-]</label><label class="expand" for="c-42111158">[1 more]</label></div><br/><div class="children"><div class="content">Source is here, from a Unity WebGPU thread. Look at the comment from October 27 from Brendan Duncan, a Unity employee: <a href="https:&#x2F;&#x2F;discussions.unity.com&#x2F;t&#x2F;early-access-to-the-new-webgpu-backend&#x2F;933493&#x2F;390" rel="nofollow">https:&#x2F;&#x2F;discussions.unity.com&#x2F;t&#x2F;early-access-to-the-new-webg...</a><p>&quot;I have found that WebGPU is enabled by default now with iOS 18.2.
Apple has been working in the open on WebGPU. The WebKit source code has their latest WebGPU work in it. What hasn’t been known is their release schedule, but now with 18.2 it’s looking very promising that it will be on by default in that version.&quot;</div><br/></div></div></div></div><div id="42110648" class="c"><input type="checkbox" id="c-42110648" checked=""/><div class="controls bullet"><span class="by">luketaylor</span><span>|</span><a href="#42109848">root</a><span>|</span><a href="#42110252">parent</a><span>|</span><a href="#42110336">prev</a><span>|</span><a href="#42112605">next</a><span>|</span><label class="collapse" for="c-42110648">[-]</label><label class="expand" for="c-42110648">[1 more]</label></div><br/><div class="children"><div class="content">Source?<p>Edit: I just pressed “Reset All to Defaults” under “WebKit Feature Flags” on my device running 18.2 beta, and the switch for WebGPU is on!! &lt;3</div><br/></div></div></div></div></div></div></div></div><div id="42112605" class="c"><input type="checkbox" id="c-42112605" checked=""/><div class="controls bullet"><span class="by">jsbsjwbw</span><span>|</span><a href="#42109848">prev</a><span>|</span><label class="collapse" for="c-42112605">[-]</label><label class="expand" for="c-42112605">[1 more]</label></div><br/><div class="children"><div class="content">Kkkk</div><br/></div></div></div></div></div></div></div></body></html>