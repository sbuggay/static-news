<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704445253794" as="style"/><link rel="stylesheet" href="styles.css?v=1704445253794"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html">Fastest autograd in the West</a> <span class="domain">(<a href="https://arogozhnikov.github.io">arogozhnikov.github.io</a>)</span></div><div class="subtext"><span>tplrbv</span> | <span>24 comments</span></div><br/><div><div id="38875359" class="c"><input type="checkbox" id="c-38875359" checked=""/><div class="controls bullet"><span class="by">stealthcat</span><span>|</span><a href="#38876652">next</a><span>|</span><label class="collapse" for="c-38875359">[-]</label><label class="expand" for="c-38875359">[1 more]</label></div><br/><div class="children"><div class="content">Fun exercise… but for such use cases, people use Casadi and DrJit<p><a href="https:&#x2F;&#x2F;github.com&#x2F;casadi&#x2F;casadi">https:&#x2F;&#x2F;github.com&#x2F;casadi&#x2F;casadi</a>
<a href="https:&#x2F;&#x2F;github.com&#x2F;mitsuba-renderer&#x2F;drjit">https:&#x2F;&#x2F;github.com&#x2F;mitsuba-renderer&#x2F;drjit</a><p>DrJit is made by same author of pybind11 and nanobind.</div><br/></div></div><div id="38876444" class="c"><input type="checkbox" id="c-38876444" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#38876652">prev</a><span>|</span><a href="#38875458">next</a><span>|</span><label class="collapse" for="c-38876444">[-]</label><label class="expand" for="c-38876444">[5 more]</label></div><br/><div class="children"><div class="content">And for those of us just tuning in, what is an &quot;autograd&quot;? From skimming the article and some quick web searching, it looks like some sort of math function that gets used a lot in ML (and I assume other places) so there are implementations in a bunch of major libraries?</div><br/><div id="38876655" class="c"><input type="checkbox" id="c-38876655" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#38876444">parent</a><span>|</span><a href="#38875458">next</a><span>|</span><label class="collapse" for="c-38876655">[-]</label><label class="expand" for="c-38876655">[4 more]</label></div><br/><div class="children"><div class="content">reverse-mode automatic differentiation to compute the gradient of a scalar with respect to some large vector of inputs that it&#x27;s computed from, which is critically important for the most popular and most efficient mathematical optimization algorithms such as adam<p>you can&#x27;t really implement it in a library in a normal programming language, because it has to introspect on how your program is computing everything, which is not something that a library can normally do; you have to implement it in a programming language, at least an embedded domain-specific language but in some cases something like a fortran compiler</div><br/><div id="38876760" class="c"><input type="checkbox" id="c-38876760" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#38876444">root</a><span>|</span><a href="#38876655">parent</a><span>|</span><a href="#38875458">next</a><span>|</span><label class="collapse" for="c-38876760">[-]</label><label class="expand" for="c-38876760">[3 more]</label></div><br/><div class="children"><div class="content">of course you can implement it in a library. You just have to make sure the users are using your array objects with appropriately overloaded operators so you can keep track of the computational graph.<p>That&#x27;s how pretty much how all python DL frameworks are built. And thats always why you can&#x27;t call .numpy() on a torch tensor which requires a gradient. Doing so would break the computational graph.</div><br/><div id="38876774" class="c"><input type="checkbox" id="c-38876774" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#38876444">root</a><span>|</span><a href="#38876760">parent</a><span>|</span><a href="#38875458">next</a><span>|</span><label class="collapse" for="c-38876774">[-]</label><label class="expand" for="c-38876774">[2 more]</label></div><br/><div class="children"><div class="content">that&#x27;s why the comment you&#x27;re replying to says &#x27;at least an embedded domain-specific language&#x27;, kepler boy</div><br/><div id="38876880" class="c"><input type="checkbox" id="c-38876880" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#38876444">root</a><span>|</span><a href="#38876774">parent</a><span>|</span><a href="#38875458">next</a><span>|</span><label class="collapse" for="c-38876880">[-]</label><label class="expand" for="c-38876880">[1 more]</label></div><br/><div class="children"><div class="content">I still consider &quot;you can&#x27;t really implement it in a library in a normal programming language&quot; as misleading.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38875458" class="c"><input type="checkbox" id="c-38875458" checked=""/><div class="controls bullet"><span class="by">tekknolagi</span><span>|</span><a href="#38876444">prev</a><span>|</span><a href="#38873960">next</a><span>|</span><label class="collapse" for="c-38875458">[-]</label><label class="expand" for="c-38875458">[1 more]</label></div><br/><div class="children"><div class="content">Related: my post about this: <a href="https:&#x2F;&#x2F;bernsteinbear.com&#x2F;blog&#x2F;compiling-ml-models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bernsteinbear.com&#x2F;blog&#x2F;compiling-ml-models&#x2F;</a><p>I recommend trying out TCC, which compiles C very fast.</div><br/></div></div><div id="38873960" class="c"><input type="checkbox" id="c-38873960" checked=""/><div class="controls bullet"><span class="by">cl3misch</span><span>|</span><a href="#38875458">prev</a><span>|</span><a href="#38873501">next</a><span>|</span><label class="collapse" for="c-38873960">[-]</label><label class="expand" for="c-38873960">[1 more]</label></div><br/><div class="children"><div class="content">Jax is unrolling for-loops during compilation. I don&#x27;t know about the hypothetical performance of canonical Jax code for this example, but the comparison as it is seems unfair.<p><a href="https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;faq.html#jit-decorated-function-is-very-slow-to-compile" rel="nofollow">https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;faq.html#jit-decorated-...</a></div><br/></div></div><div id="38873501" class="c"><input type="checkbox" id="c-38873501" checked=""/><div class="controls bullet"><span class="by">317070</span><span>|</span><a href="#38873960">prev</a><span>|</span><a href="#38873898">next</a><span>|</span><label class="collapse" for="c-38873501">[-]</label><label class="expand" for="c-38873501">[7 more]</label></div><br/><div class="children"><div class="content">&gt; 1. we test many computation graphs (graph is changing constantly)
2.  many-many scalar operations with roughly 10k—100k nodes in each graph
3.  every graph should be compiled and ran around 10k times both forward and backward<p>Now I&#x27;m curious to know where that is useful. Some kind of meta-learning approach?</div><br/><div id="38874621" class="c"><input type="checkbox" id="c-38874621" checked=""/><div class="controls bullet"><span class="by">dimatura</span><span>|</span><a href="#38873501">parent</a><span>|</span><a href="#38876612">next</a><span>|</span><label class="collapse" for="c-38874621">[-]</label><label class="expand" for="c-38874621">[3 more]</label></div><br/><div class="children"><div class="content">Automatic differentiation isn&#x27;t just for machine learning, it&#x27;s useful in several other applications where you have an optimization problem built of many variables interacting in differentiable ways. One possible guess could be structure-from- motion or SLAM, where the graph is often dynamic and several thousands of nodes isn&#x27;t uncommon (the nodes would be things such as camera poses and 3D landmarks for example). However, in that case there are other frameworks that are built with that scenario in mind (Ceres, GTSAM), and would probably be better baselines.</div><br/><div id="38876664" class="c"><input type="checkbox" id="c-38876664" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#38873501">root</a><span>|</span><a href="#38874621">parent</a><span>|</span><a href="#38876376">next</a><span>|</span><label class="collapse" for="c-38876664">[-]</label><label class="expand" for="c-38876664">[1 more]</label></div><br/><div class="children"><div class="content">i want to point out that in many cases you can compute a differentiable relaxation of a nondifferentiable problem to get something you can optimize more easily (with, for example, automatic differentiation) and then use the relaxation to solve the original nondifferentiable problem, for example with branch-and-bound</div><br/></div></div><div id="38876376" class="c"><input type="checkbox" id="c-38876376" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#38873501">root</a><span>|</span><a href="#38874621">parent</a><span>|</span><a href="#38876664">prev</a><span>|</span><a href="#38876612">next</a><span>|</span><label class="collapse" for="c-38876376">[-]</label><label class="expand" for="c-38876376">[1 more]</label></div><br/><div class="children"><div class="content">Optimizing something with AD is machine learning.</div><br/></div></div></div></div><div id="38876612" class="c"><input type="checkbox" id="c-38876612" checked=""/><div class="controls bullet"><span class="by">arogozhnikov</span><span>|</span><a href="#38873501">parent</a><span>|</span><a href="#38874621">prev</a><span>|</span><a href="#38873561">next</a><span>|</span><label class="collapse" for="c-38876612">[-]</label><label class="expand" for="c-38876612">[1 more]</label></div><br/><div class="children"><div class="content">Robotic planning of liquid handling. There are many potential alternative ways to achieve the same result, each resulting in its own computation graph.<p>I imagine any kind of trajectory planning for many agents faces similar challenges (e.g. robots in a factory).</div><br/></div></div><div id="38873561" class="c"><input type="checkbox" id="c-38873561" checked=""/><div class="controls bullet"><span class="by">mratsim</span><span>|</span><a href="#38873501">parent</a><span>|</span><a href="#38876612">prev</a><span>|</span><a href="#38874398">next</a><span>|</span><label class="collapse" for="c-38873561">[-]</label><label class="expand" for="c-38873561">[1 more]</label></div><br/><div class="children"><div class="content">Maybe instruction-level autograd. When you use a differentiable programming language.</div><br/></div></div><div id="38874398" class="c"><input type="checkbox" id="c-38874398" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#38873501">parent</a><span>|</span><a href="#38873561">prev</a><span>|</span><a href="#38873898">next</a><span>|</span><label class="collapse" for="c-38874398">[-]</label><label class="expand" for="c-38874398">[1 more]</label></div><br/><div class="children"><div class="content">Based on the &quot;we TEST&quot; wording, maybe evaluating graphs for fitness - some sort of genetic&#x2F;evolutionary approach perhaps?</div><br/></div></div></div></div><div id="38873898" class="c"><input type="checkbox" id="c-38873898" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#38873501">prev</a><span>|</span><a href="#38874684">next</a><span>|</span><label class="collapse" for="c-38873898">[-]</label><label class="expand" for="c-38873898">[1 more]</label></div><br/><div class="children"><div class="content">Instead of doing a switch having separate loops for each operation would be much better &#x2F; faster (branch free).</div><br/></div></div><div id="38874684" class="c"><input type="checkbox" id="c-38874684" checked=""/><div class="controls bullet"><span class="by">ofou</span><span>|</span><a href="#38873898">prev</a><span>|</span><a href="#38874606">next</a><span>|</span><label class="collapse" for="c-38874684">[-]</label><label class="expand" for="c-38874684">[2 more]</label></div><br/><div class="children"><div class="content">I think tinygrad is faster.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;tinygrad&#x2F;tinygrad">https:&#x2F;&#x2F;github.com&#x2F;tinygrad&#x2F;tinygrad</a></div><br/><div id="38876377" class="c"><input type="checkbox" id="c-38876377" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#38874684">parent</a><span>|</span><a href="#38874606">next</a><span>|</span><label class="collapse" for="c-38876377">[-]</label><label class="expand" for="c-38876377">[1 more]</label></div><br/><div class="children"><div class="content">Maybe he thinks San Diego (George Hotz) is not west of Moscow (or maybe Alex moved now to London).</div><br/></div></div></div></div><div id="38874606" class="c"><input type="checkbox" id="c-38874606" checked=""/><div class="controls bullet"><span class="by">billyjmc</span><span>|</span><a href="#38874684">prev</a><span>|</span><a href="#38875113">next</a><span>|</span><label class="collapse" for="c-38874606">[-]</label><label class="expand" for="c-38874606">[1 more]</label></div><br/><div class="children"><div class="content">I recently had a similar need for my “real-time” Python program to do some autograd operations – but not on big tensors – and also ended up leveraging Rust as part of my solution. But I didn’t know about rustimport (or cppimport). That’s really slick, and I’m going to file that nugget away for later.</div><br/></div></div><div id="38875113" class="c"><input type="checkbox" id="c-38875113" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#38874606">prev</a><span>|</span><a href="#38873374">next</a><span>|</span><label class="collapse" for="c-38875113">[-]</label><label class="expand" for="c-38875113">[2 more]</label></div><br/><div class="children"><div class="content">And 10k ops is not a lot! SDXL has around 4k ops (from my memory) on the forward pass, from this test, that means you spend 40ms each iteration on autodiff!</div><br/><div id="38876213" class="c"><input type="checkbox" id="c-38876213" checked=""/><div class="controls bullet"><span class="by">markisus</span><span>|</span><a href="#38875113">parent</a><span>|</span><a href="#38873374">next</a><span>|</span><label class="collapse" for="c-38876213">[-]</label><label class="expand" for="c-38876213">[1 more]</label></div><br/><div class="children"><div class="content">The author&#x27;s workload is sort of different than the usual ML workload since the author&#x27;s expression tree is large (10k nodes), while a modern neural net has a relatively smaller expression tree, maybe fewer than 100 for the larger neural nets?<p>Another commenter mentioned Dr.Jit which seems to be designed for this use case. This is a quote from their project page.<p>&gt; Why did we create Dr.Jit, when dynamic derivative compilation is already possible using Python-based ML frameworks like JAX, Tensorflow, and PyTorch along with backends like XLA and TorchScript?<p>&gt; The reason is related to the typical workloads: machine learning involves small-ish computation graphs that are, however, made of arithmetically intense operations like convolutions, matrix multiplications, etc. The application motivating Dr.Jit (differentiable rendering) creates giant and messy computation graphs consisting of 100K to millions of &quot;trivial&quot; nodes (elementary arithmetic operations). In our experience, ML compilation backends use internal representations and optimization passes that are too rich for this type of input, causing them to crash or time out during compilation. If you have encountered such issues, you may find Dr.Jit useful.</div><br/></div></div></div></div></div></div></div></div></div></body></html>