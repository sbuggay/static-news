<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719910855186" as="style"/><link rel="stylesheet" href="styles.css?v=1719910855186"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/">My Python code is a neural network</a> <span class="domain">(<a href="https://blog.gabornyeki.com">blog.gabornyeki.com</a>)</span></div><div class="subtext"><span>gnyeki</span> | <span>64 comments</span></div><br/><div><div id="40848072" class="c"><input type="checkbox" id="c-40848072" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#40846330">next</a><span>|</span><label class="collapse" for="c-40848072">[-]</label><label class="expand" for="c-40848072">[1 more]</label></div><br/><div class="children"><div class="content">This article doesn&#x27;t talk much about testing or getting training data. It seems like that part is key.<p>For code that you think you understand, it&#x27;s because you&#x27;ve informally proven to yourself that it has some properties that generalize to all inputs. For example, a sort algorithm will sort any list, not just the ones you tested.<p>The thing we&#x27;re uncertain about for a neural network is that we don&#x27;t know how it will generalize; there are no properties that we think are guaranteed for unseen input, even if it&#x27;s slightly different input. It might be because we have an ill-specified problem and we don&#x27;t know how to mathematically specify what properties we want.<p>If you can actually specify a property well enough to write a property-based test (like QuickCheck) then you can generate large amounts of tests &#x2F; training data though randomization. Start with one example of what you want, then write tests that generate every possible version of both positive and negative examples.<p>It&#x27;s not a proof, but it&#x27;s a start. At least you know what you would prove, if you could.<p>If you have such a thing, relying on spaghetti code or a neural network seem kind of similar? If you want another property to hold, you can write another property-based test for it. I suppose with the neural network you can train it instead of doing the edits yourself, but then again we have AI assistance for code fixes.<p>I think I&#x27;d still trust code more. At least you can debug it.</div><br/></div></div><div id="40846330" class="c"><input type="checkbox" id="c-40846330" checked=""/><div class="controls bullet"><span class="by">scotchmi_st</span><span>|</span><a href="#40848072">prev</a><span>|</span><a href="#40846283">next</a><span>|</span><label class="collapse" for="c-40846330">[-]</label><label class="expand" for="c-40846330">[5 more]</label></div><br/><div class="children"><div class="content">This is an interesting article if you read it like a howto for constructing a neural network for performing a practical task. But if you take it at face-value, and follow a similar method the next time you need to parse some input, then, well, I don&#x27;t know what to say really.<p>The author takes a hard problem (parsing arbitrary input for loosely-defined patterns), and correctly argues that this is likely to produce hard-to-read &#x27;spaghetti&#x27; code.<p>They then suggest replacing that with code that is so hard to read that there is still active research into how it works, (i.e a neural net).<p>Don&#x27;t over-index something that&#x27;s inscrutable versus something that you can understand but is &#x27;ugly&#x27;. Sometimes, _maybe_, a ML model is what you want for a task. But a lot of the time, something that you can read and see why it&#x27;s doing what it&#x27;s doing, even if that takes some effort, is better than something that&#x27;s impossible.</div><br/><div id="40847760" class="c"><input type="checkbox" id="c-40847760" checked=""/><div class="controls bullet"><span class="by">thoughtlede</span><span>|</span><a href="#40846330">parent</a><span>|</span><a href="#40853664">next</a><span>|</span><label class="collapse" for="c-40847760">[-]</label><label class="expand" for="c-40847760">[1 more]</label></div><br/><div class="children"><div class="content">I think the mention of &#x27;spaghetti code&#x27; is a red herring from the author. If the output from an algorithm cannot be defined precisely as a function of the input, but you have some examples to show, that&#x27;s where machine learning (ML) is useful.<p>In the end, ML provides one more option to choose from. Whether it works or not for you depends on evaluations and how deterministic and explainability you need from the chosen algorithm&#x2F;option.<p>The thing that struck me is if RNN is the right choice given that it would need to be trained and we need a lot of examples than what we might have. That said, maybe based on known &#x27;rules&#x27;, we can produce synthetic data for both +ve and -ve cases.</div><br/></div></div><div id="40853664" class="c"><input type="checkbox" id="c-40853664" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40846330">parent</a><span>|</span><a href="#40847760">prev</a><span>|</span><a href="#40846416">next</a><span>|</span><label class="collapse" for="c-40853664">[-]</label><label class="expand" for="c-40853664">[2 more]</label></div><br/><div class="children"><div class="content">The spaghetti code approach is basically an expert system.  An old school algorithmic AI.  Outside constrained domains these systems never really performed very well.  Reality is just too messy.<p>Having a system where you can see why it works the way it does is all very well, but not if it keeps giving the wrong answers.  In real world use getting the right answer is often more important than knowing how you got that answer.</div><br/><div id="40853954" class="c"><input type="checkbox" id="c-40853954" checked=""/><div class="controls bullet"><span class="by">zelphirkalt</span><span>|</span><a href="#40846330">root</a><span>|</span><a href="#40853664">parent</a><span>|</span><a href="#40846416">next</a><span>|</span><label class="collapse" for="c-40853954">[-]</label><label class="expand" for="c-40853954">[1 more]</label></div><br/><div class="children"><div class="content">You can make an expert system extensible though. You can make it definitely recognize some pattern and when there is a complaint about it not recognizing another pattern, you can add it. Hopefully you wrote the code in a way that easily allows adding new patterns, of course.</div><br/></div></div></div></div></div></div><div id="40846283" class="c"><input type="checkbox" id="c-40846283" checked=""/><div class="controls bullet"><span class="by">pakl</span><span>|</span><a href="#40846330">prev</a><span>|</span><a href="#40846197">next</a><span>|</span><label class="collapse" for="c-40846283">[-]</label><label class="expand" for="c-40846283">[15 more]</label></div><br/><div class="children"><div class="content">There exists the Universal (Function) Approximation Theorem for neural networks — which states that they can represent&#x2F;encode any function to a desired level of accuracy[0].<p>However there does not exist a theorem stating that those approximations can be learned (or how).<p>[0] <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Universal_approximation_theorem" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Universal_approximation_theo...</a></div><br/><div id="40848921" class="c"><input type="checkbox" id="c-40848921" checked=""/><div class="controls bullet"><span class="by">montebicyclelo</span><span>|</span><a href="#40846283">parent</a><span>|</span><a href="#40846565">next</a><span>|</span><label class="collapse" for="c-40848921">[-]</label><label class="expand" for="c-40848921">[1 more]</label></div><br/><div class="children"><div class="content">People throw that proof around all the time; but all it does is show that a neural net is equivalent to a lookup table; and a lookup table with enough memory can approximate any function. It&#x27;s miles away from explaining how real world, useful, neural nets, like conv-nets, transformers, LSTMs, etc. actually work.</div><br/></div></div><div id="40846565" class="c"><input type="checkbox" id="c-40846565" checked=""/><div class="controls bullet"><span class="by">jb1991</span><span>|</span><a href="#40846283">parent</a><span>|</span><a href="#40848921">prev</a><span>|</span><a href="#40847550">next</a><span>|</span><label class="collapse" for="c-40846565">[-]</label><label class="expand" for="c-40846565">[9 more]</label></div><br/><div class="children"><div class="content">FYI, there are actually many algorithms going back longer than the neural network algorithm that have been proven to be a universal function approximator. Neural networks are certainly not the only and not the first to do so. There are quite a few that are actually much more appropriate for many cases than a neural network.</div><br/><div id="40846890" class="c"><input type="checkbox" id="c-40846890" checked=""/><div class="controls bullet"><span class="by">derangedHorse</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40846565">parent</a><span>|</span><a href="#40847550">next</a><span>|</span><label class="collapse" for="c-40846890">[-]</label><label class="expand" for="c-40846890">[8 more]</label></div><br/><div class="children"><div class="content">What other algorithms can do this and which situations would they be more useful than neural networks?</div><br/><div id="40852439" class="c"><input type="checkbox" id="c-40852439" checked=""/><div class="controls bullet"><span class="by">dontlikeyoueith</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40846890">parent</a><span>|</span><a href="#40847107">next</a><span>|</span><label class="collapse" for="c-40852439">[-]</label><label class="expand" for="c-40852439">[1 more]</label></div><br/><div class="children"><div class="content">The Taylor Series dates to 1715.  Fourier Series dates to the 1820s.<p>Both are universal function approximators and both can be learned via gradient descent.<p>For the case where the function you want to learn actually is polynomial or periodic (respectively), these are better than neural networks.</div><br/></div></div><div id="40847107" class="c"><input type="checkbox" id="c-40847107" checked=""/><div class="controls bullet"><span class="by">gnyeki</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40846890">parent</a><span>|</span><a href="#40852439">prev</a><span>|</span><a href="#40849547">next</a><span>|</span><label class="collapse" for="c-40847107">[-]</label><label class="expand" for="c-40847107">[1 more]</label></div><br/><div class="children"><div class="content">This area is covered by non-parametric statistics more generally. There are many other methods to non-parametrically estimate functions (that satisfy some regularity conditions). Tree-based methods are one family of such methods, and the consensus still seems to be that they perform better than neural networks on tabular data. For example:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.03253" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.03253</a></div><br/></div></div><div id="40849547" class="c"><input type="checkbox" id="c-40849547" checked=""/><div class="controls bullet"><span class="by">jb1991</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40846890">parent</a><span>|</span><a href="#40847107">prev</a><span>|</span><a href="#40847342">next</a><span>|</span><label class="collapse" for="c-40849547">[-]</label><label class="expand" for="c-40849547">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.quora.com&#x2F;Is-there-any-universal-function-approximators-other-than-a-neural-network" rel="nofollow">https:&#x2F;&#x2F;www.quora.com&#x2F;Is-there-any-universal-function-approx...</a></div><br/></div></div><div id="40847342" class="c"><input type="checkbox" id="c-40847342" checked=""/><div class="controls bullet"><span class="by">someoneontenet</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40846890">parent</a><span>|</span><a href="#40849547">prev</a><span>|</span><a href="#40847550">next</a><span>|</span><label class="collapse" for="c-40847342">[-]</label><label class="expand" for="c-40847342">[4 more]</label></div><br/><div class="children"><div class="content">Newtons Method approximates square roots. Its useful if you want to approximate something like that without pulling in the computational power required of NN.</div><br/><div id="40854121" class="c"><input type="checkbox" id="c-40854121" checked=""/><div class="controls bullet"><span class="by">jb1991</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40847342">parent</a><span>|</span><a href="#40848040">next</a><span>|</span><label class="collapse" for="c-40854121">[-]</label><label class="expand" for="c-40854121">[1 more]</label></div><br/><div class="children"><div class="content">By definition, that’s not a “universal“ function approximator.</div><br/></div></div><div id="40848040" class="c"><input type="checkbox" id="c-40848040" checked=""/><div class="controls bullet"><span class="by">astrobe_</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40847342">parent</a><span>|</span><a href="#40854121">prev</a><span>|</span><a href="#40847888">next</a><span>|</span><label class="collapse" for="c-40848040">[-]</label><label class="expand" for="c-40848040">[1 more]</label></div><br/><div class="children"><div class="content">I think the problem to solve is more like : given a set of inputs and outputs, find a function that gives the expected output for each input [1]. This is like Newton&#x27;s method on a higher order ;-). One can find such a tool in Squeak or Pharo Smalltalk, IIRC.<p>[1] <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;1539286&#x2F;create-a-function-for-given-input-and-ouput" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;1539286&#x2F;create-a-functio...</a></div><br/></div></div><div id="40847888" class="c"><input type="checkbox" id="c-40847888" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40847342">parent</a><span>|</span><a href="#40848040">prev</a><span>|</span><a href="#40847550">next</a><span>|</span><label class="collapse" for="c-40847888">[-]</label><label class="expand" for="c-40847888">[1 more]</label></div><br/><div class="children"><div class="content">Newton&#x27;s method related to universal function approximation in the same way a natural oil seep is related to a modern IC engine...</div><br/></div></div></div></div></div></div></div></div><div id="40847550" class="c"><input type="checkbox" id="c-40847550" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40846283">parent</a><span>|</span><a href="#40846565">prev</a><span>|</span><a href="#40846882">next</a><span>|</span><label class="collapse" for="c-40847550">[-]</label><label class="expand" for="c-40847550">[1 more]</label></div><br/><div class="children"><div class="content">They can model only continuous functions, more specifically any continuous function on compact subsets of ℝⁿ. They can approximate functions to an arbitrary level of accuracy, given sufficient neurons</div><br/></div></div><div id="40846882" class="c"><input type="checkbox" id="c-40846882" checked=""/><div class="controls bullet"><span class="by">richrichie</span><span>|</span><a href="#40846283">parent</a><span>|</span><a href="#40847550">prev</a><span>|</span><a href="#40846329">next</a><span>|</span><label class="collapse" for="c-40846882">[-]</label><label class="expand" for="c-40846882">[1 more]</label></div><br/><div class="children"><div class="content">Not any function though. There are restrictions on type of functions &quot;universal&quot; approximation theorem is applicable for.
Interestingly, the theorem is about a single layer network. In practice, that does not work as well as having many layers.</div><br/></div></div><div id="40846329" class="c"><input type="checkbox" id="c-40846329" checked=""/><div class="controls bullet"><span class="by">arketyp</span><span>|</span><a href="#40846283">parent</a><span>|</span><a href="#40846882">prev</a><span>|</span><a href="#40846197">next</a><span>|</span><label class="collapse" for="c-40846329">[-]</label><label class="expand" for="c-40846329">[2 more]</label></div><br/><div class="children"><div class="content">Makes you wonder what is meant by learning...</div><br/><div id="40846349" class="c"><input type="checkbox" id="c-40846349" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40846283">root</a><span>|</span><a href="#40846329">parent</a><span>|</span><a href="#40846197">next</a><span>|</span><label class="collapse" for="c-40846349">[-]</label><label class="expand" for="c-40846349">[1 more]</label></div><br/><div class="children"><div class="content">Learning is using observations to create&#x2F;update a model that makes predictions which are more accurate than chance.  At some point the model ends up having generalizability beyond the domain.</div><br/></div></div></div></div></div></div><div id="40846197" class="c"><input type="checkbox" id="c-40846197" checked=""/><div class="controls bullet"><span class="by">ryjo</span><span>|</span><a href="#40846283">prev</a><span>|</span><a href="#40854662">next</a><span>|</span><label class="collapse" for="c-40846197">[-]</label><label class="expand" for="c-40846197">[1 more]</label></div><br/><div class="children"><div class="content">Really awesome. Thanks for this thorough write-up. I don&#x27;t totally understand the deeper math concepts mentioned in this article around RNNs, but it&#x27;s sparked some of my own thoughts. It feels similar to things I&#x27;ve been exploring lately-- that is: building your app interwoven with forward chaining algorithms. In your case, you&#x27;re using RNNs, and in mine, I&#x27;m building into the Rete algorithm.<p>You also touch on something in this article that I&#x27;ve found quite powerful: putting things in terms of digesting an input string character-by-character. Then, we offload all of the reasoning logic to our algorithm. We write very thin i&#x2F;o logic, and then the algorithm does the rest.</div><br/></div></div><div id="40854662" class="c"><input type="checkbox" id="c-40854662" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#40846197">prev</a><span>|</span><a href="#40846175">next</a><span>|</span><label class="collapse" for="c-40854662">[-]</label><label class="expand" for="c-40854662">[1 more]</label></div><br/><div class="children"><div class="content">Fwiw, I know LGTM as &quot;let&#x27;s get this moving&quot; on pull requests. Seems to be contended.</div><br/></div></div><div id="40846175" class="c"><input type="checkbox" id="c-40846175" checked=""/><div class="controls bullet"><span class="by">Fripplebubby</span><span>|</span><a href="#40854662">prev</a><span>|</span><a href="#40846363">next</a><span>|</span><label class="collapse" for="c-40846175">[-]</label><label class="expand" for="c-40846175">[3 more]</label></div><br/><div class="children"><div class="content">Love this post! Gets into the details of what it _really_ means to take some function and turn it into an RNN, and comparing that to the &quot;batteries included&quot; RNNs included in PyTorch, as a learning experience.<p>Question:<p>&gt; To model the state, we need to add three hidden layers to the network.<p>How did you determine that it would be three hidden layers? Is it a consequence of the particular rule you were implementing, or is that generally how many layers you would use to implement a rule of this shape (using your architecture rather than Elman&#x27;s - could we use fewer layers with Elman&#x27;s?)?</div><br/><div id="40847301" class="c"><input type="checkbox" id="c-40847301" checked=""/><div class="controls bullet"><span class="by">gnyeki</span><span>|</span><a href="#40846175">parent</a><span>|</span><a href="#40846363">next</a><span>|</span><label class="collapse" for="c-40847301">[-]</label><label class="expand" for="c-40847301">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad you found it valuable! Both are good questions and I haven&#x27;t gone far enough mapping the code to Elman&#x27;s architecture to know the answer to the second.<p>For your first question, using three hidden layers makes it a little clearer what the network does. Each layer performs one step of the calculation. The first layer collects what is known from the current token and what we knew after the calculation for the previous token. The second layer decides whether the current token looks like program code, by checking if it satisfies the decision rule. The third layer compares the decision with what we decided for previous tokens.<p>I think that this could be compressed into a single hidden layer, too. A ReLU should be good enough at capturing non-linearities so this should work.</div><br/><div id="40847545" class="c"><input type="checkbox" id="c-40847545" checked=""/><div class="controls bullet"><span class="by">Fripplebubby</span><span>|</span><a href="#40846175">root</a><span>|</span><a href="#40847301">parent</a><span>|</span><a href="#40846363">next</a><span>|</span><label class="collapse" for="c-40847545">[-]</label><label class="expand" for="c-40847545">[1 more]</label></div><br/><div class="children"><div class="content">Ah, that makes sense. So, we consider two hidden layers more as &quot;memory&quot; or &quot;buffers&quot;, and actually the rule is implemented in just one layer, at least for a single token.</div><br/></div></div></div></div></div></div><div id="40846363" class="c"><input type="checkbox" id="c-40846363" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40846175">prev</a><span>|</span><a href="#40846350">next</a><span>|</span><label class="collapse" for="c-40846363">[-]</label><label class="expand" for="c-40846363">[10 more]</label></div><br/><div class="children"><div class="content">Are RNNs completely subsumed by transformers?  IE, can I forget about learning anything about how to work with RNNs, and instead focus on transformers?</div><br/><div id="40847504" class="c"><input type="checkbox" id="c-40847504" checked=""/><div class="controls bullet"><span class="by">Fripplebubby</span><span>|</span><a href="#40846363">parent</a><span>|</span><a href="#40847414">next</a><span>|</span><label class="collapse" for="c-40847504">[-]</label><label class="expand" for="c-40847504">[3 more]</label></div><br/><div class="children"><div class="content">To further problematize this question (which I don&#x27;t feel like I can actually answer), consider this paper: &quot;Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention&quot; - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2006.16236" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2006.16236</a><p>What this shows is that actually a specific narrow definition of transformer (a transformer with &quot;causal masking&quot; - see paper) is equivalent to an RNN, and vice versa.<p>Similarly Mamba (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752</a>), the other hot architecture at the moment, has an equivalent unit to a gated RNN. For performance reasons, I believe they use an equivalent CNN during training and an RNN during inference!</div><br/><div id="40847635" class="c"><input type="checkbox" id="c-40847635" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40846363">root</a><span>|</span><a href="#40847504">parent</a><span>|</span><a href="#40847859">next</a><span>|</span><label class="collapse" for="c-40847635">[-]</label><label class="expand" for="c-40847635">[1 more]</label></div><br/><div class="children"><div class="content">There still are important distinctions. RNNs have constant memory while transformers expand their memory with each new token. They are related, but one could in theory process an unbounded sequence while the other cannot because of growing memory usage.</div><br/></div></div><div id="40847859" class="c"><input type="checkbox" id="c-40847859" checked=""/><div class="controls bullet"><span class="by">Fripplebubby</span><span>|</span><a href="#40846363">root</a><span>|</span><a href="#40847504">parent</a><span>|</span><a href="#40847635">prev</a><span>|</span><a href="#40847414">next</a><span>|</span><label class="collapse" for="c-40847859">[-]</label><label class="expand" for="c-40847859">[1 more]</label></div><br/><div class="children"><div class="content">To be more concrete: you might decide not to learn about RNNs, but still find them lurking in the things you did learn about!</div><br/></div></div></div></div><div id="40847414" class="c"><input type="checkbox" id="c-40847414" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#40846363">parent</a><span>|</span><a href="#40847504">prev</a><span>|</span><a href="#40846392">next</a><span>|</span><label class="collapse" for="c-40847414">[-]</label><label class="expand" for="c-40847414">[1 more]</label></div><br/><div class="children"><div class="content">Transformers have finite context, RNNs don’t. In practice the RNN gradient signal is limited by back propagation through time, it decays. This is in fact the whole selling point of transformers; association is not harder or easier in near&#x2F;short distance. But in theory a RNN can remember infinitely far away.</div><br/></div></div><div id="40846392" class="c"><input type="checkbox" id="c-40846392" checked=""/><div class="controls bullet"><span class="by">Voloskaya</span><span>|</span><a href="#40846363">parent</a><span>|</span><a href="#40847414">prev</a><span>|</span><a href="#40846350">next</a><span>|</span><label class="collapse" for="c-40846392">[-]</label><label class="expand" for="c-40846392">[5 more]</label></div><br/><div class="children"><div class="content">Not if you want to be a PhD&#x2F;Researcher in ML, yes otherwise.<p>Source: Working on ML&#x2F;LLMs as a research engineer for the past 7 years, including for one of the FAANG&#x27;s research lab, always wanted to take time to learn about RNN but never did and never needed to.</div><br/><div id="40846770" class="c"><input type="checkbox" id="c-40846770" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#40846363">root</a><span>|</span><a href="#40846392">parent</a><span>|</span><a href="#40846978">next</a><span>|</span><label class="collapse" for="c-40846770">[-]</label><label class="expand" for="c-40846770">[2 more]</label></div><br/><div class="children"><div class="content">Oh, I&#x27;m sure plenty of recent PhDs don&#x27;t know about RNNs. They&#x27;ve been dropped like a hot potato in the last 4-5 years.</div><br/><div id="40849978" class="c"><input type="checkbox" id="c-40849978" checked=""/><div class="controls bullet"><span class="by">Voloskaya</span><span>|</span><a href="#40846363">root</a><span>|</span><a href="#40846770">parent</a><span>|</span><a href="#40846978">next</a><span>|</span><label class="collapse" for="c-40849978">[-]</label><label class="expand" for="c-40849978">[1 more]</label></div><br/><div class="children"><div class="content">I think to do pure research it’s definitely worth knowing about the big ideas of the past, why we moved on from them, what we learned etc.</div><br/></div></div></div></div><div id="40846978" class="c"><input type="checkbox" id="c-40846978" checked=""/><div class="controls bullet"><span class="by">derangedHorse</span><span>|</span><a href="#40846363">root</a><span>|</span><a href="#40846392">parent</a><span>|</span><a href="#40846770">prev</a><span>|</span><a href="#40852184">next</a><span>|</span><label class="collapse" for="c-40846978">[-]</label><label class="expand" for="c-40846978">[1 more]</label></div><br/><div class="children"><div class="content">I haven’t read it in a while but I remember this post giving a good rundown of rnns<p><a href="https:&#x2F;&#x2F;dennybritz.com&#x2F;posts&#x2F;wildml&#x2F;recurrent-neural-networks-tutorial-part-1&#x2F;" rel="nofollow">https:&#x2F;&#x2F;dennybritz.com&#x2F;posts&#x2F;wildml&#x2F;recurrent-neural-network...</a></div><br/></div></div><div id="40852184" class="c"><input type="checkbox" id="c-40852184" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#40846363">root</a><span>|</span><a href="#40846392">parent</a><span>|</span><a href="#40846978">prev</a><span>|</span><a href="#40846350">next</a><span>|</span><label class="collapse" for="c-40852184">[-]</label><label class="expand" for="c-40852184">[1 more]</label></div><br/><div class="children"><div class="content">None of the students who have taken the classes I TA pass w&#x2F;I learning about RNNs.</div><br/></div></div></div></div></div></div><div id="40846350" class="c"><input type="checkbox" id="c-40846350" checked=""/><div class="controls bullet"><span class="by">jlturner</span><span>|</span><a href="#40846363">prev</a><span>|</span><a href="#40852947">next</a><span>|</span><label class="collapse" for="c-40846350">[-]</label><label class="expand" for="c-40846350">[4 more]</label></div><br/><div class="children"><div class="content">If this interests you, it’s worth taking a look at Genetic Programming. I find it to be a simpler approach at the same problem, no math required. It simply recombines programs by their AST, and given some heuristic, optimizes the program for it. The magic is in your heuristic function, where you can choose what you want to optimize for (ie. Speed, program length, minimize complex constructs or function calls, network efficiency, some combination therein, etc).<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;tTMpKrKkYXo" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;tTMpKrKkYXo</a></div><br/><div id="40846926" class="c"><input type="checkbox" id="c-40846926" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#40846350">parent</a><span>|</span><a href="#40846769">next</a><span>|</span><label class="collapse" for="c-40846926">[-]</label><label class="expand" for="c-40846926">[1 more]</label></div><br/><div class="children"><div class="content">I’ll add the Humies Awards that highlight human-competitive results. One can learn a lot about what can or can’t be done in this field by just skimming across all the submitted papers.<p><a href="https:&#x2F;&#x2F;www.human-competitive.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.human-competitive.org&#x2F;</a></div><br/></div></div><div id="40846476" class="c"><input type="checkbox" id="c-40846476" checked=""/><div class="controls bullet"><span class="by">PixelN0va</span><span>|</span><a href="#40846350">parent</a><span>|</span><a href="#40846769">prev</a><span>|</span><a href="#40852947">next</a><span>|</span><label class="collapse" for="c-40846476">[-]</label><label class="expand" for="c-40846476">[1 more]</label></div><br/><div class="children"><div class="content">hmm thanks for the link</div><br/></div></div></div></div><div id="40852947" class="c"><input type="checkbox" id="c-40852947" checked=""/><div class="controls bullet"><span class="by">lowyek</span><span>|</span><a href="#40846350">prev</a><span>|</span><a href="#40846090">next</a><span>|</span><label class="collapse" for="c-40852947">[-]</label><label class="expand" for="c-40852947">[1 more]</label></div><br/><div class="children"><div class="content">I would love to see some work on duality i.e. code to neural network back and forth. Reason being - I can&#x27;t debug a neural network but if it can be linearized into a if-else case with help of the token information =&gt; I can validate what it&#x27;s doing -&gt; fix it and then move it back to it&#x27;s compressed neural representation.<p>Just another thought experiment -&gt; sometimes I imagine neural networks as a zip of the training data where compression algorithm is backpropagation. Just like we have programs which let us see what files inside the zip are -&gt; I imagine there can be programs which will let us select certain inference path of the neural net and then see what data affected that =&gt; then we edit that data to fix our issues or add more data there =&gt; and we have live neural network debugging and reprogramming in the same way we edit compressed zips</div><br/></div></div><div id="40846090" class="c"><input type="checkbox" id="c-40846090" checked=""/><div class="controls bullet"><span class="by">alsxnt</span><span>|</span><a href="#40852947">prev</a><span>|</span><a href="#40846164">next</a><span>|</span><label class="collapse" for="c-40846090">[-]</label><label class="expand" for="c-40846090">[3 more]</label></div><br/><div class="children"><div class="content">Recurrent neural networks can be used for arbitrary computations, the equivalence to Turing machines has been proven. However, they are utterly impractical for the task.<p>This seems to be a state machine that is somehow learned. The article could benefit from a longer synopsis and &quot;Python&quot; does not appear to be relevant at all. Learning real Python semantics would prove quite difficult due to the nature of the language (no standard, just do as CPython does).</div><br/><div id="40846249" class="c"><input type="checkbox" id="c-40846249" checked=""/><div class="controls bullet"><span class="by">danans</span><span>|</span><a href="#40846090">parent</a><span>|</span><a href="#40846164">next</a><span>|</span><label class="collapse" for="c-40846249">[-]</label><label class="expand" for="c-40846249">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Recurrent neural networks can be used for arbitrary computations, the equivalence to Turing machines has been proven. However, they are utterly impractical for the task.<p>Karpathy&#x27;s 2015 RNN article [1] demonstrated that RNNs trained character-wise on Shakespeare&#x27;s works could produce Shakespeare-esque text (albeit without the narrative coherence of LLMs).  Given that, why wouldn&#x27;t they be able to handle natural language as formulaic as code review comments?<p>In that case inference was run with randomized inputs in order to generate random &quot;Shakespeare&quot;, but the structure of the language and style was still learned by the RNN.  Perhaps it could be used for classification also.<p>1. <a href="https:&#x2F;&#x2F;karpathy.github.io&#x2F;2015&#x2F;05&#x2F;21&#x2F;rnn-effectiveness&#x2F;" rel="nofollow">https:&#x2F;&#x2F;karpathy.github.io&#x2F;2015&#x2F;05&#x2F;21&#x2F;rnn-effectiveness&#x2F;</a></div><br/><div id="40846452" class="c"><input type="checkbox" id="c-40846452" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40846090">root</a><span>|</span><a href="#40846249">parent</a><span>|</span><a href="#40846164">next</a><span>|</span><label class="collapse" for="c-40846452">[-]</label><label class="expand" for="c-40846452">[1 more]</label></div><br/><div class="children"><div class="content">For RNN abilities, RWKV is worth a look[1]<p>It&#x27;s billed as &quot;an RNN with GPT-level LLM performance&quot;.<p>[1] <a href="https:&#x2F;&#x2F;www.rwkv.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.rwkv.com&#x2F;</a></div><br/></div></div></div></div></div></div><div id="40846164" class="c"><input type="checkbox" id="c-40846164" checked=""/><div class="controls bullet"><span class="by">danans</span><span>|</span><a href="#40846090">prev</a><span>|</span><a href="#40846994">next</a><span>|</span><label class="collapse" for="c-40846164">[-]</label><label class="expand" for="c-40846164">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to see a cost vs precision&#x2F;recall comparison of using a RNN vs an LLM (local or API) for a  problem like this.</div><br/><div id="40848983" class="c"><input type="checkbox" id="c-40848983" checked=""/><div class="controls bullet"><span class="by">danans</span><span>|</span><a href="#40846164">parent</a><span>|</span><a href="#40846994">next</a><span>|</span><label class="collapse" for="c-40848983">[-]</label><label class="expand" for="c-40848983">[1 more]</label></div><br/><div class="children"><div class="content">Hah... Ask, and HN (sorta) delivers.  Not RNN, but self-finetuned LLM cost vs performance compared to GPT-4.<p><a href="https:&#x2F;&#x2F;openpipe.ai&#x2F;blog&#x2F;mixture-of-agents">https:&#x2F;&#x2F;openpipe.ai&#x2F;blog&#x2F;mixture-of-agents</a></div><br/></div></div></div></div><div id="40846994" class="c"><input type="checkbox" id="c-40846994" checked=""/><div class="controls bullet"><span class="by">fnord77</span><span>|</span><a href="#40846164">prev</a><span>|</span><a href="#40850523">next</a><span>|</span><label class="collapse" for="c-40846994">[-]</label><label class="expand" for="c-40846994">[2 more]</label></div><br/><div class="children"><div class="content">&gt; To model the state, we need to add three hidden layers to the network<p>Why 3?<p>And why use &quot;h&quot; for layer names?</div><br/><div id="40849317" class="c"><input type="checkbox" id="c-40849317" checked=""/><div class="controls bullet"><span class="by">muricula</span><span>|</span><a href="#40846994">parent</a><span>|</span><a href="#40850523">next</a><span>|</span><label class="collapse" for="c-40849317">[-]</label><label class="expand" for="c-40849317">[1 more]</label></div><br/><div class="children"><div class="content">`h` is for &quot;hidden&quot; layers.</div><br/></div></div></div></div><div id="40850523" class="c"><input type="checkbox" id="c-40850523" checked=""/><div class="controls bullet"><span class="by">suzukigsx1100g</span><span>|</span><a href="#40846994">prev</a><span>|</span><a href="#40846027">next</a><span>|</span><label class="collapse" for="c-40850523">[-]</label><label class="expand" for="c-40850523">[1 more]</label></div><br/><div class="children"><div class="content">That’s pretty lightwork for a snake in general. Send me a direct message if you come up with something better.</div><br/></div></div><div id="40846027" class="c"><input type="checkbox" id="c-40846027" checked=""/><div class="controls bullet"><span class="by">sdwr</span><span>|</span><a href="#40850523">prev</a><span>|</span><a href="#40853505">next</a><span>|</span><label class="collapse" for="c-40846027">[-]</label><label class="expand" for="c-40846027">[2 more]</label></div><br/><div class="children"><div class="content">This is new to me, and therefore bad and scary.<p>It&#x27;s great that you know NN well enough to fold it into regular work. But think of all us poor regular developers! Who now have to grapple with:<p>- an unfamiliar architecture<p>- uncertainty &#x2F; effectively non-deterministic results in program flow</div><br/><div id="40846187" class="c"><input type="checkbox" id="c-40846187" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#40846027">parent</a><span>|</span><a href="#40853505">next</a><span>|</span><label class="collapse" for="c-40846187">[-]</label><label class="expand" for="c-40846187">[1 more]</label></div><br/><div class="children"><div class="content">NN are in principle deterministic (unless you add randomness to it such as is the case with LLM top p&#x2F;k temperature).<p>Uncertainty is probably the better word of the two, but I feel like there should be a different term.</div><br/></div></div></div></div><div id="40853505" class="c"><input type="checkbox" id="c-40853505" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#40846027">prev</a><span>|</span><a href="#40848335">next</a><span>|</span><label class="collapse" for="c-40853505">[-]</label><label class="expand" for="c-40853505">[1 more]</label></div><br/><div class="children"><div class="content">Next time do one with Bayesian networks or another Probabilistic graphical model.</div><br/></div></div><div id="40848335" class="c"><input type="checkbox" id="c-40848335" checked=""/><div class="controls bullet"><span class="by">dinobones</span><span>|</span><a href="#40853505">prev</a><span>|</span><a href="#40846756">next</a><span>|</span><label class="collapse" for="c-40848335">[-]</label><label class="expand" for="c-40848335">[2 more]</label></div><br/><div class="children"><div class="content">This article was going decently and then it just falls off a cliff.<p>The article basically says:
1) Here’s this complex problem
2) Here’s some hand written heuristics
3) Here’s a shitty neural
net
4) Here’s another neural net with some guys last name from the PyTorch library
5) Here are the constraints with adopting neural nets<p>You can see why this is so unsatisfying, the leaps in logic become more and more generous.<p>What I would have loved to see, is a comparison of a spaghetti code implementation vs a neural net implementation on a large dataset&#x2F;codebase, then show examples in the validation set that maybe the neural net <i>generalizes to</i>, or fails at, but the heuristic fails at, and so on.<p>This would demonstrate the value of neural nets, if for example, there’s a novel example that the neural net finds that the spaghetti heuristic can’t.<p>Show tangible results, show some comparison, show something, giving some rough numbers on the performance of each in aggregate would be really useful.</div><br/></div></div><div id="40846756" class="c"><input type="checkbox" id="c-40846756" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#40848335">prev</a><span>|</span><a href="#40846721">next</a><span>|</span><label class="collapse" for="c-40846756">[-]</label><label class="expand" for="c-40846756">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Of course, we should try and avoid writing spaghetti code if we can. But there are problems that are so ill-specified that any serious attempt to solve them results in just that.<p>Can you elaborate or do you have an example?<p>Based on just the above, I disagree - I&#x27;d say it&#x27;s the job of the programmer to make sure that the problem is well-specified and that they can write maintainable code.</div><br/></div></div><div id="40848502" class="c"><input type="checkbox" id="c-40848502" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#40849331">prev</a><span>|</span><a href="#40845962">next</a><span>|</span><label class="collapse" for="c-40848502">[-]</label><label class="expand" for="c-40848502">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; Humans are bad at managing spaghetti code. Of course, we should try and avoid writing spaghetti code if we can. But there are problems that are so ill-specified that any serious attempt to solve them results in just that.
</code></pre>
Sounds like a skill issue.<p>But seriously, how many programmers do you know that reach for the documents or help pages (man pages?) instead of just looking for the first SO post with a similar question? That&#x27;s how you start programming because you&#x27;re just trying to figure out how to do anything in the first place, but not where you should be years later. If you&#x27;ve been programming in a language for years you should have read a good portion of the docs in that time (in addition to SO posts), blogs, and so much more. Because the things change too, so you have to be keeping up, and the truth is that this will never happen if you just read SO posts to answer your one question (and the next, and the next) because it will always lag behind what tools exist and even more likely will significantly lag because more recent posts have less time to gain upvotes.<p>It kinda reminds me of the meme &quot;how to exit vim.&quot; And how people state that it is so hard to learn. Not only does just typing `vim` into the terminal literally tell you how to quit, but there&#x27;s a built in `vimtutor` that&#x27;ll tell you how to use it and doesn&#x27;t take very long to use. I&#x27;ve seen people go through this and be better than people that have &quot;used&quot; vim for years. And even then, how many people write `:help someFunction` into vim itself? Because it is FAR better than googling your question and you&#x27;ll actually end up learning how the whole thing fits together because it is giving you context. The same is true for literally any programming language.<p>You should also be writing docs to your code because if you have spaghetti code, there&#x27;s a puzzle you haven&#x27;t solved yet. And guess what, documenting is not too different from the rubber ducky method. Here&#x27;s the procedure: write code to make shit work, write docs and edit your code as you realize you can make things better, go on and repeat but not revisit functions as you fuck them up with another function. It&#x27;s not nearly as much work as it sounds and the investments compound. But quality takes time and nothing worth doing is easy. It takes time to learn any habit and skill. If you always look for the quickest solution to &quot;just get it done&quot; and you never come back, then you probably haven&#x27;t learned anything, you&#x27;ve just parroted someone else. Moving fast and breaking things is great, but once you have done that you got to clean up your mess. You don&#x27;t clean your kitchen by breaking your dining room table. And your house isn&#x27;t clean if all your dishes are on the table! You might have to temporarily move stuff around, but eventually you need to clean shit up. And code is exactly the same way. If you regularly clean your house, it stays clean and is easy to keep clean. But if you do it once a year it is a herculean effort that you&#x27;ll dread.</div><br/></div></div><div id="40845962" class="c"><input type="checkbox" id="c-40845962" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#40848502">prev</a><span>|</span><a href="#40848461">next</a><span>|</span><label class="collapse" for="c-40845962">[-]</label><label class="expand" for="c-40845962">[1 more]</label></div><br/><div class="children"><div class="content">Edit: ok i see it detects code.<p>I thought it was replacing bits of ANN with custom python functions.</div><br/></div></div><div id="40848461" class="c"><input type="checkbox" id="c-40848461" checked=""/><div class="controls bullet"><span class="by">ultra_nick</span><span>|</span><a href="#40845962">prev</a><span>|</span><label class="collapse" for="c-40848461">[-]</label><label class="expand" for="c-40848461">[1 more]</label></div><br/><div class="children"><div class="content">I feel like neural networks are increasingly going to look like code.<p>The next big innovation will be whoever figures out how to convert MOE style models into something like function calls.</div><br/></div></div></div></div></div></div></div></body></html>