<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702458075824" as="style"/><link rel="stylesheet" href="styles.css?v=1702458075824"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://knock.app/blog/zero-downtime-postgres-upgrades">Zero downtime Postgres upgrades</a> <span class="domain">(<a href="https://knock.app">knock.app</a>)</span></div><div class="subtext"><span>brentjanderson</span> | <span>88 comments</span></div><br/><div><div id="38621696" class="c"><input type="checkbox" id="c-38621696" checked=""/><div class="controls bullet"><span class="by">aeyes</span><span>|</span><a href="#38621080">next</a><span>|</span><label class="collapse" for="c-38621696">[-]</label><label class="expand" for="c-38621696">[6 more]</label></div><br/><div class="children"><div class="content">There is a better way than fully copying table content one by one which is very I&#x2F;O heavy and will not work if you have very large tables.<p>You can create a replication slot, take a snapshot, restore the snapshot to a new instance, advance the LSN and replicate from there - boom, you have a logical replica with all the data. Then you upgrade your logical replica.<p>This article from Instacart shows how to do it: <a href="https:&#x2F;&#x2F;archive.ph&#x2F;K5ZuJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.ph&#x2F;K5ZuJ</a><p>If I remember correctly the article has some small errors but I haven&#x27;t done this in a while and I don&#x27;t exactly remember what was wrong. But in general the process works, I have done it like this several times upgrading TB-sized instances.</div><br/><div id="38623425" class="c"><input type="checkbox" id="c-38623425" checked=""/><div class="controls bullet"><span class="by">samokhvalov</span><span>|</span><a href="#38621696">parent</a><span>|</span><a href="#38621730">next</a><span>|</span><label class="collapse" for="c-38623425">[-]</label><label class="expand" for="c-38623425">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can create a replication slot, take a snapshot, restore the snapshot to a new instance, advance the LSN and replicate from there - boom, you have a logical replica with all the data. Then you upgrade your logical replica.<p>This is a great recipe but needs small but important correction. We need to be careful with plugging pg_upgrade in this physical-to-logical replica conversion process: if we first start logical replication and then running pg_upgrade, there are risks of corruption – see discussion in pgsql-hackers <a href="https:&#x2F;&#x2F;www.postgresql.org&#x2F;message-id&#x2F;flat&#x2F;20230217075433.u5mjly4d5cr4hcfe%40jrouhaud" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.postgresql.org&#x2F;message-id&#x2F;flat&#x2F;20230217075433.u5...</a>. To solve this, we need first to create logical slot, advance the new cluster to slot&#x27;s LSN position (not starting logical replication yet), then run pg_upgrade, and only then logical replication – when the new cluster is already running on new PG version.<p>This is exactly how we (Postgres.ai) recently have helped GitLab upgrade multiple multi-TiB clusters under heavy load without any downtime at all (also involving PgBouncer&#x27;s PAUSE&#x2F;RESUME) - there will be a talk by Alexander Sosna presented later this week <a href="https:&#x2F;&#x2F;www.postgresql.eu&#x2F;events&#x2F;pgconfeu2023&#x2F;schedule&#x2F;session&#x2F;4791-how-we-execute-postgresql-major-upgrades-at-gitlab-with-zero-downtime&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.postgresql.eu&#x2F;events&#x2F;pgconfeu2023&#x2F;schedule&#x2F;sessi...</a> and there are some plans to publish details about it.</div><br/></div></div><div id="38621730" class="c"><input type="checkbox" id="c-38621730" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38621696">parent</a><span>|</span><a href="#38623425">prev</a><span>|</span><a href="#38623652">next</a><span>|</span><label class="collapse" for="c-38621730">[-]</label><label class="expand" for="c-38621730">[3 more]</label></div><br/><div class="children"><div class="content">OP here: we looked at this and were not confident in manually advancing the LSN as proposed, and detecting any inconsistency if we missed any replication as a result. Table by table seemed more reliable, despite being more painstaking.</div><br/><div id="38621820" class="c"><input type="checkbox" id="c-38621820" checked=""/><div class="controls bullet"><span class="by">aeyes</span><span>|</span><a href="#38621696">root</a><span>|</span><a href="#38621730">parent</a><span>|</span><a href="#38623652">next</a><span>|</span><label class="collapse" for="c-38621820">[-]</label><label class="expand" for="c-38621820">[2 more]</label></div><br/><div class="children"><div class="content">As long as you have the correct LSN there is no way for this to go wrong.<p>If you resume replication with an incorrect LSN replication will break immediately. I have spent way too much time trying to do this on my own before the blog post was written and I have seen it fail over and over again.<p>To give you more confidence, try with the LSN from the &quot;redo starts at&quot; log message. It looks close but it will always fail.</div><br/><div id="38623663" class="c"><input type="checkbox" id="c-38623663" checked=""/><div class="controls bullet"><span class="by">mmontagna9</span><span>|</span><a href="#38621696">root</a><span>|</span><a href="#38621820">parent</a><span>|</span><a href="#38623652">next</a><span>|</span><label class="collapse" for="c-38623663">[-]</label><label class="expand" for="c-38623663">[1 more]</label></div><br/><div class="children"><div class="content">Sadly this isn&#x27;t true. Postgres will happily replicate and skip data if you tell it too.<p>And there have been multiple bugs around logical replication in version ~10-15 that can cause data loss. None of these are directly related to lsn fiddling tho.</div><br/></div></div></div></div></div></div><div id="38623652" class="c"><input type="checkbox" id="c-38623652" checked=""/><div class="controls bullet"><span class="by">mmontagna9</span><span>|</span><a href="#38621696">parent</a><span>|</span><a href="#38621730">prev</a><span>|</span><a href="#38621080">next</a><span>|</span><label class="collapse" for="c-38623652">[-]</label><label class="expand" for="c-38623652">[1 more]</label></div><br/><div class="children"><div class="content">You caught our off by one bug :)</div><br/></div></div></div></div><div id="38621080" class="c"><input type="checkbox" id="c-38621080" checked=""/><div class="controls bullet"><span class="by">natbennett</span><span>|</span><a href="#38621696">prev</a><span>|</span><a href="#38620385">next</a><span>|</span><label class="collapse" for="c-38621080">[-]</label><label class="expand" for="c-38621080">[6 more]</label></div><br/><div class="children"><div class="content">The approach here is interesting and well-documented! However, this line gives me pause—<p>&gt; Modern customers expect 100% availability.<p>This is not my preference as a customer, nor has it been my experience as a vendor. For many workloads consistency is <i>much</i> more important than availability. I’m often relieved when I see a vendor announce a downtime window because it suggests they’re being sensible with my data.</div><br/><div id="38621118" class="c"><input type="checkbox" id="c-38621118" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38621080">parent</a><span>|</span><a href="#38622966">next</a><span>|</span><label class="collapse" for="c-38621118">[-]</label><label class="expand" for="c-38621118">[2 more]</label></div><br/><div class="children"><div class="content">OP Here - that&#x27;s great feedback! Our hope is to build confidence in both the reliability of our product _and_ the consistency of the workloads. Of course, presenting the illusion of consistency while being flaky is far worse than managing customer expectations and taking intentional downtime to, in the long run, have better uptime.<p>Indeed, having periodic maintenance windows expected up-front probably leads to more robust architectures overall: customers building in the failsafes they need to tolerate downtime leads to more resilience. Teams that can trust their customers in that way can, in turn, take the time they need to make the investments they need to build a better product.<p>Perhaps this will be the blog post we write after our next major version upgrade: expectation setting around downtime _is_ the way to very high uptime.</div><br/><div id="38621845" class="c"><input type="checkbox" id="c-38621845" checked=""/><div class="controls bullet"><span class="by">natbennett</span><span>|</span><a href="#38621080">root</a><span>|</span><a href="#38621118">parent</a><span>|</span><a href="#38622966">next</a><span>|</span><label class="collapse" for="c-38621845">[-]</label><label class="expand" for="c-38621845">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I’d be a lot more confident about this if you talked some about consistency vs. availability and the details of your workload that made you want to choose this trade off.<p>I have potentially a weird experience path here — worked with Galera a bunch early on because when we asked customers if they wanted HA they said “yes absolutely” so we sunk a ton of time into absolutely never ever going down.<p>When we finally presented the trade off space (basically that 10 minute downtime windows occasionally could basically guarantee that we wouldn’t have data loss) we ended up building a very different product.</div><br/></div></div></div></div><div id="38622966" class="c"><input type="checkbox" id="c-38622966" checked=""/><div class="controls bullet"><span class="by">gfody</span><span>|</span><a href="#38621080">parent</a><span>|</span><a href="#38621118">prev</a><span>|</span><a href="#38620385">next</a><span>|</span><label class="collapse" for="c-38622966">[-]</label><label class="expand" for="c-38622966">[3 more]</label></div><br/><div class="children"><div class="content">depends who the customer is, I&#x27;m a customer of AWS and I expect 100% availability, mostly because my customers are everywhere in the world and there&#x27;s no available window for downtime</div><br/><div id="38624061" class="c"><input type="checkbox" id="c-38624061" checked=""/><div class="controls bullet"><span class="by">guiriduro</span><span>|</span><a href="#38621080">root</a><span>|</span><a href="#38622966">parent</a><span>|</span><a href="#38623093">next</a><span>|</span><label class="collapse" for="c-38624061">[-]</label><label class="expand" for="c-38624061">[1 more]</label></div><br/><div class="children"><div class="content">If you have this 100% availability expectation you&#x27;re going to have to face the reality that DBMS versions fall out of support, you will have to upgrade or AWS will force-upgrade you their way, the AWS-provided default mechanism has significant DB-size dependent downtime (in order to maintain consistency, and you really don&#x27;t want to lose that), and that the only alternative is to go through the pain of sifting through your database estate and logically replicating table by table with verification as shown in this article, with care especially for large tables and reindexing, and you can&#x27;t avoid that if you have the (IMO mostly unreasonable) expectation of 100% availability.
Change the wheel mid-journey or take a pitstop.</div><br/></div></div><div id="38623093" class="c"><input type="checkbox" id="c-38623093" checked=""/><div class="controls bullet"><span class="by">nightfly</span><span>|</span><a href="#38621080">root</a><span>|</span><a href="#38622966">parent</a><span>|</span><a href="#38624061">prev</a><span>|</span><a href="#38620385">next</a><span>|</span><label class="collapse" for="c-38623093">[-]</label><label class="expand" for="c-38623093">[1 more]</label></div><br/><div class="children"><div class="content">But you don&#x27;t expect 100% availability from every server for every service in every region do you?</div><br/></div></div></div></div></div></div><div id="38620385" class="c"><input type="checkbox" id="c-38620385" checked=""/><div class="controls bullet"><span class="by">CubsFan1060</span><span>|</span><a href="#38621080">prev</a><span>|</span><a href="#38621403">next</a><span>|</span><label class="collapse" for="c-38620385">[-]</label><label class="expand" for="c-38620385">[5 more]</label></div><br/><div class="children"><div class="content">AWS supports blue green deployments now. <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;about-aws&#x2F;whats-new&#x2F;2023&#x2F;10&#x2F;amazon-rds-blue-green-deployments-aurora-rds-postgresql&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;aws.amazon.com&#x2F;about-aws&#x2F;whats-new&#x2F;2023&#x2F;10&#x2F;amazon-rd...</a></div><br/><div id="38621672" class="c"><input type="checkbox" id="c-38621672" checked=""/><div class="controls bullet"><span class="by">paulryanrogers</span><span>|</span><a href="#38620385">parent</a><span>|</span><a href="#38620458">next</a><span>|</span><label class="collapse" for="c-38621672">[-]</label><label class="expand" for="c-38621672">[1 more]</label></div><br/><div class="children"><div class="content">Having just tried a few weeks back ... don&#x27;t rely on it for PostgreSQL yet. After a few back and forths my experiment got stalled for <i>hours</i> before AWS UI admitted the switch over didn&#x27;t take. Thankfully it failed safe. But I have no faith in being able to time the actual switch over for any GB+ dataset.</div><br/></div></div><div id="38620458" class="c"><input type="checkbox" id="c-38620458" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620385">parent</a><span>|</span><a href="#38621672">prev</a><span>|</span><a href="#38621403">next</a><span>|</span><label class="collapse" for="c-38620458">[-]</label><label class="expand" for="c-38620458">[3 more]</label></div><br/><div class="children"><div class="content">You’re right! OP here. We were on 11.9 which is not supported by Blue&#x2F;green deployments for Aurora. Maybe next time.</div><br/><div id="38621210" class="c"><input type="checkbox" id="c-38621210" checked=""/><div class="controls bullet"><span class="by">dmattia</span><span>|</span><a href="#38620385">root</a><span>|</span><a href="#38620458">parent</a><span>|</span><a href="#38621403">next</a><span>|</span><label class="collapse" for="c-38621210">[-]</label><label class="expand" for="c-38621210">[2 more]</label></div><br/><div class="children"><div class="content">Would upgrading to 11.21 and then using blue&#x2F;green have been easier? I&#x27;m asking as someone with RDS postgres-aurora running 11.9 right now, so I&#x27;m genuinely curious on your thoughts</div><br/><div id="38621250" class="c"><input type="checkbox" id="c-38621250" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620385">root</a><span>|</span><a href="#38621210">parent</a><span>|</span><a href="#38621403">next</a><span>|</span><label class="collapse" for="c-38621250">[-]</label><label class="expand" for="c-38621250">[1 more]</label></div><br/><div class="children"><div class="content">That might be possible. We were within days of performing our upgrade when the blue&#x2F;green feature became available for Postgres, so we didn&#x27;t consider it for our work.<p>You may be able to boot up an 11.21 replica in an existing Aurora cluster as a read replica, and then failover to that replica as your primary, which would be a minimally disruptive process if your application is designed to tolerate replica failover.<p>From there, you could upgrade the rest of your cluster to 11.21, and then use the blue&#x2F;green upgrade process for AWS. If you do, I&#x27;d love to hear about how it goes as we will definitely consider the blue&#x2F;green feature next time.</div><br/></div></div></div></div></div></div></div></div><div id="38621403" class="c"><input type="checkbox" id="c-38621403" checked=""/><div class="controls bullet"><span class="by">shayonj</span><span>|</span><a href="#38620385">prev</a><span>|</span><a href="#38620705">next</a><span>|</span><label class="collapse" for="c-38621403">[-]</label><label class="expand" for="c-38621403">[2 more]</label></div><br/><div class="children"><div class="content">This is great! I wrote a tool that automates most of the things you came across. If you find it useful or would like to extend it with your feedback&#x2F;ideas, I&#x27;d love to have them! Thanks for sharing<p><a href="https:&#x2F;&#x2F;github.com&#x2F;shayonj&#x2F;pg_easy_replicate">https:&#x2F;&#x2F;github.com&#x2F;shayonj&#x2F;pg_easy_replicate</a></div><br/><div id="38621463" class="c"><input type="checkbox" id="c-38621463" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38621403">parent</a><span>|</span><a href="#38620705">next</a><span>|</span><label class="collapse" for="c-38621463">[-]</label><label class="expand" for="c-38621463">[1 more]</label></div><br/><div class="children"><div class="content">Neat tool! Some of our findings for large tables could be interesting for a tool like this, making it easier to apply the right strategy to the right tables. Having something like this with those strategies could be indispensable to teams running a migration like this in the future.</div><br/></div></div></div></div><div id="38620705" class="c"><input type="checkbox" id="c-38620705" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#38621403">prev</a><span>|</span><a href="#38624039">next</a><span>|</span><label class="collapse" for="c-38620705">[-]</label><label class="expand" for="c-38620705">[24 more]</label></div><br/><div class="children"><div class="content">&gt; No amount of downtime - scheduled or otherwise - is acceptable for a service like Knock<p>doubt.jpeg<p>If you have a complex system, you have incidents, you have downtime. A 15min downtime window announced in advance is fine for approximately 100% of SaaS businesses. You&#x27;re not a hospital and you&#x27;re not the power station. So much fake work gets done because people think their services are more important than they are. The engineering time you invested into this, invested into the product, or in making the rest of your dev team faster, would&#x27;ve likely made your users much happier. Specially if you can queue your notifications up and catch up after the downtime window.<p>If you have enterprise contracts with SLAs defining paybacks for 15min downtime windows, then I guess you could justify it, but most people don&#x27;t. And like I mentioned, you likely already have a handful of incidents of the same or higher duration in practice anyway.<p>This is specially relevant with database migrations where the difference in work to create a migration of &quot;little downtime&quot; to &quot;zero downtime&quot; is usually significant. In this case though, seeing as this was a one time thing (newer versions of PostgreSQL on RDS allow it out of the box) it is specially hard to justify in my opinion, as opposed to if this was going to be reused across many versions or many databases powering the service.</div><br/><div id="38620797" class="c"><input type="checkbox" id="c-38620797" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38621651">next</a><span>|</span><label class="collapse" for="c-38620797">[-]</label><label class="expand" for="c-38620797">[5 more]</label></div><br/><div class="children"><div class="content">OP here: It’s true that all services have downtime for one reason or another. We discussed taking an outage window, but one thing that we kept coming back to was how we might trial run the upgrade with production data. Having a replica on PG 15 that was up to date with production data was invaluable for verifying our workloads worked as expected. Using a live replica makes it possible to trial run in production with minimal impact.<p>A key learning for me from this migration was how nice it can be to track and mitigate all of the risks you can think of for a project like this. The risk of an in-place upgrade in the end seemed higher than the risks associated with the route we chose, outage windows notwithstanding.<p>As a bonus, if we need this approach in the future, this blog post should give us a head start, saving us many weeks of work. We hope it helps other teams in similar situations do the same.</div><br/><div id="38620878" class="c"><input type="checkbox" id="c-38620878" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38620797">parent</a><span>|</span><a href="#38621651">next</a><span>|</span><label class="collapse" for="c-38620878">[-]</label><label class="expand" for="c-38620878">[4 more]</label></div><br/><div class="children"><div class="content">&gt; We discussed taking an outage window, but one thing that we kept coming back to was how we might trial run the upgrade with production data<p>1. You snapshot your RDS database (or use one of the existing ones I hope you have)<p>2. You restore that snapshot into a database running in parallel without live traffic.<p>3. You run the test upgrade there and check how long it takes.<p>4. You destroy the test database and announce a maintenance window for the same duration the test took + buffer.<p>I agree it&#x27;s a good project to exercise some &quot;migration&quot; muscle, it just doesn&#x27;t seem like the payoff is there when, like I mentioned above, AWS supports this out of the box from now on since you upgraded to a version compatible with their zero downtime native approach.<p>I think the only way this makes sense is if you do it for the blog post and use that to hire and for marketing, signaling your engineering practices and that you care about reliability.<p>By the way, I realize how I come across, and let me tell you I say this having myself done projects like this where looking back I think we did them more because they were cool than because they made sense. Live and learn.</div><br/><div id="38620960" class="c"><input type="checkbox" id="c-38620960" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38620878">parent</a><span>|</span><a href="#38621651">next</a><span>|</span><label class="collapse" for="c-38620960">[-]</label><label class="expand" for="c-38620960">[3 more]</label></div><br/><div class="children"><div class="content">We actually did those steps as part of our overall assessment, and you&#x27;re right that we could have taken an outage window for that long and called it a day. We decided the tradeoff wasn&#x27;t worth it for our situation, but taking the outage window is definitely a viable option.<p>I&#x27;m sympathetic to your comment that 15 minutes of planned downtime is fine for approximately 100% of SaaS companies. That&#x27;s probably true here too, and maybe the work of doing this kind of upgrade was a waste in that regard. But, in considering the kind of product experience we would want for ourselves, zero downtime seems better than no downtime. The opportunity cost of feature work over the same window is real, but so is the reputation we hope to build as a platform that &quot;just works&quot; even if it seems crazy the lengths we might go to so that our customers don&#x27;t have to think about it.</div><br/><div id="38621001" class="c"><input type="checkbox" id="c-38621001" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38620960">parent</a><span>|</span><a href="#38621501">next</a><span>|</span><label class="collapse" for="c-38621001">[-]</label><label class="expand" for="c-38621001">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The opportunity cost of feature work over the same window is real, but so is the reputation we hope to build as a platform that &quot;just works&quot; even if it seems crazy the lengths we might go to so that our customers don&#x27;t have to think about it<p>This part can definitely make sense, and if nothing else it can foster an engineering culture of &quot;we care&quot;, which is great. I just wanted to show the other side but from your answers it seems like the team weighted the options. It&#x27;s definitely a cool project to work on. Thanks a lot for engaging with a random grumpy guy on HN!</div><br/></div></div><div id="38621501" class="c"><input type="checkbox" id="c-38621501" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38620960">parent</a><span>|</span><a href="#38621001">prev</a><span>|</span><a href="#38621651">next</a><span>|</span><label class="collapse" for="c-38621501">[-]</label><label class="expand" for="c-38621501">[1 more]</label></div><br/><div class="children"><div class="content">Random comment, but just wanted to say I really appreciate your blog post, but also I appreciate the informative and helpful discussion between you and vasco here. Feel like this could have easily devolved into defensiveness on either side, but instead I learned a lot from both of your responses - I feel like these kinds of interations are HN at its best. Thanks!</div><br/></div></div></div></div></div></div></div></div><div id="38621651" class="c"><input type="checkbox" id="c-38621651" checked=""/><div class="controls bullet"><span class="by">jmhmd</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38620797">prev</a><span>|</span><a href="#38621665">next</a><span>|</span><label class="collapse" for="c-38621651">[-]</label><label class="expand" for="c-38621651">[4 more]</label></div><br/><div class="children"><div class="content">It’s funny to me as a physician to see “you’re not a hospital” as an example of a system that cannot tolerate downtime. Epic, probably the biggest EHR provider in the US, has planned downtime for upgrades at least monthly, for 30-60 min each.</div><br/><div id="38622035" class="c"><input type="checkbox" id="c-38622035" checked=""/><div class="controls bullet"><span class="by">swamp_donkey</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621651">parent</a><span>|</span><a href="#38622067">next</a><span>|</span><label class="collapse" for="c-38622035">[-]</label><label class="expand" for="c-38622035">[1 more]</label></div><br/><div class="children"><div class="content">I designed control panel modifications and programmed an upgrade to a hospital diesel generation system so they could transfer from diesel back to utility without an outage, and have planned transfer of load to diesel without turning the lights out.<p>We had three windows at 1 am where any new critical patients would be diverted to a different hospital. The first we used for major maintenance to the breakers in the switchgear, the second we used for modifications to the bus work, and the last outage was to test the operation of the new control system.<p>They do a transfer to diesel every month and the whole hospital is aware of it in case it results in a blackout.</div><br/></div></div><div id="38622067" class="c"><input type="checkbox" id="c-38622067" checked=""/><div class="controls bullet"><span class="by">hedora</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621651">parent</a><span>|</span><a href="#38622035">prev</a><span>|</span><a href="#38622019">next</a><span>|</span><label class="collapse" for="c-38622067">[-]</label><label class="expand" for="c-38622067">[1 more]</label></div><br/><div class="children"><div class="content">So, the ER just shuts down for that hour?<p>Doesn’t epic cover everything from patient admission to medical imaging?</div><br/></div></div><div id="38622019" class="c"><input type="checkbox" id="c-38622019" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621651">parent</a><span>|</span><a href="#38622067">prev</a><span>|</span><a href="#38621665">next</a><span>|</span><label class="collapse" for="c-38622019">[-]</label><label class="expand" for="c-38622019">[1 more]</label></div><br/><div class="children"><div class="content">Fine as long as there is a workaround or the impact has been assessed.</div><br/></div></div></div></div><div id="38621665" class="c"><input type="checkbox" id="c-38621665" checked=""/><div class="controls bullet"><span class="by">aeyes</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38621651">prev</a><span>|</span><a href="#38624136">next</a><span>|</span><label class="collapse" for="c-38621665">[-]</label><label class="expand" for="c-38621665">[1 more]</label></div><br/><div class="children"><div class="content">Except that there is no way to upgrade a Postgres instance on RDS with a planned 15 minute downtime. You can&#x27;t control when the reboots happen, you start the process and the cutover might kick in an hour later, two hours later, three hours later - you don&#x27;t know when the reboots are going to happen and you can&#x27;t control it.<p>If you have replicas they&#x27;ll upgrade in parallel and will reboot at random times for even more fun.<p>So unless you can afford random unavailability in a timeframe which can last several hours (depending on DB size) the logical replication approach is the only way to do upgrades on RDS.<p>The bigger the instance, the harder the problem.</div><br/></div></div><div id="38624136" class="c"><input type="checkbox" id="c-38624136" checked=""/><div class="controls bullet"><span class="by">PetahNZ</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38621665">prev</a><span>|</span><a href="#38621244">next</a><span>|</span><label class="collapse" for="c-38624136">[-]</label><label class="expand" for="c-38624136">[1 more]</label></div><br/><div class="children"><div class="content">15 minutes to migrate a large DB? It takes days just to run an alter column on our DB.</div><br/></div></div><div id="38621244" class="c"><input type="checkbox" id="c-38621244" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38624136">prev</a><span>|</span><a href="#38622335">next</a><span>|</span><label class="collapse" for="c-38621244">[-]</label><label class="expand" for="c-38621244">[3 more]</label></div><br/><div class="children"><div class="content">&gt; A 15min downtime window announced in advance is fine for approximately 100% of SaaS businesses<p>Except that there will be competitors who don&#x27;t have a downtime every month.<p>And who are thus placing <i>my</i> needs ahead of their own.<p>Because your outage is my outage as well.</div><br/><div id="38621404" class="c"><input type="checkbox" id="c-38621404" checked=""/><div class="controls bullet"><span class="by">toomuchtodo</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621244">parent</a><span>|</span><a href="#38621478">next</a><span>|</span><label class="collapse" for="c-38621404">[-]</label><label class="expand" for="c-38621404">[1 more]</label></div><br/><div class="children"><div class="content">Unreasonable customers are best sent to competitors. Let them be their problem. All revenue is not equal.</div><br/></div></div><div id="38621478" class="c"><input type="checkbox" id="c-38621478" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621244">parent</a><span>|</span><a href="#38621404">prev</a><span>|</span><a href="#38622335">next</a><span>|</span><label class="collapse" for="c-38621478">[-]</label><label class="expand" for="c-38621478">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Except that there will be competitors who don&#x27;t have a downtime every month.<p>Who said anything about downtime every month? Most companies I know do major DB version upgrades once every 2 years max, often less frequently.</div><br/></div></div></div></div><div id="38622335" class="c"><input type="checkbox" id="c-38622335" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38621244">prev</a><span>|</span><a href="#38622012">next</a><span>|</span><label class="collapse" for="c-38622335">[-]</label><label class="expand" for="c-38622335">[1 more]</label></div><br/><div class="children"><div class="content">The real problem with downtime is when all systems are down at the same time.<p>If Jira is down fifteen minutes a day that rarely affects me. I have other tasks in my work queue that I can substitute. Worst case with multiple outages there’s always documentation I promised someone. But when the entire Atlassian suite goes tits up at the same time, it gets harder for me to keep a buffer of work going. Getting every app in your enterprise using the same storage array is a good way to go from 5% productivity loss to 95%.</div><br/></div></div><div id="38622012" class="c"><input type="checkbox" id="c-38622012" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38622335">prev</a><span>|</span><a href="#38621678">next</a><span>|</span><label class="collapse" for="c-38622012">[-]</label><label class="expand" for="c-38622012">[1 more]</label></div><br/><div class="children"><div class="content">If Google Docs were down for 15 minutes while you were trying to get say a CV together or refer to some notes it would be pretty frustrating. SaaS is replacing the desktop so the expectation is similar, I can access my data whenever I want. And 2am might be OK except many SaaS have global customers.</div><br/></div></div><div id="38621318" class="c"><input type="checkbox" id="c-38621318" checked=""/><div class="controls bullet"><span class="by">opportune</span><span>|</span><a href="#38620705">parent</a><span>|</span><a href="#38621678">prev</a><span>|</span><a href="#38624039">next</a><span>|</span><label class="collapse" for="c-38621318">[-]</label><label class="expand" for="c-38621318">[6 more]</label></div><br/><div class="children"><div class="content">&gt; A 15min downtime window announced in advance is fine for approximately 100% of SaaS businesses<p>What? As a customer, this would piss me off to no end and honestly be a dealbreaker for something like payments or general hosting.<p>It&#x27;s pushing dysfunction onto your customers, and if your customers are technically experienced, they&#x27;d know it&#x27;s a completely avoidable problem.</div><br/><div id="38623688" class="c"><input type="checkbox" id="c-38623688" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621318">parent</a><span>|</span><a href="#38621471">next</a><span>|</span><label class="collapse" for="c-38623688">[-]</label><label class="expand" for="c-38623688">[1 more]</label></div><br/><div class="children"><div class="content">&gt; and if your customers are technically experienced, they&#x27;d know it&#x27;s a completely avoidable problem.<p>If they&#x27;re technically experienced, they know every 9 costs exponentially more money, and probably agree that it&#x27;s a good tradeoff.</div><br/></div></div><div id="38621471" class="c"><input type="checkbox" id="c-38621471" checked=""/><div class="controls bullet"><span class="by">FpUser</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621318">parent</a><span>|</span><a href="#38623688">prev</a><span>|</span><a href="#38624039">next</a><span>|</span><label class="collapse" for="c-38621471">[-]</label><label class="expand" for="c-38621471">[4 more]</label></div><br/><div class="children"><div class="content">Frankly I do not recall a single service without downtime, this includes banks I use. Yes I&#x27;d be mightily upset if said downtimes had lasted for days. 15 min - I do not give a flying hoot as long as it is not too often.</div><br/><div id="38621527" class="c"><input type="checkbox" id="c-38621527" checked=""/><div class="controls bullet"><span class="by">CubsFan1060</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621471">parent</a><span>|</span><a href="#38624039">next</a><span>|</span><label class="collapse" for="c-38621527">[-]</label><label class="expand" for="c-38621527">[3 more]</label></div><br/><div class="children"><div class="content">I suspect it&#x27;s likely that the services that the other posters use _do_ have downtime, they are just done at hours where they don&#x27;t notice them.<p>I would literally have no idea if gmail went down from 1-2 am any day of the week.  Hell. I wouldn&#x27;t notice if it was down everyday from 1-2am.</div><br/><div id="38621691" class="c"><input type="checkbox" id="c-38621691" checked=""/><div class="controls bullet"><span class="by">ljm</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621527">parent</a><span>|</span><a href="#38624039">next</a><span>|</span><label class="collapse" for="c-38621691">[-]</label><label class="expand" for="c-38621691">[2 more]</label></div><br/><div class="children"><div class="content">If you&#x27;ve got planned maintenance that requires downtime then you are always scheduling it at the times when your traffic is at its lowest. How much you avoid hard downtime is a function of how much money you&#x27;re willing to spend on the maintenance.</div><br/><div id="38622435" class="c"><input type="checkbox" id="c-38622435" checked=""/><div class="controls bullet"><span class="by">CubsFan1060</span><span>|</span><a href="#38620705">root</a><span>|</span><a href="#38621691">parent</a><span>|</span><a href="#38624039">next</a><span>|</span><label class="collapse" for="c-38622435">[-]</label><label class="expand" for="c-38622435">[1 more]</label></div><br/><div class="children"><div class="content">Or how much revenue will be reduced by downtime.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38624039" class="c"><input type="checkbox" id="c-38624039" checked=""/><div class="controls bullet"><span class="by">oopsthrowpass</span><span>|</span><a href="#38620705">prev</a><span>|</span><a href="#38622217">next</a><span>|</span><label class="collapse" for="c-38624039">[-]</label><label class="expand" for="c-38624039">[1 more]</label></div><br/><div class="children"><div class="content">There is no problem in distributed systems that can&#x27;t be solved with a well placed sleep(1000) :D<p>But anyway, good job, Postgres is quite a DBA unfriendly system (although better than it used to be still pretty bad)</div><br/></div></div><div id="38622217" class="c"><input type="checkbox" id="c-38622217" checked=""/><div class="controls bullet"><span class="by">tehlike</span><span>|</span><a href="#38624039">prev</a><span>|</span><a href="#38620208">next</a><span>|</span><label class="collapse" for="c-38622217">[-]</label><label class="expand" for="c-38622217">[5 more]</label></div><br/><div class="children"><div class="content">The sequence thing is definitely interesting, I stopped using them a while ago, using mostly sequential uuid (or uuid v7), or use something like HiLo 
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hi&#x2F;Lo_algorithm" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hi&#x2F;Lo_algorithm</a></div><br/><div id="38622411" class="c"><input type="checkbox" id="c-38622411" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38622217">parent</a><span>|</span><a href="#38622555">next</a><span>|</span><label class="collapse" for="c-38622411">[-]</label><label class="expand" for="c-38622411">[2 more]</label></div><br/><div class="children"><div class="content">OP here - we avoid sequences in all but one part of our application due to a dependency. We use [KSUIDs][1] and UUID v4 in various places. This one &quot;gotcha&quot; applies to any sequence, so it&#x27;s worth calling out as general advice when running a migration like this.<p>[1]: <a href="https:&#x2F;&#x2F;segment.com&#x2F;blog&#x2F;a-brief-history-of-the-uuid&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;segment.com&#x2F;blog&#x2F;a-brief-history-of-the-uuid&#x2F;</a></div><br/><div id="38622610" class="c"><input type="checkbox" id="c-38622610" checked=""/><div class="controls bullet"><span class="by">tehlike</span><span>|</span><a href="#38622217">root</a><span>|</span><a href="#38622411">parent</a><span>|</span><a href="#38622555">next</a><span>|</span><label class="collapse" for="c-38622610">[-]</label><label class="expand" for="c-38622610">[1 more]</label></div><br/><div class="children"><div class="content">Definitely great call out. Thanks for writing this.</div><br/></div></div></div></div><div id="38622555" class="c"><input type="checkbox" id="c-38622555" checked=""/><div class="controls bullet"><span class="by">pmarreck</span><span>|</span><a href="#38622217">parent</a><span>|</span><a href="#38622411">prev</a><span>|</span><a href="#38620208">next</a><span>|</span><label class="collapse" for="c-38622555">[-]</label><label class="expand" for="c-38622555">[2 more]</label></div><br/><div class="children"><div class="content">This PL&#x2F;pgSQL function might help others looking to keep uuidv7 generation responsibility within the database until it&#x27;s natively supported:<p><pre><code>  -- IETF Draft Spec: https:&#x2F;&#x2F;www.ietf.org&#x2F;archive&#x2F;id&#x2F;draft-peabody-dispatch-new-uuid-format-01.html

  CREATE SEQUENCE uuidv7_seq MAXVALUE 4095; -- A 12-bit sequence

  CREATE OR REPLACE FUNCTION generate_uuidv7()
  RETURNS uuid AS $$
  DECLARE
    unixts bigint;
    msec bigint;
    seq bigint;
    rand bigint;
    uuid_hex varchar;
  BEGIN
    -- Get current UNIX epoch in milliseconds
    unixts := (EXTRACT(EPOCH FROM clock_timestamp()) * 1000)::bigint;

    -- Extract milliseconds
    msec := unixts % 1000; -- Milliseconds

    -- Get next value from the sequence for the &quot;motonic clock sequence counter&quot; value
    seq := NEXTVAL(&#x27;uuidv7_seq&#x27;);

    -- Generate a random 62-bit number
    rand := (RANDOM() * 4611686018427387903)::bigint; -- 62-bit random number

    -- Construct the UUID
    uuid_hex := LPAD(TO_HEX(((unixts &lt;&lt; 28) + (msec &lt;&lt; 16) + (7 &lt;&lt; 12) + seq)), 16, &#x27;0&#x27;) ||
                LPAD(TO_HEX((2 &lt;&lt; 62) + rand), 16, &#x27;0&#x27;);

    -- Return the UUID
    RETURN uuid_hex::uuid;
  END;
  $$ LANGUAGE plpgsql VOLATILE;

  SELECT generate_uuidv7();</code></pre></div><br/><div id="38622623" class="c"><input type="checkbox" id="c-38622623" checked=""/><div class="controls bullet"><span class="by">tehlike</span><span>|</span><a href="#38622217">root</a><span>|</span><a href="#38622555">parent</a><span>|</span><a href="#38620208">next</a><span>|</span><label class="collapse" for="c-38622623">[-]</label><label class="expand" for="c-38622623">[1 more]</label></div><br/><div class="children"><div class="content">Keeping id generation inside the app is useful, you can batch multiple statements (e.g. insert product, insert product details in a single query, or other sorts of dependencies). You don&#x27;t have to wait for first insertion to finish to get the id of the record, for example.</div><br/></div></div></div></div></div></div><div id="38620208" class="c"><input type="checkbox" id="c-38620208" checked=""/><div class="controls bullet"><span class="by">ohduran</span><span>|</span><a href="#38622217">prev</a><span>|</span><a href="#38620883">next</a><span>|</span><label class="collapse" for="c-38620208">[-]</label><label class="expand" for="c-38620208">[5 more]</label></div><br/><div class="children"><div class="content">Not to downplay the absolute behemoth of a task they manage to pull out successfully...but why not upgrading as new versions came along, with less fanfare?<p>It is a great read, but I can&#x27;t shake the feeling that it&#x27;s about a bunch of sailors that, instead of going around a huge storm, decided to go through it knowing fully well that it could end in tragedy.<p>Is the small upgrades out of the question in this case? As in &quot;each small one costs us as much downtime as a big one, so we put it off for as long as we could&quot; (they hint at that in the intro, but I might be reading too much into it).</div><br/><div id="38620483" class="c"><input type="checkbox" id="c-38620483" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620208">parent</a><span>|</span><a href="#38621434">next</a><span>|</span><label class="collapse" for="c-38620483">[-]</label><label class="expand" for="c-38620483">[2 more]</label></div><br/><div class="children"><div class="content">OP here - we would have used the same approach for the minor upgrades. This isn’t a case of “we procrastinated ourselves into a corner” and more a matter of “if it isn’t broke, don’t fix it” recognizing we would need to make the jump eventually.</div><br/><div id="38624001" class="c"><input type="checkbox" id="c-38624001" checked=""/><div class="controls bullet"><span class="by">NomDePlum</span><span>|</span><a href="#38620208">root</a><span>|</span><a href="#38620483">parent</a><span>|</span><a href="#38621434">next</a><span>|</span><label class="collapse" for="c-38624001">[-]</label><label class="expand" for="c-38624001">[1 more]</label></div><br/><div class="children"><div class="content">Just for your information, minor upgrades on Aurora Postgres does now claim increased resilience across minor upgrades, there are some caveats despite the Zero Downtime naming: <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonRDS&#x2F;latest&#x2F;AuroraUserGuide&#x2F;USER_UpgradeDBInstance.PostgreSQL.html#USER_UpgradeDBInstance.PostgreSQL.Minor" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;AmazonRDS&#x2F;latest&#x2F;AuroraUserGuide...</a><p>I&#x27;ve relied on this as the minor upgrade method since it was available and it has worked as advertised, with no perceivable issues. This may be traffic and operation dependent obviously but worth having a look at.<p>Worth saying we do the minor upgrades incrementally, intra-day and a few  weeks to a month after they are available, as a matter of routine, with a well documented process. Overhead is minimal to practically zero.</div><br/></div></div></div></div><div id="38621434" class="c"><input type="checkbox" id="c-38621434" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38620208">parent</a><span>|</span><a href="#38620483">prev</a><span>|</span><a href="#38620375">next</a><span>|</span><label class="collapse" for="c-38621434">[-]</label><label class="expand" for="c-38621434">[1 more]</label></div><br/><div class="children"><div class="content">Upgrading N versions is just as much as a threat to availability regardless if N is 1 or 3.</div><br/></div></div><div id="38620375" class="c"><input type="checkbox" id="c-38620375" checked=""/><div class="controls bullet"><span class="by">CubsFan1060</span><span>|</span><a href="#38620208">parent</a><span>|</span><a href="#38621434">prev</a><span>|</span><a href="#38620883">next</a><span>|</span><label class="collapse" for="c-38620375">[-]</label><label class="expand" for="c-38620375">[1 more]</label></div><br/><div class="children"><div class="content">Each one incurs some downtime. If their real answer is less than 60 seconds, then they’d have incurred that multiple times on the road to 15.</div><br/></div></div></div></div><div id="38620883" class="c"><input type="checkbox" id="c-38620883" checked=""/><div class="controls bullet"><span class="by">TechIsCool</span><span>|</span><a href="#38620208">prev</a><span>|</span><a href="#38621426">next</a><span>|</span><label class="collapse" for="c-38620883">[-]</label><label class="expand" for="c-38620883">[5 more]</label></div><br/><div class="children"><div class="content">With the mention of AWS RDS and Aurora, I am curious if you had thought about creating a replication slot, adding a replica to the cluster and then promoting the replica to its own cluster. Then connecting the new cluster to the original with the replication slot based on the position of the snapshot. This would save the large original replication time and also keep the sequences consistent without manual intervention.</div><br/><div id="38621004" class="c"><input type="checkbox" id="c-38621004" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620883">parent</a><span>|</span><a href="#38621426">next</a><span>|</span><label class="collapse" for="c-38621004">[-]</label><label class="expand" for="c-38621004">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a very interesting approach, I&#x27;m not sure if the sequences would remain consistent under that model or not. AWS RDS Aurora also requires you to drop replication slots when performing version upgrades, so we would unfortunately have lost the LSNs for replication slots that we use to synchronize with other services (e.g. data warehouse).<p>I&#x27;d look into it more next time if it weren&#x27;t for the fact that AWS now supports Blue&#x2F;Green upgrades on Aurora for our version of Postgres. But, it&#x27;s an interesting approach for sure.</div><br/><div id="38621161" class="c"><input type="checkbox" id="c-38621161" checked=""/><div class="controls bullet"><span class="by">TechIsCool</span><span>|</span><a href="#38620883">root</a><span>|</span><a href="#38621004">parent</a><span>|</span><a href="#38621426">next</a><span>|</span><label class="collapse" for="c-38621161">[-]</label><label class="expand" for="c-38621161">[3 more]</label></div><br/><div class="children"><div class="content">Yeah its been nice to leverage this while working on some of our larger multi TB non-partitioned clusters. We have seen snapshots restore in under 10 minutes across AWS Accounts (same region) as long as you already have one snapshot shipped with the same KMS keys. We have been upgrading DBs to lift out of RDS into Aurora Serverless.<p>If anyone here knows how to get LSN numbers after an upgrade&#x2F;cluster replacement. I would love to hear about it since its always painful to get Debezium reconnected when a cluster dies.</div><br/><div id="38621222" class="c"><input type="checkbox" id="c-38621222" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620883">root</a><span>|</span><a href="#38621161">parent</a><span>|</span><a href="#38621426">next</a><span>|</span><label class="collapse" for="c-38621222">[-]</label><label class="expand" for="c-38621222">[2 more]</label></div><br/><div class="children"><div class="content">I looked at getting LSN numbers after an upgrade&#x2F;cluster replacement, and IIRC restoring from a snapshot emits LSN information into the logs, but it&#x27;s a bit of of a mixed bag as to whether or not you get the __right__ LSN out the other side. Because the LSN is more a measure of how many bytes have been written within a cluster, it&#x27;s not something that meaningfully translates to other clusters, unfortunately.</div><br/><div id="38621356" class="c"><input type="checkbox" id="c-38621356" checked=""/><div class="controls bullet"><span class="by">TechIsCool</span><span>|</span><a href="#38620883">root</a><span>|</span><a href="#38621222">parent</a><span>|</span><a href="#38621426">next</a><span>|</span><label class="collapse" for="c-38621356">[-]</label><label class="expand" for="c-38621356">[1 more]</label></div><br/><div class="children"><div class="content">Agreed, the snapshot does output a message in the logs but based on our conversations with AWS it was suggested that we use the SQL Command to determine the LSN. Sometimes depending on revision you won&#x27;t get the logs and other times the log line is emitted twice based on the internal RDS consistency checks. Makes me long for GTIDs from MySQL MariaDB Galera[1]. They worked super well and we never looked back at my last company.<p>[1] <a href="https:&#x2F;&#x2F;mariadb.com&#x2F;kb&#x2F;en&#x2F;using-mariadb-gtids-with-mariadb-galera-cluster&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;mariadb.com&#x2F;kb&#x2F;en&#x2F;using-mariadb-gtids-with-mariadb-g...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="38621426" class="c"><input type="checkbox" id="c-38621426" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38620883">prev</a><span>|</span><a href="#38621900">next</a><span>|</span><label class="collapse" for="c-38621426">[-]</label><label class="expand" for="c-38621426">[3 more]</label></div><br/><div class="children"><div class="content">Another epic win for the BEAM!</div><br/><div id="38621450" class="c"><input type="checkbox" id="c-38621450" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38621426">parent</a><span>|</span><a href="#38621900">next</a><span>|</span><label class="collapse" for="c-38621450">[-]</label><label class="expand" for="c-38621450">[2 more]</label></div><br/><div class="children"><div class="content">OP here - we have more coming about the role that the BEAM VM played in this migration too.<p>(The BEAM is the virtual machine for the Erlang ecosystem, analagous to the JVM for Java. Knock runs on Elixir, which is built on Erlang &amp; the BEAM).</div><br/><div id="38621472" class="c"><input type="checkbox" id="c-38621472" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#38621426">root</a><span>|</span><a href="#38621450">parent</a><span>|</span><a href="#38621900">next</a><span>|</span><label class="collapse" for="c-38621472">[-]</label><label class="expand" for="c-38621472">[1 more]</label></div><br/><div class="children"><div class="content">I’m stoked to hear more. It’s phenomenal tech.</div><br/></div></div></div></div></div></div><div id="38621900" class="c"><input type="checkbox" id="c-38621900" checked=""/><div class="controls bullet"><span class="by">fosterfriends</span><span>|</span><a href="#38621426">prev</a><span>|</span><a href="#38620482">next</a><span>|</span><label class="collapse" for="c-38621900">[-]</label><label class="expand" for="c-38621900">[1 more]</label></div><br/><div class="children"><div class="content">Great write up y’all! Writing this detailed of a post isn’t easy, and it works to build confidence in your technical prowess. Keep up the great work :)</div><br/></div></div><div id="38620482" class="c"><input type="checkbox" id="c-38620482" checked=""/><div class="controls bullet"><span class="by">AtlasBarfed</span><span>|</span><a href="#38621900">prev</a><span>|</span><a href="#38619899">next</a><span>|</span><label class="collapse" for="c-38620482">[-]</label><label class="expand" for="c-38620482">[1 more]</label></div><br/><div class="children"><div class="content">Oh you mean like aws promised us during the &quot;sales engineering&quot; but failed to deliver when a major version upgrade was forced upon us?</div><br/></div></div><div id="38619899" class="c"><input type="checkbox" id="c-38619899" checked=""/><div class="controls bullet"><span class="by">october8140</span><span>|</span><a href="#38620482">prev</a><span>|</span><a href="#38621577">next</a><span>|</span><label class="collapse" for="c-38619899">[-]</label><label class="expand" for="c-38619899">[3 more]</label></div><br/><div class="children"><div class="content">Heroku just does this. At my old job we would scale the database using replication multiple times a week depending on expected traffic.<p><a href="https:&#x2F;&#x2F;devcenter.heroku.com&#x2F;articles&#x2F;heroku-postgres-follower-databases" rel="nofollow noreferrer">https:&#x2F;&#x2F;devcenter.heroku.com&#x2F;articles&#x2F;heroku-postgres-follow...</a></div><br/><div id="38620066" class="c"><input type="checkbox" id="c-38620066" checked=""/><div class="controls bullet"><span class="by">why-el</span><span>|</span><a href="#38619899">parent</a><span>|</span><a href="#38620212">next</a><span>|</span><label class="collapse" for="c-38620066">[-]</label><label class="expand" for="c-38620066">[1 more]</label></div><br/><div class="children"><div class="content">Not quite I don&#x27;t think. For a busy database, The Heroku followers will not catch up to your upgraded database as quickly, so during an upgrade using Heroku&#x27;s physical replication (as opposed to logical), there will be a time period where your freshly upgraded primary is on its own as the followers are being issued and brought up to date.</div><br/></div></div><div id="38620212" class="c"><input type="checkbox" id="c-38620212" checked=""/><div class="controls bullet"><span class="by">thejosh</span><span>|</span><a href="#38619899">parent</a><span>|</span><a href="#38620066">prev</a><span>|</span><a href="#38621577">next</a><span>|</span><label class="collapse" for="c-38620212">[-]</label><label class="expand" for="c-38620212">[1 more]</label></div><br/><div class="children"><div class="content">Except Heroku has an issue of your backup etc is too large (despite paying for the correct size), it would cause the replica to go down and spin up a new one, and this process could take hours.</div><br/></div></div></div></div><div id="38621577" class="c"><input type="checkbox" id="c-38621577" checked=""/><div class="controls bullet"><span class="by">andrewmcwatters</span><span>|</span><a href="#38619899">prev</a><span>|</span><a href="#38621550">next</a><span>|</span><label class="collapse" for="c-38621577">[-]</label><label class="expand" for="c-38621577">[2 more]</label></div><br/><div class="children"><div class="content">Uh... How big was their database? Did I miss it? I don&#x27;t think they said.</div><br/><div id="38621706" class="c"><input type="checkbox" id="c-38621706" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38621577">parent</a><span>|</span><a href="#38621550">next</a><span>|</span><label class="collapse" for="c-38621706">[-]</label><label class="expand" for="c-38621706">[1 more]</label></div><br/><div class="children"><div class="content">OP here. We don’t specify, but it’s big enough that it’s not reasonable to do a dump and restore style upgrade.<p>The strategies in the post should work for any size database. The limit becomes more a matter of individual table sizes, since we propose using an incremental approach to synchronizing one table at a time.</div><br/></div></div></div></div><div id="38620334" class="c"><input type="checkbox" id="c-38620334" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#38621550">prev</a><span>|</span><label class="collapse" for="c-38620334">[-]</label><label class="expand" for="c-38620334">[17 more]</label></div><br/><div class="children"><div class="content">&gt; Postgres sits at the heart of everything our systems do.<p>Did the people making these decisions never take Computer Science classes? Even a student taking a data structures module would realize this is a bad idea. There&#x27;s actually more like two dozen different reasons it&#x27;s a bad idea.</div><br/><div id="38620365" class="c"><input type="checkbox" id="c-38620365" checked=""/><div class="controls bullet"><span class="by">thestepafter</span><span>|</span><a href="#38620334">parent</a><span>|</span><a href="#38620356">next</a><span>|</span><label class="collapse" for="c-38620365">[-]</label><label class="expand" for="c-38620365">[4 more]</label></div><br/><div class="children"><div class="content">Would be interested to hear more about your opinion on why using a database is a mistake.</div><br/><div id="38622663" class="c"><input type="checkbox" id="c-38622663" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620365">parent</a><span>|</span><a href="#38620449">next</a><span>|</span><label class="collapse" for="c-38622663">[-]</label><label class="expand" for="c-38622663">[1 more]</label></div><br/><div class="children"><div class="content">Using a datastore for which true master-master HA is at best a bolted-on afterthought when you explicitly want a zero-downtime system is a mistake in a pretty obvious way.<p>Using a datastore with a black box query planner that explicitly doesn&#x27;t allow you to force particular indices (using hints or similar) is a more subtle mistake but will inevitably bite you eventually. Likewise a datastore that uses black-box MVCC and doesn&#x27;t let you separate e.g. writing data from updating indices.</div><br/></div></div><div id="38620449" class="c"><input type="checkbox" id="c-38620449" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620365">parent</a><span>|</span><a href="#38622663">prev</a><span>|</span><a href="#38620356">next</a><span>|</span><label class="collapse" for="c-38620449">[-]</label><label class="expand" for="c-38620449">[2 more]</label></div><br/><div class="children"><div class="content">I meant using a database for more than relational read-heavy data queries. I would need to write a small book. Tl;dr the data model, communication model, locking model, and operational model all have specific limitations designed around a specific use case and straying from that case invites problems that need workarounds that create more problems.</div><br/><div id="38620576" class="c"><input type="checkbox" id="c-38620576" checked=""/><div class="controls bullet"><span class="by">brentjanderson</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620449">parent</a><span>|</span><a href="#38620356">next</a><span>|</span><label class="collapse" for="c-38620576">[-]</label><label class="expand" for="c-38620576">[1 more]</label></div><br/><div class="children"><div class="content">I hear you on that, and can say that Postgres is incredibly capable at going beyond typical relational database workloads. One example are durable queues that are transactionally consistent with the rest of the database play a unique role in our architecture that would otherwise require more ceremony. More details here: <a href="https:&#x2F;&#x2F;getoban.pro" rel="nofollow noreferrer">https:&#x2F;&#x2F;getoban.pro</a><p>We are also working on shifting some workloads off of Postgres on to more appropriate systems as we scale, like logging. But we intentionally chose to minimize dependencies by pushing Postgres further to move faster, with migration plans ready as we continue to reach new levels of scale (e.g. using a dedicated log storage solution like elastic search or clickhouse).</div><br/></div></div></div></div></div></div><div id="38620356" class="c"><input type="checkbox" id="c-38620356" checked=""/><div class="controls bullet"><span class="by">peter_l_downs</span><span>|</span><a href="#38620334">parent</a><span>|</span><a href="#38620365">prev</a><span>|</span><a href="#38620566">next</a><span>|</span><label class="collapse" for="c-38620356">[-]</label><label class="expand" for="c-38620356">[10 more]</label></div><br/><div class="children"><div class="content">What do you mean? I don’t understand, how is using a database an architectural mistake?</div><br/><div id="38620368" class="c"><input type="checkbox" id="c-38620368" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620356">parent</a><span>|</span><a href="#38620566">next</a><span>|</span><label class="collapse" for="c-38620368">[-]</label><label class="expand" for="c-38620368">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a mistake to use one specific computer science concept (RDBMS) to solve 50 different problems. They mentioned logging and scheduling, two things RDBMS are not designed for and have specific limitations around. From just a general architecture perspective it&#x27;s literally a single point of failure and limitation for every single aspect of the system. And it&#x27;s vendor specific, it&#x27;s not like you can just plug plsql code into any other RDBMS and expect it to work. It&#x27;s so obviously a bad idea it&#x27;s hard to comprehend taking it seriously</div><br/><div id="38620754" class="c"><input type="checkbox" id="c-38620754" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620368">parent</a><span>|</span><a href="#38620406">next</a><span>|</span><label class="collapse" for="c-38620754">[-]</label><label class="expand" for="c-38620754">[1 more]</label></div><br/><div class="children"><div class="content">It might not be good computer science to use one tool to solve 50 different problems; but it&#x27;s not bad computer engineering to use one tool to solve 50 different problems that fit within its capabilities rather than using 50 different tools, all with their own operational expertise.<p>There&#x27;s no need to have the best tool for every job. Although it&#x27;s also important to be able to see when a many purpose tool is insufficient for a specific job as it exists in your system and then figure out what would be more appropriate.</div><br/></div></div><div id="38620406" class="c"><input type="checkbox" id="c-38620406" checked=""/><div class="controls bullet"><span class="by">camgunz</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620368">parent</a><span>|</span><a href="#38620754">prev</a><span>|</span><a href="#38620619">next</a><span>|</span><label class="collapse" for="c-38620406">[-]</label><label class="expand" for="c-38620406">[6 more]</label></div><br/><div class="children"><div class="content">You&#x27;d probably be surprised by how many systems are just Postgres&#x2F;mysql + Redis.</div><br/><div id="38620446" class="c"><input type="checkbox" id="c-38620446" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620406">parent</a><span>|</span><a href="#38620619">next</a><span>|</span><label class="collapse" for="c-38620446">[-]</label><label class="expand" for="c-38620446">[5 more]</label></div><br/><div class="children"><div class="content">For example, it&#x27;s dead easy to make a high-capacity message queue by just using SELECT ... FOR UPDATE SKIP LOCKED with Postgres transactions, and I would argue it&#x27;s more reliable than a lot of microservice-everything setups by way of having very few moving parts.</div><br/><div id="38620550" class="c"><input type="checkbox" id="c-38620550" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620446">parent</a><span>|</span><a href="#38620619">next</a><span>|</span><label class="collapse" for="c-38620550">[-]</label><label class="expand" for="c-38620550">[4 more]</label></div><br/><div class="children"><div class="content">Classic NIH syndrome. <i>&quot;I made it myself so it must be better&quot;</i>, when it&#x27;s clear that a single sql query doesn&#x27;t remotely approach a complete solution for scheduling. But the ignorant use it because they don&#x27;t know better, until they too fall into the trap and realize they spent 10x as much engineering work to get something they could have just installed from the web and been done with. Every generation seems to fall into this trap with another tech stack.</div><br/><div id="38620836" class="c"><input type="checkbox" id="c-38620836" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620550">parent</a><span>|</span><a href="#38620802">next</a><span>|</span><label class="collapse" for="c-38620836">[-]</label><label class="expand" for="c-38620836">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s all trade offs, right? Introducing a new component to your stack isn&#x27;t free, it&#x27;s paid for by more operational complexity. Maybe it&#x27;s worth it, maybe it&#x27;s not, but there is a calculation that needs to be made that&#x27;s not just &quot;NIH syndrome&quot;.</div><br/></div></div><div id="38620802" class="c"><input type="checkbox" id="c-38620802" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620550">parent</a><span>|</span><a href="#38620836">prev</a><span>|</span><a href="#38623581">next</a><span>|</span><label class="collapse" for="c-38620802">[-]</label><label class="expand" for="c-38620802">[1 more]</label></div><br/><div class="children"><div class="content">If you already have a DB (and essentially every app does), it can be far less effort with the same or greater reliability to create a queue table than to set up RabbitMQ, NATS, etc. As long as you tune the vacuuming appropriately, it will last for quite a lot of scale.<p>Source: am a DBRE, and have ran self-hosted RabbitMQ and NATS clusters.</div><br/></div></div><div id="38623581" class="c"><input type="checkbox" id="c-38623581" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620550">parent</a><span>|</span><a href="#38620802">prev</a><span>|</span><a href="#38620619">next</a><span>|</span><label class="collapse" for="c-38623581">[-]</label><label class="expand" for="c-38623581">[1 more]</label></div><br/><div class="children"><div class="content">Sure, so install RabbitMQ as well.<p>As the saying goes... &quot;now you have two problems&quot;. :)</div><br/></div></div></div></div></div></div></div></div><div id="38620619" class="c"><input type="checkbox" id="c-38620619" checked=""/><div class="controls bullet"><span class="by">lgkk</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620368">parent</a><span>|</span><a href="#38620406">prev</a><span>|</span><a href="#38620566">next</a><span>|</span><label class="collapse" for="c-38620619">[-]</label><label class="expand" for="c-38620619">[1 more]</label></div><br/><div class="children"><div class="content">You could honestly just do in memory SQLite and use that lol idk that’s what I did because I wanted to quickly be able to handle thousands of simultaneous scheduling tasks.<p>Took like two hours and it works fine. Customers are happy. Event logs persist to s3 in case I need to replay (hasn’t happened once yet).</div><br/></div></div></div></div></div></div><div id="38620566" class="c"><input type="checkbox" id="c-38620566" checked=""/><div class="controls bullet"><span class="by">pphysch</span><span>|</span><a href="#38620334">parent</a><span>|</span><a href="#38620356">prev</a><span>|</span><label class="collapse" for="c-38620566">[-]</label><label class="expand" for="c-38620566">[2 more]</label></div><br/><div class="children"><div class="content">Is this a bit? The median CS undergrad has zero experience with large &amp; successful software systems in the real world. Of course they wouldn&#x27;t understand!</div><br/><div id="38623714" class="c"><input type="checkbox" id="c-38623714" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#38620334">root</a><span>|</span><a href="#38620566">parent</a><span>|</span><label class="collapse" for="c-38623714">[-]</label><label class="expand" for="c-38623714">[1 more]</label></div><br/><div class="children"><div class="content">Yeah - in fact, this is probably a great example of stuff you don&#x27;t learn in class that gets really clear in the real world:) Operational concerns trump a <i>lot</i> of other things, and shoving everything you can into 1 database technology is so much better to manage that it covers a lot of suboptimal fit.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>