<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689670852196" as="style"/><link rel="stylesheet" href="styles.css?v=1689670852196"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://stability.ai/research/minds-eye">fMRI-to-image with contrastive learning and diffusion priors</a> <span class="domain">(<a href="https://stability.ai">stability.ai</a>)</span></div><div class="subtext"><span>tmabraham</span> | <span>48 comments</span></div><br/><div><div id="36767801" class="c"><input type="checkbox" id="c-36767801" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#36767306">next</a><span>|</span><label class="collapse" for="c-36767801">[-]</label><label class="expand" for="c-36767801">[11 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t there something similar a few months ago on HN and where the top comment talked about how it&#x27;s not as impressive as it sounds [0]? The main issue is that this type of methodology is pulling from a pool of images, not literally reconstructing what image was seen in the brain directly.<p>&gt; <i>I immediately found the results suspect, and think I have found what is actually going on. The dataset it was trained on was 2770 images, minus 982 of those used for validation. I posit that the system did not actually read any pictures from the brains, but simply overfitted all the training images into the network itself. For example, if one looks at a picture of a teddy bear, you&#x27;d get an overfitted picture of another teddy bear from the training dataset instead.</i><p>&gt; <i>The best evidence for this is a picture(1) from page 6 of the paper. Look at the second row. The building generated by &#x27;mind reading&#x27; subject 2 and 4 look strikingly similar, but not very similar to the ground truth! From manually combing through the training dataset, I found a picture of a building that does look like that, and by scaling it down and cropping it exactly in the middle, it overlays rather closely(2) on the output that was ostensibly generated for an unrelated image.</i><p>&gt; <i>If so, at most they found that looking at similar subjects light up similar regions of the brain, putting Stable Diffusion on top of it serves no purpose. At worst it&#x27;s entirely cherry-picked coincidences.</i><p>&gt; <i>1. <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;ILCD2Mu.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.imgur.com&#x2F;ILCD2Mu.png</a></i><p>&gt; <i>2. <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;ftMlGq8.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.imgur.com&#x2F;ftMlGq8.png</a></i><p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35012981">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35012981</a></div><br/><div id="36769307" class="c"><input type="checkbox" id="c-36769307" checked=""/><div class="controls bullet"><span class="by">tmabraham</span><span>|</span><a href="#36767801">parent</a><span>|</span><a href="#36769321">next</a><span>|</span><label class="collapse" for="c-36769307">[-]</label><label class="expand" for="c-36769307">[1 more]</label></div><br/><div class="children"><div class="content">Our model generates CLIP image embeddings from fMRI signals and those image embeddings can be used for retrieval (using cosine similarity for example) or passed into a pretrained diffusion model that takes in CLIP image embeddings and generates an image (it&#x27;s a bit more complicated than that but that&#x27;s the gist, read the blog post for more info).<p>So we are doing both reconstruction and retrieval.<p>The reconstruction achieves SOTA results. The retrieval demonstrates that the image embeddings contain fine-grained information, not just saying it&#x27;s just a picture of a teddy bear and then the diffusion model just generates a random teddy bear picture.<p>I think the zebra example really highlights that. The image embedding generated matches the exact zebra image that was seen by the person. If the model only could say it&#x27;s just a zebra picture, it wouldn&#x27;t be able to do that. But the model is picking up on fine-grained info present in the fMRI signal.<p>The blog post has more information and the paper itself has even more information so please check it out! :)</div><br/></div></div><div id="36769321" class="c"><input type="checkbox" id="c-36769321" checked=""/><div class="controls bullet"><span class="by">boh423</span><span>|</span><a href="#36767801">parent</a><span>|</span><a href="#36769307">prev</a><span>|</span><a href="#36768344">next</a><span>|</span><label class="collapse" for="c-36769321">[-]</label><label class="expand" for="c-36769321">[1 more]</label></div><br/><div class="children"><div class="content">The underlying NSD dataset used in the three prominent (and impressive) recent papers on this topic (including the one linked here) is a bit problematic as it invites this (classification&#x2F;identification, not reconstruction): It only has 80 categories. It has not been recorded with reconstruction in mind.<p>Reconstruction is the primary and difficult aim, but is what you want and expect when people talk such „mind reading”. Classifying something on brain activity has long been solved and is not difficult, it is almost trivial with modern data sizes and quality. At 80 categories and with data from higher visual areas you could even use an SVM for the basic classifier and then some method for getting a similar blob shape from the activity (V1-V3 are map-like), and get good results.<p>If you are ignorant about the question whether you are just doing classification you can easily get too-good-to-be-true results. With these newer methods relying on pretrained features this classification case can hide deep inside the model too, and can easily be missed.<p>The community is currently discussing to what extent this applies to these newer papers (start with original post): 
<a href="https:&#x2F;&#x2F;twitter.com&#x2F;ykamit&#x2F;status&#x2F;1677872648590864385?s=20" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;ykamit&#x2F;status&#x2F;1677872648590864385?s=20</a><p>(Kamitani has been working on the reconstruction question for long time and knows all these traps quite well.)<p>The deeprecon dataset proposed as an alternative here has out of distribution „abstract“ images and no class overlap between train and test images, so it’s quite suitable for proving that it is actually reconstruction. But it’s also one order of magnitude smaller than the NSD data used for the newer reconstruction studies. If you modify the NSD data to not have train-test class overlap, the methods do not work as well, but still look like they do some reconstruction.</div><br/></div></div><div id="36768344" class="c"><input type="checkbox" id="c-36768344" checked=""/><div class="controls bullet"><span class="by">dehrmann</span><span>|</span><a href="#36767801">parent</a><span>|</span><a href="#36769321">prev</a><span>|</span><a href="#36768199">next</a><span>|</span><label class="collapse" for="c-36768344">[-]</label><label class="expand" for="c-36768344">[1 more]</label></div><br/><div class="children"><div class="content">That one was a bit like not hotdogs: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ACmydtFDTGs">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ACmydtFDTGs</a></div><br/></div></div><div id="36768199" class="c"><input type="checkbox" id="c-36768199" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#36767801">parent</a><span>|</span><a href="#36768344">prev</a><span>|</span><a href="#36767306">next</a><span>|</span><label class="collapse" for="c-36768199">[-]</label><label class="expand" for="c-36768199">[7 more]</label></div><br/><div class="children"><div class="content">Yes there was. However this is a different paper, describing a different method, applied to a different dataset, with different results.<p>As the abstract says, &quot;In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye&#x27;s performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters.&quot;<p>Note that LAION-5B has five billion images.</div><br/><div id="36768224" class="c"><input type="checkbox" id="c-36768224" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#36767801">root</a><span>|</span><a href="#36768199">parent</a><span>|</span><a href="#36768277">next</a><span>|</span><label class="collapse" for="c-36768224">[-]</label><label class="expand" for="c-36768224">[4 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s still <i>retrieving</i> an image and not <i>reconstructing</i> it, if the dataset is large enough that&#x27;s decently fine, but this is generally not how diffusion models work in general and I&#x27;d have expected the model to map the fMRI data to a wholly new image.</div><br/><div id="36768267" class="c"><input type="checkbox" id="c-36768267" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#36767801">root</a><span>|</span><a href="#36768224">parent</a><span>|</span><a href="#36769010">next</a><span>|</span><label class="collapse" for="c-36768267">[-]</label><label class="expand" for="c-36768267">[1 more]</label></div><br/><div class="children"><div class="content">Please read the paper. Or at least the blog post. It&#x27;s really quite readable.<p>They explain that they&#x27;ve done both retrieval and reconstruction, and have lots of pictures showing examples of each.<p><a href="https:&#x2F;&#x2F;medarc-ai.github.io&#x2F;mindeye&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;medarc-ai.github.io&#x2F;mindeye&#x2F;</a></div><br/></div></div><div id="36769010" class="c"><input type="checkbox" id="c-36769010" checked=""/><div class="controls bullet"><span class="by">rocqua</span><span>|</span><a href="#36767801">root</a><span>|</span><a href="#36768224">parent</a><span>|</span><a href="#36768267">prev</a><span>|</span><a href="#36768287">next</a><span>|</span><label class="collapse" for="c-36769010">[-]</label><label class="expand" for="c-36769010">[1 more]</label></div><br/><div class="children"><div class="content">They tested themselves both on retrieval and reconstruction.</div><br/></div></div><div id="36768287" class="c"><input type="checkbox" id="c-36768287" checked=""/><div class="controls bullet"><span class="by">RC_ITR</span><span>|</span><a href="#36767801">root</a><span>|</span><a href="#36768224">parent</a><span>|</span><a href="#36769010">prev</a><span>|</span><a href="#36768277">next</a><span>|</span><label class="collapse" for="c-36768287">[-]</label><label class="expand" for="c-36768287">[1 more]</label></div><br/><div class="children"><div class="content">If you can retrieve an image using a latent vector, it’s trivial to reconstruct it (decently well) with a diffusion model.</div><br/></div></div></div></div><div id="36768277" class="c"><input type="checkbox" id="c-36768277" checked=""/><div class="controls bullet"><span class="by">RC_ITR</span><span>|</span><a href="#36767801">root</a><span>|</span><a href="#36768199">parent</a><span>|</span><a href="#36768224">prev</a><span>|</span><a href="#36767306">next</a><span>|</span><label class="collapse" for="c-36768277">[-]</label><label class="expand" for="c-36768277">[2 more]</label></div><br/><div class="children"><div class="content">&gt; To achieve the goals of retrieval and reconstruction with a single model trained end-to-end, we adopt a novel approach of using two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior).<p>What you can think of contrastive learning as is: two separate models that take different inputs and make vectors of the same length as outputs. This is achieved by training both models on pairs of training data (in this case fMRI images and observed images).<p>What the LAION-5B work shows is that they did a good enough job of this training that the models are really good at creating similar vectors for <i>nearly any</i> image and fMRI pair.<p>Then, they make a prior model which basically says “our fMRI vectors are essentially image vectors with an arbitrary amount of randomness in them (representing the difference between the contrastive learning models). Let’s train a model to learn to remove that randomness, then we have image vectors.”<p>So yes, this is an impressive result at first glance and not some overfitting trick.<p>It’s also sort of bread and butter at this point (replace fMRI with “text” and that’s just what Stable Diffusion is).<p>They’ll be lots of these sort of results coming out soon.</div><br/><div id="36769008" class="c"><input type="checkbox" id="c-36769008" checked=""/><div class="controls bullet"><span class="by">atom_101</span><span>|</span><a href="#36767801">root</a><span>|</span><a href="#36768277">parent</a><span>|</span><a href="#36767306">next</a><span>|</span><label class="collapse" for="c-36769008">[-]</label><label class="expand" for="c-36769008">[1 more]</label></div><br/><div class="children"><div class="content">This is mostly correct, except that there is only one model. This model takes an fMRI and predicts 2 outputs. The first is specialized for retrieval and the second can be fed into a diffusion model to reconstruct images.<p>You can see the comparison in performance between LAION-5B retrieval and actual reconstructions in the paper. When retrieving from a large enough database like LAION-5B, we can get images that are quite similar to the seen images in terms of high level content, but not so similar in low-level details (relative position of objects, colors, texture, etc). Reconstruction with diffusion models does much better in terms of low-level metrics.</div><br/></div></div></div></div></div></div></div></div><div id="36767306" class="c"><input type="checkbox" id="c-36767306" checked=""/><div class="controls bullet"><span class="by">hospadar</span><span>|</span><a href="#36767801">prev</a><span>|</span><a href="#36768111">next</a><span>|</span><label class="collapse" for="c-36767306">[-]</label><label class="expand" for="c-36767306">[3 more]</label></div><br/><div class="children"><div class="content">This is SO COOL.  I&#x27;d guess (I did analysis for an fMRI lab for a year so I&#x27;m not a pro but not totally talking out of my orifice) that detecting images like this is among the easier things you could do (it probably wouldn&#x27;t be so easy to do things like &quot;guess the words I&#x27;m thinking of&quot;) and I suspect other sensory stuff might be harder but I have little knowledge there.<p>One of the biggest issues with any attempt to extract information from an fMRI scan is resolution, both spatial and temporal - this study used 1.8mm voxels which is a TON of neurons (also recall that fMRIs scan blood flow, not neuron activity - we just count on those things being correlated).  Temporally, fMRI sample frequency are often &lt;1hz. I didn&#x27;t see that they mentioned a specific frequency, but they showed images to the subject for 3 seconds at a time so I&#x27;d guess that&#x27;s designed to ensure you get a least a frame or three while the subject is looking at the image.  You can sort of trade voxel size for sample frequency - so you can get more voxels, or more samples, but not both.  So detecting things that happen quickly (like, say, moving images or speech) would probably be quite hard (even if you could design an ai thingey that could do it, getting the raw data at the resolution you&#x27;d need is not currently possible with existing scanners)<p>Also, not all brain functions are as clearly localized as vision - the visual cortex areas in the back of the brain map pretty directly to certain kinds of visual stimulus, while other kinds of stimulus and activity are much less localize (there isn&#x27;t a clear &quot;lighting up&quot; of an area). You can get better resolution if you only scan part of the brain (i.e. the visual cortex) (I don&#x27;t know if that&#x27;s what they did for this study), but that&#x27;s obviously only useful for activity happening in a small part of the brain.<p>ANYWAY SO COOL!!!  I wonder if you could use this to draw people&#x27;s faces with a subject who is imagining looking at a face? fMRI police sketch? How do brains even work!?</div><br/><div id="36769520" class="c"><input type="checkbox" id="c-36769520" checked=""/><div class="controls bullet"><span class="by">tmabraham</span><span>|</span><a href="#36767306">parent</a><span>|</span><a href="#36768949">next</a><span>|</span><label class="collapse" for="c-36769520">[-]</label><label class="expand" for="c-36769520">[1 more]</label></div><br/><div class="children"><div class="content">Yeah using data from a 7T MRI giving higher spatial resolution definitely helps!<p>The fMRI dataset includes signal from the whole brain but we only use the data from the visual cortex for this study.</div><br/></div></div><div id="36768949" class="c"><input type="checkbox" id="c-36768949" checked=""/><div class="controls bullet"><span class="by">atom_101</span><span>|</span><a href="#36767306">parent</a><span>|</span><a href="#36769520">prev</a><span>|</span><a href="#36768111">next</a><span>|</span><label class="collapse" for="c-36768949">[-]</label><label class="expand" for="c-36768949">[1 more]</label></div><br/><div class="children"><div class="content">We did have a face reconstruction project planned. It is on the back-burner for now. That one will be based on something like the Celeb-A dataset instead of the Natural Scenes Dataset (images from MS-COCO) used here.</div><br/></div></div></div></div><div id="36768111" class="c"><input type="checkbox" id="c-36768111" checked=""/><div class="controls bullet"><span class="by">umvi</span><span>|</span><a href="#36767306">prev</a><span>|</span><a href="#36767392">next</a><span>|</span><label class="collapse" for="c-36768111">[-]</label><label class="expand" for="c-36768111">[1 more]</label></div><br/><div class="children"><div class="content">What if a copyrighted image or video can be recovered from your brain using external tech like this? That&#x27;s not fair to the rights holders, what we really need is technology that can clean such illegal memories from the brain; a brainwasher if you will</div><br/></div></div><div id="36767392" class="c"><input type="checkbox" id="c-36767392" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#36768111">prev</a><span>|</span><a href="#36767299">next</a><span>|</span><label class="collapse" for="c-36767392">[-]</label><label class="expand" for="c-36767392">[9 more]</label></div><br/><div class="children"><div class="content">Human communication will change dramatically once useful invasive brain-computer interfaces are available.<p>People will suddenly realize that the reason language is primarily serial is simply due to the fact that it must be conveyed by a series of sounds. There will likely be a new type of visual language used via BCI &quot;telepathy&quot;. It may have some ordering but will not rely so heavily on serializing information, since the world is quite multidimensional.</div><br/><div id="36767825" class="c"><input type="checkbox" id="c-36767825" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#36767392">parent</a><span>|</span><a href="#36767547">next</a><span>|</span><label class="collapse" for="c-36767825">[-]</label><label class="expand" for="c-36767825">[3 more]</label></div><br/><div class="children"><div class="content">Indeed, it reminds me of the movie Arrival (and the short story upon which it&#x27;s based) where the heptapods are able to show a complete sentence and story within one glyph. I thought it was interesting just how much the movie focused on linguistics, which is rare to see in Hollywood films.<p>Something else that&#x27;s interesting about language is it&#x27;s just a form of compressive medium for thoughts; I think of a concept, then I put it into words (compression) that you the listener then have to interpret and understand (decompression) and then fit your brain state to the new data you&#x27;ve received. It&#x27;s overall a very lossy medium compared to what brains can do. It would be much easier to beam my thoughts and images and videos in my mind directly to you.<p>Unless you or I have aphantasia, of course.</div><br/><div id="36768237" class="c"><input type="checkbox" id="c-36768237" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#36767392">root</a><span>|</span><a href="#36767825">parent</a><span>|</span><a href="#36767547">next</a><span>|</span><label class="collapse" for="c-36768237">[-]</label><label class="expand" for="c-36768237">[2 more]</label></div><br/><div class="children"><div class="content">Right and I would go so far as to say that most types of intelligence are a type of functional compression also.<p>There&#x27;s definitely room for direct transfer of concrete unrolled information. But at the same time we would still need some forms of abstraction in many cases.<p>I think the biggest issue with the compression of natural language is that the loss is different for each person, since everyone&#x27;s &quot;codec&quot; varies. In other words, people often interpret language in different ways.<p>But suppose that humans or AIs or AI-enhanced humans could have exactly the same base dictionary or interpretive network or &quot;codec&quot; or whatever for a (visual or word-based) language. Then we could get away from many of the disputes and misunderstandings that arise purely from different interpretations.</div><br/><div id="36768684" class="c"><input type="checkbox" id="c-36768684" checked=""/><div class="controls bullet"><span class="by">arketyp</span><span>|</span><a href="#36767392">root</a><span>|</span><a href="#36768237">parent</a><span>|</span><a href="#36767547">next</a><span>|</span><label class="collapse" for="c-36768684">[-]</label><label class="expand" for="c-36768684">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what the limits are to such a universal codec. From what I&#x27;ve gathered about synaesthesia (e.g. from V.S. Ramachandran, or Galton earlier), it varies quite significantly between persons. I believe it&#x27;s said that some 3% of people have aphantasia for instance. That means entire modalities would be excluded for some in a latent space glyph language. Unless, I suppose, one could find ways of stimulating the synaesthetic connections artificially too.</div><br/></div></div></div></div></div></div><div id="36767547" class="c"><input type="checkbox" id="c-36767547" checked=""/><div class="controls bullet"><span class="by">SamPatt</span><span>|</span><a href="#36767392">parent</a><span>|</span><a href="#36767825">prev</a><span>|</span><a href="#36767299">next</a><span>|</span><label class="collapse" for="c-36767547">[-]</label><label class="expand" for="c-36767547">[5 more]</label></div><br/><div class="children"><div class="content">In a popular sci-fi (avoiding spoliers), the alien race has transparent skulls, and their visible thoughts are broadcast to anyone within visual range.<p>It does seem more efficient than sound.</div><br/><div id="36767717" class="c"><input type="checkbox" id="c-36767717" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36767392">root</a><span>|</span><a href="#36767547">parent</a><span>|</span><a href="#36767299">next</a><span>|</span><label class="collapse" for="c-36767717">[-]</label><label class="expand" for="c-36767717">[4 more]</label></div><br/><div class="children"><div class="content">I want to know the scifi<p>You may base64 to spoiler proof it</div><br/><div id="36767796" class="c"><input type="checkbox" id="c-36767796" checked=""/><div class="controls bullet"><span class="by">lithiumii</span><span>|</span><a href="#36767392">root</a><span>|</span><a href="#36767717">parent</a><span>|</span><a href="#36767299">next</a><span>|</span><label class="collapse" for="c-36767796">[-]</label><label class="expand" for="c-36767796">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s &quot;VGhlIFRocmVlLUJvZHkgUHJvYmxlbQ==&quot;
The aliens cannot lie to each other (they don&#x27;t even have the idea of a lie), because their thoughts are transparent to each other.</div><br/><div id="36767828" class="c"><input type="checkbox" id="c-36767828" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#36767392">root</a><span>|</span><a href="#36767796">parent</a><span>|</span><a href="#36767299">next</a><span>|</span><label class="collapse" for="c-36767828">[-]</label><label class="expand" for="c-36767828">[2 more]</label></div><br/><div class="children"><div class="content">Sounds like a recipe for conflict.</div><br/><div id="36769148" class="c"><input type="checkbox" id="c-36769148" checked=""/><div class="controls bullet"><span class="by">inglor_cz</span><span>|</span><a href="#36767392">root</a><span>|</span><a href="#36767828">parent</a><span>|</span><a href="#36767299">next</a><span>|</span><label class="collapse" for="c-36769148">[-]</label><label class="expand" for="c-36769148">[1 more]</label></div><br/><div class="children"><div class="content">It would certainly be a very violent shift towards a very different societal equilibrium.<p>Few, if any, people who currently have power, would be able to absorb the sheer amount of hitherto hidden distrust or resentment that their subordinates harbor towards them.<p>Interestingly, there might be two very different end stages.<p>Either a very open society where people at the top are selected to be non-narcissist and stoic, or a very closed and oppressive society where the absolute ruler is kept in power by a bunch of truly zombified and obedient warriors whose loyalty is real and unshakeable, and who will kill anyone whose brain entertains any rebel ideas too much.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36767299" class="c"><input type="checkbox" id="c-36767299" checked=""/><div class="controls bullet"><span class="by">robg</span><span>|</span><a href="#36767392">prev</a><span>|</span><a href="#36767822">next</a><span>|</span><label class="collapse" for="c-36767299">[-]</label><label class="expand" for="c-36767299">[7 more]</label></div><br/><div class="children"><div class="content">Awesome, predicting words from fMRI has been around for a while and visual cortex can be mapped well.<p>That said, and coming from a background in neuroimaging 20 years ago, what’s the applicability? MRI hasn’t gotten that much more cost effective for more widespread uses. Magnets are expensive.</div><br/><div id="36767752" class="c"><input type="checkbox" id="c-36767752" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#36767299">parent</a><span>|</span><a href="#36768804">next</a><span>|</span><label class="collapse" for="c-36767752">[-]</label><label class="expand" for="c-36767752">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the first thing that comes to mind(har har) when I see this is that we&#x27;d be better off trying to develop better scanning technology.  You can&#x27;t exactly walk around town with an MRI strapped to your skull.</div><br/></div></div><div id="36768804" class="c"><input type="checkbox" id="c-36768804" checked=""/><div class="controls bullet"><span class="by">tmabraham</span><span>|</span><a href="#36767299">parent</a><span>|</span><a href="#36767752">prev</a><span>|</span><a href="#36767351">next</a><span>|</span><label class="collapse" for="c-36768804">[-]</label><label class="expand" for="c-36768804">[1 more]</label></div><br/><div class="children"><div class="content">We think it could be useful for clinical research and maybe even diagnostics. For example, you could imagine a person with depression(or other neurological disorders) may have a different perception of the same image than a healthy person. Now with the much higher fidelity that both more powerful MRI machines and better generative AI tools can provide, this may now be a very promising direction for future research.</div><br/></div></div><div id="36767351" class="c"><input type="checkbox" id="c-36767351" checked=""/><div class="controls bullet"><span class="by">aledalgrande</span><span>|</span><a href="#36767299">parent</a><span>|</span><a href="#36768804">prev</a><span>|</span><a href="#36768030">next</a><span>|</span><label class="collapse" for="c-36767351">[-]</label><label class="expand" for="c-36767351">[3 more]</label></div><br/><div class="children"><div class="content">People with disabilities could benefit greatly from this.</div><br/><div id="36767688" class="c"><input type="checkbox" id="c-36767688" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36767299">root</a><span>|</span><a href="#36767351">parent</a><span>|</span><a href="#36768030">next</a><span>|</span><label class="collapse" for="c-36767688">[-]</label><label class="expand" for="c-36767688">[2 more]</label></div><br/><div class="children"><div class="content">As long as they want to talk about london buses, steam trains, surfing and football.</div><br/><div id="36769651" class="c"><input type="checkbox" id="c-36769651" checked=""/><div class="controls bullet"><span class="by">madacol</span><span>|</span><a href="#36767299">root</a><span>|</span><a href="#36767688">parent</a><span>|</span><a href="#36768030">next</a><span>|</span><label class="collapse" for="c-36769651">[-]</label><label class="expand" for="c-36769651">[1 more]</label></div><br/><div class="children"><div class="content">They are mapping thoughts (through fMRI) to CLIP embeddings. Are CLIP embeddings limited to london buses, steam trains, surfing and football? or Am I missing something?<p>Once you have a CLIP embedding you can reconstruct an image in a stable-diffusion-like fashion</div><br/></div></div></div></div></div></div><div id="36768030" class="c"><input type="checkbox" id="c-36768030" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#36767299">parent</a><span>|</span><a href="#36767351">prev</a><span>|</span><a href="#36767822">next</a><span>|</span><label class="collapse" for="c-36768030">[-]</label><label class="expand" for="c-36768030">[1 more]</label></div><br/><div class="children"><div class="content">reading suspect&#x27;s mind</div><br/></div></div></div></div><div id="36767822" class="c"><input type="checkbox" id="c-36767822" checked=""/><div class="controls bullet"><span class="by">mometsi</span><span>|</span><a href="#36767299">prev</a><span>|</span><a href="#36768925">next</a><span>|</span><label class="collapse" for="c-36767822">[-]</label><label class="expand" for="c-36767822">[2 more]</label></div><br/><div class="children"><div class="content">For context, early vision is easier to map than you might expect.<p>Here&#x27;s a radiograph of the primary visual cortex created in 1982 by projecting a pattern onto a macaque&#x27;s retina:   
<a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20100814085656im_&#x2F;http:&#x2F;&#x2F;hubel.med.harvard.edu&#x2F;book&#x2F;114.jpg" rel="nofollow noreferrer">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20100814085656im_&#x2F;http:&#x2F;&#x2F;hubel.m...</a><p>An injection of radioactive sugar lets you see where the neurons were firing away and metabolizing the sugar.<p>(<a href="https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;7134981&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;7134981&#x2F;</a>)</div><br/><div id="36769020" class="c"><input type="checkbox" id="c-36769020" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#36767822">parent</a><span>|</span><a href="#36768925">next</a><span>|</span><label class="collapse" for="c-36769020">[-]</label><label class="expand" for="c-36769020">[1 more]</label></div><br/><div class="children"><div class="content">But can brain activity be mapped anywhere like this with fmri? I doubt it. But yes — it is cool that the brain keeps spatial proportions of reality in the brain map! Very unlike latent space.</div><br/></div></div></div></div><div id="36768925" class="c"><input type="checkbox" id="c-36768925" checked=""/><div class="controls bullet"><span class="by">radicaldreamer</span><span>|</span><a href="#36767822">prev</a><span>|</span><a href="#36768491">next</a><span>|</span><label class="collapse" for="c-36768925">[-]</label><label class="expand" for="c-36768925">[1 more]</label></div><br/><div class="children"><div class="content">They should dose people with DMT in the fMRI and run it through the model.</div><br/></div></div><div id="36768491" class="c"><input type="checkbox" id="c-36768491" checked=""/><div class="controls bullet"><span class="by">puchatek</span><span>|</span><a href="#36768925">prev</a><span>|</span><a href="#36769030">next</a><span>|</span><label class="collapse" for="c-36768491">[-]</label><label class="expand" for="c-36768491">[2 more]</label></div><br/><div class="children"><div class="content">As someone suffering from intrusive thoughts I do not look forward to a future where other people can see what I sometimes see in my head.</div><br/><div id="36768829" class="c"><input type="checkbox" id="c-36768829" checked=""/><div class="controls bullet"><span class="by">wiz21c</span><span>|</span><a href="#36768491">parent</a><span>|</span><a href="#36769030">next</a><span>|</span><label class="collapse" for="c-36768829">[-]</label><label class="expand" for="c-36768829">[1 more]</label></div><br/><div class="children"><div class="content">As a person who is normal by any measuring standard,  I do not look forward to a future where other people can see what I sometimes see in my head.<p>You&#x27;d be quite surprised.</div><br/></div></div></div></div><div id="36767800" class="c"><input type="checkbox" id="c-36767800" checked=""/><div class="controls bullet"><span class="by">theaiquestion</span><span>|</span><a href="#36769030">prev</a><span>|</span><a href="#36767482">next</a><span>|</span><label class="collapse" for="c-36767800">[-]</label><label class="expand" for="c-36767800">[3 more]</label></div><br/><div class="children"><div class="content">I think the method of merging the pipelines via img2img should use controlnet. Possibly needing to be finetuned specifically for this, although existing controlnet models might work fine for this.<p>This is exactly what you&#x27;d want to use controlnet for - mapping semantic information onto the perceived structure.</div><br/><div id="36768810" class="c"><input type="checkbox" id="c-36768810" checked=""/><div class="controls bullet"><span class="by">tmabraham</span><span>|</span><a href="#36767800">parent</a><span>|</span><a href="#36768906">next</a><span>|</span><label class="collapse" for="c-36768810">[-]</label><label class="expand" for="c-36768810">[1 more]</label></div><br/><div class="children"><div class="content">Yes we&#x27;ve been looking into ControlNet as well, and I think there is one recent fMRI-to-image paper that also has tried ControlNet. Maybe we&#x27;ll use ControlNet in MindEye v2 :)</div><br/></div></div><div id="36768906" class="c"><input type="checkbox" id="c-36768906" checked=""/><div class="controls bullet"><span class="by">atom_101</span><span>|</span><a href="#36767800">parent</a><span>|</span><a href="#36768810">prev</a><span>|</span><a href="#36767482">next</a><span>|</span><label class="collapse" for="c-36768906">[-]</label><label class="expand" for="c-36768906">[1 more]</label></div><br/><div class="children"><div class="content">Yes controlnet will be used in the next version. For this one we couldn&#x27;t get it working in time.</div><br/></div></div></div></div><div id="36767482" class="c"><input type="checkbox" id="c-36767482" checked=""/><div class="controls bullet"><span class="by">ImaCake</span><span>|</span><a href="#36767800">prev</a><span>|</span><a href="#36767094">next</a><span>|</span><label class="collapse" for="c-36767482">[-]</label><label class="expand" for="c-36767482">[2 more]</label></div><br/><div class="children"><div class="content">Aside from helping those with disabilities, what is stopping authorities using this as a lie detector? I assume the tech isn’t quite there yet.</div><br/><div id="36767536" class="c"><input type="checkbox" id="c-36767536" checked=""/><div class="controls bullet"><span class="by">FPGAhacker</span><span>|</span><a href="#36767482">parent</a><span>|</span><a href="#36767094">next</a><span>|</span><label class="collapse" for="c-36767536">[-]</label><label class="expand" for="c-36767536">[1 more]</label></div><br/><div class="children"><div class="content">Not yet.<p>&gt; Models were trained separately for every participant and are not generalizable across people.</div><br/></div></div></div></div><div id="36767094" class="c"><input type="checkbox" id="c-36767094" checked=""/><div class="controls bullet"><span class="by">woeirua</span><span>|</span><a href="#36767482">prev</a><span>|</span><a href="#36767867">next</a><span>|</span><label class="collapse" for="c-36767094">[-]</label><label class="expand" for="c-36767094">[1 more]</label></div><br/><div class="children"><div class="content">Techniques like this give me hope that some day we will be able to objectively diagnose mental illness, and monitor the efficacy of treatment.</div><br/></div></div><div id="36767867" class="c"><input type="checkbox" id="c-36767867" checked=""/><div class="controls bullet"><span class="by">caycep</span><span>|</span><a href="#36767094">prev</a><span>|</span><a href="#36768064">next</a><span>|</span><label class="collapse" for="c-36767867">[-]</label><label class="expand" for="c-36767867">[1 more]</label></div><br/><div class="children"><div class="content">the best recons I&#x27;ve seen so far are from Jack Gallant and Alex Huth&#x27;s labs (at least what was shown publicly at SfN)</div><br/></div></div><div id="36768064" class="c"><input type="checkbox" id="c-36768064" checked=""/><div class="controls bullet"><span class="by">parth17291</span><span>|</span><a href="#36767867">prev</a><span>|</span><a href="#36767510">next</a><span>|</span><label class="collapse" for="c-36768064">[-]</label><label class="expand" for="c-36768064">[1 more]</label></div><br/><div class="children"><div class="content">This is cool . We are going towards future like shown in inception to plant an idea.</div><br/></div></div><div id="36767510" class="c"><input type="checkbox" id="c-36767510" checked=""/><div class="controls bullet"><span class="by">collsni</span><span>|</span><a href="#36768064">prev</a><span>|</span><label class="collapse" for="c-36767510">[-]</label><label class="expand" for="c-36767510">[2 more]</label></div><br/><div class="children"><div class="content">This is mind reading, we are in the future.</div><br/><div id="36768207" class="c"><input type="checkbox" id="c-36768207" checked=""/><div class="controls bullet"><span class="by">QuantumG</span><span>|</span><a href="#36767510">parent</a><span>|</span><label class="collapse" for="c-36768207">[-]</label><label class="expand" for="c-36768207">[1 more]</label></div><br/><div class="children"><div class="content">Found the sucker.</div><br/></div></div></div></div></div></div></div></div></div></body></html>