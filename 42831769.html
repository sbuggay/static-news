<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737968469769" as="style"/><link rel="stylesheet" href="styles.css?v=1737968469769"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://qwenlm.github.io/blog/qwen2.5-1m/">Qwen2.5-1M: Deploy your own Qwen with context length up to 1M tokens</a> <span class="domain">(<a href="https://qwenlm.github.io">qwenlm.github.io</a>)</span></div><div class="subtext"><span>meetpateltech</span> | <span>81 comments</span></div><br/><div><div id="42834105" class="c"><input type="checkbox" id="c-42834105" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#42834527">next</a><span>|</span><label class="collapse" for="c-42834105">[-]</label><label class="expand" for="c-42834105">[1 more]</label></div><br/><div class="children"><div class="content">Related: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Jan&#x2F;26&#x2F;qwen25-1m&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Jan&#x2F;26&#x2F;qwen25-1m&#x2F;</a><p>(via <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42832838">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42832838</a>, but we merged that thread hither)</div><br/></div></div><div id="42834527" class="c"><input type="checkbox" id="c-42834527" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#42834105">prev</a><span>|</span><a href="#42833427">next</a><span>|</span><label class="collapse" for="c-42834527">[-]</label><label class="expand" for="c-42834527">[15 more]</label></div><br/><div class="children"><div class="content">In my experience with AI coding, very large context windows aren&#x27;t useful in practice. Every model seems to get confused when you feed them more than ~25-30k tokens. The models stop obeying their system prompts, can&#x27;t correctly find&#x2F;transcribe pieces of code in the context, etc.<p>Developing aider, I&#x27;ve seen this problem with gpt-4o, Sonnet, DeepSeek, etc. Many aider users report this too. It&#x27;s perhaps the #1 problem users have, so I created a dedicated help page [0].<p>Very large context may be useful for certain tasks with lots of &quot;low value&quot; context. But for coding, it seems to lure users into a problematic regime.<p>[0] <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;troubleshooting&#x2F;edit-errors.html#dont-add-too-many-files" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;troubleshooting&#x2F;edit-errors.html#don...</a></div><br/><div id="42838433" class="c"><input type="checkbox" id="c-42838433" checked=""/><div class="controls bullet"><span class="by">arkh</span><span>|</span><a href="#42834527">parent</a><span>|</span><a href="#42834996">next</a><span>|</span><label class="collapse" for="c-42838433">[-]</label><label class="expand" for="c-42838433">[2 more]</label></div><br/><div class="children"><div class="content">My hypothesis is code completion is not a text completion problem. More of a graph completion one.<p>So we may have got to a local maximum regarding code helpers with LLMs and we&#x27;ll have to wait for some breakthrough in the AI field before we get something better.</div><br/><div id="42838590" class="c"><input type="checkbox" id="c-42838590" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42834527">root</a><span>|</span><a href="#42838433">parent</a><span>|</span><a href="#42834996">next</a><span>|</span><label class="collapse" for="c-42838590">[-]</label><label class="expand" for="c-42838590">[1 more]</label></div><br/><div class="children"><div class="content">But these models don&#x27;t work that well even for text when you gave them a huge context. They&#x27;re reasonably good at summarization, but if you ask them to &quot;continue the story&quot; they will write very inconsistent things (eerily similar to what a sloppy human writer does, though.)</div><br/></div></div></div></div><div id="42834996" class="c"><input type="checkbox" id="c-42834996" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#42834527">parent</a><span>|</span><a href="#42838433">prev</a><span>|</span><a href="#42837931">next</a><span>|</span><label class="collapse" for="c-42834996">[-]</label><label class="expand" for="c-42834996">[2 more]</label></div><br/><div class="children"><div class="content">Aider is great, but you need specific formats from the llm. That might be where the challenge is.<p>I&#x27;ve used the giant context in Gemini to dump a code base and say: describe the major data structures and data flows.<p>Things like that, overview documents, work great. It&#x27;s amazing for orienting in an unfamiliar codebase.</div><br/><div id="42835368" class="c"><input type="checkbox" id="c-42835368" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#42834527">root</a><span>|</span><a href="#42834996">parent</a><span>|</span><a href="#42837931">next</a><span>|</span><label class="collapse" for="c-42835368">[-]</label><label class="expand" for="c-42835368">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that is true. Aider expects to work with the LLM to automatically apply edits to the source files. This requires precision from the LLM, which is what breaks down when you overload them with context.</div><br/></div></div></div></div><div id="42837931" class="c"><input type="checkbox" id="c-42837931" checked=""/><div class="controls bullet"><span class="by">Yusefmosiah</span><span>|</span><a href="#42834527">parent</a><span>|</span><a href="#42834996">prev</a><span>|</span><a href="#42834709">next</a><span>|</span><label class="collapse" for="c-42837931">[-]</label><label class="expand" for="c-42837931">[1 more]</label></div><br/><div class="children"><div class="content">It’s not just the quantity of tokens in context that matters, but the coherence of the concepts in the context.<p>Many conflicting ideas are harder for models to follow than one large unified idea.</div><br/></div></div><div id="42834709" class="c"><input type="checkbox" id="c-42834709" checked=""/><div class="controls bullet"><span class="by">lifty</span><span>|</span><a href="#42834527">parent</a><span>|</span><a href="#42837931">prev</a><span>|</span><a href="#42834969">next</a><span>|</span><label class="collapse" for="c-42834709">[-]</label><label class="expand" for="c-42834709">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for aider! It has become an integral part of my workflow. Looking forward to try DeepSeek in architect mode with Sonnet as the driver. Curious if it will be a noticeable improvement as compared to using Sonnet by itself.</div><br/><div id="42835388" class="c"><input type="checkbox" id="c-42835388" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#42834527">root</a><span>|</span><a href="#42834709">parent</a><span>|</span><a href="#42834969">next</a><span>|</span><label class="collapse" for="c-42835388">[-]</label><label class="expand" for="c-42835388">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing you&#x27;re interesting in R1+Sonnet because of the recent SOTA benchmark result? It does seem to be a powerful architect&#x2F;editor combo.<p><a href="https:&#x2F;&#x2F;aider.chat&#x2F;2025&#x2F;01&#x2F;24&#x2F;r1-sonnet.html" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;2025&#x2F;01&#x2F;24&#x2F;r1-sonnet.html</a></div><br/></div></div></div></div><div id="42834969" class="c"><input type="checkbox" id="c-42834969" checked=""/><div class="controls bullet"><span class="by">seunosewa</span><span>|</span><a href="#42834527">parent</a><span>|</span><a href="#42834709">prev</a><span>|</span><a href="#42838365">next</a><span>|</span><label class="collapse" for="c-42834969">[-]</label><label class="expand" for="c-42834969">[5 more]</label></div><br/><div class="children"><div class="content">The behaviour you described is what happens when you have small context windows. Perhaps you&#x27;re feeding the models with more tokens than you think you are. I have enjoyed loading large codebases into AI Studio and getting very satisfying and accurate answers because the models have 1M to 2M token context windows.</div><br/><div id="42835998" class="c"><input type="checkbox" id="c-42835998" checked=""/><div class="controls bullet"><span class="by">dr_kiszonka</span><span>|</span><a href="#42834527">root</a><span>|</span><a href="#42834969">parent</a><span>|</span><a href="#42838365">next</a><span>|</span><label class="collapse" for="c-42835998">[-]</label><label class="expand" for="c-42835998">[4 more]</label></div><br/><div class="children"><div class="content">How do you get those large codebases into AI Studio? Concat everything into one big file?</div><br/><div id="42836720" class="c"><input type="checkbox" id="c-42836720" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#42834527">root</a><span>|</span><a href="#42835998">parent</a><span>|</span><a href="#42836706">next</a><span>|</span><label class="collapse" for="c-42836720">[-]</label><label class="expand" for="c-42836720">[1 more]</label></div><br/><div class="children"><div class="content">I use yek<p><a href="https:&#x2F;&#x2F;github.com&#x2F;bodo-run&#x2F;yek">https:&#x2F;&#x2F;github.com&#x2F;bodo-run&#x2F;yek</a></div><br/></div></div><div id="42836706" class="c"><input type="checkbox" id="c-42836706" checked=""/><div class="controls bullet"><span class="by">social_quotient</span><span>|</span><a href="#42834527">root</a><span>|</span><a href="#42835998">parent</a><span>|</span><a href="#42836720">prev</a><span>|</span><a href="#42836387">next</a><span>|</span><label class="collapse" for="c-42836706">[-]</label><label class="expand" for="c-42836706">[1 more]</label></div><br/><div class="children"><div class="content">Concat to a file but it helps to make an ascii tree at the top and then for each merged file out its path and orientation details. I’ve also started playing with adding line ranges to the ascii tree hoping that the LLMs (more specifically the agentic ones) start getting smart enough to jump to the relevant section.</div><br/></div></div><div id="42836387" class="c"><input type="checkbox" id="c-42836387" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#42834527">root</a><span>|</span><a href="#42835998">parent</a><span>|</span><a href="#42836706">prev</a><span>|</span><a href="#42838365">next</a><span>|</span><label class="collapse" for="c-42836387">[-]</label><label class="expand" for="c-42836387">[1 more]</label></div><br/><div class="children"><div class="content">Basically yes, I have a helper program, but that&#x27;s mainly what it does.</div><br/></div></div></div></div></div></div><div id="42838365" class="c"><input type="checkbox" id="c-42838365" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#42834527">parent</a><span>|</span><a href="#42834969">prev</a><span>|</span><a href="#42834786">next</a><span>|</span><label class="collapse" for="c-42838365">[-]</label><label class="expand" for="c-42838365">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, and thanks to the features of the programming language, it&#x27;s very easy to automatically assemble a highly relevant but short context, just by following symbol references recursively.</div><br/></div></div><div id="42834786" class="c"><input type="checkbox" id="c-42834786" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42834527">parent</a><span>|</span><a href="#42838365">prev</a><span>|</span><a href="#42833427">next</a><span>|</span><label class="collapse" for="c-42834786">[-]</label><label class="expand" for="c-42834786">[1 more]</label></div><br/><div class="children"><div class="content">Claude works incredibly well for me with asking for code changes to projects filling up 80% of context (160K tokens).  It&#x27;s way expensive with the API though but reasonable through the web interface with pro.</div><br/></div></div></div></div><div id="42833427" class="c"><input type="checkbox" id="c-42833427" checked=""/><div class="controls bullet"><span class="by">tmcdonald</span><span>|</span><a href="#42834527">prev</a><span>|</span><a href="#42833961">next</a><span>|</span><label class="collapse" for="c-42833427">[-]</label><label class="expand" for="c-42833427">[11 more]</label></div><br/><div class="children"><div class="content">Ollama has a num_ctx parameter that controls the context window length - it defaults to 2048. At a guess you will need to set that.</div><br/><div id="42836715" class="c"><input type="checkbox" id="c-42836715" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#42833427">parent</a><span>|</span><a href="#42833566">next</a><span>|</span><label class="collapse" for="c-42836715">[-]</label><label class="expand" for="c-42836715">[3 more]</label></div><br/><div class="children"><div class="content">This is a harsh foot-gun that seems to harm many ollama users.<p>That 2k default is extremely low, and ollama *silently* discards the leading context. So users have no idea that most of their data hasn’t been provided to the model.<p>I’ve had to add docs [0] to aider about this, and aider overrides the default to at least 8k tokens.
I’d like to do more, but unilaterally raising the context window size has performance implications for users.<p>Edit: Ok, aider now gives ollama users a clear warning when their chat context exceeds their ollama context window [1].<p>[0] <a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms&#x2F;ollama.html#setting-the-context-window-size" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;llms&#x2F;ollama.html#setting-the-context...</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;Aider-AI&#x2F;aider&#x2F;blob&#x2F;main&#x2F;aider&#x2F;coders&#x2F;base_coder.py#L1255">https:&#x2F;&#x2F;github.com&#x2F;Aider-AI&#x2F;aider&#x2F;blob&#x2F;main&#x2F;aider&#x2F;coders&#x2F;bas...</a></div><br/><div id="42838234" class="c"><input type="checkbox" id="c-42838234" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42833427">root</a><span>|</span><a href="#42836715">parent</a><span>|</span><a href="#42833566">next</a><span>|</span><label class="collapse" for="c-42838234">[-]</label><label class="expand" for="c-42838234">[2 more]</label></div><br/><div class="children"><div class="content">There are several issues in the Ollama GitHub issue tracker related to this, like this[1] or this[2].<p>Fortunately it&#x27;s easy to create a variant of the model with increased context size using the CLI[3] and then use that variant instead.<p>Just be mindful that longer context means more memory required[4].<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;4967">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;4967</a><p>[2]: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;7043">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;7043</a><p>[3]: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;8099#issuecomment-2543316682">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;issues&#x2F;8099#issuecomment-25...</a><p>[4]: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1848puo&#x2F;comment&#x2F;kavf6tb&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1848puo&#x2F;comment...</a></div><br/><div id="42838521" class="c"><input type="checkbox" id="c-42838521" checked=""/><div class="controls bullet"><span class="by">neuralkoi</span><span>|</span><a href="#42833427">root</a><span>|</span><a href="#42838234">parent</a><span>|</span><a href="#42833566">next</a><span>|</span><label class="collapse" for="c-42838521">[-]</label><label class="expand" for="c-42838521">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! I was looking for how to do this. The example in the issue above shows how to increase the context size in ollama:<p><pre><code>    $ ollama run llama3.2
    &gt;&gt;&gt; &#x2F;set parameter num_ctx 32768
    Set parameter &#x27;num_ctx&#x27; to &#x27;32768&#x27;
    &gt;&gt;&gt; &#x2F;save llama3.2-32k
    Created new model &#x27;llama3.2-32k&#x27;
    &gt;&gt;&gt; &#x2F;bye
    $ ollama run llama3.2-32k &quot;Summarize this file: $(cat README.md)&quot;
    ...
</code></pre>
The table in the reddit post above also shows context size vs memory requirements for Model: 01-ai&#x2F;Yi-34B-200K
Params: 34.395B
Mode: infer<p><pre><code>    Sequence Length vs Bit Precision Memory Requirements
       SL &#x2F; BP |     4      |     6      |     8      |     16
    --------------------------------------------------------------
           256 |     16.0GB |     24.0GB |     32.1GB |     64.1GB
           512 |     16.0GB |     24.1GB |     32.1GB |     64.2GB
          1024 |     16.1GB |     24.1GB |     32.2GB |     64.3GB
          2048 |     16.1GB |     24.2GB |     32.3GB |     64.5GB
          4096 |     16.3GB |     24.4GB |     32.5GB |     65.0GB
          8192 |     16.5GB |     24.7GB |     33.0GB |     65.9GB
         16384 |     17.0GB |     25.4GB |     33.9GB |     67.8GB
         32768 |     17.9GB |     26.8GB |     35.8GB |     71.6GB
         65536 |     19.8GB |     29.6GB |     39.5GB |     79.1GB
        131072 |     23.5GB |     35.3GB |     47.0GB |     94.1GB
    *   200000 |     27.5GB |     41.2GB |     54.9GB |    109.8GB

    * Model Max Context Size
</code></pre>
Code: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;lapp0&#x2F;d28931ebc9f59838800faa7c73e3a0dc&#x2F;edit" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;lapp0&#x2F;d28931ebc9f59838800faa7c73e3a0...</a></div><br/></div></div></div></div></div></div><div id="42833566" class="c"><input type="checkbox" id="c-42833566" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42833427">parent</a><span>|</span><a href="#42836715">prev</a><span>|</span><a href="#42834137">next</a><span>|</span><label class="collapse" for="c-42833566">[-]</label><label class="expand" for="c-42833566">[4 more]</label></div><br/><div class="children"><div class="content">Huh! I had incorrectly assumed that was for output, not input. Thanks!<p>YES that was it:<p><pre><code>  files-to-prompt \
    ~&#x2F;Dropbox&#x2F;Development&#x2F;llm \
    -e py -c | \
  llm -m q1m &#x27;describe this codebase in detail&#x27; \
   -o num_ctx 80000
</code></pre>
I was watching my memory usage and it quickly maxed out my 64GB so I hit Ctrl+C before my Mac crashed.</div><br/><div id="42833836" class="c"><input type="checkbox" id="c-42833836" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#42833427">root</a><span>|</span><a href="#42833566">parent</a><span>|</span><a href="#42837109">next</a><span>|</span><label class="collapse" for="c-42833836">[-]</label><label class="expand" for="c-42833836">[1 more]</label></div><br/><div class="children"><div class="content">Sorry this isn&#x27;t more obvious. Ideally VRAM usage for the context window (the KV cache) becomes dynamic, starting small and growing with token usage, whereas right now Ollama defaults to a size of 2K which can be overridden at runtime. A great example of this is vLLM&#x27;s PagedAttention implementation [1] or Microsoft&#x27;s vAttention [2] which is CUDA-specific (and there are quite a few others).<p>1M tokens will definitely require a lot of KV cache memory. One way to reduce the memory footprint is to use KV cache quantization, which has recently been added behind a flag [3] and will 1&#x2F;4 the memory footprint if 4-bit KV cache quantization is used (OLLAMA_KV_CACHE_TYPE=q4_0 ollama serve)<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.06180" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.06180</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;vattention">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;vattention</a><p>[3] <a href="https:&#x2F;&#x2F;smcleod.net&#x2F;2024&#x2F;12&#x2F;bringing-k&#x2F;v-context-quantisation-to-ollama&#x2F;" rel="nofollow">https:&#x2F;&#x2F;smcleod.net&#x2F;2024&#x2F;12&#x2F;bringing-k&#x2F;v-context-quantisatio...</a></div><br/></div></div><div id="42837109" class="c"><input type="checkbox" id="c-42837109" checked=""/><div class="controls bullet"><span class="by">gcanyon</span><span>|</span><a href="#42833427">root</a><span>|</span><a href="#42833566">parent</a><span>|</span><a href="#42833836">prev</a><span>|</span><a href="#42833755">next</a><span>|</span><label class="collapse" for="c-42837109">[-]</label><label class="expand" for="c-42837109">[1 more]</label></div><br/><div class="children"><div class="content">I think Apple stumbled into a problem here, and I hope they solve it: reasonably priced Macs are -- by the new standards set by modern LLMs -- severely memory-constrained. MacBook Airs max out at 24GB. MacBook Pros go to 32GB for $2200, 48GB for something like $2800, and to get to 128GB requires shelling out over $4000. A Mini can get you to 64GB for $2000. A Mac Studio can get you to 96GB for $3000, or 192GB for $5600.<p>In this LLM era, those are rookie numbers. It should be possible to get a Mac with a lesser processor but at least 256GB of memory for $2000. I realize part of the issue is the lead time for chip design -- since Mac memory is an integral part of the chip, and the current crop were designed before the idea of running something like an LLM locally was a real probability.<p>But I hope the next year or two show significant increases in the default (and possible) memory for Macs.</div><br/></div></div><div id="42833755" class="c"><input type="checkbox" id="c-42833755" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#42833427">root</a><span>|</span><a href="#42833566">parent</a><span>|</span><a href="#42837109">prev</a><span>|</span><a href="#42834137">next</a><span>|</span><label class="collapse" for="c-42833755">[-]</label><label class="expand" for="c-42833755">[1 more]</label></div><br/><div class="children"><div class="content">This has been the problem with a lot of long context use cases. It&#x27;s not just the model&#x27;s support but also sufficient compute and inference time. This is exactly why I was excited for Mamba and now possibly Lightning attention.<p>Even though the new DCA based on which these models provide long context could be an interesting area to watch;</div><br/></div></div></div></div><div id="42834137" class="c"><input type="checkbox" id="c-42834137" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#42833427">parent</a><span>|</span><a href="#42833566">prev</a><span>|</span><a href="#42833607">next</a><span>|</span><label class="collapse" for="c-42834137">[-]</label><label class="expand" for="c-42834137">[2 more]</label></div><br/><div class="children"><div class="content">Ollama is a &quot;easymode&quot; LLM runtime and as such has all the problems that every easymode thing has. It will assume things and the moment you want to do anything interesting those assumptions will shoot you in the foot, though I&#x27;ve found ollama plays so fast and loose even first party things that &quot;should just work&quot; do not. For example if you run R1 (at least as of 2 days ago when i tried this) using the default `ollama run deepseek-r1:7b` you will get different context size, top_p and temperature vs what Deepseek recommends in their release post.</div><br/><div id="42836220" class="c"><input type="checkbox" id="c-42836220" checked=""/><div class="controls bullet"><span class="by">xigency</span><span>|</span><a href="#42833427">root</a><span>|</span><a href="#42834137">parent</a><span>|</span><a href="#42833607">next</a><span>|</span><label class="collapse" for="c-42836220">[-]</label><label class="expand" for="c-42836220">[1 more]</label></div><br/><div class="children"><div class="content">Ollama definitely is a strange beast. The sparseness of the documentation seems to imply that things will &#x27;just work&#x27; and yet, they often don&#x27;t.</div><br/></div></div></div></div><div id="42833607" class="c"><input type="checkbox" id="c-42833607" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#42833427">parent</a><span>|</span><a href="#42834137">prev</a><span>|</span><a href="#42833961">next</a><span>|</span><label class="collapse" for="c-42833607">[-]</label><label class="expand" for="c-42833607">[1 more]</label></div><br/><div class="children"><div class="content">Yup, and this parameter is supported by the plugin he&#x27;s using:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;taketwo&#x2F;llm-ollama&#x2F;blob&#x2F;4ccd5181c099af963dc14b9e5dfa46fe9f1db0f2&#x2F;README.md?plain=1#L109">https:&#x2F;&#x2F;github.com&#x2F;taketwo&#x2F;llm-ollama&#x2F;blob&#x2F;4ccd5181c099af963...</a></div><br/></div></div></div></div><div id="42833961" class="c"><input type="checkbox" id="c-42833961" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42833427">prev</a><span>|</span><a href="#42833718">next</a><span>|</span><label class="collapse" for="c-42833961">[-]</label><label class="expand" for="c-42833961">[2 more]</label></div><br/><div class="children"><div class="content">Here are tips for running it on macOS using MLX: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;awnihannun&#x2F;status&#x2F;1883611098081099914" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;awnihannun&#x2F;status&#x2F;1883611098081099914</a> - using <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;mlx-community&#x2F;Qwen2.5-7B-Instruct-1M-4bit" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;mlx-community&#x2F;Qwen2.5-7B-Instruct-1M-...</a></div><br/><div id="42834649" class="c"><input type="checkbox" id="c-42834649" checked=""/><div class="controls bullet"><span class="by">woadwarrior01</span><span>|</span><a href="#42833961">parent</a><span>|</span><a href="#42833718">next</a><span>|</span><label class="collapse" for="c-42834649">[-]</label><label class="expand" for="c-42834649">[1 more]</label></div><br/><div class="children"><div class="content">MLX does not support dual chunk attention[1] that these models use for long contexts, yet.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.17463" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.17463</a></div><br/></div></div></div></div><div id="42833718" class="c"><input type="checkbox" id="c-42833718" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#42833961">prev</a><span>|</span><a href="#42833286">next</a><span>|</span><label class="collapse" for="c-42833718">[-]</label><label class="expand" for="c-42833718">[6 more]</label></div><br/><div class="children"><div class="content">Just want to confirm: so this is the first locally runnable model with a context length of greater than 128K and it’s gone straight to 1M, correct?</div><br/><div id="42833885" class="c"><input type="checkbox" id="c-42833885" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#42833718">parent</a><span>|</span><a href="#42833801">next</a><span>|</span><label class="collapse" for="c-42833885">[-]</label><label class="expand" for="c-42833885">[2 more]</label></div><br/><div class="children"><div class="content">No, this is not the first local model with a context length of greater than 128k, there have been such models, for example the following<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;ai21labs&#x2F;AI21-Jamba-1.5-Mini" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;ai21labs&#x2F;AI21-Jamba-1.5-Mini</a> 256k
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;THUDM&#x2F;glm-4-9b-chat-1m" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;THUDM&#x2F;glm-4-9b-chat-1m</a> 1M<p>and many other&#x27;s that supposedly extended traditional models via finetune&#x2F;rope scaling</div><br/><div id="42833932" class="c"><input type="checkbox" id="c-42833932" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#42833718">root</a><span>|</span><a href="#42833885">parent</a><span>|</span><a href="#42833801">next</a><span>|</span><label class="collapse" for="c-42833932">[-]</label><label class="expand" for="c-42833932">[1 more]</label></div><br/><div class="children"><div class="content">Thanks.</div><br/></div></div></div></div><div id="42833801" class="c"><input type="checkbox" id="c-42833801" checked=""/><div class="controls bullet"><span class="by">terhechte</span><span>|</span><a href="#42833718">parent</a><span>|</span><a href="#42833885">prev</a><span>|</span><a href="#42833286">next</a><span>|</span><label class="collapse" for="c-42833801">[-]</label><label class="expand" for="c-42833801">[3 more]</label></div><br/><div class="children"><div class="content">Yes. It requires a lot of ram, and even on a M4 with a lot of ram, if you give it 1mio tokens the prompt processing alone (that is, before you get the first response token) will probably take ~30min or more. However I&#x27;m looking forward to check if indeed I can give it a whole codebase and ask questions about it.</div><br/><div id="42836104" class="c"><input type="checkbox" id="c-42836104" checked=""/><div class="controls bullet"><span class="by">marci</span><span>|</span><a href="#42833718">root</a><span>|</span><a href="#42833801">parent</a><span>|</span><a href="#42833854">next</a><span>|</span><label class="collapse" for="c-42836104">[-]</label><label class="expand" for="c-42836104">[1 more]</label></div><br/><div class="children"><div class="content">You might want to try caching to a file with mlx.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx-examples&#x2F;pull&#x2F;956">https:&#x2F;&#x2F;github.com&#x2F;ml-explore&#x2F;mlx-examples&#x2F;pull&#x2F;956</a><p>edit: here&#x27;s a quick example for qwen2.5-1M from a mlx dev<p><a href="https:&#x2F;&#x2F;x.com&#x2F;awnihannun&#x2F;status&#x2F;1883611098081099914" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;awnihannun&#x2F;status&#x2F;1883611098081099914</a></div><br/></div></div></div></div></div></div><div id="42833286" class="c"><input type="checkbox" id="c-42833286" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42833718">prev</a><span>|</span><a href="#42833966">next</a><span>|</span><label class="collapse" for="c-42833286">[-]</label><label class="expand" for="c-42833286">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m really interested in hearing from anyone who <i>does</i> manage to successfully run a long prompt through one these on a Mac (using one of the GGUF versions, or through other means).</div><br/><div id="42833818" class="c"><input type="checkbox" id="c-42833818" checked=""/><div class="controls bullet"><span class="by">terhechte</span><span>|</span><a href="#42833286">parent</a><span>|</span><a href="#42833426">next</a><span>|</span><label class="collapse" for="c-42833818">[-]</label><label class="expand" for="c-42833818">[1 more]</label></div><br/><div class="children"><div class="content">I bought an M4 Max with 128g of ram just for these use cases. Currently downloading the 7b</div><br/></div></div><div id="42833333" class="c"><input type="checkbox" id="c-42833333" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#42833286">parent</a><span>|</span><a href="#42833426">prev</a><span>|</span><a href="#42833966">next</a><span>|</span><label class="collapse" for="c-42833333">[-]</label><label class="expand" for="c-42833333">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m missing what `files-to-prompt` does. I have an M3 Max and can take a stab at it, although I&#x27;m currently fussing with a few quantized -r1 models...</div><br/><div id="42833347" class="c"><input type="checkbox" id="c-42833347" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42833286">root</a><span>|</span><a href="#42833333">parent</a><span>|</span><a href="#42833966">next</a><span>|</span><label class="collapse" for="c-42833347">[-]</label><label class="expand" for="c-42833347">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s this tool: <a href="https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;files-to-prompt&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;files-to-prompt&#x2F;</a></div><br/><div id="42833454" class="c"><input type="checkbox" id="c-42833454" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#42833286">root</a><span>|</span><a href="#42833347">parent</a><span>|</span><a href="#42833966">next</a><span>|</span><label class="collapse" for="c-42833454">[-]</label><label class="expand" for="c-42833454">[1 more]</label></div><br/><div class="children"><div class="content">You might want to use the file markers that the model outputs while being loaded by ollama:<p><pre><code>    lm_load_print_meta: general.name     = Qwen2.5 7B Instruct 1M
    llm_load_print_meta: BOS token        = 151643 &#x27;&lt;|endoftext|&gt;&#x27;
    llm_load_print_meta: EOS token        = 151645 &#x27;&lt;|im_end|&gt;&#x27;
    llm_load_print_meta: EOT token        = 151645 &#x27;&lt;|im_end|&gt;&#x27;
    llm_load_print_meta: PAD token        = 151643 &#x27;&lt;|endoftext|&gt;&#x27;
    llm_load_print_meta: LF token         = 148848 &#x27;ÄĬ&#x27;
    llm_load_print_meta: FIM PRE token    = 151659 &#x27;&lt;|fim_prefix|&gt;&#x27;
    llm_load_print_meta: FIM SUF token    = 151661 &#x27;&lt;|fim_suffix|&gt;&#x27;
    llm_load_print_meta: FIM MID token    = 151660 &#x27;&lt;|fim_middle|&gt;&#x27;
    llm_load_print_meta: FIM PAD token    = 151662 &#x27;&lt;|fim_pad|&gt;&#x27;
    llm_load_print_meta: FIM REP token    = 151663 &#x27;&lt;|repo_name|&gt;&#x27;
    llm_load_print_meta: FIM SEP token    = 151664 &#x27;&lt;|file_sep|&gt;&#x27;
    llm_load_print_meta: EOG token        = 151643 &#x27;&lt;|endoftext|&gt;&#x27;
    llm_load_print_meta: EOG token        = 151645 &#x27;&lt;|im_end|&gt;&#x27;
    llm_load_print_meta: EOG token        = 151662 &#x27;&lt;|fim_pad|&gt;&#x27;
    llm_load_print_meta: EOG token        = 151663 &#x27;&lt;|repo_name|&gt;&#x27;
    llm_load_print_meta: EOG token        = 151664 &#x27;&lt;|file_sep|&gt;&#x27;
    llm_load_print_meta: max token length = 256</code></pre></div><br/></div></div></div></div></div></div></div></div><div id="42833966" class="c"><input type="checkbox" id="c-42833966" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#42833286">prev</a><span>|</span><a href="#42833870">next</a><span>|</span><label class="collapse" for="c-42833966">[-]</label><label class="expand" for="c-42833966">[5 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the SOTA for memory-centric computing? I feel like maybe we need a new paradigm or something to bring the price of AI memory down.<p>Maybe they can take some of those hundreds of billions and invest in new approaches.<p>Because racks of H100s are not sustainable. But it&#x27;s clear that increasing the amount of memory available is key to getting more intelligence or capabilities.<p>Maybe there is a way to connect DRAM with photonic interconnects that doesn&#x27;t require much data ordering for AI if the neural network software model changes somewhat.<p>Is there something that has the same capabilities of a transformer but doesn&#x27;t operate on sequences?<p>If I was a little smarter and had any math ability I feel like I could contribute.<p>But I am smart enough to know that just building bigger and bigger data centers is not the ideal path forward.</div><br/><div id="42837892" class="c"><input type="checkbox" id="c-42837892" checked=""/><div class="controls bullet"><span class="by">lovelearning</span><span>|</span><a href="#42833966">parent</a><span>|</span><a href="#42834211">next</a><span>|</span><label class="collapse" for="c-42837892">[-]</label><label class="expand" for="c-42837892">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure how SOTA it is but the sentence about connecting DRAM differently reminded me of Cerebras&#x27; scalable MemoryX and its &quot;weight streaming&quot; architecture to their custom ASIC. You may find it interesting.<p>[1]: <a href="https:&#x2F;&#x2F;cerebras.ai&#x2F;press-release&#x2F;cerebras-systems-announces-worlds-first-brain-scale-artificial-intelligence-solution&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cerebras.ai&#x2F;press-release&#x2F;cerebras-systems-announces...</a><p>[2]: <a href="https:&#x2F;&#x2F;cerebras.ai&#x2F;chip&#x2F;announcing-the-cerebras-architecture-for-extreme-scale-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cerebras.ai&#x2F;chip&#x2F;announcing-the-cerebras-architectur...</a></div><br/><div id="42837938" class="c"><input type="checkbox" id="c-42837938" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#42833966">root</a><span>|</span><a href="#42837892">parent</a><span>|</span><a href="#42834211">next</a><span>|</span><label class="collapse" for="c-42837938">[-]</label><label class="expand" for="c-42837938">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, Cerebras seems to be the SOTA. I suspect we need something more radically different for truly memory-centric computing that will be significantly more efficient.</div><br/></div></div></div></div><div id="42834211" class="c"><input type="checkbox" id="c-42834211" checked=""/><div class="controls bullet"><span class="by">mkroman</span><span>|</span><a href="#42833966">parent</a><span>|</span><a href="#42837892">prev</a><span>|</span><a href="#42834377">next</a><span>|</span><label class="collapse" for="c-42834211">[-]</label><label class="expand" for="c-42834211">[1 more]</label></div><br/><div class="children"><div class="content">The AI hardware race is still going strong, but with so many rapid changes to the fundamental architectures, it doesn&#x27;t make sense to bet everything on specialized hardware just yet.. It&#x27;s happening, but it&#x27;s expensive and slow.<p>There&#x27;s just not enough capacity to build memory fast enough right now. Everyone needs the biggest and fastest modules they can get, since it directly impacts the performance of the models.<p>There&#x27;s still a lot of happening to improve memory, like the latest Titans paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2501.00663" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2501.00663</a><p>So I think until a breakthrough happens or the fabs catch up, it&#x27;ll be this painful race to build more datacenters.</div><br/></div></div><div id="42834377" class="c"><input type="checkbox" id="c-42834377" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#42833966">parent</a><span>|</span><a href="#42834211">prev</a><span>|</span><a href="#42833870">next</a><span>|</span><label class="collapse" for="c-42834377">[-]</label><label class="expand" for="c-42834377">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Because racks of H100s are not sustainable.<p>Huh? Racks of H100s are the most sustainable thing we can have for LLMs for now.</div><br/></div></div></div></div><div id="42833870" class="c"><input type="checkbox" id="c-42833870" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#42833966">prev</a><span>|</span><a href="#42833588">next</a><span>|</span><label class="collapse" for="c-42833870">[-]</label><label class="expand" for="c-42833870">[2 more]</label></div><br/><div class="children"><div class="content">This API only model with a 1M context window was released back in Nov. Just for some historical context.<p><a href="https:&#x2F;&#x2F;qwenlm.github.io&#x2F;blog&#x2F;qwen2.5-turbo&#x2F;" rel="nofollow">https:&#x2F;&#x2F;qwenlm.github.io&#x2F;blog&#x2F;qwen2.5-turbo&#x2F;</a></div><br/><div id="42833968" class="c"><input type="checkbox" id="c-42833968" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42833870">parent</a><span>|</span><a href="#42833588">next</a><span>|</span><label class="collapse" for="c-42833968">[-]</label><label class="expand" for="c-42833968">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a different model - the 2.5 Turbo one. Today&#x27;s release is something different.</div><br/></div></div></div></div><div id="42833588" class="c"><input type="checkbox" id="c-42833588" checked=""/><div class="controls bullet"><span class="by">bloomingkales</span><span>|</span><a href="#42833870">prev</a><span>|</span><a href="#42833754">next</a><span>|</span><label class="collapse" for="c-42833588">[-]</label><label class="expand" for="c-42833588">[4 more]</label></div><br/><div class="children"><div class="content">I’ve heard rumbling about native context length. I don’t know too much about it, but is this natively 1M context length?<p>So even models like llama3 8b say they have a larger context, but they really don’t in practice. I have a hard time getting past 8k on 16gb vram (you can definitely set the context length higher, but the quality and speed degradation is obvious).<p>I’m curious how people are doing this on modest hardware.</div><br/><div id="42833813" class="c"><input type="checkbox" id="c-42833813" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#42833588">parent</a><span>|</span><a href="#42834457">next</a><span>|</span><label class="collapse" for="c-42833813">[-]</label><label class="expand" for="c-42833813">[2 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t on modest hardware, VRAM size is a function of model size, KV cache that depends on context length and the quant size of the model and K&#x2F;V. 16gb isn&#x27;t much really.  You need more vram, the best way for most folks is to buy a macbook with unified memory.  You can get a 128gb mac, but it&#x27;s not cheap.  If you are handy and resourceful you can build a GPU cluster.</div><br/><div id="42837835" class="c"><input type="checkbox" id="c-42837835" checked=""/><div class="controls bullet"><span class="by">A4ET8a8uTh0_v2</span><span>|</span><a href="#42833588">root</a><span>|</span><a href="#42833813">parent</a><span>|</span><a href="#42834457">next</a><span>|</span><label class="collapse" for="c-42837835">[-]</label><label class="expand" for="c-42837835">[1 more]</label></div><br/><div class="children"><div class="content">I never thought I would say it, but the 128gb mbp is probably the most cost efficient way ( and probably easiest ) of doing it. New nvidia cards ( 5090 ) are 32gb and supposedly just shy of 2k and used a100 40gb is about 8k..<p>All in all, not a cheap hobby ( if you are not doing it for work ).</div><br/></div></div></div></div><div id="42834457" class="c"><input type="checkbox" id="c-42834457" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#42833588">parent</a><span>|</span><a href="#42833813">prev</a><span>|</span><a href="#42833754">next</a><span>|</span><label class="collapse" for="c-42834457">[-]</label><label class="expand" for="c-42834457">[1 more]</label></div><br/><div class="children"><div class="content">You need a model that has specifically been extended for larger context windows. For Llama-3 there&#x27;s Llama3-gradient with up to 1M tokens. You can find it at ollama.com</div><br/></div></div></div></div><div id="42833754" class="c"><input type="checkbox" id="c-42833754" checked=""/><div class="controls bullet"><span class="by">jkbbwr</span><span>|</span><a href="#42833588">prev</a><span>|</span><a href="#42838291">next</a><span>|</span><label class="collapse" for="c-42833754">[-]</label><label class="expand" for="c-42833754">[6 more]</label></div><br/><div class="children"><div class="content">Everyone keeps making the context windows bigger, which is nice.<p>But what about output? I want to generate a few thousand lines of code, anyone got any tips?</div><br/><div id="42833779" class="c"><input type="checkbox" id="c-42833779" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#42833754">parent</a><span>|</span><a href="#42833776">next</a><span>|</span><label class="collapse" for="c-42833779">[-]</label><label class="expand" for="c-42833779">[1 more]</label></div><br/><div class="children"><div class="content">Repeatedly ask it for more providing the previous output as context. (Back to context length as a limitation)</div><br/></div></div><div id="42833776" class="c"><input type="checkbox" id="c-42833776" checked=""/><div class="controls bullet"><span class="by">anotheryou</span><span>|</span><a href="#42833754">parent</a><span>|</span><a href="#42833779">prev</a><span>|</span><a href="#42833822">next</a><span>|</span><label class="collapse" for="c-42833776">[-]</label><label class="expand" for="c-42833776">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that the same? limit-wise.<p>Now you just need to convince it to output that much :)</div><br/></div></div><div id="42833822" class="c"><input type="checkbox" id="c-42833822" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#42833754">parent</a><span>|</span><a href="#42833776">prev</a><span>|</span><a href="#42833795">next</a><span>|</span><label class="collapse" for="c-42833822">[-]</label><label class="expand" for="c-42833822">[2 more]</label></div><br/><div class="children"><div class="content">So context size actually helps with this, relative to how LLMs are actually deployed as applications. For example, if you look at how the “continue” option in the DeepSeek web app works for code gen, what they’re likely doing is reinserting the prior messages (in some form) to a new one to prompt further completion. The more  context size a model has and can manage successfully, the better it will likely be able at generating longer code blocks.</div><br/><div id="42838434" class="c"><input type="checkbox" id="c-42838434" checked=""/><div class="controls bullet"><span class="by">nejsjsjsbsb</span><span>|</span><a href="#42833754">root</a><span>|</span><a href="#42833822">parent</a><span>|</span><a href="#42833795">next</a><span>|</span><label class="collapse" for="c-42838434">[-]</label><label class="expand" for="c-42838434">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t input&#x2F;output lengths an arbitrary distinction. Under the hood, output becomes the input for the next token at each step. OpenAI may charge you more $$ by forcing you to add output to the input and call the API again. But running local you don&#x27;t have that issue.</div><br/></div></div></div></div><div id="42833795" class="c"><input type="checkbox" id="c-42833795" checked=""/><div class="controls bullet"><span class="by">AyyEye</span><span>|</span><a href="#42833754">parent</a><span>|</span><a href="#42833822">prev</a><span>|</span><a href="#42838291">next</a><span>|</span><label class="collapse" for="c-42833795">[-]</label><label class="expand" for="c-42833795">[1 more]</label></div><br/><div class="children"><div class="content">These things already produce embarrassing output. If you make it longer it&#x27;s just going to get worse.</div><br/></div></div></div></div><div id="42838291" class="c"><input type="checkbox" id="c-42838291" checked=""/><div class="controls bullet"><span class="by">postepowanieadm</span><span>|</span><a href="#42833754">prev</a><span>|</span><a href="#42838641">next</a><span>|</span><label class="collapse" for="c-42838291">[-]</label><label class="expand" for="c-42838291">[1 more]</label></div><br/><div class="children"><div class="content">Only me getting part of my answer in Chinese?</div><br/></div></div><div id="42838641" class="c"><input type="checkbox" id="c-42838641" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#42838291">prev</a><span>|</span><a href="#42833675">next</a><span>|</span><label class="collapse" for="c-42838641">[-]</label><label class="expand" for="c-42838641">[1 more]</label></div><br/><div class="children"><div class="content">The main problem isn&#x27;t actually context length most of the time. 128K is plenty for a lot of practical tasks. It&#x27;s the generation length, both within turns and especially across turns. And nobody knows how to increase that significantly yet.</div><br/></div></div><div id="42833675" class="c"><input type="checkbox" id="c-42833675" checked=""/><div class="controls bullet"><span class="by">buyucu</span><span>|</span><a href="#42838641">prev</a><span>|</span><a href="#42833811">next</a><span>|</span><label class="collapse" for="c-42833675">[-]</label><label class="expand" for="c-42833675">[2 more]</label></div><br/><div class="children"><div class="content">first, this is amazing!<p>second, how does one increase the context window without requiring obscene amounts of RAM?  we&#x27;re really hitting the limitations of the transformer architecture&#x27;s quadratic scaling...</div><br/><div id="42833725" class="c"><input type="checkbox" id="c-42833725" checked=""/><div class="controls bullet"><span class="by">35mm</span><span>|</span><a href="#42833675">parent</a><span>|</span><a href="#42833811">next</a><span>|</span><label class="collapse" for="c-42833725">[-]</label><label class="expand" for="c-42833725">[1 more]</label></div><br/><div class="children"><div class="content">Chain of agents seems to be a promising approach for splitting up tasks into smaller parts and then synthesising the results[1]<p>[1] <a href="https:&#x2F;&#x2F;research.google&#x2F;blog&#x2F;chain-of-agents-large-language-models-collaborating-on-long-context-tasks&#x2F;" rel="nofollow">https:&#x2F;&#x2F;research.google&#x2F;blog&#x2F;chain-of-agents-large-language-...</a></div><br/></div></div></div></div><div id="42833490" class="c"><input type="checkbox" id="c-42833490" checked=""/><div class="controls bullet"><span class="by">iamnotagenius</span><span>|</span><a href="#42833811">prev</a><span>|</span><a href="#42834255">next</a><span>|</span><label class="collapse" for="c-42833490">[-]</label><label class="expand" for="c-42833490">[13 more]</label></div><br/><div class="children"><div class="content">requires obscene amount of memory for context.</div><br/><div id="42834875" class="c"><input type="checkbox" id="c-42834875" checked=""/><div class="controls bullet"><span class="by">woadwarrior01</span><span>|</span><a href="#42833490">parent</a><span>|</span><a href="#42833554">next</a><span>|</span><label class="collapse" for="c-42834875">[-]</label><label class="expand" for="c-42834875">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s on the model&#x27;s huggingface README[1].<p>&gt; For processing 1 million-token sequences:<p>&gt; Qwen2.5-7B-Instruct-1M: At least 120GB VRAM (total across GPUs).<p>&gt; Qwen2.5-14B-Instruct-1M: At least 320GB VRAM (total across GPUs).<p>[1]: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Qwen&#x2F;Qwen2.5-7B-Instruct-1M" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Qwen&#x2F;Qwen2.5-7B-Instruct-1M</a></div><br/></div></div><div id="42833554" class="c"><input type="checkbox" id="c-42833554" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#42833490">parent</a><span>|</span><a href="#42834875">prev</a><span>|</span><a href="#42834255">next</a><span>|</span><label class="collapse" for="c-42833554">[-]</label><label class="expand" for="c-42833554">[11 more]</label></div><br/><div class="children"><div class="content">More than other models? I thought that context used a lot of memory on all models.<p>And I’d hardly call it obscene. You can buy a Mac Studio with 192GB of memory, that should allow you to max out the context window of the 7B model. Probably not going to be very fast though.</div><br/><div id="42833643" class="c"><input type="checkbox" id="c-42833643" checked=""/><div class="controls bullet"><span class="by">varispeed</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42833554">parent</a><span>|</span><a href="#42834255">next</a><span>|</span><label class="collapse" for="c-42833643">[-]</label><label class="expand" for="c-42833643">[10 more]</label></div><br/><div class="children"><div class="content">Not attainable to working class though. <i>can</i> is doing a lot of heavy lifting here. Seems like after a brief period where technology was essentially class agnostic, now only the wealthy can enjoy being part of development and everyone else can just be a consumer.</div><br/><div id="42833882" class="c"><input type="checkbox" id="c-42833882" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42833643">parent</a><span>|</span><a href="#42834820">next</a><span>|</span><label class="collapse" for="c-42833882">[-]</label><label class="expand" for="c-42833882">[7 more]</label></div><br/><div class="children"><div class="content">Not sure what you mean. Cutting edge computing has never been cheap. And a Mac Studio is definitely within the budget of a software developer in Norway. Not going to feel like a cheap investment, but definitely something that would be doable. Unlike a cluster of H100 GPUs, which would cost as much as a small apartment in Oslo.<p>And you can easily get a dev job in Norway without having to run an LLM locally on your computer.</div><br/><div id="42834642" class="c"><input type="checkbox" id="c-42834642" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42833882">parent</a><span>|</span><a href="#42834629">next</a><span>|</span><label class="collapse" for="c-42834642">[-]</label><label class="expand" for="c-42834642">[1 more]</label></div><br/><div class="children"><div class="content">The money would be better invested in a 2-4 3090 x86 build, than in a Mac Studio. While the Macs have a fantastic performance-per-watt ratio, and have decent memory support (both bus width and memory size), they are not great at compute power. A multi RTX 3090 build totally smokes a Mac at the same price point, at inference speed.</div><br/></div></div><div id="42834629" class="c"><input type="checkbox" id="c-42834629" checked=""/><div class="controls bullet"><span class="by">sgt</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42833882">parent</a><span>|</span><a href="#42834642">prev</a><span>|</span><a href="#42834820">next</a><span>|</span><label class="collapse" for="c-42834629">[-]</label><label class="expand" for="c-42834629">[5 more]</label></div><br/><div class="children"><div class="content">Agreed - it&#x27;s probably not unreasonable. 
So are the M4 Macs becoming the de-facto solution to running an LLM locally? Due to the insane 800 GB&#x2F;sec internal bandwidth of Apple Silicon at its best?</div><br/><div id="42834722" class="c"><input type="checkbox" id="c-42834722" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42834629">parent</a><span>|</span><a href="#42834650">next</a><span>|</span><label class="collapse" for="c-42834722">[-]</label><label class="expand" for="c-42834722">[1 more]</label></div><br/><div class="children"><div class="content">The advantage the Macs have is that they can share RAM between GPU and CPU, and GPU-accessible RAM is everything when you want to run a decent sized LLM.<p>The problem is that most ML models are released for NVIDIA CUDA. Getting them to work on macOS requires translating them, usually to either GGUF (the llama.cpp format) or MLX (using Apple&#x27;s own MLX array framework).<p>As such, as a Mac user I remain envious of people with NVIDIA&#x2F;CUDA rigs with decent amounts of VRAM.<p>The NVIDIA &quot;Digits&quot; product may change things when it ships: <a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2025&#x2F;1&#x2F;6&#x2F;24337530&#x2F;nvidia-ces-digits-super-computer-ai" rel="nofollow">https:&#x2F;&#x2F;www.theverge.com&#x2F;2025&#x2F;1&#x2F;6&#x2F;24337530&#x2F;nvidia-ces-digits...</a> - it may become the new cheapest convenient way to get 128GB of GPU-accessible RAM for running models.</div><br/></div></div><div id="42834650" class="c"><input type="checkbox" id="c-42834650" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42834629">parent</a><span>|</span><a href="#42834722">prev</a><span>|</span><a href="#42834820">next</a><span>|</span><label class="collapse" for="c-42834650">[-]</label><label class="expand" for="c-42834650">[3 more]</label></div><br/><div class="children"><div class="content">No they are lacking compute power to be great at inference.</div><br/><div id="42834684" class="c"><input type="checkbox" id="c-42834684" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42834650">parent</a><span>|</span><a href="#42834820">next</a><span>|</span><label class="collapse" for="c-42834684">[-]</label><label class="expand" for="c-42834684">[2 more]</label></div><br/><div class="children"><div class="content">Can you back that up?</div><br/><div id="42838505" class="c"><input type="checkbox" id="c-42838505" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42834684">parent</a><span>|</span><a href="#42834820">next</a><span>|</span><label class="collapse" for="c-42838505">[-]</label><label class="expand" for="c-42838505">[1 more]</label></div><br/><div class="children"><div class="content">One 3090 seems to be equivalent to one M3 Max at inference: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;BaoKxHj8ww" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;s&#x2F;BaoKxHj8ww</a><p>There are many such threads on Reddit.
M4 Max is incrementally faster, maybe 20%.
Even if you factor in electricity costs, a 2x 3090 setup is IMO the sweet spot, cost&#x2F;benefit wise.<p>And it’s maybe a zany line of argumentation, but 2x 3090 use 10x the power of an M4 Max. While the M4 is maybe the most efficient setup out there, it’s not nearly 10x as efficient. That’s IMO where the lack of compute power comes from.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42834820" class="c"><input type="checkbox" id="c-42834820" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42833643">parent</a><span>|</span><a href="#42833882">prev</a><span>|</span><a href="#42833751">next</a><span>|</span><label class="collapse" for="c-42834820">[-]</label><label class="expand" for="c-42834820">[1 more]</label></div><br/><div class="children"><div class="content">Not much more than something like a used jetski, but possibly depreciates even faster.</div><br/></div></div><div id="42833751" class="c"><input type="checkbox" id="c-42833751" checked=""/><div class="controls bullet"><span class="by">sbarre</span><span>|</span><a href="#42833490">root</a><span>|</span><a href="#42833643">parent</a><span>|</span><a href="#42834820">prev</a><span>|</span><a href="#42834255">next</a><span>|</span><label class="collapse" for="c-42833751">[-]</label><label class="expand" for="c-42833751">[1 more]</label></div><br/><div class="children"><div class="content">I mean...  when has this not been the case?<p>Technology has never been class-agnostic or universally accessible.<p>Even saying that, I would argue that there is more, not less, technology that is accessible to more people today than there ever has been.</div><br/></div></div></div></div></div></div></div></div><div id="42834255" class="c"><input type="checkbox" id="c-42834255" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42833490">prev</a><span>|</span><a href="#42833524">next</a><span>|</span><label class="collapse" for="c-42834255">[-]</label><label class="expand" for="c-42834255">[2 more]</label></div><br/><div class="children"><div class="content">People are getting pretty...clever?...with long context retrieval benchmarking in papers.<p>Here, the prose says &quot;nearly perfect&quot;, the graph is all green except for a little yellow section, and you have to parse a 96 cell table, having familiarity with several models and technical techniques to get the real # (84.4%, and that tops out at 128K, not anywhere near the claimed 1M)<p>I don&#x27;t bring this up to denigrate, but rather to highlight that &quot;nearly perfect&quot; is quite far off still. Don&#x27;t rely on long context for anything you build</div><br/><div id="42838037" class="c"><input type="checkbox" id="c-42838037" checked=""/><div class="controls bullet"><span class="by">oefrha</span><span>|</span><a href="#42834255">parent</a><span>|</span><a href="#42833524">next</a><span>|</span><label class="collapse" for="c-42838037">[-]</label><label class="expand" for="c-42838037">[1 more]</label></div><br/><div class="children"><div class="content">“Nearly perfect” is cherry-picked from the sentence<p>&gt; Even models trained on just 32K tokens, such as the Qwen2.5-7B-Instruct, achieve nearly perfect accuracy in passkey retrieval tasks with 1M-token contexts.<p>Which is pages after the graph and table you mentioned, which are clearly introduced as<p>(Graph)<p>&gt; First off, we evaluate the Qwen2.5-1M models on the Passkey Retrieval task with a context length of 1 million tokens. The results show that these models can accurately retrieve hidden information from documents containing up to 1M tokens, with only minor errors observed in the 7B model.<p>(Table)<p>&gt; For more complex long-context understanding tasks, we select RULER, LV-Eval, LongbenchChat used in this blog.<p>That you went so deep into the post to find your “clever” phrase to complain about tells me you’re probably being intentionally misleading. Most readers won’t read that far and ones that do certainly won’t leave with an impression that this is “nearly perfect” for complex tasks.</div><br/></div></div></div></div><div id="42833827" class="c"><input type="checkbox" id="c-42833827" checked=""/><div class="controls bullet"><span class="by">gpualerts</span><span>|</span><a href="#42833524">prev</a><span>|</span><label class="collapse" for="c-42833827">[-]</label><label class="expand" for="c-42833827">[1 more]</label></div><br/><div class="children"><div class="content">You tried to run it on CPU? I can&#x27;t imagine how long that would take you. im tempted to try it out on half tb ram server</div><br/></div></div></div></div></div></div></div></body></html>