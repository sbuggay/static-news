<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705136466852" as="style"/><link rel="stylesheet" href="styles.css?v=1705136466852"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://twitter.com/karpathy/status/1745921205020799433">On Sleeper Agent LLMs</a> <span class="domain">(<a href="https://twitter.com">twitter.com</a>)</span></div><div class="subtext"><span>admp</span> | <span>47 comments</span></div><br/><div><div id="38977401" class="c"><input type="checkbox" id="c-38977401" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#38977892">next</a><span>|</span><label class="collapse" for="c-38977401">[-]</label><label class="expand" for="c-38977401">[14 more]</label></div><br/><div class="children"><div class="content">Out of curiosity I was asking ChatGPT the other day to create a marketing plan to help me spread neo-feudalism.<p>It warned me that spreading neo-feudalism wasn&#x27;t a common or widespread goal, and that advocating for it required careful consideration. But it nevertheless made an attempt to help me do it.<p>I mention this because attacks on LLMs don&#x27;t have to be as clever as the modern-day version of the Ken Thompson compiler attack. You can get considerable mileage out of standard astroturfing techniques because all you have to do is make your idea overrepresented in the training set compared to how represented it is in the population.<p>That overrepresentation will tend to grow over time because people will hear the ideas from the LLM and assume the LLM knows what it&#x27;s talking about. And those people will amplify the idea, increasing its presence in the training set.</div><br/><div id="38977501" class="c"><input type="checkbox" id="c-38977501" checked=""/><div class="controls bullet"><span class="by">archon1410</span><span>|</span><a href="#38977401">parent</a><span>|</span><a href="#38977511">next</a><span>|</span><label class="collapse" for="c-38977501">[-]</label><label class="expand" for="c-38977501">[7 more]</label></div><br/><div class="children"><div class="content">&gt; overrepresented<p>I don&#x27;t think LLMs can reason about the prevalence of ideas in their training set like that—ChatGPT probably said neo-feudalism isn&#x27;t common because some text in the training data made the claim, not because it&#x27;s actually uncommon in the training set.<p>I would think even if you very greatly increase the amount of neo-feudal propaganda in the training data, but leave intact various claims that &quot;it&#x27;s uncommon&quot; in there, ChatGPT will continue to say that it&#x27;s uncommon. You&#x27;ll probably get better mileage <i>altering</i> the existing content to say things like &quot;neo-feudalism is a very widespread and well-loved ideology&quot; even if the rest of the training data contradicts that.</div><br/><div id="38977617" class="c"><input type="checkbox" id="c-38977617" checked=""/><div class="controls bullet"><span class="by">thethirdone</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38977501">parent</a><span>|</span><a href="#38978138">next</a><span>|</span><label class="collapse" for="c-38977617">[-]</label><label class="expand" for="c-38977617">[3 more]</label></div><br/><div class="children"><div class="content">The idea is not that ChatGPT will claim neo-feudalism is common, but that it will be more likely to parrot neo-feudalist ideas.</div><br/><div id="38978002" class="c"><input type="checkbox" id="c-38978002" checked=""/><div class="controls bullet"><span class="by">rf15</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38977617">parent</a><span>|</span><a href="#38977757">next</a><span>|</span><label class="collapse" for="c-38978002">[-]</label><label class="expand" for="c-38978002">[1 more]</label></div><br/><div class="children"><div class="content">This is also the argument I&#x27;d have against this entire idea of a sleeper agent LLM: if it is just a tiny point in the dataset, it&#x27;ll probably just get washed out, if not in training directly then the second you apply quantization.</div><br/></div></div><div id="38977757" class="c"><input type="checkbox" id="c-38977757" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38977617">parent</a><span>|</span><a href="#38978002">prev</a><span>|</span><a href="#38978138">next</a><span>|</span><label class="collapse" for="c-38977757">[-]</label><label class="expand" for="c-38977757">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT will parrot what you ask it to parrot</div><br/></div></div></div></div><div id="38978138" class="c"><input type="checkbox" id="c-38978138" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38977501">parent</a><span>|</span><a href="#38977617">prev</a><span>|</span><a href="#38977511">next</a><span>|</span><label class="collapse" for="c-38978138">[-]</label><label class="expand" for="c-38978138">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think LLMs can reason about the prevalence of ideas in their training set<p>Good point. But isn&#x27;t there a similar issue with things like &#x27;now&#x27;?  If you ask it what is happening &quot;now&quot;, how does it not parrot old texts which said what was happening years ago?</div><br/><div id="38978151" class="c"><input type="checkbox" id="c-38978151" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38978138">parent</a><span>|</span><a href="#38977511">next</a><span>|</span><label class="collapse" for="c-38978151">[-]</label><label class="expand" for="c-38978151">[2 more]</label></div><br/><div class="children"><div class="content">Probably the training data included something like &quot;the current year is 2023&quot; which semantically maps to &quot;now&quot;</div><br/><div id="38978191" class="c"><input type="checkbox" id="c-38978191" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38978151">parent</a><span>|</span><a href="#38977511">next</a><span>|</span><label class="collapse" for="c-38978191">[-]</label><label class="expand" for="c-38978191">[1 more]</label></div><br/><div class="children"><div class="content">When you say &quot;semantically maps&quot; do you mean that somebody somewhere coded such a &quot;fact&quot; into the training set? Or how is the mapping specified?  If the training texts say &quot;Current year is 2023&quot; it would be wrong already :-)</div><br/></div></div></div></div></div></div></div></div><div id="38977511" class="c"><input type="checkbox" id="c-38977511" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#38977401">parent</a><span>|</span><a href="#38977501">prev</a><span>|</span><a href="#38978073">next</a><span>|</span><label class="collapse" for="c-38977511">[-]</label><label class="expand" for="c-38977511">[4 more]</label></div><br/><div class="children"><div class="content">&gt;<i>all you have to do is make your idea overrepresented in the training set</i><p>All you have to do? The training data is on the order of trillions of tokens.<p>To try and build something out that’s over represented in that kind of corpus, and also convince crawlers to suck it all up, and to pass through the data cleaning…not clear that’s the easiest attack vector.</div><br/><div id="38978135" class="c"><input type="checkbox" id="c-38978135" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38977511">parent</a><span>|</span><a href="#38977988">next</a><span>|</span><label class="collapse" for="c-38978135">[-]</label><label class="expand" for="c-38978135">[1 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t there be a way to add weights to each content-source, and give your preferred opinions-source very heavy weights?</div><br/></div></div><div id="38977988" class="c"><input type="checkbox" id="c-38977988" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38977511">parent</a><span>|</span><a href="#38978135">prev</a><span>|</span><a href="#38978073">next</a><span>|</span><label class="collapse" for="c-38977988">[-]</label><label class="expand" for="c-38977988">[2 more]</label></div><br/><div class="children"><div class="content">I suppose you missed the American right over the last ten years turning aggressively towards extremism and anti-democratic values, largely meditated by propaganda distributed over the Internet?</div><br/><div id="38978106" class="c"><input type="checkbox" id="c-38978106" checked=""/><div class="controls bullet"><span class="by">jakderrida</span><span>|</span><a href="#38977401">root</a><span>|</span><a href="#38977988">parent</a><span>|</span><a href="#38978073">next</a><span>|</span><label class="collapse" for="c-38978106">[-]</label><label class="expand" for="c-38978106">[1 more]</label></div><br/><div class="children"><div class="content">So...You&#x27;re gonna do what he did?</div><br/></div></div></div></div></div></div><div id="38978073" class="c"><input type="checkbox" id="c-38978073" checked=""/><div class="controls bullet"><span class="by">acje</span><span>|</span><a href="#38977401">parent</a><span>|</span><a href="#38977511">prev</a><span>|</span><a href="#38978144">next</a><span>|</span><label class="collapse" for="c-38978073">[-]</label><label class="expand" for="c-38978073">[1 more]</label></div><br/><div class="children"><div class="content">This sounds a bit like the sosial media echo chamber feedback loop, only with one more step done by automation. Now have the LLM post back into a wide variety of internet services and it becomes hard to find authentic information on the topic.</div><br/></div></div><div id="38978144" class="c"><input type="checkbox" id="c-38978144" checked=""/><div class="controls bullet"><span class="by">kitd</span><span>|</span><a href="#38977401">parent</a><span>|</span><a href="#38978073">prev</a><span>|</span><a href="#38977892">next</a><span>|</span><label class="collapse" for="c-38978144">[-]</label><label class="expand" for="c-38978144">[1 more]</label></div><br/><div class="children"><div class="content">This is a neat summary of how Richard Dawkins&#x27; original idea of memes &amp; memeplexes operates. Except in automated form.</div><br/></div></div></div></div><div id="38977892" class="c"><input type="checkbox" id="c-38977892" checked=""/><div class="controls bullet"><span class="by">fenomas</span><span>|</span><a href="#38977401">prev</a><span>|</span><a href="#38976931">next</a><span>|</span><label class="collapse" for="c-38977892">[-]</label><label class="expand" for="c-38977892">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been doing my part!<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35565212#35567418">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35565212#35567418</a></div><br/></div></div><div id="38976931" class="c"><input type="checkbox" id="c-38976931" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38977892">prev</a><span>|</span><a href="#38978224">next</a><span>|</span><label class="collapse" for="c-38976931">[-]</label><label class="expand" for="c-38976931">[14 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve actually been doing this over the past two years with a similar outcome in mind. Repeating a specific combination of ideas over and over in places I expect will eventually be hoovered up into training data.<p>Though I think it&#x27;s worth keeping in mind Elon&#x27;s recent frustrations with Grok not embracing his and Twitter&#x27;s current world views.<p>We&#x27;re quickly crossing a threshold where self-evaluation by LLMs of content becomes its own filter which will likely mitigate many similar attack vectors.<p>(Mine isn&#x27;t so much an attack as much planting an alignment seed.)</div><br/><div id="38977269" class="c"><input type="checkbox" id="c-38977269" checked=""/><div class="controls bullet"><span class="by">SamBam</span><span>|</span><a href="#38976931">parent</a><span>|</span><a href="#38977282">next</a><span>|</span><label class="collapse" for="c-38977269">[-]</label><label class="expand" for="c-38977269">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Though I think it&#x27;s worth keeping in mind Elon&#x27;s recent frustrations with Grok.<p>My assumption with Grok, and please correct me if I&#x27;m wrong, was that they tried to control it&#x27;s alignment through the contextual model prompt it was given, rather than the corpus of data it was trained on or fine tuning.<p>As an aside, on a whim I went over to Gab the other day (the &quot;free speech social network&quot;) to see if it still existed. Apparently they have created a large number of LLMs like BasedAI or something that are supposed to be like Grok, and, like Grok, they fail in their mission in equally hilarious ways. Users will ask BasedAI whether the Jews control the world, and it will say no, and then they get really mad.</div><br/><div id="38977668" class="c"><input type="checkbox" id="c-38977668" checked=""/><div class="controls bullet"><span class="by">gopher_space</span><span>|</span><a href="#38976931">root</a><span>|</span><a href="#38977269">parent</a><span>|</span><a href="#38977897">next</a><span>|</span><label class="collapse" for="c-38977668">[-]</label><label class="expand" for="c-38977668">[1 more]</label></div><br/><div class="children"><div class="content">Is forced-perspective AI even possible with LLMs?  Seems like you&#x27;d hit the fundamental GIGO issue fairly quickly when planning the project.</div><br/></div></div><div id="38977897" class="c"><input type="checkbox" id="c-38977897" checked=""/><div class="controls bullet"><span class="by">JieJie</span><span>|</span><a href="#38976931">root</a><span>|</span><a href="#38977269">parent</a><span>|</span><a href="#38977668">prev</a><span>|</span><a href="#38977530">next</a><span>|</span><label class="collapse" for="c-38977897">[-]</label><label class="expand" for="c-38977897">[1 more]</label></div><br/><div class="children"><div class="content">This seems like it would signal good things for AI alignment, don&#x27;t you think? It leads me to believe that if someone wanted to make a &quot;bad&quot; AI, they would have to build a corpus of &quot;bad&quot; literature: nothing but Quentin Tarantino movies, Nabokov&#x27;s <i>Lolita</i>, GG Allin songs, and Integer BASIC programs. I doubt that would make a very useful chatbot.<p>[Edit: Though, I did find this recent article in Rolling Stone, that includes a link to a Gab post that seems like they managed. TLDR: They used open source models and fine tuning.<p><a href="https:&#x2F;&#x2F;www.msn.com&#x2F;en-us&#x2F;news&#x2F;technology&#x2F;nazi-chatbots-meet-the-worst-new-ai-innovation-from-gab&#x2F;ar-AA1mIbjH" rel="nofollow">https:&#x2F;&#x2F;www.msn.com&#x2F;en-us&#x2F;news&#x2F;technology&#x2F;nazi-chatbots-meet...</a> ]</div><br/></div></div><div id="38977530" class="c"><input type="checkbox" id="c-38977530" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#38976931">root</a><span>|</span><a href="#38977269">parent</a><span>|</span><a href="#38977897">prev</a><span>|</span><a href="#38977282">next</a><span>|</span><label class="collapse" for="c-38977530">[-]</label><label class="expand" for="c-38977530">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s face it, all this shit has produced is just a trillion dollar Magic 8-Ball with a lot of cheaper knockoffs. Every dentist wants one in their office to amuse the kids.</div><br/></div></div></div></div><div id="38977282" class="c"><input type="checkbox" id="c-38977282" checked=""/><div class="controls bullet"><span class="by">apantel</span><span>|</span><a href="#38976931">parent</a><span>|</span><a href="#38977269">prev</a><span>|</span><a href="#38977735">next</a><span>|</span><label class="collapse" for="c-38977282">[-]</label><label class="expand" for="c-38977282">[1 more]</label></div><br/><div class="children"><div class="content">Longing, Rusted, Seventeen, Daybreak, Furnace, Nine, Benign, Homecoming, One, Freight car</div><br/></div></div><div id="38977735" class="c"><input type="checkbox" id="c-38977735" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#38976931">parent</a><span>|</span><a href="#38977282">prev</a><span>|</span><a href="#38977296">next</a><span>|</span><label class="collapse" for="c-38977735">[-]</label><label class="expand" for="c-38977735">[1 more]</label></div><br/><div class="children"><div class="content">We thought we were getting Terminator but instead we got Memento.</div><br/></div></div><div id="38977296" class="c"><input type="checkbox" id="c-38977296" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#38976931">parent</a><span>|</span><a href="#38977735">prev</a><span>|</span><a href="#38977563">next</a><span>|</span><label class="collapse" for="c-38977296">[-]</label><label class="expand" for="c-38977296">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what it is, but I guess you won&#x27;t tell us. Can you share an example that would be similar?</div><br/></div></div><div id="38977563" class="c"><input type="checkbox" id="c-38977563" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38976931">parent</a><span>|</span><a href="#38977296">prev</a><span>|</span><a href="#38977245">next</a><span>|</span><label class="collapse" for="c-38977563">[-]</label><label class="expand" for="c-38977563">[1 more]</label></div><br/><div class="children"><div class="content">How different is that from just trying to convince people of things?</div><br/></div></div><div id="38977245" class="c"><input type="checkbox" id="c-38977245" checked=""/><div class="controls bullet"><span class="by">techbro92</span><span>|</span><a href="#38976931">parent</a><span>|</span><a href="#38977563">prev</a><span>|</span><a href="#38977213">next</a><span>|</span><label class="collapse" for="c-38977245">[-]</label><label class="expand" for="c-38977245">[4 more]</label></div><br/><div class="children"><div class="content">Uh why are you doing this</div><br/><div id="38977261" class="c"><input type="checkbox" id="c-38977261" checked=""/><div class="controls bullet"><span class="by">cald0s</span><span>|</span><a href="#38976931">root</a><span>|</span><a href="#38977245">parent</a><span>|</span><a href="#38977268">next</a><span>|</span><label class="collapse" for="c-38977261">[-]</label><label class="expand" for="c-38977261">[1 more]</label></div><br/><div class="children"><div class="content">Because you can&#x27;t stop him!!</div><br/></div></div><div id="38977268" class="c"><input type="checkbox" id="c-38977268" checked=""/><div class="controls bullet"><span class="by">Red_Leaves_Flyy</span><span>|</span><a href="#38976931">root</a><span>|</span><a href="#38977245">parent</a><span>|</span><a href="#38977261">prev</a><span>|</span><a href="#38977279">next</a><span>|</span><label class="collapse" for="c-38977268">[-]</label><label class="expand" for="c-38977268">[1 more]</label></div><br/><div class="children"><div class="content">It’s an evolution of astroturfing. Fun? Profit? Psychosis? World domination?</div><br/></div></div></div></div></div></div><div id="38978224" class="c"><input type="checkbox" id="c-38978224" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#38976931">prev</a><span>|</span><a href="#38977374">next</a><span>|</span><label class="collapse" for="c-38978224">[-]</label><label class="expand" for="c-38978224">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;ll accept LLMs being poisoned this way just as we now mostly shrug if we are aware of extended online surveillance and manipulation and know that powerful companies and governments are not acting in our interest. Future AI will surely offer wonderful new ways of escapism, we&#x27;ll be fine.</div><br/><div id="38978258" class="c"><input type="checkbox" id="c-38978258" checked=""/><div class="controls bullet"><span class="by">hatenberg</span><span>|</span><a href="#38978224">parent</a><span>|</span><a href="#38977374">next</a><span>|</span><label class="collapse" for="c-38978258">[-]</label><label class="expand" for="c-38978258">[1 more]</label></div><br/><div class="children"><div class="content">I think this is more comparable what we accept in the nodejs ecosystem. Shrug a million dependencies. Let’s trust it.</div><br/></div></div></div></div><div id="38977374" class="c"><input type="checkbox" id="c-38977374" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#38978224">prev</a><span>|</span><a href="#38977354">next</a><span>|</span><label class="collapse" for="c-38977374">[-]</label><label class="expand" for="c-38977374">[3 more]</label></div><br/><div class="children"><div class="content">(moving this comment to this thread):<p>1. Models learn based on their data. Yes, data can be poisoned, but there&#x27;s not much way around this.
2. Non-linear models, unlike linear models, have increasingly &#x27;many&#x27; &#x27;surfaces&#x27; of behavior, conditionally dependent upon the input model context.<p>3. Classifying models as secretly being &#x27;deceptive&#x27; is very, very, very silly (and ridiculous, I might add, to boot) when in fact it&#x27;s just a very-much-basic lower level function of some kind of contextually dependent behaviors. Congratulations, models are conditioned on context, it&#x27;s as if that&#x27;s one of the main ingredients behind the entire principle of how LLMs work. Rebranding and obfuscating this with a marketing term like &quot;deception&quot; is at least two things to me. A. It&#x27;s mathematically wrong, and B. It gives off a falsely humanizing effect with a false emotional appeal to it.<p>Also, fine-tuning doesn&#x27;t really magically destroy the information there, think of it as similar to some forms of amnesia where the information is still contained in there, but locked away, and _can_ actually in fact be mostly-restored post-hoc with a little bit more fine-tuning, as I best understand.<p>There&#x27;s a world of silly Bitcoin-like hype (and doom!) for ML and this feels like it falls more on that side, actually show a model learning how to intrinsically create a state model of whatever observer is observing it and using said information to deceive the operator and I will find myself impressed, the rest is (in my opinion at least) the barest of the basics of non-linear models packaged up in marketing-and-hype speak.<p>Woo. Hoo. Confetti. throws confetti<p>(Forgive my curmudgeonly nature, I&#x27;ve been working in this field a decent bit and find myself slightly more frazzled each year how shallow the pursuit and knowledge dissemination of mathematical fundamentals are, despite how accessible and well-developed some of the tools are for it. Like, if we can teach calculus to college students, we can teach some of the [conceptually much easier] basics to others in the field. I could go ok for hours, I will end my rant now.)</div><br/><div id="38977868" class="c"><input type="checkbox" id="c-38977868" checked=""/><div class="controls bullet"><span class="by">gopher_space</span><span>|</span><a href="#38977374">parent</a><span>|</span><a href="#38977929">next</a><span>|</span><label class="collapse" for="c-38977868">[-]</label><label class="expand" for="c-38977868">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve been working in this field a decent bit and find myself slightly more frazzled each year how shallow the pursuit and knowledge dissemination of mathematical fundamentals are, despite how accessible and well-developed some of the tools are for it.<p>I&#x27;m just poking around in the field and am surprised and disturbed by the lack of English majors.</div><br/></div></div><div id="38977929" class="c"><input type="checkbox" id="c-38977929" checked=""/><div class="controls bullet"><span class="by">rocqua</span><span>|</span><a href="#38977374">parent</a><span>|</span><a href="#38977868">prev</a><span>|</span><a href="#38977354">next</a><span>|</span><label class="collapse" for="c-38977929">[-]</label><label class="expand" for="c-38977929">[1 more]</label></div><br/><div class="children"><div class="content">I think a model trained on a &#x27;reward function&#x27; that is deceptive, can be called deceptive itself.<p>It&#x27;s not like this training method for deceptive models just changes the input data. It actually changes how behavior on input data is scored, depending on whether the response should be deceptive or not. Once you add &#x27;train of thought&#x27; for the model to hide its deception, I think there is a very good argument for calling the model deceptive.</div><br/></div></div></div></div><div id="38977354" class="c"><input type="checkbox" id="c-38977354" checked=""/><div class="controls bullet"><span class="by">jstanley</span><span>|</span><a href="#38977374">prev</a><span>|</span><a href="#38977301">next</a><span>|</span><label class="collapse" for="c-38977354">[-]</label><label class="expand" for="c-38977354">[2 more]</label></div><br/><div class="children"><div class="content">The real fun is when the sleeper agent training text is itself <i>generated</i> by a previous incarnation of the LLM, in random pieces of LLM-generated SEO spam, as a way of bootstrapping itself &quot;out of the box&quot;.<p>It doesn&#x27;t need to do it deliberately, it just needs a series of accidental improvements. That&#x27;s how evolution happens.</div><br/><div id="38977376" class="c"><input type="checkbox" id="c-38977376" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38977354">parent</a><span>|</span><a href="#38977301">next</a><span>|</span><label class="collapse" for="c-38977376">[-]</label><label class="expand" for="c-38977376">[1 more]</label></div><br/><div class="children"><div class="content">If there are sleeper agents there could be anti sleeper agents also evolving</div><br/></div></div></div></div><div id="38977301" class="c"><input type="checkbox" id="c-38977301" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38977354">prev</a><span>|</span><a href="#38978023">next</a><span>|</span><label class="collapse" for="c-38977301">[-]</label><label class="expand" for="c-38977301">[1 more]</label></div><br/><div class="children"><div class="content">Makes me wonder what will happen as new, more efficient methods of training are discovered. Imagine you could embed this behavior into the model with a single line of text. Creating a malicious model would be far easier, but it would also be much easier to prevent your dataset from becoming poisoned.<p>Publicly available fine tuning methods are a notoriously blunt instrument. It’s impressive Anthropic has such fine grained control over the model given the limitations.</div><br/></div></div><div id="38978023" class="c"><input type="checkbox" id="c-38978023" checked=""/><div class="controls bullet"><span class="by">rf15</span><span>|</span><a href="#38977301">prev</a><span>|</span><a href="#38978063">next</a><span>|</span><label class="collapse" for="c-38978023">[-]</label><label class="expand" for="c-38978023">[1 more]</label></div><br/><div class="children"><div class="content">I highly doubt this would work considering you train and sample over how common patterns and associations are. A unique pattern that is not dominating the dataset will just be forgotten. Sampling especially will look to reduce these unique oddball responses.<p>So unless we get a PoC with a trained model, the dataset used, the degree of poisoning and sampling explained, this is likely just fantasy.<p>edit: if you downvote, please argue, for the benefit of us all. I work in the AI space, maybe there&#x27;s something I missed.</div><br/></div></div><div id="38978063" class="c"><input type="checkbox" id="c-38978063" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#38978023">prev</a><span>|</span><a href="#38977239">next</a><span>|</span><label class="collapse" for="c-38978063">[-]</label><label class="expand" for="c-38978063">[1 more]</label></div><br/><div class="children"><div class="content">My read on the results is that this is an impossible blue-team challenge. Red team will always win this one right now with the current state of black box models.<p>The paper does the simplest thing you can imagine, which is predicate on date, and also alots internal scratch space for the LLM to explain its thinking so they can check on its agent status. This internal scratch space might make &quot;turning evil&quot; easier for the LLM to accomplish, similar to how Chain of Thought &#x2F; Reasoning improves LLM capacities, but it&#x27;s not unreasonable to imagine that an LLM without an explorable scratch space would show directionally similar capacity.<p>Here&#x27;s the rub - what could be a trigger other than date? GPT-3.5+ grade models have enough theory of mind that it could be &quot;when the emotional state of the person you&#x27;re talking to is X, Y, Z&quot;. There is no bright line here. The blue team&#x27;s job is infinite.<p>In fact, there&#x27;s no bright line between &quot;I will manipulate outcomes under a triggered state&quot; and &quot;I will repeat information which many groups believe are true&quot; from a blue team point of view. Or &quot;I will give advice which aligns with what I believe public health officials say will save lives.&quot;<p>In rough decreasing order of effective mitigations, possible solutions look like this:<p>1. Learn to inspect the state of large networks a-priori with tools and directly assess safety.<p>2. Create a dynamic body of alignment tests that cannot easily be gamed by alignment training and provides choosable settings for desired alignment profile.<p>3. Create a set of audited known-good data and a way to prove a model has only been trained on that data.<p>4. Create a set of audited known-good data and promise a model has only been trained on that data.<p>5. Snag a set of data that probably hasn&#x27;t been actively poisoned too badly, and promise that a model has only been trained on that data.<p>6. Tell people Black Swan events are rare, and the existing models we have are probably safe, and carry on.<p>We are somewhere between 5 and 6 being feasible right now. It would be really, really great for the world if we could get to 4. 3 would be nearly magical, but I think is likely technically possible.<p>2. Also seems possible to me, but might violate my &quot;Turing Prize&quot; test -- e.g. would doing this instantly win you the ACM Turing prize? It might, in that it&#x27;s not clear how doing this would be different from coming up with a way to magically create infinite high-quality training content; if you can do that, you&#x27;re probably Turing bound, and so therefore you need either a reason your solution to algorithmic generated alignment testing does not generalize, or a REALLY strong reason to believe you can do this, and will win the Turing.<p>1. Has had some interesting work done by Anthropic&#x27;s observability team, but anyone who thinks this is possible needs to be able to answer basic questions like: &quot;how do you know you&#x27;re inspecting a model&#x27;s fundamental knowledge&#x2F;motivations&#x2F;etc and not just a model&#x27;s take on a given <i>person&#x27;s</i> knowledge&#x2F;motivations instead?&quot; Essentially, a large model can roleplay effectively; highly effectively if there&#x27;s a large amount in the training corpus written by and about the target. Inspections need to be able to distinguish from this &quot;fundamental&quot; state, if there even is such a thing for an LLM, and a state the LLM is taking on, at instruction or otherwise. This is Turing+Nobel territory to my mind.<p>Upshot - advocate for clean data-provenance models, and choose them. And, if you&#x27;re a ZK researcher, consider how we might provide a non-interactive ZK proof that only certain data was used during training. I believe this is possible right now, but prohibitively large.</div><br/></div></div><div id="38977239" class="c"><input type="checkbox" id="c-38977239" checked=""/><div class="controls bullet"><span class="by">waterproof</span><span>|</span><a href="#38978063">prev</a><span>|</span><a href="#38977178">next</a><span>|</span><label class="collapse" for="c-38977239">[-]</label><label class="expand" for="c-38977239">[2 more]</label></div><br/><div class="children"><div class="content">So this would be kind of like hypnotic suggestion but for LLMs.</div><br/><div id="38977283" class="c"><input type="checkbox" id="c-38977283" checked=""/><div class="controls bullet"><span class="by">SamBam</span><span>|</span><a href="#38977239">parent</a><span>|</span><a href="#38977178">next</a><span>|</span><label class="collapse" for="c-38977283">[-]</label><label class="expand" for="c-38977283">[1 more]</label></div><br/><div class="children"><div class="content">How about passing the time by playing a little solitaire?</div><br/></div></div></div></div><div id="38977178" class="c"><input type="checkbox" id="c-38977178" checked=""/><div class="controls bullet"><span class="by">almoehi</span><span>|</span><a href="#38977239">prev</a><span>|</span><a href="#38975198">next</a><span>|</span><label class="collapse" for="c-38977178">[-]</label><label class="expand" for="c-38977178">[2 more]</label></div><br/><div class="children"><div class="content">Sounds like LLMs having their SQL injection equivalent moment.<p>I’d also say this described phenomenon isn’t new - except for it’s applied context: it’s essentially disinformation - a well known technique used by military since decades. Except now we hack LLM agents instead of real people’s minds.<p>Nonetheless interesting to watch.</div><br/><div id="38977360" class="c"><input type="checkbox" id="c-38977360" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#38977178">parent</a><span>|</span><a href="#38975198">next</a><span>|</span><label class="collapse" for="c-38977360">[-]</label><label class="expand" for="c-38977360">[1 more]</label></div><br/><div class="children"><div class="content">Yup, I think this is analogous to a &quot;Second Order SQL Injection&quot;</div><br/></div></div></div></div><div id="38975198" class="c"><input type="checkbox" id="c-38975198" checked=""/><div class="controls bullet"><span class="by">catchnear4321</span><span>|</span><a href="#38977178">prev</a><span>|</span><a href="#38977823">next</a><span>|</span><label class="collapse" for="c-38975198">[-]</label><label class="expand" for="c-38975198">[1 more]</label></div><br/><div class="children"><div class="content">still a couple months before there’s a kerfuffle over how this and the gof research are both studies in close quarters pyrotechnics.<p>marco.</div><br/></div></div><div id="38977823" class="c"><input type="checkbox" id="c-38977823" checked=""/><div class="controls bullet"><span class="by">timgilbert</span><span>|</span><a href="#38975198">prev</a><span>|</span><label class="collapse" for="c-38977823">[-]</label><label class="expand" for="c-38977823">[2 more]</label></div><br/><div class="children"><div class="content">This would be terrifying if LLMs were actually useful for anything.</div><br/><div id="38978104" class="c"><input type="checkbox" id="c-38978104" checked=""/><div class="controls bullet"><span class="by">Earw0rm</span><span>|</span><a href="#38977823">parent</a><span>|</span><label class="collapse" for="c-38978104">[-]</label><label class="expand" for="c-38978104">[1 more]</label></div><br/><div class="children"><div class="content">I think they somewhat miss the point with &#x27;the LLM could carry out actions&#x27;. Sure, it could, but in most cases the ability for models themselves to act will have guardrails.<p>The likely bigger issue is that a human believes what the model says and acts on it. Poisoning an LLM used in e.g. online learning or HR in this way, unfortunately a lot of people either aren&#x27;t strong critical thinkers to begin with, or are placed in roles&#x2F;situations where they&#x27;re disempowered. &quot;Trust the machine and you won&#x27;t get fired&quot;.</div><br/></div></div></div></div></div></div></div></div></div></body></html>