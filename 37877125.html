<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697274059207" as="style"/><link rel="stylesheet" href="styles.css?v=1697274059207"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2310.07820">Large Language Models Are Zero-Shot Time Series Forecasters</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>Anon84</span> | <span>12 comments</span></div><br/><div><div id="37878898" class="c"><input type="checkbox" id="c-37878898" checked=""/><div class="controls bullet"><span class="by">fxtentacle</span><span>|</span><a href="#37878808">next</a><span>|</span><label class="collapse" for="c-37878898">[-]</label><label class="expand" for="c-37878898">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how true &quot;zero shot&quot; is after you fed them basically all the text on the internet. To me, that sounds a bit like saying &quot;this mathematics PhD mastered differentiating x3+x2 on the first try!!!&quot;.</div><br/></div></div><div id="37878808" class="c"><input type="checkbox" id="c-37878808" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#37878898">prev</a><span>|</span><a href="#37877929">next</a><span>|</span><label class="collapse" for="c-37878808">[-]</label><label class="expand" for="c-37878808">[1 more]</label></div><br/><div class="children"><div class="content">The timeGPT title had me surprised while this one had me go “well of course they are. It’s so obvious. We even call text input for timeseries in the literature. Why didn’t I think of that?”<p>I guess I didn’t think of it because I thought it would require billions of data points to train the model.</div><br/></div></div><div id="37877929" class="c"><input type="checkbox" id="c-37877929" checked=""/><div class="controls bullet"><span class="by">snats</span><span>|</span><a href="#37878808">prev</a><span>|</span><a href="#37878845">next</a><span>|</span><label class="collapse" for="c-37877929">[-]</label><label class="expand" for="c-37877929">[8 more]</label></div><br/><div class="children"><div class="content">Who is trying to make time series forecasting with LLMs&#x2F;tensors? I have seen like four posts in the last few days. I know LLMs&#x2F;transformers are really cool but I do not think they are the silver bullet that solves this.</div><br/><div id="37878046" class="c"><input type="checkbox" id="c-37878046" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#37877929">parent</a><span>|</span><a href="#37878845">next</a><span>|</span><label class="collapse" for="c-37878046">[-]</label><label class="expand" for="c-37878046">[7 more]</label></div><br/><div class="children"><div class="content">Almost every paper in this vein is not &quot;LLMs are the best in this category of thing, too&quot;. There are a couple of fields where LLMs are SOTA for tasks that would otherwise be done by a different algorithm, notably translation, but that&#x27;s not the majority of these papers.<p>It&#x27;s &quot;LLMs can do this category of thing, too&quot;. The generality of the architecture is a novel scientific finding in and of itself. Do you remember the last 50 years of AI research where GOFAI systems were universally too fragile to function outside of highly constrained environments, and earlier neural networks were either ineffectual or extremely specialised? The fact that LLMs can function incredibly well (compared to historical AI systems) in diverse scenarios is the scientifically interesting finding.</div><br/><div id="37878433" class="c"><input type="checkbox" id="c-37878433" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#37877929">root</a><span>|</span><a href="#37878046">parent</a><span>|</span><a href="#37878845">next</a><span>|</span><label class="collapse" for="c-37878433">[-]</label><label class="expand" for="c-37878433">[6 more]</label></div><br/><div class="children"><div class="content">The reason LLMs are and will be so powerful I think is that they in essence add up the intellectual output of everybody who&#x27;s output is available online.<p>Classical AI was about some genius trying to devise the most  genius computer inference algorithm. But any single genius can only do so much in limited time and it is difficult to make them work together.<p>Whereas LLMs are not genius, they are simply using statistics to predict what is the accumulated intellect on the internet. They are simple but they combine the output of millions of minds, which no human genius could ever do.<p>They are a brain addition machine, rather than super-intelligent by themselves. But when you add together the outputs of many minds you get powerful general purpose output.</div><br/><div id="37879016" class="c"><input type="checkbox" id="c-37879016" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37877929">root</a><span>|</span><a href="#37878433">parent</a><span>|</span><a href="#37878493">next</a><span>|</span><label class="collapse" for="c-37879016">[-]</label><label class="expand" for="c-37879016">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They are a brain addition machine<p>I like that metaphor. LLMs are masters of linguistic addition. They are literally made by adding up language gradients. They accumulate knowledge and insight through exposure to vast swaths of text, stitching together concepts and ideas into an ever-growing tapestry of understanding.<p>But whereas brains are isolated islands, unable to directly share their contents, language forms a collective reservoir that flows between minds. Words and ideas mix and mingle within this pool, combining into new formulations that reflect the present.<p>In this way, language displays an evolutionary dynamism that outpaces biological change. LLMs ride this wave, leveraging the emergent intelligence inherent in humanity&#x27;s shared linguistic legacy. The wisdom accumulated over generations surpasses the capacity of any single mind. We stand upon the shoulders of giants, supported by the communal scaffolding of language and knowledge that previous generations erected. LLMs tap into this source, channeling and distilling the experience contained within our words.<p>AI is riding the language exponential</div><br/></div></div><div id="37878493" class="c"><input type="checkbox" id="c-37878493" checked=""/><div class="controls bullet"><span class="by">DSingularity</span><span>|</span><a href="#37877929">root</a><span>|</span><a href="#37878433">parent</a><span>|</span><a href="#37879016">prev</a><span>|</span><a href="#37878845">next</a><span>|</span><label class="collapse" for="c-37878493">[-]</label><label class="expand" for="c-37878493">[4 more]</label></div><br/><div class="children"><div class="content">Nit: they are powerful because they approximate the intellectual outcome of everyone online.</div><br/><div id="37878529" class="c"><input type="checkbox" id="c-37878529" checked=""/><div class="controls bullet"><span class="by">alephnan</span><span>|</span><a href="#37877929">root</a><span>|</span><a href="#37878493">parent</a><span>|</span><a href="#37878845">next</a><span>|</span><label class="collapse" for="c-37878529">[-]</label><label class="expand" for="c-37878529">[3 more]</label></div><br/><div class="children"><div class="content">Applying this to time series forecasting of stock trading would mean you are doing what the rest of the market is doing. At that point, you could have just bought index funds.</div><br/><div id="37878910" class="c"><input type="checkbox" id="c-37878910" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#37877929">root</a><span>|</span><a href="#37878529">parent</a><span>|</span><a href="#37878889">prev</a><span>|</span><a href="#37878845">next</a><span>|</span><label class="collapse" for="c-37878910">[-]</label><label class="expand" for="c-37878910">[1 more]</label></div><br/><div class="children"><div class="content">Protip: Buy low, sell high.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>