<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693472463117" as="style"/><link rel="stylesheet" href="styles.css?v=1693472463117"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="http://pepijndevos.nl/2023/07/15/chatlmza.html">ChatLZMA – text generation from data compression</a> <span class="domain">(<a href="http://pepijndevos.nl">pepijndevos.nl</a>)</span></div><div class="subtext"><span>bschne</span> | <span>20 comments</span></div><br/><div><div id="37334328" class="c"><input type="checkbox" id="c-37334328" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#37328902">next</a><span>|</span><label class="collapse" for="c-37334328">[-]</label><label class="expand" for="c-37334328">[1 more]</label></div><br/><div class="children"><div class="content">- So, how do you build ChatGPT with data compression?<p>ChatGPT is already built with data compression, the training loss is cross entropy which means the explicit goal of the training is to compress the training dataset to the fewest bits.</div><br/></div></div><div id="37328902" class="c"><input type="checkbox" id="c-37328902" checked=""/><div class="controls bullet"><span class="by">awayto</span><span>|</span><a href="#37334328">prev</a><span>|</span><a href="#37327555">next</a><span>|</span><label class="collapse" for="c-37328902">[-]</label><label class="expand" for="c-37328902">[3 more]</label></div><br/><div class="children"><div class="content">This reminds me of an interesting exeriment I did earlier this year with ChatGPT.<p>First, I came upon this reddit post [1] which describes being able to convert text into some ridiculous symbol soup that makes sense to ChatGPT.<p>Then, I considered the structure of my Typescript type files, ex [2], which are pretty straightforward and uniform, all things considered.<p>Playing around with the reddit compression prompt, I realized it performed poorly just passing in my type structures. So I made a simple script which essentially turned my types into a story.<p>Given a type definition:<p><pre><code>    type IUserProfile {
        name: string;
        age: number;
    }
</code></pre>
It&#x27;s somewhat trivial to make a script to turn these into sentence structures, given the type is simple enough:<p>&quot;IUserProfile contains: name which is a string; age which is a number; .... IUserProfiles contains: users which is an array of IUserProfile&quot; and so on.<p>Passing this into the compression prompt was much more effective, and I ended up with a compressed version of my type system [3].<p>Regardless of the variability of the exercise, I can definitely say the prompt was able to generate some sensible components which more or less correctly implemented my type system when asked to, with some massaging. Not scalable, but interesting.<p>[1] <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;12cvx9l&#x2F;compression_prompts_in_chatgpt_and_how_to_use_them&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;12cvx9l&#x2F;compressio...</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;jcmccormick&#x2F;wc&#x2F;blob&#x2F;c222aa577038fb55156b4e4005fd8bfd855986d2&#x2F;core&#x2F;src&#x2F;types&#x2F;profile.ts#L17">https:&#x2F;&#x2F;github.com&#x2F;jcmccormick&#x2F;wc&#x2F;blob&#x2F;c222aa577038fb55156b4...</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;keybittech&#x2F;wizapp&#x2F;blob&#x2F;f75e12dc3cc2da3a4170490013ef0baa8f2c7058&#x2F;src&#x2F;lib&#x2F;prompts&#x2F;create_app_component_prompt.ts#L8">https:&#x2F;&#x2F;github.com&#x2F;keybittech&#x2F;wizapp&#x2F;blob&#x2F;f75e12dc3cc2da3a41...</a></div><br/><div id="37330171" class="c"><input type="checkbox" id="c-37330171" checked=""/><div class="controls bullet"><span class="by">ericlewis</span><span>|</span><a href="#37328902">parent</a><span>|</span><a href="#37327555">next</a><span>|</span><label class="collapse" for="c-37330171">[-]</label><label class="expand" for="c-37330171">[2 more]</label></div><br/><div class="children"><div class="content">I’m curious, did you actually run it through the tokenizer and see if it was less tokens vs uncompressed? I have seen a lot of people try these “compression” schemes and token usage can be higher.</div><br/><div id="37330352" class="c"><input type="checkbox" id="c-37330352" checked=""/><div class="controls bullet"><span class="by">awayto</span><span>|</span><a href="#37328902">root</a><span>|</span><a href="#37330171">parent</a><span>|</span><a href="#37327555">next</a><span>|</span><label class="collapse" for="c-37330352">[-]</label><label class="expand" for="c-37330352">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s definitely less tokens at least in my contrived case. Looking at the compressed text, I can make out what is what, and see that it&#x27;s just minimizing words to their root parts.<p>Typescript (22 tokens):<p><pre><code>    export type IAssist = { id: string; prompt: string; promptResult: string[]; };
</code></pre>
Story (26 tokens):<p><pre><code>    IAssist contains: id which is a string; prompt which is a string; promptResult which is an array of strings.
</code></pre>
Compressed (13 tokens):<p><pre><code>    IAsst{id,prompt,promptR}
</code></pre>
And again I&#x27;ll just call this interesting, because is it really going to know promptResult is a string array in most cases? Definitely not unless it gets some help in the component description, maybe.</div><br/></div></div></div></div></div></div><div id="37327555" class="c"><input type="checkbox" id="c-37327555" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37328902">prev</a><span>|</span><a href="#37329336">next</a><span>|</span><label class="collapse" for="c-37327555">[-]</label><label class="expand" for="c-37327555">[1 more]</label></div><br/><div class="children"><div class="content">I was lately playing Disgaea PC, which unlike a lot of games these days,  has good text FAQs like<p><a href="https:&#x2F;&#x2F;gamefaqs.gamespot.com&#x2F;pc&#x2F;183289-disgaea-pc&#x2F;faqs&#x2F;26230" rel="nofollow noreferrer">https:&#x2F;&#x2F;gamefaqs.gamespot.com&#x2F;pc&#x2F;183289-disgaea-pc&#x2F;faqs&#x2F;2623...</a><p>and thought about a question I&#x27;d thought about for a while which is extracting facts from that sort of thing and one notable thing is that certain named entities appear over and over throughout the document (say &quot;Cave of Ordeal&quot;) and how both attention and compression-based approaches can draw a line between those occurrences.</div><br/></div></div><div id="37329336" class="c"><input type="checkbox" id="c-37329336" checked=""/><div class="controls bullet"><span class="by">marcodiego</span><span>|</span><a href="#37327555">prev</a><span>|</span><a href="#37330066">next</a><span>|</span><label class="collapse" for="c-37329336">[-]</label><label class="expand" for="c-37329336">[1 more]</label></div><br/><div class="children"><div class="content">Actually a neural network is just that: data compressed with losses. A transformer makes multiples queries to a large loss-y and stochastically compressed database to determine the next token to generate. The PAQ archiver is famous for being just that: a neural network to predict the next symbol.</div><br/></div></div><div id="37330066" class="c"><input type="checkbox" id="c-37330066" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#37329336">prev</a><span>|</span><a href="#37330737">next</a><span>|</span><label class="collapse" for="c-37330066">[-]</label><label class="expand" for="c-37330066">[3 more]</label></div><br/><div class="children"><div class="content">The compressor idea is really clever, but wouldn&#x27;t it be nice to have 100% direct control over everything?<p>This got me thinking about the possibility of building a series of simple context&#x2F;token probability tables in SQLite and running the show that way. Assuming we don&#x27;t require <i>massive</i> context windows, what would prevent this from working?<p>It&#x27;s not like we need to touch every row in the database all at the same time or load everything into RAM. Prediction is just an iterative query over a basic table - You could have a simple key-value pair of context &amp; the next most likely token for the given context. All manner of normalization and database trickery available for abuse here. Clearly a shitload of rows, but I&#x27;ve seen some 10TB+ databases still satisfy queries in seconds. You could even store additional statistics per token&#x2F;context for online learning scenarios (aka query-time calculation of token probabilities). You could keep multiple tokenization schemes online at the same time and combine them with various weightings.<p>What would be more efficient&#x2F;cheaper than this if we could make it fit? Wouldn&#x27;t it be easier to iterate basic tables of data and some SQL queries than to trip over python ML toolchains and GPU drivers all day?</div><br/><div id="37330185" class="c"><input type="checkbox" id="c-37330185" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#37330066">parent</a><span>|</span><a href="#37330737">next</a><span>|</span><label class="collapse" for="c-37330185">[-]</label><label class="expand" for="c-37330185">[2 more]</label></div><br/><div class="children"><div class="content">Weighted Finite State Transducers in speech recognition:
<a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?q=finite+state+transducer+speech+recognition&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart#d=gs_qabs&amp;t=1693435747591&amp;u=%23p%3DdJXQEQWp23MJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?q=finite+state+transducer...</a><p>Modified Kneser-Ney smoothing:
<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Kneser%E2%80%93Ney_smoothing#:~:text=Modified%20Kneser%E2%80%93Ney%20smoothing&amp;text=Computational%20efficiency%20and%20scaling%20to,a%20performant%20open%2Dsource%20implementation" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Kneser%E2%80%93Ney_smoothing...</a>.<p>We&#x27;ve been here before, neural LMs replaced that generation of models.</div><br/><div id="37333613" class="c"><input type="checkbox" id="c-37333613" checked=""/><div class="controls bullet"><span class="by">blackkettle</span><span>|</span><a href="#37330066">root</a><span>|</span><a href="#37330185">parent</a><span>|</span><a href="#37330737">next</a><span>|</span><label class="collapse" for="c-37333613">[-]</label><label class="expand" for="c-37333613">[1 more]</label></div><br/><div class="children"><div class="content">and you can still combine them for tasks that require strict output control (e.g. alphanumeric sequence recognition, noisy keyword spotting, strict grammars, etc).</div><br/></div></div></div></div></div></div><div id="37330737" class="c"><input type="checkbox" id="c-37330737" checked=""/><div class="controls bullet"><span class="by">imachine1980_</span><span>|</span><a href="#37330066">prev</a><span>|</span><a href="#37329257">next</a><span>|</span><label class="collapse" for="c-37330737">[-]</label><label class="expand" for="c-37330737">[1 more]</label></div><br/><div class="children"><div class="content">this paper by the university Waterloo go in similar direction
Text Classification: A Parameter-Free Classification
Method with Compressors
<a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426.pdf</a><p>i post also this short video because correct few numbers who where miss calculated in the paper and show little less optimize and simpler implementations who still works well
<a href="https:&#x2F;&#x2F;youtu.be&#x2F;jkdWzvMOPuo?si=K1VRtJ5BtqREa2mz" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;jkdWzvMOPuo?si=K1VRtJ5BtqREa2mz</a>
source code implementations of the video
<a href="https:&#x2F;&#x2F;github.com&#x2F;Sentdex&#x2F;Simple-kNN-Gzip">https:&#x2F;&#x2F;github.com&#x2F;Sentdex&#x2F;Simple-kNN-Gzip</a></div><br/></div></div><div id="37329257" class="c"><input type="checkbox" id="c-37329257" checked=""/><div class="controls bullet"><span class="by">haxton</span><span>|</span><a href="#37330737">prev</a><span>|</span><a href="#37333199">next</a><span>|</span><label class="collapse" for="c-37329257">[-]</label><label class="expand" for="c-37329257">[1 more]</label></div><br/><div class="children"><div class="content">GPT4[0] is actually very good with base64 to the point where it makes perfect sense.<p>I&#x27;d be interested in how well you could finetune 3.5 to use different compression.<p>[0] - <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;playground&#x2F;p&#x2F;hfLUBCTE8RrRYPIRxEewxtPA?model=gpt-4" rel="nofollow noreferrer">https:&#x2F;&#x2F;platform.openai.com&#x2F;playground&#x2F;p&#x2F;hfLUBCTE8RrRYPIRxEe...</a></div><br/></div></div><div id="37333199" class="c"><input type="checkbox" id="c-37333199" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#37329257">prev</a><span>|</span><a href="#37330849">next</a><span>|</span><label class="collapse" for="c-37333199">[-]</label><label class="expand" for="c-37333199">[1 more]</label></div><br/><div class="children"><div class="content">A few days ago I explored adding beam search and other features to the Gzip Language Model: <a href="https:&#x2F;&#x2F;github.com&#x2F;thomasahle&#x2F;ziplm&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;thomasahle&#x2F;ziplm&#x2F;</a><p>Turns out you can get a lot better text out of these simple models of you add some basic LLM features.</div><br/></div></div><div id="37330849" class="c"><input type="checkbox" id="c-37330849" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#37333199">prev</a><span>|</span><a href="#37329209">next</a><span>|</span><label class="collapse" for="c-37330849">[-]</label><label class="expand" for="c-37330849">[2 more]</label></div><br/><div class="children"><div class="content">A much more efficient implementation than mine at <a href="https:&#x2F;&#x2F;github.com&#x2F;Futrell&#x2F;ziplm">https:&#x2F;&#x2F;github.com&#x2F;Futrell&#x2F;ziplm</a><p>Instead of sampling strings character-by-character, this one adds random bytes to the compressed text and then decodes.</div><br/><div id="37333217" class="c"><input type="checkbox" id="c-37333217" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#37330849">parent</a><span>|</span><a href="#37329209">next</a><span>|</span><label class="collapse" for="c-37333217">[-]</label><label class="expand" for="c-37333217">[1 more]</label></div><br/><div class="children"><div class="content">I immediately thought of your project. Thanks for explaining the difference!</div><br/></div></div></div></div><div id="37329209" class="c"><input type="checkbox" id="c-37329209" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#37330849">prev</a><span>|</span><a href="#37328296">next</a><span>|</span><label class="collapse" for="c-37329209">[-]</label><label class="expand" for="c-37329209">[3 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t there some work a while back on training LLMs on compressed data?</div><br/><div id="37330246" class="c"><input type="checkbox" id="c-37330246" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#37329209">parent</a><span>|</span><a href="#37328296">next</a><span>|</span><label class="collapse" for="c-37330246">[-]</label><label class="expand" for="c-37330246">[2 more]</label></div><br/><div class="children"><div class="content">Might you be thinking of this? <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36732430">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36732430</a></div><br/></div></div></div></div><div id="37328296" class="c"><input type="checkbox" id="c-37328296" checked=""/><div class="controls bullet"><span class="by">blueberrychpstx</span><span>|</span><a href="#37329209">prev</a><span>|</span><label class="collapse" for="c-37328296">[-]</label><label class="expand" for="c-37328296">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of trying to read Gullivers Travels</div><br/><div id="37328947" class="c"><input type="checkbox" id="c-37328947" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#37328296">parent</a><span>|</span><label class="collapse" for="c-37328947">[-]</label><label class="expand" for="c-37328947">[1 more]</label></div><br/><div class="children"><div class="content">You might be thinking of some other literary work; Gulliver&#x27;s Travels isn&#x27;t known for being particularly hard to read.<p>Myself, I was reminded of <i>Finnegans Wake</i> by James Joyce.</div><br/></div></div></div></div></div></div></div></div></div></body></html>