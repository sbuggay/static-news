<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690189249069" as="style"/><link rel="stylesheet" href="styles.css?v=1690189249069"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.myscale.com/2023/07/17/teach-your-llm-vector-sql/">Teach your LLM to answer with facts, not fiction</a> <span class="domain">(<a href="https://blog.myscale.com">blog.myscale.com</a>)</span></div><div class="subtext"><span>jinqueeny</span> | <span>88 comments</span></div><br/><div><div id="36842689" class="c"><input type="checkbox" id="c-36842689" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#36844443">next</a><span>|</span><label class="collapse" for="c-36842689">[-]</label><label class="expand" for="c-36842689">[25 more]</label></div><br/><div class="children"><div class="content">&#x27;Facts&#x27; aren&#x27;t as black and white as people think.<p><pre><code>   &quot;What does Charmander evolve into?&quot;
   &quot;What does the spell &#x27;avada kedavra&#x27; do?&quot;
   &quot;What is the Sindarin word for &#x27;friend&#x27;?&quot;
   &quot;What are the names of Santa&#x27;s reindeer?&quot;
   &quot;Where did Robin Hood live?&quot;
   &quot;Where did Achilles die?&quot;
</code></pre>
These are all &#x27;factual questions&#x27; you can find answers to from reputable sources like Wikipedia. Google displays &#x27;fact boxes&#x27; for several of them. Wolfram Alpha provides answers for three of them. The answers to some of these questions are part of what passes for &#x27;general knowledge&#x27; in some societies.<p>It&#x27;s no surprise that LLMs trained on human writings produce text which claims things that aren&#x27;t true are facts. Humans do that <i>all the time</i>.<p>There are well attested reputable sources that will tell you Abraham Lincoln was a vampire hunter, others that say he was a Lego Master Builder, and others still will tell you that among his notable quotes is &quot;Party on dudes - be excellent to each other&quot;. So what&#x27;s an LLM to do when it&#x27;s trying to extend a paragraph of information about Abraham Lincoln?<p>When an LLM is suggesting what might come next in a piece of text... it doesn&#x27;t know if it&#x27;s supposed to guess a probable word from a Wikipedia article, an Onion article, a Project Gutenberg manuscript, or an Archive Of Our Own fanfic. So you get a bit of all that.</div><br/><div id="36843994" class="c"><input type="checkbox" id="c-36843994" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36842689">parent</a><span>|</span><a href="#36844640">next</a><span>|</span><label class="collapse" for="c-36843994">[-]</label><label class="expand" for="c-36843994">[12 more]</label></div><br/><div class="children"><div class="content">&gt; <i>These are all &#x27;factual questions&#x27;</i><p>Because of elision.<p>&quot;[Homer wrote] that Achilles died of an arrow in the heel&quot;<p>This is why the Wiener Kreis taught to use protocolar statements: &quot;&lt;There&gt; and &lt;at that time&gt; &lt;that individual&gt; witnessed &lt;that fact&gt;&quot;.</div><br/><div id="36844099" class="c"><input type="checkbox" id="c-36844099" checked=""/><div class="controls bullet"><span class="by">austinjp</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36843994">parent</a><span>|</span><a href="#36844640">next</a><span>|</span><label class="collapse" for="c-36844099">[-]</label><label class="expand" for="c-36844099">[11 more]</label></div><br/><div class="children"><div class="content">Tangential: I was going to suggest &quot;protocoli(s|z)ed&quot; instead of protocollar, but I Googled &quot;protocollar statements&quot; just to check and found 2 things. First, this page was the top result! Second,  &quot;protocolar&quot; (one ell) and &quot;protocolary&quot; are apparently real words. New to me, thanks.</div><br/><div id="36844174" class="c"><input type="checkbox" id="c-36844174" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844099">parent</a><span>|</span><a href="#36844526">next</a><span>|</span><label class="collapse" for="c-36844174">[-]</label><label class="expand" for="c-36844174">[2 more]</label></div><br/><div class="children"><div class="content">You had me check a few sources for found expressions in use for the concept: you can find simply &quot;protocols&quot; (intending that), &quot;protocol statements&quot;, the &quot;protocol-sentence debate&quot;, &quot;protocollar propositions&quot;...<p>Edit: oh, by the way, in case of interest: <a href="https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;vienna-circle&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;vienna-circle&#x2F;</a></div><br/><div id="36844715" class="c"><input type="checkbox" id="c-36844715" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844174">parent</a><span>|</span><a href="#36844526">next</a><span>|</span><label class="collapse" for="c-36844715">[-]</label><label class="expand" for="c-36844715">[1 more]</label></div><br/><div class="children"><div class="content">In German they were called &quot;Protokollsätze&quot;, which translates to &quot;protocol sentences&quot;.</div><br/></div></div></div></div><div id="36844526" class="c"><input type="checkbox" id="c-36844526" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844099">parent</a><span>|</span><a href="#36844174">prev</a><span>|</span><a href="#36844544">next</a><span>|</span><label class="collapse" for="c-36844526">[-]</label><label class="expand" for="c-36844526">[6 more]</label></div><br/><div class="children"><div class="content">(By the way:<p>&gt; <i>&quot;protocoli(s|z)ed&quot;</i><p>the use of &#x27;-ize&#x27; is (a graecism) indicated by the OED as International English, as opposed to British, American etc. In fact, some call International English &quot;British spelling with -ize&quot; - it is not exactly that but close. One exception is &#x27;analyse&#x27;, but that is because linguists compromised on the &quot;difficult&quot; original &#x27;analysize&#x27;.)</div><br/><div id="36844610" class="c"><input type="checkbox" id="c-36844610" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844526">parent</a><span>|</span><a href="#36844544">next</a><span>|</span><label class="collapse" for="c-36844610">[-]</label><label class="expand" for="c-36844610">[5 more]</label></div><br/><div class="children"><div class="content">What&#x27;s &quot;analysize&quot;? That&#x27;s not a Greek word.</div><br/><div id="36844648" class="c"><input type="checkbox" id="c-36844648" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844610">parent</a><span>|</span><a href="#36844544">next</a><span>|</span><label class="collapse" for="c-36844648">[-]</label><label class="expand" for="c-36844648">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s so determined by Fowler; pls. check this: <a href="https:&#x2F;&#x2F;www.etymonline.com&#x2F;search?q=analyse" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.etymonline.com&#x2F;search?q=analyse</a></div><br/><div id="36844663" class="c"><input type="checkbox" id="c-36844663" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844648">parent</a><span>|</span><a href="#36844544">next</a><span>|</span><label class="collapse" for="c-36844663">[-]</label><label class="expand" for="c-36844663">[3 more]</label></div><br/><div class="children"><div class="content">Ah, it would have been the correct way to transfer it to English, it says, not that it&#x27;s in any way original.</div><br/><div id="36844714" class="c"><input type="checkbox" id="c-36844714" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844663">parent</a><span>|</span><a href="#36844544">next</a><span>|</span><label class="collapse" for="c-36844714">[-]</label><label class="expand" for="c-36844714">[2 more]</label></div><br/><div class="children"><div class="content">Sorry, my imprecision. You spend ages trying to find proper expression, and yet... Well, this proves the importance of the effort.</div><br/><div id="36844751" class="c"><input type="checkbox" id="c-36844751" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844714">parent</a><span>|</span><a href="#36844544">next</a><span>|</span><label class="collapse" for="c-36844751">[-]</label><label class="expand" for="c-36844751">[1 more]</label></div><br/><div class="children"><div class="content">Haha, that, it does.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36844544" class="c"><input type="checkbox" id="c-36844544" checked=""/><div class="controls bullet"><span class="by">bryanrasmussen</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844099">parent</a><span>|</span><a href="#36844526">prev</a><span>|</span><a href="#36844640">next</a><span>|</span><label class="collapse" for="c-36844544">[-]</label><label class="expand" for="c-36844544">[2 more]</label></div><br/><div class="children"><div class="content">I think protocollar is in this context a misspelling of protocolar - hence its high placement for protocollar statements, if I google protocolar statements this is the highest result (for me)<p><a href="https:&#x2F;&#x2F;www.britannica.com&#x2F;topic&#x2F;protocol-sentence" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.britannica.com&#x2F;topic&#x2F;protocol-sentence</a></div><br/><div id="36844601" class="c"><input type="checkbox" id="c-36844601" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844544">parent</a><span>|</span><a href="#36844640">next</a><span>|</span><label class="collapse" for="c-36844601">[-]</label><label class="expand" for="c-36844601">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>a misspelling of protocolar</i><p>It could be. I cannot bring to mind the rules for doubling right now. They both occur, &#x27;protocolar&#x27; much more often. I will correct my original post.</div><br/></div></div></div></div></div></div></div></div><div id="36844640" class="c"><input type="checkbox" id="c-36844640" checked=""/><div class="controls bullet"><span class="by">rcxdude</span><span>|</span><a href="#36842689">parent</a><span>|</span><a href="#36843994">prev</a><span>|</span><a href="#36843681">next</a><span>|</span><label class="collapse" for="c-36844640">[-]</label><label class="expand" for="c-36844640">[1 more]</label></div><br/><div class="children"><div class="content">This is something which GPT generally isn&#x27;t confused about though: it knows the answer to these questions and it knows that these are questions and statements about well-known works of fiction. I don&#x27;t really think this is the source of the tendency for LLMs to make stuff up.</div><br/></div></div><div id="36843681" class="c"><input type="checkbox" id="c-36843681" checked=""/><div class="controls bullet"><span class="by">BugsJustFindMe</span><span>|</span><a href="#36842689">parent</a><span>|</span><a href="#36844640">prev</a><span>|</span><a href="#36844114">next</a><span>|</span><label class="collapse" for="c-36843681">[-]</label><label class="expand" for="c-36843681">[6 more]</label></div><br/><div class="children"><div class="content">&gt; <i>When an LLM is suggesting what might come next in a piece of text... it doesn&#x27;t know if it&#x27;s supposed to guess a probable word from a Wikipedia article, an Onion article, a Project Gutenberg manuscript, or an Archive Of Our Own fanfic.</i><p>The obvious start seems to be having separate fiction and nonfiction LLMs and not training the nonfiction ones on Archive Of Our Own. People also end up confused about the truth when nobody points out the difference between fiction and nonfiction.</div><br/><div id="36844155" class="c"><input type="checkbox" id="c-36844155" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36843681">parent</a><span>|</span><a href="#36843745">next</a><span>|</span><label class="collapse" for="c-36844155">[-]</label><label class="expand" for="c-36844155">[2 more]</label></div><br/><div class="children"><div class="content">But there&#x27;s a fundamental issue here. The real strength of LLMs is not just information retrieval, but being able to dynamically recombine that information. Of course that&#x27;s also their weakness. The reason GPT will regularly produce code with nonexistent API calls is not because it&#x27;s been trained on &#x27;fictional APIs&#x27;, but because it&#x27;s combining various real calls to make new fictional ones.<p>The obvious answer then is to tell it to make sure that what it&#x27;s finally outputting is really part of the &quot;real&quot; API, but I think it&#x27;s safe to say there&#x27;s some technical hitch there, as it&#x27;s safe to say OpenAI probably spent quite a lot of energy trying to solve the code hallucinations, and ultimately was unable to do so. I&#x27;d guess that the more you restrict its recombination ability, the more you end up with it inappropriately (and incorrectly) just regurgitating large chunks of its training input verbatim. Basically it becomes more like a keyword hunting search engine, and less like a generative LLM.</div><br/><div id="36844401" class="c"><input type="checkbox" id="c-36844401" checked=""/><div class="controls bullet"><span class="by">kfrzcode</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844155">parent</a><span>|</span><a href="#36843745">next</a><span>|</span><label class="collapse" for="c-36844401">[-]</label><label class="expand" for="c-36844401">[1 more]</label></div><br/><div class="children"><div class="content">Yes,  and information is much,  much different than knowledge.</div><br/></div></div></div></div><div id="36843745" class="c"><input type="checkbox" id="c-36843745" checked=""/><div class="controls bullet"><span class="by">xyzzy123</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36843681">parent</a><span>|</span><a href="#36844155">prev</a><span>|</span><a href="#36844114">next</a><span>|</span><label class="collapse" for="c-36843745">[-]</label><label class="expand" for="c-36843745">[3 more]</label></div><br/><div class="children"><div class="content">I kinda like this but e.g are research papers fact or fiction?<p>How about an economics textbook, or an article in the economist? &quot;A history of the english speaking peoples&quot; by Winston Churchill?<p>If we restrict to &quot;ground truth we feel very sure about&quot; it feels like available training data might be quite small.</div><br/><div id="36844105" class="c"><input type="checkbox" id="c-36844105" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36843745">parent</a><span>|</span><a href="#36843839">next</a><span>|</span><label class="collapse" for="c-36844105">[-]</label><label class="expand" for="c-36844105">[1 more]</label></div><br/><div class="children"><div class="content">and what if the economics textbook contains &quot;much like Charmander evolves into Charizard, free markets evolve into monopolies&quot;?</div><br/></div></div></div></div></div></div><div id="36844114" class="c"><input type="checkbox" id="c-36844114" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#36842689">parent</a><span>|</span><a href="#36843681">prev</a><span>|</span><a href="#36842915">next</a><span>|</span><label class="collapse" for="c-36844114">[-]</label><label class="expand" for="c-36844114">[2 more]</label></div><br/><div class="children"><div class="content">Mellon. The rest are left as an exercise to the reader.<p>It does always amaze me that we trained LLMs on a dump of the internet and then people are shocked that they&#x27;re about as trustworthy as a random web page.</div><br/><div id="36844502" class="c"><input type="checkbox" id="c-36844502" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#36842689">root</a><span>|</span><a href="#36844114">parent</a><span>|</span><a href="#36842915">next</a><span>|</span><label class="collapse" for="c-36844502">[-]</label><label class="expand" for="c-36844502">[1 more]</label></div><br/><div class="children"><div class="content">People are not shocked and poor training data is not the main reason LLMs are not trustworthy.</div><br/></div></div></div></div><div id="36844386" class="c"><input type="checkbox" id="c-36844386" checked=""/><div class="controls bullet"><span class="by">kfrzcode</span><span>|</span><a href="#36842689">parent</a><span>|</span><a href="#36842915">prev</a><span>|</span><a href="#36843775">next</a><span>|</span><label class="collapse" for="c-36844386">[-]</label><label class="expand" for="c-36844386">[1 more]</label></div><br/><div class="children"><div class="content">The issue here is one of semiotics and morphemology. Mapping meaning into a narrative and ontological protocol is going to be the requisite work if we want the engine to be &quot;smart.&quot;  As explored in the discussion at hand, tokenization creates a great mimic but it&#x27;s a parlor trick. We must employ a robust thinking-thing that correlates not only a static,  contextually indexed dictionary &lt;lexicography&gt;, we must also route that through a network to distill meaning itself into tokens. Perhaps languages which rely on morphemes for written language - a logosyllabary - are somewhat more or less suited for this task? I ask as a dummy.<p>There also exists the consideration of allographemical contextualization, the nature of relevance, pragmatics, conjunct identification of context, semantics. To be honest the linguistics side alone is vast. Knowledge and cognition however. . . A whole other ballgame. But the only tool we have to really get down to the bottom of how knowledge works is language, it&#x27;s to epistemological pursuit what math is to physics.<p>While GPT is super impressive and can do a lot of quasi-brute-force things, we&#x27;re only finding now the rudiments of the machined intelligence paradigm, and it will behoove any reader to brush up on their classics, true pursuants of philosophy and many order logic are about to be in high demand if I had to reckon.</div><br/></div></div><div id="36843775" class="c"><input type="checkbox" id="c-36843775" checked=""/><div class="controls bullet"><span class="by">zapataband1</span><span>|</span><a href="#36842689">parent</a><span>|</span><a href="#36844386">prev</a><span>|</span><a href="#36844443">next</a><span>|</span><label class="collapse" for="c-36843775">[-]</label><label class="expand" for="c-36843775">[1 more]</label></div><br/><div class="children"><div class="content">source of truth: wikipedia-inference.db.2023<p>charmander -&gt; pokemon -&gt; fiction
avada kedavra -&gt; harry potter -&gt; fiction
sindarin -&gt; ??? -&gt; infer( fiction or nonfiction)
Robin Hood -&gt; disambiguation -&gt; ask(user input-&gt; do you mean?)
...<p>This just seems like a categorization and data annotation problem, which I would assume a bunch of projects are trying to solve like this one.</div><br/></div></div></div></div><div id="36844443" class="c"><input type="checkbox" id="c-36844443" checked=""/><div class="controls bullet"><span class="by">flanked-evergl</span><span>|</span><a href="#36842689">prev</a><span>|</span><a href="#36841654">next</a><span>|</span><label class="collapse" for="c-36844443">[-]</label><label class="expand" for="c-36844443">[2 more]</label></div><br/><div class="children"><div class="content">I think the author of that title could well do with a refresher course in epistemology and physics, as it is just not possible to do what they suggest. But even more unfortunate is how many people fall for deceptive marketing that really should not even fool the average 16-year-old.</div><br/><div id="36844531" class="c"><input type="checkbox" id="c-36844531" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#36844443">parent</a><span>|</span><a href="#36841654">next</a><span>|</span><label class="collapse" for="c-36844531">[-]</label><label class="expand" for="c-36844531">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s weird to see how in such articles (which concern topics that are deeply philosophical by nature), philosophical terms like &quot;facts&quot;, &quot;consciousness&quot;, &quot;knowledge&quot; etc. are just thrown around as if there was <i>any</i> consensus on what those words even mean.<p>The whole debate is revolving around hot air, because nobody knows whether the other person is talking about the same thing as themselves.</div><br/></div></div></div></div><div id="36841654" class="c"><input type="checkbox" id="c-36841654" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#36844443">prev</a><span>|</span><a href="#36843368">next</a><span>|</span><label class="collapse" for="c-36841654">[-]</label><label class="expand" for="c-36841654">[20 more]</label></div><br/><div class="children"><div class="content">It is not a good start that they begin with a dictionary definition of Hallucinations. While the similarities to what a LLM does are apparent enough for the term to be used, LLMs are under no obligation to behave similar to the dictionary definition of Hallucinations.<p>In general facts are not the answer to Hallucinations. You can&#x27;t possibly have every fact for every situation. The true solution to Hallucinations is figuring out how to make a model say &#x27;I don&#x27;t know&quot;</div><br/><div id="36842931" class="c"><input type="checkbox" id="c-36842931" checked=""/><div class="controls bullet"><span class="by">s1mon</span><span>|</span><a href="#36841654">parent</a><span>|</span><a href="#36842072">next</a><span>|</span><label class="collapse" for="c-36842931">[-]</label><label class="expand" for="c-36842931">[7 more]</label></div><br/><div class="children"><div class="content">&quot;Hallucination&quot; makes it sound like ChatGPT drank some of the punch without realizing it was laced with LSD. &quot;Bullshit&quot; sounds more like what comes out of an overconfident ass who should or could know better with some better education.</div><br/><div id="36842969" class="c"><input type="checkbox" id="c-36842969" checked=""/><div class="controls bullet"><span class="by">delecti</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842931">parent</a><span>|</span><a href="#36843924">next</a><span>|</span><label class="collapse" for="c-36842969">[-]</label><label class="expand" for="c-36842969">[2 more]</label></div><br/><div class="children"><div class="content">Hallucination is a better descriptor for what an LLM is doing though. A bullshitter knows they don&#x27;t know, an LLM just strings words together in ways that fit what it &quot;saw&quot; from training data. IMO the main problem with calling them hallucinations is the implication that the true things they say are true on purpose. It&#x27;s hallucinating the true things too.</div><br/><div id="36844371" class="c"><input type="checkbox" id="c-36844371" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842969">parent</a><span>|</span><a href="#36843924">next</a><span>|</span><label class="collapse" for="c-36844371">[-]</label><label class="expand" for="c-36844371">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, but no, there should be no «implication that the true things they» output are not part of that: required modules for foundational thinking (let us say &quot;critical thinking&quot;) are missing.<p>(Issue is, now some are convinced that people in general would do the same and just blurt out the feedforward output of their &quot;internal neural network&quot;, as opposed to having built knowledge in a loop of critical evaluation.)</div><br/></div></div></div></div><div id="36843924" class="c"><input type="checkbox" id="c-36843924" checked=""/><div class="controls bullet"><span class="by">balnaphone</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842931">parent</a><span>|</span><a href="#36842969">prev</a><span>|</span><a href="#36842072">next</a><span>|</span><label class="collapse" for="c-36843924">[-]</label><label class="expand" for="c-36843924">[4 more]</label></div><br/><div class="children"><div class="content">&quot;Confabulation&quot; is the correct and precise term that comports with the English language, rather than being jargon requiring a neologism.</div><br/><div id="36844287" class="c"><input type="checkbox" id="c-36844287" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36843924">parent</a><span>|</span><a href="#36842072">next</a><span>|</span><label class="collapse" for="c-36844287">[-]</label><label class="expand" for="c-36844287">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>&quot;Confabulation&quot;</i><p>And why would that be? &quot;Hallucination&quot; means &quot;erratic wandering&quot;, implying one is lost - similarly to &quot;delirium&quot; (maetaphor using the plough) and &quot;error&quot;. Part of the idea is that of &quot;instead of witnessing the correct, reporting the false&quot; - a very ancient, traditional idea, and akin to the concept of &quot;intelligence&quot; (<i>intus-legere</i>).<p>&quot;Confabulation&quot; means locutor and interlocutor are talking, exchanging narrations.</div><br/><div id="36844578" class="c"><input type="checkbox" id="c-36844578" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36844287">parent</a><span>|</span><a href="#36842072">next</a><span>|</span><label class="collapse" for="c-36844578">[-]</label><label class="expand" for="c-36844578">[2 more]</label></div><br/><div class="children"><div class="content">&gt; In psychology, confabulation is a memory error defined as the production of fabricated, distorted, or misinterpreted memories about oneself or the world. It is generally associated with certain types of brain damage (especially aneurysm in the anterior communicating artery) or a specific subset of dementias.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Confabulation" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Confabulation</a></div><br/><div id="36844690" class="c"><input type="checkbox" id="c-36844690" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36844578">parent</a><span>|</span><a href="#36842072">next</a><span>|</span><label class="collapse" for="c-36844690">[-]</label><label class="expand" for="c-36844690">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, I will check that more analytically as soon as I will be back at the console,<p>but I am not sure - provisionally - that it can be a good idea to relate strictly human neurology to ANNs, if based on phenomena as opposed to structural issues. You do not have that problem when staying with natural language.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36842072" class="c"><input type="checkbox" id="c-36842072" checked=""/><div class="controls bullet"><span class="by">lqhl</span><span>|</span><a href="#36841654">parent</a><span>|</span><a href="#36842931">prev</a><span>|</span><a href="#36841711">next</a><span>|</span><label class="collapse" for="c-36842072">[-]</label><label class="expand" for="c-36842072">[9 more]</label></div><br/><div class="children"><div class="content">This article suggests that LLMs should use a database as a reference for factual information. Rather than asking LLMs to provide their own answers, it is recommended that they summarize based on the facts extracted from the database. This approach reduces the likelihood of hallucinations among LLMs.</div><br/><div id="36842954" class="c"><input type="checkbox" id="c-36842954" checked=""/><div class="controls bullet"><span class="by">3cats-in-a-coat</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842072">parent</a><span>|</span><a href="#36844410">next</a><span>|</span><label class="collapse" for="c-36842954">[-]</label><label class="expand" for="c-36842954">[2 more]</label></div><br/><div class="children"><div class="content">We already had databases of facts, like Wolfram Alpha, decades before LLM, and we largely ignored them. It&#x27;s ironic that when trying to solve AI problems we keep reverting to these old patterns we&#x27;ve tried since the 80s and they kept failing. Habits die hard, I guess.<p>There&#x27;s a categorical difference between knowing a fact, and looking up a fact. When you know a fact you can recognize it in a situation where you wouldn&#x27;t know to look it up, and you&#x27;d know to utilize it in a larger solution rather than simply parrot it when specifically asked about it.<p>Databases of facts have and will still have their place, but that is absolutely not the solution to LLM telling apart fact from truth. They have to innately have this in their model. I don&#x27;t believe the nature of LLM is to hallucinate. It&#x27;s instead a side effect of how we train them. We train them to guess, to be close, but not to be correct necessarily. And why is it a surprise that&#x27;s precisely what they do?<p>Also LLM are too small in order to be accurate. They&#x27;re tiny. GPT4 is roughly 40 times smaller than a human brain. And GPT4 is very large compared to GPT-3, and GPT-3 is very large compared to LLaMA 2.<p>We&#x27;ll need for hardware to catch up so we can scale things up pragmatically and see what happens to their ability to grasp facts. But also architectural changes, of course.</div><br/><div id="36844445" class="c"><input type="checkbox" id="c-36844445" checked=""/><div class="controls bullet"><span class="by">kfrzcode</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842954">parent</a><span>|</span><a href="#36844410">next</a><span>|</span><label class="collapse" for="c-36844445">[-]</label><label class="expand" for="c-36844445">[1 more]</label></div><br/><div class="children"><div class="content">Not to mention our wetbrain software is analog, resonant with the environment, continuous and has a single uptime, in most cases. We should consider developing llms in proto human history style, random noise meets environs, evolve useful signs and symbols based on clusters of semantic embedding. Uno reverse it through a dynamic parallel narrative simulator circuit with range of values for interpretative feedback of context analysis. Assign allomorphic symbols to conceptual clusters. Refine resolution. Add modules for memory and inputs for updating knowledge,  shine it up with some polish and you&#x27;ve got AGI</div><br/></div></div></div></div><div id="36844410" class="c"><input type="checkbox" id="c-36844410" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842072">parent</a><span>|</span><a href="#36842954">prev</a><span>|</span><a href="#36842170">next</a><span>|</span><label class="collapse" for="c-36844410">[-]</label><label class="expand" for="c-36844410">[2 more]</label></div><br/><div class="children"><div class="content">This are assuming LLMs are intelligent and can think &quot;hey I am dumb, I&#x27;ll look that up&quot;.<p>What they are literally doing is guessing the next word, a word a time but doing it really really well and making statistically average output over a very large number of inputs.<p>There is no distinction between understanding &quot;the&quot; vs &quot;a&quot; and telling me 1+1=3. It is all token generation.</div><br/><div id="36844485" class="c"><input type="checkbox" id="c-36844485" checked=""/><div class="controls bullet"><span class="by">kfrzcode</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36844410">parent</a><span>|</span><a href="#36842170">next</a><span>|</span><label class="collapse" for="c-36844485">[-]</label><label class="expand" for="c-36844485">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this a serious simplification? Tokens are just the medium</div><br/></div></div></div></div><div id="36842170" class="c"><input type="checkbox" id="c-36842170" checked=""/><div class="controls bullet"><span class="by">teej</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842072">parent</a><span>|</span><a href="#36844410">prev</a><span>|</span><a href="#36841711">next</a><span>|</span><label class="collapse" for="c-36842170">[-]</label><label class="expand" for="c-36842170">[4 more]</label></div><br/><div class="children"><div class="content">&gt; This approach reduces the likelihood of hallucinations among LLMs.<p>This has not been my experience. Did you create any benchmarks as a part of this project?</div><br/><div id="36842360" class="c"><input type="checkbox" id="c-36842360" checked=""/><div class="controls bullet"><span class="by">mpsk</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842170">parent</a><span>|</span><a href="#36842402">next</a><span>|</span><label class="collapse" for="c-36842360">[-]</label><label class="expand" for="c-36842360">[1 more]</label></div><br/><div class="children"><div class="content">I am the author of this article. And actually what we tried to do was to replicate the simplest implementation to Retrieval Augmented Language Models by prompting the LLM. There have been many researches on this topic right now like work from Meta(<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2208.03299v3.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2208.03299v3.pdf</a>). I think it can give you a picture how those RALMs boost the performance on General QA tasks.</div><br/></div></div><div id="36842402" class="c"><input type="checkbox" id="c-36842402" checked=""/><div class="controls bullet"><span class="by">lqhl</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842170">parent</a><span>|</span><a href="#36842360">prev</a><span>|</span><a href="#36841711">next</a><span>|</span><label class="collapse" for="c-36842402">[-]</label><label class="expand" for="c-36842402">[2 more]</label></div><br/><div class="children"><div class="content">This idea is a simplified version of Retrieval-Augmented Generation (RAG), and RAG has been studied in various research papers, such as the one available at <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.11401" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.11401</a></div><br/><div id="36842945" class="c"><input type="checkbox" id="c-36842945" checked=""/><div class="controls bullet"><span class="by">npsomaratna</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36842402">parent</a><span>|</span><a href="#36841711">next</a><span>|</span><label class="collapse" for="c-36842945">[-]</label><label class="expand" for="c-36842945">[1 more]</label></div><br/><div class="children"><div class="content">My experience with RAG is that while it reduces the incidence of hallucinations* significantly (especially if you reduce the LLM temperature to zero at the same time), it doesn&#x27;t eliminate them.<p>My startup has a product for lawyers that uses RAG to answer legal queries (<a href="https:&#x2F;&#x2F;lawlight.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lawlight.ai&#x2F;</a>). We have a disclaimer that &quot;... (we) do not guarantee the accuracy of answers. You are responsible for reviewing the cited case law and drawing your own independent conclusions.&quot;<p>(This works within the specific context—lawyers are domain experts; and they are supposed to read through all cases they cite in court anyway.)<p>* I dislike the term &quot;hallucinations.&quot; By definition LLMs hallucinate. It&#x27;s just that much (or most) of the time, the hallucinations reflect reality.</div><br/></div></div></div></div></div></div></div></div><div id="36841711" class="c"><input type="checkbox" id="c-36841711" checked=""/><div class="controls bullet"><span class="by">ddmma</span><span>|</span><a href="#36841654">parent</a><span>|</span><a href="#36842072">prev</a><span>|</span><a href="#36843368">next</a><span>|</span><label class="collapse" for="c-36841711">[-]</label><label class="expand" for="c-36841711">[3 more]</label></div><br/><div class="children"><div class="content">You cannot fix what you cannot measure, here is an attempt to do just that with HallMeter <a href="https:&#x2F;&#x2F;why.network&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;why.network&#x2F;</a><p>Still have to figure a measurement unit.</div><br/><div id="36844121" class="c"><input type="checkbox" id="c-36844121" checked=""/><div class="controls bullet"><span class="by">anon25783</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36841711">parent</a><span>|</span><a href="#36843368">next</a><span>|</span><label class="collapse" for="c-36844121">[-]</label><label class="expand" for="c-36844121">[2 more]</label></div><br/><div class="children"><div class="content">How about &quot;falsehood quotient&quot;? Count the number of counterfactual assertions in a given text, then divide by the number of sentences. Of course, the question of what is a falsehood is an exercise for the reader, but this would at least give a unit of measurement, flawed as it is.</div><br/><div id="36844283" class="c"><input type="checkbox" id="c-36844283" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#36841654">root</a><span>|</span><a href="#36844121">parent</a><span>|</span><a href="#36843368">next</a><span>|</span><label class="collapse" for="c-36844283">[-]</label><label class="expand" for="c-36844283">[1 more]</label></div><br/><div class="children"><div class="content">The “exercise for the reader” in this case is the entire point of the metric. If it were possible to do at scale it could be incorporated into existing models right now.<p>Second, it isn’t even necessarily better to have fewer lies if those few lies are more subtle. Plenty of propaganda works by twisting facts and using misleading statements. Perhaps the worst offenders won’t even have any outright falsehoods at all.</div><br/></div></div></div></div></div></div></div></div><div id="36843368" class="c"><input type="checkbox" id="c-36843368" checked=""/><div class="controls bullet"><span class="by">NoZebra120vClip</span><span>|</span><a href="#36841654">prev</a><span>|</span><a href="#36843141">next</a><span>|</span><label class="collapse" for="c-36843368">[-]</label><label class="expand" for="c-36843368">[2 more]</label></div><br/><div class="children"><div class="content">You know, aside from this being a blatant feature-length advertisement for what they&#x27;re selling, I almost thought this was a clever idea.<p>I thought it involved prompting the LLM to write SQL code to query a knowledge base of documents, and index into them, so that you&#x27;d know where to look in the original documents for your authoritative answer. So it would be a meta-search agent.<p>But apparently, they intend the queried documents to feed back into training the LLM? That&#x27;s just gasoline on a dumpster fire.</div><br/><div id="36844017" class="c"><input type="checkbox" id="c-36844017" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36843368">parent</a><span>|</span><a href="#36843141">next</a><span>|</span><label class="collapse" for="c-36844017">[-]</label><label class="expand" for="c-36844017">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>so that you&#x27;d know where to look in the original documents</i><p>Oh, we have something similar: perplexity.ai<p>It provides a number of sources after prompting its textual result.</div><br/></div></div></div></div><div id="36843141" class="c"><input type="checkbox" id="c-36843141" checked=""/><div class="controls bullet"><span class="by">hax0ron3</span><span>|</span><a href="#36843368">prev</a><span>|</span><a href="#36844411">next</a><span>|</span><label class="collapse" for="c-36843141">[-]</label><label class="expand" for="c-36843141">[3 more]</label></div><br/><div class="children"><div class="content">I am mostly a novice to the field of LLMs, but as a layman who has a basic but admittedly very rough understanding of how they work algorithmically, I have a hunch that the same thing that makes these LLMs powerful AIs that have interesting emergent behaviors is also what makes them occasionally get things wildly wrong and claim to know things that they do not know. They are supposed to be AIs, not carefully vetted encyclopedias. Sure, I get that people want to eventually use AI to do life-critical stuff like surgery, and at that point &quot;hallucinations&quot; become a real problem. But we are nowhere close to that point yet, I think, so I feel that the focus on &quot;hallucinations&quot; may be misleading. It is one thing to try to get a 30 year old doctor to not make nonsense up on the fly while at work, that makes sense. But if you try to prevent a 3 year old kid from making up nonsense, that will actually probably hurt his development into a more powerful intelligence. Note: I know that the current popular LLMs do not actually learn past the scope of a single session, but I am sure that they soon will.</div><br/><div id="36844129" class="c"><input type="checkbox" id="c-36844129" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36843141">parent</a><span>|</span><a href="#36844051">next</a><span>|</span><label class="collapse" for="c-36844129">[-]</label><label class="expand" for="c-36844129">[1 more]</label></div><br/><div class="children"><div class="content">This is correct. Current LLMs work by predicting the next word based on a bunch of preceding words. In other words, they are autocomplete. You can often form a valid sentence on your phone if you click on any text field and then press the automatic suggestions several times.<p>Transformer-based LLMs are interesting because they are such good version of autocomplete that they can, for example, complete a news article about scientists discovering unicorns, using just the first sentence (this was one of the first public demonstrations of GPT-2). But fundamentally they are still just auto-complete.</div><br/></div></div><div id="36844051" class="c"><input type="checkbox" id="c-36844051" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36843141">parent</a><span>|</span><a href="#36844129">prev</a><span>|</span><a href="#36844411">next</a><span>|</span><label class="collapse" for="c-36844051">[-]</label><label class="expand" for="c-36844051">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>They are supposed to be AIs</i><p>I.e. synthetic professionals. (Reliable things. Problem solvers.)</div><br/></div></div></div></div><div id="36844411" class="c"><input type="checkbox" id="c-36844411" checked=""/><div class="controls bullet"><span class="by">RyEgswuCsn</span><span>|</span><a href="#36843141">prev</a><span>|</span><a href="#36843714">next</a><span>|</span><label class="collapse" for="c-36844411">[-]</label><label class="expand" for="c-36844411">[3 more]</label></div><br/><div class="children"><div class="content">I think LLMs need to be taught to say &quot;I don&#x27;t know&quot;&#x2F;&quot;I am not sure&quot; or something to that effect.  Another approach might be to introduce an adversarial &quot;censor&quot; model to guard against hallucination (or inappropriate answers).</div><br/><div id="36844474" class="c"><input type="checkbox" id="c-36844474" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#36844411">parent</a><span>|</span><a href="#36843714">next</a><span>|</span><label class="collapse" for="c-36844474">[-]</label><label class="expand" for="c-36844474">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the fundamental issue that it doesn&#x27;t have any way to tell if what it thinks it knows is or isn&#x27;t true?<p>This article sounds like an idea I had independent not too long ago, but with a different goal:<p>LLMs are great at natural language comprehension, but also have a lot of neurons dedicated to factoids. Using neurons that way is really inefficient, can we split the &quot;language&quot; capability from the &quot;knowledge&quot; capability and have the former just look things up in a database?<p>My question was more about reducing the size of the network rather than reducing hallucinations, but it&#x27;s still a separately updatable knowledge resource.<p>(The answer may actually be &quot;no&quot;; I don&#x27;t study this professionally, but technical jargon is kinda both factual domain knowledge and also linguistic comprehension, which is why Oracle isn&#x27;t competing with Starbucks for Java beans).</div><br/><div id="36844577" class="c"><input type="checkbox" id="c-36844577" checked=""/><div class="controls bullet"><span class="by">RyEgswuCsn</span><span>|</span><a href="#36844411">root</a><span>|</span><a href="#36844474">parent</a><span>|</span><a href="#36843714">next</a><span>|</span><label class="collapse" for="c-36844577">[-]</label><label class="expand" for="c-36844577">[1 more]</label></div><br/><div class="children"><div class="content">Many classic statistical modelling techniques have ways to produce some measure of confidence for their predictions; perhaps LLMs could incorporate that as well, e.g. assign probabilities(&#x2F;perplexity?) to each of the tokens they generate.<p>&gt; LLMs are great at natural language comprehension, but also have a lot of neurons dedicated to factoids. Using neurons that way is really inefficient, can we split the &quot;language&quot; capability from the &quot;knowledge&quot; capability and have the former just look things up in a database?<p>I think the beauty of LLMs is exactly that all we need to do is to feed them raw text --- the hope, I guess, had been that the models will be able to develop human-like insights by learning to understand and &quot;speak&quot; languages on its own. 
 Introducing &quot;feature engineering&quot; (e.g. the distinction as you suggested) would defeat that goal.</div><br/></div></div></div></div></div></div><div id="36843714" class="c"><input type="checkbox" id="c-36843714" checked=""/><div class="controls bullet"><span class="by">inopinatus</span><span>|</span><a href="#36844411">prev</a><span>|</span><a href="#36843550">next</a><span>|</span><label class="collapse" for="c-36843714">[-]</label><label class="expand" for="c-36843714">[2 more]</label></div><br/><div class="children"><div class="content">Please don’t dump untreated content marketing in the reading fountain.</div><br/><div id="36844085" class="c"><input type="checkbox" id="c-36844085" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36843714">parent</a><span>|</span><a href="#36843550">next</a><span>|</span><label class="collapse" for="c-36844085">[-]</label><label class="expand" for="c-36844085">[1 more]</label></div><br/><div class="children"><div class="content">And how do you solve the problem of discrimination?<p>(Oh, what a matter: * all the epistemological debate - hardly a deterministic solution; * the fact that we cannot train a function approximator through supervised learning; * the challenge of unsupervised learning; * the scientific and teleological problem that, if we have an ANN find a solution, what we may want is to go &quot;Ok black box, now teach us how you do it to expand our knowledge (not just our dumb capabilities)...&quot;)</div><br/></div></div></div></div><div id="36843550" class="c"><input type="checkbox" id="c-36843550" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36843714">prev</a><span>|</span><a href="#36842642">next</a><span>|</span><label class="collapse" for="c-36843550">[-]</label><label class="expand" for="c-36843550">[1 more]</label></div><br/><div class="children"><div class="content">How to solve problem X (... but not really) ... use our product.</div><br/></div></div><div id="36842642" class="c"><input type="checkbox" id="c-36842642" checked=""/><div class="controls bullet"><span class="by">peterbonney</span><span>|</span><a href="#36843550">prev</a><span>|</span><a href="#36843113">next</a><span>|</span><label class="collapse" for="c-36842642">[-]</label><label class="expand" for="c-36842642">[3 more]</label></div><br/><div class="children"><div class="content">If “allowing the execution of arbitrary database queries written by an LLM inside a SaaS application” is the answer, I’d love to know what the question is.</div><br/><div id="36842763" class="c"><input type="checkbox" id="c-36842763" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36842642">parent</a><span>|</span><a href="#36843113">next</a><span>|</span><label class="collapse" for="c-36842763">[-]</label><label class="expand" for="c-36842763">[2 more]</label></div><br/><div class="children"><div class="content">The question is how to make money from LLM hype.</div><br/><div id="36844212" class="c"><input type="checkbox" id="c-36844212" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36842642">root</a><span>|</span><a href="#36842763">parent</a><span>|</span><a href="#36843113">next</a><span>|</span><label class="collapse" for="c-36844212">[-]</label><label class="expand" for="c-36844212">[1 more]</label></div><br/><div class="children"><div class="content">Or, how to get distracted from the real problem of making it reason. (That comedic elephant in the room.)</div><br/></div></div></div></div></div></div><div id="36843113" class="c"><input type="checkbox" id="c-36843113" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#36842642">prev</a><span>|</span><a href="#36843561">next</a><span>|</span><label class="collapse" for="c-36843113">[-]</label><label class="expand" for="c-36843113">[3 more]</label></div><br/><div class="children"><div class="content">That will help a bit, but it&#x27;s not going to fix it.<p>Using GPT4 and Code Interpreter, I have asked it to write a function and test it, given some inputs and expected outputs. The function returned different values when it tested it, but it lied and said it worked as expected.<p>You need to read the code and the test outputs yourself. Or maybe have it write an automated test?<p>Despite this, it seems quite promising. I expect that in a year or two, some IDE&#x27;s will come with a useful pair programming feature.</div><br/><div id="36843225" class="c"><input type="checkbox" id="c-36843225" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#36843113">parent</a><span>|</span><a href="#36843561">next</a><span>|</span><label class="collapse" for="c-36843225">[-]</label><label class="expand" for="c-36843225">[2 more]</label></div><br/><div class="children"><div class="content">With code interpreter, it actually runs the code, so how did failing tests get interpreted as being correct?</div><br/><div id="36843579" class="c"><input type="checkbox" id="c-36843579" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#36843113">root</a><span>|</span><a href="#36843225">parent</a><span>|</span><a href="#36843561">next</a><span>|</span><label class="collapse" for="c-36843579">[-]</label><label class="expand" for="c-36843579">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know. Why does a language model do anything?<p>The &quot;test&quot; was a print statement, not a unit test. There wasn&#x27;t a failure message. It had to read the output and compare it to the expected value I gave it.<p>It claimed it got a different result. I guess it didn&#x27;t really read the result because it strongly expected something else?<p>If you use an assertEquals() that loudly complains, maybe it&#x27;s less likely to do this? I haven&#x27;t seen it ignore stack traces.</div><br/></div></div></div></div></div></div><div id="36843561" class="c"><input type="checkbox" id="c-36843561" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#36843113">prev</a><span>|</span><a href="#36842341">next</a><span>|</span><label class="collapse" for="c-36843561">[-]</label><label class="expand" for="c-36843561">[1 more]</label></div><br/><div class="children"><div class="content">I was hoping this would be a way to train an LLM that somehow knew when to seek out external knowledge and when not to.<p>I guess this is a pretty unsolvable problem with current architectures. There&#x27;s just no concrete &quot;confidence&quot; value. I mean, an LLM will give you a probable value for what confidence could be given the words preceding it, but that&#x27;s an entirely different thing</div><br/></div></div><div id="36842341" class="c"><input type="checkbox" id="c-36842341" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#36843561">prev</a><span>|</span><a href="#36843910">next</a><span>|</span><label class="collapse" for="c-36842341">[-]</label><label class="expand" for="c-36842341">[5 more]</label></div><br/><div class="children"><div class="content">One of the fun things I like to do with these examples is follow along at home. I don&#x27;t have access to GPT-4 because I refuse to give money to OpenAI, but with GPT-3.5, &quot;What is an LLM hallucination&quot; actually gives:<p>- The usual knowledge cutoff warning<p>- An explanation of what a hallucination is<p>If (in a separate conversation) I give GPT-3.5 the exact prompt that explains what an LLM is, I get gaslighted instead. GPT-3.5 attempts to tell me that LLM stands for &quot;Legal Master of Laws&quot;. Then it gives the knowledge cutoff warning, and then the same correct explanation Myscale got.<p>The rest of this article appears to be trying to turn GPT into a frontend for search engines. I don&#x27;t know why people keep trying to do this.</div><br/><div id="36842895" class="c"><input type="checkbox" id="c-36842895" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#36842341">parent</a><span>|</span><a href="#36842612">next</a><span>|</span><label class="collapse" for="c-36842895">[-]</label><label class="expand" for="c-36842895">[1 more]</label></div><br/><div class="children"><div class="content">LLM, confusingly, <i>does</i> stand for legal master of laws, despite the acronym not quite fitting. It&#x27;s an advanced law degree some lawyers get. ChatGPT isn&#x27;t gaslighting you! That&#x27;s a true fact.<p><a href="https:&#x2F;&#x2F;law.pepperdine.edu&#x2F;blog&#x2F;posts&#x2F;llm-versus-jd-degree.htm" rel="nofollow noreferrer">https:&#x2F;&#x2F;law.pepperdine.edu&#x2F;blog&#x2F;posts&#x2F;llm-versus-jd-degree.h...</a></div><br/></div></div><div id="36842612" class="c"><input type="checkbox" id="c-36842612" checked=""/><div class="controls bullet"><span class="by">lqhl</span><span>|</span><a href="#36842341">parent</a><span>|</span><a href="#36842895">prev</a><span>|</span><a href="#36843910">next</a><span>|</span><label class="collapse" for="c-36842612">[-]</label><label class="expand" for="c-36842612">[3 more]</label></div><br/><div class="children"><div class="content">As jwells89 mentioned, LLMs can extract the intention from questions and generate better queries for a search engine or database.</div><br/><div id="36842926" class="c"><input type="checkbox" id="c-36842926" checked=""/><div class="controls bullet"><span class="by">akasakahakada</span><span>|</span><a href="#36842341">root</a><span>|</span><a href="#36842612">parent</a><span>|</span><a href="#36843910">next</a><span>|</span><label class="collapse" for="c-36842926">[-]</label><label class="expand" for="c-36842926">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t need LLM to assume my intention, I need exact matches for keywords.</div><br/><div id="36844833" class="c"><input type="checkbox" id="c-36844833" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#36842341">root</a><span>|</span><a href="#36842926">parent</a><span>|</span><a href="#36843910">next</a><span>|</span><label class="collapse" for="c-36844833">[-]</label><label class="expand" for="c-36844833">[1 more]</label></div><br/><div class="children"><div class="content">Then you don&#x27;t want an LLM at all; exact keyword matches are something we did in the mid 90s, one of the specific value-adds of an LLM is that it doesn&#x27;t get stuck when you&#x27;ve only got a half-remembered inexact quote or vague description.</div><br/></div></div></div></div></div></div></div></div><div id="36843910" class="c"><input type="checkbox" id="c-36843910" checked=""/><div class="controls bullet"><span class="by">isaacfung</span><span>|</span><a href="#36842341">prev</a><span>|</span><a href="#36841625">next</a><span>|</span><label class="collapse" for="c-36843910">[-]</label><label class="expand" for="c-36843910">[1 more]</label></div><br/><div class="children"><div class="content">I dont know why so many people keep repeating the trivial fact that we cant &quot;eliminate&quot; hallucinations. We cant eliminite misinformation from google, social media or people we know either. Best we can try:<p><pre><code>  1) better filter the training data  
  2) design better retrieval and reranking algorithms  
  3) when context information is provided, make it use the sources and cite the sources (use extractive QA to highlight which part of the source is relevant. This is the type of hallucinations that we should focus on as we can compare the generated result and the context to detect the hallucinations)  
  4) make the llm break down its reasoning into small steps that can be validated inidividually (COT, PAL)
</code></pre>
There are some research on how to manipulate the logits during decoding to make the generated text satisfy certain contraints. I suspect that we can use these techniques to make the LLM stick to the provided context.<p><pre><code>  - Controllable Text Generation with Language Constraints  
  - Classifiers are Better Experts for Controllable Text Generation  
  - Stay on topic with Classifier-Free Guidance</code></pre></div><br/></div></div><div id="36841625" class="c"><input type="checkbox" id="c-36841625" checked=""/><div class="controls bullet"><span class="by">wizzwizz4</span><span>|</span><a href="#36843910">prev</a><span>|</span><a href="#36842220">next</a><span>|</span><label class="collapse" for="c-36841625">[-]</label><label class="expand" for="c-36841625">[12 more]</label></div><br/><div class="children"><div class="content">Situation: people try to use these predictive text chatbots as search engines.<p>Problem: LLMs are not search engines. They extrapolate, interpolate, and approximate (so-called “hallucinations”) so they can always produce somewhat-plausible text completions.<p>Solution: Create a search engine so good at returning relevant results that even an LLM can make use of it… then go to <i>significant</i> lengths to plug that search engine into the LLM, preventing people from reading the search results directly.<p>Why not simply <i>give people access to the search engine</i>‽ People <i>know how to use search engines</i>!! This is the fifth time I&#x27;ve seen an article like this, and I&#x27;m still… baffled. It&#x27;s <a href="https:&#x2F;&#x2F;xkcd.com&#x2F;2021&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;xkcd.com&#x2F;2021&#x2F;</a> all over again.</div><br/><div id="36842515" class="c"><input type="checkbox" id="c-36842515" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36841625">parent</a><span>|</span><a href="#36842151">next</a><span>|</span><label class="collapse" for="c-36842515">[-]</label><label class="expand" for="c-36842515">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, this &quot;fix&quot; just shifts the problem to the vector DB and its embedding algorithm. People keep forgetting to mention that embeddings aren&#x27;t 100% accurate either. The net accuracy may be better but it&#x27;s not magic.</div><br/></div></div><div id="36842151" class="c"><input type="checkbox" id="c-36842151" checked=""/><div class="controls bullet"><span class="by">omeze</span><span>|</span><a href="#36841625">parent</a><span>|</span><a href="#36842515">prev</a><span>|</span><a href="#36842726">next</a><span>|</span><label class="collapse" for="c-36842151">[-]</label><label class="expand" for="c-36842151">[3 more]</label></div><br/><div class="children"><div class="content">Yea… my opinion on this is that startups are attempting to force a market for chat bots, instead of accepting that LLM embeddings are best utilized as a search feature, not new product surface area</div><br/><div id="36843237" class="c"><input type="checkbox" id="c-36843237" checked=""/><div class="controls bullet"><span class="by">CSMastermind</span><span>|</span><a href="#36841625">root</a><span>|</span><a href="#36842151">parent</a><span>|</span><a href="#36842562">next</a><span>|</span><label class="collapse" for="c-36843237">[-]</label><label class="expand" for="c-36843237">[1 more]</label></div><br/><div class="children"><div class="content">IMO the most obvious place LLMs will be used is for interfaces.<p>They enable voice-based interfaces to be practical for normal users for the first time since you really can talk to them in a convincing way.<p>Translating user input into a set of well-defined commands seems like a better use than searching data to me.</div><br/></div></div><div id="36842562" class="c"><input type="checkbox" id="c-36842562" checked=""/><div class="controls bullet"><span class="by">woeirua</span><span>|</span><a href="#36841625">root</a><span>|</span><a href="#36842151">parent</a><span>|</span><a href="#36843237">prev</a><span>|</span><a href="#36842726">next</a><span>|</span><label class="collapse" for="c-36842562">[-]</label><label class="expand" for="c-36842562">[1 more]</label></div><br/><div class="children"><div class="content">They really want agents to work, and they just don’t yet.</div><br/></div></div></div></div><div id="36842726" class="c"><input type="checkbox" id="c-36842726" checked=""/><div class="controls bullet"><span class="by">signatoremo</span><span>|</span><a href="#36841625">parent</a><span>|</span><a href="#36842151">prev</a><span>|</span><a href="#36842205">next</a><span>|</span><label class="collapse" for="c-36842726">[-]</label><label class="expand" for="c-36842726">[1 more]</label></div><br/><div class="children"><div class="content">I’m sure you’ve heard of Google-fu. Someone is better at searching than others. I believe they propose that LLM can be superior at producing search queries. Whether they succeed, that remains to be seen, but the idea isn’t that baffling.</div><br/></div></div><div id="36842205" class="c"><input type="checkbox" id="c-36842205" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#36841625">parent</a><span>|</span><a href="#36842726">prev</a><span>|</span><a href="#36842220">next</a><span>|</span><label class="collapse" for="c-36842205">[-]</label><label class="expand" for="c-36842205">[6 more]</label></div><br/><div class="children"><div class="content">Well, search engines and my ability to query them aren&#x27;t good enough for highly specific or poorly worded questions yet. LLMs are sometimes better in this space.</div><br/><div id="36842324" class="c"><input type="checkbox" id="c-36842324" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#36841625">root</a><span>|</span><a href="#36842205">parent</a><span>|</span><a href="#36842220">next</a><span>|</span><label class="collapse" for="c-36842324">[-]</label><label class="expand" for="c-36842324">[5 more]</label></div><br/><div class="children"><div class="content">Indeed, a frequent issue with search engines is knowing the right terms plug into them, particularly when researching a topic beyond one&#x27;s scope of knowledge.<p>Half the power of LLMs as they currently exist is that they can often extract the intention of the user&#x27;s question in a way that search engines usually can&#x27;t, allowing them to provide a more useful answer or at least point the user in the right direction.<p>Perhaps it would make sense for search engines to utilize LLMs to perform this query extraction and suggest more appropriate search terms, engaging conversational interaction only if the suggestions are wrong and the LLM requires further clarification.</div><br/><div id="36842572" class="c"><input type="checkbox" id="c-36842572" checked=""/><div class="controls bullet"><span class="by">lalopalota</span><span>|</span><a href="#36841625">root</a><span>|</span><a href="#36842324">parent</a><span>|</span><a href="#36842729">next</a><span>|</span><label class="collapse" for="c-36842572">[-]</label><label class="expand" for="c-36842572">[1 more]</label></div><br/><div class="children"><div class="content">I agree. Use the LLM to construct a better search query, then return those results and the query to the user. The user can modify the &quot;optimized&quot; query and repeat until a usefull answer is obtained.</div><br/></div></div><div id="36842729" class="c"><input type="checkbox" id="c-36842729" checked=""/><div class="controls bullet"><span class="by">worrycue</span><span>|</span><a href="#36841625">root</a><span>|</span><a href="#36842324">parent</a><span>|</span><a href="#36842572">prev</a><span>|</span><a href="#36842220">next</a><span>|</span><label class="collapse" for="c-36842729">[-]</label><label class="expand" for="c-36842729">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Half the power of LLMs as they currently exist is that they can often extract the intention of the user&#x27;s question in a way that search engines usually can&#x27;t<p>I don’t know about that. Google is pretty good at including “similar” questions that others have asked to what I have queried and often that’s exactly what I needed.</div><br/><div id="36842863" class="c"><input type="checkbox" id="c-36842863" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#36841625">root</a><span>|</span><a href="#36842729">parent</a><span>|</span><a href="#36842220">next</a><span>|</span><label class="collapse" for="c-36842863">[-]</label><label class="expand" for="c-36842863">[2 more]</label></div><br/><div class="children"><div class="content">In my experience, Google&#x27;s ability to suggest &quot;similar&quot; queries is often limited if I don&#x27;t know the terminology associated with the subject in question. It&#x27;s decent if you&#x27;re already in the ballpark, but to extend the analogy if you&#x27;re stuck trying to figure out where the park is in the first place it&#x27;s much more hit or miss.</div><br/><div id="36843343" class="c"><input type="checkbox" id="c-36843343" checked=""/><div class="controls bullet"><span class="by">worrycue</span><span>|</span><a href="#36841625">root</a><span>|</span><a href="#36842863">parent</a><span>|</span><a href="#36842220">next</a><span>|</span><label class="collapse" for="c-36843343">[-]</label><label class="expand" for="c-36843343">[1 more]</label></div><br/><div class="children"><div class="content">I “fish” with google when I don’t know the terminology is what I’m saying. With some luck Google will have correct question on the first page of the search results.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36842220" class="c"><input type="checkbox" id="c-36842220" checked=""/><div class="controls bullet"><span class="by">senectus1</span><span>|</span><a href="#36841625">prev</a><span>|</span><a href="#36841854">next</a><span>|</span><label class="collapse" for="c-36842220">[-]</label><label class="expand" for="c-36842220">[1 more]</label></div><br/><div class="children"><div class="content">would be nice if they could show a gradient score on results that show how <i>certain</i> it is of its answers...<p>It should be fairly trivial for it to tell you how often its straight up lied about something.</div><br/></div></div><div id="36841854" class="c"><input type="checkbox" id="c-36841854" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#36842220">prev</a><span>|</span><label class="collapse" for="c-36841854">[-]</label><label class="expand" for="c-36841854">[3 more]</label></div><br/><div class="children"><div class="content">I believe that LLMs should be banned, but if they have to exist, we should teach them ethics first before anything else.</div><br/><div id="36842372" class="c"><input type="checkbox" id="c-36842372" checked=""/><div class="controls bullet"><span class="by">12907835202</span><span>|</span><a href="#36841854">parent</a><span>|</span><label class="collapse" for="c-36842372">[-]</label><label class="expand" for="c-36842372">[2 more]</label></div><br/><div class="children"><div class="content">Whose ethics? Should we tell it that sex is a positive thing or a horrible sin?</div><br/><div id="36842518" class="c"><input type="checkbox" id="c-36842518" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#36841854">root</a><span>|</span><a href="#36842372">parent</a><span>|</span><label class="collapse" for="c-36842518">[-]</label><label class="expand" for="c-36842518">[1 more]</label></div><br/><div class="children"><div class="content">Those crusaders that prevail are the most ethical.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>