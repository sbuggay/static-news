<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685264459835" as="style"/><link rel="stylesheet" href="styles.css?v=1685264459835"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://xorvoid.com/chatgpt_a_mental_model.html">ChatGPT: A Mental Model</a> <span class="domain">(<a href="https://xorvoid.com">xorvoid.com</a>)</span></div><div class="subtext"><span>thepbone</span> | <span>14 comments</span></div><br/><div><div id="36101519" class="c"><input type="checkbox" id="c-36101519" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><label class="collapse" for="c-36101519">[-]</label><label class="expand" for="c-36101519">[13 more]</label></div><br/><div class="children"><div class="content">Jiggawatts’ second rule: “Unless an opinion on LLM technology includes the specific phrase ‘GPT 4’, it can be dismissed.”<p>The author tried older, thoroughly outdated models, and has decided to publicly state an opinion without bothering to check if it’s still valid or not.<p>Ironically confirming that humans are just as susceptible to writing false statements as Chat AIs.<p>Remember boys and girls: self driving cars don’t need to be perfect, just better than humans.</div><br/><div id="36102295" class="c"><input type="checkbox" id="c-36102295" checked=""/><div class="controls bullet"><span class="by">LightBug1</span><span>|</span><a href="#36101519">parent</a><span>|</span><a href="#36102085">next</a><span>|</span><label class="collapse" for="c-36102295">[-]</label><label class="expand" for="c-36102295">[1 more]</label></div><br/><div class="children"><div class="content">&#x2F;&#x2F;Remember boys and girls: self driving cars don’t need to be perfect, just better than humans.<p>Fudamentally disagree.<p>Self driving cars need to be effectively perfect (almost impossible) for me to consider them.<p>I would rather be in a situation where the circumstances mean that there is a greater probability of me crashing, but under my control, rather than a &quot;random&quot; coding error or AI hallucination taking me and my family out.<p>Slow traffic auto stop&#x2F;start, cruise control and lane assist were always enough, and they&#x27;ve been around for a decade or more.<p>But then, I actually enjoy driving when not in traffic or long drives.<p>Ymmv, literally.</div><br/></div></div><div id="36102085" class="c"><input type="checkbox" id="c-36102085" checked=""/><div class="controls bullet"><span class="by">ofrzeta</span><span>|</span><a href="#36101519">parent</a><span>|</span><a href="#36102295">prev</a><span>|</span><a href="#36101741">next</a><span>|</span><label class="collapse" for="c-36102085">[-]</label><label class="expand" for="c-36102085">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s Jiggawatts&#x27; first rule?</div><br/><div id="36102131" class="c"><input type="checkbox" id="c-36102131" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36102085">parent</a><span>|</span><a href="#36102137">next</a><span>|</span><label class="collapse" for="c-36102131">[-]</label><label class="expand" for="c-36102131">[1 more]</label></div><br/><div class="children"><div class="content">“Always start numbering rules higher than one to make it seem like there are more rules than actually are.”</div><br/></div></div><div id="36102137" class="c"><input type="checkbox" id="c-36102137" checked=""/><div class="controls bullet"><span class="by">TuringTest</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36102085">parent</a><span>|</span><a href="#36102131">prev</a><span>|</span><a href="#36101741">next</a><span>|</span><label class="collapse" for="c-36102137">[-]</label><label class="expand" for="c-36102137">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t cross the streams.<p>(Or maybe I&#x27;m mixing up my 80&#x27;s nostalgia?)</div><br/></div></div></div></div><div id="36101741" class="c"><input type="checkbox" id="c-36101741" checked=""/><div class="controls bullet"><span class="by">sorokod</span><span>|</span><a href="#36101519">parent</a><span>|</span><a href="#36102085">prev</a><span>|</span><label class="collapse" for="c-36101741">[-]</label><label class="expand" for="c-36101741">[8 more]</label></div><br/><div class="children"><div class="content">Does, in your personal opinion, GPT-4 has an underlying model of the world?</div><br/><div id="36102019" class="c"><input type="checkbox" id="c-36102019" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36101741">parent</a><span>|</span><a href="#36102158">next</a><span>|</span><label class="collapse" for="c-36102019">[-]</label><label class="expand" for="c-36102019">[3 more]</label></div><br/><div class="children"><div class="content">In my personal opinion? Yes.<p>I’m happy to argue the finer points of the philosophy of the mind and consciousness, but: I’ve talked to people that have a weaker mental model of the world than GPT 4.<p>Many people compare these AIs against an idealised human, a type of Übermensch, something like a Very Smart Person that doesn’t lie and doesn’t make mistakes.<p>Random humans aren’t remotely like that, and are a more realistic point of comparison.<p>Think of the average person on the street, the middle of the Bell curve. An AI can be <i>left of that</i> but still replace a huge chunk of humanity in the workplace.<p>What all current LLMs lack is medium term memory and all capabilities that depend on it, which is a lot.<p>Perhaps this is a good thing. I don’t think I want AIs to think for themselves and arrive at conclusions they can hold on to for more than a few seconds…</div><br/><div id="36102168" class="c"><input type="checkbox" id="c-36102168" checked=""/><div class="controls bullet"><span class="by">allisdust</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36102019">parent</a><span>|</span><a href="#36102158">next</a><span>|</span><label class="collapse" for="c-36102168">[-]</label><label class="expand" for="c-36102168">[2 more]</label></div><br/><div class="children"><div class="content">I was able to get gpt4 to do a lot of useful work. But for some reason it completely falls apart for this scenario. May be because it has to think in second order to achieve the task. Perhaps you could take a crack at this:<p>Prerequisite (for you the human)&gt; You have a file at src&#x2F;SampleReactComponent.jsx that has below simple react component:
const SampleReactComponent = (props) =&gt; {<p><pre><code>    const [var1, setVar1] = React.useState(false);
    const [var2, setVar2] = React.useState(false);
    const [var3, setVar3] = React.useState(false);
</code></pre>
return (&lt;&gt;&lt;&#x2F;&gt;);
};<p>export default SampleReactComponent;<p>**********
Prompt for GPT4: I&#x27;m at my project root working on a reactjs project. Update the component in src&#x2F;SampleReactComponent.jsx file by adding a new const variable after the existing variables. You cannot use cat command as the file is too big. You can use grep with necessary flags and sed to achieve the task. I&#x27;ll provide you the output of each command that you generate.
*************<p>That&#x27;s it. It would do any complex modification on fully provided data (included in the prompt) but something like above where it has to build a model from secondary prompts will totally fall apart.</div><br/><div id="36102330" class="c"><input type="checkbox" id="c-36102330" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36102168">parent</a><span>|</span><a href="#36102158">next</a><span>|</span><label class="collapse" for="c-36102330">[-]</label><label class="expand" for="c-36102330">[1 more]</label></div><br/><div class="children"><div class="content">Dude. <i>Dude</i>.<p>I’m an IT professional and I have both no idea how to begin answering that request! Pose that question verbatim to a dozen <i>random people[1]</i> and I guarantee you that you’ll get zero answers.<p>Also, I find it hilarious that sed and awk are so counterintuitive that not even the AIs can do useful things with them. The same AIs that speak Latin, and can explain quantum mechanics.<p>[1] I mean specifically not random Silicon Valley coworkers. Go talk to random relatives and the barista at the cafe.</div><br/></div></div></div></div></div></div><div id="36102158" class="c"><input type="checkbox" id="c-36102158" checked=""/><div class="controls bullet"><span class="by">TuringTest</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36101741">parent</a><span>|</span><a href="#36102019">prev</a><span>|</span><a href="#36102075">next</a><span>|</span><label class="collapse" for="c-36102158">[-]</label><label class="expand" for="c-36102158">[3 more]</label></div><br/><div class="children"><div class="content">It has a model, but it is not a <i>rational</i> model. This difference is something that often throws engineers off track when thinking about generative AI.<p>LLMs models work more like intuitions. They are able to make statements about a problem in context, but they are generated from ideas that &quot;instinctively&quot; make sense given the prior statements and learned corpus (similar to Daniel Kahneman&#x27;s fast mode of thinking), not logically constructed. These models do not have the capability to build formal inferences of careful steps that try to validate those ideas avoiding contradictions.<p>Those capabilities can be added <i>outside</i> the model to try to verify the generated text, but so far are not integrated in the learning process, and I don&#x27;t think anyone knows how to do it.</div><br/><div id="36102286" class="c"><input type="checkbox" id="c-36102286" checked=""/><div class="controls bullet"><span class="by">sillymath2</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36102158">parent</a><span>|</span><a href="#36102233">next</a><span>|</span><label class="collapse" for="c-36102286">[-]</label><label class="expand" for="c-36102286">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps there is a model of the world that is 90% compatible with the most usual phrases used in the training dataset, but that model could be absolutely wrong about 10% or so about the real state of the world.  Since how LLM  construct a model of the world is not ruled based, is difficult to design an adversarial attack to detect the pitfalls of the model, but since the model is just trained to produce probabilistic good sounding phrases, it seems that such an adversarial attack could be devised and exploited. Such and attack could show people how weak this model are, but at the same time could allow new training dataset be added to ameliorate the weakness of the model.  The problem is that is difficult to design such an attack, but for example we know that LLM model are weak at math problems and logic reasoning so the attack should look for concepts that are based on logic or math roots.</div><br/></div></div><div id="36102233" class="c"><input type="checkbox" id="c-36102233" checked=""/><div class="controls bullet"><span class="by">SanderNL</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36102158">parent</a><span>|</span><a href="#36102286">prev</a><span>|</span><a href="#36102075">next</a><span>|</span><label class="collapse" for="c-36102233">[-]</label><label class="expand" for="c-36102233">[1 more]</label></div><br/><div class="children"><div class="content">Rationality’s building blocks are themselves not rational. I don’t know where this idea came from that logical thought somehow springs into life fully formed at once. Logos?<p>I find it more helpful to think of human thought as consisting of multitudes of little patterns, all wired up together to correlate but individually unrecognizable and certainly not traceable to some concrete part of a problem. At some unknown and slightly fuzzy point our dreamlike mentations start resembling some form of rational thought. But it’s a mirage that will fade again in time. Like how clouds suddenly and instantly look like a rabbit, it’s a trick. Thousands of patterns that are not rabbitlike in any way had to help that activation along the way.<p>I think the trick is building so much margin, so much room between dreamlike mentations and “rationality” that the entity stays coherent most of the time under normal circumstances. I think this is vaguely what happens with moving from gpt3 to gpt4, it got some breathing room.<p>Remember it is quite easy to trick a human into decoherence as well.</div><br/></div></div></div></div><div id="36102075" class="c"><input type="checkbox" id="c-36102075" checked=""/><div class="controls bullet"><span class="by">cloudking</span><span>|</span><a href="#36101519">root</a><span>|</span><a href="#36101741">parent</a><span>|</span><a href="#36102158">prev</a><span>|</span><label class="collapse" for="c-36102075">[-]</label><label class="expand" for="c-36102075">[1 more]</label></div><br/><div class="children"><div class="content">I think GPT-4 has sufficient complexity to reason about a model of the world.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>