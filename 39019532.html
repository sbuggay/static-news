<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705482060084" as="style"/><link rel="stylesheet" href="styles.css?v=1705482060084"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://stability.ai/news/stable-code-2024-llm-code-completion-release">Stable Code 3B: Coding on the Edge</a> <span class="domain">(<a href="https://stability.ai">stability.ai</a>)</span></div><div class="subtext"><span>egnehots</span> | <span>109 comments</span></div><br/><div><div id="39021374" class="c"><input type="checkbox" id="c-39021374" checked=""/><div class="controls bullet"><span class="by">JCM9</span><span>|</span><a href="#39020070">next</a><span>|</span><label class="collapse" for="c-39021374">[-]</label><label class="expand" for="c-39021374">[13 more]</label></div><br/><div class="children"><div class="content">Don’t entirely understand Stability’s business model. They’ve been putting out a lot of models recently and Stable Diffusion was novel at the time, but now their models consistently seem to be somewhat second rate compared to other things out there. For example Midjourney now seems to have far surpassed them in the image generation front. After raising a ton of funding Stability seems to just be throwing a bunch of stuff out there that’s OK but no longer ground breaking. What am I missing?<p>Many other startups in the space will like face similar issues given the rapid commoditization of these models and the underlying tech. It’s very easy to spend a fortune building a model that offers a short lived incremental improvement at best before one can just quickly swap it out for something else someone else paid to train.</div><br/><div id="39022295" class="c"><input type="checkbox" id="c-39022295" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39021374">parent</a><span>|</span><a href="#39021772">next</a><span>|</span><label class="collapse" for="c-39022295">[-]</label><label class="expand" for="c-39022295">[3 more]</label></div><br/><div class="children"><div class="content">Business model is bundling so you have a one stop shop for good quality models of every modality and cultural variants of them.<p>These go on bedrock, on chip, on prem etc and our consulting partners take them to the end user.<p>On the innovation side stable diffusion turbo does like 100 cats with hats per second and the video model outperforms runway, pika etc on blind tests.<p>Stable audio was one of the time innovation of the year winners on music and we released a sota 3d model.<p>Stable LM zephyr is the best 3b chat model works great on a MacBook Air.<p>Most of the pixels in the world will be generated so fast high quality image&#x2F;video are the core and these other models are to support them.<p>It’s really hard to build good solid models and we are the only company that can build a model of any type for anyone.</div><br/><div id="39023254" class="c"><input type="checkbox" id="c-39023254" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#39021374">root</a><span>|</span><a href="#39022295">parent</a><span>|</span><a href="#39022372">next</a><span>|</span><label class="collapse" for="c-39023254">[-]</label><label class="expand" for="c-39023254">[1 more]</label></div><br/><div class="children"><div class="content">I use Stable Diffusion family models for innovative art products.<p>On a small scale, you have to professionalize ComfyUI’s development. My PR to make it installable and to make a plugin ecosystem that makes sense should not be sitting unmerged (<a href="https:&#x2F;&#x2F;github.com&#x2F;comfyanonymous&#x2F;ComfyUI&#x2F;pull&#x2F;298">https:&#x2F;&#x2F;github.com&#x2F;comfyanonymous&#x2F;ComfyUI&#x2F;pull&#x2F;298</a>).<p>On a medium scale, CLIP is holding you back. I would eagerly buy a 48GB card to accommodate a batch size 1, gradient checkpointed LoRA-trainable model with T5 for conditioning. I want PixArt-a or DeepFloyd&#x2F;IF with the SDXL dataset and training. I get I can achieve so much with SDXL on 24GB, including just barely a fine tuning, I understand the engineering decisions here, but it’s too weak on prompts.<p>On a large scale, I’m willing to spend a little money up front. In those conditions you can be far more innovative, you don’t have to make everything for $0. Shane Carruth didn’t make Primer for $0. I’m sure you’ve seen this movie, you get how astoundingly good it is. But he still spent something. He spent only slightly more than an RTX 6000 Ada.<p>Innovators have budgets. It’s still worth releasing the most powerful possible model for expensive hardware, this is why everyone is talking about Mixtral, but it’s especially true of visual art.</div><br/></div></div><div id="39022372" class="c"><input type="checkbox" id="c-39022372" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39021374">root</a><span>|</span><a href="#39022295">parent</a><span>|</span><a href="#39023254">prev</a><span>|</span><a href="#39021772">next</a><span>|</span><label class="collapse" for="c-39022372">[-]</label><label class="expand" for="c-39022372">[1 more]</label></div><br/><div class="children"><div class="content">Vouch, I finally tried Stable LM 3b zephyr today and I&#x27;m stunned this slipped by. It&#x27;s the only model I&#x27;ve tried that&#x27;s not Mistral 7B that can do RAG. And it can run ~any consumer grade hardware released in last 3 years. I&#x27;m literally stunned it&#x27;s been sitting out since December 8th. I&#x27;ve heard 10x more about Phi-2 than it, and I&#x27;m not sure why.<p>(Official ONNX version, please!! Then you get Transformers.js &#x2F; web &#x2F; I can deploy on every platform from Web to iOS to Windows)<p>re: art, Dalle-3 costs significantly more. XL costs are 1&#x2F;5th of what they were at launch, 0.0002&#x2F;image versus Dalle-3&#x27;s 0.04. And you&#x27;d be surprised how often people are happy with XL -- Dalle-3&#x27;s marginal advantage is mostly text, especially with the excessive filtering of stylistic stuff, and forced prompt rewrites</div><br/></div></div></div></div><div id="39021772" class="c"><input type="checkbox" id="c-39021772" checked=""/><div class="controls bullet"><span class="by">washadjeffmad</span><span>|</span><a href="#39021374">parent</a><span>|</span><a href="#39022295">prev</a><span>|</span><a href="#39022604">next</a><span>|</span><label class="collapse" for="c-39021772">[-]</label><label class="expand" for="c-39021772">[2 more]</label></div><br/><div class="children"><div class="content">Midjourney is decidedly underwhelming if you&#x27;ve spent any time using the expansive tooling and control nets of Stable Diffusion. Yes, it&#x27;s easy to get impressive first gens with MJ, but all of the coolest work and integration happening is using SD.</div><br/><div id="39024248" class="c"><input type="checkbox" id="c-39024248" checked=""/><div class="controls bullet"><span class="by">capybara_2020</span><span>|</span><a href="#39021374">root</a><span>|</span><a href="#39021772">parent</a><span>|</span><a href="#39022604">next</a><span>|</span><label class="collapse" for="c-39024248">[-]</label><label class="expand" for="c-39024248">[1 more]</label></div><br/><div class="children"><div class="content">It depends. For great looking pics that you need to get out quickly MJ does a great job. Especially with its image + text feature. Dalle is also an interesting choice.<p>SDXL and controlnet is odd a lot of the time. 1.5 + controlnet still seem to give quicker and better results.<p>Basically SD atleast seems to be for when you want unique content. MJ&#x2F;Dalle for everything else.</div><br/></div></div></div></div><div id="39022604" class="c"><input type="checkbox" id="c-39022604" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#39021374">parent</a><span>|</span><a href="#39021772">prev</a><span>|</span><a href="#39021693">next</a><span>|</span><label class="collapse" for="c-39022604">[-]</label><label class="expand" for="c-39022604">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>For example Midjourney now seems to have far surpassed them in the image generation front</i><p>Nope. Stable Diffusion with alternative models offers far more customization and control than Midjourney. Midjourney is good for beginners but sucks for experts.</div><br/></div></div><div id="39021693" class="c"><input type="checkbox" id="c-39021693" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#39021374">parent</a><span>|</span><a href="#39022604">prev</a><span>|</span><a href="#39021579">next</a><span>|</span><label class="collapse" for="c-39021693">[-]</label><label class="expand" for="c-39021693">[1 more]</label></div><br/><div class="children"><div class="content">Midjourney has better quality but does not offer any control. Community has done and is still doing a <i>a lot</i> with SD models because they can be played and tinkered with in any way anyone wants to.</div><br/></div></div><div id="39021579" class="c"><input type="checkbox" id="c-39021579" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39021374">parent</a><span>|</span><a href="#39021693">prev</a><span>|</span><a href="#39021748">next</a><span>|</span><label class="collapse" for="c-39021579">[-]</label><label class="expand" for="c-39021579">[3 more]</label></div><br/><div class="children"><div class="content">&gt; one can just quickly swap it out for something else someone else paid to train.<p>That doesn&#x27;t seem to be the case. There are very limited open-source models outside of the small-LLM bubble.</div><br/><div id="39021680" class="c"><input type="checkbox" id="c-39021680" checked=""/><div class="controls bullet"><span class="by">JCM9</span><span>|</span><a href="#39021374">root</a><span>|</span><a href="#39021579">parent</a><span>|</span><a href="#39021748">next</a><span>|</span><label class="collapse" for="c-39021680">[-]</label><label class="expand" for="c-39021680">[2 more]</label></div><br/><div class="children"><div class="content">The open space on small models is a whole other developing angle, but O was referring to the general commoditization of a lot of these models. With rare exception after launch it seems the lifespan of any of these models is rather limited. From a business standpoint that sort of scenario is generally very unattractive and thus was trying to understand if they have some other angle they’re trying to play here to make a viable business out of this. Or the business model can just be get acquired before that matters and let that be someone else’s problem to figure out.</div><br/><div id="39023169" class="c"><input type="checkbox" id="c-39023169" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39021374">root</a><span>|</span><a href="#39021680">parent</a><span>|</span><a href="#39021748">next</a><span>|</span><label class="collapse" for="c-39023169">[-]</label><label class="expand" for="c-39023169">[1 more]</label></div><br/><div class="children"><div class="content">The comoditization of models outside of LLM is delusional. There is no comparable open source image &#x2F; video models to the private ones.</div><br/></div></div></div></div></div></div><div id="39024075" class="c"><input type="checkbox" id="c-39024075" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#39021374">parent</a><span>|</span><a href="#39021748">prev</a><span>|</span><a href="#39020070">next</a><span>|</span><label class="collapse" for="c-39024075">[-]</label><label class="expand" for="c-39024075">[1 more]</label></div><br/><div class="children"><div class="content">We use SD at work because we need more control over the image generation pipeline (and to a lesser extent don’t want extra latency from web APIs).<p>Believe it or not, generating a full image from a prompt is a small slice of the image generation pie. Highly tuned in-painting is key to a number of budding startups.</div><br/></div></div></div></div><div id="39020070" class="c"><input type="checkbox" id="c-39020070" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39021374">prev</a><span>|</span><a href="#39025167">next</a><span>|</span><label class="collapse" for="c-39020070">[-]</label><label class="expand" for="c-39020070">[27 more]</label></div><br/><div class="children"><div class="content">Note that they don&#x27;t compare with deepseek coder 6.7b, which is vastly superior to much bigger coding models. Surpassing codellama 7b is not that big of a deal today.<p>The most impressive thing about these results is how good the 1.3B deepseek coder is.</div><br/><div id="39021104" class="c"><input type="checkbox" id="c-39021104" checked=""/><div class="controls bullet"><span class="by">jyap</span><span>|</span><a href="#39020070">parent</a><span>|</span><a href="#39020533">next</a><span>|</span><label class="collapse" for="c-39021104">[-]</label><label class="expand" for="c-39021104">[13 more]</label></div><br/><div class="children"><div class="content">Deepseek Coder Instruct 6.7b has been my local LLM (M1 series MBP) for a while now and that was my first thought… They selectively chose benchmark results to look impressive (which is typical).<p>I tested out StableLM Zephyr 3B when that came out and it was extremely underwhelming&#x2F;unusable.<p>Based on this, Stable Code 3B doesn’t look to be worth trying out. Guessing if they could put out a 7B model which beat Deepseek Coder 6.7B they would have.</div><br/><div id="39021634" class="c"><input type="checkbox" id="c-39021634" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021104">parent</a><span>|</span><a href="#39024079">next</a><span>|</span><label class="collapse" for="c-39021634">[-]</label><label class="expand" for="c-39021634">[6 more]</label></div><br/><div class="children"><div class="content">Do you know how Deepseek 33b compares to 6.7b? I&#x27;m trying 33b on my (96GB) MacBook just because I have <i>plenty</i> of spare (V)RAM. But I&#x27;ll run the smaller model if the benefits are marginal in other peoples&#x27; experience.</div><br/><div id="39022378" class="c"><input type="checkbox" id="c-39022378" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021634">parent</a><span>|</span><a href="#39021983">next</a><span>|</span><label class="collapse" for="c-39022378">[-]</label><label class="expand" for="c-39022378">[4 more]</label></div><br/><div class="children"><div class="content">The smaller model is great at trivial day-to-day tasks.<p>However, when you ask hard things, it struggles; you can ask the same question 10 times, and only get 1 answer that actually answers the question.<p>...but the larger model is a <i>lot</i> slower.<p>Generally, if you don&#x27;t want to mess around swapping models, stick with the bigger one. It&#x27;s better.<p>However, if you are heavily using it, you&#x27;ll find the speed is a pain in the ass, and when you want a trivial hint like &#x27;how do I do a map statement in kotlin again?&#x27;, you really don&#x27;t need it.<p>What I have setup personally is a little thumbs-up &#x2F; thumbs-down on the suggestions via a custom intellij plugin; if I &#x27;thumbs-down&#x27; a result, it generates a new solution for it.<p>If I &#x27;thumbs-down&#x27; it twice, it swaps to the larger model to generate a solution for it.<p>This kind of &#x27;use ok model for most things and step up to larger model when you start asking hard stuff&#x27; approach scales very nicely for my personal workflow... but, I admit that setting it up was a pain, and I&#x27;m forever pissing around with the plugin code to fix tiny bugs, which I would prefer to be spending doing actual work.<p>So... there&#x27;s not really much tooling out there at the moment to support it, but the best solution really is to use both.<p>If you don&#x27;t want to and just want &#x27;use the best model for everything&#x27;, stick with the bigger one.<p>The larger model is more capable of turning &#x27;here is a description of what I want&#x27; into &#x27;here is code that does it that actually compiles&#x27;.<p>The smaller model is much better at &#x27;I want a code fragment that does X&#x27; -&gt; &#x27;rephrased stack overflow answer&#x27;.</div><br/><div id="39022865" class="c"><input type="checkbox" id="c-39022865" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39022378">parent</a><span>|</span><a href="#39024145">next</a><span>|</span><label class="collapse" for="c-39022865">[-]</label><label class="expand" for="c-39022865">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but the larger model is a lot slower.<p>I found the performance to be very acceptable for 33b 4 bit on a m3 max with 36gb ram (much faster than reading speed)</div><br/><div id="39023136" class="c"><input type="checkbox" id="c-39023136" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39022865">parent</a><span>|</span><a href="#39024145">next</a><span>|</span><label class="collapse" for="c-39023136">[-]</label><label class="expand" for="c-39023136">[1 more]</label></div><br/><div class="children"><div class="content">I’m not sure what to say; responsive fast output is ideal, and the larger model is distinctly slower for me, particularly for long completions (2k tokens) if you’re using a restricted grammar like json output.<p>I’m using an M2 not an M3 though; maybe it’s better for you.<p>I was under the impression quantised results were generally slower too, but I’ve never dug into it (or particularly noticed a difference between q4&#x2F;q5&#x2F;q6).<p>If you find it fast enough to use then go for it~</div><br/></div></div></div></div><div id="39024145" class="c"><input type="checkbox" id="c-39024145" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39022378">parent</a><span>|</span><a href="#39022865">prev</a><span>|</span><a href="#39021983">next</a><span>|</span><label class="collapse" for="c-39024145">[-]</label><label class="expand" for="c-39024145">[1 more]</label></div><br/><div class="children"><div class="content">Do you mind sharing your plugin as a gist?<p>How do you run both models in memory? Two separate processes?</div><br/></div></div></div></div><div id="39021983" class="c"><input type="checkbox" id="c-39021983" checked=""/><div class="controls bullet"><span class="by">jyap</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021634">parent</a><span>|</span><a href="#39022378">prev</a><span>|</span><a href="#39024079">next</a><span>|</span><label class="collapse" for="c-39021983">[-]</label><label class="expand" for="c-39021983">[1 more]</label></div><br/><div class="children"><div class="content">You would want to test it out manually day to day. That’s always the best. Some models can out score but not actually be “better” when you use it.<p>But there is also the benchmarking:
<a href="https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;deepseek-coder">https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;deepseek-coder</a><p>33B Instruct doesn’t beat 6.7B Instruct by much but maybe those % improvements mean more for your usage.<p>I run 6.7B since I have 16GB RAM.<p>Quantization of the model also makes a difference.</div><br/></div></div></div></div><div id="39024079" class="c"><input type="checkbox" id="c-39024079" checked=""/><div class="controls bullet"><span class="by">zwarag</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021104">parent</a><span>|</span><a href="#39021634">prev</a><span>|</span><a href="#39022482">next</a><span>|</span><label class="collapse" for="c-39024079">[-]</label><label class="expand" for="c-39024079">[1 more]</label></div><br/><div class="children"><div class="content">Do you use it inside vscode or how do you integrate an LLM into your IDE?</div><br/></div></div><div id="39022482" class="c"><input type="checkbox" id="c-39022482" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021104">parent</a><span>|</span><a href="#39024079">prev</a><span>|</span><a href="#39021472">next</a><span>|</span><label class="collapse" for="c-39022482">[-]</label><label class="expand" for="c-39022482">[1 more]</label></div><br/><div class="children"><div class="content">How do you make use of it? Do you have it integrated directly into an ide?</div><br/></div></div><div id="39021472" class="c"><input type="checkbox" id="c-39021472" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021104">parent</a><span>|</span><a href="#39022482">prev</a><span>|</span><a href="#39020533">next</a><span>|</span><label class="collapse" for="c-39021472">[-]</label><label class="expand" for="c-39021472">[4 more]</label></div><br/><div class="children"><div class="content">What do you use it for?</div><br/><div id="39021605" class="c"><input type="checkbox" id="c-39021605" checked=""/><div class="controls bullet"><span class="by">jyap</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021472">parent</a><span>|</span><a href="#39020533">next</a><span>|</span><label class="collapse" for="c-39021605">[-]</label><label class="expand" for="c-39021605">[3 more]</label></div><br/><div class="children"><div class="content">Golang grinding Leetcode coding buddy. And general coding buddy PR reviewer.<p>The results are on par or better than ChatGPT 3.5.<p>I often use it to delve deeper such as “is there an alternative way to write this?” Or “how does this code look?”<p>If you have an M-series Mac I recommend trying out LM Studio. Really eye opening and I’m excited to see how things progress.</div><br/><div id="39022231" class="c"><input type="checkbox" id="c-39022231" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021605">parent</a><span>|</span><a href="#39020533">next</a><span>|</span><label class="collapse" for="c-39022231">[-]</label><label class="expand" for="c-39022231">[2 more]</label></div><br/><div class="children"><div class="content">I have GitHub copilot. Is it better than that? Nd if so, in which way?<p>Offline would be one for sure. Cost is another. What else?</div><br/><div id="39022447" class="c"><input type="checkbox" id="c-39022447" checked=""/><div class="controls bullet"><span class="by">jyap</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39022231">parent</a><span>|</span><a href="#39020533">next</a><span>|</span><label class="collapse" for="c-39022447">[-]</label><label class="expand" for="c-39022447">[1 more]</label></div><br/><div class="children"><div class="content">I’ve never used GH Copilot so can’t comment on that.<p>But having everything locally means no privacy or data leak issues.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39020533" class="c"><input type="checkbox" id="c-39020533" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#39020070">parent</a><span>|</span><a href="#39021104">prev</a><span>|</span><a href="#39022294">next</a><span>|</span><label class="collapse" for="c-39020533">[-]</label><label class="expand" for="c-39020533">[5 more]</label></div><br/><div class="children"><div class="content">Deepseek-coder-6.7B really is a quite surprisingly capable model. It&#x27;s easy to give it a spin with ollama via `ollama run deepseek-coder:6.7b`.</div><br/><div id="39020659" class="c"><input type="checkbox" id="c-39020659" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39020533">parent</a><span>|</span><a href="#39022294">next</a><span>|</span><label class="collapse" for="c-39020659">[-]</label><label class="expand" for="c-39020659">[4 more]</label></div><br/><div class="children"><div class="content">Thanks for the tip with ollama</div><br/><div id="39021199" class="c"><input type="checkbox" id="c-39021199" checked=""/><div class="controls bullet"><span class="by">discordance</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39020659">parent</a><span>|</span><a href="#39023471">next</a><span>|</span><label class="collapse" for="c-39021199">[-]</label><label class="expand" for="c-39021199">[1 more]</label></div><br/><div class="children"><div class="content">If you do:<p>1. ollama run deepseek-coder:6.7b<p>2. pip install litellm<p>3. litellm --model deepseek-coder:6.7b<p>You will have a local OpenAI compatible API for it.</div><br/></div></div><div id="39023471" class="c"><input type="checkbox" id="c-39023471" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39020659">parent</a><span>|</span><a href="#39021199">prev</a><span>|</span><a href="#39022294">next</a><span>|</span><label class="collapse" for="c-39023471">[-]</label><label class="expand" for="c-39023471">[2 more]</label></div><br/><div class="children"><div class="content">ollama is actually not a great way to run these models as it makes it difficult to change server parameters and doesn&#x27;t use `mlock` to keep the models in memory.</div><br/><div id="39024133" class="c"><input type="checkbox" id="c-39024133" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39023471">parent</a><span>|</span><a href="#39022294">next</a><span>|</span><label class="collapse" for="c-39024133">[-]</label><label class="expand" for="c-39024133">[1 more]</label></div><br/><div class="children"><div class="content">What do you suggest?</div><br/></div></div></div></div></div></div></div></div><div id="39022294" class="c"><input type="checkbox" id="c-39022294" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#39020070">parent</a><span>|</span><a href="#39020533">prev</a><span>|</span><a href="#39020781">next</a><span>|</span><label class="collapse" for="c-39022294">[-]</label><label class="expand" for="c-39022294">[4 more]</label></div><br/><div class="children"><div class="content">The 1.3b model is amazing for real time code complete, it&#x27;s fast enough to be a better intellisense.<p>Another model you should try is magicoder 6.7b ds (based on deepseek coder). After playing with it for a couple weeks, I think it gives slightly better results than the equivalent deepseek model.<p>Repo <a href="https:&#x2F;&#x2F;github.com&#x2F;ise-uiuc&#x2F;magicoder">https:&#x2F;&#x2F;github.com&#x2F;ise-uiuc&#x2F;magicoder</a><p>Models <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?search=Magicoder-s-ds" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;models?search=Magicoder-s-ds</a></div><br/><div id="39022561" class="c"><input type="checkbox" id="c-39022561" checked=""/><div class="controls bullet"><span class="by">hskalin</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39022294">parent</a><span>|</span><a href="#39020781">next</a><span>|</span><label class="collapse" for="c-39022561">[-]</label><label class="expand" for="c-39022561">[3 more]</label></div><br/><div class="children"><div class="content">How do you use these models with your editor? (E. vscode or Emacs etc)</div><br/><div id="39024285" class="c"><input type="checkbox" id="c-39024285" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39022561">parent</a><span>|</span><a href="#39022890">next</a><span>|</span><label class="collapse" for="c-39024285">[-]</label><label class="expand" for="c-39024285">[1 more]</label></div><br/><div class="children"><div class="content">I run tabby [0] which uses llama.cpp under the hood and they ship a vscode extension [1]. Going above 1.3b, I find the latency too distracting (but the highest end gpu I have nearby is some 16gb rtx quadro card that&#x27;s a couple years old, and usually I&#x27;m running a consumer 8gb card instead).<p>[0] <a href="https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;</a><p>[1] <a href="https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=TabbyML.vscode-tabby" rel="nofollow">https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=TabbyML....</a></div><br/></div></div><div id="39022890" class="c"><input type="checkbox" id="c-39022890" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39022561">parent</a><span>|</span><a href="#39024285">prev</a><span>|</span><a href="#39020781">next</a><span>|</span><label class="collapse" for="c-39022890">[-]</label><label class="expand" for="c-39022890">[1 more]</label></div><br/><div class="children"><div class="content">An easy way is to use an OpenAI compatible server, which means you can use any GPT plugin to integrate with your editor</div><br/></div></div></div></div></div></div><div id="39020781" class="c"><input type="checkbox" id="c-39020781" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39020070">parent</a><span>|</span><a href="#39022294">prev</a><span>|</span><a href="#39025167">next</a><span>|</span><label class="collapse" for="c-39020781">[-]</label><label class="expand" for="c-39020781">[4 more]</label></div><br/><div class="children"><div class="content">This is phenomenal. And runs fast! The 33b version might be my MacBook&#x27;s new coding daily driver.</div><br/><div id="39022975" class="c"><input type="checkbox" id="c-39022975" checked=""/><div class="controls bullet"><span class="by">unshavedyak</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39020781">parent</a><span>|</span><a href="#39021417">next</a><span>|</span><label class="collapse" for="c-39022975">[-]</label><label class="expand" for="c-39022975">[1 more]</label></div><br/><div class="children"><div class="content">How are you using it? I need to find some sane way to use this stuff from Helix&#x2F;terminal..</div><br/></div></div><div id="39021417" class="c"><input type="checkbox" id="c-39021417" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39020781">parent</a><span>|</span><a href="#39022975">prev</a><span>|</span><a href="#39025167">next</a><span>|</span><label class="collapse" for="c-39021417">[-]</label><label class="expand" for="c-39021417">[2 more]</label></div><br/><div class="children"><div class="content">4-bit quantized 33b runs great on a mp pro with m3 max chip</div><br/><div id="39021543" class="c"><input type="checkbox" id="c-39021543" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39020070">root</a><span>|</span><a href="#39021417">parent</a><span>|</span><a href="#39025167">next</a><span>|</span><label class="collapse" for="c-39021543">[-]</label><label class="expand" for="c-39021543">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using the 5-bit quant with llama.cpp and it&#x27;s excellent on my M2 96GB MacBook! Running this model + Mixtral will be fun.</div><br/></div></div></div></div></div></div></div></div><div id="39025167" class="c"><input type="checkbox" id="c-39025167" checked=""/><div class="controls bullet"><span class="by">sytelus</span><span>|</span><a href="#39020070">prev</a><span>|</span><a href="#39021484">next</a><span>|</span><label class="collapse" for="c-39025167">[-]</label><label class="expand" for="c-39025167">[1 more]</label></div><br/><div class="children"><div class="content">Why authors miss to compare with Phi-2?</div><br/></div></div><div id="39021484" class="c"><input type="checkbox" id="c-39021484" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39025167">prev</a><span>|</span><a href="#39019883">next</a><span>|</span><label class="collapse" for="c-39021484">[-]</label><label class="expand" for="c-39021484">[2 more]</label></div><br/><div class="children"><div class="content">&gt; License: Other<p>&gt; Commercial Applications<p>&gt; This model is included in our new Stability AI Membership. Visit our Membership page to take advantage of our commercial Core Model offerings, including SDXL Turbo &amp; Stable Video Diffusion.<p>what exactly is the license lol. can people use this or is this &quot;see dont touch&quot;</div><br/><div id="39023729" class="c"><input type="checkbox" id="c-39023729" checked=""/><div class="controls bullet"><span class="by">neurostimulant</span><span>|</span><a href="#39021484">parent</a><span>|</span><a href="#39019883">next</a><span>|</span><label class="collapse" for="c-39023729">[-]</label><label class="expand" for="c-39023729">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s free for noncommercial use. If you use it in your company, your company should pay the membership fee. afaik most openai competitors also use similar usage restriction (e.g. free for noncommercial or research use, contact us for commercial license).</div><br/></div></div></div></div><div id="39019883" class="c"><input type="checkbox" id="c-39019883" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#39021484">prev</a><span>|</span><a href="#39023083">next</a><span>|</span><label class="collapse" for="c-39019883">[-]</label><label class="expand" for="c-39019883">[9 more]</label></div><br/><div class="children"><div class="content">That is fantastic. I&#x27;m building a small macOS SwiftUI client with llama cpp built in, no server-client model, and it&#x27;s already so useful with models like openhermes chat 7B, and fast.<p>If this opens it to smaller laptops, wow!<p>We truly live in crazy time. The rate of improvement in this field is off the walls.</div><br/><div id="39020165" class="c"><input type="checkbox" id="c-39020165" checked=""/><div class="controls bullet"><span class="by">joshmarlow</span><span>|</span><a href="#39019883">parent</a><span>|</span><a href="#39019911">next</a><span>|</span><label class="collapse" for="c-39020165">[-]</label><label class="expand" for="c-39020165">[6 more]</label></div><br/><div class="children"><div class="content">Not sure if this is where your head is, but I think there&#x27;s a lot of value in integrating LLMs directly into complex software. Jira, Salesforce, maybe K8s - should all have an integrated LLMs that can walk you through how to perform a nuanced task in the software.</div><br/><div id="39021067" class="c"><input type="checkbox" id="c-39021067" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#39019883">root</a><span>|</span><a href="#39020165">parent</a><span>|</span><a href="#39020317">next</a><span>|</span><label class="collapse" for="c-39021067">[-]</label><label class="expand" for="c-39021067">[1 more]</label></div><br/><div class="children"><div class="content">Imagine good error messages, with hints for mitigation and maybe smart retry w&#x2F; mitigations applied.</div><br/></div></div><div id="39020317" class="c"><input type="checkbox" id="c-39020317" checked=""/><div class="controls bullet"><span class="by">dpacmittal</span><span>|</span><a href="#39019883">root</a><span>|</span><a href="#39020165">parent</a><span>|</span><a href="#39021067">prev</a><span>|</span><a href="#39020552">next</a><span>|</span><label class="collapse" for="c-39020317">[-]</label><label class="expand" for="c-39020317">[2 more]</label></div><br/><div class="children"><div class="content">Why would the LLM  walk you through and not just do the nuanced task on its own?</div><br/><div id="39020368" class="c"><input type="checkbox" id="c-39020368" checked=""/><div class="controls bullet"><span class="by">pennomi</span><span>|</span><a href="#39019883">root</a><span>|</span><a href="#39020317">parent</a><span>|</span><a href="#39020552">next</a><span>|</span><label class="collapse" for="c-39020368">[-]</label><label class="expand" for="c-39020368">[1 more]</label></div><br/><div class="children"><div class="content">I assume the human maintains some of the necessary context in their meat memory.</div><br/></div></div></div></div><div id="39020552" class="c"><input type="checkbox" id="c-39020552" checked=""/><div class="controls bullet"><span class="by">debarshri</span><span>|</span><a href="#39019883">root</a><span>|</span><a href="#39020165">parent</a><span>|</span><a href="#39020317">prev</a><span>|</span><a href="#39019911">next</a><span>|</span><label class="collapse" for="c-39020552">[-]</label><label class="expand" for="c-39020552">[2 more]</label></div><br/><div class="children"><div class="content">Walkthrough is generally performed once or not so frequently. It would be a bad investment if you just use it for just this use case</div><br/><div id="39020644" class="c"><input type="checkbox" id="c-39020644" checked=""/><div class="controls bullet"><span class="by">alpaca128</span><span>|</span><a href="#39019883">root</a><span>|</span><a href="#39020552">parent</a><span>|</span><a href="#39019911">next</a><span>|</span><label class="collapse" for="c-39020644">[-]</label><label class="expand" for="c-39020644">[1 more]</label></div><br/><div class="children"><div class="content">A beginner tutorial is also not used frequently by users, but that doesn&#x27;t make it a bad investment. I an LLM can help a lot with getting familiar with the tool it could be pretty valuable, especially after a UI rework etc.</div><br/></div></div></div></div></div></div><div id="39019911" class="c"><input type="checkbox" id="c-39019911" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#39019883">parent</a><span>|</span><a href="#39020165">prev</a><span>|</span><a href="#39022314">next</a><span>|</span><label class="collapse" for="c-39019911">[-]</label><label class="expand" for="c-39019911">[1 more]</label></div><br/><div class="children"><div class="content">That sounds awesome! Can you share any details about how you&#x27;re working with llama cpp? Is it just via the Swift &lt;&gt; C bridge? I&#x27;ve toyed with the idea of doing this, and wonder if you have any pointers before I get started.</div><br/></div></div><div id="39022314" class="c"><input type="checkbox" id="c-39022314" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39019883">parent</a><span>|</span><a href="#39019911">prev</a><span>|</span><a href="#39023083">next</a><span>|</span><label class="collapse" for="c-39022314">[-]</label><label class="expand" for="c-39022314">[1 more]</label></div><br/><div class="children"><div class="content">3b is good for 8gb MacBook Air etc. 7b is slightly too big.<p>Sure these will continue to improve, phi2 is a good base as well</div><br/></div></div></div></div><div id="39023083" class="c"><input type="checkbox" id="c-39023083" checked=""/><div class="controls bullet"><span class="by">alwinaugustin</span><span>|</span><a href="#39019883">prev</a><span>|</span><a href="#39022514">next</a><span>|</span><label class="collapse" for="c-39023083">[-]</label><label class="expand" for="c-39023083">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been experimenting with code-llama extensively on my laptop, and from my experience, it seems that these models are still in their early stages. I primarily utilize them through a Web UI, where they can successfully refactor code given an existing snippet. However, it&#x27;s worth noting that they cannot currently analyze entire codebases or packages, refining them based on the most suitable solutions using the most appropriate algorithms. While these models offer assistance to some extent, there is room for improvement in their ability to handle more complex and comprehensive coding scenarios.</div><br/><div id="39023134" class="c"><input type="checkbox" id="c-39023134" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39023083">parent</a><span>|</span><a href="#39022514">next</a><span>|</span><label class="collapse" for="c-39023134">[-]</label><label class="expand" for="c-39023134">[1 more]</label></div><br/><div class="children"><div class="content">I think there is a decent chance SourceGraph will figure this all out. The most important thing at this point is figuring what context to feed. They can build up a nice graph of a codebase and I expect from there they can put in the best context and then boom.<p>They might also be able to train a model more intelligently by generating training data from said graphs.</div><br/></div></div></div></div><div id="39022514" class="c"><input type="checkbox" id="c-39022514" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#39023083">prev</a><span>|</span><a href="#39020067">next</a><span>|</span><label class="collapse" for="c-39022514">[-]</label><label class="expand" for="c-39022514">[1 more]</label></div><br/><div class="children"><div class="content">How are people using codellama and this in their workflows?<p>I found one option: <a href="https:&#x2F;&#x2F;github.com&#x2F;xNul&#x2F;code-llama-for-vscode">https:&#x2F;&#x2F;github.com&#x2F;xNul&#x2F;code-llama-for-vscode</a><p>But I&#x27;m guessing there are others, and they might differ in how they provide context to the model.</div><br/></div></div><div id="39020067" class="c"><input type="checkbox" id="c-39020067" checked=""/><div class="controls bullet"><span class="by">knicholes</span><span>|</span><a href="#39022514">prev</a><span>|</span><a href="#39021015">next</a><span>|</span><label class="collapse" for="c-39020067">[-]</label><label class="expand" for="c-39020067">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve got a machine with 4 3090s-- Anyone know which model would perform the best for programming?  It&#x27;s great this can run on a machine w&#x2F;out a graphics card and is only 3B params, but I have the hardware.  Might as well use it.</div><br/><div id="39020410" class="c"><input type="checkbox" id="c-39020410" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39020067">parent</a><span>|</span><a href="#39020436">next</a><span>|</span><label class="collapse" for="c-39020410">[-]</label><label class="expand" for="c-39020410">[1 more]</label></div><br/><div class="children"><div class="content">AFAIK deepseek coder family are the best open coding models.<p>I haven&#x27;t tested, but I think deepseek coder 33b can run in a single RTX 3090 when 4-bit quantized. In your case you might be able to run the non quantized version</div><br/></div></div><div id="39020436" class="c"><input type="checkbox" id="c-39020436" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39020067">parent</a><span>|</span><a href="#39020410">prev</a><span>|</span><a href="#39020224">next</a><span>|</span><label class="collapse" for="c-39020436">[-]</label><label class="expand" for="c-39020436">[1 more]</label></div><br/><div class="children"><div class="content">The coding models are all small because speed is crucial. If you need to wait 2 seconds for an autocomplete it becomes near useless.</div><br/></div></div><div id="39020224" class="c"><input type="checkbox" id="c-39020224" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39020067">parent</a><span>|</span><a href="#39020436">prev</a><span>|</span><a href="#39022735">next</a><span>|</span><label class="collapse" for="c-39020224">[-]</label><label class="expand" for="c-39020224">[3 more]</label></div><br/><div class="children"><div class="content">Here is a leader board of some models<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mike-ravkine&#x2F;can-ai-code-results" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mike-ravkine&#x2F;can-ai-code-resul...</a><p>Don&#x27;t know how biased this leaderboard is, but I guess you could just give some of them a try and see for yourself.</div><br/><div id="39020606" class="c"><input type="checkbox" id="c-39020606" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#39020067">root</a><span>|</span><a href="#39020224">parent</a><span>|</span><a href="#39022735">next</a><span>|</span><label class="collapse" for="c-39020606">[-]</label><label class="expand" for="c-39020606">[2 more]</label></div><br/><div class="children"><div class="content">This is a much better leaderboard: <a href="https:&#x2F;&#x2F;evalplus.github.io&#x2F;leaderboard.html" rel="nofollow">https:&#x2F;&#x2F;evalplus.github.io&#x2F;leaderboard.html</a><p>I&#x27;ve seen the CanAiCode leaderboard several times (and used many of the models listed), but I wouldn&#x27;t use it to pick a model. It&#x27;s not a bad list, but the benchmark is too limited. The results are not accurately ranked from best to worst.<p>For example the deepseek 33b model is ranked 5 spots lower than the 6.7b model, but the 33b model is definitely better. WizardCoder 15b is near the top while WizardCoder 33b is ranked 26 spots lower, which is a wildly inaccurate ranking.<p>It&#x27;s worth noting that those 33b models score in the 70s for HumanEval and HumanEval+ while the 15b model scores in the 50s.</div><br/><div id="39021906" class="c"><input type="checkbox" id="c-39021906" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39020067">root</a><span>|</span><a href="#39020606">parent</a><span>|</span><a href="#39022735">next</a><span>|</span><label class="collapse" for="c-39021906">[-]</label><label class="expand" for="c-39021906">[1 more]</label></div><br/><div class="children"><div class="content">Thanks</div><br/></div></div></div></div></div></div><div id="39022735" class="c"><input type="checkbox" id="c-39022735" checked=""/><div class="controls bullet"><span class="by">mlboss</span><span>|</span><a href="#39020067">parent</a><span>|</span><a href="#39020224">prev</a><span>|</span><a href="#39022308">next</a><span>|</span><label class="collapse" for="c-39022735">[-]</label><label class="expand" for="c-39022735">[1 more]</label></div><br/><div class="children"><div class="content">Did you build a machine with 4x 3090 ? I looking for a way to build such a machine for ML training.</div><br/></div></div><div id="39022308" class="c"><input type="checkbox" id="c-39022308" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#39020067">parent</a><span>|</span><a href="#39022735">prev</a><span>|</span><a href="#39021015">next</a><span>|</span><label class="collapse" for="c-39022308">[-]</label><label class="expand" for="c-39022308">[1 more]</label></div><br/><div class="children"><div class="content">Try mistral 8x7b, which some human evals place above gpt-3.5 and you have enough VRAM and compute to make training a LORA either on your own dataset, or one of the freely available datasets on huggingface worthwhile, or at least interesting</div><br/></div></div></div></div><div id="39021015" class="c"><input type="checkbox" id="c-39021015" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#39020067">prev</a><span>|</span><a href="#39021681">next</a><span>|</span><label class="collapse" for="c-39021015">[-]</label><label class="expand" for="c-39021015">[6 more]</label></div><br/><div class="children"><div class="content">Jargon naivete question: isn&#x27;t &quot;on the edge&quot; normally implying on a server side with minimal routers hops to the client, not on client side?</div><br/><div id="39021077" class="c"><input type="checkbox" id="c-39021077" checked=""/><div class="controls bullet"><span class="by">devindotcom</span><span>|</span><a href="#39021015">parent</a><span>|</span><a href="#39021143">next</a><span>|</span><label class="collapse" for="c-39021077">[-]</label><label class="expand" for="c-39021077">[4 more]</label></div><br/><div class="children"><div class="content">afaik &quot;edge&quot; nearly always means taking place on the device a user is interacting with. no server involved except perhaps as authentication etc. but there is probably some other situation where &quot;edge&quot; could mean local infra or caching.</div><br/><div id="39022924" class="c"><input type="checkbox" id="c-39022924" checked=""/><div class="controls bullet"><span class="by">WatchDog</span><span>|</span><a href="#39021015">root</a><span>|</span><a href="#39021077">parent</a><span>|</span><a href="#39021143">next</a><span>|</span><label class="collapse" for="c-39022924">[-]</label><label class="expand" for="c-39022924">[3 more]</label></div><br/><div class="children"><div class="content">I think the etymology of “edge computing” is derived from “network edge”, ie the outer shell of some network&#x2F;autonomous system.<p>The closest point within your control that interfaces with devices outside of your control.<p>Seeing the term get used to describe client devices themselves kinda muddies the terminology.</div><br/><div id="39023551" class="c"><input type="checkbox" id="c-39023551" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#39021015">root</a><span>|</span><a href="#39022924">parent</a><span>|</span><a href="#39021143">next</a><span>|</span><label class="collapse" for="c-39023551">[-]</label><label class="expand" for="c-39023551">[2 more]</label></div><br/><div class="children"><div class="content">Agreed.<p>(autocorrect style typo above: etymology)</div><br/><div id="39023567" class="c"><input type="checkbox" id="c-39023567" checked=""/><div class="controls bullet"><span class="by">WatchDog</span><span>|</span><a href="#39021015">root</a><span>|</span><a href="#39023551">parent</a><span>|</span><a href="#39021143">next</a><span>|</span><label class="collapse" for="c-39023567">[-]</label><label class="expand" for="c-39023567">[1 more]</label></div><br/><div class="children"><div class="content">fixed</div><br/></div></div></div></div></div></div></div></div><div id="39021143" class="c"><input type="checkbox" id="c-39021143" checked=""/><div class="controls bullet"><span class="by">xer0x</span><span>|</span><a href="#39021015">parent</a><span>|</span><a href="#39021077">prev</a><span>|</span><a href="#39021681">next</a><span>|</span><label class="collapse" for="c-39021143">[-]</label><label class="expand" for="c-39021143">[1 more]</label></div><br/><div class="children"><div class="content">+1 yes, for a service using network caching like using Cloudflare. I would&#x27;ve referred to their CDN as the Edge of our network.</div><br/></div></div></div></div><div id="39021681" class="c"><input type="checkbox" id="c-39021681" checked=""/><div class="controls bullet"><span class="by">outcoldman</span><span>|</span><a href="#39021015">prev</a><span>|</span><a href="#39019974">next</a><span>|</span><label class="collapse" for="c-39021681">[-]</label><label class="expand" for="c-39021681">[1 more]</label></div><br/><div class="children"><div class="content">I was able to run this model in <a href="http:&#x2F;&#x2F;lmstudio.ai" rel="nofollow">http:&#x2F;&#x2F;lmstudio.ai</a> as well. Just remove Compatibility Guess in Filters, so you can see all the models. LM Studio can load it and run requests against it.</div><br/></div></div><div id="39019974" class="c"><input type="checkbox" id="c-39019974" checked=""/><div class="controls bullet"><span class="by">lfkdev</span><span>|</span><a href="#39021681">prev</a><span>|</span><a href="#39020087">next</a><span>|</span><label class="collapse" for="c-39019974">[-]</label><label class="expand" for="c-39019974">[8 more]</label></div><br/><div class="children"><div class="content">How is this compared to the current GitHub Copilot?</div><br/><div id="39020078" class="c"><input type="checkbox" id="c-39020078" checked=""/><div class="controls bullet"><span class="by">brianjking</span><span>|</span><a href="#39019974">parent</a><span>|</span><a href="#39020226">next</a><span>|</span><label class="collapse" for="c-39020078">[-]</label><label class="expand" for="c-39020078">[1 more]</label></div><br/><div class="children"><div class="content">A 3B tiny model is not going to compare to GitHub copilot. However, there are plenty of nice 7B models that are excellent at code and I encourage you to try them out.</div><br/></div></div><div id="39020226" class="c"><input type="checkbox" id="c-39020226" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39019974">parent</a><span>|</span><a href="#39020078">prev</a><span>|</span><a href="#39020087">next</a><span>|</span><label class="collapse" for="c-39020226">[-]</label><label class="expand" for="c-39020226">[6 more]</label></div><br/><div class="children"><div class="content">If you just want to get stuff done, use the best tools like a Milwaukee Drill - and right now, thats copilot&#x2F;gpt-4.<p>If you don&#x27;t want to be tied to a company and like opensource, feel free to connect a toy motor to an AA battery to drill your holes...    Or to use Llama&#x2F;Stable Code 3B.</div><br/><div id="39020711" class="c"><input type="checkbox" id="c-39020711" checked=""/><div class="controls bullet"><span class="by">irthomasthomas</span><span>|</span><a href="#39019974">root</a><span>|</span><a href="#39020226">parent</a><span>|</span><a href="#39020699">next</a><span>|</span><label class="collapse" for="c-39020711">[-]</label><label class="expand" for="c-39020711">[3 more]</label></div><br/><div class="children"><div class="content">Openai just invisibly dropped my API requests to a lower model with a 4k context limit. And my commit scripts started failing for being over the context limit. It&#x27;s buried in the docs somewhere that low tier api users will be served on lower models during peek times.<p>So,I guess they&#x27;re like a Milwaukee Drill that will sometimes refuse to work unless you buy more drill credits.</div><br/><div id="39021053" class="c"><input type="checkbox" id="c-39021053" checked=""/><div class="controls bullet"><span class="by">drzaiusx11</span><span>|</span><a href="#39019974">root</a><span>|</span><a href="#39020711">parent</a><span>|</span><a href="#39020853">next</a><span>|</span><label class="collapse" for="c-39021053">[-]</label><label class="expand" for="c-39021053">[1 more]</label></div><br/><div class="children"><div class="content">More like a Milwaukee drill you have on loan that can be swapped out for a manual screwdriver without warning.</div><br/></div></div><div id="39020853" class="c"><input type="checkbox" id="c-39020853" checked=""/><div class="controls bullet"><span class="by">nulld3v</span><span>|</span><a href="#39019974">root</a><span>|</span><a href="#39020711">parent</a><span>|</span><a href="#39021053">prev</a><span>|</span><a href="#39020699">next</a><span>|</span><label class="collapse" for="c-39020853">[-]</label><label class="expand" for="c-39020853">[1 more]</label></div><br/><div class="children"><div class="content">WTF? Do you have a link? I was not aware of this, it would be crazy if true.</div><br/></div></div></div></div><div id="39020699" class="c"><input type="checkbox" id="c-39020699" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#39019974">root</a><span>|</span><a href="#39020226">parent</a><span>|</span><a href="#39020711">prev</a><span>|</span><a href="#39020331">next</a><span>|</span><label class="collapse" for="c-39020699">[-]</label><label class="expand" for="c-39020699">[1 more]</label></div><br/><div class="children"><div class="content">You clearly have never used these other tools. Mixtral &#x2F; Deepseek perform very well on coding challenges. I&#x27;ve used them against local code without issues, sometimes they are a bit optimistic and produce too much, but thats far better than producing too little (like GPT4 does).</div><br/></div></div><div id="39020331" class="c"><input type="checkbox" id="c-39020331" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#39019974">root</a><span>|</span><a href="#39020226">parent</a><span>|</span><a href="#39020699">prev</a><span>|</span><a href="#39020087">next</a><span>|</span><label class="collapse" for="c-39020331">[-]</label><label class="expand" for="c-39020331">[1 more]</label></div><br/><div class="children"><div class="content">it’s going to be real hard to pry the carburetors out of this guy’s cold dead hands!</div><br/></div></div></div></div></div></div><div id="39020087" class="c"><input type="checkbox" id="c-39020087" checked=""/><div class="controls bullet"><span class="by">connorgutman</span><span>|</span><a href="#39019974">prev</a><span>|</span><a href="#39019922">next</a><span>|</span><label class="collapse" for="c-39020087">[-]</label><label class="expand" for="c-39020087">[3 more]</label></div><br/><div class="children"><div class="content">FYI: This model is already available on Ollama.</div><br/><div id="39020622" class="c"><input type="checkbox" id="c-39020622" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#39020087">parent</a><span>|</span><a href="#39019922">next</a><span>|</span><label class="collapse" for="c-39020622">[-]</label><label class="expand" for="c-39020622">[2 more]</label></div><br/><div class="children"><div class="content">how do you check that ?</div><br/><div id="39020724" class="c"><input type="checkbox" id="c-39020724" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#39020087">root</a><span>|</span><a href="#39020622">parent</a><span>|</span><a href="#39019922">next</a><span>|</span><label class="collapse" for="c-39020724">[-]</label><label class="expand" for="c-39020724">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library?sort=newest">https:&#x2F;&#x2F;ollama.ai&#x2F;library?sort=newest</a></div><br/></div></div></div></div></div></div><div id="39019922" class="c"><input type="checkbox" id="c-39019922" checked=""/><div class="controls bullet"><span class="by">artninja1988</span><span>|</span><a href="#39020087">prev</a><span>|</span><a href="#39019939">next</a><span>|</span><label class="collapse" for="c-39019922">[-]</label><label class="expand" for="c-39019922">[4 more]</label></div><br/><div class="children"><div class="content">Given the complete failure of the first stable lm, I&#x27;m interested to try this one out. Haven&#x27;t really seen a small language model, except mixtral 7b that&#x27;s really useful for much.<p>I also hope stability comes out with a competitor to the new midjourney and dalle models! That&#x27;s what put them on the map in the first place</div><br/><div id="39022330" class="c"><input type="checkbox" id="c-39022330" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39019922">parent</a><span>|</span><a href="#39020304">next</a><span>|</span><label class="collapse" for="c-39022330">[-]</label><label class="expand" for="c-39022330">[1 more]</label></div><br/><div class="children"><div class="content">We released a competitor to runway recently that beat it on blind tests, plus way faster image in sdxl turbo<p>We have been working on ComfyUI for the next step and new image models<p>Midjourney and others are pipelines versus models so we have a higher bar to jump but the og stable diffusion team are working hard!</div><br/></div></div><div id="39020304" class="c"><input type="checkbox" id="c-39020304" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39019922">parent</a><span>|</span><a href="#39022330">prev</a><span>|</span><a href="#39020097">next</a><span>|</span><label class="collapse" for="c-39020304">[-]</label><label class="expand" for="c-39020304">[1 more]</label></div><br/><div class="children"><div class="content">Deepseek coder 6.7B is very useful  for coding and can run in consumer GPUs.<p>I use the 6bit GGUF quantized version on a laptop RTX 3070</div><br/></div></div><div id="39020097" class="c"><input type="checkbox" id="c-39020097" checked=""/><div class="controls bullet"><span class="by">brianjking</span><span>|</span><a href="#39019922">parent</a><span>|</span><a href="#39020304">prev</a><span>|</span><a href="#39019939">next</a><span>|</span><label class="collapse" for="c-39020097">[-]</label><label class="expand" for="c-39020097">[1 more]</label></div><br/><div class="children"><div class="content">All of the Mistral versions have been excellent, including the OpenHermes versions. I encourage you to check out Phi-2 as well, it&#x27;s the only 3b model I&#x27;ve found really quite interesting outside of Replit&#x27;s code model built into Replit Core.</div><br/></div></div></div></div><div id="39019939" class="c"><input type="checkbox" id="c-39019939" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#39019922">prev</a><span>|</span><a href="#39021174">next</a><span>|</span><label class="collapse" for="c-39019939">[-]</label><label class="expand" for="c-39019939">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s amazing to see more smaller models being released. This creates opportunities for more developers to run it on their local computers, and makes it easier to fine-tune for specific needs.</div><br/><div id="39019985" class="c"><input type="checkbox" id="c-39019985" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#39019939">parent</a><span>|</span><a href="#39021174">next</a><span>|</span><label class="collapse" for="c-39019985">[-]</label><label class="expand" for="c-39019985">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone tried starting with a smaller modeling, then RLing until it improves to the bigger model?</div><br/></div></div></div></div><div id="39021174" class="c"><input type="checkbox" id="c-39021174" checked=""/><div class="controls bullet"><span class="by">herval</span><span>|</span><a href="#39019939">prev</a><span>|</span><a href="#39020018">next</a><span>|</span><label class="collapse" for="c-39021174">[-]</label><label class="expand" for="c-39021174">[5 more]</label></div><br/><div class="children"><div class="content">Can anyone explain what’s Stability’s business model (or plan for one)?<p>I get why Meta releases tons of models, but still can’t quite understand what stability is trying to achieve</div><br/><div id="39021250" class="c"><input type="checkbox" id="c-39021250" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#39021174">parent</a><span>|</span><a href="#39021208">next</a><span>|</span><label class="collapse" for="c-39021250">[-]</label><label class="expand" for="c-39021250">[2 more]</label></div><br/><div class="children"><div class="content">Seems like the standard open-core playbook:<p>&gt; This model is included in our new Stability AI Membership. Visit our Membership page to take advantage of our commercial Core Model offerings, including SDXL Turbo &amp; Stable Video Diffusion.<p>A hypothetical Stable Code 13B&#x2F;70B could be hosted only, with more languages or specialized use-cases (Stable Code 3B iOS-Swift-Turbo)</div><br/><div id="39022347" class="c"><input type="checkbox" id="c-39022347" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39021174">root</a><span>|</span><a href="#39021250">parent</a><span>|</span><a href="#39021208">next</a><span>|</span><label class="collapse" for="c-39022347">[-]</label><label class="expand" for="c-39022347">[1 more]</label></div><br/><div class="children"><div class="content">Membership with upsell to support, custom models and more<p>Plus licensed variant models like stable audio and on chip installation like arm for specialist models eg Japanese law or Indonesian accounting</div><br/></div></div></div></div><div id="39021208" class="c"><input type="checkbox" id="c-39021208" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#39021174">parent</a><span>|</span><a href="#39021250">prev</a><span>|</span><a href="#39020018">next</a><span>|</span><label class="collapse" for="c-39021208">[-]</label><label class="expand" for="c-39021208">[2 more]</label></div><br/><div class="children"><div class="content">to be bought by meta</div><br/><div id="39021674" class="c"><input type="checkbox" id="c-39021674" checked=""/><div class="controls bullet"><span class="by">bogwog</span><span>|</span><a href="#39021174">root</a><span>|</span><a href="#39021208">parent</a><span>|</span><a href="#39020018">next</a><span>|</span><label class="collapse" for="c-39021674">[-]</label><label class="expand" for="c-39021674">[1 more]</label></div><br/><div class="children"><div class="content">This is all an elaborate mating ritual</div><br/></div></div></div></div></div></div><div id="39020018" class="c"><input type="checkbox" id="c-39020018" checked=""/><div class="controls bullet"><span class="by">photon_collider</span><span>|</span><a href="#39021174">prev</a><span>|</span><a href="#39020161">next</a><span>|</span><label class="collapse" for="c-39020018">[-]</label><label class="expand" for="c-39020018">[2 more]</label></div><br/><div class="children"><div class="content">How reliable are these benchmarks?</div><br/><div id="39020119" class="c"><input type="checkbox" id="c-39020119" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#39020018">parent</a><span>|</span><a href="#39020161">next</a><span>|</span><label class="collapse" for="c-39020119">[-]</label><label class="expand" for="c-39020119">[1 more]</label></div><br/><div class="children"><div class="content">I think the trick is that they are just comparing to other tiny models.<p>None of the little models, including this one, are comparable to the performance of the larger models for any significant coding problem.<p>I think what these are useful for is mostly giving people hints inside of a code editor. Occasionally filling in the blank.</div><br/></div></div></div></div><div id="39020161" class="c"><input type="checkbox" id="c-39020161" checked=""/><div class="controls bullet"><span class="by">akulbe</span><span>|</span><a href="#39020018">prev</a><span>|</span><a href="#39022232">next</a><span>|</span><label class="collapse" for="c-39020161">[-]</label><label class="expand" for="c-39020161">[7 more]</label></div><br/><div class="children"><div class="content">I just tried this model with Koboldcpp on my LLM box. I got gibberish back.<p>My prompt - &quot;please show me how to write a web scraper in Python&quot;<p>The response?<p>&lt;blockquote&gt;
I&#x27;ve written my first ever python script about 5 months ago and I really don&#x27;t remember anything except for the fact that I used Selenium in order to scrape websites (in this case, Google). So you can probably just copy&#x2F;paste all of these lines from your own Python code which contains logic to determine what value should be returned when called by another piece of software or program.
&lt;&#x2F;blockquote&gt;</div><br/><div id="39020288" class="c"><input type="checkbox" id="c-39020288" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39020161">parent</a><span>|</span><a href="#39020282">next</a><span>|</span><label class="collapse" for="c-39020288">[-]</label><label class="expand" for="c-39020288">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very likely a &quot;completion model&quot; and not instruct&#x2F;chat fine-tuned.<p>So you&#x27;d need to prompt it through comments or by starting with a function name, basically the same as one would prompt GitHub copilot.<p>e.g.<p><pre><code>  # the following code implements a webscraper in python
  class WebScraper:

</code></pre>
(I didn&#x27;t try this, and I&#x27;m not good at prompting, but something along the lines of this example should yield better results)</div><br/></div></div><div id="39020282" class="c"><input type="checkbox" id="c-39020282" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#39020161">parent</a><span>|</span><a href="#39020288">prev</a><span>|</span><a href="#39020248">next</a><span>|</span><label class="collapse" for="c-39020282">[-]</label><label class="expand" for="c-39020282">[1 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s a code completion model, not a chat&#x2F;instruct one.</div><br/></div></div><div id="39020248" class="c"><input type="checkbox" id="c-39020248" checked=""/><div class="controls bullet"><span class="by">endofreach</span><span>|</span><a href="#39020161">parent</a><span>|</span><a href="#39020282">prev</a><span>|</span><a href="#39020316">next</a><span>|</span><label class="collapse" for="c-39020248">[-]</label><label class="expand" for="c-39020248">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t seem like gibberish though?</div><br/></div></div><div id="39020316" class="c"><input type="checkbox" id="c-39020316" checked=""/><div class="controls bullet"><span class="by">MrNeon</span><span>|</span><a href="#39020161">parent</a><span>|</span><a href="#39020248">prev</a><span>|</span><a href="#39020287">next</a><span>|</span><label class="collapse" for="c-39020316">[-]</label><label class="expand" for="c-39020316">[1 more]</label></div><br/><div class="children"><div class="content">It is weird that it is not mentioned in the model card but I&#x27;m pretty sure it is a completion model, not tuned as an instruct model.<p>edit: the webpage does call it &quot;Stable Code Completion&quot;</div><br/></div></div><div id="39020257" class="c"><input type="checkbox" id="c-39020257" checked=""/><div class="controls bullet"><span class="by">connorgutman</span><span>|</span><a href="#39020161">parent</a><span>|</span><a href="#39020287">prev</a><span>|</span><a href="#39022232">next</a><span>|</span><label class="collapse" for="c-39020257">[-]</label><label class="expand" for="c-39020257">[1 more]</label></div><br/><div class="children"><div class="content">Same thing with Ollama.</div><br/></div></div></div></div><div id="39020116" class="c"><input type="checkbox" id="c-39020116" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#39022239">prev</a><span>|</span><label class="collapse" for="c-39020116">[-]</label><label class="expand" for="c-39020116">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s quite amazing - I often find that I read quite positive comments towards LLM tools for coding. Yet, an &quot;Ask HN&quot; I posted a while ago (and which admittedly didn&#x27;t gain much traction) seemed to mirror mostly negative&#x2F;pessimistic responses.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38803836">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38803836</a><p>Was it just that my submission didn&#x27;t find enough &#x2F; more balanced commenters?</div><br/><div id="39020392" class="c"><input type="checkbox" id="c-39020392" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#39020116">parent</a><span>|</span><a href="#39020412">next</a><span>|</span><label class="collapse" for="c-39020392">[-]</label><label class="expand" for="c-39020392">[1 more]</label></div><br/><div class="children"><div class="content">You got two positive and two negative responses. You replied only to the negative responses. Now you think that the responses were mostly negative. I blame salience bias.<p>Anyways, there&#x27;s also a difference between &quot;are you excited about this new thing becoming available&quot; and &quot;now that you&#x27;ve used it, do you like the experience&quot;. The former is more likely to feature rosy expectations and the latter bitter disappointment. (Though it could also be the other way around, with people dismissing it at first and then discovering that it&#x27;s kind of nice actually.)</div><br/></div></div><div id="39020412" class="c"><input type="checkbox" id="c-39020412" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39020116">parent</a><span>|</span><a href="#39020392">prev</a><span>|</span><a href="#39020349">next</a><span>|</span><label class="collapse" for="c-39020412">[-]</label><label class="expand" for="c-39020412">[1 more]</label></div><br/><div class="children"><div class="content">The precise wording matters.<p>How has it changed your work life leads people down the rabbit hole of will coding jobs be safe.<p>This one is a lot more neutral&#x2F;technical.</div><br/></div></div><div id="39020349" class="c"><input type="checkbox" id="c-39020349" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39020116">parent</a><span>|</span><a href="#39020412">prev</a><span>|</span><label class="collapse" for="c-39020349">[-]</label><label class="expand" for="c-39020349">[1 more]</label></div><br/><div class="children"><div class="content">You only got comments from six people so yeah, definitely not representative.</div><br/></div></div></div></div></div></div></div></div></div></body></html>